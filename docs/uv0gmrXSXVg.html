<html><head><title>Lesson 10: Cutting Edge Deep Learning for Coders</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 10: Cutting Edge Deep Learning for Coders</h2><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg"><img src="https://i.ytimg.com/vi_webp/uv0gmrXSXVg/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=60">1:0</a> Slobs Youve Wonder<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=275">4:35</a> Overshoot<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=460">7:40</a> Study Groups<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=555">9:15</a> Last Week Recap<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=640">10:40</a> Resize Images<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=763">12:43</a> Center Cropping<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=902">15:2</a> Parallel Processing<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1059">17:39</a> General Approach<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1143">19:3</a> Append Image<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1210">20:10</a> Threading Local<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1374">22:54</a> Results<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1598">26:38</a> Preprocessing<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1660">27:40</a> Finetuning<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2319">38:39</a> Big Holes<br><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2627">43:47</a> Linear Model<br><br><div style="text-align: left;"><a href="./uv0gmrXSXVg.html">Whisper Transcript</a> | <a href="./transcript_uv0gmrXSXVg.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Some really fun stuff appearing on the forums this week, and one of the really great projects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=12" target="_blank">00:00:12.600</a></span> | <span class="t">which was created by I believe our sole Bulgarian participant in the course, Slav Ivanov, wrote</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=19" target="_blank">00:00:19.080</a></span> | <span class="t">a great post about picking an optimizer for style transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=24" target="_blank">00:00:24.380</a></span> | <span class="t">This post came from a forum discussion in which I made an off-hand remark about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=31" target="_blank">00:00:31.800</a></span> | <span class="t">I know that in theory BFGS is a deterministic optimizer, it uses a line search, it approximates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=39" target="_blank">00:00:39.360</a></span> | <span class="t">the Hessian, it ought to work on this kind of deterministic problem better, but I hadn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=44" target="_blank">00:00:44.600</a></span> | <span class="t">tried it myself and I hadn't seen anybody try it and so maybe somebody should try it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=49" target="_blank">00:00:49.480</a></span> | <span class="t">I don't know if you've noticed, but pretty much every week I say something like that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=52" target="_blank">00:00:52.480</a></span> | <span class="t">a number of times and every time I do I'm always hoping that somebody might go, "Oh,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=56" target="_blank">00:00:56.000</a></span> | <span class="t">I wonder as well."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=57" target="_blank">00:00:57.000</a></span> | <span class="t">And so Slav did wonder and so he posted a really interesting blog post about that exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=64" target="_blank">00:01:04.200</a></span> | <span class="t">question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=66" target="_blank">00:01:06.720</a></span> | <span class="t">I was thrilled to see that the blog post got a lot of pick-up on the machine learning Reddit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=76" target="_blank">00:01:16.080</a></span> | <span class="t">It got 55 upvotes which for that subreddit is put in second place on the front page.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=85" target="_blank">00:01:25.240</a></span> | <span class="t">It also got picked up by the WildML mailing list weekly summary of interesting things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=94" target="_blank">00:01:34.560</a></span> | <span class="t">in AI as the second post that was listed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=99" target="_blank">00:01:39.600</a></span> | <span class="t">So that was great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=103" target="_blank">00:01:43.360</a></span> | <span class="t">For those of you that have looked at it and kind of wondered what is it about this post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=107" target="_blank">00:01:47.160</a></span> | <span class="t">that causes it to get noticed whereas other ones don't, I'm not sure I know the secret,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=114" target="_blank">00:01:54.160</a></span> | <span class="t">but as soon as I read it I kind of thought, "Okay, I think a lot of people are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=116" target="_blank">00:01:56.840</a></span> | <span class="t">read this."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=117" target="_blank">00:01:57.840</a></span> | <span class="t">It gives some background, it assumes an intelligent reader, but it assumes an intelligent reader</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=123" target="_blank">00:02:03.600</a></span> | <span class="t">who doesn't necessarily know all about this, something like you guys six months ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=130" target="_blank">00:02:10.760</a></span> | <span class="t">And so it describes this is what it is and this is where this kind of thing is used and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=136" target="_blank">00:02:16.120</a></span> | <span class="t">gives some examples and then goes ahead and sets up the question of different optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=145" target="_blank">00:02:25.320</a></span> | <span class="t">algorithms and then shows lots of examples of both learning curves as well as pictures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=153" target="_blank">00:02:33.140</a></span> | <span class="t">that come out of these different experiments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=156" target="_blank">00:02:36.560</a></span> | <span class="t">And I think hopefully it's been a great experience for Slav as well because in the Reddit thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=162" target="_blank">00:02:42.120</a></span> | <span class="t">there's all kinds of folks pointing out other things that he could try, questions that weren't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=169" target="_blank">00:02:49.760</a></span> | <span class="t">quite clear, and so now there's a whole kind of, actually kind of summarized in that thread</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=175" target="_blank">00:02:55.640</a></span> | <span class="t">a list of things that perhaps could be done next to open up a whole interesting question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=183" target="_blank">00:03:03.480</a></span> | <span class="t">Another post which I'm not even sure if it's officially posted yet, but I got the early</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=189" target="_blank">00:03:09.200</a></span> | <span class="t">bird version from Brad, is this crazy thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=194" target="_blank">00:03:14.200</a></span> | <span class="t">Here is Kanye drawn using a brush of Captain Jean-Luc Picard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=199" target="_blank">00:03:19.000</a></span> | <span class="t">In case you're wondering, is that really him, I will show you his zoomed in version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=206" target="_blank">00:03:26.520</a></span> | <span class="t">It really is Jean-Luc Picard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=210" target="_blank">00:03:30.640</a></span> | <span class="t">And this is a really interesting idea because he points out that generally speaking when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=216" target="_blank">00:03:36.600</a></span> | <span class="t">you try to use a non-artwork as your style image, it doesn't actually give very good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=225" target="_blank">00:03:45.200</a></span> | <span class="t">results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=226" target="_blank">00:03:46.200</a></span> | <span class="t">It's another example of a non-artwork, it doesn't give good results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=232" target="_blank">00:03:52.800</a></span> | <span class="t">It's kind of interesting, but it's not quite what he was looking for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=236" target="_blank">00:03:56.200</a></span> | <span class="t">But if you tile it, you totally get it, so here's Kanye using a Nintendo game controller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=246" target="_blank">00:04:06.880</a></span> | <span class="t">brush.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=247" target="_blank">00:04:07.880</a></span> | <span class="t">So then he tried out this Jean-Luc Picard and got okay results and kind of realized that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=256" target="_blank">00:04:16.760</a></span> | <span class="t">actually the size of the texture is pretty critical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=260" target="_blank">00:04:20.480</a></span> | <span class="t">And I've never seen anybody do this before, so I think when this image gets shared on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=268" target="_blank">00:04:28.100</a></span> | <span class="t">Twitter it's going to go everywhere because it's just the freakiest thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=273" target="_blank">00:04:33.040</a></span> | <span class="t">Freaky is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=276" target="_blank">00:04:36.520</a></span> | <span class="t">So I think I warned you guys about your projects when I first mentioned them as being something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=282" target="_blank">00:04:42.500</a></span> | <span class="t">that's very easy to overshoot a little bit and spend weeks and weeks talking about what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=288" target="_blank">00:04:48.320</a></span> | <span class="t">you're eventually going to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=291" target="_blank">00:04:51.560</a></span> | <span class="t">You've had a couple of weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=295" target="_blank">00:04:55.040</a></span> | <span class="t">Really it would have been nice to have something done by now rather than spending a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=297" target="_blank">00:04:57.960</a></span> | <span class="t">of weeks wondering about what to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=300" target="_blank">00:05:00.180</a></span> | <span class="t">So if your team is being a bit slow agreeing on something, just start working on something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=303" target="_blank">00:05:03.840</a></span> | <span class="t">yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=304" target="_blank">00:05:04.840</a></span> | <span class="t">Or as a team, just pick something that you can do by next Monday and write up something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=313" target="_blank">00:05:13.520</a></span> | <span class="t">brief about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=314" target="_blank">00:05:14.520</a></span> | <span class="t">So for example, if you're thinking, okay, we might do the $1 million data science poll.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=320" target="_blank">00:05:20.280</a></span> | <span class="t">That's fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=321" target="_blank">00:05:21.280</a></span> | <span class="t">You're not going to finish it by Monday, but maybe by Monday you could have written a blog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=324" target="_blank">00:05:24.740</a></span> | <span class="t">post introducing what you can learn in a week about medical imaging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=330" target="_blank">00:05:30.280</a></span> | <span class="t">Oh, it turns out it uses something called DICOM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=332" target="_blank">00:05:32.360</a></span> | <span class="t">Here are the Python DICOM libraries, and we tried to use them, and these were the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=336" target="_blank">00:05:36.240</a></span> | <span class="t">that got us kind of confused, and these are the ways that we solved them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=340" target="_blank">00:05:40.240</a></span> | <span class="t">And here's a Python notebook which shows you some of the main ways you can look at these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=344" target="_blank">00:05:44.840</a></span> | <span class="t">DICOM, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=346" target="_blank">00:05:46.880</a></span> | <span class="t">So split up your project into little pieces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=349" target="_blank">00:05:49.960</a></span> | <span class="t">It's like when you enter a Kaggle competition, I always tell people submit every single day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=358" target="_blank">00:05:58.160</a></span> | <span class="t">and try and put in at least half an hour a day to make it slightly better than yesterday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=362" target="_blank">00:06:02.720</a></span> | <span class="t">So how do you put in the first day's submission?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=366" target="_blank">00:06:06.080</a></span> | <span class="t">What I always do on the first day is to submit the benchmark script, which is generally like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=371" target="_blank">00:06:11.520</a></span> | <span class="t">all zeroes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=372" target="_blank">00:06:12.520</a></span> | <span class="t">And then the next day I try to improve it, so I'll put in all 0.5s, and the next day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=377" target="_blank">00:06:17.720</a></span> | <span class="t">I'll try to improve it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=378" target="_blank">00:06:18.720</a></span> | <span class="t">I'll be like, "Okay, what's the average for cats?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=380" target="_blank">00:06:20.240</a></span> | <span class="t">The average for dogs?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=381" target="_blank">00:06:21.240</a></span> | <span class="t">I'll submit that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=382" target="_blank">00:06:22.240</a></span> | <span class="t">And if you do that every day for 90 days, you'll be amazed at how much you can achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=388" target="_blank">00:06:28.320</a></span> | <span class="t">Whereas if you wait two months and spend all that time reading papers and theorizing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=393" target="_blank">00:06:33.480</a></span> | <span class="t">thinking about the best possible approach, you'll discover that you don't get any submissions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=397" target="_blank">00:06:37.240</a></span> | <span class="t">yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=398" target="_blank">00:06:38.240</a></span> | <span class="t">Or you finally get your perfect submission in and it goes terribly and now you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=401" target="_blank">00:06:41.440</a></span> | <span class="t">have time to make it better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=406" target="_blank">00:06:46.120</a></span> | <span class="t">I think those tips are equally useful for Kaggle competitions as well as for making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=411" target="_blank">00:06:51.120</a></span> | <span class="t">sure that at the end of this part of the course you have something that you're proud of, something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=418" target="_blank">00:06:58.320</a></span> | <span class="t">that you feel you did a good job in a small amount of time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=424" target="_blank">00:07:04.680</a></span> | <span class="t">If you try and publish something every week on the same topic, you'll be able to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=429" target="_blank">00:07:09.520</a></span> | <span class="t">going further and further on that thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=431" target="_blank">00:07:11.120</a></span> | <span class="t">I don't know what Slav's plans are, but maybe next week he'll follow up on some of the interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=438" target="_blank">00:07:18.200</a></span> | <span class="t">research angles that came up on Reddit, or maybe Brad will follow up on some of his additional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=443" target="_blank">00:07:23.040</a></span> | <span class="t">ideas from his post.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=446" target="_blank">00:07:26.600</a></span> | <span class="t">There's a lesson 10 wiki up already which has the notebooks, and just do a git pull</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=455" target="_blank">00:07:35.620</a></span> | <span class="t">on the GitHub repo to get the most up-to-date on Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=460" target="_blank">00:07:40.600</a></span> | <span class="t">Another thing that I wanted to point out is that in study groups, so we've been having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=467" target="_blank">00:07:47.520</a></span> | <span class="t">study groups each Friday here, and I know some of you have had study groups elsewhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=471" target="_blank">00:07:51.880</a></span> | <span class="t">around the Bay Area. I don't understand this gray matrix stuff. I don't get what's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=480" target="_blank">00:08:00.520</a></span> | <span class="t">on. I understand the symbols, I understand the math, but what's going on?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=485" target="_blank">00:08:05.680</a></span> | <span class="t">I said maybe if you had a spreadsheet, it would all make sense. Maybe I'll create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=499" target="_blank">00:08:19.240</a></span> | <span class="t">spreadsheet. Yes, do that! And 20 minutes later I turned to him and I said, "So how do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=504" target="_blank">00:08:24.600</a></span> | <span class="t">feel about gray matrices now?" And he goes, "I totally understand them." And I looked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=508" target="_blank">00:08:28.440</a></span> | <span class="t">over and he created a spreadsheet. This was the spreadsheet he created. It's a very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=514" target="_blank">00:08:34.200</a></span> | <span class="t">spreadsheet where it's like here's an image where the pixels are just 1, -1, and 0. It</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=519" target="_blank">00:08:39.480</a></span> | <span class="t">has two filters, either 1 or -1. He has the flattened convolutions next to each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=528" target="_blank">00:08:48.520</a></span> | <span class="t">and then he's created the little dot product matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=534" target="_blank">00:08:54.160</a></span> | <span class="t">I haven't been doing so much Excel stuff myself, but I think you'll learn a lot more by trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=541" target="_blank">00:09:01.120</a></span> | <span class="t">it yourself. Particularly if you try it yourself and can't figure out how to do it in Excel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=546" target="_blank">00:09:06.520</a></span> | <span class="t">then we have programs. I love Excel, so if you ask me questions about Excel, I will have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=552" target="_blank">00:09:12.160</a></span> | <span class="t">a great time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=569" target="_blank">00:09:29.440</a></span> | <span class="t">So last week we talked about the idea of learning with larger datasets. Our goal was to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=586" target="_blank">00:09:46.320</a></span> | <span class="t">and replicate the device paper. To remind you, the device paper is the one where we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=593" target="_blank">00:09:53.700</a></span> | <span class="t">a regular CNN, but the thing that we're trying to predict is not a one-hot encoding of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=601" target="_blank">00:10:01.520</a></span> | <span class="t">category, but it's the word vector of the category.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=607" target="_blank">00:10:07.480</a></span> | <span class="t">So it's an interesting problem, but one of the things interesting about it is we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=613" target="_blank">00:10:13.280</a></span> | <span class="t">to use all of ImageNet, which has its own challenges. So last week we got to the point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=621" target="_blank">00:10:21.120</a></span> | <span class="t">where we had created the word vectors. And to remember the word vectors, we then had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=626" target="_blank">00:10:26.640</a></span> | <span class="t">to map them to ImageNet categories. There are 1000 ImageNet categories, so we had to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=631" target="_blank">00:10:31.320</a></span> | <span class="t">create the word vector for each one. We didn't quite get all of them to match, but something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=635" target="_blank">00:10:35.720</a></span> | <span class="t">like 2/3 of them matched, so we're working on 2/3 of ImageNet. We've got as far as reading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=643" target="_blank">00:10:43.080</a></span> | <span class="t">all the file names for ImageNet, and then we're going to resize our images to 224x224.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=658" target="_blank">00:10:58.640</a></span> | <span class="t">I think it's a good idea to do some of this pre-processing upfront. Something that TensorFlow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=666" target="_blank">00:11:06.720</a></span> | <span class="t">and PyTorch both do and Keras recently started doing is that if you use a generator, it actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=673" target="_blank">00:11:13.040</a></span> | <span class="t">does the image pre-processing in a number of separate threads in parallel behind the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=679" target="_blank">00:11:19.700</a></span> | <span class="t">scenes. So some of this is a little less important than it was 6 months ago when Keras didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=685" target="_blank">00:11:25.520</a></span> | <span class="t">do that. It used to be that we had to spend a long time waiting for our data to get processed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=692" target="_blank">00:11:32.720</a></span> | <span class="t">before it could get into the CNN. Having said that, particularly image resizing, when you've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=702" target="_blank">00:11:42.780</a></span> | <span class="t">got large JPEGs, just reading them off the hard disk and resizing them can take quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=709" target="_blank">00:11:49.200</a></span> | <span class="t">a long time. So I always like to put it into do all that resizing upfront and end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=714" target="_blank">00:11:54.700</a></span> | <span class="t">something in a nice convenient V-coles array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=719" target="_blank">00:11:59.500</a></span> | <span class="t">Amongst other things, it means that unless you have enough money to have a huge NVMe or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=726" target="_blank">00:12:06.600</a></span> | <span class="t">SSD drive, which you can put the entirety of ImageNet on, you probably have your big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=732" target="_blank">00:12:12.480</a></span> | <span class="t">data sets on some kind of pretty slow spinning disk or slow rate array. One of the nice things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=739" target="_blank">00:12:19.360</a></span> | <span class="t">about doing the resizing first is that it makes it a lot smaller, and you probably can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=742" target="_blank">00:12:22.960</a></span> | <span class="t">then fit it on your SSD. There's lots of reasons that I think this is good. I'm going to resize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=749" target="_blank">00:12:29.480</a></span> | <span class="t">all of the ImageNet images, put them in a V-coles array on my SSD. So here's the path,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=757" target="_blank">00:12:37.840</a></span> | <span class="t">and dpath is the path to my fast SSD mount point. We talked briefly about the resizing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=770" target="_blank">00:12:50.760</a></span> | <span class="t">and we're going to do a different kind of resizing. In the past, we've done the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=774" target="_blank">00:12:54.160</a></span> | <span class="t">kind of resizing that Keras does, which is to add a black border. If you start with something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=779" target="_blank">00:12:59.200</a></span> | <span class="t">that's not square, and you make it square, you resize the largest axis to be the size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=786" target="_blank">00:13:06.120</a></span> | <span class="t">of your square, which means you're left with a black border. I was concerned that any model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=794" target="_blank">00:13:14.280</a></span> | <span class="t">where you have that is going to have to learn to model the black border, a) and b) that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=800" target="_blank">00:13:20.880</a></span> | <span class="t">you're kind of throwing away information. You're not using the full size of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=805" target="_blank">00:13:25.400</a></span> | <span class="t">And indeed, every other library or pretty much paper I've seen uses a different approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=812" target="_blank">00:13:32.560</a></span> | <span class="t">which is to resize the smaller side of the image to the square. Now the larger size is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=820" target="_blank">00:13:40.440</a></span> | <span class="t">now too big for your square, so you crop off the top and bottom, or crop off the left and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=825" target="_blank">00:13:45.760</a></span> | <span class="t">right. So this is called a center-cropping approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=839" target="_blank">00:13:59.760</a></span> | <span class="t">Okay, that's true. What you're doing is you're throwing away compute. Like with the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=855" target="_blank">00:14:15.720</a></span> | <span class="t">where you do center-crop, you have a complete 224 thing full of meaningful pixels. Whereas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=860" target="_blank">00:14:20.440</a></span> | <span class="t">with a black border, you have a 180 by 224 bit with meaningful pixels and a whole bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=865" target="_blank">00:14:25.060</a></span> | <span class="t">of black pixels. Yeah, that can be a problem. It works well for ImageNet because ImageNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=877" target="_blank">00:14:37.720</a></span> | <span class="t">things are generally somewhat centered. You may need to do some kind of initial step to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=884" target="_blank">00:14:44.800</a></span> | <span class="t">do a heat map or something like we did in lesson 7 to figure out roughly where the thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=888" target="_blank">00:14:48.440</a></span> | <span class="t">is before you decide where to center the crop. So these things are all compromises. But I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=893" target="_blank">00:14:53.520</a></span> | <span class="t">got to say, since I switched to using this approach, I feel like my models have trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=897" target="_blank">00:14:57.680</a></span> | <span class="t">a lot faster and given better results, certainly the super resolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=903" target="_blank">00:15:03.320</a></span> | <span class="t">I said last week that we were going to start looking at parallel processing. If you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=909" target="_blank">00:15:09.040</a></span> | <span class="t">wondering about last week's homework, we're going to get there, but some of the techniques</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=912" target="_blank">00:15:12.680</a></span> | <span class="t">we're about to learn, we're going to use to do last week's homework even better. So what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=919" target="_blank">00:15:19.880</a></span> | <span class="t">I want to do is I've got a CPU with something like 10 cores on it, and then each of those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=928" target="_blank">00:15:28.800</a></span> | <span class="t">cores have hyperthreading, so each of those cores can do kind of two things at once. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=933" target="_blank">00:15:33.680</a></span> | <span class="t">I really want to be able to have a couple of dozen processes going on, each one resizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=939" target="_blank">00:15:39.280</a></span> | <span class="t">an image. That's called parallel processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=944" target="_blank">00:15:44.440</a></span> | <span class="t">Just to remind you, this is as opposed to vectorization, or SIMD, which is where a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=949" target="_blank">00:15:49.560</a></span> | <span class="t">thread operates on a bunch of things at a time. So we learned that to get SIMD working,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=954" target="_blank">00:15:54.480</a></span> | <span class="t">you just have to install fellow SIMD, and it just happens, 600% speedup. I tried it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=962" target="_blank">00:16:02.160</a></span> | <span class="t">it works. Now we're going to, as well as the 600% speedup, also get another 10 or 20x speedup</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=968" target="_blank">00:16:08.720</a></span> | <span class="t">by doing parallel processing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=972" target="_blank">00:16:12.280</a></span> | <span class="t">The basic approach to parallel processing in Python 3 is to set up something called either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=979" target="_blank">00:16:19.240</a></span> | <span class="t">a process pool or a thread pool. So the idea here is that we've got a number of little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=985" target="_blank">00:16:25.340</a></span> | <span class="t">programs running, threads or processes, and when we set up that pool, we say how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=991" target="_blank">00:16:31.120</a></span> | <span class="t">of those little programs do we want to fire up. And then what we do is we say, okay, now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=997" target="_blank">00:16:37.840</a></span> | <span class="t">I want you to use workers. I want you to use all of those workers to do some thing. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1006" target="_blank">00:16:46.240</a></span> | <span class="t">the easiest way to do a thing in Python 3 is to use Map. How many of you have used Map</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1012" target="_blank">00:16:52.800</a></span> | <span class="t">before?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1013" target="_blank">00:16:53.800</a></span> | <span class="t">So for those of you who haven't, Map is a very common functional programming construct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1018" target="_blank">00:16:58.480</a></span> | <span class="t">that's found its way into lots of other languages, which simply says, loop through a collection</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1023" target="_blank">00:17:03.680</a></span> | <span class="t">and call a function on everything in that collection and return a new collection, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1029" target="_blank">00:17:09.360</a></span> | <span class="t">is the result of calling that function on that thing. In our case, the function is resize,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1034" target="_blank">00:17:14.280</a></span> | <span class="t">and the collection is imageNet_images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1036" target="_blank">00:17:16.440</a></span> | <span class="t">In fact, the collection is a bunch of numbers, 0, 1, 2, 3, 4, and so forth, and what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1045" target="_blank">00:17:25.240</a></span> | <span class="t">resize image is going to do is it's going to open that image off disk. So it's turning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1052" target="_blank">00:17:32.560</a></span> | <span class="t">the number 3 into the third image resized, 224x224, and we'll return that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1059" target="_blank">00:17:39.880</a></span> | <span class="t">So the general approach here, this is basically what it looks like to do parallel processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1066" target="_blank">00:17:46.440</a></span> | <span class="t">in Python. It may look a bit weird. We're going result equals exec.map. This is a function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1074" target="_blank">00:17:54.960</a></span> | <span class="t">I want, this is the thing to map over, and then I'm saying for each thing in that list,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1080" target="_blank">00:18:00.040</a></span> | <span class="t">do something. Now this might make you think, well wait, does that mean this list has to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1085" target="_blank">00:18:05.480</a></span> | <span class="t">have enough memory for every single resized image? And the answer is no, no it doesn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1092" target="_blank">00:18:12.520</a></span> | <span class="t">One of the things that Python 3 uses a lot more is using these things they call generators,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1099" target="_blank">00:18:19.800</a></span> | <span class="t">which is basically, it's something that looks like a list, but it's lazy. It only creates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1104" target="_blank">00:18:24.800</a></span> | <span class="t">that thing when you ask for it. So as I append each image, it's going to give me that image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1110" target="_blank">00:18:30.920</a></span> | <span class="t">And if this mapping is not yet finished creating it, it will wait. So this approach looks like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1117" target="_blank">00:18:37.080</a></span> | <span class="t">it's going to use heaps of memory, but it doesn't. It uses only the minimum amount of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1122" target="_blank">00:18:42.480</a></span> | <span class="t">memory necessary and it does everything in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1127" target="_blank">00:18:47.280</a></span> | <span class="t">So resizeImage is something which is going to open up the image, it's going to turn it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1136" target="_blank">00:18:56.040</a></span> | <span class="t">into a NumPy array, and then it's going to resize it. And so then the resize does the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1141" target="_blank">00:19:01.520</a></span> | <span class="t">center cropping we just mentioned, and then after it's resized it's going to get appended.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1147" target="_blank">00:19:07.520</a></span> | <span class="t">What does appendImage do? So this is a bit weird. What's going on here? What it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1155" target="_blank">00:19:15.600</a></span> | <span class="t">is it's going to actually stick it into what we call a pre-allocated array. We're learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1163" target="_blank">00:19:23.120</a></span> | <span class="t">a lot of computer science concepts here. Anybody that's done computer science before will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1166" target="_blank">00:19:26.800</a></span> | <span class="t">familiar with all of this already. If you haven't, you probably won't. But it's important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1172" target="_blank">00:19:32.040</a></span> | <span class="t">to know that the slowest thing in your computer, generally speaking, is allocating memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1179" target="_blank">00:19:39.660</a></span> | <span class="t">It's finding some memory, it's reading stuff from that memory, it's writing to that memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1184" target="_blank">00:19:44.040</a></span> | <span class="t">unless of course it's like cache or something. And generally speaking, if you create lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1189" target="_blank">00:19:49.800</a></span> | <span class="t">and lots of arrays and then throw them away again, that's likely to be really, really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1194" target="_blank">00:19:54.280</a></span> | <span class="t">slow. So what I wanted to do was create a single 224x224 array which is going to contain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1201" target="_blank">00:20:01.000</a></span> | <span class="t">my resized image, and then I'm going to append that to my bcol's tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1208" target="_blank">00:20:08.320</a></span> | <span class="t">So the way you do that in Python, it's wonderfully easy. You can create a variable from this thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1220" target="_blank">00:20:20.960</a></span> | <span class="t">called threading.local. It's basically something that looks a bit like a dictionary, but it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1228" target="_blank">00:20:28.800</a></span> | <span class="t">a very special kind of dictionary. It's going to create a separate copy of it for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1233" target="_blank">00:20:33.280</a></span> | <span class="t">thread or process. Normally when you've got lots of things happening at once, it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1239" target="_blank">00:20:39.360</a></span> | <span class="t">to be a real pain because if two things try to use it at the same time, you get bad results</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1245" target="_blank">00:20:45.520</a></span> | <span class="t">or even crashes. But if you allocate a variable like this, it automatically creates a separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1252" target="_blank">00:20:52.200</a></span> | <span class="t">copy in every thread. You don't have to worry about locks, you don't have to worry about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1256" target="_blank">00:20:56.320</a></span> | <span class="t">race conditions, whatever. Once I've created this special threading.local variable, I then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1264" target="_blank">00:21:04.120</a></span> | <span class="t">create a placeholder inside it which is just an array of zeros of size 224x224x3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1272" target="_blank">00:21:12.000</a></span> | <span class="t">So then later on, I create my bcol's array, which is where I'm going to put everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1277" target="_blank">00:21:17.040</a></span> | <span class="t">eventually, and to append the image, I grab the bit of the image that I want and I put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1283" target="_blank">00:21:23.600</a></span> | <span class="t">it into that preallocated thread local variable, and then I append that to my bcol's array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1294" target="_blank">00:21:34.200</a></span> | <span class="t">So there's lots of detail here in terms of using parallel processing effectively. I wanted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1303" target="_blank">00:21:43.200</a></span> | <span class="t">to briefly mention it not because I think somebody who hasn't studied computer science</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1307" target="_blank">00:21:47.480</a></span> | <span class="t">is now going to go, "Okay, I totally understood all that," but to give you some of the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1311" target="_blank">00:21:51.240</a></span> | <span class="t">to like search for and learn about over the next week if you haven't done any parallel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1316" target="_blank">00:21:56.200</a></span> | <span class="t">programming before. You're going to need to understand thread local storage and race conditions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1323" target="_blank">00:22:03.880</a></span> | <span class="t">In Python, there's something called the global interpreter lock, which is one of the many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1342" target="_blank">00:22:22.160</a></span> | <span class="t">awful things about Python, which is that in theory two things can't happen at the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1348" target="_blank">00:22:28.760</a></span> | <span class="t">time because Python wasn't really written in a thread-safe way. The good news is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1357" target="_blank">00:22:37.040</a></span> | <span class="t">lots of libraries are written in a thread-safe way. So if you're using a library where most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1362" target="_blank">00:22:42.960</a></span> | <span class="t">of its work is being done in C, as is the case with PLOS-AMD, actually you don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1368" target="_blank">00:22:48.960</a></span> | <span class="t">to worry about that. And I can prove it to you even because I drew a little picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1375" target="_blank">00:22:55.080</a></span> | <span class="t">Where is the result of serial versus parallel? The serial without SIMD version is 6 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1384" target="_blank">00:23:04.320</a></span> | <span class="t">bigger than this, so the default Python code you would have written maybe before today's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1390" target="_blank">00:23:10.600</a></span> | <span class="t">course would have been 120 seconds process 2000 images. With SIMD, it's 25 seconds. With</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1401" target="_blank">00:23:21.320</a></span> | <span class="t">the process pull, it's 8 seconds. For 3 workers, for 6 workers, it's 5 seconds. The thread pull</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1408" target="_blank">00:23:28.800</a></span> | <span class="t">is even better, 3.6 seconds for 12 workers, 3.2 seconds for 16 workers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1416" target="_blank">00:23:36.160</a></span> | <span class="t">Your mileage will vary depending on what CPU you have. Given that probably quite a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1422" target="_blank">00:23:42.200</a></span> | <span class="t">you are using the P2 still, unless you've got your deep learning box up and running,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1426" target="_blank">00:23:46.040</a></span> | <span class="t">you'll have the same performance as other people using the P2. You should try something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1430" target="_blank">00:23:50.120</a></span> | <span class="t">like this, which is to try different numbers of workers and see what's the optimal for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1436" target="_blank">00:23:56.040</a></span> | <span class="t">that particular CPU. Now once you've done that, you know. Once I went beyond 16, I didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1441" target="_blank">00:24:01.360</a></span> | <span class="t">really get improvements. So I know that on that computer, a thread pull of size 16 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1447" target="_blank">00:24:07.520</a></span> | <span class="t">a pretty good choice. As you can see, once you get into the right general vicinity, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1452" target="_blank">00:24:12.480</a></span> | <span class="t">doesn't vary too much. So as long as you're roughly okay, just behind you, Rachel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1463" target="_blank">00:24:23.040</a></span> | <span class="t">So that's the general approach here, is run through something in parallel, each time append</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1467" target="_blank">00:24:27.400</a></span> | <span class="t">it to my big holes array. And at the end of that, I've got a big holes array which I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1472" target="_blank">00:24:32.680</a></span> | <span class="t">use again and again. So I don't re-run that code very often anymore. I've got all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1476" target="_blank">00:24:36.880</a></span> | <span class="t">image net resized into each of 72x72, 224, and 288. And I give them different names and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1484" target="_blank">00:24:44.440</a></span> | <span class="t">I just use them just like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1489" target="_blank">00:24:49.000</a></span> | <span class="t">In fact, I think that's what Keras does now. I think it squishes. Okay, so here's one of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1510" target="_blank">00:25:10.440</a></span> | <span class="t">these things. I'm not quite sure. My guess was that I don't think it's a good idea because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1516" target="_blank">00:25:16.720</a></span> | <span class="t">you're now going to have dogs of various different squish levels and you'll see an end is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1521" target="_blank">00:25:21.520</a></span> | <span class="t">to have to learn that thing. It's got another type of symmetry to learn about, level of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1531" target="_blank">00:25:31.360</a></span> | <span class="t">squishiness. Whereas if we keep everything of the same aspect ratio, I think it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1540" target="_blank">00:25:40.120</a></span> | <span class="t">to be easier to learn so we'll get better results with less epochs of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1545" target="_blank">00:25:45.960</a></span> | <span class="t">That's my theory and I'd be fascinated for somebody to do a really in-depth analysis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1549" target="_blank">00:25:49.160</a></span> | <span class="t">of black borders versus center cropping versus squishing with image net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1557" target="_blank">00:25:57.620</a></span> | <span class="t">So for now we can just open the big holes array and there we go. So we're now ready</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1563" target="_blank">00:26:03.260</a></span> | <span class="t">to create our model. I'll run through this pretty quickly because most of it's pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1567" target="_blank">00:26:07.320</a></span> | <span class="t">boring. The basic idea here is that we need to create an array of labels which are called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1573" target="_blank">00:26:13.000</a></span> | <span class="t">VEX, which contains for every image in my big holes array, it needs to contain the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1583" target="_blank">00:26:23.760</a></span> | <span class="t">word vector for that image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1586" target="_blank">00:26:26.200</a></span> | <span class="t">Just to remind you, last week we randomly ordered the file names, so this big holes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1595" target="_blank">00:26:35.200</a></span> | <span class="t">array is in random order. We've got our labels, which is the word vectors for every image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1604" target="_blank">00:26:44.360</a></span> | <span class="t">We need to do our normal pre-processing. This is a handy way to pre-process in the new version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1613" target="_blank">00:26:53.520</a></span> | <span class="t">of Keras. We're using the normal Keras ResNet model, the one that comes in keras.applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1622" target="_blank">00:27:02.000</a></span> | <span class="t">It doesn't do the pre-processing for you, but if you create a lambda layer that does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1628" target="_blank">00:27:08.200</a></span> | <span class="t">the pre-processing then you can use that lambda layer as the input tensor. So this whole thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1636" target="_blank">00:27:16.080</a></span> | <span class="t">now will do the pre-processing automatically without you having to worry about it. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1641" target="_blank">00:27:21.760</a></span> | <span class="t">a good little trick. I'm not sure it's quite as neat as what we did in part 1 where we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1646" target="_blank">00:27:26.800</a></span> | <span class="t">put it in the model itself, but at least this way we don't have to maintain a whole separate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1652" target="_blank">00:27:32.680</a></span> | <span class="t">version of all of the models. So that's kind of what I'm doing nowadays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1664" target="_blank">00:27:44.840</a></span> | <span class="t">When you're working on really big datasets, you don't want to process things any more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1670" target="_blank">00:27:50.560</a></span> | <span class="t">than necessary and any more times than necessary. I know ahead of time that I'm going to want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1675" target="_blank">00:27:55.640</a></span> | <span class="t">to do some fine-tuning. What I decided to do was I decided this is the particular layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1684" target="_blank">00:28:04.200</a></span> | <span class="t">where I'm going to do my fine-tuning. So I decided to first of all create a model which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1689" target="_blank">00:28:09.480</a></span> | <span class="t">started at the input and went as far as this layer. So my first step was to create that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1698" target="_blank">00:28:18.680</a></span> | <span class="t">model and save the results of that. The next step will be to take that intermediate step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1707" target="_blank">00:28:27.160</a></span> | <span class="t">and take it to the next stage I want to fine-tune to and save that. So it's a little shortcut.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1713" target="_blank">00:28:33.920</a></span> | <span class="t">There's a couple of really important intricacies to be aware of here though. The first one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1719" target="_blank">00:28:39.480</a></span> | <span class="t">is you'll notice that ResNet and Inception are not used very often for transfer learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1730" target="_blank">00:28:50.680</a></span> | <span class="t">This is something which I've not seen studied, and I actually think this is a really important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1734" target="_blank">00:28:54.360</a></span> | <span class="t">thing to study. Which of these things work best for transfer learning? But I think one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1739" target="_blank">00:28:59.280</a></span> | <span class="t">of the difficulties is that ResNet and Inception are harder. The reason they're harder is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1745" target="_blank">00:29:05.240</a></span> | <span class="t">if you look at ResNet, you've got lots and lots of layers which make no sense on their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1751" target="_blank">00:29:11.260</a></span> | <span class="t">own. Ditto for Inception. They keep on splitting into 2 bits and then merging again. So what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1761" target="_blank">00:29:21.160</a></span> | <span class="t">I did was I looked at the Keras source code to find out how each block is named. What</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1769" target="_blank">00:29:29.720</a></span> | <span class="t">I wanted to do was to say we've got a ResNet block, we've just had a merge, and then it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1777" target="_blank">00:29:37.320</a></span> | <span class="t">goes out and it does a couple of convolutions, and then it comes back and does an addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1784" target="_blank">00:29:44.840</a></span> | <span class="t">Basically I want to get one of these. Unfortunately for some reason Keras does not name these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1793" target="_blank">00:29:53.960</a></span> | <span class="t">merge cells. So what I had to do was get the next cell and then go back by 1. So it kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1804" target="_blank">00:30:04.640</a></span> | <span class="t">of shows you how little people have been working with ResNet with transfer learning. Literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1809" target="_blank">00:30:09.480</a></span> | <span class="t">the only bits of it that make sense to transfer learn from are nameless in one of the most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1815" target="_blank">00:30:15.240</a></span> | <span class="t">popular things for transfer learning, Keras. There's a second complexity when working with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1826" target="_blank">00:30:26.000</a></span> | <span class="t">ResNet. We haven't discussed this much, but ResNet actually has two kinds of ResNet blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1833" target="_blank">00:30:33.080</a></span> | <span class="t">One is this kind, which is an identity block, and the second time is a ResNet convolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1843" target="_blank">00:30:43.280</a></span> | <span class="t">block, which they also call a bottleneck block. What this is is it's pretty similar. One thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1856" target="_blank">00:30:56.120</a></span> | <span class="t">that's going up through a couple of convolutions and then goes and gets added together, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1860" target="_blank">00:31:00.160</a></span> | <span class="t">the other side is not an identity. The other side is a single convolution. In ResNet they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1869" target="_blank">00:31:09.360</a></span> | <span class="t">throw in one of these every half a dozen blocks or so. Why is that? The reason is that if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1878" target="_blank">00:31:18.200</a></span> | <span class="t">only have identity blocks, then all it can really do is to continually fine-tune where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1886" target="_blank">00:31:26.000</a></span> | <span class="t">it's at so far. We've learned quite a few times now that these identity blocks map to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1892" target="_blank">00:31:32.760</a></span> | <span class="t">the residual, so they keep trying to fine-tune the types of features that we have so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1899" target="_blank">00:31:39.440</a></span> | <span class="t">Whereas these bottleneck blocks actually force it from time to time to create a whole different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1905" target="_blank">00:31:45.240</a></span> | <span class="t">type of features because there is no identity path through here. The shortest path still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1911" target="_blank">00:31:51.240</a></span> | <span class="t">goes through a single convolution. When you think about transfer learning from ResNet,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1917" target="_blank">00:31:57.360</a></span> | <span class="t">you kind of need to think about should I transfer learn from an identity block before or after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1923" target="_blank">00:32:03.480</a></span> | <span class="t">or from a bottleneck block before or after. Again, I don't think anybody has studied this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1930" target="_blank">00:32:10.080</a></span> | <span class="t">or at least I haven't seen anybody write it down. I've played around with it a bit and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1934" target="_blank">00:32:14.840</a></span> | <span class="t">I'm not sure I have a totally decisive suggestion for you. Clearly my guess is that the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1948" target="_blank">00:32:28.440</a></span> | <span class="t">point to grab in ResNet is the end of the block immediately before a bottleneck block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1956" target="_blank">00:32:36.480</a></span> | <span class="t">And the reason for that is that at that level of receptive field, obviously because each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1962" target="_blank">00:32:42.760</a></span> | <span class="t">bottleneck block is changing the receptive field, and at that level of semantic complexity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1970" target="_blank">00:32:50.440</a></span> | <span class="t">this is the most sophisticated version of it because it's been through a whole bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1974" target="_blank">00:32:54.160</a></span> | <span class="t">of identity blocks to get there. So my belief is that you want to get just before that bottleneck</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1986" target="_blank">00:33:06.840</a></span> | <span class="t">is the best place to transfer learn from. So that's what this is. This is the spot just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=1996" target="_blank">00:33:16.920</a></span> | <span class="t">before the last bottleneck layer in ResNet. So it's pretty late, and so as we know very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2006" target="_blank">00:33:26.120</a></span> | <span class="t">well from part 1 with transfer learning, when you're doing something which is not too different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2011" target="_blank">00:33:31.520</a></span> | <span class="t">and in this case we're switching from one-hot encoding to word vectors, which is not too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2015" target="_blank">00:33:35.360</a></span> | <span class="t">different. You probably don't want to transfer learn from too early, so that's why I picked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2021" target="_blank">00:33:41.920</a></span> | <span class="t">this fairly late stage, which is just before the final bottleneck block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2030" target="_blank">00:33:50.880</a></span> | <span class="t">So the second complexity here is that this bottleneck block has these dimensions. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2037" target="_blank">00:33:57.880</a></span> | <span class="t">output is 14x14x1024. So we have about a million images, so a million by 14x14x1024 is more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2048" target="_blank">00:34:08.760</a></span> | <span class="t">than I wanted to deal with. So I did something very simple, which was I popped in one more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2057" target="_blank">00:34:17.600</a></span> | <span class="t">layer after this, which is an average pooling layer, 7x7. So that's going to take my 14x14</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2067" target="_blank">00:34:27.520</a></span> | <span class="t">output and turn it into a 2x2 output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2071" target="_blank">00:34:31.520</a></span> | <span class="t">So let's say one of those activations was looking for bird's eyeballs, then it's saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2077" target="_blank">00:34:37.880</a></span> | <span class="t">in each of the 14x14 spots, how likely is it that this is a bird's eyeball? And so after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2084" target="_blank">00:34:44.000</a></span> | <span class="t">this it's now saying in each of these 4 spots, on average, how much were those cells looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2090" target="_blank">00:34:50.980</a></span> | <span class="t">like bird's eyeballs? This is losing information. If I had a bigger SSD and more time, I wouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2103" target="_blank">00:35:03.120</a></span> | <span class="t">have done this. But it's a good trick when you're working with these fully convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2107" target="_blank">00:35:07.160</a></span> | <span class="t">architectures. You can pop an average pooling layer anywhere and decrease the resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2113" target="_blank">00:35:13.320</a></span> | <span class="t">to something that you feel like you can deal with. So in this case, my decision was to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2118" target="_blank">00:35:18.920</a></span> | <span class="t">go to 2x2 by 1024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2122" target="_blank">00:35:22.000</a></span> | <span class="t">We had a question. I was going to ask, have we talked about why we do the merge operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2128" target="_blank">00:35:28.840</a></span> | <span class="t">in some of these more complex models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2131" target="_blank">00:35:31.960</a></span> | <span class="t">We have quite a few times, which is basically the merge was the thing which does the plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2137" target="_blank">00:35:37.680</a></span> | <span class="t">here. That's the trick to making it into a ResNet block, is having the addition of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2145" target="_blank">00:35:45.240</a></span> | <span class="t">identity with the result factor of how the convolutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2155" target="_blank">00:35:55.520</a></span> | <span class="t">So recently I was trying to go from many filters. So you kind of just talked about downsizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2161" target="_blank">00:36:01.600</a></span> | <span class="t">the size of the geometry. Is there a good best practice on going from, let's say, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2167" target="_blank">00:36:07.360</a></span> | <span class="t">512 filters down to less? Or is it just as simple as doing convolution with less filters?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2174" target="_blank">00:36:14.880</a></span> | <span class="t">Yeah, there's not exactly a best practice for that. But in a sense, every single successful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2187" target="_blank">00:36:27.800</a></span> | <span class="t">architecture gives you some insights about that. Because every one of them eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2191" target="_blank">00:36:31.720</a></span> | <span class="t">has to end up with 1,000 categories if it's ResNet or three channels of 1.255 continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2201" target="_blank">00:36:41.440</a></span> | <span class="t">if it's generative. So the best thing you can really do is, well, there's two things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2206" target="_blank">00:36:46.440</a></span> | <span class="t">one is to kind of look at the successful architectures. Another thing is, although this week is kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2212" target="_blank">00:36:52.120</a></span> | <span class="t">of the last week where we're mainly going to be looking at images, I am going to briefly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2216" target="_blank">00:36:56.200</a></span> | <span class="t">next week open with a quick run through some of the things that you can look at to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2221" target="_blank">00:37:01.200</a></span> | <span class="t">more. And one of them is going to be a paper. In fact, two different papers which have like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2226" target="_blank">00:37:06.360</a></span> | <span class="t">best practices, you know, really nice kind of descriptions of these hundred different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2231" target="_blank">00:37:11.960</a></span> | <span class="t">things, these hundred different results. But all this stuff, it's still pretty artisanal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2239" target="_blank">00:37:19.520</a></span> | <span class="t">Good question. So we initially resized images to 224, right? And it ended up being as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2252" target="_blank">00:37:32.840</a></span> | <span class="t">big cause already, right? Yes. So a couple it's like 50 giga or something. Yes. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2264" target="_blank">00:37:44.080</a></span> | <span class="t">compressed and uncompressed. It's like a couple of hundred giga. But, well, if you load it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2270" target="_blank">00:37:50.280</a></span> | <span class="t">into memory... I'm not going to load it into memory, you'll see. So what you do is kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2274" target="_blank">00:37:54.360</a></span> | <span class="t">of place the load. It's getting there. Yeah. So that's exactly the right segue I was looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2281" target="_blank">00:38:01.600</a></span> | <span class="t">for, so thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2283" target="_blank">00:38:03.440</a></span> | <span class="t">So what we're going to do now is we want to run this model we just built, just call basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2289" target="_blank">00:38:09.640</a></span> | <span class="t">dot predict on it and save the predictions. The problem is that the size of those predictions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2295" target="_blank">00:38:15.960</a></span> | <span class="t">is going to be bigger than the amount of RAM I have, so I need to do it a batch at a time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2301" target="_blank">00:38:21.360</a></span> | <span class="t">and save it a batch at a time. They've got a million things, each one with this many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2306" target="_blank">00:38:26.480</a></span> | <span class="t">activations. And this is going to happen quite often, right? You're either working on a smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2312" target="_blank">00:38:32.000</a></span> | <span class="t">computer or you're working with a bigger dataset, or you're working with a dataset where you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2316" target="_blank">00:38:36.520</a></span> | <span class="t">using a larger number of activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2320" target="_blank">00:38:40.480</a></span> | <span class="t">This is actually very easy to handle. You just create your bcols array where you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2325" target="_blank">00:38:45.720</a></span> | <span class="t">going to store it. And then all I do is I go from 0 to the length of my array, my source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2335" target="_blank">00:38:55.480</a></span> | <span class="t">array, a batch at a time. So this is creating the numbers 0, 0 plus 128, 128 plus 128, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2344" target="_blank">00:39:04.360</a></span> | <span class="t">so on and so forth. And then I take the slice of my source array from originally 0 to 128,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2352" target="_blank">00:39:12.000</a></span> | <span class="t">then from 128 to 256 and so forth. So this is now going to contain a slice of my source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2359" target="_blank">00:39:19.760</a></span> | <span class="t">bcols array. This is going to create a generator which is going to have all of those slices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2369" target="_blank">00:39:29.520</a></span> | <span class="t">and of course being a generator it's going to be lazy. So I can then enumerate through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2373" target="_blank">00:39:33.800</a></span> | <span class="t">each of those slices, and I can append to my bcols array the result of predicting just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2381" target="_blank">00:39:41.520</a></span> | <span class="t">on that one batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2385" target="_blank">00:39:45.240</a></span> | <span class="t">So you've seen like predict and evaluate and fit and so forth, and the generator versions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2395" target="_blank">00:39:55.280</a></span> | <span class="t">Also in Keras there's generally an on-batch version, so there's a train on-batch and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2401" target="_blank">00:40:01.280</a></span> | <span class="t">predict on-batch. What these do is they basically have no smarts to them at all. This is like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2407" target="_blank">00:40:07.480</a></span> | <span class="t">the most basic thing. So this is just going to take whatever you give it and call predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2411" target="_blank">00:40:11.720</a></span> | <span class="t">on this thing. It won't shuffle it, it won't batch it, it's just going to throw it directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2416" target="_blank">00:40:16.720</a></span> | <span class="t">into the computation graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2418" target="_blank">00:40:18.280</a></span> | <span class="t">So I'm just going to take a model, it's going to call predict on just this batch of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2425" target="_blank">00:40:25.000</a></span> | <span class="t">And then from time to time I print out how far I've gone just so that I know how I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2429" target="_blank">00:40:29.480</a></span> | <span class="t">going. Also from time to time I call .flush, that's the thing in bcols that actually writes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2436" target="_blank">00:40:36.240</a></span> | <span class="t">it to disk. So this thing doesn't actually take very long to run. And one of the nice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2444" target="_blank">00:40:44.080</a></span> | <span class="t">things I can do here is I can do some data augmentation as well. So I've added a direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2449" target="_blank">00:40:49.800</a></span> | <span class="t">parameter, and what I'm going to do is I'm going to have a second copy of all of my images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2455" target="_blank">00:40:55.200</a></span> | <span class="t">which is flipped horizontally. So to flip things horizontally, that's interesting, I think I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2464" target="_blank">00:41:04.040</a></span> | <span class="t">screwed this up. To flip things horizontally, you've got batch, height, and then this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2480" target="_blank">00:41:20.120</a></span> | <span class="t">columns. So if we pass in a -1 here, then it's going to flip it horizontally. That explains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2491" target="_blank">00:41:31.600</a></span> | <span class="t">why some of my results haven't been quite as good as I hoped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2496" target="_blank">00:41:36.280</a></span> | <span class="t">So when you run this, we're going to end up with a big big holes array that's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2501" target="_blank">00:41:41.560</a></span> | <span class="t">contain two copies of every three sites image-net-image, the activations at the layer that we have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2510" target="_blank">00:41:50.640</a></span> | <span class="t">one layer before this. So I call it once with direction forwards and one with direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2517" target="_blank">00:41:57.960</a></span> | <span class="t">backwards. So at the end of that, I've now got nearly 2 million activations of 2x2x1024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2528" target="_blank">00:42:08.280</a></span> | <span class="t">So that's pretty close to the end of ResNet. I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2539" target="_blank">00:42:19.120</a></span> | <span class="t">then just copied and pasted from the Keras code the last few steps of ResNet. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2544" target="_blank">00:42:24.720</a></span> | <span class="t">is the last few blocks. I added in one extra identity block just because I had a feeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2551" target="_blank">00:42:31.160</a></span> | <span class="t">that might help things along a little bit. Again, people have not really studied this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2555" target="_blank">00:42:35.120</a></span> | <span class="t">yet, so I haven't had a chance to properly experiment, but it seemed to work quite well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2562" target="_blank">00:42:42.080</a></span> | <span class="t">This is basically copied and pasted from Keras's code. I then need to copy the weights from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2568" target="_blank">00:42:48.600</a></span> | <span class="t">Keras for those last few layers of ResNet. So now I'm going to repeat the same process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2574" target="_blank">00:42:54.240</a></span> | <span class="t">again, which is to predict on these last few layers. The input will be the output from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2581" target="_blank">00:43:01.280</a></span> | <span class="t">the previous one. So we went like 2/3 of the way into ResNet and got those activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2587" target="_blank">00:43:07.880</a></span> | <span class="t">and put those activations into the last few stages of ResNet to get those activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2594" target="_blank">00:43:14.000</a></span> | <span class="t">Now the outputs from this are actually just a vector of length 2048, which does fit in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2603" target="_blank">00:43:23.280</a></span> | <span class="t">my RAM, so I didn't bother with calling predict on batch, I can just call .predict. If you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2609" target="_blank">00:43:29.880</a></span> | <span class="t">try this at home and don't have enough memory, you can use the predict on batch trick again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2616" target="_blank">00:43:36.280</a></span> | <span class="t">Any time you ran out of memory when calling predict, you can always just use this pattern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2627" target="_blank">00:43:47.560</a></span> | <span class="t">So at the end of all that, I've now got the activations from the penultimate layer of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2634" target="_blank">00:43:54.400</a></span> | <span class="t">ResNet, and so I can do a usual transfer learning trick of creating a linear model. My linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2644" target="_blank">00:44:04.160</a></span> | <span class="t">model is now going to try to use the number of dimensions in my word vectors as its output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2652" target="_blank">00:44:12.520</a></span> | <span class="t">and you'll see it doesn't have any activation function. That's because I'm not doing one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2658" target="_blank">00:44:18.480</a></span> | <span class="t">hot encoding, my word vectors could be any size numbers, so I just leave it as linear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2665" target="_blank">00:44:25.920</a></span> | <span class="t">And then I compile it, and then I fit it, and so this linear model is now my very first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2671" target="_blank">00:44:31.640</a></span> | <span class="t">-- this is almost the same as what we did in Lesson 1. Cryptocs vs. cats. We're fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2678" target="_blank">00:44:38.520</a></span> | <span class="t">tuning a model to a slightly different target to what it was originally trained with. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2688" target="_blank">00:44:48.040</a></span> | <span class="t">just that we're doing it with a lot more data, so we have to be a bit more thoughtful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2693" target="_blank">00:44:53.480</a></span> | <span class="t">There's one other difference here, which is I'm using a custom loss function. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2698" target="_blank">00:44:58.560</a></span> | <span class="t">loss function I'm using is cosine distance. You can lock that up at home if you're not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2704" target="_blank">00:45:04.240</a></span> | <span class="t">familiar with it, but basically cosine distance says for these two points in space, what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2709" target="_blank">00:45:09.600</a></span> | <span class="t">the angle between them, rather than how far away are they? The reason we're doing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2714" target="_blank">00:45:14.560</a></span> | <span class="t">is because we're about to start using k nearest neighbors. So k nearest neighbors, we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2719" target="_blank">00:45:19.440</a></span> | <span class="t">to basically say here's the word vector we predicted, which is the word vector which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2724" target="_blank">00:45:24.600</a></span> | <span class="t">is closest to it. It turns out that in really really high dimensional space, the concept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2730" target="_blank">00:45:30.520</a></span> | <span class="t">of how far away something is, is nearly meaningless. And the reason why is that in really really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2736" target="_blank">00:45:36.440</a></span> | <span class="t">high dimensional space, everything sits on the edge of that space. Basically because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2743" target="_blank">00:45:43.960</a></span> | <span class="t">you can imagine as you add each additional dimension, the probability that something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2748" target="_blank">00:45:48.480</a></span> | <span class="t">is on the edge in that dimension, let's say the probability that it's right on the edge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2753" target="_blank">00:45:53.280</a></span> | <span class="t">is like 1/10. Then if you've only got one dimension, you've got a probability of 1/10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2758" target="_blank">00:45:58.000</a></span> | <span class="t">It's on the edge in one dimension. If you've got two dimensions, it's basically multiplicatively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2763" target="_blank">00:46:03.800</a></span> | <span class="t">decreasing the probability that that happens. So in a few hundred dimensional spaces, everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2769" target="_blank">00:46:09.440</a></span> | <span class="t">is on the edge. And when everything's on the edge, everything is kind of an equal distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2774" target="_blank">00:46:14.480</a></span> | <span class="t">away from each other, more or less. And so distances aren't very helpful. But the angle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2779" target="_blank">00:46:19.560</a></span> | <span class="t">between things varies. So when you're doing anything with trying to find nearest neighbors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2788" target="_blank">00:46:28.360</a></span> | <span class="t">it's a really good idea to train things using cosine distance. And this is the formula for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2794" target="_blank">00:46:34.920</a></span> | <span class="t">cosine distance. Again, this is one of these things where I'm skipping over something that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2800" target="_blank">00:46:40.460</a></span> | <span class="t">you'd probably spend a week in undergrad studying. There's heaps of information about cosine distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2806" target="_blank">00:46:46.660</a></span> | <span class="t">on the web. So for those of you already familiar with it, I won't waste your time. For those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2810" target="_blank">00:46:50.640</a></span> | <span class="t">of you not, it's a very very good idea to become familiar with this. And feel free to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2816" target="_blank">00:46:56.360</a></span> | <span class="t">ask on the forums if you can't find any material that makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2822" target="_blank">00:47:02.000</a></span> | <span class="t">So we've fitted our linear model. As per usual, we save our weights. And we can see how we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2827" target="_blank">00:47:07.940</a></span> | <span class="t">going. So what we've got now is something where we can fit in an image, and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2834" target="_blank">00:47:14.720</a></span> | <span class="t">spit out a word vector. But it's something that looks like a word vector. It has the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2840" target="_blank">00:47:20.600</a></span> | <span class="t">same dimensionality as a word vector. But it's very unlikely that it's going to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2845" target="_blank">00:47:25.120</a></span> | <span class="t">exact same vector as one of our thousand target word vectors. So if the word vector for a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2852" target="_blank">00:47:32.800</a></span> | <span class="t">pug is this list of 200 floats, even if we have a perfectly puggy pug, we're not going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2860" target="_blank">00:47:40.680</a></span> | <span class="t">to get that exact list of 2000 floats. We'll have something that is similar. And when we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2866" target="_blank">00:47:46.120</a></span> | <span class="t">say similar, we probably mean that the cosine distance between the perfect platonic pug</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2871" target="_blank">00:47:51.560</a></span> | <span class="t">and our pug is pretty small. So that's why after we get our predictions, we then have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2885" target="_blank">00:48:05.800</a></span> | <span class="t">to use nearest neighbors as a second step to basically say, for each of those predictions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2891" target="_blank">00:48:11.520</a></span> | <span class="t">what are the three word vectors that are the closest to that prediction?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2898" target="_blank">00:48:18.440</a></span> | <span class="t">So we can now take those nearest neighbors and find out for a bunch of our images what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2905" target="_blank">00:48:25.100</a></span> | <span class="t">are the three things it thinks it might be. For example, for this image here, its best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2912" target="_blank">00:48:32.080</a></span> | <span class="t">guess was trombone, next was flute, and third was cello. This gives us some hope that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2919" target="_blank">00:48:39.520</a></span> | <span class="t">approach seems to be working okay. It's not great yet, but it's recognized these things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2924" target="_blank">00:48:44.560</a></span> | <span class="t">are musical instruments, and its third guess was in fact the correct musical instrument.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2929" target="_blank">00:48:49.320</a></span> | <span class="t">So we know what to do next. What we do next is to fine-tune more layers. And because we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2935" target="_blank">00:48:55.240</a></span> | <span class="t">have already saved the intermediate results from an earlier layer, that fine-tuning is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2940" target="_blank">00:49:00.640</a></span> | <span class="t">going to be much faster to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2942" target="_blank">00:49:02.960</a></span> | <span class="t">Two more things I briefly mentioned. One is that there's a couple of different ways to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2948" target="_blank">00:49:08.920</a></span> | <span class="t">do nearest neighbors. One is what's called the brute force approach, which is literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2953" target="_blank">00:49:13.200</a></span> | <span class="t">to go through everyone and see how far away it is. There's another approach which is approximate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2963" target="_blank">00:49:23.960</a></span> | <span class="t">nearest neighbors. And when you've got lots and lots of things, you're trying to look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2968" target="_blank">00:49:28.640</a></span> | <span class="t">for nearest neighbors, the brute force approach is going to be n^2 time. It's going to be super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2975" target="_blank">00:49:35.000</a></span> | <span class="t">slow. Approximate nearest neighbors are generally n log n time. So orders of magnitude faster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2984" target="_blank">00:49:44.480</a></span> | <span class="t">if you've got a large dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2988" target="_blank">00:49:48.360</a></span> | <span class="t">The particular approach I'm using here is something called locality-sensitive hashing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2993" target="_blank">00:49:53.120</a></span> | <span class="t">It's a fascinating and wonderful algorithm. Anybody who's interested in algorithms, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=2997" target="_blank">00:49:57.840</a></span> | <span class="t">strongly recommend you go read about it. Let me know if you need a hand with it. My favorite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3004" target="_blank">00:50:04.900</a></span> | <span class="t">kind of algorithms are these approximate algorithms. In data science, you almost never need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3013" target="_blank">00:50:13.280</a></span> | <span class="t">know something exactly, yet nearly every algorithm that people learn at university and certainly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3018" target="_blank">00:50:18.640</a></span> | <span class="t">at high school are exact. We learn exact nearest neighbor algorithms and exact indexing algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3024" target="_blank">00:50:24.520</a></span> | <span class="t">and exact median algorithms. Pretty much for every algorithm out there, there's an approximate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3029" target="_blank">00:50:29.800</a></span> | <span class="t">version that runs an order of n or log n over n faster. One of the cool things is that once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3038" target="_blank">00:50:38.960</a></span> | <span class="t">you start realizing that, you suddenly discover that all of the libraries you've been using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3042" target="_blank">00:50:42.400</a></span> | <span class="t">for ages were written by people who didn't know this. And then you realize that every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3047" target="_blank">00:50:47.520</a></span> | <span class="t">sub-algorithm they've written, they could have used an approximate version. The next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3051" target="_blank">00:50:51.160</a></span> | <span class="t">thing you've got to know, you've got something that runs a thousand times faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3055" target="_blank">00:50:55.040</a></span> | <span class="t">The other cool thing about approximate algorithms is that they're generally written to provably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3060" target="_blank">00:51:00.440</a></span> | <span class="t">be accurate to within so close. And it can tell you with your parameters how close is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3065" target="_blank">00:51:05.760</a></span> | <span class="t">so close, which means that if you want to make it more accurate, you run it more times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3072" target="_blank">00:51:12.400</a></span> | <span class="t">with different random seeds. This thing called LSH forest is a locality-sensitive hashing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3078" target="_blank">00:51:18.680</a></span> | <span class="t">forest which means it creates a bunch of these locality-sensitive hashes. And the amazingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3084" target="_blank">00:51:24.000</a></span> | <span class="t">great thing about approximate algorithms is that each time you create another version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3088" target="_blank">00:51:28.360</a></span> | <span class="t">of it, you're exponentially increasing the accuracy, or multiplicatively increasing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3094" target="_blank">00:51:34.200</a></span> | <span class="t">accuracy, but only linearly increasing the time. So if the error on one call of LSH was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3103" target="_blank">00:51:43.480</a></span> | <span class="t">e, then the error on two calls is 1 - e^2. And 3 calls is 1 - e^3. And the time you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3113" target="_blank">00:51:53.600</a></span> | <span class="t">taking is now 2n and 3n. So when you've got something where you can make it as accurate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3121" target="_blank">00:52:01.160</a></span> | <span class="t">as you like with only linear increasing time, this is incredibly powerful. This is a great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3128" target="_blank">00:52:08.440</a></span> | <span class="t">approximation algorithm. I wish we had more time, so I'd love to tell you all about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3136" target="_blank">00:52:16.860</a></span> | <span class="t">So I generally use LSH forest when I'm doing nearest neighbors because it's arbitrarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3141" target="_blank">00:52:21.400</a></span> | <span class="t">close and much faster when you've got lots of word vectors. The time that becomes important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3149" target="_blank">00:52:29.640</a></span> | <span class="t">is when I move beyond ImageNet, which I'm going to do now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3154" target="_blank">00:52:34.440</a></span> | <span class="t">So let's say I've got a picture, and I don't just want to say which one of the thousand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3160" target="_blank">00:52:40.620</a></span> | <span class="t">ImageNet categories is it. Which one of the 100,000 WordNet nouns is it? That's a much</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3168" target="_blank">00:52:48.240</a></span> | <span class="t">harder thing to do. And that's something that no previous model could do. When you trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3174" target="_blank">00:52:54.880</a></span> | <span class="t">an ImageNet model, the only thing you could do is recognize pictures of things that were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3178" target="_blank">00:52:58.720</a></span> | <span class="t">in ImageNet. But now we've got a word vector model, and so we can put in an image that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3186" target="_blank">00:53:06.520</a></span> | <span class="t">spits out a word vector, and that word vector could be closer to things that are not in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3191" target="_blank">00:53:11.480</a></span> | <span class="t">ImageNet at all. Or it could be some higher level of the hierarchy, so we could look for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3196" target="_blank">00:53:16.840</a></span> | <span class="t">a dog rather than a pug, or a plane rather than a 747.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3204" target="_blank">00:53:24.220</a></span> | <span class="t">So here we bring in the entire set of word vectors. I'll have to remember to share these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3210" target="_blank">00:53:30.600</a></span> | <span class="t">with you because these are actually quite hard to create. And this is where I definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3216" target="_blank">00:53:36.040</a></span> | <span class="t">want LSH_FOREST because this is going to be pretty slow. And we can now do the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3223" target="_blank">00:53:43.280</a></span> | <span class="t">And not surprisingly, it's got worse. The thing that was actually cello, now cello is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3227" target="_blank">00:53:47.680</a></span> | <span class="t">even in the top 3. So this is a harder problem. So let's try fine-tuning. So fine-tuning is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3236" target="_blank">00:53:56.360</a></span> | <span class="t">the final trick I'm going to show you, just behind you Retschoff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3240" target="_blank">00:54:00.960</a></span> | <span class="t">You might remember last week we looked at creating our word vectors, and what we did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3257" target="_blank">00:54:17.800</a></span> | <span class="t">was actually I created a list. I went to WordNet and I downloaded the whole of WordNet, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3272" target="_blank">00:54:32.680</a></span> | <span class="t">then I figured out which things were nouns, and then I used a Retschoff to pass out those,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3277" target="_blank">00:54:37.480</a></span> | <span class="t">and then I saved that. So we actually have the entirety of WordNet nouns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3285" target="_blank">00:54:45.320</a></span> | <span class="t">Because it's not a good enough model yet. So now that there's 80,000 nouns, there's a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3300" target="_blank">00:55:00.040</a></span> | <span class="t">more ways to be wrong. So when it only has to say which of these thousand things is it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3305" target="_blank">00:55:05.600</a></span> | <span class="t">that's pretty easy. Which of these 80,000 things is it? It's pretty hard. To fine-tune it, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3319" target="_blank">00:55:19.680</a></span> | <span class="t">looks very similar to our usual way of fine-tuning things, which is that we take our two models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3327" target="_blank">00:55:27.360</a></span> | <span class="t">and stick them back to back, and we're now going to train the whole thing rather than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3335" target="_blank">00:55:35.400</a></span> | <span class="t">just the linear model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3337" target="_blank">00:55:37.320</a></span> | <span class="t">The problem is that the input to this model is too big to fit in RAM. So how are we going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3346" target="_blank">00:55:46.880</a></span> | <span class="t">to call fit or fit generator when we have an array that's too big to fit in RAM? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3354" target="_blank">00:55:54.840</a></span> | <span class="t">one obvious thing to do would be to pass in the bcols array. Because to most things in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3360" target="_blank">00:56:00.280</a></span> | <span class="t">Python, a bcols array looks just like a regular array. It doesn't really look any different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3367" target="_blank">00:56:07.500</a></span> | <span class="t">The way a bcols array is actually stored is actually stored in a directory, as I'm sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3381" target="_blank">00:56:21.720</a></span> | <span class="t">you've noticed. And in that directory, it's got something called chunk length, I set it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3393" target="_blank">00:56:33.960</a></span> | <span class="t">to 32 when I created these bcols arrays. What it does is it takes every 32 images and it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3402" target="_blank">00:56:42.280</a></span> | <span class="t">puts them into a separate file. Each one of these has 32 images in it, or 32 of the leading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3415" target="_blank">00:56:55.760</a></span> | <span class="t">axis of the array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3418" target="_blank">00:56:58.400</a></span> | <span class="t">Now if you then try to take this whole array and pass it to .fit in Keras with shuffle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3428" target="_blank">00:57:08.600</a></span> | <span class="t">it's going to try and grab one thing from here and one thing from here and one thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3432" target="_blank">00:57:12.720</a></span> | <span class="t">from here. Here's the bad news. For bcols to get one thing out of a chunk, it has to read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3440" target="_blank">00:57:20.320</a></span> | <span class="t">and decompress the whole thing. It has to read and decompress 32 images in order to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3445" target="_blank">00:57:25.800</a></span> | <span class="t">you the one image you asked for. That would be a disaster. That would be ridiculously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3450" target="_blank">00:57:30.640</a></span> | <span class="t">horribly slow. We didn't have to worry about that when we called predict on batch. We were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3459" target="_blank">00:57:39.960</a></span> | <span class="t">going not shuffling, but we were going in order. So it was just grabbing one. It was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3469" target="_blank">00:57:49.520</a></span> | <span class="t">never grabbing a single image out of a chunk. But now that we want to shuffle, it would.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3478" target="_blank">00:57:58.240</a></span> | <span class="t">So what we've done is somebody very helpfully actually on a Kaggle forum provided something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3487" target="_blank">00:58:07.080</a></span> | <span class="t">called a bcols array iterator. The bcols array iterator, which was kindly discovered on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3499" target="_blank">00:58:19.000</a></span> | <span class="t">forums by somebody named MP Janssen, originally written by this fellow, provides a Keras-compatible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3514" target="_blank">00:58:34.120</a></span> | <span class="t">generator which grabs an entire chunk at a time. So it's a little bit less random, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3523" target="_blank">00:58:43.680</a></span> | <span class="t">given that if this has got 2 million images in and the chunk length is 32, then it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3529" target="_blank">00:58:49.480</a></span> | <span class="t">to basically create a batch of chunks rather than a batch of images. And so that means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3536" target="_blank">00:58:56.700</a></span> | <span class="t">we have none of the performance problems, and particularly because we randomly shuffled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3543" target="_blank">00:59:03.120</a></span> | <span class="t">our files. So this whole thing is randomly shuffled anyway. So this is a good trick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3550" target="_blank">00:59:10.140</a></span> | <span class="t">So you'll find the bcols array iterator on GitHub. Feel free to take a look at the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3557" target="_blank">00:59:17.000</a></span> | <span class="t">It's pretty straightforward. There were a few issues with the original version, so MP</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3565" target="_blank">00:59:25.560</a></span> | <span class="t">Janssen and I have tried to fix it up and I've written some tests for it and he's written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3569" target="_blank">00:59:29.320</a></span> | <span class="t">some documentation for it. But if you just want to use it, then it's as simple as writing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3576" target="_blank">00:59:36.520</a></span> | <span class="t">this. Blah equals bcols array iterator, this is your data, these are your labels, shuffle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3585" target="_blank">00:59:45.240</a></span> | <span class="t">equals true, batch size equals whatever, and then you can just call fit generator as per</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3589" target="_blank">00:59:49.640</a></span> | <span class="t">usual passing in that iterator and that iterator's number of items.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3598" target="_blank">00:59:58.200</a></span> | <span class="t">So to all of you guys who have been asking how to deal with data that's bigger than memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3606" target="_blank">01:00:06.240</a></span> | <span class="t">this is how you do it. So hopefully that will make life easier for a lot of people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3614" target="_blank">01:00:14.000</a></span> | <span class="t">So we fine-tune it for a while, we do some learning annealing for a while, and this basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3621" target="_blank">01:00:21.880</a></span> | <span class="t">runs overnight for me. It takes about 6 hours to run. And so I come back the next morning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3629" target="_blank">01:00:29.280</a></span> | <span class="t">and I just copy and paste my k nearest neighbors, so I get my predicted word vectors. For each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3638" target="_blank">01:00:38.040</a></span> | <span class="t">word vector, I then pass it into nearest neighbors. This is my just 1000 categories. And lo and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3645" target="_blank">01:00:45.280</a></span> | <span class="t">behold, we now have cello in the top spot as we hoped.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3650" target="_blank">01:00:50.880</a></span> | <span class="t">How did it go in the harder problem of looking at the 100,000 or so nouns in English? Pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3656" target="_blank">01:00:56.520</a></span> | <span class="t">good. I've got this one right. And just to pick another one at random, let's pick the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3661" target="_blank">01:01:01.680</a></span> | <span class="t">first one. It said throne. That sure looks like a throne. So looking pretty good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3669" target="_blank">01:01:09.120</a></span> | <span class="t">So here's something interesting. Now that we have brought images and words into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3674" target="_blank">01:01:14.440</a></span> | <span class="t">same space, let's play with it some more. So why don't we use nearest neighbors with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3682" target="_blank">01:01:22.400</a></span> | <span class="t">those predictions? To the word vector which Google created, but the subset of those which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3707" target="_blank">01:01:47.200</a></span> | <span class="t">are nouns according to WordNet, map to their sin set IDs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3720" target="_blank">01:02:00.200</a></span> | <span class="t">The word vectors are just the word2vec vectors that we can download off the internet. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3725" target="_blank">01:02:05.640</a></span> | <span class="t">were pre-trained by Google. We're saying here is this image spits out a vector from a thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3750" target="_blank">01:02:30.280</a></span> | <span class="t">we just trained. We have 100,000 word2vec vectors for all the nouns in English. Which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3758" target="_blank">01:02:38.480</a></span> | <span class="t">one of those is the closest to the thing that came out of our model? And the answer was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3764" target="_blank">01:02:44.520</a></span> | <span class="t">throne.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3765" target="_blank">01:02:45.520</a></span> | <span class="t">Hold that thought. We'll be doing language translation starting next week. No, we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3779" target="_blank">01:02:59.780</a></span> | <span class="t">quite do it that way, but you can think of it like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3782" target="_blank">01:03:02.800</a></span> | <span class="t">So let's do something interesting. Let's create a nearest neighbors not for all of the word2vec</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3791" target="_blank">01:03:11.120</a></span> | <span class="t">vectors, but for all of our image-predicted vectors. And now we can do the opposite. Let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3797" target="_blank">01:03:17.360</a></span> | <span class="t">take a word, we pick it random. Let's look it up in our word2vec dictionary, and let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3803" target="_blank">01:03:23.120</a></span> | <span class="t">find the nearest neighbors for that in our images. There it is. So this is pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3816" target="_blank">01:03:36.200</a></span> | <span class="t">You can now find the images that are the most like whatever word you come up with. Okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3826" target="_blank">01:03:46.560</a></span> | <span class="t">that's crazy, but we can do crazier. Here is a random thing I picked. Now notice I picked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3832" target="_blank">01:03:52.520</a></span> | <span class="t">it from the validation set of ImageNet, so we've never seen this image before. And honestly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3837" target="_blank">01:03:57.100</a></span> | <span class="t">when I opened it up, my heart sank because I don't know what it is. So this is a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3842" target="_blank">01:04:02.800</a></span> | <span class="t">What is that? So what we can do is we can call.predict on that image, and we can then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3855" target="_blank">01:04:15.720</a></span> | <span class="t">do a nearest neighbors of all of our other images. There's the first, there's the second,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3863" target="_blank">01:04:23.880</a></span> | <span class="t">and the third one is even somebody putting their hand on it, which is slightly crazy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3870" target="_blank">01:04:30.440</a></span> | <span class="t">but that was what the original one looked like. In fact, if I can find it, I ran it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3880" target="_blank">01:04:40.600</a></span> | <span class="t">again on a different image. I actually looked around for something weird. This is pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3894" target="_blank">01:04:54.200</a></span> | <span class="t">weird, right? Is this a net or is it a fish? So when we then ask for nearest neighbors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3899" target="_blank">01:04:59.840</a></span> | <span class="t">we get fish in nets. So it's like, I don't know, sometimes deep learning is so magic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3907" target="_blank">01:05:07.040</a></span> | <span class="t">you just kind of go out and they're possibly what's just behind you, Rachel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3914" target="_blank">01:05:14.240</a></span> | <span class="t">Only a little bit, and maybe in a future course we might look at Dask. I think maybe even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3933" target="_blank">01:05:33.000</a></span> | <span class="t">in your numerical and algebra course you might be looking at Dask. I don't think we'll cover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3937" target="_blank">01:05:37.360</a></span> | <span class="t">it this course. But do look at Dask, D-A-S-K, it's super cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3948" target="_blank">01:05:48.400</a></span> | <span class="t">No, not at all. So these were actually labeled as this particular kind of fish. In fact that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3961" target="_blank">01:06:01.960</a></span> | <span class="t">the other thing is it's not only found fish in nets, but it's actually found more or less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3965" target="_blank">01:06:05.960</a></span> | <span class="t">the same breed of fish in the nets. But when we called dot predict on those, it created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3977" target="_blank">01:06:17.520</a></span> | <span class="t">a word vector which was probably like halfway between that kind of fish and a net because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3987" target="_blank">01:06:27.080</a></span> | <span class="t">it doesn't know what to do, right? So sometimes when it sees things like that, it would have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3990" target="_blank">01:06:30.800</a></span> | <span class="t">been marked in imageNet as a net, and sometimes it would have been a fish. So the best way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=3995" target="_blank">01:06:35.200</a></span> | <span class="t">to minimize the loss function would have been to kind of hedge. So it hedged and as a result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4000" target="_blank">01:06:40.760</a></span> | <span class="t">the images that were closest were the ones which actually were halfway between the two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4005" target="_blank">01:06:45.080</a></span> | <span class="t">themselves. So it's kind of a convenient accident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4008" target="_blank">01:06:48.840</a></span> | <span class="t">You absolutely can and I have, but really for nearest neighbors I haven't found anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4038" target="_blank">01:07:18.680</a></span> | <span class="t">nearly as good as cosine and that's true in all of the things I looked up as well. By</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4045" target="_blank">01:07:25.640</a></span> | <span class="t">the way, I should mention when you use locality-sensitive hashing in Python, by default it uses something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4052" target="_blank">01:07:32.280</a></span> | <span class="t">that's equivalent to the cosine metric, so that's why the nearest neighbors work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4056" target="_blank">01:07:36.520</a></span> | <span class="t">So starting next week we're going to be learning about sequence-to-sequence models and memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4070" target="_blank">01:07:50.880</a></span> | <span class="t">and attention methods. They're going to show us how we can take an input such as a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4076" target="_blank">01:07:56.960</a></span> | <span class="t">in English and an output such as a sentence in French, which is the particular case study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4082" target="_blank">01:08:02.280</a></span> | <span class="t">we're going to be spending 2 or 3 weeks on. When you combine that with this, you get image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4087" target="_blank">01:08:07.680</a></span> | <span class="t">captioning. I'm not sure if we're going to have time to do it ourselves, but it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4092" target="_blank">01:08:12.480</a></span> | <span class="t">literally be trivial for you guys to take the two things and combine them and do image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4098" target="_blank">01:08:18.680</a></span> | <span class="t">captioning. It's just those two techniques together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4105" target="_blank">01:08:25.940</a></span> | <span class="t">So we're now going to switch to -- actually before we take a break, I want to show you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4115" target="_blank">01:08:35.760</a></span> | <span class="t">the homework. Hopefully you guys noticed I gave you some tips because it was a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4122" target="_blank">01:08:42.440</a></span> | <span class="t">challenging one. Even though in a sense it was kind of straightforward, which was take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4128" target="_blank">01:08:48.240</a></span> | <span class="t">everything that we've already learned about super-resolution and slightly change the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4131" target="_blank">01:08:51.960</a></span> | <span class="t">function so that it does perceptual losses for style transfer instead, the details were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4136" target="_blank">01:08:56.840</a></span> | <span class="t">tricky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4137" target="_blank">01:08:57.840</a></span> | <span class="t">I'm going to quickly show you two things. First of all, I'm going to show you how I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4141" target="_blank">01:09:01.600</a></span> | <span class="t">did the homework because I actually hadn't done it last week. Luckily I have enough RAM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4147" target="_blank">01:09:07.640</a></span> | <span class="t">that I could read the two things all into memory, so don't forget you can just do that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4151" target="_blank">01:09:11.680</a></span> | <span class="t">with a bcols array to return it into a NumPy array in memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4157" target="_blank">01:09:17.520</a></span> | <span class="t">So one thing I did was I created my up-sampling block to get rid of the checkerboard patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4162" target="_blank">01:09:22.920</a></span> | <span class="t">That was literally as simple as saying up-sampling 2D and then a 1x1 conv. So that got rid of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4169" target="_blank">01:09:29.040</a></span> | <span class="t">my checkerboard patterns. The next thing I did was I changed my loss function and I decided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4178" target="_blank">01:09:38.420</a></span> | <span class="t">before I tried to do style transfer with perceptual losses, let's try and do super-resolution with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4186" target="_blank">01:09:46.400</a></span> | <span class="t">multiple content-loss layers. That's one thing I'm going to have to do for style transfer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4192" target="_blank">01:09:52.560</a></span> | <span class="t">is be able to use multiple layers. So I always like to start with something that works and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4197" target="_blank">01:09:57.720</a></span> | <span class="t">make small little changes so it keeps working at every point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4201" target="_blank">01:10:01.640</a></span> | <span class="t">So in this case, I thought, let's first of all slightly change the loss function for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4208" target="_blank">01:10:08.760</a></span> | <span class="t">super-resolution so that it uses multiple layers. So here's how I did that. I changed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4214" target="_blank">01:10:14.720</a></span> | <span class="t">my get output layer. Sorry, I changed my BGG content so it created a list of outputs, conv1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4224" target="_blank">01:10:24.920</a></span> | <span class="t">from each of the first, second and third blocks. And then I changed my loss function so it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4232" target="_blank">01:10:32.800</a></span> | <span class="t">went through and added the mean squared difference for each of those three layers. I also decided</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4240" target="_blank">01:10:40.000</a></span> | <span class="t">to add a weight just for fun. So I decided to go 0.1, 0.8, 0.1 because this is the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4246" target="_blank">01:10:46.280</a></span> | <span class="t">that they used in the paper. But let's have a little bit of more precise super-resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4253" target="_blank">01:10:53.320</a></span> | <span class="t">and a little bit of more semantic super-resolution and see how it goes. I created this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4259" target="_blank">01:10:59.880</a></span> | <span class="t">to do a more general mean squared error. And that was basically it. Other than that line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4267" target="_blank">01:11:07.960</a></span> | <span class="t">everything else was the same, so that gave me super-resolution working on multiple layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4275" target="_blank">01:11:15.960</a></span> | <span class="t">One of the things I found fascinating is that this is the original low-res, and it's done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4282" target="_blank">01:11:22.560</a></span> | <span class="t">a good job of upscaling it, but it's also fixed up the weird white balance, which really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4287" target="_blank">01:11:27.880</a></span> | <span class="t">surprised me. It's taken this obviously over-yellow shot, and this is what ceramic should look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4294" target="_blank">01:11:34.880</a></span> | <span class="t">like, it should be white. And somehow it's adjusted everything, so the serviette or whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4300" target="_blank">01:11:40.040</a></span> | <span class="t">it is in the background has gone from a yellowy-brown to a nice white, as with these cups here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4305" target="_blank">01:11:45.560</a></span> | <span class="t">It's figured out that these slightly pixelated things are actually meant to be upside-down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4309" target="_blank">01:11:49.080</a></span> | <span class="t">handles. This is on only 20,000 images. I'm very surprised that it's fixing the color</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4319" target="_blank">01:11:59.200</a></span> | <span class="t">because we never asked it to, but I guess it knows what a cup is meant to look like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4326" target="_blank">01:12:06.800</a></span> | <span class="t">and so this is what it's decided to do, is to make a cup the way it thinks it's meant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4330" target="_blank">01:12:10.900</a></span> | <span class="t">to look. So that was pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4338" target="_blank">01:12:18.640</a></span> | <span class="t">So then to go from there to style-transfer was pretty straightforward. I had to read</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4343" target="_blank">01:12:23.040</a></span> | <span class="t">in my style as before. This is the code to do this special kind of resnet block where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4350" target="_blank">01:12:30.560</a></span> | <span class="t">we use valid convolutions, which means we lose two pixels each time, and so therefore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4356" target="_blank">01:12:36.920</a></span> | <span class="t">we have to do a center crop. So don't forget, lambda layers are great for this kind of thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4363" target="_blank">01:12:43.080</a></span> | <span class="t">Whatever code you can write, chuck it in a lambda layer, and suddenly it's a Keras layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4367" target="_blank">01:12:47.560</a></span> | <span class="t">So do my center crop. This is now a resnet block which does valid comms. This is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4374" target="_blank">01:12:54.920</a></span> | <span class="t">all exactly the same. We have to do a few downsamplings, and then the computation, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4380" target="_blank">01:13:00.720</a></span> | <span class="t">our upsampling, just like the supplemental paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4385" target="_blank">01:13:05.420</a></span> | <span class="t">So the loss function looks a lot like the loss function did before, but we've got two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4390" target="_blank">01:13:10.760</a></span> | <span class="t">extra things. One is the Gram matrix. So here is a version of the Gram matrix which works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4397" target="_blank">01:13:17.160</a></span> | <span class="t">a batch at a time. If any of you tried to do this a single image at a time, you would have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4401" target="_blank">01:13:21.640</a></span> | <span class="t">gone crazy with how slow it took. I saw a few of you trying to do that. So here's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4406" target="_blank">01:13:26.480</a></span> | <span class="t">batch-wise version of Gram matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4410" target="_blank">01:13:30.040</a></span> | <span class="t">And then the second thing I needed to do was somehow feed in my style target. Another thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4415" target="_blank">01:13:35.280</a></span> | <span class="t">I saw some of you do was feed in the style target every time feed in that array into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4425" target="_blank">01:13:45.320</a></span> | <span class="t">your loss function. You can obviously calculate your style target by just calling .predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4433" target="_blank">01:13:53.640</a></span> | <span class="t">with the thing which gives you all your different style target layers, but the problem is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4438" target="_blank">01:13:58.880</a></span> | <span class="t">thing here returns a NumPy array. It's a pretty big NumPy array, which means that then when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4444" target="_blank">01:14:04.600</a></span> | <span class="t">you want to use it as a style target in training, it has to copy that back to the GPU. And copying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4451" target="_blank">01:14:11.680</a></span> | <span class="t">to the GPU is very, very slow. And this is a really big thing to copy to the GPU. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4456" target="_blank">01:14:16.760</a></span> | <span class="t">any of you who tried this, and I saw some of you try it, it took forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4461" target="_blank">01:14:21.760</a></span> | <span class="t">So here's the trick. Call .variable on it. Turning something into a variable picks it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4469" target="_blank">01:14:29.440</a></span> | <span class="t">on the GPU for you. So once you've done that, you can now treat this as a list of symbolic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4477" target="_blank">01:14:37.640</a></span> | <span class="t">entities which are the GPU versions of this. So I can now use this inside my GPU code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4486" target="_blank">01:14:46.600</a></span> | <span class="t">So here are my style targets I can use inside my loss function, and it doesn't have to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4497" target="_blank">01:14:57.800</a></span> | <span class="t">any copying backwards and forwards. So there's a subtlety, but if you don't get that subtlety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4503" target="_blank">01:15:03.520</a></span> | <span class="t">right, you're going to be waiting for a week or so for your code to finish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4510" target="_blank">01:15:10.000</a></span> | <span class="t">So those were the little subtleties which were necessary to get this to work. And once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4515" target="_blank">01:15:15.400</a></span> | <span class="t">you get it to work, it does exactly the same thing basically as before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4520" target="_blank">01:15:20.200</a></span> | <span class="t">So where this gets combined with device is I wanted to try something interesting, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4526" target="_blank">01:15:26.920</a></span> | <span class="t">is in the original Perceptual Losses paper, they trained it on the COCO dataset which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4534" target="_blank">01:15:34.480</a></span> | <span class="t">has 80,000 images, which didn't seem like many. I wanted to know what would happen if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4539" target="_blank">01:15:39.680</a></span> | <span class="t">we trained it on Olive ImageNet. So I did. So I decided to train a super-resolution network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4549" target="_blank">01:15:49.120</a></span> | <span class="t">on Olive ImageNet. And the code's all identical, so I'm not going to explain it. Other than,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4557" target="_blank">01:15:57.320</a></span> | <span class="t">you'll notice we don't have the square bracket colon square bracket here anymore because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4562" target="_blank">01:16:02.480</a></span> | <span class="t">we don't want to try and read in the entirety of ImageNet into RAM. So these are still b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4566" target="_blank">01:16:06.600</a></span> | <span class="t">coles arrays. All the other code is identical until we get to here. So I use a bcoles array</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4579" target="_blank">01:16:19.760</a></span> | <span class="t">iterator. I can't just call .fit because .fit or .fit generator assumes that your iterator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4593" target="_blank">01:16:33.160</a></span> | <span class="t">is returning your data and your labels. In our case, we don't have data and labels. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4600" target="_blank">01:16:40.920</a></span> | <span class="t">have two things that both get fed in as two inputs, and our labels are just a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4606" target="_blank">01:16:46.720</a></span> | <span class="t">zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4607" target="_blank">01:16:47.720</a></span> | <span class="t">So here's a good trick. This answers your earlier question about how do you do multi-input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4616" target="_blank">01:16:56.600</a></span> | <span class="t">models on large datasets. The answer is create your own training loop which loops through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4625" target="_blank">01:17:05.200</a></span> | <span class="t">a bunch of iterations, and then you can grab as many batches of data from as many different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4631" target="_blank">01:17:11.160</a></span> | <span class="t">iterators as you like, and then call train on batch. So in my case, my bcoles array iterator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4638" target="_blank">01:17:18.720</a></span> | <span class="t">is going to return my high resolution and low resolution batch of images. So I go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4644" target="_blank">01:17:24.000</a></span> | <span class="t">a bunch of iterations, grab one batch of high res and low res images, and pass them as my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4651" target="_blank">01:17:31.240</a></span> | <span class="t">two inputs to train on batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4654" target="_blank">01:17:34.920</a></span> | <span class="t">So this is the only code I changed other than changing .fit generator to actually calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4664" target="_blank">01:17:44.600</a></span> | <span class="t">train. So as you can see, this took me 4.5 hours to train and I then decreased the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4675" target="_blank">01:17:55.440</a></span> | <span class="t">rate and I trained for another 4.5 hours. Actually, I did it overnight last night and I only had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4680" target="_blank">01:18:00.080</a></span> | <span class="t">enough time to do about half of ImageNet, so this isn't even the whole thing. But check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4684" target="_blank">01:18:04.720</a></span> | <span class="t">this out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4686" target="_blank">01:18:06.040</a></span> | <span class="t">So take that model and we're going to call .predict. This is the original high res image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4694" target="_blank">01:18:14.920</a></span> | <span class="t">Here's the low res version. And here's the version that we've created. And as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4702" target="_blank">01:18:22.360</a></span> | <span class="t">see, it's done a pretty extraordinarily good job. When you look at the original ball, there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4709" target="_blank">01:18:29.800</a></span> | <span class="t">was this kind of vague yellow thing here. It's kind of turned it into a nice little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4714" target="_blank">01:18:34.000</a></span> | <span class="t">version. You can see that her eyes was like two grey blobs. It's kind of turned it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4720" target="_blank">01:18:40.520</a></span> | <span class="t">some eyes. You could just tell that that's an A, maybe if you look carefully. Now it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4728" target="_blank">01:18:48.520</a></span> | <span class="t">very clearly an A. So you can see it does an amazing job of upscaling this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4738" target="_blank">01:18:58.120</a></span> | <span class="t">All that's still is this is a fully convolutional net and therefore is not specific to any particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4743" target="_blank">01:19:03.680</a></span> | <span class="t">input resolution. So what I can do is I can create another version of the model using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4751" target="_blank">01:19:11.280</a></span> | <span class="t">our high res as the input. So now we're going to call .predict with the high res input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4761" target="_blank">01:19:21.520</a></span> | <span class="t">and that's what we get back. So look at that, we can now see all of this detail on the basketball,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4772" target="_blank">01:19:32.440</a></span> | <span class="t">which simply, none of that really existed here. It was there, but pretty hard to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4779" target="_blank">01:19:39.160</a></span> | <span class="t">what it was. And look at her hair, this kind of grey blob here. Here you can see it knows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4791" target="_blank">01:19:51.440</a></span> | <span class="t">it's like little bits of pulled back hair. So we can take any sized image and make it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4799" target="_blank">01:19:59.080</a></span> | <span class="t">bigger. This to me is one of the most amazing results I've seen in deepwinding. When we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4808" target="_blank">01:20:08.280</a></span> | <span class="t">train something on nearly all of ImageNet, it's a single epoch, so there's definitely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4813" target="_blank">01:20:13.000</a></span> | <span class="t">no overfitting. And it's able to recognize what hair is meant to look like when pulled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4818" target="_blank">01:20:18.480</a></span> | <span class="t">back into a bun is a pretty extraordinary result, I think. Something else which I only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4825" target="_blank">01:20:25.960</a></span> | <span class="t">realized later is that it's all a bit fuzzy, right? And there's this arm in the background</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4835" target="_blank">01:20:35.440</a></span> | <span class="t">that's a bit fuzzy. The model knows that that is meant to stay fuzzy. It knows what out-of-focus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4844" target="_blank">01:20:44.680</a></span> | <span class="t">things look like. Equally cool is not just how that A is now incredibly precise and accurate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4855" target="_blank">01:20:55.320</a></span> | <span class="t">but the fact that it knows that blurry things need to stay blurry. I don't know if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4861" target="_blank">01:21:01.800</a></span> | <span class="t">as amazed as this as I am, but I thought this was a pretty cool result. We could run this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4868" target="_blank">01:21:08.240</a></span> | <span class="t">over a 24-hour period on maybe two epochs of all of ImageNet, and presumably it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4875" target="_blank">01:21:15.160</a></span> | <span class="t">get even better still. Okay, so let's take a 7-minute break and see you back here at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4879" target="_blank">01:21:19.240</a></span> | <span class="t">5 past 8. Okay, thanks everybody. That was fun. So we're going to do something else fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4901" target="_blank">01:21:41.840</a></span> | <span class="t">And that is to look at -- oh, before I continue, I did want to mention one thing in the homework</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4911" target="_blank">01:21:51.600</a></span> | <span class="t">that I changed, which is I realized in my manually created loss function, I was already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4924" target="_blank">01:22:04.400</a></span> | <span class="t">doing a mean squared error in the loss function. But then when I told Teras to make that thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4935" target="_blank">01:22:15.400</a></span> | <span class="t">as close to 0 as possible, I had to also give it a loss function, and I was giving it MSE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4941" target="_blank">01:22:21.440</a></span> | <span class="t">And effectively that was kind of squaring my squared errors, it seemed wrong. So I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4945" target="_blank">01:22:25.640</a></span> | <span class="t">changed it to M-A-E, mean absolute error. So when you look back over the notebooks, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4951" target="_blank">01:22:31.840</a></span> | <span class="t">why, because this is just to say, hey, get the loss as close to 0 as possible. I didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4959" target="_blank">01:22:39.240</a></span> | <span class="t">really want to re-square it. That didn't make any sense. So that's why you'll see that minor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4966" target="_blank">01:22:46.280</a></span> | <span class="t">change. The other thing to mention is I didn't notice that when I retrained my super resolution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4975" target="_blank">01:22:55.040</a></span> | <span class="t">on my new images that didn't have the black border, it gave good results much, much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4981" target="_blank">01:23:01.800</a></span> | <span class="t">And so I really think that thing of learning to put the black border back in seemed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4985" target="_blank">01:23:05.720</a></span> | <span class="t">take quite a lot of effort for it. So again, hopefully some of you are going to look into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4991" target="_blank">01:23:11.560</a></span> | <span class="t">that in more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=4996" target="_blank">01:23:16.200</a></span> | <span class="t">So we're going to learn about general adversarial networks. This will kind of close off our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5002" target="_blank">01:23:22.600</a></span> | <span class="t">deep dive into generative models as applied to images. And just to remind you, the purpose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5010" target="_blank">01:23:30.880</a></span> | <span class="t">of this has been to learn about generative models, not to specifically learn about super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5015" target="_blank">01:23:35.880</a></span> | <span class="t">resolution or artistic style. But remember, these things can be used to create all kinds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5022" target="_blank">01:23:42.600</a></span> | <span class="t">of images. So one of the groups is interested in taking a 2D photo and trying to turn it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5028" target="_blank">01:23:48.320</a></span> | <span class="t">into something that you can rotate in 3D, or at least show a different angle of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5032" target="_blank">01:23:52.720</a></span> | <span class="t">2D photo. And that's a great example of something that this should totally work for. It's just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5039" target="_blank">01:23:59.240</a></span> | <span class="t">a mapping from one image to some different image, which is like what would this image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5042" target="_blank">01:24:02.840</a></span> | <span class="t">look like from above versus from the front.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5047" target="_blank">01:24:07.000</a></span> | <span class="t">So keep in mind the purpose of this is just like in Part 1, we learned about classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5054" target="_blank">01:24:14.320</a></span> | <span class="t">which you can use for 1000 things. Now we're learning about generative models that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5058" target="_blank">01:24:18.200</a></span> | <span class="t">can use for different 1000 things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5060" target="_blank">01:24:20.800</a></span> | <span class="t">Now any generative model you build, you can make it better by adding on top of it again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5069" target="_blank">01:24:29.040</a></span> | <span class="t">a generative adversarial network. And this is something I don't really feel like has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5073" target="_blank">01:24:33.880</a></span> | <span class="t">been fully appreciated. People I've seen generally treat GANs as a different way of creating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5079" target="_blank">01:24:39.480</a></span> | <span class="t">a generative model. But I think of this more as like, why not create a generative model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5085" target="_blank">01:24:45.820</a></span> | <span class="t">using the kind of techniques we've been talking about. But then think of it this way. Think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5090" target="_blank">01:24:50.800</a></span> | <span class="t">of all the artistic style stuff we were doing in my terrible attempt at a Simpsons cartoon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5101" target="_blank">01:25:01.000</a></span> | <span class="t">version of a picture. It looked nothing like a Simpsons. So what would be one way to improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5109" target="_blank">01:25:09.120</a></span> | <span class="t">that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5110" target="_blank">01:25:10.120</a></span> | <span class="t">One way to improve that would be to create two networks. There would be one network that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5118" target="_blank">01:25:18.200</a></span> | <span class="t">takes our picture, which is actually not the Simpsons, and takes another picture that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5125" target="_blank">01:25:25.280</a></span> | <span class="t">is the Simpsons. And maybe we can train a neural network that takes those two images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5132" target="_blank">01:25:32.720</a></span> | <span class="t">and spits out something saying, Is that a real Simpsons image or not? And this thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5141" target="_blank">01:25:41.760</a></span> | <span class="t">we'll call the discriminator. So we could easily train a discriminator right now. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5152" target="_blank">01:25:52.560</a></span> | <span class="t">just a classification network. Just use the same techniques we used in Part 1. We feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5157" target="_blank">01:25:57.720</a></span> | <span class="t">it the two images, and it's going to spit out a 1 if it's a real Simpsons cartoon, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5164" target="_blank">01:26:04.040</a></span> | <span class="t">a 0 if it's Jeremy's crappy generative model of Simpsons. That's easy, right? We know how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5169" target="_blank">01:26:09.960</a></span> | <span class="t">to do that right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5174" target="_blank">01:26:14.360</a></span> | <span class="t">Now, go and build another model. There's two images as inputs. So you would feed it one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5197" target="_blank">01:26:37.520</a></span> | <span class="t">thing that's a Simpsons and one thing that's a generative output. It's up to you to feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5203" target="_blank">01:26:43.560</a></span> | <span class="t">it one of each. Or alternatively, you could feed it one thing. In fact, probably easier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5212" target="_blank">01:26:52.120</a></span> | <span class="t">is to just feed it one thing and it spits out, Is it the Simpsons or isn't it the Simpsons?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5217" target="_blank">01:26:57.360</a></span> | <span class="t">And you could just mix them and match them. Actually, it's the latter that we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5220" target="_blank">01:27:00.720</a></span> | <span class="t">to do, so that's probably easier. We're going to have one thing which is either not a Simpsons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5231" target="_blank">01:27:11.720</a></span> | <span class="t">or it is a Simpsons, and we're going to have a mix of 50/50 of those two, and we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5238" target="_blank">01:27:18.360</a></span> | <span class="t">to have something come out saying, "What do you think? Is it real or not?" So this thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5245" target="_blank">01:27:25.480</a></span> | <span class="t">this discriminator, from now on we'll probably generally be calling it D. So there's a thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5250" target="_blank">01:27:30.400</a></span> | <span class="t">called D. And we can think of that as a function. D is a function that takes some input, x, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5258" target="_blank">01:27:38.760</a></span> | <span class="t">is an image, and spits out a 1 or a 0, or maybe a probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5268" target="_blank">01:27:48.360</a></span> | <span class="t">So what we could now do is create another neural network. And what this neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5275" target="_blank">01:27:55.200</a></span> | <span class="t">is going to do is it's going to take as input some random noise, just like all of our generators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5283" target="_blank">01:28:03.440</a></span> | <span class="t">have so far. And it's going to spit out an image. And the loss function is going to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5294" target="_blank">01:28:14.120</a></span> | <span class="t">if you take that image and stick it through D, did you manage to fool it? So could you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5304" target="_blank">01:28:24.440</a></span> | <span class="t">create something where in fact we wanted to say, "Oh yeah, totally, that's a real Simpsons."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5312" target="_blank">01:28:32.160</a></span> | <span class="t">So if that was our loss function, we're going to call the generator, we'll call it G. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5317" target="_blank">01:28:37.440</a></span> | <span class="t">just something exactly like our perceptual losses style transfer model. It could be exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5322" target="_blank">01:28:42.960</a></span> | <span class="t">the same model. But the loss function is now going to be take the output of that and stick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5328" target="_blank">01:28:48.840</a></span> | <span class="t">it through D, the discriminator, and try to trick it. So the generator is doing well if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5335" target="_blank">01:28:55.680</a></span> | <span class="t">the discriminator is getting it wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5339" target="_blank">01:28:59.320</a></span> | <span class="t">So one way to do this would be to take our discriminator and train it as best as we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5345" target="_blank">01:29:05.960</a></span> | <span class="t">to recognize the difference between our crappy Simpsons and real Simpsons, and then get a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5351" target="_blank">01:29:11.680</a></span> | <span class="t">generator and train it to trick that discriminator. But now at the end of that, it's probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5357" target="_blank">01:29:17.560</a></span> | <span class="t">still not very good because you realize that actually the discriminator didn't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5361" target="_blank">01:29:21.320</a></span> | <span class="t">be very good before because my Simpsons generators were so bad. So I could now go back and retrain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5367" target="_blank">01:29:27.200</a></span> | <span class="t">the discriminator based on my better generated images, and then I could go back and retrain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5373" target="_blank">01:29:33.360</a></span> | <span class="t">the generator. And back and forth I go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5377" target="_blank">01:29:37.560</a></span> | <span class="t">And that is the general approach of a GAN, is to keep going back between two things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5383" target="_blank">01:29:43.440</a></span> | <span class="t">which is training a discriminator and training a generator using a discriminator as a loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5390" target="_blank">01:29:50.400</a></span> | <span class="t">function. So we've got one thing which is discriminator on some image, and another thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5399" target="_blank">01:29:59.200</a></span> | <span class="t">which is a discriminator on a generator on some noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5417" target="_blank">01:30:17.480</a></span> | <span class="t">In practice, these things are going to spit out probabilities. So that's the general idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5429" target="_blank">01:30:29.960</a></span> | <span class="t">In practice, they found it very difficult to do this like train the discriminator as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5436" target="_blank">01:30:36.560</a></span> | <span class="t">best as we can, stop train the generator as best as we can, stop and so on and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5443" target="_blank">01:30:43.000</a></span> | <span class="t">So instead, the original GAN paper is called Generative Adversarial Nets. And here you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5456" target="_blank">01:30:56.240</a></span> | <span class="t">can see they've actually specified this loss function. So here it is in notation. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5463" target="_blank">01:31:03.240</a></span> | <span class="t">call it minimizing the generator whilst maximizing the discriminator. This is what min max is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5471" target="_blank">01:31:11.040</a></span> | <span class="t">referring to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5474" target="_blank">01:31:14.040</a></span> | <span class="t">What they do in practice is they do it a batch at a time. So they have a loop, I'm going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5478" target="_blank">01:31:18.480</a></span> | <span class="t">to go through a loop and do a single batch, put it through the discriminator, that same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5482" target="_blank">01:31:22.760</a></span> | <span class="t">batch, stick it through the generator, and so we're going to do it a batch at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5488" target="_blank">01:31:28.000</a></span> | <span class="t">So let's look at that. So here's the original GAN from that paper, and we're going to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5495" target="_blank">01:31:35.040</a></span> | <span class="t">it on MNIST. And what we're going to do is we're going to see if we can start from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5499" target="_blank">01:31:39.880</a></span> | <span class="t">to create something which can create images which the discriminator cannot tell whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5509" target="_blank">01:31:49.120</a></span> | <span class="t">they're real or fake. And it's a discriminator that has learned to be good at discriminating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5514" target="_blank">01:31:54.640</a></span> | <span class="t">real from fake pictures of MNIST images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5518" target="_blank">01:31:58.800</a></span> | <span class="t">So we're loaded in MNIST, and the first thing they do in the paper is just use a standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5524" target="_blank">01:32:04.120</a></span> | <span class="t">multilayer perceptron. So I'm just going to skip over that and let's get to the perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5532" target="_blank">01:32:12.800</a></span> | <span class="t">So here's our generator. It's just a standard multilayer perceptron. And here's our discriminator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5539" target="_blank">01:32:19.840</a></span> | <span class="t">which is also a standard multilayer perceptron. The generator has a sigmoid activation, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5546" target="_blank">01:32:26.400</a></span> | <span class="t">in other words, we're going to spit out an image where all of the pixels are between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5550" target="_blank">01:32:30.680</a></span> | <span class="t">0 and 1. So if you want to print it out, we'll just multiply it by 255, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5556" target="_blank">01:32:36.240</a></span> | <span class="t">So there's our generator, there's our discriminator. So there's then the combination of the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5562" target="_blank">01:32:42.540</a></span> | <span class="t">So take the generator and stick it into the discriminator. We can just use sequential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5566" target="_blank">01:32:46.360</a></span> | <span class="t">for that. And this is actually therefore the loss function that I want on my generator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5572" target="_blank">01:32:52.720</a></span> | <span class="t">Generate something and then see if you can fool the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5576" target="_blank">01:32:56.520</a></span> | <span class="t">So there's all my architectures set up. So the next thing I need to do is set up this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5582" target="_blank">01:33:02.980</a></span> | <span class="t">thing called train, which is going to do this adversarial training. Let's go back and have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5588" target="_blank">01:33:08.040</a></span> | <span class="t">a look at train. So what train is going to do is go through a bunch of epochs. And notice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5595" target="_blank">01:33:15.260</a></span> | <span class="t">here I wrap it in this TQDM. This is the thing that creates a nice little progress bar. Doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5600" target="_blank">01:33:20.360</a></span> | <span class="t">do anything else, it just creates a little progress bar. We learned about that last week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5605" target="_blank">01:33:25.140</a></span> | <span class="t">So the first thing I need to do is to generate some data to feed the discriminator. So I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5611" target="_blank">01:33:31.480</a></span> | <span class="t">created a little function for that. And here's my little function. So it's going to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5616" target="_blank">01:33:36.380</a></span> | <span class="t">a little bit of data that's real and a little bit of data that's fake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5620" target="_blank">01:33:40.780</a></span> | <span class="t">So my real data is okay, let's go into my actual training set and grab some randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5627" target="_blank">01:33:47.080</a></span> | <span class="t">selected MNIST digits. So that's my real bit. And then let's create some fake. So noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5637" target="_blank">01:33:57.460</a></span> | <span class="t">is a function that I've just created up here, which creates 100 random numbers. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5642" target="_blank">01:34:02.900</a></span> | <span class="t">create some noise called g.predict. And then I'll concatenate the two together. So now I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5649" target="_blank">01:34:09.920</a></span> | <span class="t">got some real data and some fake data. And so this is going to try and predict whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5656" target="_blank">01:34:16.700</a></span> | <span class="t">or not something is fake. So 1 means fake, 0 means real. So I'm going to return my data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5666" target="_blank">01:34:26.320</a></span> | <span class="t">and my labels, which is a bunch of 0s to say they're all real and a bunch of 1s to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5670" target="_blank">01:34:30.900</a></span> | <span class="t">they're all fake. So that's my discriminator's data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5674" target="_blank">01:34:34.620</a></span> | <span class="t">So go ahead and create a set of data for the discriminator, and then do one batch of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5685" target="_blank">01:34:45.500</a></span> | <span class="t">Now I'm going to do the same thing for the generator. But when I train the generator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5690" target="_blank">01:34:50.220</a></span> | <span class="t">I don't want to change the discriminator's weights. So make_trainable simply goes through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5696" target="_blank">01:34:56.100</a></span> | <span class="t">each layer and says it's not trainable. So make my discriminator non-trainable and do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5701" target="_blank">01:35:01.640</a></span> | <span class="t">one batch of training where I'm taking noise as my inputs. And my goal is to get the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5710" target="_blank">01:35:10.740</a></span> | <span class="t">to think that they are actually real. So that's why I'm passing in a bunch of 0s, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5717" target="_blank">01:35:17.260</a></span> | <span class="t">remember 0 means real. And that's it. And then make discriminator trainable again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5723" target="_blank">01:35:23.120</a></span> | <span class="t">So keep looking through this. Train the discriminator on a batch of half real, half fake. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5730" target="_blank">01:35:30.860</a></span> | <span class="t">train the generator to try and trick the discriminator using all fake. Repeat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5739" target="_blank">01:35:39.340</a></span> | <span class="t">So that's the training loop. That's a basic GAN. Because we use TQDM, we get a nice little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5745" target="_blank">01:35:45.220</a></span> | <span class="t">progress bar. I kept track of the loss at each step, so there's our loss for the discriminator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5754" target="_blank">01:35:54.820</a></span> | <span class="t">and there's our loss for the generator. So our question is, what do these loss curves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5759" target="_blank">01:35:59.180</a></span> | <span class="t">mean? Are they good or bad? How do we know? And the answer is, for this kind of GAN, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5766" target="_blank">01:36:06.780</a></span> | <span class="t">mean nothing at all. The generator could get fantastic, but it could be because the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5772" target="_blank">01:36:12.580</a></span> | <span class="t">is terrible. And they don't really know whether each one is good or not, so even the order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5778" target="_blank">01:36:18.300</a></span> | <span class="t">of magnitude of both of them is meaningless. So these curves mean nothing. The direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5783" target="_blank">01:36:23.340</a></span> | <span class="t">of the curves mean nothing. And this is one of the real difficulties with training GANs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5789" target="_blank">01:36:29.980</a></span> | <span class="t">And here's what happens when I plot 12 randomly selected random noise vectors stuck through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5796" target="_blank">01:36:36.940</a></span> | <span class="t">there. And we have not got things that look terribly like MNIST digits and they also don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5801" target="_blank">01:36:41.380</a></span> | <span class="t">look terribly much like they have a lot of variety. This is called ModeClass. Very common</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5812" target="_blank">01:36:52.920</a></span> | <span class="t">problem when training GANs. And what it means is that the generator and the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5819" target="_blank">01:36:59.060</a></span> | <span class="t">have kind of reached a stalemate where neither of them basically knows how to go from here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5826" target="_blank">01:37:06.440</a></span> | <span class="t">And in terms of optimization, we've basically found a local minimum. So okay, that was not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5833" target="_blank">01:37:13.340</a></span> | <span class="t">very successful. Can we do better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5835" target="_blank">01:37:15.500</a></span> | <span class="t">So the next major paper that came along was this one. Let's go to the top so you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5845" target="_blank">01:37:25.580</a></span> | <span class="t">it. Unsupervised representation learning with deep convolutional derivative adversarial networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5851" target="_blank">01:37:31.780</a></span> | <span class="t">So this created something that they called DCGANs. And the main page that you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5859" target="_blank">01:37:39.020</a></span> | <span class="t">look at here is page 3 where they say, "Call to our approach is doing these three things."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5866" target="_blank">01:37:46.900</a></span> | <span class="t">And basically what they do is they just do exactly the same thing as GANs, but they do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5870" target="_blank">01:37:50.860</a></span> | <span class="t">three things. One is to use the kinds of -- well in fact all of them is to learn the tricks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5876" target="_blank">01:37:56.980</a></span> | <span class="t">that we've been learning for generative models. Use an all-convolutional net, get rid of max</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5883" target="_blank">01:38:03.060</a></span> | <span class="t">pooling and use strata convolutions instead, get rid of fully connected layers and use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5888" target="_blank">01:38:08.260</a></span> | <span class="t">lots of convolutional features instead, and add in batch null. And then use a CNN rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5893" target="_blank">01:38:13.580</a></span> | <span class="t">than MLP. So here is that. This will look very familiar, it looks just like last lesson stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5904" target="_blank">01:38:24.260</a></span> | <span class="t">So the generator is going to take in a random grid of inputs. It's going to do a batch norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5914" target="_blank">01:38:34.500</a></span> | <span class="t">up sample -- you'll notice that I'm doing even newer than this paper, I'm doing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5918" target="_blank">01:38:38.460</a></span> | <span class="t">up sampling approach because we know that's better. Up sample, 1x1 conv, batch norm, up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5923" target="_blank">01:38:43.960</a></span> | <span class="t">sample, 1x1 conv, batch norm, and then a final conv layer. The discriminator basically does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5931" target="_blank">01:38:51.860</a></span> | <span class="t">the opposite, which is some 2x2 sub-samplings, so down sampling in the discriminator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5940" target="_blank">01:39:00.660</a></span> | <span class="t">Another trick that I think it's mentioned in the paper is before you do the back and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5946" target="_blank">01:39:06.820</a></span> | <span class="t">forth batch for the discriminator and a batch for the generator is to train the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5953" target="_blank">01:39:13.460</a></span> | <span class="t">for a fraction of an epoch, like do a few batches through the discriminator. So at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5957" target="_blank">01:39:17.500</a></span> | <span class="t">it knows how to recognize the difference between a random image and a real image a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5963" target="_blank">01:39:23.820</a></span> | <span class="t">So you can see here I actually just start by calling discriminator.fit with just a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5969" target="_blank">01:39:29.140</a></span> | <span class="t">small amount of data. So this is kind of like bootstrapping the discriminator. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5976" target="_blank">01:39:36.300</a></span> | <span class="t">I just go ahead and call the same train as we had before with my better architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5984" target="_blank">01:39:44.340</a></span> | <span class="t">And again, these curves are totally meaningless. But we have something which if you squint,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5991" target="_blank">01:39:51.260</a></span> | <span class="t">you could almost convince yourself that that's a vibe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=5995" target="_blank">01:39:55.660</a></span> | <span class="t">So until a week or two before this forth started, this was kind of about as good as we had.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6006" target="_blank">01:40:06.260</a></span> | <span class="t">People were much better at the artisanal details of this than I was, and indeed there's a whole</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6011" target="_blank">01:40:11.700</a></span> | <span class="t">page called GANhacks, which had lots of tips. But then, a couple of weeks before this class</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6022" target="_blank">01:40:22.060</a></span> | <span class="t">started, as I mentioned in the first class, along came the Wasserstein GAN. And the Wasserstein</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6029" target="_blank">01:40:29.220</a></span> | <span class="t">GAN got rid of all of these problems. And here is the Wasserstein GAN paper. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6040" target="_blank">01:40:40.820</a></span> | <span class="t">paper is quite an extraordinary paper. And it's particularly extraordinary because -- and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6048" target="_blank">01:40:48.820</a></span> | <span class="t">I think I mentioned this in the first class of this part -- most papers tend to either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6055" target="_blank">01:40:55.060</a></span> | <span class="t">be math theory that goes nowhere, or kind of nice experiments in engineering where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6061" target="_blank">01:41:01.540</a></span> | <span class="t">theory bits kind of hacked on at the end and kind of meaningless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6066" target="_blank">01:41:06.220</a></span> | <span class="t">This paper is entirely driven by theory, and then the theory goes on to show this is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6074" target="_blank">01:41:14.380</a></span> | <span class="t">the theory means, this is what we do, and suddenly all the problems go away. The loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6078" target="_blank">01:41:18.340</a></span> | <span class="t">curves are going to actually mean something, and we're going to be able to do what I said</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6082" target="_blank">01:41:22.220</a></span> | <span class="t">we wanted to do right at the start of this GAN section, which is to train the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6089" target="_blank">01:41:29.500</a></span> | <span class="t">a whole bunch of steps and then do a generator, and then discriminator a whole bunch of steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6093" target="_blank">01:41:33.820</a></span> | <span class="t">and do the generator. And all that is going to suddenly start working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6098" target="_blank">01:41:38.780</a></span> | <span class="t">How do we get it to work? In fact, despite the fact that this paper is both long and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6107" target="_blank">01:41:47.260</a></span> | <span class="t">full of equations and theorems and proofs, and there's a whole bunch of appendices at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6113" target="_blank">01:41:53.300</a></span> | <span class="t">the back with more theorems and proofs, there's actually only two things we need to do. One</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6118" target="_blank">01:41:58.180</a></span> | <span class="t">is remove the log from the loss function. So rather than using cross-entropy loss, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6125" target="_blank">01:42:05.220</a></span> | <span class="t">just going to use mean squared error. That's one change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6128" target="_blank">01:42:08.180</a></span> | <span class="t">The second change is we're going to constrain the weights so that they lie between -0.01</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6136" target="_blank">01:42:16.180</a></span> | <span class="t">and +0.01. So we're going to constrain the weights to make them small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6140" target="_blank">01:42:20.820</a></span> | <span class="t">Now in the process of saying that's all we're going to do is to kind of massively not give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6146" target="_blank">01:42:26.980</a></span> | <span class="t">credit to this paper, because what this paper is is they figured out that that's what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6150" target="_blank">01:42:30.620</a></span> | <span class="t">need to do. On the forums, some of you have been reading through this paper and I've already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6156" target="_blank">01:42:36.820</a></span> | <span class="t">given you some tips as to some really great walkthrough. I'll put it on our wiki that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6164" target="_blank">01:42:44.420</a></span> | <span class="t">explains all the math from scratch. But basically what the math says is this, the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6172" target="_blank">01:42:52.820</a></span> | <span class="t">for a GAN is not really the loss function you put into Keras. We thought we were just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6178" target="_blank">01:42:58.300</a></span> | <span class="t">putting in a cross-entropy loss function, but in fact what we really care about is the difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6184" target="_blank">01:43:04.380</a></span> | <span class="t">between two distributions, the difference between the discriminator and the generator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6189" target="_blank">01:43:09.460</a></span> | <span class="t">And the difference between two loss functions has a very different shape for the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6195" target="_blank">01:43:15.300</a></span> | <span class="t">on its own. So it turns out that the difference between the two cross-entropy loss functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6202" target="_blank">01:43:22.460</a></span> | <span class="t">is something called the Jensen-Shannon distance. And this paper shows that that loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6212" target="_blank">01:43:32.620</a></span> | <span class="t">is hideous. It is not differentiable, and it does not have a nice smooth shape at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6224" target="_blank">01:43:44.340</a></span> | <span class="t">So it kind of explains why it is that we kept getting this mode collapse and failing to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6229" target="_blank">01:43:49.620</a></span> | <span class="t">find nice minimums. Mathematically, this loss function does not behave the way a good loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6235" target="_blank">01:43:55.980</a></span> | <span class="t">function should. And previously we've not come across anything like this because we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6242" target="_blank">01:44:02.500</a></span> | <span class="t">been training a single function at a time. We really understand those loss functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6248" target="_blank">01:44:08.980</a></span> | <span class="t">mean squared error, cross-entropy. Even though we haven't already always derived the math</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6254" target="_blank">01:44:14.160</a></span> | <span class="t">in detail, plenty of people have. We know that they're kind of nice and smooth and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6258" target="_blank">01:44:18.620</a></span> | <span class="t">they have pretty nice shapes and they do what we want them to do. In this case, by training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6263" target="_blank">01:44:23.620</a></span> | <span class="t">two things kind of adversarially to each other, we're actually doing something quite different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6269" target="_blank">01:44:29.540</a></span> | <span class="t">This paper just absolutely fantastically shows, with both examples and with theory, why that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6276" target="_blank">01:44:36.660</a></span> | <span class="t">just never going to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6289" target="_blank">01:44:49.620</a></span> | <span class="t">So the cosine distance is the difference between two things, whereas these distances that we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6302" target="_blank">01:45:02.020</a></span> | <span class="t">talking about here are the distances between two distributions, which is a much more tricky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6307" target="_blank">01:45:07.180</a></span> | <span class="t">problem to deal with. The cosine distance, actually if you look at the notebook during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6312" target="_blank">01:45:12.940</a></span> | <span class="t">the week, you'll see it's basically the same as the Euclidean distance, but you normalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6320" target="_blank">01:45:20.020</a></span> | <span class="t">the data first. So it has all the same nice properties that the Euclidean distance did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6329" target="_blank">01:45:29.460</a></span> | <span class="t">The authors of this paper released their code in PyTorch. Luckily, PyTorch, the first kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6339" target="_blank">01:45:39.820</a></span> | <span class="t">of pre-release came out in mid-January. You won't be surprised to hear that one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6345" target="_blank">01:45:45.300</a></span> | <span class="t">authors of the paper is the main author of PyTorch. So he was writing this before he</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6351" target="_blank">01:45:51.580</a></span> | <span class="t">even released the code. There's lots of reasons we want to learn PyTorch anyway, so here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6357" target="_blank">01:45:57.580</a></span> | <span class="t">a good reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6358" target="_blank">01:45:58.820</a></span> | <span class="t">So let's look at the Wasserstein GAN in PyTorch. Most of the code, in fact other than this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6365" target="_blank">01:46:05.140</a></span> | <span class="t">pretty much all the code I'm showing you in this part of the course, is very loosely based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6371" target="_blank">01:46:11.700</a></span> | <span class="t">on lots of bits of other code, which I had to massively rewrite because all of it was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6375" target="_blank">01:46:15.500</a></span> | <span class="t">wrong and hideous. This code actually I only did some minor refactoring to simplify things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6381" target="_blank">01:46:21.620</a></span> | <span class="t">so this is actually very close to their code. So it was a very nice paper with very nice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6387" target="_blank">01:46:27.220</a></span> | <span class="t">code, so that's a great thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6390" target="_blank">01:46:30.460</a></span> | <span class="t">So before we look at the Wasserstein GAN in PyTorch, let's look briefly at PyTorch. Basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6402" target="_blank">01:46:42.460</a></span> | <span class="t">what you're going to see is that PyTorch looks a lot like NumPy, which is nice. We don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6409" target="_blank">01:46:49.100</a></span> | <span class="t">have to create a computational graph using variables and placeholders and later on run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6416" target="_blank">01:46:56.120</a></span> | <span class="t">in a session. I'm sure you've seen by now Keras with TensorFlow, you try to print something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6424" target="_blank">01:47:04.540</a></span> | <span class="t">out with some intermediate output, it just prints out like Tensor and tells you how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6428" target="_blank">01:47:08.540</a></span> | <span class="t">dimensions it has. And that's because all that thing is is a symbolic part of a computational</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6434" target="_blank">01:47:14.180</a></span> | <span class="t">graph. PyTorch doesn't work that way. PyTorch is what's called a defined-by-run framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6441" target="_blank">01:47:21.780</a></span> | <span class="t">It's basically designed to be so fast to take your code and compile it that you don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6451" target="_blank">01:47:31.100</a></span> | <span class="t">to create that graph in advance. Every time you run a piece of code, it puts it on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6456" target="_blank">01:47:36.580</a></span> | <span class="t">GPU, runs it, sends it back all in one go. So it makes things look very simple. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6463" target="_blank">01:47:43.260</a></span> | <span class="t">is a slightly cut-down version of the PyTorch tutorial that PyTorch provides on their website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6469" target="_blank">01:47:49.620</a></span> | <span class="t">So you can grab that from there. So rather than creating np.array, you create torch.tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6477" target="_blank">01:47:57.780</a></span> | <span class="t">But other than that, it's identical. So here's a random torch.tensor. APIs are all a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6490" target="_blank">01:48:10.940</a></span> | <span class="t">bit different. Rather than dot shape, it's dot size. But you can see it looks very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6498" target="_blank">01:48:18.500</a></span> | <span class="t">And so unlike in TensorFlow or Theano, we can just say x + y, and there it is. We don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6505" target="_blank">01:48:25.260</a></span> | <span class="t">have to say z = x + y, f = function, x and y as inputs, set as output, and function dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6513" target="_blank">01:48:33.660</a></span> | <span class="t">a vowel. No, you just go x + y, and there it is. So you can see why it's called defined-by-run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6520" target="_blank">01:48:40.780</a></span> | <span class="t">We just provide the code and it just runs it. Generally speaking, most operations in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6526" target="_blank">01:48:46.440</a></span> | <span class="t">Torch as well as having this infix version. There's also a prefix version, so this is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6533" target="_blank">01:48:53.180</a></span> | <span class="t">the same thing. You can often in fact nearly always add an out equals, and that puts the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6539" target="_blank">01:48:59.900</a></span> | <span class="t">result in this preallocated memory. We've already talked about why it's really important</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6544" target="_blank">01:49:04.340</a></span> | <span class="t">to preallocate memory. It's particularly important on GPUs. So if you write your own algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6550" target="_blank">01:49:10.580</a></span> | <span class="t">in PyTorch, you'll need to be very careful of this. Perhaps the best trick is that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6555" target="_blank">01:49:15.740</a></span> | <span class="t">can stick an underscore on the end of most things, and it causes it to do in place. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6560" target="_blank">01:49:20.140</a></span> | <span class="t">is basically y + = x. That's what this underscore at the end means.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6566" target="_blank">01:49:26.380</a></span> | <span class="t">So there's some good little tricks. You can do slicing just like numpy. You can turn numpy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6573" target="_blank">01:49:33.300</a></span> | <span class="t">stuff into Torch tensors and vice versa by simply going dot numpy. One thing to be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6581" target="_blank">01:49:41.820</a></span> | <span class="t">aware of is that A and B are now referring to the same thing. So if I now add underscore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6592" target="_blank">01:49:52.660</a></span> | <span class="t">A + = 1, it also changes B. Vice versa, you can turn numpy into Torch by calling Torch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6602" target="_blank">01:50:02.860</a></span> | <span class="t">from numpy. And again, same thing. If you change the numpy, it changes the Torch. All of that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6611" target="_blank">01:50:11.800</a></span> | <span class="t">so far has been running on the CPU. To turn anything into something that runs on the GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6616" target="_blank">01:50:16.900</a></span> | <span class="t">you chuck dot CUDA at the end of it. So this x + y just ran on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6625" target="_blank">01:50:25.280</a></span> | <span class="t">So where things get cool is that something like this knows not just how to do that piece</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6632" target="_blank">01:50:32.220</a></span> | <span class="t">of arithmetic, but it also knows how to take the gradient of that. To make anything into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6637" target="_blank">01:50:37.700</a></span> | <span class="t">something which calculates gradients, you just take your Torch tensor, wrap it in variable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6644" target="_blank">01:50:44.900</a></span> | <span class="t">and add this parameter to it. From now on, anything I do to x, it's going to remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6649" target="_blank">01:50:49.860</a></span> | <span class="t">what I did so that it can take the gradient of it. For example, x + 2, I get x3 just like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6658" target="_blank">01:50:58.140</a></span> | <span class="t">a normal tensor. So a variable and a tensor have the same API except that I can keep doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6664" target="_blank">01:51:04.740</a></span> | <span class="t">things to it. Square times 3, dot mean. Later on, I can go dot backward and dot grad and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6674" target="_blank">01:51:14.620</a></span> | <span class="t">I can get the gradient. So that's the critical difference between a tensor and a variable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6681" target="_blank">01:51:21.780</a></span> | <span class="t">They have exactly the same API except variable also has dot backward and that gets you the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6688" target="_blank">01:51:28.740</a></span> | <span class="t">gradient. When I say dot gradient, the reason that this is dout dx is because I typed out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6695" target="_blank">01:51:35.720</a></span> | <span class="t">dot backward. So this is the thing the derivative is respect to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6704" target="_blank">01:51:44.900</a></span> | <span class="t">So this is kind of crazy. You can do things like while loops and get the gradients of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6710" target="_blank">01:51:50.260</a></span> | <span class="t">them. It's this kind of thing pretty tricky to do with TensorFlow or Theano, these kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6716" target="_blank">01:51:56.580</a></span> | <span class="t">of computation graph approaches. So it gives you a whole lot of flexibility to define things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6723" target="_blank">01:52:03.260</a></span> | <span class="t">in much more natural ways. So you can really write PyTorch just like you're writing regular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6729" target="_blank">01:52:09.660</a></span> | <span class="t">old NumPy stuff. It has plenty of libraries, so if you want to create a neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6739" target="_blank">01:52:19.700</a></span> | <span class="t">here's how you do a CNN. I warned you early on that if you don't know about OO in Python,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6746" target="_blank">01:52:26.340</a></span> | <span class="t">you need to learn it. So here's why. Because in PyTorch, everything's kind of done using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6752" target="_blank">01:52:32.100</a></span> | <span class="t">OO. I really like this. In TensorFlow, they kind of invent their own weird way of programming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6763" target="_blank">01:52:43.740</a></span> | <span class="t">rather than use Python OO. Or else PyTorch just goes, "Oh, we already have these features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6768" target="_blank">01:52:48.980</a></span> | <span class="t">in the language. Let's just use them." So it's way easier, in my opinion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6774" target="_blank">01:52:54.980</a></span> | <span class="t">So to create a neural net, you create a new class, you derive from module, and then in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6781" target="_blank">01:53:01.380</a></span> | <span class="t">the constructor, you create all of the things that have weights. So conv1 is now something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6791" target="_blank">01:53:11.980</a></span> | <span class="t">that has some weights. It's a 2D conv. Conv2 is something with some weights. PolyConnected1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6796" target="_blank">01:53:16.580</a></span> | <span class="t">is something with some weights. So there's all of your layers, and then you get to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6803" target="_blank">01:53:23.820</a></span> | <span class="t">exactly what happens in your forward pass. Because MaxPool2D doesn't have any weights,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6810" target="_blank">01:53:30.900</a></span> | <span class="t">and Relyu doesn't have any weights, there's no need to define them in the initializer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6816" target="_blank">01:53:36.900</a></span> | <span class="t">You can just call them as functions. But these things have weights, so they need to be kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6822" target="_blank">01:53:42.500</a></span> | <span class="t">of stateful and persistent. So in my forward pass, you literally just define what are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6829" target="_blank">01:53:49.380</a></span> | <span class="t">things that happen. .vue is the same as reshape. The whole API has different names for everything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6840" target="_blank">01:54:00.460</a></span> | <span class="t">which is mildly annoying for the first week, but you kind of get used to it. .reshape is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6844" target="_blank">01:54:04.180</a></span> | <span class="t">called .vue. During the week, if you try to use PyTorch and you're like, "How do you say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6849" target="_blank">01:54:09.380</a></span> | <span class="t">blah in PyTorch?" and you can't find it, feel free to post on the forum. Having said that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6855" target="_blank">01:54:15.780</a></span> | <span class="t">PyTorch has its own discourse-based forums. And as you can see, it is just as busy and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6863" target="_blank">01:54:23.980</a></span> | <span class="t">friendly as our forums. People are posting on these all the time. So I find it a really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6871" target="_blank">01:54:31.060</a></span> | <span class="t">great, helpful community. So feel free to ask over there or over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6885" target="_blank">01:54:45.360</a></span> | <span class="t">You can then put all of that computation onto the GPU by calling .kuda. You can then take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6894" target="_blank">01:54:54.740</a></span> | <span class="t">some input, put that on the GPU with .kuda. You can then calculate your derivatives, calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6904" target="_blank">01:55:04.140</a></span> | <span class="t">your loss, and then later on you can optimize it. This is just one step of the optimizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6916" target="_blank">01:55:16.240</a></span> | <span class="t">so we have to kind of put that in the word. So there's the basic pieces. At the end here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6921" target="_blank">01:55:21.180</a></span> | <span class="t">there's a complete process, but I think more fun will be to see the process in the Wasserstein</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6926" target="_blank">01:55:26.740</a></span> | <span class="t">GAN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6927" target="_blank">01:55:27.740</a></span> | <span class="t">So here it is. I've kind of got this TorchUtils thing which you'll find in GitHub which has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6935" target="_blank">01:55:35.100</a></span> | <span class="t">the basic stuff you'll want for Torch all there, so you can just import that. So we set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6945" target="_blank">01:55:45.980</a></span> | <span class="t">up the batch size, the size of each image, the size of our noise vector. And look how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6952" target="_blank">01:55:52.700</a></span> | <span class="t">cool it is. I really like this. This is how you import datasets. It has a datasets module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6959" target="_blank">01:55:59.000</a></span> | <span class="t">already in the TorchVision library. Here's the scifi10 dataset. It will automatically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6966" target="_blank">01:56:06.940</a></span> | <span class="t">download it to this path for you if you say download equals true. And rather than having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6971" target="_blank">01:56:11.660</a></span> | <span class="t">to figure out how to do the preprocessing, you can create a list of transforms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6980" target="_blank">01:56:20.260</a></span> | <span class="t">So I think this is a really lovely API. The reason that this is so new yet has such a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6985" target="_blank">01:56:25.540</a></span> | <span class="t">nice API is because this comes from a lower library called Torch that's been around for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6990" target="_blank">01:56:30.220</a></span> | <span class="t">many years, and so these guys are basically started off by copying what they already had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=6995" target="_blank">01:56:35.500</a></span> | <span class="t">and what already works well. So I think this is very elegant. So I've got two different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7003" target="_blank">01:56:43.620</a></span> | <span class="t">things you can look at here. They're both in the paper. One is scifi10, which are these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7007" target="_blank">01:56:47.740</a></span> | <span class="t">tiny little images. Another is something we haven't seen before, which is called lsun,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7013" target="_blank">01:56:53.700</a></span> | <span class="t">which is a really nice dataset. It's a huge dataset with millions of images, 3 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7023" target="_blank">01:57:03.340</a></span> | <span class="t">bedroom images, for example. We can use either one. This is pretty cool. We can then create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7034" target="_blank">01:57:14.200</a></span> | <span class="t">a data loader, say how many workers to use. We already know what workers are. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7038" target="_blank">01:57:18.700</a></span> | <span class="t">all built into the framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7041" target="_blank">01:57:21.700</a></span> | <span class="t">Now that you know how many workers your CPU likes to use, you can just go ahead and put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7046" target="_blank">01:57:26.420</a></span> | <span class="t">that number in here. Use your CPU to load in this data in parallel in the background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7054" target="_blank">01:57:34.620</a></span> | <span class="t">We're going to start with scifi10. We've got 47,000 of those images. We'll skip over very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7072" target="_blank">01:57:52.660</a></span> | <span class="t">quickly because it's really straightforward. Here's a conv block that consists of a conv2D,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7079" target="_blank">01:57:59.340</a></span> | <span class="t">a batchnorm2D, and a leakyrelu. In my initializer, I can go ahead and say, "Okay, we'll start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7087" target="_blank">01:58:07.020</a></span> | <span class="t">with a conv block. Optionally have a few extra conv blocks." This is really nice. Here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7096" target="_blank">01:58:16.260</a></span> | <span class="t">a while loop that says keep adding more down sampling blocks until you've got as many as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7107" target="_blank">01:58:27.980</a></span> | <span class="t">you need. That's a really nice kind of use of a while loop to simplify creating our architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7116" target="_blank">01:58:36.660</a></span> | <span class="t">And then a final conv block at the end to actually create the thing we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7122" target="_blank">01:58:42.340</a></span> | <span class="t">And then this is pretty nifty. If you pass in n GPU greater than 1, then it will call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7131" target="_blank">01:58:51.780</a></span> | <span class="t">parallel.data parallel passing in those GPU IDs and it will do automatic multi-GPU training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7140" target="_blank">01:59:00.140</a></span> | <span class="t">This is by far the easiest multi-GPU training I've ever seen. That's it. That's the forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7148" target="_blank">01:59:08.540</a></span> | <span class="t">pass behind here. We'll learn more about this over the next couple of weeks. In fact, given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7166" target="_blank">01:59:26.260</a></span> | <span class="t">we're a little short of time, let's discuss that next week and let me know if you don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7171" target="_blank">01:59:31.820</a></span> | <span class="t">think we cover it. Here's the generator. It looks very, very similar. Again, there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7177" target="_blank">01:59:37.740</a></span> | <span class="t">while loop to make sure we've gone through the right number of decom blocks. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7185" target="_blank">01:59:45.740</a></span> | <span class="t">actually interesting. This would probably be better off with an up-sampling block followed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7190" target="_blank">01:59:50.020</a></span> | <span class="t">by a one-by-one convolution. Maybe at home you could try this and see if you get better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7194" target="_blank">01:59:54.420</a></span> | <span class="t">results because this has probably got the checkerboard pattern problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7198" target="_blank">01:59:58.300</a></span> | <span class="t">This is our generator and our discriminator. It's only 75 lines of code, nice and easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7210" target="_blank">02:00:10.020</a></span> | <span class="t">Everything's a little bit different in PyTorch. If we want to say what initializer to use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7214" target="_blank">02:00:14.360</a></span> | <span class="t">again we're going to use a little bit more decoupled. Maybe at first it's a little more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7221" target="_blank">02:00:21.740</a></span> | <span class="t">complex but there's less things you have to learn. In this case we can call something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7226" target="_blank">02:00:26.300</a></span> | <span class="t">called apply, which takes some function and passes it to everything in our architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7234" target="_blank">02:00:34.060</a></span> | <span class="t">This function is something that says, "Is this a conv2D or a convtranspose2D? If so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7240" target="_blank">02:00:40.740</a></span> | <span class="t">use this initialization function." Or if it's a batch norm, use this initialization function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7246" target="_blank">02:00:46.180</a></span> | <span class="t">Everything's a little bit different. There isn't a separate initializer parameter. This</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7252" target="_blank">02:00:52.980</a></span> | <span class="t">is, in my opinion, much more flexible. I really like it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7263" target="_blank">02:01:03.300</a></span> | <span class="t">As before, we need something that creates some noise. Let's go ahead and create some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7270" target="_blank">02:01:10.580</a></span> | <span class="t">fixed noise. We're going to have an optimizer for the discriminator. We've got an optimizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7274" target="_blank">02:01:14.580</a></span> | <span class="t">for the generator. Here is something that does one step of the discriminator. We're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7280" target="_blank">02:01:20.060</a></span> | <span class="t">going to call the forward pass, then we call the backward pass, then we return the error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7286" target="_blank">02:01:26.420</a></span> | <span class="t">Just like before, we've got something called make_trainable. This is how we make something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7292" target="_blank">02:01:32.060</a></span> | <span class="t">trainable or not trainable in PyTorch. Just like before, we have a train loop. The train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7298" target="_blank">02:01:38.540</a></span> | <span class="t">loop has got a little bit more going on, partly because of the vasa_stain_gan, partly because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7305" target="_blank">02:01:45.140</a></span> | <span class="t">of PyTorch. But the basic idea is the same. For each epoch, for each batch, make the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7318" target="_blank">02:01:58.260</a></span> | <span class="t">trainable, and then this is the number of iterations to train the discriminator for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7327" target="_blank">02:02:07.340</a></span> | <span class="t">Remember I told you one of the nice things about the vasa_stain_gan is that we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7332" target="_blank">02:02:12.100</a></span> | <span class="t">to do one batch discriminator, one batch generator, one batch discriminator, one batch generator,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7336" target="_blank">02:02:16.100</a></span> | <span class="t">but we can actually train the discriminator properly for a bunch of batches. In the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7342" target="_blank">02:02:22.620</a></span> | <span class="t">they suggest using 5 batches of discriminator training each time through the loop, unless</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7355" target="_blank">02:02:35.420</a></span> | <span class="t">you're still in the first 25 iterations. They say if you're in the first 25 iterations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7362" target="_blank">02:02:42.580</a></span> | <span class="t">do 100 batches. And then they also say from time to time, do 100 batches. So it's kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7369" target="_blank">02:02:49.580</a></span> | <span class="t">of nice by having the flexibility here to really change things, we can do exactly what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7375" target="_blank">02:02:55.460</a></span> | <span class="t">the paper wants us to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7376" target="_blank">02:02:56.580</a></span> | <span class="t">So basically at first we're going to train the discriminator carefully, and also from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7383" target="_blank">02:03:03.340</a></span> | <span class="t">time to time, train the discriminator very carefully. Otherwise we'll just do 5 batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7389" target="_blank">02:03:09.860</a></span> | <span class="t">So this is where we go ahead and train the discriminator. And you'll see here, we clamp</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7396" target="_blank">02:03:16.700</a></span> | <span class="t">-- this is the same as clip -- the weights in the discriminator to fall in this range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7404" target="_blank">02:03:24.540</a></span> | <span class="t">And if you're interested in reading the paper, the paper explains that basically the reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7408" target="_blank">02:03:28.780</a></span> | <span class="t">for this is that their assumptions are only true in this kind of small area. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7419" target="_blank">02:03:39.060</a></span> | <span class="t">why we have to make sure that the weights stay in this small area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7423" target="_blank">02:03:43.900</a></span> | <span class="t">So then we go ahead and do a single step with the discriminator. Then we create some noise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7430" target="_blank">02:03:50.460</a></span> | <span class="t">and do a single step with the generator. We get our fake data for the discriminator. Then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7441" target="_blank">02:04:01.580</a></span> | <span class="t">we can subtract the fake from the real to get our error for the discriminator. So there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7446" target="_blank">02:04:06.380</a></span> | <span class="t">one step with the discriminator. We do that either 5 or 100 times. Make our discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7457" target="_blank">02:04:17.900</a></span> | <span class="t">not trainable, and then do one step of the generator. You can see here, we call the generator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7464" target="_blank">02:04:24.500</a></span> | <span class="t">with some noise, and then pass it into the discriminator to see if we tricked it or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7470" target="_blank">02:04:30.140</a></span> | <span class="t">During the week, you can look at these two different versions and you're going to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7475" target="_blank">02:04:35.740</a></span> | <span class="t">basically the PyTorch and the Keras version of basically the same thing. The only difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7481" target="_blank">02:04:41.720</a></span> | <span class="t">is in the two things. One is the presence of this clamping, and the second is that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7488" target="_blank">02:04:48.940</a></span> | <span class="t">loss function is mean squared error rather than cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7495" target="_blank">02:04:55.620</a></span> | <span class="t">So let's see what happens. Here is some examples from SciFAR 10. They're certainly a lot better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7509" target="_blank">02:05:09.540</a></span> | <span class="t">than our crappy DC GAN MNIST examples, but they're not great. Why are they not great?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7520" target="_blank">02:05:20.740</a></span> | <span class="t">So probably the reason they're not great is because SciFAR 10 has quite a few different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7527" target="_blank">02:05:27.860</a></span> | <span class="t">kinds of categories of different kinds of things. So it doesn't really know what it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7532" target="_blank">02:05:32.340</a></span> | <span class="t">meant to be drawing a picture of. Sometimes I guess it kind of figures it out. This must</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7536" target="_blank">02:05:36.660</a></span> | <span class="t">be a plane, I think. But a lot of the time it hedges and kind of draws a picture of something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7543" target="_blank">02:05:43.580</a></span> | <span class="t">that looks like it might be a reasonable picture, but it's not a picture of anything in particular.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7548" target="_blank">02:05:48.180</a></span> | <span class="t">On the other hand, the Lsun dataset has 3 million bedrooms. So we would hope that when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7557" target="_blank">02:05:57.540</a></span> | <span class="t">we train the Wasserstein GAN on Lsun bedrooms, we might get better results. Here's the real</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7566" target="_blank">02:06:06.540</a></span> | <span class="t">SciFAR 10, by the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7571" target="_blank">02:06:11.700</a></span> | <span class="t">Here are our fake bedrooms, and they are pretty freaking awesome. So literally they started</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7581" target="_blank">02:06:21.420</a></span> | <span class="t">out as random noise, and everyone has been turned in like that. It's definitely a bedroom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7589" target="_blank">02:06:29.220</a></span> | <span class="t">They're all definitely bedrooms. And then here is the real bedrooms to compare.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7596" target="_blank">02:06:36.860</a></span> | <span class="t">You can kind of see here that imagine if you took this and stuck it on the end of any kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7606" target="_blank">02:06:46.860</a></span> | <span class="t">of generator. I think you could really use this to make your generator much more believable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7618" target="_blank">02:06:58.020</a></span> | <span class="t">Any time you kind of look at it and you say, "Oh, that doesn't look like the real X," maybe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7621" target="_blank">02:07:01.740</a></span> | <span class="t">you could try using a WGAN to try to make it look more like a real X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7632" target="_blank">02:07:12.700</a></span> | <span class="t">So this paper is so important. Here's the other thing. The loss function for these actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7647" target="_blank">02:07:27.440</a></span> | <span class="t">makes sense. The discriminator and the generator loss functions actually decrease as they get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7653" target="_blank">02:07:33.300</a></span> | <span class="t">better. So you can actually tell if your thing is training properly. You can't exactly compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7660" target="_blank">02:07:40.580</a></span> | <span class="t">two different architectures to each other still, but you can certainly see that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7666" target="_blank">02:07:46.180</a></span> | <span class="t">training curves are working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7668" target="_blank">02:07:48.100</a></span> | <span class="t">So now that we have, in my opinion, a GAN that actually really works reliably for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7676" target="_blank">02:07:56.720</a></span> | <span class="t">first time ever, I feel like this changes the whole equation for what generators can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7685" target="_blank">02:08:05.060</a></span> | <span class="t">and can't do. And this has not been applied to anything yet. So you can take any old paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7694" target="_blank">02:08:14.400</a></span> | <span class="t">that produces 3D outputs or segmentations or depth outputs or colorization or whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7702" target="_blank">02:08:22.380</a></span> | <span class="t">and add this. And it would be great to see what happens, because none of that has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7708" target="_blank">02:08:28.580</a></span> | <span class="t">done before. It's not been done before because we haven't had a good way to train GANs before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7714" target="_blank">02:08:34.900</a></span> | <span class="t">So this is kind of, I think, something where anybody who's interested in a project, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7727" target="_blank">02:08:47.220</a></span> | <span class="t">this would be a great project and something that maybe you can do reasonably quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7733" target="_blank">02:08:53.940</a></span> | <span class="t">Another thing you could do as a project is to convert this into Keras. So you can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7741" target="_blank">02:09:01.020</a></span> | <span class="t">the Keras DC GAN notebook that we've already got and change the loss function at the weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7747" target="_blank">02:09:07.300</a></span> | <span class="t">clipping, try training on this lsunbedroom data set, and you should get the same results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7754" target="_blank">02:09:14.800</a></span> | <span class="t">And then you can add this on top of any of your Keras stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7759" target="_blank">02:09:19.460</a></span> | <span class="t">So there's so much you could do this week. I don't feel like I want to give you an assignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7767" target="_blank">02:09:27.220</a></span> | <span class="t">per se, because there's a thousand assignments you could do. I think as per usual, you should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7773" target="_blank">02:09:33.100</a></span> | <span class="t">go back and look at the papers. The original GAN paper is a fairly easy read. There's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7781" target="_blank">02:09:41.900</a></span> | <span class="t">section called Theoretical Results, which is kind of like the pointless math bit. Here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7789" target="_blank">02:09:49.900</a></span> | <span class="t">some theoretical stuff. It's actually interesting to read this now because you go back and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7794" target="_blank">02:09:54.860</a></span> | <span class="t">look at this stuff where they prove various nice things about their GAN. So they're talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7801" target="_blank">02:10:01.860</a></span> | <span class="t">about how the generative model perfectly replicates the data generating process. It's interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7806" target="_blank">02:10:06.460</a></span> | <span class="t">to go back and look and say, okay, so they've proved these things, but it turned out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7813" target="_blank">02:10:13.740</a></span> | <span class="t">be totally pointless. It still didn't work. It didn't really work. So it's kind of interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7820" target="_blank">02:10:20.460</a></span> | <span class="t">to look back and say, which is not to say this isn't a good paper, it is a good paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7825" target="_blank">02:10:25.900</a></span> | <span class="t">but it is interesting to see when is the theoretical stuff useful and when not. Then you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7831" target="_blank">02:10:31.900</a></span> | <span class="t">the Wasserstein GAN theoretical sections, and it spends a lot of time talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7838" target="_blank">02:10:38.780</a></span> | <span class="t">why their theory actually matters. So they have this really cool example where they say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7845" target="_blank">02:10:45.340</a></span> | <span class="t">let's create something really simple. What if you want to learn just parallel lines,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7851" target="_blank">02:10:51.140</a></span> | <span class="t">and they show why it is that the old way of doing GANs can't learn parallel lines, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7858" target="_blank">02:10:58.060</a></span> | <span class="t">then they show how their different objective function can learn parallel lines. So I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7864" target="_blank">02:11:04.300</a></span> | <span class="t">anybody who's interested in getting into the theory a little bit, it's very interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7871" target="_blank">02:11:11.380</a></span> | <span class="t">to look at why did the proof of convergence show something that didn't show something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7880" target="_blank">02:11:20.180</a></span> | <span class="t">that really turned out to matter. Where else in this paper the theory turned out to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7885" target="_blank">02:11:25.120</a></span> | <span class="t">super important and basically created something that allowed GANs to work for the first time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7890" target="_blank">02:11:30.980</a></span> | <span class="t">So there's lots of stuff you can get out of these papers if you're interested. In terms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7897" target="_blank">02:11:37.060</a></span> | <span class="t">of the notation, we might look at some of the notation a little bit more next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7905" target="_blank">02:11:45.020</a></span> | <span class="t">But if we look, for example, at the algorithm sections, I think in general the bit I find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7922" target="_blank">02:12:02.580</a></span> | <span class="t">the most useful is the bit where they actually write the pseudocode. Even that, it's useful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7929" target="_blank">02:12:09.060</a></span> | <span class="t">to learn some kind of nomenclature. For each iteration, for each step, what does this mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7941" target="_blank">02:12:21.000</a></span> | <span class="t">Noise samples from noise prior. There's a lot of probability nomenclature which you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7948" target="_blank">02:12:28.460</a></span> | <span class="t">can very quickly translate. A prior simply means np.random.something. In this case, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7960" target="_blank">02:12:40.380</a></span> | <span class="t">probably like np.random.normal. So this just means some random number generator that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7967" target="_blank">02:12:47.140</a></span> | <span class="t">get to pick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7969" target="_blank">02:12:49.500</a></span> | <span class="t">This one here, sample from a data generating distribution, that means randomly picks some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7975" target="_blank">02:12:55.180</a></span> | <span class="t">stuff from your array. So these are the two steps. Generate some random numbers, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7981" target="_blank">02:13:01.940</a></span> | <span class="t">randomly select some things from your array. The bit where it talks about the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7990" target="_blank">02:13:10.140</a></span> | <span class="t">you can kind of largely ignore, except the bit in the middle is your lost function. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=7995" target="_blank">02:13:15.580</a></span> | <span class="t">can see here, these things here is your noise, that's your noise. So noise, generator on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8003" target="_blank">02:13:23.740</a></span> | <span class="t">noise, discriminator on generator on noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8006" target="_blank">02:13:26.220</a></span> | <span class="t">So there's the bit where we're trying to fool the discriminator, and we're trying to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8010" target="_blank">02:13:30.660</a></span> | <span class="t">that tricker, so that's why we do 1-minus. And then here's getting the discriminator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8015" target="_blank">02:13:35.620</a></span> | <span class="t">to be accurate, because these x's is the real data. So that's the math version of what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8022" target="_blank">02:13:42.540</a></span> | <span class="t">just learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8025" target="_blank">02:13:45.180</a></span> | <span class="t">The Wasserstein-Gann also has an algorithm section, so it's kind of interesting to compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8034" target="_blank">02:13:54.540</a></span> | <span class="t">the two. So here we go with Wasserstein-Gann, here's the algorithm, and basically this says</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8041" target="_blank">02:14:01.740</a></span> | <span class="t">exactly the same thing as the last one said, but I actually find this one a bit clearer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8048" target="_blank">02:14:08.220</a></span> | <span class="t">Sample from the real data, sample from your priors. So hopefully that's enough to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8055" target="_blank">02:14:15.340</a></span> | <span class="t">going and look forward to talking on the forums and see how everybody gets along. Thanks everybody.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=uv0gmrXSXVg&t=8061" target="_blank">02:14:21.060</a></span> | <span class="t">(audience applauds)</span></div></div></body></html>