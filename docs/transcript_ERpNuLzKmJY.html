<html><head><title>The inside story of how ChatGPT was built – OpenAI cofounder John Schulman</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>The inside story of how ChatGPT was built – OpenAI cofounder John Schulman</h2><a href="https://www.youtube.com/watch?v=ERpNuLzKmJY" target="_blank"><img src="https://i.ytimg.com/vi_webp/ERpNuLzKmJY/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>You led the creation of ChatGBT. At what point did you realize, first of all, these LLMs are the path to go and then a chatbot would be, or some way to instruct them would be a useful thing to do? Before ChatGBT, OpenAI had these instruction following models. And the idea there was we had base models and people can prompt them in elaborate ways.</p><p>But they're also kind of hard to prompt. You had to basically do autocomplete. So you have to set up a very good prompt with some examples. So people at OpenAI were working on just taking the base models and making them easier to prompt so that if you just wrote a question, it would answer the question instead of giving you more questions or something.</p><p>So we had these instruction following models, which were kind of like base models, but a little easier to use. And those are the original ones deployed in the API. Or after GPT-3, those were the next generation of models. Then at the same time, there were definitely a lot of people thinking about chat.</p><p>So Google had some papers. They had Lambda and earlier Mina. So they had these chatbots. And it was more like a base model that was really specialized to the task of chat, really good at chat. And I think at least looking at the examples from the paper, it was more used for fun applications, like where the model would take on some persona and pretend to be that persona.</p><p>It was not so functional, like help me refactor my code. So yeah, there are definitely people thinking about chat. I had worked on a project before looking at chat called WebGPT, which is more about doing question answering with the help of web browsing and retrieval. And well, when you do question answering, it really wants to be in a chat, because you always want to ask follow-up questions.</p><p>Or sometimes the model should ask a clarifying question, because the question is ambiguous. So it was kind of clear after we did the first version of that that the next version should be conversational. So anyway, we started working on a conversational chat assistant. And this was built on top of GPD 3.5, which was done training at the beginning of 2022.</p><p>And that model was quite good at language and code. So we quickly realized that it was actually quite good at coding help. And that was one of the things we were excited about. So yeah, we worked on that for most of the year. And we had browsing as another feature in it, though we ended up deemphasizing that later on, because the model's internal knowledge was so good that the browsing wasn't the most interesting thing about it.</p><p>And then we were thinking about-- we had it out for beta testing to friends and family for a while. And we were thinking about doing a public release. But at that time, actually, GPD 4 finished training in August or-- yeah, in August that year. And actually, the flagship RL effort at OpenAI was the instruction following effort, because that was the models that were being deployed into production.</p><p>So the first fine tunes of GPD 4 used that whole stack. And that was-- yeah, those models were really good. And everyone got really excited about that after seeing the Instruct fine tune GPD 4s. But so they were really, really good. They would occasionally give you amazing outputs. But they were also a little bit-- the model was clearly pretty unreliable.</p><p>It would sometimes hallucinate a lot. And it was pretty-- it would sometimes give you pretty unhinged outputs. So it was clearly not quite ready for prime time. But it was obviously very good. And yeah, so I guess that-- people forgot about chat for a little while after that about this alternative branch.</p><p>But then we ended up-- we pushed it further. And we ended up mixing together all the data sets, like the Instruct and the chat data, and to try to get something that was the best of both worlds. And I think the models we-- the chat models were clearly more-- it was easier to use.</p><p>It was sort of more-- it sort of automatically had much more sensible behavior in terms of the model knowing its own limitations. The other thing about chat was that when we had these Instruct models, the task of complete this text, but in a nice way or in a helpful way, that's a pretty poorly defined task.</p><p>So I think that task is both confusing for the model and for the human who's supposed to do the data labeling. Whereas for chat, I think people had an intuitive sense of what a helpful robot should be like. So I think it was just much easier to tell people-- for people to get the idea of what the model was supposed to do.</p><p>And so as a result, I think the model had a much more coherent personality. And it was much easier to get pretty sensible behavior robustly. Is it the case that anybody could have made chat GBT using your publicly available fine tuning API? Not exactly. I mean, they could have-- I don't remember the status of which models were available for fine tuning.</p><p>Assuming we had 3.5 available for fine tuning at the time, you could have made something pretty decently close. But I'm not sure you would have-- I don't think you would have been able to do just one iteration of fine tuning where you have purely human written data and you fine tune on that.</p><p>I think you would want to do several iterations. If you're not going to do RL, which we did, you'd want to do some kind of iterative supervised fine tuning where you have humans edit the model generated outputs. Because it's really hard to get people to-- if you train on human generated data, even if it's really high quality, it's just hard for a model to fit that data perfectly.</p><p>Because it might not be something a model is capable of outputting. So you need to do something iterative that looks a little bit more like RL. So I think if you had done that, you could have gotten something pretty close. But that would have been kind of non-trivial. But we also had another instruction following model trained with RL that was released a little before chat GBT.</p><p>So I think if you put a chat wrapper on that, you would get something decently close. But that model, if you just prompted it with chat. But that model had some differences in strengths. That model was pretty good at writing and poetry and so forth. But it wasn't as good at knowing its limitations and at factuality and so forth.</p></div></div></body></html>