<html><head><title>NVIDIA Cosmos: World Foundation Model Platform for Physical AI - w/ Ethan He</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>NVIDIA Cosmos: World Foundation Model Platform for Physical AI - w/ Ethan He</h2><a href="https://www.youtube.com/watch?v=og59L4JECz4"><img src="https://i.ytimg.com/vi/og59L4JECz4/sddefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./og59L4JECz4.html">Whisper Transcript</a> | <a href="./transcript_og59L4JECz4.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">the 75 pages of the report. I can't cover everything in one hour. I can talk about it for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=7" target="_blank">00:00:07.040</a></span> | <span class="t">hours. So I'll just cover what I focus on as a data scaling and model scaling. First, I'll do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=15" target="_blank">00:00:15.520</a></span> | <span class="t">an introduction of Cosmos for people who are not familiar with it. I guess the introduction is best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=22" target="_blank">00:00:22.720</a></span> | <span class="t">to Saru Jensa himself. It includes autoregressive world foundation models, diffusion-based world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">foundation models, advanced tokenizers, and an NVIDIA CUDA, an AI-accelerated data pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=40" target="_blank">00:00:40.400</a></span> | <span class="t">Cosmos models ingest text, image, or video prompts and generate virtual world states as videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=50" target="_blank">00:00:50.400</a></span> | <span class="t">Cosmos generations prioritize the unique requirements of AV and robotics use cases,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=55" target="_blank">00:00:55.600</a></span> | <span class="t">like real world environments, lighting, and object permanence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=60" target="_blank">00:01:00.400</a></span> | <span class="t">Developers use NVIDIA Omniverse to build physics-based geospatially accurate scenarios,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=67" target="_blank">00:01:07.760</a></span> | <span class="t">then output Omniverse renders into Cosmos, which generates photoreal physically-based synthetic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=74" target="_blank">00:01:14.720</a></span> | <span class="t">data. Whether diverse objects or environments, conditions like weather or time of day,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=96" target="_blank">00:01:36.400</a></span> | <span class="t">or edge case scenarios, developers use Cosmos to generate worlds for reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=104" target="_blank">00:01:44.320</a></span> | <span class="t">AI feedback to improve policy models or to test and validate model performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=111" target="_blank">00:01:51.040</a></span> | <span class="t">Even across multi-sensor views, Cosmos can generate tokens in real time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=119" target="_blank">00:01:59.920</a></span> | <span class="t">bringing the power of foresight and multiverse simulation to AI models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=125" target="_blank">00:02:05.440</a></span> | <span class="t">generating every possible future to help the model select the right path.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=129" target="_blank">00:02:09.680</a></span> | <span class="t">Working with the world's developer ecosystem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=134" target="_blank">00:02:14.400</a></span> | <span class="t">NVIDIA is helping advance the next wave of physical AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=138" target="_blank">00:02:18.160</a></span> | <span class="t">Okay, so what's a world model? A world model, it takes past observations, acts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=151" target="_blank">00:02:31.840</a></span> | <span class="t">and also perturbations. See, it can predict the future predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=159" target="_blank">00:02:39.200</a></span> | <span class="t">The perturbation can take any forms, like it can be actions from the physical AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=166" target="_blank">00:02:46.320</a></span> | <span class="t">or it just can be some random perturbation, or a text description of the perturbation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=174" target="_blank">00:02:54.160</a></span> | <span class="t">So, in the Cosmos 1.0, we open-sourced a family of models. We have two sets of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=182" target="_blank">00:03:02.960</a></span> | <span class="t">forward quantization models. One is based on diffusion, while the other is based on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=188" target="_blank">00:03:08.240</a></span> | <span class="t">autoregressive models. For each family, we also built two base models and two derivatives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=195" target="_blank">00:03:15.040</a></span> | <span class="t">To achieve the best generation quality, we also built an upsampler for the diffusion model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=201" target="_blank">00:03:21.600</a></span> | <span class="t">and also a diffusion decoder to improve the video generated from the autoregressive model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=207" target="_blank">00:03:27.200</a></span> | <span class="t">So, these are already open-sourced on GitHub. You can feel free to try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=212" target="_blank">00:03:32.560</a></span> | <span class="t">So, for the diffusion world model, this is the architecture overview of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=221" target="_blank">00:03:41.040</a></span> | <span class="t">So, the input video goes through a video tokenizer. Now, here it's called CV8x8x8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=231" target="_blank">00:03:51.360</a></span> | <span class="t">Basically, the time spatial are both compressed by 8. If you have 8 frames, it's going to go into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=244" target="_blank">00:04:04.080</a></span> | <span class="t">one frame. I assume everyone knows diffusion. The tokens are corrupted, then go through a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=252" target="_blank">00:04:12.320</a></span> | <span class="t">diffusion transformer. The model then generates the reconstructed video during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=262" target="_blank">00:04:22.160</a></span> | <span class="t">This is an example video generated from the diffusion world model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=268" target="_blank">00:04:28.880</a></span> | <span class="t">For the autoregressive world model, it goes through a similar process. As a tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=279" target="_blank">00:04:39.280</a></span> | <span class="t">instead, it goes from discrete instead of continuous. Discrete tokenizer is very similar to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=287" target="_blank">00:04:47.840</a></span> | <span class="t">LLMs. This discrete tokenizer converts video patches into one of the vocabularies. There's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=302" target="_blank">00:05:02.560</a></span> | <span class="t">64k vocabulary. These discrete tokens are fed into a transformer with a similar architecture as LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=315" target="_blank">00:05:15.280</a></span> | <span class="t">Then, discrete tokens are generated. Then, there's a decoder, which is also a discrete decoder that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=324" target="_blank">00:05:24.320</a></span> | <span class="t">decodes these tokens into videos. There has been debate on whether diffusion or autoregressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=334" target="_blank">00:05:34.160</a></span> | <span class="t">models are better since we don't know. So, we built both of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=343" target="_blank">00:05:43.280</a></span> | <span class="t">For example, here, this is an input image for the autoregressive model. You can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=351" target="_blank">00:05:51.520</a></span> | <span class="t">use this as a pre-filling word for the transformer. Then, in the decoding process,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=360" target="_blank">00:06:00.240</a></span> | <span class="t">it can decode into videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=366" target="_blank">00:06:06.480</a></span> | <span class="t">Autoregressive, if you want better quality of the generated result, you can go with the diffusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=376" target="_blank">00:06:16.160</a></span> | <span class="t">model. If you want the model to be faster, you can try the autoregressive model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=383" target="_blank">00:06:23.520</a></span> | <span class="t">Autoregressive also plays very well into other modalities. You can easily combine other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=393" target="_blank">00:06:33.520</a></span> | <span class="t">like text tokens or action tokens. But here, our autoregressive model is trained purely on videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=401" target="_blank">00:06:41.760</a></span> | <span class="t">We also released post-training scripts for these models. Right now, in the Cosmos paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=415" target="_blank">00:06:55.360</a></span> | <span class="t">we discuss several post-training examples of the Cosmos foundation models for different physical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=423" target="_blank">00:07:03.600</a></span> | <span class="t">texts. Right now, in the GitHub, we support general post-training. This fine-tunes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=431" target="_blank">00:07:11.440</a></span> | <span class="t">word models to generate a target distribution of the videos based on a custom dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=437" target="_blank">00:07:17.200</a></span> | <span class="t">The target distribution could include a specific camera spec or a specific domain such as FAC3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=446" target="_blank">00:07:26.000</a></span> | <span class="t">Here is an example. We took a few videos of humanoid. Here is a jetty robot, a jetty humanoid,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=457" target="_blank">00:07:37.840</a></span> | <span class="t">and just roughly five videos of this humanoid. The video is in simulation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=469" target="_blank">00:07:49.280</a></span> | <span class="t">After fine-tuning the diffusion model, you can generate novel videos of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=477" target="_blank">00:07:57.040</a></span> | <span class="t">robot doing something else. The model is able to remember the characteristics of this robot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=484" target="_blank">00:08:04.880</a></span> | <span class="t">while generating novel tasks which are not possible in either simulation or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=494" target="_blank">00:08:14.000</a></span> | <span class="t">in the real world through tele-opt. There are more post-training scripts that are coming soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=506" target="_blank">00:08:26.960</a></span> | <span class="t">For example, instruction control. Post-training models for robotic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=514" target="_blank">00:08:34.320</a></span> | <span class="t">manual motivation to produce a video based on textual instruction. You can instruct the robots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=521" target="_blank">00:08:41.760</a></span> | <span class="t">to perform some tasks like folding clothes or picking up objects. Also, action control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=529" target="_blank">00:08:49.440</a></span> | <span class="t">The post-trained robots can predict both the next video frame and the next action frame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=539" target="_blank">00:08:59.040</a></span> | <span class="t">Here, the example shows a camera control. Adding the camera pose as a condition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=547" target="_blank">00:09:07.840</a></span> | <span class="t">you can generate 3D consistent video simulation from a single image or video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=553" target="_blank">00:09:13.440</a></span> | <span class="t">This can enable drastic navigation in virtual environments. You can also do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=564" target="_blank">00:09:24.000</a></span> | <span class="t">multi-view generation, especially for autonomous driving. You can generate synchronized multi-view</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=576" target="_blank">00:09:36.160</a></span> | <span class="t">videos from text prompts then simulate the driving scenarios with multiple camera perspectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=583" target="_blank">00:09:43.840</a></span> | <span class="t">Next, I'll dive into technical details. First, I'll go over data scaling. It's a model scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=602" target="_blank">00:10:02.560</a></span> | <span class="t">So, we open-sourced a training framework. The data curation part, you can sign up for it. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=615" target="_blank">00:10:15.360</a></span> | <span class="t">coming soon. The training framework is open-sourced. When we curate the data for text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=631" target="_blank">00:10:31.680</a></span> | <span class="t">you can just grab the text online and the label is basically next token prediction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=639" target="_blank">00:10:39.840</a></span> | <span class="t">which is relatively straightforward and cheap to curate. However, for videos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=647" target="_blank">00:10:47.680</a></span> | <span class="t">for example, you have a video shot of someone playing basketball. You need to label a basketball</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=655" target="_blank">00:10:55.600</a></span> | <span class="t">player as dribbling the ball and shooting it into the hoop. Labeling video data requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=662" target="_blank">00:11:02.400</a></span> | <span class="t">good AI models for automatic captioning. We want to control the AI models to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=671" target="_blank">00:11:11.040</a></span> | <span class="t">using text we specify. Also, another challenge is that video signals are less refined compared to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=681" target="_blank">00:11:21.680</a></span> | <span class="t">text. Maybe out of like an hour of videos, there might only be a second of interesting stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=688" target="_blank">00:11:28.560</a></span> | <span class="t">This is very computationally challenging and very expensive. We use distributed computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=696" target="_blank">00:11:36.800</a></span> | <span class="t">to solve this problem. This is a life cycle of curation. So, on top of DGS cloud platform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=707" target="_blank">00:11:47.760</a></span> | <span class="t">we use real data based on streaming pipeline running on thousands of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=714" target="_blank">00:11:54.400</a></span> | <span class="t">The long video goes into the pipeline and then the videos are splitted and then transcoded into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=724" target="_blank">00:12:04.720</a></span> | <span class="t">shorter clips. Then, different AI models are running on the short clips to detect high-quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=734" target="_blank">00:12:14.800</a></span> | <span class="t">videos. Another NVIDIA BLM captioning model running using TensorRT LLM is used to caption the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=747" target="_blank">00:12:27.600</a></span> | <span class="t">And finally, we get a training dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=753" target="_blank">00:12:33.840</a></span> | <span class="t">Data curation for the video foundation models are very challenging. The scale of the video data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=764" target="_blank">00:12:44.480</a></span> | <span class="t">are hundreds of petabytes, much bigger than the previous image models. Orchestration at scale,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=772" target="_blank">00:12:52.720</a></span> | <span class="t">heterogeneous computer requirements of tens of AI models running efficiently together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=780" target="_blank">00:13:00.640</a></span> | <span class="t">is also very challenging. You have the captioning model, you have models to detect the scene change,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=788" target="_blank">00:13:08.720</a></span> | <span class="t">you have models to detect the video consistency, aesthetic, etc. Multiple concurrent streams of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=798" target="_blank">00:13:18.480</a></span> | <span class="t">high-throughput data exchange between AI models also impose bandwidth challenges to the cluster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=806" target="_blank">00:13:26.320</a></span> | <span class="t">Every single step of the curation pipeline needs to be GPU-accelerated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=816" target="_blank">00:13:36.320</a></span> | <span class="t">We also need to manage the resiliency of the GPU-based data pipeline at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=822" target="_blank">00:13:42.320</a></span> | <span class="t">So, each inference model needs to run at speed of light. We go from the baseline,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=837" target="_blank">00:13:57.360</a></span> | <span class="t">where the model is run on PyTorch, to use TRT LLM to accelerate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=845" target="_blank">00:14:05.600</a></span> | <span class="t">And then we run it on a larger batch to further accelerate it. And today, we use FP8 quantization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=854" target="_blank">00:14:14.480</a></span> | <span class="t">to further accelerate it to 7x compared to the baseline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=859" target="_blank">00:14:19.200</a></span> | <span class="t">So, video understanding, so filtering the high-quality clips and auto-labeling is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=870" target="_blank">00:14:30.240</a></span> | <span class="t">enough for building a video foundation model. We need to understand a lot more about the videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=877" target="_blank">00:14:37.920</a></span> | <span class="t">for specific domain training. We remove the duplicated content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=883" target="_blank">00:14:43.920</a></span> | <span class="t">and visual search understandings of the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=887" target="_blank">00:14:47.840</a></span> | <span class="t">So, these are the next last cycle of the video data curation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=897" target="_blank">00:14:57.680</a></span> | <span class="t">After the captioning, we need to do clustering to group the data into different categories,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=906" target="_blank">00:15:06.000</a></span> | <span class="t">sports, entertainment, robotics, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=911" target="_blank">00:15:11.920</a></span> | <span class="t">Then there is a semantic deduplication to remove the redundant data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=924" target="_blank">00:15:24.560</a></span> | <span class="t">Finally, video taxonomy to further help researchers to pick the data they want to train on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=932" target="_blank">00:15:32.720</a></span> | <span class="t">The takeaway for the video data curation is we build the video processing capabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=951" target="_blank">00:15:51.680</a></span> | <span class="t">into Nemo Curator to enable the developers to curate high-quality data and train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=957" target="_blank">00:15:57.680</a></span> | <span class="t">highly accurate video foundation models. By leveraging end-to-end GPU acceleration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=965" target="_blank">00:16:05.280</a></span> | <span class="t">and optimizing the data orchestration through the pipeline,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=968" target="_blank">00:16:08.720</a></span> | <span class="t">Nemo Curator can scale to over 100 petabytes of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=974" target="_blank">00:16:14.800</a></span> | <span class="t">Other optimizations reduce the processing time and lower the total cost of ownership.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=980" target="_blank">00:16:20.800</a></span> | <span class="t">The models are optimized for high throughput and enhancing overall pipeline efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=988" target="_blank">00:16:28.560</a></span> | <span class="t">Next, let's go over the model scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=999" target="_blank">00:16:39.680</a></span> | <span class="t">So, using Nemo Video Foundation Model Training Framework, you can scale these video models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1008" target="_blank">00:16:48.960</a></span> | <span class="t">up to 20 times larger than traditional frameworks. The framework is capable of training models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1017" target="_blank">00:16:57.440</a></span> | <span class="t">like diffusion or autoregressive or foundation models up to 100 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1026" target="_blank">00:17:06.480</a></span> | <span class="t">The throughput is highly optimized. We achieve roughly 450</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1031" target="_blank">00:17:11.600</a></span> | <span class="t">teraflops. That's close to 50% MFU on the H100 chips.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1043" target="_blank">00:17:23.680</a></span> | <span class="t">These are very close to the training efficiency of the LLM training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1054" target="_blank">00:17:34.640</a></span> | <span class="t">Previously, we talked about the scale of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1057" target="_blank">00:17:37.520</a></span> | <span class="t">data curation. We have hundreds of petabytes of data going into the curation pipeline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1065" target="_blank">00:17:45.600</a></span> | <span class="t">After curation, the data set we get are short video clips and images with text embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1078" target="_blank">00:17:58.720</a></span> | <span class="t">Even though the scale of the data is much smaller, these are still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1083" target="_blank">00:18:03.360</a></span> | <span class="t">considered relatively big if we want to train on the clusters today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1089" target="_blank">00:18:09.280</a></span> | <span class="t">For example, the images are on the O1 billion scales, and the videos are roughly 100 million</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1101" target="_blank">00:18:21.760</a></span> | <span class="t">video scales. In the paper, we use image and video tokenizer compression rate of 8x8x8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1114" target="_blank">00:18:34.720</a></span> | <span class="t">At this scale, the images are compressed to roughly 200 kilobytes. For 1 billion images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1127" target="_blank">00:18:47.120</a></span> | <span class="t">it's roughly at the level of 100 terabytes. For the videos, it's on petabyte level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1135" target="_blank">00:18:55.600</a></span> | <span class="t">A solution can be storing this data on the cluster or storing them on cloud storage like S3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1151" target="_blank">00:19:11.520</a></span> | <span class="t">Storing them on the cluster has some huge costs. Most of the clusters don't have huge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1159" target="_blank">00:19:19.440</a></span> | <span class="t">storage on the clusters themselves. We provide both solutions in the open source framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1174" target="_blank">00:19:34.240</a></span> | <span class="t">We leverage Megatron Energon, which is another open source library from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1179" target="_blank">00:19:39.920</a></span> | <span class="t">NVIDIA, to load data efficiently. It allows you to load data from web source like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1189" target="_blank">00:19:49.120</a></span> | <span class="t">AWS S3 very efficiently without the GPU idling during training. It allows you to deterministically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1201" target="_blank">00:20:01.200</a></span> | <span class="t">save and restore the data loader, which is one of the biggest challenges in loading from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1208" target="_blank">00:20:08.320</a></span> | <span class="t">web dataset. In web dataset, usually the data is loaded sequentially. When your training is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1216" target="_blank">00:20:16.880</a></span> | <span class="t">interrupted, the traditional way of training, you have to randomize loading. You won't be able to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1228" target="_blank">00:20:28.640</a></span> | <span class="t">load non-repetitive data without Megatron Energon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1232" target="_blank">00:20:32.960</a></span> | <span class="t">Another challenge in loading the data is variable input data shapes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1246" target="_blank">00:20:46.560</a></span> | <span class="t">The data types are different. You have image, video, and you also have different durations of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1255" target="_blank">00:20:55.440</a></span> | <span class="t">the videos. You have one second, 10 seconds, or even 50 seconds. The resolutions are different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1264" target="_blank">00:21:04.640</a></span> | <span class="t">so 60p, 720p, 1080p. There are also different aspect ratios, 16 by 9, 9 by 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1272" target="_blank">00:21:12.240</a></span> | <span class="t">When you're training on text, you don't have this kind of problem. In video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1284" target="_blank">00:21:24.640</a></span> | <span class="t">this can cause a very big problem in efficiency if we batch the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1289" target="_blank">00:21:29.840</a></span> | <span class="t">Traditional approach is batching the data. For each different shape of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1302" target="_blank">00:21:42.960</a></span> | <span class="t">input, for example, image, we batch the images into a few samples of batch. For the videos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1313" target="_blank">00:21:53.760</a></span> | <span class="t">for the very large videos, you can just take one video as input. For medium size,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1320" target="_blank">00:22:00.240</a></span> | <span class="t">maybe you can batch two or four into one batch. The pros is that this is commonly used for most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1331" target="_blank">00:22:11.200</a></span> | <span class="t">of the models nowadays. For example, in ImageNet training, traditionally people just resize all of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1340" target="_blank">00:22:20.640</a></span> | <span class="t">the image into 512 by 512 to mitigate this problem. But the challenge here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1349" target="_blank">00:22:29.760</a></span> | <span class="t">if you want to train on different aspect ratios and different resolutions, you need a complicated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1358" target="_blank">00:22:38.720</a></span> | <span class="t">data loading logic to ensure that during training, at each iteration, at least the data shape is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1366" target="_blank">00:22:46.240</a></span> | <span class="t">same. And the efficiency is not very high because not all of the data shapes can be efficiently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1374" target="_blank">00:22:54.320</a></span> | <span class="t">utilized by GPU. And also constantly changing the shape of the input data can cause challenge to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1382" target="_blank">00:23:02.800</a></span> | <span class="t">the fused kernels. On GPU, if all of your tensor operation shapes are the same for all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1393" target="_blank">00:23:13.040</a></span> | <span class="t">iterations, we can optimize for it and it runs more efficiently than dynamic shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1400" target="_blank">00:23:20.240</a></span> | <span class="t">The data loading scheme we open source is called Pack Sequencing or Sequence Packing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1414" target="_blank">00:23:34.640</a></span> | <span class="t">Different from the traditional SBHD format, this one allows you to use different image, video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1425" target="_blank">00:23:45.680</a></span> | <span class="t">multi-model, whatever, also different aspect ratios, duration, resolution. The key is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1436" target="_blank">00:23:56.800</a></span> | <span class="t">reshape all of the data into one-dimensional sequence and then pack them together into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1444" target="_blank">00:24:04.400</a></span> | <span class="t">one batch. And when you pass this into transformer, outside of self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1452" target="_blank">00:24:12.720</a></span> | <span class="t">there's no problem at all. The MLP operation of the transformer is just a per-token operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1461" target="_blank">00:24:21.760</a></span> | <span class="t">But for self-attention, we will need to mask, create a block diagonal mask so that each of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1470" target="_blank">00:24:30.640</a></span> | <span class="t">the samples in the sequence are computing self-attention on themselves. And this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1478" target="_blank">00:24:38.240</a></span> | <span class="t">operation is automatically done on the fused CUDA kernel. You only need to supply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1489" target="_blank">00:24:49.360</a></span> | <span class="t">sequence length in our training code and that's all you need to enable Pack Sequence training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1495" target="_blank">00:24:55.840</a></span> | <span class="t">With this data loading scheme, the training efficiency is very high and you can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1505" target="_blank">00:25:05.920</a></span> | <span class="t">in the end there is a padding. If you have large enough max sequence, the padding is already very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1514" target="_blank">00:25:14.480</a></span> | <span class="t">small. And the training efficiency is very close to when you have all of the samples with the exact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1522" target="_blank">00:25:22.800</a></span> | <span class="t">same shape. Next, I'm covering the parallelism. The training on videos is one of the biggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1540" target="_blank">00:25:40.560</a></span> | <span class="t">challenges in the context lens. Traditionally, in pre-training LLMs, the context lens is really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1549" target="_blank">00:25:49.360</a></span> | <span class="t">like 4K. Nowadays, it's 8K on LLAMA. But training on videos, the context lens is much larger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1557" target="_blank">00:25:57.600</a></span> | <span class="t">Say we have five seconds of videos, encoding it with a 8x8x8 tokenizer, it goes into roughly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1570" target="_blank">00:26:10.320</a></span> | <span class="t">60K or 70K of tokens. This is 10 times larger than the sequence length of LLMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1579" target="_blank">00:26:19.360</a></span> | <span class="t">Context parallelism or ring attention is one of the key techniques we use to scale</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1588" target="_blank">00:26:28.320</a></span> | <span class="t">the Euclidean transformer or the autoregressive word model to up to 1 million tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1598" target="_blank">00:26:38.160</a></span> | <span class="t">Using context parallelism, you can place the activations of the entire transformer along the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1604" target="_blank">00:26:44.480</a></span> | <span class="t">sequence dimension. This exploits the permutation invariance of attention to distribute the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1612" target="_blank">00:26:52.320</a></span> | <span class="t">in ring topology. Hey, quick question, Ethan. I know for some LLM models, like even the LLAMA</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1624" target="_blank">00:27:04.240</a></span> | <span class="t">models that are trained up to 128K context, something that they do is they do the bulk of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1629" target="_blank">00:27:09.440</a></span> | <span class="t">the training, like the majority of the five trillion tokens are done at a smaller context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1634" target="_blank">00:27:14.480</a></span> | <span class="t">line. Then in that post-training, they continually train on longer context. Is that a thing in video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1642" target="_blank">00:27:22.640</a></span> | <span class="t">gen? Can you train the majority of the model at a short clip length and then extend this and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1650" target="_blank">00:27:30.160</a></span> | <span class="t">extrapolate it out? Yes, that's a good question. I think the bottleneck here is we don't have a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1661" target="_blank">00:27:41.120</a></span> | <span class="t">very efficient video compressor. Even a five second video is like 60K tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1673" target="_blank">00:27:53.280</a></span> | <span class="t">If we say we train on shorter videos like one second, that also works. But for the majority</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1680" target="_blank">00:28:00.160</a></span> | <span class="t">of the training, the video foundation models, they are 10 times longer context compared to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1688" target="_blank">00:28:08.160</a></span> | <span class="t">the LLMs. For post-training, the video models are extended to even longer context,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1699" target="_blank">00:28:19.120</a></span> | <span class="t">say like one million tokens, to be able to generate a video roughly like one minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1706" target="_blank">00:28:26.000</a></span> | <span class="t">That makes sense. It's basically the same problem, it's just a 10x scale on both sides,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1712" target="_blank">00:28:32.880</a></span> | <span class="t">so even the short context is still there. Yes. Thank you. I'd say if we have a very good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1720" target="_blank">00:28:40.320</a></span> | <span class="t">tokenizer in the future that can efficiently reconstruct the videos, maybe it's a paradigm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1727" target="_blank">00:28:47.120</a></span> | <span class="t">of change. Right now, the video tokenizer customers release are 8x8x8 or 8x16x16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1738" target="_blank">00:28:58.320</a></span> | <span class="t">Spatial-wise, 16x16 is already near the limit. If you go beyond that, a lot of the reconstruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1751" target="_blank">00:29:11.040</a></span> | <span class="t">artifacts will appear. Makes sense. Thank you. For video generation and inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1762" target="_blank">00:29:22.400</a></span> | <span class="t">we also employ context parallel. In the open source repository, you can already use context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1770" target="_blank">00:29:30.880</a></span> | <span class="t">parallel to accelerate the inference. For example, on 8 GPUs, using context parallel 8, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1778" target="_blank">00:29:38.880</a></span> | <span class="t">generate a 5-second video under 30 seconds. Using more across different nodes, you can generate a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1789" target="_blank">00:29:49.520</a></span> | <span class="t">video in a matter of seconds. Another challenge brought by diffusion transformer is challenges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1805" target="_blank">00:30:05.360</a></span> | <span class="t">to pipeline parallelism. Traditionally, in LLMs, for different pipeline stages, you only need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1813" target="_blank">00:30:13.440</a></span> | <span class="t">pass the hidden states to the next pipeline stage. But diffusion transformers have a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1820" target="_blank">00:30:20.160</a></span> | <span class="t">conditioning and adapt to their norm, and also conditioning on text, which creates difficulty for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1830" target="_blank">00:30:30.000</a></span> | <span class="t">the pipeline parallelism. So we provide a solution to generate the additional conditionings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1839" target="_blank">00:30:39.920</a></span> | <span class="t">on each pipeline parallel ranks. This value is slightly more compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1845" target="_blank">00:30:45.840</a></span> | <span class="t">but reduces the communication cost a lot, which leads to improved performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1858" target="_blank">00:30:58.000</a></span> | <span class="t">Okay. I think that's all of my presentation. Thank you for listening. Any questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1864" target="_blank">00:31:04.800</a></span> | <span class="t">Hi, Ethan. Thanks a lot for joining us again. This is RJ. I asked a question at the beginning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1881" target="_blank">00:31:21.760</a></span> | <span class="t">of the chat. I'm a little unclear about how the encoder gets, like, the encoder to the 8x8</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1893" target="_blank">00:31:33.760</a></span> | <span class="t">latent space gets trained. Is that just part of the diffusion training, or is there something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1902" target="_blank">00:31:42.560</a></span> | <span class="t">like, some sort of, like, a separate step that is used to train that encoder?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1911" target="_blank">00:31:51.600</a></span> | <span class="t">Yeah, that's a good question. So a separate step is used to train the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1915" target="_blank">00:31:55.920</a></span> | <span class="t">Tokenizer is a fancy name of this, but this is VectorQuant has the variational autoencoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1927" target="_blank">00:32:07.280</a></span> | <span class="t">VQVE. Okay, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1929" target="_blank">00:32:09.520</a></span> | <span class="t">Yeah, you would basically train it for the task of reconstructing the videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1938" target="_blank">00:32:18.480</a></span> | <span class="t">Okay, right. So – but how do you get it to create a 3D – what's it like, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1945" target="_blank">00:32:25.440</a></span> | <span class="t">TLDR, and how to get it to create a 3D latent space like that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1948" target="_blank">00:32:28.720</a></span> | <span class="t">Yeah, so the model architecture itself is a causal convolutional neural network. It can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1961" target="_blank">00:32:41.040</a></span> | <span class="t">reconstruct – the encoder and decoder structure reconstructs the video. So the training objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1969" target="_blank">00:32:49.520</a></span> | <span class="t">is basically reconstructing the video. The process is you need to collect some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1975" target="_blank">00:32:55.280</a></span> | <span class="t">diverse set of different videos, ideally in your domain, and then train this causal CNN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1984" target="_blank">00:33:04.160</a></span> | <span class="t">to reconstruct those videos. The codebook here for continuous tokens are just those continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=1992" target="_blank">00:33:12.960</a></span> | <span class="t">tokens, but for discrete tokenizers, you would do vector quantization to quantize in 264-QIF codebase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2002" target="_blank">00:33:22.480</a></span> | <span class="t">Okay. Is it – sorry, I didn't have time to pre-read the paper. Is this covered in the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2009" target="_blank">00:33:29.120</a></span> | <span class="t">or is there a separate paper for this? Yeah, this is covered.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2012" target="_blank">00:33:32.880</a></span> | <span class="t">Okay, got it, got it. Thank you. This is really super interesting, exciting work. Thank you very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2019" target="_blank">00:33:39.280</a></span> | <span class="t">much for joining us. So additionally, the tokenizer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2024" target="_blank">00:33:44.560</a></span> | <span class="t">phrased during the training of the transformer, because if you don't phrase the tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2033" target="_blank">00:33:53.440</a></span> | <span class="t">it can lead to catastrophe forgetting. You can – you just generate – if you just predict the error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2043" target="_blank">00:34:03.920</a></span> | <span class="t">and the loss is there. Okay, got it, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2051" target="_blank">00:34:11.360</a></span> | <span class="t">Sorry, I have also a question. I didn't find any reaction button that I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2059" target="_blank">00:34:19.040</a></span> | <span class="t">raise my hand. Can I ask the question right now? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2062" target="_blank">00:34:22.480</a></span> | <span class="t">Okay, perfect. So my question is about the open source framework for pre-training that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2068" target="_blank">00:34:28.480</a></span> | <span class="t">mentioned. I think it was NEMO, right? Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2071" target="_blank">00:34:31.600</a></span> | <span class="t">Yes. So do you think, potentially, if I have a set of videos, but those videos, originally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2080" target="_blank">00:34:40.240</a></span> | <span class="t">they were not necessarily in the RGB space, okay? So I don't know, for example, satellites,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2086" target="_blank">00:34:46.000</a></span> | <span class="t">or anything, a spectral wavelength, or whatever. And I just somehow mapped them to videos. Do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2091" target="_blank">00:34:51.600</a></span> | <span class="t">think I can still customize your framework and just pre-train my own tokenizer, or basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2100" target="_blank">00:35:00.800</a></span> | <span class="t">whatever else that exists in that framework? Yeah, if your data domain is different from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2111" target="_blank">00:35:11.200</a></span> | <span class="t">video, it's recommended to fine-tune the tokenizers. So just fine-tuning, do you think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2118" target="_blank">00:35:18.560</a></span> | <span class="t">you're going to work? So, because if the tokenizer is not fine-tuned, it might produce some artifacts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2130" target="_blank">00:35:30.560</a></span> | <span class="t">for your data if your data domain is different. Sorry, yeah, go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2139" target="_blank">00:35:39.600</a></span> | <span class="t">After fine-tuning the tokenizer, you might also want to fine-tune the diffusion transformer or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2146" target="_blank">00:35:46.720</a></span> | <span class="t">autoregressive transformer. Yeah, both of these are supported in the framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2151" target="_blank">00:35:51.200</a></span> | <span class="t">Awesome. And, you know, I can also pre-train the tokenizer using the current framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2158" target="_blank">00:35:58.560</a></span> | <span class="t">Or fine-tune. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2160" target="_blank">00:36:00.400</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2162" target="_blank">00:36:02.080</a></span> | <span class="t">Thanks for the presentation. I had a quick question related to some of the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2170" target="_blank">00:36:10.720</a></span> | <span class="t">well, you mentioned it's coming soon, for multi-view generation and more camera control.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2175" target="_blank">00:36:15.200</a></span> | <span class="t">So, curious if you could speak any more towards how you're approaching multi-view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2181" target="_blank">00:36:21.040</a></span> | <span class="t">or how to make sure that the camera intrinsics correlate between one another,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2187" target="_blank">00:36:27.120</a></span> | <span class="t">you know, if they're all video-based generation versus having a true, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2191" target="_blank">00:36:31.120</a></span> | <span class="t">grounded scene understanding, how you guys are approaching that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2194" target="_blank">00:36:34.960</a></span> | <span class="t">Yes, that's a good question. So, these are coming soon, but the techniques are covered in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2204" target="_blank">00:36:44.000</a></span> | <span class="t">For example, for multi-view generation, the different number of views are folded into the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2210" target="_blank">00:36:50.800</a></span> | <span class="t">into one of the dimensions in the data. So, the model input is still roughly the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2218" target="_blank">00:36:58.960</a></span> | <span class="t">It seems to have, in fact, it's falling into the time, the time axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2224" target="_blank">00:37:04.000</a></span> | <span class="t">And for the camera intrinsics, it's not, it's not used now, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2233" target="_blank">00:37:13.440</a></span> | <span class="t">if you have a consistent intrinsics, we don't, you don't have this problem, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2239" target="_blank">00:37:19.040</a></span> | <span class="t">if your intrinsics is going to change across different training data, I guess it's helpful to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2247" target="_blank">00:37:27.280</a></span> | <span class="t">include that in the conditioning information. At least in the example, we use consistent intrinsics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2262" target="_blank">00:37:42.640</a></span> | <span class="t">Yeah, so you're saying it has more to do with, perhaps, more the training data that you're using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2267" target="_blank">00:37:47.600</a></span> | <span class="t">to post-train these models, to have it be consistent and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2271" target="_blank">00:37:51.120</a></span> | <span class="t">have similar intrinsics? Is that sort of what you're saying?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2275" target="_blank">00:37:55.040</a></span> | <span class="t">Yeah, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2277" target="_blank">00:37:57.120</a></span> | <span class="t">All right, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2288" target="_blank">00:38:08.400</a></span> | <span class="t">I, I can, I can answer questions in the, in the chat. Yeah, I wasn't looking at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2295" target="_blank">00:38:15.200</a></span> | <span class="t">Yeah, so what does the token represent in this case? One pixel of video? So, the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2305" target="_blank">00:38:25.360</a></span> | <span class="t">yeah, the tokens, the tokens are a patch of video. Say, for an image, an 8x8 patch is one pixel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2318" target="_blank">00:38:38.720</a></span> | <span class="t">For a video, an 8x8 patch is, is one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2324" target="_blank">00:38:44.560</a></span> | <span class="t">That means, roughly, for one second, the video is, if it's 30 frames,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2330" target="_blank">00:38:50.560</a></span> | <span class="t">in the, in the time domain, you have, like, four, you have, like, four tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2339" target="_blank">00:38:59.040</a></span> | <span class="t">And spatially, that depends on your resolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2347" target="_blank">00:39:07.840</a></span> | <span class="t">Yeah, so the video doesn't have a depth map, but it can be,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2352" target="_blank">00:39:12.080</a></span> | <span class="t">you can add it in the post-training process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2356" target="_blank">00:39:16.000</a></span> | <span class="t">What's the different, difference between post-training and fine-tuning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2364" target="_blank">00:39:24.400</a></span> | <span class="t">I'd say, like, post-training, it's a fancy word of fine-tuning. And now it's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2373" target="_blank">00:39:33.040</a></span> | <span class="t">now it's fine-tuning specifically referenced to, like, it's all special techniques, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2380" target="_blank">00:39:40.320</a></span> | <span class="t">just continue pre-training. I would say these two words are interchangeably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2385" target="_blank">00:39:45.680</a></span> | <span class="t">Oh, number of tokens for each of these foundation models trained. So, for, yeah, for pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2402" target="_blank">00:40:02.080</a></span> | <span class="t">it's a, it's a hundred million video clips level. And I, I have the equation in the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2410" target="_blank">00:40:10.080</a></span> | <span class="t">and according to, so each video clip is roughly five seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2415" target="_blank">00:40:15.120</a></span> | <span class="t">And using, using that information, you can calculate the number of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2421" target="_blank">00:40:21.360</a></span> | <span class="t">I'd say it's roughly on the scale of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2424" target="_blank">00:40:24.160</a></span> | <span class="t">10, 10 trillion, at least 10 trillion tokens or more. You can calculate it for yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2432" target="_blank">00:40:32.000</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2446" target="_blank">00:40:46.480</a></span> | <span class="t">Yeah, what type of hardware is adequate for post-training on our own data? So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2455" target="_blank">00:40:55.520</a></span> | <span class="t">the post-training, the open-source now needs, like, eight, eight H100 for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2461" target="_blank">00:41:01.840</a></span> | <span class="t">diffusion and two H100 for the autoregressive model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2468" target="_blank">00:41:08.160</a></span> | <span class="t">But with some technique, like activation offloading or LoRa,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2475" target="_blank">00:41:15.280</a></span> | <span class="t">I believe LoRa and GPUs can also be used for post-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2480" target="_blank">00:41:20.080</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2495" target="_blank">00:41:35.440</a></span> | <span class="t">So, so the, the word, the word in our model name, we, we want to emphasize that the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2506" target="_blank">00:41:46.720</a></span> | <span class="t">the model has spatial consistency and we're aiming to provide the best foundation model for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2514" target="_blank">00:41:54.560</a></span> | <span class="t">robotics post-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2517" target="_blank">00:41:57.440</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2530" target="_blank">00:42:10.880</a></span> | <span class="t">Okay, I think that's all the questions in the chat. Any more questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2536" target="_blank">00:42:16.640</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2538" target="_blank">00:42:18.800</a></span> | <span class="t">Hi Ethan, thanks for the talk. I had a question. So, for, you said, for identifying high-quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2547" target="_blank">00:42:27.040</a></span> | <span class="t">videos, you, high-quality clips, you filter them out first, right? How do you do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2553" target="_blank">00:42:33.200</a></span> | <span class="t">Do you use, like, some already available open models or do you train your own models for that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2558" target="_blank">00:42:38.320</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2561" target="_blank">00:42:41.600</a></span> | <span class="t">Yeah, that's a good question. So, so there are different, there are different metrics for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2568" target="_blank">00:42:48.160</a></span> | <span class="t">filtering videos. For, there are both heuristic and some, some models. Heuristic, like, if the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2575" target="_blank">00:42:55.760</a></span> | <span class="t">video is static, it's, it's basic image, it's not a good video, or you can also train a model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2584" target="_blank">00:43:04.640</a></span> | <span class="t">classify, classify the quality of this, this video, like, aesthetic score. So, that, that might need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2595" target="_blank">00:43:15.120</a></span> | <span class="t">some other training and labeling, and also motion scoring, like, how much motion it's in the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2601" target="_blank">00:43:21.520</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2605" target="_blank">00:43:25.520</a></span> | <span class="t">So, in your case, you guys trained a custom model for that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2609" target="_blank">00:43:29.360</a></span> | <span class="t">based on these metrics, maybe motion or based on the aesthetics?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2613" target="_blank">00:43:33.040</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2614" target="_blank">00:43:34.640</a></span> | <span class="t">There, there are a lot of, like, available models open source already. You, you can check it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2621" target="_blank">00:43:41.520</a></span> | <span class="t">Like, there are aesthetic, aesthetic classifier, etc. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2627" target="_blank">00:43:47.040</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2629" target="_blank">00:43:49.120</a></span> | <span class="t">Okay, thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2630" target="_blank">00:43:50.000</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2640" target="_blank">00:44:00.560</a></span> | <span class="t">Um, another quick question is, you know, as Cosmos develops or releases more iterations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2646" target="_blank">00:44:06.960</a></span> | <span class="t">how do you foresee approaches to adding more controllability within the scenario?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2653" target="_blank">00:44:13.760</a></span> | <span class="t">So, more refined control over what's happening in the scene, and what variables you want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2659" target="_blank">00:44:19.120</a></span> | <span class="t">change versus not to change? Sort of inherent to, you know, video generation in general, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2664" target="_blank">00:44:24.320</a></span> | <span class="t">you don't have as much control, and curious if you're seeing that as a requirement, and how,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2668" target="_blank">00:44:28.560</a></span> | <span class="t">how you're thinking of approaching it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2669" target="_blank">00:44:29.840</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2671" target="_blank">00:44:31.920</a></span> | <span class="t">Yeah, I think that's very important for post-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2675" target="_blank">00:44:35.360</a></span> | <span class="t">That also depends on different use cases. Say, you have, um, depending on your data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2685" target="_blank">00:44:45.120</a></span> | <span class="t">if your, say, if your data have more different parameters you can use as conditioning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2691" target="_blank">00:44:51.920</a></span> | <span class="t">I think adding, adding them into the training would definitely help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2696" target="_blank">00:44:56.240</a></span> | <span class="t">Yeah, if you have, if you have additional, like, camera intrinsics, if you have additional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2704" target="_blank">00:45:04.160</a></span> | <span class="t">cameras as condition, additional signals, like audio, all of them can use as conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2711" target="_blank">00:45:11.680</a></span> | <span class="t">The model, it's, it's quite flexible for, to add additional conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2718" target="_blank">00:45:18.720</a></span> | <span class="t">For the diffusion model, they allow you to add it through cross-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2723" target="_blank">00:45:23.840</a></span> | <span class="t">and similar for autoregressive model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2726" target="_blank">00:45:26.800</a></span> | <span class="t">[silence]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2737" target="_blank">00:45:37.440</a></span> | <span class="t">Ethan, I have another question that's somewhat related.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2741" target="_blank">00:45:41.840</a></span> | <span class="t">How much, I was a little confused about how much of the, sort of, the, the ability to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2753" target="_blank">00:45:53.040</a></span> | <span class="t">realistic, you know, sort of, physics and physical model, well, like, sort of, world models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2760" target="_blank">00:46:00.640</a></span> | <span class="t">is due to training versus some, like, inductive bias in the model, and, like, what were the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2766" target="_blank">00:46:06.960</a></span> | <span class="t">if, if the, in as much as it was inductive bias, what, what were the key things there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2773" target="_blank">00:46:13.840</a></span> | <span class="t">So, I, I think two key things are data and scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2781" target="_blank">00:46:21.360</a></span> | <span class="t">So, I, the, the model itself, as they grow larger and larger, a lot of the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2792" target="_blank">00:46:32.720</a></span> | <span class="t">a lot of the 3D capability, consistency physics intrinsics automatically appear when the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2801" target="_blank">00:46:41.440</a></span> | <span class="t">is bigger. And another thing is data. I think in the data, you need to have enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2808" target="_blank">00:46:48.480</a></span> | <span class="t">demonstration of different physical property for the model to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2815" target="_blank">00:46:55.440</a></span> | <span class="t">It says the model itself doesn't have a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2818" target="_blank">00:46:58.160</a></span> | <span class="t">inductive bias. It's just, we're just using transformers. There's no,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2825" target="_blank">00:47:05.360</a></span> | <span class="t">like, spatial attention, temporal attention, those kind of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2830" target="_blank">00:47:10.800</a></span> | <span class="t">Okay, got it. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2836" target="_blank">00:47:16.240</a></span> | <span class="t">[no audio]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2854" target="_blank">00:47:34.240</a></span> | <span class="t">If, if there aren't other questions, I actually have one more. So, in, in the, sort of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2862" target="_blank">00:47:42.000</a></span> | <span class="t">like, the original diagram of the architecture, there's some, some things that I didn't understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2870" target="_blank">00:47:50.320</a></span> | <span class="t">about the, the positional embeddings. Like, there's the, there's, like, two different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2877" target="_blank">00:47:57.840</a></span> | <span class="t">positional embeddings, or three different positional embeddings, I think. Yeah, so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2882" target="_blank">00:48:02.240</a></span> | <span class="t">there's, like, this absolute positional embedding. And then, actually, there's another diagram that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2887" target="_blank">00:48:07.600</a></span> | <span class="t">where there's another positional embedding that goes into the cross attention, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2892" target="_blank">00:48:12.080</a></span> | <span class="t">Yeah, this, or, well, it's, I'm not sure what that is, that time step in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2896" target="_blank">00:48:16.880</a></span> | <span class="t">scale shift gate. So, I got, I was kind of confused about what the purpose of all these are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2901" target="_blank">00:48:21.520</a></span> | <span class="t">Yeah, yeah. So, so, the timestamps is specific to the diffusion models. You know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2910" target="_blank">00:48:30.240</a></span> | <span class="t">the diffusion process, you're going through multiple steps to diffuse noise, and it becomes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2917" target="_blank">00:48:37.760</a></span> | <span class="t">a clear, clear and crisp video, right? So, during training, the process is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2925" target="_blank">00:48:45.760</a></span> | <span class="t">you randomly apply some noise to the tokens, and for, you also need to indicate the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2934" target="_blank">00:48:54.560</a></span> | <span class="t">like, how much noise is added. If there are more noise, the timestamps, it's an earlier step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2941" target="_blank">00:49:01.760</a></span> | <span class="t">The less noise, the timestamp is, is close to the end of the generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2948" target="_blank">00:49:08.000</a></span> | <span class="t">So, during inference, the model can gradually remove the noise and the condition on the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2957" target="_blank">00:49:17.600</a></span> | <span class="t">which timestamp it is on. For the absolute positional embedding and 3D robe,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2964" target="_blank">00:49:24.880</a></span> | <span class="t">those, those tell the model, for this token, which, which position it is in the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2973" target="_blank">00:49:33.760</a></span> | <span class="t">Sure. No, I guess I was just confused about the, what is the need for both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2983" target="_blank">00:49:43.120</a></span> | <span class="t">the rotary positional embedding and also the absolute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2986" target="_blank">00:49:46.480</a></span> | <span class="t">positional embedding? Like, why is that, why are both of those needed?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=2989" target="_blank">00:49:49.920</a></span> | <span class="t">So, not necessarily, but this can improve the model. In fact, if you just use absolute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3001" target="_blank">00:50:01.040</a></span> | <span class="t">positional embedding, it can also work. Okay, I see. Okay, got it, got it. Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3011" target="_blank">00:50:11.120</a></span> | <span class="t">Yeah, Ethan, can I ask a question? Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3016" target="_blank">00:50:16.560</a></span> | <span class="t">Yeah. So, there are, there was a comment in the chat about the use of vector quantization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3025" target="_blank">00:50:25.920</a></span> | <span class="t">Now, how is that used, actually? I don't think that it's used for selecting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3035" target="_blank">00:50:35.920</a></span> | <span class="t">patches, but it could be used for the discrete latent space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3041" target="_blank">00:50:41.040</a></span> | <span class="t">It's a, it's a training technique. It's basically for, for the autoregressive part of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3049" target="_blank">00:50:49.280</a></span> | <span class="t">you need to, you need to have fixed set of vocabulary and the input are basically indices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3058" target="_blank">00:50:58.080</a></span> | <span class="t">like this word is number, number one, number two, etc. So, when you're training the tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3067" target="_blank">00:51:07.280</a></span> | <span class="t">you need to quantize them into the codebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3072" target="_blank">00:51:12.480</a></span> | <span class="t">When it's choosing, for each patch, it basically looks for the closest vector in the codebook</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3084" target="_blank">00:51:24.400</a></span> | <span class="t">and pick it out. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3090" target="_blank">00:51:30.160</a></span> | <span class="t">Thanks, Ethan. I had a question about the size of the models that were posted to HuggingFace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3111" target="_blank">00:51:51.600</a></span> | <span class="t">How did you guys select those sizes? Did you experiment with larger sizes? Yeah, those are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3118" target="_blank">00:51:58.000</a></span> | <span class="t">my questions. So, yeah, this is a first release of Cosmos 1.0. There might be bigger models in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3132" target="_blank">00:52:12.400</a></span> | <span class="t">the future, and because we, when doing research, we want to go from small to big. We're not doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3143" target="_blank">00:52:23.520</a></span> | <span class="t">it blank shot, and I think we're still in the infancy of word foundation models, and they're,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3150" target="_blank">00:52:30.720</a></span> | <span class="t">let's say, it's kind of like GPT-1 or GPT-2 stage of word foundation model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3159" target="_blank">00:52:39.600</a></span> | <span class="t">Bigger models will definitely come in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3162" target="_blank">00:52:42.720</a></span> | <span class="t">Got it. Thanks. Was there any thinking in terms of, well, this is good enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3170" target="_blank">00:52:50.320</a></span> | <span class="t">for most of the applications we see from, I don't know, customers or partners?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3175" target="_blank">00:52:55.840</a></span> | <span class="t">It's not good enough yet. It can be better and better. The model now has some emerging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3189" target="_blank">00:53:09.280</a></span> | <span class="t">physics property in the generative video. I would think it can get better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3199" target="_blank">00:53:19.840</a></span> | <span class="t">Thanks. Guys, so it looks like Sviks passed the baton to me. He had to drop off a call,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3228" target="_blank">00:53:48.240</a></span> | <span class="t">or for an in real life meeting, and so I want to, if there are any other questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3236" target="_blank">00:53:56.400</a></span> | <span class="t">encourage you to ask. Otherwise, I think we can take a little bit of time to discuss the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3244" target="_blank">00:54:04.080</a></span> | <span class="t">paper, and I actually have to have a hard stop at, in three minutes, so I need to drop off</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3254" target="_blank">00:54:14.240</a></span> | <span class="t">at that time. So, first of all, Ethan, this is fantastic. I hope you keep coming back to these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3261" target="_blank">00:54:21.600</a></span> | <span class="t">paper club meetings, and even if you want to present someone else's paper rather than your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3273" target="_blank">00:54:33.840</a></span> | <span class="t">own, certainly anytime you publish a paper, we definitely want to see you here. But if others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3279" target="_blank">00:54:39.600</a></span> | <span class="t">publish paper and you think it's exciting and you want to share it, we definitely would love to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3284" target="_blank">00:54:44.560</a></span> | <span class="t">you as well. Thank you. Thank you for hosting. Yeah, I mean, Sviks, but I'm happy to facilitate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3294" target="_blank">00:54:54.560</a></span> | <span class="t">where I can. Are there other questions for Ethan before we, I'm not sure how much time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3301" target="_blank">00:55:01.520</a></span> | <span class="t">we have really to discuss the next paper, but, okay, does anyone want to volunteer? I think that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3314" target="_blank">00:55:14.160</a></span> | <span class="t">I saw some chat, and I'm not sure about this, but I saw some noise on the Discord about people just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3321" target="_blank">00:55:21.360</a></span> | <span class="t">picking things from the list of papers that are in our backlog, and then just giving brief, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3329" target="_blank">00:55:29.280</a></span> | <span class="t">sort of very fast discussions of those. Maybe I think that in the past we've taken 10 or 15 minutes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3336" target="_blank">00:55:36.880</a></span> | <span class="t">to just go over, summarize the paper for everyone. Probably you'll, people won't probably pre-read,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3343" target="_blank">00:55:43.360</a></span> | <span class="t">but it'll just be a good, you know, sort of way to understand in some detail what are the key</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3349" target="_blank">00:55:49.760</a></span> | <span class="t">points from the paper. So, maybe I can post that. I think it's already in Discord, but I can post</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3356" target="_blank">00:55:56.800</a></span> | <span class="t">that in Discord. If there are people who are not on Discord, maybe I can ask, I can suggest Sviks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3364" target="_blank">00:56:04.320</a></span> | <span class="t">also post that in the, like, in, like, Twitter or whatever. Is that, is there, unless, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3373" target="_blank">00:56:13.520</a></span> | <span class="t">someone wants to volunteer to present a paper next week? Okay. Somebody asked that, for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3386" target="_blank">00:56:26.480</a></span> | <span class="t">Discord channel, if no one can dig that up, I suggest, I think it's on the LatentSpace, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3396" target="_blank">00:56:36.720</a></span> | <span class="t">on, you can, you can dig through the LatentSpace sub stack, or, like, maybe there's a, I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3402" target="_blank">00:56:42.640</a></span> | <span class="t">there's a website, too, and you can find it there. Otherwise, you can hit me on X, and I'll find it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3411" target="_blank">00:56:51.920</a></span> | <span class="t">for you, or LinkedIn, as well. I'm, on both of them, my user handle is Haneke, or you can obviously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3421" target="_blank">00:57:01.360</a></span> | <span class="t">also, Sviks, or anyone else here. Oh, there it goes. Okay, great. Okay, guys, so, grab that if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3429" target="_blank">00:57:09.280</a></span> | <span class="t">you need it. I'm going to end the meeting, and, yeah, I got to go. So, I'm going to, I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3434" target="_blank">00:57:14.000</a></span> | <span class="t">stop recording. Actually, I probably was supposed to stop recording, and guess what,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3438" target="_blank">00:57:18.400</a></span> | <span class="t">the edit, whatever. And thank you very much. We'll see you next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3443" target="_blank">00:57:23.360</a></span> | <span class="t">Goodbye.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=og59L4JECz4&t=3449" target="_blank">00:57:29.280</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>