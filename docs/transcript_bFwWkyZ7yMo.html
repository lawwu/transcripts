<html><head><title>Multimodality: The Next Big Step (Demis Hassabis - Google DeepMind CEO)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Multimodality: The Next Big Step (Demis Hassabis - Google DeepMind CEO)</h2><a href="https://www.youtube.com/watch?v=bFwWkyZ7yMo" target="_blank"><img src="https://i.ytimg.com/vi_webp/bFwWkyZ7yMo/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>I think we're just at the beginning of actually understanding what a full multimodal model system, how exciting that might be to interact with. And it'll be quite different to I think what we're used to today with the chatbots. I think the next versions of this over in the next year, 18 months, you know, maybe we'll have some contextual understanding around the environment around you through a camera or whatever it is, a phone.</p><p>You know, I could imagine that as the next awesome glasses or the next step. And then I think that we'll start becoming more fluid in understanding, oh, let's sample from a video. Let's use voice. Maybe even eventually things like touch and, you know, if you think about robotics and other things, you know, sensors, other types of sensors.</p><p>As these systems and things like Gemini are becoming more multimodal and we start ingesting things like video and, you know, audio visual data as well as text data, and then, you know, the system starts correlating those things together. I think that is a form of proper grounding, actually. So I do think our systems are going to start to understand, you know, the physics of the real world better, where you're starting to learn about what your actions do in the world and how that affects the world itself, but also what next learning episode you're getting.</p><p>So, you know, these RL agents we've always been working on and pioneered like AlphaZero and AlphaGo, they're active learners. What they decide to do next affects what the next learning piece of data or experience they're going to get. So there's this very interesting sort of feedback loop. And of course, if we ever want to be good at things like robotics, we're going to have to understand how to act in the real world.</p><p>On the robotics subject, Ilya said when he was on the podcast that the reason OpenAI gave up on robotics was because they didn't have enough data in that domain, at least at the time they were pursuing it. I mean, you guys have put out different things like RoboTransformer and other things.</p><p>Do you think that's still a bottleneck for robotics progress? So Ilya is right that that is more challenging because of the data problem. But it's also I think we're starting to see the beginnings of these large models being transferable to the robotics regime, learning in the general domain, language domain and other things.</p><p>And then just treating tokens like Gato as any type of token, you know, the token could be an action, it could be a word, it could be part of an image, a pixel or whatever it is. And that's what I think true multimodality is. And to begin with, it's harder to train a system like that than a straightforward text language system.</p><p>But actually, you know, going back to our earlier conversation of transfer learning, you start seeing that a true multimodal system, the other modalities benefit some different modalities. So you get better at language because you now understand a little bit about video. So I do think it's harder to get going, but actually ultimately, we'll have a more general, capable system like that.</p></div></div></body></html>