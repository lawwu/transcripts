<html><head><title>All-In Summit: Stephen Wolfram on computation, AI, and the nature of the universe</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>All-In Summit: Stephen Wolfram on computation, AI, and the nature of the universe</h2><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M" target="_blank"><img src="https://i.ytimg.com/vi_webp/2cQmQIYNI5M/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=0 target="_blank"">0:0</a> Dave welcomes Stephen Wolfram to All-In Summit ‘23!<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=157 target="_blank"">2:37</a> Computational irreducibility<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=298 target="_blank"">4:58</a> The paradox of simple heuristics<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=409 target="_blank"">6:49</a> AI<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=537 target="_blank"">8:57</a> Cellular automata<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=850 target="_blank"">14:10</a> Limitations of AI<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=1093 target="_blank"">18:13</a> Syntax, logic, LLMs and other high-potential AI realms<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=1437 target="_blank"">23:57</a> Generative AI and interconcept space<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=1580 target="_blank"">26:20</a> The nature of the universe<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=1794 target="_blank"">29:54</a> Electrons – size, topology and structure<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=1878 target="_blank"">31:18</a> Time, spacetime, gravity and the boundaries of human observers<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=2213 target="_blank"">36:53</a> Persistence and other elements of consciousness humans take for granted<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=2289 target="_blank"">38:9</a> The concept of the ruliad<br><a href="https://www.youtube.com/watch?v=2cQmQIYNI5M&t=2493 target="_blank"">41:33</a> Joy<br><h3>Transcript</h3><div class='max-width'><p>(audience applauding) - Solo, no besties. - Yeah, everybody else was scared away, I'm afraid. - Yeah, or you scared them away. Yeah, I mean, it was a challenging prompt. Interview Stephen Wolfram on stage in 40 minutes. So here we go. (upbeat music) ♪ Dash ride ♪ ♪ Rain man David Sackman ♪ ♪ I'm going all in ♪ ♪ And it said ♪ ♪ We open sourced it to the fans ♪ ♪ And they've just gone crazy ♪ ♪ I'll be the queen of Kinwans ♪ ♪ I'm going all in ♪ - It's a huge honor to talk to Stephen Wolfram, creator of Mathematica, Wolfram Alpha, and the Wolfram Language, the author of A New Kind of Science, the originator of Wolfram Physics Project, and head of Wolfram Research.</p><p>Stephen first used a computer in 1973, and quickly became a leader in the emerging field of scientific computing. In 1979, he began the construction of SMP, the first modern computer algebra system. He published his first scientific paper at age 15, and had received his PhD in theoretical physics from Caltech by 20.</p><p>Wolfram's early scientific work was mainly in high energy physics, quantum field theory, cosmology, and complexity, discovering a number of fundamental connections between computation and nature, and inventing such concepts as computational irreducibility, which we'll talk about today. Wolfram's work led to a wide range of applications and provided the main scientific foundation for such initiatives of complexity theory and artificial life.</p><p>Wolfram used his ideas to develop a new randomness generation system and a new approach to computational fluid dynamics, both of which are now in widespread use. The release of Wolfram Alpha in May of 2009 was a historic step that has defined a new dimension for computation and AI, now relied on by millions of people to compute answers both directly and through intelligent assistants, such as Siri and Alexa and so on, among others.</p><p>So, thank you for being here. - Thanks for having me. - I worked at the Lawrence Berkeley National Lab at the Center for Beam Physics for two and a half years, and it was when I was an undergrad, and I worked exclusively in Mathematica, as I shared with you the other night.</p><p>So, that's when I first got to know about you and your work. Who here has seen an interview that Steven's done before, just to get a sense? - Okay. - I wanna try and guide the conversation a little bit. So, maybe we could start with computers. - Okay. - You talk about this concept of computational irreducibility.</p><p>- Yes. - Maybe you can just, and I wanna try and connect with a broad audience in this conversation. - Yeah, yeah, right. So, what is computation? - What is computation? - Okay, so, at the base, computation is about you specify rules, and then you let those rules, you figure out what the consequences of those rules are.</p><p>Computers are really good at that. You give a little program, the computer will run your program, it will generate output. I would say that the bigger picture of this is how do you formalize anything? We can just use words, we can talk vaguely about things. How do you actually put something down that is a kind of precise formalism that lets you work out what will happen?</p><p>That's been done in logic, it's been done in mathematics, it's done in its most general form in computation, where the rules can be kind of anything you can specify to a computer. Then the question is, given that you have the rules, is that the end of the story? Once you have the rules, do you then know everything about what will happen?</p><p>Well, that's kind of what one would assume, that the traditional view of science is once you work out the equations and so on, then you're done. Then you can predict everything, you've done all the hard work. Turns out this is not true. Turns out that even with very simple rules, the consequences of those rules can be arbitrarily hard to work out.</p><p>It's kind of like if you just run the rules step by step, step by step, it's making some pattern on screen or whatever else, you can just run all those steps, see what happens, that's all good. Then you can ask yourself, can you jump ahead? Can you say, I know what's going to happen.</p><p>I know the answer is going to be 42 or something at the end. Well, the point is that that isn't in general possible. That in general, you have to go through all the steps to work out what will happen. And that's kind of a fundamental limitation of kind of the sort of prediction in science.</p><p>And it's something people have gotten very used to the idea that with science, we can predict everything. But from within science, we see this whole phenomenon of computational irreducibility. It's related to things like Godel's theorem, undecidability, halting problem, all kinds of other kinds of ideas. But from within science, we see this fundamental limitation, this fundamental inability for us to be able to say what will happen.</p><p>- So we can't just skip ahead in a lot of cases. We can't just create simple heuristics or simple solves that avoid all of the hard work to simulate something, to calculate something, to come up with the computational output of something we're trying to figure out. - In a sense, this is a good thing for us because it's kind of we lead our lives, time progresses, things happen.</p><p>If we could just say, we don't need to go through all of those steps of time. We could just say, and the end, it will be 37 or something. That would be a bad feature for us feeling that it was worthwhile to sort of lead our lives and see time progress.</p><p>It will be a kind of, you don't really need time to progress. You can always just say what the answer will be. I mean, this kind of idea has many consequences. I mean, for example, when it comes to AIs, you say, well, you've got this AI system and it's doing all kinds of things.</p><p>Can you figure out what it will do? You might want to figure out what it will do 'cause you might want to say, I never want the AI system to do this very bad thing that I'm trying to prevent. So then you say, well, can I work out what it will do?</p><p>Can I be sure that these rules that I'm putting in for the AI system will never have it do this very bad thing? Well, computational irreducibility says you can't guarantee that. You kind of have this trade-off with an AI system. You can either say, let's constrain it a lot.</p><p>Then we can know what it will do. But then, or let's let it sort of have its way and do what it does. If we don't let it have its way, it's not really making use of the computational capabilities that it has. We kind of have this trade-off. We either can understand what's gonna happen, in which case we don't let our AIs really do what they can do, or we always say, okay, we're going to run the risk of the AI doing something unexpected.</p><p>- So can we just, so AI, a lot of what people call AI are predictive models that are effectively built off of statistics that make some prediction of what the right next step in a sequence of things should be, whether it's a pixel to generate an image or a series of words to generate a chat response through an LLM model like chat GPT or BARD or what have you.</p><p>Those are statistical models trained on past data. Are they, is that different than the problems in computation that you're talking about, about better understanding the universe, the nature of the universe, solving bigger problems, that AI has its limitation in how we think and talk about it today, and maybe you can connect computation and this idea of AI being this very simple, heuristical, statistical thing that just predicts stuff.</p><p>- Right, well, I mean, the computational universe of possible programs, possible rules is vast. And there are many-- - Sorry, I just wanna, I just wanna make sure everyone understands that. The, should say the computational-- - Yeah, the computational universe. So, you know-- - The thing, the set of things you can compute that you wanna try and compute.</p><p>- Yes, I mean, so people are used to writing programs that are intended for particular human purposes. But let's say you just write a program at random. You just put in the program, it's a random program. Question is, what does the random program do? So a big thing that I discovered in the 1980s is the thing that greatly surprised me was even a very simple program can do very complicated things.</p><p>I had assumed that if you want to do complicated things, you would have to set up a complicated program. Turns out that's not true. Turns out that in nature, nature kind of discovered this trick in a sense. You know, we see all this complexity in nature. It seems like the big origin of that complexity is just this phenomenon that even a very simple program can do very complicated things.</p><p>So, I mean, that's the, so this sort of universe-- - Just give a quick example, if you wouldn't mind. Like-- - Yeah, yeah, so I mean-- - Like three genes in a genome, in DNA. - Okay, so my favorite example-- - Yes. - Are things called cellular automata. - Yes.</p><p>- And they work like this. They have a line of cells. Each one is either black or white. It's an infinite line of cells. And you have a rule that says you go down the page, making successive lines of cells. You go down the page and you say the color of a cell on the next line will be determined by a very simple kind of lookup from the color of the cell right above it and to its left and right.</p><p>Okay, so very simple setup. There are 256 possible rules with just two colors and nearest neighbors. You can just look at what all of them do. Many of them do very simple things. They'll just make some triangle of black-- - It looks like a pyramid or triangle when it's done.</p><p>- Right, right, right. And then my all-time favorite, it's kind of you turn the telescope into this computational universe and see what's out there. My all-time favorite discovery is rule 30 and the numbering of these rules. You can specify that. Rule 30, you start it off from one black cell and it makes this really complicated pattern.</p><p>It makes a pattern where if you just saw it, you would say somebody must have gone to a huge amount of effort-- - It looks designed. - Yes. - It looks like there was an architect-- - Yes. - That came in and designed that thing. And that's the only way that thing could have been created 'cause it's so beautiful and intricate and-- - Right, right.</p><p>- Resonates. But-- - But in fact-- - It had two simple rules. Change, be black if the left is black and the right is white and be white if-- - Yeah, right. - Right. - Those kinds of things. So that's the kind of setup. And for example, when you look at rule 30, you look at the center column of cells, it looks for all practical purposes completely random.</p><p>Even though you know that it was really made from some simple rule, when you see it produced, it looks random. It's kind of like a good analogy if you know, you know, digits of pi, people memorize, you know, 3.14159, that's about as far as I can go. - Me too.</p><p>You were one ahead of me. - It's, I, that's why I built software to do these things. We can go to, you know-- - I don't know about that PhD at 20. (audience laughing) - But, you know, the point is that the rule for generating those digits, it's, you know, the ratio of the circumference diameter of a circle, there's a very definite rule, but once you've generated those digits, they seem completely random.</p><p>- Yes. - It's the same kind of phenomenon, but, you know, you can have a simple rule, it produces things that look very complicated. - Yes. - Now, so-- - So that's a simple computer. That's a simple computational exercise. - By the way, it's not such a simple computer.</p><p>Because it turns out, when you kind of try and rank, you know, you set up a computer, you make it with electronics, or you might make it, you know, some mechanical computer from the past, or something like this. You ask the question, you build a computer of a certain kind, how sophisticated are the computations it can do?</p><p>Is it just an adding machine? Is it just a multiplying machine? - Yes. - How far does it get? Big discovery from the 1930s is, you can make a fixed piece of hardware that's capable of running any program, that's capable of doing any computation. That's the discovery that launched the possibility of software, launched most of modern technology.</p><p>So, one might have thought, you have to go to a lot of effort to make a universal computer. Turns out that's not true. It's a thing I call the principle of computational equivalence, which kind of tells one something about how far one has to go. And the answer is, pretty much, as soon as you see complicated behavior, the chances are, you can kind of use that complicated behavior to do any computation you want.</p><p>And that's actually, that's the reason for this computational irreducibility phenomenon, because here's how it works. So, let's say you've got some system, and it's doing what it does, and you're trying to predict what it's going to do. So, both the system itself, and you as the predictor, are computational systems.</p><p>So then the question is, can you, the predictor, be so much smarter than the system you're predicting, you can just jump ahead and say, I know what you're going to do, I've got the answer. Or are you stuck being kind of equivalent in computational sophistication to the system you're trying to predict?</p><p>So, this principle of computational equivalence says, whether you have a brain, or a computer, or mathematics, or statistics, or whatever else, you are really just equivalent in your computational sophistication to the system that you're trying to predict. And that's why you can't make that prediction. That's why computational irreducibility happens.</p><p>But, you know, you were asking about AI. I mean, the thing that we have only just started mining is this computational universe of all possible programs. Most programs that we use today were engineered by people. People said, I'm going to put this piece in, and that piece in, and that piece in.</p><p>- So now we have a program that can make programs. - Yes, well, we have sort of a vast universe of possible programs. We can say, if we know what we want the program to do, we can just search this computational universe and find a program that does it.</p><p>Often, I've done this for many years, for many purposes. - Just give me an example there. - Well, so, very simple example, actually, from Rule 30, is you want to make a random number generator. You say, how do I make something that makes good randomness? Well, you can just search the set of possible simple programs, and pretty soon you find one that makes good randomness.</p><p>You ask me, why does it make good randomness? I don't know. It's not something, there's no narrative explanation. - So now with AI, we're generating a ton of programs, and we now have a bigger space of programs, or bigger library to go select from, to solve problems or figure stuff out for us?</p><p>Is that-- - Actually, I think AI is sort of, AI is very limited in the computational universe. - Yes. - I mean, the computational universe-- - This is the connection I wanted to make, because, yeah. - I mean, it's, you know, the computational universe is all these possible rules.</p><p>We can talk later about whether the universe-- - Sorry, I just want to be clear, 'cause you used the term universe in the sense of all the things, and I want to disconnect that from everyone's concept of universe. - Yes, yes. We're gonna talk about whether the, I hope we're gonna talk about whether the physical universe-- - We are gonna talk about the universe and the nature of the universe, which is, we're gonna talk about consciousness, then we're gonna smoke weed, and then we're gonna go to lunch.</p><p>(audience laughing) - We're gonna talk about, you know, whether our physical universe is part of this computational universe, but when I say computational universe, I just mean this very abstract idea of all these possible programs. - All the programs, the library of possible programs. - Yes. - Yeah. - And so, there are many, many things that those programs can do.</p><p>Most of those things are things that we humans just look at them and say, "Well, that's kind of interesting. "I don't know what the significance of that is." They're very non-human kinds of things. - Yeah. - So, what have we done in AI? What we've done is we've given, for example, a large language model, we've given it, you know, four billion web pages, for example.</p><p>We've given it kind of the specific parts of essentially the computational universe that we humans have selected that we care about. - Yes. - We've shown what we care about. And then what it's doing is to say, "Okay, I know what you humans care about, "so I'm going to make things "that are like what you said you care about." - Yes.</p><p>- And that's a tiny part of the computational universe. - Right. Just like we saw in Caleb's video, his AI said, "This video is me telling you "that all the stuff you've talked about AI "is Terminators and shit-blowing stuff up, "and that's the limit of what we've done." - Right.</p><p>- And it can only construct stuff from the limit of what we've done, recorded, seen, our data sets. - So, I mean, the thing is-- - So, in terms of, so give us an example of something that needs to be computed. A computational exercise, something we gotta figure out, something we wanna solve, outside of what AI is possibly able to solve for today.</p><p>- Well, I mean, any of these computationally irreducible problems, anything where we're asking-- - Example, yeah. So just to connect it for people. - Yeah, yeah, yeah, right. I mean, oh gosh, to pick an area, I mean-- - Without esoteric topology in algebra or something. - Yeah, yeah, right.</p><p>I mean, you know, okay, here's an example. So, you've got a biological system, you've got a good model-- - Great example, yeah, biology. - Okay, so we've got something we're trying to figure out. This collection of cells, it behaves in this way. Is it gonna grow forever and be really bad and make a tumor, or is it going to eventually halt and stop growing?</p><p>Okay, that's a classic kind of computational irreducible type of problem, where, you know, we could, if we knew enough detail, we could simulate what every cell's gonna do. - Every molecule, every atom, every cell, every interaction. If we knew enough, we could simulate each of those steps. - Right, and we could-- - And there's no easy way to solve that, answer that question.</p><p>- Right, you can't jump ahead and say, so I know this thing is never gonna turn into a tumor, for example. - Right, and so, simulating the physical universe, whether you're simulating atoms in a cell, or space-time and discrete space-time or non-discrete space-time itself, becomes this thing where we don't have a simple heuristic, a simple equation that says, based on this condition, this is how things are gonna end up, but you have to actually go through a lot of calculated steps.</p><p>- We haven't known that. I mean, people have hoped that you can just write down a formula for how physics works, and then work out the answer directly from that. That was the big advance. I mean, if you go back to antiquity, people were just trying to sort of reason about how the universe works, sort of philosophically, and then late 1600s, sort of big advance.</p><p>We can write down a mathematical formula, we can use calculus, we can kind of just, essentially, jump ahead and say what's gonna happen in the universe. We can make a prediction, we can say, the comet is gonna be in this place at this time, and so on. - So there's all these hard problems that we can't solve with AI we have today, or can we?</p><p>And can you just help me and help everyone understand, what are you excited about with respect to AI? What is it that has happened in the last couple of months and years that you were excited about, and what does that allow us to do that we couldn't do before using just raw approaches to computation?</p><p>- Okay, so I mean, several different things here. I mean, the first thing to say is, we humans have been interested in a small part of what's computationally possible. AI is reflecting the part that we have been interested in. That sort of AI is doing those kinds of things.</p><p>In terms of what's happened with AI, I mean, the big thing that happened a year ago was the arrival of successful large language models. And what does that tell us? I think that it was a surprise to everybody, including the people who were working on large language models, that we kind of got past, got to this point where they seemed reasonable to us humans, where they were producing text that was reasonable to us humans, and not just completely boring and irrelevant and so on.</p><p>And I think there's this jump that happened now in 2012. There was a sort of previous jump in machine learning that happened with images and things like that, image recognition and so on. So what's the significance of large language models? Well, one question is, why do they work? Why is it possible to make this neural net that can successfully kind of complete an essay or something?</p><p>And I think the answer is that it's kind of telling us a piece of science that in a sense we should be embarrassed we hadn't figured out before. It's a question of sort of how do you construct language? And we've known forever that there's kind of a syntactic grammar of language, you know, noun, verb, noun, et cetera, et cetera, et cetera.</p><p>But what the LLMs are showing us is that there is a kind of semantic grammar of language. There's a way of putting together sentences that could make sense. And, you know, for example, people are always impressed that the LLMs have figured out how to, quote, "reason." And I think what's happening is, you know, logic is this thing that's kind of this formalization of everyday language, and it's a formalization that was discovered, you know, by Aristotle and people in antiquity.</p><p>And in a sense, probably one can think about the way they discovered it is they looked at lots of speeches people had given, and they said, "Which ones make sense?" Okay, there's a certain pattern of how things are said that makes sense. Let's formalize that. That's logic. That's exactly what the LLM has done as well.</p><p>It's noticed that there are these patterns of language that you can kind of use again, and that we call logic or reasoning or something like that. So, you know, I think as a practical matter, the LLMs provide this kind of linguistic user interface. We've had kind of graphical user interfaces and so on.</p><p>Now we have this linguistic user interface. You say, you know, you've got some very small set of points you want to make. You say, "I'm going to feed it to an LLM. It's going to puff it up into a big report. I'm going to send that report to somebody else.</p><p>They're probably going to feed it to their own LLM. It's going to grind it down to some small set of results." It's kind of, you know, it's allowing one to use language as a transport layer. I think that's a, you know, there are a lot of these practical use cases for this.</p><p>-It's always seemed to me like the rate-limiting step in humans is communication. Like, the rate at which you and I are speaking to one another is pretty low bandwidth. Like, just a couple words a minute or something. -The question is, what really is communication? You know, in our brains, there are all these neurons that are firing.</p><p>-Right. -There's 100 billion of them in each of our brains. -And there's a lot of sensory input besides the words that you're saying that are traveling through vibrations in the air to my ear. And that's some information from you. But there's so much more information that the human brain can gather and is building models around all the time, making predictions around whether this light's gonna be on or off or that person's gonna go to the bathroom or sit back down or what have you.</p><p>But it's -- -But, you know, I think one of the things that's sort of interesting is this, you know, we've got stuff in our brains. We are trying to package up those thoughts. You know, the structure of each of our brains is different. So the particular nerve firings are different.</p><p>But we're trying to package up those thoughts in a kind of transportable way. That's what language tends to do. It kind of packages concepts into something transportable. -And that's what these LLMs have done. -Yes. I mean, they're -- -Because they're outputting a packet of Communicate to me. -Yes. Yes.</p><p>I mean, so -- -But is there anything else that's exciting to you from a computational perspective? What else can the AIs do? And what else -- -Oh, okay. We learn a lot from the AIs. -Yes. -You know, they're telling us there is a science of LLMs, which is completely not worked out yet.</p><p>There's kind of a bulk science of knowledge and things that the LLMs are kind of showing us is there. There's a kind of a science of the semantics of language, which LLMs are showing us is there. -Yes. -But we haven't found it yet. -Yes. -It's kind of like we just saw some new piece of nature, and we now get to make science about that kind of nature.</p><p>Now, it's not obvious that we can make sort of science where we can tell a narrative story about what's going on. -Right. -It could be that we're just -- we're just sort of dumped into computational irreducibility, and we just say it does this because -- -So there's this black box that the training model created.</p><p>That black box, we don't know what it does. I put a bunch of words in, a bunch of words come out. It's amazing. Now you're saying we're going to try and understand the nature, the graphs, the nature of that box, and that'll tell us a little bit something about -- -I think we'll discover that, for example, we'll discover that human language is much less -- it's much simpler to describe human language than we had thought.</p><p>-Yeah. -In other words, it's showing us rules of human language that we didn't know. -Yes. -And that's an interesting thing. Now, if you ask, "What else do we learn from the AI?" So I'll give you another example of something I was playing with recently. So, you know, you use image generation, generative AI for making images, as we can now see also making videos and so on.</p><p>There's this question of you go inside the AI, and you say, you know, inside the AI, you know, the concept of a cat is represented by some vector of 1,000 numbers, let's say, the concept of a dog and other 1,000 numbers. You just say, "Let's take these vectors of numbers, and let's just take arbitrary numbers.</p><p>What does the AI think -- What is the thing that corresponds to the sort of arbitrary vector of numbers?" Okay? So you can have these definite concepts, like cat and dog, their particular numbers in this sort of space of all possible concepts. And there's this idea -- I've been calling it inter-concept space.</p><p>What's between kind of the concept of a cat and the concept of a dog? And the answer is there's a huge amount of stuff, even from a generative AI that's learned from us. -Yeah. -It is finding these kind of inter-concept things... -Right. -...that are in between the things for which we have words.</p><p>-Right. -Sort of embarrassing to us that, you know, simple estimate, simple case. If you say what fraction of the space of all possible concepts, so to speak, is now filled with actual words that we have? -Yes. -Of the 50,000 words that are common in English, for example. -50,000. Okay.</p><p>-What, um... You know, what -- Yeah, that's a wrong number. I didn't know that number. That's interesting. Yeah. Yeah, right. If you're an LLM person, that's the -- You know, when it produces the -- When it says, "What's the next word going to be?" 'Cause that's what LLMs are always doing.</p><p>They're just trying to predict the next word. What it's doing is it says, "Here's this list of 50,000 numbers, which are the probabilities for each of the possible 50,000 words in English." And then it'll pick the most likely one or the next most likely one or whatever else. But in any case, the -- You know, this -- When you ask the question, in the space of sort of all possible concepts, how many do we have words for?</p><p>The answer is it's one in 10 to the 600. Wow. -It's like we have -- We have explored a tiny, tiny fraction of even the kind of concepts that are revealed by the things that sort of we put out there on the Web and so on. -That's more than there are atoms in the universe.</p><p>Yeah, there are 10 to the 80th atoms in the universe. Right. 10 to the 80th is very small compared to a lot of the numbers one deals with. But, you know, I think the --  Yeah, it's a -- We should talk about the universe. I want to talk about the universe.</p><p>Yeah, right. -It's pretty cool. So, um... People want to talk about AI. That's why I wanted to just make sure. Yeah, yeah. Please. -We got your point of view. So it's appreciated. What is the nature of the universe? And what do we have wrong? What's that? -And what do we have wrong?</p><p>Yeah, so I mean -- -What does consensus have wrong? Yeah, right. Well, so, I mean, the physics -- This is like where you, like, let the rains go, and then you go. Yeah, so, like, we had a great chat at dinner the other night, so I'm excited. Yeah, go ahead.</p><p>-It's, you know, physics as we know it right now was kind of -- 100 years ago, big advances made in physics, three big theories in physics -- general relativity, the theory of gravity; quantum mechanics, the theory of small kinds of things; and statistical mechanics, the theory of heat; and sort of how in the second law of thermodynamics, law of entropy, increased things like this.</p><p>Okay, so those three big theories that were invented about 100 years ago. Physics has been sort of on a gradual, incremental trajectory since then. I've been interested in trying to understand kind of what could be underneath everything that we see in the world, and I think we figured it out, which is kind of exciting.</p><p>Not something I expected to see in my lifetime, not something I kind of expected to see for 50 or 100, more than that, years. And kind of the number-one thing that you say, "What do people get wrong?" One question is, "What is space?" Well, you know, people just sort of say, "Space is a continuous thing.</p><p>You put things in any different place in space." People have been arguing about whether space is continuous or discrete since antiquity. -Discrete means that it's broken up into little pieces. -Yes, yes. So, you know, this argument also happened for matter. Is matter continuous or discrete? You know, you have water.</p><p>Is it just this continuous fluid, or is it made of discrete kinds of things? That got answered about 120 years ago. The answer is water is made of discrete molecules. Same thing happened for light. Light is made of discrete photons. Space got kind of left out. Space, people still think it's a continuous kind of thing.</p><p>So the kind of starting point of things I've tried to figure out is actually, no, that's not true. Space is discrete. There are atoms of space, so to speak, not like physical atoms like hydrogen and helium and things. They're just these sort of points. -They're slots where things can fit.</p><p>Is that a way to think about it? -They're really just things. Nothing fits in them. I mean, they're just all that one knows about them. They're abstract things. All one knows is that one is different from another. -So there are these two discrete things next to each other. -There's no next.</p><p>-There's no next, right, because there is no space. -We're about to build space. -So there is a graph. Is that a way to think about it? -Yeah. -A relationship between two things. -Right. Well, a relationship between several things. I mean, it's kind of like a giant friend network of the atoms of space.</p><p>-Right. -And that's all there is. That's what our universe is made of. And all the things that we observe-- electrons, black holes, all these kinds of things-- they're all just features of this network. And it's kind of like you might say, how could that possibly be the way things are?</p><p>If you think about something like water, you think about a little eddy in the water. That eddy is a thing where you can say there's a definite eddy that's moving through the water, yet it's made of lots of discrete molecules of water, yet we can identify it as a definite thing.</p><p>And so it is with electrons, et cetera, et cetera. In the universe, it's all features of this giant network. And that's the way it seems to be. -So does each particle, as we know it-- an electron, a proton-- have a feature that defines its relative connectedness to other particles?</p><p>And that definition, that little number, is what we look at as space as a whole? Is that a way to think about it? -No, an electron is a pretty big thing relative to the atoms of space. We don't know exactly how big, but it's a big, floppy thing. The feature-- I mean, right now, it has been assumed that electrons are actually infinitesimally small, but that doesn't seem to be true.</p><p>But the thing that kind of defines something like an electron is kind of like it's sort of a topological kind of thing. It's like you can have a piece of string, and you can either knot it or you can not knot it. And there are lots of different ways you can make the knot, but it's still-- it's either knotted or it's not knotted.</p><p>And there's either an electron or there isn't an electron. So the structure of space, it's kind of much like what happens between molecules and a fluid like water. There are all these discrete atoms of space, and they have these relations to each other. And then if you look at a large scale, what are all these-- and I should say, by the way, that these atoms of space, the main thing that's happening is pieces of this network are getting rewritten.</p><p>So a little piece of network-- This is really important. Yes. I think this is really important because I'm going to ask the follow on question. All right. So what is time? Time is this kind of computational process where this network that represents-- The change through this physical network is time.</p><p>Yes. Yes. So time is a computational phenomenon. Time is the progressive change of the network. One particle touches another one, touches another one. This thing changes this one. This changes this one. Yeah. Sounds to me like you're describing your cellular automata. Yes, except it's very much like that. It's a computational process.</p><p>And it happens to be operating on these hypergraphs rather than just lines of black and white cells. So the universe itself is a computer-- Yes. --running a computational exercise. Yes. I don't know whether you'd call it an exercise, but yes. It is computing. Yes, it is computing. And that's what the progress of time is.</p><p>And time is the progress of that computation. Yes. And that is what people mean when they say, are we living in a simulation? No. No, I think that's a philosophically rather confused thing. I mean, there's a little bit deeper in the rabbit hole that you have to go to understand why that-- We'll do that at lunch.</p><p>--doesn't really make sense. No, go ahead. But no, so I mean, then the question is, just like you have all these molecules bouncing around, they make sort of continuous fluids like water, you can ask, what do all these atoms of space make? Turns out they make Einstein's equations that describe gravity and describe the structure of space-time.</p><p>So that's kind of a neat thing. Yes. Because it's not been imagined that one could derive the properties of something like that from something lower level. I mean, I should say, OK, this is a big, complicated subject. But the thing that-- You guys doing OK? OK, good. The thing that's pretty interesting in all of this is the way that the nature of us as observers is critical to the kind of thing that we observe in-- This is the most important thing I've heard in the last year when I watched your interview a few weeks ago.</p><p>So I just want you to walk everyone through this statement again, because this is so important. Well, all right. So the-- OK. The question is, for example, if we're looking at a bunch of molecules bouncing around in a gas, and we say, what do we see in those molecules?</p><p>Well, we see things like the gas laws, pressure, volume, things like this. We see things like the gas tends to get more random, things like this. That's what we see, because we're observers with certain characteristics. If we were observers who could trace every molecule, do all the computations to figure out what all the molecules would do, we would make quite different conclusions about what happens in gases.</p><p>It's because we are observers who are bounded in our computational capabilities. We're not capable of untangling all those kinds of detail. To see all those atoms, we have to look at the whole can of gas. We can't look at each atom individually. Right. We can't trace the motion of each atom individually.</p><p>So our heuristic is the PV equals NRT, the gas laws. Exactly. We look at that gas, and we say, this is the ratio of temperature, which is the total energy of the system, as opposed to the energy of each individual atom. And there's a lot of different energy states of all these different atoms.</p><p>So we sum everything up. We take an average of a lot of stuff to understand it. Right. And the point is that people haven't understood the fact that we have to take that average, because what's underneath is computationally irreducible, but we-- We can't compute. --are computationally bounded. And it turns out that exact same phenomenon is the origin of space time and gravity, that all these atoms of space operating in this network, it's all this computationally irreducible process.</p><p>But observers like us, observers who are computationally bounded, necessarily observe this kind of sort of aggregate behavior, which turns out to correspond to the structure of space time. Same happens in quantum mechanics. Quantum mechanics is a little bit harder to understand. One of the big features of quantum mechanics is, in classical physics, you say, I throw a ball.</p><p>It follows a definite trajectory. In quantum mechanics, you say, I do something. There are many possible paths of history that can be followed. We only get to work out things about averages of those paths. So it's sort of there are many different paths of history that are being pursued in these networks and things.</p><p>What's happening is that there are many different rewrites to the network that can occur. We get these many branches of history. And so then the question is-- and like a quantum computer is trying to make use of the fact that there are many branches of history that it can kind of follow in parallel.</p><p>But then the question is, well, how do we observe what's actually happening? Well, we are embedded in this whole system that has all these branching paths of history and so on. And we're limited. Well, we, our brains are full of sort of branching behavior and so on. But we are sort of computationally bounded in what we can do.</p><p>We are effectively-- the question you have to ask is, how does sort of the branching brain perceive the branching universe? And as soon as the brain has this sort of computational boundedness in its characteristics, and it also has one other assumption, which is it assumes that it is persistent in time.</p><p>So it's like we are-- at every moment, we are made of different atoms of space. Yet we believe that it's the same us at every successive moment. Right. As soon as we-- So our concept of consciousness precludes us from being able to see perhaps a different nature of the universe, a different-- OK.</p><p>Yes. I told you guys we were going to go like really-- Right. So it's kind of like observers like us who are computationally bounded believe they're persistent in time. The big result is that observers like us inevitably observe laws of physics like the ones we know. And so imagine sort of the alien that isn't like us, that isn't computationally bounded, doesn't believe it's persistent in time.</p><p>It will observe different laws of the universe. And so the laws of the universe are only based on our nature. Yes. But a very-- That's what I thought was so interesting. Right. It's a very coarse feature of our nature. It's not like we have to know every detail of us.</p><p>The laws of physics only require these-- and actually, I suspect that as we-- there are other things that we probably take for granted about the nature of us as observers. Yes. And as we start putting these in, we'll probably actually find more things that are inevitable about the way that physics works.</p><p>So the belief, as I use that term, I kind of feel like I'm deluding myself, because I am not the same Adams as I was a second ago, a second ago, a second ago, a second ago. This belief that we have a self-- talk about your understanding. I know this is a bit far-fetched from maybe your specialty, but I'm sure you have a point of view.</p><p>And then what is this concept of consciousness that we have where we think we're persistent in time, where we have this concept of self-identity? Where does this all arise? And how do you think about this notion of consciousness and the observer in the context of the universe? I'm observing stuff in the universe, and I think I'm a human body, and I'm really-- I'm a bunch of atoms floating around with a bunch of other atoms.</p><p>I used to think there was a sort of hierarchy where consciousness was at the top, but I don't think that anymore. I think consciousness is a very-- it's just like the AIs are not doing all possible computation. We are actually rather limited in our observation of the universe. We're localized in space.</p><p>We have this belief that we're persistent in time and so on. Imagine what you would feel like, so to speak, if you were much more extended in the universe. If you were-- in fact, one of the things that we see in our models is this thing we call the Rouliad, which is this entangled limit of all possible computations.</p><p>And we are-- every mind, in a sense, is just at some small point, some small region in this Rouliad. And so it's-- you imagine what happens if you-- by the way, as we learn more in science, we're effectively expanding in this kind of Roulial space, where it's just like we can send spacecraft out-- Our aggregate consciousness.</p><p>Yes. As a species. Yes. Yes. So just like we can send spacecraft out that explore more of the physical universe, so as we expand our science, as we expand the ideas that we use to describe the universe, we're expanding in this Roulial space. And so you might say, well, what happens if we expand?</p><p>That should be the future of civilization, to expand in Roulial space, to expand our domain of understanding of things. This is a shifting consciousness, like hippie type question, but there's this guy in the UK named Darren Brown. He's a mentalist. He puts these two advertising execs in a room, and he tells them, hey, come up with an ad.</p><p>The name of the company, come up with a logo, come up with a catchphrase. He goes out for a few hours, comes back, pulls off a thing. He copied exactly what they-- he had written down exactly what they were going to do. The way he did it is, as they drove over, he subliminally put a little image in the cab.</p><p>He had some kids walk across the street with a logo. And they just basically were programmed to output what he asked them to do. And they thought that they were creative geniuses. They're like, we're these high-paid ad execs. Look at our genius. Look at what we did. And it always struck me as like the human is just the unconscious computer.</p><p>We're just the node in the neural net that takes the input, gets sensory program output, and we're part of the computational exercise. Is that a way to think about this idea that we are part of this broader computation? And as we do that, this consciousness-- I mean, the person you mentioned, they're cheating computational irreducibility, so to speak.</p><p>They're saying, I'm going to put this thing which is going to be the answer. And that's-- the more interesting way to lead life, in a sense, is just by this process of letting time progress and this irreducible computation occur. And is that what gives you joy? Yeah, I think so.</p><p>I mean, I think it's a funny thing, because when you think you know what's underneath the universe and how all these ideas fit together, and you realize that I'm a person who likes people. And so it seems very bizarre that I should be interested in these things that deconstruct everything about humans.</p><p>And I realized at some point, one of the things about doing science that's one of the more difficult human things about doing science is you have to get rid of your prejudices about what might be true and just follow what the science actually says. And so I've done that for years.</p><p>And I realized, actually, it turns out the thing that I've done puts humans right back in the middle of the picture with these things about the fact that it matters what the observer is like, realizing that in this space of possibilities, that what we care about is this part that is the result of human history and so on.</p><p>You guys asked for more Science Corner, so I hope this fit the bill. Guys, please join me in thanking Stephen Wolfram. Let your winners ride. Rain Man, David Saks. And instead, we open source it to the fans, and they've just gone crazy with it. Love you, SKC. I'm the queen of Kinwam.</p></div></div></body></html>