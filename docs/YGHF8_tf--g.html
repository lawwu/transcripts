<html><head><title>Stanford CS25: V5 I Transformers for Video Generation, Andrew Brown of Meta</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS25: V5 I Transformers for Video Generation, Andrew Brown of Meta</h2><a href="https://www.youtube.com/watch?v=YGHF8_tf--g"><img src="https://i.ytimg.com/vi/YGHF8_tf--g/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./YGHF8_tf--g.html">Whisper Transcript</a> | <a href="./transcript_YGHF8_tf--g.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">I'm very happy to end it off with a bang here with Andrew.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=8" target="_blank">00:00:08.180</a></span> | <span class="t">So he's a research scientist at Meta's Gen AI team, focusing on media generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=14" target="_blank">00:00:14.760</a></span> | <span class="t">Over the past few years, his team has focused on publishing research papers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=18" target="_blank">00:00:18.940</a></span> | <span class="t">that push the frontiers of video generative models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=22" target="_blank">00:00:22.860</a></span> | <span class="t">including emu video as well as movie gen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=27" target="_blank">00:00:27.160</a></span> | <span class="t">Prior to working on Meta, Andrew completed his PhD at Oxford's Visual Geometry Group, VGG,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=32" target="_blank">00:00:32.400</a></span> | <span class="t">under the supervision of Professor Andrew Zisserman.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=36" target="_blank">00:00:36.160</a></span> | <span class="t">So without further ado, I'll hand it off to Andrew to take it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=39" target="_blank">00:00:39.780</a></span> | <span class="t">Yeah, thank you for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=41" target="_blank">00:00:41.160</a></span> | <span class="t">Like the intro said, I'm Andrew Brown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=45" target="_blank">00:00:45.180</a></span> | <span class="t">I'm a researcher in Gen AI at MESA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=48" target="_blank">00:00:48.080</a></span> | <span class="t">If you guys haven't heard of that, Gen AI is the research organization that releases MESA's generative models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=53" target="_blank">00:00:53.960</a></span> | <span class="t">things like LAMA for the text LLMs and our media generation models as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=60" target="_blank">00:01:00.260</a></span> | <span class="t">I've been there for about two and a half years, ever since I finished my PhD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=65" target="_blank">00:01:05.340</a></span> | <span class="t">And like Stephen said, over that period, we've released a bunch of frontier-pushing video generation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=73" target="_blank">00:01:13.700</a></span> | <span class="t">So thank you so much for inviting me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=75" target="_blank">00:01:15.560</a></span> | <span class="t">I'm very honored.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=76" target="_blank">00:01:16.240</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=77" target="_blank">00:01:17.020</a></span> | <span class="t">And yeah, today's talk is Transformers for video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=80" target="_blank">00:01:20.960</a></span> | <span class="t">So I saw in the list of seminar speakers already you've had a bunch on NLP and some media generation stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=87" target="_blank">00:01:27.180</a></span> | <span class="t">I hope this is new compared to what you've heard already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=92" target="_blank">00:01:32.280</a></span> | <span class="t">Okay, so text-to-video models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=94" target="_blank">00:01:34.800</a></span> | <span class="t">You guys might have seen videos like this on the internet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=98" target="_blank">00:01:38.020</a></span> | <span class="t">But given a text prompt, contemporary text-to-video generation models can now create these incredibly high-quality videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=107" target="_blank">00:01:47.920</a></span> | <span class="t">Complex motion, fantastical scenes, very high-quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=112" target="_blank">00:01:52.940</a></span> | <span class="t">These things are amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=114" target="_blank">00:01:54.360</a></span> | <span class="t">Another example I quite like, this is a ghost in a white bedsheet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=122" target="_blank">00:02:02.380</a></span> | <span class="t">These things are amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=124" target="_blank">00:02:04.440</a></span> | <span class="t">Not only are they super high-quality, they seem to have learned some notion of the laws of physics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=129" target="_blank">00:02:09.760</a></span> | <span class="t">The thing I like about this one is if you look closely, you can see the reflection of the ghost is shown in the mirror there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=135" target="_blank">00:02:15.220</a></span> | <span class="t">So these models have clearly learned something very, like I was saying, if you look very closely, you can see this reflection of the ghost in the mirror.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=142" target="_blank">00:02:22.700</a></span> | <span class="t">These things are amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=143" target="_blank">00:02:23.480</a></span> | <span class="t">And it doesn't stop there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=145" target="_blank">00:02:25.460</a></span> | <span class="t">So these text-to-video generation models can be used for other amazing capabilities as well, like editing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=152" target="_blank">00:02:32.200</a></span> | <span class="t">So, for example, you can give these models an input video, like on the top left, and specify some edit prompt, like turn the runner into an inflatable dinosaur or turn it into a cactus desert.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=163" target="_blank">00:02:43.780</a></span> | <span class="t">These things are amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=166" target="_blank">00:02:46.500</a></span> | <span class="t">Now, I don't know how long a lot of you have been paying attention to the media generation field or even the machine learning field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=174" target="_blank">00:02:54.540</a></span> | <span class="t">But if you've been only paying attention for the last year, year and a half, this is all that you're used to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=179" target="_blank">00:02:59.560</a></span> | <span class="t">You're only used to seeing generated videos that are completely indistinguishable from real ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=184" target="_blank">00:03:04.100</a></span> | <span class="t">But this is an incredibly recent development.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=187" target="_blank">00:03:07.780</a></span> | <span class="t">So on the right here, I have the same video I showed you on the first slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=192" target="_blank">00:03:12.460</a></span> | <span class="t">This is from a model that was released in October 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=195" target="_blank">00:03:15.400</a></span> | <span class="t">And on the left, I have what was an amazing states-of-the-art approach from September 2022.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=201" target="_blank">00:03:21.800</a></span> | <span class="t">This is another model that was released by my team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=206" target="_blank">00:03:26.200</a></span> | <span class="t">And I can't stress enough how amazing this was at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=209" target="_blank">00:03:29.140</a></span> | <span class="t">And this gap, you'll see, is two years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=211" target="_blank">00:03:31.500</a></span> | <span class="t">I'm sure every speaker at this seminar has come in and said there's been, like, amazing progress in this machine learning subfield or that one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=220" target="_blank">00:03:40.880</a></span> | <span class="t">And there has been everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=222" target="_blank">00:03:42.500</a></span> | <span class="t">But nowhere is it more clear than video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=226" target="_blank">00:03:46.940</a></span> | <span class="t">So how did this happen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=229" target="_blank">00:03:49.760</a></span> | <span class="t">This is what the subject of today's talk is going to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=236" target="_blank">00:03:56.700</a></span> | <span class="t">How do we train models to generate videos like this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=240" target="_blank">00:04:00.100</a></span> | <span class="t">So all of the videos you saw in the previous slides are from a paper we released in October 2024 called MovieGen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=247" target="_blank">00:04:07.880</a></span> | <span class="t">Here's a little snapshot of the abstract.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=249" target="_blank">00:04:09.760</a></span> | <span class="t">A little spoiler for how we did this and a spoiler for why I was invited to this is that we did it using transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=259" target="_blank">00:04:19.040</a></span> | <span class="t">I saw this little guy included in previous talks, so I had to put him there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=261" target="_blank">00:04:21.640</a></span> | <span class="t">So today's conclusions are going to be twofold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=266" target="_blank">00:04:26.620</a></span> | <span class="t">First, I'm going to talk you through every detail of how you train a transformer to generate videos like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=273" target="_blank">00:04:33.860</a></span> | <span class="t">We publish all of the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=275" target="_blank">00:04:35.820</a></span> | <span class="t">We're going to step through everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=276" target="_blank">00:04:36.960</a></span> | <span class="t">Some of the concepts are going to be familiar, things like transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=280" target="_blank">00:04:40.440</a></span> | <span class="t">Some are going to be new.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=281" target="_blank">00:04:41.680</a></span> | <span class="t">We're going to go through it all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=283" target="_blank">00:04:43.280</a></span> | <span class="t">The second takeaway is going to be this conclusion that I'll keep on saying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=287" target="_blank">00:04:47.700</a></span> | <span class="t">That throughout this project, we learned that scaling data, compute, and model parameters for a simple transformer also works for video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=296" target="_blank">00:04:56.420</a></span> | <span class="t">We've seen it work in all kinds of machine learning fields.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=299" target="_blank">00:04:59.740</a></span> | <span class="t">We also saw it work here for video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=302" target="_blank">00:05:02.040</a></span> | <span class="t">Okay, so just a little bit of personal background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=308" target="_blank">00:05:08.140</a></span> | <span class="t">I think most of the speakers that you've had are from NLP, which makes sense because transformers came from NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=313" target="_blank">00:05:13.460</a></span> | <span class="t">I'm a computer vision researcher, and I've been in visual generation for a few years now, and it hasn't always been as popular as it is now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=323" target="_blank">00:05:23.220</a></span> | <span class="t">So how I got into this, I was sat in this lecture hall in the engineering department at Oxford in, what, the second year of my PhD,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=333" target="_blank">00:05:33.880</a></span> | <span class="t">and we were having a talk by Professor Antonio Tarabla from MIT, and he was presenting this work called GAN dissection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=340" target="_blank">00:05:40.620</a></span> | <span class="t">Some of you may have heard of GANs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=343" target="_blank">00:05:43.220</a></span> | <span class="t">The state-of-the-art image generation approach at the time were these generative adversarial networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=347" target="_blank">00:05:47.860</a></span> | <span class="t">and in 2019, you could generate like a blurry face or a blurry kitchen or a blurry bedroom, and that was amazing at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=359" target="_blank">00:05:59.780</a></span> | <span class="t">And this paper here was showing that you could activate or deactivate certain neurons in the GAN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=365" target="_blank">00:06:05.960</a></span> | <span class="t">and in doing so, you could make certain concepts appear or disappear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=369" target="_blank">00:06:09.820</a></span> | <span class="t">So these are the kind of images that we were generating at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=374" target="_blank">00:06:14.340</a></span> | <span class="t">So this is like a blurry kitchen, and this was like near state-of-the-art.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=379" target="_blank">00:06:19.600</a></span> | <span class="t">So you can see, you know, the visual concepts are pretty messed up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=383" target="_blank">00:06:23.560</a></span> | <span class="t">It doesn't make a huge amount of sense, but I can't stress how, like, good this was at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=387" target="_blank">00:06:27.820</a></span> | <span class="t">I'm sure a lot of you have seen, like, Dolly and stable diffusion and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=392" target="_blank">00:06:32.840</a></span> | <span class="t">Things were not always this good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=395" target="_blank">00:06:35.060</a></span> | <span class="t">So the point of the paper, or at least in this example, was they were showing that you could activate certain neurons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=400" target="_blank">00:06:40.360</a></span> | <span class="t">and make windows appear in the kitchen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=402" target="_blank">00:06:42.680</a></span> | <span class="t">You could imagine how your kitchen would look with windows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=405" target="_blank">00:06:45.460</a></span> | <span class="t">And the outcome was this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=408" target="_blank">00:06:48.060</a></span> | <span class="t">So some, like, pretty janky windows on the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=412" target="_blank">00:06:52.580</a></span> | <span class="t">And my mind was completely blown, as I can sense, like, all of yours are as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=418" target="_blank">00:06:58.620</a></span> | <span class="t">This was amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=421" target="_blank">00:07:01.480</a></span> | <span class="t">So what they showed here is not only that the model had learned a physically plausible place to put the windows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=428" target="_blank">00:07:08.180</a></span> | <span class="t">but importantly, they'd also shown that the model had learned some notion of physics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=432" target="_blank">00:07:12.340</a></span> | <span class="t">The model had learned that if you put windows on the left-hand side of the room,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=436" target="_blank">00:07:16.280</a></span> | <span class="t">light will come through them, and there'll be a reflection of the marble countertop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=440" target="_blank">00:07:20.140</a></span> | <span class="t">And you can see that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=441" target="_blank">00:07:21.580</a></span> | <span class="t">And the model had just learned this by looking at images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=444" target="_blank">00:07:24.960</a></span> | <span class="t">So this had, like, a really profound impact for me as a young PhD student,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=449" target="_blank">00:07:29.840</a></span> | <span class="t">and I've been in visual generation ever since.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=452" target="_blank">00:07:32.920</a></span> | <span class="t">Okay, so the body of the talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=458" target="_blank">00:07:38.240</a></span> | <span class="t">There's going to be five parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=460" target="_blank">00:07:40.100</a></span> | <span class="t">I'll give an overview of the model that we trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=463" target="_blank">00:07:43.600</a></span> | <span class="t">talk about architecture, data, and training recipe results and applications,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=468" target="_blank">00:07:48.300</a></span> | <span class="t">and a little discussion on what is next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=470" target="_blank">00:07:50.720</a></span> | <span class="t">So first, I included a bit of historical context on movie gen and video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=479" target="_blank">00:07:59.720</a></span> | <span class="t">Maybe some of you are quite new to the video generation field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=482" target="_blank">00:08:02.480</a></span> | <span class="t">The field is quite new inherently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=485" target="_blank">00:08:05.020</a></span> | <span class="t">This snapshot is maybe three years, from 2024 back to around 2022.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=491" target="_blank">00:08:11.820</a></span> | <span class="t">This is a century in machine learning research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=496" target="_blank">00:08:16.300</a></span> | <span class="t">I haven't included all of the works here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=498" target="_blank">00:08:18.380</a></span> | <span class="t">There are some very relevant, important works here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=502" target="_blank">00:08:22.120</a></span> | <span class="t">I've just included a snapshot to make a couple of points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=504" target="_blank">00:08:24.460</a></span> | <span class="t">There have been two milestone events in video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=510" target="_blank">00:08:30.360</a></span> | <span class="t">The first was in 2022, when people started using diffusion modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=514" target="_blank">00:08:34.040</a></span> | <span class="t">This is when the whole community started using diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=517" target="_blank">00:08:37.300</a></span> | <span class="t">That was a big step up in visual quality at that point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=521" target="_blank">00:08:41.760</a></span> | <span class="t">The second one was in 2024, and this is the point of today's talk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=526" target="_blank">00:08:46.020</a></span> | <span class="t">Before 2024, people were using quite small-scale, specialized architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=532" target="_blank">00:08:52.220</a></span> | <span class="t">I say small because the definition of small and large has been moving all over the place recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=539" target="_blank">00:08:59.060</a></span> | <span class="t">But these were specialized architectures for computer vision, things like CNNs, UNETs, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=546" target="_blank">00:09:06.480</a></span> | <span class="t">And then around 2024, video generation sort of boarded this architecture unification setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=553" target="_blank">00:09:13.760</a></span> | <span class="t">So all over machine learning fields, we've seen people ditch specialized architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=559" target="_blank">00:09:19.420</a></span> | <span class="t">and move towards this simple transformer setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=561" target="_blank">00:09:21.680</a></span> | <span class="t">The reason is because all of these different fields are seeing the benefits of efficiency and scalability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=568" target="_blank">00:09:28.500</a></span> | <span class="t">by moving to these transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=570" target="_blank">00:09:30.060</a></span> | <span class="t">So in 2024, the video generation community started doing the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=574" target="_blank">00:09:34.780</a></span> | <span class="t">and that's where Movie Jam comes in, which I'll talk about today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=578" target="_blank">00:09:38.660</a></span> | <span class="t">So a quick overview on what Movie Jam is before we get into the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=585" target="_blank">00:09:45.320</a></span> | <span class="t">So Movie Jam was a cast of foundation models that generates high-quality 1080p HD videos</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=591" target="_blank">00:09:51.360</a></span> | <span class="t">with different aspect ratios and synchronized audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=593" target="_blank">00:09:53.740</a></span> | <span class="t">Today, I'm just going to be talking about the text-to-video model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=596" target="_blank">00:09:56.460</a></span> | <span class="t">I'll show some fun examples later of the other models that we trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=602" target="_blank">00:10:02.140</a></span> | <span class="t">And again, like I've been saying, the point of the paper was showing that scaling data, training compute,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=607" target="_blank">00:10:07.900</a></span> | <span class="t">and model parameters for a simple transformer trained with flow matching, I'll cover that later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=613" target="_blank">00:10:13.120</a></span> | <span class="t">yielded state-of-the-art results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=615" target="_blank">00:10:15.240</a></span> | <span class="t">We also presented a few sort of innovations and simplifications along the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=621" target="_blank">00:10:21.060</a></span> | <span class="t">So Movie Jam video, the model, was a 30 billion parameter foundation model for joints, text-to-image, and text-to-video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=630" target="_blank">00:10:30.400</a></span> | <span class="t">The model was trained on the order of around 100 million videos and 1 billion images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=640" target="_blank">00:10:40.020</a></span> | <span class="t">Okay, so on to the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=642" target="_blank">00:10:42.220</a></span> | <span class="t">There are three main things that I want to cover today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=646" target="_blank">00:10:46.600</a></span> | <span class="t">The first is the representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=649" target="_blank">00:10:49.320</a></span> | <span class="t">What representation are we learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=651" target="_blank">00:10:51.380</a></span> | <span class="t">The second is what learning objective that we used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=654" target="_blank">00:10:54.220</a></span> | <span class="t">And the third is what model architecture we used for learning it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=658" target="_blank">00:10:58.280</a></span> | <span class="t">If I do a good job here, then all of you will know all there is to know about video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=662" target="_blank">00:11:02.840</a></span> | <span class="t">I'm imagining most of you have more of a NLP background with sort of text-to-aggressive models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=670" target="_blank">00:11:10.700</a></span> | <span class="t">I'm going to try and contextualize all of this in relation to text and how it differs from large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=681" target="_blank">00:11:21.620</a></span> | <span class="t">Okay, so the representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=683" target="_blank">00:11:23.600</a></span> | <span class="t">What do we mean by that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=685" target="_blank">00:11:25.600</a></span> | <span class="t">Well, we mean how are we going to represent the data for the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=688" target="_blank">00:11:28.940</a></span> | <span class="t">We're doing generative modeling here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=691" target="_blank">00:11:31.140</a></span> | <span class="t">We're learning P of X.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=692" target="_blank">00:11:32.200</a></span> | <span class="t">The question here is what should X be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=694" target="_blank">00:11:34.580</a></span> | <span class="t">We know that X is going to be derived in some way from videos, but there's an open question of how exactly do we do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=701" target="_blank">00:11:41.220</a></span> | <span class="t">So to motivate what we ended up doing, I'm just going to talk a bit about the differences between text and media.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=711" target="_blank">00:11:51.540</a></span> | <span class="t">So let's look at some text data, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=715" target="_blank">00:11:55.220</a></span> | <span class="t">And this piece of data, a sentence, an image of a cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=719" target="_blank">00:11:59.500</a></span> | <span class="t">What has happened when this data sample has been created is a human has put in a huge amount of thought into compressing what they're thinking into this very well-designed, semantically rich language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=735" target="_blank">00:12:15.620</a></span> | <span class="t">It's very compressed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=737" target="_blank">00:12:17.620</a></span> | <span class="t">Every word packs a huge amount of information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=740" target="_blank">00:12:20.380</a></span> | <span class="t">And it's also inherently discrete.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=742" target="_blank">00:12:22.280</a></span> | <span class="t">So in practice, when people are training large language models, they can use a representation that's quite close to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=748" target="_blank">00:12:28.320</a></span> | <span class="t">So they might use a sort of simple tokenizer before feeding it into the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=753" target="_blank">00:12:33.620</a></span> | <span class="t">Media data is incredibly different to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=758" target="_blank">00:12:38.380</a></span> | <span class="t">So let's look at a related piece of data, an actual image of a cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=762" target="_blank">00:12:42.340</a></span> | <span class="t">No compression has really occurred here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=766" target="_blank">00:12:46.620</a></span> | <span class="t">This image is just continuous raw data that's been captured by your camera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=770" target="_blank">00:12:50.580</a></span> | <span class="t">Maybe the only sort of human effort that's gone into this is positioning the camera and framing the cat in the middle of the camera, but nothing else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=780" target="_blank">00:13:00.880</a></span> | <span class="t">As a result of this, there's tons of redundancy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=783" target="_blank">00:13:03.080</a></span> | <span class="t">So what I mean by this is if you know what a cat is, and then let's say the middle pixel you know is this white fur on the cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=793" target="_blank">00:13:13.340</a></span> | <span class="t">Well, if you know what a cat is, you know the next pixel along is probably going to be white fur and the next one along from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=799" target="_blank">00:13:19.500</a></span> | <span class="t">And if it's a video, you know the same pixels one frame along are also going to be white cat fur.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=805" target="_blank">00:13:25.140</a></span> | <span class="t">There's a huge amount of redundancy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=806" target="_blank">00:13:26.760</a></span> | <span class="t">So this begs the question of perhaps this raw data could be transformed into something that more closely resembles language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=814" target="_blank">00:13:34.980</a></span> | <span class="t">So what do we actually do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=819" target="_blank">00:13:39.400</a></span> | <span class="t">So if you forget everything that I just said on the previous slide, one thing you could do, a very simple approach, is you just model the pixels directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=828" target="_blank">00:13:48.000</a></span> | <span class="t">So let's say you took an image or a video, you unraveled it into a long sequence of pixels, and then trained, let's say, next token prediction on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=836" target="_blank">00:13:56.860</a></span> | <span class="t">And in a way, that's what some prior works did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=839" target="_blank">00:13:59.800</a></span> | <span class="t">Things like imagine video or image GPT back in the day, along with a little bit of patching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=845" target="_blank">00:14:05.240</a></span> | <span class="t">So this is a very sort of conceptually simple method, but it's very computationally constraining.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=854" target="_blank">00:14:14.520</a></span> | <span class="t">So the thing about modeling pixels directly is the number of pixels scales quadrastically with the image or video resolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=862" target="_blank">00:14:22.440</a></span> | <span class="t">It's bad for images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=864" target="_blank">00:14:24.540</a></span> | <span class="t">It's even worse for video when you have a temporal dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=867" target="_blank">00:14:27.860</a></span> | <span class="t">In practice, what this means is these models can only model very low-resolution images or videos, things like 64 by 64 videos, which is not ideal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=877" target="_blank">00:14:37.760</a></span> | <span class="t">And if we want to generate an actual large HD video, these methods had to employ a huge cascade of upsampling stages, things like super-resolution models, frame interpolation models, to increase the size of the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=892" target="_blank">00:14:52.400</a></span> | <span class="t">So this really isn't ideal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=894" target="_blank">00:14:54.400</a></span> | <span class="t">So instead, what prior work does is they learn a compressed latent representation using something like a VAE or a VQVAE trained offline.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=907" target="_blank">00:15:07.120</a></span> | <span class="t">This is what every sort of text-to-image model or text-to-video model you've seen on social media has been doing for a long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=913" target="_blank">00:15:13.460</a></span> | <span class="t">The advantages here are twofold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=916" target="_blank">00:15:16.140</a></span> | <span class="t">If you're modeling a compressed version of your data, then you can natively model larger data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=922" target="_blank">00:15:22.760</a></span> | <span class="t">We don't need to go down to 64 by 64.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=924" target="_blank">00:15:24.960</a></span> | <span class="t">We can natively model something bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=927" target="_blank">00:15:27.140</a></span> | <span class="t">The other advantage is that this offline-trained VAE or VQVAE can remove some of the computational burden from the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=935" target="_blank">00:15:35.560</a></span> | <span class="t">For example, these autoencoders could handle the modeling of how two separate blades of grass differ from each other in an image or a video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=944" target="_blank">00:15:44.920</a></span> | <span class="t">And it can take that burden off the sort of downstream language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=950" target="_blank">00:15:50.760</a></span> | <span class="t">Okay, so that is what we do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=953" target="_blank">00:15:53.600</a></span> | <span class="t">From an architecture, we train something called a temporal autoencoder for spatial-temporal video compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=962" target="_blank">00:16:02.640</a></span> | <span class="t">This is basically just a variational autoencoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=966" target="_blank">00:16:06.100</a></span> | <span class="t">How does this work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=968" target="_blank">00:16:08.520</a></span> | <span class="t">Well, you take a video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=970" target="_blank">00:16:10.320</a></span> | <span class="t">You'll feed it through the TAE encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=973" target="_blank">00:16:13.360</a></span> | <span class="t">A VAE consists of an encoder and a decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=976" target="_blank">00:16:16.600</a></span> | <span class="t">After the encoder, the representation will be compressed, and you end up with this latent representation at the bottleneck in the middle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=984" target="_blank">00:16:24.300</a></span> | <span class="t">And this is the representation that we're going to use downstream.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=988" target="_blank">00:16:28.440</a></span> | <span class="t">How you train these things is quite simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=992" target="_blank">00:16:32.060</a></span> | <span class="t">Some of you may have seen it before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=993" target="_blank">00:16:33.220</a></span> | <span class="t">You take your video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=994" target="_blank">00:16:34.020</a></span> | <span class="t">You feed it through the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=995" target="_blank">00:16:35.140</a></span> | <span class="t">You get to the bottleneck latent representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=997" target="_blank">00:16:37.920</a></span> | <span class="t">You then decode it back to pixel space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1000" target="_blank">00:16:40.760</a></span> | <span class="t">And you have a bunch of losses between the output and the input, things like L1 losses, adversarial losses, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1008" target="_blank">00:16:48.260</a></span> | <span class="t">So this representation in the middle is what we're going to model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1012" target="_blank">00:16:52.540</a></span> | <span class="t">When we talk about learning P of X, we're learning the distribution of this latent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1017" target="_blank">00:16:57.920</a></span> | <span class="t">What this means is when we train a generative model on this, it generates in this space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1023" target="_blank">00:17:03.280</a></span> | <span class="t">It doesn't generate in RGB space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1025" target="_blank">00:17:05.300</a></span> | <span class="t">So after we generate a video, we then need to decode it back to RGB space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1030" target="_blank">00:17:10.140</a></span> | <span class="t">So the TAE that we trained had 8x compression in each dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1038" target="_blank">00:17:18.580</a></span> | <span class="t">8x in height, width, and time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1041" target="_blank">00:17:21.280</a></span> | <span class="t">And this was pretty high compression at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1047" target="_blank">00:17:27.040</a></span> | <span class="t">It's not the highest compression anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1049" target="_blank">00:17:29.140</a></span> | <span class="t">This was published like six months ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1051" target="_blank">00:17:31.600</a></span> | <span class="t">And six months is, again, like a decade in machine learning research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1055" target="_blank">00:17:35.340</a></span> | <span class="t">But at the time, this was very high compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1056" target="_blank">00:17:36.900</a></span> | <span class="t">And like I said, this means that we can natively model very high-resolution videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1062" target="_blank">00:17:42.580</a></span> | <span class="t">As an example, the largest video that we model in this work is 768x768 pixels, 16 seconds, 16 FPS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1074" target="_blank">00:17:54.080</a></span> | <span class="t">Now, if we were to model pixels directly, and we took a video of that size, we unraveled everything, we treated one pixel as one token, and we flattened the whole thing, it would result in 150 million tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1086" target="_blank">00:18:06.980</a></span> | <span class="t">Even with very long context training methods with language models at the moment, this is completely unfeasible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1095" target="_blank">00:18:15.460</a></span> | <span class="t">But using this temporal autoencoder, the same video is just compressed to 73,000 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1102" target="_blank">00:18:22.680</a></span> | <span class="t">So this is suddenly completely computationally feasible using like today's parallelism approaches, today's infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1112" target="_blank">00:18:32.220</a></span> | <span class="t">If anyone is doing this math offline, there is also a patch of high layer, if someone thinks my math is wrong here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1118" target="_blank">00:18:38.660</a></span> | <span class="t">Okay, so that's everything for the representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1124" target="_blank">00:18:44.180</a></span> | <span class="t">Does anyone have any questions at this point?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1126" target="_blank">00:18:46.780</a></span> | <span class="t">No.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1134" target="_blank">00:18:54.960</a></span> | <span class="t">It's a great point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1136" target="_blank">00:18:56.180</a></span> | <span class="t">A lot of autoencoders for videos do use causality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1142" target="_blank">00:19:02.260</a></span> | <span class="t">Some nice outcomes of that is that when you encode images, they can be encoded completely independently of subsequent frames and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1154" target="_blank">00:19:14.700</a></span> | <span class="t">But no, this isn't causal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1158" target="_blank">00:19:18.960</a></span> | <span class="t">Okay, so next up is which generative modeling learning objective do we use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1164" target="_blank">00:19:24.020</a></span> | <span class="t">So probably in most of the talks that you've had so far, you've heard about autoregression and next token prediction for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1170" target="_blank">00:19:30.720</a></span> | <span class="t">In media generation, we haven't been doing that for a couple of years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1176" target="_blank">00:19:36.620</a></span> | <span class="t">So the de facto approach in most media generation has been using diffusion modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1181" target="_blank">00:19:41.320</a></span> | <span class="t">We use something called flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1188" target="_blank">00:19:48.620</a></span> | <span class="t">So what is flow matching?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1190" target="_blank">00:19:50.940</a></span> | <span class="t">Flow matching is, in a way, a simpler generalization of diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1197" target="_blank">00:19:57.580</a></span> | <span class="t">Now, if any of you have watched a talk on diffusion modeling or read anything about diffusion or flow matching, I'm sure you've seen a figure like this before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1207" target="_blank">00:20:07.440</a></span> | <span class="t">I'm going to give a brief sort of explainer of what is similar between flow matching and diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1213" target="_blank">00:20:13.380</a></span> | <span class="t">And then we're going to go over what a training step looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1217" target="_blank">00:20:17.660</a></span> | <span class="t">So both diffusion and flow matching have a very similar setup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1222" target="_blank">00:20:22.360</a></span> | <span class="t">So you assume that you have some unknown data distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1226" target="_blank">00:20:26.280</a></span> | <span class="t">This is the sort of distribution of images in this figure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1229" target="_blank">00:20:29.000</a></span> | <span class="t">In this case, like images of cats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1233" target="_blank">00:20:33.040</a></span> | <span class="t">This is the distribution you're trying to learn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1234" target="_blank">00:20:34.780</a></span> | <span class="t">This is the distribution we want to learn and then sample from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1237" target="_blank">00:20:37.700</a></span> | <span class="t">You also assume you have a known data distribution on the right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1242" target="_blank">00:20:42.820</a></span> | <span class="t">And we model this as normally just like Gaussian noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1248" target="_blank">00:20:48.240</a></span> | <span class="t">Both assume then that we have this fixed forward process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1251" target="_blank">00:20:51.720</a></span> | <span class="t">What this means is we have a method of translating between the unknown data distribution and the known one by iteratively adding noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1260" target="_blank">00:21:00.500</a></span> | <span class="t">Both assume that if you keep on adding noise, you basically end up at this known data distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1268" target="_blank">00:21:08.400</a></span> | <span class="t">Then both of them train a neural network to do the reverse process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1272" target="_blank">00:21:12.380</a></span> | <span class="t">So they train a neural network to take one of these images and iteratively denoise those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1277" target="_blank">00:21:17.740</a></span> | <span class="t">And then at inference time, you can iteratively use this neural network to go from a sample that's pure noise back to a sample from this data distribution that you've just learned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1287" target="_blank">00:21:27.600</a></span> | <span class="t">And that's how we end up sampling images and videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1290" target="_blank">00:21:30.880</a></span> | <span class="t">So diffusion and flow matching are very similar in a lot of ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1295" target="_blank">00:21:35.760</a></span> | <span class="t">Flow matching is, in a way, a simpler generalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1299" target="_blank">00:21:39.040</a></span> | <span class="t">It's been very recently shown to result in more robust training and more efficient probability paths that are easier and faster to sample from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1307" target="_blank">00:21:47.920</a></span> | <span class="t">So this paper came out pretty recently in 2023 from some colleagues at Massa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1316" target="_blank">00:21:56.820</a></span> | <span class="t">Importantly, it's been shown to work better than diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1319" target="_blank">00:21:59.380</a></span> | <span class="t">And I'm not going to go into a huge amount of detail here, but we're going to go over how a training step looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1327" target="_blank">00:22:07.920</a></span> | <span class="t">So there are lots of equations here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1331" target="_blank">00:22:11.520</a></span> | <span class="t">We're going to step through them pretty easily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1332" target="_blank">00:22:12.940</a></span> | <span class="t">It's a three-step process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1335" target="_blank">00:22:15.520</a></span> | <span class="t">We first take a training data sample, X1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1339" target="_blank">00:22:19.600</a></span> | <span class="t">This is your image of a cat on the previous slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1343" target="_blank">00:22:23.100</a></span> | <span class="t">This is just an image from your data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1345" target="_blank">00:22:25.780</a></span> | <span class="t">We then sample a time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1347" target="_blank">00:22:27.480</a></span> | <span class="t">This is a float between zero and one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1349" target="_blank">00:22:29.340</a></span> | <span class="t">And we sample from the known data distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1351" target="_blank">00:22:31.940</a></span> | <span class="t">This just means taking a sample from a normal Gaussian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1355" target="_blank">00:22:35.140</a></span> | <span class="t">We then construct a training sample, Xt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1359" target="_blank">00:22:39.680</a></span> | <span class="t">What is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1361" target="_blank">00:22:41.140</a></span> | <span class="t">This is just this sort of intermediate image, a somewhat noise image of a cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1367" target="_blank">00:22:47.440</a></span> | <span class="t">There are lots of different ways of constructing Xt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1370" target="_blank">00:22:50.440</a></span> | <span class="t">We use what's called the simple linear interpolation from the flow matching paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1375" target="_blank">00:22:55.460</a></span> | <span class="t">And the equation is shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1376" target="_blank">00:22:56.900</a></span> | <span class="t">So this is how we go from the three things we sampled above to this intermediate training sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1382" target="_blank">00:23:02.460</a></span> | <span class="t">Then in flow matching, what you do is you train the model to predict the velocity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1388" target="_blank">00:23:08.220</a></span> | <span class="t">This is a value which moves the training sample back in the direction of the data sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1394" target="_blank">00:23:14.680</a></span> | <span class="t">In actuality, it's very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1396" target="_blank">00:23:16.160</a></span> | <span class="t">So this is how we compute the velocity, simply by differentiating the equation above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1401" target="_blank">00:23:21.100</a></span> | <span class="t">This is how we get our ground truth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1403" target="_blank">00:23:23.060</a></span> | <span class="t">And then on the right, we have our actual learning objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1406" target="_blank">00:23:26.180</a></span> | <span class="t">So this is the mean squared error between the model prediction and the ground truth velocity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1411" target="_blank">00:23:31.300</a></span> | <span class="t">Here, the model prediction is parametrized by U.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1414" target="_blank">00:23:34.960</a></span> | <span class="t">It takes as input the training sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1417" target="_blank">00:23:37.900</a></span> | <span class="t">It's conditioned on two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1419" target="_blank">00:23:39.980</a></span> | <span class="t">It's conditioned on the text prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1422" target="_blank">00:23:42.340</a></span> | <span class="t">That's P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1422" target="_blank">00:23:42.880</a></span> | <span class="t">Remember, we're doing text to video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1424" target="_blank">00:23:44.840</a></span> | <span class="t">So we need to condition the generation on the text prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1427" target="_blank">00:23:47.520</a></span> | <span class="t">I'll cover later how we do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1429" target="_blank">00:23:49.400</a></span> | <span class="t">You condition on the time step as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1432" target="_blank">00:23:52.100</a></span> | <span class="t">And then theta are the model parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1434" target="_blank">00:23:54.580</a></span> | <span class="t">Okay, you know how to do flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1441" target="_blank">00:24:01.220</a></span> | <span class="t">Inference is also pretty simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1443" target="_blank">00:24:03.080</a></span> | <span class="t">So you start by sampling from this known data distribution, Gaussian noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1447" target="_blank">00:24:07.020</a></span> | <span class="t">And then we use an ordinary differential equation solver to go back to the data distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1453" target="_blank">00:24:13.180</a></span> | <span class="t">given a series of time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1455" target="_blank">00:24:15.240</a></span> | <span class="t">So very simply, you'll sample some noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1458" target="_blank">00:24:18.160</a></span> | <span class="t">You'll sample a stream of time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1459" target="_blank">00:24:19.900</a></span> | <span class="t">At each time step, you compute the model's prediction for the velocity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1463" target="_blank">00:24:23.660</a></span> | <span class="t">and use the solver to move the sample in the direction of the unknown data distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1469" target="_blank">00:24:29.820</a></span> | <span class="t">And at the end of that, you have your sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1472" target="_blank">00:24:32.660</a></span> | <span class="t">Low-level detail, we use a quite simple solver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1478" target="_blank">00:24:38.420</a></span> | <span class="t">There are lots of different options you can choose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1483" target="_blank">00:24:43.620</a></span> | <span class="t">Okay, so lastly, which model architecture do we use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1489" target="_blank">00:24:49.960</a></span> | <span class="t">Now, I already said that we're using transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1494" target="_blank">00:24:54.400</a></span> | <span class="t">The big goal of this paper was to benefit from, like I've said about seven times already,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1501" target="_blank">00:25:01.080</a></span> | <span class="t">scaling data, training compute, and model parameters with transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1505" target="_blank">00:25:05.760</a></span> | <span class="t">But there is a question about which transformer to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1507" target="_blank">00:25:07.920</a></span> | <span class="t">So, in my research organization, we train these things called LAMAs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1514" target="_blank">00:25:14.380</a></span> | <span class="t">LAMA is the large language model that messes open sources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1519" target="_blank">00:25:19.480</a></span> | <span class="t">We take the LAMA3 model, hence why I sort of pasted this L3 on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1525" target="_blank">00:25:25.840</a></span> | <span class="t">LAMA3 is quite a classic, dense, fully connected, decoder-only language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1534" target="_blank">00:25:34.480</a></span> | <span class="t">So, what we did in MovieGen is you take your videos, you encode them with the TAE that we discussed earlier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1540" target="_blank">00:25:40.840</a></span> | <span class="t">you flatten the tokens, this gives you your input sequence, and we just throw it into LAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1546" target="_blank">00:25:46.180</a></span> | <span class="t">So, very, very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1551" target="_blank">00:25:51.980</a></span> | <span class="t">Now, when I say LAMA, I don't mean a pre-trained LAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1556" target="_blank">00:25:56.400</a></span> | <span class="t">I don't mean one that's been trained for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1559" target="_blank">00:25:59.320</a></span> | <span class="t">What I mean is the architecture, so a randomly initialized architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1564" target="_blank">00:26:04.740</a></span> | <span class="t">But this is still very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1567" target="_blank">00:26:07.320</a></span> | <span class="t">Training large language models at scale is incredibly difficult.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1572" target="_blank">00:26:12.180</a></span> | <span class="t">Every time you change anything about the architecture, you need different hyper-premises, they scale differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1578" target="_blank">00:26:18.860</a></span> | <span class="t">It's incredibly tricky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1581" target="_blank">00:26:21.320</a></span> | <span class="t">So, the fact that we already know, in our research organization, how to scale this architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1587" target="_blank">00:26:27.340</a></span> | <span class="t">and the fact that we have the infra set up already to train these things at scale, makes a huge difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1592" target="_blank">00:26:32.440</a></span> | <span class="t">So, that's why the simplest thing for us to do was to go with the LAMA architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1596" target="_blank">00:26:36.780</a></span> | <span class="t">We didn't do that for this project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1606" target="_blank">00:26:46.640</a></span> | <span class="t">But I agree, that would be a really cool thing to try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1610" target="_blank">00:26:50.660</a></span> | <span class="t">Yeah, it's not entirely clear why that would work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1620" target="_blank">00:27:00.040</a></span> | <span class="t">Obviously, like, these are very different modalities, different learning objectives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1624" target="_blank">00:27:04.400</a></span> | <span class="t">But, you know, in a lot of ways, there's a lot of sort of shared structure between these modalities that would maybe benefit from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1637" target="_blank">00:27:17.640</a></span> | <span class="t">Okay, so that last slide was, like, very deceptively oversimplified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1643" target="_blank">00:27:23.220</a></span> | <span class="t">There are some changes that we needed to make to LAMA 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1645" target="_blank">00:27:25.720</a></span> | <span class="t">So, importantly, LAMA 3 is a model for auto-aggressive text generation, and we are doing text-to-video generation using flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1653" target="_blank">00:27:33.320</a></span> | <span class="t">So, there are three changes that we need to make.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1655" target="_blank">00:27:35.020</a></span> | <span class="t">I'm going to go over all of them exhaustively to sort of hammer the point that we barely changed the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1663" target="_blank">00:27:43.720</a></span> | <span class="t">So, the first thing that we need to do is incorporate the text conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1666" target="_blank">00:27:46.560</a></span> | <span class="t">You'll have seen on the previous slide that our input sequence is just made up of video tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1671" target="_blank">00:27:51.000</a></span> | <span class="t">We're doing text-to-video generation, so we need to incorporate the text conditioning somehow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1676" target="_blank">00:27:56.620</a></span> | <span class="t">And we do this using cross-attention layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1680" target="_blank">00:28:00.760</a></span> | <span class="t">So, very simply, we construct a sequence made up of our text conditioning, and we add cross-attention layers into the transformer block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1689" target="_blank">00:28:09.200</a></span> | <span class="t">So, these go between the self-attention layers and the feedforward network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1692" target="_blank">00:28:12.640</a></span> | <span class="t">It's a very common way of adding text conditioning for media generation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1697" target="_blank">00:28:17.920</a></span> | <span class="t">There is a question, as well, of what should your text representation be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1705" target="_blank">00:28:25.120</a></span> | <span class="t">How should you construct this sequence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1707" target="_blank">00:28:27.000</a></span> | <span class="t">One, like, very simple thing to do would be you just tokenize the prompt, the caption, and then you feed that in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1715" target="_blank">00:28:35.620</a></span> | <span class="t">But when you do that, you're very much burdening your model with learning this text representation from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1721" target="_blank">00:28:41.140</a></span> | <span class="t">So, instead, we use pre-trained text representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1724" target="_blank">00:28:44.220</a></span> | <span class="t">We use three, in fact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1727" target="_blank">00:28:47.760</a></span> | <span class="t">Three that are complementary to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1730" target="_blank">00:28:50.120</a></span> | <span class="t">The first two, all of them are pre-trained frozen text models, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1734" target="_blank">00:28:54.500</a></span> | <span class="t">The first two have very sort of semantic level representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1738" target="_blank">00:28:58.520</a></span> | <span class="t">UL2 is a large-scale encoder-decoder model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1741" target="_blank">00:29:01.620</a></span> | <span class="t">Metaclip is our internal clip model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1744" target="_blank">00:29:04.940</a></span> | <span class="t">And the third one has more of a sort of character-level text representation by T5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1750" target="_blank">00:29:10.720</a></span> | <span class="t">So, we encode the text prompt using all three of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1755" target="_blank">00:29:15.940</a></span> | <span class="t">We project them all to the model dimension, and we concatenate, and that gives us our text sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1765" target="_blank">00:29:25.860</a></span> | <span class="t">The second thing we need to do, you'll remember from the learning objective, that we also need to condition on the time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1773" target="_blank">00:29:33.780</a></span> | <span class="t">So, what we do here is we do this in adaptive layer norm blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1776" target="_blank">00:29:36.900</a></span> | <span class="t">So, we've already added a cross-attention block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1779" target="_blank">00:29:39.300</a></span> | <span class="t">We also add this adaptive layer norm block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1781" target="_blank">00:29:41.180</a></span> | <span class="t">This might seem like quite a strange way of adding some conditioning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1786" target="_blank">00:29:46.780</a></span> | <span class="t">It's something that was popularized in the diffusion transformer paper, which was the first paper that used diffusion with transformers, obviously, by the name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1798" target="_blank">00:29:58.060</a></span> | <span class="t">It might seem a little bit random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1799" target="_blank">00:29:59.500</a></span> | <span class="t">It's basically very computationally cheap, and works super well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1802" target="_blank">00:30:02.660</a></span> | <span class="t">Okay, number three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1806" target="_blank">00:30:06.220</a></span> | <span class="t">There are only three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1808" target="_blank">00:30:08.960</a></span> | <span class="t">We use full bidirectional attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1810" target="_blank">00:30:10.900</a></span> | <span class="t">LLAMA is an auto-aggressive text model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1814" target="_blank">00:30:14.920</a></span> | <span class="t">They use causal masking for next token prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1817" target="_blank">00:30:17.180</a></span> | <span class="t">For the flow matching objective, we have no such constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1821" target="_blank">00:30:21.220</a></span> | <span class="t">We want every video token to see every other video token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1824" target="_blank">00:30:24.260</a></span> | <span class="t">We don't care about causal masking, so we take that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1827" target="_blank">00:30:27.440</a></span> | <span class="t">A very low-level detail is that because of this, we use multi-head attention instead of groups query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1834" target="_blank">00:30:34.080</a></span> | <span class="t">But that's everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1835" target="_blank">00:30:35.360</a></span> | <span class="t">Other than that, it's the LLAMA architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1837" target="_blank">00:30:37.000</a></span> | <span class="t">Okay, so we now have our full architecture diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1844" target="_blank">00:30:44.740</a></span> | <span class="t">I was very wary of putting this earlier because it's pretty complicated, but I think it should all make sense at this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1850" target="_blank">00:30:50.680</a></span> | <span class="t">So from left to right, during training, we take one of our training videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1856" target="_blank">00:30:56.780</a></span> | <span class="t">We encode it with the TAE to get to our compressed latent representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1860" target="_blank">00:31:00.900</a></span> | <span class="t">It goes through a small Petrify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1862" target="_blank">00:31:02.780</a></span> | <span class="t">This just does some extra compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1865" target="_blank">00:31:05.520</a></span> | <span class="t">And we flatten it, and that's how we get our input sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1869" target="_blank">00:31:09.040</a></span> | <span class="t">During training, we'll construct our training sample by combining it with this Gaussian noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1875" target="_blank">00:31:15.840</a></span> | <span class="t">During inference, this whole sequence will start off being Gaussian noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1880" target="_blank">00:31:20.880</a></span> | <span class="t">The sequence goes through these LLAMA transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1884" target="_blank">00:31:24.480</a></span> | <span class="t">We add in the conditioning, and then we get our output sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1889" target="_blank">00:31:29.680</a></span> | <span class="t">During training, we'd compute our loss and backpropagate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1892" target="_blank">00:31:32.960</a></span> | <span class="t">During inference, we would do this iterative denoising process, and finally decode back to the RGB space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1900" target="_blank">00:31:40.000</a></span> | <span class="t">Any questions on the architecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1905" target="_blank">00:31:45.680</a></span> | <span class="t">Any questions on the architecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1910" target="_blank">00:31:50.880</a></span> | <span class="t">So, at inference for a given text prompt, we add this conditioning into the model always.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1928" target="_blank">00:32:08.620</a></span> | <span class="t">So the input sequence is just Gaussian noise, but that's not where the text information comes in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1933" target="_blank">00:32:13.020</a></span> | <span class="t">The text information comes in through these cross-attention layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1936" target="_blank">00:32:16.400</a></span> | <span class="t">So even though the input sequence is just noise, the model is still seeing these clean text information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1942" target="_blank">00:32:22.380</a></span> | <span class="t">We don't noise the text or anything like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1944" target="_blank">00:32:24.300</a></span> | <span class="t">Does that make sense?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1946" target="_blank">00:32:26.620</a></span> | <span class="t">That's just a hyperbrameter that we keep constant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1975" target="_blank">00:32:55.460</a></span> | <span class="t">I cannot remember off the top of my head exactly what that hyperparameter is for, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1981" target="_blank">00:33:01.620</a></span> | <span class="t">I'm going to talk to you about that after.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1983" target="_blank">00:33:03.840</a></span> | <span class="t">The next question is, how many of you know these steps?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1986" target="_blank">00:33:06.640</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1987" target="_blank">00:33:07.660</a></span> | <span class="t">Is it typically required?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1988" target="_blank">00:33:08.920</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1989" target="_blank">00:33:09.160</a></span> | <span class="t">So, it's a very good point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1993" target="_blank">00:33:13.360</a></span> | <span class="t">So, during inference, you'll sample a series of time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=1996" target="_blank">00:33:16.280</a></span> | <span class="t">Usually, with these flow-based models, the more you sample, the better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2000" target="_blank">00:33:20.440</a></span> | <span class="t">You'll better approximate these probability paths.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2002" target="_blank">00:33:22.680</a></span> | <span class="t">In practice, we use 250, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2006" target="_blank">00:33:26.000</a></span> | <span class="t">One of the advantages of flow matching is that the probability paths are theoretically straighter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2011" target="_blank">00:33:31.380</a></span> | <span class="t">So, you should be able to require less function evaluations to approximate this path.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2017" target="_blank">00:33:37.420</a></span> | <span class="t">That's one of the advantages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2018" target="_blank">00:33:38.920</a></span> | <span class="t">And is it 250, like, so, is it not a pre-described number of Ts during training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2029" target="_blank">00:33:49.180</a></span> | <span class="t">Yeah, that's continuous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2029" target="_blank">00:33:49.960</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2030" target="_blank">00:33:50.840</a></span> | <span class="t">But during inference, it's, we just sample some discrete ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2034" target="_blank">00:33:54.660</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2035" target="_blank">00:33:55.820</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2037" target="_blank">00:33:57.080</a></span> | <span class="t">So, I have a question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2038" target="_blank">00:33:58.560</a></span> | <span class="t">On the two videos that you show the difference between 2022 and 2024, was the improvement just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2045" target="_blank">00:34:05.380</a></span> | <span class="t">algorithmic what you used with the hardware in terms of TPUs, or it was just a pure algorithm?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2050" target="_blank">00:34:10.780</a></span> | <span class="t">So, I think everything comes down to scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2057" target="_blank">00:34:17.080</a></span> | <span class="t">There are quite a lot of things that have changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2060" target="_blank">00:34:20.720</a></span> | <span class="t">There's an architecture change moving towards transformers, and then there's the scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2065" target="_blank">00:34:25.000</a></span> | <span class="t">And that includes a whole load of things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2067" target="_blank">00:34:27.360</a></span> | <span class="t">Scaling the data, scaling the amount of compute, in order for that to be, like, tractable, better, like, GPU hardware, it does help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2079" target="_blank">00:34:39.440</a></span> | <span class="t">There wasn't a huge improvement in GPU hardware over those two years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2082" target="_blank">00:34:42.260</a></span> | <span class="t">It might have been one or two generations of NVIDIA's stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2086" target="_blank">00:34:46.860</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2088" target="_blank">00:34:48.140</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2091" target="_blank">00:34:51.220</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2095" target="_blank">00:34:55.840</a></span> | <span class="t">So, now we have an architecture that we're pretty confident scales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2099" target="_blank">00:34:59.760</a></span> | <span class="t">Longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2100" target="_blank">00:35:00.580</a></span> | <span class="t">We have a learning objective that we think should work in flow matching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2104" target="_blank">00:35:04.640</a></span> | <span class="t">That's pretty much it for the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2107" target="_blank">00:35:07.280</a></span> | <span class="t">It's not the entire story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2108" target="_blank">00:35:08.700</a></span> | <span class="t">But that's a lot of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2110" target="_blank">00:35:10.680</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2114" target="_blank">00:35:14.260</a></span> | <span class="t">So, the last technical details are about data and the training recipe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2119" target="_blank">00:35:19.660</a></span> | <span class="t">So, data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2122" target="_blank">00:35:22.520</a></span> | <span class="t">I think in a lot of ways, this is the most important slide of the entire talk today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2129" target="_blank">00:35:29.260</a></span> | <span class="t">Data is so important for training large language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2133" target="_blank">00:35:33.820</a></span> | <span class="t">And, by the way, when I say large language models, I'm sort of just talking about transformers at scale for any modality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2138" target="_blank">00:35:38.540</a></span> | <span class="t">People use, like, different definitions there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2143" target="_blank">00:35:43.780</a></span> | <span class="t">But these models are incredibly data-hungry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2147" target="_blank">00:35:47.260</a></span> | <span class="t">They require internet-scale data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2149" target="_blank">00:35:49.100</a></span> | <span class="t">And they require the data to be clean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2151" target="_blank">00:35:51.680</a></span> | <span class="t">The scaling laws depend on the data being clean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2155" target="_blank">00:35:55.400</a></span> | <span class="t">Otherwise, the scaling laws don't hold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2158" target="_blank">00:35:58.040</a></span> | <span class="t">And the model output quality depends on the data being clean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2161" target="_blank">00:36:01.060</a></span> | <span class="t">As a result of this, research groups at these big companies spend a huge amount of resources on data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2168" target="_blank">00:36:08.800</a></span> | <span class="t">This is something that I find isn't really talked about so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2172" target="_blank">00:36:12.380</a></span> | <span class="t">But they'll spend huge amounts of resources in terms of GPUs and also actual researchers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2176" target="_blank">00:36:16.760</a></span> | <span class="t">Often, on these research teams, the data teams massively outnumber the modeling teams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2182" target="_blank">00:36:22.880</a></span> | <span class="t">Which was something very new to me after my little PhD.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2190" target="_blank">00:36:30.620</a></span> | <span class="t">So, why is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2191" target="_blank">00:36:31.640</a></span> | <span class="t">Well, you know, remember we're training generative models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2195" target="_blank">00:36:35.540</a></span> | <span class="t">We are learning this distribution of our training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2199" target="_blank">00:36:39.840</a></span> | <span class="t">And then we're sampling videos from it that are likely according to that training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2203" target="_blank">00:36:43.960</a></span> | <span class="t">So, if we want to sample the kinds of videos that I showed you on the first two slides,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2208" target="_blank">00:36:48.720</a></span> | <span class="t">then all of our training data needs to look like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2214" target="_blank">00:36:54.900</a></span> | <span class="t">I'm mainly talking about pre-training here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2216" target="_blank">00:36:56.660</a></span> | <span class="t">You obviously have a post-training phase as well where you can align your videos to be more high-quality perhaps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2223" target="_blank">00:37:03.060</a></span> | <span class="t">But your pre-training data still needs to look great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2225" target="_blank">00:37:05.440</a></span> | <span class="t">So, this is a huge challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2229" target="_blank">00:37:09.500</a></span> | <span class="t">We trained the model on the order of around 100 million videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2234" target="_blank">00:37:14.020</a></span> | <span class="t">How do we get to this number?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2236" target="_blank">00:37:16.480</a></span> | <span class="t">Well, we can predict a sort of training budget that we had for this project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2240" target="_blank">00:37:20.920</a></span> | <span class="t">And you want as many videos as you can that you don't epoch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2244" target="_blank">00:37:24.720</a></span> | <span class="t">Thank you very much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2245" target="_blank">00:37:25.360</a></span> | <span class="t">So, the challenge was how to get this many videos that are high enough quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2249" target="_blank">00:37:29.800</a></span> | <span class="t">At the time of MovieGen, we constructed this incredibly detailed complex pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2256" target="_blank">00:37:36.140</a></span> | <span class="t">with a bunch of handcrafted and model-based filters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2260" target="_blank">00:37:40.900</a></span> | <span class="t">I'll just talk through a few bits of it because of the sheer amount of work that went into this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2267" target="_blank">00:37:47.500</a></span> | <span class="t">So, you start with a large pool of videos from some corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2273" target="_blank">00:37:53.640</a></span> | <span class="t">They may be different lengths, long tail of concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2277" target="_blank">00:37:57.040</a></span> | <span class="t">We did a bunch of visual filtering on these, removing videos that are too small, scene changes, bad aesthetics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2284" target="_blank">00:38:04.940</a></span> | <span class="t">We removed a bunch of videos that have bad motion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2287" target="_blank">00:38:07.880</a></span> | <span class="t">It turns out a bunch of videos in any large corpus have really slow motion, janky motion, motion effects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2295" target="_blank">00:38:15.840</a></span> | <span class="t">We removed all of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2298" target="_blank">00:38:18.080</a></span> | <span class="t">We then did a content filtering step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2303" target="_blank">00:38:23.580</a></span> | <span class="t">This is first deduplication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2305" target="_blank">00:38:25.160</a></span> | <span class="t">But the really important one here is resampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2307" target="_blank">00:38:27.500</a></span> | <span class="t">Large language models do not work well when they're trained on a very imbalanced data set in terms of concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2315" target="_blank">00:38:35.120</a></span> | <span class="t">So, something with a very long tail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2317" target="_blank">00:38:37.140</a></span> | <span class="t">They work best if the concept distribution is roughly, roughly uniform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2321" target="_blank">00:38:41.480</a></span> | <span class="t">And that uniformity doesn't occur if you just take a random set of videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2327" target="_blank">00:38:47.320</a></span> | <span class="t">So, we do this very sort of complex visual concept extraction, clustering, the upweights, certain clusters, downweights, certain clusters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2336" target="_blank">00:38:56.280</a></span> | <span class="t">This, all of this will give us a set of videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2340" target="_blank">00:39:00.420</a></span> | <span class="t">We also need captions because we're doing text-to-video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2344" target="_blank">00:39:04.320</a></span> | <span class="t">And we generate these automatically using Lama 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2347" target="_blank">00:39:07.900</a></span> | <span class="t">So, that is data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2351" target="_blank">00:39:11.300</a></span> | <span class="t">Very lastly, the training recipe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2355" target="_blank">00:39:15.360</a></span> | <span class="t">So, this multi-stage recipe here was optimized for conversion speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2361" target="_blank">00:39:21.180</a></span> | <span class="t">We start off with a 256-pixel T2i stage image generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2366" target="_blank">00:39:26.360</a></span> | <span class="t">Here, the model can whip through a bunch of samples in very few, relatively, GPU hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2373" target="_blank">00:39:33.440</a></span> | <span class="t">We then move on to a pre-training stage with joint text-to-image and text-to-video generation where we progressively increase the resolution from 256p to 768p.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2385" target="_blank">00:39:45.140</a></span> | <span class="t">At the highest resolution here, 768p, that's where we have the sequence length of 73k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2392" target="_blank">00:39:52.700</a></span> | <span class="t">And we train this on 6,000 GPUs, around 1,500 batch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2397" target="_blank">00:39:57.300</a></span> | <span class="t">At this point, the model splinters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2401" target="_blank">00:40:01.100</a></span> | <span class="t">So, we have a text-to-video post-training stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2404" target="_blank">00:40:04.160</a></span> | <span class="t">This is just SFT on, like, a very small set of very high-quality videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2409" target="_blank">00:40:09.420</a></span> | <span class="t">And then we also branch off into these different capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2412" target="_blank">00:40:12.080</a></span> | <span class="t">I'm not going to talk about those a bunch, but I'll show you some examples later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2415" target="_blank">00:40:15.740</a></span> | <span class="t">Any questions, Owen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2418" target="_blank">00:40:18.840</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2420" target="_blank">00:40:20.140</a></span> | <span class="t">For, like, sort of a wider-tailed ASF for brain research videos, is there sort of, is there free training you can do specifically to address those types of challenges?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2430" target="_blank">00:40:30.860</a></span> | <span class="t">Yeah, I think, definitely, I guess the question is about certain concepts in your pre-training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2440" target="_blank">00:40:40.640</a></span> | <span class="t">So, I guess, yeah, in a very large corpus like we'll be training on, there is an incredibly long tail of concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2446" target="_blank">00:40:46.720</a></span> | <span class="t">Pretty much every concept that you might want to generate would probably appear at some point in that data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2452" target="_blank">00:40:52.860</a></span> | <span class="t">Okay, anything else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2459" target="_blank">00:40:59.940</a></span> | <span class="t">Okay, so, lastly, the results and applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2464" target="_blank">00:41:04.040</a></span> | <span class="t">So, putting everything together that I just showed you, and that's all of the technical details, by the way, like, we published all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2469" target="_blank">00:41:09.920</a></span> | <span class="t">We get videos like this, and like the ones that I showed at the beginning of the talk as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2476" target="_blank">00:41:16.340</a></span> | <span class="t">And this pretty much sums up the whole point of this project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2482" target="_blank">00:41:22.080</a></span> | <span class="t">I really want to hammer home that these kinds of videos were nowhere near possible before people started scaling transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2489" target="_blank">00:41:29.740</a></span> | <span class="t">And we showed here that, again, this classic architecture unification story, scaling data, model parameters, and compute for a simple transformer, ended up in a model that can reason about objects, motion, and physics just by watching videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2507" target="_blank">00:41:47.840</a></span> | <span class="t">So, none of this was possible, and it's scaling transformers that unlocks all of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2512" target="_blank">00:41:52.400</a></span> | <span class="t">I have to keep on pressing replay, because I couldn't figure out how to auto-replay on Google Slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2520" target="_blank">00:42:00.120</a></span> | <span class="t">How many PhDs does it take to auto-replay?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2524" target="_blank">00:42:04.500</a></span> | <span class="t">Okay, so, another one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2526" target="_blank">00:42:06.120</a></span> | <span class="t">Here's a sloth with pink sunglasses on a donut float.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2529" target="_blank">00:42:09.300</a></span> | <span class="t">Again, I think the physics of sloth-like donut flotation here are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2539" target="_blank">00:42:19.060</a></span> | <span class="t">This slide isn't only a joke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2540" target="_blank">00:42:20.840</a></span> | <span class="t">It does actually highlight something quite important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2544" target="_blank">00:42:24.400</a></span> | <span class="t">When we're training generative models, one important thing is how well it generalizes to concepts that may be one in its pre-training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2552" target="_blank">00:42:32.320</a></span> | <span class="t">We can't be sure that there are no, like, slots and floats in the pre-training data, but, you know, by common sense, there probably aren't too many.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2561" target="_blank">00:42:41.060</a></span> | <span class="t">So, the fact that it can generate this is testament to its generalizing capabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2566" target="_blank">00:42:46.440</a></span> | <span class="t">Okay, a few other things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2573" target="_blank">00:42:53.020</a></span> | <span class="t">I showed you some of these at the beginning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2574" target="_blank">00:42:54.300</a></span> | <span class="t">We also trained a MovieGen edit model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2579" target="_blank">00:42:59.140</a></span> | <span class="t">Like I said before, you take the original video on the top left, and you can provide these precise edit instructions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2584" target="_blank">00:43:04.900</a></span> | <span class="t">This is really magic to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2589" target="_blank">00:43:09.080</a></span> | <span class="t">I didn't get a chance to cover this too much today, but the team trained this with paired data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2595" target="_blank">00:43:15.580</a></span> | <span class="t">It's incredibly hard to get paired data for this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2599" target="_blank">00:43:19.560</a></span> | <span class="t">These pairs of input and outputs, they came up with, like, a fascinating self-supervised approach for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2604" target="_blank">00:43:24.180</a></span> | <span class="t">I'd really recommend reading the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2605" target="_blank">00:43:25.760</a></span> | <span class="t">Hey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2606" target="_blank">00:43:26.280</a></span> | <span class="t">Where did you guys get the data from on this project?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2610" target="_blank">00:43:30.020</a></span> | <span class="t">The videos are completely licensed videos by matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2615" target="_blank">00:43:35.020</a></span> | <span class="t">Five minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2617" target="_blank">00:43:37.100</a></span> | <span class="t">And one more example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2629" target="_blank">00:43:49.320</a></span> | <span class="t">You can take, you know, everyone wants to take a video of their penguin and put some Victorian outfits on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2635" target="_blank">00:43:55.100</a></span> | <span class="t">So, this is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2641" target="_blank">00:44:01.680</a></span> | <span class="t">One other model that we trained is the personalization model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2646" target="_blank">00:44:06.280</a></span> | <span class="t">So, here, this is MovieGen video, but with the added capability that you can condition on an image of yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2654" target="_blank">00:44:14.300</a></span> | <span class="t">So, here, the model can generate a video that is faithful to this text prompt, but also contains the person in the conditioning image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2661" target="_blank">00:44:21.840</a></span> | <span class="t">So, that's really fun.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2665" target="_blank">00:44:25.820</a></span> | <span class="t">And another funny example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2667" target="_blank">00:44:27.780</a></span> | <span class="t">This is my colleague that I worked on the project with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2670" target="_blank">00:44:30.080</a></span> | <span class="t">This makes me laugh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2671" target="_blank">00:44:31.480</a></span> | <span class="t">So, yeah, if anyone is interested in this, please go read the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2675" target="_blank">00:44:35.720</a></span> | <span class="t">A bunch of work went into this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2679" target="_blank">00:44:39.200</a></span> | <span class="t">Very lastly, we're going to see now if the theater has audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2682" target="_blank">00:44:42.160</a></span> | <span class="t">It doesn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2685" target="_blank">00:44:45.520</a></span> | <span class="t">So, we trained this MovieGen audio model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2689" target="_blank">00:44:49.200</a></span> | <span class="t">Again, I didn't have time to go into this today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2692" target="_blank">00:44:52.080</a></span> | <span class="t">But this is a model that will condition on text and a video, either real or generated, and generate synchronized audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2699" target="_blank">00:44:59.340</a></span> | <span class="t">So, this way, we can add audio to our generated videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2702" target="_blank">00:45:02.380</a></span> | <span class="t">It's an amazing research team that did this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2705" target="_blank">00:45:05.360</a></span> | <span class="t">So, I'd recommend going to check out the paper if you're interested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2707" target="_blank">00:45:07.960</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2710" target="_blank">00:45:10.200</a></span> | <span class="t">So, there are a couple of just points left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2713" target="_blank">00:45:13.220</a></span> | <span class="t">Firstly, some quantitative results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2715" target="_blank">00:45:15.160</a></span> | <span class="t">It's incredibly hard to do fair comparisons in video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2721" target="_blank">00:45:21.280</a></span> | <span class="t">We don't have automated metrics or anything like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2725" target="_blank">00:45:25.780</a></span> | <span class="t">So, what we did was we did a very extensive human evaluation study.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2730" target="_blank">00:45:30.400</a></span> | <span class="t">We came up with a bunch of metrics that are somewhat orthogonal to each other that test every aspect of video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2737" target="_blank">00:45:37.220</a></span> | <span class="t">Things like motion quality, how well the videos follow the text prompts, visual quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2743" target="_blank">00:45:43.240</a></span> | <span class="t">These metrics are shown on the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2746" target="_blank">00:45:46.680</a></span> | <span class="t">I won't go into a huge bunch of detail here, but they're very sort of fully defined in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2751" target="_blank">00:45:51.960</a></span> | <span class="t">We put in a load of work to make sure that the human evaluators had sort of low standard deviation when multiple evaluators were rating the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2761" target="_blank">00:46:01.120</a></span> | <span class="t">So, we compared to all of the methods that were released in the same year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2766" target="_blank">00:46:06.760</a></span> | <span class="t">This is 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2767" target="_blank">00:46:07.540</a></span> | <span class="t">So, things like runways, models, Luma, Sora at the time, and Kling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2775" target="_blank">00:46:15.640</a></span> | <span class="t">And these are net win rates you're seeing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2778" target="_blank">00:46:18.780</a></span> | <span class="t">So, a score above zero means that our model was preferred.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2781" target="_blank">00:46:21.600</a></span> | <span class="t">At the time of release, Movie Jam outperformed all of the prior work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2786" target="_blank">00:46:26.640</a></span> | <span class="t">So, this is, that's great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2788" target="_blank">00:46:28.060</a></span> | <span class="t">It's very hard to draw conclusions here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2793" target="_blank">00:46:33.220</a></span> | <span class="t">The one conclusion we can draw is that Movie Jam was better than these at the time of release.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2800" target="_blank">00:46:40.580</a></span> | <span class="t">We are, like researchers, what we like to do at this point is to look into all of the technical reports of the prior work and see what they did differently and what did Movie Jam do differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2813" target="_blank">00:46:53.120</a></span> | <span class="t">Because we'd like to conclude, right, what led to these improvements?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2816" target="_blank">00:46:56.640</a></span> | <span class="t">Was it more compute?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2817" target="_blank">00:46:57.700</a></span> | <span class="t">Was it flow matching?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2819" target="_blank">00:46:59.000</a></span> | <span class="t">Was it better data filtration?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2821" target="_blank">00:47:01.700</a></span> | <span class="t">Things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2824" target="_blank">00:47:04.020</a></span> | <span class="t">But it's unfortunately not possible in today's age to do that because we don't see research publications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2830" target="_blank">00:47:10.160</a></span> | <span class="t">But what we do know is that all of the technical details I just presented work, and they work really, really well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2836" target="_blank">00:47:16.480</a></span> | <span class="t">And they're a good starting point for anyone who's looking to improve text-to-video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2841" target="_blank">00:47:21.700</a></span> | <span class="t">And we hope the community does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2844" target="_blank">00:47:24.740</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2846" target="_blank">00:47:26.740</a></span> | <span class="t">The very last technical thing, scaling laws.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2849" target="_blank">00:47:29.660</a></span> | <span class="t">I've talked throughout the whole talk today about architecture unification across modalities and learning objectives, and this was a really nice result that we found at the end of the project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2864" target="_blank">00:47:44.040</a></span> | <span class="t">So what you're seeing here is a scaling law graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2866" target="_blank">00:47:46.460</a></span> | <span class="t">How we use these when we're training large language models is often when you start training like a, you know, a GPT or a LAMA or whatever, you have a certain training budget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2881" target="_blank">00:48:01.620</a></span> | <span class="t">This is how many months you have and how many GPUs you have for that period.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2886" target="_blank">00:48:06.540</a></span> | <span class="t">But if you know that training compute budget, an open question is how big your model should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2893" target="_blank">00:48:13.480</a></span> | <span class="t">What should be the optimal model size for that compute budget?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2895" target="_blank">00:48:15.920</a></span> | <span class="t">It could be a very small model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2897" target="_blank">00:48:17.060</a></span> | <span class="t">You train for more, you train for like more restorations or a larger model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2902" target="_blank">00:48:22.800</a></span> | <span class="t">You train for less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2903" target="_blank">00:48:23.680</a></span> | <span class="t">These scaling law curves are for estimating the optimal model size for a given compute budget.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2909" target="_blank">00:48:29.320</a></span> | <span class="t">So just looking at the blue crosses, we plotted a few of these data points for a movie gen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2915" target="_blank">00:48:35.740</a></span> | <span class="t">Remember movie gen, text-to-video model, LAMA 3 architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2919" target="_blank">00:48:39.500</a></span> | <span class="t">And looking at the blue crosses, we can see this nice correlation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2923" target="_blank">00:48:43.160</a></span> | <span class="t">We often see in scaling laws with transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2925" target="_blank">00:48:45.400</a></span> | <span class="t">We then overlaid the LAMA 3 scaling law.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2929" target="_blank">00:48:49.020</a></span> | <span class="t">So this is the scaling law for the text model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2931" target="_blank">00:48:51.080</a></span> | <span class="t">And amazingly, we see that the LAMA 3 scaling law for this text-only model serves as a reasonable predictor for model size and compute for video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2944" target="_blank">00:49:04.480</a></span> | <span class="t">And this seems to hint that scaling laws for transformers are maybe modality independent, which is pretty fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2952" target="_blank">00:49:12.320</a></span> | <span class="t">Okay, so we're on the last part now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2958" target="_blank">00:49:18.920</a></span> | <span class="t">What is next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2963" target="_blank">00:49:23.080</a></span> | <span class="t">Movie gen did not solve video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2965" target="_blank">00:49:25.880</a></span> | <span class="t">There are still lots of problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2971" target="_blank">00:49:31.380</a></span> | <span class="t">The model will struggle with generating complex motions from complex prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2977" target="_blank">00:49:37.720</a></span> | <span class="t">An example here is a dramatic scene of two cars colliding at an intersection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2983" target="_blank">00:49:43.580</a></span> | <span class="t">So it's looking pretty good at this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2985" target="_blank">00:49:45.960</a></span> | <span class="t">And then at some point near the end, they sort of, I don't know what you'd call that, they sort of independently implode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=2993" target="_blank">00:49:53.660</a></span> | <span class="t">At one point, the silver car kind of turns into two cars as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3000" target="_blank">00:50:00.040</a></span> | <span class="t">So this is a random generation from our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3002" target="_blank">00:50:02.200</a></span> | <span class="t">So, you know, text-to-video generation is not solved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3005" target="_blank">00:50:05.240</a></span> | <span class="t">So my final slide is some ideas about where I think video generation is going next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3017" target="_blank">00:50:17.240</a></span> | <span class="t">Things that I'm sure we'll see some version of at some point soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3021" target="_blank">00:50:21.500</a></span> | <span class="t">So what is next?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3024" target="_blank">00:50:24.140</a></span> | <span class="t">How can we solve the issues that you saw in the previous slides?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3029" target="_blank">00:50:29.520</a></span> | <span class="t">Well, the first kind of obvious one is scaling everything more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3035" target="_blank">00:50:35.540</a></span> | <span class="t">This has been like the story of machine learning for the last six years or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3041" target="_blank">00:50:41.920</a></span> | <span class="t">And I think this would like definitely work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3044" target="_blank">00:50:44.600</a></span> | <span class="t">Movie gen was a 30 billion parameter model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3047" target="_blank">00:50:47.120</a></span> | <span class="t">It was based on Lama 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3048" target="_blank">00:50:48.600</a></span> | <span class="t">The largest Lama 3 model was 405b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3050" target="_blank">00:50:50.660</a></span> | <span class="t">I think scaling everything more would definitely result in far higher quality generations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3059" target="_blank">00:50:59.000</a></span> | <span class="t">Some challenges there would be around scaling data by like an order of magnitude.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3063" target="_blank">00:51:03.960</a></span> | <span class="t">Number two is reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3068" target="_blank">00:51:08.660</a></span> | <span class="t">We've all seen the amazing benefits in language modeling that have come from reasoning in the last year or two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3081" target="_blank">00:51:21.020</a></span> | <span class="t">Here, the reasoning gives the model the ability to sort of pause, think, generate a chain of thoughts, self-correct before generating an answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3094" target="_blank">00:51:34.080</a></span> | <span class="t">I think it's very clear that video generation models would benefit from this kind of reasoning capability as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3099" target="_blank">00:51:39.920</a></span> | <span class="t">I think that could result in a step change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3102" target="_blank">00:51:42.460</a></span> | <span class="t">When all of us were looking at the videos on the previous slides, it was very clear, right, that something was wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3107" target="_blank">00:51:47.520</a></span> | <span class="t">It looked like very obvious to us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3111" target="_blank">00:51:51.200</a></span> | <span class="t">It doesn't seem too unfathomable to be able to imbue some video generation model with the capability to self-correct, maybe see that there are some errors in the video that it generates and correct it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3128" target="_blank">00:52:08.940</a></span> | <span class="t">There are lots of really interesting research questions here, like what does it mean to generate a reasoning trace for media generation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3136" target="_blank">00:52:16.000</a></span> | <span class="t">You might have seen these like chain of thoughts that are generated by like R1 or R3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3142" target="_blank">00:52:22.860</a></span> | <span class="t">I wonder what that looks like for media generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3145" target="_blank">00:52:25.040</a></span> | <span class="t">The other issue is how to verify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3148" target="_blank">00:52:28.260</a></span> | <span class="t">So, these latest state-of-the-art reasoning approaches are all trained with RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3155" target="_blank">00:52:35.200</a></span> | <span class="t">RL requires verification models to verify the correctness of the outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3160" target="_blank">00:52:40.440</a></span> | <span class="t">It's an open research question what that means for video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3164" target="_blank">00:52:44.820</a></span> | <span class="t">How do you verify the correctness of a video?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3169" target="_blank">00:52:49.700</a></span> | <span class="t">Very lastly, we have native generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3172" target="_blank">00:52:52.920</a></span> | <span class="t">So, recent large language models are natively multimodal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3178" target="_blank">00:52:58.600</a></span> | <span class="t">They can generate text, they can do image understanding, they can do video understanding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3183" target="_blank">00:53:03.740</a></span> | <span class="t">Some can even do image generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3185" target="_blank">00:53:05.900</a></span> | <span class="t">So, it's an interesting question as to whether video generation would also benefit from being thrown into this native mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3196" target="_blank">00:53:16.320</a></span> | <span class="t">And if so, there are interesting questions around how you would train such a thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3202" target="_blank">00:53:22.900</a></span> | <span class="t">I've just talked you through how flow matching seems to work the best for video generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3206" target="_blank">00:53:26.840</a></span> | <span class="t">Is there a way that you can have multiple learning objectives?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3210" target="_blank">00:53:30.280</a></span> | <span class="t">Do you need to unify the learning objectives?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3212" target="_blank">00:53:32.160</a></span> | <span class="t">Things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3216" target="_blank">00:53:36.240</a></span> | <span class="t">So, very lastly, it was a huge team that worked on MovieGen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3222" target="_blank">00:53:42.060</a></span> | <span class="t">These are a bunch of amazing researchers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3226" target="_blank">00:53:46.420</a></span> | <span class="t">I had so much fun on this project, learned so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3228" target="_blank">00:53:48.660</a></span> | <span class="t">There are loads of good friends here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3230" target="_blank">00:53:50.220</a></span> | <span class="t">So, yeah, shout-outs to all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3236" target="_blank">00:53:56.480</a></span> | <span class="t">So, yeah, I'm going to leave it on this slide, but we can do questions now, if there are any.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3255" target="_blank">00:54:15.200</a></span> | <span class="t">And a lot of these types of architectures are unit-based, the KOMNet kind of structures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3260" target="_blank">00:54:20.140</a></span> | <span class="t">And that, like, so the action-and-diffusion policy, surprisingly, like, the KOMNet architecture outperforms the transformer architecture for action-and-diffusion for robotics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3269" target="_blank">00:54:29.300</a></span> | <span class="t">And they make an argument that it has an inductive bias about smoothing, so that there's more consistency in the spatial and the time domain and all that great stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3281" target="_blank">00:54:41.680</a></span> | <span class="t">And, you know, what do you see that for that piece?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3283" target="_blank">00:54:43.040</a></span> | <span class="t">Yeah, so I haven't read that particular paper, but it does sound like a familiar point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3291" target="_blank">00:54:51.820</a></span> | <span class="t">I guess the, you know, the whole theory around architecture unification is that we might have these specialized architecture, like CNNs, and they hold these inductive biases around visual data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3309" target="_blank">00:55:09.620</a></span> | <span class="t">Like, they prioritize these local interactions with the convolutional mask and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3314" target="_blank">00:55:14.620</a></span> | <span class="t">And the going idea has been that when you're training at small scale, having these inductive biases helps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3321" target="_blank">00:55:21.980</a></span> | <span class="t">But when you're training at large scale and you have enough data, you can sort of learn all of these yourself with a transformer in a less constrained setting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3333" target="_blank">00:55:33.180</a></span> | <span class="t">We have found that scaling transformers here works better than scaling these specialized architectures, for a few reasons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3341" target="_blank">00:55:41.720</a></span> | <span class="t">I mean, it's not even to say that I don't think CNNs could scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3346" target="_blank">00:55:46.680</a></span> | <span class="t">It seems as though it's easier to scale transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3350" target="_blank">00:55:50.620</a></span> | <span class="t">It's more straightforward to know in which direction to scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3353" target="_blank">00:55:53.820</a></span> | <span class="t">All the infrastructure already exists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3357" target="_blank">00:55:57.780</a></span> | <span class="t">But yeah, it's a very big debate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3359" target="_blank">00:55:59.200</a></span> | <span class="t">I'm not, I'm not, like, calling it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3360" target="_blank">00:56:00.760</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3361" target="_blank">00:56:01.820</a></span> | <span class="t">Hey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3363" target="_blank">00:56:03.520</a></span> | <span class="t">Can this model and approach be used for 3D generation to, like, video games and stuff like that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3369" target="_blank">00:56:09.120</a></span> | <span class="t">So, if you go back to the architecture, it's totally modality independent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3381" target="_blank">00:56:21.300</a></span> | <span class="t">So, for any, this is for videos and images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3387" target="_blank">00:56:27.220</a></span> | <span class="t">The important thing here is that we've turned a modality into a sequence of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3393" target="_blank">00:56:33.520</a></span> | <span class="t">At this point, after that, everything that's happening in the architecture is modality independent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3398" target="_blank">00:56:38.920</a></span> | <span class="t">So, really for any new kind of data, all you need is a way of turning it into a sequence of tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3407" target="_blank">00:56:47.940</a></span> | <span class="t">It may be more challenging to encode, like, that kind of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3414" target="_blank">00:56:54.700</a></span> | <span class="t">But as long as you can encode it in some way to a series of tokens, you can use the exact same approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3420" target="_blank">00:57:00.040</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3420" target="_blank">00:57:00.720</a></span> | <span class="t">We have a lot of questions online as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3426" target="_blank">00:57:06.940</a></span> | <span class="t">So, we can, I can read a few of those right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3430" target="_blank">00:57:10.940</a></span> | <span class="t">So, one is, so, 16 seconds seems like the longest videos you can effectively generate right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3435" target="_blank">00:57:15.660</a></span> | <span class="t">What are the main obstacles or things keeping us from getting to longer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3439" target="_blank">00:57:19.920</a></span> | <span class="t">For example, real movie length generations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3443" target="_blank">00:57:23.240</a></span> | <span class="t">So, the main issue is a computational one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3449" target="_blank">00:57:29.620</a></span> | <span class="t">Well, there is so many different answers to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3452" target="_blank">00:57:32.660</a></span> | <span class="t">If you're thinking from the movie gen setup, the issue is sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3457" target="_blank">00:57:37.380</a></span> | <span class="t">Given the level of compression that we had, 73K was pretty much the longest sequence length we could feasibly train at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3466" target="_blank">00:57:46.420</a></span> | <span class="t">If you want to train on 32-second videos, that's going to double.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3471" target="_blank">00:57:51.900</a></span> | <span class="t">There are multiple ways around this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3473" target="_blank">00:57:53.260</a></span> | <span class="t">You could train an encoder with far more compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3476" target="_blank">00:57:56.680</a></span> | <span class="t">And then you could get to videos that long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3479" target="_blank">00:57:59.620</a></span> | <span class="t">There's another question about learning objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3484" target="_blank">00:58:04.880</a></span> | <span class="t">Here, we generate everything at once.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3487" target="_blank">00:58:07.980</a></span> | <span class="t">There are lots of papers out there that will iteratively generate videos along the temporal axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3493" target="_blank">00:58:13.780</a></span> | <span class="t">You will generate a chunk and then generate a new chunk conditioned on the previous trunk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3498" target="_blank">00:58:18.360</a></span> | <span class="t">It's kind of like next token prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3500" target="_blank">00:58:20.060</a></span> | <span class="t">So, I don't think there's necessarily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3503" target="_blank">00:58:23.900</a></span> | <span class="t">There are lots of ways around it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3506" target="_blank">00:58:26.060</a></span> | <span class="t">And there are lots of interesting papers that generate sort of infinite length videos using these like iterative processes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3511" target="_blank">00:58:31.260</a></span> | <span class="t">Let me see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3517" target="_blank">00:58:37.560</a></span> | <span class="t">There's some questions on like synthetic data compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3521" target="_blank">00:58:41.500</a></span> | <span class="t">So, someone says, you know, meta is a rare example of having an abundance of training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3527" target="_blank">00:58:47.680</a></span> | <span class="t">What if, you know, you run out of video, then what's next, I guess?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3531" target="_blank">00:58:51.260</a></span> | <span class="t">I really know how to answer that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3537" target="_blank">00:58:57.540</a></span> | <span class="t">If we run out of video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3541" target="_blank">00:59:01.260</a></span> | <span class="t">Look, I think there's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3548" target="_blank">00:59:08.960</a></span> | <span class="t">I think that there's a lot of interesting work to be done on improving the data filtration steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3556" target="_blank">00:59:16.820</a></span> | <span class="t">So, you know, in this slide, we were optimizing this for high precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3562" target="_blank">00:59:22.860</a></span> | <span class="t">So, making sure that all the videos at the end were very high quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3566" target="_blank">00:59:26.240</a></span> | <span class="t">Recall was sacrificed throughout this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3569" target="_blank">00:59:29.400</a></span> | <span class="t">These are like very computer vision terms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3572" target="_blank">00:59:32.040</a></span> | <span class="t">But we would have lost a lot of good data in this like very complex pipeline here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3576" target="_blank">00:59:36.140</a></span> | <span class="t">One like way forward for getting more data is moving to smarter ways of doing this process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3583" target="_blank">00:59:43.200</a></span> | <span class="t">Maybe completely language model based, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3588" target="_blank">00:59:48.620</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3589" target="_blank">00:59:49.060</a></span> | <span class="t">I think that's one answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3590" target="_blank">00:59:50.760</a></span> | <span class="t">A question about compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3594" target="_blank">00:59:54.560</a></span> | <span class="t">This is more general, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3596" target="_blank">00:59:56.080</a></span> | <span class="t">How can academic researchers contribute to video generation without having, you know, access to thousands of GPUs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3604" target="_blank">01:00:04.280</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3604" target="_blank">01:00:04.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3606" target="_blank">01:00:06.780</a></span> | <span class="t">It's obviously very tough to do this level of pre-training outside of industry labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3613" target="_blank">01:00:13.180</a></span> | <span class="t">But, you know, throughout this paper, we've used a bunch of innovations that came from academia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3618" target="_blank">01:00:18.840</a></span> | <span class="t">For example, a lot of the flow matching work was done at universities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3624" target="_blank">01:00:24.080</a></span> | <span class="t">The main paper we take, Inspiration Forum, did come from Masa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3629" target="_blank">01:00:29.700</a></span> | <span class="t">But this kind of research can be done at small scale, and then all of us can learn from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3634" target="_blank">01:00:34.600</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3636" target="_blank">01:00:36.880</a></span> | <span class="t">The pre-training is very tough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3639" target="_blank">01:00:39.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3645" target="_blank">01:00:45.220</a></span> | <span class="t">I think the pre-training is tough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3652" target="_blank">01:00:52.420</a></span> | <span class="t">But things like learning objectives, I think post-training schemes, we've seen a lot of, like, grade work coming out of academia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3659" target="_blank">01:00:59.400</a></span> | <span class="t">For that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3660" target="_blank">01:01:00.740</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3661" target="_blank">01:01:01.640</a></span> | <span class="t">A few questions all related to text, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3666" target="_blank">01:01:06.700</a></span> | <span class="t">Some folks, you know, are saying there's a lot of work of cleaning and processing video data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3672" target="_blank">01:01:12.820</a></span> | <span class="t">How do you make sure that the actual text, for example, the LAMA 3 generated captions are high quality and complete?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3686" target="_blank">01:01:26.180</a></span> | <span class="t">Yeah, good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3687" target="_blank">01:01:27.280</a></span> | <span class="t">We put a lot of work into training this LAMA 3 captioner, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3693" target="_blank">01:01:33.880</a></span> | <span class="t">So this is a video-conditioned LAMA model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3696" target="_blank">01:01:36.700</a></span> | <span class="t">This went through its own sort of large-scale training in order to generate good-looking captions that were aligned with what we wanted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3706" target="_blank">01:01:46.640</a></span> | <span class="t">But certainly, there's a lot of room for improvement there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3709" target="_blank">01:01:49.280</a></span> | <span class="t">These captions are not as good as human-written captions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3714" target="_blank">01:01:54.900</a></span> | <span class="t">There are a lot of architectural reasons for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3716" target="_blank">01:01:56.920</a></span> | <span class="t">A lot of these, like, video-conditioned language models cannot see the whole video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3723" target="_blank">01:02:03.660</a></span> | <span class="t">If it's a 16-second video at 16 FPS, often it's far too much video for the model to be conditioned on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3732" target="_blank">01:02:12.620</a></span> | <span class="t">So often with a lot of these open-source models, like not only LAMA but Jemma and so on, you have to subsample frames.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3741" target="_blank">01:02:21.160</a></span> | <span class="t">And so you're blocking the model, the language model, from seeing a lot of the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3744" target="_blank">01:02:24.600</a></span> | <span class="t">It's going to lead to some issues, some missed things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3747" target="_blank">01:02:27.240</a></span> | <span class="t">So we do our best by training and post-training a captioning model, doing a bunch of evaluations on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3753" target="_blank">01:02:33.560</a></span> | <span class="t">But that's definitely something that can be improved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3755" target="_blank">01:02:35.740</a></span> | <span class="t">And there's a bunch of really cool results out there from the text-to-image community showing that if you improve captions, your image quality gets better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3764" target="_blank">01:02:44.020</a></span> | <span class="t">It's not entirely clear why, but that keeps on happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3767" target="_blank">01:02:47.160</a></span> | <span class="t">Someone asked a related thing, which is how much roller importance does the text-to-prompt encoder play?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3775" target="_blank">01:02:55.120</a></span> | <span class="t">I think there were some image generation works which showed, like, replacing the text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3780" target="_blank">01:03:00.780</a></span> | <span class="t">I think it was from clip to, like, T5, like, really helped improve performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3785" target="_blank">01:03:05.560</a></span> | <span class="t">Did you guys play around with, like, several different text encoders?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3790" target="_blank">01:03:10.900</a></span> | <span class="t">So this particular series of text encodings was sort of precedent in our team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3797" target="_blank">01:03:17.280</a></span> | <span class="t">We took motivation from a recent, like, state-of-the-art text-to-image paper that used this series of text encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3805" target="_blank">01:03:25.660</a></span> | <span class="t">But, you know, it's worth pointing out here that it is quite strange what we've done here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3810" target="_blank">01:03:30.380</a></span> | <span class="t">You would think it would be, like, intuitive that you want your best possible text representation here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3819" target="_blank">01:03:39.040</a></span> | <span class="t">All of these text representations are nowhere near state-of-the-art.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3821" target="_blank">01:03:41.920</a></span> | <span class="t">They're not LAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3822" target="_blank">01:03:42.740</a></span> | <span class="t">They're not GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3823" target="_blank">01:03:43.800</a></span> | <span class="t">There's been a few works and empirical findings showing that, in this setup at least, decoder-only text representations don't work as well for some reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3838" target="_blank">01:03:58.800</a></span> | <span class="t">Some reasons for that could be, or some have hypothesized, that you need a text representation that is more aligned with the media space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3847" target="_blank">01:04:07.640</a></span> | <span class="t">That's why a lot of people you'll see conditioning on clip, which is what we do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3851" target="_blank">01:04:11.440</a></span> | <span class="t">So, yeah, a bunch of cool work to do here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3855" target="_blank">01:04:15.940</a></span> | <span class="t">We didn't ablate this in this project, though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3860" target="_blank">01:04:20.400</a></span> | <span class="t">I have a question followed up on the text part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3863" target="_blank">01:04:23.140</a></span> | <span class="t">How well would this do with, like, very detailed prompts?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3867" target="_blank">01:04:27.300</a></span> | <span class="t">Like, I want to move a video about this person wearing this specific color, and here's what happens later on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3875" target="_blank">01:04:35.300</a></span> | <span class="t">And then, like, a very detailed script, rather than, you know, just a video of some penguins, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3882" target="_blank">01:04:42.020</a></span> | <span class="t">We definitely did observe that the model can do these sequential actions, but not always completely accurately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3892" target="_blank">01:04:52.120</a></span> | <span class="t">Some of that might be issues in the captioning of the pre-training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3897" target="_blank">01:04:57.980</a></span> | <span class="t">Like, are you accurately captioning all of these things happening?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3901" target="_blank">01:05:01.800</a></span> | <span class="t">But, yeah, that will be one of the places where it struggles, if you were to detail, like, three or four things happening in sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3909" target="_blank">01:05:09.020</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3911" target="_blank">01:05:11.160</a></span> | <span class="t">We have one question, which is, could you somehow hard code in some priors related to physics and real-world common sense to improve the realism and accuracy of generated videos?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3921" target="_blank">01:05:21.100</a></span> | <span class="t">I guess that might be related to that video of, like, the car splitting and all of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3925" target="_blank">01:05:25.800</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3928" target="_blank">01:05:28.720</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3935" target="_blank">01:05:35.000</a></span> | <span class="t">So, in a way, it's kind of like the antithesis of...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3940" target="_blank">01:05:40.300</a></span> | <span class="t">the things that we were trying here, of, like, removing all of the inductive biases and just scaling compute and data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3948" target="_blank">01:05:48.160</a></span> | <span class="t">But I do think those are interesting things to try, simply because maybe if you are trying to learn the laws of physics better, just random videos from, like, a large pool aren't maybe the best thing to use there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3965" target="_blank">01:06:05.240</a></span> | <span class="t">And there are other cool works that have been released where they're trained, like, entirely on video game data, things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3972" target="_blank">01:06:12.100</a></span> | <span class="t">Yeah, I think encoding some sort of priors would be cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3979" target="_blank">01:06:19.300</a></span> | <span class="t">Yeah, because, you know, there are certain things about the natural world that we do know for sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3982" target="_blank">01:06:22.740</a></span> | <span class="t">Certain, like, computer vision principles and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3986" target="_blank">01:06:26.240</a></span> | <span class="t">But, yeah, we didn't do that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3988" target="_blank">01:06:28.580</a></span> | <span class="t">Do you know how to possibly encode those?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3990" target="_blank">01:06:30.780</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3991" target="_blank">01:06:31.600</a></span> | <span class="t">That's, like, a super open question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3994" target="_blank">01:06:34.040</a></span> | <span class="t">Super open question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3995" target="_blank">01:06:35.000</a></span> | <span class="t">Okay, makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3995" target="_blank">01:06:35.960</a></span> | <span class="t">We've got a couple others online, and then we can go back to in-person.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=3999" target="_blank">01:06:39.260</a></span> | <span class="t">One's more on, you know, deep fakes and malicious use of video generation models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4004" target="_blank">01:06:44.720</a></span> | <span class="t">Is there any work on things like watermarking to sort of deal with that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4009" target="_blank">01:06:49.920</a></span> | <span class="t">There's definitely a bunch of work on watermarking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4012" target="_blank">01:06:52.460</a></span> | <span class="t">We, yeah, there's definitely a bunch of work on watermarking coming from a few groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4022" target="_blank">01:07:02.140</a></span> | <span class="t">DeepMind has released a few interesting papers on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4025" target="_blank">01:07:05.380</a></span> | <span class="t">Masa has a team working on this as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4028" target="_blank">01:07:08.000</a></span> | <span class="t">So, yeah, very important work to be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4031" target="_blank">01:07:11.300</a></span> | <span class="t">And then we can go back to some in-person questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4039" target="_blank">01:07:19.380</a></span> | <span class="t">One's about GANs, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4041" target="_blank">01:07:21.660</a></span> | <span class="t">I've noticed some other models have a GAN discriminator before or in the decoder for temporal coherence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4048" target="_blank">01:07:28.260</a></span> | <span class="t">Is this still useful, or does scale overcome this need?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4051" target="_blank">01:07:31.860</a></span> | <span class="t">They mean in the VAE decoder?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4055" target="_blank">01:07:35.740</a></span> | <span class="t">I think so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4058" target="_blank">01:07:38.900</a></span> | <span class="t">Yeah, so we also have, again, discriminator there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4061" target="_blank">01:07:41.660</a></span> | <span class="t">It's not necessarily for temporal consistency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4067" target="_blank">01:07:47.440</a></span> | <span class="t">That might be one of the outcomes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4071" target="_blank">01:07:51.060</a></span> | <span class="t">There's sort of a historical precedent for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4074" target="_blank">01:07:54.820</a></span> | <span class="t">When you are training these, where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4079" target="_blank">01:07:59.760</a></span> | <span class="t">When you're training these VAEs, folk found, it was the stable diffusion researchers, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4086" target="_blank">01:08:06.060</a></span> | <span class="t">while they were still at college in Germany.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4089" target="_blank">01:08:09.040</a></span> | <span class="t">They published this paper called VQGAN back in 2021, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4094" target="_blank">01:08:14.240</a></span> | <span class="t">And they showed that normally how you train these VAEs is that you just use L1 losses between the input and the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4102" target="_blank">01:08:22.780</a></span> | <span class="t">But they found that the amount of compression you could do was very limited when you were doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4107" target="_blank">01:08:27.180</a></span> | <span class="t">So what they started doing was adding some adversarial losses from the GAN literature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4112" target="_blank">01:08:32.340</a></span> | <span class="t">What this does is it tells the VAE that I don't have to decode exactly what was given to me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4121" target="_blank">01:08:41.880</a></span> | <span class="t">Because the GAN losses, they're not L1 losses, they're not pixel level losses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4125" target="_blank">01:08:45.700</a></span> | <span class="t">These losses are just how well the GAN can tell if it's a real image or a fake one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4131" target="_blank">01:08:51.640</a></span> | <span class="t">So when folks started training VAEs with these losses, the outputs were given a bit more freedom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4140" target="_blank">01:09:00.260</a></span> | <span class="t">So the model could generate a range of things for a sudden input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4144" target="_blank">01:09:04.200</a></span> | <span class="t">And it's not getting penalized from the loss as long as they look real.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4147" target="_blank">01:09:07.440</a></span> | <span class="t">And I realize there's a bunch of detail there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4150" target="_blank">01:09:10.780</a></span> | <span class="t">But basically, by adding these losses, we were able to get another 2x level of compression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4156" target="_blank">01:09:16.760</a></span> | <span class="t">So we are also using adversarial losses in the VAE, if that was their question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4162" target="_blank">01:09:22.180</a></span> | <span class="t">Someone added an interesting question just now, which is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4165" target="_blank">01:09:25.520</a></span> | <span class="t">how might you make sure all the videos in your training data are real?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4170" target="_blank">01:09:30.960</a></span> | <span class="t">What if some of them are fake?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4172" target="_blank">01:09:32.700</a></span> | <span class="t">I guess that's related to some work nowadays in language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4177" target="_blank">01:09:37.280</a></span> | <span class="t">But once more and more training data becomes synthetically generated, does that lead to any issues, I guess?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4185" target="_blank">01:09:45.980</a></span> | <span class="t">Definitely this idea of, like, data set poisoning, it's often called, is a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4191" target="_blank">01:09:51.140</a></span> | <span class="t">I think there are a couple interesting answers to this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4197" target="_blank">01:09:57.020</a></span> | <span class="t">Firstly, in our pre-training data, if it's low quality, then we don't want to train on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4203" target="_blank">01:10:03.800</a></span> | <span class="t">And hopefully, we would find it and get rid of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4206" target="_blank">01:10:06.320</a></span> | <span class="t">But training on generated data is not always a bad thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4210" target="_blank">01:10:10.040</a></span> | <span class="t">Most contemporary post-training approaches for language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4214" target="_blank">01:10:14.980</a></span> | <span class="t">are based upon training on generated data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4217" target="_blank">01:10:17.360</a></span> | <span class="t">You train on generations from the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4220" target="_blank">01:10:20.000</a></span> | <span class="t">So it's not like training on generated data is always bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4223" target="_blank">01:10:23.460</a></span> | <span class="t">If there's some really, like, bad generated videos in the pre-training data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4228" target="_blank">01:10:28.480</a></span> | <span class="t">we do want to get rid of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4229" target="_blank">01:10:29.780</a></span> | <span class="t">But it's not always bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4235" target="_blank">01:10:35.880</a></span> | <span class="t">We've got time for a couple more in-person questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4238" target="_blank">01:10:38.420</a></span> | <span class="t">If...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4239" target="_blank">01:10:39.000</a></span> | <span class="t">In the paper, we included, like, full details of the training infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4245" target="_blank">01:10:45.140</a></span> | <span class="t">I don't think we included, like, in the paper, like, full details of the training infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4253" target="_blank">01:10:53.320</a></span> | <span class="t">That will be there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4253" target="_blank">01:10:53.800</a></span> | <span class="t">I don't think we included any details about the inference infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4262" target="_blank">01:11:02.820</a></span> | <span class="t">In the paper, we included, like, full details of the training infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4267" target="_blank">01:11:07.180</a></span> | <span class="t">That will be there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4268" target="_blank">01:11:08.880</a></span> | <span class="t">I don't think we included any details about the inference infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4274" target="_blank">01:11:14.320</a></span> | <span class="t">There are lots of details on the inference infrastructure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4276" target="_blank">01:11:16.700</a></span> | <span class="t">There are lots of details on the inference in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4280" target="_blank">01:11:20.240</a></span> | <span class="t">There are lots of details on the inference infrastructure that I've shown here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4286" target="_blank">01:11:26.580</a></span> | <span class="t">Any other questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4287" target="_blank">01:11:27.580</a></span> | <span class="t">I don't know if I missed that, but for the videos you generated, do they come with audio?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4293" target="_blank">01:11:33.240</a></span> | <span class="t">Yeah, so this text-to-video model that I've shown here, it generates video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4301" target="_blank">01:11:41.860</a></span> | <span class="t">But with the publication, we released a, well, published about a, where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4312" target="_blank">01:11:52.980</a></span> | <span class="t">Actually, no, we're not going over that problem again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4315" target="_blank">01:11:55.940</a></span> | <span class="t">There's no audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4319" target="_blank">01:11:59.160</a></span> | <span class="t">Yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4321" target="_blank">01:12:01.480</a></span> | <span class="t">So there were, yeah, there was a separate MovieGen audio model that we trained that will add audio</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4328" target="_blank">01:12:08.500</a></span> | <span class="t">to a generated video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4329" target="_blank">01:12:09.500</a></span> | <span class="t">Video asked two separate models, and the audio is, like, another level of complexity where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4339" target="_blank">01:12:19.560</a></span> | <span class="t">there are, like, multiple chats, and if you want to train on mini-files, whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4343" target="_blank">01:12:23.800</a></span> | <span class="t">So what is, like, your current progress in this oil generation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4353" target="_blank">01:12:33.560</a></span> | <span class="t">So, we have a really great audio research team that works on this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4364" target="_blank">01:12:44.200</a></span> | <span class="t">One very nice thing to do, I think, would be to generate everything at once, like video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4369" target="_blank">01:12:49.320</a></span> | <span class="t">and audio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4371" target="_blank">01:12:51.240</a></span> | <span class="t">The two modalities are very correlated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4373" target="_blank">01:12:53.120</a></span> | <span class="t">There's a lot of shared information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4374" target="_blank">01:12:54.620</a></span> | <span class="t">So, theoretically, both modalities should benefit from being trained together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4380" target="_blank">01:13:00.680</a></span> | <span class="t">The audio even encodes some information about videos that are not present in the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4384" target="_blank">01:13:04.680</a></span> | <span class="t">So, the issue is one of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4389" target="_blank">01:13:09.360</a></span> | <span class="t">It's very hard to get high-quality video data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4391" target="_blank">01:13:11.360</a></span> | <span class="t">It's even harder to get high-quality video with good audio data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4395" target="_blank">01:13:15.240</a></span> | <span class="t">So, that's part of the reason why we didn't do that for this project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4403" target="_blank">01:13:23.920</a></span> | <span class="t">Awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4404" target="_blank">01:13:24.920</a></span> | <span class="t">Thanks so much, Andrew, for the very insightful talk and answering all our questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4408" target="_blank">01:13:28.300</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4409" target="_blank">01:13:29.300</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=YGHF8_tf--g&t=4410" target="_blank">01:13:30.300</a></span> | <span class="t">Thank you.</span></div></div></body></html>