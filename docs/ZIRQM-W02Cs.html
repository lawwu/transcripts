<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 6: RoBERTa I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 6: RoBERTa I Spring 2023</h2><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs"><img src="https://i.ytimg.com/vi/ZIRQM-W02Cs/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=15">0:15</a> Addressing the known limitations with BERT<br><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=88">1:28</a> Robustly optimized BERT approach<br><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=263">4:23</a> ROBERTA results informing final system design<br><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=467">7:47</a> ROBERTA: Core model releases<br><br><div style="text-align: left;"><a href="./ZIRQM-W02Cs.html">Whisper Transcript</a> | <a href="./transcript_ZIRQM-W02Cs.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=6" target="_blank">00:00:06.080</a></span> | <span class="t">This is part six in our series on contextual representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=9" target="_blank">00:00:09.560</a></span> | <span class="t">We're going to focus on RoBERTa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=11" target="_blank">00:00:11.080</a></span> | <span class="t">RoBERTa stands for robustly optimized BERT approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=14" target="_blank">00:00:14.680</a></span> | <span class="t">You might recall that I finished</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=16" target="_blank">00:00:16.600</a></span> | <span class="t">the BERT screencast by listing out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=18" target="_blank">00:00:18.360</a></span> | <span class="t">some key known limitations of the BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=21" target="_blank">00:00:21.680</a></span> | <span class="t">The top item on that list was just an observation that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=25" target="_blank">00:00:25.280</a></span> | <span class="t">the BERT team originally did an admirably detailed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=28" target="_blank">00:00:28.800</a></span> | <span class="t">but still very partial set of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=30" target="_blank">00:00:30.840</a></span> | <span class="t">ablation studies and optimization studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=33" target="_blank">00:00:33.880</a></span> | <span class="t">That gave us some glimpses of how to best optimize BERT models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=38" target="_blank">00:00:38.960</a></span> | <span class="t">but it was hardly a thorough exploration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=41" target="_blank">00:00:41.480</a></span> | <span class="t">That's where the RoBERTa team is going to take over and try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=45" target="_blank">00:00:45.120</a></span> | <span class="t">do a more thorough exploration of this design space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=48" target="_blank">00:00:48.600</a></span> | <span class="t">I think this is a really interesting development</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=51" target="_blank">00:00:51.160</a></span> | <span class="t">because at a meta level,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=53" target="_blank">00:00:53.240</a></span> | <span class="t">it points to a shift in methodologies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=55" target="_blank">00:00:55.640</a></span> | <span class="t">The RoBERTa team does do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=57" target="_blank">00:00:57.640</a></span> | <span class="t">a much fuller exploration of the design space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=60" target="_blank">00:01:00.320</a></span> | <span class="t">but it's nowhere near the exhaustive exploration of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=64" target="_blank">00:01:04.120</a></span> | <span class="t">hyperparameters that we used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=65" target="_blank">00:01:05.760</a></span> | <span class="t">see especially in the pre-deep learning era.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=68" target="_blank">00:01:08.880</a></span> | <span class="t">I think what we're seeing with RoBERTa is that it is simply too</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=72" target="_blank">00:01:12.440</a></span> | <span class="t">expensive in terms of money or compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=75" target="_blank">00:01:15.040</a></span> | <span class="t">or time to be completely thorough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=77" target="_blank">00:01:17.960</a></span> | <span class="t">Even RoBERTa is a very heuristic and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=81" target="_blank">00:01:21.160</a></span> | <span class="t">partial exploration of the design space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=84" target="_blank">00:01:24.000</a></span> | <span class="t">But nonetheless, I think it was extremely instructive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=87" target="_blank">00:01:27.520</a></span> | <span class="t">For this slide, I'm going to list out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=89" target="_blank">00:01:29.920</a></span> | <span class="t">key differences between BERT and RoBERTa,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=92" target="_blank">00:01:32.560</a></span> | <span class="t">and then we'll explore some of the evidence in favor of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=95" target="_blank">00:01:35.560</a></span> | <span class="t">these decisions just after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=98" target="_blank">00:01:38.560</a></span> | <span class="t">First item on the list,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=100" target="_blank">00:01:40.080</a></span> | <span class="t">BERT used a static masking approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=102" target="_blank">00:01:42.920</a></span> | <span class="t">What that means is that they copied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=104" target="_blank">00:01:44.960</a></span> | <span class="t">their training data some number of times and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=107" target="_blank">00:01:47.560</a></span> | <span class="t">applied different masks to each copy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=110" target="_blank">00:01:50.560</a></span> | <span class="t">But then that set of copies of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=113" target="_blank">00:01:53.280</a></span> | <span class="t">the dataset with its masking was used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=115" target="_blank">00:01:55.280</a></span> | <span class="t">repeatedly during epochs of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=118" target="_blank">00:01:58.000</a></span> | <span class="t">What that means is that the same masking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=120" target="_blank">00:02:00.280</a></span> | <span class="t">was seen repeatedly by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=122" target="_blank">00:02:02.560</a></span> | <span class="t">You might have an intuition that we'll get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=124" target="_blank">00:02:04.880</a></span> | <span class="t">more and better diversity into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=127" target="_blank">00:02:07.320</a></span> | <span class="t">this training regime if we dynamically mask examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=130" target="_blank">00:02:10.200</a></span> | <span class="t">which would just mean that as we load individual batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=133" target="_blank">00:02:13.320</a></span> | <span class="t">we apply some random dynamic masking to those so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=137" target="_blank">00:02:17.680</a></span> | <span class="t">subsequent batches containing the same examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=140" target="_blank">00:02:20.960</a></span> | <span class="t">have different masking applied to them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=143" target="_blank">00:02:23.360</a></span> | <span class="t">Clearly, that's going to introduce some diversity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=145" target="_blank">00:02:25.720</a></span> | <span class="t">into the training regime and that could be useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=148" target="_blank">00:02:28.760</a></span> | <span class="t">For BERT, the inputs to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=152" target="_blank">00:02:32.840</a></span> | <span class="t">were two concatenated document segments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=155" target="_blank">00:02:35.760</a></span> | <span class="t">and that's actually crucial to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=157" target="_blank">00:02:37.000</a></span> | <span class="t">their next sentence prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=159" target="_blank">00:02:39.240</a></span> | <span class="t">Whereas for RoBERTa, inputs are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=161" target="_blank">00:02:41.240</a></span> | <span class="t">sentence sequences that may even span document boundaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=165" target="_blank">00:02:45.520</a></span> | <span class="t">Obviously, that's going to be disruptive to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=167" target="_blank">00:02:47.800</a></span> | <span class="t">the next sentence prediction objective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=170" target="_blank">00:02:50.000</a></span> | <span class="t">but correspondingly, whereas BERT had that NSP objective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=173" target="_blank">00:02:53.960</a></span> | <span class="t">RoBERTa simply dropped it on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=175" target="_blank">00:02:55.520</a></span> | <span class="t">the grounds that it was not earning its keep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=178" target="_blank">00:02:58.480</a></span> | <span class="t">For BERT, the training batches contained 256 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=183" target="_blank">00:03:03.600</a></span> | <span class="t">RoBERTa upped that to 2,000 examples per batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=187" target="_blank">00:03:07.160</a></span> | <span class="t">a substantial increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=189" target="_blank">00:03:09.280</a></span> | <span class="t">BERT used a wordpiece tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=192" target="_blank">00:03:12.000</a></span> | <span class="t">whereas RoBERTa used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=193" target="_blank">00:03:13.240</a></span> | <span class="t">a character level byte-pair encoding algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=197" target="_blank">00:03:17.080</a></span> | <span class="t">BERT was trained on a lot of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=199" target="_blank">00:03:19.640</a></span> | <span class="t">Books, Corpus, and English Wikipedia.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=201" target="_blank">00:03:21.960</a></span> | <span class="t">RoBERTa leveled up on the amount of data by training on Books,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=205" target="_blank">00:03:25.840</a></span> | <span class="t">Corpus, Wikipedia, CC News,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=208" target="_blank">00:03:28.080</a></span> | <span class="t">Open Web Text, and Stories,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=209" target="_blank">00:03:29.800</a></span> | <span class="t">and the result of that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=211" target="_blank">00:03:31.400</a></span> | <span class="t">a substantial increase in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=212" target="_blank">00:03:32.840</a></span> | <span class="t">the amount of data that the model saw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=215" target="_blank">00:03:35.440</a></span> | <span class="t">BERT was trained for one million steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=218" target="_blank">00:03:38.280</a></span> | <span class="t">whereas RoBERTa was trained for 500,000 steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=221" target="_blank">00:03:41.840</a></span> | <span class="t">Pause there. You might think that means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=224" target="_blank">00:03:44.200</a></span> | <span class="t">RoBERTa was trained for less time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=226" target="_blank">00:03:46.760</a></span> | <span class="t">but remember the batch sizes are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=228" target="_blank">00:03:48.720</a></span> | <span class="t">substantially larger and so the net effect of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=231" target="_blank">00:03:51.240</a></span> | <span class="t">these two choices is that RoBERTa was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=233" target="_blank">00:03:53.400</a></span> | <span class="t">trained for a lot more instances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=237" target="_blank">00:03:57.120</a></span> | <span class="t">Then finally, for the BERT team,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=239" target="_blank">00:03:59.440</a></span> | <span class="t">there was an intuition that it would be useful for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=241" target="_blank">00:04:01.480</a></span> | <span class="t">optimization to train on short sequences first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=245" target="_blank">00:04:05.080</a></span> | <span class="t">The RoBERTa team simply dropped that and trained on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=247" target="_blank">00:04:07.880</a></span> | <span class="t">full-length sequences throughout the training regime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=251" target="_blank">00:04:11.560</a></span> | <span class="t">I think those are the high-level changes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=254" target="_blank">00:04:14.160</a></span> | <span class="t">between BERT and RoBERTa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=255" target="_blank">00:04:15.560</a></span> | <span class="t">There are some additional differences and I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=257" target="_blank">00:04:17.720</a></span> | <span class="t">refer to Section 3.1 of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=260" target="_blank">00:04:20.000</a></span> | <span class="t">the paper for the details on those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=262" target="_blank">00:04:22.920</a></span> | <span class="t">Let's dive into some of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=265" target="_blank">00:04:25.040</a></span> | <span class="t">the evidence that they used for these choices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=267" target="_blank">00:04:27.520</a></span> | <span class="t">beginning with that first shift from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=270" target="_blank">00:04:30.160</a></span> | <span class="t">static masking to dynamic masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=273" target="_blank">00:04:33.000</a></span> | <span class="t">This table summarizes their evidence for this choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=276" target="_blank">00:04:36.240</a></span> | <span class="t">They're using SQuAD, Multi-NLI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=278" target="_blank">00:04:38.600</a></span> | <span class="t">and Binary Stanford Sentiment Treebank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=281" target="_blank">00:04:41.440</a></span> | <span class="t">as their benchmarks to make this decision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=284" target="_blank">00:04:44.400</a></span> | <span class="t">You can see that for SQuAD and SST,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=286" target="_blank">00:04:46.960</a></span> | <span class="t">there's a pretty clear win,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=288" target="_blank">00:04:48.200</a></span> | <span class="t">dynamic masking is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=289" target="_blank">00:04:49.840</a></span> | <span class="t">For Multi-NLI, it looks like there was a small regression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=293" target="_blank">00:04:53.000</a></span> | <span class="t">but on average, the results look better for dynamic masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=296" target="_blank">00:04:56.880</a></span> | <span class="t">I will say that to augment these results,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=299" target="_blank">00:04:59.800</a></span> | <span class="t">there is a clear intuition that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=301" target="_blank">00:05:01.480</a></span> | <span class="t">dynamic masking is going to be useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=303" target="_blank">00:05:03.600</a></span> | <span class="t">Even if it's not reflected in these benchmarks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=306" target="_blank">00:05:06.200</a></span> | <span class="t">we might still think that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=308" target="_blank">00:05:08.080</a></span> | <span class="t">a wise choice if we can afford to train in that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=312" target="_blank">00:05:12.120</a></span> | <span class="t">We talked briefly about how examples are presented to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=316" target="_blank">00:05:16.560</a></span> | <span class="t">these models. I would say the two competitors that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=319" target="_blank">00:05:19.800</a></span> | <span class="t">Roberta thoroughly evaluated were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=322" target="_blank">00:05:22.320</a></span> | <span class="t">full sentences and doc sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=325" target="_blank">00:05:25.000</a></span> | <span class="t">Doc sentences will be where we limit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=327" target="_blank">00:05:27.480</a></span> | <span class="t">training instances to pairs of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=329" target="_blank">00:05:29.000</a></span> | <span class="t">sentences that come from the same document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=331" target="_blank">00:05:31.400</a></span> | <span class="t">which you would think would give us a clear intuition about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=334" target="_blank">00:05:34.360</a></span> | <span class="t">something like discourse coherence for those instances.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=338" target="_blank">00:05:38.080</a></span> | <span class="t">We can also compare that against full sentences in which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=341" target="_blank">00:05:41.320</a></span> | <span class="t">present examples even though they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=343" target="_blank">00:05:43.880</a></span> | <span class="t">might span document boundaries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=346" target="_blank">00:05:46.480</a></span> | <span class="t">We have less of a guarantee of discourse coherence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=349" target="_blank">00:05:49.760</a></span> | <span class="t">Although doc sentences comes out a little bit ahead in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=353" target="_blank">00:05:53.160</a></span> | <span class="t">this benchmark that they have set up across squad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=355" target="_blank">00:05:55.800</a></span> | <span class="t">Multi-NLI, SST2, and race,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=358" target="_blank">00:05:58.800</a></span> | <span class="t">they chose full sentences on the grounds that there is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=361" target="_blank">00:06:01.720</a></span> | <span class="t">more at play here than just accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=364" target="_blank">00:06:04.960</a></span> | <span class="t">We should also think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=366" target="_blank">00:06:06.880</a></span> | <span class="t">the efficiency of the training regime.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=369" target="_blank">00:06:09.200</a></span> | <span class="t">Since full sentences makes it much easier to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=371" target="_blank">00:06:11.760</a></span> | <span class="t">create efficient batches of examples,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=374" target="_blank">00:06:14.320</a></span> | <span class="t">they opted for that instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=376" target="_blank">00:06:16.120</a></span> | <span class="t">That's also very welcome to my mind because it's showing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=379" target="_blank">00:06:19.360</a></span> | <span class="t">again, that there's more at stake in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=381" target="_blank">00:06:21.000</a></span> | <span class="t">this new era than just accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=383" target="_blank">00:06:23.640</a></span> | <span class="t">We should also consider our resources.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=386" target="_blank">00:06:26.880</a></span> | <span class="t">This table summarizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=389" target="_blank">00:06:29.760</a></span> | <span class="t">their evidence for the larger batch sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=392" target="_blank">00:06:32.120</a></span> | <span class="t">They're using various metrics here, perplexity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=394" target="_blank">00:06:34.680</a></span> | <span class="t">which is a pseudo perplexity given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=396" target="_blank">00:06:36.680</a></span> | <span class="t">that BERT uses bidirectional context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=399" target="_blank">00:06:39.600</a></span> | <span class="t">They're also benchmarking against Multi-NLI and SST2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=403" target="_blank">00:06:43.320</a></span> | <span class="t">What they find is that clearly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=405" target="_blank">00:06:45.560</a></span> | <span class="t">there's a win for having</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=407" target="_blank">00:06:47.040</a></span> | <span class="t">this very large batch size at 2,000 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=411" target="_blank">00:06:51.640</a></span> | <span class="t">Then finally, just the raw amount of data that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=415" target="_blank">00:06:55.240</a></span> | <span class="t">these models are trained on is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=416" target="_blank">00:06:56.640</a></span> | <span class="t">interesting and also the amount</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=418" target="_blank">00:06:58.080</a></span> | <span class="t">of training time that they get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=419" target="_blank">00:06:59.680</a></span> | <span class="t">What they found is that they got</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=421" target="_blank">00:07:01.640</a></span> | <span class="t">the best results for Roberta by training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=424" target="_blank">00:07:04.240</a></span> | <span class="t">for as long as they could possibly afford</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=426" target="_blank">00:07:06.400</a></span> | <span class="t">to on as much data as they could include.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=430" target="_blank">00:07:10.200</a></span> | <span class="t">You can see the amount of data going up to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=432" target="_blank">00:07:12.240</a></span> | <span class="t">160 gigabytes here versus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=434" target="_blank">00:07:14.680</a></span> | <span class="t">the largest BERT model at 13,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=436" target="_blank">00:07:16.800</a></span> | <span class="t">a substantial increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=438" target="_blank">00:07:18.560</a></span> | <span class="t">The step size going all the way up to 500,000,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=441" target="_blank">00:07:21.640</a></span> | <span class="t">whereas for BERT, it was a million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=443" target="_blank">00:07:23.280</a></span> | <span class="t">But remember, overall, there are many more examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=446" target="_blank">00:07:26.040</a></span> | <span class="t">being presented as a result of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=447" target="_blank">00:07:27.600</a></span> | <span class="t">the batch size being so much larger for the Roberta models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=452" target="_blank">00:07:32.240</a></span> | <span class="t">Again, another familiar lesson</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=454" target="_blank">00:07:34.880</a></span> | <span class="t">from the deep learning era,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=456" target="_blank">00:07:36.280</a></span> | <span class="t">more is better in terms of data and training time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=459" target="_blank">00:07:39.880</a></span> | <span class="t">especially when our goal is to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=462" target="_blank">00:07:42.120</a></span> | <span class="t">these pre-trained artifacts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=464" target="_blank">00:07:44.520</a></span> | <span class="t">that are useful for fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=467" target="_blank">00:07:47.280</a></span> | <span class="t">To round this out, I thought I'd mention that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=470" target="_blank">00:07:50.120</a></span> | <span class="t">the Roberta team released two models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=472" target="_blank">00:07:52.280</a></span> | <span class="t">BASE and LARGE, which are directly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=474" target="_blank">00:07:54.600</a></span> | <span class="t">comparable to the corresponding BERT artifacts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=477" target="_blank">00:07:57.520</a></span> | <span class="t">The BASE model has 12 layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=480" target="_blank">00:08:00.280</a></span> | <span class="t">dimensionality of 768,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=482" target="_blank">00:08:02.320</a></span> | <span class="t">and a feed-forward layer of 3072 for a total of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=485" target="_blank">00:08:05.480</a></span> | <span class="t">125 million parameters which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=487" target="_blank">00:08:07.960</a></span> | <span class="t">more or less the same as BERT BASE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=490" target="_blank">00:08:10.080</a></span> | <span class="t">Then correspondingly, BERT LARGE has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=492" target="_blank">00:08:12.240</a></span> | <span class="t">all the same basic settings as BERT BASE,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=495" target="_blank">00:08:15.680</a></span> | <span class="t">and correspondingly, essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=497" target="_blank">00:08:17.360</a></span> | <span class="t">the same number of parameters at 355 million.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=501" target="_blank">00:08:21.680</a></span> | <span class="t">As I said at the start of this screencast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=504" target="_blank">00:08:24.600</a></span> | <span class="t">Roberta was thorough, but even that is only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=506" target="_blank">00:08:26.760</a></span> | <span class="t">a very partial exploration of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=508" target="_blank">00:08:28.440</a></span> | <span class="t">the full design space suggested by the BERT model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=511" target="_blank">00:08:31.360</a></span> | <span class="t">For many more results,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=513" target="_blank">00:08:33.320</a></span> | <span class="t">I highly recommend this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=514" target="_blank">00:08:34.960</a></span> | <span class="t">a primer in BERTology from Rogers et al.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=517" target="_blank">00:08:37.720</a></span> | <span class="t">It's a little bit of an old paper at this point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=520" target="_blank">00:08:40.280</a></span> | <span class="t">so lots has happened since it was released,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=522" target="_blank">00:08:42.240</a></span> | <span class="t">but nonetheless, it's very thorough and contains</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=525" target="_blank">00:08:45.000</a></span> | <span class="t">lots of insights about how best to set up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=527" target="_blank">00:08:47.680</a></span> | <span class="t">these BERT style models for doing various things in NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=531" target="_blank">00:08:51.040</a></span> | <span class="t">So highly recommended as a companion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=533" target="_blank">00:08:53.200</a></span> | <span class="t">to this little screencast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ZIRQM-W02Cs&t=535" target="_blank">00:08:55.760</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>