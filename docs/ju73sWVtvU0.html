<html><head><title>Building AI For All: Amjad Masad & Michele Catasta</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Building AI For All: Amjad Masad & Michele Catasta</h2><a href="https://www.youtube.com/watch?v=ju73sWVtvU0"><img src="https://i.ytimg.com/vi/ju73sWVtvU0/sddefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=0">0:0</a> Introduction - Amjad Masad<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=42">0:42</a> Historical perspective<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=142">2:22</a> How AI can change software<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=269">4:29</a> Ô∏èüì¢ Announcing AI for all!<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=393">6:33</a> A tale of Code LLM & GPU-Poor - Michele Catasta<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=449">7:29</a> How Replit's code completion works<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=519">8:39</a> Ô∏èüì¢ Announcing Replit's new model!<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=825">13:45</a> Ô∏èüì¢ Announcing the new model is open source!<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=846">14:6</a> Model training<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=926">15:26</a> Model evaluation<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1051">17:31</a> Model data & training<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1125">18:45</a> Model evaluation<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1191">19:51</a> Model inference<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1330">22:10</a> Why open source?<br><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1430">23:50</a> Morph Labs Collaboration<br><br><div style="text-align: left;"><a href="./ju73sWVtvU0.html">Whisper Transcript</a> | <a href="./transcript_ju73sWVtvU0.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=15" target="_blank">00:00:15.000</a></span> | <span class="t">Excited to be here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=16" target="_blank">00:00:16.000</a></span> | <span class="t">I agree with Swix and Ben that it feels like a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=21" target="_blank">00:00:21.000</a></span> | <span class="t">It feels like a historical moment here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=24" target="_blank">00:00:24.000</a></span> | <span class="t">My name is Amjad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=25" target="_blank">00:00:25.000</a></span> | <span class="t">I'm the co-founder of Replit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=26" target="_blank">00:00:26.000</a></span> | <span class="t">where we aspire to be the fastest way to get from an idea</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=29" target="_blank">00:00:29.000</a></span> | <span class="t">to a deployed software that you can scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=32" target="_blank">00:00:32.000</a></span> | <span class="t">So I'm going to take you back a little bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=34" target="_blank">00:00:34.000</a></span> | <span class="t">not like Swix to the 600 AD,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=37" target="_blank">00:00:37.000</a></span> | <span class="t">but perhaps to the start of computing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=41" target="_blank">00:00:41.000</a></span> | <span class="t">All right, so very early computers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=44" target="_blank">00:00:44.000</a></span> | <span class="t">the ENIAC was the first during complete,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=47" target="_blank">00:00:47.000</a></span> | <span class="t">programmable Von Neumann machine computer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">The way you programmed it is like you literally punched cards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=54" target="_blank">00:00:54.000</a></span> | <span class="t">Not physically, but you had a machine that sort of punched these cards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=58" target="_blank">00:00:58.000</a></span> | <span class="t">These are sort of binary code for the machine to interpret.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=61" target="_blank">00:01:01.000</a></span> | <span class="t">It was really hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=62" target="_blank">00:01:02.000</a></span> | <span class="t">There wasn't really a software industry because this was really difficult.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=65" target="_blank">00:01:05.000</a></span> | <span class="t">It automated some tasks that human computers did at the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=69" target="_blank">00:01:09.000</a></span> | <span class="t">but it didn't create the software industry yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=72" target="_blank">00:01:12.000</a></span> | <span class="t">But then we moved to texts from punch cards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=76" target="_blank">00:01:16.000</a></span> | <span class="t">And we had first assembly, and then we had compilers and higher-level languages such as C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=85" target="_blank">00:01:25.000</a></span> | <span class="t">and then someone invented JavaScript,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=87" target="_blank">00:01:27.000</a></span> | <span class="t">and it's all been downhill since then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=89" target="_blank">00:01:29.000</a></span> | <span class="t">But text editors were really -- or like text-based programming was at minimum a 10x improvement,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=98" target="_blank">00:01:38.000</a></span> | <span class="t">if not a 100x improvement in programming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=100" target="_blank">00:01:40.000</a></span> | <span class="t">So we've had these moments where we've had orders of magnitude improvements in programming before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=109" target="_blank">00:01:49.000</a></span> | <span class="t">And then, you know, the IDE became a thing because, you know, we had large-scale software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=112" target="_blank">00:01:52.000</a></span> | <span class="t">This is a screenshot from like 2017 or '18 when we added LSP to every programming environment on Replit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=121" target="_blank">00:02:01.000</a></span> | <span class="t">so anyone with an account can get IntelliSense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=124" target="_blank">00:02:04.000</a></span> | <span class="t">And we're really proud about that at the time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=127" target="_blank">00:02:07.000</a></span> | <span class="t">We're burning a lot of CPU doing sort of inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=130" target="_blank">00:02:10.000</a></span> | <span class="t">And, you know, if you've run TypeScript server, that's like a lot of RAM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=134" target="_blank">00:02:14.000</a></span> | <span class="t">But we're really proud that we're giving everyone in the world tools to create professional-grade software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=140" target="_blank">00:02:20.000</a></span> | <span class="t">About three, four years ago, we started kind of thinking about how AI could change software.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=149" target="_blank">00:02:29.000</a></span> | <span class="t">It actually started much sooner than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=152" target="_blank">00:02:32.000</a></span> | <span class="t">But with GPT-2, you know, you could sort of kind of, you know, give it some code and kind of complete part of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=159" target="_blank">00:02:39.000</a></span> | <span class="t">And we're like, okay, this thing is actually happening, and we better be part of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=164" target="_blank">00:02:44.000</a></span> | <span class="t">And so we started building, and we built this product called Ghostwriter, which does auto-complete, chat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=171" target="_blank">00:02:51.000</a></span> | <span class="t">and all sorts of things inside the IDE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=174" target="_blank">00:02:54.000</a></span> | <span class="t">And in just those two years, I mean, the pace of progress across the industry, the tools, basically AI, you know, was deployed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=186" target="_blank">00:03:06.000</a></span> | <span class="t">and a lot of different engineers were using it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=189" target="_blank">00:03:09.000</a></span> | <span class="t">The AI-enhanced engineer, as Wix kind of called it, everyone is sort of using these tools.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=194" target="_blank">00:03:14.000</a></span> | <span class="t">And so we have a world now where a lot of people are gaining huge amount of productivity improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=200" target="_blank">00:03:20.000</a></span> | <span class="t">I don't think we're at a mode of magnitude improvement yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">We're probably in the 50, 80, perhaps 100% improvement for some people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=209" target="_blank">00:03:29.000</a></span> | <span class="t">But we're still at the start of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=211" target="_blank">00:03:31.000</a></span> | <span class="t">And we think that's going to be 10x, 100x, perhaps 1,000x over the next decade.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=219" target="_blank">00:03:39.000</a></span> | <span class="t">The problem, however, Replit's mission has always been about access.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=222" target="_blank">00:03:42.000</a></span> | <span class="t">Our mission is to empower the next billion developers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=226" target="_blank">00:03:46.000</a></span> | <span class="t">And so we really didn't want to create this world where some people have access to Ghostwriter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=231" target="_blank">00:03:51.000</a></span> | <span class="t">and other people don't have access to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=234" target="_blank">00:03:54.000</a></span> | <span class="t">And we started thinking about, okay, what is it, if you really take into heart everything that the AI engineer conference is about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=241" target="_blank">00:04:01.000</a></span> | <span class="t">that we're at a moment where software is changing, where AI is going to be part of the software stack,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=246" target="_blank">00:04:06.000</a></span> | <span class="t">then you have to really step back a little bit and try to rethink how programming changes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=251" target="_blank">00:04:11.000</a></span> | <span class="t">So our view is these programming add-ons such as Copilot and Coding and Ghostwriter and all these things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=257" target="_blank">00:04:17.000</a></span> | <span class="t">we're giving them cute names, we think that's not the way forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=261" target="_blank">00:04:21.000</a></span> | <span class="t">We think that AI needs to be really infused in every programming interaction that you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=267" target="_blank">00:04:27.000</a></span> | <span class="t">And it needs to be part of the default experience of Replit and I'm sure other products in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=271" target="_blank">00:04:31.000</a></span> | <span class="t">That's why we're announcing today that we're giving AI for our millions of users that are coding on Replit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=277" target="_blank">00:04:37.000</a></span> | <span class="t">And so we think this is going to be the biggest deployment of AI-enhanced coding in the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=286" target="_blank">00:04:46.000</a></span> | <span class="t">We're going to be burning as much GPU as we're burning CPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=290" target="_blank">00:04:50.000</a></span> | <span class="t">So pray for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=292" target="_blank">00:04:52.000</a></span> | <span class="t">We have people all over the world coding on all sorts of devices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=306" target="_blank">00:05:06.000</a></span> | <span class="t">We have people coding on Android phones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=308" target="_blank">00:05:08.000</a></span> | <span class="t">And they're all going to get AI now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=311" target="_blank">00:05:11.000</a></span> | <span class="t">So they're all going to be AI-enhanced engineers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=314" target="_blank">00:05:14.000</a></span> | <span class="t">But as we showed, it's not just about AI-enhanced engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=318" target="_blank">00:05:18.000</a></span> | <span class="t">There's also product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=320" target="_blank">00:05:20.000</a></span> | <span class="t">So AI being part of the software creation stack makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=324" target="_blank">00:05:24.000</a></span> | <span class="t">But AI part of the call stack is also where a lot of value is created.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=328" target="_blank">00:05:28.000</a></span> | <span class="t">So that's why we're also -- we have this new product called Model Farm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=336" target="_blank">00:05:36.000</a></span> | <span class="t">And Model Farm basically gives you access to models right into your IDE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=343" target="_blank">00:05:43.000</a></span> | <span class="t">So all it takes is three lines of code to start doing inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=346" target="_blank">00:05:46.000</a></span> | <span class="t">We launched with Google Cloud LLMs, but we're adding LLAMA pretty soon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=353" target="_blank">00:05:53.000</a></span> | <span class="t">We're adding stable diffusion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=355" target="_blank">00:05:55.000</a></span> | <span class="t">And if you're an LLM provider and want to work with us and provide this on our platform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=359" target="_blank">00:05:59.000</a></span> | <span class="t">we'd love to talk to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=360" target="_blank">00:06:00.000</a></span> | <span class="t">But basically, everyone will get -- there's some free tier here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=365" target="_blank">00:06:05.000</a></span> | <span class="t">Everyone will get free access, at least until the end of the year, to Model Farm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=370" target="_blank">00:06:10.000</a></span> | <span class="t">so you can start doing inference and start building AI-based products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=376" target="_blank">00:06:16.000</a></span> | <span class="t">So next up, I'm going to bring up my colleague, the head of AI,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=381" target="_blank">00:06:21.000</a></span> | <span class="t">Mikaela Katasta, to talk about how we train our own AI models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=385" target="_blank">00:06:25.000</a></span> | <span class="t">And we have one more announcement for you coming up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=389" target="_blank">00:06:29.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=408" target="_blank">00:06:48.000</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=409" target="_blank">00:06:49.000</a></span> | <span class="t">Hi, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=410" target="_blank">00:06:50.000</a></span> | <span class="t">So today I'm going to be talking about how we're training LLM for code at Replit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=415" target="_blank">00:06:55.000</a></span> | <span class="t">And I will explain why this weird title.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=418" target="_blank">00:06:58.000</a></span> | <span class="t">If you've been around Twitter, I think a bit more than a month ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=421" target="_blank">00:07:01.000</a></span> | <span class="t">you must have read this study from Semi-Analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=424" target="_blank">00:07:04.000</a></span> | <span class="t">And their point was it's meaningless to work on small models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=429" target="_blank">00:07:09.000</a></span> | <span class="t">train on a limited amount of GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">And that came as a shock to us because we had a very good success story back in May</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=437" target="_blank">00:07:17.000</a></span> | <span class="t">where we started to train our models from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=439" target="_blank">00:07:19.000</a></span> | <span class="t">And then, you know, Amjad and I and the AI team started to think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=443" target="_blank">00:07:23.000</a></span> | <span class="t">are we really wasting our time here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=446" target="_blank">00:07:26.000</a></span> | <span class="t">I'm going to try to convince this actually is not the case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=449" target="_blank">00:07:29.000</a></span> | <span class="t">So our code completion feature, or Replit, is powered by our own bespoke large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=456" target="_blank">00:07:36.000</a></span> | <span class="t">We train open source code, both published on GitHub and also developed by the Replit user base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=462" target="_blank">00:07:42.000</a></span> | <span class="t">It's a very low latency feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=464" target="_blank">00:07:44.000</a></span> | <span class="t">So we try to find a different sweet spot compared to what you might use with other plugins.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=469" target="_blank">00:07:49.000</a></span> | <span class="t">We try to keep our P95 latency below 250 milliseconds, such as the developer experience is almost instantaneous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=476" target="_blank">00:07:56.000</a></span> | <span class="t">You don't even have to think about it, and the code is going to be completed for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=479" target="_blank">00:07:59.000</a></span> | <span class="t">At the model size that we're using, we have been state of the art across the past few months.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=485" target="_blank">00:08:05.000</a></span> | <span class="t">And let's do a show of hands.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=488" target="_blank">00:08:08.000</a></span> | <span class="t">Who has heard about our B1 model back in May?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=492" target="_blank">00:08:12.000</a></span> | <span class="t">All right, that feels good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=494" target="_blank">00:08:14.000</a></span> | <span class="t">For a second I feel like an AI star.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=497" target="_blank">00:08:17.000</a></span> | <span class="t">Jokes aside, so we released Replit code B1.3b back in May.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=502" target="_blank">00:08:22.000</a></span> | <span class="t">We got a lot of adoption, a lot of love, and also a lot of contribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=505" target="_blank">00:08:25.000</a></span> | <span class="t">And that's one of the key reasons why we decided to give it back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=509" target="_blank">00:08:29.000</a></span> | <span class="t">Rapid history has been built on the shoulders of giants, of all the people contributing to the open source space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=516" target="_blank">00:08:36.000</a></span> | <span class="t">So we thought we should do exactly the same year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=518" target="_blank">00:08:38.000</a></span> | <span class="t">We should give back our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=520" target="_blank">00:08:40.000</a></span> | <span class="t">And today, I'm going to be announcing Replit code B1.5.3b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=525" target="_blank">00:08:45.000</a></span> | <span class="t">So the evolution of the model that we released back in May.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=529" target="_blank">00:08:49.000</a></span> | <span class="t">Let's go in detail, as Amjad was saying.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=532" target="_blank">00:08:52.000</a></span> | <span class="t">So the next 10 minutes, we're going to do a technical deep dive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=535" target="_blank">00:08:55.000</a></span> | <span class="t">and I'm going to tell you how we built it and why it's so powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=538" target="_blank">00:08:58.000</a></span> | <span class="t">So first of all, we followed a slightly different recipe compared to the last time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=542" target="_blank">00:09:02.000</a></span> | <span class="t">If you recall, back in May of our V1 was a Lama-style code model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=548" target="_blank">00:09:08.000</a></span> | <span class="t">which means we followed a lot of the best recipes that Meta pioneered.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=551" target="_blank">00:09:11.000</a></span> | <span class="t">Now we went, you know, one level up, and we are training up to 300 tokens per parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=557" target="_blank">00:09:17.000</a></span> | <span class="t">So if you have been following a big history of LLMs, even in, you know, two years ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=562" target="_blank">00:09:22.000</a></span> | <span class="t">most of the models were under-trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=564" target="_blank">00:09:24.000</a></span> | <span class="t">Pardon me for the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=566" target="_blank">00:09:26.000</a></span> | <span class="t">It's not exactly, you know, technically speaking, it's not correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=569" target="_blank">00:09:29.000</a></span> | <span class="t">But the truth is, you know, mid-2022, the Chinchilla paper from DeepMind came out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=574" target="_blank">00:09:34.000</a></span> | <span class="t">and it was like a big warning for the old field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=577" target="_blank">00:09:37.000</a></span> | <span class="t">Basically, what the paper tells us is that we were under-training our models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=580" target="_blank">00:09:40.000</a></span> | <span class="t">we should give them way more high-quality data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=583" target="_blank">00:09:43.000</a></span> | <span class="t">and in exchange, we could train smaller models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=586" target="_blank">00:09:46.000</a></span> | <span class="t">So in a sense, we're amortizing training time for inference time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=590" target="_blank">00:09:50.000</a></span> | <span class="t">Spending more compute to train a smaller, more powerful model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=593" target="_blank">00:09:53.000</a></span> | <span class="t">and then at inference time, the latency would be lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=596" target="_blank">00:09:56.000</a></span> | <span class="t">And that's the key insight that we're going to be carrying along, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=599" target="_blank">00:09:59.000</a></span> | <span class="t">this whole keynote today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=601" target="_blank">00:10:01.000</a></span> | <span class="t">Now, differently from the V1, this time we also doubled the amount of high-quality data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=607" target="_blank">00:10:07.000</a></span> | <span class="t">So we train it up to one trillion tokens of code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=610" target="_blank">00:10:10.000</a></span> | <span class="t">The data mixture is roughly 200 billion tokens across five epochs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=614" target="_blank">00:10:14.000</a></span> | <span class="t">plus a linear cooldown at the end that really allows us to squeeze the best possible performance for the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=620" target="_blank">00:10:20.000</a></span> | <span class="t">And RapidCode V1.5 this time supports 30 programming languages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=625" target="_blank">00:10:25.000</a></span> | <span class="t">and we also added a mixture coming from Stack Exchange,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=629" target="_blank">00:10:29.000</a></span> | <span class="t">posts that are oriented towards developers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=631" target="_blank">00:10:31.000</a></span> | <span class="t">So questions about coding, questions about software engineering, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=636" target="_blank">00:10:36.000</a></span> | <span class="t">So this is the basis of our data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=639" target="_blank">00:10:39.000</a></span> | <span class="t">Now let's go ahead and take a look inside of the dataset that we used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=642" target="_blank">00:10:42.000</a></span> | <span class="t">So we started from the Stack, which is an initiative led by BigCode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=646" target="_blank">00:10:46.000</a></span> | <span class="t">It's a group, you know, under the Hagen-Phase umbrella.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=649" target="_blank">00:10:49.000</a></span> | <span class="t">Very grateful about the work that these people have been doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=653" target="_blank">00:10:53.000</a></span> | <span class="t">Basically, they have built a big pipeline, getting data from GitHub,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=657" target="_blank">00:10:57.000</a></span> | <span class="t">selecting top repositories, cleaning up parts of the data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=660" target="_blank">00:11:00.000</a></span> | <span class="t">and then especially leaving only code that is licensed under permissive licenses,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=665" target="_blank">00:11:05.000</a></span> | <span class="t">such as MIT, BSD, Apache 2, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=669" target="_blank">00:11:09.000</a></span> | <span class="t">Out of this mixture, we selected 30 top languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=673" target="_blank">00:11:13.000</a></span> | <span class="t">And then, really, the key secret ingredient here is how much time we spent working on the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=681" target="_blank">00:11:21.000</a></span> | <span class="t">You must have been hearing this again and again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=683" target="_blank">00:11:23.000</a></span> | <span class="t">And every time you go to an LLM talk, there is a ground stage saying,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=686" target="_blank">00:11:26.000</a></span> | <span class="t">"Hey, you should pay attention about the data quality."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=688" target="_blank">00:11:28.000</a></span> | <span class="t">I'm here to tell you exactly the same once again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=690" target="_blank">00:11:30.000</a></span> | <span class="t">That's probably the most important thing that you could be spending your time on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=694" target="_blank">00:11:34.000</a></span> | <span class="t">especially because the model I'm talking about today is trained from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=698" target="_blank">00:11:38.000</a></span> | <span class="t">So this is not a fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=699" target="_blank">00:11:39.000</a></span> | <span class="t">All the models that we released have been trained from the very first token prepared by us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=704" target="_blank">00:11:44.000</a></span> | <span class="t">So it's extremely important to have high data quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=707" target="_blank">00:11:47.000</a></span> | <span class="t">So we took inspiration from the initial quality pipelines built by Codex, by the Pound paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=714" target="_blank">00:11:54.000</a></span> | <span class="t">and then we applied way more heuristics there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=717" target="_blank">00:11:57.000</a></span> | <span class="t">So we're filtering for code that is being auto-generated, minified, non-parceable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=721" target="_blank">00:12:01.000</a></span> | <span class="t">basically all the code that you wouldn't want your model to recommend back to you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=725" target="_blank">00:12:05.000</a></span> | <span class="t">because it's not something that you would be writing yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=728" target="_blank">00:12:08.000</a></span> | <span class="t">We also removed toxic content, and all this pipeline had been built on Spark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=733" target="_blank">00:12:13.000</a></span> | <span class="t">So I'm trying to encourage you to also think of working on your own models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=737" target="_blank">00:12:17.000</a></span> | <span class="t">because pretty much a lot of the base components are out there available open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=742" target="_blank">00:12:22.000</a></span> | <span class="t">So you could really build the whole pipeline to train and serve an LLM with a lot of open source components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=748" target="_blank">00:12:28.000</a></span> | <span class="t">And as Wix was saying, you have seen this crazy acceleration in the last nine months.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=752" target="_blank">00:12:32.000</a></span> | <span class="t">If you wanted to do this in 2022, good luck with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=756" target="_blank">00:12:36.000</a></span> | <span class="t">It feels like we're a decade ahead compared to last year, so it's pretty amazing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=760" target="_blank">00:12:40.000</a></span> | <span class="t">and I didn't even expect in myself the speed to move this fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=764" target="_blank">00:12:44.000</a></span> | <span class="t">The other insight that we kind of pioneered for our V1 model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=769" target="_blank">00:12:49.000</a></span> | <span class="t">and it turns out to be very powerful also for this new one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=772" target="_blank">00:12:52.000</a></span> | <span class="t">So when we released the V1, a few weeks after, coincidentally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=776" target="_blank">00:12:56.000</a></span> | <span class="t">a very interesting paper has been published called Scaling Data Constraint Language Models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=781" target="_blank">00:13:01.000</a></span> | <span class="t">And I highly recommend it. It's a great read,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=783" target="_blank">00:13:03.000</a></span> | <span class="t">and it's probably one of the most interesting results in LLM, in my opinion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=788" target="_blank">00:13:08.000</a></span> | <span class="t">And this intuition allowed us to basically train the model to completion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=792" target="_blank">00:13:12.000</a></span> | <span class="t">Rather than making trade-offs on the data quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=795" target="_blank">00:13:15.000</a></span> | <span class="t">it allowed us to select a small, high-quality subset of data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=799" target="_blank">00:13:19.000</a></span> | <span class="t">and then repeat it several times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=801" target="_blank">00:13:21.000</a></span> | <span class="t">The key finding of this paper is basically in these two plots.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=803" target="_blank">00:13:23.000</a></span> | <span class="t">I'm going to be sharing the slides so you can go and check the links.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=806" target="_blank">00:13:26.000</a></span> | <span class="t">And the idea is your loss curve, after you repeat data four or five times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=811" target="_blank">00:13:31.000</a></span> | <span class="t">is going to be comparable to training on a novel data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=814" target="_blank">00:13:34.000</a></span> | <span class="t">Okay?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=815" target="_blank">00:13:35.000</a></span> | <span class="t">Now, not only this is very useful because it allowed us to work only on high-quality data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=819" target="_blank">00:13:39.000</a></span> | <span class="t">it also allowed us to work with data that is exclusively released under permissive license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=824" target="_blank">00:13:44.000</a></span> | <span class="t">Therefore, once again, for our 1.5 model, we're going to be releasing it open source,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=830" target="_blank">00:13:50.000</a></span> | <span class="t">and it's going to be released with a commercially permissive license.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=833" target="_blank">00:13:53.000</a></span> | <span class="t">So you can use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=835" target="_blank">00:13:55.000</a></span> | <span class="t">There we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=836" target="_blank">00:13:56.000</a></span> | <span class="t">Just shoot us an email when you use it, because I'm very curious if you're having a good time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=846" target="_blank">00:14:06.000</a></span> | <span class="t">So, details about the model training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=848" target="_blank">00:14:08.000</a></span> | <span class="t">We changed a few things here and there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=850" target="_blank">00:14:10.000</a></span> | <span class="t">It's a slightly larger model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=851" target="_blank">00:14:11.000</a></span> | <span class="t">It's a 3.3B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=852" target="_blank">00:14:12.000</a></span> | <span class="t">It's 4K context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=853" target="_blank">00:14:13.000</a></span> | <span class="t">The old one was a 2K.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=855" target="_blank">00:14:15.000</a></span> | <span class="t">We train a new domain-specific vocabulary, 32K, so a small one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=860" target="_blank">00:14:20.000</a></span> | <span class="t">It helps us to achieve even higher compression on the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=864" target="_blank">00:14:24.000</a></span> | <span class="t">If you've been reading, again, about LLMs, you know that from a simplistic point of view,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=869" target="_blank">00:14:29.000</a></span> | <span class="t">there are data compressors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=870" target="_blank">00:14:30.000</a></span> | <span class="t">Lots of data compressors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=871" target="_blank">00:14:31.000</a></span> | <span class="t">So if your vocabulary allows you to pack even more data on fewer tokens, then you're basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=877" target="_blank">00:14:37.000</a></span> | <span class="t">bringing more signals to the model while you're training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=880" target="_blank">00:14:40.000</a></span> | <span class="t">And with this new vocabulary, we're squeezing a few percent extra, and it's a better vocabulary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=884" target="_blank">00:14:44.000</a></span> | <span class="t">for code compared to what StarCoder or CodeLAM are using today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=889" target="_blank">00:14:49.000</a></span> | <span class="t">We trained on 128 H100 80GB GPUs, which are as rare as, you know, gold at this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=896" target="_blank">00:14:56.000</a></span> | <span class="t">We have been on the Mosaic ML platform for a week, and to our knowledge, this is the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=901" target="_blank">00:15:01.000</a></span> | <span class="t">model officially announced to be trained on H100s and release open source.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=905" target="_blank">00:15:05.000</a></span> | <span class="t">So we're very excited about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=908" target="_blank">00:15:08.000</a></span> | <span class="t">And we follow a list of LLM best practices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=911" target="_blank">00:15:11.000</a></span> | <span class="t">So, of course, we support flash attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=913" target="_blank">00:15:13.000</a></span> | <span class="t">We have group queue retention, which allow us to achieve better inference performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=918" target="_blank">00:15:18.000</a></span> | <span class="t">Alibi position embedding, latest optimizers in the game, and that, you know, is really the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=923" target="_blank">00:15:23.000</a></span> | <span class="t">reason why at the end you will see very exciting numbers that I don't want to spoil right away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=927" target="_blank">00:15:27.000</a></span> | <span class="t">So let's start from the base model, and then there is surprise coming.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=932" target="_blank">00:15:32.000</a></span> | <span class="t">So, this is the evaluation process one on YumiNaval.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=935" target="_blank">00:15:35.000</a></span> | <span class="t">For those of you who never heard about it, YumiNaval is a benchmark release back in 2021</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=940" target="_blank">00:15:40.000</a></span> | <span class="t">by OpenAI, if I recall correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=942" target="_blank">00:15:42.000</a></span> | <span class="t">The format is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=943" target="_blank">00:15:43.000</a></span> | <span class="t">You have a natural language description of a task in English, and then expect the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=948" target="_blank">00:15:48.000</a></span> | <span class="t">to generate a self-contained Python snippet that then is going to be tested with a test harness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=955" target="_blank">00:15:55.000</a></span> | <span class="t">So you generate code, and then you execute it, and you see if the values in output are exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=961" target="_blank">00:16:01.000</a></span> | <span class="t">what you expect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=962" target="_blank">00:16:02.000</a></span> | <span class="t">Now, an interesting evolution in the last few months in the field is we were not content</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=967" target="_blank">00:16:07.000</a></span> | <span class="t">on benchmarking exclusively on Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=970" target="_blank">00:16:10.000</a></span> | <span class="t">So we're also doing that across several different programming languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=974" target="_blank">00:16:14.000</a></span> | <span class="t">And this is coming from the multilingual code EvalArness, again, built by BigCode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=979" target="_blank">00:16:19.000</a></span> | <span class="t">And they also maintain a very interesting leaderboard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=981" target="_blank">00:16:21.000</a></span> | <span class="t">So what they do is they take models across, you know, several companies and several open source</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=986" target="_blank">00:16:26.000</a></span> | <span class="t">contributors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=987" target="_blank">00:16:27.000</a></span> | <span class="t">They run devals themselves, and then they compile this very interesting leaderboard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=991" target="_blank">00:16:31.000</a></span> | <span class="t">So you will find us there, I guess, in a few days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=995" target="_blank">00:16:35.000</a></span> | <span class="t">So from the left column, we have StartCoder3b, which, as of yesterday, was a state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1000" target="_blank">00:16:40.000</a></span> | <span class="t">model at the 3b parameter size across languages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1005" target="_blank">00:16:45.000</a></span> | <span class="t">And today, our WIP 1.5 is basically optimal across every single language that you see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1011" target="_blank">00:16:51.000</a></span> | <span class="t">on the list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1012" target="_blank">00:16:52.000</a></span> | <span class="t">But what gets me excited is not that much of the fact that we are more powerful than StartCoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1017" target="_blank">00:16:57.000</a></span> | <span class="t">which has been released a few months ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1019" target="_blank">00:16:59.000</a></span> | <span class="t">So what got me hyped, you know, when we were training it is that we're very, very close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1023" target="_blank">00:17:03.000</a></span> | <span class="t">to call Llama 7b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1025" target="_blank">00:17:05.000</a></span> | <span class="t">So as a reminder, call Llama 7b is a Llama 2 model from Meta, the 7b version, which has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1031" target="_blank">00:17:11.000</a></span> | <span class="t">been trained on 2 trillion tokens of natural language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1034" target="_blank">00:17:14.000</a></span> | <span class="t">And then it has an additional pre-training phase of 500 billion tokens exclusively on code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1039" target="_blank">00:17:19.000</a></span> | <span class="t">So it's a model that is twice the size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1042" target="_blank">00:17:22.000</a></span> | <span class="t">It's 2.5x more data, way more GPU compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1046" target="_blank">00:17:26.000</a></span> | <span class="t">So you see where I'm going, you know, we're getting very close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1049" target="_blank">00:17:29.000</a></span> | <span class="t">How do we surpass Code Llama?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1052" target="_blank">00:17:32.000</a></span> | <span class="t">Here is the trick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1053" target="_blank">00:17:33.000</a></span> | <span class="t">This is the other model that we have been training in parallel, and this is the REPL tune version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1059" target="_blank">00:17:39.000</a></span> | <span class="t">And it means the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1060" target="_blank">00:17:40.000</a></span> | <span class="t">We further pre-trained it on 200 billion tokens of code, this time coming from our own developers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1067" target="_blank">00:17:47.000</a></span> | <span class="t">So on Replit, when you create a public REPL, it's automatically published under IMT license,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1074" target="_blank">00:17:54.000</a></span> | <span class="t">so we use this code to further pre-train our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1077" target="_blank">00:17:57.000</a></span> | <span class="t">And we extract, again, 30 billion tokens of code, same languages, same data filtering pipeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1083" target="_blank">00:18:03.000</a></span> | <span class="t">to retain only the top quality ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1086" target="_blank">00:18:06.000</a></span> | <span class="t">We do these three epochs, then we do also linear cooldown, and we are using basically the languages</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1092" target="_blank">00:18:12.000</a></span> | <span class="t">that are predominantly popular for Replit users.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1095" target="_blank">00:18:15.000</a></span> | <span class="t">So not the same list as we saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1098" target="_blank">00:18:18.000</a></span> | <span class="t">If you go Replit, I would say 95% of the people are mostly writing Python and JavaScript.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1103" target="_blank">00:18:23.000</a></span> | <span class="t">These are the cool languages of today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1106" target="_blank">00:18:26.000</a></span> | <span class="t">Another key insight is our cutoff for this model is literally a few weeks ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1112" target="_blank">00:18:32.000</a></span> | <span class="t">So if there is a cool new library that everyone is writing software for in the last month,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1118" target="_blank">00:18:38.000</a></span> | <span class="t">our model is going to be capable of generating code that follows that library.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1122" target="_blank">00:18:42.000</a></span> | <span class="t">And we are going to keep, basically, these models up to date so that we can follow the trends,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1127" target="_blank">00:18:47.000</a></span> | <span class="t">and we can make our developers more happy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1131" target="_blank">00:18:51.000</a></span> | <span class="t">Here is the table that I love.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1133" target="_blank">00:18:53.000</a></span> | <span class="t">So we are back to this back-to-back comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1137" target="_blank">00:18:57.000</a></span> | <span class="t">On the very left, we have our base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1140" target="_blank">00:19:00.000</a></span> | <span class="t">We didn't add StartCoder here for the sake of space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1143" target="_blank">00:19:03.000</a></span> | <span class="t">And also, the base model is already topping it on every other language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1148" target="_blank">00:19:08.000</a></span> | <span class="t">so it didn't make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1149" target="_blank">00:19:09.000</a></span> | <span class="t">Now we have Colama in between, and you can see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1152" target="_blank">00:19:12.000</a></span> | <span class="t">We are, on pretty much every language, substantially better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1156" target="_blank">00:19:16.000</a></span> | <span class="t">So we have 36% on the OpenAI U-MiniVault benchmark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1161" target="_blank">00:19:21.000</a></span> | <span class="t">As a reminder, when I was working on PalmCoder, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1166" target="_blank">00:19:26.000</a></span> | <span class="t">that was our Passed-1 result that we published in early 2022.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1172" target="_blank">00:19:32.000</a></span> | <span class="t">That model was at 540 billion tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1175" target="_blank">00:19:35.000</a></span> | <span class="t">so almost 200x larger than this model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1178" target="_blank">00:19:38.000</a></span> | <span class="t">and it achieves exactly the same U-MiniVault Passed-1 performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1181" target="_blank">00:19:41.000</a></span> | <span class="t">Same code DaVinci 001, if you go back to the paper, is getting exactly 36%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1188" target="_blank">00:19:48.000</a></span> | <span class="t">So we were pretty much amazed when this happened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1192" target="_blank">00:19:52.000</a></span> | <span class="t">Now, why do we go through all this struggle of training our models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1196" target="_blank">00:19:56.000</a></span> | <span class="t">Not only because it's cool, you know, we love to do this stuff,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1199" target="_blank">00:19:59.000</a></span> | <span class="t">but there is a rationale behind it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1202" target="_blank">00:20:02.000</a></span> | <span class="t">So we really want to go as fast as possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1206" target="_blank">00:20:06.000</a></span> | <span class="t">with the most powerful small model we could train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1209" target="_blank">00:20:09.000</a></span> | <span class="t">And the reason is, all of our models are actually optimized for inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1214" target="_blank">00:20:14.000</a></span> | <span class="t">rather than for being awesome at benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1217" target="_blank">00:20:17.000</a></span> | <span class="t">The fact that that happens gives us a lot of pride,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1220" target="_blank">00:20:20.000</a></span> | <span class="t">and also makes us feel good when we do a vibe check with the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1223" target="_blank">00:20:23.000</a></span> | <span class="t">and it performs as we expect, or even better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1226" target="_blank">00:20:26.000</a></span> | <span class="t">But it turns out that our key result is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1228" target="_blank">00:20:28.000</a></span> | <span class="t">on a single model, with no batching,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1230" target="_blank">00:20:30.000</a></span> | <span class="t">we're generating above 200 tokens per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1234" target="_blank">00:20:34.000</a></span> | <span class="t">And we tune the architecture for speed in every possible way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1238" target="_blank">00:20:38.000</a></span> | <span class="t">We're training a smaller vocabulary, as I was saying before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1240" target="_blank">00:20:40.000</a></span> | <span class="t">We're using a flash attention with a Triton kernel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1244" target="_blank">00:20:44.000</a></span> | <span class="t">We're using the latest GQA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1246" target="_blank">00:20:46.000</a></span> | <span class="t">So every single aspect is there to make sure that we can go as fast as we can.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1250" target="_blank">00:20:50.000</a></span> | <span class="t">And we optimize, basically, for the usage on the Triton inference server</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1254" target="_blank">00:20:54.000</a></span> | <span class="t">and acceleration framework, such as Stensor RTLLM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1257" target="_blank">00:20:57.000</a></span> | <span class="t">They really squeeze, you know, the last drop for NBita GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1261" target="_blank">00:21:01.000</a></span> | <span class="t">But the other very interesting insight is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1264" target="_blank">00:21:04.000</a></span> | <span class="t">we work very hard also to make the model deployment go much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1269" target="_blank">00:21:09.000</a></span> | <span class="t">So if you ever, you know, had the bad luck to work with Kubernetes in your life,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1273" target="_blank">00:21:13.000</a></span> | <span class="t">you know, you know how painful it can get, you know, to get your pod,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1278" target="_blank">00:21:18.000</a></span> | <span class="t">download all the dependencies, and build it, and yada, yada.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1281" target="_blank">00:21:21.000</a></span> | <span class="t">You know, so the very first time we brought this infrastructure up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1284" target="_blank">00:21:24.000</a></span> | <span class="t">it took 18 minutes to go, you know, from clicking until the model was deployed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1288" target="_blank">00:21:28.000</a></span> | <span class="t">Now, if you want to, you know, adapt to the load that the application is receiving,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1292" target="_blank">00:21:32.000</a></span> | <span class="t">18 minutes, you know, looks like an eternity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1295" target="_blank">00:21:35.000</a></span> | <span class="t">Like, if there is a traffic spike, good luck with that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1298" target="_blank">00:21:38.000</a></span> | <span class="t">So one of our awesome engineers, Bradley, you're going to find him at the booth later today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1302" target="_blank">00:21:42.000</a></span> | <span class="t">brought this number from 18 minutes to just two minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1306" target="_blank">00:21:46.000</a></span> | <span class="t">There is a long list of tricks that he used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1308" target="_blank">00:21:48.000</a></span> | <span class="t">I'm not going to go through them, just talk to Brad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1311" target="_blank">00:21:51.000</a></span> | <span class="t">The cool insight here is the fact, now, whenever we get more load,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1315" target="_blank">00:21:55.000</a></span> | <span class="t">we can react very quickly, and that's how we serve a very large user base.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1319" target="_blank">00:21:59.000</a></span> | <span class="t">So the moment that Amjad announced AI4ALL literally 10 minutes ago,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1323" target="_blank">00:22:03.000</a></span> | <span class="t">we flipped the switch, and our code completion is in front of our users.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1327" target="_blank">00:22:07.000</a></span> | <span class="t">And that's the way we made this happen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1329" target="_blank">00:22:09.000</a></span> | <span class="t">Now, I've been asked several times, guys, why are you losing your model open source?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1335" target="_blank">00:22:15.000</a></span> | <span class="t">You put so much effort. Maybe not. That's an advantage for a company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1339" target="_blank">00:22:19.000</a></span> | <span class="t">It turns out that the moment we did it, we got a lot of adoption.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1344" target="_blank">00:22:24.000</a></span> | <span class="t">And apart from a lot of log, which always feels good,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1347" target="_blank">00:22:27.000</a></span> | <span class="t">and it feels good to chat with other people in AI that are using what we build,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1351" target="_blank">00:22:31.000</a></span> | <span class="t">we also started to get fine-tuned versions, instruct-tuned versions of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1355" target="_blank">00:22:35.000</a></span> | <span class="t">And we have seen a lot of people using our small model deployed in local,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1360" target="_blank">00:22:40.000</a></span> | <span class="t">say with GGML, which goes super fast on Apple Silicon,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1364" target="_blank">00:22:44.000</a></span> | <span class="t">and they built their own custom privacy-aware, like GitHub Copilot Alternative with Rapid V1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1371" target="_blank">00:22:51.000</a></span> | <span class="t">So we expect the same to happen with V1.5 in the next few days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1376" target="_blank">00:22:56.000</a></span> | <span class="t">As we speak also, if you go on again phase, the model is available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1380" target="_blank">00:23:00.000</a></span> | <span class="t">We're working on the readme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1381" target="_blank">00:23:01.000</a></span> | <span class="t">Come to Tolwin Madhava, the boot is the mastermind behind it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1384" target="_blank">00:23:04.000</a></span> | <span class="t">so it's going to tell you every single detail on how to make it run in production.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1387" target="_blank">00:23:07.000</a></span> | <span class="t">And we're going to be here until tonight, so more than happy to play with the model together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1391" target="_blank">00:23:11.000</a></span> | <span class="t">Now, in the last minute that I've left, I want to give you like a teaser</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1395" target="_blank">00:23:15.000</a></span> | <span class="t">of what we're going to be doing in the next few weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1398" target="_blank">00:23:18.000</a></span> | <span class="t">So we're allowing a few very exciting collaborations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1401" target="_blank">00:23:21.000</a></span> | <span class="t">The first one is with Glaive AI, and it's a company that is building synthetic datasets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1406" target="_blank">00:23:26.000</a></span> | <span class="t">And we're working on an IFT version of our model, so an Instruct Fintune version,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1412" target="_blank">00:23:32.000</a></span> | <span class="t">over 210,000 coding instructions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1417" target="_blank">00:23:37.000</a></span> | <span class="t">We're already seeing very exciting results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1420" target="_blank">00:23:40.000</a></span> | <span class="t">We want to triple-check them and, you know, follow our Twitters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1423" target="_blank">00:23:43.000</a></span> | <span class="t">and the moment that we're sure that this is performing as we expect,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1427" target="_blank">00:23:47.000</a></span> | <span class="t">it's going to be out there, and we are going to be able to play with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1430" target="_blank">00:23:50.000</a></span> | <span class="t">Second announcement, we're also collaborating with Morph Labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1434" target="_blank">00:23:54.000</a></span> | <span class="t">I think Jesse is here today, and he's going to run a session later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1438" target="_blank">00:23:58.000</a></span> | <span class="t">explaining you exactly what this new format does.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1441" target="_blank">00:24:01.000</a></span> | <span class="t">I'm going to give you a teaser, and then, you know, go to Jesse's talk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1443" target="_blank">00:24:03.000</a></span> | <span class="t">and he's going to explain you all the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1445" target="_blank">00:24:05.000</a></span> | <span class="t">So we are design partners on the FIST format, which is fill in the syntax tree.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1451" target="_blank">00:24:11.000</a></span> | <span class="t">You might have heard of fill in the middle, this concept where you can take your file,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1455" target="_blank">00:24:15.000</a></span> | <span class="t">split it in half, and then basically if you're writing code in between,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1459" target="_blank">00:24:19.000</a></span> | <span class="t">you can tell the LLM that the top of the file is your prefix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1463" target="_blank">00:24:23.000</a></span> | <span class="t">the bottom of the file is your suffix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1465" target="_blank">00:24:25.000</a></span> | <span class="t">and you give this context to the model so that it knows which part it should fill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1469" target="_blank">00:24:29.000</a></span> | <span class="t">Now, we found that this format is even more powerful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1472" target="_blank">00:24:32.000</a></span> | <span class="t">is aware of the abstract syntax tree underlying the source code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1476" target="_blank">00:24:36.000</a></span> | <span class="t">We're seeing very promising results already, and again, this will be out, you know,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1481" target="_blank">00:24:41.000</a></span> | <span class="t">in just a matter of like a few days or weeks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1483" target="_blank">00:24:43.000</a></span> | <span class="t">Last thing, we have collaborations with the Perplexity AI guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1487" target="_blank">00:24:47.000</a></span> | <span class="t">You might have used their labs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1489" target="_blank">00:24:49.000</a></span> | <span class="t">So it's a place where the host models incredibly fast,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1493" target="_blank">00:24:53.000</a></span> | <span class="t">and the Rapid B1.5 will appear there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1496" target="_blank">00:24:56.000</a></span> | <span class="t">and you can start to play with it and get a vibe check by tonight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1499" target="_blank">00:24:59.000</a></span> | <span class="t">Thanks, everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1500" target="_blank">00:25:00.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1501" target="_blank">00:25:01.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1502" target="_blank">00:25:02.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1503" target="_blank">00:25:03.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1504" target="_blank">00:25:04.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1505" target="_blank">00:25:05.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1506" target="_blank">00:25:06.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1507" target="_blank">00:25:07.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1508" target="_blank">00:25:08.000</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ju73sWVtvU0&t=1508" target="_blank">00:25:08.000</a></span> | <span class="t">We'll see you next time.</span></div></div></body></html>