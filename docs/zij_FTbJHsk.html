<html><head><title>Foundations of Deep Learning (Hugo Larochelle, Twitter)</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Foundations of Deep Learning (Hugo Larochelle, Twitter)</h2><a href="https://www.youtube.com/watch?v=zij_FTbJHsk"><img src="https://i.ytimg.com/vi_webp/zij_FTbJHsk/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=0">0:0</a> Intro<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=60">1:0</a> FOUNDATIONS OF DEEP LEARNING<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=585">9:45</a> CAPACITY OF NEURAL NETWORK<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=662">11:2</a> MACHINE LEARNING<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=945">15:45</a> LOSS FUNCTION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1149">19:9</a> BACKPROPAGATION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1499">24:59</a> ACTIVATION FUNCTION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1650">27:30</a> FLOW GRAPH<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1778">29:38</a> REGULARIZATION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1824">30:24</a> INITIALIZATION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2006">33:26</a> MODEL SELECTION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2193">36:33</a> KNOWING WHEN TO STOP<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2287">38:7</a> OTHER TRICKS OF THE TRADE<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2547">42:27</a> GRADIENT CHECKING<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2616">43:36</a> DEBUGGING ON SMALL DATASET<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3054">50:54</a> DROPOUT<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3310">55:10</a> BATCH NORMALIZATION<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3514">58:34</a> UNSUPERVISED PRE-TRAINING<br><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3517">58:37</a> NEURAL NETWORK ONLINE COURSE<br><br><div style="text-align: left;"><a href="./zij_FTbJHsk.html">Whisper Transcript</a> | <a href="./transcript_zij_FTbJHsk.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">That's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=0" target="_blank">00:00:00.840</a></span> | <span class="t">All right, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1" target="_blank">00:00:01.440</a></span> | <span class="t">So yeah, so I was asked to give this presentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=4" target="_blank">00:00:04.880</a></span> | <span class="t">on the foundations of deep learning, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=7" target="_blank">00:00:07.080</a></span> | <span class="t">is mostly going over basic feedforward neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=11" target="_blank">00:00:11.120</a></span> | <span class="t">and motivating a little bit deep learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=13" target="_blank">00:00:13.640</a></span> | <span class="t">and some of the more recent developments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=15" target="_blank">00:00:15.960</a></span> | <span class="t">and some of the topics that you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=17" target="_blank">00:00:17.480</a></span> | <span class="t">see across the next two days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=20" target="_blank">00:00:20.120</a></span> | <span class="t">So as Andrew mentioned, I have just an hour.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=26" target="_blank">00:00:26.080</a></span> | <span class="t">So I'm going to go fairly quickly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=27" target="_blank">00:00:27.520</a></span> | <span class="t">on a lot of these things, which I think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=29" target="_blank">00:00:29.160</a></span> | <span class="t">would mostly be fine if you're familiar enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=31" target="_blank">00:00:31.600</a></span> | <span class="t">with some machine learning and a little bit about neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=35" target="_blank">00:00:35.320</a></span> | <span class="t">But if you'd like to go into some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=36" target="_blank">00:00:36.800</a></span> | <span class="t">of the more specific details, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=38" target="_blank">00:00:38.280</a></span> | <span class="t">can go check out my online lectures on YouTube.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=41" target="_blank">00:00:41.320</a></span> | <span class="t">It's now taught by a much younger version of myself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=44" target="_blank">00:00:44.400</a></span> | <span class="t">And so just search for Hugo Larochelle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=47" target="_blank">00:00:47.800</a></span> | <span class="t">And I am not the guy doing a bunch of skateboarding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=50" target="_blank">00:00:50.640</a></span> | <span class="t">I'm the geek teaching about neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=52" target="_blank">00:00:52.920</a></span> | <span class="t">So go check those out if you want more details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=56" target="_blank">00:00:56.560</a></span> | <span class="t">But so what I'll cover today is--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=58" target="_blank">00:00:58.880</a></span> | <span class="t">I'll start with just describing and laying out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=62" target="_blank">00:01:02.280</a></span> | <span class="t">the notation on feedforward neural networks, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=64" target="_blank">00:01:04.920</a></span> | <span class="t">is, models that take an input vector x--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=67" target="_blank">00:01:07.280</a></span> | <span class="t">that might be an image or some text--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=69" target="_blank">00:01:09.320</a></span> | <span class="t">and produces an output f of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=71" target="_blank">00:01:11.360</a></span> | <span class="t">So I'll just describe forward propagation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=73" target="_blank">00:01:13.120</a></span> | <span class="t">and the different types of units and the type of functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=76" target="_blank">00:01:16.040</a></span> | <span class="t">we can represent with those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=77" target="_blank">00:01:17.680</a></span> | <span class="t">And then I'll talk about how we actually train neural nets,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=80" target="_blank">00:01:20.720</a></span> | <span class="t">describing things like loss functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=82" target="_blank">00:01:22.600</a></span> | <span class="t">backpropagation that allows us to get a gradient for training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=86" target="_blank">00:01:26.160</a></span> | <span class="t">with stochastic gradient descent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=87" target="_blank">00:01:27.700</a></span> | <span class="t">and mention a few tricks of the trade,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=89" target="_blank">00:01:29.840</a></span> | <span class="t">so some of the things we do in practice to successfully train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=92" target="_blank">00:01:32.720</a></span> | <span class="t">neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=93" target="_blank">00:01:33.680</a></span> | <span class="t">And then I'll end by talking about some developments that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=97" target="_blank">00:01:37.680</a></span> | <span class="t">are specifically useful in the context of deep learning, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=101" target="_blank">00:01:41.240</a></span> | <span class="t">is, neural networks with several hidden layers that came out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=105" target="_blank">00:01:45.220</a></span> | <span class="t">at the very--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=106" target="_blank">00:01:46.920</a></span> | <span class="t">after the beginning of deep learning, say, in 2006.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=109" target="_blank">00:01:49.880</a></span> | <span class="t">That is, things like dropout, batch normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=112" target="_blank">00:01:52.200</a></span> | <span class="t">and if I have some time, unsupervised pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=115" target="_blank">00:01:55.800</a></span> | <span class="t">So let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=117" target="_blank">00:01:57.560</a></span> | <span class="t">And just talk about, assuming we have some neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=120" target="_blank">00:02:00.400</a></span> | <span class="t">how do they actually function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=121" target="_blank">00:02:01.680</a></span> | <span class="t">How do they make predictions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=124" target="_blank">00:02:04.280</a></span> | <span class="t">So let me lay down the notation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=126" target="_blank">00:02:06.800</a></span> | <span class="t">So a multilayer feedforward neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=130" target="_blank">00:02:10.240</a></span> | <span class="t">is a model that takes as input some vector x, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=134" target="_blank">00:02:14.200</a></span> | <span class="t">I'm representing here with a different node</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=136" target="_blank">00:02:16.400</a></span> | <span class="t">for each of the dimensions in my input vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=139" target="_blank">00:02:19.480</a></span> | <span class="t">So each dimension is essentially a unit in that neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=143" target="_blank">00:02:23.640</a></span> | <span class="t">And then it eventually produces, at its output layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=147" target="_blank">00:02:27.080</a></span> | <span class="t">an output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=148" target="_blank">00:02:28.680</a></span> | <span class="t">And we'll focus on classification mostly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=151" target="_blank">00:02:31.160</a></span> | <span class="t">So you'd have multiple units here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=153" target="_blank">00:02:33.280</a></span> | <span class="t">And each unit would correspond to one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=155" target="_blank">00:02:35.160</a></span> | <span class="t">of the potential classes in which we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=157" target="_blank">00:02:37.280</a></span> | <span class="t">want to classify our input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=158" target="_blank">00:02:38.680</a></span> | <span class="t">So if we're identifying digits in handwritten character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=162" target="_blank">00:02:42.960</a></span> | <span class="t">images, and say we're focusing on digits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=166" target="_blank">00:02:46.120</a></span> | <span class="t">you'd have 10 digits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=167" target="_blank">00:02:47.120</a></span> | <span class="t">So you would have a sort of 0 from 0 to 9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=170" target="_blank">00:02:50.120</a></span> | <span class="t">So you'd have 10 output units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=172" target="_blank">00:02:52.420</a></span> | <span class="t">And to produce an output, the neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=174" target="_blank">00:02:54.820</a></span> | <span class="t">will go through a series of hidden layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=178" target="_blank">00:02:58.360</a></span> | <span class="t">And those will be essentially the components</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=181" target="_blank">00:03:01.000</a></span> | <span class="t">that introduce non-linearity that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=182" target="_blank">00:03:02.380</a></span> | <span class="t">allows us to capture and perform very sophisticated types</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=186" target="_blank">00:03:06.340</a></span> | <span class="t">of classification functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=188" target="_blank">00:03:08.600</a></span> | <span class="t">So if we have L hidden layers, the way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=191" target="_blank">00:03:11.500</a></span> | <span class="t">we compute all the layers in our neural net is as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=196" target="_blank">00:03:16.040</a></span> | <span class="t">We first start by computing what I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=197" target="_blank">00:03:17.940</a></span> | <span class="t">going to call a pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=200" target="_blank">00:03:20.040</a></span> | <span class="t">I'm going to know that A. And I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=202" target="_blank">00:03:22.200</a></span> | <span class="t">going to index the layers by k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=203" target="_blank">00:03:23.920</a></span> | <span class="t">So A k is just the pre-activation at layer k.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=208" target="_blank">00:03:28.080</a></span> | <span class="t">And that is only simply going to be a linear transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=212" target="_blank">00:03:32.320</a></span> | <span class="t">of the previous layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=214" target="_blank">00:03:34.080</a></span> | <span class="t">So I'm going to note h k as the activation on the layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=218" target="_blank">00:03:38.320</a></span> | <span class="t">And by default, I'll assume that layer 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=221" target="_blank">00:03:41.200</a></span> | <span class="t">is going to be the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=223" target="_blank">00:03:43.080</a></span> | <span class="t">And so using that notation, the pre-activation at layer k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=226" target="_blank">00:03:46.960</a></span> | <span class="t">is going to correspond to taking the activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=229" target="_blank">00:03:49.720</a></span> | <span class="t">at the previous layer, k minus 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=232" target="_blank">00:03:52.200</a></span> | <span class="t">multiplying it by a matrix, Wk.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=234" target="_blank">00:03:54.680</a></span> | <span class="t">Those are the parameters of the layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=237" target="_blank">00:03:57.640</a></span> | <span class="t">Those essentially corresponds to the connections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=240" target="_blank">00:04:00.520</a></span> | <span class="t">between the units between adjacent layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=243" target="_blank">00:04:03.080</a></span> | <span class="t">And I'm going to add a bias vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=245" target="_blank">00:04:05.120</a></span> | <span class="t">That's another parameter in my layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=247" target="_blank">00:04:07.360</a></span> | <span class="t">So that gives me the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=249" target="_blank">00:04:09.640</a></span> | <span class="t">And then next, I'm going to get a hidden layer activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=252" target="_blank">00:04:12.280</a></span> | <span class="t">by applying an activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=254" target="_blank">00:04:14.680</a></span> | <span class="t">This will introduce some non-linearity in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=257" target="_blank">00:04:17.760</a></span> | <span class="t">So I'm going to call that function g.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=259" target="_blank">00:04:19.320</a></span> | <span class="t">And we'll go over a few choices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=262" target="_blank">00:04:22.000</a></span> | <span class="t">So we have four common choices for the activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=266" target="_blank">00:04:26.160</a></span> | <span class="t">And so I do this from layer 1 to layer L.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=269" target="_blank">00:04:29.120</a></span> | <span class="t">And when it comes to the output layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=271" target="_blank">00:04:31.320</a></span> | <span class="t">I'll also compute a pre-activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=273" target="_blank">00:04:33.480</a></span> | <span class="t">by performing a linear transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=276" target="_blank">00:04:36.000</a></span> | <span class="t">But then I'll usually apply a different activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=278" target="_blank">00:04:38.600</a></span> | <span class="t">depending on the problem I'm trying to solve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=281" target="_blank">00:04:41.960</a></span> | <span class="t">So having said that, let's go to some of the choices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=287" target="_blank">00:04:47.000</a></span> | <span class="t">for the activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=288" target="_blank">00:04:48.260</a></span> | <span class="t">So some of the activation functions you'll see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=290" target="_blank">00:04:50.800</a></span> | <span class="t">One common one is this sigmoid activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=293" target="_blank">00:04:53.680</a></span> | <span class="t">It's this function here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=294" target="_blank">00:04:54.880</a></span> | <span class="t">It's just 1 divided by 1 plus the exponential</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=298" target="_blank">00:04:58.680</a></span> | <span class="t">of minus the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=301" target="_blank">00:05:01.360</a></span> | <span class="t">The shape of this function, you can focus on that, is this here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=304" target="_blank">00:05:04.720</a></span> | <span class="t">It takes the pre-activation, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=306" target="_blank">00:05:06.160</a></span> | <span class="t">can vary from minus infinite to plus infinite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=308" target="_blank">00:05:08.520</a></span> | <span class="t">And it squashes this between 0 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=311" target="_blank">00:05:11.680</a></span> | <span class="t">So it's bounded by below and above, below by 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=315" target="_blank">00:05:15.720</a></span> | <span class="t">and above by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=317" target="_blank">00:05:17.480</a></span> | <span class="t">So it's a function that saturates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=319" target="_blank">00:05:19.600</a></span> | <span class="t">if you have very large magnitude positive or negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=324" target="_blank">00:05:24.440</a></span> | <span class="t">pre-activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=326" target="_blank">00:05:26.760</a></span> | <span class="t">Another common choice is the hyperbolic tangent or tanh</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=329" target="_blank">00:05:29.880</a></span> | <span class="t">activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=331" target="_blank">00:05:31.680</a></span> | <span class="t">This picture here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=332" target="_blank">00:05:32.600</a></span> | <span class="t">So it squashes everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=333" target="_blank">00:05:33.720</a></span> | <span class="t">But instead of being between 0 and 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=335" target="_blank">00:05:35.840</a></span> | <span class="t">it's between minus 1 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=338" target="_blank">00:05:38.720</a></span> | <span class="t">And one that's become quite popular in neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=342" target="_blank">00:05:42.600</a></span> | <span class="t">is what's known as the rectified linear activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=346" target="_blank">00:05:46.120</a></span> | <span class="t">Or in papers, you will see the ReLU unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=350" target="_blank">00:05:50.400</a></span> | <span class="t">that refers to the use of this activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=354" target="_blank">00:05:54.880</a></span> | <span class="t">So this one is different from the others</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=356" target="_blank">00:05:56.560</a></span> | <span class="t">in that it's not bounded above, but it is bounded below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=360" target="_blank">00:06:00.340</a></span> | <span class="t">And it will output 0's exactly if the pre-activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=365" target="_blank">00:06:05.640</a></span> | <span class="t">is negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=368" target="_blank">00:06:08.040</a></span> | <span class="t">So those are the choices of activation functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=370" target="_blank">00:06:10.120</a></span> | <span class="t">for the hidden layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=372" target="_blank">00:06:12.060</a></span> | <span class="t">And for the output layer, if we're performing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=373" target="_blank">00:06:13.960</a></span> | <span class="t">classification, as I said, in our output layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=376" target="_blank">00:06:16.760</a></span> | <span class="t">we will have as many units as there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=378" target="_blank">00:06:18.640</a></span> | <span class="t">are classes in which an input could belong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=381" target="_blank">00:06:21.440</a></span> | <span class="t">And what we'd like is potentially--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=384" target="_blank">00:06:24.720</a></span> | <span class="t">and what we often do is interpret each unit's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=387" target="_blank">00:06:27.600</a></span> | <span class="t">activation as the probability, according</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=390" target="_blank">00:06:30.320</a></span> | <span class="t">to the neural network, that the input belongs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=393" target="_blank">00:06:33.600</a></span> | <span class="t">to the corresponding class, that its label y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=396" target="_blank">00:06:36.480</a></span> | <span class="t">is the corresponding class C. So C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=399" target="_blank">00:06:39.600</a></span> | <span class="t">would be like the index of that unit in the output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=403" target="_blank">00:06:43.120</a></span> | <span class="t">So we need an activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=404" target="_blank">00:06:44.520</a></span> | <span class="t">that produces probabilities, produces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=406" target="_blank">00:06:46.840</a></span> | <span class="t">a multinomial distribution over all the different classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=410" target="_blank">00:06:50.080</a></span> | <span class="t">And the activation function we use for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=412" target="_blank">00:06:52.120</a></span> | <span class="t">is known as the softmax activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=415" target="_blank">00:06:55.280</a></span> | <span class="t">It is simply as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=417" target="_blank">00:06:57.400</a></span> | <span class="t">You take your pre-activations, and you exponentiate them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=420" target="_blank">00:07:00.440</a></span> | <span class="t">So that's going to give us positive numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=422" target="_blank">00:07:02.800</a></span> | <span class="t">And then we divide each of the exponentiated pre-activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=426" target="_blank">00:07:06.440</a></span> | <span class="t">by the sum of all the exponentiated pre-activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=431" target="_blank">00:07:11.200</a></span> | <span class="t">So because I'm normalizing this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=433" target="_blank">00:07:13.040</a></span> | <span class="t">it means that all my values in my output layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=436" target="_blank">00:07:16.640</a></span> | <span class="t">are going to sum to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=437" target="_blank">00:07:17.920</a></span> | <span class="t">And they're positive because I took the exponential.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=440" target="_blank">00:07:20.080</a></span> | <span class="t">So I can interpret that as a multinomial distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=443" target="_blank">00:07:23.080</a></span> | <span class="t">over the choice of all the C different classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=446" target="_blank">00:07:26.760</a></span> | <span class="t">So that's what I'll use as the activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=449" target="_blank">00:07:29.040</a></span> | <span class="t">at the output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=452" target="_blank">00:07:32.160</a></span> | <span class="t">And now, beyond the math in terms of conceptually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=455" target="_blank">00:07:35.100</a></span> | <span class="t">and also in the way we're going to program neural networks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=458" target="_blank">00:07:38.480</a></span> | <span class="t">often what we'll do is that all these different operations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=461" target="_blank">00:07:41.020</a></span> | <span class="t">the linear transformations, the different types of activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=463" target="_blank">00:07:43.520</a></span> | <span class="t">functions, we'll essentially implement all of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=467" target="_blank">00:07:47.440</a></span> | <span class="t">as an object, an object that take arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=472" target="_blank">00:07:52.200</a></span> | <span class="t">And the arguments would essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=473" target="_blank">00:07:53.660</a></span> | <span class="t">be what other things are being combined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=475" target="_blank">00:07:55.400</a></span> | <span class="t">to produce the next value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=477" target="_blank">00:07:57.600</a></span> | <span class="t">So for instance, we would have an object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=479" target="_blank">00:07:59.520</a></span> | <span class="t">that might correspond to the computation of pre-activation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=482" target="_blank">00:08:02.760</a></span> | <span class="t">which would take as argument what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=484" target="_blank">00:08:04.920</a></span> | <span class="t">is the weight matrix and the bias vector for that layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=488" target="_blank">00:08:08.280</a></span> | <span class="t">and take some layer to transform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=490" target="_blank">00:08:10.920</a></span> | <span class="t">And this object would compute its value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=493" target="_blank">00:08:13.560</a></span> | <span class="t">by applying the linear activation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=495" target="_blank">00:08:15.840</a></span> | <span class="t">the linear transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=497" target="_blank">00:08:17.120</a></span> | <span class="t">And then we might have objects that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=498" target="_blank">00:08:18.620</a></span> | <span class="t">correspond to specific activation functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=501" target="_blank">00:08:21.880</a></span> | <span class="t">so like a sigmoid object or a tanh object or a ReLU object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=505" target="_blank">00:08:25.360</a></span> | <span class="t">And we just combine these objects together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=507" target="_blank">00:08:27.200</a></span> | <span class="t">chain them into what ends up being a graph, which I refer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=510" target="_blank">00:08:30.960</a></span> | <span class="t">to as a flow graph, that represents the computation done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=514" target="_blank">00:08:34.680</a></span> | <span class="t">when you do a forward pass in your neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=517" target="_blank">00:08:37.400</a></span> | <span class="t">up until you reach the output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=519" target="_blank">00:08:39.520</a></span> | <span class="t">So I mention it now because you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=521" target="_blank">00:08:41.520</a></span> | <span class="t">see the different softwares that we presented over the weekend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=525" target="_blank">00:08:45.840</a></span> | <span class="t">will essentially exploit some of that representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=530" target="_blank">00:08:50.000</a></span> | <span class="t">of the computation in neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=531" target="_blank">00:08:51.600</a></span> | <span class="t">It will also be handy for computing gradients, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=533" target="_blank">00:08:53.880</a></span> | <span class="t">I'll talk about in a few minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=537" target="_blank">00:08:57.720</a></span> | <span class="t">And so that's how we perform predictions in neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=542" target="_blank">00:09:02.120</a></span> | <span class="t">So we get an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=543" target="_blank">00:09:03.560</a></span> | <span class="t">We eventually reach an output layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=545" target="_blank">00:09:05.120</a></span> | <span class="t">that gives us a distribution over classes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=546" target="_blank">00:09:06.960</a></span> | <span class="t">if we're performing classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=548" target="_blank">00:09:08.680</a></span> | <span class="t">If I want to actually classify, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=550" target="_blank">00:09:10.360</a></span> | <span class="t">would just assign the class corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=553" target="_blank">00:09:13.400</a></span> | <span class="t">to the unit that has the highest activation, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=556" target="_blank">00:09:16.440</a></span> | <span class="t">would correspond to classifying to the class that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=559" target="_blank">00:09:19.160</a></span> | <span class="t">has the highest probability according to the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=561" target="_blank">00:09:21.720</a></span> | <span class="t">But then you might ask the question, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=566" target="_blank">00:09:26.200</a></span> | <span class="t">what kind of problems can we solve with neural networks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=569" target="_blank">00:09:29.240</a></span> | <span class="t">Or more technically, what kind of functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=571" target="_blank">00:09:31.440</a></span> | <span class="t">can we represent mapping from some input x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=574" target="_blank">00:09:34.240</a></span> | <span class="t">into some arbitrary output?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=576" target="_blank">00:09:36.520</a></span> | <span class="t">And so if you go look at my videos,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=579" target="_blank">00:09:39.480</a></span> | <span class="t">I try to give more intuition as to why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=581" target="_blank">00:09:41.920</a></span> | <span class="t">we have this result here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=583" target="_blank">00:09:43.160</a></span> | <span class="t">But essentially, if we have a single hidden layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=585" target="_blank">00:09:45.960</a></span> | <span class="t">neural network, it's been shown that with a linear output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=588" target="_blank">00:09:48.680</a></span> | <span class="t">we can approximate any continuous function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=591" target="_blank">00:09:51.040</a></span> | <span class="t">arbitrarily well as long as we have enough hidden units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=594" target="_blank">00:09:54.560</a></span> | <span class="t">So that is, there's a value for these biases and these weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=597" target="_blank">00:09:57.360</a></span> | <span class="t">such that any continuous function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=599" target="_blank">00:09:59.240</a></span> | <span class="t">I can actually represent it as well as I want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=601" target="_blank">00:10:01.720</a></span> | <span class="t">I just need to add enough hidden units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=604" target="_blank">00:10:04.760</a></span> | <span class="t">So this result applies if you use activation functions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=607" target="_blank">00:10:07.560</a></span> | <span class="t">non-linear activation functions like sigmoid and tanh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=611" target="_blank">00:10:11.360</a></span> | <span class="t">So as I said in my video, if you want a bit more intuition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=614" target="_blank">00:10:14.000</a></span> | <span class="t">as to why that would be, you can go check that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=618" target="_blank">00:10:18.040</a></span> | <span class="t">But that's a really nice result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=619" target="_blank">00:10:19.920</a></span> | <span class="t">It means that by focusing on this family of machine learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=623" target="_blank">00:10:23.880</a></span> | <span class="t">models that are neural networks, I can pretty much potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=627" target="_blank">00:10:27.720</a></span> | <span class="t">represent any kind of classification function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=630" target="_blank">00:10:30.640</a></span> | <span class="t">However, this result does not tell us</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=632" target="_blank">00:10:32.560</a></span> | <span class="t">how do we actually find the weights and the bias values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=635" target="_blank">00:10:35.680</a></span> | <span class="t">such that I can represent a given function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=638" target="_blank">00:10:38.040</a></span> | <span class="t">It doesn't essentially tell us how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=639" target="_blank">00:10:39.560</a></span> | <span class="t">do we train a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=641" target="_blank">00:10:41.640</a></span> | <span class="t">And so that's what we'll discuss next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=644" target="_blank">00:10:44.800</a></span> | <span class="t">So let's talk about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=645" target="_blank">00:10:45.960</a></span> | <span class="t">How do we actually, from a data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=648" target="_blank">00:10:48.280</a></span> | <span class="t">train a neural network to perform good classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=651" target="_blank">00:10:51.560</a></span> | <span class="t">for that problem?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=654" target="_blank">00:10:54.280</a></span> | <span class="t">So what we'll typically do is use a framework that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=658" target="_blank">00:10:58.100</a></span> | <span class="t">very generic in machine learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=659" target="_blank">00:10:59.900</a></span> | <span class="t">known as empirical risk minimization or structural risk</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=663" target="_blank">00:11:03.060</a></span> | <span class="t">minimization if you're using regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=665" target="_blank">00:11:05.660</a></span> | <span class="t">So this framework essentially transforms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=668" target="_blank">00:11:08.940</a></span> | <span class="t">a problem of learning as a problem of optimizing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=672" target="_blank">00:11:12.860</a></span> | <span class="t">So what we'll do is that we'll first choose a loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=676" target="_blank">00:11:16.380</a></span> | <span class="t">that I'm noting as L. And the loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=679" target="_blank">00:11:19.540</a></span> | <span class="t">it compares the output of my model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=682" target="_blank">00:11:22.100</a></span> | <span class="t">so the output layer of my neural network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=683" target="_blank">00:11:23.980</a></span> | <span class="t">with the actual target.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=685" target="_blank">00:11:25.660</a></span> | <span class="t">So I'm indexing with an exponent here with t</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=688" target="_blank">00:11:28.540</a></span> | <span class="t">to essentially as the index over all my different examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=692" target="_blank">00:11:32.660</a></span> | <span class="t">in my training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=694" target="_blank">00:11:34.860</a></span> | <span class="t">And so my loss function will tell me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=696" target="_blank">00:11:36.940</a></span> | <span class="t">is this output good or bad given that the label is actually y?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=702" target="_blank">00:11:42.660</a></span> | <span class="t">And what I'll do, I'll also define a regularizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=707" target="_blank">00:11:47.760</a></span> | <span class="t">So theta here is--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=709" target="_blank">00:11:49.620</a></span> | <span class="t">you can think of it as just a concatenation of all my biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=712" target="_blank">00:11:52.860</a></span> | <span class="t">and all of my weights in my neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=714" target="_blank">00:11:54.460</a></span> | <span class="t">So those are all the parameters of my neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=718" target="_blank">00:11:58.200</a></span> | <span class="t">And the regularizer will essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=720" target="_blank">00:12:00.020</a></span> | <span class="t">penalize certain values of these weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=723" target="_blank">00:12:03.220</a></span> | <span class="t">So as I'll talk more specifically later on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=725" target="_blank">00:12:05.980</a></span> | <span class="t">for instance, you might want to have your weights not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=728" target="_blank">00:12:08.500</a></span> | <span class="t">be too far from 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=729" target="_blank">00:12:09.940</a></span> | <span class="t">That's a frequent intuition that we implement with regularizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=734" target="_blank">00:12:14.200</a></span> | <span class="t">And so the optimization problem that we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=736" target="_blank">00:12:16.660</a></span> | <span class="t">try to solve when learning is to minimize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=739" target="_blank">00:12:19.740</a></span> | <span class="t">the average loss of my neural network over my training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=743" target="_blank">00:12:23.900</a></span> | <span class="t">examples, so summing over all training examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=746" target="_blank">00:12:26.100</a></span> | <span class="t">I have capital T examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=748" target="_blank">00:12:28.860</a></span> | <span class="t">Plus some weight here that's known as the weight decay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=753" target="_blank">00:12:33.340</a></span> | <span class="t">some hyperparameter lambda, times my regularizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=757" target="_blank">00:12:37.100</a></span> | <span class="t">So in other words, I'm going to try to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=759" target="_blank">00:12:39.280</a></span> | <span class="t">my loss on my training set the smallest possible over all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=763" target="_blank">00:12:43.140</a></span> | <span class="t">the training example and also try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=765" target="_blank">00:12:45.260</a></span> | <span class="t">to satisfy my regularizer as much as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=768" target="_blank">00:12:48.660</a></span> | <span class="t">And so now we have this optimization problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=771" target="_blank">00:12:51.420</a></span> | <span class="t">And learning will just correspond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=773" target="_blank">00:12:53.140</a></span> | <span class="t">to trying to solve this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=775" target="_blank">00:12:55.380</a></span> | <span class="t">So finding this arg min here for over my weights and my biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=780" target="_blank">00:13:00.940</a></span> | <span class="t">And if I want to do this, I can just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=782" target="_blank">00:13:02.460</a></span> | <span class="t">invoke some optimization procedure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=785" target="_blank">00:13:05.100</a></span> | <span class="t">from the optimization community.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=788" target="_blank">00:13:08.860</a></span> | <span class="t">And the one algorithm that you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=790" target="_blank">00:13:10.340</a></span> | <span class="t">see constantly in deep learning is stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=794" target="_blank">00:13:14.420</a></span> | <span class="t">This is the optimization algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=796" target="_blank">00:13:16.220</a></span> | <span class="t">that we'll often use for training neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=799" target="_blank">00:13:19.780</a></span> | <span class="t">So SGD, stochastic gradient descent, functions as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=803" target="_blank">00:13:23.620</a></span> | <span class="t">You first initialize all of your parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=806" target="_blank">00:13:26.580</a></span> | <span class="t">That is finding initial values for all my weight matrices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=809" target="_blank">00:13:29.740</a></span> | <span class="t">and all of my biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=812" target="_blank">00:13:32.060</a></span> | <span class="t">And then for a certain number of epochs--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=814" target="_blank">00:13:34.140</a></span> | <span class="t">so an epoch will be a full pass over all my examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=817" target="_blank">00:13:37.740</a></span> | <span class="t">That's what I'll call an epoch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=820" target="_blank">00:13:40.100</a></span> | <span class="t">So for a certain number of full iterations over my training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=824" target="_blank">00:13:44.620</a></span> | <span class="t">set, I'll draw each training example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=827" target="_blank">00:13:47.900</a></span> | <span class="t">So I pair x, input x, target y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=831" target="_blank">00:13:51.620</a></span> | <span class="t">And then I'll compute what is the gradient of my loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=835" target="_blank">00:13:55.900</a></span> | <span class="t">with respect to my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=838" target="_blank">00:13:58.580</a></span> | <span class="t">All of my parameters, all my weights, and all my biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=841" target="_blank">00:14:01.040</a></span> | <span class="t">This is what this notation here--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=843" target="_blank">00:14:03.180</a></span> | <span class="t">so nabla for the gradient of the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=846" target="_blank">00:14:06.740</a></span> | <span class="t">And here I'm indexing with respect to which parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=850" target="_blank">00:14:10.300</a></span> | <span class="t">I want the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=852" target="_blank">00:14:12.060</a></span> | <span class="t">So I'm going to compute what is the gradient of my loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=854" target="_blank">00:14:14.700</a></span> | <span class="t">function with respect to my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=857" target="_blank">00:14:17.240</a></span> | <span class="t">And plus lambda times the gradient of my regularizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=860" target="_blank">00:14:20.220</a></span> | <span class="t">as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=861" target="_blank">00:14:21.160</a></span> | <span class="t">And then I'm going to get a direction in which I should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=863" target="_blank">00:14:23.500</a></span> | <span class="t">move my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=865" target="_blank">00:14:25.220</a></span> | <span class="t">Since the gradient tells me how to increase the loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=868" target="_blank">00:14:28.620</a></span> | <span class="t">I want to go in the opposite direction and decrease it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=871" target="_blank">00:14:31.060</a></span> | <span class="t">So my direction will be the opposite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=872" target="_blank">00:14:32.820</a></span> | <span class="t">So that's why I have a minus here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=875" target="_blank">00:14:35.540</a></span> | <span class="t">And so this delta is going to be the direction in which I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=878" target="_blank">00:14:38.140</a></span> | <span class="t">move my parameters by taking a step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=880" target="_blank">00:14:40.780</a></span> | <span class="t">And the step is just a step size alpha,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=883" target="_blank">00:14:43.940</a></span> | <span class="t">which is often referred to as a learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=886" target="_blank">00:14:46.500</a></span> | <span class="t">times my direction, which I just add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=889" target="_blank">00:14:49.300</a></span> | <span class="t">to my current values of my parameters, my biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=892" target="_blank">00:14:52.300</a></span> | <span class="t">and my weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=893" target="_blank">00:14:53.360</a></span> | <span class="t">And that's going to give me my new value for all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=896" target="_blank">00:14:56.120</a></span> | <span class="t">of my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=897" target="_blank">00:14:57.180</a></span> | <span class="t">And I iterate like that, going over all pairs x, y's,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=901" target="_blank">00:15:01.260</a></span> | <span class="t">computing my gradient, taking a step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=903" target="_blank">00:15:03.420</a></span> | <span class="t">side in the opposite direction, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=905" target="_blank">00:15:05.280</a></span> | <span class="t">doing that several times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=907" target="_blank">00:15:07.620</a></span> | <span class="t">So that's how stochastic gradient descent works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=910" target="_blank">00:15:10.620</a></span> | <span class="t">And that's essentially the learning procedure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=912" target="_blank">00:15:12.540</a></span> | <span class="t">It's represented by this procedure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=916" target="_blank">00:15:16.100</a></span> | <span class="t">So in this algorithm, there are a few things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=917" target="_blank">00:15:17.900</a></span> | <span class="t">we need to specify to be able to implement it and execute it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=920" target="_blank">00:15:20.860</a></span> | <span class="t">We need a loss function, a choice for the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=923" target="_blank">00:15:23.620</a></span> | <span class="t">We need a procedure that's efficient for computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=926" target="_blank">00:15:26.860</a></span> | <span class="t">the gradient of the loss with respect to my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=930" target="_blank">00:15:30.620</a></span> | <span class="t">We need to choose a regularizer if we want one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=933" target="_blank">00:15:33.300</a></span> | <span class="t">And we need a way of initializing my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=935" target="_blank">00:15:35.940</a></span> | <span class="t">So next, what I'll do is I'll go through each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=937" target="_blank">00:15:37.960</a></span> | <span class="t">of these four different things we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=939" target="_blank">00:15:39.780</a></span> | <span class="t">need to choose before actually being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=941" target="_blank">00:15:41.700</a></span> | <span class="t">able to execute stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=945" target="_blank">00:15:45.660</a></span> | <span class="t">So first, the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=948" target="_blank">00:15:48.060</a></span> | <span class="t">So as I said, we will interpret the output layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=950" target="_blank">00:15:50.980</a></span> | <span class="t">as assigning probabilities to each potential class in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=953" target="_blank">00:15:53.900</a></span> | <span class="t">I can classify my input x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=957" target="_blank">00:15:57.460</a></span> | <span class="t">Well, in this case, something that would be natural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=959" target="_blank">00:15:59.780</a></span> | <span class="t">is to try to maximize the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=962" target="_blank">00:16:02.120</a></span> | <span class="t">of the correct class, the actual class in which my example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=965" target="_blank">00:16:05.500</a></span> | <span class="t">x t belongs to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=966" target="_blank">00:16:06.340</a></span> | <span class="t">I'd like to increase the value of the probability assigned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=969" target="_blank">00:16:09.340</a></span> | <span class="t">by--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=969" target="_blank">00:16:09.900</a></span> | <span class="t">computed by my neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=972" target="_blank">00:16:12.820</a></span> | <span class="t">And so because we set up the problem in which we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=976" target="_blank">00:16:16.380</a></span> | <span class="t">have a loss that we minimize, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=978" target="_blank">00:16:18.700</a></span> | <span class="t">of maximizing the probability, what we'll actually do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=980" target="_blank">00:16:20.980</a></span> | <span class="t">is minimize the negative and the actual log probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=985" target="_blank">00:16:25.380</a></span> | <span class="t">so the log likelihood of assigning x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=988" target="_blank">00:16:28.300</a></span> | <span class="t">to the correct class y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=990" target="_blank">00:16:30.340</a></span> | <span class="t">So this is represented here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=992" target="_blank">00:16:32.100</a></span> | <span class="t">So given my output layer and the true label y,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=995" target="_blank">00:16:35.420</a></span> | <span class="t">my loss will be minus the log of the probability of y</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1000" target="_blank">00:16:40.820</a></span> | <span class="t">according to my neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1002" target="_blank">00:16:42.140</a></span> | <span class="t">And that would be, well, take my output layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1004" target="_blank">00:16:44.980</a></span> | <span class="t">and look at the unit, so index the unit corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1008" target="_blank">00:16:48.500</a></span> | <span class="t">to the correct class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1010" target="_blank">00:16:50.020</a></span> | <span class="t">So that's why I'm indexing by y here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1013" target="_blank">00:16:53.500</a></span> | <span class="t">We take the log because numerically it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1015" target="_blank">00:16:55.740</a></span> | <span class="t">turns out to be more stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1016" target="_blank">00:16:56.940</a></span> | <span class="t">We get nicer-looking gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1019" target="_blank">00:16:59.180</a></span> | <span class="t">And sometimes in certain softwares,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1021" target="_blank">00:17:01.340</a></span> | <span class="t">you'll see instead of talking about the negative log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1023" target="_blank">00:17:03.460</a></span> | <span class="t">likelihood or log probability, you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1025" target="_blank">00:17:05.020</a></span> | <span class="t">see it referred as the cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1027" target="_blank">00:17:07.980</a></span> | <span class="t">And that's because you can think of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1031" target="_blank">00:17:11.820</a></span> | <span class="t">as performing a sum over all possible classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1035" target="_blank">00:17:15.620</a></span> | <span class="t">And then for each class, checking, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1037" target="_blank">00:17:17.540</a></span> | <span class="t">is this potential class the target class?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1040" target="_blank">00:17:20.820</a></span> | <span class="t">So I have an indicator function that is 1 if y is equal to c,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1044" target="_blank">00:17:24.660</a></span> | <span class="t">so if my iterator class c is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1048" target="_blank">00:17:28.100</a></span> | <span class="t">equal to the real class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1049" target="_blank">00:17:29.700</a></span> | <span class="t">I'm going to multiply that by the log of the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1053" target="_blank">00:17:33.140</a></span> | <span class="t">actually assigned to that class c.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1055" target="_blank">00:17:35.660</a></span> | <span class="t">And this function here, so this expression here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1059" target="_blank">00:17:39.380</a></span> | <span class="t">is like a cross-entropy between the empirical distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1062" target="_blank">00:17:42.900</a></span> | <span class="t">which assigns zero probability to all the other classes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1066" target="_blank">00:17:46.100</a></span> | <span class="t">but a probability of 1 to the correct class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1068" target="_blank">00:17:48.500</a></span> | <span class="t">and the actual distribution over classes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1070" target="_blank">00:17:50.900</a></span> | <span class="t">that my neural net is computing, which is f of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1074" target="_blank">00:17:54.660</a></span> | <span class="t">That's just a technical detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1076" target="_blank">00:17:56.060</a></span> | <span class="t">You can just think about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1077" target="_blank">00:17:57.540</a></span> | <span class="t">Here, I only mention it because in certain libraries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1079" target="_blank">00:17:59.700</a></span> | <span class="t">it's actually mentioned as the cross-entropy loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1083" target="_blank">00:18:03.740</a></span> | <span class="t">So that's for the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1086" target="_blank">00:18:06.060</a></span> | <span class="t">Then we need also a procedure for computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1088" target="_blank">00:18:08.260</a></span> | <span class="t">what is the gradient of my loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1090" target="_blank">00:18:10.460</a></span> | <span class="t">to all of my parameters in my neural net,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1092" target="_blank">00:18:12.620</a></span> | <span class="t">so the biases and the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1095" target="_blank">00:18:15.820</a></span> | <span class="t">You can go look at my videos if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1097" target="_blank">00:18:17.300</a></span> | <span class="t">want the actual derivation of all the details for all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1100" target="_blank">00:18:20.220</a></span> | <span class="t">of these different expressions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1101" target="_blank">00:18:21.780</a></span> | <span class="t">I don't have time for that, so all I'll do--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1103" target="_blank">00:18:23.740</a></span> | <span class="t">and presumably, a lot of you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1106" target="_blank">00:18:26.300</a></span> | <span class="t">have seen these derivations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1108" target="_blank">00:18:28.620</a></span> | <span class="t">If you haven't, just go check out the videos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1110" target="_blank">00:18:30.700</a></span> | <span class="t">In any case, I'm going to go through what the algorithm is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1113" target="_blank">00:18:33.980</a></span> | <span class="t">I'm going to highlight some of the key points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1115" target="_blank">00:18:35.900</a></span> | <span class="t">that will come up later in understanding how actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1119" target="_blank">00:18:39.220</a></span> | <span class="t">backpropagation functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1121" target="_blank">00:18:41.380</a></span> | <span class="t">So the basic idea is that we'll compute gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1124" target="_blank">00:18:44.820</a></span> | <span class="t">by exploiting the chain rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1126" target="_blank">00:18:46.420</a></span> | <span class="t">And we'll go from the top layer all the way to the bottom,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1130" target="_blank">00:18:50.220</a></span> | <span class="t">computing gradients for layers that are closer and closer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1133" target="_blank">00:18:53.820</a></span> | <span class="t">to the input as we go, and exploiting the chain rule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1136" target="_blank">00:18:56.540</a></span> | <span class="t">to exploit or reuse previous computations we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1139" target="_blank">00:18:59.820</a></span> | <span class="t">made at upper layers to compute the gradients at the layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1143" target="_blank">00:19:03.460</a></span> | <span class="t">below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1144" target="_blank">00:19:04.980</a></span> | <span class="t">So we usually start by computing what is the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1148" target="_blank">00:19:08.140</a></span> | <span class="t">at the output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1149" target="_blank">00:19:09.300</a></span> | <span class="t">So what's the gradient of my loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1152" target="_blank">00:19:12.460</a></span> | <span class="t">to my output layer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1153" target="_blank">00:19:13.820</a></span> | <span class="t">And actually, it's more convenient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1155" target="_blank">00:19:15.380</a></span> | <span class="t">to compute the loss with respect to the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1158" target="_blank">00:19:18.340</a></span> | <span class="t">It's actually a very simple expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1161" target="_blank">00:19:21.140</a></span> | <span class="t">So that's why I have the gradient of this vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1164" target="_blank">00:19:24.180</a></span> | <span class="t">a l plus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1165" target="_blank">00:19:25.140</a></span> | <span class="t">That's the pre-activation at the very last layer of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1168" target="_blank">00:19:28.980</a></span> | <span class="t">function, which is minus the log f of x, y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1172" target="_blank">00:19:32.460</a></span> | <span class="t">And it turns out this gradient is super simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1175" target="_blank">00:19:35.060</a></span> | <span class="t">It's minus E of y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1177" target="_blank">00:19:37.100</a></span> | <span class="t">So that's the one-hot vector for class y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1180" target="_blank">00:19:40.180</a></span> | <span class="t">So what this means is E of y is just a vector filled</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1183" target="_blank">00:19:43.820</a></span> | <span class="t">with a bunch of 0's and then the 1 at the correct class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1187" target="_blank">00:19:47.860</a></span> | <span class="t">So if y was the fourth class, then in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1191" target="_blank">00:19:51.100</a></span> | <span class="t">it would be this vector, where I have a 1 at the fourth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1193" target="_blank">00:19:53.260</a></span> | <span class="t">dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1194" target="_blank">00:19:54.740</a></span> | <span class="t">So E of y is just a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1196" target="_blank">00:19:56.580</a></span> | <span class="t">We call it the one-hot vector full of 0's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1198" target="_blank">00:19:58.700</a></span> | <span class="t">And the single 1 at the position corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1201" target="_blank">00:20:01.620</a></span> | <span class="t">to the correct class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1203" target="_blank">00:20:03.540</a></span> | <span class="t">So what this part of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1205" target="_blank">00:20:05.140</a></span> | <span class="t">is essentially saying is that I'm going to increase--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1207" target="_blank">00:20:07.620</a></span> | <span class="t">I want to increase the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1209" target="_blank">00:20:09.460</a></span> | <span class="t">of the correct class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1210" target="_blank">00:20:10.580</a></span> | <span class="t">I want to increase the pre-activation, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1212" target="_blank">00:20:12.520</a></span> | <span class="t">will increase the probability of the correct class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1215" target="_blank">00:20:15.460</a></span> | <span class="t">And I'm going to subtract what is the current probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1218" target="_blank">00:20:18.660</a></span> | <span class="t">assigned by my neural net to all of the classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1221" target="_blank">00:20:21.860</a></span> | <span class="t">So f of x, that's my output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1223" target="_blank">00:20:23.580</a></span> | <span class="t">And that's the current beliefs of the neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1226" target="_blank">00:20:26.420</a></span> | <span class="t">as to in which class, what's the probability of assigning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1230" target="_blank">00:20:30.100</a></span> | <span class="t">the input to each class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1231" target="_blank">00:20:31.740</a></span> | <span class="t">So what this is doing is essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1233" target="_blank">00:20:33.740</a></span> | <span class="t">trying to decrease the probability of everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1236" target="_blank">00:20:36.260</a></span> | <span class="t">and specifically decrease it as much as the neural net currently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1239" target="_blank">00:20:39.900</a></span> | <span class="t">believes that the input belongs to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1242" target="_blank">00:20:42.900</a></span> | <span class="t">And so if you think about the subtraction of these two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1245" target="_blank">00:20:45.420</a></span> | <span class="t">things, well, for the class that's the correct class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1248" target="_blank">00:20:48.260</a></span> | <span class="t">I'm going to have 1 minus some number between 0 and 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1251" target="_blank">00:20:51.140</a></span> | <span class="t">because it's a probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1252" target="_blank">00:20:52.420</a></span> | <span class="t">So that's going to be positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1253" target="_blank">00:20:53.700</a></span> | <span class="t">So I'm going to increase the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1255" target="_blank">00:20:55.380</a></span> | <span class="t">of the correct class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1256" target="_blank">00:20:56.500</a></span> | <span class="t">And for everything else, it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1258" target="_blank">00:20:58.000</a></span> | <span class="t">to be 0 minus a positive number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1259" target="_blank">00:20:59.820</a></span> | <span class="t">So it's going to be negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1261" target="_blank">00:21:01.180</a></span> | <span class="t">So I'm actually going to decrease the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1263" target="_blank">00:21:03.180</a></span> | <span class="t">of everything else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1264" target="_blank">00:21:04.180</a></span> | <span class="t">So intuitively, it makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1265" target="_blank">00:21:05.620</a></span> | <span class="t">This gradient has the right behavior.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1268" target="_blank">00:21:08.740</a></span> | <span class="t">And I'm going to take that pre-activation gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1271" target="_blank">00:21:11.580</a></span> | <span class="t">I'm going to propagate it from the top to the bottom</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1275" target="_blank">00:21:15.340</a></span> | <span class="t">and essentially iterating from the last layer, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1279" target="_blank">00:21:19.740</a></span> | <span class="t">is the output layer, L plus 1, all the way down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1282" target="_blank">00:21:22.380</a></span> | <span class="t">to the first layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1283" target="_blank">00:21:23.980</a></span> | <span class="t">And as I'm going down, I'm going to compute the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1286" target="_blank">00:21:26.780</a></span> | <span class="t">with respect to my parameters and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1288" target="_blank">00:21:28.500</a></span> | <span class="t">compute what's the gradient for the pre-activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1291" target="_blank">00:21:31.420</a></span> | <span class="t">at the layer below and then iterate like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1294" target="_blank">00:21:34.580</a></span> | <span class="t">So at each iteration of that loop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1298" target="_blank">00:21:38.180</a></span> | <span class="t">I take what is the current gradient of the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1302" target="_blank">00:21:42.900</a></span> | <span class="t">with respect to the pre-activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1304" target="_blank">00:21:44.420</a></span> | <span class="t">at the current layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1306" target="_blank">00:21:46.220</a></span> | <span class="t">And I can compute the gradient of the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1309" target="_blank">00:21:49.380</a></span> | <span class="t">with respect to my weight matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1311" target="_blank">00:21:51.500</a></span> | <span class="t">So not doing the derivation here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1314" target="_blank">00:21:54.460</a></span> | <span class="t">it's actually simply this vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1318" target="_blank">00:21:58.180</a></span> | <span class="t">So in my notation, I assume that all the vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1320" target="_blank">00:22:00.580</a></span> | <span class="t">are column vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1322" target="_blank">00:22:02.220</a></span> | <span class="t">So this pre-activation gradient vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1325" target="_blank">00:22:05.180</a></span> | <span class="t">and I multiply it by the transpose of the activations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1329" target="_blank">00:22:09.020</a></span> | <span class="t">so the value of the layer right below, the layer k minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1334" target="_blank">00:22:14.540</a></span> | <span class="t">So because I take the transpose, that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1336" target="_blank">00:22:16.100</a></span> | <span class="t">a multiplication like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1337" target="_blank">00:22:17.380</a></span> | <span class="t">And you can see if I do the outer product,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1339" target="_blank">00:22:19.180</a></span> | <span class="t">essentially, between these two vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1340" target="_blank">00:22:20.800</a></span> | <span class="t">I'm going to get a matrix of the same size as my weight matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1344" target="_blank">00:22:24.260</a></span> | <span class="t">So it all checks out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1345" target="_blank">00:22:25.940</a></span> | <span class="t">That makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1347" target="_blank">00:22:27.500</a></span> | <span class="t">Turns out that the gradient of the loss with respect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1349" target="_blank">00:22:29.680</a></span> | <span class="t">to the bias is exactly the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1351" target="_blank">00:22:31.940</a></span> | <span class="t">of the loss with respect to the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1354" target="_blank">00:22:34.700</a></span> | <span class="t">So that's very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1356" target="_blank">00:22:36.220</a></span> | <span class="t">So that gives me now my gradients for my parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1358" target="_blank">00:22:38.700</a></span> | <span class="t">Now I need to compute, OK, what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1360" target="_blank">00:22:40.420</a></span> | <span class="t">going to be the gradient of the pre-activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1362" target="_blank">00:22:42.660</a></span> | <span class="t">at the layer below?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1364" target="_blank">00:22:44.700</a></span> | <span class="t">Well, first, I'm going to get the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1368" target="_blank">00:22:48.980</a></span> | <span class="t">function with respect to the activation at the layer below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1374" target="_blank">00:22:54.220</a></span> | <span class="t">Well, that's just taking my pre-activation gradient vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1377" target="_blank">00:22:57.980</a></span> | <span class="t">and multiplying it by--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1379" target="_blank">00:22:59.940</a></span> | <span class="t">for some reason, it doesn't show here--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1381" target="_blank">00:23:01.780</a></span> | <span class="t">and multiply it by the transpose of my weight matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1384" target="_blank">00:23:04.660</a></span> | <span class="t">Super simple operation, just a linear transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1387" target="_blank">00:23:07.580</a></span> | <span class="t">of my gradients at layer k, linear and transformed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1390" target="_blank">00:23:10.580</a></span> | <span class="t">to get my gradients of the activation at the layer k</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1393" target="_blank">00:23:13.780</a></span> | <span class="t">minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1395" target="_blank">00:23:15.180</a></span> | <span class="t">And then to get the gradients of the pre-activation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1397" target="_blank">00:23:17.900</a></span> | <span class="t">so before the activation function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1401" target="_blank">00:23:21.020</a></span> | <span class="t">I'm going to take this gradient here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1402" target="_blank">00:23:22.940</a></span> | <span class="t">which is the gradient of the activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1405" target="_blank">00:23:25.540</a></span> | <span class="t">at the layer k minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1407" target="_blank">00:23:27.220</a></span> | <span class="t">And then I apply the gradient corresponding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1409" target="_blank">00:23:29.900</a></span> | <span class="t">to the partial derivative of my nonlinear activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1413" target="_blank">00:23:33.700</a></span> | <span class="t">So this here, this refers to an element-wise product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1417" target="_blank">00:23:37.060</a></span> | <span class="t">So I'm taking these two vectors, this vector here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1419" target="_blank">00:23:39.860</a></span> | <span class="t">and this vector here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1420" target="_blank">00:23:40.740</a></span> | <span class="t">I'm going to do an element-wise product between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1423" target="_blank">00:23:43.740</a></span> | <span class="t">And this vector here is just the partial derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1427" target="_blank">00:23:47.020</a></span> | <span class="t">of the activation function for each unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1429" target="_blank">00:23:49.620</a></span> | <span class="t">individually that I've put together into a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1432" target="_blank">00:23:52.700</a></span> | <span class="t">This is what this corresponds to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1435" target="_blank">00:23:55.180</a></span> | <span class="t">Now, the key things to notice is first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1437" target="_blank">00:23:57.060</a></span> | <span class="t">that this pass, computing all the gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1439" target="_blank">00:23:59.580</a></span> | <span class="t">and doing all these iterations, is actually fairly cheap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1442" target="_blank">00:24:02.820</a></span> | <span class="t">Complexity is essentially the same as the one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1445" target="_blank">00:24:05.540</a></span> | <span class="t">that's doing a forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1447" target="_blank">00:24:07.700</a></span> | <span class="t">So all I'm doing are linear transformations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1451" target="_blank">00:24:11.180</a></span> | <span class="t">multiplying by matrices, in this case, the transpose of my weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1454" target="_blank">00:24:14.220</a></span> | <span class="t">matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1455" target="_blank">00:24:15.060</a></span> | <span class="t">And then I'm also doing this nonlinear operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1457" target="_blank">00:24:17.620</a></span> | <span class="t">where I'm multiplying by the gradient of the activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1460" target="_blank">00:24:20.120</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1461" target="_blank">00:24:21.020</a></span> | <span class="t">So that's the first thing to notice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1462" target="_blank">00:24:22.860</a></span> | <span class="t">And the second thing to notice is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1464" target="_blank">00:24:24.220</a></span> | <span class="t">that here I'm doing this element-wise product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1467" target="_blank">00:24:27.420</a></span> | <span class="t">So if any of these terms here for a unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1470" target="_blank">00:24:30.060</a></span> | <span class="t">is very close to 0, then the pre-activation gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1473" target="_blank">00:24:33.900</a></span> | <span class="t">is going to be 0 for the next layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1476" target="_blank">00:24:36.460</a></span> | <span class="t">And I highlight this point because essentially whenever--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1479" target="_blank">00:24:39.740</a></span> | <span class="t">that's something to think about a lot when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1481" target="_blank">00:24:41.500</a></span> | <span class="t">you're training neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1482" target="_blank">00:24:42.860</a></span> | <span class="t">Whenever this gradient here, these partial derivatives,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1486" target="_blank">00:24:46.060</a></span> | <span class="t">come close to 0, then it means the gradient will not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1488" target="_blank">00:24:48.660</a></span> | <span class="t">propagate well to the next layer, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1490" target="_blank">00:24:50.420</a></span> | <span class="t">means that you're not going to get a good gradient to update</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1493" target="_blank">00:24:53.120</a></span> | <span class="t">your parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1494" target="_blank">00:24:54.980</a></span> | <span class="t">Now, when does that happen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1496" target="_blank">00:24:56.380</a></span> | <span class="t">When will you see these terms here being close to 0?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1499" target="_blank">00:24:59.380</a></span> | <span class="t">Well, that's going to be when the partial derivatives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1501" target="_blank">00:25:01.580</a></span> | <span class="t">of these nonlinear activation functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1503" target="_blank">00:25:03.700</a></span> | <span class="t">are close to 0 or 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1505" target="_blank">00:25:05.780</a></span> | <span class="t">So we can look at the partial derivatives, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1508" target="_blank">00:25:08.160</a></span> | <span class="t">of the sigmoid function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1510" target="_blank">00:25:10.140</a></span> | <span class="t">It turns out it's super easy to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1512" target="_blank">00:25:12.460</a></span> | <span class="t">It's just the sigmoid itself times 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1515" target="_blank">00:25:15.340</a></span> | <span class="t">minus the sigmoid itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1518" target="_blank">00:25:18.100</a></span> | <span class="t">So that means that whenever the activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1520" target="_blank">00:25:20.160</a></span> | <span class="t">of the unit for a sigmoid unit is close to 1 or close to 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1523" target="_blank">00:25:23.500</a></span> | <span class="t">I essentially get a partial derivative that's close to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1527" target="_blank">00:25:27.540</a></span> | <span class="t">You can kind of see it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1528" target="_blank">00:25:28.780</a></span> | <span class="t">The slope here is essentially flat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1530" target="_blank">00:25:30.380</a></span> | <span class="t">and the slope here is flat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1531" target="_blank">00:25:31.700</a></span> | <span class="t">That's the value of the partial derivative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1535" target="_blank">00:25:35.260</a></span> | <span class="t">So in other words, if my pre-activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1537" target="_blank">00:25:37.940</a></span> | <span class="t">are very negative or very positive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1539" target="_blank">00:25:39.860</a></span> | <span class="t">or if my unit is very saturated, then gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1542" target="_blank">00:25:42.940</a></span> | <span class="t">will have a hard time propagating to the next layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1546" target="_blank">00:25:46.500</a></span> | <span class="t">That's the key insight here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1549" target="_blank">00:25:49.020</a></span> | <span class="t">Same thing for the tanh function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1551" target="_blank">00:25:51.820</a></span> | <span class="t">So it turns out the partial derivative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1553" target="_blank">00:25:53.500</a></span> | <span class="t">is also easy to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1554" target="_blank">00:25:54.940</a></span> | <span class="t">You just take the tanh value, square it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1557" target="_blank">00:25:57.580</a></span> | <span class="t">and you're going to subtract it to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1559" target="_blank">00:25:59.980</a></span> | <span class="t">And indeed, if it's close to minus 1 or close to 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1564" target="_blank">00:26:04.420</a></span> | <span class="t">you can see that the slope is flat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1567" target="_blank">00:26:07.180</a></span> | <span class="t">So again, if the unit is saturating,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1569" target="_blank">00:26:09.420</a></span> | <span class="t">gradients will have a hard time propagating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1571" target="_blank">00:26:11.900</a></span> | <span class="t">to the next layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1574" target="_blank">00:26:14.140</a></span> | <span class="t">And for the ReLU, the rectified linear activation function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1578" target="_blank">00:26:18.140</a></span> | <span class="t">the gradient is even simpler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1581" target="_blank">00:26:21.180</a></span> | <span class="t">You just check whether the pre-activation is greater than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1583" target="_blank">00:26:23.580</a></span> | <span class="t">0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1584" target="_blank">00:26:24.080</a></span> | <span class="t">If it is, the partial derivative is 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1586" target="_blank">00:26:26.300</a></span> | <span class="t">If it's not, it's 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1587" target="_blank">00:26:27.780</a></span> | <span class="t">So actually, you're going to multiply by 1 or 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1590" target="_blank">00:26:30.020</a></span> | <span class="t">You essentially get a binary mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1591" target="_blank">00:26:31.760</a></span> | <span class="t">when you're performing the propagation through the ReLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1595" target="_blank">00:26:35.220</a></span> | <span class="t">And you can see it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1596" target="_blank">00:26:36.180</a></span> | <span class="t">The slope here is flat, and otherwise, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1598" target="_blank">00:26:38.100</a></span> | <span class="t">have a linear function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1600" target="_blank">00:26:40.020</a></span> | <span class="t">So actually, here, the shrinking of the gradient towards 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1603" target="_blank">00:26:43.940</a></span> | <span class="t">is even harder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1604" target="_blank">00:26:44.860</a></span> | <span class="t">It's exactly multiplying by 0 if you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1607" target="_blank">00:26:47.780</a></span> | <span class="t">a unit that's saturating below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1612" target="_blank">00:26:52.140</a></span> | <span class="t">And beyond all the math, in terms of actually using those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1616" target="_blank">00:26:56.820</a></span> | <span class="t">in practice, during the weekend, you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1618" target="_blank">00:26:58.800</a></span> | <span class="t">see three different libraries that essentially allows you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1621" target="_blank">00:27:01.780</a></span> | <span class="t">to compute these gradients for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1623" target="_blank">00:27:03.220</a></span> | <span class="t">You actually usually don't write down backprop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1626" target="_blank">00:27:06.000</a></span> | <span class="t">You just use all of these modules</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1628" target="_blank">00:27:08.020</a></span> | <span class="t">that you've implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1629" target="_blank">00:27:09.180</a></span> | <span class="t">And it turns out there's a way of automatically differentiating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1633" target="_blank">00:27:13.380</a></span> | <span class="t">your loss function and getting gradients for free</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1636" target="_blank">00:27:16.220</a></span> | <span class="t">in terms of effort, in terms of programming effort,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1639" target="_blank">00:27:19.460</a></span> | <span class="t">with respect to your parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1641" target="_blank">00:27:21.500</a></span> | <span class="t">So conceptually, the way you do this--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1643" target="_blank">00:27:23.940</a></span> | <span class="t">and you'll see essentially three different libraries doing it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1646" target="_blank">00:27:26.460</a></span> | <span class="t">in slightly different ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1648" target="_blank">00:27:28.580</a></span> | <span class="t">What you do is you augment your flow graph</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1651" target="_blank">00:27:31.340</a></span> | <span class="t">by adding, at the very end, the computation of your loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1654" target="_blank">00:27:34.420</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1655" target="_blank">00:27:35.600</a></span> | <span class="t">And then each of these boxes, which are conceptually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1658" target="_blank">00:27:38.020</a></span> | <span class="t">objects that are taking arguments and computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1660" target="_blank">00:27:40.540</a></span> | <span class="t">a value, you're going to augment them to also have a method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1665" target="_blank">00:27:45.420</a></span> | <span class="t">that's a backprop or a bprop method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1667" target="_blank">00:27:47.600</a></span> | <span class="t">You'll often see, actually, this expression being used, bprop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1670" target="_blank">00:27:50.660</a></span> | <span class="t">And what this method should do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1672" target="_blank">00:27:52.980</a></span> | <span class="t">that it should take as input, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1674" target="_blank">00:27:54.500</a></span> | <span class="t">is the gradient of the loss with respect to myself?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1677" target="_blank">00:27:57.300</a></span> | <span class="t">And then it should propagate to its arguments,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1680" target="_blank">00:28:00.340</a></span> | <span class="t">so the things that its parents in the flow graph,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1682" target="_blank">00:28:02.780</a></span> | <span class="t">the things it takes to compute its own value,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1684" target="_blank">00:28:04.780</a></span> | <span class="t">it's going to propagate them using the chain rule, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1687" target="_blank">00:28:07.100</a></span> | <span class="t">is their gradients with respect to the loss?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1690" target="_blank">00:28:10.780</a></span> | <span class="t">So what this means is that you would start the process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1694" target="_blank">00:28:14.220</a></span> | <span class="t">by initializing, well, the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1696" target="_blank">00:28:16.860</a></span> | <span class="t">with respect to itself is 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1699" target="_blank">00:28:19.360</a></span> | <span class="t">And then you pass the bprop method here 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1702" target="_blank">00:28:22.260</a></span> | <span class="t">And then it's going to propagate to its argument, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1706" target="_blank">00:28:26.420</a></span> | <span class="t">is, by using the chain rule, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1708" target="_blank">00:28:28.240</a></span> | <span class="t">is the gradient of the loss with respect to f of x?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1712" target="_blank">00:28:32.040</a></span> | <span class="t">And then you're going to call bprop on this object here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1714" target="_blank">00:28:34.800</a></span> | <span class="t">And it's going to compute, well, I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1716" target="_blank">00:28:36.220</a></span> | <span class="t">have the gradient of the loss with respect to myself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1718" target="_blank">00:28:38.460</a></span> | <span class="t">f of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1719" target="_blank">00:28:39.300</a></span> | <span class="t">From this, I can compute what's the gradient of my argument,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1722" target="_blank">00:28:42.700</a></span> | <span class="t">which is the pre-activation at layer 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1725" target="_blank">00:28:45.140</a></span> | <span class="t">with respect to the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1726" target="_blank">00:28:46.580</a></span> | <span class="t">So I'm going to reuse the computation I just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1728" target="_blank">00:28:48.380</a></span> | <span class="t">got and update it using my--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1730" target="_blank">00:28:50.860</a></span> | <span class="t">what is essentially the Jacobian.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1733" target="_blank">00:28:53.020</a></span> | <span class="t">And then I'm going to take the pre-activation here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1735" target="_blank">00:28:55.140</a></span> | <span class="t">which now knows what is the gradient of the loss with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1737" target="_blank">00:28:57.340</a></span> | <span class="t">respect to itself, the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1739" target="_blank">00:28:59.180</a></span> | <span class="t">It's going to propagate to the weights and the biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1742" target="_blank">00:29:02.020</a></span> | <span class="t">and the layer below, updating them with--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1744" target="_blank">00:29:04.200</a></span> | <span class="t">informing them of what is the gradient of the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1746" target="_blank">00:29:06.280</a></span> | <span class="t">with respect to themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1747" target="_blank">00:29:07.420</a></span> | <span class="t">And you continue like this, essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1749" target="_blank">00:29:09.080</a></span> | <span class="t">going through the flow graph, but in the opposite direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1752" target="_blank">00:29:12.880</a></span> | <span class="t">So the library torch, the basic library torch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1755" target="_blank">00:29:15.400</a></span> | <span class="t">essentially functions like this quite explicitly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1758" target="_blank">00:29:18.440</a></span> | <span class="t">You construct-- you chain these elements together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1761" target="_blank">00:29:21.000</a></span> | <span class="t">And then when you're performing backpropagation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1763" target="_blank">00:29:23.000</a></span> | <span class="t">you're going in the reverse order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1764" target="_blank">00:29:24.340</a></span> | <span class="t">of these chained elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1765" target="_blank">00:29:25.880</a></span> | <span class="t">And then you have libraries like Torchautograd and Theano</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1769" target="_blank">00:29:29.280</a></span> | <span class="t">and TensorFlow, which you'll learn about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1771" target="_blank">00:29:31.240</a></span> | <span class="t">which are doing things slightly more sophisticated there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1774" target="_blank">00:29:34.340</a></span> | <span class="t">And you'll learn about that later on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1778" target="_blank">00:29:38.540</a></span> | <span class="t">OK, so that's a discussion of how you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1780" target="_blank">00:29:40.940</a></span> | <span class="t">compute gradients of the loss with respect to the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1784" target="_blank">00:29:44.620</a></span> | <span class="t">So that's another component we need in stochastic gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1787" target="_blank">00:29:47.540</a></span> | <span class="t">descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1788" target="_blank">00:29:48.640</a></span> | <span class="t">We can choose a regularizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1790" target="_blank">00:29:50.100</a></span> | <span class="t">One that's often used is the L2 regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1793" target="_blank">00:29:53.500</a></span> | <span class="t">So that's just the sum of the squared of all the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1797" target="_blank">00:29:57.100</a></span> | <span class="t">And the gradient of that is just twice times the weight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1801" target="_blank">00:30:01.280</a></span> | <span class="t">So it's a super simple gradient to compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1803" target="_blank">00:30:03.400</a></span> | <span class="t">We usually don't regularize the biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1806" target="_blank">00:30:06.840</a></span> | <span class="t">There's no particularly important reason for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1810" target="_blank">00:30:10.960</a></span> | <span class="t">There are much fewer biases, so it seems less important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1815" target="_blank">00:30:15.200</a></span> | <span class="t">And often, this L2 regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1817" target="_blank">00:30:17.020</a></span> | <span class="t">is often referred to as weight decay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1818" target="_blank">00:30:18.760</a></span> | <span class="t">So if you hear about weight decay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1820" target="_blank">00:30:20.320</a></span> | <span class="t">that often refers to L2 regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1824" target="_blank">00:30:24.400</a></span> | <span class="t">And then finally, and this is also a very important point,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1828" target="_blank">00:30:28.660</a></span> | <span class="t">you have to initialize the parameters before you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1831" target="_blank">00:30:31.080</a></span> | <span class="t">start doing backprop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1832" target="_blank">00:30:32.120</a></span> | <span class="t">And there are a few tricky cases you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1834" target="_blank">00:30:34.160</a></span> | <span class="t">need to make sure that you don't fall into.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1837" target="_blank">00:30:37.640</a></span> | <span class="t">So the biases, often we initialize them to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1840" target="_blank">00:30:40.600</a></span> | <span class="t">There are certain exceptions, but for the most part,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1842" target="_blank">00:30:42.880</a></span> | <span class="t">we initialize them to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1844" target="_blank">00:30:44.720</a></span> | <span class="t">But for the weights, there are a few things we can't do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1847" target="_blank">00:30:47.600</a></span> | <span class="t">So we can't initialize the weights to 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1850" target="_blank">00:30:50.200</a></span> | <span class="t">and especially if you have tanh activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1853" target="_blank">00:30:53.960</a></span> | <span class="t">The reason-- and I won't explain it here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1856" target="_blank">00:30:56.160</a></span> | <span class="t">but it's not a bad exercise to try to figure out why--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1859" target="_blank">00:30:59.240</a></span> | <span class="t">is that essentially, when you do your first pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1862" target="_blank">00:31:02.100</a></span> | <span class="t">you're going to get gradients for all your parameters that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1864" target="_blank">00:31:04.520</a></span> | <span class="t">are going to be 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1865" target="_blank">00:31:05.920</a></span> | <span class="t">So you're going to be stuck at this 0 initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1868" target="_blank">00:31:08.960</a></span> | <span class="t">So we can't do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1871" target="_blank">00:31:11.280</a></span> | <span class="t">We also can't initialize all the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1873" target="_blank">00:31:13.160</a></span> | <span class="t">to exactly the same value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1876" target="_blank">00:31:16.440</a></span> | <span class="t">Again, you think about it a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1878" target="_blank">00:31:18.720</a></span> | <span class="t">What's going to happen is essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1880" target="_blank">00:31:20.600</a></span> | <span class="t">that all the weights coming into a unit within the layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1884" target="_blank">00:31:24.720</a></span> | <span class="t">are going to have exactly the same gradients, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1887" target="_blank">00:31:27.640</a></span> | <span class="t">means they're going to be updated exactly the same way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1890" target="_blank">00:31:30.000</a></span> | <span class="t">which means they're going to stay constant the same--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1892" target="_blank">00:31:32.280</a></span> | <span class="t">not constant, but they're going to stay the same--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1894" target="_blank">00:31:34.360</a></span> | <span class="t">the whole time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1895" target="_blank">00:31:35.080</a></span> | <span class="t">So it's as if you have multiple copies of the same unit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1898" target="_blank">00:31:38.320</a></span> | <span class="t">So you essentially have to break that initial symmetry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1900" target="_blank">00:31:40.920</a></span> | <span class="t">that you would create if you initialized everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1903" target="_blank">00:31:43.080</a></span> | <span class="t">to the same value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1904" target="_blank">00:31:44.520</a></span> | <span class="t">So what we end up doing most of the time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1906" target="_blank">00:31:46.260</a></span> | <span class="t">is initialize the weights to some randomly generated value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1910" target="_blank">00:31:50.480</a></span> | <span class="t">Often, we generate them--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1912" target="_blank">00:31:52.080</a></span> | <span class="t">there are a few other recipes, but one of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1914" target="_blank">00:31:54.120</a></span> | <span class="t">is to initialize them from some uniform distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1916" target="_blank">00:31:56.480</a></span> | <span class="t">between lower and upper bound.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1919" target="_blank">00:31:59.360</a></span> | <span class="t">This is a recipe here that is often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1921" target="_blank">00:32:01.440</a></span> | <span class="t">used that has some theoretical grounding that was derived</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1925" target="_blank">00:32:05.440</a></span> | <span class="t">specifically for the tanh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1926" target="_blank">00:32:06.880</a></span> | <span class="t">There's this paper here by Xavier Guerroux and Yoshua</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1929" target="_blank">00:32:09.880</a></span> | <span class="t">Bengio you can check out for some intuition as to how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1933" target="_blank">00:32:13.120</a></span> | <span class="t">you should initialize the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1934" target="_blank">00:32:14.460</a></span> | <span class="t">But essentially, they should be initially random,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1937" target="_blank">00:32:17.100</a></span> | <span class="t">and they should be initially close to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1939" target="_blank">00:32:19.320</a></span> | <span class="t">Random to break symmetry, and close to 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1943" target="_blank">00:32:23.160</a></span> | <span class="t">so that initially the units are not already saturated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1947" target="_blank">00:32:27.040</a></span> | <span class="t">Because if the units are saturated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1948" target="_blank">00:32:28.680</a></span> | <span class="t">then there are no gradients that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1949" target="_blank">00:32:29.800</a></span> | <span class="t">going to pass through the units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1951" target="_blank">00:32:31.280</a></span> | <span class="t">You're essentially going to get gradients very close to 0</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1953" target="_blank">00:32:33.680</a></span> | <span class="t">at the lower layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1955" target="_blank">00:32:35.120</a></span> | <span class="t">So that's the main intuition, is to have weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1957" target="_blank">00:32:37.360</a></span> | <span class="t">that are small and random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1960" target="_blank">00:32:40.200</a></span> | <span class="t">So those are all the pieces we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1965" target="_blank">00:32:45.360</a></span> | <span class="t">for running stochastic gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1967" target="_blank">00:32:47.200</a></span> | <span class="t">So that allows us to take a training set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1968" target="_blank">00:32:48.860</a></span> | <span class="t">and run a certain number of epochs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1970" target="_blank">00:32:50.720</a></span> | <span class="t">and have the neural net learn from that training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1973" target="_blank">00:32:53.920</a></span> | <span class="t">Now, there are other quantities in our neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1977" target="_blank">00:32:57.120</a></span> | <span class="t">that we haven't specified how to choose them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1979" target="_blank">00:32:59.240</a></span> | <span class="t">So those are the hyperparameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1982" target="_blank">00:33:02.560</a></span> | <span class="t">So usually, we're going to have a separate validation set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1985" target="_blank">00:33:05.240</a></span> | <span class="t">Most people here are familiar with machine learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1986" target="_blank">00:33:06.880</a></span> | <span class="t">so that's a typical procedure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1988" target="_blank">00:33:08.440</a></span> | <span class="t">And then we need to select things like, OK,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1990" target="_blank">00:33:10.320</a></span> | <span class="t">how many layers do I want?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1991" target="_blank">00:33:11.560</a></span> | <span class="t">How many units per layer do I want?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1994" target="_blank">00:33:14.200</a></span> | <span class="t">What's the step size, the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1996" target="_blank">00:33:16.120</a></span> | <span class="t">of my stochastic gradient descent procedure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1997" target="_blank">00:33:17.920</a></span> | <span class="t">that alpha number?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=1999" target="_blank">00:33:19.540</a></span> | <span class="t">What is the weight decay that I'm going to use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2002" target="_blank">00:33:22.300</a></span> | <span class="t">So a standard thing in machine learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2004" target="_blank">00:33:24.340</a></span> | <span class="t">is to perform a grid search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2007" target="_blank">00:33:27.380</a></span> | <span class="t">That is, if I have two hyperparameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2009" target="_blank">00:33:29.340</a></span> | <span class="t">I list out a bunch of values I want to try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2011" target="_blank">00:33:31.320</a></span> | <span class="t">So for the number of hidden units,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2012" target="_blank">00:33:32.780</a></span> | <span class="t">maybe I want to try 100, 1,000, and 2,000, say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2016" target="_blank">00:33:36.940</a></span> | <span class="t">And then for the learning rate, maybe I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2018" target="_blank">00:33:38.540</a></span> | <span class="t">want to try 0.01 and 0.001.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2022" target="_blank">00:33:42.420</a></span> | <span class="t">So a grid search would just try all combinations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2024" target="_blank">00:33:44.580</a></span> | <span class="t">of these three values for the hidden units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2026" target="_blank">00:33:46.900</a></span> | <span class="t">and these two values for the learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2029" target="_blank">00:33:49.820</a></span> | <span class="t">So that means that the more hyperparameters there are,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2033" target="_blank">00:33:53.340</a></span> | <span class="t">the number of configurations you have to try out blows up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2037" target="_blank">00:33:57.420</a></span> | <span class="t">and grows exponentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2039" target="_blank">00:33:59.620</a></span> | <span class="t">So another procedure that is now more and more common,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2043" target="_blank">00:34:03.180</a></span> | <span class="t">which is more practical, is to perform a form of random search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2047" target="_blank">00:34:07.580</a></span> | <span class="t">In this case, what you do is for each parameter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2050" target="_blank">00:34:10.020</a></span> | <span class="t">you actually determine a distribution of likely values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2053" target="_blank">00:34:13.460</a></span> | <span class="t">you'd like to try.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2054" target="_blank">00:34:14.220</a></span> | <span class="t">So it could be--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2056" target="_blank">00:34:16.100</a></span> | <span class="t">so for the number of hidden units,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2057" target="_blank">00:34:17.560</a></span> | <span class="t">maybe I do a uniform distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2059" target="_blank">00:34:19.500</a></span> | <span class="t">over all integers from 100 to 1,000, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2062" target="_blank">00:34:22.700</a></span> | <span class="t">or maybe a log uniform distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2065" target="_blank">00:34:25.220</a></span> | <span class="t">And for the learning rate, maybe, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2066" target="_blank">00:34:26.920</a></span> | <span class="t">the log uniform distribution, but from 0.001 to 0.01, say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2072" target="_blank">00:34:32.840</a></span> | <span class="t">And then to get an experiment, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2075" target="_blank">00:34:35.260</a></span> | <span class="t">to get values for my hyperparameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2077" target="_blank">00:34:37.060</a></span> | <span class="t">to do an experiment with and get a performance on my validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2079" target="_blank">00:34:39.940</a></span> | <span class="t">set, I just independently sample from these distributions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2083" target="_blank">00:34:43.140</a></span> | <span class="t">for each hyperparameter to get a full configuration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2086" target="_blank">00:34:46.460</a></span> | <span class="t">for my experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2087" target="_blank">00:34:47.820</a></span> | <span class="t">And then because I have this way of getting one experiment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2090" target="_blank">00:34:50.720</a></span> | <span class="t">I do it independently for all of my jobs, all of my experiments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2093" target="_blank">00:34:53.740</a></span> | <span class="t">that I will do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2094" target="_blank">00:34:54.620</a></span> | <span class="t">So in this case, if I know I have enough compute power</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2098" target="_blank">00:34:58.020</a></span> | <span class="t">to do 50 experiments, I just sample 50 independent samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2102" target="_blank">00:35:02.120</a></span> | <span class="t">from these distributions for hyperparameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2104" target="_blank">00:35:04.100</a></span> | <span class="t">perform these 50 experiments, and I just take the best one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2107" target="_blank">00:35:07.860</a></span> | <span class="t">What's nice about it is that there are no--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2109" target="_blank">00:35:09.880</a></span> | <span class="t">unlike grid search, there are never any holes in the grid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2112" target="_blank">00:35:12.620</a></span> | <span class="t">That is, you just specify how many experiments you do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2115" target="_blank">00:35:15.260</a></span> | <span class="t">If one of your jobs died, well, you just have one less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2118" target="_blank">00:35:18.460</a></span> | <span class="t">But there's no hole in your experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2121" target="_blank">00:35:21.900</a></span> | <span class="t">And also, one reason why it's particularly useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2124" target="_blank">00:35:24.700</a></span> | <span class="t">this approach, is that if you have a specific value in grid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2129" target="_blank">00:35:29.380</a></span> | <span class="t">search for one of the hyperparameters that just makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2132" target="_blank">00:35:32.100</a></span> | <span class="t">the experiment not work at all-- so learning rates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2135" target="_blank">00:35:35.100</a></span> | <span class="t">are a lot like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2136" target="_blank">00:35:36.300</a></span> | <span class="t">If you have a learning rate that's too high,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2138" target="_blank">00:35:38.620</a></span> | <span class="t">it's quite possible that convergence of the optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2142" target="_blank">00:35:42.180</a></span> | <span class="t">will not converge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2143" target="_blank">00:35:43.540</a></span> | <span class="t">Well, if you're using a grid search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2145" target="_blank">00:35:45.180</a></span> | <span class="t">it means that for all the experiments that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2147" target="_blank">00:35:47.060</a></span> | <span class="t">use that specific value of the learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2148" target="_blank">00:35:48.980</a></span> | <span class="t">they're all going to be garbage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2150" target="_blank">00:35:50.460</a></span> | <span class="t">They're all not going to be useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2152" target="_blank">00:35:52.340</a></span> | <span class="t">And you don't really get this sort of big waste of computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2156" target="_blank">00:35:56.220</a></span> | <span class="t">if you do a random search, because most likely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2158" target="_blank">00:35:58.540</a></span> | <span class="t">all the values of your hyperparameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2160" target="_blank">00:36:00.060</a></span> | <span class="t">are going to be unique, because they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2161" target="_blank">00:36:01.660</a></span> | <span class="t">samples, say, from a uniform distribution over some range.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2166" target="_blank">00:36:06.140</a></span> | <span class="t">So that actually works quite well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2168" target="_blank">00:36:08.620</a></span> | <span class="t">and it's quite recommended.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2170" target="_blank">00:36:10.580</a></span> | <span class="t">And there are more advanced methods,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2172" target="_blank">00:36:12.420</a></span> | <span class="t">like methods based on machine learning, Bayesian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2175" target="_blank">00:36:15.220</a></span> | <span class="t">optimization, or sometimes known as sequential model-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2178" target="_blank">00:36:18.460</a></span> | <span class="t">optimization, that I won't talk about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2181" target="_blank">00:36:21.300</a></span> | <span class="t">but that works a bit better than random search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2185" target="_blank">00:36:25.740</a></span> | <span class="t">And that's another alternative if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2187" target="_blank">00:36:27.780</a></span> | <span class="t">think you have an issue finding good hyperparameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2189" target="_blank">00:36:29.940</a></span> | <span class="t">is to investigate some of these more advanced methods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2194" target="_blank">00:36:34.380</a></span> | <span class="t">Now, you do this for most of your hyperparameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2197" target="_blank">00:36:37.180</a></span> | <span class="t">but for the number of epochs, the number of times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2200" target="_blank">00:36:40.060</a></span> | <span class="t">you go through all of your examples in your training set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2204" target="_blank">00:36:44.700</a></span> | <span class="t">what we usually do is not grid search or random search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2209" target="_blank">00:36:49.020</a></span> | <span class="t">but we use a thing known as early stopping.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2211" target="_blank">00:36:51.580</a></span> | <span class="t">The idea here is that if I've trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2213" target="_blank">00:36:53.460</a></span> | <span class="t">a neural net for 10 epochs, while training a neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2216" target="_blank">00:36:56.700</a></span> | <span class="t">with all the other hyperparameters kept constant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2219" target="_blank">00:36:59.340</a></span> | <span class="t">but one more epoch is easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2221" target="_blank">00:37:01.300</a></span> | <span class="t">I just do one more epoch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2222" target="_blank">00:37:02.820</a></span> | <span class="t">So I shouldn't start over and then do, say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2226" target="_blank">00:37:06.340</a></span> | <span class="t">11 epochs from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2228" target="_blank">00:37:08.580</a></span> | <span class="t">And so what we would do is we would just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2230" target="_blank">00:37:10.300</a></span> | <span class="t">track what is the performance on the validation set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2232" target="_blank">00:37:12.820</a></span> | <span class="t">as I do more and more epochs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2234" target="_blank">00:37:14.740</a></span> | <span class="t">And what we will typically see is the training error</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2237" target="_blank">00:37:17.100</a></span> | <span class="t">will go down, but the validation set performance will go down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2240" target="_blank">00:37:20.980</a></span> | <span class="t">and eventually go up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2242" target="_blank">00:37:22.900</a></span> | <span class="t">The intuition here is that the gap</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2245" target="_blank">00:37:25.080</a></span> | <span class="t">between the performance on the training set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2247" target="_blank">00:37:27.420</a></span> | <span class="t">and the performance on the validation set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2249" target="_blank">00:37:29.120</a></span> | <span class="t">will tend to increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2251" target="_blank">00:37:31.260</a></span> | <span class="t">And since the training curve cannot go below, usually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2254" target="_blank">00:37:34.460</a></span> | <span class="t">some bound, then eventually the validation set performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2258" target="_blank">00:37:38.280</a></span> | <span class="t">has to go up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2259" target="_blank">00:37:39.820</a></span> | <span class="t">Sometimes it won't necessarily go up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2261" target="_blank">00:37:41.320</a></span> | <span class="t">but it sort of stays stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2262" target="_blank">00:37:42.700</a></span> | <span class="t">So with early stopping, what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2264" target="_blank">00:37:44.080</a></span> | <span class="t">is that if we reach a point where the validation set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2266" target="_blank">00:37:46.260</a></span> | <span class="t">performance hasn't improved from some certain number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2269" target="_blank">00:37:49.020</a></span> | <span class="t">of iterations, which we refer to as the look ahead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2272" target="_blank">00:37:52.340</a></span> | <span class="t">we just stop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2273" target="_blank">00:37:53.340</a></span> | <span class="t">We go back to the neural net that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2274" target="_blank">00:37:54.660</a></span> | <span class="t">had the best performance overall in the validation set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2276" target="_blank">00:37:56.960</a></span> | <span class="t">and that's my neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2278" target="_blank">00:37:58.700</a></span> | <span class="t">So I have now a very cheap way of actually getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2281" target="_blank">00:38:01.460</a></span> | <span class="t">the number of iterations or the number of epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2283" target="_blank">00:38:03.960</a></span> | <span class="t">over my training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2287" target="_blank">00:38:07.220</a></span> | <span class="t">A few more tricks of the trade.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2289" target="_blank">00:38:09.500</a></span> | <span class="t">So it's always useful to normalize your data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2293" target="_blank">00:38:13.060</a></span> | <span class="t">It will often have the effect of speeding up training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2296" target="_blank">00:38:16.780</a></span> | <span class="t">If you have real value data for binary data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2299" target="_blank">00:38:19.340</a></span> | <span class="t">that's usually keep it as it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2301" target="_blank">00:38:21.940</a></span> | <span class="t">So what I mean by that is just subtract for each dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2304" target="_blank">00:38:24.940</a></span> | <span class="t">what is the average in the training set of that dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2307" target="_blank">00:38:27.700</a></span> | <span class="t">and then dividing by the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2309" target="_blank">00:38:29.660</a></span> | <span class="t">of each dimension again in my input space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2313" target="_blank">00:38:33.500</a></span> | <span class="t">So this can speed up training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2315" target="_blank">00:38:35.980</a></span> | <span class="t">We often use a decay on the learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2320" target="_blank">00:38:40.020</a></span> | <span class="t">There are a few methods for doing this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2321" target="_blank">00:38:41.660</a></span> | <span class="t">One that's very simple is to start with a large learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2325" target="_blank">00:38:45.300</a></span> | <span class="t">rate and then track the performance on the validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2327" target="_blank">00:38:47.460</a></span> | <span class="t">set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2328" target="_blank">00:38:48.140</a></span> | <span class="t">And once on the validation set it stops improving,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2330" target="_blank">00:38:50.980</a></span> | <span class="t">you decrease your learning rate by some ratio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2333" target="_blank">00:38:53.060</a></span> | <span class="t">Maybe you divide it by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2334" target="_blank">00:38:54.780</a></span> | <span class="t">And then you continue training for some time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2336" target="_blank">00:38:56.980</a></span> | <span class="t">Hopefully, the validation set performance starts improving.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2340" target="_blank">00:39:00.620</a></span> | <span class="t">And then at some point, it stops improving, and then you stop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2344" target="_blank">00:39:04.020</a></span> | <span class="t">Or you divide again by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2345" target="_blank">00:39:05.460</a></span> | <span class="t">So that sort of gives you an adaptive--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2348" target="_blank">00:39:08.180</a></span> | <span class="t">using the validation set, an adaptive way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2350" target="_blank">00:39:10.260</a></span> | <span class="t">of changing your learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2351" target="_blank">00:39:11.700</a></span> | <span class="t">And that can, again, work better than having a very small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2354" target="_blank">00:39:14.900</a></span> | <span class="t">learning rate than waiting for a longer time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2356" target="_blank">00:39:16.860</a></span> | <span class="t">So making very fast progress initially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2358" target="_blank">00:39:18.660</a></span> | <span class="t">and then slower progress towards the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2360" target="_blank">00:39:20.500</a></span> | <span class="t">Also, I've described so far the approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2366" target="_blank">00:39:26.260</a></span> | <span class="t">for training neural nets that is based on a single example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2370" target="_blank">00:39:30.740</a></span> | <span class="t">at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2371" target="_blank">00:39:31.300</a></span> | <span class="t">But in practice, we actually use what's called mini-batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2373" target="_blank">00:39:33.800</a></span> | <span class="t">That is, we compute the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2376" target="_blank">00:39:36.100</a></span> | <span class="t">on a small subset of examples, say, 64, 128.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2380" target="_blank">00:39:40.580</a></span> | <span class="t">And then we take the average of the loss of all these examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2383" target="_blank">00:39:43.580</a></span> | <span class="t">in that mini-batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2385" target="_blank">00:39:45.020</a></span> | <span class="t">And that's actually-- we compute the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2387" target="_blank">00:39:47.260</a></span> | <span class="t">of this average loss on that mini-batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2389" target="_blank">00:39:49.880</a></span> | <span class="t">The reason why we do this is that it turns out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2393" target="_blank">00:39:53.100</a></span> | <span class="t">that you can very efficiently implement the forward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2396" target="_blank">00:39:56.780</a></span> | <span class="t">over all of these 64, 128 examples in my mini-batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2401" target="_blank">00:40:01.260</a></span> | <span class="t">in one pass by, instead of doing vector matrix multiplications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2405" target="_blank">00:40:05.300</a></span> | <span class="t">when we compute the pre-activations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2407" target="_blank">00:40:07.280</a></span> | <span class="t">doing matrix-matrix multiplications, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2409" target="_blank">00:40:09.760</a></span> | <span class="t">are faster than doing multiple matrix-vector multiplications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2413" target="_blank">00:40:13.880</a></span> | <span class="t">So in your code, often, there will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2415" target="_blank">00:40:15.800</a></span> | <span class="t">be this other hyperparameter, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2417" target="_blank">00:40:17.480</a></span> | <span class="t">is mostly optimized for speed in terms of how quickly training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2421" target="_blank">00:40:21.080</a></span> | <span class="t">will proceed of the number of examples in your mini-batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2425" target="_blank">00:40:25.240</a></span> | <span class="t">Other things to improve optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2427" target="_blank">00:40:27.200</a></span> | <span class="t">might be using a thing like momentum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2429" target="_blank">00:40:29.560</a></span> | <span class="t">That is, instead of using, as the descent direction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2433" target="_blank">00:40:33.320</a></span> | <span class="t">the gradient of the loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2435" target="_blank">00:40:35.220</a></span> | <span class="t">I'm actually going to track a descent direction, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2439" target="_blank">00:40:39.040</a></span> | <span class="t">I'm going to compute as the current gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2441" target="_blank">00:40:41.160</a></span> | <span class="t">for my current example or mini-batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2444" target="_blank">00:40:44.080</a></span> | <span class="t">plus some fraction of the previous update,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2447" target="_blank">00:40:47.040</a></span> | <span class="t">the previous direction of update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2450" target="_blank">00:40:50.200</a></span> | <span class="t">And beta now is a hyperparameter you have to optimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2452" target="_blank">00:40:52.640</a></span> | <span class="t">So what this does is, if all the update directions agree</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2456" target="_blank">00:40:56.840</a></span> | <span class="t">across multiple updates, then it will start picking up momentum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2460" target="_blank">00:41:00.720</a></span> | <span class="t">and actually make bigger steps in those directions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2465" target="_blank">00:41:05.840</a></span> | <span class="t">And then there are multiple, even more advanced methods</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2468" target="_blank">00:41:08.760</a></span> | <span class="t">for adding adaptive types of learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2472" target="_blank">00:41:12.560</a></span> | <span class="t">I mentioned them here very quickly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2474" target="_blank">00:41:14.060</a></span> | <span class="t">because you might see them in papers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2475" target="_blank">00:41:15.600</a></span> | <span class="t">There's a method known as AdaGrad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2477" target="_blank">00:41:17.480</a></span> | <span class="t">where the learning rate is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2479" target="_blank">00:41:19.440</a></span> | <span class="t">scaled for each dimension, so for each weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2483" target="_blank">00:41:23.260</a></span> | <span class="t">and each biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2484" target="_blank">00:41:24.440</a></span> | <span class="t">It's going to be scaled by what is the square root</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2488" target="_blank">00:41:28.800</a></span> | <span class="t">of the cumulative sum of the squared gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2491" target="_blank">00:41:31.920</a></span> | <span class="t">So what I track is I take my gradient vector at each step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2495" target="_blank">00:41:35.360</a></span> | <span class="t">I do an element-wise square of all the dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2499" target="_blank">00:41:39.120</a></span> | <span class="t">of my gradients, my gradient vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2501" target="_blank">00:41:41.160</a></span> | <span class="t">And then I accumulate that in some variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2503" target="_blank">00:41:43.160</a></span> | <span class="t">that I'm noting as gamma here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2504" target="_blank">00:41:44.920</a></span> | <span class="t">And then for my descent direction, I take the gradient,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2507" target="_blank">00:41:47.840</a></span> | <span class="t">and I do an element-wise division</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2510" target="_blank">00:41:50.080</a></span> | <span class="t">by the square root of this cumulative sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2512" target="_blank">00:41:52.920</a></span> | <span class="t">of squared gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2514" target="_blank">00:41:54.720</a></span> | <span class="t">There's also RMSProp, which is essentially like AdaGrad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2517" target="_blank">00:41:57.440</a></span> | <span class="t">but instead of doing a cumulative sum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2519" target="_blank">00:41:59.640</a></span> | <span class="t">we're going to do an exponential moving average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2522" target="_blank">00:42:02.080</a></span> | <span class="t">So we take the previous value times some factor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2524" target="_blank">00:42:04.760</a></span> | <span class="t">plus 1 minus this factor times the current squared gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2528" target="_blank">00:42:08.960</a></span> | <span class="t">So that's RMSProp.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2530" target="_blank">00:42:10.380</a></span> | <span class="t">And then there's Adam, which is essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2532" target="_blank">00:42:12.720</a></span> | <span class="t">a combination of RMSProp with momentum, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2535" target="_blank">00:42:15.380</a></span> | <span class="t">is more involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2536" target="_blank">00:42:16.200</a></span> | <span class="t">And I won't have time to describe it here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2537" target="_blank">00:42:17.960</a></span> | <span class="t">but that's another method that's often actually implemented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2541" target="_blank">00:42:21.440</a></span> | <span class="t">in these different softwares and that people seem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2544" target="_blank">00:42:24.400</a></span> | <span class="t">to use with a lot of success.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2548" target="_blank">00:42:28.120</a></span> | <span class="t">And finally, in terms of actually debugging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2551" target="_blank">00:42:31.400</a></span> | <span class="t">your implementations--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2553" target="_blank">00:42:33.160</a></span> | <span class="t">so for instance, if you're lucky,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2555" target="_blank">00:42:35.280</a></span> | <span class="t">you can build your neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2556" target="_blank">00:42:36.720</a></span> | <span class="t">without difficulty using the current tools that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2558" target="_blank">00:42:38.680</a></span> | <span class="t">are available in Torch or TensorFlow or Tiano.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2561" target="_blank">00:42:41.240</a></span> | <span class="t">But maybe sometimes you actually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2562" target="_blank">00:42:42.840</a></span> | <span class="t">to implement certain gradients for a new module</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2565" target="_blank">00:42:45.640</a></span> | <span class="t">and a new box in your flow graph that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2567" target="_blank">00:42:47.920</a></span> | <span class="t">isn't currently supported.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2569" target="_blank">00:42:49.520</a></span> | <span class="t">If you do this, you should check that you've implemented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2571" target="_blank">00:42:51.960</a></span> | <span class="t">your gradients correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2573" target="_blank">00:42:53.760</a></span> | <span class="t">And one way of doing that is to actually compare</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2576" target="_blank">00:42:56.560</a></span> | <span class="t">the gradients computed by your code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2578" target="_blank">00:42:58.560</a></span> | <span class="t">with a finite difference of estimate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2581" target="_blank">00:43:01.240</a></span> | <span class="t">So what you do is, for each parameter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2583" target="_blank">00:43:03.160</a></span> | <span class="t">you add some very small epsilon value, say 10 to the minus 6,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2587" target="_blank">00:43:07.220</a></span> | <span class="t">and you compute what is the output of your module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2590" target="_blank">00:43:10.760</a></span> | <span class="t">And then you subtract the same thing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2592" target="_blank">00:43:12.360</a></span> | <span class="t">but where you've subtracted the small quantity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2595" target="_blank">00:43:15.600</a></span> | <span class="t">and then you divide by 2 epsilon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2597" target="_blank">00:43:17.400</a></span> | <span class="t">So if epsilon converges to 0, then you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2600" target="_blank">00:43:20.520</a></span> | <span class="t">get the partial derivative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2601" target="_blank">00:43:21.960</a></span> | <span class="t">But if it's just small, it's going to be an approximate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2604" target="_blank">00:43:24.240</a></span> | <span class="t">And usually, this finite difference estimate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2606" target="_blank">00:43:26.600</a></span> | <span class="t">will be very close to a correct implementation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2609" target="_blank">00:43:29.520</a></span> | <span class="t">of the real gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2610" target="_blank">00:43:30.840</a></span> | <span class="t">So you should definitely do that if you've actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2613" target="_blank">00:43:33.380</a></span> | <span class="t">implemented some of the gradients in your code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2616" target="_blank">00:43:36.160</a></span> | <span class="t">And then another useful thing to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2617" target="_blank">00:43:37.900</a></span> | <span class="t">is to actually do a very small experiment on a small data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2621" target="_blank">00:43:41.880</a></span> | <span class="t">before you actually run your full experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2624" target="_blank">00:43:44.320</a></span> | <span class="t">on your complete data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2625" target="_blank">00:43:45.880</a></span> | <span class="t">So use, say, 50 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2627" target="_blank">00:43:47.760</a></span> | <span class="t">So just taking a random subset of 50 examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2630" target="_blank">00:43:50.400</a></span> | <span class="t">from your data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2632" target="_blank">00:43:52.000</a></span> | <span class="t">Actually, just make sure that your code can overfit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2634" target="_blank">00:43:54.640</a></span> | <span class="t">to that data, can essentially classify it perfectly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2638" target="_blank">00:43:58.600</a></span> | <span class="t">given enough capacity that you would think it should get it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2643" target="_blank">00:44:03.040</a></span> | <span class="t">So if it's not the case, then there's a few things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2646" target="_blank">00:44:06.300</a></span> | <span class="t">that you might want to investigate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2648" target="_blank">00:44:08.320</a></span> | <span class="t">Maybe your initialization is such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2649" target="_blank">00:44:09.920</a></span> | <span class="t">that the units are already saturated initially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2652" target="_blank">00:44:12.620</a></span> | <span class="t">and so there's no actual optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2654" target="_blank">00:44:14.800</a></span> | <span class="t">happening because some of the gradients on some of the weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2657" target="_blank">00:44:17.440</a></span> | <span class="t">are exactly zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2659" target="_blank">00:44:19.040</a></span> | <span class="t">So you might want to check your initialization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2662" target="_blank">00:44:22.160</a></span> | <span class="t">Maybe your gradients are just--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2663" target="_blank">00:44:23.760</a></span> | <span class="t">you're using a model you implemented gradients for,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2666" target="_blank">00:44:26.040</a></span> | <span class="t">and maybe your gradients are not properly implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2668" target="_blank">00:44:28.920</a></span> | <span class="t">Maybe you haven't normalized your input, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2671" target="_blank">00:44:31.040</a></span> | <span class="t">creates some instability, making it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2673" target="_blank">00:44:33.040</a></span> | <span class="t">harder for stochastic gradient descent to work successfully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2678" target="_blank">00:44:38.400</a></span> | <span class="t">Maybe your learning rate is too large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2680" target="_blank">00:44:40.080</a></span> | <span class="t">Then you should consider trying smaller learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2682" target="_blank">00:44:42.840</a></span> | <span class="t">That's actually a pretty good way of adding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2684" target="_blank">00:44:44.940</a></span> | <span class="t">some idea of the magnitude of the learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2687" target="_blank">00:44:47.640</a></span> | <span class="t">you should be using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2689" target="_blank">00:44:49.320</a></span> | <span class="t">And then once you actually overfit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2691" target="_blank">00:44:51.680</a></span> | <span class="t">in your small training set, you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2693" target="_blank">00:44:53.060</a></span> | <span class="t">ready to do a full experiment on a larger data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2696" target="_blank">00:44:56.640</a></span> | <span class="t">That said, this is not a replacement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2698" target="_blank">00:44:58.880</a></span> | <span class="t">for gradient checking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2700" target="_blank">00:45:00.240</a></span> | <span class="t">So backprop and stochastic gradient descent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2703" target="_blank">00:45:03.440</a></span> | <span class="t">it's a great algorithm that's very bug resistant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2706" target="_blank">00:45:06.800</a></span> | <span class="t">You will potentially see some learning happening,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2710" target="_blank">00:45:10.540</a></span> | <span class="t">even if some of your gradients are wrong,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2712" target="_blank">00:45:12.400</a></span> | <span class="t">or say, exactly zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2713" target="_blank">00:45:13.840</a></span> | <span class="t">So that's great if you're an engineer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2716" target="_blank">00:45:16.160</a></span> | <span class="t">and you're implementing things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2717" target="_blank">00:45:17.800</a></span> | <span class="t">It's fun when code is somewhat bug resistant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2719" target="_blank">00:45:19.960</a></span> | <span class="t">But if you're actually doing science</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2721" target="_blank">00:45:21.600</a></span> | <span class="t">and trying to understand what's going on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2723" target="_blank">00:45:23.960</a></span> | <span class="t">that can be a complication.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2725" target="_blank">00:45:25.160</a></span> | <span class="t">So do both, gradient checking and a small experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2729" target="_blank">00:45:29.160</a></span> | <span class="t">like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2731" target="_blank">00:45:31.120</a></span> | <span class="t">All right, and so for the last few minutes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2733" target="_blank">00:45:33.000</a></span> | <span class="t">I'll actually try to motivate what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2735" target="_blank">00:45:35.040</a></span> | <span class="t">you'll be learning quite a bit about in the next two days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2740" target="_blank">00:45:40.280</a></span> | <span class="t">That is, the specific case for deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2743" target="_blank">00:45:43.880</a></span> | <span class="t">So I've already told you that if I have a neural net with enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2747" target="_blank">00:45:47.760</a></span> | <span class="t">hidden units, theoretically, I can potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2749" target="_blank">00:45:49.880</a></span> | <span class="t">represent pretty much any function, any classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2753" target="_blank">00:45:53.000</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2753" target="_blank">00:45:53.920</a></span> | <span class="t">So why would I want multiple layers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2756" target="_blank">00:45:56.360</a></span> | <span class="t">So there are a few motivations behind this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2759" target="_blank">00:45:59.000</a></span> | <span class="t">The first one is taken directly from our own brains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2762" target="_blank">00:46:02.160</a></span> | <span class="t">So we know in the visual cortex that the light that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2765" target="_blank">00:46:05.320</a></span> | <span class="t">hits our retina eventually goes through several regions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2768" target="_blank">00:46:08.520</a></span> | <span class="t">in the visual cortex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2769" target="_blank">00:46:09.880</a></span> | <span class="t">Eventually reaching an area known as V1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2772" target="_blank">00:46:12.520</a></span> | <span class="t">where you have units that are-- or neurons that are essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2776" target="_blank">00:46:16.040</a></span> | <span class="t">tuned to small forms like edges.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2778" target="_blank">00:46:18.840</a></span> | <span class="t">And then it goes on to V4, where it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2780" target="_blank">00:46:20.480</a></span> | <span class="t">slightly more complex patterns that the units are tuned for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2783" target="_blank">00:46:23.880</a></span> | <span class="t">And then you reach AIT, where you actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2785" target="_blank">00:46:25.680</a></span> | <span class="t">have neurons that are specific to certain objects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2787" target="_blank">00:46:27.680</a></span> | <span class="t">or certain units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2788" target="_blank">00:46:28.840</a></span> | <span class="t">And so the idea here is that perhaps that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2790" target="_blank">00:46:30.960</a></span> | <span class="t">also what we want in an artificial, say, vision system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2795" target="_blank">00:46:35.800</a></span> | <span class="t">We'd like it, if it's detecting faces,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2797" target="_blank">00:46:37.760</a></span> | <span class="t">to have a first layer that detects simple edges,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2801" target="_blank">00:46:41.080</a></span> | <span class="t">and then another layer that perhaps puts these edges</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2803" target="_blank">00:46:43.520</a></span> | <span class="t">together, detecting slightly more complex things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2805" target="_blank">00:46:45.840</a></span> | <span class="t">like a nose or a mouth or eyes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2807" target="_blank">00:46:47.880</a></span> | <span class="t">And then eventually have a layer that combines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2809" target="_blank">00:46:49.880</a></span> | <span class="t">these slightly less abstract or more abstract units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2814" target="_blank">00:46:54.520</a></span> | <span class="t">to get something even more abstract,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2816" target="_blank">00:46:56.200</a></span> | <span class="t">like a complete face.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2818" target="_blank">00:46:58.540</a></span> | <span class="t">There's also some theoretical justification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2820" target="_blank">00:47:00.680</a></span> | <span class="t">for using multiple layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2824" target="_blank">00:47:04.160</a></span> | <span class="t">So the early results were mostly based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2826" target="_blank">00:47:06.360</a></span> | <span class="t">on studying Boolean functions, or a function that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2828" target="_blank">00:47:08.920</a></span> | <span class="t">takes as input--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2830" target="_blank">00:47:10.160</a></span> | <span class="t">can think of it as a vector of just zeros and ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2832" target="_blank">00:47:12.640</a></span> | <span class="t">And you could show that there are certain functions that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2837" target="_blank">00:47:17.080</a></span> | <span class="t">if you had essentially a Boolean neural network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2839" target="_blank">00:47:19.960</a></span> | <span class="t">or essentially a Boolean circuit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2842" target="_blank">00:47:22.840</a></span> | <span class="t">and you restricted the number of layers of that circuit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2846" target="_blank">00:47:26.280</a></span> | <span class="t">that there are certain functions that, in this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2848" target="_blank">00:47:28.480</a></span> | <span class="t">to represent certain Boolean functions exactly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2851" target="_blank">00:47:31.160</a></span> | <span class="t">you would need an exponential number of units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2853" target="_blank">00:47:33.600</a></span> | <span class="t">in each of these layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2855" target="_blank">00:47:35.160</a></span> | <span class="t">Whereas if you allowed yourself to have multiple layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2857" target="_blank">00:47:37.160</a></span> | <span class="t">then you could represent these functions more compactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2859" target="_blank">00:47:39.840</a></span> | <span class="t">And so that's another motivation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2861" target="_blank">00:47:41.560</a></span> | <span class="t">that perhaps with more layers, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2862" target="_blank">00:47:42.960</a></span> | <span class="t">can represent fairly complex functions in a more compact way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2868" target="_blank">00:47:48.520</a></span> | <span class="t">And then there's the reason that they just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2871" target="_blank">00:47:51.160</a></span> | <span class="t">So we've seen in the past few years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2873" target="_blank">00:47:53.920</a></span> | <span class="t">great success in speech recognition, where it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2876" target="_blank">00:47:56.440</a></span> | <span class="t">essentially revolutionized the field, where everyone's using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2879" target="_blank">00:47:59.020</a></span> | <span class="t">deep learning for speech recognition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2880" target="_blank">00:48:00.940</a></span> | <span class="t">and same thing for visual object recognition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2883" target="_blank">00:48:03.840</a></span> | <span class="t">where, again, deep learning is sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2885" target="_blank">00:48:05.400</a></span> | <span class="t">of the method of choice for identifying objects in images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2890" target="_blank">00:48:10.520</a></span> | <span class="t">So then why are we doing this only recently?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2893" target="_blank">00:48:13.760</a></span> | <span class="t">Why didn't we do deep learning way back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2896" target="_blank">00:48:16.640</a></span> | <span class="t">when backprop was invented, which is essentially in 1980s</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2900" target="_blank">00:48:20.760</a></span> | <span class="t">and even before that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2902" target="_blank">00:48:22.840</a></span> | <span class="t">So it turns out training deep neural networks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2904" target="_blank">00:48:24.760</a></span> | <span class="t">is actually not that easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2906" target="_blank">00:48:26.040</a></span> | <span class="t">There are a few hurdles that one can be confronted with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2910" target="_blank">00:48:30.000</a></span> | <span class="t">I've already mentioned one of the issues, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2912" target="_blank">00:48:32.120</a></span> | <span class="t">is that some of the gradients might be fading as you go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2915" target="_blank">00:48:35.640</a></span> | <span class="t">from the top layer to the bottom layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2917" target="_blank">00:48:37.280</a></span> | <span class="t">because we keep multiplying by the derivative of the activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2919" target="_blank">00:48:39.940</a></span> | <span class="t">function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2920" target="_blank">00:48:40.520</a></span> | <span class="t">So that makes training hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2921" target="_blank">00:48:41.960</a></span> | <span class="t">It could be that the lower layers at very small gradients</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2924" target="_blank">00:48:44.560</a></span> | <span class="t">are barely moving and exploring the space of correct features</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2929" target="_blank">00:48:49.040</a></span> | <span class="t">to learn for a given problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2931" target="_blank">00:48:51.320</a></span> | <span class="t">Sometimes that's the problem you find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2932" target="_blank">00:48:52.960</a></span> | <span class="t">You have a hard time just fitting your data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2934" target="_blank">00:48:54.880</a></span> | <span class="t">and you're essentially underfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2937" target="_blank">00:48:57.080</a></span> | <span class="t">Or it could be that with deeper neural nets or bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2940" target="_blank">00:49:00.360</a></span> | <span class="t">neural nets, we have more parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2942" target="_blank">00:49:02.200</a></span> | <span class="t">So perhaps sometimes we're actually overfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2944" target="_blank">00:49:04.240</a></span> | <span class="t">We're in a situation where all the functions that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2947" target="_blank">00:49:07.680</a></span> | <span class="t">can represent with the same neural net represented</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2950" target="_blank">00:49:10.840</a></span> | <span class="t">by this gray area function actually includes, yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2954" target="_blank">00:49:14.040</a></span> | <span class="t">the right function, but it's so large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2955" target="_blank">00:49:15.660</a></span> | <span class="t">that for a finite training set, the odds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2958" target="_blank">00:49:18.280</a></span> | <span class="t">that I'm going to find the one that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2959" target="_blank">00:49:19.780</a></span> | <span class="t">close to the true classifying function, the real system</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2962" target="_blank">00:49:22.840</a></span> | <span class="t">that I'd like to have, is going to be very different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2965" target="_blank">00:49:25.720</a></span> | <span class="t">So in this case, I'm essentially overfitting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2968" target="_blank">00:49:28.080</a></span> | <span class="t">and that might also be a situation we're in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2971" target="_blank">00:49:31.200</a></span> | <span class="t">And unfortunately, there are many situations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2975" target="_blank">00:49:35.880</a></span> | <span class="t">where one problem is observed, overfitting or underfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2980" target="_blank">00:49:40.920</a></span> | <span class="t">And so we essentially have, in the field,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2983" target="_blank">00:49:43.480</a></span> | <span class="t">developed tools for fighting both situations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2986" target="_blank">00:49:46.140</a></span> | <span class="t">And I'm going to rapidly touch a few of those, which you will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2989" target="_blank">00:49:49.600</a></span> | <span class="t">see will come up later on in multiple talks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2993" target="_blank">00:49:53.560</a></span> | <span class="t">So one of the first hypotheses, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2996" target="_blank">00:49:56.320</a></span> | <span class="t">might be that you're underfitting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2997" target="_blank">00:49:57.760</a></span> | <span class="t">well, you can essentially just fight this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=2999" target="_blank">00:49:59.960</a></span> | <span class="t">by waiting longer, so training longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3001" target="_blank">00:50:01.880</a></span> | <span class="t">If you have your gradients are too small,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3003" target="_blank">00:50:03.600</a></span> | <span class="t">and this is essentially why you're progressing very slowly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3005" target="_blank">00:50:05.800</a></span> | <span class="t">when you're training, well, if you're using GPUs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3008" target="_blank">00:50:08.220</a></span> | <span class="t">and are able to do more iterations over the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3011" target="_blank">00:50:11.480</a></span> | <span class="t">training set in less time, that might just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3015" target="_blank">00:50:15.120</a></span> | <span class="t">solve your problem of underfitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3016" target="_blank">00:50:16.800</a></span> | <span class="t">And I think we've seen some of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3018" target="_blank">00:50:18.760</a></span> | <span class="t">and this is partly why GPUs have been so game-changing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3021" target="_blank">00:50:21.340</a></span> | <span class="t">for deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3022" target="_blank">00:50:22.520</a></span> | <span class="t">Or you can use just better optimization methods also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3026" target="_blank">00:50:26.120</a></span> | <span class="t">And if you're overfitting, well, we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3028" target="_blank">00:50:28.200</a></span> | <span class="t">need better regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3031" target="_blank">00:50:31.560</a></span> | <span class="t">I've been involved early on in my PhD</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3033" target="_blank">00:50:33.680</a></span> | <span class="t">on using unsupervised learning as a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3036" target="_blank">00:50:36.040</a></span> | <span class="t">to regularize neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3038" target="_blank">00:50:38.820</a></span> | <span class="t">If I have time, I'll talk a little bit about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3040" target="_blank">00:50:40.940</a></span> | <span class="t">And there's another method you might have heard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3043" target="_blank">00:50:43.200</a></span> | <span class="t">about known as dropout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3044" target="_blank">00:50:44.840</a></span> | <span class="t">So I'll try to touch at least two methods that are essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3049" target="_blank">00:50:49.560</a></span> | <span class="t">trying to address some of these issues.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3051" target="_blank">00:50:51.360</a></span> | <span class="t">So the first one that I'll talk about is dropout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3054" target="_blank">00:50:54.720</a></span> | <span class="t">It's actually very easy, very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3057" target="_blank">00:50:57.920</a></span> | <span class="t">So the idea of if our neural net is essentially overfitting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3061" target="_blank">00:51:01.120</a></span> | <span class="t">so it's too good at training on the training set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3064" target="_blank">00:51:04.520</a></span> | <span class="t">well, we're essentially going to cripple training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3066" target="_blank">00:51:06.880</a></span> | <span class="t">We're going to make it harder to fit the training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3069" target="_blank">00:51:09.240</a></span> | <span class="t">And the way we're going to do that in dropout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3071" target="_blank">00:51:11.240</a></span> | <span class="t">is that we will stochastically remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3073" target="_blank">00:51:13.880</a></span> | <span class="t">hidden units independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3076" target="_blank">00:51:16.120</a></span> | <span class="t">So for each hidden unit, before we do a forward pass,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3078" target="_blank">00:51:18.880</a></span> | <span class="t">we'll flip a coin.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3080" target="_blank">00:51:20.240</a></span> | <span class="t">And with probability half, we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3082" target="_blank">00:51:22.880</a></span> | <span class="t">multiply the activation by 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3084" target="_blank">00:51:24.800</a></span> | <span class="t">And with probability half, we'll multiply it by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3087" target="_blank">00:51:27.500</a></span> | <span class="t">So what this means is that if a unit is multiplied by 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3090" target="_blank">00:51:30.840</a></span> | <span class="t">it's effectively not in the neural net anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3094" target="_blank">00:51:34.120</a></span> | <span class="t">And we're doing this independently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3096" target="_blank">00:51:36.360</a></span> | <span class="t">for each hidden units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3097" target="_blank">00:51:37.880</a></span> | <span class="t">So that means that in a layer, a unit cannot rely anymore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3101" target="_blank">00:51:41.840</a></span> | <span class="t">on the presence on any other units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3104" target="_blank">00:51:44.760</a></span> | <span class="t">to try to sort of synchronize and adapt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3107" target="_blank">00:51:47.880</a></span> | <span class="t">to perform a complex classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3110" target="_blank">00:51:50.600</a></span> | <span class="t">or learn a complex feature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3112" target="_blank">00:51:52.360</a></span> | <span class="t">And that was partly the motivation behind dropout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3114" target="_blank">00:51:54.680</a></span> | <span class="t">is that this procedure might encourage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3117" target="_blank">00:51:57.480</a></span> | <span class="t">types of features that are not co-adapted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3119" target="_blank">00:51:59.760</a></span> | <span class="t">and are less likely to overfit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3122" target="_blank">00:52:02.880</a></span> | <span class="t">So we often use 0.5 as the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3125" target="_blank">00:52:05.960</a></span> | <span class="t">of dropping out a unit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3128" target="_blank">00:52:08.040</a></span> | <span class="t">It turns out it often, surprisingly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3130" target="_blank">00:52:10.040</a></span> | <span class="t">is the best value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3131" target="_blank">00:52:11.160</a></span> | <span class="t">But that's another hyperparameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3132" target="_blank">00:52:12.920</a></span> | <span class="t">you might want to tune.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3135" target="_blank">00:52:15.060</a></span> | <span class="t">And in terms of how it impacts an implementation of backdrop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3138" target="_blank">00:52:18.560</a></span> | <span class="t">it's very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3140" target="_blank">00:52:20.000</a></span> | <span class="t">So the forward pass, before I do it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3141" target="_blank">00:52:21.600</a></span> | <span class="t">I just sample my binary masks for all my layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3144" target="_blank">00:52:24.760</a></span> | <span class="t">And then when I'm performing backdrop, well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3148" target="_blank">00:52:28.240</a></span> | <span class="t">my gradient on the--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3150" target="_blank">00:52:30.000</a></span> | <span class="t">oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3150" target="_blank">00:52:30.480</a></span> | <span class="t">So that's the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3151" target="_blank">00:52:31.580</a></span> | <span class="t">I'm just multiplying by this binary mask here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3154" target="_blank">00:52:34.760</a></span> | <span class="t">So super simple change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3156" target="_blank">00:52:36.640</a></span> | <span class="t">And then in terms of backdrop, well, I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3159" target="_blank">00:52:39.000</a></span> | <span class="t">also going to multiply by the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3161" target="_blank">00:52:41.160</a></span> | <span class="t">when I get my gradient on the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3163" target="_blank">00:52:43.800</a></span> | <span class="t">And also, don't forget that the activations are now different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3167" target="_blank">00:52:47.080</a></span> | <span class="t">They actually include the mask in my notation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3169" target="_blank">00:52:49.920</a></span> | <span class="t">So it's a very simple change in the forward and backward pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3172" target="_blank">00:52:52.860</a></span> | <span class="t">when you're training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3174" target="_blank">00:52:54.280</a></span> | <span class="t">And also, another thing that I should emphasize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3176" target="_blank">00:52:56.280</a></span> | <span class="t">is that the mask is being resampled for every example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3179" target="_blank">00:52:59.600</a></span> | <span class="t">So before you do a forward pass, you resample the mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3182" target="_blank">00:53:02.160</a></span> | <span class="t">You don't keep it--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3183" target="_blank">00:53:03.280</a></span> | <span class="t">sample it once and then use it the whole time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3187" target="_blank">00:53:07.560</a></span> | <span class="t">And then at test time, because we don't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3189" target="_blank">00:53:09.560</a></span> | <span class="t">like a model that sort of randomly changes its output,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3193" target="_blank">00:53:13.600</a></span> | <span class="t">because it will if we stochastically change the masks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3196" target="_blank">00:53:16.840</a></span> | <span class="t">what we do is we replace the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3198" target="_blank">00:53:18.920</a></span> | <span class="t">by the probability of dropping out a unit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3203" target="_blank">00:53:23.600</a></span> | <span class="t">or actually of keeping a unit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3205" target="_blank">00:53:25.480</a></span> | <span class="t">So if we're using 0.5, that's just 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3209" target="_blank">00:53:29.200</a></span> | <span class="t">We can actually show that if you have a neural net</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3211" target="_blank">00:53:31.360</a></span> | <span class="t">with a single hidden layer, doing this transformation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3214" target="_blank">00:53:34.720</a></span> | <span class="t">at test time, multiplying by 0.5 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3216" target="_blank">00:53:36.760</a></span> | <span class="t">equivalent to doing a geometric average of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3219" target="_blank">00:53:39.640</a></span> | <span class="t">the possible neural networks with all the different binary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3222" target="_blank">00:53:42.120</a></span> | <span class="t">mask patterns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3223" target="_blank">00:53:43.360</a></span> | <span class="t">So it's essentially one way of thinking about dropout</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3226" target="_blank">00:53:46.600</a></span> | <span class="t">in the single layer case is that it's kind of an assembling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3229" target="_blank">00:53:49.020</a></span> | <span class="t">method where you have a lot of models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3231" target="_blank">00:53:51.040</a></span> | <span class="t">an exponential number of models, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3232" target="_blank">00:53:52.760</a></span> | <span class="t">are all sharing the same weights but have different masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3236" target="_blank">00:53:56.040</a></span> | <span class="t">That intuition, though, doesn't transfer for deep neural nets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3239" target="_blank">00:53:59.520</a></span> | <span class="t">in the sense that you cannot show this result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3241" target="_blank">00:54:01.480</a></span> | <span class="t">It really only applies to a single hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3245" target="_blank">00:54:05.920</a></span> | <span class="t">So in practice, it's very effective,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3248" target="_blank">00:54:08.480</a></span> | <span class="t">but do expect some slowdown in training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3250" target="_blank">00:54:10.640</a></span> | <span class="t">So often, we tend to see that training a network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3253" target="_blank">00:54:13.640</a></span> | <span class="t">to completion will take twice as many epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3256" target="_blank">00:54:16.160</a></span> | <span class="t">if you're using dropout with 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3258" target="_blank">00:54:18.360</a></span> | <span class="t">And here, you have the reference if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3259" target="_blank">00:54:19.980</a></span> | <span class="t">want to learn more about different variations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3261" target="_blank">00:54:21.920</a></span> | <span class="t">of dropouts and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3263" target="_blank">00:54:23.000</a></span> | <span class="t">And I probably won't talk about unsupervised retraining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3268" target="_blank">00:54:28.920</a></span> | <span class="t">for lack of time, but I'll talk about another thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3271" target="_blank">00:54:31.040</a></span> | <span class="t">that you'll definitely probably hear about,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3273" target="_blank">00:54:33.440</a></span> | <span class="t">and that's implemented in these different packages, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3275" target="_blank">00:54:35.740</a></span> | <span class="t">is batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3277" target="_blank">00:54:37.360</a></span> | <span class="t">Batch normalization is kind of interesting in the sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3279" target="_blank">00:54:39.660</a></span> | <span class="t">that it's been shown to better optimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3282" target="_blank">00:54:42.840</a></span> | <span class="t">That is, certain networks that would otherwise underfit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3286" target="_blank">00:54:46.000</a></span> | <span class="t">would not underfit as much anymore</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3288" target="_blank">00:54:48.440</a></span> | <span class="t">if you use batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3289" target="_blank">00:54:49.840</a></span> | <span class="t">But also, it's been shown that when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3291" target="_blank">00:54:51.440</a></span> | <span class="t">you use batch normalization, dropout is not as useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3294" target="_blank">00:54:54.720</a></span> | <span class="t">And dropout being a regularization method,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3296" target="_blank">00:54:56.720</a></span> | <span class="t">that suggests that perhaps batch normalization is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3299" target="_blank">00:54:59.760</a></span> | <span class="t">regularizing in some way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3301" target="_blank">00:55:01.280</a></span> | <span class="t">So these things are not one or the other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3303" target="_blank">00:55:03.800</a></span> | <span class="t">They're not mutually exclusive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3305" target="_blank">00:55:05.160</a></span> | <span class="t">You can have a regularizer that also, it turns out,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3307" target="_blank">00:55:07.440</a></span> | <span class="t">helps you better optimize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3310" target="_blank">00:55:10.240</a></span> | <span class="t">So the intuition behind batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3314" target="_blank">00:55:14.760</a></span> | <span class="t">is much like I've suggested that normalizing your inputs actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3319" target="_blank">00:55:19.560</a></span> | <span class="t">can help speeding up training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3321" target="_blank">00:55:21.120</a></span> | <span class="t">Well, how about we also normalize all the hidden layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3324" target="_blank">00:55:24.380</a></span> | <span class="t">when I'm doing my forward pass?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3327" target="_blank">00:55:27.600</a></span> | <span class="t">Now, the problem in doing this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3328" target="_blank">00:55:28.960</a></span> | <span class="t">that I can compute the mean and the standard deviations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3331" target="_blank">00:55:31.880</a></span> | <span class="t">of my inputs once and for all because they're constant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3335" target="_blank">00:55:35.000</a></span> | <span class="t">But my hidden layers are constantly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3336" target="_blank">00:55:36.640</a></span> | <span class="t">changing because I'm training these parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3339" target="_blank">00:55:39.080</a></span> | <span class="t">So the mean and the standard deviation of my units</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3341" target="_blank">00:55:41.640</a></span> | <span class="t">will change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3342" target="_blank">00:55:42.800</a></span> | <span class="t">And so it would be very expensive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3345" target="_blank">00:55:45.920</a></span> | <span class="t">if every time I did an update on my parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3348" target="_blank">00:55:48.120</a></span> | <span class="t">I recomputed the means and the standard deviations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3350" target="_blank">00:55:50.540</a></span> | <span class="t">of all of my units.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3352" target="_blank">00:55:52.400</a></span> | <span class="t">So batch normalization addresses some of these issues</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3354" target="_blank">00:55:54.980</a></span> | <span class="t">as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3356" target="_blank">00:55:56.360</a></span> | <span class="t">So the way it works is first, the normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3359" target="_blank">00:55:59.520</a></span> | <span class="t">is going to be applied on actually the pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3362" target="_blank">00:56:02.240</a></span> | <span class="t">So not the activation of the unit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3363" target="_blank">00:56:03.660</a></span> | <span class="t">but before the non-linearity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3366" target="_blank">00:56:06.480</a></span> | <span class="t">During training, to address the issue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3368" target="_blank">00:56:08.600</a></span> | <span class="t">that we don't want to compute means over the full training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3371" target="_blank">00:56:11.120</a></span> | <span class="t">set because that would be too slow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3372" target="_blank">00:56:12.720</a></span> | <span class="t">I'm actually going to compute it on each mini-batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3375" target="_blank">00:56:15.960</a></span> | <span class="t">So I have to do mini-batch training here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3378" target="_blank">00:56:18.120</a></span> | <span class="t">I'm going to take my small mini-batch of 64, 128 examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3381" target="_blank">00:56:21.560</a></span> | <span class="t">And that's the set of examples on which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3383" target="_blank">00:56:23.440</a></span> | <span class="t">I'm going to compute my means and standard deviations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3386" target="_blank">00:56:26.800</a></span> | <span class="t">And then when I do backprop, I'm actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3388" target="_blank">00:56:28.920</a></span> | <span class="t">going to take into account the normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3391" target="_blank">00:56:31.160</a></span> | <span class="t">So now there's going to be a gradient going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3393" target="_blank">00:56:33.280</a></span> | <span class="t">through the computation of the mean and the standard deviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3396" target="_blank">00:56:36.360</a></span> | <span class="t">because they depend on the parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3398" target="_blank">00:56:38.320</a></span> | <span class="t">of the neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3400" target="_blank">00:56:40.160</a></span> | <span class="t">And then at test time, we'll just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3401" target="_blank">00:56:41.540</a></span> | <span class="t">use the global mean and global standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3404" target="_blank">00:56:44.200</a></span> | <span class="t">Once I finish training, I can actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3405" target="_blank">00:56:45.880</a></span> | <span class="t">do a full pass over the whole training set and get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3408" target="_blank">00:56:48.120</a></span> | <span class="t">all of my means and standard deviations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3412" target="_blank">00:56:52.000</a></span> | <span class="t">So that's essentially the pseudocode for that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3414" target="_blank">00:56:54.680</a></span> | <span class="t">taken out of the paper directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3417" target="_blank">00:56:57.360</a></span> | <span class="t">So if x is a pre-activation for a unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3420" target="_blank">00:57:00.520</a></span> | <span class="t">and have multiple pre-activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3422" target="_blank">00:57:02.640</a></span> | <span class="t">for a single unit across my mini-batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3425" target="_blank">00:57:05.240</a></span> | <span class="t">I would compute what is the average for that unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3428" target="_blank">00:57:08.280</a></span> | <span class="t">pre-activation across my examples in my mini-batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3431" target="_blank">00:57:11.400</a></span> | <span class="t">compute my variance, and then subtract the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3435" target="_blank">00:57:15.000</a></span> | <span class="t">and divide by the square root of the variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3437" target="_blank">00:57:17.760</a></span> | <span class="t">plus some epsilon for numerical stability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3439" target="_blank">00:57:19.680</a></span> | <span class="t">in case the variance is too close to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3442" target="_blank">00:57:22.160</a></span> | <span class="t">And then another thing is that actually batch normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3445" target="_blank">00:57:25.200</a></span> | <span class="t">doesn't just perform this normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3448" target="_blank">00:57:28.080</a></span> | <span class="t">and outputs the normalized pre-activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3450" target="_blank">00:57:30.400</a></span> | <span class="t">It then actually performs a linear transformation on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3454" target="_blank">00:57:34.640</a></span> | <span class="t">So it multiplies it by this parameter gamma,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3457" target="_blank">00:57:37.160</a></span> | <span class="t">which is going to be trained by gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3460" target="_blank">00:57:40.160</a></span> | <span class="t">And it's often called the gain parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3463" target="_blank">00:57:43.240</a></span> | <span class="t">of batch normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3465" target="_blank">00:57:45.760</a></span> | <span class="t">And it adds a bias beta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3467" target="_blank">00:57:47.960</a></span> | <span class="t">And the reason is that if I'm subtracting by the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3471" target="_blank">00:57:51.360</a></span> | <span class="t">then each of these units have the bias parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3474" target="_blank">00:57:54.620</a></span> | <span class="t">So if I subtract it, then this essentially here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3478" target="_blank">00:57:58.280</a></span> | <span class="t">there's no bias anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3479" target="_blank">00:57:59.680</a></span> | <span class="t">It was present here, it was present here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3481" target="_blank">00:58:01.360</a></span> | <span class="t">and now it's been subtracted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3482" target="_blank">00:58:02.720</a></span> | <span class="t">So I have to add the bias, but after the batch normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3485" target="_blank">00:58:05.580</a></span> | <span class="t">essentially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3486" target="_blank">00:58:06.360</a></span> | <span class="t">So these betas here are essentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3488" target="_blank">00:58:08.240</a></span> | <span class="t">the new bias parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3490" target="_blank">00:58:10.380</a></span> | <span class="t">And those will actually be trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3491" target="_blank">00:58:11.800</a></span> | <span class="t">So we do gradient descent also on those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3493" target="_blank">00:58:13.760</a></span> | <span class="t">So batch normalization adds a few parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3498" target="_blank">00:58:18.120</a></span> | <span class="t">All right, and as I said, I'm just going to skip over this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3500" target="_blank">00:58:20.640</a></span> | <span class="t">And I'm not showing what the gradients are when you back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3503" target="_blank">00:58:23.200</a></span> | <span class="t">prop through the mean and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3504" target="_blank">00:58:24.760</a></span> | <span class="t">It's described in the paper if you want to see the gradients.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3507" target="_blank">00:58:27.300</a></span> | <span class="t">But otherwise, in the different packages,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3511" target="_blank">00:58:31.200</a></span> | <span class="t">you'll get the gradients automatically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3513" target="_blank">00:58:33.160</a></span> | <span class="t">It's usually been implemented.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3515" target="_blank">00:58:35.580</a></span> | <span class="t">Skipping over that, I'll just finish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3518" target="_blank">00:58:38.080</a></span> | <span class="t">If you actually want to learn about unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3520" target="_blank">00:58:40.160</a></span> | <span class="t">pre-training and why it works, I have videos on that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3522" target="_blank">00:58:42.480</a></span> | <span class="t">So you can check that out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3524" target="_blank">00:58:44.200</a></span> | <span class="t">And I guess that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3526" target="_blank">00:58:46.160</a></span> | <span class="t">Thank you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3526" target="_blank">00:58:46.960</a></span> | <span class="t">[APPLAUSE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3529" target="_blank">00:58:49.880</a></span> | <span class="t">Thanks, Hugo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3536" target="_blank">00:58:56.040</a></span> | <span class="t">So we have a few minutes for questions which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3538" target="_blank">00:58:58.400</a></span> | <span class="t">are intermingled with a break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3540" target="_blank">00:59:00.520</a></span> | <span class="t">So feel free to either go for a break or ask questions to Hugo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3544" target="_blank">00:59:04.800</a></span> | <span class="t">I believe there are microphones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3546" target="_blank">00:59:06.220</a></span> | <span class="t">And I'll also stick around.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3547" target="_blank">00:59:07.760</a></span> | <span class="t">So if you want to ask me questions offline,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3549" target="_blank">00:59:09.880</a></span> | <span class="t">that's also fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3550" target="_blank">00:59:10.960</a></span> | <span class="t">If anyone has questions, you can go to the mic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3553" target="_blank">00:59:13.040</a></span> | <span class="t">Go to the microphone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3563" target="_blank">00:59:23.080</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3563" target="_blank">00:59:23.580</a></span> | <span class="t">Hi.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3564" target="_blank">00:59:24.080</a></span> | <span class="t">You mentioned the ReLU adds sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3566" target="_blank">00:59:26.340</a></span> | <span class="t">Can you explain why?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3568" target="_blank">00:59:28.440</a></span> | <span class="t">Yeah, so the first thing is that it's observed in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3574" target="_blank">00:59:34.400</a></span> | <span class="t">And it adds some sparsity in part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3577" target="_blank">00:59:37.200</a></span> | <span class="t">because you have the non-linearity at 0 below.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3580" target="_blank">00:59:40.520</a></span> | <span class="t">So it means that units are going to be potentially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3583" target="_blank">00:59:43.560</a></span> | <span class="t">exactly sparse, essentially absent of the hidden layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3589" target="_blank">00:59:49.480</a></span> | <span class="t">There are a few reasons to explain why you get sparsity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3594" target="_blank">00:59:54.400</a></span> | <span class="t">It turns out that this process of doing a linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3596" target="_blank">00:59:56.900</a></span> | <span class="t">transformation followed by the ReLU activation function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3600" target="_blank">01:00:00.460</a></span> | <span class="t">is very close to some of the steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3602" target="_blank">01:00:02.160</a></span> | <span class="t">you would do when you're optimizing for sparse codes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3604" target="_blank">01:00:04.840</a></span> | <span class="t">in a sparse coding model, if you know about sparse coding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3607" target="_blank">01:00:07.700</a></span> | <span class="t">So they're essentially an optimization method</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3610" target="_blank">01:00:10.280</a></span> | <span class="t">that, given some sparse coding model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3612" target="_blank">01:00:12.720</a></span> | <span class="t">will find what is the sparse representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3615" target="_blank">01:00:15.640</a></span> | <span class="t">hidden representation for some input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3617" target="_blank">01:00:17.580</a></span> | <span class="t">And it's mostly a sequence of linear transformations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3620" target="_blank">01:00:20.920</a></span> | <span class="t">followed by this ReLU-like activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3625" target="_blank">01:00:25.120</a></span> | <span class="t">And I think this is partly the explanation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3627" target="_blank">01:00:27.600</a></span> | <span class="t">Otherwise, I don't know of a solid explanation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3631" target="_blank">01:00:31.280</a></span> | <span class="t">for why that is beyond what's observed in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3634" target="_blank">01:00:34.280</a></span> | <span class="t">Any more questions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3641" target="_blank">01:00:41.240</a></span> | <span class="t">If not, let's thank Hugo again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3643" target="_blank">01:00:43.120</a></span> | <span class="t">[APPLAUSE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=zij_FTbJHsk&t=3647" target="_blank">01:00:47.600</a></span> | <span class="t">And we are reconvening in 10 minutes.</span></div></div></body></html>