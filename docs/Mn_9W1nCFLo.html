<html><head><title>LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>LLaMA explained: KV-Cache, Rotary Positional Embedding, RMS Norm, Grouped Query Attention, SwiGLU</h2><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo"><img src="https://i.ytimg.com/vi/Mn_9W1nCFLo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=140">2:20</a> Transformer vs LLaMA<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=320">5:20</a> LLaMA 1<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=382">6:22</a> LLaMA 2<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=419">6:59</a> Input Embeddings<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=532">8:52</a> Normalization & RMSNorm<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1471">24:31</a> Rotary Positional Embeddings<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2239">37:19</a> Review of Self-Attention<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2422">40:22</a> KV Cache<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3240">54:0</a> Grouped Multi-Query Attention<br><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3847">64:7</a> SwiGLU Activation function<br><br><div style="text-align: left;"><a href="./Mn_9W1nCFLo.html">Whisper Transcript</a> | <a href="./transcript_Mn_9W1nCFLo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys! Welcome to my new video about Lama. In this video we will be seeing what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=5" target="_blank">00:00:05.400</a></span> | <span class="t">is Lama, how it is made, how it is structurally different from the transformer and we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=12" target="_blank">00:00:12.560</a></span> | <span class="t">be building each block that makes up Lama. So I will not only explain you concept-wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=18" target="_blank">00:00:18.400</a></span> | <span class="t">what is each block doing but we will also explore it from the mathematical point of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=22" target="_blank">00:00:22.480</a></span> | <span class="t">view and also from the coding point of view, so that we can unify theory with practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=29" target="_blank">00:00:29.440</a></span> | <span class="t">I can guarantee that if you watch this video you will have a deep understanding of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=35" target="_blank">00:00:35.380</a></span> | <span class="t">makes Lama the model it is. So you will not only understand how the blocks interact with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=42" target="_blank">00:00:42.960</a></span> | <span class="t">each other but how they function and why we needed these blocks in the first place. In</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=49" target="_blank">00:00:49.440</a></span> | <span class="t">this video we will be reviewing a lot of topics, so we will start from the architectural differences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=55" target="_blank">00:00:55.360</a></span> | <span class="t">between the vanilla transformer and the Lama model. We will be watching what is the new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=59" target="_blank">00:00:59.600</a></span> | <span class="t">normalization, the RMS normalization, rotary positional embedding, KV cache, multi-query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=64" target="_blank">00:01:04.240</a></span> | <span class="t">attention, grouped multi-query attention, the ZWIGLU activation function for the feed-forward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=68" target="_blank">00:01:08.400</a></span> | <span class="t">layer. But of course I take for granted that you have some background knowledge. First</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=74" target="_blank">00:01:14.280</a></span> | <span class="t">of all I highly recommend that you watch my previous video about the transformer because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=78" target="_blank">00:01:18.440</a></span> | <span class="t">you need to know how the transformer works. And in my previous video I also explored the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=83" target="_blank">00:01:23.040</a></span> | <span class="t">concept of training and inferencing a transformer model. It's about 45 minutes and I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=89" target="_blank">00:01:29.200</a></span> | <span class="t">worth a watch because it will really give you a deep understanding of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=93" target="_blank">00:01:33.560</a></span> | <span class="t">After you have that knowledge you can watch this video. Anyway, for those who have already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=98" target="_blank">00:01:38.120</a></span> | <span class="t">watched the video but forgot some things, I will review most of the concepts as we proceed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=104" target="_blank">00:01:44.520</a></span> | <span class="t">through the topics. I also take for granted that you have some basic linear algebra knowledge,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=111" target="_blank">00:01:51.240</a></span> | <span class="t">so matrix multiplication, dot product, basic stuff anyway. And also, because we will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=116" target="_blank">00:01:56.600</a></span> | <span class="t">using the rotary positional embeddings, some knowledge about the complex numbers, even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=120" target="_blank">00:02:00.440</a></span> | <span class="t">if it's not fundamental. So if you don't remember the complex numbers or how they work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=126" target="_blank">00:02:06.040</a></span> | <span class="t">or the LRS formula, it doesn't matter. You will understand the concept, not the math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=130" target="_blank">00:02:10.680</a></span> | <span class="t">It's not really fundamental. Sometimes I will be reviewing topics that maybe you are already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=136" target="_blank">00:02:16.040</a></span> | <span class="t">familiar with, so feel free to skip those parts. Let's start our journey by reviewing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=141" target="_blank">00:02:21.800</a></span> | <span class="t">the architectural differences between the vanilla transformer and Lama. This picture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=147" target="_blank">00:02:27.400</a></span> | <span class="t">was built by me on the right side because I couldn't find the architectural picture on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=153" target="_blank">00:02:33.400</a></span> | <span class="t">the paper. So let's review the differences. As you remember, in the vanilla transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=159" target="_blank">00:02:39.320</a></span> | <span class="t">we have an encoder part and a decoder part. And let me highlight it. So this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=166" target="_blank">00:02:46.040</a></span> | <span class="t">encoder and the right side here is the decoder. While in Lama, we only have an encoder. First</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=173" target="_blank">00:02:53.720</a></span> | <span class="t">of all, because the Lama is a large language model, it has been trained on the next prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=179" target="_blank">00:02:59.240</a></span> | <span class="t">token task. So basically, we only need the self-attention to predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=184" target="_blank">00:03:04.120</a></span> | <span class="t">And we will see all these concepts. So we will see what is the next prediction task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=187" target="_blank">00:03:07.400</a></span> | <span class="t">how it works, and how this new self-attention works. The second difference that we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=193" target="_blank">00:03:13.080</a></span> | <span class="t">from these pictures is that we have here, at the beginning, we have the embedding and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=199" target="_blank">00:03:19.080</a></span> | <span class="t">also we had the embedding here on the original transformer. But right after the embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=203" target="_blank">00:03:23.640</a></span> | <span class="t">we don't have the positional encoding, but we have this RMS norm. And actually, all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=208" target="_blank">00:03:28.600</a></span> | <span class="t">norms have been moved before the blocks. So before we had the multi-head attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=214" target="_blank">00:03:34.280</a></span> | <span class="t">and then we had the add-end norm, which is this plus sign here. So it's a concatenation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=220" target="_blank">00:03:40.120</a></span> | <span class="t">of a skip connection and the output of the multi-head attention, and the normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=224" target="_blank">00:03:44.840</a></span> | <span class="t">And we also have this normalization here, here, here. So after every block. But here in Lama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=230" target="_blank">00:03:50.360</a></span> | <span class="t">we have it before every block. And we will review what is the normalization and why</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=235" target="_blank">00:03:55.080</a></span> | <span class="t">it works like the way it is. Right after the normalization, we have this query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=241" target="_blank">00:04:01.400</a></span> | <span class="t">key, and values input for the self-attention. One thing we can notice is that the positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=247" target="_blank">00:04:07.960</a></span> | <span class="t">encodings are not anymore the positional encodings of the transformer, but they have become the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=253" target="_blank">00:04:13.000</a></span> | <span class="t">rotary positional encodings, and they are only applied to the query and the keys, but not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=257" target="_blank">00:04:17.960</a></span> | <span class="t">values. And we will see also why. Another thing is the self-attention is now the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=265" target="_blank">00:04:25.320</a></span> | <span class="t">with KV cache. We will see what is the KV cache and how it works. And also we have this grouped</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=271" target="_blank">00:04:31.560</a></span> | <span class="t">multi-query attention. Another thing that changed is this feed-forward layer. In the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=278" target="_blank">00:04:38.440</a></span> | <span class="t">feed-forward layer of the vanilla transformer, we had the relu activation function for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=285" target="_blank">00:04:45.640</a></span> | <span class="t">feed-forward block. But in Lama, we are using the zwiglu function, and we will see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=291" target="_blank">00:04:51.160</a></span> | <span class="t">This nx means that this block here in the dashed lines is repeated n times one after another,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=299" target="_blank">00:04:59.800</a></span> | <span class="t">such that the output of the last layer is then fed to this rms norm, then to the linear layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=305" target="_blank">00:05:05.400</a></span> | <span class="t">and then to the softmax. And we will build each of these blocks from the bottom. So I will show</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=311" target="_blank">00:05:11.960</a></span> | <span class="t">you exactly what these blocks do, how they work, how they interact with each other, what is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=316" target="_blank">00:05:16.680</a></span> | <span class="t">math behind, what is the problem they were trying to solve. So we will have a deep knowledge of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=321" target="_blank">00:05:21.560</a></span> | <span class="t">these models. Let's start our journey with reviewing the models introduced by Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=327" target="_blank">00:05:27.880</a></span> | <span class="t">So Lama1 came out in February 2023, and they had four dimensions for this model. One model was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=337" target="_blank">00:05:37.160</a></span> | <span class="t">with 6.7 billion parameters, 13, 32, 65. And then we have these numbers. What do they mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=343" target="_blank">00:05:43.880</a></span> | <span class="t">The dimension here indicates the size of the embedding vector. So as you can see here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=350" target="_blank">00:05:50.760</a></span> | <span class="t">we have these input embeddings that we will review later. This is basically, they convert</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=355" target="_blank">00:05:55.960</a></span> | <span class="t">each token into a vector of size indicated by this dimension. Then we have the number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=361" target="_blank">00:06:01.560</a></span> | <span class="t">So how many heads the attention has, the number of layers. If you remember from the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=368" target="_blank">00:06:08.680</a></span> | <span class="t">transformer, the dimension was 512. The number of heads was eight. The number of layers, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=374" target="_blank">00:06:14.600</a></span> | <span class="t">was six. And then we have the number of tokens each model was trained upon. So 1 trillion and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=382" target="_blank">00:06:22.280</a></span> | <span class="t">1.4 trillion. With Lama2, most of the numbers have doubled. So the context length is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=389" target="_blank">00:06:29.160</a></span> | <span class="t">the sequence length. So what is the longest sequence the model can be fed? And then the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=397" target="_blank">00:06:37.400</a></span> | <span class="t">number of tokens upon which the model have been trained is also doubled. So from 1 to 2 trillion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=402" target="_blank">00:06:42.120</a></span> | <span class="t">for each size of the model, while the parameters more or less remain the same. Then we have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=407" target="_blank">00:06:47.560</a></span> | <span class="t">column here, GQA, that indicates that these two sizes of the model, so the 34 billion and 70</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=414" target="_blank">00:06:54.040</a></span> | <span class="t">billion, they use the grouped query attention. And we will see how it works. Let's start by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=420" target="_blank">00:07:00.200</a></span> | <span class="t">reviewing what is the embeddings layer here. And for that, I will use the slides from my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=425" target="_blank">00:07:05.480</a></span> | <span class="t">previous video. If you remember my previous video, we introduced the embedding like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=430" target="_blank">00:07:10.040</a></span> | <span class="t">So we have a sentence that is made of six words. What we do is we tokenize the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=435" target="_blank">00:07:15.880</a></span> | <span class="t">so it converts into tokens. The tokenization usually is done not by space, but by the BPE</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=442" target="_blank">00:07:22.440</a></span> | <span class="t">tokenizer. So actually, each word will be split into subwords also. But for clarity, for simplicity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=448" target="_blank">00:07:28.760</a></span> | <span class="t">we just tokenize our sentence by using the whitespace as separator. So each token is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=455" target="_blank">00:07:35.880</a></span> | <span class="t">separated by whitespace from other tokens. And each token is then mapped into its position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=462" target="_blank">00:07:42.680</a></span> | <span class="t">into the vocabulary. So the vocabulary is the list of the words that our model recognizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=471" target="_blank">00:07:51.480</a></span> | <span class="t">They don't have to be words, of course. They could be anything. They are just tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=476" target="_blank">00:07:56.920</a></span> | <span class="t">So each token occupies a position in this vocabulary, and the input IDs indicate the number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=482" target="_blank">00:08:02.520</a></span> | <span class="t">occupied by each token in the vocabulary. Then we map each input ID into a vector of size 512</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=491" target="_blank">00:08:11.320</a></span> | <span class="t">in the original transformer. But in Lama, it becomes 4096. And these embeddings are vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=499" target="_blank">00:08:19.880</a></span> | <span class="t">that are learnable. So they are parameters for the model. And while the model will be trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=504" target="_blank">00:08:24.920</a></span> | <span class="t">this embedding will change in such a way that they will capture the meaning of the word they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=509" target="_blank">00:08:29.720</a></span> | <span class="t">are mapping. So we hope that, for example, the word "cat" and "dog" will have similar embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=517" target="_blank">00:08:37.320</a></span> | <span class="t">because kind of they map similar-- at least they are in the same semantic group. And also,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=523" target="_blank">00:08:43.880</a></span> | <span class="t">the word "house" and "building," they will be very close to each other if we check the two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=529" target="_blank">00:08:49.880</a></span> | <span class="t">And this is the idea behind the embedding. Now let's check what is normalization. Because this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=537" target="_blank">00:08:57.480</a></span> | <span class="t">is the layer right after the embeddings. And for that, let's introduce some review of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=543" target="_blank">00:09:03.880</a></span> | <span class="t">neural networks and how they work. So suppose we have a feed-forward neural network with an input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=550" target="_blank">00:09:10.760</a></span> | <span class="t">a hidden layer made of neurons, another hidden layer made of another five neurons,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=558" target="_blank">00:09:18.680</a></span> | <span class="t">which then maps to an output. We usually have a target. And comparing the output with the target,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=564" target="_blank">00:09:24.680</a></span> | <span class="t">we produce a loss. The loss is then propagated back to the two hidden layers by means of back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=571" target="_blank">00:09:31.400</a></span> | <span class="t">propagation. So what we do is we calculate the gradient of the loss with respect to each weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=577" target="_blank">00:09:37.240</a></span> | <span class="t">of these two hidden layers. And we modify these weights of the hidden layer accordingly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=583" target="_blank">00:09:43.800</a></span> | <span class="t">also according to the learning rate that we have set. To check why we need to normalize and what is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=590" target="_blank">00:09:50.760</a></span> | <span class="t">the need of normalization, I will make a simplification of the neural network. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=596" target="_blank">00:09:56.680</a></span> | <span class="t">suppose our neural network is actually a factory, a factory that makes phones. So to make a phone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=602" target="_blank">00:10:02.600</a></span> | <span class="t">we start with some raw material that are given to a hardware team that will take the raw material</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=608" target="_blank">00:10:08.760</a></span> | <span class="t">and produce some hardware. For example, they may select the Bluetooth device, they may select the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=614" target="_blank">00:10:14.760</a></span> | <span class="t">display, they may select the microphone, the camera, etc, etc. And they make up the hardware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=620" target="_blank">00:10:20.920</a></span> | <span class="t">of this phone. The hardware team then gives this prototype to the software team, which then creates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=626" target="_blank">00:10:26.840</a></span> | <span class="t">the software for this hardware. And then the output of the software team is the complete phone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=632" target="_blank">00:10:32.120</a></span> | <span class="t">with hardware and software and is given as the output. The output is then compared with what was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=638" target="_blank">00:10:38.760</a></span> | <span class="t">the original design of the phone. And then we compute a loss. So what is the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=644" target="_blank">00:10:44.440</a></span> | <span class="t">the target we had for our phone and what we actually produced? So suppose the loss is our CEO.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=650" target="_blank">00:10:50.680</a></span> | <span class="t">And the loss is quite big, suppose. So our CEO will talk with the hardware team and with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=657" target="_blank">00:10:57.880</a></span> | <span class="t">software team and will tell them to adjust their strategy so as to go closer to the target next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=663" target="_blank">00:11:03.720</a></span> | <span class="t">time. So suppose that the hardware was too expensive. So the CEO will tell the hardware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=668" target="_blank">00:11:08.600</a></span> | <span class="t">team to use maybe a smaller display, to use a cheaper camera, to change the Bluetooth to a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=674" target="_blank">00:11:14.200</a></span> | <span class="t">lower range one, or to change the Wi-Fi to a low energy one, to change the battery, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=679" target="_blank">00:11:19.320</a></span> | <span class="t">And we'll also talk with the software team to adjust their strategy and maybe tell the software</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=685" target="_blank">00:11:25.160</a></span> | <span class="t">team to concentrate less on refactoring, to concentrate less on training, to hire more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=692" target="_blank">00:11:32.120</a></span> | <span class="t">interns and not care too much about the employees because the costs are too high, etc. And he will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=699" target="_blank">00:11:39.880</a></span> | <span class="t">adjust the strategy of the software and the hardware team. So the next time we start with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=705" target="_blank">00:11:45.560</a></span> | <span class="t">raw material again. So let's go back. We start with the raw material again. And the hardware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=713" target="_blank">00:11:53.800</a></span> | <span class="t">team, according to the new strategy set by the CEO, will produce a new hardware. Now the problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=720" target="_blank">00:12:00.520</a></span> | <span class="t">arises. The software team now will receive a hardware that the software team has never seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=725" target="_blank">00:12:05.800</a></span> | <span class="t">before because the display has been changed, the Bluetooth has been changed, the Wi-Fi has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=731" target="_blank">00:12:11.160</a></span> | <span class="t">changed, everything has been changed. So the software team needs to redo a lot of work and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=737" target="_blank">00:12:17.480</a></span> | <span class="t">especially they need to adjust their strategy a lot because they are dealing with something they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=743" target="_blank">00:12:23.400</a></span> | <span class="t">have never seen before. So the output of the software team will be much different compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=748" target="_blank">00:12:28.840</a></span> | <span class="t">to what they previously output. And maybe it will be even further from the target because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=755" target="_blank">00:12:35.960</a></span> | <span class="t">the software team was not ready to make all these adjustments. So maybe they wasted a lot of time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=760" target="_blank">00:12:40.280</a></span> | <span class="t">so maybe they wasted a lot of resources, so they maybe could not even reach the target,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=765" target="_blank">00:12:45.320</a></span> | <span class="t">even get closer to the target. So this time maybe the loss is even higher. So as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=770" target="_blank">00:12:50.920</a></span> | <span class="t">the problem arises by the fact that the loss function modifies the weights of the hardware</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=777" target="_blank">00:12:57.000</a></span> | <span class="t">team and the software team. But then the software team at the next iteration receives an input that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=784" target="_blank">00:13:04.520</a></span> | <span class="t">it has never seen before and this input makes it produce an output that is much divergent compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=792" target="_blank">00:13:12.520</a></span> | <span class="t">to the one it used to produce before. This will make the model oscillate kind of in the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=798" target="_blank">00:13:18.760</a></span> | <span class="t">and will make the training very slower. Now let's look what happens at the math level to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=805" target="_blank">00:13:25.160</a></span> | <span class="t">how the normalization works. So let's review some maths. Suppose that we have a linear layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=811" target="_blank">00:13:31.320</a></span> | <span class="t">defined as nn.linear with three input features and five output features with bias. This is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=819" target="_blank">00:13:39.000</a></span> | <span class="t">linear layer as defined in PyTorch. The linear layer will create two matrices, one called W,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=825" target="_blank">00:13:45.960</a></span> | <span class="t">the weight, and one called B, the bias. Suppose we have an input of shape 10 rows by 3 columns,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=833" target="_blank">00:13:53.240</a></span> | <span class="t">the output of this linear layer with this input x will be 10 rows by 5 columns. But how does this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=840" target="_blank">00:14:00.920</a></span> | <span class="t">happen mathematically? Let's review it. So imagine we have our input which is 10 by 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=846" target="_blank">00:14:06.440</a></span> | <span class="t">which means that we have 10 items and each item has 10 features. The W matrix created by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=853" target="_blank">00:14:13.400</a></span> | <span class="t">linear layer will be 5 by 3, so the output features by the 3 input features. And we can think of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=861" target="_blank">00:14:21.880</a></span> | <span class="t">of this row as one neuron, each of them having three weights, one weight for each of the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=869" target="_blank">00:14:29.560</a></span> | <span class="t">features of the x input. Then we have the bias vector and the bias vector is one weight for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=879" target="_blank">00:14:39.240</a></span> | <span class="t">neuron because the bias is one for every neuron. And this will produce an output which is 10 by 5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=886" target="_blank">00:14:46.920</a></span> | <span class="t">which means we have 10 items with 5 features. Let's try to understand what is the flow of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=894" target="_blank">00:14:54.360</a></span> | <span class="t">information in these matrices. The flow of information is governed by this expression,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=901" target="_blank">00:15:01.800</a></span> | <span class="t">so the output is equal to the x multiplied by the transpose of the W matrix plus B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=909" target="_blank">00:15:09.880</a></span> | <span class="t">So let's suppose we have this input x and we have one item and the item 1 has three features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=917" target="_blank">00:15:17.640</a></span> | <span class="t">A1, A2 and A3. The transpose of Wt is this matrix here, so in which we swap the row with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=925" target="_blank">00:15:25.240</a></span> | <span class="t">columns because according to the formula we need to make the transpose of that matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=929" target="_blank">00:15:29.240</a></span> | <span class="t">So we have neuron 1 with the three weights W1, W2, W3. We multiply the two and we obtain this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=936" target="_blank">00:15:36.120</a></span> | <span class="t">matrix, so x multiplied by the transpose of W produces this matrix here, in which this row 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=943" target="_blank">00:15:43.640</a></span> | <span class="t">is the dot product of this row vector with this column vector. Then we add the B row vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=955" target="_blank">00:15:55.160</a></span> | <span class="t">As you can see, to add two matrices they need to have the same dimension, but in PyTorch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=961" target="_blank">00:16:01.960</a></span> | <span class="t">because of broadcasting, this row will be added to this row here and then to independently to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=968" target="_blank">00:16:08.360</a></span> | <span class="t">this row and to this row etc etc because of the broadcasting. And then we will have this output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=975" target="_blank">00:16:15.000</a></span> | <span class="t">And the first item here will be Z1. What is Z1? Well, Z1 is equal to R1 plus B1. But what is R1?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=985" target="_blank">00:16:25.480</a></span> | <span class="t">R1 is the dot product of this column with this row or this row with this column. So it's this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=991" target="_blank">00:16:31.320</a></span> | <span class="t">expression here. So the output of the neuron 1 for the item 1 only depends on the features of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=998" target="_blank">00:16:38.040</a></span> | <span class="t">the item 1. Usually after this output we also apply a non-linearity like the ReLU function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1004" target="_blank">00:16:44.280</a></span> | <span class="t">which and the argument of the ReLU function is referred to as the activation of the neuron 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1011" target="_blank">00:16:51.000</a></span> | <span class="t">Now, as we can see, the output of the neuron 1 only depends on the input features of each item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1019" target="_blank">00:16:59.480</a></span> | <span class="t">So the output of a neuron for a data item depends on the features of the input data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1024" target="_blank">00:17:04.200</a></span> | <span class="t">item and the neuron's parameter. We can think of the input to a neuron as the output of a previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1029" target="_blank">00:17:09.960</a></span> | <span class="t">layer. So, for example, that input that we saw before, the X, it may as well be the output of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1034" target="_blank">00:17:14.840</a></span> | <span class="t">the previous layer. If the previous layer, after its weight are updated because of the gradient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1041" target="_blank">00:17:21.320</a></span> | <span class="t">descent, changes drastically the output, like we did before, for example, because the CEO realigned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1047" target="_blank">00:17:27.640</a></span> | <span class="t">the strategy of the hardware team, so the previous layer, the hardware team, will produce an output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1051" target="_blank">00:17:31.720</a></span> | <span class="t">that is drastically different compared to what it used to produce, the next layer will have its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1058" target="_blank">00:17:38.040</a></span> | <span class="t">output changed also drastically. So, because it will be forced to readjust its weight drastically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1065" target="_blank">00:17:45.320</a></span> | <span class="t">at the next step of the gradient descent. So what we don't like is the fact that the weight,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1070" target="_blank">00:17:50.760</a></span> | <span class="t">the output of the previous layer changes too much, so that the next layer also has to change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1076" target="_blank">00:17:56.440</a></span> | <span class="t">its output a lot, because it's to adhere to the strategy defined by the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1083" target="_blank">00:18:03.720</a></span> | <span class="t">So this phenomenon, by which the distribution of the internal nodes of a neuron change, is referred</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1089" target="_blank">00:18:09.960</a></span> | <span class="t">to as internal covariate shift. And we want to avoid it, because it makes training the network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1095" target="_blank">00:18:15.240</a></span> | <span class="t">slower, as the neurons are forced to readjust drastically their weights in one direction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1100" target="_blank">00:18:20.280</a></span> | <span class="t">or another, because of drastic changes in the output of the previous layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1105" target="_blank">00:18:25.000</a></span> | <span class="t">So what do we do? We do layer normalization, at least in the vanilla transformer. So let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1110" target="_blank">00:18:30.440</a></span> | <span class="t">review how the layer normalization works. Imagine we still have our input x defined with 10 rows by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1117" target="_blank">00:18:37.640</a></span> | <span class="t">3 columns, and for each of these items, independently, we calculate two statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1126" target="_blank">00:18:46.760</a></span> | <span class="t">One is the mu, so the mean, and one is the sigma, so the variance. And then we normalize the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1135" target="_blank">00:18:55.960</a></span> | <span class="t">in this matrix according to this formula. So we take basically x minus its mu, so each item minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1144" target="_blank">00:19:04.120</a></span> | <span class="t">the mu, divided by the square root of the variance plus epsilon, where epsilon is a very small number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1151" target="_blank">00:19:11.160</a></span> | <span class="t">so that we never divide by zero in this way, even if the variance is very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1155" target="_blank">00:19:15.160</a></span> | <span class="t">And each of these numbers is then multiplied with the two parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1160" target="_blank">00:19:20.840</a></span> | <span class="t">one is gamma, and one is beta. They are both learnable by the model, and they are useful,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1166" target="_blank">00:19:26.760</a></span> | <span class="t">because the model can adjust this gamma and beta to amplify the values that it needs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1172" target="_blank">00:19:32.040</a></span> | <span class="t">So before we had layer normalization, we used to normalize with batch normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1180" target="_blank">00:19:40.600</a></span> | <span class="t">and with batch normalization, the only difference is that instead of calculating the statistics by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1185" target="_blank">00:19:45.400</a></span> | <span class="t">rows, we calculated them by columns. So the feature 1, feature 2, and feature 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1191" target="_blank">00:19:51.000</a></span> | <span class="t">With layer normalization, we do it by row. So each row will have its own mu and sigma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1197" target="_blank">00:19:57.000</a></span> | <span class="t">So by using the layer normalization, basically, we transform the initial distribution of features,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1203" target="_blank">00:20:03.320</a></span> | <span class="t">no matter what they are, into normalized numbers that are distributed with 0 mean and 1 variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1210" target="_blank">00:20:10.840</a></span> | <span class="t">So this formula actually comes from probability statistics, and if you remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1214" target="_blank">00:20:14.360</a></span> | <span class="t">let me use the pen, okay, if you remember, basically, if we have a variable x, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1223" target="_blank">00:20:23.160</a></span> | <span class="t">distributed like a normal variable with a mean, let's say 5, and a variance of 36,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1231" target="_blank">00:20:31.000</a></span> | <span class="t">if we do x minus its mean, so 5 divided by the square root of the variance, so 36,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1241" target="_blank">00:20:41.800</a></span> | <span class="t">this one, this variable here, let's call it z, will be distributed like n, 0, 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1250" target="_blank">00:20:50.600</a></span> | <span class="t">So it will become a standard Gaussian, and this is exactly what we are doing here. So we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1256" target="_blank">00:20:56.600</a></span> | <span class="t">transforming them into standard Gaussians, so that this value, most of the times will be close to 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1263" target="_blank">00:21:03.000</a></span> | <span class="t">I mean, will be distributed around 0. Now let's talk about root-mean-square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1269" target="_blank">00:21:09.080</a></span> | <span class="t">normalization, the one used by Lama. The root-mean-square normalization was introduced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1277" target="_blank">00:21:17.240</a></span> | <span class="t">in this paper, root-mean-square layer normalization, from these two researchers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1283" target="_blank">00:21:23.320</a></span> | <span class="t">and let's read the paper together. A well-known explanation of the success of layer norm is its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1289" target="_blank">00:21:29.800</a></span> | <span class="t">re-centering and re-scaling invariance property. So what do they mean? What is the re-centering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1296" target="_blank">00:21:36.040</a></span> | <span class="t">and the re-scaling invariance? The fact that the features, no matter what they are, they will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1301" target="_blank">00:21:41.080</a></span> | <span class="t">re-centered around the zero mean, and re-scaled to have a variance of 1. The former enables the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1307" target="_blank">00:21:47.800</a></span> | <span class="t">model to be insensitive to shift noises on both input and weights, and the latter keeps the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1314" target="_blank">00:21:54.120</a></span> | <span class="t">representations intact when both input and weight are randomly scaled. In this paper, we hypothesize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1320" target="_blank">00:22:00.520</a></span> | <span class="t">that the re-scaling invariance is the reason for success of layer norm, rather than the re-centering</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1326" target="_blank">00:22:06.680</a></span> | <span class="t">invariance. So what they claim in this paper is that, basically, the success of layer norm is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1334" target="_blank">00:22:14.360</a></span> | <span class="t">because of the re-centering and the re-scaling, but mostly because of the re-scaling, so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1341" target="_blank">00:22:21.240</a></span> | <span class="t">division by the variance, basically, so to have a variance of 1. And what they do is, basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1347" target="_blank">00:22:27.960</a></span> | <span class="t">they said, okay, can we find another statistic that doesn't depend on the mean because we believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1353" target="_blank">00:22:33.720</a></span> | <span class="t">that it's not necessary? Well, yes. They use this root-mean-square statistic, so this statistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1361" target="_blank">00:22:41.480</a></span> | <span class="t">defined here, the statistic defined here, and as you can see from the expression of this statistic,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1371" target="_blank">00:22:51.960</a></span> | <span class="t">we don't use the mean to calculate it anymore, because the previous statistics here, so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1376" target="_blank">00:22:56.840</a></span> | <span class="t">variance, to be calculated you need the mean, because if you remember, the variance to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1383" target="_blank">00:23:03.320</a></span> | <span class="t">calculated needs the mean, so the variance is equal to the summation of x minus mu to the power of 2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1391" target="_blank">00:23:11.240</a></span> | <span class="t">divided by n. So we need the mean to calculate the variance. So what the authors wanted to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1398" target="_blank">00:23:18.040</a></span> | <span class="t">in this paper, they said, okay, because we don't need to re-center, because we believe, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1403" target="_blank">00:23:23.400</a></span> | <span class="t">hypothesize that the re-centering is not needed to obtain the effect of the layer normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1408" target="_blank">00:23:28.680</a></span> | <span class="t">we want to find a statistic that doesn't depend on the mean, and the RMS statistic doesn't depend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1413" target="_blank">00:23:33.400</a></span> | <span class="t">on the mean. So they do exactly the same thing that they did in the layer normalization, so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1419" target="_blank">00:23:39.800</a></span> | <span class="t">calculate the RMS statistic by rows, so one for each row, and then they normalize according to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1426" target="_blank">00:23:46.840</a></span> | <span class="t">this formula here, so they just divide by the statistic, RMS statistic, and then multiply by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1432" target="_blank">00:23:52.120</a></span> | <span class="t">this gamma parameter, which is learnable. Now, why root-mean-square normalization? Well,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1440" target="_blank">00:24:00.200</a></span> | <span class="t">it requires less computation compared to layer normalization, because we are not computing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1445" target="_blank">00:24:05.240</a></span> | <span class="t">two statistics, so we are not computing the mean and the sigma, we are only computing one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1450" target="_blank">00:24:10.360</a></span> | <span class="t">so it gives you a computational advantage. And it works well in practice, so actually what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1457" target="_blank">00:24:17.480</a></span> | <span class="t">authors of the paper hypothesized is actually true, we only need the invariance to obtain the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1463" target="_blank">00:24:23.720</a></span> | <span class="t">effect made by the layer normalization, we don't need the re-centering. At least, this is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1469" target="_blank">00:24:29.080</a></span> | <span class="t">happens with Lama. The next topic we will be talking about is the positional encodings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1474" target="_blank">00:24:34.200</a></span> | <span class="t">but before we introduce the rotary positional encodings, let's review the positional encodings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1479" target="_blank">00:24:39.320</a></span> | <span class="t">in the vanilla transformer. As you remember, after we transform our tokens into embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1485" target="_blank">00:24:45.320</a></span> | <span class="t">so vectors of size 512, in the vanilla transformer, then we sum another vector to these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1491" target="_blank">00:24:51.320</a></span> | <span class="t">embeddings, that indicate the position of each token inside the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1499" target="_blank">00:24:59.000</a></span> | <span class="t">and these positional embeddings are fixed, so they are not learned by the model, they are computed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1504" target="_blank">00:25:04.600</a></span> | <span class="t">once and then they are reused for every sentence during training and inference, and each word gets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1512" target="_blank">00:25:12.280</a></span> | <span class="t">his own vector of size 512. We have a new kind of positional encoding called rotary positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1518" target="_blank">00:25:18.760</a></span> | <span class="t">encoding, so absolute positional encodings are fixed vectors that are added to the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1524" target="_blank">00:25:24.680</a></span> | <span class="t">of a token to represent its absolute position in the sentence, so the token number 1 gets its own</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1530" target="_blank">00:25:30.600</a></span> | <span class="t">vector, the token number 2 gets its own vector, the token number 3 gets its own vector, so the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1536" target="_blank">00:25:36.600</a></span> | <span class="t">absolute positional encoding deals with one token at a time. You can think of it as the pair latitude</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1542" target="_blank">00:25:42.680</a></span> | <span class="t">and longitude on a map, each point on the earth will have its own unique latitude and longitude,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1549" target="_blank">00:25:49.000</a></span> | <span class="t">so that's an absolute indication of the position of each point on the earth, and this is the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1554" target="_blank">00:25:54.520</a></span> | <span class="t">what happens with absolute positional encoding in the vanilla transformer. We have one vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1559" target="_blank">00:25:59.000</a></span> | <span class="t">that represents exactly that position, which is added to that particular token in that position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1564" target="_blank">00:26:04.280</a></span> | <span class="t">With relative positional encodings, on the other hand, it deals with two tokens at a time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1570" target="_blank">00:26:10.760</a></span> | <span class="t">and it is involved when we calculate the attention. Since the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1575" target="_blank">00:26:15.320</a></span> | <span class="t">captures the intensity of how much two words are related to each other, relative positional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1580" target="_blank">00:26:20.600</a></span> | <span class="t">encodings tell the attention mechanism the distance between the two words involved in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1585" target="_blank">00:26:25.880</a></span> | <span class="t">this attention mechanism. So, given two tokens, we create a vector that represents their distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1593" target="_blank">00:26:33.000</a></span> | <span class="t">This is why it's called relative, because it's relative to the distance between two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1598" target="_blank">00:26:38.440</a></span> | <span class="t">Relative positional encodings were first introduced in the following paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1602" target="_blank">00:26:42.520</a></span> | <span class="t">from Google, and you can notice that Vasvani, I think, is the same author of the transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1610" target="_blank">00:26:50.200</a></span> | <span class="t">So, now, with absolute positional encoding, so from the attention is all you need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1616" target="_blank">00:26:56.520</a></span> | <span class="t">when we calculate the dot product in the attention mechanism, so if you remember the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1624" target="_blank">00:27:04.200</a></span> | <span class="t">attention mechanism, the formula, let me write it, the attention is equal to the query multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1638" target="_blank">00:27:18.040</a></span> | <span class="t">the transpose of the key divided by the square root of d model, d model, all of this, then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1648" target="_blank">00:27:28.040</a></span> | <span class="t">do the softmax, and then we multiply it by v, etc., etc., but we only concentrate on the q</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1653" target="_blank">00:27:33.160</a></span> | <span class="t">multiplied by the k transposed in this case, and this is what we see here. So, when we calculate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1661" target="_blank">00:27:41.080</a></span> | <span class="t">this dot product, the attention mechanism is calculating the dot product between two tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1667" target="_blank">00:27:47.800</a></span> | <span class="t">that already have the absolute position encoded into them, because we already added the absolute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1674" target="_blank">00:27:54.440</a></span> | <span class="t">positional encoding to each token. So, in this attention mechanism from the vanilla transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1679" target="_blank">00:27:59.480</a></span> | <span class="t">we have two tokens and the attention mechanism, while in relative positional encodings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1683" target="_blank">00:28:03.880</a></span> | <span class="t">we have three vectors. We have the token one, the token two, and then we have this vector here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1695" target="_blank">00:28:15.960</a></span> | <span class="t">we have this vector here, that represents the distance between these two tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1701" target="_blank">00:28:21.480</a></span> | <span class="t">and so we have three vectors involved in this attention mechanism, and we want the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1708" target="_blank">00:28:28.200</a></span> | <span class="t">mechanism to actually match this token differently based on this vector here. So, this vector will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1715" target="_blank">00:28:35.160</a></span> | <span class="t">indicate to the attention mechanism, so to the dot product, how to relate these two words that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1720" target="_blank">00:28:40.840</a></span> | <span class="t">are at this particular distance. With rotary positional embeddings, we do a similar job,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1728" target="_blank">00:28:48.120</a></span> | <span class="t">and they were introduced with this paper, so Reformer, and they are from a Chinese company.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1735" target="_blank">00:28:55.160</a></span> | <span class="t">So, the dot product used in the attention mechanism is a type of inner product. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1741" target="_blank">00:29:01.720</a></span> | <span class="t">if you remember from linear algebra, the dot product is a kind of operation that has some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1747" target="_blank">00:29:07.480</a></span> | <span class="t">properties, and these properties are the kind of properties that every inner product must have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1753" target="_blank">00:29:13.400</a></span> | <span class="t">So, the inner product can be thought of as a generalization of the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1757" target="_blank">00:29:17.400</a></span> | <span class="t">What the authors of the paper wanted to do is, can we find an inner product over the two-vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1765" target="_blank">00:29:25.640</a></span> | <span class="t">query and key used in the attention mechanism that only depends on the two vectors themselves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1772" target="_blank">00:29:32.280</a></span> | <span class="t">and the relative distance of the token they represent. That is, given two vectors, query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1779" target="_blank">00:29:39.480</a></span> | <span class="t">and key, that only contain the embedding of the word that they represent, and their position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1786" target="_blank">00:29:46.120</a></span> | <span class="t">inside of the sentence, so this m is actually an absolute number, so it's a scalar, it represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1792" target="_blank">00:29:52.440</a></span> | <span class="t">the position of the word inside of the sentence, and this n represents the position of the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1797" target="_blank">00:29:57.240</a></span> | <span class="t">word inside of the sentence. What they wanted to say is, can we find an inner product, so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1803" target="_blank">00:30:03.080</a></span> | <span class="t">particular parenthesis we see here is an inner product between these two vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1809" target="_blank">00:30:09.000</a></span> | <span class="t">that behaves like this function g, that only depends on the embedding of xn, so the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1817" target="_blank">00:30:17.080</a></span> | <span class="t">token, of xn, the second token, and the relative distance between them, and no other information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1824" target="_blank">00:30:24.760</a></span> | <span class="t">So this function will be given only the embedding of the first token, the embedding of the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1829" target="_blank">00:30:29.640</a></span> | <span class="t">token, and a number that represents the relative position of these two tokens, relative distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1835" target="_blank">00:30:35.960</a></span> | <span class="t">of these two tokens. Yes, we can find such a function, and the function is the one defined</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1843" target="_blank">00:30:43.320</a></span> | <span class="t">here. So we can define a function g, like the following, that only needs, only depends on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1849" target="_blank">00:30:49.640</a></span> | <span class="t">two embedding vectors q and k, and the relative distance. And this function is defined in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1856" target="_blank">00:30:56.840</a></span> | <span class="t">complex number space, and it can be converted by using the Euler formula into this form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1863" target="_blank">00:31:03.160</a></span> | <span class="t">And another thing to notice is that this function here, the one we are watching, is defined for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1869" target="_blank">00:31:09.720</a></span> | <span class="t">vectors of dimension 2. Of course later we will see what happens when the dimension is bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1877" target="_blank">00:31:17.560</a></span> | <span class="t">And when we convert this expression here, which is in the complex number space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1882" target="_blank">00:31:22.680</a></span> | <span class="t">into it's matrix form, through the Euler's formula, we can recognize this matrix here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1889" target="_blank">00:31:29.160</a></span> | <span class="t">as the rotation matrix. So this matrix here basically represents the rotation of a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1894" target="_blank">00:31:34.760</a></span> | <span class="t">For example, this one here, so this product here, will be a vector, and this rotation matrix will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1902" target="_blank">00:31:42.920</a></span> | <span class="t">rotate this vector into the space by the amount described by m theta, so the angle m theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1910" target="_blank">00:31:50.840</a></span> | <span class="t">Let's see an example. So imagine we have a vector v0, and we want to rotate it by theta,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1918" target="_blank">00:31:58.840</a></span> | <span class="t">by an angle theta here, to arrive to the vector v prime. So what we do is, we multiply the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1925" target="_blank">00:32:05.320</a></span> | <span class="t">v0 with this matrix, exactly this one, in which the values are calculated like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1931" target="_blank">00:32:11.000</a></span> | <span class="t">cosine of theta, minus sine of theta, sine of theta, and cosine of theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1935" target="_blank">00:32:15.080</a></span> | <span class="t">And the resulting vector will be the same vector, so the same length, but rotated by this angle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1942" target="_blank">00:32:22.040</a></span> | <span class="t">And this is why they are called rotary positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1946" target="_blank">00:32:26.440</a></span> | <span class="t">because this vector represents a rotation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1949" target="_blank">00:32:29.160</a></span> | <span class="t">Now, when the vector is not two-dimensional, but we have n dimensions, for example in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1956" target="_blank">00:32:36.680</a></span> | <span class="t">original transformer model our embedding size is 512, and in Lama it's 4096, we need to use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1964" target="_blank">00:32:44.680</a></span> | <span class="t">form. Now, I want you to notice not what are the numbers in this matrix, but the fact that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1971" target="_blank">00:32:51.480</a></span> | <span class="t">matrix is sparse, so it is not convenient to use it to compute the positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1977" target="_blank">00:32:57.080</a></span> | <span class="t">because if we multiply by this embedding, our tensorflow, our gpu, our computer will do a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1982" target="_blank">00:33:02.680</a></span> | <span class="t">of operations that are useless, because we already know that most of the products will be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1987" target="_blank">00:33:07.160</a></span> | <span class="t">So, is there a better way, a more computationally efficient way to do this computation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1992" target="_blank">00:33:12.200</a></span> | <span class="t">Well, there is, this form here. So, given a token with the embedding vector x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=1999" target="_blank">00:33:19.160</a></span> | <span class="t">and the position m of the token inside the sentence, this is how we compute the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2004" target="_blank">00:33:24.760</a></span> | <span class="t">embedding for the token. We take the dimensions of the token, we multiply by this matrix here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2011" target="_blank">00:33:31.640</a></span> | <span class="t">computed like the following, where the theta are fixed, m is the position of the token, x1, x2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2018" target="_blank">00:33:38.680</a></span> | <span class="t">x3 are the dimensions of the embedding, so the first dimension of the embedding, the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2022" target="_blank">00:33:42.360</a></span> | <span class="t">dimension of the embedding, etc., plus, minus the second embedding, this vector computed like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2030" target="_blank">00:33:50.120</a></span> | <span class="t">with the following positions, so minus x2, which is the negative value of the second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2037" target="_blank">00:33:57.400</a></span> | <span class="t">dimension of the embedding of the vector x, multiplied by this matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2043" target="_blank">00:34:03.240</a></span> | <span class="t">So, there is nothing we have to learn in this matrix, everything is fixed, because if we watch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2047" target="_blank">00:34:07.240</a></span> | <span class="t">the previous slide, we can see that this theta actually is computed like this, one for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2053" target="_blank">00:34:13.240</a></span> | <span class="t">dimension, and so there is nothing to learn. So, basically, they are just like the absolute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2060" target="_blank">00:34:20.040</a></span> | <span class="t">positional encoding, so we compute them once, and then we can reuse them for all the sentences that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2067" target="_blank">00:34:27.080</a></span> | <span class="t">we will train the model upon. Another interesting property of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2070" target="_blank">00:34:30.760</a></span> | <span class="t">rotary positional embeddings is the long-term decay. So, what the authors did, they calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2077" target="_blank">00:34:37.000</a></span> | <span class="t">an upper bound for the inner product that we saw before, so the g function, by varying the distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2082" target="_blank">00:34:42.760</a></span> | <span class="t">between the two tokens, and then they proved that no matter what are the two tokens, there is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2088" target="_blank">00:34:48.840</a></span> | <span class="t">upper bound that decreases as the distance between the two tokens grow. And if you remember that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2097" target="_blank">00:34:57.560</a></span> | <span class="t">inner product or the dot product that we are computing is for the calculation of the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2102" target="_blank">00:35:02.760</a></span> | <span class="t">this dot product represents the intensity of relationship between the two tokens for which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2107" target="_blank">00:35:07.640</a></span> | <span class="t">we are computing the attention. And what these rotary positional embeddings do, they will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2112" target="_blank">00:35:12.920</a></span> | <span class="t">basically decay this relationship, the strength of this relationship between the two tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2119" target="_blank">00:35:19.560</a></span> | <span class="t">if the two tokens that we are matching are distant from each other. And this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2128" target="_blank">00:35:28.120</a></span> | <span class="t">what we want. So, we want two words that are very far from each other to have a less strong</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2133" target="_blank">00:35:33.240</a></span> | <span class="t">relationship, and two words that are close to each other to have a stronger relationship.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2137" target="_blank">00:35:37.480</a></span> | <span class="t">And this is a desired property that we want from these rotary positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2142" target="_blank">00:35:42.200</a></span> | <span class="t">Now, the rotary positional embeddings are only applied to the query and the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2148" target="_blank">00:35:48.200</a></span> | <span class="t">but not to the values. Let's see why. Well, the first consideration is that they basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2154" target="_blank">00:35:54.440</a></span> | <span class="t">they come into play when we are calculating the attention. So, when we calculate the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2159" target="_blank">00:35:59.080</a></span> | <span class="t">it's the attention mechanism that will change the score. So, as you remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2165" target="_blank">00:36:05.400</a></span> | <span class="t">the attention mechanism is kind of a score that tells how much strong is the relationship between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2170" target="_blank">00:36:10.920</a></span> | <span class="t">two tokens. So, this relationship will be stronger or less stronger or will change according to also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2178" target="_blank">00:36:18.920</a></span> | <span class="t">the position of these two tokens inside of the sentence and the relative distance between these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2184" target="_blank">00:36:24.760</a></span> | <span class="t">two tokens. Another thing is that the rotation, rotary positional embeddings are applied after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2190" target="_blank">00:36:30.040</a></span> | <span class="t">the vector Q and K have been multiplied by the W matrix in the attention mechanism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2195" target="_blank">00:36:35.480</a></span> | <span class="t">while in the vanilla transformer, they are applied before. So, in the vanilla transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2200" target="_blank">00:36:40.200</a></span> | <span class="t">the position embeddings are applied right after we transform the tokens into embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2207" target="_blank">00:36:47.240</a></span> | <span class="t">But in the rotary positional embeddings, so in Lama, we don't do this. We basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2213" target="_blank">00:36:53.240</a></span> | <span class="t">right after we multiply by the W matrix in the attention mechanism. So, the W matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2219" target="_blank">00:36:59.080</a></span> | <span class="t">if you remember, is the matrix of parameters that each head has, each attention head has.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2227" target="_blank">00:37:07.160</a></span> | <span class="t">And so, in the Lama, basically, we apply the rotary position encoding after we multiply the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2236" target="_blank">00:37:16.200</a></span> | <span class="t">vectors Q and K by the W matrix. Now comes the interesting part, in which we will watch how the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2241" target="_blank">00:37:21.880</a></span> | <span class="t">self-attention works in Lama. But before we can talk about the self-attention as used in Lama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2247" target="_blank">00:37:27.560</a></span> | <span class="t">we need to review, at least briefly, the self-attention in the vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2253" target="_blank">00:37:33.000</a></span> | <span class="t">So, if you remember the self-attention in the vanilla transformer, we start with the matrix Q,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2258" target="_blank">00:37:38.760</a></span> | <span class="t">which is a matrix of sequence by the model, which means that we have on the rows, the tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2265" target="_blank">00:37:45.480</a></span> | <span class="t">and on the columns, the dimensions of the embedding vector. So, we can think of it like the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2271" target="_blank">00:37:51.400</a></span> | <span class="t">Let me. Okay. So, we can think of it like having six rows, one, and each of these rows is a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2281" target="_blank">00:38:01.240</a></span> | <span class="t">of dimension 512 that represents the embedding of that token. And now, let me delete.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2288" target="_blank">00:38:08.040</a></span> | <span class="t">And then, we multiply according to this formula. So, Q multiplied by the transpose of the K. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2296" target="_blank">00:38:16.280</a></span> | <span class="t">transpose of the K divided by the square root of 512, which is the dimension of the embedding vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2302" target="_blank">00:38:22.040</a></span> | <span class="t">where the K is equal to Q and V is also equal to Q, because this is a self-attention. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2309" target="_blank">00:38:29.000</a></span> | <span class="t">the three matrices are actually the same sequence. Then, we apply the softmax and we obtain this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2316" target="_blank">00:38:36.120</a></span> | <span class="t">matrix. So, we had the matrix that was 6 by 512 multiplied by another one that is 512 by 6. We</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2323" target="_blank">00:38:43.400</a></span> | <span class="t">will obtain a matrix that is 6 by 6, where each item in this matrix represents the dot product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2330" target="_blank">00:38:50.440</a></span> | <span class="t">of the first token with itself, then the first token with the second token, the first token with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2336" target="_blank">00:38:56.760</a></span> | <span class="t">the third token, the first token with the fourth token, etc. So, this matrix captures the intensity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2343" target="_blank">00:39:03.720</a></span> | <span class="t">of relationship between two tokens. Then, the output of this softmax is multiplied by the V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2353" target="_blank">00:39:13.640</a></span> | <span class="t">matrix to obtain the attention sequence. So, the output of the self-attention is another matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2361" target="_blank">00:39:21.080</a></span> | <span class="t">that has the same dimensions as the initial matrix. So, it will produce a sequence where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2368" target="_blank">00:39:28.280</a></span> | <span class="t">the embeddings now not only capture the meaning of each token, not only they capture the position</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2374" target="_blank">00:39:34.600</a></span> | <span class="t">of each token, but they also capture kind of the relationship between that token and every other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2380" target="_blank">00:39:40.760</a></span> | <span class="t">token. If you didn't understand this concept, please go back and watch my previous video about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2385" target="_blank">00:39:45.800</a></span> | <span class="t">the transformer where I explain it very carefully and in much more detail. Now, let's have a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2391" target="_blank">00:39:51.320</a></span> | <span class="t">at the multi-head attention very briefly. So, the multi-head attention basically means that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2397" target="_blank">00:39:57.720</a></span> | <span class="t">an input sequence, we take it, we copy it into Q, K, and V, so they are the same matrix, we multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2405" target="_blank">00:40:05.400</a></span> | <span class="t">by parameter matrices, and then we split into multiple smaller matrices, one for each head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2412" target="_blank">00:40:12.200</a></span> | <span class="t">and we calculate the attention between these heads. So, head 1, head 2, head 3, head 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2417" target="_blank">00:40:17.160</a></span> | <span class="t">Then, we concatenate the output of these heads, we multiply by the output matrix W_O,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2422" target="_blank">00:40:22.920</a></span> | <span class="t">and finally we have the output of the multi-head attention. Let's look at what is the first KV</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2428" target="_blank">00:40:28.520</a></span> | <span class="t">cache. So, before we introduce the KV cache, we need to understand how Lama was trained,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2435" target="_blank">00:40:35.320</a></span> | <span class="t">and we need to understand what is the next token prediction task. So, Lama, just like most of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2442" target="_blank">00:40:42.120</a></span> | <span class="t">large language models, have been trained on the next token prediction task, which means that given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2447" target="_blank">00:40:47.880</a></span> | <span class="t">a sequence, it will try to predict what is the next token, the most likely next token, to continue</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2454" target="_blank">00:40:54.760</a></span> | <span class="t">the prompt. So, for example, if we tell him a poem, for example, without the last word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2461" target="_blank">00:41:01.880</a></span> | <span class="t">probably it will come up with the last word that is missing from that poem. In this case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2467" target="_blank">00:41:07.640</a></span> | <span class="t">I will be using one very famous passage from Dante Alighieri, and I will not use the Italian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2473" target="_blank">00:41:13.640</a></span> | <span class="t">translation, but we will use the English translation here. So, I will only deal with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2477" target="_blank">00:41:17.640</a></span> | <span class="t">the first line you can see here, "Love that can quickly seize the gentle heart".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2481" target="_blank">00:41:21.480</a></span> | <span class="t">So, let's train Lama on this sentence. How does the training work? Well, we give the input to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2489" target="_blank">00:41:29.480</a></span> | <span class="t">the model, the input is built in such a way that we first prepare the start of sentence token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2495" target="_blank">00:41:35.400</a></span> | <span class="t">and then the target is built such that we append an end of sentence token. Why? Because the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2503" target="_blank">00:41:43.160</a></span> | <span class="t">model, this transformer model, is a sequence-to-sequence model, which maps each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2510" target="_blank">00:41:50.040</a></span> | <span class="t">position in the input sequence into another position in the output sequence. So, basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2518" target="_blank">00:41:58.280</a></span> | <span class="t">the first token of the input sequence will be mapped to the first token of the output sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2524" target="_blank">00:42:04.040</a></span> | <span class="t">and the second token of the input sequence will be mapped to the second token of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2528" target="_blank">00:42:08.440</a></span> | <span class="t">sequence, etc., etc., etc. This also means that if we give our model the input "sos",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2534" target="_blank">00:42:14.920</a></span> | <span class="t">it will produce the first token as output, so "love", then if we give the first two tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2541" target="_blank">00:42:21.320</a></span> | <span class="t">it will produce the second token as output, so "love that", and if we give the first three tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2549" target="_blank">00:42:29.400</a></span> | <span class="t">it will produce the output, the third token as output. Of course, the model will also produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2555" target="_blank">00:42:35.960</a></span> | <span class="t">the output for the previous two tokens, but let's see it with an example. So, if you remember from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2562" target="_blank">00:42:42.440</a></span> | <span class="t">my previous video, also in which I do the inferencing, when we train the model, we only do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2567" target="_blank">00:42:47.080</a></span> | <span class="t">it in one step, so we give the input, we give the target, we calculate the loss, and we don't have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2573" target="_blank">00:42:53.320</a></span> | <span class="t">any for loop to train the model for one single sentence, but for the inference, we need to do it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2581" target="_blank">00:43:01.240</a></span> | <span class="t">token by token. So, in this inferencing, we start with a time step, time step one, in which we only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2590" target="_blank">00:43:10.200</a></span> | <span class="t">give the input "sos", so start of sentence, and the output is "love". Then, we take the output token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2597" target="_blank">00:43:17.720</a></span> | <span class="t">here, "love", and we append it to the input, and we give it again to the model, and the model will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2603" target="_blank">00:43:23.960</a></span> | <span class="t">produce the next token, "love that". Then, we take the last token output by the model, "that",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2611" target="_blank">00:43:31.080</a></span> | <span class="t">we append it again to the input, and the model will produce the next token. And then, we again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2616" target="_blank">00:43:36.920</a></span> | <span class="t">take the next token, so "can", we append it to the input, and we feed it again to the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2623" target="_blank">00:43:43.160</a></span> | <span class="t">and the model will output the next token quickly. And we do it for all the steps that are necessary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2629" target="_blank">00:43:49.880</a></span> | <span class="t">until we reach the end of sentence token. Then, that's when we know that the model has finished</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2636" target="_blank">00:43:56.120</a></span> | <span class="t">outputting its output. Now, this is not how Lama was trained, actually, but this is a good example</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2644" target="_blank">00:44:04.840</a></span> | <span class="t">to show you how the next token prediction task works. Now, there is a problem with this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2653" target="_blank">00:44:13.320</a></span> | <span class="t">Let's see why. At every step of the inference, we are only interested in the last token output by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2660" target="_blank">00:44:20.920</a></span> | <span class="t">the model, because we already have the previous ones. However, the model needs to access all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2667" target="_blank">00:44:27.480</a></span> | <span class="t">previous tokens to decide on which token to output, since they constitute its context, or the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2673" target="_blank">00:44:33.320</a></span> | <span class="t">So, what I mean by this is that to output, for example, the word "D", the model has to see all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2680" target="_blank">00:44:40.280</a></span> | <span class="t">the input here. We cannot just give the "Cs". The model needs to see all the input to output this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2685" target="_blank">00:44:45.960</a></span> | <span class="t">last token, "D". But, the point is, this is a sequence-to-sequence model, so it will produce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2692" target="_blank">00:44:52.600</a></span> | <span class="t">this sequence as output, even if we only care about the last token. So, there is a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2698" target="_blank">00:44:58.040</a></span> | <span class="t">unnecessary computation we are doing to calculate these tokens, again, that we already actually have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2703" target="_blank">00:45:03.480</a></span> | <span class="t">from the previous time steps. So, let's find a way to not do this useless computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2708" target="_blank">00:45:08.760</a></span> | <span class="t">And this is what we do with the KVCache. So, the KVCache is a way to do less computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2716" target="_blank">00:45:16.120</a></span> | <span class="t">on the tokens that we have already seen during inferencing. So, it's only applied during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2722" target="_blank">00:45:22.680</a></span> | <span class="t">inferencing in a transformer model, and it not only applies to the transformer like the one in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2731" target="_blank">00:45:31.160</a></span> | <span class="t">Lama, but to all transformer models, because all transformer models work in this way. This is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2736" target="_blank">00:45:36.520</a></span> | <span class="t">description, it's a picture of how the self-attention works during the next token prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2742" target="_blank">00:45:42.360</a></span> | <span class="t">task. So, as you saw also in my previous slides, we have a query matrix here with N tokens, then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2750" target="_blank">00:45:50.120</a></span> | <span class="t">have the transposed of the keys, so the query can be taught as rows of vectors, where the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2756" target="_blank">00:45:56.680</a></span> | <span class="t">vector represents the first token, the second token, etc. Then the transposed of the keys is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2761" target="_blank">00:46:01.240</a></span> | <span class="t">the same tokens but transposed, so the rows become columns. This produces a matrix that is N by N,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2768" target="_blank">00:46:08.360</a></span> | <span class="t">so if the initial input matrix is 9, the output maximum will be 9 by 9. Then we multiply it by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2775" target="_blank">00:46:15.560</a></span> | <span class="t">the V matrix, and this will produce the attention. The attention is then fed to the linear layer of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2783" target="_blank">00:46:23.080</a></span> | <span class="t">the transformer, then the linear layer will produce the logits, and the logits are fed to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2789" target="_blank">00:46:29.000</a></span> | <span class="t">the softmax, and the softmax allow us to decide which is the token from our vocabulary. Again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2796" target="_blank">00:46:36.040</a></span> | <span class="t">if you are not familiar with this, please watch my previous video of the transformer about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2800" target="_blank">00:46:40.840</a></span> | <span class="t">inferencing of the transformer, and you will see this clearly. So, this is a description of what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2808" target="_blank">00:46:48.120</a></span> | <span class="t">happens at a general level in the self-attention. Now, let's watch it step by step. So, imagine at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2814" target="_blank">00:46:54.840</a></span> | <span class="t">inference step 1, we only have the first token. If you remember before, we were only using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2820" target="_blank">00:47:00.840</a></span> | <span class="t">start of sentence token. So, we take the start of sentence token, we multiply it by itself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2826" target="_blank">00:47:06.120</a></span> | <span class="t">so the transposed, it will produce a matrix that is 1 by 1, so this matrix is 1 by 4096,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2832" target="_blank">00:47:12.680</a></span> | <span class="t">multiplied by another matrix that is 4096 by 1, it will produce a 1 by 1 matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2837" target="_blank">00:47:17.640</a></span> | <span class="t">Why 4096? Because the embedding vector in Lama is 4096. Then the output, so this 1 by 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2845" target="_blank">00:47:25.560</a></span> | <span class="t">is multiplied by the V, and it will produce the output token here, and this will be our first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2852" target="_blank">00:47:32.040</a></span> | <span class="t">token of the output. And then we take the output token, this one, and we append it to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2860" target="_blank">00:47:40.040</a></span> | <span class="t">at the next step. So, now we have two tokens as input. They are multiplied by itself, but with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2866" target="_blank">00:47:46.600</a></span> | <span class="t">the transposed version of itself, and it will produce a 2 by 2 matrix, which is then multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2872" target="_blank">00:47:52.280</a></span> | <span class="t">by the V matrix, and it will produce two output tokens. But we are only interested in the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2877" target="_blank">00:47:57.240</a></span> | <span class="t">token's output by the model, so this one, attention 2, which is then appended to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2883" target="_blank">00:48:03.720</a></span> | <span class="t">matrix at the time step 3. So, now we have three tokens in the time step 3, which are multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2890" target="_blank">00:48:10.200</a></span> | <span class="t">by the transposed version of itself, and it will produce a 3 by 3 matrix, which is then multiplied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2896" target="_blank">00:48:16.200</a></span> | <span class="t">by the V matrix, and we have these three tokens as output. But we are only interested in the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2903" target="_blank">00:48:23.880</a></span> | <span class="t">token output by the model, so we append it again as input to the Q matrix, which is now four tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2910" target="_blank">00:48:30.440</a></span> | <span class="t">which is multiplied by the transposed version of itself, and it will produce a 4 by 4 matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2917" target="_blank">00:48:37.080</a></span> | <span class="t">as output, which is then multiplied by this matrix here, and it will produce this attention matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2923" target="_blank">00:48:43.000</a></span> | <span class="t">here. But we are only interested in the last attention, which will be then added again to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2928" target="_blank">00:48:48.520</a></span> | <span class="t">input of the next step. But we notice already something. First of all, we already here in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2935" target="_blank">00:48:55.720</a></span> | <span class="t">matrix, where we compute the dot product between this token and this, this token and this, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2941" target="_blank">00:49:01.240</a></span> | <span class="t">token and this. So this matrix is all the dot products between these two matrices. We can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2947" target="_blank">00:49:07.000</a></span> | <span class="t">something. The first thing is that we already computed these dot products in the previous step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2953" target="_blank">00:49:13.160</a></span> | <span class="t">Can we cache them? So let's go back. As you can see, this matrix is growing. Two, three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2959" target="_blank">00:49:19.880</a></span> | <span class="t">four. See, there is a lot of attention, because every time we are inferencing the transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2966" target="_blank">00:49:26.360</a></span> | <span class="t">we are giving the transformer some input, so it's re-computing all these dot products,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2972" target="_blank">00:49:32.280</a></span> | <span class="t">which is inconvenient, because we actually already computed them in the previous time step. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2977" target="_blank">00:49:37.080</a></span> | <span class="t">is there a way to not compute them again? Can we kind of cache them? Yes, we can. And then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2984" target="_blank">00:49:44.360</a></span> | <span class="t">since the model is causal, we don't care about the attention of a token with its predecessors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2990" target="_blank">00:49:50.520</a></span> | <span class="t">but only with a token before it. So as you remember, in the self-attention, we apply a mask,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=2996" target="_blank">00:49:56.600</a></span> | <span class="t">right? So the mask is basically, we don't want the dot product of one word with the word that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3002" target="_blank">00:50:02.280</a></span> | <span class="t">comes after it, but only the one that comes before it. So basically, we don't want all the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3008" target="_blank">00:50:08.760</a></span> | <span class="t">above the principal diagonal of this matrix. And that's why we applied the mask in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3015" target="_blank">00:50:15.400</a></span> | <span class="t">self-attention. But okay, the point is, we don't need to compute all these dot products. The only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3021" target="_blank">00:50:21.800</a></span> | <span class="t">dot products that we are interested in is this last row. So because we added the token 4 as input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3029" target="_blank">00:50:29.880</a></span> | <span class="t">compared to the last time step, so we only have this new token, token 4, and we want this token 4</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3035" target="_blank">00:50:35.800</a></span> | <span class="t">how it is interacting with all the other tokens. So basically, we are only interested in this last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3042" target="_blank">00:50:42.440</a></span> | <span class="t">row here. And also, as we only care about the attention of the last token, because we want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3049" target="_blank">00:50:49.960</a></span> | <span class="t">to select the word from the vocabulary, so we only care about the last row, we don't care about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3054" target="_blank">00:50:54.680</a></span> | <span class="t">producing these two, these three attention score here in the output sequence of the self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3062" target="_blank">00:51:02.280</a></span> | <span class="t">we only care about the last one. So is there a way to remove all these redundant calculations?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3067" target="_blank">00:51:07.880</a></span> | <span class="t">Yes, we can do it with the KV cache. Let's see how. So with the KV cache, basically, what we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3076" target="_blank">00:51:16.280</a></span> | <span class="t">is we cache the query, sorry, the keys and the values. And every time we have a new token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3085" target="_blank">00:51:25.560</a></span> | <span class="t">we append it to the key and the values, while the query is only the output of the previous step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3092" target="_blank">00:51:32.360</a></span> | <span class="t">So at the beginning, we don't have any output from the previous step, so we only use the first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3097" target="_blank">00:51:37.880</a></span> | <span class="t">So the first, the time step one of the inference is the same as without the cache. So we have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3104" target="_blank">00:51:44.360</a></span> | <span class="t">token one with itself, will produce a matrix one by one, multiplied with one token, and it will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3110" target="_blank">00:51:50.040</a></span> | <span class="t">produce one attention. However, at the time step two, we don't append it to the previous query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3119" target="_blank">00:51:59.240</a></span> | <span class="t">we just replace the previous token with the new token we have here. However, we keep the cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3125" target="_blank">00:52:05.240</a></span> | <span class="t">of the keys. So we keep the previous token in the keys, and we append the last output to the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3131" target="_blank">00:52:11.880</a></span> | <span class="t">here, and also to the values. And if you do this multiplication, it will produce a matrix that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3138" target="_blank">00:52:18.680</a></span> | <span class="t">one by two, where the first item is the dot product of the token two with the token one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3144" target="_blank">00:52:24.840</a></span> | <span class="t">the token two with the token two. This is actually what we want. And if we then multiply with the V</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3150" target="_blank">00:52:30.760</a></span> | <span class="t">matrix, it will only produce one attention score, which is exactly the one we want. And we do again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3156" target="_blank">00:52:36.920</a></span> | <span class="t">so we take this attention two, and this will become the input of the next inference step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3163" target="_blank">00:52:43.720</a></span> | <span class="t">So this token three, we append it to the previously cached K matrix and also to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3169" target="_blank">00:52:49.560</a></span> | <span class="t">previously cached V matrix. This multiplication will produce an output matrix that we can see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3175" target="_blank">00:52:55.960</a></span> | <span class="t">here. The multiplication of this output matrix with this V matrix will produce one token in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3183" target="_blank">00:53:03.000</a></span> | <span class="t">output, which is this one, and we know which token to select using this one. Then we use it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3188" target="_blank">00:53:08.680</a></span> | <span class="t">as an input for the next inferencing step by appending it to the cached keys and appending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3194" target="_blank">00:53:14.280</a></span> | <span class="t">to the cached V matrix. We do this multiplication, and we will get this matrix, which is four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3203" target="_blank">00:53:23.160</a></span> | <span class="t">one by four, which is the dot product of the token four with the token one, the token four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3208" target="_blank">00:53:28.600</a></span> | <span class="t">with the token two, token four with the token three, and the token four with itself. We multiply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3213" target="_blank">00:53:33.560</a></span> | <span class="t">by the V matrix, and this will only produce one attention, which is exactly what we want to select</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3219" target="_blank">00:53:39.080</a></span> | <span class="t">the output token. This is the reason why it's called the KV cache, because we are keeping a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3224" target="_blank">00:53:44.920</a></span> | <span class="t">cache of the keys and the values. As you can see, the KV cache allow us to save a lot of computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3232" target="_blank">00:53:52.120</a></span> | <span class="t">because we are not doing a lot of dot products that we used to do before, and this makes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3239" target="_blank">00:53:59.720</a></span> | <span class="t">inferencing faster. The next layer that we will be talking about is the grouped multi-query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3244" target="_blank">00:54:04.440</a></span> | <span class="t">attention, but before we talk about the grouped multi-query attention, we need to introduce its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3249" target="_blank">00:54:09.560</a></span> | <span class="t">predecessor, the multi-query attention. Let's see. So let's start with the problem. The problem is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3256" target="_blank">00:54:16.680</a></span> | <span class="t">that the GPUs are too fast. If you watch this datasheet, this is from the A1 GPU from NVIDIA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3266" target="_blank">00:54:26.280</a></span> | <span class="t">we can see that the GPU is very fast at computing, at performing calculations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3271" target="_blank">00:54:31.160</a></span> | <span class="t">but not so much, not so fast at transferring data from its memory. That means, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3279" target="_blank">00:54:39.880</a></span> | <span class="t">that the A100 can do 19.5 tera floating point operations per second by using a 32-bit precision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3290" target="_blank">00:54:50.840</a></span> | <span class="t">while it can only transfer 1.9 thousand gigabytes per second. It's nearly 10 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3300" target="_blank">00:55:00.920</a></span> | <span class="t">more slower at transferring data than it is at performing calculations, and this means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3310" target="_blank">00:55:10.840</a></span> | <span class="t">sometimes the bottleneck is not how many operations we perform, but how much data transfer our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3316" target="_blank">00:55:16.840</a></span> | <span class="t">operations need, and that depends on the size and the quantity of the tensors involved in our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3322" target="_blank">00:55:22.680</a></span> | <span class="t">calculations. For example, if we compute the same operations on the same tensor n times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3329" target="_blank">00:55:29.240</a></span> | <span class="t">it may be faster than computing the same operations on n different tokens, even if they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3334" target="_blank">00:55:34.840</a></span> | <span class="t">have the same size. This is because the GPU may need to move these tensors around. So this means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3341" target="_blank">00:55:41.560</a></span> | <span class="t">that our goal should not only be to optimize the number of operations we do with our algorithms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3348" target="_blank">00:55:48.040</a></span> | <span class="t">but also minimize the memory access and the memory transfers that our algorithms perform,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3353" target="_blank">00:55:53.960</a></span> | <span class="t">because the memory access and memory transfer are more expensive in terms of time compared to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3362" target="_blank">00:56:02.440</a></span> | <span class="t">computations. And this also happens with software when we do I/O, for example. If we copy, for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3368" target="_blank">00:56:08.840</a></span> | <span class="t">example, we do some multiplications in the CPU or we read some data from the hard disk, reading from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3374" target="_blank">00:56:14.920</a></span> | <span class="t">the hard disk is much more slower than doing a lot of computations on the CPU. And this is a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3381" target="_blank">00:56:21.880</a></span> | <span class="t">Now, in this paper, we introduced the multi-query attention. This paper is from Noam Shazir,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3387" target="_blank">00:56:27.960</a></span> | <span class="t">who is also one of the authors of the attention paper. So attention is all you need. And in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3394" target="_blank">00:56:34.360</a></span> | <span class="t">paper, he introduced the problem. He said, well, let's look at the multi-head attention. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3401" target="_blank">00:56:41.720</a></span> | <span class="t">batched multi-head attention. This is the multi-head attention as presented in the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3406" target="_blank">00:56:46.840</a></span> | <span class="t">paper. Attention is all you need. Let's look at the algorithm and let's calculate the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3411" target="_blank">00:56:51.880</a></span> | <span class="t">arithmetic operations performed and also the total memory involved in these operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3418" target="_blank">00:56:58.440</a></span> | <span class="t">So he calculated that the number of arithmetic operations is performed in O(1), O(b) and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3425" target="_blank">00:57:05.320</a></span> | <span class="t">d^2, where b is the batch size, n is the sequence length, and d is the size of the embedding vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3431" target="_blank">00:57:11.960</a></span> | <span class="t">While the total memory involved in the operations, given by the sum of all the tensors involved in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3439" target="_blank">00:57:19.160</a></span> | <span class="t">the calculations, including the derived ones, is equal to O(b) and d^2 + b*h*n^2, where h is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3449" target="_blank">00:57:29.640</a></span> | <span class="t">number of heads in this multi-head attention, plus d^2. Now, if we compute the ratio between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3456" target="_blank">00:57:36.920</a></span> | <span class="t">the total memory and the number of arithmetic operations, we get this expression here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3463" target="_blank">00:57:43.240</a></span> | <span class="t">1/k + 1/b. In this case, the ratio is much smaller than 1, which means that the number of memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3470" target="_blank">00:57:50.920</a></span> | <span class="t">accesses that we perform is much less than the number of arithmetic operations. So the memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3476" target="_blank">00:57:56.040</a></span> | <span class="t">access in this case is not the bottleneck. So what I mean to say is that the bottleneck of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3484" target="_blank">00:58:04.120</a></span> | <span class="t">this algorithm is not the memory access, it is actually the number of computations. And as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3489" target="_blank">00:58:09.240</a></span> | <span class="t">saw before, when we introduced the KV cache, the problem we were trying to solve is the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3493" target="_blank">00:58:13.880</a></span> | <span class="t">computations, but by introducing the KV cache, we created a new bottleneck, and it's not the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3507" target="_blank">00:58:27.560</a></span> | <span class="t">computation anymore. So this algorithm here is the multi-head self-attention, but using the KV cache,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3515" target="_blank">00:58:35.000</a></span> | <span class="t">and this reduces the number of operations performed. So if we look at the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3520" target="_blank">00:58:40.120</a></span> | <span class="t">arithmetic operations performed, it's bnd^2. The total memory involved in the operation is bn^2d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3528" target="_blank">00:58:48.280</a></span> | <span class="t">+ ndd^2, and the ratio between the two is this, O(n/d + 1/b), so the ratio between the total memory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3538" target="_blank">00:58:58.840</a></span> | <span class="t">and the number of arithmetic operations. This means that when n is very similar to d,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3546" target="_blank">00:59:06.600</a></span> | <span class="t">this ratio will become 1, or when b is very similar to 1, or in the limit of 1, so the batch size is 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3553" target="_blank">00:59:13.960</a></span> | <span class="t">this ratio will become 1. And this is a problem, because now when this condition is verified,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3560" target="_blank">00:59:20.360</a></span> | <span class="t">it's true, then the memory access becomes the bottleneck of the algorithm. And this also means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3568" target="_blank">00:59:28.040</a></span> | <span class="t">that either we keep the dimension of the embedding vector much bigger than the sequence length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3576" target="_blank">00:59:36.760</a></span> | <span class="t">but if we increase the sequence length without making the dimension of the embedding vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3582" target="_blank">00:59:42.120</a></span> | <span class="t">much bigger, the memory access will become the bottleneck. So what we can do is, we need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3589" target="_blank">00:59:49.800</a></span> | <span class="t">find a better way. To solve the problem of the previous algorithm, in which the memory became</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3594" target="_blank">00:59:54.920</a></span> | <span class="t">the bottleneck, we introduced the multi-query attention. So what the author did was to remove</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3601" target="_blank">01:00:01.080</a></span> | <span class="t">the h dimension from the k and the v, while keeping it for the q. So it's still a multi-head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3609" target="_blank">01:00:09.880</a></span> | <span class="t">attention, but only with respect to q, that's why it's called multi-query attention. So we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3615" target="_blank">01:00:15.800</a></span> | <span class="t">have multiple heads only for the q, but the k and v will be shared by all the heads. And if we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3622" target="_blank">01:00:22.680</a></span> | <span class="t">this algorithm, the ratio becomes this, 1/d + n/dh + 1/b. So we compare it to the previous one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3633" target="_blank">01:00:33.960</a></span> | <span class="t">in which was n/d, now it's n/dh. So we reduced the n/d factor, the ratio n/d by a factor of h,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3646" target="_blank">01:00:46.040</a></span> | <span class="t">because we removed the h number of heads for the k and v. So the gains, the performance gains are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3653" target="_blank">01:00:53.400</a></span> | <span class="t">important actually, because now it happens less, it is less likely that this ratio will become 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3661" target="_blank">01:01:01.320</a></span> | <span class="t">But of course, by removing the heads from the k and v, our model will also have less parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3670" target="_blank">01:01:10.840</a></span> | <span class="t">it will also have less degrees of freedom and complexity, which may degrade the quality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3677" target="_blank">01:01:17.320</a></span> | <span class="t">of the model. And it actually does degrade the quality of the model, but only slightly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3682" target="_blank">01:01:22.120</a></span> | <span class="t">and we will see. So if we compare, for example, the blue score on a translation task from English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3687" target="_blank">01:01:27.320</a></span> | <span class="t">to German, we can see that the multi-head attention, so the attention that was in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3692" target="_blank">01:01:32.040</a></span> | <span class="t">original attention paper, has a blue score of 26.7, while the multi-query has a blue score of 26.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3701" target="_blank">01:01:41.560</a></span> | <span class="t">The author also compared it with the multi-head local and multi-query local, where local means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3709" target="_blank">01:01:49.720</a></span> | <span class="t">that they restrict the attention calculation only to the previous 31 positions of each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3718" target="_blank">01:01:58.600</a></span> | <span class="t">And we can see it here. But the performance gains by reducing the heads of the k and v is great,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3727" target="_blank">01:02:07.160</a></span> | <span class="t">because you can see the inference time, for example, on the original multi-head attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3731" target="_blank">01:02:11.560</a></span> | <span class="t">and the multi-query attention. The inference time went from 1.7 microseconds plus 46 microseconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3740" target="_blank">01:02:20.200</a></span> | <span class="t">for the decoder to 1.5 microseconds plus 3.8 microseconds for the decoder. So in total here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3748" target="_blank">01:02:28.040</a></span> | <span class="t">more or less, we took 48 microseconds, while here we more or less take 6 microseconds for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3756" target="_blank">01:02:36.360</a></span> | <span class="t">the multi-query. So it's a great benefit from a performance point of view during the inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3765" target="_blank">01:02:45.160</a></span> | <span class="t">Let's talk about grouped multi-query attention, because now we just introduced the kvcache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3771" target="_blank">01:02:51.560</a></span> | <span class="t">and the multi-query attention. But the next step of the multi-query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3776" target="_blank">01:02:56.520</a></span> | <span class="t">is the grouped multi-query attention, which is the one that is used in llama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3780" target="_blank">01:03:00.360</a></span> | <span class="t">So let's have a look at it. With multi-query, we only have multiple heads for the queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3786" target="_blank">01:03:06.440</a></span> | <span class="t">but only one head for the key and the values. With grouped multi-query attention, basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3792" target="_blank">01:03:12.520</a></span> | <span class="t">we divide the queries into groups. So for example, this is the group 1, this is the group 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3798" target="_blank">01:03:18.760</a></span> | <span class="t">group 3 and group 4. And for each group, we have one different head of k and v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3806" target="_blank">01:03:26.600</a></span> | <span class="t">This is a good compromise between the multi-head, in which there is a one-to-one correspondence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3812" target="_blank">01:03:32.280</a></span> | <span class="t">and the multi-query, where there is a n-to-one correspondence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3816" target="_blank">01:03:36.680</a></span> | <span class="t">So in this case, we have still multiple heads for the keys and values, but they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3822" target="_blank">01:03:42.600</a></span> | <span class="t">less numerically compared to the number of heads of the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3826" target="_blank">01:03:46.040</a></span> | <span class="t">And this is a good compromise between the quality of the model and the speed of the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3832" target="_blank">01:03:52.840</a></span> | <span class="t">because anyway, here we benefit from the computational benefit of the reduction in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3841" target="_blank">01:04:01.320</a></span> | <span class="t">the number of heads of key and values, but we don't sacrifice too much on the quality side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3847" target="_blank">01:04:07.560</a></span> | <span class="t">And now the last part of the model. As you can see here, the feedforward in the llama model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3853" target="_blank">01:04:13.880</a></span> | <span class="t">has been converted into, has its activation function changed with the zwiglu function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3860" target="_blank">01:04:20.760</a></span> | <span class="t">Let's have a look at how it works. So the zwiglu function was analyzed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3865" target="_blank">01:04:25.640</a></span> | <span class="t">in this famous paper from Noam Shazir, who is also one of the authors of the attention model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3870" target="_blank">01:04:30.920</a></span> | <span class="t">who is also one of the authors of the multi-query attention that we saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3876" target="_blank">01:04:36.200</a></span> | <span class="t">So let's have a look at this paper. So the author compared the performance of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3882" target="_blank">01:04:42.120</a></span> | <span class="t">the transformer model by using different activation functions in the feedforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3886" target="_blank">01:04:46.680</a></span> | <span class="t">layer of the transformer architecture. And the one we are interested in is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3891" target="_blank">01:04:51.800</a></span> | <span class="t">zwiglu here, which is basically the swish function with beta equal to one calculated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3898" target="_blank">01:04:58.600</a></span> | <span class="t">in the X multiplied by a W matrix, which is a parameter matrix, which is then multiplied with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3905" target="_blank">01:05:05.080</a></span> | <span class="t">the X multiplied by V, V is also another parameter matrix, and W2, which is another parameter matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3911" target="_blank">01:05:11.480</a></span> | <span class="t">So compare this with the original feedforward network and here we have three parameter matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3919" target="_blank">01:05:19.000</a></span> | <span class="t">while in the original feedforward network, we only had two. So to make the comparison fair,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3926" target="_blank">01:05:26.040</a></span> | <span class="t">the author reduced the number of the size of these matrices to have two such that the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3933" target="_blank">01:05:33.720</a></span> | <span class="t">model's total number of parameters remains the same with the vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3938" target="_blank">01:05:38.120</a></span> | <span class="t">In the vanilla transformer, we had this feedforward network, which was the relu function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3942" target="_blank">01:05:42.840</a></span> | <span class="t">So this max zero, et cetera, is the relu function. And we only had the two parameter matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3948" target="_blank">01:05:48.760</a></span> | <span class="t">Actually, some successor version of the transformer didn't have the bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3954" target="_blank">01:05:54.200</a></span> | <span class="t">So this is, I took this formula from the paper, but there are many implementations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3957" target="_blank">01:05:57.640</a></span> | <span class="t">without the bias actually. And while in Lama, we use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3962" target="_blank">01:06:02.840</a></span> | <span class="t">computation for the feedforward network. And this is the code I took from the repository from Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3969" target="_blank">01:06:09.560</a></span> | <span class="t">And as you can see, it's just what the model says. It's the silu function. Why the silu function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3975" target="_blank">01:06:15.400</a></span> | <span class="t">Because it's the swish function with beta equal to one. And when the swish function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3980" target="_blank">01:06:20.040</a></span> | <span class="t">that has this expression, we give beta equal to one, it's called the sigmoid linear unit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3986" target="_blank">01:06:26.360</a></span> | <span class="t">that has this graph and it's called silu. So the silu function evaluated in the w1 of x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=3995" target="_blank">01:06:35.080</a></span> | <span class="t">then multiplied by w3, which is then we apply it to w2. So we have three matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4001" target="_blank">01:06:41.960</a></span> | <span class="t">And these three matrices are basically linear layers. Now they use the parallelized version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4007" target="_blank">01:06:47.720</a></span> | <span class="t">of this linear layer, but it's a linear layer. And if we look at the graph of this silu function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4014" target="_blank">01:06:54.360</a></span> | <span class="t">we can see that it's kind of like a relu, but in this here before the zero, we don't cancel out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4024" target="_blank">01:07:04.760</a></span> | <span class="t">immediately the activation. We keep a little tail here so that even values that are very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4030" target="_blank">01:07:10.920</a></span> | <span class="t">close to zero from the negative side are not automatically canceled out by the function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4036" target="_blank">01:07:16.840</a></span> | <span class="t">So let's see how does it perform. So this is wiglu function actually performs very well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4042" target="_blank">01:07:22.200</a></span> | <span class="t">Here they evaluate the log complexity, perplexity of the model when we use this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4050" target="_blank">01:07:30.200</a></span> | <span class="t">particular function. And we can see that the perplexity here is the lowest. The perplexity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4056" target="_blank">01:07:36.360</a></span> | <span class="t">basically means how unsure is the model about its choices. And the wiglu function is performing well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4065" target="_blank">01:07:45.880</a></span> | <span class="t">Then they also run the comparison on many benchmarks. And we see that this wiglu function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4073" target="_blank">01:07:53.160</a></span> | <span class="t">is performing quite well on a lot of them. So why is this wiglu activation function working so well?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4081" target="_blank">01:08:01.080</a></span> | <span class="t">If we look at the conclusion of this paper, we see that we offer no explanation as to why this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4087" target="_blank">01:08:07.000</a></span> | <span class="t">architecture seems to work. We attribute their success as all else to divine benevolence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4093" target="_blank">01:08:13.640</a></span> | <span class="t">Actually, this is kind of funny, but it's also kind of true. Because in most of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4098" target="_blank">01:08:18.600</a></span> | <span class="t">deep learning research, we do not know why things work in the way they do. Because imagine you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4105" target="_blank">01:08:25.640</a></span> | <span class="t">a model of 70 billion parameters. How can you prove what is happening to each one of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4112" target="_blank">01:08:32.440</a></span> | <span class="t">after you modify one activation function? It's not easy to come up with a model that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4118" target="_blank">01:08:38.840</a></span> | <span class="t">can explain why the model is reacting in a particular way. What usually we do, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4125" target="_blank">01:08:45.880</a></span> | <span class="t">either simplify the model, so we can work with a very small model, and then make some assumptions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4131" target="_blank">01:08:51.320</a></span> | <span class="t">on why things work the way they do. Or we can just do it on a practical level. So we take a model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4138" target="_blank">01:08:58.440</a></span> | <span class="t">we modify it a little bit, we do some ablation study, and we check which one is performing better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4143" target="_blank">01:09:03.560</a></span> | <span class="t">And this also happens in a lot of areas of machine learning. For example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4147" target="_blank">01:09:07.560</a></span> | <span class="t">we do a lot of grid search to find the right parameters for a model, because we cannot know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4153" target="_blank">01:09:13.000</a></span> | <span class="t">beforehand which one will work well, or which one to increase, or which one to decrease. Because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4157" target="_blank">01:09:17.960</a></span> | <span class="t">depends on a lot of factors, not only on the algorithm used, but also on the data, also on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4163" target="_blank">01:09:23.480</a></span> | <span class="t">the particular computations used, also on the normalization used. So there is a lot of factors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4168" target="_blank">01:09:28.120</a></span> | <span class="t">there is no formula for everything to explain everything. So this is why the research needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4175" target="_blank">01:09:35.240</a></span> | <span class="t">to do a lot of study on the variants of models, to come up with something that works maybe in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4180" target="_blank">01:09:40.920</a></span> | <span class="t">one domain and doesn't work well in other domains. So in this case, we use the Zwiglu,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4185" target="_blank">01:09:45.160</a></span> | <span class="t">mostly because in practice it works well with this kind of models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4188" target="_blank">01:09:48.920</a></span> | <span class="t">Thank you guys for watching this long video. I hope that you learned in a deeper level what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4196" target="_blank">01:09:56.200</a></span> | <span class="t">happens in Lama, and why it is different from a standard transformer model. I know that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4201" target="_blank">01:10:01.240</a></span> | <span class="t">video has been quite long, and I know that it has been hard on some parts to follow,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4205" target="_blank">01:10:05.880</a></span> | <span class="t">so I actually kind of suggest to re-watch it multiple times, especially the parts that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4210" target="_blank">01:10:10.920</a></span> | <span class="t">are less familiar with, and to integrate this video with my previous video about the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4215" target="_blank">01:10:15.560</a></span> | <span class="t">So you can, I will put the chapters so you can easily find the part that you want, but this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4222" target="_blank">01:10:22.360</a></span> | <span class="t">what you need to do. You need to watch multiple times the same concept to actually master it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4227" target="_blank">01:10:27.960</a></span> | <span class="t">And I hope to make another video in which we code the Lama model from zero, so we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4232" target="_blank">01:10:32.120</a></span> | <span class="t">put all this theory into practice. But as you know, I am doing this on my free time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4239" target="_blank">01:10:39.320</a></span> | <span class="t">and my free time is not so much. So thank you guys for watching my video, and please subscribe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4244" target="_blank">01:10:44.200</a></span> | <span class="t">to my channel, because this is the best motivation for me to keep posting amazing content on AI and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=Mn_9W1nCFLo&t=4250" target="_blank">01:10:50.360</a></span> | <span class="t">machine learning. Thank you for watching, and have an amazing rest of the day.</span></div></div></body></html>