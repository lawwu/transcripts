<html><head><title>Lesson 14: Cutting Edge Deep Learning for Coders</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 14: Cutting Edge Deep Learning for Coders</h2><a href="https://www.youtube.com/watch?v=1-NYPQw5THU"><img src="https://i.ytimg.com/vi/1-NYPQw5THU/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=45">0:45</a> Time Series Data<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=590">9:50</a> Neural Network<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=768">12:48</a> Continuous Variables<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=945">15:45</a> Projection Method<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1079">17:59</a> Importing<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1158">19:18</a> Basic Tables<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1201">20:1</a> pandas<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1429">23:49</a> pandas summary<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1519">25:19</a> data cleaning<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1557">25:57</a> joining tables<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1656">27:36</a> categorical variables<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1828">30:28</a> pandas indexing<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2261">37:41</a> join<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2587">43:7</a> time series feature manipulation<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2932">48:52</a> time until event<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2973">49:33</a> index<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3057">50:57</a> rolling<br><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3142">52:22</a> final result<br><br><div style="text-align: left;"><a href="./1-NYPQw5THU.html">Whisper Transcript</a> | <a href="./transcript_1-NYPQw5THU.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay, so welcome to lesson 14, the final lesson for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=13" target="_blank">00:00:13.960</a></span> | <span class="t">And we'll talk at the end about what's next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=17" target="_blank">00:00:17.920</a></span> | <span class="t">As you can see from what's increasingly been happening, what next is very much about you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=23" target="_blank">00:00:23.200</a></span> | <span class="t">with us rather than us leading you or telling you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=27" target="_blank">00:00:27.440</a></span> | <span class="t">So we're a community now and we can figure these stuff out together and obviously USF</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=33" target="_blank">00:00:33.840</a></span> | <span class="t">is a wonderful ally to have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=39" target="_blank">00:00:39.440</a></span> | <span class="t">So for now, this is the last of these lessons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=46" target="_blank">00:00:46.840</a></span> | <span class="t">One of the things that was great to see this week was this terrific article in Forbes that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=52" target="_blank">00:00:52.740</a></span> | <span class="t">talked about deep learning education and it was written by one of our terrific students,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=58" target="_blank">00:00:58.400</a></span> | <span class="t">Maria.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=59" target="_blank">00:00:59.400</a></span> | <span class="t">It focuses on the great work of some of the students that have come through this course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=66" target="_blank">00:01:06.800</a></span> | <span class="t">So I wanted to say thank you very much and congratulations on this great article.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=70" target="_blank">00:01:10.280</a></span> | <span class="t">I hope everybody would check it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=74" target="_blank">00:01:14.960</a></span> | <span class="t">Very beautifully written as well and terrific stories, I found it quite inspiring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=85" target="_blank">00:01:25.860</a></span> | <span class="t">So today we are going to be talking about a couple of things, but we're going to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=91" target="_blank">00:01:31.920</a></span> | <span class="t">with time series and structured data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=101" target="_blank">00:01:41.120</a></span> | <span class="t">Time series, I wanted to start very briefly by talking about something which I think you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=105" target="_blank">00:01:45.080</a></span> | <span class="t">basically know how to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=107" target="_blank">00:01:47.920</a></span> | <span class="t">This is a fantastic paper because it is not by deep mind, nobody's heard of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=114" target="_blank">00:01:54.280</a></span> | <span class="t">It actually comes from the Children's Hospital of Los Angeles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=117" target="_blank">00:01:57.880</a></span> | <span class="t">Believe it or not, perhaps the epicenter of practical applied AI and medicine data is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=125" target="_blank">00:02:05.520</a></span> | <span class="t">in Southern California, and specifically Southern California Pediatrics, the Children's Hospital</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=129" target="_blank">00:02:09.760</a></span> | <span class="t">of Orange County, CHOC, and the Children's Hospital of Los Angeles, CHLA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=135" target="_blank">00:02:15.240</a></span> | <span class="t">CHLA, which this paper comes from, actually has this thing they call V-PICU, the Virtual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=142" target="_blank">00:02:22.080</a></span> | <span class="t">Pediatric Intensive Care Unit, where for many, many years they've been tracking every electronic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=148" target="_blank">00:02:28.320</a></span> | <span class="t">signal about how every patient, every kid in the hospital was treated and what all their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=154" target="_blank">00:02:34.760</a></span> | <span class="t">ongoing sensor readings are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=160" target="_blank">00:02:40.440</a></span> | <span class="t">One of the extraordinary things they do is when the doctors there do rounds, data scientists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=165" target="_blank">00:02:45.360</a></span> | <span class="t">come with them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=166" target="_blank">00:02:46.360</a></span> | <span class="t">I don't know anywhere else in the world that this happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=172" target="_blank">00:02:52.080</a></span> | <span class="t">And so a couple of months ago they released a draft of this amazing paper where they talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=179" target="_blank">00:02:59.840</a></span> | <span class="t">about how they pulled out all this data from the EMR and from the sensors and attempted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=186" target="_blank">00:03:06.440</a></span> | <span class="t">to predict patient mortality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=188" target="_blank">00:03:08.880</a></span> | <span class="t">The reason this is interesting is that when a kid goes into the ICU, if a model starts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=195" target="_blank">00:03:15.720</a></span> | <span class="t">saying this kid is looking like they might die, then that's the thing that sets the alarms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=201" target="_blank">00:03:21.160</a></span> | <span class="t">going and everybody rushes over and starts looking after them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=205" target="_blank">00:03:25.040</a></span> | <span class="t">And they found that they built a model that was more accurate than any existing model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=208" target="_blank">00:03:28.840</a></span> | <span class="t">Those existing models were built on many years of deep clinical input and they used an RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=215" target="_blank">00:03:35.640</a></span> | <span class="t">Now this kind of time series data is what I'm going to refer to as signal-type time series</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=224" target="_blank">00:03:44.040</a></span> | <span class="t">data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=225" target="_blank">00:03:45.040</a></span> | <span class="t">So let's say you've got a series of blood pressure readings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=231" target="_blank">00:03:51.960</a></span> | <span class="t">So they come in and their blood pressure is kind of low and it's kind of all over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=237" target="_blank">00:03:57.040</a></span> | <span class="t">place and then suddenly it shoots up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=242" target="_blank">00:04:02.600</a></span> | <span class="t">And then in addition to that, maybe there's other readings such as at which points they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=252" target="_blank">00:04:12.520</a></span> | <span class="t">receive some kind of medical intervention, there was one here and one here and then there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=257" target="_blank">00:04:17.560</a></span> | <span class="t">was like six here and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=261" target="_blank">00:04:21.960</a></span> | <span class="t">So these kinds of things, generally speaking, the state of health at time t is probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=268" target="_blank">00:04:28.680</a></span> | <span class="t">best predicted by all of the various sensor readings at t-minus-1 and t-minus-2 and t-minus-3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=275" target="_blank">00:04:35.100</a></span> | <span class="t">So in statistical terms we would refer to that as autocorrelation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=281" target="_blank">00:04:41.760</a></span> | <span class="t">Autocorrelation means correlation with previous time periods.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=285" target="_blank">00:04:45.720</a></span> | <span class="t">For this kind of signal, I think it's very likely that an RNN is the way to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=294" target="_blank">00:04:54.960</a></span> | <span class="t">Obviously you could probably get a better result using a bidirectional RNN, but that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=298" target="_blank">00:04:58.920</a></span> | <span class="t">not going to be any help in the ICU because you don't have the future time period sensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=302" target="_blank">00:05:02.960</a></span> | <span class="t">So be careful of this data leakage issue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=307" target="_blank">00:05:07.040</a></span> | <span class="t">And indeed this is what this team at the VPCU at Children's Hospital of Los Angeles did,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=312" target="_blank">00:05:12.720</a></span> | <span class="t">they used an RNN to get this data-to-the-art result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=317" target="_blank">00:05:17.400</a></span> | <span class="t">I'm not really going to teach you more about this because basically you already know how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=320" target="_blank">00:05:20.120</a></span> | <span class="t">to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=322" target="_blank">00:05:22.640</a></span> | <span class="t">You can check out the paper, you'll see there's almost nothing special.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=325" target="_blank">00:05:25.800</a></span> | <span class="t">The only thing which was quite clever was that their sensor readings were not necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=334" target="_blank">00:05:34.960</a></span> | <span class="t">equally spaced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=336" target="_blank">00:05:36.680</a></span> | <span class="t">For example, did they receive some particular medical intervention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=342" target="_blank">00:05:42.880</a></span> | <span class="t">Clearly they're very widely spaced and they're not equally spaced.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=346" target="_blank">00:05:46.880</a></span> | <span class="t">So rather than having the RNN have basically a sequence of interventions that gets fed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=357" target="_blank">00:05:57.440</a></span> | <span class="t">to the RNN, instead they actually have two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=360" target="_blank">00:06:00.740</a></span> | <span class="t">One is the signal, and the other is the time since the last signal was read.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=369" target="_blank">00:06:09.200</a></span> | <span class="t">So each point at the RNN is basically some function f.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=373" target="_blank">00:06:13.400</a></span> | <span class="t">It's receiving two things, it's receiving the signal at time t and the value of t itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=382" target="_blank">00:06:22.400</a></span> | <span class="t">What is the time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=383" target="_blank">00:06:23.400</a></span> | <span class="t">Or the difference in time?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=384" target="_blank">00:06:24.400</a></span> | <span class="t">How long was it since the last one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=387" target="_blank">00:06:27.880</a></span> | <span class="t">That doesn't require any different deep learning, that's just concatenating one extra thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=393" target="_blank">00:06:33.480</a></span> | <span class="t">under your vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=394" target="_blank">00:06:34.840</a></span> | <span class="t">They actually show mathematically that this makes a certain amount of sense as a way to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=400" target="_blank">00:06:40.400</a></span> | <span class="t">deal with this, and then they find empirically that it does actually seem to work pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=404" target="_blank">00:06:44.240</a></span> | <span class="t">well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=408" target="_blank">00:06:48.240</a></span> | <span class="t">I can't tell you whether this is state-of-the-art for anything because I just haven't seen deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=415" target="_blank">00:06:55.000</a></span> | <span class="t">comparative papers or competitions or anything that really have this kind of data, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=421" target="_blank">00:07:01.320</a></span> | <span class="t">is weird because a lot of the world runs on this kind of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=424" target="_blank">00:07:04.000</a></span> | <span class="t">This kind of data effectively thinks it's super valuable, like if you're an oil and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=429" target="_blank">00:07:09.100</a></span> | <span class="t">gas company, what's the drill head telling you, what's the signals coming out of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=434" target="_blank">00:07:14.040</a></span> | <span class="t">pipe telling you, and so on and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=437" target="_blank">00:07:17.400</a></span> | <span class="t">There we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=438" target="_blank">00:07:18.400</a></span> | <span class="t">It's not the kind of cool thing that the Google kids work on, so who knows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=445" target="_blank">00:07:25.760</a></span> | <span class="t">So I'm not going to talk more about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=448" target="_blank">00:07:28.020</a></span> | <span class="t">That's how you can do time series with this kind of signal data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=453" target="_blank">00:07:33.040</a></span> | <span class="t">You can also incorporate all of the other stuff we're about to talk about, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=458" target="_blank">00:07:38.120</a></span> | <span class="t">the other kind of time series data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=461" target="_blank">00:07:41.120</a></span> | <span class="t">For example, there was a Kaggle competition which was looking at forecasting sales for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=480" target="_blank">00:08:00.440</a></span> | <span class="t">each store at this big company in Europe called Rossman based on the date and what promotions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=494" target="_blank">00:08:14.600</a></span> | <span class="t">are going on and what the competitors are doing and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=504" target="_blank">00:08:24.640</a></span> | <span class="t">Or maybe it will have some kind of trend to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=520" target="_blank">00:08:40.720</a></span> | <span class="t">So these kinds of seasonal time series are very widely analyzed by econometricians.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=532" target="_blank">00:08:52.480</a></span> | <span class="t">They're everywhere, particularly in business, if you're trying to predict how many widgets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=539" target="_blank">00:08:59.600</a></span> | <span class="t">you have to buy next month, or whether to increase or decrease your prices, or all kinds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=548" target="_blank">00:09:08.160</a></span> | <span class="t">of operational type things tend to look like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=552" target="_blank">00:09:12.520</a></span> | <span class="t">How full your planes are going to be, whether you should add promotions, so on and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=560" target="_blank">00:09:20.160</a></span> | <span class="t">So it turns out that the state of the art for this kind of approach is not necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=566" target="_blank">00:09:26.320</a></span> | <span class="t">fun to use in RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=569" target="_blank">00:09:29.120</a></span> | <span class="t">I'm actually going to look at the third place result from this competition because the third</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=575" target="_blank">00:09:35.200</a></span> | <span class="t">place result was nearly as good as places 1 and 2, but way, way, way simpler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=582" target="_blank">00:09:42.880</a></span> | <span class="t">And also it turns out that there's stuff that we can build on top of for almost every model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=587" target="_blank">00:09:47.960</a></span> | <span class="t">of this kind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=591" target="_blank">00:09:51.240</a></span> | <span class="t">And basically, surprise, surprise, it turns out that the answer is to use a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=599" target="_blank">00:09:59.040</a></span> | <span class="t">So I need to warn you again, what I'm going to teach you here is very, very uncool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=608" target="_blank">00:10:08.440</a></span> | <span class="t">You'll never read about it from DeepMind or OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=612" target="_blank">00:10:12.320</a></span> | <span class="t">It doesn't involve any robot arms, it doesn't involve thousands of GPU's, it's the kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=618" target="_blank">00:10:18.400</a></span> | <span class="t">of boring stuff that normal companies use to make more money or spend less money or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=624" target="_blank">00:10:24.960</a></span> | <span class="t">satisfy their customers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=627" target="_blank">00:10:27.160</a></span> | <span class="t">So I apologize deeply for that oversight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=633" target="_blank">00:10:33.320</a></span> | <span class="t">Having said that, in the 25 years or more, I've been doing machine learning work applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=641" target="_blank">00:10:41.800</a></span> | <span class="t">in the real world, 98% of it has been this kind of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=649" target="_blank">00:10:49.480</a></span> | <span class="t">Whether it be when I was working in agriculture, I've worked in wool, macadamia nuts and rice,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=654" target="_blank">00:10:54.760</a></span> | <span class="t">and we were figuring out how full our barrels were going to be, whether we needed more,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=661" target="_blank">00:11:01.760</a></span> | <span class="t">we were figuring out how to set futures markets, prices for agricultural goods, whatever, worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=669" target="_blank">00:11:09.360</a></span> | <span class="t">in mining and brewing, which required analyzing all kinds of engineering data and sales data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=676" target="_blank">00:11:16.560</a></span> | <span class="t">I've worked in banking, that required looking at transaction account pricing and risk and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=683" target="_blank">00:11:23.480</a></span> | <span class="t">fraud.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=684" target="_blank">00:11:24.480</a></span> | <span class="t">All of these areas basically involve this kind of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=689" target="_blank">00:11:29.380</a></span> | <span class="t">So I think although no one publishes stuff about this, because anybody who comes out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=697" target="_blank">00:11:37.320</a></span> | <span class="t">of a Stanford PhD and goes to Google doesn't know about any of those things, it's probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=703" target="_blank">00:11:43.760</a></span> | <span class="t">the most useful thing for the vast majority of people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=710" target="_blank">00:11:50.880</a></span> | <span class="t">And excitingly it turns out that you don't need to learn any new techniques at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=717" target="_blank">00:11:57.360</a></span> | <span class="t">In fact, the model that they got this third-place result with, a very simple model, is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=726" target="_blank">00:12:06.120</a></span> | <span class="t">one where each different categorical variable was one hot encoded and chucked into an embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=732" target="_blank">00:12:12.880</a></span> | <span class="t">layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=734" target="_blank">00:12:14.080</a></span> | <span class="t">The embedding layers were concatenated and chucked through a dense layer, then a second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=737" target="_blank">00:12:17.680</a></span> | <span class="t">dense layer, then it went through a sigmoid function into an output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=746" target="_blank">00:12:26.400</a></span> | <span class="t">Very simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=748" target="_blank">00:12:28.120</a></span> | <span class="t">The continuous variables they haven't drawn here, and all these pictures are going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=752" target="_blank">00:12:32.520</a></span> | <span class="t">come straight from this paper, which these folks that came third kindly wrote a paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=758" target="_blank">00:12:38.280</a></span> | <span class="t">about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=760" target="_blank">00:12:40.360</a></span> | <span class="t">The continuous variables basically get fed directly into the dense layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=764" target="_blank">00:12:44.880</a></span> | <span class="t">So that's the structure of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=770" target="_blank">00:12:50.680</a></span> | <span class="t">How well does it work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=772" target="_blank">00:12:52.560</a></span> | <span class="t">So the short answer is, compared to K-nearest neighbors, random forests and GBMs, just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=781" target="_blank">00:13:01.120</a></span> | <span class="t">simple neural network beats all of those approaches, just with standard one-hot encoding, whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=789" target="_blank">00:13:09.360</a></span> | <span class="t">But then the EE is Entity Embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=792" target="_blank">00:13:12.760</a></span> | <span class="t">So adding in this idea of using embeddings, interestingly you can take the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=799" target="_blank">00:13:19.160</a></span> | <span class="t">trained by a neural network and feed them into a KNN, or a random forest, or a GBM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=805" target="_blank">00:13:25.600</a></span> | <span class="t">And in fact, using embeddings with every one of those things is way better than anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=814" target="_blank">00:13:34.320</a></span> | <span class="t">other than neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=816" target="_blank">00:13:36.560</a></span> | <span class="t">So that's pretty interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=818" target="_blank">00:13:38.320</a></span> | <span class="t">And then if you use the embeddings with a neural network, you get the best results still.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=823" target="_blank">00:13:43.260</a></span> | <span class="t">So this actually is kind of fascinating, because training this neural network took me some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=835" target="_blank">00:13:55.380</a></span> | <span class="t">hours on a Titan X, whereas training the GBM took I think less than a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=845" target="_blank">00:14:05.760</a></span> | <span class="t">It was so fast, I thought I had screwed something up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=850" target="_blank">00:14:10.260</a></span> | <span class="t">And then I tried running it, and it's like holy shit, it's giving accurate predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=854" target="_blank">00:14:14.760</a></span> | <span class="t">So GBMs and random forests are so fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=857" target="_blank">00:14:17.640</a></span> | <span class="t">So in your organization, you could try taking everything that you could think of as a categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=865" target="_blank">00:14:25.000</a></span> | <span class="t">variable and once a month train a neural net with embeddings, and then store those embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=873" target="_blank">00:14:33.340</a></span> | <span class="t">in a database table and tell all of your business users, "Hey, anytime you want to create a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=879" target="_blank">00:14:39.600</a></span> | <span class="t">model that incorporates day of week or a store ID or a customer ID, you can go grab the embeddings."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=889" target="_blank">00:14:49.420</a></span> | <span class="t">And so they're basically like word vectors, but they're customer vectors and store vectors</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=894" target="_blank">00:14:54.280</a></span> | <span class="t">and product vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=896" target="_blank">00:14:56.280</a></span> | <span class="t">So I've never seen anybody write about this other than this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=902" target="_blank">00:15:02.800</a></span> | <span class="t">And even in this paper they don't really get to this hugely important idea of what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=909" target="_blank">00:15:09.760</a></span> | <span class="t">could do with these embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=911" target="_blank">00:15:11.320</a></span> | <span class="t">What's the difference between A and B and C?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=915" target="_blank">00:15:15.040</a></span> | <span class="t">Is it like different data types flowing in?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=917" target="_blank">00:15:17.280</a></span> | <span class="t">A and B and C, yeah, we're going to get to that in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=921" target="_blank">00:15:21.880</a></span> | <span class="t">Basically the different things is like A might be the store ID, B might be the product ID,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=927" target="_blank">00:15:27.280</a></span> | <span class="t">and C might be the day of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=933" target="_blank">00:15:33.320</a></span> | <span class="t">One of the really nice things that they did in this paper was to then draw some projections</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=940" target="_blank">00:15:40.060</a></span> | <span class="t">of some of these embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=941" target="_blank">00:15:41.080</a></span> | <span class="t">They just used T-SNE, it doesn't really matter what the projection method is, but here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=948" target="_blank">00:15:48.080</a></span> | <span class="t">some interesting ideas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=950" target="_blank">00:15:50.720</a></span> | <span class="t">They took each state of Germany, or based in Germany, and did a projection of the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=958" target="_blank">00:15:58.640</a></span> | <span class="t">from the state field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=961" target="_blank">00:16:01.400</a></span> | <span class="t">And here is those projections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=963" target="_blank">00:16:03.720</a></span> | <span class="t">And I've drawn around them different colored circles, and you might notice the different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=968" target="_blank">00:16:08.640</a></span> | <span class="t">colored circles exactly correspond to the different colored circles on a map of Germany.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=973" target="_blank">00:16:13.960</a></span> | <span class="t">Now this were just random embeddings trained with SGD, trying to predict sales in stores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=982" target="_blank">00:16:22.600</a></span> | <span class="t">at Rossman, and yet somehow they've drawn a map of Germany.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=988" target="_blank">00:16:28.960</a></span> | <span class="t">So obviously the reason why is because things close to each other in Germany have similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=993" target="_blank">00:16:33.840</a></span> | <span class="t">behaviors around how they respond to events, and who buys what kinds of products, and so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1000" target="_blank">00:16:40.320</a></span> | <span class="t">on and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1002" target="_blank">00:16:42.000</a></span> | <span class="t">So that's crazy fascinating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1007" target="_blank">00:16:47.120</a></span> | <span class="t">Here's the kind of bigger picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1008" target="_blank">00:16:48.560</a></span> | <span class="t">Every one of these dots is the distance between two stores, and this shows the correlation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1018" target="_blank">00:16:58.640</a></span> | <span class="t">between the distance in embedding space versus the actual distance between the stores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1024" target="_blank">00:17:04.680</a></span> | <span class="t">So you can basically see that there's a strong correlation between things being close to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1030" target="_blank">00:17:10.040</a></span> | <span class="t">each other in real life and close to each other in these SGD-trained embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1035" target="_blank">00:17:15.040</a></span> | <span class="t">Here's a couple more pictures of all the lines drawn on top of mine, but everything else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1040" target="_blank">00:17:20.600</a></span> | <span class="t">is just there from the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1042" target="_blank">00:17:22.920</a></span> | <span class="t">On the left is days of the week embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1045" target="_blank">00:17:25.400</a></span> | <span class="t">And you can see the days of the week that are near each other have ended up embedded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1048" target="_blank">00:17:28.800</a></span> | <span class="t">close together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1050" target="_blank">00:17:30.400</a></span> | <span class="t">On the right is months of the year embedding, again, same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1055" target="_blank">00:17:35.320</a></span> | <span class="t">And you can see that the weekend is fairly separate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1061" target="_blank">00:17:41.920</a></span> | <span class="t">So that's where we're going to get to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1067" target="_blank">00:17:47.560</a></span> | <span class="t">I'm actually going to take you through the end-to-end process, and I rebuilt the end-to-end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1073" target="_blank">00:17:53.840</a></span> | <span class="t">process from scratch and tried to make it in as few lines as possible because we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1081" target="_blank">00:18:01.680</a></span> | <span class="t">haven't really looked at any of these structured data type problems before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1088" target="_blank">00:18:08.680</a></span> | <span class="t">So it's kind of a very different process and even a different set of techniques.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1103" target="_blank">00:18:23.120</a></span> | <span class="t">We import the usual stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1105" target="_blank">00:18:25.880</a></span> | <span class="t">When you try to do this stuff yourself, you'll find three or four libraries we haven't used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1111" target="_blank">00:18:31.040</a></span> | <span class="t">before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1112" target="_blank">00:18:32.040</a></span> | <span class="t">So when you hit something that says "module not found", you can just pip install all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1119" target="_blank">00:18:39.800</a></span> | <span class="t">things if you're a Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1122" target="_blank">00:18:42.200</a></span> | <span class="t">We'll talk about them as we get to them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1125" target="_blank">00:18:45.940</a></span> | <span class="t">So the data that comes from Kaggle comes down as a bunch of CSV files, and I wrote a quick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1132" target="_blank">00:18:52.520</a></span> | <span class="t">thing to combine some of those CSVs together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1137" target="_blank">00:18:57.160</a></span> | <span class="t">This was one of those competitions where people were allowed to use additional external data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1143" target="_blank">00:19:03.120</a></span> | <span class="t">as long as they were shared on the forum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1146" target="_blank">00:19:06.280</a></span> | <span class="t">So the data I'll share with you, I'm going to combine it all into one place for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1152" target="_blank">00:19:12.360</a></span> | <span class="t">So I've commented on these apps because the stuff I'll give you will have already run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1156" target="_blank">00:19:16.040</a></span> | <span class="t">this concatenation process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1158" target="_blank">00:19:18.660</a></span> | <span class="t">So the basic tables that you're going to get access to is the training set itself, a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1164" target="_blank">00:19:24.920</a></span> | <span class="t">of stores, a list of which state each store is in, a list of the abbreviation and the name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1174" target="_blank">00:19:34.000</a></span> | <span class="t">of each state in Germany, a list of data from Google Trends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1179" target="_blank">00:19:39.840</a></span> | <span class="t">So if you've used Google Trends you can basically see how particular keywords change over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1184" target="_blank">00:19:44.480</a></span> | <span class="t">I don't actually know which keywords they used, but somebody found that there were some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1192" target="_blank">00:19:52.360</a></span> | <span class="t">Google Trends keywords that correlated well, so we've got access to those, some information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1197" target="_blank">00:19:57.800</a></span> | <span class="t">about the weather, and then a test set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1201" target="_blank">00:20:01.200</a></span> | <span class="t">So I'm not sure that we've really used pandas much yet, so let's talk a bit about pandas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1208" target="_blank">00:20:08.320</a></span> | <span class="t">Pandas lets us take this kind of structured data and manipulate it in similar ways to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1213" target="_blank">00:20:13.480</a></span> | <span class="t">the way you would manipulate it in a database.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1216" target="_blank">00:20:16.400</a></span> | <span class="t">So the first thing you do, so pandas, just like NumPy, tends to become np, pandas tends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1223" target="_blank">00:20:23.080</a></span> | <span class="t">to become pd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1224" target="_blank">00:20:24.080</a></span> | <span class="t">So pd.read_csv is going to return a data frame.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1228" target="_blank">00:20:28.440</a></span> | <span class="t">So a data frame is like a database table if you've used R, it's called the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1235" target="_blank">00:20:35.640</a></span> | <span class="t">So this read_csv is going to return a data frame containing the information from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1240" target="_blank">00:20:40.400</a></span> | <span class="t">CSV file, and we're going to go through each one of those table names and read the CSV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1247" target="_blank">00:20:47.880</a></span> | <span class="t">So this list comprehension is going to return a list of data frames.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1255" target="_blank">00:20:55.220</a></span> | <span class="t">So I can now go ahead and display the head, so the first five rows from each table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1263" target="_blank">00:21:03.280</a></span> | <span class="t">And that's a good way to get a sense of what these tables are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1267" target="_blank">00:21:07.080</a></span> | <span class="t">So the first one is the trading set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1272" target="_blank">00:21:12.320</a></span> | <span class="t">So for some store, on some date, they had some level of sales to some number of customers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1283" target="_blank">00:21:23.120</a></span> | <span class="t">And they were either open or closed, they either had a promotion on or they didn't,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1286" target="_blank">00:21:26.720</a></span> | <span class="t">it either was a holiday or it wasn't for state and school, and then some additional information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1292" target="_blank">00:21:32.240</a></span> | <span class="t">about the date.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1294" target="_blank">00:21:34.040</a></span> | <span class="t">So that's the basic information we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1297" target="_blank">00:21:37.320</a></span> | <span class="t">And then everything else, we join onto that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1300" target="_blank">00:21:40.320</a></span> | <span class="t">So for example, for each store, we can join up some kind of categorical variable about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1308" target="_blank">00:21:48.160</a></span> | <span class="t">what kind of store it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1309" target="_blank">00:21:49.880</a></span> | <span class="t">I have no idea what this is, it might be a different brand or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1314" target="_blank">00:21:54.360</a></span> | <span class="t">What kinds of products do they carry, again it's just a letter, I don't know what it means,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1318" target="_blank">00:21:58.480</a></span> | <span class="t">but maybe it's like some are electronics, some are supermarkets, some are full spectrum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1325" target="_blank">00:22:05.680</a></span> | <span class="t">How far away is the nearest competitor?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1330" target="_blank">00:22:10.120</a></span> | <span class="t">And what year and month did the competitor open for business?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1335" target="_blank">00:22:15.880</a></span> | <span class="t">Notice that sometimes the competitor opened for business quite late in the game, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1342" target="_blank">00:22:22.560</a></span> | <span class="t">later than some of the data we're looking at, so that's going to be a little bit confusing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1349" target="_blank">00:22:29.000</a></span> | <span class="t">And then this thing called Promo2, which as far as I understand it is basically is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1353" target="_blank">00:22:33.480</a></span> | <span class="t">a store which has some kind of standard promotion timing going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1358" target="_blank">00:22:38.400</a></span> | <span class="t">So you can see here that this store has standard promotions in January, April, July and October.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1365" target="_blank">00:22:45.720</a></span> | <span class="t">So that's the stores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1369" target="_blank">00:22:49.880</a></span> | <span class="t">We also know for each store what state they're in based on the abbreviation, and then we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1374" target="_blank">00:22:54.600</a></span> | <span class="t">find out for each state what is the name of that state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1379" target="_blank">00:22:59.040</a></span> | <span class="t">And then for each, this is slightly weird, this is the state abbreviation, the last two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1383" target="_blank">00:23:03.120</a></span> | <span class="t">letters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1384" target="_blank">00:23:04.120</a></span> | <span class="t">In this state, during this week, this was the Google Trend data for some keyword, I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1390" target="_blank">00:23:10.560</a></span> | <span class="t">not sure what keyword it was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1395" target="_blank">00:23:15.360</a></span> | <span class="t">For this state name, on this date is the temperature, dewpoint, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1404" target="_blank">00:23:24.520</a></span> | <span class="t">And then finally here's the test set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1406" target="_blank">00:23:26.760</a></span> | <span class="t">It's identical to the training set, but we don't have the number of customers, and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1410" target="_blank">00:23:30.520</a></span> | <span class="t">don't have the number of sales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1413" target="_blank">00:23:33.680</a></span> | <span class="t">So this is a pretty standard kind of industry data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1420" target="_blank">00:23:40.240</a></span> | <span class="t">You've got a central table, various tables related to that, and some things representing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1426" target="_blank">00:23:46.080</a></span> | <span class="t">time periods or time points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1428" target="_blank">00:23:48.360</a></span> | <span class="t">One of the nice things you can do in Pandas is to use this Pandas summary module called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1437" target="_blank">00:23:57.520</a></span> | <span class="t">DataFrame summary for a table.summary, and that will return a whole bunch of information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1443" target="_blank">00:24:03.040</a></span> | <span class="t">about every field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1444" target="_blank">00:24:04.040</a></span> | <span class="t">So I'm not going to go through all of it in detail, but you can see for example for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1448" target="_blank">00:24:08.840</a></span> | <span class="t">sales, on average 5,800 sales, standard deviation of 3,800, sometimes the sales goes all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1456" target="_blank">00:24:16.200</a></span> | <span class="t">way down to 0, sometimes all the way up to 41,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1459" target="_blank">00:24:19.880</a></span> | <span class="t">There's no missing to sales, that's good to know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1465" target="_blank">00:24:25.160</a></span> | <span class="t">So this is the kind of thing that's good to scroll through and identify, okay, competition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1470" target="_blank">00:24:30.160</a></span> | <span class="t">open since month is missing about a third of the time, that's good to know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1479" target="_blank">00:24:39.240</a></span> | <span class="t">There's 12 unique states, that might be worth checking because there's actually 16 things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1485" target="_blank">00:24:45.360</a></span> | <span class="t">in our state table for some reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1489" target="_blank">00:24:49.680</a></span> | <span class="t">Google trend data is never missing, that's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1494" target="_blank">00:24:54.280</a></span> | <span class="t">The year goes from 2012 through 2015.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1500" target="_blank">00:25:00.520</a></span> | <span class="t">The weather data is never missing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1504" target="_blank">00:25:04.720</a></span> | <span class="t">And then here's our test set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1509" target="_blank">00:25:09.600</a></span> | <span class="t">This is the kind of thing that might screw up a model, it's like actually sometimes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1512" target="_blank">00:25:12.560</a></span> | <span class="t">test set is missing the information about whether that store was open or not, so that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1516" target="_blank">00:25:16.800</a></span> | <span class="t">something to be careful of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1519" target="_blank">00:25:19.800</a></span> | <span class="t">So we can take that list of tables and just de-structure it out into a whole bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1523" target="_blank">00:25:23.720</a></span> | <span class="t">different table names, find out how big the training set is, how big the test set is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1530" target="_blank">00:25:30.840</a></span> | <span class="t">And then with this kind of problem, there's going to be a whole bunch of data cleaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1539" target="_blank">00:25:39.440</a></span> | <span class="t">and a whole bunch of feature engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1542" target="_blank">00:25:42.000</a></span> | <span class="t">So neural nets don't make any of that go away, particularly because we're using this style</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1550" target="_blank">00:25:50.620</a></span> | <span class="t">of neural net where we're basically feeding in a whole bunch of separate continuous and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1554" target="_blank">00:25:54.680</a></span> | <span class="t">categorical variables.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1556" target="_blank">00:25:56.520</a></span> | <span class="t">So simplify things a bit, turn state holidays into Booleans, and then I'm going to join</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1564" target="_blank">00:26:04.400</a></span> | <span class="t">all of these tables together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1570" target="_blank">00:26:10.200</a></span> | <span class="t">I always use a default join type of an outer join, so you can see here this is how we join</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1576" target="_blank">00:26:16.940</a></span> | <span class="t">in pandas. We say table.merge, table2, and then to make a left outer join, how equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1584" target="_blank">00:26:24.640</a></span> | <span class="t">left, and then you say what's the name of the fields that you're going to join on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1589" target="_blank">00:26:29.040</a></span> | <span class="t">left hand side, what are the fields you're going to join on the right hand side, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1593" target="_blank">00:26:33.360</a></span> | <span class="t">then if both tables have some fields with the same name, what are you going to suffix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1600" target="_blank">00:26:40.360</a></span> | <span class="t">those fields with?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1601" target="_blank">00:26:41.720</a></span> | <span class="t">So on the left hand side we're not going to add any suffix, on the right hand side we'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1605" target="_blank">00:26:45.640</a></span> | <span class="t">put in _y. So again, I try to refactor things as much as I can, so we're going to join lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1612" target="_blank">00:26:52.000</a></span> | <span class="t">of things. Let's create one function to do the joining, and then we can call it lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1615" target="_blank">00:26:55.840</a></span> | <span class="t">of times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1617" target="_blank">00:26:57.720</a></span> | <span class="t">Was there any fields different to the same value but named differently?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1626" target="_blank">00:27:06.520</a></span> | <span class="t">Not that I saw, no. It wouldn't matter too much if there were, because when we run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1635" target="_blank">00:27:15.440</a></span> | <span class="t">model, no problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1637" target="_blank">00:27:17.200</a></span> | <span class="t">Question - Would you liken the use of embeddings from a neural network to extraction of implicit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1646" target="_blank">00:27:26.280</a></span> | <span class="t">features, or can we think of it more like what a PCA would do, like dimensionality reduction?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1657" target="_blank">00:27:37.360</a></span> | <span class="t">Let's talk about it more when we get there. Basically, when you deal with categorical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1662" target="_blank">00:27:42.760</a></span> | <span class="t">variables in any kind of model, you have to decide what to do with them. One of my favorite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1671" target="_blank">00:27:51.800</a></span> | <span class="t">data scientists, or a pair of them actually, who are very nearly neighbors of Rachel and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1677" target="_blank">00:27:57.440</a></span> | <span class="t">mine, have this fantastic R package called Vtreat, which has a bunch of state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1689" target="_blank">00:28:09.160</a></span> | <span class="t">approaches to dealing with stuff like categorical variable encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1700" target="_blank">00:28:20.000</a></span> | <span class="t">The obvious way to do categorical variable encoding is to just do a one-hot encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1706" target="_blank">00:28:26.680</a></span> | <span class="t">and that's the way nearly everybody puts it into their gradient-boosting machines or random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1711" target="_blank">00:28:31.200</a></span> | <span class="t">forests or whatever. One of the things that Vtreat does is it has some much more interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1720" target="_blank">00:28:40.040</a></span> | <span class="t">techniques. For example, you could look at the univariate mean of sales for each day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1735" target="_blank">00:28:55.160</a></span> | <span class="t">of week, and you could encode day of week using a continuous variable which represents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1741" target="_blank">00:29:01.920</a></span> | <span class="t">the mean of sales. But then you have to think about, "Would I take that mean from the trading</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1748" target="_blank">00:29:08.000</a></span> | <span class="t">set or the test set or the validation set? How do I avoid a fitting?" There's all kinds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1752" target="_blank">00:29:12.800</a></span> | <span class="t">of complex statistical subtleties to think about that Vtreat handles all this stuff automatically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1765" target="_blank">00:29:25.040</a></span> | <span class="t">There's a lot of great techniques, but they're kind of complicated and in the end they tend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1769" target="_blank">00:29:29.240</a></span> | <span class="t">to make a whole bunch of assumptions about linearity or univariate correlations or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1776" target="_blank">00:29:36.740</a></span> | <span class="t">Whereas with embeddings, we're using SGD to learn how to deal with it, just like we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1785" target="_blank">00:29:45.400</a></span> | <span class="t">when we build an NLP model or a collaborative filtering model. We provide some initially</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1793" target="_blank">00:29:53.800</a></span> | <span class="t">random embeddings and the system learns how the movies vary compared to each other, or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1801" target="_blank">00:30:01.400</a></span> | <span class="t">uses vary, or words vary or whatever. This is to me the ultimate pure technique.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1810" target="_blank">00:30:10.040</a></span> | <span class="t">Of course the other nice thing about embeddings is we get to pick the dimensionality of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1813" target="_blank">00:30:13.560</a></span> | <span class="t">embedding so we can decide how much complexity and how much learning are we going to put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1819" target="_blank">00:30:19.080</a></span> | <span class="t">into each of the categorical variables. We'll see how to do that in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1830" target="_blank">00:30:30.360</a></span> | <span class="t">One complexity was that the weather uses the name of the state rather than the abbreviation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1842" target="_blank">00:30:42.760</a></span> | <span class="t">of the state, so we can just go ahead and join weather to states to get the abbreviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1851" target="_blank">00:30:51.040</a></span> | <span class="t">The Google Trend information about the week, week from A to B, we can split that apart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1859" target="_blank">00:30:59.320</a></span> | <span class="t">You can see here one of the things that happens in the Google Trend data is that one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1866" target="_blank">00:31:06.280</a></span> | <span class="t">states is called ni, or else in the rest of the data is called hb, ni. So this is a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1872" target="_blank">00:31:12.420</a></span> | <span class="t">opportunity to learn about pandas indexing. So pandas indexing, most of the time you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1878" target="_blank">00:31:18.360</a></span> | <span class="t">to use this .ix method. And the .ix method is your general indexing method. It's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1886" target="_blank">00:31:26.760</a></span> | <span class="t">to take two things, a list of rows to select and a list of columns to select. You can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1892" target="_blank">00:31:32.840</a></span> | <span class="t">it in pretty standard intuitive ways. This is a lot like numpy. This here is going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1897" target="_blank">00:31:37.920</a></span> | <span class="t">return a list of Booleans, which things are in this state. And if you pass the list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1903" target="_blank">00:31:43.520</a></span> | <span class="t">Booleans to the pandas row selector, it will just return the rows where that Boolean is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1908" target="_blank">00:31:48.960</a></span> | <span class="t">true. So therefore this is just going to return the rows from Google Trend, where googletrend.state</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1915" target="_blank">00:31:55.880</a></span> | <span class="t">is ni. And then the second thing we pass in is a list of columns, in this case we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1921" target="_blank">00:32:01.640</a></span> | <span class="t">got one column. And one very important thing to remember, again just like numpy, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1926" target="_blank">00:32:06.720</a></span> | <span class="t">put this kind of thing on the left-hand side of an equal sign. In computer science we call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1930" target="_blank">00:32:10.880</a></span> | <span class="t">this an L-value, so you can use it as an L-value. So we can take this state field, four things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1937" target="_blank">00:32:17.120</a></span> | <span class="t">which are equal to ni, and change their value to this. So this is like a very nice simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1945" target="_blank">00:32:25.360</a></span> | <span class="t">technique that you'll use all the time in pandas, both for looking at things and for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1951" target="_blank">00:32:31.040</a></span> | <span class="t">changing things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1958" target="_blank">00:32:38.620</a></span> | <span class="t">We have a few questions. One is, in this particular example, do you think the granularity of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1964" target="_blank">00:32:44.280</a></span> | <span class="t">data matter, as in per day or per week, is one better than the other?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1969" target="_blank">00:32:49.600</a></span> | <span class="t">Yeah, I mean I would want to have the lower granularity so that I can capture that. Ideally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1980" target="_blank">00:33:00.760</a></span> | <span class="t">you at one time as well. It kind of depends on how the organization is going to use it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1988" target="_blank">00:33:08.680</a></span> | <span class="t">What are they going to do with this information? It's probably for purchasing and stuff, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1991" target="_blank">00:33:11.680</a></span> | <span class="t">maybe they don't care about an hourly level. Clearly the difference between Sunday sales</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=1997" target="_blank">00:33:17.640</a></span> | <span class="t">and Wednesday sales will be quite significant. This is mainly a kind of business context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2005" target="_blank">00:33:25.320</a></span> | <span class="t">or domain understanding question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2007" target="_blank">00:33:27.280</a></span> | <span class="t">Another question is, do you know if there's any work that compares for structured data,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2012" target="_blank">00:33:32.120</a></span> | <span class="t">supervised embeddings like these, to embeddings that come from an unsupervised paradigm such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2017" target="_blank">00:33:37.000</a></span> | <span class="t">as an autoencoder? It seems like you'd get more useful for prediction embeddings with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2022" target="_blank">00:33:42.040</a></span> | <span class="t">the former case, but if you wanted general purpose embeddings you might prefer the latter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2026" target="_blank">00:33:46.400</a></span> | <span class="t">Yeah, I think you guys are aware of my feelings about autoencoders. It's like giving up on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2031" target="_blank">00:33:51.280</a></span> | <span class="t">life. You can always come up with a loss function that's more interesting than an autoencoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2037" target="_blank">00:33:57.480</a></span> | <span class="t">loss function basically. I would be very surprised if embeddings that came from a sales model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2043" target="_blank">00:34:03.860</a></span> | <span class="t">were not more useful for just about everything than something that came from an unsupervised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2047" target="_blank">00:34:07.600</a></span> | <span class="t">model. These things are easily tested, and if you do find a model that they don't work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2053" target="_blank">00:34:13.080</a></span> | <span class="t">as well with, then you can come up with a different set of supervised embeddings for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2056" target="_blank">00:34:16.800</a></span> | <span class="t">that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2057" target="_blank">00:34:17.800</a></span> | <span class="t">There's also just a note that .ix is deprecated and we should use .loc instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2063" target="_blank">00:34:23.840</a></span> | <span class="t">I was going to mention Pandas is changing a lot. Because I've been running this course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2070" target="_blank">00:34:30.400</a></span> | <span class="t">I have not been keeping track of the recent versions of Pandas, so thank you. In Pandas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2082" target="_blank">00:34:42.580</a></span> | <span class="t">there's a whole page called Advanced Indexing Methods. I don't find the Pandas documentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2087" target="_blank">00:34:47.080</a></span> | <span class="t">terribly clear to be honest, but there is a fantastic book by the author of Pandas called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2091" target="_blank">00:34:51.800</a></span> | <span class="t">Python for Data Analysis. There is a new edition out, and it covers Pandas, NumPy, Matplotlib,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2101" target="_blank">00:35:01.280</a></span> | <span class="t">whatever. That's the best way by far to actually understand Pandas because the documentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2109" target="_blank">00:35:09.920</a></span> | <span class="t">is a bit of a nightmare and it keeps changing so the new version has all the new stuff in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2114" target="_blank">00:35:14.400</a></span> | <span class="t">it. With these kind of indexing methods, Pandas tries really hard to be intuitive, which means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2122" target="_blank">00:35:22.200</a></span> | <span class="t">that quite often you'll read the documentation for these methods and it will say if you pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2128" target="_blank">00:35:28.600</a></span> | <span class="t">it a Boolean it will behave in this way, if you pass it a float it will behave this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2132" target="_blank">00:35:32.520</a></span> | <span class="t">if it's an index it's this way unless this other thing happens. I don't find it intuitive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2139" target="_blank">00:35:39.480</a></span> | <span class="t">at all because in the end I need to know how something works in order to use it correctly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2142" target="_blank">00:35:42.780</a></span> | <span class="t">and so you end up having to remember this huge list of things. I think Pandas is great,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2147" target="_blank">00:35:47.880</a></span> | <span class="t">but this is one thing to be very careful of, is to really make sure you understand how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2153" target="_blank">00:35:53.480</a></span> | <span class="t">all these indexing methods actually work. I know Rachel's laughing because she's been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2157" target="_blank">00:35:57.440</a></span> | <span class="t">there and probably laughing in disgust at what we all have to go through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2162" target="_blank">00:36:02.960</a></span> | <span class="t">Another question, when you use embeddings from a supervised model in another model, do you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2176" target="_blank">00:36:16.080</a></span> | <span class="t">always have to worry about data leakage? I think that's a great point. I don't think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2180" target="_blank">00:36:20.960</a></span> | <span class="t">I've got anything to add to that. You can figure out easily enough if there's data leakage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2190" target="_blank">00:36:30.420</a></span> | <span class="t">So there's this kind of standard set of steps that I take for every single structured machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2210" target="_blank">00:36:50.160</a></span> | <span class="t">learning model I do. One of those is every time I see a date, I always do this. I always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2218" target="_blank">00:36:58.440</a></span> | <span class="t">create four more fields, the year, the month of year, the week of year and the day of week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2228" target="_blank">00:37:08.620</a></span> | <span class="t">This is something which should be automatically built into every data loader, I feel. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2238" target="_blank">00:37:18.220</a></span> | <span class="t">so important because these are the kinds of structures that you see, and once every single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2243" target="_blank">00:37:23.880</a></span> | <span class="t">date has got this added to it, you're doing great. So you can see that I add that into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2252" target="_blank">00:37:32.260</a></span> | <span class="t">all of my tables that have a date field, so we'll have that from now on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2262" target="_blank">00:37:42.420</a></span> | <span class="t">So now I go ahead and do all of these outer joins. You'll see that the first thing I do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2269" target="_blank">00:37:49.340</a></span> | <span class="t">after every outer join is check whether the thing I just joined with has any nulls. Even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2278" target="_blank">00:37:58.540</a></span> | <span class="t">if you're sure that these things match perfectly, I would still never ever do an inner join.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2286" target="_blank">00:38:06.500</a></span> | <span class="t">Do the outer join and then check for nulls, and that way if anything changes ever or if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2291" target="_blank">00:38:11.020</a></span> | <span class="t">you ever make a mistake, one of these things will not be zero. If this was happening in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2297" target="_blank">00:38:17.660</a></span> | <span class="t">a production process, this would be an assert. This would be emailing Henry at 2am to say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2306" target="_blank">00:38:26.140</a></span> | <span class="t">something you're relying on is not working the way it was meant to look at. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2312" target="_blank">00:38:32.940</a></span> | <span class="t">why I always do it this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2315" target="_blank">00:38:35.460</a></span> | <span class="t">So you can see I'm basically joining my training to everything else until it's all in there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2323" target="_blank">00:38:43.740</a></span> | <span class="t">together in one big thing. So that table "everything joined together" is called "joined", and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2333" target="_blank">00:38:53.580</a></span> | <span class="t">I do a whole bunch more thinking about -- well, I didn't do the thinking, the people that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2338" target="_blank">00:38:58.220</a></span> | <span class="t">won this competition, then I replicated their results from scratch -- think about what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2344" target="_blank">00:39:04.700</a></span> | <span class="t">all the other things you might want to do with these dates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2347" target="_blank">00:39:07.300</a></span> | <span class="t">So competition open, we noticed before, a third of the time they're empty. So we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2353" target="_blank">00:39:13.700</a></span> | <span class="t">fill in the empties with some kind of sentinel value because a lot of machine learning systems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2361" target="_blank">00:39:21.180</a></span> | <span class="t">don't like missing values. Fill in the missing months with some sentinel value. Again, keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2368" target="_blank">00:39:28.860</a></span> | <span class="t">on filling in missing data. So fill_na is a really important thing to be aware of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2382" target="_blank">00:39:42.580</a></span> | <span class="t">I guess the answer is yes, it is a problem. In this case, I happen to know that every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2399" target="_blank">00:39:59.260</a></span> | <span class="t">time a year is empty, a month is also empty, and we only ever use both of them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2404" target="_blank">00:40:04.100</a></span> | <span class="t">So we don't really care when the competition store was opened, what we really care about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2425" target="_blank">00:40:25.540</a></span> | <span class="t">is how long is it between when they were opened and the particular row that we're looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2430" target="_blank">00:40:30.020</a></span> | <span class="t">at. The sales on the 2nd of February 2014, how long was it between 2nd of February 2014</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2437" target="_blank">00:40:37.260</a></span> | <span class="t">and when the competition opened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2439" target="_blank">00:40:39.100</a></span> | <span class="t">So you can see here we use this very important .apply function which just runs a Python function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2448" target="_blank">00:40:48.300</a></span> | <span class="t">on every row of a data frame. In this case, the function is to create a new date from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2456" target="_blank">00:40:56.180</a></span> | <span class="t">the open-since year and the open-since month. We're just going to assume that it's the middle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2459" target="_blank">00:40:59.740</a></span> | <span class="t">of the month. That's our competition open-since, and then we can get our days opened by just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2466" target="_blank">00:41:06.020</a></span> | <span class="t">doing a subtract.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2470" target="_blank">00:41:10.580</a></span> | <span class="t">In pandas, every date field has this special magical dt property, which is what all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2478" target="_blank">00:41:18.940</a></span> | <span class="t">days, month, year, all that stuff sits inside this little dt property. Sometimes, as I mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2491" target="_blank">00:41:31.580</a></span> | <span class="t">the competition actually opened later than the particular observation we're looking at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2496" target="_blank">00:41:36.900</a></span> | <span class="t">So that would give us a negative, so we replace our negatives with zero. We're going to use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2506" target="_blank">00:41:46.220</a></span> | <span class="t">an embedding for this, so that's why we replace days open with months open so we have less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2512" target="_blank">00:41:52.820</a></span> | <span class="t">values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2518" target="_blank">00:41:58.740</a></span> | <span class="t">I didn't actually try replacing this with a continuous variable. I suspect it wouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2523" target="_blank">00:42:03.580</a></span> | <span class="t">make too much difference, but this is what they do. In order to make the embedding again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2531" target="_blank">00:42:11.100</a></span> | <span class="t">not too big, they replaced anything that was bigger than 2 years with 2 years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2538" target="_blank">00:42:18.700</a></span> | <span class="t">So there's our unique values. Every time we do something, print something out to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2544" target="_blank">00:42:24.380</a></span> | <span class="t">sure the thing you thought you did is what you actually did. It's much easier if we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2549" target="_blank">00:42:29.260</a></span> | <span class="t">using Excel because you see straight away what you're doing. In Python, this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2554" target="_blank">00:42:34.860</a></span> | <span class="t">kind of stuff that you have to really be rigorous about checking your work at every step. When</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2562" target="_blank">00:42:42.940</a></span> | <span class="t">I build stuff like this, I generally make at least one error in every cell, so check</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2570" target="_blank">00:42:50.860</a></span> | <span class="t">carefully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2571" target="_blank">00:42:51.860</a></span> | <span class="t">Okay, do the same thing for the promo days, turn those into weeks. So that's some basic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2581" target="_blank">00:43:01.780</a></span> | <span class="t">pre-processing, you get the idea of how pandas works hopefully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2587" target="_blank">00:43:07.820</a></span> | <span class="t">So the next thing that they did in the paper was a very common kind of time series feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2598" target="_blank">00:43:18.220</a></span> | <span class="t">manipulation, one to be aware of. They basically wanted to say, "Okay, every time there's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2607" target="_blank">00:43:27.820</a></span> | <span class="t">promotion, every time there's a holiday, I want to create some additional fields for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2614" target="_blank">00:43:34.980</a></span> | <span class="t">every one of our training set rows," which is on a particular date. "On that date, how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2619" target="_blank">00:43:39.660</a></span> | <span class="t">long is it until the next holiday? How long is it until the previous holiday? How long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2625" target="_blank">00:43:45.020</a></span> | <span class="t">is it until the next promotion? How long is it since the previous promotion?" So if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2629" target="_blank">00:43:49.620</a></span> | <span class="t">basically create those fields, this is the kind of thing which is super difficult for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2638" target="_blank">00:43:58.620</a></span> | <span class="t">any GBM or random forest or neural net to figure out how to calculate itself. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2647" target="_blank">00:44:07.020</a></span> | <span class="t">no obvious kind of mathematical function that it's going to build on its own. So this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2651" target="_blank">00:44:11.060</a></span> | <span class="t">the kind of feature engineering that we have to do in order to allow us to use these kinds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2656" target="_blank">00:44:16.220</a></span> | <span class="t">of techniques effectively on time series data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2659" target="_blank">00:44:19.660</a></span> | <span class="t">So a lot of people who work with time series data, particularly in academia outside of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2667" target="_blank">00:44:27.380</a></span> | <span class="t">industry, they're just not aware of the fact that the state-of-the-art approaches really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2673" target="_blank">00:44:33.140</a></span> | <span class="t">involve all these heuristics. Separating out your dates into their components, turning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2680" target="_blank">00:44:40.220</a></span> | <span class="t">everything you can into durations both forward and backwards, and also running averages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2692" target="_blank">00:44:52.100</a></span> | <span class="t">When I used to do a lot of this kind of work, I had a bunch of library functions that I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2699" target="_blank">00:44:59.660</a></span> | <span class="t">would run on every file that came in and would automatically do these things for every combination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2704" target="_blank">00:45:04.700</a></span> | <span class="t">of dates. So this thing of how long until the next promotion, how long since the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2712" target="_blank">00:45:12.580</a></span> | <span class="t">promotion is not easy to do in any database system pretty much, or indeed in pandas. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2721" target="_blank">00:45:21.940</a></span> | <span class="t">generally speaking, these kind of systems are looking for relationships between tables,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2726" target="_blank">00:45:26.700</a></span> | <span class="t">but we're trying to look at relationships between rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2730" target="_blank">00:45:30.060</a></span> | <span class="t">So I had to create this tiny little simple little class to do this. So basically what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2736" target="_blank">00:45:36.820</a></span> | <span class="t">happens is, let's say I'm looking at school holiday. So I sort my data frame by store,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2745" target="_blank">00:45:45.300</a></span> | <span class="t">and then by date, and I call this little function called add_elapsed_school_holiday_after. What</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2753" target="_blank">00:45:53.420</a></span> | <span class="t">does add_elapsed do? Add_elapsed is going to create an instance of this class called elapsed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2760" target="_blank">00:46:00.220</a></span> | <span class="t">and in this case it's going to be called with school_holiday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2764" target="_blank">00:46:04.580</a></span> | <span class="t">So what this class is going to do, we're going to be calling this apply function again. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2768" target="_blank">00:46:08.980</a></span> | <span class="t">going to run on every single row, and it's going to call my elapsed_class.debt for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2775" target="_blank">00:46:15.620</a></span> | <span class="t">row. So I'm going to go through every row in order of store, in order of date, and I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2781" target="_blank">00:46:21.780</a></span> | <span class="t">trying to find how long has it been since the last school holiday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2786" target="_blank">00:46:26.580</a></span> | <span class="t">So when I create this object, I just have to keep track of what field is it, school_holiday.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2794" target="_blank">00:46:34.080</a></span> | <span class="t">Initialize, when was the last time we saw a school holiday? The answer is we haven't,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2800" target="_blank">00:46:40.860</a></span> | <span class="t">so let's initialize it to not a number. And we also have to know each time we cross over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2807" target="_blank">00:46:47.740</a></span> | <span class="t">to a new store. When we cross over to a new store, we just have to re-initialize. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2811" target="_blank">00:46:51.860</a></span> | <span class="t">previous store was 0. So every time we call get, we basically check. Have we crossed over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2817" target="_blank">00:46:57.900</a></span> | <span class="t">to a new store? And if so, just initialize both of those things back again. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2825" target="_blank">00:47:05.260</a></span> | <span class="t">we just say, Is this a school holiday? If so, then the last time you saw a school holiday</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2832" target="_blank">00:47:12.980</a></span> | <span class="t">is today. And then finally return, how long is it between today and the last time you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2838" target="_blank">00:47:18.840</a></span> | <span class="t">saw a school holiday?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2840" target="_blank">00:47:20.620</a></span> | <span class="t">So it's basically this class is a way of keeping track of some memory about when did I last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2848" target="_blank">00:47:28.620</a></span> | <span class="t">see this observation. So then by just calling df.apply, it's going to keep track of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2855" target="_blank">00:47:35.460</a></span> | <span class="t">for every single row. So then I can call that for school_holiday, after and before. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2861" target="_blank">00:47:41.380</a></span> | <span class="t">only difference being that for before I just sort my dates in ascending order. State_holiday</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2869" target="_blank">00:47:49.220</a></span> | <span class="t">and promo. So that's going to add in the end 6 fields, how long until and how long since</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2876" target="_blank">00:47:56.700</a></span> | <span class="t">the last school holiday, state_holiday and promotion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2879" target="_blank">00:47:59.700</a></span> | <span class="t">And then there's two questions. One asking, Is this similar to a windowing function?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2890" target="_blank">00:48:10.020</a></span> | <span class="t">Not quite, we're about to do a windowing function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2892" target="_blank">00:48:12.420</a></span> | <span class="t">And then is there a reason to think that the current approach would be problematic with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2896" target="_blank">00:48:16.220</a></span> | <span class="t">sparse data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2897" target="_blank">00:48:17.620</a></span> | <span class="t">I don't see why, but I'm not sure I quite follow. So we don't care about absolute days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2907" target="_blank">00:48:27.540</a></span> | <span class="t">We care about time deltas between events.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2910" target="_blank">00:48:30.700</a></span> | <span class="t">We care about two things. We do care about the dates, but we care about what year is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2917" target="_blank">00:48:37.740</a></span> | <span class="t">it, what date week it is. And we also care about the elapsed time between the date I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2926" target="_blank">00:48:46.580</a></span> | <span class="t">predicting sales for and the previous and next of various events.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2933" target="_blank">00:48:53.220</a></span> | <span class="t">And then windowing functions, for the features that are time until an event, how do you deal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2939" target="_blank">00:48:59.700</a></span> | <span class="t">with that given that you might not know when the last event is in the data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2946" target="_blank">00:49:06.340</a></span> | <span class="t">Well all I do is I've sorted descending, and then we initialize last with not a number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2957" target="_blank">00:49:17.780</a></span> | <span class="t">So basically when we then go subtract, here we are subtract, and it tries to subtract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2963" target="_blank">00:49:23.940</a></span> | <span class="t">not a number, we'll end up with a null. So basically anything that's an unknown time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2970" target="_blank">00:49:30.620</a></span> | <span class="t">because it's at one end or the other is going to end up null, which is why we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2979" target="_blank">00:49:39.860</a></span> | <span class="t">replace those nulls with zeros. Pandas has this slightly strange way of thinking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2988" target="_blank">00:49:48.540</a></span> | <span class="t">about indexes, but once you get used to it, it's fine. At any point you can call DataFrame.setIndex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=2994" target="_blank">00:49:54.900</a></span> | <span class="t">and pass in a field. You then have to just kind of remember what field you have as the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3001" target="_blank">00:50:01.500</a></span> | <span class="t">index, because quite a few methods in Pandas use the currently active index by default,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3007" target="_blank">00:50:07.980</a></span> | <span class="t">and of course things all run faster when you do stuff with the currently active index. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3013" target="_blank">00:50:13.660</a></span> | <span class="t">you can pass multiple fields, in which case you end up with a multiple key index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3020" target="_blank">00:50:20.260</a></span> | <span class="t">So the next thing we do is these windowing functions. So a windowing function in Pandas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3026" target="_blank">00:50:26.420</a></span> | <span class="t">we can use this rolling. So this is like a rolling mean, rolling min, rolling max, whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3031" target="_blank">00:50:31.180</a></span> | <span class="t">you like. So this basically says let's take our DataFrame with the columns we're interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3040" target="_blank">00:50:40.940</a></span> | <span class="t">in, school holiday, state holiday and promo, and we're going to keep track of how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3045" target="_blank">00:50:45.980</a></span> | <span class="t">holidays are there in the next week and the previous week. How many promos are there in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3052" target="_blank">00:50:52.220</a></span> | <span class="t">the next week and the previous week? To do that we can sort, here we are, by date, group</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3063" target="_blank">00:51:03.060</a></span> | <span class="t">by, store, and then rolling will be applied to each group. So within each group, create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3069" target="_blank">00:51:09.740</a></span> | <span class="t">a rolling 7-day sum. It's the kind of notation I'm never likely to remember, but you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3085" target="_blank">00:51:25.220</a></span> | <span class="t">just look it up. This is how you do group by type stuff. Pandas actually has quite a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3092" target="_blank">00:51:32.460</a></span> | <span class="t">lot of time series functions, and this rolling function is one of the most useful ones. Wes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3099" target="_blank">00:51:39.220</a></span> | <span class="t">McKinney had a background as a quant who memory serves correctly, and so the quants love their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3104" target="_blank">00:51:44.740</a></span> | <span class="t">time series functions, so I think that was a lot of the history of Pandas. So if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3109" target="_blank">00:51:49.100</a></span> | <span class="t">interested in time series stuff, you'll find a lot of time series stuff in Pandas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3121" target="_blank">00:52:01.980</a></span> | <span class="t">One helpful parameter that sits inside a lot of methods is inPlace = true. That means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3128" target="_blank">00:52:08.180</a></span> | <span class="t">rather than returning a new data frame with this change made, it changes the data frame</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3133" target="_blank">00:52:13.460</a></span> | <span class="t">you already have, and when your data frames are quite big this is going to save a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3137" target="_blank">00:52:17.740</a></span> | <span class="t">of time and memory. That's a good little trick to know about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3143" target="_blank">00:52:23.100</a></span> | <span class="t">So now we merge all these together, and we can now see that we've got all these after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3148" target="_blank">00:52:28.060</a></span> | <span class="t">school holidays, before school holidays, and our backward and forward running means. Then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3157" target="_blank">00:52:37.860</a></span> | <span class="t">we join that up to our original data frame, and here we have our final result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3166" target="_blank">00:52:46.940</a></span> | <span class="t">So there it is. We started out with a pretty small set of fields in the training set, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3173" target="_blank">00:52:53.980</a></span> | <span class="t">we've done this feature engineering. This feature engineering is not arbitrary. Although I didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3181" target="_blank">00:53:01.760</a></span> | <span class="t">create this solution, I was just re-implementing the solution that came from the competition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3188" target="_blank">00:53:08.700</a></span> | <span class="t">that place getters -- this is nearly exactly the set of feature engineering steps I would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3196" target="_blank">00:53:16.100</a></span> | <span class="t">have done. It's just a really standard way of thinking about a time series. So you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3201" target="_blank">00:53:21.900</a></span> | <span class="t">definitely borrow these ideas pretty closely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3212" target="_blank">00:53:32.300</a></span> | <span class="t">So now that we've got this table, we've done our feature engineering, we now want to feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3225" target="_blank">00:53:45.140</a></span> | <span class="t">it into a neural network. To feed it into a neural network we have to do a few things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3231" target="_blank">00:53:51.900</a></span> | <span class="t">The categorical variables have to be turned into one-hot encoded variables, or at least</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3239" target="_blank">00:53:59.360</a></span> | <span class="t">into contiguous integers. And the continuous variables we probably want to normalize to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3246" target="_blank">00:54:06.660</a></span> | <span class="t">a zero-mean one-standard deviation. There's a very little-known package called sklearn_pandas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3256" target="_blank">00:54:16.180</a></span> | <span class="t">And actually I contributed some new stuff to it for this course to make this even easier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3259" target="_blank">00:54:19.940</a></span> | <span class="t">to use. If you use this data frame mapper from sklearn_pandas, as you'll see, it makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3266" target="_blank">00:54:26.180</a></span> | <span class="t">life very easy. Without it, life is very hard. And because very few people know about it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3272" target="_blank">00:54:32.420</a></span> | <span class="t">the vast majority of code you will find on the internet makes life look very hard. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3277" target="_blank">00:54:37.460</a></span> | <span class="t">use this code, not the other code. Actually I was talking to some of the students the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3284" target="_blank">00:54:44.020</a></span> | <span class="t">other day and they were saying for their project they were stealing lots of code from part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3289" target="_blank">00:54:49.060</a></span> | <span class="t">one of the course because they just couldn't find anywhere else people writing any of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3295" target="_blank">00:54:55.460</a></span> | <span class="t">kinds of code that we've used. The stuff that we've learned throughout this course is on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3300" target="_blank">00:55:00.740</a></span> | <span class="t">the whole not code that lives elsewhere very much at all. So feel free to use a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3307" target="_blank">00:55:07.860</a></span> | <span class="t">these functions in your own work because I've really tried to make them the best version</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3313" target="_blank">00:55:13.520</a></span> | <span class="t">of that function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3315" target="_blank">00:55:15.780</a></span> | <span class="t">So one way to do the embeddings and the way that they did it in the paper is to basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3321" target="_blank">00:55:21.900</a></span> | <span class="t">say for each categorical variable they just manually decided what embedding dimensionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3327" target="_blank">00:55:27.900</a></span> | <span class="t">to use. They don't say in the paper how they pick these dimensionalities, but generally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3333" target="_blank">00:55:33.020</a></span> | <span class="t">speaking things with a larger number of separate levels tend to have more dimensions. So I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3339" target="_blank">00:55:39.620</a></span> | <span class="t">think there's like 1000 stores, so that has a big embedding dimensionality, where else</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3346" target="_blank">00:55:46.020</a></span> | <span class="t">obviously things like promo, forward and backward, or they have weak or whatever have much smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3352" target="_blank">00:55:52.900</a></span> | <span class="t">ones. So this is this dictionary I created that basically goes from the name of the field</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3358" target="_blank">00:55:58.740</a></span> | <span class="t">to the embedding dimensionality. Again, this is all code that you guys can use in your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3362" target="_blank">00:56:02.100</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3365" target="_blank">00:56:05.300</a></span> | <span class="t">So then all I do is I say my categorical variables is go through my dictionary, sort it in reverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3374" target="_blank">00:56:14.420</a></span> | <span class="t">order of the value, and then get the first thing from that. So that's just going to give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3381" target="_blank">00:56:21.180</a></span> | <span class="t">me the keys from this in reverse order of dimensionality. Continuous variables is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3391" target="_blank">00:56:31.820</a></span> | <span class="t">a list. Just make sure that there's no nulls, so continuous variables replace nulls with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3400" target="_blank">00:56:40.460</a></span> | <span class="t">zeros, categorical variables replace nulls with empties.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3404" target="_blank">00:56:44.380</a></span> | <span class="t">And then here's where we use the DataFrameMapper. A DataFrameMapper takes a list of tuples with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3413" target="_blank">00:56:53.780</a></span> | <span class="t">just two items in. The first item is the name of the variable, so in this case I'm looping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3418" target="_blank">00:56:58.140</a></span> | <span class="t">through each categorical variable name. The second thing in the tuple is an instance of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3425" target="_blank">00:57:05.460</a></span> | <span class="t">a class which is going to do your preprocessing. And there's really just two that you're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3432" target="_blank">00:57:12.180</a></span> | <span class="t">to use almost all the time. The categorical variables, sklearn comes with something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3437" target="_blank">00:57:17.660</a></span> | <span class="t">label encoder. It's really badly documented, in fact misleadingly documented, but this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3446" target="_blank">00:57:26.620</a></span> | <span class="t">is exactly what you want. It's something that takes a column, figures out what are all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3451" target="_blank">00:57:31.820</a></span> | <span class="t">unique values that appear in that column, and replaces them with a set of contiguous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3456" target="_blank">00:57:36.100</a></span> | <span class="t">integers. So if you've got the days of the week, Monday through Sunday, it'll replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3461" target="_blank">00:57:41.500</a></span> | <span class="t">them with zeros through sevens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3465" target="_blank">00:57:45.300</a></span> | <span class="t">And then very importantly, this is critically important, you need to make sure that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3470" target="_blank">00:57:50.500</a></span> | <span class="t">training set and the test set have the same codes. There's no point in having Sunday be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3474" target="_blank">00:57:54.860</a></span> | <span class="t">zero in the training set and one in the test set. So because we're actually instantiating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3481" target="_blank">00:58:01.460</a></span> | <span class="t">this class here, this object is going to actually keep track of which codes it's using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3488" target="_blank">00:58:08.780</a></span> | <span class="t">And then ditto for the continuous, we want to normalize them to a 0, 1 variables. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3495" target="_blank">00:58:15.180</a></span> | <span class="t">again, we need to remember what was the mean that we subtracted, what was the standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3499" target="_blank">00:58:19.020</a></span> | <span class="t">deviation we divided by, so that we can do exactly the same thing to the test set. Otherwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3503" target="_blank">00:58:23.660</a></span> | <span class="t">again our models are going to be nearly totally useless. So the way the dataframe mapper works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3508" target="_blank">00:58:28.260</a></span> | <span class="t">is that it's using this instantiated object, it's going to keep track with this information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3513" target="_blank">00:58:33.260</a></span> | <span class="t">So this is basically code you can copy and paste in every one of your models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3518" target="_blank">00:58:38.420</a></span> | <span class="t">Once we've got those mappings, you just pass those to a dataframe mapper, and then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3524" target="_blank">00:58:44.540</a></span> | <span class="t">call .fit passing in your dataset. And so this thing now is a special object which has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3532" target="_blank">00:58:52.900</a></span> | <span class="t">a .features property that's going to contain all of the pre-processed features that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3542" target="_blank">00:59:02.260</a></span> | <span class="t">want. Categorical columns contains the result of doing this mapping, basically doing this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3549" target="_blank">00:59:09.100</a></span> | <span class="t">label encoding. In some ways the details of how this works doesn't matter too much because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3557" target="_blank">00:59:17.020</a></span> | <span class="t">you can just use exactly this code in every one of your models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3560" target="_blank">00:59:20.180</a></span> | <span class="t">Same for continuous, it's exactly the same code, but of course continuous, it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3566" target="_blank">00:59:26.020</a></span> | <span class="t">to be using standard scalar, which is the scikit-learn thing that turns it into a zero-mean-one standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3572" target="_blank">00:59:32.840</a></span> | <span class="t">deviation variable. So we've now got continuous columns that have all been standardized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3579" target="_blank">00:59:39.020</a></span> | <span class="t">Here's an example of the first five rows from the zeroth column for a categorical, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3591" target="_blank">00:59:51.420</a></span> | <span class="t">ditto for a continuous. You can see these have been turned into integers and these have been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3599" target="_blank">00:59:59.100</a></span> | <span class="t">turned into numbers which are going to average to zero and have a standard deviation of one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3605" target="_blank">01:00:05.400</a></span> | <span class="t">One of the nice things about this dataframe mapper is that you can now take that object</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3611" target="_blank">01:00:11.940</a></span> | <span class="t">and actually store it, pickle it. So now you can use those categorical encodings and scaling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3620" target="_blank">01:00:20.260</a></span> | <span class="t">parameters elsewhere. By just unpickling it, you've immediately got those same parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3630" target="_blank">01:00:30.180</a></span> | <span class="t">For my categorical variables, you can see here the number of unique classes in every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3637" target="_blank">01:00:37.340</a></span> | <span class="t">one. So here's my 1,100 stores, 31 days of the month, 7 days of the week, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3648" target="_blank">01:00:48.940</a></span> | <span class="t">So that's the kind of key pre-processing that has to be done. So here is their big mistake,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3659" target="_blank">01:00:59.020</a></span> | <span class="t">and I think if they didn't do this big mistake, they probably would have won. Their big mistake</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3663" target="_blank">01:01:03.820</a></span> | <span class="t">is that they went join.sales, not equal to zero. So they've removed all of the rows with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3672" target="_blank">01:01:12.940</a></span> | <span class="t">no sales. Those are all of the rows where the store was closed. Why was this a big mistake?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3681" target="_blank">01:01:21.420</a></span> | <span class="t">Because if you go to the Rossman Store Sales Competition website and click on "Kernels"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3689" target="_blank">01:01:29.300</a></span> | <span class="t">and look at the kernel that got the highest rating. I'll show you a couple of pictures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3711" target="_blank">01:01:51.940</a></span> | <span class="t">Here is an example of a store, Store 708, and these are all from this kernel. Here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3718" target="_blank">01:01:58.460</a></span> | <span class="t">a period of time where it was closed to refurbishment. This happens a lot in Rossman stores. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3725" target="_blank">01:02:05.340</a></span> | <span class="t">get these periods of time when you get zeros for sales, lots in a row. Look what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3731" target="_blank">01:02:11.500</a></span> | <span class="t">immediately before and after. So in the data set that we're looking at, our unfortunate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3739" target="_blank">01:02:19.740</a></span> | <span class="t">third place winners deleted all of these. So they had no ability to build a feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3745" target="_blank">01:02:25.980</a></span> | <span class="t">that could find this. So this Store 708. Look, here's another one where it was closed. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3755" target="_blank">01:02:35.100</a></span> | <span class="t">this turns out to be super common. The second place winner actually built a feature. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3764" target="_blank">01:02:44.700</a></span> | <span class="t">going to be exactly the same feature we've seen before. How many days since they're closing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3768" target="_blank">01:02:48.820</a></span> | <span class="t">and how many days until when they're closing. If they had just done that, I'm pretty sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3773" target="_blank">01:02:53.220</a></span> | <span class="t">they would have won. So that was their big mistake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3779" target="_blank">01:02:59.940</a></span> | <span class="t">This kernel has a number of interesting analyses in it. Here's another one which I think our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3788" target="_blank">01:03:08.380</a></span> | <span class="t">neural net can capture, although it might have been better to be explicit. Some stores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3796" target="_blank">01:03:16.980</a></span> | <span class="t">opened on Sundays. Most didn't, but some did. For those stores that opened on Sundays, their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3805" target="_blank">01:03:25.340</a></span> | <span class="t">sales on Sundays were far higher than on any other day. I guess that's because in Germany</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3810" target="_blank">01:03:30.460</a></span> | <span class="t">I guess not many shops opened on Sundays. So something else that they didn't explicitly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3815" target="_blank">01:03:35.620</a></span> | <span class="t">do was create a "is store open on Sunday" field. Having said that, I think the neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3823" target="_blank">01:03:43.340</a></span> | <span class="t">net may have been able to put that in the embedding. So if you're interested during</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3828" target="_blank">01:03:48.100</a></span> | <span class="t">the week, you could try adding this field and see if it actually improves it or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3831" target="_blank">01:03:51.780</a></span> | <span class="t">It would certainly be interesting to hear if you try adding this field. Do you find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3836" target="_blank">01:03:56.420</a></span> | <span class="t">that you actually would win the competition?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3841" target="_blank">01:04:01.460</a></span> | <span class="t">This Sunday thing, these are all from the same Kaggle kernel, here's the day of week</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3847" target="_blank">01:04:07.620</a></span> | <span class="t">and here's the sales as a box plot. You can see normally on a Sunday, it's not that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3854" target="_blank">01:04:14.540</a></span> | <span class="t">sales are much higher. So it's really explicitly just for these particular stores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3862" target="_blank">01:04:22.980</a></span> | <span class="t">That's the kind of visualization stuff which is really helpful to do as you work through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3870" target="_blank">01:04:30.900</a></span> | <span class="t">these kinds of problems. I don't know, just draw lots of pictures. Those pictures were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3875" target="_blank">01:04:35.900</a></span> | <span class="t">drawn in R, and R is actually pretty good for this kind of structured data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3880" target="_blank">01:04:40.340</a></span> | <span class="t">I have a question. For categorical fields, they're converted by the numbers not with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3888" target="_blank">01:04:48.020</a></span> | <span class="t">me and zero. They were just messages Monday is zero, Tuesday is one, whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3894" target="_blank">01:04:54.900</a></span> | <span class="t">As is, they will send to a neural network just like...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3899" target="_blank">01:04:59.440</a></span> | <span class="t">We're going to get there. We're going to use embeddings. Just like we did with word embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3905" target="_blank">01:05:05.880</a></span> | <span class="t">remember, we turned every word into a word index. So our sentences, rather than being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3913" target="_blank">01:05:13.220</a></span> | <span class="t">like the dog ate the beans, it would be 3, 6, 12, 2, whatever. We're going to do the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3921" target="_blank">01:05:21.940</a></span> | <span class="t">same basic thing. We've done the same basic thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3925" target="_blank">01:05:25.780</a></span> | <span class="t">So now that we've done our terrible mistake, we've now still got 824,000 rows left. As</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3934" target="_blank">01:05:34.460</a></span> | <span class="t">per usual, I made it really easy for me to create a random sample and did most of my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3940" target="_blank">01:05:40.000</a></span> | <span class="t">analysis with a random sample, but can just as easily not do the random sample. So now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3946" target="_blank">01:05:46.260</a></span> | <span class="t">I've got a separate sample version of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3951" target="_blank">01:05:51.980</a></span> | <span class="t">Split it into training and test. Notice here, the way I split it into training and test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3958" target="_blank">01:05:58.820</a></span> | <span class="t">is not randomly. The reason it's not randomly is because in the Kaggle competition, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3964" target="_blank">01:06:04.500</a></span> | <span class="t">set it up the smart way. The smart way to set up a test set in a time series is to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3971" target="_blank">01:06:11.580</a></span> | <span class="t">your test set the most recent period of time. If you choose random points, you've got two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3978" target="_blank">01:06:18.420</a></span> | <span class="t">problems. The first is you're predicting tomorrow's sales where you always have the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3983" target="_blank">01:06:23.500</a></span> | <span class="t">day's sales which is very rarely the way things really work. And then secondly, you're ignoring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3992" target="_blank">01:06:32.580</a></span> | <span class="t">the fact that in the real world, you're always trying to model a few days or a few weeks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=3998" target="_blank">01:06:38.620</a></span> | <span class="t">or a few months in the future that haven't happened yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4001" target="_blank">01:06:41.420</a></span> | <span class="t">So the way you want to set up, if you were setting up the data for such a model yourself,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4008" target="_blank">01:06:48.500</a></span> | <span class="t">you would need to be deciding how often am I going to be rerunning this model, how long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4013" target="_blank">01:06:53.540</a></span> | <span class="t">is it going to take for those model results to get into the field, to be used in however</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4017" target="_blank">01:06:57.820</a></span> | <span class="t">they're being used. In this case, I can't remember, I think it's like a month or two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4025" target="_blank">01:07:05.300</a></span> | <span class="t">So in that case I should make sure there's a month or two test set, which is the last</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4033" target="_blank">01:07:13.900</a></span> | <span class="t">bit. So you can see here, I've taken the last 10% of my validation set and it's literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4040" target="_blank">01:07:20.620</a></span> | <span class="t">just here's the first bit and here's the last bit, and since it was already sorted by date,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4049" target="_blank">01:07:29.340</a></span> | <span class="t">this ensures that I have it done the way I want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4052" target="_blank">01:07:32.260</a></span> | <span class="t">I just wanted to point out that it's 10 to 8, so we should probably take a break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4068" target="_blank">01:07:48.660</a></span> | <span class="t">This is how you take that data frame map object we created earlier, we call .fit, in order</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4074" target="_blank">01:07:54.620</a></span> | <span class="t">to learn the transformation parameters, we then call transform to actually do it. So take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4085" target="_blank">01:08:05.100</a></span> | <span class="t">my training set and transform it to grab the categorical variables, and then the continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4092" target="_blank">01:08:12.060</a></span> | <span class="t">preprocessing is the same thing for my continuous map. So preprocess my training set and grab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4098" target="_blank">01:08:18.140</a></span> | <span class="t">my continuous variables. So that's nearly done. The only final piece is in their solution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4112" target="_blank">01:08:32.580</a></span> | <span class="t">they modified their target, their sales value. And the way they modified it was that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4120" target="_blank">01:08:40.260</a></span> | <span class="t">found the highest amount of sales, and they took the log of that, and then they modified</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4129" target="_blank">01:08:49.060</a></span> | <span class="t">all of their y values to take the log of sales divided by the maximum log of sales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4136" target="_blank">01:08:56.460</a></span> | <span class="t">So what this means is that the y values are going to be no higher than 1. And furthermore,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4144" target="_blank">01:09:04.900</a></span> | <span class="t">remember how they had a long tail, the average was 5,000, the maximum was 40-something thousand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4151" target="_blank">01:09:11.100</a></span> | <span class="t">This is really common, like most financial data, sales data, so forth, generally has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4156" target="_blank">01:09:16.420</a></span> | <span class="t">a nicer shape when it's logged than it does not. So taking a log is a really good idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4162" target="_blank">01:09:22.280</a></span> | <span class="t">The reason that as well as taking the log they also did this division is it means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4167" target="_blank">01:09:27.860</a></span> | <span class="t">what we can now do is we can use an activation function in our neural net of a sigmoid, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4173" target="_blank">01:09:33.700</a></span> | <span class="t">goes between 0 and 1, and then just multiply by the maximum log. So that's basically going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4180" target="_blank">01:09:40.100</a></span> | <span class="t">to ensure that the data is in the right scaling area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4183" target="_blank">01:09:43.820</a></span> | <span class="t">I actually tried taking this out, and this technique doesn't really seem to help. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4190" target="_blank">01:09:50.900</a></span> | <span class="t">it actually reminds me of the style transfer paper where they mentioned they originally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4197" target="_blank">01:09:57.500</a></span> | <span class="t">had a hyperbolic tan layer at the end for exactly the same reason, to make sure everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4202" target="_blank">01:10:02.620</a></span> | <span class="t">was between 0 and 255. It actually turns out if you just use a linear activation it worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4206" target="_blank">01:10:06.820</a></span> | <span class="t">just as well. So interestingly this idea of using sigmoids at the end in order to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4214" target="_blank">01:10:14.060</a></span> | <span class="t">the right range doesn't seem to be that helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4217" target="_blank">01:10:17.620</a></span> | <span class="t">My guess is the reason why is because for a sigmoid it's really difficult to get the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4223" target="_blank">01:10:23.020</a></span> | <span class="t">maximum. And I think actually what they should have done is they probably should have, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4228" target="_blank">01:10:28.820</a></span> | <span class="t">of using maximum, they should have used maximum times 1.25 so that they never have to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4235" target="_blank">01:10:35.780</a></span> | <span class="t">1, because it's impossible to predict 1 because it's a sigmoid.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4240" target="_blank">01:10:40.380</a></span> | <span class="t">Someone asked, "Is there any issue in fitting the preprocessors on the full training and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4246" target="_blank">01:10:46.780</a></span> | <span class="t">validation data? Shouldn't they be fit only to the training set?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4251" target="_blank">01:10:51.620</a></span> | <span class="t">No, it's fine. In fact, for the categorical variables, if you don't include the test set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4261" target="_blank">01:11:01.460</a></span> | <span class="t">then you're going to have some codes that aren't there at all. Or else this way there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4265" target="_blank">01:11:05.860</a></span> | <span class="t">going to be random, which is better than failing. As for deciding what to divide and subtract</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4274" target="_blank">01:11:14.380</a></span> | <span class="t">in order to get a 0, 1 random variable, it doesn't really matter. There's no leakage</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4279" target="_blank">01:11:19.860</a></span> | <span class="t">involved because that's what you're worried about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4286" target="_blank">01:11:26.120</a></span> | <span class="t">Root means squared percent error is what the Kaggle competition used as the official loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4291" target="_blank">01:11:31.660</a></span> | <span class="t">function, so this is just calculating that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4294" target="_blank">01:11:34.640</a></span> | <span class="t">So before we take a break, we'll finally take a look at the definition of the model. I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4301" target="_blank">01:11:41.860</a></span> | <span class="t">kind of work backwards. Here's the basic model. Get our embeddings, combine the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4313" target="_blank">01:11:53.620</a></span> | <span class="t">with the continuous variables, a tiny bit of dropout, one dense layer, two dense layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4319" target="_blank">01:11:59.420</a></span> | <span class="t">more dropout, and then the final sigmoid activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4325" target="_blank">01:12:05.060</a></span> | <span class="t">You'll see that I've got commented out stuff all over the place. This is because I had</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4330" target="_blank">01:12:10.340</a></span> | <span class="t">a lot of questions, we're going to cover this after the break, a lot of questions about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4333" target="_blank">01:12:13.580</a></span> | <span class="t">some of the details of why did they do things certain ways, some of the things they did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4339" target="_blank">01:12:19.620</a></span> | <span class="t">were so weird, I just thought they couldn't possibly be right. So I did some experimenting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4343" target="_blank">01:12:23.620</a></span> | <span class="t">we'll learn more about that in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4347" target="_blank">01:12:27.020</a></span> | <span class="t">So the embeddings, as per usual, I create a little function to create an embedding, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4354" target="_blank">01:12:34.740</a></span> | <span class="t">first of all creates my regular Keras input layer, and then it creates my embedding layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4363" target="_blank">01:12:43.500</a></span> | <span class="t">and then how many embedding dimensions I'm going to use. Sometimes I looked them up in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4368" target="_blank">01:12:48.460</a></span> | <span class="t">that dictionary I had earlier, and sometimes I calculated them using this simple approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4374" target="_blank">01:12:54.860</a></span> | <span class="t">of saying I will use however many levels there are in the categorical variable divided by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4380" target="_blank">01:13:00.340</a></span> | <span class="t">2 with a maximum of 50. These were two different techniques I was playing with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4387" target="_blank">01:13:07.740</a></span> | <span class="t">Normally with word embeddings, you have a whole sentence, and so you've got to feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4393" target="_blank">01:13:13.740</a></span> | <span class="t">it to an RNN, and so you have time steps. So normally you have an input length equal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4398" target="_blank">01:13:18.540</a></span> | <span class="t">to the length of your sentence. This is the time steps for an RNN. We don't have an RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4405" target="_blank">01:13:25.140</a></span> | <span class="t">we don't have any time steps. We just have one element in one column. So therefore I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4412" target="_blank">01:13:32.100</a></span> | <span class="t">have to pass flatten after this because it's going to have this redundant unit 1 time axis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4420" target="_blank">01:13:40.580</a></span> | <span class="t">that I don't want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4422" target="_blank">01:13:42.220</a></span> | <span class="t">So this is just because people don't normally do this kind of stuff with embeddings, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4426" target="_blank">01:13:46.940</a></span> | <span class="t">they're assuming that you're going to want it in a format ready to go to an RNN, so this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4431" target="_blank">01:13:51.100</a></span> | <span class="t">is just turning it back into a normal format. So we grab each embedding, we end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4437" target="_blank">01:13:57.660</a></span> | <span class="t">a whole list of those. We then combine all of those embeddings with all of our continuous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4443" target="_blank">01:14:03.180</a></span> | <span class="t">variables into a single list of variables. And so then our model is going to have all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4450" target="_blank">01:14:10.340</a></span> | <span class="t">of those embedding inputs and all of our continuous inputs, and then we can compile it and train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4457" target="_blank">01:14:17.380</a></span> | <span class="t">it. So let's take a break and see you back here at 5 past 8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4486" target="_blank">01:14:46.700</a></span> | <span class="t">So we've got our neural net set up. We train it in the usual way, go.fit, and away we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4505" target="_blank">01:15:05.220</a></span> | <span class="t">So that's basically that. It trains reasonably quickly, 6 minutes in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4518" target="_blank">01:15:18.800</a></span> | <span class="t">So we've got two questions that came in. One of them is, for the normalization, is it possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4529" target="_blank">01:15:29.700</a></span> | <span class="t">to use another function other than log, such as sigmoid?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4540" target="_blank">01:15:40.020</a></span> | <span class="t">I don't think you'd want to use sigmoid. A kind of financial data and sales data tends</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4545" target="_blank">01:15:45.620</a></span> | <span class="t">to be of a shape where log will make it more linear, which is generally what you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4552" target="_blank">01:15:52.080</a></span> | <span class="t">And then when we log transform our target variable, we're also transforming the squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4557" target="_blank">01:15:57.540</a></span> | <span class="t">error. Is this a problem? Or is it helping the model to find a better minimum error in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4562" target="_blank">01:16:02.260</a></span> | <span class="t">the untransformed space?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4563" target="_blank">01:16:03.260</a></span> | <span class="t">Yeah, so you've got to be careful about what loss function you want. In this case the Kaggle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4567" target="_blank">01:16:07.100</a></span> | <span class="t">competition is trying to minimize root and mean squared percent error. So I actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4572" target="_blank">01:16:12.540</a></span> | <span class="t">then said I want you to do mean absolute error because in log space that's basically doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4581" target="_blank">01:16:21.900</a></span> | <span class="t">the same thing. The percent is a ratio, so this is the absolute error between two logs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4588" target="_blank">01:16:28.460</a></span> | <span class="t">which is basically the same as a ratio. So you need to make sure your loss function is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4592" target="_blank">01:16:32.620</a></span> | <span class="t">appropriate in that space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4598" target="_blank">01:16:38.540</a></span> | <span class="t">I think this is one of the things that I didn't do in the original competition. As you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4602" target="_blank">01:16:42.900</a></span> | <span class="t">see I tried changing it and I think it helped. By the way, XGBoost is fantastic. Here is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4617" target="_blank">01:16:57.660</a></span> | <span class="t">same series of steps to run this model with XGBoost. As you can see, I just concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4625" target="_blank">01:17:05.020</a></span> | <span class="t">my categorical and continuous for training and my validation set. Here is a set of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4632" target="_blank">01:17:12.780</a></span> | <span class="t">which tends to work pretty well. XGBoost has a data type called DMatrix, which is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4643" target="_blank">01:17:23.300</a></span> | <span class="t">a normal matrix but it keeps track of the names of the features, so it prints out better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4651" target="_blank">01:17:31.100</a></span> | <span class="t">information. Then you go .train and this takes less than a second to run. It's not massively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4659" target="_blank">01:17:39.020</a></span> | <span class="t">worse than our previous result. This is a good way to get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4667" target="_blank">01:17:47.460</a></span> | <span class="t">The reason that XGBoost and Random Forest is particularly helpful is because it does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4673" target="_blank">01:17:53.220</a></span> | <span class="t">something called variable importance. This is how you get the variable importance for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4678" target="_blank">01:17:58.060</a></span> | <span class="t">an XGBoost model. It takes a second and suddenly here is the information you need. When I was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4685" target="_blank">01:18:05.700</a></span> | <span class="t">having trouble replicating the original results from the third place winners, one of the things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4694" target="_blank">01:18:14.460</a></span> | <span class="t">that helped me a lot was to look at this feature importance plot and say, "Competition distance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4701" target="_blank">01:18:21.140</a></span> | <span class="t">holy cow, that's really really important. Let's make sure that my competition distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4706" target="_blank">01:18:26.420</a></span> | <span class="t">results pre-processing really is exactly the same." On the other hand, events doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4716" target="_blank">01:18:36.540</a></span> | <span class="t">matter at all, so I'm not going to worry really at all about checking my events. This feature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4723" target="_blank">01:18:43.980</a></span> | <span class="t">importance or variable importance plot, also as it's known, you can also create with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4728" target="_blank">01:18:48.460</a></span> | <span class="t">random forest. These are amazing. Because you're using a tree ensemble, it doesn't matter the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4738" target="_blank">01:18:58.740</a></span> | <span class="t">shape of anything, it doesn't matter if you have or don't have interactions, this is all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4744" target="_blank">01:19:04.060</a></span> | <span class="t">totally assumption free. In real life, this is the first thing I do. The first thing I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4752" target="_blank">01:19:12.500</a></span> | <span class="t">do is try to get a feature importance plot printed. Often it turns out that there's only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4759" target="_blank">01:19:19.220</a></span> | <span class="t">three or four variables in that. If you've got 10,000 variables, so I worked on a big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4764" target="_blank">01:19:24.940</a></span> | <span class="t">credit scoring problem a couple of years ago, I had 9,500 variables. It turned out that only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4770" target="_blank">01:19:30.220</a></span> | <span class="t">nine of them mattered. So the company I was working for literally had spent something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4775" target="_blank">01:19:35.900</a></span> | <span class="t">like $5 million on this big management consulting project, and this big management consulting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4780" target="_blank">01:19:40.700</a></span> | <span class="t">project had told them all these ways in which they can capture all this information in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4785" target="_blank">01:19:45.220</a></span> | <span class="t">really clean way for their credit scoring models. Of course none of those things were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4790" target="_blank">01:19:50.220</a></span> | <span class="t">in these nine that mattered, so they could have saved $5 billion, but they didn't because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4797" target="_blank">01:19:57.740</a></span> | <span class="t">management consulting companies don't use random forests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4801" target="_blank">01:20:01.980</a></span> | <span class="t">I can't overstate the importance of this plot, but this is a deep learning course, so we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4809" target="_blank">01:20:09.480</a></span> | <span class="t">not really going to spend time talking about it. Now I mentioned that I had a whole bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4817" target="_blank">01:20:17.940</a></span> | <span class="t">of really weird things in the way that the competition playscaders did things. For one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4831" target="_blank">01:20:31.580</a></span> | <span class="t">they didn't normalize their continuous variables. Who does that? But then when people do well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4837" target="_blank">01:20:37.700</a></span> | <span class="t">in a competition, something's working. The ways in which they initialized their embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4850" target="_blank">01:20:50.540</a></span> | <span class="t">were really, really weird. But all these things were really, really weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4855" target="_blank">01:20:55.420</a></span> | <span class="t">So what I did was I wrote a little script, Rusman Experiments, and what I did was basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4872" target="_blank">01:21:12.340</a></span> | <span class="t">I copied and pasted all the important code out of my notebook. Remember I've already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4877" target="_blank">01:21:17.460</a></span> | <span class="t">pickled the parameters for the label encoder and the scalar, so I didn't have to worry</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4886" target="_blank">01:21:26.260</a></span> | <span class="t">about doing those again. Once I copied and pasted all that code in, so this is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4890" target="_blank">01:21:30.900</a></span> | <span class="t">all the code you just saw, I then had this bunch of for loops. Pretty inelegant. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4905" target="_blank">01:21:45.820</a></span> | <span class="t">these are all of the things that I wanted to basically find out. Does it matter whether</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4910" target="_blank">01:21:50.460</a></span> | <span class="t">or not you use 1.0 scaling? Does it matter whether you use their weird approach to initializing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4918" target="_blank">01:21:58.500</a></span> | <span class="t">embeddings? Does it matter whether you use their particular dictionary of embedding dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4924" target="_blank">01:22:04.660</a></span> | <span class="t">or use my simple little formula?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4928" target="_blank">01:22:08.580</a></span> | <span class="t">Something else I tried is they basically took all their continuous variables and put them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4932" target="_blank">01:22:12.980</a></span> | <span class="t">through a separate little dense layer each. I was like, why don't we put them all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4937" target="_blank">01:22:17.420</a></span> | <span class="t">I also tried some other things like batch normalization. So I ran this and got back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4944" target="_blank">01:22:24.540</a></span> | <span class="t">every possible combination of these. This is where you want to be using the script.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4950" target="_blank">01:22:30.140</a></span> | <span class="t">I'm not going to tell you that I jumped straight to this. First of all, I spent days screwing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4957" target="_blank">01:22:37.280</a></span> | <span class="t">around with experiments in a notebook by hand, continually forgetting what I had just done,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4964" target="_blank">01:22:44.160</a></span> | <span class="t">until eventually it took me like an hour to write this. And then of course I pasted it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4972" target="_blank">01:22:52.620</a></span> | <span class="t">into Excel. And here it is. Chucked it into a pivot table, used conditional formatting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4982" target="_blank">01:23:02.140</a></span> | <span class="t">and here's my results. You can see all my different combinations, with and without normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4988" target="_blank">01:23:08.380</a></span> | <span class="t">with my special function versus their dictionary, using a single dense matrix versus putting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=4994" target="_blank">01:23:14.260</a></span> | <span class="t">everything together, using their weird init versus not using a weird init. And here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5003" target="_blank">01:23:23.740</a></span> | <span class="t">this dark blue here is what they did. It's full of weird to me. But as you can see, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5015" target="_blank">01:23:35.540</a></span> | <span class="t">actually the darkest blue. It actually is the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5019" target="_blank">01:23:39.380</a></span> | <span class="t">But then when you zoom out, you realize there's a whole corner over here that's got a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5026" target="_blank">01:23:46.300</a></span> | <span class="t">of eight-sixes, it's nearly as good, but seems much more consistent. And also more consistent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5034" target="_blank">01:23:54.020</a></span> | <span class="t">with sanity. Like yes, do normalize your data. And yes, do use an appropriate initialization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5041" target="_blank">01:24:01.020</a></span> | <span class="t">function. And if you do those two things, it doesn't really matter what else you do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5044" target="_blank">01:24:04.340</a></span> | <span class="t">it's all going to work fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5046" target="_blank">01:24:06.180</a></span> | <span class="t">So what I then did was I created a little sparkline in Excel for the actual training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5052" target="_blank">01:24:12.780</a></span> | <span class="t">graphs. And so here's their winning one, again, .085. But here's the variance of getting there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5064" target="_blank">01:24:24.980</a></span> | <span class="t">And as you can see, their approach was pretty bumpy, up and down, up and down, up and down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5069" target="_blank">01:24:29.020</a></span> | <span class="t">The second best on the other hand, .086 rather than .085, is going down very smoothly. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5078" target="_blank">01:24:38.580</a></span> | <span class="t">so that made me think, given that it's in this very stable part of the world, and given</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5083" target="_blank">01:24:43.140</a></span> | <span class="t">it's training much better, I actually think this is just random chance. It just happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5087" target="_blank">01:24:47.780</a></span> | <span class="t">to be low in this point. I actually thought this is a better approach. It's more sensible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5095" target="_blank">01:24:55.940</a></span> | <span class="t">and it's more consistent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5099" target="_blank">01:24:59.060</a></span> | <span class="t">So this kind of approach to running experiments, I thought I'd just show you to say when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5106" target="_blank">01:25:06.980</a></span> | <span class="t">run experiments, try and do it in a rigorous way and track both the stability of the approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5114" target="_blank">01:25:14.380</a></span> | <span class="t">as well as the actual result of the approach. So this one here makes so much sense. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5119" target="_blank">01:25:19.740</a></span> | <span class="t">like use my simple function rather than the weird dictionary, use normalization, use a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5126" target="_blank">01:25:26.020</a></span> | <span class="t">single dense matrix, and use a thoughtful initialization. And you do all of those things,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5130" target="_blank">01:25:30.140</a></span> | <span class="t">you end up with something that's basically as good and much more stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5136" target="_blank">01:25:36.100</a></span> | <span class="t">That's all I wanted to say about Rossman. I'm going to very briefly mention another competition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5147" target="_blank">01:25:47.500</a></span> | <span class="t">which is the Kaggle Taxi Destination competition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5160" target="_blank">01:26:00.580</a></span> | <span class="t">You were saying that you did a couple of experiments. One, you figured out the embeddings and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5167" target="_blank">01:26:07.460</a></span> | <span class="t">put the embeddings into random forests, and then put embeddings again into neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5173" target="_blank">01:26:13.620</a></span> | <span class="t">I didn't do that, that was from the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5176" target="_blank">01:26:16.420</a></span> | <span class="t">Yeah, so I don't understand because you just use one neural network to do everything together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5181" target="_blank">01:26:21.700</a></span> | <span class="t">no?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5182" target="_blank">01:26:22.700</a></span> | <span class="t">Yeah, so what they did was, for this one here, this 115, they trained the neural network I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5187" target="_blank">01:26:27.580</a></span> | <span class="t">just showed you. They then threw away the neural network and trained a GBM model, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5196" target="_blank">01:26:36.180</a></span> | <span class="t">for the categorical variables, rather than using 100 encodings, they used the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5200" target="_blank">01:26:40.900</a></span> | <span class="t">That's all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5204" target="_blank">01:26:44.700</a></span> | <span class="t">So the taxi competition was won by the team with this Unicode name, which is pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5215" target="_blank">01:26:55.060</a></span> | <span class="t">And it's actually turned out to be a team run by Yoshua Bengio, who's one of the people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5220" target="_blank">01:27:00.140</a></span> | <span class="t">that stuck it out through the AI winter and is now one of the leading lights in deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5227" target="_blank">01:27:07.660</a></span> | <span class="t">And interestingly, the thing I just showed you, the Rossman competition, this paper they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5234" target="_blank">01:27:14.300</a></span> | <span class="t">wrote in the Rossman competition claimed to have invented this idea of categorical embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5240" target="_blank">01:27:20.500</a></span> | <span class="t">But actually, Yoshua Bengio's team won this competition a year earlier with this same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5245" target="_blank">01:27:25.980</a></span> | <span class="t">technique. But again, it's so uncool, nobody noticed even though it was Yoshua Bengio.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5252" target="_blank">01:27:32.900</a></span> | <span class="t">So I want to quickly show you what they did. This is the paper they wrote. And their approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5262" target="_blank">01:27:42.020</a></span> | <span class="t">to picking an embedding size was very simple. Use 10. So the data was which customer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5272" target="_blank">01:27:52.540</a></span> | <span class="t">taking this taxi, which taxi are they in, which taxi stand did they get the taxi from,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5280" target="_blank">01:28:00.380</a></span> | <span class="t">and then quarter hour of the day, day of the week, week of the year. And they didn't add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5287" target="_blank">01:28:07.180</a></span> | <span class="t">all kinds of other stuff, this is basically it. And so then they said we're going to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5293" target="_blank">01:28:13.980</a></span> | <span class="t">embeddings inspired by NLP. So actually to my knowledge, this is the first time this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5301" target="_blank">01:28:21.700</a></span> | <span class="t">appears in the literature. Having said that, I'm sure a thousand people have done it before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5306" target="_blank">01:28:26.660</a></span> | <span class="t">it's just not obvious to make it into a paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5311" target="_blank">01:28:31.420</a></span> | <span class="t">>> As a quick sanity check, if you have day of the week, like with seven, even one hot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5319" target="_blank">01:28:39.660</a></span> | <span class="t">variable potentials, and embedding size of 10, that doesn't make any sense, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5326" target="_blank">01:28:46.900</a></span> | <span class="t">>> Yeah, so I used to think that. But actually it does. Since the last few months quite often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5335" target="_blank">01:28:55.460</a></span> | <span class="t">ended up with bigger embeddings than my original patternality. And often it does give better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5342" target="_blank">01:29:02.220</a></span> | <span class="t">results. And I think it's just like when you realize that it's just a dense layer on top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5348" target="_blank">01:29:08.100</a></span> | <span class="t">of a one-hot encoding, it's like okay, why shouldn't the dense layer have more information?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5354" target="_blank">01:29:14.180</a></span> | <span class="t">I found it weird too, I still find it a little weird, but it definitely seems to be something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5358" target="_blank">01:29:18.940</a></span> | <span class="t">that's quite useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5367" target="_blank">01:29:27.300</a></span> | <span class="t">>> It does, it helps. I have absolutely found plenty of times now where I need a bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5378" target="_blank">01:29:38.460</a></span> | <span class="t">embedding metric's dimensionality than my cardinality of my categorical variable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5386" target="_blank">01:29:46.620</a></span> | <span class="t">Now in this competition, again it's a time series competition really, because the main</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5392" target="_blank">01:29:52.660</a></span> | <span class="t">thing you're given other than all this metadata is a series of GPS points, which is every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5398" target="_blank">01:29:58.660</a></span> | <span class="t">GPS point along a route. And at some point for the test set, the route is cut off and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5405" target="_blank">01:30:05.300</a></span> | <span class="t">you have to figure out what the final GPS point would have been. Where are they going?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5413" target="_blank">01:30:13.660</a></span> | <span class="t">Here's the model that they won with. It turns out to be very simple. You take all the metadata</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5422" target="_blank">01:30:22.180</a></span> | <span class="t">we just saw and chuck it through the embeddings. You then take the first 5 GPS points and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5430" target="_blank">01:30:30.340</a></span> | <span class="t">last 5 GPS points and concatenate them together with the embeddings. Chuck them through a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5436" target="_blank">01:30:36.940</a></span> | <span class="t">hidden layer, then through a softmax. This is quite interesting. What they then do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5444" target="_blank">01:30:44.260</a></span> | <span class="t">they take the result of this softmax and they combine it with clusters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5450" target="_blank">01:30:50.460</a></span> | <span class="t">Now what are these clusters? They used mean shift clustering, and they used mean shift</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5456" target="_blank">01:30:56.460</a></span> | <span class="t">clustering to figure out where are the places people tend to go. So with taxis, people tend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5462" target="_blank">01:31:02.940</a></span> | <span class="t">to go to the airport or they tend to go to the hospital or they tend to go to the shopping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5467" target="_blank">01:31:07.020</a></span> | <span class="t">strip. So using mean shift clustering, I think it was about 3,000 clusters, x, y coordinates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5477" target="_blank">01:31:17.380</a></span> | <span class="t">of places that people tend to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5480" target="_blank">01:31:20.740</a></span> | <span class="t">However, people don't always go to those 3,000 places. So this is a really cool thing. By</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5487" target="_blank">01:31:27.700</a></span> | <span class="t">using a softmax, and then they took the softmax and they multiplied it and took a weighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5494" target="_blank">01:31:34.900</a></span> | <span class="t">average using the softmax as the weights and the cluster centers as the thing that you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5500" target="_blank">01:31:40.340</a></span> | <span class="t">taking the weighted average of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5502" target="_blank">01:31:42.280</a></span> | <span class="t">So in other words, if they're going to the airport for sure, the softmax will end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5507" target="_blank">01:31:47.500</a></span> | <span class="t">giving a p of very close to 1 for the airport cluster. On the other hand, if it's not really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5513" target="_blank">01:31:53.300</a></span> | <span class="t">that clear whether they're going to this shopping strip or this movie, then those two cluster</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5521" target="_blank">01:32:01.140</a></span> | <span class="t">centers could both have a softmax of about 0.5, and so it's going to end up predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5525" target="_blank">01:32:05.880</a></span> | <span class="t">somewhere halfway between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5529" target="_blank">01:32:09.100</a></span> | <span class="t">So this is really interesting. They've built a different kind of architecture to anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5537" target="_blank">01:32:17.260</a></span> | <span class="t">we've seen before, where the softmax is not the last thing we do. It's being used to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5543" target="_blank">01:32:23.260</a></span> | <span class="t">average a bunch of clusters. So this is really smart because the softmax forces it to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5551" target="_blank">01:32:31.820</a></span> | <span class="t">easier for it to pick a specific destination that's very common, but also makes it possible</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5557" target="_blank">01:32:37.900</a></span> | <span class="t">for it to predict any destination anywhere by combining the average of a number of clusters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5564" target="_blank">01:32:44.100</a></span> | <span class="t">together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5565" target="_blank">01:32:45.100</a></span> | <span class="t">I think this is really elegant architecture engineering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5573" target="_blank">01:32:53.540</a></span> | <span class="t">Last 5 GPS points that were given. To create the training set, what they did was they took</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5588" target="_blank">01:33:08.220</a></span> | <span class="t">all of the roots and truncated them randomly. So every time they sampled another root, think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5598" target="_blank">01:33:18.540</a></span> | <span class="t">of the data generator. Basically the data generator would randomly slice it off somewhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5606" target="_blank">01:33:26.220</a></span> | <span class="t">So this was the last 5 points which we have access to, and the first 5 points. The reason</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5613" target="_blank">01:33:33.580</a></span> | <span class="t">it's not all the points is because they're using a standard multilayer perceptron here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5619" target="_blank">01:33:39.220</a></span> | <span class="t">So it's a variable length, a, and also you don't want it to be too big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5625" target="_blank">01:33:45.180</a></span> | <span class="t">There's a question. So the prefix is not fed into an RNN, it's just fed into a dense layer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5631" target="_blank">01:33:51.500</a></span> | <span class="t">Correct. So we just get 10 points, concatenate it together into a dense layer. So surprisingly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5637" target="_blank">01:33:57.940</a></span> | <span class="t">simple. How good was it? Look at the results, 2-1-4, 2-1-4, 2-1-3, 2-1-3, 2-1-1, 2-1-2. Everybody</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5654" target="_blank">01:34:14.900</a></span> | <span class="t">is clustered together. One person's a bit better at 208, and they're way better at 203.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5661" target="_blank">01:34:21.180</a></span> | <span class="t">And then they mentioned in the paper that they didn't actually have time to finish training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5665" target="_blank">01:34:25.380</a></span> | <span class="t">so when they actually finished training, it was actually 1.87. They won so easily, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5673" target="_blank">01:34:33.220</a></span> | <span class="t">not funny. And interestingly in the paper, they actually mentioned the test set was so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5677" target="_blank">01:34:37.380</a></span> | <span class="t">small that they knew the only way they could be sure to win was to make sure they won easily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5685" target="_blank">01:34:45.100</a></span> | <span class="t">Now because the test set was so small, the leaderboard is actually not statistically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5689" target="_blank">01:34:49.980</a></span> | <span class="t">that great. So they created a custom test set and tried to see if they could find something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5695" target="_blank">01:34:55.820</a></span> | <span class="t">that's even better still on the custom test set. And it turns out that actually an RNN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5700" target="_blank">01:35:00.780</a></span> | <span class="t">is better still. It still would have won the competition, but there's not enough data in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5706" target="_blank">01:35:06.980</a></span> | <span class="t">the Kaggle test set that this is a statistically significant result. In this case it is statistically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5712" target="_blank">01:35:12.540</a></span> | <span class="t">significant. A regular RNN wasn't better, but what they did instead was take an RNN where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5721" target="_blank">01:35:21.420</a></span> | <span class="t">we pass in 5 points at a time into the RNN basically. I think what probably would have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5729" target="_blank">01:35:29.140</a></span> | <span class="t">been even better would be to have had a convolutional layer first and then passed that into an RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5735" target="_blank">01:35:35.060</a></span> | <span class="t">They didn't try it as far as I can see from the paper. Importantly, a bidirectional RNN</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5742" target="_blank">01:35:42.660</a></span> | <span class="t">which ensures that the initial points and the last points tend to have more weight because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5748" target="_blank">01:35:48.760</a></span> | <span class="t">we know that RNN's state generally reflects things they've seen more recently. So this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5755" target="_blank">01:35:55.180</a></span> | <span class="t">result is this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5758" target="_blank">01:35:58.220</a></span> | <span class="t">So Paul Longsuffering intern Brad has been trying to replicate this result. He had at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5764" target="_blank">01:36:04.100</a></span> | <span class="t">least two all-nighters in the last two weeks but hasn't quite managed to yet. So I'm not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5768" target="_blank">01:36:08.220</a></span> | <span class="t">going to show you the code, but hopefully once Brad starts sleeping again he'll be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5773" target="_blank">01:36:13.140</a></span> | <span class="t">to finish it off and we can show you the notebook during the week on the forum that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5778" target="_blank">01:36:18.900</a></span> | <span class="t">re-implements this thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5781" target="_blank">01:36:21.780</a></span> | <span class="t">It was an interesting process to watch Brad try to replicate this because the vast majority</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5790" target="_blank">01:36:30.300</a></span> | <span class="t">of the time in my experience when people say they've tried a model and the model didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5794" target="_blank">01:36:34.940</a></span> | <span class="t">work out and they've given up on the model, it turns out that it's actually because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5798" target="_blank">01:36:38.220</a></span> | <span class="t">screwed something up, not because of the problem with the model. And if you weren't comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5804" target="_blank">01:36:44.500</a></span> | <span class="t">to Yoshua Bengio's team's result, knowing that you haven't replicated it yet, at which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5810" target="_blank">01:36:50.740</a></span> | <span class="t">point do you give up and say, "Oh my model's not working" versus saying, "No, I've still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5815" target="_blank">01:36:55.820</a></span> | <span class="t">got bugs!" It's very difficult to debug machine learning models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5823" target="_blank">01:37:03.540</a></span> | <span class="t">What Brad's actually had to end up doing is literally take the original Bengio team code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5829" target="_blank">01:37:09.300</a></span> | <span class="t">run it line by line, and then try to replicate it in Keras line by line in literally np.allclose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5836" target="_blank">01:37:16.100</a></span> | <span class="t">every time. Because to build a model like this, it doesn't look that complex, but there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5842" target="_blank">01:37:22.260</a></span> | <span class="t">just so many places that you can make little mistakes. No normal person will make like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5849" target="_blank">01:37:29.260</a></span> | <span class="t">zero mistakes. In fact, normal people like me will make dozens of mistakes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5854" target="_blank">01:37:34.340</a></span> | <span class="t">So when you build a model like this, you need to find a way to test every single line of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5859" target="_blank">01:37:39.420</a></span> | <span class="t">code. Any line of code you don't test, I guarantee you'll end up with a bug and you won't know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5863" target="_blank">01:37:43.720</a></span> | <span class="t">you have a bug and there's no way to ever find out you had a bug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5868" target="_blank">01:37:48.500</a></span> | <span class="t">So we have several questions. One is a note that pi*ci is very similar to what happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5878" target="_blank">01:37:58.260</a></span> | <span class="t">in the memory network paper. In that case, the output embeddings are weighted by the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5882" target="_blank">01:38:02.300</a></span> | <span class="t">attention probability. It's a lot like a regular attentional language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5890" target="_blank">01:38:10.940</a></span> | <span class="t">Can you talk more about the idea you have about first having the convolutional layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5894" target="_blank">01:38:14.780</a></span> | <span class="t">and passing that to an RNN? What do you mean by that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5899" target="_blank">01:38:19.300</a></span> | <span class="t">So here is a fantastic paper. We looked at these kind of subword encodings last week</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5928" target="_blank">01:38:48.900</a></span> | <span class="t">for language models. I don't know if any of you thought about this and wondered what if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5933" target="_blank">01:38:53.220</a></span> | <span class="t">we just had individual characters. There's a really fascinating paper called Fully Character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5940" target="_blank">01:39:00.180</a></span> | <span class="t">Level Machine Translation with no explicit segmentation from November of last year. They</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5948" target="_blank">01:39:08.100</a></span> | <span class="t">actually get fantastic results on just character level, beating pretty much everything, including</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5957" target="_blank">01:39:17.460</a></span> | <span class="t">the BPE approach we saw last time. So they looked at lots of different approaches and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5968" target="_blank">01:39:28.620</a></span> | <span class="t">comparing BPE to individual character, and most of the time they got the best results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5976" target="_blank">01:39:36.180</a></span> | <span class="t">Their model looks like this. They start out with every individual character. It goes through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5982" target="_blank">01:39:42.420</a></span> | <span class="t">a character embedding, just like we've used character embeddings lots of times. Then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5986" target="_blank">01:39:46.620</a></span> | <span class="t">take those character embeddings and you pass it through a one-dimensional convolution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5993" target="_blank">01:39:53.700</a></span> | <span class="t">I don't know if you guys remember, but in Part 1 of this course, Ben actually had a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=5999" target="_blank">01:39:59.020</a></span> | <span class="t">blog post about showing how you can do multiple size convolutions and concatenate them altogether.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6005" target="_blank">01:40:05.380</a></span> | <span class="t">So you could use that approach. Or you could just pick a single size. So you end up basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6011" target="_blank">01:40:11.460</a></span> | <span class="t">scrolling your convolutional window across your sets of characters. So you end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6018" target="_blank">01:40:18.460</a></span> | <span class="t">the same number of convolution outputs as you started out with letters, but they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6023" target="_blank">01:40:23.540</a></span> | <span class="t">now representing the information in a window around that letter. In this case, they then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6031" target="_blank">01:40:31.800</a></span> | <span class="t">did max pooling. So they basically said which window, assuming that we had a different size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6041" target="_blank">01:40:41.340</a></span> | <span class="t">as a size 4, a size 3, and a size 5. Which bits seem to have the highest activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6049" target="_blank">01:40:49.580</a></span> | <span class="t">around here. Then they took those max pooled things and they put them through a second</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6054" target="_blank">01:40:54.580</a></span> | <span class="t">set of segment embeddings. They then put that through something called a highway network</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6059" target="_blank">01:40:59.260</a></span> | <span class="t">which the details don't matter too much. It's kind of something like a DenseNet, like we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6063" target="_blank">01:41:03.540</a></span> | <span class="t">learned about last week. This is a slightly older approach than the DenseNet. Then finally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6069" target="_blank">01:41:09.180</a></span> | <span class="t">after doing all that, stick that through an RNN. So the idea here in this model was they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6075" target="_blank">01:41:15.260</a></span> | <span class="t">basically did as much learnt pre-processing as possible, and then finally put that into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6085" target="_blank">01:41:25.220</a></span> | <span class="t">an RNN. Because we've got these max pooling layers, this RNN ends up with a lot less time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6090" target="_blank">01:41:30.540</a></span> | <span class="t">points, which is really important to minimize the amount of processing in the RNN. So I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6099" target="_blank">01:41:39.660</a></span> | <span class="t">not going to go into detail on this, but check out this paper because it's really interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6106" target="_blank">01:41:46.720</a></span> | <span class="t">Next question is, for the destinations we would have more error for the peripheral points?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6112" target="_blank">01:41:52.260</a></span> | <span class="t">Are we taking a centroid of clusters? I don't understand that, sorry. All we're doing is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6122" target="_blank">01:42:02.840</a></span> | <span class="t">we're taking the softmax p, multiply by the cluster c, multiply them and add them up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6130" target="_blank">01:42:10.300</a></span> | <span class="t">I thought the first part was asking that with destinations that are more peripheral, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6136" target="_blank">01:42:16.060</a></span> | <span class="t">would have higher error because they would be harder to predict this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6139" target="_blank">01:42:19.140</a></span> | <span class="t">That probably, which is fine because by definition they're not close to a cluster center so they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6145" target="_blank">01:42:25.340</a></span> | <span class="t">not common.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6146" target="_blank">01:42:26.340</a></span> | <span class="t">Then going back, there was a question on the Rossman example. What does MAPE with neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6153" target="_blank">01:42:33.480</a></span> | <span class="t">network mean? I would have expected that result to be the same, why is it lower?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6157" target="_blank">01:42:37.980</a></span> | <span class="t">This is just using a one-hot encoding without an embedding layer. We kind of run out of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6170" target="_blank">01:42:50.580</a></span> | <span class="t">time a bit quickly, but I really want to show you this. The students and I have been trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6178" target="_blank">01:42:58.500</a></span> | <span class="t">to get a new approach to segmentation working, and I finally got it working in the last day</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6184" target="_blank">01:43:04.260</a></span> | <span class="t">or two, and I really wanted to show it to you. We talked last week about DenseNet, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6189" target="_blank">01:43:09.540</a></span> | <span class="t">I mentioned that DenseNet is arse-kickingly good at doing image classification with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6196" target="_blank">01:43:16.820</a></span> | <span class="t">small number of data points, like crazily good. But I also mentioned that it's the basis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6204" target="_blank">01:43:24.120</a></span> | <span class="t">of this thing called the 100-phase tiramisu, which is an approach to segmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6209" target="_blank">01:43:29.100</a></span> | <span class="t">So segmentation refers to taking a picture, an image, and figuring out where's the tree,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6218" target="_blank">01:43:38.260</a></span> | <span class="t">where's the dog, where's the bicycle and so forth. So it seems like we're not sure of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6225" target="_blank">01:43:45.260</a></span> | <span class="t">NGO fans today because this is one of his group's papers as well. Let me set the scene.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6240" target="_blank">01:44:00.760</a></span> | <span class="t">So Brendan, one of our students, who many of you have seen a lot of his blog posts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6246" target="_blank">01:44:06.820</a></span> | <span class="t">he has successfully got a PyTorch of this working, so I've shared that on our files.vast.ai.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6254" target="_blank">01:44:14.120</a></span> | <span class="t">And I got the Keras version of it working. So I'll show you the Keras version because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6257" target="_blank">01:44:17.700</a></span> | <span class="t">I actually understand it. And if anybody's interested in asking questions about the PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6262" target="_blank">01:44:22.060</a></span> | <span class="t">version, hopefully Brendan will be happy to answer them during the week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6267" target="_blank">01:44:27.900</a></span> | <span class="t">So the data looks like this. There's an image, and then there's a labels. So that's basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6290" target="_blank">01:44:50.620</a></span> | <span class="t">what it looks like. So you can see here, you've got traffic lights, you've got poles, you've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6294" target="_blank">01:44:54.220</a></span> | <span class="t">got trees, buildings, paths, roads. Interestingly, the dataset we're using is something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6302" target="_blank">01:45:02.940</a></span> | <span class="t">CanVid. The dataset is actually frames from a video. So a lot of the frames look very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6308" target="_blank">01:45:08.820</a></span> | <span class="t">similar to each other. And there's only like 600 frames in total, so there's very variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6316" target="_blank">01:45:16.980</a></span> | <span class="t">data in this CanVid dataset. Furthermore, we're not going to do any pre-training. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6324" target="_blank">01:45:24.340</a></span> | <span class="t">we're going to try and build the state-of-the-art classification system on video, which is already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6329" target="_blank">01:45:29.900</a></span> | <span class="t">much lower information content because most of the frames are pretty similar, using just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6334" target="_blank">01:45:34.380</a></span> | <span class="t">600 frames without pre-training. Now if you were to ask me a month ago, I would have told</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6338" target="_blank">01:45:38.260</a></span> | <span class="t">you it's not possible. This just seems like an incredibly difficult thing to do. But just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6344" target="_blank">01:45:44.620</a></span> | <span class="t">watch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6347" target="_blank">01:45:47.780</a></span> | <span class="t">So I'm going to skip to the answer first. Here's an example of a particular frame we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6358" target="_blank">01:45:58.220</a></span> | <span class="t">trying to match. Here is the ground truth for that frame. You can see there's a tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6367" target="_blank">01:46:07.300</a></span> | <span class="t">car here and a little car here. There are those little cars. There's a tree. Trees are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6372" target="_blank">01:46:12.780</a></span> | <span class="t">really difficult. They're incredibly fine, funny things. And here is my trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6382" target="_blank">01:46:22.060</a></span> | <span class="t">And as you can see, it's done really, really well. It's interesting to look at the mistakes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6388" target="_blank">01:46:28.620</a></span> | <span class="t">it made. This little thing here is a person. But you can see that the person, their head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6397" target="_blank">01:46:37.940</a></span> | <span class="t">looks a lot like traffic light and their jacket looks a lot like mailbox. Whereas these tiny</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6403" target="_blank">01:46:43.420</a></span> | <span class="t">little people here, it's done perfectly, or else this person got a little bit confused.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6415" target="_blank">01:46:55.820</a></span> | <span class="t">Another example of where it's gone wrong is this should be a road, or else it wasn't quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6419" target="_blank">01:46:59.700</a></span> | <span class="t">sure what was road and what was footpath, which makes sense because the colors do look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6424" target="_blank">01:47:04.020</a></span> | <span class="t">very similar. But had we have pre-trained something, a pre-trained network would have understood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6431" target="_blank">01:47:11.500</a></span> | <span class="t">that crossroads tend to go straight across, they don't tend to look like that. So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6438" target="_blank">01:47:18.060</a></span> | <span class="t">can kind of see where the minor mistakes it made, it also would have learned, had it looked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6444" target="_blank">01:47:24.540</a></span> | <span class="t">at more than a couple of hundred examples of people, that people generally are a particular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6450" target="_blank">01:47:30.420</a></span> | <span class="t">day. So there's just not enough data for it to have learned some of these things. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6454" target="_blank">01:47:34.420</a></span> | <span class="t">nonetheless, it is extraordinarily effective. Look at this traffic light, it's surrounded</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6461" target="_blank">01:47:41.340</a></span> | <span class="t">by a sign, so the ground truth actually has the traffic light and then a tiny little edge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6466" target="_blank">01:47:46.860</a></span> | <span class="t">of sign, and it's even got that right. So it's an incredibly accurate model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6476" target="_blank">01:47:56.220</a></span> | <span class="t">So how does it work? And in particular, how does it do these amazing trees? So the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6482" target="_blank">01:48:02.340</a></span> | <span class="t">is in this picture. Basically, this is inspired by a model called UNET. Until the UNET model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6495" target="_blank">01:48:15.220</a></span> | <span class="t">came along, everybody was doing these kinds of segmentation models using an approach just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6502" target="_blank">01:48:22.420</a></span> | <span class="t">like what we did for style transfer, which is basically you have a number of convolutional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6510" target="_blank">01:48:30.700</a></span> | <span class="t">layers with max pooling, or with a stride of 2, which gradually make the image smaller</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6516" target="_blank">01:48:36.980</a></span> | <span class="t">and smaller, a bigger receptive field. And then you go back up the other side using upsampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6522" target="_blank">01:48:42.660</a></span> | <span class="t">or deconvolutions until you get back to the original size, and then your final layer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6529" target="_blank">01:48:49.060</a></span> | <span class="t">the same size as your starting layer and has a bunch of different classes that you're trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6533" target="_blank">01:48:53.460</a></span> | <span class="t">to use in the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6538" target="_blank">01:48:58.940</a></span> | <span class="t">The problem with that is that you end up with, in fact I'll show you an example. There's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6545" target="_blank">01:49:05.420</a></span> | <span class="t">a really nice paper called UNET. UNET is not only an incredibly accurate model for segmentation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6554" target="_blank">01:49:14.300</a></span> | <span class="t">but it's also incredibly fast. It actually can run in real time. You can actually run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6559" target="_blank">01:49:19.740</a></span> | <span class="t">it on a video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6560" target="_blank">01:49:20.740</a></span> | <span class="t">But the mistakes it makes, look at this chair. This chair has a big gap here and here and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6566" target="_blank">01:49:26.220</a></span> | <span class="t">here, but UNET gets it totally wrong. And the reason why is because they use a very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6572" target="_blank">01:49:32.460</a></span> | <span class="t">traditional downsampling-upsampling approach. And by the time they get to the bottom, they've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6578" target="_blank">01:49:38.860</a></span> | <span class="t">just lost track of the fine detail. So the trick are these connections here. What we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6587" target="_blank">01:49:47.900</a></span> | <span class="t">do is we start with our input, we do a standard initial convolution, just like we did with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6592" target="_blank">01:49:52.900</a></span> | <span class="t">style transfer. We then have a DenseNet block, which we learned about last week. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6599" target="_blank">01:49:59.660</a></span> | <span class="t">that block, we keep going down, we do a MaxPooling type thing, another DenseNet block, MaxPooling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6605" target="_blank">01:50:05.260</a></span> | <span class="t">type thing, keep going down. And then as we go up the other side, so we do a DenseBlock,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6616" target="_blank">01:50:16.260</a></span> | <span class="t">we take the output from the DenseBlock on the way down and we actually copy it over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6623" target="_blank">01:50:23.540</a></span> | <span class="t">to here and concatenate the two together. So actually Brendan a few days ago actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6632" target="_blank">01:50:32.380</a></span> | <span class="t">drew this on our whiteboard when we were explaining it to Melissa, and so he's shown us every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6636" target="_blank">01:50:36.520</a></span> | <span class="t">stage here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6637" target="_blank">01:50:37.520</a></span> | <span class="t">We start out with a 224x224 input, it goes through the convolutions with 48 filters, goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6648" target="_blank">01:50:48.660</a></span> | <span class="t">through our DenseBlock, adds another 80 filters. It then goes through our, they call it a transition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6654" target="_blank">01:50:54.940</a></span> | <span class="t">down, so basically a MaxPooling. So it's now size 112. We keep doing that. DenseBlock, transition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6661" target="_blank">01:51:01.700</a></span> | <span class="t">down, so it's now 56x56, 28x28, 14x14, 7x7. And then on the way up again, we go transition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6670" target="_blank">01:51:10.540</a></span> | <span class="t">up, it's now 14x14. We copy across the results of the 14x14 from the transition down and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6678" target="_blank">01:51:18.380</a></span> | <span class="t">concatenate together. Then we do a DenseBlock, transition up, it's now 28x28, so we copy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6684" target="_blank">01:51:24.580</a></span> | <span class="t">across our 28x28 from the transition down and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6689" target="_blank">01:51:29.180</a></span> | <span class="t">So by the time we get all the way back up here, we're actually copying across something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6695" target="_blank">01:51:35.740</a></span> | <span class="t">that was originally of size 224x224. It hadn't had much done to it, it had only been through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6704" target="_blank">01:51:44.580</a></span> | <span class="t">one convolutional layer and one DenseBlock, so it hadn't really got much rich computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6709" target="_blank">01:51:49.380</a></span> | <span class="t">being done. But the thing is, by the time it gets back up all the way up here, the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6716" target="_blank">01:51:56.300</a></span> | <span class="t">knows pretty much this is a tree and this is a person and this is a house, and it just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6721" target="_blank">01:52:01.900</a></span> | <span class="t">needs to get the fine little details. Where exactly does this leaf finish? Where exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6726" target="_blank">01:52:06.500</a></span> | <span class="t">does the person's hat finish? So it's basically copying across something which is very high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6733" target="_blank">01:52:13.260</a></span> | <span class="t">resolution but doesn't have that much rich information, but that's fine because it really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6738" target="_blank">01:52:18.780</a></span> | <span class="t">only needs to fill in the details.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6741" target="_blank">01:52:21.260</a></span> | <span class="t">So these things here, they're called skip-connections. They were really inspired by this paper called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6746" target="_blank">01:52:26.100</a></span> | <span class="t">Unet, which has been one of many Kaggle competitions. But it's using dense blocks rather than normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6754" target="_blank">01:52:34.860</a></span> | <span class="t">fully connected blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6758" target="_blank">01:52:38.500</a></span> | <span class="t">So let me show you. We're not going to have time to go into this in detail, but I've done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6765" target="_blank">01:52:45.940</a></span> | <span class="t">all this coding Keras from scratch. This is actually a fantastic fit for Keras. I didn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6771" target="_blank">01:52:51.260</a></span> | <span class="t">have to create any custom layers, I didn't really have to do anything weird at all, except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6776" target="_blank">01:52:56.660</a></span> | <span class="t">for one thing, data augmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6780" target="_blank">01:53:00.980</a></span> | <span class="t">So the data augmentation was we start with 480x360 images, we randomly crop some 224x224</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6790" target="_blank">01:53:10.820</a></span> | <span class="t">part, and also randomly we may flip it horizontally. That's all perfectly fine. Keras doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6799" target="_blank">01:53:19.380</a></span> | <span class="t">have the random crops, unfortunately. But more importantly, whatever we do to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6803" target="_blank">01:53:23.940</a></span> | <span class="t">image, we also have to do to the target image. We need to get the same 224x224 crop, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6810" target="_blank">01:53:30.780</a></span> | <span class="t">we need to do the same horizontal flip.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6812" target="_blank">01:53:32.660</a></span> | <span class="t">So I had to write a data generator, which you guys may actually find useful anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6824" target="_blank">01:53:44.700</a></span> | <span class="t">So this is my data generator. Basically I called it a segment generator. It's just a standard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6832" target="_blank">01:53:52.780</a></span> | <span class="t">generator so it's got a next function. Each time you call next, it grabs some random bunch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6838" target="_blank">01:53:58.220</a></span> | <span class="t">of indexes, it goes through each one of those indexes and grabs the necessary item, grabbing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6846" target="_blank">01:54:06.220</a></span> | <span class="t">a random slice, sometimes randomly flipping it horizontally, and then it's doing this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6851" target="_blank">01:54:11.900</a></span> | <span class="t">to both the x's and the y's, returning them back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6859" target="_blank">01:54:19.220</a></span> | <span class="t">Along with this segment generator, in order to randomly grab a batch of random indices</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6866" target="_blank">01:54:26.220</a></span> | <span class="t">each time, I created this little class called batch indices, which can basically do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6873" target="_blank">01:54:33.700</a></span> | <span class="t">And it can have either shuffle true or shuffle false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6877" target="_blank">01:54:37.660</a></span> | <span class="t">So this pair of classes you guys might find really helpful for creating your own data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6882" target="_blank">01:54:42.980</a></span> | <span class="t">generators. This batch indices class in particular, now that I've written it, you can see how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6888" target="_blank">01:54:48.900</a></span> | <span class="t">it works, right? If I say batch indices from a data set of size 10, I want to grab three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6895" target="_blank">01:54:55.740</a></span> | <span class="t">indices at a time. So then let's grab five batches. Now in this case I've got by default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6902" target="_blank">01:55:02.940</a></span> | <span class="t">shuffle = false, so it just returns 0.1.2, 0.3.4.5, 0.6.7.8, 0.9, I'm finished, go back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6909" target="_blank">01:55:09.100</a></span> | <span class="t">to the start, 0.1.2. On the other hand, if I say shuffle = true, it returns them in random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6915" target="_blank">01:55:15.380</a></span> | <span class="t">order but it still makes sure it captures all of them. And then when we're done it starts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6919" target="_blank">01:55:19.940</a></span> | <span class="t">a new random order. So this makes it really easy to create random generators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6926" target="_blank">01:55:26.580</a></span> | <span class="t">So that was the only thing I had to add to Keras to get this order work. Other than that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6935" target="_blank">01:55:35.360</a></span> | <span class="t">we wrote the tiramisu. And the tiramisu looks very very similar to the DenseNet that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6940" target="_blank">01:55:40.620</a></span> | <span class="t">saw last week. We've got all our pieces, the relu, the dropout, the batchnorm, the relu</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6950" target="_blank">01:55:50.260</a></span> | <span class="t">on top of batchnorm, the concat layer, so this is something I had to add, my convolution2d</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6958" target="_blank">01:55:58.100</a></span> | <span class="t">followed by dropout, and then finally my batchnorm followed by relu followed by convolution2d.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6964" target="_blank">01:56:04.940</a></span> | <span class="t">So this is just the dense block that we saw last week. So a dense block is something where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6970" target="_blank">01:56:10.420</a></span> | <span class="t">we just keep grabbing 12 or 16 filters at a time, concatenating them to the last set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6976" target="_blank">01:56:16.820</a></span> | <span class="t">and doing that a few times. That's what a dense block is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6983" target="_blank">01:56:23.100</a></span> | <span class="t">So here's something interesting. The original paper for its down sampling, they call it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=6991" target="_blank">01:56:31.420</a></span> | <span class="t">transition down, did a 1x1 convolution followed by a max pooling. I actually discovered that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7003" target="_blank">01:56:43.340</a></span> | <span class="t">doing a stride2 convolution gives better results. So you'll see I actually have not followed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7010" target="_blank">01:56:50.140</a></span> | <span class="t">the paper. The one that's in commented out here is what the paper did, but actually this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7015" target="_blank">01:56:55.920</a></span> | <span class="t">works better. So that was interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7021" target="_blank">01:57:01.060</a></span> | <span class="t">Interestingly though, on the transition up side, do you remember that checkerboard artifacts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7028" target="_blank">01:57:08.140</a></span> | <span class="t">blog post we saw that showed that upsampling2d followed by a convolutional layer works better?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7035" target="_blank">01:57:15.340</a></span> | <span class="t">It does not work better for this. In fact, a deconvolution works better for this. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7041" target="_blank">01:57:21.780</a></span> | <span class="t">that's why you can see I've got this deconvolution layer. So I thought that was interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7049" target="_blank">01:57:29.660</a></span> | <span class="t">So basically you can see when I go downsampling a bunch of times, it's basically do a dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7055" target="_blank">01:57:35.940</a></span> | <span class="t">block and then I have to keep track of my skip connections. So basically keep a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7061" target="_blank">01:57:41.900</a></span> | <span class="t">of all of those skip connections. So I've got to hang on to all of these. So every one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7067" target="_blank">01:57:47.300</a></span> | <span class="t">of these skip connections I just stick in this little array, depending on them after</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7072" target="_blank">01:57:52.300</a></span> | <span class="t">every dense block. So then I keep them all and then I pass them to my upward path. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7080" target="_blank">01:58:00.620</a></span> | <span class="t">I basically do my transition up and then I concatenate that with that skip connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7089" target="_blank">01:58:09.020</a></span> | <span class="t">So that's the basic approach. So then the actual Tirabisu model itself with those pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7096" target="_blank">01:58:16.980</a></span> | <span class="t">is less than a screen of code. It's basically just do a 3x3 conv, do my down path, do my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7104" target="_blank">01:58:24.100</a></span> | <span class="t">up path using those skip connections, then a 1x1 conv at the end, and a softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7119" target="_blank">01:58:39.580</a></span> | <span class="t">So these dense nets, and indeed this fully convolutional dense net or this Tirabisu model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7127" target="_blank">01:58:47.460</a></span> | <span class="t">they actually take quite a long time to train. They don't have very many parameters, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7131" target="_blank">01:58:51.460</a></span> | <span class="t">is why I think they work so well with these tiny datasets. But they do still take a long</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7135" target="_blank">01:58:55.860</a></span> | <span class="t">time to train. Each epoch took a couple of minutes, and in the end I had to do many hundreds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7142" target="_blank">01:59:02.740</a></span> | <span class="t">of epochs. And I was also doing a bunch of learning rate annealing. So in the end this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7149" target="_blank">01:59:09.580</a></span> | <span class="t">kind of really had to train overnight, even though I had only about 500-600 friends. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7168" target="_blank">01:59:28.620</a></span> | <span class="t">in the end I got a really good result. I was a bit nervous at first. I was getting this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7180" target="_blank">01:59:40.620</a></span> | <span class="t">87.6% accuracy. In the paper they were getting 90% plus. It turns out that 3% of the pixels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7188" target="_blank">01:59:48.060</a></span> | <span class="t">are marked as void. I don't know why they're marked as void, but in the paper they actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7192" target="_blank">01:59:52.780</a></span> | <span class="t">remove them. So you'll see when you get to my results section, I've got this bit where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7196" target="_blank">01:59:56.980</a></span> | <span class="t">I remove those void ones. And I ended up with 89.5%. None of us in class managed to replicate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7208" target="_blank">02:00:08.940</a></span> | <span class="t">the paper. The paper got 91.5% or 91.2%. We tried the lasagna code they provided. We tried</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7218" target="_blank">02:00:18.740</a></span> | <span class="t">Brendan's PyTorch. We tried my Keras. Even though we couldn't replicate their result,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7225" target="_blank">02:00:25.700</a></span> | <span class="t">this is still better than any other result I've found. So this is still super accurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7233" target="_blank">02:00:33.020</a></span> | <span class="t">A couple of quick notes about this. First is they tried training also on something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7240" target="_blank">02:00:40.980</a></span> | <span class="t">the GATech dataset, which is another video dataset. The degree to which this is an amazing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7247" target="_blank">02:00:47.160</a></span> | <span class="t">model is really clear here. This 76% is from a model which is specifically built for video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7255" target="_blank">02:00:55.580</a></span> | <span class="t">so it actually includes the time component, which is absolutely critical, and it uses</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7260" target="_blank">02:01:00.100</a></span> | <span class="t">a pre-trained network, so it's used like a million images to pre-train, and it's still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7265" target="_blank">02:01:05.780</a></span> | <span class="t">not as good as this model. So that is an extraordinary comparison.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7272" target="_blank">02:01:12.620</a></span> | <span class="t">This is the CamTech comparison. Here's the model we were just looking at. Again, I actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7278" target="_blank">02:01:18.780</a></span> | <span class="t">looked into this. I thought 91.5%, whereas this one here 88%, wow, it actually looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7286" target="_blank">02:01:26.860</a></span> | <span class="t">like it's not that much better. I'm really surprised. Like even tree, I really thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7291" target="_blank">02:01:31.940</a></span> | <span class="t">it should win easily on tree, but it doesn't win by very much. So I actually went back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7297" target="_blank">02:01:37.000</a></span> | <span class="t">and looked at this paper, and it turns out that the authors of the DenseNet paper (this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7302" target="_blank">02:01:42.180</a></span> | <span class="t">is the paper by the way, modest scale that they're comparing to) turned out that they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7309" target="_blank">02:01:49.060</a></span> | <span class="t">actually trained on props of 852x852, so they actually used a way higher resolution image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7317" target="_blank">02:01:57.500</a></span> | <span class="t">to start with. You've got to be really careful when you read these comparisons. Sometimes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7324" target="_blank">02:02:04.500</a></span> | <span class="t">people actually shoot themselves in the foot, so these guys were comparing their result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7328" target="_blank">02:02:08.660</a></span> | <span class="t">to another model that was using like twice as big a picture. So again, this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7336" target="_blank">02:02:16.100</a></span> | <span class="t">way better than they actually made it look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7339" target="_blank">02:02:19.580</a></span> | <span class="t">Another one, like this one here, this 88 also looks impressive. But then I looked across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7344" target="_blank">02:02:24.300</a></span> | <span class="t">here and I noticed that the dilution8 model is like way better than this model on every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7352" target="_blank">02:02:32.940</a></span> | <span class="t">single category, way better. And yet somehow the average is only 0.3 better, and I realized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7359" target="_blank">02:02:39.060</a></span> | <span class="t">this actually has to be an error. So this model is actually a lot better than this table</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7368" target="_blank">02:02:48.620</a></span> | <span class="t">gives the impression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7374" target="_blank">02:02:54.260</a></span> | <span class="t">So I briefly mentioned that there's a model which doesn't have any skip connections called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7380" target="_blank">02:03:00.860</a></span> | <span class="t">eNet, which is actually better than the tiramisu on everything except for tree. But on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7388" target="_blank">02:03:08.460</a></span> | <span class="t">tree it's terrible. It's 77.8 versus, oh hang on, 77.3. That's not right. I take that back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7404" target="_blank">02:03:24.020</a></span> | <span class="t">I'm sure it was less good than this model, but now I can't find that data. Anyway, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7412" target="_blank">02:03:32.540</a></span> | <span class="t">reason I wanted to mention this is that Eugenio is about to release a new model which combines</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7423" target="_blank">02:03:43.040</a></span> | <span class="t">these approaches with skip connections. It's called LinkNet. So keep an eye on the forum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7428" target="_blank">02:03:48.460</a></span> | <span class="t">because I'll be looking into that shortly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7440" target="_blank">02:04:00.940</a></span> | <span class="t">Let's answer them on the forum. I actually wanted to talk about this briefly. A lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7450" target="_blank">02:04:10.380</a></span> | <span class="t">you have come up to me and been like, "We're finishing! What do we do now?" The answer is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7460" target="_blank">02:04:20.300</a></span> | <span class="t">we have now created a community of all these people who have spent well over 100 hours</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7466" target="_blank">02:04:26.020</a></span> | <span class="t">working on deep learning for many, many months and have built their own boxes and written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7472" target="_blank">02:04:32.380</a></span> | <span class="t">blog posts and done all kinds of stuff, set up social impact talks, written articles in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7484" target="_blank">02:04:44.460</a></span> | <span class="t">Forbes. Okay, this community is happening. It doesn't make any sense in my opinion for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7491" target="_blank">02:04:51.540</a></span> | <span class="t">Rachel and I to now be saying here's what happens next. So just like Elena has decided,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7499" target="_blank">02:04:59.220</a></span> | <span class="t">"Okay, I want a book club." So she talked to Mindy and we now have a book club and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7503" target="_blank">02:05:03.660</a></span> | <span class="t">couple of months time, you can all come to the book club.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7508" target="_blank">02:05:08.260</a></span> | <span class="t">So what's next? The forums will continue forever. We all know each other. Let's do good shit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7520" target="_blank">02:05:20.980</a></span> | <span class="t">Most importantly, write code. Please write code. Build apps. Take your work projects and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7533" target="_blank">02:05:33.300</a></span> | <span class="t">try doing them with deep learning. Build libraries to make things easier. Maybe go back to stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7539" target="_blank">02:05:39.780</a></span> | <span class="t">from part 1 of the course and look back and think, "Why didn't we do it this other way?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7544" target="_blank">02:05:44.340</a></span> | <span class="t">I can make this simpler." Write papers. I showed you that amazing result of the new style transfer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7553" target="_blank">02:05:53.060</a></span> | <span class="t">approach from Vincent last week. Hopefully that might take you into a paper. Write blog</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7558" target="_blank">02:05:58.700</a></span> | <span class="t">posts. In a few weeks' time, all the MOOC guys are going to be coming through and doing part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7566" target="_blank">02:06:06.740</a></span> | <span class="t">2 of the course. So help them out on the forum. Teaching is the best way to learn yourself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7573" target="_blank">02:06:13.820</a></span> | <span class="t">I really want to hear the success stories. People don't believe that what you've done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7582" target="_blank">02:06:22.260</a></span> | <span class="t">is possible. I know that because as recently as yesterday, there was the highest ranked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7591" target="_blank">02:06:31.140</a></span> | <span class="t">Hacker News comment on a story about deep learning was about how it's pointless trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7595" target="_blank">02:06:35.860</a></span> | <span class="t">to do deep learning unless you have years of mathematical background and you know C++</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7601" target="_blank">02:06:41.020</a></span> | <span class="t">and you're an expert in machine learning techniques across the board and otherwise there's no</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7605" target="_blank">02:06:45.620</a></span> | <span class="t">way that you're going to be able to do anything useful in the real world project. That today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7610" target="_blank">02:06:50.620</a></span> | <span class="t">is what everybody believes. We now know that's not true. Rachel and I are going to start</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7620" target="_blank">02:07:00.420</a></span> | <span class="t">up a podcast where we're going to try to both help deep learning learners. But one of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7628" target="_blank">02:07:08.820</a></span> | <span class="t">key things you want to do is tell your stories. So if you've done something interesting at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7634" target="_blank">02:07:14.660</a></span> | <span class="t">work or you've got an interesting new result or you're just in the middle of a project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7639" target="_blank">02:07:19.460</a></span> | <span class="t">and it's kind of fun, please tell us. Either on the forum or private message or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7646" target="_blank">02:07:26.100</a></span> | <span class="t">Please tell us because we really want to share your story. And if it's not a story yet, tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7653" target="_blank">02:07:33.460</a></span> | <span class="t">us enough that we can help you and that the community can help you. Get together, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7663" target="_blank">02:07:43.620</a></span> | <span class="t">book club, if you're watching this on the MOOC, organize other people in your geography</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7669" target="_blank">02:07:49.260</a></span> | <span class="t">to get together and meet up or your workplace. In this group here I know we've got people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7675" target="_blank">02:07:55.900</a></span> | <span class="t">from Apple and Uber and Airbnb who started doing this in kind of lunchtime MOOC chats</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7683" target="_blank">02:08:03.660</a></span> | <span class="t">and now they're here at this course. Yes, Rachel?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7686" target="_blank">02:08:06.740</a></span> | <span class="t">I also wanted to recommend it would be great to start meetups to help lead other people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7692" target="_blank">02:08:12.220</a></span> | <span class="t">through, say, part one of the course, kind of assist them going through it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7697" target="_blank">02:08:17.940</a></span> | <span class="t">So Rachel and I really just want to spend the next 6 to 12 months focused on supporting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7710" target="_blank">02:08:30.140</a></span> | <span class="t">your projects. So I'm very interested in working on this lung cancer stuff, but I'm also interested</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7721" target="_blank">02:08:41.220</a></span> | <span class="t">in every project that you guys are working on. I want to help with that. I also want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7726" target="_blank">02:08:46.220</a></span> | <span class="t">to help people who want to teach this. So Yannette is going to go from being a student</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7732" target="_blank">02:08:52.900</a></span> | <span class="t">to a teacher hopefully soon. We'll be teaching USF students about deep learning and hopefully</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7737" target="_blank">02:08:57.880</a></span> | <span class="t">the next batch of people about deep learning. Anybody who's interested in teaching, let</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7742" target="_blank">02:09:02.340</a></span> | <span class="t">us know. This is the best high leverage activity is to teach the teachers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7750" target="_blank">02:09:10.820</a></span> | <span class="t">So yeah, I don't know where this is going to end up, but my hope is really that basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7757" target="_blank">02:09:17.300</a></span> | <span class="t">I would say the experiment has worked. You guys are all here. You're reading papers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7763" target="_blank">02:09:23.300</a></span> | <span class="t">you're writing code, you're understanding the most cutting edge research level deep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7769" target="_blank">02:09:29.040</a></span> | <span class="t">learning that exists today. We've gone beyond some of the cutting edge research in many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7774" target="_blank">02:09:34.780</a></span> | <span class="t">situations. Some of you have gone beyond the cutting edge research. So yeah, let's build</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7782" target="_blank">02:09:42.180</a></span> | <span class="t">from here as a community and anything that Rachel and I can do to help, please tell us,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7788" target="_blank">02:09:48.700</a></span> | <span class="t">because we just want you to be successful and the community to be successful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7795" target="_blank">02:09:55.260</a></span> | <span class="t">So will you be still active in the forums? Very active. My job is to make you guys successful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7805" target="_blank">02:10:05.660</a></span> | <span class="t">So thank you all so much for coming, and congratulations to all of you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7808" target="_blank">02:10:08.060</a></span> | <span class="t">(audience applauds)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=1-NYPQw5THU&t=7811" target="_blank">02:10:11.060</a></span> | <span class="t">(audience applauding)</span></div></div></body></html>