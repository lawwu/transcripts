<html><head><title>Q* - Clues to the Puzzle?</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Q* - Clues to the Puzzle?</h2><a href="https://www.youtube.com/watch?v=ARf0WyFau0A"><img src="https://i.ytimg.com/vi/ARf0WyFau0A/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./ARf0WyFau0A.html">Whisper Transcript</a> | <a href="./transcript_ARf0WyFau0A.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">As you might expect, I have been researching nonstop about this apparent powerful AI discovery</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=6" target="_blank">00:00:06.200</a></span> | <span class="t">that insiders at OpenAI said could threaten humanity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=10" target="_blank">00:00:10.000</a></span> | <span class="t">I've spoken to every insider I know and done a ton of research and I am not claiming to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=15" target="_blank">00:00:15.500</a></span> | <span class="t">have solved the puzzle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=16" target="_blank">00:00:16.800</a></span> | <span class="t">But I can provide some genuine clues that I think will be at least part of the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=22" target="_blank">00:00:22.600</a></span> | <span class="t">Normally, I like to be a lot firmer than that, but this is the best I can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=26" target="_blank">00:00:26.880</a></span> | <span class="t">Now the first thing to note of course is that OpenAI have now denied that Sam Altman's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=31" target="_blank">00:00:31.800</a></span> | <span class="t">Alsthur was precipitated by this safety letter to the board.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=35" target="_blank">00:00:35.440</a></span> | <span class="t">As my previous three videos have shown, there was certainly a lot else going on, so it might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=41" target="_blank">00:00:41.240</a></span> | <span class="t">well not just be about this AI breakthrough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=44" target="_blank">00:00:44.000</a></span> | <span class="t">Second, I just want to quickly debunk this clip that's doing the rounds where people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=48" target="_blank">00:00:48.440</a></span> | <span class="t">are claiming that Sam Altman called this new creation a creature, not just a tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=52" target="_blank">00:00:52.960</a></span> | <span class="t">Actually, if you watch to the end, he's very much saying he's glad that people now think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=56" target="_blank">00:00:56.960</a></span> | <span class="t">of it as part of the toolbox.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=58" target="_blank">00:00:58.960</a></span> | <span class="t">So despite this frantic headline, I am not trying to overhype things, but I genuinely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=63" target="_blank">00:01:03.400</a></span> | <span class="t">think I figured a couple of things out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=65" target="_blank">00:01:05.600</a></span> | <span class="t">Let's get to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=66" target="_blank">00:01:06.600</a></span> | <span class="t">Later in the article, these insiders, these researchers flagged up work by an AI scientist</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=71" target="_blank">00:01:11.760</a></span> | <span class="t">team, the existence of which multiple sources confirmed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=74" target="_blank">00:01:14.800</a></span> | <span class="t">This AI scientist team was formed by combining earlier co-gen and math gen teams at OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=81" target="_blank">00:01:21.720</a></span> | <span class="t">Their work on exploring how to optimize existing AI models to improve their reasoning was flagged</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=86" target="_blank">00:01:26.560</a></span> | <span class="t">in the letter to the board.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=88" target="_blank">00:01:28.120</a></span> | <span class="t">Now there is very little public information about either the co-gen or math gen teams,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=93" target="_blank">00:01:33.520</a></span> | <span class="t">but I dredged up this old tweet from Sam Altman.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=96" target="_blank">00:01:36.040</a></span> | <span class="t">Yes, by the way, I am now on Twitter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=98" target="_blank">00:01:38.040</a></span> | <span class="t">I finally succumbed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=99" target="_blank">00:01:39.040</a></span> | <span class="t">Anyway, Sam Altman said, "Really exciting process supervision result from our math gen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=103" target="_blank">00:01:43.640</a></span> | <span class="t">team," and he linked to a critical paper that I covered back in the spring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=108" target="_blank">00:01:48.440</a></span> | <span class="t">It's called Let's Verify Step-by-Step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=110" target="_blank">00:01:50.680</a></span> | <span class="t">And that paper is the crux of the video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=112" target="_blank">00:01:52.960</a></span> | <span class="t">That's what I think the former math gen, now AI scientist team were working on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=118" target="_blank">00:01:58.100</a></span> | <span class="t">And take this tweet in September by Noam Brown at OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=121" target="_blank">00:02:01.720</a></span> | <span class="t">"My teammates and I at OpenAI are hiring ML engineers for research on LLM multi-step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=127" target="_blank">00:02:07.280</a></span> | <span class="t">reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=128" target="_blank">00:02:08.280</a></span> | <span class="t">We recently hit a state of the art 78% on the math benchmark," which I'll get to in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=132" target="_blank">00:02:12.120</a></span> | <span class="t">a second, "Our new plans are even more ambitious.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=135" target="_blank">00:02:15.520</a></span> | <span class="t">I'm only just getting started, but you might already be glimpsing why I think this paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=140" target="_blank">00:02:20.640</a></span> | <span class="t">is the crux of the new development."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=142" target="_blank">00:02:22.840</a></span> | <span class="t">Here's another reason.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=143" target="_blank">00:02:23.840</a></span> | <span class="t">What's much more ambitious than 78%?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=146" target="_blank">00:02:26.400</a></span> | <span class="t">Acing such tests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=147" target="_blank">00:02:27.880</a></span> | <span class="t">That's apparently what they've achieved behind the scenes, at least according to Reuters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=151" target="_blank">00:02:31.600</a></span> | <span class="t">And what's another bit of evidence?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=153" target="_blank">00:02:33.160</a></span> | <span class="t">Well, we have this exclusive report from the information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=156" target="_blank">00:02:36.280</a></span> | <span class="t">Again, similar frantic headline, "OpenAI made an AI breakthrough before Altman firing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=161" target="_blank">00:02:41.200</a></span> | <span class="t">stoking excitement and concern."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=162" target="_blank">00:02:42.880</a></span> | <span class="t">But who led that breakthrough?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=164" target="_blank">00:02:44.240</a></span> | <span class="t">The technical breakthrough, they say, was spearheaded by OpenAI chief scientist, Ilya</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=169" target="_blank">00:02:49.040</a></span> | <span class="t">Sutskov.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=170" target="_blank">00:02:50.040</a></span> | <span class="t">And the most recent paper on Archive listing Ilya Sutskov as an author is the "Let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=174" target="_blank">00:02:54.840</a></span> | <span class="t">Verify Step-by-Step" paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=176" target="_blank">00:02:56.480</a></span> | <span class="t">Now at this point, I know you want me to get to what it means, but with so many theories</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=180" target="_blank">00:03:00.720</a></span> | <span class="t">floating out there, I want to give you yet more evidence that at least one of the breakthroughs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=186" target="_blank">00:03:06.400</a></span> | <span class="t">links to "Let's Verify Step-by-Step."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=189" target="_blank">00:03:09.000</a></span> | <span class="t">And I think I might even know what the other breakthrough is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=191" target="_blank">00:03:11.780</a></span> | <span class="t">The same article in the information talks about Sutskov working on ways to allow language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=196" target="_blank">00:03:16.560</a></span> | <span class="t">models to solve tasks that involve reasoning, like math or science problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=201" target="_blank">00:03:21.100</a></span> | <span class="t">It talks about how he had this secret program called GPT-0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=205" target="_blank">00:03:25.240</a></span> | <span class="t">And here's where it gets interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=206" target="_blank">00:03:26.400</a></span> | <span class="t">The team hypothesized that giving language models more time and computing power to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=211" target="_blank">00:03:31.480</a></span> | <span class="t">responses to questions could allow them to develop new academic breakthroughs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=215" target="_blank">00:03:35.960</a></span> | <span class="t">And Lucas Kaiser, who we will definitely be seeing more of in this video, indeed he appears</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=220" target="_blank">00:03:40.640</a></span> | <span class="t">in the thumbnail, apparently held a key role in the GPT-0 project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=225" target="_blank">00:03:45.600</a></span> | <span class="t">And look at this, among the techniques the team experimented with was an ML concept known</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=229" target="_blank">00:03:49.800</a></span> | <span class="t">as "test-time computation" that's apparently meant to boost language models' problem-solving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=234" target="_blank">00:03:54.880</a></span> | <span class="t">abilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=235" target="_blank">00:03:55.880</a></span> | <span class="t">And we have a hundred speculations online about what this might mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=239" target="_blank">00:03:59.440</a></span> | <span class="t">But I knew that name rang a bell and I dug up this 2021 paper which cites test-time compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=246" target="_blank">00:04:06.620</a></span> | <span class="t">And what is this random paper that Philip is talking about?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=249" target="_blank">00:04:09.400</a></span> | <span class="t">Well, it's from OpenAI and look at some of the co-authors, Carl Cobb of the MathGen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=254" target="_blank">00:04:14.680</a></span> | <span class="t">team and Lucas Kaiser cited in the information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=258" target="_blank">00:04:18.600</a></span> | <span class="t">This was actually one of the precursor papers to Let's Verify Step-by-Step, which I will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=263" target="_blank">00:04:23.640</a></span> | <span class="t">explain in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=264" target="_blank">00:04:24.920</a></span> | <span class="t">It introduced the now famous in ML circles GSM 8K dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=270" target="_blank">00:04:30.600</a></span> | <span class="t">That's 8,000 grade school math problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=273" target="_blank">00:04:33.120</a></span> | <span class="t">More importantly though, it trialed this method of at test time, generating many candidates'</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=278" target="_blank">00:04:38.320</a></span> | <span class="t">solutions and selecting the one ranked highest by a verifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=283" target="_blank">00:04:43.120</a></span> | <span class="t">And I'm going to massively oversimplify at this point and just say that a verifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=287" target="_blank">00:04:47.480</a></span> | <span class="t">is a separate model trained only at this point in this paper to spot good solutions, solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=293" target="_blank">00:04:53.480</a></span> | <span class="t">that get the answer correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=295" target="_blank">00:04:55.120</a></span> | <span class="t">And what the paper proposed was getting the base LLM to generate hundreds of solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=300" target="_blank">00:05:00.400</a></span> | <span class="t">and then getting this separate verifier to spot the ones that were likely the most correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=305" target="_blank">00:05:05.400</a></span> | <span class="t">And in a nutshell, the authors noticed that if they invested more computing power in generating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=309" target="_blank">00:05:09.840</a></span> | <span class="t">more solutions and taking a majority vote among the top verifier ranked solutions, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=316" target="_blank">00:05:16.280</a></span> | <span class="t">had a massive effect on performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=319" target="_blank">00:05:19.280</a></span> | <span class="t">And that's what it means by test time compute, investing your computing power while you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=324" target="_blank">00:05:24.000</a></span> | <span class="t">taking the test, not during training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=326" target="_blank">00:05:26.340</a></span> | <span class="t">So the model stays the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=327" target="_blank">00:05:27.480</a></span> | <span class="t">You're not further training it or fine tuning it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=329" target="_blank">00:05:29.680</a></span> | <span class="t">You're investing that computing power during test time, again, to generate potential solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=335" target="_blank">00:05:35.200</a></span> | <span class="t">and take majority votes amongst them, self-consistency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=338" target="_blank">00:05:38.120</a></span> | <span class="t">They found that using a verifier in this way was better than fine tuning because verification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=343" target="_blank">00:05:43.440</a></span> | <span class="t">was a simpler task than generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=346" target="_blank">00:05:46.560</a></span> | <span class="t">It's easier to check your answers than generate good ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=349" target="_blank">00:05:49.680</a></span> | <span class="t">So investing time in checking answers was more fruitful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=352" target="_blank">00:05:52.880</a></span> | <span class="t">How much more fruitful?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=353" target="_blank">00:05:53.880</a></span> | <span class="t">Well, the use of verifiers results in approximately the same performance boost as a 30 times model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=359" target="_blank">00:05:59.540</a></span> | <span class="t">size increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=360" target="_blank">00:06:00.660</a></span> | <span class="t">And then it gets prophetic when it says, and verifiers scale significantly better with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=365" target="_blank">00:06:05.360</a></span> | <span class="t">increased data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=366" target="_blank">00:06:06.640</a></span> | <span class="t">And to hammer the point home, they said a 6 billion parameter model with verification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=370" target="_blank">00:06:10.920</a></span> | <span class="t">slightly outperforms a fine tuned 175 billion parameter model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=376" target="_blank">00:06:16.360</a></span> | <span class="t">Again, thereby offering a boost approximately equivalent to a 30X model size increase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=381" target="_blank">00:06:21.520</a></span> | <span class="t">And that team, by the way, with Lucas Kaiser was probably drawing on work by a single dude</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=387" target="_blank">00:06:27.520</a></span> | <span class="t">done six months earlier in April, 2021.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=390" target="_blank">00:06:30.920</a></span> | <span class="t">While studying the board game Hex, the researcher, Andy Jones found an interesting result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=395" target="_blank">00:06:35.480</a></span> | <span class="t">He said, along with our main result, we further show that the test time and train time compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=400" target="_blank">00:06:40.120</a></span> | <span class="t">available to an agent can be traded off as we've seen while maintaining performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=404" target="_blank">00:06:44.600</a></span> | <span class="t">I read that paper in full and it was cited by no other than Noam Brown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=409" target="_blank">00:06:49.560</a></span> | <span class="t">He's the guy we saw earlier who joined OpenAI in July of this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=413" target="_blank">00:06:53.720</a></span> | <span class="t">He said he's investigating how to make reasoning methods truly general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=417" target="_blank">00:06:57.720</a></span> | <span class="t">If successful, we may one day see LLMs that are a thousand times better than GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=423" target="_blank">00:07:03.680</a></span> | <span class="t">And later in that thread, he cites that same paper from Andy Jones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=427" target="_blank">00:07:07.360</a></span> | <span class="t">And he concludes with this, all those prior methods are specific to the games they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=431" target="_blank">00:07:11.000</a></span> | <span class="t">talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=432" target="_blank">00:07:12.000</a></span> | <span class="t">But if we can discover a general version, the benefits could be huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=435" target="_blank">00:07:15.160</a></span> | <span class="t">Yes, inference may be a thousand X slower and more costly, but what inference cost would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=440" target="_blank">00:07:20.200</a></span> | <span class="t">we pay for a new cancer drug or for a proof of the famous mathematical Riemann hypothesis?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=445" target="_blank">00:07:25.720</a></span> | <span class="t">And we'll come back to mathematical hypotheses in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=448" target="_blank">00:07:28.480</a></span> | <span class="t">Anyway, improved capabilities are always risky, he went on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=451" target="_blank">00:07:31.440</a></span> | <span class="t">But if this research succeeds, it could be valuable for safety research as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=455" target="_blank">00:07:35.880</a></span> | <span class="t">Notice that point about capabilities being risky.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=458" target="_blank">00:07:38.560</a></span> | <span class="t">It does seem to me to be linking together into that Reuters story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=462" target="_blank">00:07:42.200</a></span> | <span class="t">Imagine he says being able to spend $1 million on inference to see what a more capable future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=467" target="_blank">00:07:47.080</a></span> | <span class="t">model might look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=468" target="_blank">00:07:48.400</a></span> | <span class="t">It would give us a warning that we would otherwise lack.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=471" target="_blank">00:07:51.840</a></span> | <span class="t">Is that what OpenAI did when they recently pushed back the veil of ignorance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=476" target="_blank">00:07:56.520</a></span> | <span class="t">Did they just spend a million dollars on inference and see what a more capable future model might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=482" target="_blank">00:08:02.000</a></span> | <span class="t">look like?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=483" target="_blank">00:08:03.000</a></span> | <span class="t">So that's test time compute, but what about let's verify step by step?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=486" target="_blank">00:08:06.520</a></span> | <span class="t">Well going back to that original 2021 verifier paper, they said this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=490" target="_blank">00:08:10.480</a></span> | <span class="t">The problem that they noticed with their approach back in 2021 was that their models were rewarding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=495" target="_blank">00:08:15.400</a></span> | <span class="t">correct solutions, but sometimes there would be false positives.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=499" target="_blank">00:08:19.240</a></span> | <span class="t">Getting to the correct final answer using flawed reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=502" target="_blank">00:08:22.520</a></span> | <span class="t">They knew this was a problem and so they worked on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=505" target="_blank">00:08:25.140</a></span> | <span class="t">And then in May of this year, they came out with let's verify step by step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=510" target="_blank">00:08:30.040</a></span> | <span class="t">In this paper, by getting a verifier or reward model to focus on the process, the P, instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=516" target="_blank">00:08:36.160</a></span> | <span class="t">of the outcome, the O, results were far more dramatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=520" target="_blank">00:08:40.160</a></span> | <span class="t">Next, notice how the graph is continuing to rise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=523" target="_blank">00:08:43.700</a></span> | <span class="t">If they just had more, let's say test time compute, this could continue rising higher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=529" target="_blank">00:08:49.640</a></span> | <span class="t">And I actually speculated on that back on June the 1st.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=533" target="_blank">00:08:53.240</a></span> | <span class="t">That difference of about 10% is more than half of the difference between GPT-3 and GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=539" target="_blank">00:08:59.500</a></span> | <span class="t">And also, is it me or is that line continuing to grow?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=542" target="_blank">00:09:02.980</a></span> | <span class="t">Suggesting that when more compute is available, the difference could be even more stark.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=547" target="_blank">00:09:07.840</a></span> | <span class="t">Imagine a future where GPT-4 or 5 can sample, say, a trillion 10 to the 12 solutions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=554" target="_blank">00:09:14.100</a></span> | <span class="t">So you're beginning to see my hypothesis emerging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=556" target="_blank">00:09:16.420</a></span> | <span class="t">A new and improved let's verify step by step called Q*, drawing upon enhanced inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=562" target="_blank">00:09:22.300</a></span> | <span class="t">time compute to push the graph toward 100%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=566" target="_blank">00:09:26.080</a></span> | <span class="t">If you want more details on that process reward model, check out the video I did back then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=570" target="_blank">00:09:30.580</a></span> | <span class="t">called Double the Performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=572" target="_blank">00:09:32.500</a></span> | <span class="t">But the very short version is that they trained a reward model to notice the individual steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=578" target="_blank">00:09:38.100</a></span> | <span class="t">in a reasoning sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=580" target="_blank">00:09:40.100</a></span> | <span class="t">That reward model then got very good at spotting erroneous steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=584" target="_blank">00:09:44.380</a></span> | <span class="t">Furthermore, when that model concluded that there were no erroneous steps, as we've seen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=588" target="_blank">00:09:48.520</a></span> | <span class="t">from the graphs, that was highly indicative of a correct solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=592" target="_blank">00:09:52.940</a></span> | <span class="t">Notice also that sometimes it could pick out such a correct solution when the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=597" target="_blank">00:09:57.580</a></span> | <span class="t">generator, GPT-4, only outputted that correct solution one time in a thousand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=603" target="_blank">00:10:03.260</a></span> | <span class="t">Furthermore, the method somewhat generalized out of distribution, going beyond mathematics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=608" target="_blank">00:10:08.320</a></span> | <span class="t">to boost performance in chemistry, physics, and other subjects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=612" target="_blank">00:10:12.780</a></span> | <span class="t">So was it potentially the million dollar inference run that spooked the researchers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=617" target="_blank">00:10:17.980</a></span> | <span class="t">Or was it the potential to make radical breakthroughs in science?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=622" target="_blank">00:10:22.060</a></span> | <span class="t">And a further bit of supporting evidence for this theory comes again from the information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=626" target="_blank">00:10:26.980</a></span> | <span class="t">It gives us a rough timeline in the months following the breakthrough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=630" target="_blank">00:10:30.740</a></span> | <span class="t">Sutskve had reservations about the technology and in July he formed the super alignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=636" target="_blank">00:10:36.300</a></span> | <span class="t">team.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=637" target="_blank">00:10:37.300</a></span> | <span class="t">So the original breakthrough, whatever it was, had to have come way before July.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=641" target="_blank">00:10:41.080</a></span> | <span class="t">That would fit much more with it being associated with let's verify step by step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=645" target="_blank">00:10:45.540</a></span> | <span class="t">Or again, maybe that combination of process reward modeling and inference time compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=650" target="_blank">00:10:50.500</a></span> | <span class="t">And in a genuinely fascinating recent conference call, this is what Ilya Sutskve said in reply</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=655" target="_blank">00:10:55.420</a></span> | <span class="t">to a question asking about if models could produce mathematical conjectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=660" target="_blank">00:11:00.380</a></span> | <span class="t">How original or creative are the latest large language models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=667" target="_blank">00:11:07.480</a></span> | <span class="t">Of course we know that for instance AlphaGo did some pretty creative moves when it won</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=675" target="_blank">00:11:15.920</a></span> | <span class="t">its match in South Korea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=678" target="_blank">00:11:18.880</a></span> | <span class="t">So that's possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=680" target="_blank">00:11:20.800</a></span> | <span class="t">But to be very concrete, do you think the existing models or some, you know, the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=688" target="_blank">00:11:28.920</a></span> | <span class="t">GPT-4, say GPT-5 or so, would be able to state a new non-trivial mathematical conjecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=699" target="_blank">00:11:39.800</a></span> | <span class="t">I'm not saying proving it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=701" target="_blank">00:11:41.880</a></span> | <span class="t">I'm saying stating it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=704" target="_blank">00:11:44.120</a></span> | <span class="t">Who thinks it's possible within the next five years?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=709" target="_blank">00:11:49.280</a></span> | <span class="t">Are you sure that the current model cannot do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=714" target="_blank">00:11:54.840</a></span> | <span class="t">I'm not sure, absolutely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=717" target="_blank">00:11:57.480</a></span> | <span class="t">Do you know whether it can?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=719" target="_blank">00:11:59.720</a></span> | <span class="t">I mean, let me give you an example of something creative that GPT-4 can already do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=726" target="_blank">00:12:06.840</a></span> | <span class="t">Obviously we would have all loved to hear Ilya Sutskve's full answer to that question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=731" target="_blank">00:12:11.120</a></span> | <span class="t">but we never got a chance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=732" target="_blank">00:12:12.520</a></span> | <span class="t">But here's where we get to arguably the strongest bit of evidence that I've got.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=736" target="_blank">00:12:16.820</a></span> | <span class="t">Remember that name again, Lucas Kaiser cited in the exclusive information report.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=741" target="_blank">00:12:21.640</a></span> | <span class="t">If you remember, he had that key role on the GPT-0 project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=745" target="_blank">00:12:25.720</a></span> | <span class="t">He was a co-author in both of the papers that I've brought up, and he was focused on test</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=750" target="_blank">00:12:30.160</a></span> | <span class="t">time computation to boost language models, problem solving abilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=753" target="_blank">00:12:33.560</a></span> | <span class="t">Well, as the article says, even more significantly, he was one of the co-authors of the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=758" target="_blank">00:12:38.680</a></span> | <span class="t">attention is all you need transformers paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=761" target="_blank">00:12:41.200</a></span> | <span class="t">And so presumably with that much pedigree, it would take quite a lot to get him excited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=766" target="_blank">00:12:46.640</a></span> | <span class="t">So I'm now going to show you some extracts from two YouTube videos, both of which have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=770" target="_blank">00:12:50.980</a></span> | <span class="t">hardly any views, despite him being mentioned in these exclusives about Q*.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=776" target="_blank">00:12:56.140</a></span> | <span class="t">And Lucas Kaiser will describe in these videos how significant he thinks a variation of let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=781" target="_blank">00:13:01.800</a></span> | <span class="t">think step by step could be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=783" target="_blank">00:13:03.920</a></span> | <span class="t">First he'll give you some further background on the breakthrough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=786" target="_blank">00:13:06.900</a></span> | <span class="t">It needs to do this thinking inside its layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=789" target="_blank">00:13:09.860</a></span> | <span class="t">That may not be enough time and space to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=792" target="_blank">00:13:12.560</a></span> | <span class="t">Like tell me what you're thinking and only then give the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=797" target="_blank">00:13:17.320</a></span> | <span class="t">And if you do that, there is a recent paper that says the model can basically do any computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=802" target="_blank">00:13:22.560</a></span> | <span class="t">It can even execute programs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=804" target="_blank">00:13:24.900</a></span> | <span class="t">It's Turing complete.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=806" target="_blank">00:13:26.380</a></span> | <span class="t">And does this really help?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=808" target="_blank">00:13:28.880</a></span> | <span class="t">So on mathematics, you can tell the model, hey, do this thinking, but do it like number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=815" target="_blank">00:13:35.560</a></span> | <span class="t">each step, like one, two, three, four, five, six, seven, as you see here, and be very precise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=821" target="_blank">00:13:41.360</a></span> | <span class="t">about each step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=822" target="_blank">00:13:42.760</a></span> | <span class="t">And then you can try to verify each of these steps of thinking separately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=826" target="_blank">00:13:46.040</a></span> | <span class="t">You can even ask the model, well, was step three correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=828" target="_blank">00:13:48.480</a></span> | <span class="t">Was step four correct?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=830" target="_blank">00:13:50.340</a></span> | <span class="t">And when you do that, like this MATH dataset, which is a little bit tougher math problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=835" target="_blank">00:13:55.340</a></span> | <span class="t">than like the pure arithmetic, it was especially made to show like what the models cannot do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=842" target="_blank">00:14:02.240</a></span> | <span class="t">If you add this thinking, you can get to like almost 80% just by allowing the model to think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=849" target="_blank">00:14:09.880</a></span> | <span class="t">And at least on mathematical questions, this gives like insane gains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=855" target="_blank">00:14:15.240</a></span> | <span class="t">And insane is the fair word to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=859" target="_blank">00:14:19.080</a></span> | <span class="t">Like if you-- a transformer has a limitation in running time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=864" target="_blank">00:14:24.400</a></span> | <span class="t">It runs in n squared time for input of n, and that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=870" target="_blank">00:14:30.320</a></span> | <span class="t">When you allow it to produce chains of thought, it's as computationally powerful as anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=876" target="_blank">00:14:36.880</a></span> | <span class="t">you can imagine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=877" target="_blank">00:14:37.880</a></span> | <span class="t">But, okay, so these two ingredients give you something that generalizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=882" target="_blank">00:14:42.000</a></span> | <span class="t">Could we make them even more powerful?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=885" target="_blank">00:14:45.080</a></span> | <span class="t">And this is called chain of thought mostly, and chain of hindsight, and programs of thought</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=890" target="_blank">00:14:50.440</a></span> | <span class="t">and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=891" target="_blank">00:14:51.440</a></span> | <span class="t">But I think this has turned out to be the method that makes transformers more powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=897" target="_blank">00:14:57.080</a></span> | <span class="t">And it's not just mathematics where you can build this thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=902" target="_blank">00:15:02.080</a></span> | <span class="t">He even goes as far as describing it as a major focus for deep learning in 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=908" target="_blank">00:15:08.480</a></span> | <span class="t">If you think what is coming in the next years, I think there'll be a lot of focus on doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=916" target="_blank">00:15:16.240</a></span> | <span class="t">this thinking thing with deep learning, with language models, probably with chains of thought,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=922" target="_blank">00:15:22.600</a></span> | <span class="t">but also these chains of thought currently, they're just prompted, but maybe you need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=927" target="_blank">00:15:27.040</a></span> | <span class="t">to do some fine tuning, some learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=930" target="_blank">00:15:30.800</a></span> | <span class="t">There'll be a lot of work there, but this is a very hard problem to solve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=935" target="_blank">00:15:35.200</a></span> | <span class="t">And we'll start with much simpler exercises and probably move forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=940" target="_blank">00:15:40.600</a></span> | <span class="t">But I think this is something that the coming years will see a lot of work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=948" target="_blank">00:15:48.640</a></span> | <span class="t">And in a sign of the significance he attributes to this method, he said it could even be revolutionary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=954" target="_blank">00:15:54.240</a></span> | <span class="t">for multimodality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=955" target="_blank">00:15:55.960</a></span> | <span class="t">I think you also need these chains of thought that, like you need to give the model the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=961" target="_blank">00:16:01.000</a></span> | <span class="t">ability to think longer than it has layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=967" target="_blank">00:16:07.240</a></span> | <span class="t">But it can be combined with multimodality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=969" target="_blank">00:16:09.000</a></span> | <span class="t">So in the future, the models will have this knowledge of the world and this generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=973" target="_blank">00:16:13.880</a></span> | <span class="t">which we call chain of thought and text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=976" target="_blank">00:16:16.520</a></span> | <span class="t">But multimodality, this means just it's a chain of frames of what's going to happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=981" target="_blank">00:16:21.240</a></span> | <span class="t">to the world, which is basically how we sometimes think, you know, what will, if I go, what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=987" target="_blank">00:16:27.760</a></span> | <span class="t">will happen to me?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=989" target="_blank">00:16:29.240</a></span> | <span class="t">And I think that will indeed be, so it will be multimodality and this ability to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=995" target="_blank">00:16:35.400</a></span> | <span class="t">sequences of things before you give an answer that will resemble much more what we call</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1003" target="_blank">00:16:43.720</a></span> | <span class="t">reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1004" target="_blank">00:16:44.720</a></span> | <span class="t">He then described how this method will help models to generalize from much less data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1010" target="_blank">00:16:50.360</a></span> | <span class="t">Layers are not the end, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1013" target="_blank">00:16:53.160</a></span> | <span class="t">You can do chains of thought to extend them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1015" target="_blank">00:16:55.720</a></span> | <span class="t">You can do GNNs, you can do recurrence in depth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1020" target="_blank">00:17:00.600</a></span> | <span class="t">How do you see the next two years of deep learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1024" target="_blank">00:17:04.440</a></span> | <span class="t">Yeah, I think there'll be as interesting as any previous two years or even more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1031" target="_blank">00:17:11.080</a></span> | <span class="t">I think there'll be a lot on the chain of thought, but very generally speaking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1036" target="_blank">00:17:16.720</a></span> | <span class="t">So also on the agents, building libraries of knowledge, possibly multimodal where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1042" target="_blank">00:17:22.880</a></span> | <span class="t">chain of thought is basically a simulation of the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1048" target="_blank">00:17:28.240</a></span> | <span class="t">So I think that will be like one big topic and I think this will make the models generalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1052" target="_blank">00:17:32.000</a></span> | <span class="t">much better from less data too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1054" target="_blank">00:17:34.360</a></span> | <span class="t">And that might remind you something, going back to Reuters and the information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1058" target="_blank">00:17:38.920</a></span> | <span class="t">Sarscopha's breakthrough allowed OpenAI to overcome limitations on obtaining enough high</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1063" target="_blank">00:17:43.820</a></span> | <span class="t">quality data to train new models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1066" target="_blank">00:17:46.400</a></span> | <span class="t">According to the insider with knowledge, a major obstacle for developing next generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1070" target="_blank">00:17:50.680</a></span> | <span class="t">models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1071" target="_blank">00:17:51.680</a></span> | <span class="t">So according to my theory, this breakthrough is less about generating trillions and trillions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1075" target="_blank">00:17:55.920</a></span> | <span class="t">of tokens worth of synthetic data, but more about using the data you've got much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1080" target="_blank">00:18:00.880</a></span> | <span class="t">efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1081" target="_blank">00:18:01.880</a></span> | <span class="t">But now, alas, we must get to the bits that my theory can't explain, namely the name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1087" target="_blank">00:18:07.120</a></span> | <span class="t">The information sites two top researchers at OpenAI building on top of Sarscopha's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1091" target="_blank">00:18:11.960</a></span> | <span class="t">method a model called Q*.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1094" target="_blank">00:18:14.520</a></span> | <span class="t">Now I've tested every link I could possibly find to the name Q* with my theory about let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1101" target="_blank">00:18:21.040</a></span> | <span class="t">verify step by step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1102" target="_blank">00:18:22.360</a></span> | <span class="t">And while I do have some ideas, honestly, it's still an open question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1106" target="_blank">00:18:26.720</a></span> | <span class="t">And of course, I like everyone has to admit that there's a chance that I'm entirely</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1111" target="_blank">00:18:31.000</a></span> | <span class="t">wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1112" target="_blank">00:18:32.000</a></span> | <span class="t">When I put my idea to a senior ML researcher at a top AGI lab, he thought it had real legs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1117" target="_blank">00:18:37.800</a></span> | <span class="t">It was a genuine possibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1119" target="_blank">00:18:39.400</a></span> | <span class="t">And he said one link to the name Q* could be in a generic sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1123" target="_blank">00:18:43.440</a></span> | <span class="t">Without getting too technical, Q* refers to the optimal Q function or optimal policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1128" target="_blank">00:18:48.800</a></span> | <span class="t">Another possibility is that the Q references Q learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1131" target="_blank">00:18:51.960</a></span> | <span class="t">Generically, that's a reinforcement learning technique where an agent learns to make optimal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1135" target="_blank">00:18:55.880</a></span> | <span class="t">decisions by exploring its environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1138" target="_blank">00:18:58.200</a></span> | <span class="t">An agent chooses actions, see how they go, and then updates their policy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1142" target="_blank">00:19:02.520</a></span> | <span class="t">Basically trial and error trading off exploration of new steps, new actions versus exploitation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1147" target="_blank">00:19:07.760</a></span> | <span class="t">of actions you know have some good reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1150" target="_blank">00:19:10.660</a></span> | <span class="t">And here's where the analogy gets a little bit tenuous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1153" target="_blank">00:19:13.600</a></span> | <span class="t">Picking the reasoning steps in let's verify step by step could be like choosing an action.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1158" target="_blank">00:19:18.520</a></span> | <span class="t">After all, in the original paper, using test time compute in this way was described as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1162" target="_blank">00:19:22.920</a></span> | <span class="t">a kind of search.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1164" target="_blank">00:19:24.560</a></span> | <span class="t">And in let's verify, they hinted at a step forward involving reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1169" target="_blank">00:19:29.080</a></span> | <span class="t">They said we do not attempt to improve the generator, the model coming up with solutions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1173" target="_blank">00:19:33.960</a></span> | <span class="t">with reinforcement learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1175" target="_blank">00:19:35.280</a></span> | <span class="t">We do not discuss any supervision the generator would receive from the reward model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1180" target="_blank">00:19:40.360</a></span> | <span class="t">If trained with RL.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1181" target="_blank">00:19:41.840</a></span> | <span class="t">And here's the key sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1182" target="_blank">00:19:42.960</a></span> | <span class="t">Although fine tuning the generator with reinforcement learning is a natural next step, it is intentionally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1189" target="_blank">00:19:49.160</a></span> | <span class="t">not the focus of this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1190" target="_blank">00:19:50.880</a></span> | <span class="t">Is that the follow up work that they did?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1192" target="_blank">00:19:52.840</a></span> | <span class="t">I mean, you can kind of think of Q learning for process supervision as minimizing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1198" target="_blank">00:19:58.280</a></span> | <span class="t">cumulative probability of failure, which is the equivalent of maximizing the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1203" target="_blank">00:20:03.400</a></span> | <span class="t">of success.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1204" target="_blank">00:20:04.400</a></span> | <span class="t">And after all, maximizing a sum of rewards over multiple steps is exactly what Q learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1209" target="_blank">00:20:09.480</a></span> | <span class="t">aims to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1210" target="_blank">00:20:10.480</a></span> | <span class="t">If any of you, though, have better guesses for the analogy, and I'm sure you do, do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1214" target="_blank">00:20:14.000</a></span> | <span class="t">let me know in the comments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1215" target="_blank">00:20:15.560</a></span> | <span class="t">But what about the star?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1216" target="_blank">00:20:16.560</a></span> | <span class="t">Well, again, here I am truly speculating.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1219" target="_blank">00:20:19.400</a></span> | <span class="t">Unlike the earlier parts of the video in which I am much more confident, this is much more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1224" target="_blank">00:20:24.400</a></span> | <span class="t">speculative and tenuous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1226" target="_blank">00:20:26.000</a></span> | <span class="t">Peter Liu of Google DeepMind had this idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1228" target="_blank">00:20:28.920</a></span> | <span class="t">Remember the leak talked about acing math tests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1231" target="_blank">00:20:31.480</a></span> | <span class="t">He said, "Sounds like OpenAI got some good numbers on GSM 8K."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1236" target="_blank">00:20:36.200</a></span> | <span class="t">Remember that's the set of questions made for that original Verifier paper back in 2021.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1240" target="_blank">00:20:40.640</a></span> | <span class="t">He said he's speculating, but there's a star in this paper, a technique that fine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1245" target="_blank">00:20:45.880</a></span> | <span class="t">tunes a model to its own better outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1248" target="_blank">00:20:48.520</a></span> | <span class="t">In a nutshell, it involves fine tuning a model on the outputs it generated that happened</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1253" target="_blank">00:20:53.480</a></span> | <span class="t">to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1254" target="_blank">00:20:54.480</a></span> | <span class="t">Keep going until you generate rationales that get the correct answer and then fine tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1259" target="_blank">00:20:59.320</a></span> | <span class="t">on all of those rationales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1261" target="_blank">00:21:01.100</a></span> | <span class="t">And they say that we show that star significantly improves performance on multiple datasets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1266" target="_blank">00:21:06.000</a></span> | <span class="t">compared to a model fine tuned to directly predict final answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1269" target="_blank">00:21:09.800</a></span> | <span class="t">Does that remind you of Let's Verify?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1271" target="_blank">00:21:11.720</a></span> | <span class="t">And performs comparably to fine tuning a 30X larger state-of-the-art language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1277" target="_blank">00:21:17.360</a></span> | <span class="t">He went on that GSM 8K and the math benchmark featured in Let's Verify are great testbeds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1282" target="_blank">00:21:22.960</a></span> | <span class="t">for self-improvement because model outputs can be evaluated for correctness more or less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1287" target="_blank">00:21:27.600</a></span> | <span class="t">automatically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1288" target="_blank">00:21:28.680</a></span> | <span class="t">This brings us on to another strand in what all of this actually means for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1292" target="_blank">00:21:32.680</a></span> | <span class="t">He said for more open-ended generation, humans often provide the feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1296" target="_blank">00:21:36.760</a></span> | <span class="t">However, as LLMs have gotten more capable, an interesting emerging ability is that they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1301" target="_blank">00:21:41.080</a></span> | <span class="t">getting better at evaluation for other things, not just math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1304" target="_blank">00:21:44.680</a></span> | <span class="t">At some point, if self-evaluation or self-critique works reliably, you get general self-improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1310" target="_blank">00:21:50.240</a></span> | <span class="t">beyond math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1311" target="_blank">00:21:51.240</a></span> | <span class="t">So this is a further possibility for what might have freaked out those researchers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1316" target="_blank">00:21:56.360</a></span> | <span class="t">Generalizing beyond math though is hard, as Andrei Karpathy pointed out this week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1321" target="_blank">00:22:01.480</a></span> | <span class="t">I think a lot of people are broadly inspired by what happened with AlphaGo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1326" target="_blank">00:22:06.440</a></span> | <span class="t">In AlphaGo, this was a Go playing program developed by DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1330" target="_blank">00:22:10.880</a></span> | <span class="t">AlphaGo actually had two major stages, the first release of it did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1334" target="_blank">00:22:14.680</a></span> | <span class="t">In the first stage, you learn by imitating human expert players.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1337" target="_blank">00:22:17.680</a></span> | <span class="t">So you take lots of games that were played by humans, you kind of like just filter to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1342" target="_blank">00:22:22.440</a></span> | <span class="t">the games played by really good humans, and you learn by imitation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1346" target="_blank">00:22:26.320</a></span> | <span class="t">You're getting the neural network to just imitate really good players.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1349" target="_blank">00:22:29.280</a></span> | <span class="t">This works and this gives you a pretty good Go playing program, but it can't surpass human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1355" target="_blank">00:22:35.160</a></span> | <span class="t">It's only as good as the best human that gives you the training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1359" target="_blank">00:22:39.140</a></span> | <span class="t">So DeepMind figured out a way to actually surpass humans, and the way this was done</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1362" target="_blank">00:22:42.760</a></span> | <span class="t">is by self-improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1364" target="_blank">00:22:44.860</a></span> | <span class="t">Now in the case of Go, this is a simple closed sandbox environment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1369" target="_blank">00:22:49.920</a></span> | <span class="t">You have a game and you can play lots of games in the sandbox and you can have a very simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1374" target="_blank">00:22:54.440</a></span> | <span class="t">reward function, which is just winning the game.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1377" target="_blank">00:22:57.720</a></span> | <span class="t">So you can query this reward function that tells you if whatever you've done was good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1381" target="_blank">00:23:01.640</a></span> | <span class="t">or bad, did you win, yes or no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1383" target="_blank">00:23:03.400</a></span> | <span class="t">This is something that is available, very cheap to evaluate, and automatic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1387" target="_blank">00:23:07.840</a></span> | <span class="t">And so because of that, you can play millions and millions of games and kind of perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1391" target="_blank">00:23:11.340</a></span> | <span class="t">the system just based on the probability of winning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1394" target="_blank">00:23:14.660</a></span> | <span class="t">So there's no need to imitate, you can go beyond human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1397" target="_blank">00:23:17.500</a></span> | <span class="t">And that's in fact what the system ended up doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1399" target="_blank">00:23:19.960</a></span> | <span class="t">So here on the right, we have the Elo rating and AlphaGo took 40 days in this case to overcome</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1405" target="_blank">00:23:25.820</a></span> | <span class="t">some of the best human players by self-improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1409" target="_blank">00:23:29.920</a></span> | <span class="t">So I think a lot of people are kind of interested in what is the equivalent of this step number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1413" target="_blank">00:23:33.520</a></span> | <span class="t">two for large language models, because today we're only doing step one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1417" target="_blank">00:23:37.480</a></span> | <span class="t">We are imitating humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1419" target="_blank">00:23:39.000</a></span> | <span class="t">As I mentioned, there are human labelers writing out these answers and we're imitating their</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1422" target="_blank">00:23:42.800</a></span> | <span class="t">responses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1423" target="_blank">00:23:43.800</a></span> | <span class="t">And we can have very good human labelers, but fundamentally, it would be hard to go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1427" target="_blank">00:23:47.200</a></span> | <span class="t">above sort of human response accuracy if we only train on the humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1432" target="_blank">00:23:52.720</a></span> | <span class="t">So that's the big question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1433" target="_blank">00:23:53.720</a></span> | <span class="t">What is the step two equivalent in the domain of open language modeling?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1438" target="_blank">00:23:58.920</a></span> | <span class="t">And the main challenge here is that there's a lack of reward criterion in the general</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1442" target="_blank">00:24:02.520</a></span> | <span class="t">case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1443" target="_blank">00:24:03.520</a></span> | <span class="t">So because we are in a space of language, everything is a lot more open and there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1446" target="_blank">00:24:06.520</a></span> | <span class="t">all these different types of tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1448" target="_blank">00:24:08.420</a></span> | <span class="t">And fundamentally, there's no like simple reward function you can access that just tells</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1451" target="_blank">00:24:11.880</a></span> | <span class="t">you if whatever you did, whatever you sampled was good or bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1455" target="_blank">00:24:15.480</a></span> | <span class="t">There's no easy to evaluate fast criterion or reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1458" target="_blank">00:24:18.760</a></span> | <span class="t">If models can get good at generalization using reinforcement learning with any of these techniques,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1463" target="_blank">00:24:23.680</a></span> | <span class="t">Ilya Satsykova has a slight warning that he put out earlier this year.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1468" target="_blank">00:24:28.000</a></span> | <span class="t">He compared the creative results we might get to outbursts from Bing Sydney.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1473" target="_blank">00:24:33.920</a></span> | <span class="t">Reinforcement learning has a much more significant challenge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1478" target="_blank">00:24:38.800</a></span> | <span class="t">It is creative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1481" target="_blank">00:24:41.720</a></span> | <span class="t">Reinforcement learning is actually creative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1486" target="_blank">00:24:46.880</a></span> | <span class="t">Every single stunning example of creativity in AI comes from a reinforcement learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1491" target="_blank">00:24:51.680</a></span> | <span class="t">system.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1492" target="_blank">00:24:52.680</a></span> | <span class="t">For example, AlphaZero has invented a whole new way of playing a game that humans have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1499" target="_blank">00:24:59.200</a></span> | <span class="t">perfected for thousands of years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1501" target="_blank">00:25:01.080</a></span> | <span class="t">It is reinforcement learning that can come up creative solutions to problems, solutions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1505" target="_blank">00:25:05.880</a></span> | <span class="t">which we might not be able to understand at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1508" target="_blank">00:25:08.740</a></span> | <span class="t">And so what happens if you do reinforcement learning on long or even medium time horizon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1514" target="_blank">00:25:14.760</a></span> | <span class="t">when your AI is interacting with the real world, trying to achieve some kind of a beneficial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1521" target="_blank">00:25:21.240</a></span> | <span class="t">outcome, let's say, as judged by us, but while being very, very, very creative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1527" target="_blank">00:25:27.000</a></span> | <span class="t">This does not mean that this problem is unsolvable, but it means that it is a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1531" target="_blank">00:25:31.200</a></span> | <span class="t">And it means that some of the more naive approaches will suffer from some unexpected creativity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1536" target="_blank">00:25:36.120</a></span> | <span class="t">that will make the antics of Sydney seem very modest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1539" target="_blank">00:25:39.960</a></span> | <span class="t">So that's as far as I've gotten.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1541" target="_blank">00:25:41.360</a></span> | <span class="t">I might be completely wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1543" target="_blank">00:25:43.200</a></span> | <span class="t">Let me know what you think in the comments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1545" target="_blank">00:25:45.200</a></span> | <span class="t">I think the development is likely a big step forward for narrow domains like mathematics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1550" target="_blank">00:25:50.520</a></span> | <span class="t">but is in no way yet a solution for AGI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1554" target="_blank">00:25:54.040</a></span> | <span class="t">The world is still a bit too complex for this to work yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1558" target="_blank">00:25:58.040</a></span> | <span class="t">Anyway, time to move on to something more positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1560" target="_blank">00:26:00.480</a></span> | <span class="t">After all, even Sam Altman can now get along with Adam D'Angelo.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1564" target="_blank">00:26:04.280</a></span> | <span class="t">So anything is possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1565" target="_blank">00:26:05.280</a></span> | <span class="t">I'm going to end with some positive and amazing news about music generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1569" target="_blank">00:26:09.640</a></span> | <span class="t">But first I want to introduce you to the AI Explained bot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1573" target="_blank">00:26:13.400</a></span> | <span class="t">If you're feeling bored, you may even want to discuss the contents of this video and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1577" target="_blank">00:26:17.280</a></span> | <span class="t">Q* with the AI Explained bot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1579" target="_blank">00:26:19.760</a></span> | <span class="t">It has access to the transcripts of my videos, including this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1583" target="_blank">00:26:23.320</a></span> | <span class="t">I'm proud to announce that they're sponsoring this video and their playground is honestly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1587" target="_blank">00:26:27.400</a></span> | <span class="t">amazing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1588" target="_blank">00:26:28.400</a></span> | <span class="t">In fact, I reached out to them about sponsorship.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1590" target="_blank">00:26:30.880</a></span> | <span class="t">It's that good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1591" target="_blank">00:26:31.880</a></span> | <span class="t">Their playground is super easy to use, even if you're not from a coding background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1595" target="_blank">00:26:35.680</a></span> | <span class="t">And as they know, their speech to text model, Conforma 2, is state of the art and it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1600" target="_blank">00:26:40.240</a></span> | <span class="t">particularly good on alphanumerics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1602" target="_blank">00:26:42.820</a></span> | <span class="t">A perfect example of that is how it can transcribe a GPT-4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1607" target="_blank">00:26:47.040</a></span> | <span class="t">That is something in my transcripts that so many models struggled with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1610" target="_blank">00:26:50.480</a></span> | <span class="t">Anyway, I have honestly thought that their playground is amazing for anyone to use for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1614" target="_blank">00:26:54.520</a></span> | <span class="t">months now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1615" target="_blank">00:26:55.760</a></span> | <span class="t">And yes, it's literally just clicking to upload your audio file and then pressing transcribe.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1620" target="_blank">00:27:00.960</a></span> | <span class="t">Anyway, thanks to Assembly AI, you can now play about with the AI Explained bot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1625" target="_blank">00:27:05.920</a></span> | <span class="t">After such a heavy video, I think it's only appropriate to end with a bit of music, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1630" target="_blank">00:27:10.880</a></span> | <span class="t">not any music, music generated by Google DeepMind.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1634" target="_blank">00:27:14.200</a></span> | <span class="t">Their new Lyra model can convert your hums into an orchestra.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=ARf0WyFau0A&t=1659" target="_blank">00:27:39.440</a></span> | <span class="t">As always, thank you so much for watching and whatever happens, have a wonderful day.</span></div></div></body></html>