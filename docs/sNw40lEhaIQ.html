<html><head><title>Stanford XCS224U: NLU I Contextual Word Representations, Part 4: GPT I Spring 2023</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford XCS224U: NLU I Contextual Word Representations, Part 4: GPT I Spring 2023</h2><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ"><img src="https://i.ytimg.com/vi/sNw40lEhaIQ/sddefault.jpg?sqp=-oaymwEmCIAFEOAD8quKqQMa8AEB-AHUBoAC4AOKAgwIABABGCggZSg0MA8=&rs=AOn4CLCdRKrDUFUo7pJHe0160vRoRofkFw" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./sNw40lEhaIQ.html">Whisper Transcript</a> | <a href="./transcript_sNw40lEhaIQ.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome back everyone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=6" target="_blank">00:00:06.080</a></span> | <span class="t">This is part four in our series on contextual representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=9" target="_blank">00:00:09.440</a></span> | <span class="t">We have come to what might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=11" target="_blank">00:00:11.200</a></span> | <span class="t">the most famous transformer-based architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=13" target="_blank">00:00:13.880</a></span> | <span class="t">and that is GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=15" target="_blank">00:00:15.960</a></span> | <span class="t">I thought I would start this discussion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=18" target="_blank">00:00:18.280</a></span> | <span class="t">in a technical place,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=19" target="_blank">00:00:19.720</a></span> | <span class="t">that is the autoregressive loss function that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=21" target="_blank">00:00:21.960</a></span> | <span class="t">usually used for neural language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=24" target="_blank">00:00:24.680</a></span> | <span class="t">Then I'm going to try to support</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=26" target="_blank">00:00:26.180</a></span> | <span class="t">that technical piece with a bunch of illustrations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=28" target="_blank">00:00:28.960</a></span> | <span class="t">Here is the full loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=30" target="_blank">00:00:30.880</a></span> | <span class="t">There are a lot of mathematical details here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=32" target="_blank">00:00:32.780</a></span> | <span class="t">I think the smart thing to do is zoom in on the numerator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=36" target="_blank">00:00:36.520</a></span> | <span class="t">What we're saying here is that at position T,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=39" target="_blank">00:00:39.760</a></span> | <span class="t">we're going to look up the token representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=42" target="_blank">00:00:42.400</a></span> | <span class="t">for T in our embedding layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=44" target="_blank">00:00:44.360</a></span> | <span class="t">and do a dot product of that vector representation with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=48" target="_blank">00:00:48.120</a></span> | <span class="t">the hidden representation that we have built up in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=51" target="_blank">00:00:51.100</a></span> | <span class="t">our model to the time-step preceding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=53" target="_blank">00:00:53.560</a></span> | <span class="t">the one that is in focus here, time-step T.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=56" target="_blank">00:00:56.420</a></span> | <span class="t">The rest of this is softmax normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=59" target="_blank">00:00:59.540</a></span> | <span class="t">So we do that same calculation for every item in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=63" target="_blank">00:01:03.620</a></span> | <span class="t">Then we take the log of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=65" target="_blank">00:01:05.140</a></span> | <span class="t">and we are looking for parameters that will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=66" target="_blank">00:01:06.980</a></span> | <span class="t">maximize this log probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=69" target="_blank">00:01:09.340</a></span> | <span class="t">But again, the thing to keep an eye on is that the scoring is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=72" target="_blank">00:01:12.420</a></span> | <span class="t">based in the dot product of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=74" target="_blank">00:01:14.560</a></span> | <span class="t">the embedding representation for the token we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=77" target="_blank">00:01:17.620</a></span> | <span class="t">predict and the hidden representation that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=80" target="_blank">00:01:20.420</a></span> | <span class="t">built up until the time before that time-step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=84" target="_blank">00:01:24.540</a></span> | <span class="t">Here's that same thing by way of an illustration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=88" target="_blank">00:01:28.460</a></span> | <span class="t">Our sequence is the rock rules,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=90" target="_blank">00:01:30.380</a></span> | <span class="t">and for this language modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=91" target="_blank">00:01:31.900</a></span> | <span class="t">I think it's good to keep track of start and end tokens here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=95" target="_blank">00:01:35.780</a></span> | <span class="t">We begin modeling with that start token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=98" target="_blank">00:01:38.260</a></span> | <span class="t">which is given, that is at position T1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=101" target="_blank">00:01:41.100</a></span> | <span class="t">and we look up its embedding representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=103" target="_blank">00:01:43.660</a></span> | <span class="t">and then we form some hidden representation H1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=107" target="_blank">00:01:47.660</a></span> | <span class="t">Then to predict the,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=109" target="_blank">00:01:49.780</a></span> | <span class="t">which is now at time-step 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=112" target="_blank">00:01:52.380</a></span> | <span class="t">we are going to use H1 here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=116" target="_blank">00:01:56.180</a></span> | <span class="t">dot producted with the embedding representation for the.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=119" target="_blank">00:01:59.820</a></span> | <span class="t">Remember, that's the numerator in that scoring function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=123" target="_blank">00:02:03.100</a></span> | <span class="t">At time-step 2, we now copy over the, and we continue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=126" target="_blank">00:02:06.860</a></span> | <span class="t">we get its embedding representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=129" target="_blank">00:02:09.060</a></span> | <span class="t">Here I'm depicting like a recurrent neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=132" target="_blank">00:02:12.180</a></span> | <span class="t">So we're traveling left to right just to keep things simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=134" target="_blank">00:02:14.860</a></span> | <span class="t">I'll talk about how GPT handles this in a bit later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=138" target="_blank">00:02:18.460</a></span> | <span class="t">So we traveled left to right,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=139" target="_blank">00:02:19.820</a></span> | <span class="t">and we got a second hidden representation H2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=142" target="_blank">00:02:22.260</a></span> | <span class="t">and again, the scoring is the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=143" target="_blank">00:02:23.980</a></span> | <span class="t">To predict the rock at position 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=146" target="_blank">00:02:26.140</a></span> | <span class="t">we use H2 and the embedding for rock.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=149" target="_blank">00:02:29.260</a></span> | <span class="t">Then the sequence modeling continues in exactly that same way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=152" target="_blank">00:02:32.940</a></span> | <span class="t">until we predict our end token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=155" target="_blank">00:02:35.820</a></span> | <span class="t">But for each one of these time-steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=157" target="_blank">00:02:37.980</a></span> | <span class="t">remember what we're doing is getting a score for the rock that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=161" target="_blank">00:02:41.500</a></span> | <span class="t">proportional to the dot product of the embedding for that token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=166" target="_blank">00:02:46.420</a></span> | <span class="t">and the hidden representation just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=168" target="_blank">00:02:48.780</a></span> | <span class="t">prior to that point that we're making the prediction at.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=171" target="_blank">00:02:51.580</a></span> | <span class="t">Then we exponentiate that for the sake of doing that softmax scoring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=176" target="_blank">00:02:56.020</a></span> | <span class="t">When we move to GPT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=178" target="_blank">00:02:58.180</a></span> | <span class="t">we're essentially just doing this in the context of a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=181" target="_blank">00:03:01.340</a></span> | <span class="t">So to depict that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=183" target="_blank">00:03:03.060</a></span> | <span class="t">I've got at the bottom here a traditional absolute encoding scheme for positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=187" target="_blank">00:03:07.900</a></span> | <span class="t">We look up all those static vector representations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=191" target="_blank">00:03:11.860</a></span> | <span class="t">and we get our first contextual representations in green as usual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=196" target="_blank">00:03:16.700</a></span> | <span class="t">Then for GPT, we might stack lots and lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=199" target="_blank">00:03:19.700</a></span> | <span class="t">transformer blocks on top of each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=202" target="_blank">00:03:22.660</a></span> | <span class="t">Eventually though, we will get some output representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=206" target="_blank">00:03:26.180</a></span> | <span class="t">Those are the ones that I've depicted in green here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=208" target="_blank">00:03:28.740</a></span> | <span class="t">and those will be the basis for language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=211" target="_blank">00:03:31.420</a></span> | <span class="t">We will add on top of those some language modeling specific parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=216" target="_blank">00:03:36.260</a></span> | <span class="t">which could just be the embedding layer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=218" target="_blank">00:03:38.220</a></span> | <span class="t">comes from the word embeddings down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=220" target="_blank">00:03:40.980</a></span> | <span class="t">That will be the basis for predicting the actual sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=225" target="_blank">00:03:45.300</a></span> | <span class="t">We get an error signal to the extent that we're making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=228" target="_blank">00:03:48.060</a></span> | <span class="t">predictions into this space that don't correspond to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=231" target="_blank">00:03:51.820</a></span> | <span class="t">the actual one-hot encoding vectors that correspond to the sequence itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=237" target="_blank">00:03:57.140</a></span> | <span class="t">In essence though, this is just more of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=240" target="_blank">00:04:00.500</a></span> | <span class="t">that conditional language modeling using the transformer architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=244" target="_blank">00:04:04.500</a></span> | <span class="t">Maybe the one thing to keep in mind here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=246" target="_blank">00:04:06.980</a></span> | <span class="t">because of the nature of the attention mechanisms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=249" target="_blank">00:04:09.700</a></span> | <span class="t">we need to do some masking to make sure that we don't in effect look into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=254" target="_blank">00:04:14.620</a></span> | <span class="t">So let's build that up a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=256" target="_blank">00:04:16.780</a></span> | <span class="t">We start with position A. At that first position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=261" target="_blank">00:04:21.300</a></span> | <span class="t">the only attending we can do is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=263" target="_blank">00:04:23.220</a></span> | <span class="t">ourselves because we can't look into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=265" target="_blank">00:04:25.700</a></span> | <span class="t">I haven't depicted that, but we could do self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=268" target="_blank">00:04:28.900</a></span> | <span class="t">When we move to position B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=270" target="_blank">00:04:30.700</a></span> | <span class="t">we now have the opportunity to look back into position A and get that dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=275" target="_blank">00:04:35.900</a></span> | <span class="t">We could self-attend, as I said before,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=278" target="_blank">00:04:38.020</a></span> | <span class="t">although I didn't depict that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=279" target="_blank">00:04:39.700</a></span> | <span class="t">Then finally, when we get to position C,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=281" target="_blank">00:04:41.740</a></span> | <span class="t">we can look back to the previous two positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=284" target="_blank">00:04:44.620</a></span> | <span class="t">The attention mask is going to have this look where we go backwards,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=288" target="_blank">00:04:48.700</a></span> | <span class="t">but not forwards so that we don't end up looking into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=291" target="_blank">00:04:51.820</a></span> | <span class="t">the future into tokens that we have not yet generated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=295" target="_blank">00:04:55.420</a></span> | <span class="t">In a little more detail, again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=298" target="_blank">00:04:58.140</a></span> | <span class="t">I would like to belabor these points because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=300" target="_blank">00:05:00.220</a></span> | <span class="t">there are a lot of technical details hiding here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=302" target="_blank">00:05:02.780</a></span> | <span class="t">What I'm going to depict on this slide is very specifically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=306" target="_blank">00:05:06.540</a></span> | <span class="t">training of a GPT style model with what's called teacher forcing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=311" target="_blank">00:05:11.460</a></span> | <span class="t">This is going to mean that no matter what token we predict at every time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=316" target="_blank">00:05:16.340</a></span> | <span class="t">we will use the actual token at the next time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=320" target="_blank">00:05:20.180</a></span> | <span class="t">Let's be really pedantic about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=322" target="_blank">00:05:22.500</a></span> | <span class="t">At the bottom here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=323" target="_blank">00:05:23.860</a></span> | <span class="t">I have our input sequence as represented as a series of one-hot vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=329" target="_blank">00:05:29.420</a></span> | <span class="t">These are lookup devices that will give us back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=332" target="_blank">00:05:32.660</a></span> | <span class="t">the embedding representations for words from our embedding space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=337" target="_blank">00:05:37.260</a></span> | <span class="t">Here's our embedding space in gray.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=339" target="_blank">00:05:39.460</a></span> | <span class="t">We do those lookups and that gives us a bunch of vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=342" target="_blank">00:05:42.500</a></span> | <span class="t">As a shorthand, I have depicted the names of those vectors,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=346" target="_blank">00:05:46.780</a></span> | <span class="t">B for beginning of sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=348" target="_blank">00:05:48.860</a></span> | <span class="t">the rock rules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=350" target="_blank">00:05:50.500</a></span> | <span class="t">That's the sequence there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=352" target="_blank">00:05:52.300</a></span> | <span class="t">Then we have a whole bunch of those transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=355" target="_blank">00:05:55.900</a></span> | <span class="t">What I've done is summarize them in green here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=358" target="_blank">00:05:58.620</a></span> | <span class="t">Just a reminder, I've got all those arrows showing you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=362" target="_blank">00:06:02.180</a></span> | <span class="t">the attention pattern so that we always look into the past,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=365" target="_blank">00:06:05.340</a></span> | <span class="t">but never into the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=367" target="_blank">00:06:07.580</a></span> | <span class="t">Then on top of that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=369" target="_blank">00:06:09.740</a></span> | <span class="t">we're going to use our embedding parameters again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=372" target="_blank">00:06:12.900</a></span> | <span class="t">These are the same parameters that I've got depicted down here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=377" target="_blank">00:06:17.140</a></span> | <span class="t">Now we are going to compare essentially the scores that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=381" target="_blank">00:06:21.580</a></span> | <span class="t">predict in each one of those spaces with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=384" target="_blank">00:06:24.500</a></span> | <span class="t">the one-hot vectors that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=387" target="_blank">00:06:27.100</a></span> | <span class="t">actually correspond to the sequence that we want to predict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=390" target="_blank">00:06:30.060</a></span> | <span class="t">This was the start token and conditional on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=393" target="_blank">00:06:33.220</a></span> | <span class="t">we want to predict the.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=394" target="_blank">00:06:34.860</a></span> | <span class="t">This is the down here and conditional on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=397" target="_blank">00:06:37.460</a></span> | <span class="t">we want to predict rock and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=399" target="_blank">00:06:39.940</a></span> | <span class="t">You do get this offset where the input sequence and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=402" target="_blank">00:06:42.900</a></span> | <span class="t">the output sequence are staggered by one so that we're always</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=406" target="_blank">00:06:46.340</a></span> | <span class="t">conditional on what we've seen predicting the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=411" target="_blank">00:06:51.020</a></span> | <span class="t">Imagine that these are the scores that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=413" target="_blank">00:06:53.860</a></span> | <span class="t">get out of this final layer here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=415" target="_blank">00:06:55.580</a></span> | <span class="t">I've depicted them as integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=417" target="_blank">00:06:57.020</a></span> | <span class="t">but they could be floats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=418" target="_blank">00:06:58.700</a></span> | <span class="t">The idea is that that is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=420" target="_blank">00:07:00.660</a></span> | <span class="t">the comparison that we make to get our error signal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=423" target="_blank">00:07:03.460</a></span> | <span class="t">We can look at the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=425" target="_blank">00:07:05.140</a></span> | <span class="t">this vector here and this vector here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=428" target="_blank">00:07:08.220</a></span> | <span class="t">and use that as a gradient signal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=430" target="_blank">00:07:10.260</a></span> | <span class="t">to update the parameters of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=432" target="_blank">00:07:12.460</a></span> | <span class="t">That's how this actually happens in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=434" target="_blank">00:07:14.860</a></span> | <span class="t">We always think of language modeling as predicting tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=439" target="_blank">00:07:19.260</a></span> | <span class="t">but really and truly it predicts scores over the entire vocabulary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=444" target="_blank">00:07:24.100</a></span> | <span class="t">and then we make a choice about which token was actually predicted by,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=448" target="_blank">00:07:28.220</a></span> | <span class="t">for example, picking the token that had the largest score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=453" target="_blank">00:07:33.780</a></span> | <span class="t">I've mentioned teacher forcing in this context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=457" target="_blank">00:07:37.020</a></span> | <span class="t">This is really important here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=458" target="_blank">00:07:38.620</a></span> | <span class="t">I think we should imagine that at this time step here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=462" target="_blank">00:07:42.140</a></span> | <span class="t">what the model did is give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=464" target="_blank">00:07:44.100</a></span> | <span class="t">the highest score to whatever word was in the final position here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=468" target="_blank">00:07:48.180</a></span> | <span class="t">but the actual token was rules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=470" target="_blank">00:07:50.780</a></span> | <span class="t">This will give us an error signal because we have in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=473" target="_blank">00:07:53.180</a></span> | <span class="t">effect made a mistake that we can learn from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=476" target="_blank">00:07:56.660</a></span> | <span class="t">The teacher forcing aspect of this is that I do not use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=480" target="_blank">00:08:00.220</a></span> | <span class="t">the vector consisting of all zeros and a one down here at this time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=484" target="_blank">00:08:04.420</a></span> | <span class="t">but rather I use the actual token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=487" target="_blank">00:08:07.500</a></span> | <span class="t">If we back off from teacher forcing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=490" target="_blank">00:08:10.260</a></span> | <span class="t">we could go into a mode where at least some of the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=493" target="_blank">00:08:13.260</a></span> | <span class="t">we use the vector consisting of all zeros and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=496" target="_blank">00:08:16.140</a></span> | <span class="t">a one down here as the input at the next time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=499" target="_blank">00:08:19.100</a></span> | <span class="t">That would effectively be using the model's predictions rather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=502" target="_blank">00:08:22.860</a></span> | <span class="t">than the gold sequence as part of training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=505" target="_blank">00:08:25.820</a></span> | <span class="t">and that could introduce some useful diversity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=508" target="_blank">00:08:28.780</a></span> | <span class="t">into the learned representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=510" target="_blank">00:08:30.860</a></span> | <span class="t">But usually, we do something like teacher forcing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=513" target="_blank">00:08:33.780</a></span> | <span class="t">where even though we got an error signal here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=516" target="_blank">00:08:36.100</a></span> | <span class="t">we use the actual thing that we wanted to have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=518" target="_blank">00:08:38.380</a></span> | <span class="t">predicted down at the next time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=521" target="_blank">00:08:41.540</a></span> | <span class="t">That is part of training the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=523" target="_blank">00:08:43.980</a></span> | <span class="t">and then when we move to generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=525" target="_blank">00:08:45.660</a></span> | <span class="t">we do something that's very similar,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=527" target="_blank">00:08:47.620</a></span> | <span class="t">although with some twists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=529" target="_blank">00:08:49.100</a></span> | <span class="t">What I've depicted on the slide here is something like imagining that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=532" target="_blank">00:08:52.420</a></span> | <span class="t">the user has prompted the model with the sequence start token and the.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=538" target="_blank">00:08:58.300</a></span> | <span class="t">The model has predicted rock as the next sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=541" target="_blank">00:09:01.860</a></span> | <span class="t">We copy that over that representation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=545" target="_blank">00:09:05.140</a></span> | <span class="t">we process with the transformer as usual,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=548" target="_blank">00:09:08.260</a></span> | <span class="t">and we get another prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=549" target="_blank">00:09:09.700</a></span> | <span class="t">In this case, it was the sequence rolls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=551" target="_blank">00:09:11.940</a></span> | <span class="t">Now we have generated the rock rolls as our sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=555" target="_blank">00:09:15.940</a></span> | <span class="t">We copy that over into the next time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=559" target="_blank">00:09:19.460</a></span> | <span class="t">and then we get along as the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=562" target="_blank">00:09:22.340</a></span> | <span class="t">Notice that you might have expected that this would be the rock rules,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=566" target="_blank">00:09:26.060</a></span> | <span class="t">and the model ended up predicting something different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=568" target="_blank">00:09:28.540</a></span> | <span class="t">That might be in its nature.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=570" target="_blank">00:09:30.100</a></span> | <span class="t">Maybe that was a mistake, maybe it wasn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=572" target="_blank">00:09:32.340</a></span> | <span class="t">But the point is that in generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=574" target="_blank">00:09:34.500</a></span> | <span class="t">we no longer have the possibility of doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=576" target="_blank">00:09:36.740</a></span> | <span class="t">teacher forcing because we are creating new tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=580" target="_blank">00:09:40.100</a></span> | <span class="t">We have to use the scores that we got up here to infer a next token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=585" target="_blank">00:09:45.580</a></span> | <span class="t">copy that over, condition the model on that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=588" target="_blank">00:09:48.500</a></span> | <span class="t">and have the generation process repeat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=591" target="_blank">00:09:51.300</a></span> | <span class="t">But throughout this entire process, again, a reminder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=595" target="_blank">00:09:55.380</a></span> | <span class="t">the model does not predict tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=597" target="_blank">00:09:57.700</a></span> | <span class="t">The model predicts scores over the vocabulary as depicted here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=602" target="_blank">00:10:02.500</a></span> | <span class="t">and we do some inferencing to figure out which token that ought to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=606" target="_blank">00:10:06.780</a></span> | <span class="t">As we'll discuss later in the course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=608" target="_blank">00:10:08.420</a></span> | <span class="t">there are lots of schemes for doing that sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=611" target="_blank">00:10:11.900</a></span> | <span class="t">You could fix the max scoring one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=613" target="_blank">00:10:13.740</a></span> | <span class="t">but you could also roll out over a whole bunch of time steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=618" target="_blank">00:10:18.260</a></span> | <span class="t">looking at all the different predictions and generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=622" target="_blank">00:10:22.420</a></span> | <span class="t">the sequence that maximizes that overall probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=625" target="_blank">00:10:25.900</a></span> | <span class="t">That would be more like beam search,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=628" target="_blank">00:10:28.060</a></span> | <span class="t">and that's very different from the maximum probability step that I'm depicting here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=633" target="_blank">00:10:33.900</a></span> | <span class="t">That's a nice reminder that in generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=637" target="_blank">00:10:37.260</a></span> | <span class="t">what we're doing is applying a decision rule on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=639" target="_blank">00:10:39.860</a></span> | <span class="t">top of the representations that these models have created.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=643" target="_blank">00:10:43.420</a></span> | <span class="t">It's not so intrinsic to the models themselves,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=646" target="_blank">00:10:46.660</a></span> | <span class="t">that they follow that particular decision rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=649" target="_blank">00:10:49.660</a></span> | <span class="t">That's a complexity of generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=653" target="_blank">00:10:53.100</a></span> | <span class="t">Final step here, when we think about fine-tuning GPT,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=657" target="_blank">00:10:57.620</a></span> | <span class="t">the standard mode is to process a sequence and then use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=661" target="_blank">00:11:01.700</a></span> | <span class="t">the final output state as the basis for some task-specific parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=666" target="_blank">00:11:06.340</a></span> | <span class="t">that you use to fine-tune on whatever supervised learning task you're focused on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=671" target="_blank">00:11:11.220</a></span> | <span class="t">But of course, we're not limited to do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=673" target="_blank">00:11:13.180</a></span> | <span class="t">We could also think about using all of the output states that the model has created,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=677" target="_blank">00:11:17.740</a></span> | <span class="t">maybe by doing some max or mean pooling over them to gather</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=681" target="_blank">00:11:21.940</a></span> | <span class="t">more information from the sequence that is just contained in that final output state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=686" target="_blank">00:11:26.900</a></span> | <span class="t">But for example, in the first GPT paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=689" target="_blank">00:11:29.820</a></span> | <span class="t">their fine-tuning is based entirely,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=691" target="_blank">00:11:31.780</a></span> | <span class="t">I believe, on that final output state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=695" target="_blank">00:11:35.340</a></span> | <span class="t">To round this out, I thought I'd just show you some GPTs that have been released,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=705" target="_blank">00:11:45.020</a></span> | <span class="t">along with some information about how they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=707" target="_blank">00:11:47.180</a></span> | <span class="t">structured to the extent that we know how they're structured.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=710" target="_blank">00:11:50.020</a></span> | <span class="t">The first GPT had 12 layers and a model dimensionality of 768,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=715" target="_blank">00:11:55.980</a></span> | <span class="t">and a feed-forward dimensionality of 3072.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=718" target="_blank">00:11:58.980</a></span> | <span class="t">That's that one point in the model where you can expand out before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=722" target="_blank">00:12:02.300</a></span> | <span class="t">collapsing back to decay inside the feed-forward layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=726" target="_blank">00:12:06.260</a></span> | <span class="t">That gave rise to a model that had 117 million parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=731" target="_blank">00:12:11.220</a></span> | <span class="t">GPT-2 scaled that up considerably to 48 layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=736" target="_blank">00:12:16.100</a></span> | <span class="t">1600 dimensionality for the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=739" target="_blank">00:12:19.100</a></span> | <span class="t">and 1600 for the feed-forward layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=741" target="_blank">00:12:21.780</a></span> | <span class="t">for a total of about 1.5 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=745" target="_blank">00:12:25.860</a></span> | <span class="t">Then GPT-3 had 96 layers and a massive model dimensionality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=752" target="_blank">00:12:32.220</a></span> | <span class="t">over 12,000 for its model dimensionality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=755" target="_blank">00:12:35.580</a></span> | <span class="t">As far as I know, we don't know the dimensionality of that inside feed-forward layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=760" target="_blank">00:12:40.300</a></span> | <span class="t">but it might also be 12,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=763" target="_blank">00:12:43.020</a></span> | <span class="t">That gave rise to a model that had 175 billion parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=768" target="_blank">00:12:48.700</a></span> | <span class="t">By the way, the GPT-3 paper reports</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=771" target="_blank">00:12:51.580</a></span> | <span class="t">on models that are intermediate in those sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=774" target="_blank">00:12:54.860</a></span> | <span class="t">All of those models are from OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=777" target="_blank">00:12:57.260</a></span> | <span class="t">If you want to think about truly open alternatives,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=779" target="_blank">00:12:59.860</a></span> | <span class="t">here is a fast summary of the models that I know about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=783" target="_blank">00:13:03.540</a></span> | <span class="t">This table is probably already hopelessly out of date by the time you are viewing it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=788" target="_blank">00:13:08.540</a></span> | <span class="t">but it does give you a sense for the kinds of things that have happened on the open-source side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=794" target="_blank">00:13:14.220</a></span> | <span class="t">I would say that the hopeful aspect of this is that there are a lot of these models now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=798" target="_blank">00:13:18.460</a></span> | <span class="t">and some of them are quite competitive in terms of their overall size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=802" target="_blank">00:13:22.220</a></span> | <span class="t">For example, the Bloom model there has 176 billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=807" target="_blank">00:13:27.260</a></span> | <span class="t">and it's truly gargantuan in terms of its dimensionalities and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=811" target="_blank">00:13:31.540</a></span> | <span class="t">There are some other smaller ones here that are obviously very performant,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=816" target="_blank">00:13:36.060</a></span> | <span class="t">but very powerful, very interesting artifacts in the GPT mode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=sNw40lEhaIQ&t=820" target="_blank">00:13:40.300</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>