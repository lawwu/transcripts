<html><head><title>Segment Anything - Model explanation with code</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Segment Anything - Model explanation with code</h2><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM"><img src="https://i.ytimg.com/vi_webp/eYhvJR4zFUM/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=80">1:20</a> Image Segmentation<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=208">3:28</a> Segment Anything<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=418">6:58</a> Task<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=500">8:20</a> Model (Overview)<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=591">9:51</a> Image Encoder<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=607">10:7</a> Vision Transformer<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=750">12:30</a> Masked Autoencoder Vision Transformer<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=932">15:32</a> Prompt Encoder<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1275">21:15</a> Positional Encodings<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1492">24:52</a> Mask Decoder<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2143">35:43</a> Intersection Over Union<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2228">37:8</a> Loss Functions<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2350">39:10</a> Data Engine and Dataset<br><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2495">41:35</a> Non Maximal Suppression<br><br><div style="text-align: left;"><a href="./eYhvJR4zFUM.html">Whisper Transcript</a> | <a href="./transcript_eYhvJR4zFUM.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome to my new video about the new model from Meta called Segment Anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=6" target="_blank">00:00:06.240</a></span> | <span class="t">As you have heard from the internet, Segment Anything is a model that allows you to segment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=11" target="_blank">00:00:11.660</a></span> | <span class="t">an image into masks and without caring about what kind of image are we talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=18" target="_blank">00:00:18.500</a></span> | <span class="t">So before, for example, we had segmentation models for medical applications or for pedestrian</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=24" target="_blank">00:00:24.420</a></span> | <span class="t">detection or for some other objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=27" target="_blank">00:00:27.420</a></span> | <span class="t">But Segment Anything can work with any kind of image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=30" target="_blank">00:00:30.840</a></span> | <span class="t">And the second novelty is that it allows you to work with prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=34" target="_blank">00:00:34.540</a></span> | <span class="t">So just like you work in NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=36" target="_blank">00:00:36.420</a></span> | <span class="t">So given a prompt, like a list of points or a bounding box or a text, it can segment the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=42" target="_blank">00:00:42.300</a></span> | <span class="t">image given your input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=44" target="_blank">00:00:44.920</a></span> | <span class="t">And this makes it a powerful foundation model just like BERT, just like GPT for NLP applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=51" target="_blank">00:00:51.240</a></span> | <span class="t">So it means that it can be later fine-tuned by working on the prompt and not only on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=55" target="_blank">00:00:55.880</a></span> | <span class="t">data to apply to specific tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=59" target="_blank">00:00:59.920</a></span> | <span class="t">In this video, we will watch what is the model, how does it work, what is its composition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=65" target="_blank">00:01:05.720</a></span> | <span class="t">the data set it was trained upon, and also I will create a parallel with the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=69" target="_blank">00:01:09.900</a></span> | <span class="t">So I will also show you the code of this model along with an explanation of how it works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=74" target="_blank">00:01:14.840</a></span> | <span class="t">so that you can see things from a higher level to the bottom level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=81" target="_blank">00:01:21.240</a></span> | <span class="t">Let's start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=82" target="_blank">00:01:22.880</a></span> | <span class="t">So first of all, what is image segmentation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=85" target="_blank">00:01:25.420</a></span> | <span class="t">Image segmentation is the process of partitioning a digital image into multiple regions such</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=90" target="_blank">00:01:30.600</a></span> | <span class="t">that pixels that belong to the same region share some characteristics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=95" target="_blank">00:01:35.360</a></span> | <span class="t">For example, if we are given this image, I think it was a painting from Van Gogh, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=101" target="_blank">00:01:41.160</a></span> | <span class="t">if we partition it using segmentation, it will be partitioned into these masks, in which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=106" target="_blank">00:01:46.120</a></span> | <span class="t">for example, we have one mask for the grass, one for this house, one for this tree, one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=110" target="_blank">00:01:50.920</a></span> | <span class="t">for this other tree, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=113" target="_blank">00:01:53.900</a></span> | <span class="t">And before we had segment anything, we had many models, one specifically tuned for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=117" target="_blank">00:01:57.840</a></span> | <span class="t">application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=118" target="_blank">00:01:58.840</a></span> | <span class="t">For example, for medical imaging, we may want to locate, given an image of cells, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=124" target="_blank">00:02:04.000</a></span> | <span class="t">one are the tumor cells and which one are not tumor cells.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=128" target="_blank">00:02:08.520</a></span> | <span class="t">Or in object detection, we may want to know where are the pedestrians in our image in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=133" target="_blank">00:02:13.760</a></span> | <span class="t">self-driving cars, or for example, in satellite images, we want to segment, for example, rivers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=140" target="_blank">00:02:20.880</a></span> | <span class="t">and the mountains and urban areas, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=145" target="_blank">00:02:25.480</a></span> | <span class="t">But it also had many challenges, this task, because, first of all, to create a dataset</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=150" target="_blank">00:02:30.240</a></span> | <span class="t">for image segmentation was very expensive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=152" target="_blank">00:02:32.440</a></span> | <span class="t">I mean, imagine you need an operator who, pixel by pixel, has to define what this pixel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=159" target="_blank">00:02:39.600</a></span> | <span class="t">belongs to, or what this pixel belongs to, or what this pixel belongs to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=163" target="_blank">00:02:43.400</a></span> | <span class="t">So it takes a lot of time to annotate images for image segmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=168" target="_blank">00:02:48.920</a></span> | <span class="t">Just as I said before, the models usually were application-specific, and also the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=173" target="_blank">00:02:53.200</a></span> | <span class="t">models were not promptable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=174" target="_blank">00:02:54.840</a></span> | <span class="t">That is, we could not tell the model, ah, please just select all the masks for cats</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=180" target="_blank">00:03:00.200</a></span> | <span class="t">or for dogs or for trees or for houses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=183" target="_blank">00:03:03.880</a></span> | <span class="t">So we could not build a prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=185" target="_blank">00:03:05.120</a></span> | <span class="t">If the model was trained to detect that kind of mask, it detected it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=190" target="_blank">00:03:10.200</a></span> | <span class="t">Otherwise it didn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=191" target="_blank">00:03:11.200</a></span> | <span class="t">So it was all or nothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=193" target="_blank">00:03:13.680</a></span> | <span class="t">But now we can actually ask the model which kind of object we want to build the mask for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=198" target="_blank">00:03:18.200</a></span> | <span class="t">And we can do that by using points, by using bounding box, by using text also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=205" target="_blank">00:03:25.300</a></span> | <span class="t">And let's have a look on the website from Meta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=208" target="_blank">00:03:28.640</a></span> | <span class="t">So if we go to segmentanything.com, we have this page called demo, in which, okay, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=214" target="_blank">00:03:34.240</a></span> | <span class="t">accept the conditions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=216" target="_blank">00:03:36.960</a></span> | <span class="t">And we can select any image, let's say one of these bears.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=221" target="_blank">00:03:41.640</a></span> | <span class="t">And the model can work with clicks, as I said before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=225" target="_blank">00:03:45.240</a></span> | <span class="t">So if we click here, it tells the model that we want something from here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=230" target="_blank">00:03:50.840</a></span> | <span class="t">But imagine the model selected too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=233" target="_blank">00:03:53.700</a></span> | <span class="t">Maybe the model selected we wanted only the face of the bear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=238" target="_blank">00:03:58.000</a></span> | <span class="t">So we can remove some area by clicking on something that we want to remove.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=242" target="_blank">00:04:02.920</a></span> | <span class="t">So if we click the belly of this bear, it will remove the bottom part of the body.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=248" target="_blank">00:04:08.920</a></span> | <span class="t">The second thing we can do is by using a box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=251" target="_blank">00:04:11.200</a></span> | <span class="t">For example, we may say, okay, select all the animals in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=255" target="_blank">00:04:15.520</a></span> | <span class="t">But now it only selected the box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=257" target="_blank">00:04:17.360</a></span> | <span class="t">Then we can guide the model by adding some points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=261" target="_blank">00:04:21.280</a></span> | <span class="t">For example, this point was not included even if we wanted all the animals in this box.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=265" target="_blank">00:04:25.800</a></span> | <span class="t">So we can tell him to add this animal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=268" target="_blank">00:04:28.740</a></span> | <span class="t">But suppose that the model included the ears, or we also wanted to exclude something from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=277" target="_blank">00:04:37.600</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=278" target="_blank">00:04:38.600</a></span> | <span class="t">For example, we want to exclude, let's say, this paw here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=283" target="_blank">00:04:43.140</a></span> | <span class="t">So we can add another point with remove area and put it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=287" target="_blank">00:04:47.280</a></span> | <span class="t">And hopefully it will remove the paw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=289" target="_blank">00:04:49.580</a></span> | <span class="t">So of course, the model is not perfect, because the prompt is kind of, can be ambiguous in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=295" target="_blank">00:04:55.360</a></span> | <span class="t">some case, even for us humans.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=298" target="_blank">00:04:58.040</a></span> | <span class="t">And so of course, the model is not perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=301" target="_blank">00:05:01.080</a></span> | <span class="t">But it's, I mean, it looks very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=302" target="_blank">00:05:02.800</a></span> | <span class="t">And the second thing to notice is that the model is running in my browser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=306" target="_blank">00:05:06.280</a></span> | <span class="t">There is no back processing on a server.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=309" target="_blank">00:05:09.760</a></span> | <span class="t">It's happening in real time in my browser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=311" target="_blank">00:05:11.800</a></span> | <span class="t">So it's quite fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=313" target="_blank">00:05:13.720</a></span> | <span class="t">And let's go back to our slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=318" target="_blank">00:05:18.320</a></span> | <span class="t">So segment anything introduces three innovations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=321" target="_blank">00:05:21.360</a></span> | <span class="t">The first is the task itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=323" target="_blank">00:05:23.240</a></span> | <span class="t">It's called a promptable segmentation task, which can work with points, with boxes, with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=327" target="_blank">00:05:27.960</a></span> | <span class="t">text, or a combination of the above, for example, a box and a few points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=333" target="_blank">00:05:33.920</a></span> | <span class="t">Which introduces a model that is a fast model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=336" target="_blank">00:05:36.560</a></span> | <span class="t">It's an encoder decoder model that takes around 50 milliseconds in the web browser to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=341" target="_blank">00:05:41.920</a></span> | <span class="t">a mask given a prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=344" target="_blank">00:05:44.320</a></span> | <span class="t">And it's also ambiguity aware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=346" target="_blank">00:05:46.160</a></span> | <span class="t">For example, given a point, that point may correspond to multiple objects.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=350" target="_blank">00:05:50.540</a></span> | <span class="t">For example, if we click here, for example, in this area, it may indicate this vegetable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=357" target="_blank">00:05:57.480</a></span> | <span class="t">or all this vegetable, or only the white part of this vegetable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=362" target="_blank">00:06:02.520</a></span> | <span class="t">And this means that the model cannot know, of course, what is our intent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=366" target="_blank">00:06:06.040</a></span> | <span class="t">So the model will return the three most likely masks, indicating the part, the sub part and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=371" target="_blank">00:06:11.360</a></span> | <span class="t">the whole.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=373" target="_blank">00:06:13.960</a></span> | <span class="t">Then of course, the model was trained on data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=376" target="_blank">00:06:16.920</a></span> | <span class="t">And this data was a lot of was a big data set composed of 1.1 billion masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=382" target="_blank">00:06:22.880</a></span> | <span class="t">And the interesting thing is that these masks were actually generated by the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=387" target="_blank">00:06:27.700</a></span> | <span class="t">So they started the model with a very small data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=391" target="_blank">00:06:31.360</a></span> | <span class="t">Then they use this data model, which was created on a small data set to create an even bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=396" target="_blank">00:06:36.480</a></span> | <span class="t">data set with the help of operators, of course, manual operators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=400" target="_blank">00:06:40.200</a></span> | <span class="t">And then after a while, they ask the model to generate all the masks automatically without</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=406" target="_blank">00:06:46.520</a></span> | <span class="t">any human help, and then train the model on this automatically generated masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=412" target="_blank">00:06:52.760</a></span> | <span class="t">And the result is the one you just saw on the browser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=415" target="_blank">00:06:55.240</a></span> | <span class="t">It's a model that can segment anything with a very high precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=418" target="_blank">00:06:58.280</a></span> | <span class="t">The authors took inspiration from NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=420" target="_blank">00:07:00.840</a></span> | <span class="t">As you remember, in NLP, we have the next token prediction task, which is used by most</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=425" target="_blank">00:07:05.000</a></span> | <span class="t">language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=426" target="_blank">00:07:06.760</a></span> | <span class="t">And so basically, we give a prompt to a model and the model has to come up with to complete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=433" target="_blank">00:07:13.280</a></span> | <span class="t">the sentence with something meaningful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=435" target="_blank">00:07:15.120</a></span> | <span class="t">This is what happens with GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=436" target="_blank">00:07:16.500</a></span> | <span class="t">This is what happens with BERT and all the other language models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=440" target="_blank">00:07:20.240</a></span> | <span class="t">And this is exactly what they wanted to do here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=443" target="_blank">00:07:23.580</a></span> | <span class="t">They wanted to use a prompt to build a foundation model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=447" target="_blank">00:07:27.120</a></span> | <span class="t">So a foundation model is a model that is trained on a lot of data and that can be fine-tuned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=453" target="_blank">00:07:33.000</a></span> | <span class="t">let's say, for a specific task by working on the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=457" target="_blank">00:07:37.160</a></span> | <span class="t">And this prompt was made so that it can handle ambiguity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=462" target="_blank">00:07:42.500</a></span> | <span class="t">For example, if the single click that we make is referring to multiple objects, the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=469" target="_blank">00:07:49.240</a></span> | <span class="t">must return at least one mask that is reasonable for that click.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=474" target="_blank">00:07:54.120</a></span> | <span class="t">So this is the requirements that the author set for their model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=478" target="_blank">00:07:58.260</a></span> | <span class="t">And we saw one ambiguous mask before, but for example, here we can see another case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=482" target="_blank">00:08:02.720</a></span> | <span class="t">For example, if we click on the Z here, this point could refer to the Z itself or to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=490" target="_blank">00:08:10.120</a></span> | <span class="t">entire text here or to the entire wall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=494" target="_blank">00:08:14.840</a></span> | <span class="t">So the model has to return at least one of these three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=498" target="_blank">00:08:18.360</a></span> | <span class="t">And in the best case, of course, all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=501" target="_blank">00:08:21.320</a></span> | <span class="t">Now let's overview the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=504" target="_blank">00:08:24.840</a></span> | <span class="t">What is the model architecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=506" target="_blank">00:08:26.640</a></span> | <span class="t">The model, as we saw before, is an encoder-decoder model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=510" target="_blank">00:08:30.360</a></span> | <span class="t">And it's composed of these parts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=512" target="_blank">00:08:32.120</a></span> | <span class="t">There is an image encoder that creates an embedding, given an image creates an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=518" target="_blank">00:08:38.080</a></span> | <span class="t">Then we have a prompt encoder that can encode the prompts given by the user, which can be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=524" target="_blank">00:08:44.160</a></span> | <span class="t">points, boxes, text, or a combination.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=527" target="_blank">00:08:47.960</a></span> | <span class="t">Then we will see later what is this mask here, but basically it means that if we run the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=532" target="_blank">00:08:52.440</a></span> | <span class="t">model with an initial prompt, for example, a single point, the model will build an initial</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=538" target="_blank">00:08:58.360</a></span> | <span class="t">mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=539" target="_blank">00:08:59.360</a></span> | <span class="t">Then if we want to modify our prompt by adding another point, we can, instead of letting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=544" target="_blank">00:09:04.680</a></span> | <span class="t">the model guess what we want, we can reuse the previous output to guide the model into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=550" target="_blank">00:09:10.160</a></span> | <span class="t">telling the model that, okay, the previous mask was good, but not perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=554" target="_blank">00:09:14.400</a></span> | <span class="t">So I am giving you, again, the previous mask as a hint, as a starting point, plus some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=559" target="_blank">00:09:19.380</a></span> | <span class="t">few points to guide you into telling you what I want you to remove or add to the mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=565" target="_blank">00:09:25.920</a></span> | <span class="t">This is the whole idea of this mask we can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=569" target="_blank">00:09:29.040</a></span> | <span class="t">So it's a mask, is the result of a previous prompting of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=574" target="_blank">00:09:34.920</a></span> | <span class="t">And then the model has a decoder that is given the prompt, the previous mask, and the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=580" target="_blank">00:09:40.160</a></span> | <span class="t">of the image, it has to predict the mask along with the scores of confidence score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=586" target="_blank">00:09:46.880</a></span> | <span class="t">Now let's go and watch what is the image encoder and how does it work?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=592" target="_blank">00:09:52.360</a></span> | <span class="t">In the paper, they say that they use the MAE pre-trained vision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=597" target="_blank">00:09:57.640</a></span> | <span class="t">So MAE means that it's a masked autoencoder and a vision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=602" target="_blank">00:10:02.720</a></span> | <span class="t">So let's review these two terms, what they mean and how they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=605" target="_blank">00:10:05.560</a></span> | <span class="t">Let's first review the vision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=608" target="_blank">00:10:08.440</a></span> | <span class="t">The vision transformer was introduced in a very famous paper, I think a few years ago.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=613" target="_blank">00:10:13.920</a></span> | <span class="t">The paper name is "An image is worth 16 by 16 words" and it's from Google Research, Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=619" target="_blank">00:10:19.560</a></span> | <span class="t">Brain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=621" target="_blank">00:10:21.360</a></span> | <span class="t">Basically what they did is, they take a picture, in this case, this one, they divide it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=626" target="_blank">00:10:26.160</a></span> | <span class="t">patches of 16 by 16, and then they flatten these patches, so create a sequence of these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=632" target="_blank">00:10:32.680</a></span> | <span class="t">patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=633" target="_blank">00:10:33.680</a></span> | <span class="t">For example, this is the first patch, this is the second one, this is the third one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=636" target="_blank">00:10:36.720</a></span> | <span class="t">the fourth, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=638" target="_blank">00:10:38.800</a></span> | <span class="t">And then they created embedding by using a linear projection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=642" target="_blank">00:10:42.480</a></span> | <span class="t">So the embedding that captures somehow the information from each of this patch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=649" target="_blank">00:10:49.120</a></span> | <span class="t">They feed all this sequence of patches and actually the embedding of these patches along</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=655" target="_blank">00:10:55.480</a></span> | <span class="t">with the position encoding to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=659" target="_blank">00:10:59.860</a></span> | <span class="t">But they not only feed the list of patches, but also another token here that is prepended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=668" target="_blank">00:11:08.400</a></span> | <span class="t">to this sequence of patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=670" target="_blank">00:11:10.260</a></span> | <span class="t">And this token is called the class embedding, the class token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=676" target="_blank">00:11:16.040</a></span> | <span class="t">And the idea comes from the BERT paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=679" target="_blank">00:11:19.160</a></span> | <span class="t">Basically if you remember the transformer, when we have a sequence in the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=683" target="_blank">00:11:23.160</a></span> | <span class="t">encoder, the transformer basically allows, with its self-attention mechanism, to relate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=688" target="_blank">00:11:28.360</a></span> | <span class="t">the tokens to each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=690" target="_blank">00:11:30.040</a></span> | <span class="t">So in the output we will have again a sequence of tokens, but the embedding of each token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=697" target="_blank">00:11:37.240</a></span> | <span class="t">will somehow capture the interaction of that token with all the other tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=702" target="_blank">00:11:42.620</a></span> | <span class="t">And this is the idea behind adding this class token we have here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=706" target="_blank">00:11:46.920</a></span> | <span class="t">So we send it to the transformer, and at the output of the transformer, as soon as we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=712" target="_blank">00:11:52.680</a></span> | <span class="t">get another sequence in the output of the transformer encoder, we just take the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=717" target="_blank">00:11:57.740</a></span> | <span class="t">token here, and we ask this token to map this token to a multilayer perceptron that has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=726" target="_blank">00:12:06.480</a></span> | <span class="t">to predict the class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=728" target="_blank">00:12:08.640</a></span> | <span class="t">Why do we use this token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=730" target="_blank">00:12:10.080</a></span> | <span class="t">Because this token, because of the self-attention mechanism, has interacted with all of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=734" target="_blank">00:12:14.480</a></span> | <span class="t">other patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=735" target="_blank">00:12:15.920</a></span> | <span class="t">So this token somehow captures the information of the other patches, and then we force the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=743" target="_blank">00:12:23.040</a></span> | <span class="t">model to convey all this information to this single token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=747" target="_blank">00:12:27.120</a></span> | <span class="t">So this is the idea of the vision transformer and of the class token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=754" target="_blank">00:12:34.180</a></span> | <span class="t">They took the vision transformer and they transformed it into a masked autoencoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=758" target="_blank">00:12:38.080</a></span> | <span class="t">vision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=759" target="_blank">00:12:39.240</a></span> | <span class="t">And this happened in another paper called Masked Autoencoders are Scalable Vision Learners</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=764" target="_blank">00:12:44.160</a></span> | <span class="t">from Facebook, from Meta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=766" target="_blank">00:12:46.920</a></span> | <span class="t">Now in this case they still have an input image, in this case this one, but what they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=773" target="_blank">00:12:53.360</a></span> | <span class="t">did is they split it into patches, but then they masked it out, they deleted some patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=779" target="_blank">00:12:59.120</a></span> | <span class="t">and replaced it with zeros, so they hide some patches here, and if I remember correctly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=786" target="_blank">00:13:06.840</a></span> | <span class="t">it's 75% of the patches are masked out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=790" target="_blank">00:13:10.920</a></span> | <span class="t">Then they only take the visible patches, create a sequence, a linear sequence here, they give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=795" target="_blank">00:13:15.880</a></span> | <span class="t">it to the encoder of a transformer, which still produces as output a sequence as we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=801" target="_blank">00:13:21.960</a></span> | <span class="t">saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=804" target="_blank">00:13:24.640</a></span> | <span class="t">Then what they do, they take this sequence that is the output of the encoder of the transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=811" target="_blank">00:13:31.160</a></span> | <span class="t">they again recreate the original image sequence, so if we knew that the first patch was empty,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=820" target="_blank">00:13:40.160</a></span> | <span class="t">then they put an empty space here, then an empty, then the third one was visible, so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=824" target="_blank">00:13:44.680</a></span> | <span class="t">they used the first embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=826" target="_blank">00:13:46.800</a></span> | <span class="t">Then the fourth, the fifth and the sixth were deleted, so four, five and six were deleted,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=832" target="_blank">00:13:52.440</a></span> | <span class="t">then they take the next one visible and they put it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=835" target="_blank">00:13:55.000</a></span> | <span class="t">So basically to the decoder they give the visible patches and the non-visible patches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=840" target="_blank">00:14:00.920</a></span> | <span class="t">along with the geometric information of the visibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=844" target="_blank">00:14:04.540</a></span> | <span class="t">So they are added in the same sequence in which they were cancelled out in the original</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=847" target="_blank">00:14:07.880</a></span> | <span class="t">image, and then they ask the decoder to predict the original image, only being able to visualize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=854" target="_blank">00:14:14.600</a></span> | <span class="t">the embedding of the visible patches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=856" target="_blank">00:14:16.960</a></span> | <span class="t">So basically the decoder has to come up with a full image, being only able to access 25%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=863" target="_blank">00:14:23.120</a></span> | <span class="t">of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=864" target="_blank">00:14:24.320</a></span> | <span class="t">And what they saw is that the decoder was actually able to rebuild the original image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=869" target="_blank">00:14:29.120</a></span> | <span class="t">maybe not with the perfect quality, but a reasonable quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=873" target="_blank">00:14:33.800</a></span> | <span class="t">And what the authors of the segment editing paper did, they took this part of the masked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=883" target="_blank">00:14:43.200</a></span> | <span class="t">autoencoder of the vision transformer, because they are interested in this, the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=888" target="_blank">00:14:48.680</a></span> | <span class="t">learned by the encoder, so the output of the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=892" target="_blank">00:14:52.160</a></span> | <span class="t">Because if the model is able to predict the original image, given only this embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=897" target="_blank">00:14:57.440</a></span> | <span class="t">of the visible patches, it means that these embeddings capture most of the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=902" target="_blank">00:15:02.320</a></span> | <span class="t">of the image, which can be then reused to rebuild the original image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=906" target="_blank">00:15:06.320</a></span> | <span class="t">So this is what they want, but this is what we want from an encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=909" target="_blank">00:15:09.960</a></span> | <span class="t">We want the encoder to create a representation of something that captures most of its salient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=914" target="_blank">00:15:14.840</a></span> | <span class="t">information without caring about the extra information that is not necessary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=920" target="_blank">00:15:20.160</a></span> | <span class="t">So this allows you to reduce the dimensionality of the original image while preserving the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=925" target="_blank">00:15:25.280</a></span> | <span class="t">information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=926" target="_blank">00:15:26.280</a></span> | <span class="t">And this is why they use the encoder of the masked autoencoder vision transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=933" target="_blank">00:15:33.880</a></span> | <span class="t">Now that we have seen what is the image encoder, which is basically creating an embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=939" target="_blank">00:15:39.600</a></span> | <span class="t">this one here, now we go to the next part, which is the prompt encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=943" target="_blank">00:15:43.240</a></span> | <span class="t">So the job of the prompt encoder is also to encode the prompt, which is the list of points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=948" target="_blank">00:15:48.000</a></span> | <span class="t">chosen by the user, the boxes selected by the user and the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=952" target="_blank">00:15:52.640</a></span> | <span class="t">We will not visualize what is the text encoder, which is basically just the encoder of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=956" target="_blank">00:15:56.960</a></span> | <span class="t">clip model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=958" target="_blank">00:15:58.160</a></span> | <span class="t">So if you are not familiar with the clip model, I suggest you watch my previous video about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=963" target="_blank">00:16:03.640</a></span> | <span class="t">the clip model, and it's quite interesting, actually, it's per se, so it deserves its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=970" target="_blank">00:16:10.280</a></span> | <span class="t">own video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=971" target="_blank">00:16:11.560</a></span> | <span class="t">But basically the idea is the same as with the image encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=974" target="_blank">00:16:14.520</a></span> | <span class="t">So we have a text and we want some representation that captures most of the information about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=979" target="_blank">00:16:19.600</a></span> | <span class="t">this text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=981" target="_blank">00:16:21.000</a></span> | <span class="t">And this is done by the encoder of the clip text encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=986" target="_blank">00:16:26.400</a></span> | <span class="t">Let's have a look at the prompt encoder now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=989" target="_blank">00:16:29.220</a></span> | <span class="t">Now in the prompt in their paper, segment anything, they say that they consider two</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=993" target="_blank">00:16:33.680</a></span> | <span class="t">type of prompts, the sparse prompts, which is the points, boxes and text, and the dense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=998" target="_blank">00:16:38.600</a></span> | <span class="t">prompts, which is the mask we saw before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1001" target="_blank">00:16:41.040</a></span> | <span class="t">For the text encoder, they just use the text encoder from clip, we can see here, while</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1006" target="_blank">00:16:46.160</a></span> | <span class="t">the other two prompts, so the points and the boxes are basically they take the points,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1012" target="_blank">00:16:52.440</a></span> | <span class="t">they create a representation of this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1014" target="_blank">00:16:54.800</a></span> | <span class="t">So an embedding that tells the model what is this point referring to inside of the image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1020" target="_blank">00:17:00.220</a></span> | <span class="t">using the positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1022" target="_blank">00:17:02.980</a></span> | <span class="t">Let's see how does it work on a code level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1026" target="_blank">00:17:06.420</a></span> | <span class="t">Here we can see that basically, they take the sparse prompts, and they are mapped to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1030" target="_blank">00:17:10.660</a></span> | <span class="t">256 dimensional vector embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1034" target="_blank">00:17:14.520</a></span> | <span class="t">So 256 dimensional vector embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1039" target="_blank">00:17:19.620</a></span> | <span class="t">Basically here is how we encode the points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1043" target="_blank">00:17:23.960</a></span> | <span class="t">We have the points, and then we have labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1047" target="_blank">00:17:27.020</a></span> | <span class="t">The points are a sequence of X and Ys, while the labels indicate if the point is additive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1054" target="_blank">00:17:34.700</a></span> | <span class="t">so we want the model to add something to our mask, or subtractive, we want the model to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1060" target="_blank">00:17:40.180</a></span> | <span class="t">remove something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1061" target="_blank">00:17:41.660</a></span> | <span class="t">Here they are called foreground or background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1064" target="_blank">00:17:44.480</a></span> | <span class="t">Foreground means that we want the model to add something, background we want the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1067" target="_blank">00:17:47.660</a></span> | <span class="t">to remove something, just like we did with the example on the website before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1071" target="_blank">00:17:51.540</a></span> | <span class="t">So the first thing they do is they create the positional encoding of these points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1076" target="_blank">00:17:56.500</a></span> | <span class="t">So they convert the X and the Y into positional encodings, exactly like we do in the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1081" target="_blank">00:18:01.860</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1082" target="_blank">00:18:02.860</a></span> | <span class="t">As you remember in the transformer model we have positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1086" target="_blank">00:18:06.180</a></span> | <span class="t">They are special vectors that tell the model, that are combined with the embedding of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1091" target="_blank">00:18:11.620</a></span> | <span class="t">token to tell the model what is the position of the token inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1096" target="_blank">00:18:16.620</a></span> | <span class="t">And here the idea is the same, even if the positional encodings are different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1102" target="_blank">00:18:22.220</a></span> | <span class="t">I mean the idea is the same, so that it's a vector with the dimension 256, but they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1107" target="_blank">00:18:27.640</a></span> | <span class="t">are built in a different way, and we will see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1111" target="_blank">00:18:31.580</a></span> | <span class="t">But we have to think that they transform the X and the Y into vectors, each of them representing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1117" target="_blank">00:18:37.400</a></span> | <span class="t">the position of the point inside of the image using the positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1122" target="_blank">00:18:42.860</a></span> | <span class="t">Then they need to tell the model what is this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1126" target="_blank">00:18:46.700</a></span> | <span class="t">The model cannot know if that point is foreground or background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1130" target="_blank">00:18:50.700</a></span> | <span class="t">So how do we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1132" target="_blank">00:18:52.420</a></span> | <span class="t">Basically all the foreground points are summed to another embedding here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1138" target="_blank">00:18:58.500</a></span> | <span class="t">That indicates it's an embedding that indicates that that is a foreground point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1143" target="_blank">00:19:03.460</a></span> | <span class="t">And all the background points are summed to another embedding that indicates to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1148" target="_blank">00:19:08.580</a></span> | <span class="t">that that point is a background point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1150" target="_blank">00:19:10.700</a></span> | <span class="t">And if the point is a padding, because we don't have enough points, then they use another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1155" target="_blank">00:19:15.420</a></span> | <span class="t">special embedding here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1158" target="_blank">00:19:18.060</a></span> | <span class="t">And this is how they build the embedding for the points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1161" target="_blank">00:19:21.660</a></span> | <span class="t">While for the boxes, the boxes are defined using the top left corner, so X and Y of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1167" target="_blank">00:19:27.540</a></span> | <span class="t">top left corner, and the bottom right corner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1171" target="_blank">00:19:31.360</a></span> | <span class="t">And they do the same with the boxes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1173" target="_blank">00:19:33.760</a></span> | <span class="t">So basically they transform these two points, so the top left and the bottom right, using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1180" target="_blank">00:19:40.100</a></span> | <span class="t">the positional encodings to tell the model what is that X and Y corresponding to inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1184" target="_blank">00:19:44.700</a></span> | <span class="t">of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1186" target="_blank">00:19:46.780</a></span> | <span class="t">And then they sum one embedding to indicate that it's a top left point and another embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1192" target="_blank">00:19:52.660</a></span> | <span class="t">to indicate that it's a bottom right point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1195" target="_blank">00:19:55.660</a></span> | <span class="t">And this is how they build the encoding for the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1199" target="_blank">00:19:59.540</a></span> | <span class="t">Why do we want to create 256 dimensional vector embeddings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1204" target="_blank">00:20:04.380</a></span> | <span class="t">Because the 266 dimension is also the one used for the image embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1211" target="_blank">00:20:11.420</a></span> | <span class="t">Because then we can combine them using a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1216" target="_blank">00:20:16.120</a></span> | <span class="t">So the mask we saw before, what is the role of the mask?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1218" target="_blank">00:20:18.740</a></span> | <span class="t">Now let's go into the detail of how it's combined with the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1223" target="_blank">00:20:23.200</a></span> | <span class="t">So basically, the masks are called a dense prompt in the segment editing model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1228" target="_blank">00:20:28.900</a></span> | <span class="t">And what they do is, if the mask is specified, so here, okay, if the mask is specified, basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1237" target="_blank">00:20:37.820</a></span> | <span class="t">they run it through a sequence of layers of convolutions to downscale this mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1248" target="_blank">00:20:48.540</a></span> | <span class="t">And then if no mask is specified, they create a special embedding called no mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1255" target="_blank">00:20:55.860</a></span> | <span class="t">And it's defined here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1257" target="_blank">00:20:57.220</a></span> | <span class="t">So as you can see, it's just an embedding with a given dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1261" target="_blank">00:21:01.260</a></span> | <span class="t">How they combine this mask with the image, they just use a pointwise sum, as you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1267" target="_blank">00:21:07.140</a></span> | <span class="t">see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1268" target="_blank">00:21:08.460</a></span> | <span class="t">So they take the image and they just add the dense prompt embeddings, which is the mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1275" target="_blank">00:21:15.580</a></span> | <span class="t">embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1276" target="_blank">00:21:16.580</a></span> | <span class="t">Now, as we saw before, we have to use the positional encoding to tell the model what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1281" target="_blank">00:21:21.320</a></span> | <span class="t">are the points that we are feeding to the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1285" target="_blank">00:21:25.180</a></span> | <span class="t">So the model cannot know X and Y, the model has need to know some other information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1291" target="_blank">00:21:31.700</a></span> | <span class="t">We cannot just feed a list of X and Y to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1294" target="_blank">00:21:34.320</a></span> | <span class="t">We need to tell the model something more, something that can be learned by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1299" target="_blank">00:21:39.520</a></span> | <span class="t">And since the transformer models were very good at detecting the position using the sinusoidal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1306" target="_blank">00:21:46.740</a></span> | <span class="t">positional encoding using the vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1309" target="_blank">00:21:49.060</a></span> | <span class="t">So if you remember in the vanilla transformer, so the transformer that was introduced in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1312" target="_blank">00:21:52.780</a></span> | <span class="t">the paper, attention is all you need, if you remember, the positional encodings were built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1317" target="_blank">00:21:57.980</a></span> | <span class="t">using sinusoidal functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1320" target="_blank">00:22:00.380</a></span> | <span class="t">So sines and cosines combined together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1323" target="_blank">00:22:03.980</a></span> | <span class="t">And these vectors told the model what is the position of the token inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1329" target="_blank">00:22:09.060</a></span> | <span class="t">Now, this was fine as long as we worked with text, because text only move along one dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1334" target="_blank">00:22:14.780</a></span> | <span class="t">that is, we have the token number zero, the token number one, the token number two, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1339" target="_blank">00:22:19.860</a></span> | <span class="t">But pixels don't move in one direction, they move into two directions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1343" target="_blank">00:22:23.700</a></span> | <span class="t">So one person, of course, one could think, why not use the one positional encoding for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1350" target="_blank">00:22:30.300</a></span> | <span class="t">the X coordinate to convert the X coordinate into a vector and another to map the Y coordinate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1357" target="_blank">00:22:37.140</a></span> | <span class="t">into a vector?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1358" target="_blank">00:22:38.140</a></span> | <span class="t">Yeah, we could do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1359" target="_blank">00:22:39.980</a></span> | <span class="t">But the problem is, if we do in this way, suppose we convert the center position of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1366" target="_blank">00:22:46.980</a></span> | <span class="t">the image into two vectors, one encoded using the X coordinate and one encoded using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1373" target="_blank">00:22:53.140</a></span> | <span class="t">Y coordinate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1374" target="_blank">00:22:54.820</a></span> | <span class="t">What we do if we check the similarity with the other position in the image is we get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1378" target="_blank">00:22:58.780</a></span> | <span class="t">some hitmap like this, in which the zero, this position here is very similar to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1384" target="_blank">00:23:04.300</a></span> | <span class="t">position here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1385" target="_blank">00:23:05.980</a></span> | <span class="t">But it's not similar to this position here, which is not good, because in the in an image,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1392" target="_blank">00:23:12.580</a></span> | <span class="t">we have the Euclidean distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1394" target="_blank">00:23:14.420</a></span> | <span class="t">So pixel at the same Euclidean distance should have similarity with another point at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1401" target="_blank">00:23:21.020</a></span> | <span class="t">same distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1402" target="_blank">00:23:22.020</a></span> | <span class="t">So basically, this point and this point should have a similarity that is the same as this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1407" target="_blank">00:23:27.220</a></span> | <span class="t">point and this point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1410" target="_blank">00:23:30.860</a></span> | <span class="t">Because we have a spatial representation, so pixels that are close to each other should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1415" target="_blank">00:23:35.060</a></span> | <span class="t">be very similar, pixels that are far from each other should not be very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1420" target="_blank">00:23:40.100</a></span> | <span class="t">But this is not what happens in this hitmap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1423" target="_blank">00:23:43.340</a></span> | <span class="t">What we want is something like this, that is, if we have a point here, all the points</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1427" target="_blank">00:23:47.540</a></span> | <span class="t">in the radius of, let's say, 10 pixels are very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1431" target="_blank">00:23:51.500</a></span> | <span class="t">All the points in the radius of 20, so distance 20, are less similar, but still, depending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1441" target="_blank">00:24:01.740</a></span> | <span class="t">on the radius, they are similar in the same way as the other points with the same radius.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1447" target="_blank">00:24:07.380</a></span> | <span class="t">And the more we go far from the center, the more we become distant, the more we become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1453" target="_blank">00:24:13.220</a></span> | <span class="t">different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1454" target="_blank">00:24:14.220</a></span> | <span class="t">And this is what we want from positional encodings for an image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1458" target="_blank">00:24:18.620</a></span> | <span class="t">And this idea was introduced in this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1462" target="_blank">00:24:22.180</a></span> | <span class="t">You can see learnable Fourier features from multidimensional spatial positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1468" target="_blank">00:24:28.020</a></span> | <span class="t">And however, this is not the paper used by Segment Anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1471" target="_blank">00:24:31.020</a></span> | <span class="t">For Segment Anything, they use this paper here, but you understood why we needed a new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1475" target="_blank">00:24:35.780</a></span> | <span class="t">kind of positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1478" target="_blank">00:24:38.100</a></span> | <span class="t">Basically because we need to map two-dimensional mapping of X and Y to... we need to give an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1487" target="_blank">00:24:47.340</a></span> | <span class="t">X and Y mapping to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1489" target="_blank">00:24:49.820</a></span> | <span class="t">We cannot just give them independent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1492" target="_blank">00:24:52.300</a></span> | <span class="t">And now let's look at the most important part of the model, which is the decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1496" target="_blank">00:24:56.900</a></span> | <span class="t">Now before we look at the decoder, I want to remind you that in the model that we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1501" target="_blank">00:25:01.860</a></span> | <span class="t">before on the web browser, we could add the points on the real time in the browser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1509" target="_blank">00:25:09.940</a></span> | <span class="t">That is, we loaded the image, and then I clicked on the model, and basically after a few milliseconds,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1516" target="_blank">00:25:16.860</a></span> | <span class="t">let's say 100 milliseconds or half a second, I saw the output of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1521" target="_blank">00:25:21.700</a></span> | <span class="t">This could happen because the decoder is very fast, and the prompt encoder is very fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1527" target="_blank">00:25:27.660</a></span> | <span class="t">But the image encoder doesn't have to be very fast, because we only encode the image once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1532" target="_blank">00:25:32.800</a></span> | <span class="t">when we load the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1534" target="_blank">00:25:34.040</a></span> | <span class="t">Then we can do multiple prompts, so we can save the image embeddings, and then we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1540" target="_blank">00:25:40.580</a></span> | <span class="t">change the prompt embeddings and run them through the decoder to get the new masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1545" target="_blank">00:25:45.580</a></span> | <span class="t">Which means basically that the image encoder can be powerful, even if it's slow, but the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1551" target="_blank">00:25:51.380</a></span> | <span class="t">mask decoder has to be lightweight and fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1555" target="_blank">00:25:55.740</a></span> | <span class="t">And the same goes for the prompt encoder, and this is actually the case, because that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1559" target="_blank">00:25:59.340</a></span> | <span class="t">why we could use it on my browser in a reasonable time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1564" target="_blank">00:26:04.740</a></span> | <span class="t">So the mask decoder is made in this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1569" target="_blank">00:26:09.460</a></span> | <span class="t">It's made of two layers, so we have to think that there is this block here, is repeated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1576" target="_blank">00:26:16.980</a></span> | <span class="t">again with another block that is after this one, where the output of this big block is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1582" target="_blank">00:26:22.860</a></span> | <span class="t">fed to the other block, and the output of that block is actually sent to the model here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1588" target="_blank">00:26:28.460</a></span> | <span class="t">Let me delete that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1593" target="_blank">00:26:33.300</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1594" target="_blank">00:26:34.980</a></span> | <span class="t">Now let's look at the input of this decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1598" target="_blank">00:26:38.540</a></span> | <span class="t">First of all, we have the prompts here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1601" target="_blank">00:26:41.020</a></span> | <span class="t">So the prompts sent by the user, so the clicks, the boxes, and then we have the image embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1607" target="_blank">00:26:47.260</a></span> | <span class="t">plus the mask we can see in the picture here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1610" target="_blank">00:26:50.540</a></span> | <span class="t">So the image has already been combined with the mask through this layer here, through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1615" target="_blank">00:26:55.420</a></span> | <span class="t">this addition, element-wise addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1620" target="_blank">00:27:00.060</a></span> | <span class="t">The first thing the model, the decoder does is the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1624" target="_blank">00:27:04.100</a></span> | <span class="t">So the self-attention between the prompt tokens and with the prompt tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1630" target="_blank">00:27:10.360</a></span> | <span class="t">But here we can also see that there are these output tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1634" target="_blank">00:27:14.000</a></span> | <span class="t">So before we proceed to see these steps, let's watch what are the output tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1639" target="_blank">00:27:19.880</a></span> | <span class="t">The output tokens take the idea also from BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1643" target="_blank">00:27:23.180</a></span> | <span class="t">So as you remember before, we saw the vision transformer, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1646" target="_blank">00:27:26.760</a></span> | <span class="t">In the vision transformer, when they fed the patches to the transformer encoder, they also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1652" target="_blank">00:27:32.060</a></span> | <span class="t">prepended another token called the class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1655" target="_blank">00:27:35.380</a></span> | <span class="t">And the same idea is reused by segment anything, in which they append some tokens before the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1661" target="_blank">00:27:41.860</a></span> | <span class="t">promptable tokens, so the boxes, the clicks made by the user.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1667" target="_blank">00:27:47.940</a></span> | <span class="t">And then at the output of this decoder, they check again only these tokens and force the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1674" target="_blank">00:27:54.220</a></span> | <span class="t">model to put all the information into these tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1677" target="_blank">00:27:57.740</a></span> | <span class="t">So in this case, we have one token that tells the IOU, so the intersection over union scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1685" target="_blank">00:28:05.080</a></span> | <span class="t">of the predicted masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1688" target="_blank">00:28:08.040</a></span> | <span class="t">And we will see later what is the IOU, if you're not familiar with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1691" target="_blank">00:28:11.880</a></span> | <span class="t">And then there are three mask tokens, so one token for each mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1697" target="_blank">00:28:17.700</a></span> | <span class="t">And basically, we feed these four tokens, so one IOU and three masks to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1704" target="_blank">00:28:24.340</a></span> | <span class="t">We take them here at the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1706" target="_blank">00:28:26.920</a></span> | <span class="t">And then we use the first token, so the IOU token, before we map it to a multi-layer perceptron,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1714" target="_blank">00:28:34.580</a></span> | <span class="t">to force the model to learn the IOU score into these tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1718" target="_blank">00:28:38.580</a></span> | <span class="t">And then the other three are used to predict the three masks as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1724" target="_blank">00:28:44.580</a></span> | <span class="t">And we can see that here, they use the idea just like in the BERT paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1731" target="_blank">00:28:51.900</a></span> | <span class="t">And this is the reference to the BERT paper in which they introduced the CLS token, in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1736" target="_blank">00:28:56.020</a></span> | <span class="t">which in BERT was used for classification tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1739" target="_blank">00:28:59.060</a></span> | <span class="t">So basically, also in BERT, they prepended this token called the CLS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1745" target="_blank">00:29:05.140</a></span> | <span class="t">And then, at the output of the transformer, they just took the token corresponding to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1749" target="_blank">00:29:09.980</a></span> | <span class="t">this CLS, which was the first one, and they forced the model to learn all the information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1756" target="_blank">00:29:16.340</a></span> | <span class="t">it needed to classify into this CLS token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1759" target="_blank">00:29:19.580</a></span> | <span class="t">Why this works?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1760" target="_blank">00:29:20.580</a></span> | <span class="t">Because the CLS token could interact with all the other tokens through the self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1764" target="_blank">00:29:24.860</a></span> | <span class="t">mechanism.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1766" target="_blank">00:29:26.040</a></span> | <span class="t">And the same idea is reused here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1768" target="_blank">00:29:28.140</a></span> | <span class="t">So we feed the model with output tokens combined with the PROM tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1773" target="_blank">00:29:33.260</a></span> | <span class="t">And you can see here, they just concatenate the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1776" target="_blank">00:29:36.060</a></span> | <span class="t">They take the IOU token and the mask token, they concatenate together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1779" target="_blank">00:29:39.420</a></span> | <span class="t">And then they concatenate these output tokens with the PROM tokens you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1784" target="_blank">00:29:44.820</a></span> | <span class="t">Now the second part, they run attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1790" target="_blank">00:29:50.360</a></span> | <span class="t">So first, they run the self-attention with the tokens, which are the output tokens plus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1795" target="_blank">00:29:55.400</a></span> | <span class="t">the PROM tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1796" target="_blank">00:29:56.620</a></span> | <span class="t">And this part is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1798" target="_blank">00:29:58.540</a></span> | <span class="t">You can see that the query, the key, and the values are the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1801" target="_blank">00:30:01.820</a></span> | <span class="t">And they are the PROM tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1805" target="_blank">00:30:05.140</a></span> | <span class="t">The comments have been added by me, they are not present in the original code to make your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1808" target="_blank">00:30:08.800</a></span> | <span class="t">life easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1810" target="_blank">00:30:10.180</a></span> | <span class="t">Even if I found it really hard to follow this nomenclature, because sometimes they use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1814" target="_blank">00:30:14.660</a></span> | <span class="t">name is called Q, but then they pass it as K or P, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1818" target="_blank">00:30:18.940</a></span> | <span class="t">But hopefully it's clear enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1821" target="_blank">00:30:21.420</a></span> | <span class="t">What we want to get from this code is actually not the single instructions, but the overall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1829" target="_blank">00:30:29.500</a></span> | <span class="t">concepts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1830" target="_blank">00:30:30.700</a></span> | <span class="t">So the output of this self-attention is then fed to a cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1835" target="_blank">00:30:35.820</a></span> | <span class="t">What is a cross-attention?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1836" target="_blank">00:30:36.820</a></span> | <span class="t">Basically, a cross-attention is an attention in which the query comes from one side, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1842" target="_blank">00:30:42.220</a></span> | <span class="t">the key and the value come from another side.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1844" target="_blank">00:30:44.820</a></span> | <span class="t">If you remember my video about the transformer model, in a translation task usually, imagine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1851" target="_blank">00:30:51.760</a></span> | <span class="t">we are translating from English to Italian, or English to French, or English to Chinese.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1857" target="_blank">00:30:57.340</a></span> | <span class="t">Basically, we first run in the encoder a self-attention between all the input sentence, so all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1863" target="_blank">00:31:03.380</a></span> | <span class="t">tokens of the input sentence related to all the other tokens of the input sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1868" target="_blank">00:31:08.180</a></span> | <span class="t">And then in the decoder, we have this cross-attention in which we take the queries coming from one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1874" target="_blank">00:31:14.560</a></span> | <span class="t">language and the key and the values coming from another language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1878" target="_blank">00:31:18.780</a></span> | <span class="t">And this is usually done to combine two different sequences, to relate two different sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1886" target="_blank">00:31:26.800</a></span> | <span class="t">with each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1888" target="_blank">00:31:28.120</a></span> | <span class="t">In this case, what we want is to relate the tokens, so our prompt, with the image embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1893" target="_blank">00:31:33.180</a></span> | <span class="t">This is why we do a cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1895" target="_blank">00:31:35.300</a></span> | <span class="t">So the first cross-attention is the tokens used as queries, while the image embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1901" target="_blank">00:31:41.140</a></span> | <span class="t">are used as keys and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1904" target="_blank">00:31:44.220</a></span> | <span class="t">And this is the first cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1907" target="_blank">00:31:47.220</a></span> | <span class="t">Then there is a multilayer perceptron, so it's just linear layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1912" target="_blank">00:31:52.880</a></span> | <span class="t">And finally, we have another cross-attention, but this time the opposite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1916" target="_blank">00:31:56.740</a></span> | <span class="t">So in this case, the queries are the image embeddings and the keys and the values are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1920" target="_blank">00:32:00.900</a></span> | <span class="t">the prompt tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1923" target="_blank">00:32:03.160</a></span> | <span class="t">Why do we want two cross-attentions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1925" target="_blank">00:32:05.100</a></span> | <span class="t">Because we want two outputs from this transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1928" target="_blank">00:32:08.020</a></span> | <span class="t">One will be the sequence of tokens of the prompt, from which we will extract the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1932" target="_blank">00:32:12.700</a></span> | <span class="t">tokens, one indicating the IOU score and three indicating the mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1937" target="_blank">00:32:17.600</a></span> | <span class="t">And one is the image embedding that we will then combine with the output tokens of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1942" target="_blank">00:32:22.860</a></span> | <span class="t">mask to build the mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1944" target="_blank">00:32:24.880</a></span> | <span class="t">But we will see this later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1947" target="_blank">00:32:27.980</a></span> | <span class="t">Another thing here highlighted in the paper is that to ensure that the coder has critical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1952" target="_blank">00:32:32.760</a></span> | <span class="t">geometric information, the positional encodings are added to the image embedding whenever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1957" target="_blank">00:32:37.540</a></span> | <span class="t">they participate in an attention layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1959" target="_blank">00:32:39.940</a></span> | <span class="t">And as you can see, this is done not only for the image, but also for the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1963" target="_blank">00:32:43.820</a></span> | <span class="t">So every time to the prompt, they add the positional encoding, and every time to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1968" target="_blank">00:32:48.020</a></span> | <span class="t">image embedding, they also add the positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1971" target="_blank">00:32:51.440</a></span> | <span class="t">Why do we keep adding them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1972" target="_blank">00:32:52.960</a></span> | <span class="t">Because we don't want to lose this information after all these layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1978" target="_blank">00:32:58.080</a></span> | <span class="t">And this is usually done with a skip connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1980" target="_blank">00:33:00.760</a></span> | <span class="t">And in this case, they just add them back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1984" target="_blank">00:33:04.560</a></span> | <span class="t">And now let's have a look at the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1987" target="_blank">00:33:07.160</a></span> | <span class="t">As we saw before, we have a special layer of tokens added to the input of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1995" target="_blank">00:33:15.440</a></span> | <span class="t">One is for the IOU prediction and three are for the mask prediction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=1999" target="_blank">00:33:19.040</a></span> | <span class="t">And you can see them here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2002" target="_blank">00:33:22.040</a></span> | <span class="t">Because our transformer model has two cross-attentions, we have two outputs, two sequences outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2008" target="_blank">00:33:28.720</a></span> | <span class="t">One is the output sequence of the tokens, and one is the output sequence of the embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2013" target="_blank">00:33:33.240</a></span> | <span class="t">of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2015" target="_blank">00:33:35.980</a></span> | <span class="t">And they extract the IOU token, which is the first token added to the sequence, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2021" target="_blank">00:33:41.680</a></span> | <span class="t">they extract the mask tokens, which are the first three, skipping the first one, the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2027" target="_blank">00:33:47.040</a></span> | <span class="t">three tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2029" target="_blank">00:33:49.380</a></span> | <span class="t">Then what they do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2030" target="_blank">00:33:50.380</a></span> | <span class="t">They give the IOU tokens, they just give it to a prediction head to predict the IOU scores,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2035" target="_blank">00:33:55.760</a></span> | <span class="t">and we can see that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2040" target="_blank">00:34:00.040</a></span> | <span class="t">And then they take the output tokens for the masks, we can see them here, and they combine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2049" target="_blank">00:34:09.680</a></span> | <span class="t">them with the upscaled embedding of the image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2054" target="_blank">00:34:14.280</a></span> | <span class="t">So they take the output of the transformer for the image, so SRC in this case, the variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2061" target="_blank">00:34:21.060</a></span> | <span class="t">name is SRC, they upscale it here, and then they run each of the mask output tokens through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2072" target="_blank">00:34:32.400</a></span> | <span class="t">its own MLP layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2074" target="_blank">00:34:34.520</a></span> | <span class="t">So here you can see we have multiple MLP blocks here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2078" target="_blank">00:34:38.880</a></span> | <span class="t">Each of the tokens have its own MLP block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2082" target="_blank">00:34:42.000</a></span> | <span class="t">They run each of them through its own, they get the output, and then they combine this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2087" target="_blank">00:34:47.360</a></span> | <span class="t">output of this MLP, one for each token, one for each mask, with the upscale embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2093" target="_blank">00:34:53.160</a></span> | <span class="t">of the image to produce the output mask here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2098" target="_blank">00:34:58.000</a></span> | <span class="t">Another interesting part of the paper is this section here, making the model ambiguity aware.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2103" target="_blank">00:35:03.200</a></span> | <span class="t">So as I was saying before, we not only predict one mask, but we predict three masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2108" target="_blank">00:35:08.200</a></span> | <span class="t">And this happens when we do not have more than one prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2113" target="_blank">00:35:13.760</a></span> | <span class="t">So if we only click one, for example, once, the model will produce three masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2118" target="_blank">00:35:18.960</a></span> | <span class="t">But if we have more than one prompt, because the ambiguity becomes less, at least theoretically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2126" target="_blank">00:35:26.000</a></span> | <span class="t">the authors decided to add a fourth token that predicts another mask that is used only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2133" target="_blank">00:35:33.960</a></span> | <span class="t">when we have more than one prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2137" target="_blank">00:35:37.380</a></span> | <span class="t">And this mask is never returned for the single prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2144" target="_blank">00:35:44.360</a></span> | <span class="t">Now let's have a look at what is intersection over union.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2147" target="_blank">00:35:47.560</a></span> | <span class="t">Intersection over union allows us to understand how good is our prediction given the ground</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2154" target="_blank">00:35:54.040</a></span> | <span class="t">truth, especially in segmentation models or object detection models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2160" target="_blank">00:36:00.440</a></span> | <span class="t">So for example, imagine we are using object detection and our ground truth box is this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2166" target="_blank">00:36:06.200</a></span> | <span class="t">green box here, but our object, our model produced this red prediction as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2172" target="_blank">00:36:12.680</a></span> | <span class="t">So because you can see that even if there is some overlapping, but it doesn't cover</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2176" target="_blank">00:36:16.680</a></span> | <span class="t">the entire image, the prediction is quite poor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2181" target="_blank">00:36:21.580</a></span> | <span class="t">But this improves when the box becomes bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2186" target="_blank">00:36:26.000</a></span> | <span class="t">So the red box becomes bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2187" target="_blank">00:36:27.700</a></span> | <span class="t">So there is more intersection, but also more union.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2192" target="_blank">00:36:32.080</a></span> | <span class="t">And finally, it becomes excellent when the intersection is covering all the box and it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2198" target="_blank">00:36:38.840</a></span> | <span class="t">covering all the union.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2200" target="_blank">00:36:40.120</a></span> | <span class="t">So the union of the two, basically the same box and they cover as most as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2205" target="_blank">00:36:45.520</a></span> | <span class="t">This area here, so the area that is predicted, but was not asked in the ground truth is called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2212" target="_blank">00:36:52.520</a></span> | <span class="t">false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2213" target="_blank">00:36:53.520</a></span> | <span class="t">This one here is called false positive, while the area that should have been predicted,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2219" target="_blank">00:36:59.240</a></span> | <span class="t">but was not predicted is called false negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2222" target="_blank">00:37:02.960</a></span> | <span class="t">This is a commonly used term also in this kind of scenarios.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2230" target="_blank">00:37:10.080</a></span> | <span class="t">Now let's have a look at the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2231" target="_blank">00:37:11.680</a></span> | <span class="t">The loss of the model is a combination of two loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2234" target="_blank">00:37:14.120</a></span> | <span class="t">One is called the focal loss and one is the dice loss, and they are used in a ratio of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2238" target="_blank">00:37:18.040</a></span> | <span class="t">20 to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2240" target="_blank">00:37:20.100</a></span> | <span class="t">Let's have a look at the focal loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2242" target="_blank">00:37:22.440</a></span> | <span class="t">The focal loss takes his idea from the cross entropy, but with a modification that is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2248" target="_blank">00:37:28.440</a></span> | <span class="t">focal loss is adjusted for class imbalance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2252" target="_blank">00:37:32.440</a></span> | <span class="t">So why do we have a class imbalance in this case?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2255" target="_blank">00:37:35.560</a></span> | <span class="t">Because imagine we are using a segmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2258" target="_blank">00:37:38.280</a></span> | <span class="t">We are trying to predict the map, the mask for a particular object in our image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2265" target="_blank">00:37:45.160</a></span> | <span class="t">But of course, usually the mask is not covering the entire image, but it's only very few pixels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2271" target="_blank">00:37:51.080</a></span> | <span class="t">compared to the total image are actually participating in this mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2275" target="_blank">00:37:55.960</a></span> | <span class="t">And the instances of big mask are actually not so many.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2279" target="_blank">00:37:59.360</a></span> | <span class="t">So we have a class imbalance here because most of our pixel will be non mask and only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2283" target="_blank">00:38:03.600</a></span> | <span class="t">a small percentage of our pixel will be mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2286" target="_blank">00:38:06.600</a></span> | <span class="t">So we cannot use cross entropy in this case because the cross entropy doesn't pay attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2290" target="_blank">00:38:10.120</a></span> | <span class="t">to this class imbalance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2291" target="_blank">00:38:11.560</a></span> | <span class="t">So this is why they use focal loss to pay attention to this class imbalance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2295" target="_blank">00:38:15.760</a></span> | <span class="t">But the focal loss derives from the cross entropy and it was introduced in this paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2300" target="_blank">00:38:20.640</a></span> | <span class="t">by Facebook research, you can see here focal loss for dense object detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2306" target="_blank">00:38:26.720</a></span> | <span class="t">The next loss is the dice loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2309" target="_blank">00:38:29.100</a></span> | <span class="t">The dice loss comes from the Soren dice coefficient and it's also called the F1 score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2314" target="_blank">00:38:34.880</a></span> | <span class="t">And it's calculated as the total to twice the intersection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2318" target="_blank">00:38:38.600</a></span> | <span class="t">So twice the area of overlap divided by the total area.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2323" target="_blank">00:38:43.360</a></span> | <span class="t">And this is the actually a measure of similarity of between two sets of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2329" target="_blank">00:38:49.640</a></span> | <span class="t">To get the loss, we just do one minus the dice score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2333" target="_blank">00:38:53.180</a></span> | <span class="t">If you want more information about this dice score, which is very commonly used, I suggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2337" target="_blank">00:38:57.480</a></span> | <span class="t">you click on this link.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2338" target="_blank">00:38:58.820</a></span> | <span class="t">It's on Medium.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2340" target="_blank">00:39:00.740</a></span> | <span class="t">It's a nice article on how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2345" target="_blank">00:39:05.160</a></span> | <span class="t">And the dice loss was introduced in this paper VNet, I think it's from 2015.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2351" target="_blank">00:39:11.740</a></span> | <span class="t">Another interesting thing is that a segment anything built its own data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2355" target="_blank">00:39:15.840</a></span> | <span class="t">And this is remarkable because we saw before that the segment anything has been trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2359" target="_blank">00:39:19.720</a></span> | <span class="t">on 1.1 billion masks using millions of images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2365" target="_blank">00:39:25.360</a></span> | <span class="t">And the data engine that was used to build this data set of 1.1 billion mask is composed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2372" target="_blank">00:39:32.200</a></span> | <span class="t">of three stages.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2373" target="_blank">00:39:33.880</a></span> | <span class="t">The first one was a manual stage, then a semi automatic stage and then a fully automatic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2378" target="_blank">00:39:38.160</a></span> | <span class="t">stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2379" target="_blank">00:39:39.160</a></span> | <span class="t">Let's review them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2381" target="_blank">00:39:41.920</a></span> | <span class="t">In the assisted manual stage, so the manual stage, basically they hired a team of professional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2386" target="_blank">00:39:46.280</a></span> | <span class="t">annotators that manually labeled the images using only the brush and the eraser tool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2392" target="_blank">00:39:52.700</a></span> | <span class="t">So basically you have to think that there are many people who are using only pixel by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2397" target="_blank">00:39:57.480</a></span> | <span class="t">pixel mapping this pixel to masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2400" target="_blank">00:40:00.920</a></span> | <span class="t">So this is what we would do to create a data set from zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2405" target="_blank">00:40:05.120</a></span> | <span class="t">Then they train this model on this manually created masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2409" target="_blank">00:40:09.680</a></span> | <span class="t">And then they went to the semi automatic stage, that is, some of the masks were already generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2415" target="_blank">00:40:15.240</a></span> | <span class="t">by our model, which was trained on the manually generated mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2419" target="_blank">00:40:19.720</a></span> | <span class="t">And then the operators only had to adjust this mask to annotate any additional annotated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2426" target="_blank">00:40:26.580</a></span> | <span class="t">objects that were missed from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2430" target="_blank">00:40:30.120</a></span> | <span class="t">And finally, this create even more samples and they train the model on this sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2434" target="_blank">00:40:34.480</a></span> | <span class="t">And finally, they created the fully automatic stage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2440" target="_blank">00:40:40.720</a></span> | <span class="t">In this fully automatic stage, the model, there is no operator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2444" target="_blank">00:40:44.760</a></span> | <span class="t">The model is building the data set by itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2448" target="_blank">00:40:48.000</a></span> | <span class="t">How does it do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2449" target="_blank">00:40:49.000</a></span> | <span class="t">They take an image, they create a grid of 32 by 32 points.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2453" target="_blank">00:40:53.760</a></span> | <span class="t">And then for each of these points, they ask the model to predict the masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2459" target="_blank">00:40:59.480</a></span> | <span class="t">Of course, this will produce a lot, a large number of masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2462" target="_blank">00:41:02.600</a></span> | <span class="t">So they only take the one with the highest confidence score and also the only one that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2467" target="_blank">00:41:07.460</a></span> | <span class="t">are stable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2468" target="_blank">00:41:08.640</a></span> | <span class="t">And by stable, they mean that if they threshold the probability map at 0.5 minus delta and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2474" target="_blank">00:41:14.680</a></span> | <span class="t">0.5 plus delta, they result in similar masks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2479" target="_blank">00:41:19.360</a></span> | <span class="t">Next, because we have a lot of masks and some of them may be overlapping with each other,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2484" target="_blank">00:41:24.520</a></span> | <span class="t">some of them may be duplicate, actually, we need to remove some of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2489" target="_blank">00:41:29.440</a></span> | <span class="t">So we use an algorithm called the non-maximal suppression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2492" target="_blank">00:41:32.360</a></span> | <span class="t">This is very famous also in object detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2494" target="_blank">00:41:34.760</a></span> | <span class="t">Let's review how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2496" target="_blank">00:41:36.800</a></span> | <span class="t">So non-maximal suppression usually works like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2500" target="_blank">00:41:40.480</a></span> | <span class="t">Imagine we have an object detection model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2503" target="_blank">00:41:43.760</a></span> | <span class="t">Usually when we detect a bounding box for an object, we get a lot of bounding boxes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2509" target="_blank">00:41:49.020</a></span> | <span class="t">And how do we only select one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2511" target="_blank">00:41:51.120</a></span> | <span class="t">Well, basically, we take the one with the highest confidence score, and then we delete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2516" target="_blank">00:41:56.480</a></span> | <span class="t">all the other bounding boxes that have an IOU threshold with the one that we selected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2524" target="_blank">00:42:04.680</a></span> | <span class="t">higher than one threshold that is given as parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2528" target="_blank">00:42:08.740</a></span> | <span class="t">This allow us to eliminate all the bounding boxes that are similar to the one we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2532" target="_blank">00:42:12.960</a></span> | <span class="t">selected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2533" target="_blank">00:42:13.960</a></span> | <span class="t">And which one did we select?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2535" target="_blank">00:42:15.160</a></span> | <span class="t">The one with the highest score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2537" target="_blank">00:42:17.240</a></span> | <span class="t">And then we do this for all the remaining boxes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2541" target="_blank">00:42:21.220</a></span> | <span class="t">And the algorithm is very simple, and it's also very effective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2546" target="_blank">00:42:26.720</a></span> | <span class="t">Thank you guys for watching my video about the segment anything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2550" target="_blank">00:42:30.640</a></span> | <span class="t">I hope that most of the information was clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2554" target="_blank">00:42:34.440</a></span> | <span class="t">If not, please let me know in the comments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2556" target="_blank">00:42:36.160</a></span> | <span class="t">I will try to complement my errors or some misunderstanding or something that I should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2562" target="_blank">00:42:42.320</a></span> | <span class="t">have said better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2565" target="_blank">00:42:45.000</a></span> | <span class="t">I please subscribe to my channel because I will be uploading more videos in the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=eYhvJR4zFUM&t=2571" target="_blank">00:42:51.360</a></span> | <span class="t">And hopefully see you again.</span></div></div></body></html>