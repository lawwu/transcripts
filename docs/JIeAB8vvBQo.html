<html><head><title>Build a Custom Transformer Tokenizer - Transformers From Scratch #2</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Build a Custom Transformer Tokenizer - Transformers From Scratch #2</h2><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo"><img src="https://i.ytimg.com/vi/JIeAB8vvBQo/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=0">0:0</a> <Untitled Chapter 1><br><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=190">3:10</a> Training the Tokenizer<br><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=321">5:21</a> Vocab Size<br><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=747">12:27</a> Encoding Text<br><br><div style="text-align: left;"><a href="./JIeAB8vvBQo.html">Whisper Transcript</a> | <a href="./transcript_JIeAB8vvBQo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi and welcome to the video. We're going to have a look at how we can build our own tokenizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=6" target="_blank">00:00:06.400</a></span> | <span class="t">in Transformers from scratch. So this is the second video in our Transformers from scratch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=14" target="_blank">00:00:14.960</a></span> | <span class="t">series and what we're going to be covering is the actual tokenizer itself. So we've already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=24" target="_blank">00:00:24.160</a></span> | <span class="t">got our data so we can cross off now onto the tokenizer. So let's move over to our code. So in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=33" target="_blank">00:00:33.040</a></span> | <span class="t">the previous video we created all these files here. So these are just a lot of text files</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=44" target="_blank">00:00:44.160</a></span> | <span class="t">that contain the Italian subset from the Oscar data set. Now let's maybe open one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=55" target="_blank">00:00:55.360</a></span> | <span class="t">Ignore that and we just we get all this Italian. Now each sample in this text file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=66" target="_blank">00:01:06.640</a></span> | <span class="t">is separated by a newline character. So let's go ahead and begin using that data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=81" target="_blank">00:01:21.600</a></span> | <span class="t">to build our tokenizer. So we first want to get a list of all the paths to our files.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=91" target="_blank">00:01:31.600</a></span> | <span class="t">So we are going to be using path lib. You could also use OS list there as well. It's up to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=98" target="_blank">00:01:38.080</a></span> | <span class="t">Import. So sorry import path. So from path lib.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=106" target="_blank">00:01:46.000</a></span> | <span class="t">Import path. I'm using this one because I don't know I've noticed that people are using this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=114" target="_blank">00:01:54.880</a></span> | <span class="t">a lot at the moment for machine learning stuff. I'm not sure why you would do it over OS list there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=122" target="_blank">00:02:02.160</a></span> | <span class="t">But it's what people are using. So let's give it a go. See how it is. So we have this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=130" target="_blank">00:02:10.960</a></span> | <span class="t">And we just want to create a string from each path object that we get. So for x in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=141" target="_blank">00:02:21.520</a></span> | <span class="t">And then in here we need to write path. And in here we just want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=148" target="_blank">00:02:28.000</a></span> | <span class="t">basically tell this where to look. So we're using path here and we're just in the same directory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=156" target="_blank">00:02:36.000</a></span> | <span class="t">So it's not we don't really need to do anything here. That's fine. And then at the end we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=161" target="_blank">00:02:41.200</a></span> | <span class="t">going to use glob here. I think this is why people are using this. And we just create like a wild</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=168" target="_blank">00:02:48.720</a></span> | <span class="t">card like we want all text files in this directory. So we just write that. Now let's do that. I'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=181" target="_blank">00:03:01.760</a></span> | <span class="t">look at the first five and see that we have our text files now. So that's good. And what we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=188" target="_blank">00:03:08.480</a></span> | <span class="t">now do is move on to actually training the tokenizer. So the tokenizer that we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=194" target="_blank">00:03:14.880</a></span> | <span class="t">be using is a byte level byte pair encoding tokenizer or BP tokenizer. And essentially what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=206" target="_blank">00:03:26.480</a></span> | <span class="t">that means is that it's going to break down our text into into bytes. So with most tokenizers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=216" target="_blank">00:03:36.320</a></span> | <span class="t">that we probably use, unless you've used this one before, we tend to have like unknown tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=225" target="_blank">00:03:45.840</a></span> | <span class="t">So like for BERT, we use sentence piece encodings and we have to have this unknown token for when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=233" target="_blank">00:03:53.760</a></span> | <span class="t">we don't have a token for a specific word, like for some new word. Now with the BPE tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=245" target="_blank">00:04:05.680</a></span> | <span class="t">we are breaking things down into bytes. So essentially we don't actually need an unknown</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=250" target="_blank">00:04:10.880</a></span> | <span class="t">token anymore. So that's I think pretty cool. Now to use that, we need to do from tokenizers. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=259" target="_blank">00:04:19.920</a></span> | <span class="t">this is a another hugging face package. So maybe you need to, you might need to install that. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=268" target="_blank">00:04:28.160</a></span> | <span class="t">if install tokenizers and you want to do byte level BP tokenizer like that. Okay. Now we take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=279" target="_blank">00:04:39.840</a></span> | <span class="t">that and we're going to initialize our tokenizer. So we just write that. That's our tokenizer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=291" target="_blank">00:04:51.760</a></span> | <span class="t">initialized. We haven't trained it yet. To train it, we need to write tokenizer train.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=299" target="_blank">00:04:59.040</a></span> | <span class="t">And then in here we need to include the files that we're training on. So this is why we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=306" target="_blank">00:05:06.880</a></span> | <span class="t">that pass variable up here. So this is just a list of all the text files that we created,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=312" target="_blank">00:05:12.480</a></span> | <span class="t">which are all separated by new line characters. Each sample is separated by a new line character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=321" target="_blank">00:05:21.200</a></span> | <span class="t">Now the vocab size, we're going to be using a Roberta model here. And I think the Roberta</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=332" target="_blank">00:05:32.320</a></span> | <span class="t">model, typical Roberta model, vocab size is 50K. Now you can use that if you want. It's up to you,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=341" target="_blank">00:05:41.360</a></span> | <span class="t">but I'm going to stick with the typical BERT size just because I don't think we need that much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=350" target="_blank">00:05:50.160</a></span> | <span class="t">You know, we're just figuring things out here. So, you know, this is going to mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=354" target="_blank">00:05:54.960</a></span> | <span class="t">less training time and that's a good thing in my opinion. We haven't set the min frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=363" target="_blank">00:06:03.040</a></span> | <span class="t">So this is saying what is the minimum number of times you want to see a word or a part of a word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=371" target="_blank">00:06:11.760</a></span> | <span class="t">or a byte. So it's kind of weird with this tokenizer before you add it into our vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=379" target="_blank">00:06:19.680</a></span> | <span class="t">So that's all that is. Okay. And then we also need to include our special tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=386" target="_blank">00:06:26.720</a></span> | <span class="t">So we're using the Roberta special tokens here. So special tokens. And then in here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=392" target="_blank">00:06:32.720</a></span> | <span class="t">we have our starter sequence token. So I'm going to put this on a new line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=396" target="_blank">00:06:36.640</a></span> | <span class="t">Not like that, like this. So we have this starter sequence token, the padding token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=409" target="_blank">00:06:49.600</a></span> | <span class="t">end of sequence, which is like this, the unknown token, which with it being a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=417" target="_blank">00:06:57.440</a></span> | <span class="t">byte level encoding, you'd hope it doesn't need to use this very much, but it's there anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=424" target="_blank">00:07:04.240</a></span> | <span class="t">And the masking token. So that's everything we need to train our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=437" target="_blank">00:07:17.920</a></span> | <span class="t">Okay. And one thing I do remember is if you train on all of that, all of those files,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=447" target="_blank">00:07:27.120</a></span> | <span class="t">it takes a really very, very long time, which is fine if you're training it overnight or something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=453" target="_blank">00:07:33.040</a></span> | <span class="t">but that's not what we're doing here. So I'm just going to shorten that to the first 100</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=460" target="_blank">00:07:40.640</a></span> | <span class="t">tokens, and maybe I'll train it after this with the full set. Let's see. So I will leave that to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=470" target="_blank">00:07:50.000</a></span> | <span class="t">train for a while and I'll be back when it's done. Okay. So it's finished training our tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=477" target="_blank">00:07:57.920</a></span> | <span class="t">and we can go ahead and actually save it. So I'm going to import OS. I'm just doing this so I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=486" target="_blank">00:08:06.400</a></span> | <span class="t">make a new directory to store the tokenizer files in. And a typical Italian name, or so I've been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=495" target="_blank">00:08:15.440</a></span> | <span class="t">told, is Filiberto, which fits really well with BERT. So this is our Italian BERT model name,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=505" target="_blank">00:08:25.440</a></span> | <span class="t">Filiberto. So that is our new directory. And if we just come over to here, we have this working</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=518" target="_blank">00:08:38.000</a></span> | <span class="t">directory, which is what I'm in. And then we have this new directory, Filiberto, in here. That's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=525" target="_blank">00:08:45.680</a></span> | <span class="t">where we're going to save our tokenizer. So we just write tokenizer, save model. And here we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=533" target="_blank">00:08:53.600</a></span> | <span class="t">see here, we can do save or save model. Save just saves a JSON file with our tokenizer data inside</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=542" target="_blank">00:09:02.160</a></span> | <span class="t">it. But I don't think that's the standard way of doing it. I think this is the way that you want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=549" target="_blank">00:09:09.680</a></span> | <span class="t">to be doing it. And we're saving it as Filiberto, like that. So we'll do that. And we see that we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=559" target="_blank">00:09:19.360</a></span> | <span class="t">get these two new files, vocab.json and merges.txt. Now, if we look over here, we see both of those.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=567" target="_blank">00:09:27.520</a></span> | <span class="t">And these are essentially like two steps of tokenization for our tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=575" target="_blank">00:09:35.280</a></span> | <span class="t">So when we feed text into our tokenizer, it first goes to merges.txt. And in here, we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=586" target="_blank">00:09:46.560</a></span> | <span class="t">characters, words, so on. And they are translated into these tokens. So these are characters on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=596" target="_blank">00:09:56.640</a></span> | <span class="t">right, tokens on the left. So we scroll down. We can see different ones. We can keep going.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=603" target="_blank">00:10:03.360</a></span> | <span class="t">So here, we have zione. That's like, although my Italian's very bad, that is like the English t-i-o-n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=616" target="_blank">00:10:16.080</a></span> | <span class="t">So t-i-o-n. And we would say stuff like attention, right? Italians have the same,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=625" target="_blank">00:10:25.680</a></span> | <span class="t">but they have like attenzione. So that's what we have there. So it's part of a word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=633" target="_blank">00:10:33.840</a></span> | <span class="t">and it's pretty common. And that gets translated into this token here. Now, after that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=641" target="_blank">00:10:41.600</a></span> | <span class="t">our tokenizer moves into vocab.json. And I don't know why it started at the bottom there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=650" target="_blank">00:10:50.800</a></span> | <span class="t">Go to the top. If I clean this up quickly, we can see we have a JSON object. It's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=661" target="_blank">00:11:01.520</a></span> | <span class="t">a dictionary in Python. And we have all of our tokens and the token IDs that they will get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=668" target="_blank">00:11:08.960</a></span> | <span class="t">translated into. So if we scroll down here, we should be able to find, was it VA, I think?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=676" target="_blank">00:11:16.000</a></span> | <span class="t">Okay, so VA, which is our zione into this token here. And then that eventually gets</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=684" target="_blank">00:11:24.640</a></span> | <span class="t">converted into this token ID. So that's our full tokenizer process.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=694" target="_blank">00:11:34.400</a></span> | <span class="t">Just open that file back up. If we wanted to load that, we would do that like we normally would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=700" target="_blank">00:11:40.880</a></span> | <span class="t">with Transformers. So we just write from Transformers, import</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=704" target="_blank">00:11:44.960</a></span> | <span class="t">Roberta. So we're using a Roberta tokenizer here. So Roberta tokenizer. We can use either</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=715" target="_blank">00:11:55.280</a></span> | <span class="t">Roberta tokenizer or the fast version. It's up to you. And we just initialize our tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=723" target="_blank">00:12:03.440</a></span> | <span class="t">Like that. We front pre-trained. And in here, rather than putting a model name from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=732" target="_blank">00:12:12.000</a></span> | <span class="t">HuggingFace website, we would put the path, the local path to our directory, our model directory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=739" target="_blank">00:12:19.680</a></span> | <span class="t">So it's Filiberto for us. And then we can use that to begin encoding text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=749" target="_blank">00:12:29.360</a></span> | <span class="t">So we go, "Ciao, come va," which is like, "Hi, how are you?" If we write that, we can see that we get,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=758" target="_blank">00:12:38.480</a></span> | <span class="t">these are the tokens here. I wonder if we did a 10. So I'll do it. I'll try in a minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=768" target="_blank">00:12:48.800</a></span> | <span class="t">So we have the sort of sequence token here and the sequence token here. So the S and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=778" target="_blank">00:12:58.400</a></span> | <span class="t">D, S like that. So we have those at the sign end of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=784" target="_blank">00:13:04.400</a></span> | <span class="t">sequence. And we can also add padding in there. So padding equals max length. And also max length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=792" target="_blank">00:13:12.960</a></span> | <span class="t">needs to have a value as well. So max length, like 12. And then we get these padding tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=799" target="_blank">00:13:19.760</a></span> | <span class="t">which are the ones. So that's pretty cool. And I just want to, purely out of curiosity than anything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=805" target="_blank">00:13:25.680</a></span> | <span class="t">else. So we have "attenzione." Let's see if we, if that, if we recognize the number there. So no,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=813" target="_blank">00:13:33.280</a></span> | <span class="t">we don't. So I suppose this is probably the full word. In fact, it is. So this is the full token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=822" target="_blank">00:13:42.960</a></span> | <span class="t">here. If we just do this, maybe we will get, I can't remember what number it was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=830" target="_blank">00:13:50.800</a></span> | <span class="t">The 3, 3, 2, 2. Maybe, maybe that's right. I'm not sure. But anyway, that's, that's how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=837" target="_blank">00:13:57.120</a></span> | <span class="t">everything works. So that, that's it for this video. In the next video, we will take a look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=844" target="_blank">00:14:04.720</a></span> | <span class="t">how we can use this tokenizer to build out our input pipeline for training our actual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=JIeAB8vvBQo&t=851" target="_blank">00:14:11.440</a></span> | <span class="t">transformer model. So that's everything and I'll see you in the next one.</span></div></div></body></html>