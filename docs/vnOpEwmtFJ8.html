<html><head><title>Lesson 12 (2019) - Advanced training techniques; ULMFiT from scratch</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Lesson 12 (2019) - Advanced training techniques; ULMFiT from scratch</h2><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8"><img src="https://i.ytimg.com/vi_webp/vnOpEwmtFJ8/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=65">1:5</a> Learner refactor<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=223">3:43</a> Mixup<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=477">7:57</a> Data augmentation<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1120">18:40</a> Label smoothing<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1310">21:50</a> Half precision floating point<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1420">23:40</a> Nvidia Apex<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1455">24:15</a> Loss scale<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1565">26:5</a> Mixups<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1680">28:0</a> ResNet<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1910">31:50</a> Coma Flare<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2175">36:15</a> Res Blocks<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2805">46:45</a> Results<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2915">48:35</a> Transfer learning<br><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3005">50:5</a> Training from scratch<br><br><div style="text-align: left;"><a href="./vnOpEwmtFJ8.html">Whisper Transcript</a> | <a href="./transcript_vnOpEwmtFJ8.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome to lesson 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4" target="_blank">00:00:04.080</a></span> | <span class="t">Wow, we're moving along.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6" target="_blank">00:00:06.560</a></span> | <span class="t">And this is an exciting lesson because it's where we're going to wrap up all the pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=11" target="_blank">00:00:11.400</a></span> | <span class="t">both for computer vision and for NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=14" target="_blank">00:00:14.000</a></span> | <span class="t">And you might be surprised to hear that we're going to wrap up all the pieces for NLP because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=17" target="_blank">00:00:17.760</a></span> | <span class="t">we haven't really done any NLP yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=19" target="_blank">00:00:19.600</a></span> | <span class="t">But actually everything we've done is equally applicable to NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=24" target="_blank">00:00:24.560</a></span> | <span class="t">So there's very little to do to get a state-of-the-art result on IMDB sentiment analysis from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=32" target="_blank">00:00:32.080</a></span> | <span class="t">So that's what we're going to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=34" target="_blank">00:00:34.200</a></span> | <span class="t">Before we do, let's finally finish off this slide we've been going through for three lessons</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=38" target="_blank">00:00:38.960</a></span> | <span class="t">now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=39" target="_blank">00:00:39.960</a></span> | <span class="t">I promised, not promised, that we would get something state-of-the-art on ImageNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=45" target="_blank">00:00:45.180</a></span> | <span class="t">Turns out we did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=46" target="_blank">00:00:46.180</a></span> | <span class="t">So you're going to see that today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=48" target="_blank">00:00:48.000</a></span> | <span class="t">So we're going to finish off, mix up, label smoothing, and resnets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=55" target="_blank">00:00:55.200</a></span> | <span class="t">Okay, so let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=58" target="_blank">00:00:58.600</a></span> | <span class="t">Before we look at the new stuff, 09B learner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=66" target="_blank">00:01:06.840</a></span> | <span class="t">I've made a couple of minor changes that I thought you might be interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=71" target="_blank">00:01:11.240</a></span> | <span class="t">It's kind of like as you refactor things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=72" target="_blank">00:01:12.680</a></span> | <span class="t">So remember last week we refactored the learner to get rid of that awful separate runner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=77" target="_blank">00:01:17.280</a></span> | <span class="t">So there's just now one thing, made a lot of our code a lot easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=80" target="_blank">00:01:20.280</a></span> | <span class="t">There's still this concept left behind that when you started fitting, you had to tell</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=85" target="_blank">00:01:25.320</a></span> | <span class="t">each callback what its learner or runner was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=88" target="_blank">00:01:28.400</a></span> | <span class="t">I've moved that, because they're all totally attached now, I've moved that to the init.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=96" target="_blank">00:01:36.240</a></span> | <span class="t">And so now you can call add cbs to add a whole bunch of callbacks, or add cb to add one callback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=102" target="_blank">00:01:42.600</a></span> | <span class="t">And that happens automatically at the start of training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=105" target="_blank">00:01:45.220</a></span> | <span class="t">That's a very minor thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=106" target="_blank">00:01:46.960</a></span> | <span class="t">More interesting was when I did this little reformatting exercise where I took all these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=113" target="_blank">00:01:53.920</a></span> | <span class="t">callbacks that used to be on the line underneath the thing before them and lined them up over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=118" target="_blank">00:01:58.360</a></span> | <span class="t">here and suddenly realized that now I can answer all the questions I have in my head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=123" target="_blank">00:02:03.120</a></span> | <span class="t">about our callback system, which is what exactly are the steps in the training loop?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=129" target="_blank">00:02:09.440</a></span> | <span class="t">What exactly are the callbacks that you can use in the training loop?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=133" target="_blank">00:02:13.600</a></span> | <span class="t">Which step goes with which callback?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=137" target="_blank">00:02:17.000</a></span> | <span class="t">Which steps don't have a callback?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=138" target="_blank">00:02:18.760</a></span> | <span class="t">Are there any callbacks that don't have a step?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=142" target="_blank">00:02:22.200</a></span> | <span class="t">So it's one of these interesting things where I really don't like the idea of automating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=150" target="_blank">00:02:30.620</a></span> | <span class="t">your formatting and creating rules for formatting when something like this can just, as soon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=155" target="_blank">00:02:35.240</a></span> | <span class="t">as I did this, I understood my code better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=158" target="_blank">00:02:38.560</a></span> | <span class="t">And for me, understanding my code is the only way to make it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=161" target="_blank">00:02:41.840</a></span> | <span class="t">Because debugging machine learning code is awful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=165" target="_blank">00:02:45.920</a></span> | <span class="t">So you've got to make sure that the thing you write makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=168" target="_blank">00:02:48.720</a></span> | <span class="t">It's got to be simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=169" target="_blank">00:02:49.720</a></span> | <span class="t">It's got to be really simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=170" target="_blank">00:02:50.720</a></span> | <span class="t">So this is really simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=176" target="_blank">00:02:56.440</a></span> | <span class="t">Then more interestingly, we used to create the optimizer in init.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=182" target="_blank">00:03:02.680</a></span> | <span class="t">And you could actually pass in an already created optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=185" target="_blank">00:03:05.640</a></span> | <span class="t">I removed that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=186" target="_blank">00:03:06.960</a></span> | <span class="t">And the only thing now you can pass in is an optimization function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=190" target="_blank">00:03:10.040</a></span> | <span class="t">So something that will create an optimizer, which is what we've always been doing anyway.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=194" target="_blank">00:03:14.060</a></span> | <span class="t">And by doing that, we can now create our optimizer when we start fitting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=199" target="_blank">00:03:19.120</a></span> | <span class="t">And that turns out to be really important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=201" target="_blank">00:03:21.560</a></span> | <span class="t">Because when we do things like discriminative learning rates and gradual unfreezing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=205" target="_blank">00:03:25.080</a></span> | <span class="t">layer groups and stuff, we can change things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=207" target="_blank">00:03:27.720</a></span> | <span class="t">And then when we fit, it will all just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=210" target="_blank">00:03:30.440</a></span> | <span class="t">So that's a more significant -- it's like one line of code, but it's conceptually a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=214" target="_blank">00:03:34.200</a></span> | <span class="t">very significant change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=216" target="_blank">00:03:36.040</a></span> | <span class="t">Okay. So that's some minor changes to 9B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=222" target="_blank">00:03:42.760</a></span> | <span class="t">And now let's move on to mixup and label smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=226" target="_blank">00:03:46.720</a></span> | <span class="t">So I'm really excited about the stuff we saw at the end of the last lesson where we saw</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=232" target="_blank">00:03:52.120</a></span> | <span class="t">how we can use the GPU to do data augmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=235" target="_blank">00:03:55.440</a></span> | <span class="t">Fully randomized, fully GPU accelerated data augmentation using just plain PyTorch operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=241" target="_blank">00:04:01.680</a></span> | <span class="t">I think that's a big win.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=244" target="_blank">00:04:04.560</a></span> | <span class="t">But it's quite possible we don't need that kind of data augmentation anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=250" target="_blank">00:04:10.680</a></span> | <span class="t">Because in our experimentation with this data augmentation called mixup, we found we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=255" target="_blank">00:04:15.600</a></span> | <span class="t">remove most other data augmentation and get amazingly good results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=260" target="_blank">00:04:20.960</a></span> | <span class="t">So it's just a kind of a simplicity result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=263" target="_blank">00:04:23.760</a></span> | <span class="t">And also when you use mixup, you can train for a really long time and get really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=267" target="_blank">00:04:27.760</a></span> | <span class="t">results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=269" target="_blank">00:04:29.200</a></span> | <span class="t">So let me show you mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=271" target="_blank">00:04:31.040</a></span> | <span class="t">And in terms of the results, you can get -- what happened in the bag of tricks paper was they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=278" target="_blank">00:04:38.920</a></span> | <span class="t">-- when they turned mixup on, they also started training for 200 epochs instead of 120.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=288" target="_blank">00:04:48.480</a></span> | <span class="t">So be a bit careful when you interpret their paper table when it goes from label smoothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=296" target="_blank">00:04:56.520</a></span> | <span class="t">94.1 to mixup without distillation 94.6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=304" target="_blank">00:05:04.200</a></span> | <span class="t">They're also nearly doubling the number of epochs they do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=307" target="_blank">00:05:07.120</a></span> | <span class="t">But you can kind of get a sense that you can get big decrease in error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=312" target="_blank">00:05:12.640</a></span> | <span class="t">The other thing they mention in the paper is distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=315" target="_blank">00:05:15.120</a></span> | <span class="t">I'm not going to talk about that because it's a thing where you pre-train some much bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=319" target="_blank">00:05:19.880</a></span> | <span class="t">model like a ResNet-152, and then you try and train something that predicts the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=323" target="_blank">00:05:23.960</a></span> | <span class="t">of that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=324" target="_blank">00:05:24.960</a></span> | <span class="t">The idea of training a really big model, to train a smaller model, it's interesting, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=329" target="_blank">00:05:29.560</a></span> | <span class="t">it's not exactly training in the way I normally think about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=332" target="_blank">00:05:32.440</a></span> | <span class="t">So we're not looking at distillation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=333" target="_blank">00:05:33.800</a></span> | <span class="t">It would be an interesting assignment if somebody wanted to try adding it to the notebooks though.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=339" target="_blank">00:05:39.680</a></span> | <span class="t">You have all the information and I think all the skills you need to do that now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=343" target="_blank">00:05:43.280</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=345" target="_blank">00:05:45.740</a></span> | <span class="t">So mixup, we start by grabbing our ImageNet data set and we grab the MakeRGB and resize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=355" target="_blank">00:05:55.960</a></span> | <span class="t">and turn it into a float tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=358" target="_blank">00:05:58.360</a></span> | <span class="t">This is just our quick and dirty resize, we're already doing this for testing purposes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=362" target="_blank">00:06:02.000</a></span> | <span class="t">Split it up, create a data bunch, all the normal stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=367" target="_blank">00:06:07.060</a></span> | <span class="t">But what we're going to do is we're going to take an image like this and an image like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=375" target="_blank">00:06:15.080</a></span> | <span class="t">this and we're going to combine them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=379" target="_blank">00:06:19.560</a></span> | <span class="t">We're going to take 0.3 times this image plus 0.7 times this image and this is what it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=387" target="_blank">00:06:27.840</a></span> | <span class="t">going to look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=389" target="_blank">00:06:29.620</a></span> | <span class="t">Unfortunately, Silva and I have different orderings of file names on our thing, so I wrote, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=395" target="_blank">00:06:35.200</a></span> | <span class="t">a French horn and a tench but actually Silva clearly doesn't have French horn or tenches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=399" target="_blank">00:06:39.200</a></span> | <span class="t">but you get the idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=400" target="_blank">00:06:40.520</a></span> | <span class="t">It's a mixup of two different images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=402" target="_blank">00:06:42.880</a></span> | <span class="t">So we're going to create a greater augmentation where every time we predict something we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=409" target="_blank">00:06:49.200</a></span> | <span class="t">going to be predicting a mix of two things like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=412" target="_blank">00:06:52.160</a></span> | <span class="t">So we're going to both take the linear combination, 0.3 and 0.7, of the two images but then we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=421" target="_blank">00:07:01.080</a></span> | <span class="t">going to have to do that for the labels as well, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=423" target="_blank">00:07:03.520</a></span> | <span class="t">There's no point predicting the one hot encoded output of this breed of doggy where there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=431" target="_blank">00:07:11.500</a></span> | <span class="t">also a bit of a gas pump.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=434" target="_blank">00:07:14.000</a></span> | <span class="t">So we're also going to have, we're not going to have one hot encoded output, we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=437" target="_blank">00:07:17.560</a></span> | <span class="t">to have a 0.7 encoded doggy and a 0.3 encoded gas pump.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=444" target="_blank">00:07:24.480</a></span> | <span class="t">So that's the basic idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=448" target="_blank">00:07:28.340</a></span> | <span class="t">So the mixup paper was super cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=454" target="_blank">00:07:34.060</a></span> | <span class="t">Wow, there are people talking about things that aren't deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=460" target="_blank">00:07:40.720</a></span> | <span class="t">I guess that's their priorities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=466" target="_blank">00:07:46.640</a></span> | <span class="t">So the paper's a pretty nice, easy read by paper standards and I would definitely suggest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=472" target="_blank">00:07:52.340</a></span> | <span class="t">you check it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=477" target="_blank">00:07:57.560</a></span> | <span class="t">So I've told you what we're going to do, implementation-wise, we have to decide what number to use here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=483" target="_blank">00:08:03.360</a></span> | <span class="t">Is it 0.3 or 0.1 or 0.5 or what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=486" target="_blank">00:08:06.600</a></span> | <span class="t">And this is a data augmentation method, so the answer is we'll randomize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=490" target="_blank">00:08:10.560</a></span> | <span class="t">But we're not going to randomize it from 0 to 1 uniform or 0 to 0.5 uniform, but instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=496" target="_blank">00:08:16.960</a></span> | <span class="t">we're going to randomize it using shapes like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=502" target="_blank">00:08:22.240</a></span> | <span class="t">In other words, when we grab a random number, most of the time it'll be really close to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=507" target="_blank">00:08:27.760</a></span> | <span class="t">0 or really close to 1, and just occasionally it'll be close to 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=511" target="_blank">00:08:31.680</a></span> | <span class="t">So that way most of the time it'll be pretty easy for our model because it'll be predicting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=515" target="_blank">00:08:35.960</a></span> | <span class="t">one and only one thing, and just occasionally it'll be predicting something that's a pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=521" target="_blank">00:08:41.080</a></span> | <span class="t">evenly mixed combination.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=523" target="_blank">00:08:43.360</a></span> | <span class="t">So the ability to grab random numbers, that this is basically the histogram, the smoothed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=530" target="_blank">00:08:50.120</a></span> | <span class="t">histogram of how often we're going to see those numbers, is called sampling from a probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=537" target="_blank">00:08:57.880</a></span> | <span class="t">distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=538" target="_blank">00:08:58.880</a></span> | <span class="t">And basically in nearly all these cases you can start with a uniform random number or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=543" target="_blank">00:09:03.320</a></span> | <span class="t">a normal random number and put it through some kind of function or process to turn it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=547" target="_blank">00:09:07.780</a></span> | <span class="t">into something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=548" target="_blank">00:09:08.780</a></span> | <span class="t">So the details don't matter at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=553" target="_blank">00:09:13.240</a></span> | <span class="t">But the paper points out that this particular shape is nicely characterized by something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=557" target="_blank">00:09:17.240</a></span> | <span class="t">called the beta distribution, so that's what we're going to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=561" target="_blank">00:09:21.360</a></span> | <span class="t">So it was interesting drawing these because it requires a few interesting bits of math,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=569" target="_blank">00:09:29.800</a></span> | <span class="t">which some of you may be less comfortable with or entirely uncomfortable with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=575" target="_blank">00:09:35.480</a></span> | <span class="t">For me, every time I see this function, which is called the gamma function, I kind of break</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=582" target="_blank">00:09:42.480</a></span> | <span class="t">out in sweats, not just because I've got a cold, but it's like the idea of functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=587" target="_blank">00:09:47.240</a></span> | <span class="t">that I don't-- like how do you describe this thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=590" target="_blank">00:09:50.400</a></span> | <span class="t">But actually, it turns out that like most things, once you look at it, it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=596" target="_blank">00:09:56.080</a></span> | <span class="t">pretty straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=597" target="_blank">00:09:57.080</a></span> | <span class="t">And we're going to be using this function, so I'll just quickly explain what's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=599" target="_blank">00:09:59.520</a></span> | <span class="t">on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=600" target="_blank">00:10:00.520</a></span> | <span class="t">We're going to start with a factorial function, so 1 times 2 times 3 times 4, whatever, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=606" target="_blank">00:10:06.960</a></span> | <span class="t">And here these red dots is just the value of the factorial function for a few different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=611" target="_blank">00:10:11.720</a></span> | <span class="t">places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=614" target="_blank">00:10:14.720</a></span> | <span class="t">But don't think of the factorial function as being 1 times 2 times 3 times 4, or times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=620" target="_blank">00:10:20.360</a></span> | <span class="t">n, whatever, but divide both sides by n, and now you've got-- or divide both sides by n,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=628" target="_blank">00:10:28.720</a></span> | <span class="t">and now you've got like factorial n divided by n equals 1 times 2 times 3, so it equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=637" target="_blank">00:10:37.120</a></span> | <span class="t">the factorial of n minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=639" target="_blank">00:10:39.960</a></span> | <span class="t">And so when you define it like that, you suddenly realize there's no reason that you kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=643" target="_blank">00:10:43.440</a></span> | <span class="t">have a function that's not just on the integers-- not just on the integers, but is everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=649" target="_blank">00:10:49.120</a></span> | <span class="t">This is the point where I stop with the math, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=651" target="_blank">00:10:51.000</a></span> | <span class="t">Because to me, if I need a sine function, or a log function, or an x-punk fin, or whatever,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=655" target="_blank">00:10:55.040</a></span> | <span class="t">I type it into my computer and I get it, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=657" target="_blank">00:10:57.140</a></span> | <span class="t">So the actual how you get it is not at all important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=660" target="_blank">00:11:00.480</a></span> | <span class="t">But the fact of knowing what these functions are and how they're defined is useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=665" target="_blank">00:11:05.680</a></span> | <span class="t">PyTorch doesn't have this function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=667" target="_blank">00:11:07.640</a></span> | <span class="t">Weirdly enough, they have a log gamma function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=669" target="_blank">00:11:09.760</a></span> | <span class="t">So we can take log gamma and go e to the power of that to get a gamma function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=673" target="_blank">00:11:13.480</a></span> | <span class="t">And you'll see here, I am breaking my no Greek letters rule.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=677" target="_blank">00:11:17.880</a></span> | <span class="t">And the reason I'm breaking that rule is because a function like this doesn't have a kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=684" target="_blank">00:11:24.200</a></span> | <span class="t">domain-specific meaning, or a pure physical analogy, which is how we always think about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=690" target="_blank">00:11:30.360</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=691" target="_blank">00:11:31.360</a></span> | <span class="t">It's just a math function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=692" target="_blank">00:11:32.560</a></span> | <span class="t">And so we call it gamma, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=694" target="_blank">00:11:34.480</a></span> | <span class="t">And so if you're going to call it gamma, you may as well write it like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=698" target="_blank">00:11:38.280</a></span> | <span class="t">And why this matters is when you start using it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=702" target="_blank">00:11:42.400</a></span> | <span class="t">Like look at the difference between writing it out with the actual Unicode and operators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=709" target="_blank">00:11:49.280</a></span> | <span class="t">versus what would happen if you wrote it out long form in Python.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=714" target="_blank">00:11:54.400</a></span> | <span class="t">Like when you're comparing something to a paper, you want something that you can look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=717" target="_blank">00:11:57.920</a></span> | <span class="t">at and straight away say like, oh, that looks very familiar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=722" target="_blank">00:12:02.080</a></span> | <span class="t">And as long as it's not familiar, you might want to think about how to make it more familiar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=726" target="_blank">00:12:06.160</a></span> | <span class="t">So I just briefly mentioned that writing these math symbols nowadays is actually pretty easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=732" target="_blank">00:12:12.480</a></span> | <span class="t">On Linux, there's a thing called a compose key which is probably already set up for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=736" target="_blank">00:12:16.880</a></span> | <span class="t">And if you Google it, you can learn how to turn it on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=738" target="_blank">00:12:18.800</a></span> | <span class="t">And it's basically like you'll press like the right alt button or the caps lock button.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=742" target="_blank">00:12:22.400</a></span> | <span class="t">You can choose what your compose key is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=744" target="_blank">00:12:24.360</a></span> | <span class="t">And then a few more letters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=746" target="_blank">00:12:26.080</a></span> | <span class="t">So for example, all the Greek letters are compose and then star, and then the English</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=750" target="_blank">00:12:30.000</a></span> | <span class="t">letter that corresponds with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=751" target="_blank">00:12:31.560</a></span> | <span class="t">So for example, if I want to do lambda, I would go composed L. So it's just as quick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=758" target="_blank">00:12:38.200</a></span> | <span class="t">as typing non Unicode characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=760" target="_blank">00:12:40.920</a></span> | <span class="t">Most of the Greek letters are available on a Mac keyboard just with option.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=763" target="_blank">00:12:43.760</a></span> | <span class="t">Unfortunately, nobody's created a decent compose key for Mac yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=766" target="_blank">00:12:46.960</a></span> | <span class="t">There's a great compose key for Windows called win compose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=769" target="_blank">00:12:49.920</a></span> | <span class="t">Anybody who's working with, you know, Greek letters should definitely install and learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=775" target="_blank">00:12:55.120</a></span> | <span class="t">to use these things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=778" target="_blank">00:12:58.360</a></span> | <span class="t">So there's our gamma function nice and concise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=781" target="_blank">00:13:01.520</a></span> | <span class="t">It looks exactly like the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=783" target="_blank">00:13:03.320</a></span> | <span class="t">And so it turns out that this is how you calculate the value of the beta function, which is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=787" target="_blank">00:13:07.080</a></span> | <span class="t">beta distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=788" target="_blank">00:13:08.160</a></span> | <span class="t">And so now here it is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=789" target="_blank">00:13:09.160</a></span> | <span class="t">So as I said, the details aren't important, but they're the tools that you can use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=793" target="_blank">00:13:13.120</a></span> | <span class="t">The basic idea is that we now have something where we can pick some parameter, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=797" target="_blank">00:13:17.720</a></span> | <span class="t">called alpha, where if it's high, then it's much more likely that we get a equal mix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=805" target="_blank">00:13:25.040</a></span> | <span class="t">And if it's low, it's very unlikely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=807" target="_blank">00:13:27.200</a></span> | <span class="t">And this is really important because for data augmentation, we need to be able to tune a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=810" target="_blank">00:13:30.240</a></span> | <span class="t">lever that says how much regularization am I doing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=814" target="_blank">00:13:34.040</a></span> | <span class="t">How much augmentation am I doing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=815" target="_blank">00:13:35.400</a></span> | <span class="t">So you can move your alpha up and down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=817" target="_blank">00:13:37.920</a></span> | <span class="t">And the reason it's important to be able to print these plots out is that when you change</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=821" target="_blank">00:13:41.560</a></span> | <span class="t">your alpha, you want to plot it out and see what it looks like, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=825" target="_blank">00:13:45.000</a></span> | <span class="t">Make sure it looks sensible, okay?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=829" target="_blank">00:13:49.600</a></span> | <span class="t">So it turns out that all we need to do then is we don't actually have to 0.7 hot encode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=839" target="_blank">00:13:59.640</a></span> | <span class="t">one thing and 0.3 hot encode another thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=842" target="_blank">00:14:02.360</a></span> | <span class="t">It's actually identical to simply go, I guess it is lambda times the first loss plus 1 minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=851" target="_blank">00:14:11.760</a></span> | <span class="t">lambda times the second loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=852" target="_blank">00:14:12.920</a></span> | <span class="t">I guess we're using t here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=855" target="_blank">00:14:15.200</a></span> | <span class="t">So that's actually all we need to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=858" target="_blank">00:14:18.160</a></span> | <span class="t">So this is our mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=863" target="_blank">00:14:23.160</a></span> | <span class="t">And again, as you can see, we're using the same letters that we'd expect to see in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=865" target="_blank">00:14:25.920</a></span> | <span class="t">paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=866" target="_blank">00:14:26.920</a></span> | <span class="t">So everything should look very familiar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=868" target="_blank">00:14:28.720</a></span> | <span class="t">And mixup, remember, is something which is going to change our loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=873" target="_blank">00:14:33.400</a></span> | <span class="t">So we need to know what loss function to change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=875" target="_blank">00:14:35.680</a></span> | <span class="t">So when you begin fitting, you find out what the old loss function on the learner was when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=882" target="_blank">00:14:42.200</a></span> | <span class="t">you store it away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=883" target="_blank">00:14:43.720</a></span> | <span class="t">And then when we calculate loss, we can just go ahead and say, oh, if it's invalidation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=889" target="_blank">00:14:49.680</a></span> | <span class="t">there's no mixup involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=891" target="_blank">00:14:51.300</a></span> | <span class="t">And if we're training, then we'll calculate the loss on two different sets of images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=897" target="_blank">00:14:57.400</a></span> | <span class="t">One is just the regular set, and the second is we'll grab all the other images and randomly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=904" target="_blank">00:15:04.120</a></span> | <span class="t">permute one and randomly pick one to share with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=908" target="_blank">00:15:08.720</a></span> | <span class="t">So we do that for the image, and we do that for the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=913" target="_blank">00:15:13.940</a></span> | <span class="t">And that's basically it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=917" target="_blank">00:15:17.480</a></span> | <span class="t">Couple of minor things to mention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=919" target="_blank">00:15:19.160</a></span> | <span class="t">In the last lesson, I created an EWMA function, Exponentially Weighted Moving Average Function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=925" target="_blank">00:15:25.880</a></span> | <span class="t">which is a really dumb name for it, because actually it was just a linear combination</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=930" target="_blank">00:15:30.000</a></span> | <span class="t">of two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=931" target="_blank">00:15:31.000</a></span> | <span class="t">It was like V times alpha plus V1 times alpha plus V2 times 1 minus alpha.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=938" target="_blank">00:15:38.280</a></span> | <span class="t">You create exponentially weighted moving averages with it by applying it multiple times, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=942" target="_blank">00:15:42.600</a></span> | <span class="t">the actual function is a linear combination, so I've renamed that to linear combination,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=946" target="_blank">00:15:46.880</a></span> | <span class="t">and you'll see that so many places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=948" target="_blank">00:15:48.240</a></span> | <span class="t">So this mixup is a linear combination of our actual images and some randomly permuted images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=955" target="_blank">00:15:55.440</a></span> | <span class="t">in that mini-batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=957" target="_blank">00:15:57.640</a></span> | <span class="t">And our loss is a linear combination of the loss of our two different parts, our normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=964" target="_blank">00:16:04.320</a></span> | <span class="t">mini-batch and our randomly permuted mini-batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=966" target="_blank">00:16:06.800</a></span> | <span class="t">One of the nice things about this is if you think about it, this is all being applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=969" target="_blank">00:16:09.840</a></span> | <span class="t">on the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=971" target="_blank">00:16:11.380</a></span> | <span class="t">So this is pretty much instant.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=974" target="_blank">00:16:14.160</a></span> | <span class="t">So super powerful augmentation system, which isn't going to add any overhead to our code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=982" target="_blank">00:16:22.480</a></span> | <span class="t">One thing to be careful of is that we're actually replacing the loss function, and loss functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=990" target="_blank">00:16:30.960</a></span> | <span class="t">have something called a reduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=994" target="_blank">00:16:34.280</a></span> | <span class="t">And most PyTorch loss functions, you can say, after calculating the loss function for everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=999" target="_blank">00:16:39.400</a></span> | <span class="t">in the mini-batch, either return a rank 1 tensor of all of the loss functions for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1004" target="_blank">00:16:44.880</a></span> | <span class="t">mini-batch, or add them all up, or take the average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1009" target="_blank">00:16:49.200</a></span> | <span class="t">We pretty much always take the average.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1010" target="_blank">00:16:50.200</a></span> | <span class="t">But we just have to make sure that we do the right thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1013" target="_blank">00:16:53.800</a></span> | <span class="t">So I've just got a little function here that does the mean or sum, or nothing at all, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1018" target="_blank">00:16:58.400</a></span> | <span class="t">requested.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1019" target="_blank">00:16:59.960</a></span> | <span class="t">And so then we need to make sure that we create our new loss function, that at the end, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1027" target="_blank">00:17:07.240</a></span> | <span class="t">going to reduce it in the way that they actually asked for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1031" target="_blank">00:17:11.400</a></span> | <span class="t">But then we have to turn off the reduction when we actually do mixup, because we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1036" target="_blank">00:17:16.640</a></span> | <span class="t">need to calculate the loss on every image for both halves of our mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1043" target="_blank">00:17:23.280</a></span> | <span class="t">So this is a good place to use a context manager, which we've seen before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1047" target="_blank">00:17:27.640</a></span> | <span class="t">So we just created a tiny little context manager, which will just find out what the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1052" target="_blank">00:17:32.020</a></span> | <span class="t">reduction was, save it away, get rid of it, and then put it back when it's finished.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1058" target="_blank">00:17:38.420</a></span> | <span class="t">So there's a lot of minor details there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1061" target="_blank">00:17:41.280</a></span> | <span class="t">But with that in place, the actual mixup itself is very little code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1064" target="_blank">00:17:44.600</a></span> | <span class="t">It's a single callback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1066" target="_blank">00:17:46.320</a></span> | <span class="t">And we can then run it in the usual way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1069" target="_blank">00:17:49.440</a></span> | <span class="t">Just add mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1072" target="_blank">00:17:52.440</a></span> | <span class="t">Our default alpha here is 0.4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1075" target="_blank">00:17:55.560</a></span> | <span class="t">And I've been mainly playing with alpha at 0.2, so this is a bit more than I'm used to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1079" target="_blank">00:17:59.380</a></span> | <span class="t">But somewhere around that vicinity is pretty normal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1084" target="_blank">00:18:04.500</a></span> | <span class="t">So that's mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1085" target="_blank">00:18:05.660</a></span> | <span class="t">And that's like-- it's really interesting, because you could use this for layers other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1093" target="_blank">00:18:13.320</a></span> | <span class="t">than the input layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1095" target="_blank">00:18:15.020</a></span> | <span class="t">You could use it on the first layer, maybe with the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1098" target="_blank">00:18:18.040</a></span> | <span class="t">So you could do mixup augmentation in NLP, for instance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1103" target="_blank">00:18:23.920</a></span> | <span class="t">That's something which people haven't really dug into deeply yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1108" target="_blank">00:18:28.000</a></span> | <span class="t">But it seems to be an opportunity to add augmentation in many places where we don't really see it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1114" target="_blank">00:18:34.880</a></span> | <span class="t">at the moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1115" target="_blank">00:18:35.880</a></span> | <span class="t">Which means we can train better models with less data, which is why we're here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1121" target="_blank">00:18:41.680</a></span> | <span class="t">So here's a problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1122" target="_blank">00:18:42.760</a></span> | <span class="t">How does Softmax interact with this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1125" target="_blank">00:18:45.440</a></span> | <span class="t">So now we've drawn some random number lambda.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1129" target="_blank">00:18:49.120</a></span> | <span class="t">It's 0.7.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1130" target="_blank">00:18:50.120</a></span> | <span class="t">So I've got 0.7 of a dog and 0.3 of a gas station.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1133" target="_blank">00:18:53.640</a></span> | <span class="t">And the correct answer would be a rank one tensor which has 0.7 in one spot and 0.3 in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1141" target="_blank">00:19:01.520</a></span> | <span class="t">the other spot and 0 everywhere else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1144" target="_blank">00:19:04.840</a></span> | <span class="t">Softmax isn't going to want to do that for me, because Softmax really wants just one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1148" target="_blank">00:19:08.400</a></span> | <span class="t">of my values to be high, because it's got an e to the top, as we've talked about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1154" target="_blank">00:19:14.860</a></span> | <span class="t">So we-- to really use mixup well-- and not just use mixup well, but any time your data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1161" target="_blank">00:19:21.560</a></span> | <span class="t">is-- the labels on the data, you're not 100% sure they're correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1165" target="_blank">00:19:25.720</a></span> | <span class="t">You don't want to be asking your model to predict one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1170" target="_blank">00:19:30.320</a></span> | <span class="t">You want to be-- don't predict, I'm 100% sure it's this label, because you've got label</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1175" target="_blank">00:19:35.040</a></span> | <span class="t">noise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1176" target="_blank">00:19:36.040</a></span> | <span class="t">You've got incorrect labels, or you've got mixup, mixing, or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1178" target="_blank">00:19:38.960</a></span> | <span class="t">So instead, we say, oh, don't use one hot encoding for the dependent variable, but use a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1185" target="_blank">00:19:45.880</a></span> | <span class="t">bit less than one hot encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1187" target="_blank">00:19:47.640</a></span> | <span class="t">So say 0.9 hot encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1190" target="_blank">00:19:50.000</a></span> | <span class="t">So then the correct answer is to say, I'm 90% sure this is the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1194" target="_blank">00:19:54.720</a></span> | <span class="t">And then all of your probabilities have to add to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1197" target="_blank">00:19:57.360</a></span> | <span class="t">So then all of the negatives, you just put 0.1 divided by n minus one, and all the rest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1203" target="_blank">00:20:03.080</a></span> | <span class="t">And that's called label smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1205" target="_blank">00:20:05.640</a></span> | <span class="t">And it's a really simple but astonishingly effective way to handle noisy labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1213" target="_blank">00:20:13.600</a></span> | <span class="t">I keep on hearing people saying, oh, we can't use deep learning in this medical problem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1220" target="_blank">00:20:20.560</a></span> | <span class="t">because the diagnostic labels in the reports are not perfect, and we don't have a gold</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1224" target="_blank">00:20:24.980</a></span> | <span class="t">standard and whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1226" target="_blank">00:20:26.980</a></span> | <span class="t">It actually turns out that particularly if you lose label smoothing, noisy data is generally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1231" target="_blank">00:20:31.400</a></span> | <span class="t">not an option.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1232" target="_blank">00:20:32.400</a></span> | <span class="t">Like, there's plenty of examples of people using this where they literally randomly permute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1238" target="_blank">00:20:38.440</a></span> | <span class="t">half the labels to make them like 50% wrong, and they still get good results, really good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1242" target="_blank">00:20:42.680</a></span> | <span class="t">results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1244" target="_blank">00:20:44.160</a></span> | <span class="t">So don't listen to people in your organization saying, we can't start modeling until we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1250" target="_blank">00:20:50.880</a></span> | <span class="t">all this cleanup work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1252" target="_blank">00:20:52.900</a></span> | <span class="t">Start modeling right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1253" target="_blank">00:20:53.900</a></span> | <span class="t">See if the results are OK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1256" target="_blank">00:20:56.200</a></span> | <span class="t">And if they are, then maybe you can skip all the cleanup work or do them simultaneously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1261" target="_blank">00:21:01.360</a></span> | <span class="t">So label smoothing ends up just being the cross entropy loss as before times if epsilon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1269" target="_blank">00:21:09.960</a></span> | <span class="t">is 0.1 and 0.9 plus 0.1 times the cross entropy for everything divided by n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1277" target="_blank">00:21:17.800</a></span> | <span class="t">And the nice thing is that's another linear combination.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1281" target="_blank">00:21:21.040</a></span> | <span class="t">So once you kind of create one of these little mathematical refactorings that tend to pop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1284" target="_blank">00:21:24.520</a></span> | <span class="t">up everywhere and make your code a little bit easier to read and a little bit harder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1288" target="_blank">00:21:28.760</a></span> | <span class="t">to stuff up, every time I have to write a piece of code, there's a very high probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1293" target="_blank">00:21:33.520</a></span> | <span class="t">that I'm going to screw it up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1294" target="_blank">00:21:34.860</a></span> | <span class="t">So the less I have to write, the less debugging I'm going to have to do later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1299" target="_blank">00:21:39.540</a></span> | <span class="t">So we can just pop that in as a loss function and away we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1306" target="_blank">00:21:46.400</a></span> | <span class="t">So that's a super powerful technique which has been around for a couple of years, those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1313" target="_blank">00:21:53.400</a></span> | <span class="t">two techniques, but not nearly as widely used as they should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1318" target="_blank">00:21:58.840</a></span> | <span class="t">Then if you're using a Volta, Tensor Core, 2080, any kind of pretty much any current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1326" target="_blank">00:22:06.120</a></span> | <span class="t">generation Nvidia graphics card, you can train using half precision floating point in theory</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1333" target="_blank">00:22:13.680</a></span> | <span class="t">like 10 times faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1336" target="_blank">00:22:16.040</a></span> | <span class="t">In practice it doesn't quite work out that way because there's other things going on,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1338" target="_blank">00:22:18.920</a></span> | <span class="t">but we certainly often see 3x speedups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1342" target="_blank">00:22:22.880</a></span> | <span class="t">So the other thing we've got is some work here to allow you to train in half precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1348" target="_blank">00:22:28.600</a></span> | <span class="t">floating point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1350" target="_blank">00:22:30.560</a></span> | <span class="t">Now the reason it's not as simple as saying model.half, which would convert all of your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1355" target="_blank">00:22:35.080</a></span> | <span class="t">weights and biases and everything to half precision floating point, is because of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1361" target="_blank">00:22:41.200</a></span> | <span class="t">This is from Nvidia's materials and what they point out is that you can't just use half</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1368" target="_blank">00:22:48.840</a></span> | <span class="t">precision everywhere because it's not accurate, it's bumpy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1373" target="_blank">00:22:53.800</a></span> | <span class="t">So it's hard to get good useful gradients if you do everything in half precision, particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1379" target="_blank">00:22:59.680</a></span> | <span class="t">often things will round off to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1382" target="_blank">00:23:02.300</a></span> | <span class="t">So instead what we do is we do the forward pass in FP16, we do the backward pass in FP16,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1389" target="_blank">00:23:09.980</a></span> | <span class="t">so all the hard work is done in half precision floating point, and pretty much everywhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1394" target="_blank">00:23:14.800</a></span> | <span class="t">else we convert things to full precision floating point and do everything else in full precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1400" target="_blank">00:23:20.680</a></span> | <span class="t">So for example, when we actually apply the gradients by multiplying the value of the learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1403" target="_blank">00:23:23.800</a></span> | <span class="t">rate, we do that in FP32, single precision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1408" target="_blank">00:23:28.780</a></span> | <span class="t">And that means that if your learning rate's really small, in FP16 it might basically round</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1414" target="_blank">00:23:34.960</a></span> | <span class="t">down to zero, so we do it in FP32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1420" target="_blank">00:23:40.280</a></span> | <span class="t">In FastAI version one, we wrote all this by hand.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1424" target="_blank">00:23:44.720</a></span> | <span class="t">For the lessons, we're experimenting with using a library from Nvidia called Apex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1430" target="_blank">00:23:50.120</a></span> | <span class="t">Apex basically have some of the functions to do this there for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1435" target="_blank">00:23:55.520</a></span> | <span class="t">So we're using it here, and basically you can see there's a thing called model to half</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1441" target="_blank">00:24:01.380</a></span> | <span class="t">where we just go model to half, batch norm, goes to float, and so forth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1445" target="_blank">00:24:05.700</a></span> | <span class="t">So these are not particularly interesting, but they're just going through each one and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1449" target="_blank">00:24:09.480</a></span> | <span class="t">making sure that the right layers have the right types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1453" target="_blank">00:24:13.160</a></span> | <span class="t">So once we've got those kind of utility functions in place, the actual callback's really quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1460" target="_blank">00:24:20.120</a></span> | <span class="t">small and you'll be able to map every stage to that picture I showed you before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1466" target="_blank">00:24:26.520</a></span> | <span class="t">So you'll be able to see when we start fitting, we convert the network to half-precision floating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1471" target="_blank">00:24:31.640</a></span> | <span class="t">point, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1473" target="_blank">00:24:33.240</a></span> | <span class="t">One of the things that's kind of interesting is there's something here called loss scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1478" target="_blank">00:24:38.920</a></span> | <span class="t">After the backward pass, well probably more interestingly, after the loss is calculated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1488" target="_blank">00:24:48.080</a></span> | <span class="t">we multiply it by this number called loss scale, which is generally something around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1491" target="_blank">00:24:51.480</a></span> | <span class="t">512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1492" target="_blank">00:24:52.640</a></span> | <span class="t">The reason we do that is that losses tend to be pretty small in a region where half-precision</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1497" target="_blank">00:24:57.880</a></span> | <span class="t">floating point's not very accurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1499" target="_blank">00:24:59.720</a></span> | <span class="t">So we just multiply it by 512, put it in a region that is accurate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1503" target="_blank">00:25:03.360</a></span> | <span class="t">And then later on, in the backward step, we just divide by that again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1506" target="_blank">00:25:06.600</a></span> | <span class="t">So that's a little tweak, but it's the difference we find generally between things working and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1511" target="_blank">00:25:11.280</a></span> | <span class="t">not working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1512" target="_blank">00:25:12.840</a></span> | <span class="t">So the nice thing is now, we have something which you can just add mixed precision and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1520" target="_blank">00:25:20.360</a></span> | <span class="t">train and you will get often 2x, 3x speed up, certainly on vision models, also on transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1532" target="_blank">00:25:32.200</a></span> | <span class="t">quite a few places.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1534" target="_blank">00:25:34.440</a></span> | <span class="t">One obvious question is, is 512 the right number?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1539" target="_blank">00:25:39.480</a></span> | <span class="t">And it turns out getting this number right actually does make quite a difference to your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1542" target="_blank">00:25:42.560</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1543" target="_blank">00:25:43.720</a></span> | <span class="t">And so something slightly more recently is called dynamic loss scaling, which literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1548" target="_blank">00:25:48.200</a></span> | <span class="t">tries a few different values of loss scale to find out at what point does it become infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1554" target="_blank">00:25:54.040</a></span> | <span class="t">And so it dynamically figures out the highest loss scale we can go to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1559" target="_blank">00:25:59.160</a></span> | <span class="t">And so this version just has the dynamic loss scaling added.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1563" target="_blank">00:26:03.680</a></span> | <span class="t">It's interesting that sometimes training with half-precision gives you better results than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1568" target="_blank">00:26:08.800</a></span> | <span class="t">training with FP32 because there's just, I don't know, a bit more randomness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1573" target="_blank">00:26:13.120</a></span> | <span class="t">Maybe it regularizes a little bit, but generally it's super, super similar, just faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1578" target="_blank">00:26:18.280</a></span> | <span class="t">We have a question about mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1580" target="_blank">00:26:20.160</a></span> | <span class="t">Great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1581" target="_blank">00:26:21.560</a></span> | <span class="t">Is there an intuitive way to understand why mixup is better than other data augmentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1586" target="_blank">00:26:26.120</a></span> | <span class="t">techniques?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1590" target="_blank">00:26:30.600</a></span> | <span class="t">I think one of the things that's really nice about mixup is that it doesn't require any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1596" target="_blank">00:26:36.160</a></span> | <span class="t">domain-specific thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1598" target="_blank">00:26:38.360</a></span> | <span class="t">Do we flip horizontally or also vertically?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1600" target="_blank">00:26:40.840</a></span> | <span class="t">How much can we rotate?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1603" target="_blank">00:26:43.440</a></span> | <span class="t">It doesn't create any kind of lossiness, like in the corners, there's no reflection padding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1607" target="_blank">00:26:47.160</a></span> | <span class="t">or black padding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1608" target="_blank">00:26:48.160</a></span> | <span class="t">So it's kind of quite nice and clean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1612" target="_blank">00:26:52.800</a></span> | <span class="t">It's also almost infinite in terms of the number of different images it can create.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1619" target="_blank">00:26:59.240</a></span> | <span class="t">So you've kind of got this permutation of every image with every other image, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1624" target="_blank">00:27:04.040</a></span> | <span class="t">is already giant, and then in different mixes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1626" target="_blank">00:27:06.800</a></span> | <span class="t">So it's just a lot of augmentation that you can do with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1633" target="_blank">00:27:13.920</a></span> | <span class="t">And there are other similar things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1636" target="_blank">00:27:16.600</a></span> | <span class="t">So there's another thing which, there's something called cutout where you just delete a square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1642" target="_blank">00:27:22.240</a></span> | <span class="t">and replace it with black.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1643" target="_blank">00:27:23.680</a></span> | <span class="t">There's another one where you delete a square and replace it with random pixels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1647" target="_blank">00:27:27.200</a></span> | <span class="t">Something I haven't seen, but I'd really like to see people do, is to delete a square and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1650" target="_blank">00:27:30.640</a></span> | <span class="t">replace it with a different image.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1652" target="_blank">00:27:32.120</a></span> | <span class="t">So I'd love somebody to try doing mix-up, but instead of taking the linear combination,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1658" target="_blank">00:27:38.080</a></span> | <span class="t">instead pick an alpha-sized, sorry, a lambda percent of the pixels, like in a square, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1665" target="_blank">00:27:45.280</a></span> | <span class="t">paste them on top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1667" target="_blank">00:27:47.320</a></span> | <span class="t">There's another one which basically finds four different images and puts them in four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1672" target="_blank">00:27:52.000</a></span> | <span class="t">corners.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1673" target="_blank">00:27:53.000</a></span> | <span class="t">So there's a few different variations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1675" target="_blank">00:27:55.280</a></span> | <span class="t">And they really get great results, and I'm surprised how few people are using them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1682" target="_blank">00:28:02.440</a></span> | <span class="t">So let's put it all together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1684" target="_blank">00:28:04.320</a></span> | <span class="t">So here's emotionet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1686" target="_blank">00:28:06.720</a></span> | <span class="t">So let's use our random resize crop, a minimum scale of 0.35 we find works pretty well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1695" target="_blank">00:28:15.240</a></span> | <span class="t">And we're not going to do any other, other than flip, we're not going to do any other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1698" target="_blank">00:28:18.760</a></span> | <span class="t">augmentation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1701" target="_blank">00:28:21.320</a></span> | <span class="t">And now we need to create a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1705" target="_blank">00:28:25.000</a></span> | <span class="t">So far, all of our models have been boring convolutional models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1710" target="_blank">00:28:30.760</a></span> | <span class="t">But obviously what we really want to be using is a resnet model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1716" target="_blank">00:28:36.000</a></span> | <span class="t">We have the xresnet, which there's some debate about whether this is the mutant version of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1721" target="_blank">00:28:41.320</a></span> | <span class="t">resnet or the extended version of resnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1724" target="_blank">00:28:44.300</a></span> | <span class="t">So you can choose what the x stands for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1727" target="_blank">00:28:47.840</a></span> | <span class="t">And basically the xresnet is the bag of tricks, is basically the bag of tricks resnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1738" target="_blank">00:28:58.780</a></span> | <span class="t">So they have a few suggested tweaks to resnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1747" target="_blank">00:29:07.240</a></span> | <span class="t">And here they are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1749" target="_blank">00:29:09.520</a></span> | <span class="t">So these are their little tweaks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1753" target="_blank">00:29:13.200</a></span> | <span class="t">So the first tweak is something that we've kind of talked about, and they call it resnet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1757" target="_blank">00:29:17.360</a></span> | <span class="t">c.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1758" target="_blank">00:29:18.360</a></span> | <span class="t">And it's basically, hey, let's not do a big seven by seven convolution as our first layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1764" target="_blank">00:29:24.280</a></span> | <span class="t">because that's super inefficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1766" target="_blank">00:29:26.920</a></span> | <span class="t">And it's just a single linear model, which doesn't have much kind of richness to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1773" target="_blank">00:29:33.180</a></span> | <span class="t">So instead, let's do three comms in a row, three by three, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1778" target="_blank">00:29:38.940</a></span> | <span class="t">And so three, three by three comms in a row, if you think about it, the receptive field</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1783" target="_blank">00:29:43.040</a></span> | <span class="t">of that final one is still going to be about seven by seven, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1788" target="_blank">00:29:48.160</a></span> | <span class="t">But it's got there through a much richer set of things that it can learn, because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1792" target="_blank">00:29:52.560</a></span> | <span class="t">a three layer neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1794" target="_blank">00:29:54.920</a></span> | <span class="t">So that's the first thing that we do in our xresnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1799" target="_blank">00:29:59.600</a></span> | <span class="t">So here is xresnet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1803" target="_blank">00:30:03.840</a></span> | <span class="t">And when we create it, we set up how many filters are they going to be for each of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1809" target="_blank">00:30:09.240</a></span> | <span class="t">first three layers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1810" target="_blank">00:30:10.240</a></span> | <span class="t">So the first three layers will start with channels in, inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1814" target="_blank">00:30:14.280</a></span> | <span class="t">So that'll default to three, because normally we have three channel images, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1818" target="_blank">00:30:18.280</a></span> | <span class="t">And the number of outputs that we'll use for the first layer will be that plus one times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1822" target="_blank">00:30:22.740</a></span> | <span class="t">eight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1824" target="_blank">00:30:24.440</a></span> | <span class="t">Why is that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1825" target="_blank">00:30:25.640</a></span> | <span class="t">It's a bit of a long story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1827" target="_blank">00:30:27.700</a></span> | <span class="t">One reason is that that gives you 32 at the second layer, which is the same as what the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1834" target="_blank">00:30:34.840</a></span> | <span class="t">bag of tricks paper recommends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1841" target="_blank">00:30:41.400</a></span> | <span class="t">As you can see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1842" target="_blank">00:30:42.840</a></span> | <span class="t">The second reason is that I've kind of played around with this quite a lot to try to figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1850" target="_blank">00:30:50.120</a></span> | <span class="t">out what makes sense in terms of the receptive field, and I think this gives you the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1855" target="_blank">00:30:55.360</a></span> | <span class="t">amount.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1856" target="_blank">00:30:56.360</a></span> | <span class="t">Sometimes eight is here because video graphics cards like everything to be a multiple of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1864" target="_blank">00:31:04.560</a></span> | <span class="t">eight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1865" target="_blank">00:31:05.560</a></span> | <span class="t">So if this is not eight, it's probably going to be slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1867" target="_blank">00:31:07.300</a></span> | <span class="t">But one of the things here is now if you have like a one channel input, like black and white,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1872" target="_blank">00:31:12.900</a></span> | <span class="t">or a five channel input, like some kind of hyperspectral imaging or microscopy, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1877" target="_blank">00:31:17.600</a></span> | <span class="t">you're actually changing your model dynamically to say, oh, if I've got more inputs, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1882" target="_blank">00:31:22.680</a></span> | <span class="t">my first layer should have more activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1884" target="_blank">00:31:24.640</a></span> | <span class="t">Which is not something I've seen anybody do before, but it's a kind of really simple,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1889" target="_blank">00:31:29.000</a></span> | <span class="t">nice way to improve your ResNet for different kinds of domains.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1893" target="_blank">00:31:33.880</a></span> | <span class="t">So that's the number of filters we have for each layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1897" target="_blank">00:31:37.360</a></span> | <span class="t">So our stem, so the stem is the very start of a CNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1901" target="_blank">00:31:41.620</a></span> | <span class="t">So our stem is just those three conflayers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1906" target="_blank">00:31:46.160</a></span> | <span class="t">So that's all the paper says.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1909" target="_blank">00:31:49.040</a></span> | <span class="t">What's a conflayer?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1910" target="_blank">00:31:50.840</a></span> | <span class="t">A conflayer is a sequential containing a bunch of layers, which starts with a conf of some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1918" target="_blank">00:31:58.520</a></span> | <span class="t">stride, followed by a batch norm, and then optionally followed by an activation function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1925" target="_blank">00:32:05.920</a></span> | <span class="t">And our activation function, we're just going to use ReLU for now, because that's what they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1928" target="_blank">00:32:08.520</a></span> | <span class="t">using in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1931" target="_blank">00:32:11.200</a></span> | <span class="t">The batch norm, we do something interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1933" target="_blank">00:32:13.980</a></span> | <span class="t">This is another tweak from the bag of tricks, although it goes back a couple more years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1937" target="_blank">00:32:17.160</a></span> | <span class="t">than that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1938" target="_blank">00:32:18.420</a></span> | <span class="t">We initialize the batch norm, sometimes to have weights of 1, and sometimes to have weights</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1947" target="_blank">00:32:27.840</a></span> | <span class="t">of 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1950" target="_blank">00:32:30.320</a></span> | <span class="t">Why do we do that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1952" target="_blank">00:32:32.000</a></span> | <span class="t">Well, all right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1954" target="_blank">00:32:34.960</a></span> | <span class="t">Have a look here at ResNet D. This is a standard ResNet block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1961" target="_blank">00:32:41.480</a></span> | <span class="t">This path here normally doesn't have the conv and the average pool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1965" target="_blank">00:32:45.120</a></span> | <span class="t">So pretend they're not there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1966" target="_blank">00:32:46.120</a></span> | <span class="t">We'll talk about why they're there sometimes in a moment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1968" target="_blank">00:32:48.080</a></span> | <span class="t">But then this is just the identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1970" target="_blank">00:32:50.320</a></span> | <span class="t">And the other goes 1 by 1 conv, 3 by 3 conv, 1 by 1 conv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1975" target="_blank">00:32:55.920</a></span> | <span class="t">And remember, in each case, it's conv batch norm ReLU, conv batch norm ReLU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1979" target="_blank">00:32:59.720</a></span> | <span class="t">And then what actually happens is it then goes conv batch norm, and then the ReLU happens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1983" target="_blank">00:33:03.720</a></span> | <span class="t">after the plus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1985" target="_blank">00:33:05.600</a></span> | <span class="t">There's another variant where the ReLU happens before the plus, which is called preact or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1989" target="_blank">00:33:09.920</a></span> | <span class="t">preactivation ResNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1990" target="_blank">00:33:10.920</a></span> | <span class="t">Turns out it doesn't work quite as well for smaller models, so we're using the non-preact</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1996" target="_blank">00:33:16.520</a></span> | <span class="t">version.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=1997" target="_blank">00:33:17.520</a></span> | <span class="t">Now, see this conv here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2000" target="_blank">00:33:20.920</a></span> | <span class="t">What if we set the batch norm layer weights there to 0?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2005" target="_blank">00:33:25.280</a></span> | <span class="t">What's going to happen?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2006" target="_blank">00:33:26.280</a></span> | <span class="t">Well, we've got an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2007" target="_blank">00:33:27.700</a></span> | <span class="t">This is identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2009" target="_blank">00:33:29.040</a></span> | <span class="t">This does some conv, some conv, some conv, and then batch norm where the weights are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2013" target="_blank">00:33:33.560</a></span> | <span class="t">0, so everything gets multiplied by 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2015" target="_blank">00:33:35.800</a></span> | <span class="t">And so out of here comes 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2019" target="_blank">00:33:39.280</a></span> | <span class="t">So why is that interesting?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2020" target="_blank">00:33:40.480</a></span> | <span class="t">Because now we're adding 0 to the identity block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2023" target="_blank">00:33:43.800</a></span> | <span class="t">So in other words, the whole block does nothing at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2027" target="_blank">00:33:47.840</a></span> | <span class="t">That's a great way to initialize a model, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2030" target="_blank">00:33:50.560</a></span> | <span class="t">Because we really don't want to be in a position, as we've seen, where if you've got a thousand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2034" target="_blank">00:33:54.200</a></span> | <span class="t">layers deep model, that any layer is even slightly changing the variance because they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2039" target="_blank">00:33:59.160</a></span> | <span class="t">kind of cause the gradients to spiral off to 0 or to infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2043" target="_blank">00:34:03.320</a></span> | <span class="t">This way, literally, the entire activations are the same all the way through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2049" target="_blank">00:34:09.240</a></span> | <span class="t">So that's what we do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2050" target="_blank">00:34:10.640</a></span> | <span class="t">We set the 1, 2, 3 third conv layer to have 0 in that batch norm layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2061" target="_blank">00:34:21.760</a></span> | <span class="t">And this lets us train very deep models at very high learning rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2066" target="_blank">00:34:26.240</a></span> | <span class="t">You'll see nearly all of the academic literature about this talks about large batch sizes because,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2070" target="_blank">00:34:30.800</a></span> | <span class="t">of course, academics, particularly at big companies like Google and OpenAI and Nvidia</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2075" target="_blank">00:34:35.360</a></span> | <span class="t">and Facebook, love to show off their giant data centers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2079" target="_blank">00:34:39.760</a></span> | <span class="t">And so they like to say, oh, if we do 1,000 TPUs, how big a batch size can we create?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2085" target="_blank">00:34:45.040</a></span> | <span class="t">But for us normal people, these are also interesting because the exact same things tell us how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2090" target="_blank">00:34:50.000</a></span> | <span class="t">high a learning rate can we go, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2092" target="_blank">00:34:52.220</a></span> | <span class="t">So the exact same things that let you create really big batch sizes, so you do a giant</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2095" target="_blank">00:34:55.320</a></span> | <span class="t">batch and then you take a giant step, well, we can just take a normal sized batch, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2100" target="_blank">00:35:00.680</a></span> | <span class="t">a much bigger than usual step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2102" target="_blank">00:35:02.600</a></span> | <span class="t">And by using higher learning rates, we train faster and we generalize better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2107" target="_blank">00:35:07.400</a></span> | <span class="t">And so that's all good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2108" target="_blank">00:35:08.400</a></span> | <span class="t">So this is a really good little trick.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2110" target="_blank">00:35:10.840</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2113" target="_blank">00:35:13.040</a></span> | <span class="t">So that's conv layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2117" target="_blank">00:35:17.680</a></span> | <span class="t">So there's our stem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2120" target="_blank">00:35:20.040</a></span> | <span class="t">And then we're going to create a bunch of res blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2124" target="_blank">00:35:24.160</a></span> | <span class="t">So a res block is one of these, except this is an identity path, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2130" target="_blank">00:35:30.640</a></span> | <span class="t">Unless we're doing a res net 34 or a res net 18, in which case one of these comms goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2138" target="_blank">00:35:38.840</a></span> | <span class="t">away.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2139" target="_blank">00:35:39.840</a></span> | <span class="t">So res net 34 and res net 18 only have two cons here and res net 50 onwards have three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2145" target="_blank">00:35:45.400</a></span> | <span class="t">cons here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2147" target="_blank">00:35:47.120</a></span> | <span class="t">So and then in res net 50 and above, the second conv, they actually squish the number of channels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2154" target="_blank">00:35:54.200</a></span> | <span class="t">down by four and then they expand it back up again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2157" target="_blank">00:35:57.760</a></span> | <span class="t">So it could go like 64 channels to 16 channels to 64 channels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2162" target="_blank">00:36:02.600</a></span> | <span class="t">Let's call it a bottleneck layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2164" target="_blank">00:36:04.560</a></span> | <span class="t">So a bottleneck block is the normal block for larger res nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2168" target="_blank">00:36:08.420</a></span> | <span class="t">And then just two three by three comms is the normal for smaller res nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2174" target="_blank">00:36:14.200</a></span> | <span class="t">So you can see in our res block that we pass in this thing called expansion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2179" target="_blank">00:36:19.360</a></span> | <span class="t">It's either one or four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2180" target="_blank">00:36:20.860</a></span> | <span class="t">It's one if it's res net 18 or 34, and it's four if it's bigger, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2186" target="_blank">00:36:26.240</a></span> | <span class="t">And so if it's four, well, if it's expansion equals one, then we just add one extra conv,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2191" target="_blank">00:36:31.960</a></span> | <span class="t">right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2192" target="_blank">00:36:32.960</a></span> | <span class="t">Oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2193" target="_blank">00:36:33.960</a></span> | <span class="t">The first conv is always a one by one, and then we add a three by three conv, or if expansion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2198" target="_blank">00:36:38.600</a></span> | <span class="t">equals four, we add two extra comms.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2202" target="_blank">00:36:42.680</a></span> | <span class="t">So that's what the res blocks are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2206" target="_blank">00:36:46.440</a></span> | <span class="t">Now I mentioned that there's two other things here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2210" target="_blank">00:36:50.600</a></span> | <span class="t">Why are there two other things here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2212" target="_blank">00:36:52.960</a></span> | <span class="t">Well, we can't use standard res blocks all the way through our model, can we?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2218" target="_blank">00:36:58.120</a></span> | <span class="t">Because a res block can't change the grid size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2221" target="_blank">00:37:01.120</a></span> | <span class="t">We can't have a stride two anywhere here, because if we had a stride two somewhere here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2227" target="_blank">00:37:07.260</a></span> | <span class="t">we can't add it back to the identity because they're now different sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2231" target="_blank">00:37:11.120</a></span> | <span class="t">Also we can't change the number of channels, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2234" target="_blank">00:37:14.260</a></span> | <span class="t">Because if we change the number of channels, we can't add it to the identity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2237" target="_blank">00:37:17.440</a></span> | <span class="t">So what do we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2238" target="_blank">00:37:18.680</a></span> | <span class="t">Well, as you know, from time to time, we do like to throw in a stride two, and generally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2243" target="_blank">00:37:23.840</a></span> | <span class="t">when we throw in a stride two, we like to double the number of channels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2247" target="_blank">00:37:27.520</a></span> | <span class="t">And so when we do that, we're going to add to the identity path two extra layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2252" target="_blank">00:37:32.520</a></span> | <span class="t">We'll add an average pooling layer, so that's going to cause the grid size to shift down</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2257" target="_blank">00:37:37.200</a></span> | <span class="t">by two in each dimension, and we'll add a one by one conv to change the number of filters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2263" target="_blank">00:37:43.300</a></span> | <span class="t">So that's what this is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2265" target="_blank">00:37:45.060</a></span> | <span class="t">And this particular way of doing it is specific to the x res net, and it gives you a nice</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2271" target="_blank">00:37:51.360</a></span> | <span class="t">little boost over the standard approach, and so you can see that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2278" target="_blank">00:37:58.600</a></span> | <span class="t">If the number of inputs is different to the number of filters, then we add an extra conv</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2282" target="_blank">00:38:02.800</a></span> | <span class="t">layer, otherwise we just do no op, no operation, which is defined here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2290" target="_blank">00:38:10.480</a></span> | <span class="t">And if the stride is something other than one, we add an average pooling, otherwise it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2295" target="_blank">00:38:15.160</a></span> | <span class="t">a no op, and so here is our final res net block calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2300" target="_blank">00:38:20.580</a></span> | <span class="t">So that's the res block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2303" target="_blank">00:38:23.680</a></span> | <span class="t">So tweak for res net d is this way of doing the, they call it a downsampling path.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2312" target="_blank">00:38:32.840</a></span> | <span class="t">And then the final tweak is the actual ordering here of where the stride two is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2317" target="_blank">00:38:37.400</a></span> | <span class="t">Usually the stride two in normal res net is at the start, and then there's a three by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2322" target="_blank">00:38:42.880</a></span> | <span class="t">three after that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2324" target="_blank">00:38:44.200</a></span> | <span class="t">Doing a stride two on a one by one conv is a terrible idea, because you're literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2328" target="_blank">00:38:48.400</a></span> | <span class="t">throwing away three quarters of the data, and it's interesting, it took people years</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2333" target="_blank">00:38:53.240</a></span> | <span class="t">to realize they're literally throwing away three quarters of the data, so the bag of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2337" target="_blank">00:38:57.080</a></span> | <span class="t">tricks folks said, let's just move the stride two to the three by three, and that makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2341" target="_blank">00:39:01.160</a></span> | <span class="t">a lot more sense, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2342" target="_blank">00:39:02.160</a></span> | <span class="t">Because a stride two, three by three, you're actually hitting every pixel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2347" target="_blank">00:39:07.480</a></span> | <span class="t">So the reason I'm mentioning these details is so that you can read that paper and spend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2352" target="_blank">00:39:12.760</a></span> | <span class="t">time thinking about each of those res net tweaks, do you understand why they did that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2358" target="_blank">00:39:18.280</a></span> | <span class="t">Right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2359" target="_blank">00:39:19.280</a></span> | <span class="t">It wasn't some neural architecture search, try everything, brainless, use all our computers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2365" target="_blank">00:39:25.800</a></span> | <span class="t">approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2366" target="_blank">00:39:26.800</a></span> | <span class="t">So let's sit back and think about how do we actually use all the inputs we have, and how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2373" target="_blank">00:39:33.040</a></span> | <span class="t">do we actually take advantage of all the computation that we're doing, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2377" target="_blank">00:39:37.600</a></span> | <span class="t">So it's a very, most of the tweaks are stuff that exists from before, and they've cited</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2383" target="_blank">00:39:43.220</a></span> | <span class="t">all those, but if you put them all together, it's just a nice, like, here's how to think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2387" target="_blank">00:39:47.000</a></span> | <span class="t">through architecture design.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2392" target="_blank">00:39:52.400</a></span> | <span class="t">And that's about it, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2393" target="_blank">00:39:53.640</a></span> | <span class="t">So we create a res net block for every res layer, and so here it is, creating the res</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2402" target="_blank">00:40:02.080</a></span> | <span class="t">net block, and so now we can create all of our res nets by simply saying, this is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2409" target="_blank">00:40:09.320</a></span> | <span class="t">many blocks we have in each layer, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2412" target="_blank">00:40:12.560</a></span> | <span class="t">So res net 18 is just two, two, two, two, 34 is three, four, six, three, and then secondly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2417" target="_blank">00:40:17.560</a></span> | <span class="t">is changing the expansion factor, which as I said for 18 and 34 is one, and for the bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2423" target="_blank">00:40:23.520</a></span> | <span class="t">ones is four.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2426" target="_blank">00:40:26.200</a></span> | <span class="t">So that's a lot of information there, and if you haven't spent time thinking about architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2430" target="_blank">00:40:30.440</a></span> | <span class="t">before, it might take you a few reads and lessons to put the sink in, but I think it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2435" target="_blank">00:40:35.120</a></span> | <span class="t">a really good idea to try to spend time thinking about that, and also to, like, experiment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2441" target="_blank">00:40:41.000</a></span> | <span class="t">right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2442" target="_blank">00:40:42.000</a></span> | <span class="t">And try to think about what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2444" target="_blank">00:40:44.580</a></span> | <span class="t">The other thing to point out here is that this -- the way I've written this, it's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2453" target="_blank">00:40:53.000</a></span> | <span class="t">this is the whole -- this is the whole res net, right, other than the definition of conflayer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2459" target="_blank">00:40:59.200</a></span> | <span class="t">this is the whole res net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2460" target="_blank">00:41:00.320</a></span> | <span class="t">It fits on the screen, and this is really unusual.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2463" target="_blank">00:41:03.720</a></span> | <span class="t">Most res nets you see, even without the bag of tricks, 500, 600, 700 lines of code, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2470" target="_blank">00:41:10.720</a></span> | <span class="t">And if every single line of code has a different arbitrary number at 16 here and 32 there and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2477" target="_blank">00:41:17.480</a></span> | <span class="t">average pool here and something else there, like, how are you going to get it right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2480" target="_blank">00:41:20.800</a></span> | <span class="t">And how are you going to be able to look at it and say, what if I did this a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2484" target="_blank">00:41:24.280</a></span> | <span class="t">differently?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2486" target="_blank">00:41:26.240</a></span> | <span class="t">So for research and for production, you want to get your code refactored like this for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2492" target="_blank">00:41:32.880</a></span> | <span class="t">your architecture so that you can look at it and say, what exactly is going on, is it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2497" target="_blank">00:41:37.960</a></span> | <span class="t">written correctly, okay, I want to change this to be in a different layer, how do I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2502" target="_blank">00:41:42.680</a></span> | <span class="t">do it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2505" target="_blank">00:41:45.000</a></span> | <span class="t">It's really important for effective practitioners to be able to write nice, concise architectures</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2512" target="_blank">00:41:52.440</a></span> | <span class="t">so that you can change them and understand them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2515" target="_blank">00:41:55.520</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2517" target="_blank">00:41:57.000</a></span> | <span class="t">So that's our X res net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2518" target="_blank">00:41:58.960</a></span> | <span class="t">We can train it with or without mixup, it's up to us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2523" target="_blank">00:42:03.520</a></span> | <span class="t">Label smoothing cross entropy is probably always a good idea, unless you know that your labels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2527" target="_blank">00:42:07.480</a></span> | <span class="t">are basically perfect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2528" target="_blank">00:42:08.480</a></span> | <span class="t">Let's just create a little res net 18.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2533" target="_blank">00:42:13.320</a></span> | <span class="t">And let's check out to see what our model is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2536" target="_blank">00:42:16.800</a></span> | <span class="t">So we've already got a model summary, but we're just going to rewrite it to use our,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2540" target="_blank">00:42:20.680</a></span> | <span class="t">the new version of learner that doesn't have runner anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2543" target="_blank">00:42:23.940</a></span> | <span class="t">And so we can print out and see what happens to our shapes as they go through the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2550" target="_blank">00:42:30.200</a></span> | <span class="t">And you can change this print mod here to true, and it'll print out the entire blocks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2555" target="_blank">00:42:35.000</a></span> | <span class="t">and then show you what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2556" target="_blank">00:42:36.440</a></span> | <span class="t">So that would be a really useful thing to help you understand what's going on in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2559" target="_blank">00:42:39.640</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2560" target="_blank">00:42:40.640</a></span> | <span class="t">All right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2561" target="_blank">00:42:41.940</a></span> | <span class="t">So here's our architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2564" target="_blank">00:42:44.220</a></span> | <span class="t">It's nice and easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2565" target="_blank">00:42:45.600</a></span> | <span class="t">We can tell you how many channels are coming in, how many channels are coming out, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2569" target="_blank">00:42:49.840</a></span> | <span class="t">it'll adapt automatically to our data that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2574" target="_blank">00:42:54.020</a></span> | <span class="t">So we can create our learner, we can do our LR find.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2578" target="_blank">00:42:58.920</a></span> | <span class="t">And now that we've done that, let's create a one cycle learning rate annealing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2585" target="_blank">00:43:05.000</a></span> | <span class="t">So one cycle learning rate annealing, we've seen all this before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2589" target="_blank">00:43:09.420</a></span> | <span class="t">We keep on creating these things like 0.3, 0.7 for the two phases or 0.3, 0.2, 0.5 for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2595" target="_blank">00:43:15.160</a></span> | <span class="t">three phases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2596" target="_blank">00:43:16.160</a></span> | <span class="t">So I add a little create phases that will build those for us automatically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2601" target="_blank">00:43:21.400</a></span> | <span class="t">This one we've built before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2603" target="_blank">00:43:23.200</a></span> | <span class="t">So here's our standard one cycle annealing, and here's our parameter scheduler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2610" target="_blank">00:43:30.640</a></span> | <span class="t">And so one other thing I did last week was I made it that callbacks, you don't have to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2616" target="_blank">00:43:36.960</a></span> | <span class="t">pass to the initializer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2618" target="_blank">00:43:38.440</a></span> | <span class="t">You can also pass them to the fit function, and it'll just run those callbacks to the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2622" target="_blank">00:43:42.120</a></span> | <span class="t">fit functions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2623" target="_blank">00:43:43.120</a></span> | <span class="t">This is a great way to do parameter scheduling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2625" target="_blank">00:43:45.480</a></span> | <span class="t">And there we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2626" target="_blank">00:43:46.800</a></span> | <span class="t">And so 83.2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2631" target="_blank">00:43:51.760</a></span> | <span class="t">So I would love to see people beat my benchmarks here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2635" target="_blank">00:43:55.420</a></span> | <span class="t">So here's the image net site.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2637" target="_blank">00:43:57.880</a></span> | <span class="t">And so so far, the best I've got for 128, 5 epochs is 84.6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2643" target="_blank">00:44:03.780</a></span> | <span class="t">So yeah, we're super close.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2647" target="_blank">00:44:07.800</a></span> | <span class="t">So maybe with some fiddling around, you can find something that's even better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2651" target="_blank">00:44:11.560</a></span> | <span class="t">And with these kind of leaderboards, where a lot of these things can train in, this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2655" target="_blank">00:44:15.720</a></span> | <span class="t">two and a half minutes on a standard, I think it was a GTX 1080 Ti, you can quickly try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2662" target="_blank">00:44:22.000</a></span> | <span class="t">things out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2663" target="_blank">00:44:23.240</a></span> | <span class="t">And what I've noticed is that the results I get in 5 epochs on 128 pixel image net models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2670" target="_blank">00:44:30.060</a></span> | <span class="t">carry over a lot to image net training or bigger models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2674" target="_blank">00:44:34.920</a></span> | <span class="t">So you can learn a lot by not trying to train giant models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2680" target="_blank">00:44:40.320</a></span> | <span class="t">So compete on this leaderboard to become a better practitioner to try out things, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2685" target="_blank">00:44:45.680</a></span> | <span class="t">And if you do have some more time, you can go all the way to 400 epochs, that might take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2688" target="_blank">00:44:48.440</a></span> | <span class="t">a couple of hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2689" target="_blank">00:44:49.440</a></span> | <span class="t">And then of course, also we've got image wolf, which is just doggy photos, and is much harder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2696" target="_blank">00:44:56.200</a></span> | <span class="t">And actually, this one, I find an even better test case, because it's a more difficult data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2701" target="_blank">00:45:01.320</a></span> | <span class="t">set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2702" target="_blank">00:45:02.320</a></span> | <span class="t">So we've got a 90% is my best for this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2704" target="_blank">00:45:04.800</a></span> | <span class="t">So I hope somebody can beat me.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2707" target="_blank">00:45:07.120</a></span> | <span class="t">I really do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2710" target="_blank">00:45:10.400</a></span> | <span class="t">So we can refactor all that stuff of adding all these different callbacks and stuff into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2718" target="_blank">00:45:18.640</a></span> | <span class="t">a single function called CNN learner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2721" target="_blank">00:45:21.680</a></span> | <span class="t">And we can just pass in an architecture and our data and our loss function and our optimization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2725" target="_blank">00:45:25.440</a></span> | <span class="t">function and what kind of callbacks do we want, just yes or no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2730" target="_blank">00:45:30.220</a></span> | <span class="t">And we'll just set everything up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2732" target="_blank">00:45:32.640</a></span> | <span class="t">And if you don't pass in C in and C out, we'll grab it from your data for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2737" target="_blank">00:45:37.140</a></span> | <span class="t">And then we'll just pass that off to the learner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2741" target="_blank">00:45:41.160</a></span> | <span class="t">So that makes things easier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2742" target="_blank">00:45:42.160</a></span> | <span class="t">So now if you want to create a CNN, it's just one line of code, adding in whatever we want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2747" target="_blank">00:45:47.120</a></span> | <span class="t">except label smoothing, blah, blah, blah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2750" target="_blank">00:45:50.320</a></span> | <span class="t">And so we get the same result when we fit it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2753" target="_blank">00:45:53.080</a></span> | <span class="t">So we can see this all put together in this ImageNet training script, which is in fast</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2759" target="_blank">00:45:59.640</a></span> | <span class="t">AI, in example, slash train ImageNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2762" target="_blank">00:46:02.760</a></span> | <span class="t">And this entire thing will look entirely familiar to you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2767" target="_blank">00:46:07.320</a></span> | <span class="t">It's all stuff that we've now built from scratch, with one exception, which is this bit, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2773" target="_blank">00:46:13.920</a></span> | <span class="t">is using multiple GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2775" target="_blank">00:46:15.540</a></span> | <span class="t">So we're not covering that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2777" target="_blank">00:46:17.160</a></span> | <span class="t">But that's just an acceleration tweak.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2781" target="_blank">00:46:21.680</a></span> | <span class="t">And you can easily use multiple GPUs by simply doing data parallel or too distributed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2788" target="_blank">00:46:28.900</a></span> | <span class="t">Other than that, yeah, this is all stuff that you see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2792" target="_blank">00:46:32.360</a></span> | <span class="t">And there's label smoothing cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2795" target="_blank">00:46:35.360</a></span> | <span class="t">There's mixup.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2799" target="_blank">00:46:39.460</a></span> | <span class="t">Here's something we haven't written.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2801" target="_blank">00:46:41.080</a></span> | <span class="t">Save the model after every epoch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2802" target="_blank">00:46:42.940</a></span> | <span class="t">Maybe you want to write that one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2803" target="_blank">00:46:43.940</a></span> | <span class="t">That would be a good exercise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2806" target="_blank">00:46:46.720</a></span> | <span class="t">So what happens if we try to train this for just 60 epochs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2813" target="_blank">00:46:53.680</a></span> | <span class="t">This is what happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2814" target="_blank">00:46:54.680</a></span> | <span class="t">So benchmark results on ImageNet, these are all the Keras and PyTorch models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2818" target="_blank">00:46:58.500</a></span> | <span class="t">It's very hard to compare them because they have different input sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2821" target="_blank">00:47:01.900</a></span> | <span class="t">So we really should compare the ones with our input size, which is 224.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2825" target="_blank">00:47:05.760</a></span> | <span class="t">So a standard ResNet -- oh, it scrolled off the screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2832" target="_blank">00:47:12.560</a></span> | <span class="t">So ResNet 50 is so bad, it's actually scrolled off the screen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2835" target="_blank">00:47:15.280</a></span> | <span class="t">So let's take ResNet 101 as a 93.3% accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2839" target="_blank">00:47:19.760</a></span> | <span class="t">So that's twice as many layers as we used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2841" target="_blank">00:47:21.520</a></span> | <span class="t">And it was also trained for 90 epochs, so trained for 50% longer, 93.3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2846" target="_blank">00:47:26.720</a></span> | <span class="t">When I trained this on ImageNet, I got 94.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2850" target="_blank">00:47:30.960</a></span> | <span class="t">So this, like, extremely simple architecture that fits on a single screen and was built</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2857" target="_blank">00:47:37.360</a></span> | <span class="t">entirely using common sense, trained for just 60 epochs, actually gets us even above ResNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2863" target="_blank">00:47:43.960</a></span> | <span class="t">152.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2864" target="_blank">00:47:44.960</a></span> | <span class="t">Because that's 93.8.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2865" target="_blank">00:47:45.960</a></span> | <span class="t">We've got 94.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2866" target="_blank">00:47:46.960</a></span> | <span class="t">So the only things above it were trained on much, much larger images.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2873" target="_blank">00:47:53.240</a></span> | <span class="t">And also, like, NASNet large is so big, I can't train it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2877" target="_blank">00:47:57.560</a></span> | <span class="t">I just keep on running out of memory in time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2880" target="_blank">00:48:00.040</a></span> | <span class="t">And Inception ResNet version 2 is really, really fiddly and also really, really slow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2884" target="_blank">00:48:04.280</a></span> | <span class="t">So we've now got, you know, this beautiful nice ResNet, XResNet 50 model, which, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2892" target="_blank">00:48:12.720</a></span> | <span class="t">know, is built in this very first principles common sense way and gets astonishingly great</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2899" target="_blank">00:48:19.160</a></span> | <span class="t">results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2900" target="_blank">00:48:20.240</a></span> | <span class="t">So I really don't think we all need to be running to neural architecture search and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2907" target="_blank">00:48:27.680</a></span> | <span class="t">hyperparameter optimization and blah, blah, blah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2909" target="_blank">00:48:29.960</a></span> | <span class="t">We just need to use, you know, good common sense thinking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2914" target="_blank">00:48:34.260</a></span> | <span class="t">So I'm super excited to see how well that worked out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2921" target="_blank">00:48:41.420</a></span> | <span class="t">So now that we have a nice model, we want to be able to do transfer learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2925" target="_blank">00:48:45.700</a></span> | <span class="t">So how do we do transfer learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2928" target="_blank">00:48:48.120</a></span> | <span class="t">I mean, you all know how to do transfer learning, but let's do it from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2932" target="_blank">00:48:52.540</a></span> | <span class="t">So what I'm going to do is I'm going to transfer learn from ImageWolf to the pets data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2938" target="_blank">00:48:58.140</a></span> | <span class="t">that we used in lesson one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2941" target="_blank">00:49:01.260</a></span> | <span class="t">That's our goal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2942" target="_blank">00:49:02.600</a></span> | <span class="t">So we start by grabbing ImageWolf.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2944" target="_blank">00:49:04.600</a></span> | <span class="t">We do the standard data block stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2949" target="_blank">00:49:09.000</a></span> | <span class="t">Let's use label-smoothing cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2950" target="_blank">00:49:10.840</a></span> | <span class="t">Notice how we're using all the stuff we've built.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2952" target="_blank">00:49:12.320</a></span> | <span class="t">This is our atom optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2953" target="_blank">00:49:13.720</a></span> | <span class="t">This is our label-smoothing cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2955" target="_blank">00:49:15.320</a></span> | <span class="t">This is the data block API we wrote.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2957" target="_blank">00:49:17.440</a></span> | <span class="t">So we're still not using anything from fast AI v1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2962" target="_blank">00:49:22.320</a></span> | <span class="t">This is all stuff that if you want to know what's going on, you can go back to that previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2966" target="_blank">00:49:26.240</a></span> | <span class="t">lesson and see what did we build and how did we build it and step through the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2971" target="_blank">00:49:31.160</a></span> | <span class="t">There's a CNN learner that we just built in the last notebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2976" target="_blank">00:49:36.260</a></span> | <span class="t">These five lines of code I got sick of typing, so let's dump them into a single function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2980" target="_blank">00:49:40.300</a></span> | <span class="t">called schedule1cycle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2981" target="_blank">00:49:41.300</a></span> | <span class="t">It's going to create our phases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2984" target="_blank">00:49:44.780</a></span> | <span class="t">It's going to create our momentum annealing and our learning rate annealing and create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2989" target="_blank">00:49:49.000</a></span> | <span class="t">our schedulers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2990" target="_blank">00:49:50.500</a></span> | <span class="t">So now with that we can just say schedule1cycle with a learning rate, what percentage of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2994" target="_blank">00:49:54.700</a></span> | <span class="t">the epochs are at the start, batches I should say at the start, and we could go ahead and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2998" target="_blank">00:49:58.820</a></span> | <span class="t">fit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=2999" target="_blank">00:49:59.820</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3000" target="_blank">00:50:00.820</a></span> | <span class="t">For transfer learning we should try and fit a decent model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3003" target="_blank">00:50:03.660</a></span> | <span class="t">So I did 40 epochs at 11 seconds per epoch on a 1080ti.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3009" target="_blank">00:50:09.480</a></span> | <span class="t">So a few minutes later we've got 79.6% accuracy, which is pretty good, you know, training from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3018" target="_blank">00:50:18.380</a></span> | <span class="t">scratch for 10 different dog breeds with a ResNet 18.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3023" target="_blank">00:50:23.060</a></span> | <span class="t">So let's try and use this to create a good pets model that's going to be a little bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3029" target="_blank">00:50:29.620</a></span> | <span class="t">tricky because the pets dataset has cats as well, and this model's never seen cats.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3034" target="_blank">00:50:34.700</a></span> | <span class="t">And also this model has only been trained on I think less than 10,000 images, so it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3039" target="_blank">00:50:39.860</a></span> | <span class="t">kind of unusually small thing that we're trying to do here, so it's an interesting experiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3044" target="_blank">00:50:44.500</a></span> | <span class="t">to see if this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3046" target="_blank">00:50:46.220</a></span> | <span class="t">So the first thing we have to do is we have to save the model so that we can load it into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3049" target="_blank">00:50:49.700</a></span> | <span class="t">a pets model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3051" target="_blank">00:50:51.480</a></span> | <span class="t">So when we save a model, what we do is we grab its state dict.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3057" target="_blank">00:50:57.740</a></span> | <span class="t">Now we actually haven't written this, but it would be like three lines of code if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3060" target="_blank">00:51:00.700</a></span> | <span class="t">want to write it yourself, because all it does is it literally creates a dictionary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3064" target="_blank">00:51:04.800</a></span> | <span class="t">an order dict is just a Python standard library dictionary that has an order, where the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3069" target="_blank">00:51:09.980</a></span> | <span class="t">are just the names of all the layers, and for sequential the index of each one, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3075" target="_blank">00:51:15.260</a></span> | <span class="t">then you can look up, say, 10.bias, and it just returns the weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3080" target="_blank">00:51:20.100</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3081" target="_blank">00:51:21.100</a></span> | <span class="t">So you can easily turn a module into a dictionary, and so then we can create somewhere to save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3086" target="_blank">00:51:26.840</a></span> | <span class="t">our model, and torch.save will save that dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3090" target="_blank">00:51:30.860</a></span> | <span class="t">You can actually just use pickle here, works fine, and actually behind the scenes, torch.save</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3095" target="_blank">00:51:35.540</a></span> | <span class="t">is using pickle, but they kind of like add some header to it to say like it's basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3101" target="_blank">00:51:41.700</a></span> | <span class="t">a magic number that when they read it back, they make sure it is a PyTorch model file</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3105" target="_blank">00:51:45.820</a></span> | <span class="t">and that it's the right version and stuff like that, but you can totally use pickle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3111" target="_blank">00:51:51.540</a></span> | <span class="t">And so the nice thing is now that we know that the thing we've saved is just a dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3116" target="_blank">00:51:56.660</a></span> | <span class="t">So you can fiddle with it, but if you have trouble loading something in the future, just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3121" target="_blank">00:52:01.980</a></span> | <span class="t">open up, just go torch.load, put it into a dictionary, and look at the keys and look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3125" target="_blank">00:52:05.860</a></span> | <span class="t">at the values and see what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3128" target="_blank">00:52:08.380</a></span> | <span class="t">So let's try and use this for pets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3130" target="_blank">00:52:10.780</a></span> | <span class="t">So we've seen pets before, so the nice thing is that we've never used pets in part two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3135" target="_blank">00:52:15.620</a></span> | <span class="t">but our data blocks API totally works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3139" target="_blank">00:52:19.180</a></span> | <span class="t">And in this case, there's one images directory that contains all the images, and there isn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3145" target="_blank">00:52:25.260</a></span> | <span class="t">a separate validation set directory, so we can't use that label with -- sorry, yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3152" target="_blank">00:52:32.060</a></span> | <span class="t">label with -- sorry, split with grandparent thing, so we're going to have to split it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3157" target="_blank">00:52:37.460</a></span> | <span class="t">randomly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3158" target="_blank">00:52:38.460</a></span> | <span class="t">But remember how we've already created split by func?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3161" target="_blank">00:52:41.340</a></span> | <span class="t">So let's just write a function that returns true or false, depending on whether some random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3167" target="_blank">00:52:47.340</a></span> | <span class="t">number is large or small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3171" target="_blank">00:52:51.060</a></span> | <span class="t">And so now, we can just pass that to our split by func, and we're done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3177" target="_blank">00:52:57.940</a></span> | <span class="t">So the nice thing is, when you kind of understand what's going on behind the scenes, it's super</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3182" target="_blank">00:53:02.660</a></span> | <span class="t">easy for you to customize things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3185" target="_blank">00:53:05.940</a></span> | <span class="t">And fast.i.v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3186" target="_blank">00:53:06.940</a></span> | <span class="t">1 is basically identical, there's a split by func that you do the same thing for.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3192" target="_blank">00:53:12.920</a></span> | <span class="t">So now that's split into training and validation, and you can see how nice it is that we created</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3199" target="_blank">00:53:19.460</a></span> | <span class="t">that dunder repress so that we can print things out so easily to see what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3203" target="_blank">00:53:23.900</a></span> | <span class="t">So if something doesn't have a nice representation, you should monkey-patch in a dunder repress</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3209" target="_blank">00:53:29.220</a></span> | <span class="t">so you can print out what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3211" target="_blank">00:53:31.300</a></span> | <span class="t">Now we have to label it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3213" target="_blank">00:53:33.540</a></span> | <span class="t">So we can't label it by folder, because they're not put into folders.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3216" target="_blank">00:53:36.580</a></span> | <span class="t">Instead, we have to look at the file name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3219" target="_blank">00:53:39.560</a></span> | <span class="t">So let's grab one file name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3221" target="_blank">00:53:41.480</a></span> | <span class="t">So I need to build all this stuff in a Jupyter notebook just interactively to see what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3225" target="_blank">00:53:45.260</a></span> | <span class="t">going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3228" target="_blank">00:53:48.980</a></span> | <span class="t">So in this case, we'll grab one name, and then let's try to construct a regular expression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3234" target="_blank">00:53:54.660</a></span> | <span class="t">that grabs just the doggy's name from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3238" target="_blank">00:53:58.500</a></span> | <span class="t">And once we've got it, we can now turn that into a function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3241" target="_blank">00:54:01.900</a></span> | <span class="t">And we can now go ahead and use that category processor we built last week to label it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3246" target="_blank">00:54:06.740</a></span> | <span class="t">And there we go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3247" target="_blank">00:54:07.740</a></span> | <span class="t">There's all the kinds of doggy we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3248" target="_blank">00:54:08.740</a></span> | <span class="t">We're not just doggies now, doggies and kitties.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3253" target="_blank">00:54:13.020</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3255" target="_blank">00:54:15.020</a></span> | <span class="t">So now we can train from scratch pets, 37%, not great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3261" target="_blank">00:54:21.860</a></span> | <span class="t">So maybe with transfer learning, we can do better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3265" target="_blank">00:54:25.860</a></span> | <span class="t">So transfer learning, we can read in that imagewoof model, and then we will customize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3274" target="_blank">00:54:34.660</a></span> | <span class="t">it for pets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3277" target="_blank">00:54:37.980</a></span> | <span class="t">So let's create a CNN for pets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3281" target="_blank">00:54:41.220</a></span> | <span class="t">This is now the pet's data bunch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3285" target="_blank">00:54:45.580</a></span> | <span class="t">But let's tell it to create a model with ten filters out, ten activations at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3293" target="_blank">00:54:53.100</a></span> | <span class="t">Because remember, imagewoof has ten types of dog, ten breeds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3296" target="_blank">00:54:56.980</a></span> | <span class="t">So to load in the pre-trained model, we're going to need to ask for a learner with ten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3302" target="_blank">00:55:02.460</a></span> | <span class="t">activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3303" target="_blank">00:55:03.980</a></span> | <span class="t">So that is something we can now grab our state dictionary that we saved earlier, and we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3309" target="_blank">00:55:09.820</a></span> | <span class="t">load it into our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3314" target="_blank">00:55:14.220</a></span> | <span class="t">So this is now an imagewoof model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3318" target="_blank">00:55:18.220</a></span> | <span class="t">But the learner for it is pointing at the pet's data bunch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3324" target="_blank">00:55:24.120</a></span> | <span class="t">So what we now have to do is remove the final linear layer and replace it with one that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3330" target="_blank">00:55:30.060</a></span> | <span class="t">has the right number of activations to handle all these, which I think is 37 pet breeds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3339" target="_blank">00:55:39.300</a></span> | <span class="t">So what we do is we look through all the children of the model, and we try to find the adaptive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3344" target="_blank">00:55:44.300</a></span> | <span class="t">average pooling layer, because that's that kind of penultimate bit, and we grab the index</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3348" target="_blank">00:55:48.640</a></span> | <span class="t">of that, and then let's create a new model that has everything up to but not including</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3354" target="_blank">00:55:54.360</a></span> | <span class="t">that bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3355" target="_blank">00:55:55.360</a></span> | <span class="t">So this is everything before the adaptive average pooling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3358" target="_blank">00:55:58.940</a></span> | <span class="t">So this is the body.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3361" target="_blank">00:56:01.820</a></span> | <span class="t">So now we need to attach a new head to this body, which is going to have 37 activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3368" target="_blank">00:56:08.180</a></span> | <span class="t">in the linear layer instead of 10, which is a bit tricky because we need to know how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3374" target="_blank">00:56:14.140</a></span> | <span class="t">inputs are going to be required in this new linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3378" target="_blank">00:56:18.180</a></span> | <span class="t">And the number of inputs will be however many outputs come out of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3383" target="_blank">00:56:23.820</a></span> | <span class="t">So in other words, just before the average pooling happens in the x res net, how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3392" target="_blank">00:56:32.940</a></span> | <span class="t">activations are there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3395" target="_blank">00:56:35.020</a></span> | <span class="t">How many channels?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3396" target="_blank">00:56:36.020</a></span> | <span class="t">Well, there's an easy way to find out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3399" target="_blank">00:56:39.660</a></span> | <span class="t">Grab a batch of data, put it through a cut down model, and look at the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3405" target="_blank">00:56:45.860</a></span> | <span class="t">And the answer is, there's 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3407" target="_blank">00:56:47.860</a></span> | <span class="t">Okay?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3408" target="_blank">00:56:48.860</a></span> | <span class="t">So we've got a 128 mini batch of 512 4x4 activations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3415" target="_blank">00:56:55.620</a></span> | <span class="t">So that pred dot shape one is the number of inputs to our head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3420" target="_blank">00:57:00.400</a></span> | <span class="t">And so we can now create our head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3422" target="_blank">00:57:02.980</a></span> | <span class="t">This is basically it here, our linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3426" target="_blank">00:57:06.020</a></span> | <span class="t">But remember, we tend to not just use a max pool or just an average pool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3431" target="_blank">00:57:11.980</a></span> | <span class="t">We tend to do both and concatenate them together, which is something we've been doing in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3438" target="_blank">00:57:18.460</a></span> | <span class="t">course forever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3439" target="_blank">00:57:19.460</a></span> | <span class="t">But a couple of years, somebody finally did actually write a paper about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3442" target="_blank">00:57:22.500</a></span> | <span class="t">So I think this is actually an official thing now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3445" target="_blank">00:57:25.480</a></span> | <span class="t">And it generally gives a nice little boost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3448" target="_blank">00:57:28.740</a></span> | <span class="t">So our linear layer needs twice as many inputs because we've got two sets of pooling we did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3455" target="_blank">00:57:35.420</a></span> | <span class="t">So our new model contains the whole head, plus a adaptive concat pooling, platen, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3462" target="_blank">00:57:42.940</a></span> | <span class="t">our linear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3464" target="_blank">00:57:44.420</a></span> | <span class="t">And so let's replace the model with that new model we created and fit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3469" target="_blank">00:57:49.380</a></span> | <span class="t">And look at that, 71% by fine tuning versus 37% training from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3477" target="_blank">00:57:57.600</a></span> | <span class="t">So that looks good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3479" target="_blank">00:57:59.380</a></span> | <span class="t">So we have a simple transfer learning working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3486" target="_blank">00:58:06.100</a></span> | <span class="t">So what I did then, I do this in Jupyter all the time, I basically grabbed all the cells.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3493" target="_blank">00:58:13.660</a></span> | <span class="t">I hit C to copy, and then I hit V to paste.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3496" target="_blank">00:58:16.780</a></span> | <span class="t">And then I grabbed them all, and I hit shift M to merge, and chucked a function header</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3501" target="_blank">00:58:21.620</a></span> | <span class="t">on top.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3502" target="_blank">00:58:22.620</a></span> | <span class="t">So now I've got a function that does all the-- so these are all the lines you saw just before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3506" target="_blank">00:58:26.460</a></span> | <span class="t">And I've just stuck them all together into a function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3509" target="_blank">00:58:29.180</a></span> | <span class="t">I call it adapt model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3510" target="_blank">00:58:30.580</a></span> | <span class="t">It's going to take a learner and adapt it for the new data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3514" target="_blank">00:58:34.940</a></span> | <span class="t">So these are all the lines of code you've already seen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3517" target="_blank">00:58:37.140</a></span> | <span class="t">And so now we can just go CNN learner, load the state dict, adapt the model, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3524" target="_blank">00:58:44.700</a></span> | <span class="t">we can start training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3526" target="_blank">00:58:46.020</a></span> | <span class="t">But of course, what we really like to do is to first of all train only the head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3532" target="_blank">00:58:52.420</a></span> | <span class="t">So let's grab all the parameters in the body.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3536" target="_blank">00:58:56.060</a></span> | <span class="t">And remember, when we did that nn.sequential, the body is just the first thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3542" target="_blank">00:59:02.520</a></span> | <span class="t">That's the whole ResNet body.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3545" target="_blank">00:59:05.100</a></span> | <span class="t">So let's grab all the parameters in the body and set them to requires grad equals false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3553" target="_blank">00:59:13.500</a></span> | <span class="t">So it's frozen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3555" target="_blank">00:59:15.420</a></span> | <span class="t">And so now we can train just the head, and we get 54%, which is great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3560" target="_blank">00:59:20.520</a></span> | <span class="t">So now we, as you know, unfreeze and train some more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3566" target="_blank">00:59:26.700</a></span> | <span class="t">Uh-oh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3568" target="_blank">00:59:28.420</a></span> | <span class="t">So it's better than not fine tuning, but interestingly, it's worse-- 71 versus 56-- it's worse than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3581" target="_blank">00:59:41.700</a></span> | <span class="t">the kind of naive fine tuning, where we didn't do any freezing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3586" target="_blank">00:59:46.080</a></span> | <span class="t">So what's going on there?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3589" target="_blank">00:59:49.220</a></span> | <span class="t">Anytime something weird happens in your neural net, it's almost certainly because of batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3592" target="_blank">00:59:52.420</a></span> | <span class="t">norm, because batch norm makes everything weird.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3595" target="_blank">00:59:55.700</a></span> | <span class="t">And that's true here, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3597" target="_blank">00:59:57.580</a></span> | <span class="t">What happened was our frozen part of our model, which was designed for ImageWolf, those layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3606" target="_blank">01:00:06.380</a></span> | <span class="t">were tuned for some particular set of mean and standard deviations, because remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3613" target="_blank">01:00:13.740</a></span> | <span class="t">the batch norm is going to subtract the mean and divide by the standard deviation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3620" target="_blank">01:00:20.580</a></span> | <span class="t">But the PETS data set has different means and standard deviations, not for the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3626" target="_blank">01:00:26.580</a></span> | <span class="t">but inside the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3629" target="_blank">01:00:29.080</a></span> | <span class="t">So then when we unfroze this, it basically said this final layer was getting trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3637" target="_blank">01:00:37.100</a></span> | <span class="t">for everything being frozen, but that was for a different set of batch norm statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3642" target="_blank">01:00:42.780</a></span> | <span class="t">So then when we unfroze it, everything tried to catch up, and it would be very interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3650" target="_blank">01:00:50.180</a></span> | <span class="t">to look at the histograms and stuff that we did earlier in the course and see what's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3655" target="_blank">01:00:55.020</a></span> | <span class="t">going on, because I haven't really seen anybody-- I haven't really seen a paper about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3660" target="_blank">01:01:00.720</a></span> | <span class="t">Something we've been doing in FastAI for a few years now, but I think this is the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3665" target="_blank">01:01:05.020</a></span> | <span class="t">course where we've actually drawn attention to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3670" target="_blank">01:01:10.180</a></span> | <span class="t">That's something that's been hidden away in the library before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3672" target="_blank">01:01:12.580</a></span> | <span class="t">But as you can see, it's a huge difference, the difference between 56 versus 71.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3678" target="_blank">01:01:18.940</a></span> | <span class="t">So the good news is it's easily fixed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3682" target="_blank">01:01:22.580</a></span> | <span class="t">And the trick is to not freeze all of the body parameters, but freeze all of the body</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3689" target="_blank">01:01:29.860</a></span> | <span class="t">parameters that aren't in the batch norm layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3693" target="_blank">01:01:33.940</a></span> | <span class="t">And that way, when we fine-tune the final layer, we're also fine-tuning all of the batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3698" target="_blank">01:01:38.660</a></span> | <span class="t">norm layers' weights and biases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3701" target="_blank">01:01:41.140</a></span> | <span class="t">So we can create, just like before, adapt the model, and let's create something called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3707" target="_blank">01:01:47.080</a></span> | <span class="t">setGradient, which says, oh, if it's a linear layer at the end or a batch norm layer in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3711" target="_blank">01:01:51.980</a></span> | <span class="t">the middle, return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3713" target="_blank">01:01:53.820</a></span> | <span class="t">Don't change the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3715" target="_blank">01:01:55.020</a></span> | <span class="t">Otherwise, if it's got weights, set requires grad2, whatever you asked for, which we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3720" target="_blank">01:02:00.180</a></span> | <span class="t">going to start false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3724" target="_blank">01:02:04.060</a></span> | <span class="t">Here's a little convenient function that will apply any function you pass to it recursively</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3729" target="_blank">01:02:09.740</a></span> | <span class="t">to all of the children of a model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3731" target="_blank">01:02:11.820</a></span> | <span class="t">So now that we have apply to a model, or apply to a module, I guess, we can just pass in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3737" target="_blank">01:02:17.860</a></span> | <span class="t">a module, and that will be applied throughout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3742" target="_blank">01:02:22.220</a></span> | <span class="t">So this way, we freeze just the non-batch norm layers, and of course, not the last layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3750" target="_blank">01:02:30.000</a></span> | <span class="t">And so actually, fine-tuning immediately is a bit better, goes from 54 to 58.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3755" target="_blank">01:02:35.320</a></span> | <span class="t">But more importantly, then when we unfreeze, we're back into the 70s again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3762" target="_blank">01:02:42.500</a></span> | <span class="t">So this is just a super important thing to remember, if you're doing fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3766" target="_blank">01:02:46.460</a></span> | <span class="t">And I don't think there's any library other than fast.ai that does this, weirdly enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3771" target="_blank">01:02:51.260</a></span> | <span class="t">So if you're using TensorFlow or something, you'll have to write this yourself to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3776" target="_blank">01:02:56.360</a></span> | <span class="t">sure that you don't freeze ever, don't ever freeze the weights in the batch norm layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3782" target="_blank">01:03:02.300</a></span> | <span class="t">any time you're doing partial layer training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3785" target="_blank">01:03:05.940</a></span> | <span class="t">Oh, by the way, that apply mod, I only wrote it because we're not allowed to use stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3790" target="_blank">01:03:10.620</a></span> | <span class="t">in PyTorch, but actually PyTorch has its own, it's called model.apply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3794" target="_blank">01:03:14.580</a></span> | <span class="t">So you can use that now, it's the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3798" target="_blank">01:03:18.940</a></span> | <span class="t">Okay, so finally, for this half of the course, we're going to look at discriminative learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3806" target="_blank">01:03:26.380</a></span> | <span class="t">rates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3807" target="_blank">01:03:27.880</a></span> | <span class="t">So for discriminative learning rates, there's a few things we can do with them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3811" target="_blank">01:03:31.660</a></span> | <span class="t">One is it's a simple way to do layer freezing without actually worrying about setting requires</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3817" target="_blank">01:03:37.100</a></span> | <span class="t">grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3818" target="_blank">01:03:38.100</a></span> | <span class="t">We could just set the learning rate to zero for some layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3820" target="_blank">01:03:40.820</a></span> | <span class="t">So let's start by doing that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3823" target="_blank">01:03:43.460</a></span> | <span class="t">So what we're going to do is we're going to split our parameters into two or more groups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3831" target="_blank">01:03:51.100</a></span> | <span class="t">with a function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3832" target="_blank">01:03:52.100</a></span> | <span class="t">Here's our function, it's called bnsplitter, it's going to create two groups of parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3841" target="_blank">01:04:01.400</a></span> | <span class="t">and it's going to pass the body to underscore bnsplitter, which will recursively look for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3847" target="_blank">01:04:07.580</a></span> | <span class="t">batch norm layers and put them in the second group or anything else with a weight goes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3852" target="_blank">01:04:12.500</a></span> | <span class="t">in the first group and then do it recursively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3856" target="_blank">01:04:16.240</a></span> | <span class="t">And then also the second group will add everything after the head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3860" target="_blank">01:04:20.060</a></span> | <span class="t">So this is basically doing something where we're putting all our parameters into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3864" target="_blank">01:04:24.620</a></span> | <span class="t">two groups we want to treat differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3868" target="_blank">01:04:28.060</a></span> | <span class="t">So we can check, for example, that when we do bnsplitter on a model that the number of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3872" target="_blank">01:04:32.860</a></span> | <span class="t">parameters in the two halves is equal to the total number of parameters in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3878" target="_blank">01:04:38.580</a></span> | <span class="t">And so now I want to check this works, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3881" target="_blank">01:04:41.180</a></span> | <span class="t">I want to make sure that if I pass this, because we now have a splitter function in the learner,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3886" target="_blank">01:04:46.340</a></span> | <span class="t">and that's another thing I added this week, that when you start training, it's literally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3892" target="_blank">01:04:52.060</a></span> | <span class="t">just this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3893" target="_blank">01:04:53.180</a></span> | <span class="t">When we create an optimizer, it passes the model to self.splitter, which by default does</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3899" target="_blank">01:04:59.580</a></span> | <span class="t">nothing at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3901" target="_blank">01:05:01.660</a></span> | <span class="t">And so we're going to be using our bnsplitter to split it into multiple parameter groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3906" target="_blank">01:05:06.660</a></span> | <span class="t">And so how do we debug that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3907" target="_blank">01:05:07.700</a></span> | <span class="t">How do we make sure it's working?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3908" target="_blank">01:05:08.700</a></span> | <span class="t">Because this is one of these things that if I screw it up, I probably won't get an error,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3913" target="_blank">01:05:13.500</a></span> | <span class="t">but instead it probably won't train my last layer, or it'll train all the layers at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3917" target="_blank">01:05:17.580</a></span> | <span class="t">same learning rate, or it would be hard to know if the model was bad because I screwed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3922" target="_blank">01:05:22.460</a></span> | <span class="t">up my code or not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3923" target="_blank">01:05:23.580</a></span> | <span class="t">So we need a way to debug it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3926" target="_blank">01:05:26.960</a></span> | <span class="t">We can't just look inside and make sure it's working, because what we're going to be doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3931" target="_blank">01:05:31.740</a></span> | <span class="t">is we're going to be passing it, let's see this one, we're going to be passing it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3942" target="_blank">01:05:42.700</a></span> | <span class="t">the splitter parameter when we create the learner, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3946" target="_blank">01:05:46.220</a></span> | <span class="t">So after this, it set the splitter parameter, and then when we start training, we're hoping</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3951" target="_blank">01:05:51.420</a></span> | <span class="t">that it's going to create these two layer groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3953" target="_blank">01:05:53.300</a></span> | <span class="t">So we need some way to look inside the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3956" target="_blank">01:05:56.180</a></span> | <span class="t">So of course, we're going to use a callback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3958" target="_blank">01:05:58.460</a></span> | <span class="t">And this is something that's super cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3959" target="_blank">01:05:59.620</a></span> | <span class="t">Do you remember how I told you that you can actually override dundercall itself?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3965" target="_blank">01:06:05.220</a></span> | <span class="t">You don't just have to override a specific callback?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3967" target="_blank">01:06:07.900</a></span> | <span class="t">And by overriding dundercall itself, we can actually say, which callback do we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3973" target="_blank">01:06:13.860</a></span> | <span class="t">debug?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3975" target="_blank">01:06:15.620</a></span> | <span class="t">And when we hit that callback, please run this function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3979" target="_blank">01:06:19.540</a></span> | <span class="t">And if you don't pass in a function, it just jumps into the debugger as soon as that callback</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3983" target="_blank">01:06:23.180</a></span> | <span class="t">is hit, otherwise call the function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3985" target="_blank">01:06:25.860</a></span> | <span class="t">So this is super handy, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3987" target="_blank">01:06:27.380</a></span> | <span class="t">Because now I can create a function called print details that just prints out how many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3992" target="_blank">01:06:32.040</a></span> | <span class="t">parameter groups there are and what the hyperparameters there are, and then immediately raises the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3996" target="_blank">01:06:36.180</a></span> | <span class="t">cancel train exception to stop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=3999" target="_blank">01:06:39.140</a></span> | <span class="t">And so then I can fit with my discriminative LR scheduler and my debug callback, and my</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4005" target="_blank">01:06:45.780</a></span> | <span class="t">discriminative LR scheduler is something that now doesn't just take a learning rate, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4010" target="_blank">01:06:50.380</a></span> | <span class="t">an array of learning rates and creates a scheduler for every learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4015" target="_blank">01:06:55.220</a></span> | <span class="t">And so I can pass that in, so I'm going to use 0 and 0.02.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4020" target="_blank">01:07:00.620</a></span> | <span class="t">So in other words, no training for the body and 0.03 for the head and the batch norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4031" target="_blank">01:07:11.480</a></span> | <span class="t">And so as soon as I fit, it immediately stops because the cancel train exception was raised,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4037" target="_blank">01:07:17.100</a></span> | <span class="t">and it prints out and says there's two parameter groups, which is what we want, and the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4041" target="_blank">01:07:21.460</a></span> | <span class="t">parameter group has a learning rate of 0, which is what we want, and the second is 0.003,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4047" target="_blank">01:07:27.420</a></span> | <span class="t">which is right because it's 0.03, and we're using the learning rate scheduler so it starts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4054" target="_blank">01:07:34.120</a></span> | <span class="t">out 10 times smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4056" target="_blank">01:07:36.180</a></span> | <span class="t">So this is just a way of saying if you're anything like me, every time you write code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4061" target="_blank">01:07:41.940</a></span> | <span class="t">it will always be wrong, and for this kind of code, you won't know it's wrong, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4067" target="_blank">01:07:47.380</a></span> | <span class="t">could be writing a paper or doing a project at work or whatever in which you're not using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4072" target="_blank">01:07:52.940</a></span> | <span class="t">discriminative learning rates at all because of some bug because you didn't know how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4076" target="_blank">01:07:56.580</a></span> | <span class="t">check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4077" target="_blank">01:07:57.580</a></span> | <span class="t">So make sure you can check and always assume that you screw up everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4083" target="_blank">01:08:03.420</a></span> | <span class="t">Okay, so now we can train with zero learning rate on the first layer group, and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4093" target="_blank">01:08:13.300</a></span> | <span class="t">can use discriminative learning rates with 1 and a 3 and 1 and a 2 and train a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4096" target="_blank">01:08:16.620</a></span> | <span class="t">bit more, and that all works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4100" target="_blank">01:08:20.020</a></span> | <span class="t">Okay, so that's all the tweaks we have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4104" target="_blank">01:08:24.680</a></span> | <span class="t">Any questions, Rachel?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4105" target="_blank">01:08:25.680</a></span> | <span class="t">A bit too tangential questions come up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4108" target="_blank">01:08:28.140</a></span> | <span class="t">They're my favorite.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4110" target="_blank">01:08:30.300</a></span> | <span class="t">The first is we heard that you're against cross-validation for deep learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4115" target="_blank">01:08:35.360</a></span> | <span class="t">We heard that you're against cross-validation for deep learning and wanted to know why that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4119" target="_blank">01:08:39.260</a></span> | <span class="t">is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4120" target="_blank">01:08:40.780</a></span> | <span class="t">And the second question...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4121" target="_blank">01:08:41.780</a></span> | <span class="t">Let's do it one at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4123" target="_blank">01:08:43.780</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4124" target="_blank">01:08:44.780</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4125" target="_blank">01:08:45.780</a></span> | <span class="t">So cross-validation is a very useful technique for getting a reasonably sized validation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4130" target="_blank">01:08:50.900</a></span> | <span class="t">set if you don't have enough data to otherwise create a reasonably sized validation set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4136" target="_blank">01:08:56.380</a></span> | <span class="t">So it was particularly popular in the days when most studies were like 50 or 60 rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4142" target="_blank">01:09:02.740</a></span> | <span class="t">If you've got a few thousand rows, it's just pointless, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4146" target="_blank">01:09:06.580</a></span> | <span class="t">Like the kind of statistical significance is going to be there regardless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4150" target="_blank">01:09:10.220</a></span> | <span class="t">So I wouldn't say I'm against it, just most of the time you don't need it because if you've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4155" target="_blank">01:09:15.100</a></span> | <span class="t">got a thousand things in the validation set and you only care whether it's like plus or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4158" target="_blank">01:09:18.820</a></span> | <span class="t">minus 1%, it's totally pointless.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4162" target="_blank">01:09:22.780</a></span> | <span class="t">So yeah, have a look and see how much your validation set accuracy is varying from run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4167" target="_blank">01:09:27.100</a></span> | <span class="t">to run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4168" target="_blank">01:09:28.100</a></span> | <span class="t">And if it's too much that you can't make the decisions you need to make, then you can add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4173" target="_blank">01:09:33.020</a></span> | <span class="t">cross-validation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4177" target="_blank">01:09:37.420</a></span> | <span class="t">And what are your best tips for debugging deep learning?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4185" target="_blank">01:09:45.080</a></span> | <span class="t">So Chris Latner asked me this today as well, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4188" target="_blank">01:09:48.780</a></span> | <span class="t">So I'll answer the same answer to him, which is don't make mistakes in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4197" target="_blank">01:09:57.180</a></span> | <span class="t">And the only way to do that is to make your code so simple that it can't possibly have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4202" target="_blank">01:10:02.100</a></span> | <span class="t">a mistake and to check every single intermediate result along the way to make sure it doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4207" target="_blank">01:10:07.700</a></span> | <span class="t">have a mistake.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4209" target="_blank">01:10:09.260</a></span> | <span class="t">Otherwise, your last month might have been like my last month.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4213" target="_blank">01:10:13.300</a></span> | <span class="t">What happened in my last month?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4215" target="_blank">01:10:15.060</a></span> | <span class="t">Well, a month ago, I got 94.1% accuracy on ImageNet, and I was very happy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4223" target="_blank">01:10:23.460</a></span> | <span class="t">And then I started a couple of weeks ago trying various tweaks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4227" target="_blank">01:10:27.340</a></span> | <span class="t">And none of the tweaks seemed to help.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4229" target="_blank">01:10:29.860</a></span> | <span class="t">And after a while, I got so frustrated, I thought I'd just repeat the previous training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4233" target="_blank">01:10:33.020</a></span> | <span class="t">to see if it was like what was going on with the Fluke.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4236" target="_blank">01:10:36.020</a></span> | <span class="t">And I couldn't repeat it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4237" target="_blank">01:10:37.180</a></span> | <span class="t">I was now getting 93.5 instead of 94.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4242" target="_blank">01:10:42.360</a></span> | <span class="t">And I trained it like a bunch of times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4243" target="_blank">01:10:43.900</a></span> | <span class="t">And every time I trained it, it was costing me $150 of AWS credits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4249" target="_blank">01:10:49.060</a></span> | <span class="t">So I wasn't thrilled about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4250" target="_blank">01:10:50.860</a></span> | <span class="t">And it was six hours of waiting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4256" target="_blank">01:10:56.300</a></span> | <span class="t">So that was quite a process to even realize like it's broken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4259" target="_blank">01:10:59.020</a></span> | <span class="t">This is the kind of thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4260" target="_blank">01:11:00.020</a></span> | <span class="t">Like when something, when you've written that kind of code wrong, it gets broken in ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4264" target="_blank">01:11:04.320</a></span> | <span class="t">you don't even notice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4265" target="_blank">01:11:05.320</a></span> | <span class="t">It was broken for weeks in fast AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4268" target="_blank">01:11:08.460</a></span> | <span class="t">And nobody noticed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4270" target="_blank">01:11:10.900</a></span> | <span class="t">So eventually, I realized, yeah, I mean, so the first thing I'll say is, you've got to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4276" target="_blank">01:11:16.940</a></span> | <span class="t">be a great scientist, which means you need a journal notebook, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4280" target="_blank">01:11:20.300</a></span> | <span class="t">You need to keep track of your journal results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4283" target="_blank">01:11:23.540</a></span> | <span class="t">So I had a good journal, I pasted everything that was going on, all my models into a file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4288" target="_blank">01:11:28.980</a></span> | <span class="t">So I went back, I confirmed it really was 94.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4293" target="_blank">01:11:33.020</a></span> | <span class="t">I could see exactly when it was.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4295" target="_blank">01:11:35.580</a></span> | <span class="t">And so then I could revert to the exact commit that was in fast AI at that time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4300" target="_blank">01:11:40.940</a></span> | <span class="t">And I reran it, and I got 94.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4304" target="_blank">01:11:44.540</a></span> | <span class="t">So I now had to figure out which change in the previous month of the entire fast AI code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4311" target="_blank">01:11:51.720</a></span> | <span class="t">base caused this to break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4314" target="_blank">01:11:54.260</a></span> | <span class="t">So the first thing I tried to do was try to find a way to quickly figure out whether something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4317" target="_blank">01:11:57.740</a></span> | <span class="t">was broken.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4318" target="_blank">01:11:58.900</a></span> | <span class="t">But after doing a few runs and plotting them in Excel, it was very clear that the training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4324" target="_blank">01:12:04.740</a></span> | <span class="t">was identical until epoch 50.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4328" target="_blank">01:12:08.220</a></span> | <span class="t">So until epoch 50 out of 60.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4330" target="_blank">01:12:10.540</a></span> | <span class="t">So there was no shortcut.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4333" target="_blank">01:12:13.140</a></span> | <span class="t">And so I did a bisection search one module at a time, looking through the 15 modules</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4338" target="_blank">01:12:18.740</a></span> | <span class="t">that had changed in that diff until eventually I find it was in the mixed precision module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4342" target="_blank">01:12:22.980</a></span> | <span class="t">And then I went through each change that happened in the mixed position module.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4345" target="_blank">01:12:25.460</a></span> | <span class="t">So like $5,000 later, I finally found the one line of code where we had forgotten to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4350" target="_blank">01:12:30.940</a></span> | <span class="t">write the four letters dot opt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4353" target="_blank">01:12:33.520</a></span> | <span class="t">And so by failing to write dot opt, it meant that we were wrapping an Optim wrapper in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4357" target="_blank">01:12:37.940</a></span> | <span class="t">an Optim wrapper, rather than wrapping an Optim wrapper with an optimizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4361" target="_blank">01:12:41.600</a></span> | <span class="t">And that meant that weight decay was being applied twice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4365" target="_blank">01:12:45.420</a></span> | <span class="t">So that tiny difference, like, was so insignificant that no one using the library even noticed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4372" target="_blank">01:12:52.260</a></span> | <span class="t">it wasn't working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4374" target="_blank">01:12:54.260</a></span> | <span class="t">I didn't notice it wasn't working until I started trying to, you know, get state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4378" target="_blank">01:12:58.680</a></span> | <span class="t">results on ImageNet in 60 epochs with ResNet 50.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4385" target="_blank">01:13:05.540</a></span> | <span class="t">So yeah, I mean, debugging is hard, and worth still is most of the time you don't know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4392" target="_blank">01:13:12.060</a></span> | <span class="t">So I mean, honestly, training models sucks, and deep learning is a miserable experience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4398" target="_blank">01:13:18.380</a></span> | <span class="t">and you shouldn't do it, but on the other hand, it gives you much better results than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4403" target="_blank">01:13:23.740</a></span> | <span class="t">anything else, and it's taking over the world.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4405" target="_blank">01:13:25.460</a></span> | <span class="t">So it's either that or get eaten by everybody else, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4409" target="_blank">01:13:29.860</a></span> | <span class="t">So yeah, I mean, it's so much easier to write normal code where, like, oh, you have to implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4415" target="_blank">01:13:35.020</a></span> | <span class="t">a wealth authentication in your web service, and so you go in and you say, oh, here's the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4419" target="_blank">01:13:39.500</a></span> | <span class="t">API, and we have to take these five steps, and after each one I check that this has happened,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4423" target="_blank">01:13:43.580</a></span> | <span class="t">and you check off each one, and at the end you're done, and you push it, and you have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4426" target="_blank">01:13:46.740</a></span> | <span class="t">integration tests, and that's it, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4429" target="_blank">01:13:49.500</a></span> | <span class="t">Even testing, it requires a totally different mindset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4433" target="_blank">01:13:53.620</a></span> | <span class="t">So you don't want reproducible tests.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4436" target="_blank">01:13:56.580</a></span> | <span class="t">You want tests with randomness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4437" target="_blank">01:13:57.860</a></span> | <span class="t">You want to be able to see if something's changing just occasionally, because if it tests correctly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4442" target="_blank">01:14:02.700</a></span> | <span class="t">all the time with a random set of 42, be sure it's going to work with a random set of 41.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4447" target="_blank">01:14:07.660</a></span> | <span class="t">So you want non-reproducible tests, you want randomness, you want tests that aren't guaranteed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4453" target="_blank">01:14:13.060</a></span> | <span class="t">to always pass, but the accuracy of this integration test should be better than 0.9 nearly all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4458" target="_blank">01:14:18.380</a></span> | <span class="t">time. You want to be warned if something looks off, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4464" target="_blank">01:14:24.980</a></span> | <span class="t">And this means it's a very different software development process, because if you push something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4467" target="_blank">01:14:27.980</a></span> | <span class="t">to the fast AI repo and a test fails, it might not be your fault, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4472" target="_blank">01:14:32.980</a></span> | <span class="t">It might be that Jeremy screwed something up a month ago, and one test fails one out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4476" target="_blank">01:14:36.660</a></span> | <span class="t">of every thousand times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4478" target="_blank">01:14:38.580</a></span> | <span class="t">So as soon as that happens, then we try to write a test that fails every time, you know?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4485" target="_blank">01:14:45.220</a></span> | <span class="t">So once you realize there's a problem with this thing, you try to find a way to make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4488" target="_blank">01:14:48.740</a></span> | <span class="t">it fail every time, but it's -- yeah, debugging is difficult, and in the end, you just have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4495" target="_blank">01:14:55.300</a></span> | <span class="t">to go through each step, look at your data, make sure it looks sensible, plot it, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4502" target="_blank">01:15:02.980</a></span> | <span class="t">try not to make mistakes in the first place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4505" target="_blank">01:15:05.460</a></span> | <span class="t">Great. Well, let's have a break and see you back here at 7.55.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4512" target="_blank">01:15:12.500</a></span> | <span class="t">So we've all done ULM fit in part one, and there's been a lot of stuff happening in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4523" target="_blank">01:15:23.860</a></span> | <span class="t">-- oh, okay. Let's do the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4534" target="_blank">01:15:34.180</a></span> | <span class="t">>> What do you mean by a scientific journal?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4536" target="_blank">01:15:36.940</a></span> | <span class="t">>> Ah. Yeah, that's a good one. This is something I'm quite passionate about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4545" target="_blank">01:15:45.020</a></span> | <span class="t">When you look at the great scientists in history, they all, that I can tell, had careful scientific</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4554" target="_blank">01:15:54.300</a></span> | <span class="t">journal practices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4557" target="_blank">01:15:57.020</a></span> | <span class="t">In my case, my scientific journal is a file in a piece of software called Windows Notepad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4565" target="_blank">01:16:05.140</a></span> | <span class="t">and I paste things into it at the bottom, and when I want to find something, I press</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4569" target="_blank">01:16:09.260</a></span> | <span class="t">control F. It just needs to be something that has a record</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4575" target="_blank">01:16:15.540</a></span> | <span class="t">of what you're doing and what the results of that are, because scientists -- scientists</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4584" target="_blank">01:16:24.980</a></span> | <span class="t">who make breakthroughs generally make the breakthrough because they look at something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4588" target="_blank">01:16:28.980</a></span> | <span class="t">that shouldn't be, and they go, oh, that's odd. I wonder what's going on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4595" target="_blank">01:16:35.420</a></span> | <span class="t">So the discovery of the noble gases was because the scientists saw, like, one little bubble</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4600" target="_blank">01:16:40.340</a></span> | <span class="t">left in a beaker, which they were pretty sure there shouldn't have been a little bubble</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4603" target="_blank">01:16:43.580</a></span> | <span class="t">there anymore. Most people would just be like, oops, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4606" target="_blank">01:16:46.900</a></span> | <span class="t">a bubble, or we wouldn't even notice, but they studied the bubble, and they found noble</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4610" target="_blank">01:16:50.540</a></span> | <span class="t">gases, or penicillin was discovered because of a, oh, that's odd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4619" target="_blank">01:16:59.620</a></span> | <span class="t">And I find in deep learning, this is true as well. Like, I spent a lot of time studying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4625" target="_blank">01:17:05.980</a></span> | <span class="t">batch normalization in transfer learning, because a few years ago in Keras, I was getting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4634" target="_blank">01:17:14.700</a></span> | <span class="t">terrible transfer learning results for something I thought should be much more accurate, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4638" target="_blank">01:17:18.780</a></span> | <span class="t">I thought, oh, that's odd. And I spent weeks changing everything I could, and then almost</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4647" target="_blank">01:17:27.140</a></span> | <span class="t">randomly tried changing batch norm. So the problem is that all this fiddling around,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4656" target="_blank">01:17:36.220</a></span> | <span class="t">you know, 90% of it doesn't really go anywhere, but it's the other 10% that you won't be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4660" target="_blank">01:17:40.020</a></span> | <span class="t">to pick it out unless you can go back and say, like, okay, that really did happen. I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4665" target="_blank">01:17:45.900</a></span> | <span class="t">copied and pasted the log here. So that's all I mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4671" target="_blank">01:17:51.660</a></span> | <span class="t">>> Are you also linking to your GitHub commits and datasets, sir?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4675" target="_blank">01:17:55.380</a></span> | <span class="t">>> No, because I've got the date there and the time. So I know the GitHub commit. So I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4681" target="_blank">01:18:01.980</a></span> | <span class="t">do make sure I'm pushing all the time. So, yeah. Okay. Yeah, so there's been a lot happening</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4694" target="_blank">01:18:14.140</a></span> | <span class="t">in NLP transfer learning recently, the famous GPT2 from OpenAI and BERT and stuff like that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4700" target="_blank">01:18:20.460</a></span> | <span class="t">lots of interest in transformers, which we will cover in a future lesson. One could think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4708" target="_blank">01:18:28.660</a></span> | <span class="t">that LSTMs are out of favor and not interesting anymore. But when you look at actually recent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4716" target="_blank">01:18:36.580</a></span> | <span class="t">competitive machine learning results, you see ULMFIT beating BERT. Now, I should say this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4724" target="_blank">01:18:44.140</a></span> | <span class="t">is not just ULMFIT beating BERT. The guys at Mwaves are super smart, amazing people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4730" target="_blank">01:18:50.500</a></span> | <span class="t">So it's like two super smart, amazing people using ULMFIT bits and other people doing BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4735" target="_blank">01:18:55.820</a></span> | <span class="t">It's definitely not true that RNNs are in the past. I think what's happened is, in fact,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4746" target="_blank">01:19:06.700</a></span> | <span class="t">as you'll see, transformers and CNNs for text have a lot of problems. They basically don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4753" target="_blank">01:19:13.060</a></span> | <span class="t">have state. So if you're doing speech recognition, every sample you look at, you have to do an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4760" target="_blank">01:19:20.220</a></span> | <span class="t">entire analysis of all the samples around it again and again and again. It's ridiculously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4765" target="_blank">01:19:25.860</a></span> | <span class="t">wasteful or else RNNs have state. But they're fiddly and they're hard to deal with, as you'll</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4776" target="_blank">01:19:36.020</a></span> | <span class="t">see, when you want to actually do research and change things. But partly, RNNs have state,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4785" target="_blank">01:19:45.900</a></span> | <span class="t">but also partly, RNNs are the only thing which has had the level of carefulness around regularization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4794" target="_blank">01:19:54.780</a></span> | <span class="t">that AWD LSTM did. So Stephen Meridy looked at what are all the ways I can regularize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4801" target="_blank">01:20:01.780</a></span> | <span class="t">this model and came up with a great set of hyperparameters for that. And there's nothing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4807" target="_blank">01:20:07.220</a></span> | <span class="t">like that outside of the RNN world. So, at the moment, my go-to choice definitely is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4817" target="_blank">01:20:17.300</a></span> | <span class="t">still ULM fit for most real-world NLP tasks. And if people find BERT or GPT2 or whatever</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4832" target="_blank">01:20:32.460</a></span> | <span class="t">better for some real-world tasks, that would be fascinating. I would love that to happen,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4836" target="_blank">01:20:36.060</a></span> | <span class="t">but I haven't been hearing that from people that are actually working in industry yet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4840" target="_blank">01:20:40.500</a></span> | <span class="t">I'm not seeing them win competitive machine learning stuff and so forth. So I still think</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4847" target="_blank">01:20:47.220</a></span> | <span class="t">RNNs should be our focus, but we will also learn about transformers later. And so ULM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4854" target="_blank">01:20:54.060</a></span> | <span class="t">fit is just the normal transfer learning path applied to an RNN, which could be on text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4863" target="_blank">01:21:03.260</a></span> | <span class="t">Interestingly, there's also been a lot of state of the art results recently on genomics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4867" target="_blank">01:21:07.540</a></span> | <span class="t">applications and on chemical bonding analysis and drug discovery. There's lots of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4875" target="_blank">01:21:15.860</a></span> | <span class="t">that are sequences and it turns out, and we're still just at the tip of the iceberg, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4882" target="_blank">01:21:22.020</a></span> | <span class="t">Because most people that are studying like drug discovery or chemical bonding or genomics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4887" target="_blank">01:21:27.660</a></span> | <span class="t">have never heard of ULM fit, right? So it's still the tip of the iceberg. But those who</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4890" target="_blank">01:21:30.940</a></span> | <span class="t">are trying it are consistently getting breakthrough results. So I think it's really interesting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4896" target="_blank">01:21:36.100</a></span> | <span class="t">not just for NLP, but for all kinds of sequence classification tasks. So the basic process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4901" target="_blank">01:21:41.420</a></span> | <span class="t">is going to be create a language model on some large data set. And notice a language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4908" target="_blank">01:21:48.580</a></span> | <span class="t">model is a very general term. It means predict the next item in the sequence. So it could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4913" target="_blank">01:21:53.900</a></span> | <span class="t">be an audio language model that predicts the next sample in a piece of music or speech.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4919" target="_blank">01:21:59.900</a></span> | <span class="t">It could be predicting the next genome in a sequence or whatever, right? So that's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4927" target="_blank">01:22:07.100</a></span> | <span class="t">I mean by language model. And then we fine-tune it, that language model using our in-domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4934" target="_blank">01:22:14.580</a></span> | <span class="t">corpus, which in this case is going to be IMDB. And then in each case, we first have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4939" target="_blank">01:22:19.260</a></span> | <span class="t">to pre-process our data sets to get them ready for using an RNN on them. Language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4949" target="_blank">01:22:29.900</a></span> | <span class="t">require one kind of pre-processing. Classification models require another one. And then finally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4954" target="_blank">01:22:34.700</a></span> | <span class="t">we can fine-tune our IMDB language model for classification. So this is the process we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4960" target="_blank">01:22:40.580</a></span> | <span class="t">going to go through from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4963" target="_blank">01:22:43.920</a></span> | <span class="t">So Sylvain has done an amazing thing in the last week, which is basically to recreate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4970" target="_blank">01:22:50.060</a></span> | <span class="t">the entire AWD LSTM and ULM fit process from scratch in the next four notebooks. And there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4979" target="_blank">01:22:59.300</a></span> | <span class="t">quite a lot in here, but a lot of it's kind of specific to text processing. And so some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4986" target="_blank">01:23:06.660</a></span> | <span class="t">of it I might skip over a little bit quickly, but we'll talk about which bits are interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4991" target="_blank">01:23:11.340</a></span> | <span class="t">So we're going to start with the IMDB data set as we have before. And to remind you it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=4997" target="_blank">01:23:17.140</a></span> | <span class="t">contains a training folder, an unsupervised folder, and a testing folder. So the first</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5003" target="_blank">01:23:23.980</a></span> | <span class="t">thing we need to do is we need to create a data blocks item list subclass for text. Believe</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5009" target="_blank">01:23:29.780</a></span> | <span class="t">it or not, that's the entire code. Because we already have a get files, so here's a get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5016" target="_blank">01:23:36.080</a></span> | <span class="t">files with dot text. And all you have to do is override get to open a text file like so.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5024" target="_blank">01:23:44.420</a></span> | <span class="t">And we're now ready to create an item list. So this is like the data blocks API is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5030" target="_blank">01:23:50.100</a></span> | <span class="t">so super easy to create, you know, to handle your domain. So if you've got genomic sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5036" target="_blank">01:23:56.740</a></span> | <span class="t">or audio or whatever, this is basically what you need to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5042" target="_blank">01:24:02.980</a></span> | <span class="t">So now we've got an item list with 100,000 things in it. We've got the train, the test,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5047" target="_blank">01:24:07.340</a></span> | <span class="t">and the unsupervised. And we can index into it and see a text. So here's a movie review.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5054" target="_blank">01:24:14.540</a></span> | <span class="t">And we can use all the same stuff that we've used before. So for the previous notebook,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5058" target="_blank">01:24:18.500</a></span> | <span class="t">we just built a random splitter. So now we can use it on texts. So the nice thing about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5062" target="_blank">01:24:22.820</a></span> | <span class="t">this decoupled API is that we can mix and match things and things just work, right? And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5067" target="_blank">01:24:27.620</a></span> | <span class="t">can see the representation of them. They just work. Okay, so we can't throw this movie review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5074" target="_blank">01:24:34.580</a></span> | <span class="t">into a model. It needs to be numbers. And so as you know, we need to tokenize and numericalize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5081" target="_blank">01:24:41.140</a></span> | <span class="t">this. So let's look at the details. We use spacey for tokenizing. And we do a few things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5089" target="_blank">01:24:49.900</a></span> | <span class="t">as we tokenize. One thing we do is we have a few pre rules. These are these are bits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5096" target="_blank">01:24:56.580</a></span> | <span class="t">of code that get run before tokenization. So for example, if we find br slash, we replace</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5103" target="_blank">01:25:03.940</a></span> | <span class="t">it with a new line. Or if we find a slash or a hash, we put spaces around it. If we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5109" target="_blank">01:25:09.660</a></span> | <span class="t">find more than two spaces in a row, we just make it one space. Then we have these special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5117" target="_blank">01:25:17.020</a></span> | <span class="t">tokens. And this is what they look like as strings that we use symbolic names for them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5124" target="_blank">01:25:24.140</a></span> | <span class="t">essentially. And these different tokens have various special meanings. For example, if we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5129" target="_blank">01:25:29.820</a></span> | <span class="t">see some non-whitespace character more than three times in a row, we replace it with this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5142" target="_blank">01:25:42.300</a></span> | <span class="t">is really cool, right? In Python substitution, you can pass in a function, right? So rep.sub</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5149" target="_blank">01:25:49.740</a></span> | <span class="t">here is going to look for this and then it's going to replace it with the result of calling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5153" target="_blank">01:25:53.580</a></span> | <span class="t">this function, which is really nice. And so what we're going to do is we're going to stick</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5157" target="_blank">01:25:57.880</a></span> | <span class="t">in the TK rep special token. So this means that there was a repeating token where they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5165" target="_blank">01:26:05.940</a></span> | <span class="t">going to put a number, which is how many times it repeated. And then the thing that was actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5170" target="_blank">01:26:10.140</a></span> | <span class="t">there. We'll do the same thing with words. There's a lot of bits of little crappy things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5175" target="_blank">01:26:15.580</a></span> | <span class="t">that we see in texts that we replace mainly HTML entities. We call those our default pre-rules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5184" target="_blank">01:26:24.540</a></span> | <span class="t">And then this is our default list of special tokens. So for example, replace rep C C C</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5189" target="_blank">01:26:29.020</a></span> | <span class="t">would be XX rep for C. Or replace W rep, would, would, would, would, would, would, would, would,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5193" target="_blank">01:26:33.940</a></span> | <span class="t">would be XXW rep 5 would. Why? Well, think about the alternatives, right? So what if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5204" target="_blank">01:26:44.420</a></span> | <span class="t">you read a tweet that said this was amazing 28 exclamation marks. So you can either treat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5213" target="_blank">01:26:53.820</a></span> | <span class="t">those 28 exclamation marks as one token. And so now you have a vocab item that is specifically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5220" target="_blank">01:27:00.220</a></span> | <span class="t">28 exclamation marks. You probably never see that again, so probably won't even end up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5224" target="_blank">01:27:04.420</a></span> | <span class="t">in your vocab. And if it did, you know, it's, it's going to be so rare that you won't be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5229" target="_blank">01:27:09.540</a></span> | <span class="t">able to learn anything interesting about it. But if instead we replaced it with XX rep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5235" target="_blank">01:27:15.420</a></span> | <span class="t">28 exclamation mark, then this is just three tokens where it can learn that lots of repeating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5244" target="_blank">01:27:24.380</a></span> | <span class="t">exclamation marks is a general concept that has certain semantics to it, right? So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5250" target="_blank">01:27:30.260</a></span> | <span class="t">what we're trying to do in NLP is we're trying to make it so that the things in our vocab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5255" target="_blank">01:27:35.860</a></span> | <span class="t">are as meaningful as possible. And the nice thing is that because we're using an LSTM,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5260" target="_blank">01:27:40.860</a></span> | <span class="t">we can have multi-word sequences and be confident that the LSTM will create some stateful computation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5267" target="_blank">01:27:47.520</a></span> | <span class="t">that can handle that sequence. Another alternative is we could have turned the 28 exclamation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5273" target="_blank">01:27:53.660</a></span> | <span class="t">marks into 28 tokens in a row, each one of the single exclamation mark. But now we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5278" target="_blank">01:27:58.980</a></span> | <span class="t">asking our LSTM to hang on to that state for 28 time steps, which is just a lot more work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5285" target="_blank">01:28:05.540</a></span> | <span class="t">for it to do. And it's not going to do as good a job, right? So we want to make things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5288" target="_blank">01:28:08.820</a></span> | <span class="t">easy for our models. That's what pre-processing is all about. So same with all caps, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5294" target="_blank">01:28:14.960</a></span> | <span class="t">If you've got, I am shouting, then it's pretty likely that there's going to be exclamation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5299" target="_blank">01:28:19.980</a></span> | <span class="t">marks after that. There might be swearing after that. Like the fact that there's lots</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5303" target="_blank">01:28:23.820</a></span> | <span class="t">of capitalized words is semantic of itself. So we replace capitalized words with a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5309" target="_blank">01:28:29.980</a></span> | <span class="t">saying this is a capitalized word. And then we replace it with the lowercase word. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5314" target="_blank">01:28:34.300</a></span> | <span class="t">we don't have a separate vocab item for capital am, capital shouting, capital, every damn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5319" target="_blank">01:28:39.140</a></span> | <span class="t">word in the dictionary. Okay. Same thing for mixed case. So I don't know, I haven't come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5330" target="_blank">01:28:50.340</a></span> | <span class="t">across other libraries that do this kind of pre-processing. There's little bits and pieces</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5334" target="_blank">01:28:54.580</a></span> | <span class="t">in various papers, but I think this is a pretty good default set of rules. Notice that these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5340" target="_blank">01:29:00.940</a></span> | <span class="t">rules have to happen after tokenization because they're happening at a word level. So we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5344" target="_blank">01:29:04.980</a></span> | <span class="t">default post rules. And then this one here adds a beginning of stream and an end of stream</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5352" target="_blank">01:29:12.460</a></span> | <span class="t">on either side of a list of tokens. Why do we do that? These tokens turn out to be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5361" target="_blank">01:29:21.340</a></span> | <span class="t">important because when your language model sees like an end of stream character token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5369" target="_blank">01:29:29.340</a></span> | <span class="t">meaning like that's the end of a document, that it knows the next document is something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5374" target="_blank">01:29:34.540</a></span> | <span class="t">new. So it's going to have to learn the kind of reset its state to say like, oh, we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5379" target="_blank">01:29:39.660</a></span> | <span class="t">not talking about the old thing anymore. So we're doing Wikipedia. We were talking about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5383" target="_blank">01:29:43.660</a></span> | <span class="t">Melbourne, Australia. Oh, and now there's a new token. Then we're talking about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5388" target="_blank">01:29:48.420</a></span> | <span class="t">Emmys, right? So when it sees EOS, it has to learn to kind of reset its state somehow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5397" target="_blank">01:29:57.340</a></span> | <span class="t">So you need to make sure that you have the tokens in place to allow your model to know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5402" target="_blank">01:30:02.660</a></span> | <span class="t">that these things are happening. Tokenization is kind of slow because Spacey does it so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5410" target="_blank">01:30:10.180</a></span> | <span class="t">carefully. I thought it couldn't possibly be necessary to do it so carefully because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5414" target="_blank">01:30:14.700</a></span> | <span class="t">it just doesn't seem that important. So last year I tried removing Spacey and replacing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5419" target="_blank">01:30:19.420</a></span> | <span class="t">it with something much simpler. My IMDB accuracy went down a lot. So actually it seems like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5427" target="_blank">01:30:27.020</a></span> | <span class="t">Spacey's sophisticated parser-based tokenization actually does better. So at least we can try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5434" target="_blank">01:30:34.460</a></span> | <span class="t">and make it fast. So Python comes with something called a process pool executor, which runs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5440" target="_blank">01:30:40.060</a></span> | <span class="t">things in parallel. And I wrap it around with this little thing called parallel. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5445" target="_blank">01:30:45.180</a></span> | <span class="t">here's my thing that runs, look, compose, appears everywhere. Compose the pre-rules</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5449" target="_blank">01:30:49.660</a></span> | <span class="t">on every chunk, run the tokenizer, compose the post rules on every dock. That's processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5455" target="_blank">01:30:55.980</a></span> | <span class="t">one chunk. So run them all in parallel for all the chunks. So that's that. So this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5463" target="_blank">01:31:03.020</a></span> | <span class="t">a processor, which we saw last week, and this is a processor which tokenizes. And so we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5471" target="_blank">01:31:11.220</a></span> | <span class="t">can try it out. So we can create one and try, here's a bit of text, and let's try tokenizing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5478" target="_blank">01:31:18.180</a></span> | <span class="t">And so you can see we've got beginning of stream, did int, so int is a token, comma is a token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5487" target="_blank">01:31:27.420</a></span> | <span class="t">xx, match, da1, so that was a capital D, and so forth. All right, so now we need to turn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5493" target="_blank">01:31:33.700</a></span> | <span class="t">those into numbers, not just to have a list of words. We can turn them into numbers by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5499" target="_blank">01:31:39.940</a></span> | <span class="t">numericalizing, which is another processor, which basically when you call it, we find</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5504" target="_blank">01:31:44.820</a></span> | <span class="t">out, do we have a vocab yet? Because numericalizing is just saying, what are all the unique words?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5510" target="_blank">01:31:50.300</a></span> | <span class="t">And the list of unique words is the vocab. So if we don't have a vocab, we'll create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5515" target="_blank">01:31:55.460</a></span> | <span class="t">it, okay? And then after we create it, it's just a case of calling object to int on each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5525" target="_blank">01:32:05.940</a></span> | <span class="t">one. So O to I is just a dictionary, right? Or if deprocessing is just grabbing each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5531" target="_blank">01:32:11.940</a></span> | <span class="t">thing from the vocab. So that's just an array. Okay, so we can tokenize, numericalize, run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5538" target="_blank">01:32:18.580</a></span> | <span class="t">it for two and a half minutes. And so we've got the xobj is the thing which returns the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5547" target="_blank">01:32:27.980</a></span> | <span class="t">object version, so as opposed to the numericalized version, and so we can put it back together</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5552" target="_blank">01:32:32.100</a></span> | <span class="t">and this is what we have after it's been turned into numbers and back again. So since that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5556" target="_blank">01:32:36.460</a></span> | <span class="t">takes a couple of minutes, good idea to dump the labeled list so that we can then load</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5563" target="_blank">01:32:43.180</a></span> | <span class="t">it again later without having to rerun that. All right, this is the bit which a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5570" target="_blank">01:32:50.740</a></span> | <span class="t">people get confused about, which is how do we batch up language model data? So here's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5578" target="_blank">01:32:58.620</a></span> | <span class="t">this bit of text. It's very meta, it's a bit of text which is from this notebook. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5587" target="_blank">01:33:07.700</a></span> | <span class="t">first thing we're going to do is we're going to say, let's create some batch sizes, create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5591" target="_blank">01:33:11.380</a></span> | <span class="t">a small one for showing you what's going on, six. So let's go through and create six batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5596" target="_blank">01:33:16.740</a></span> | <span class="t">which is just all the tokens for each of those six batches. So here's, in this notebook,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5601" target="_blank">01:33:21.540</a></span> | <span class="t">we will go back over the example of is the first element of, so this is the first row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5606" target="_blank">01:33:26.740</a></span> | <span class="t">and then of classifying movie reviews we studied in part one, this is the second. So we just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5611" target="_blank">01:33:31.220</a></span> | <span class="t">put it into six groups, right? And then let's say we have a BPTT of five, so it's kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5619" target="_blank">01:33:39.660</a></span> | <span class="t">like our backprop through time sequence length of five, then we can split these up into groups</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5625" target="_blank">01:33:45.520</a></span> | <span class="t">of five. And so that'll create three of them. In this notebook, we will go back over the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5631" target="_blank">01:33:51.700</a></span> | <span class="t">example of classifying movie reviews we studied in part one. These three things then are three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5638" target="_blank">01:33:58.060</a></span> | <span class="t">mini batches. And this is where people get confused because it's not that each one has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5643" target="_blank">01:34:03.340</a></span> | <span class="t">a different bunch of documents. Each one has the same documents over consecutive time steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5650" target="_blank">01:34:10.880</a></span> | <span class="t">This is really important. Why is it important? Because this row here in the RNN is going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5658" target="_blank">01:34:18.700</a></span> | <span class="t">to be getting some state about this document. So when it goes to the next batch, it needs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5663" target="_blank">01:34:23.580</a></span> | <span class="t">to use that state. And then it goes to the next batch, needs to use that state. So from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5667" target="_blank">01:34:27.700</a></span> | <span class="t">batch to batch, the state that it's building up needs to be consistent. That's why we do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5673" target="_blank">01:34:33.220</a></span> | <span class="t">the batches this way. >> I wanted to ask if you did any other preprocessing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5681" target="_blank">01:34:41.540</a></span> | <span class="t">such as removing stop words, stemming, or limitization? >> Yeah, great question. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5693" target="_blank">01:34:53.580</a></span> | <span class="t">in traditional NLP, those are important things to do. Removing stop words is removing words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5698" target="_blank">01:34:58.380</a></span> | <span class="t">like "ah" and "on." Stemming is like getting rid of the "ing" suffix or stuff like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5707" target="_blank">01:35:07.800</a></span> | <span class="t">It's kind of like universal in traditional NLP. It's an absolutely terrible idea. Never</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5714" target="_blank">01:35:14.060</a></span> | <span class="t">ever do this. Because -- well, the first question is like why would you do it? Why would you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5720" target="_blank">01:35:20.860</a></span> | <span class="t">remove information from your neural net which might be useful? And the fact is it is useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5727" target="_blank">01:35:27.340</a></span> | <span class="t">Like stop words, your use of stop words tells you a lot about what style of language, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5734" target="_blank">01:35:34.140</a></span> | <span class="t">So you'll often have a lot less kind of articles and stuff if you're like really angry and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5738" target="_blank">01:35:38.860</a></span> | <span class="t">speaking really quickly. You know, the tense you're talking about is obviously very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5745" target="_blank">01:35:45.180</a></span> | <span class="t">So stemming gets rid of it. So yeah, all that kind of stuff is in the past. You basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5752" target="_blank">01:35:52.700</a></span> | <span class="t">never want to do it. And in general, preprocessing data for neural nets, leave it as raw as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5760" target="_blank">01:36:00.220</a></span> | <span class="t">can is the kind of rule of thumb. So for a language model, each mini batch is basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5768" target="_blank">01:36:08.780</a></span> | <span class="t">going to look something like this for the independent variable. And then the dependent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5773" target="_blank">01:36:13.220</a></span> | <span class="t">variable will be exactly the same thing but shifted over by one word. So let's create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5778" target="_blank">01:36:18.260</a></span> | <span class="t">that. This thing is called LM freeloader. It would actually be better off being called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5783" target="_blank">01:36:23.540</a></span> | <span class="t">an LM data set. Why don't we do it right now? LM pre freeloader. LM data set. That's really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5799" target="_blank">01:36:39.020</a></span> | <span class="t">what it is. Okay. So an LM data set is a data set for a language model. Remember that a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5808" target="_blank">01:36:48.340</a></span> | <span class="t">data set is defined as something with a length and a get item. So this is a data set which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5815" target="_blank">01:36:55.360</a></span> | <span class="t">you can index into it. And it will grab an independent variable and a dependent variable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5822" target="_blank">01:37:02.020</a></span> | <span class="t">And the independent variable is just the text from wherever you asked for, for BPTT. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5829" target="_blank">01:37:09.780</a></span> | <span class="t">the dependent variable is the same thing offset by one. So you can see it here. We can create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5836" target="_blank">01:37:16.060</a></span> | <span class="t">a data loader using that data set. Remember that's how data loaders work. You pass them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5840" target="_blank">01:37:20.780</a></span> | <span class="t">a data set. And now we have something that we can iterate through, grabbing a mini batch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5845" target="_blank">01:37:25.300</a></span> | <span class="t">at a time. And you can see here X is XXBOS well worth watching. And Y is just well worth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5853" target="_blank">01:37:33.460</a></span> | <span class="t">watching. Okay. And then you can see the second batch, best performance to date. So make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5861" target="_blank">01:37:41.340</a></span> | <span class="t">you print out things that all make sense. So that's stuff that we can all dump into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5867" target="_blank">01:37:47.140</a></span> | <span class="t">function and use it again later and chuck it into a data bunch. So that's all we need for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5875" target="_blank">01:37:55.100</a></span> | <span class="t">a data bunch for language models. We're also going to need a data bunch for classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5881" target="_blank">01:38:01.780</a></span> | <span class="t">And that one's going to be super easy because we already know how to create data bunches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5885" target="_blank">01:38:05.060</a></span> | <span class="t">for classification because we've already done it for lots of image models. And for NLP it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5890" target="_blank">01:38:10.500</a></span> | <span class="t">going to be exactly the same. So we create an item list. We split. We label. That's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5901" target="_blank">01:38:21.740</a></span> | <span class="t">So the stuff we did for image is not different. Only thing we've added is two preprocesses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5910" target="_blank">01:38:30.140</a></span> | <span class="t">>> Question. What are the tradeoffs to consider between batch size and back propagation through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5920" target="_blank">01:38:40.340</a></span> | <span class="t">time? For example, BPTT10 with BS100 versus BPTT100 with BS10. Both would be passing a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5928" target="_blank">01:38:48.380</a></span> | <span class="t">thousand tokens at a time to the model. What should you consider when tuning the ratio?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5934" target="_blank">01:38:54.340</a></span> | <span class="t">>> It's a great question. I don't know the answer. I would love to know. So try it. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5939" target="_blank">01:38:59.900</a></span> | <span class="t">I haven't had time to fiddle with it. I haven't seen anybody else experiment with it. So that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5943" target="_blank">01:39:03.780</a></span> | <span class="t">would make a super great experiment. I think the batch size is the thing that lets it parallelize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5955" target="_blank">01:39:15.740</a></span> | <span class="t">So if you don't have a large enough batch size it's just going to be really slow. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5960" target="_blank">01:39:20.740</a></span> | <span class="t">on the other hand, the large batch size with a short BPTT, depending on how you use it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5966" target="_blank">01:39:26.060</a></span> | <span class="t">you may end up kind of ending up with less state that's being back propagated. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5970" target="_blank">01:39:30.120</a></span> | <span class="t">question of how much that matters, I'm not sure. And when we get to our ULM classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5975" target="_blank">01:39:35.500</a></span> | <span class="t">model I'll actually show you this, kind of where this comes in. Okay. So here's a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5983" target="_blank">01:39:43.100</a></span> | <span class="t">of examples of a document and a dependent variable. And what we're going to be doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=5992" target="_blank">01:39:52.340</a></span> | <span class="t">is we're going to be creating data loaders for them. But we do have one trick here. Which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6000" target="_blank">01:40:00.300</a></span> | <span class="t">is that with images, our images were always, by the time we got to modeling they were all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6007" target="_blank">01:40:07.020</a></span> | <span class="t">the same size. Now this is probably not how things should be. And we have started doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6013" target="_blank">01:40:13.060</a></span> | <span class="t">some experiments with training with rectangular images of different sizes. But we're not quite</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6016" target="_blank">01:40:16.700</a></span> | <span class="t">ready to show you that work because it's still a little bit fiddly. But for text we can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6024" target="_blank">01:40:24.620</a></span> | <span class="t">avoid it. You know, we've got different sized texts coming in. So we have to deal with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6029" target="_blank">01:40:29.900</a></span> | <span class="t">And the way we deal with it is almost identical to how actually we're going to end up dealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6034" target="_blank">01:40:34.700</a></span> | <span class="t">with when we do do rectangular images. So if you are interested in rectangular images,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6038" target="_blank">01:40:38.700</a></span> | <span class="t">try and basically copy this approach. Here's the approach. We are going to pad each document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6049" target="_blank">01:40:49.100</a></span> | <span class="t">by adding a bunch of padding tokens. So we just pick some arbitrary token which we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6053" target="_blank">01:40:53.420</a></span> | <span class="t">going to tell PyTorch this token isn't text. It's just thrown in there because we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6059" target="_blank">01:40:59.460</a></span> | <span class="t">to put in something to make a rectangular tensor. If we have a mini batch with a 1,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6067" target="_blank">01:41:07.100</a></span> | <span class="t">word document and then a 2,000 word document and then a 20 word document, the 20 word document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6072" target="_blank">01:41:12.380</a></span> | <span class="t">is going to end up with 1,980 padding tokens on the end. And as we go through the RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6078" target="_blank">01:41:18.020</a></span> | <span class="t">we're going to be totally pointlessly calculating on all these padding tokens. We don't want</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6084" target="_blank">01:41:24.820</a></span> | <span class="t">to do that. So the trick is to sort the data first by length. So that way your first mini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6094" target="_blank">01:41:34.260</a></span> | <span class="t">batch will contain your really long documents and your last mini batch will create your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6099" target="_blank">01:41:39.300</a></span> | <span class="t">really short documents and each mini batch will not contain a very wide variety of lengths</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6103" target="_blank">01:41:43.580</a></span> | <span class="t">of documents. So there won't be much padding and so there won't be much wasted computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6110" target="_blank">01:41:50.020</a></span> | <span class="t">So we've already looked at samplers. If you've forgotten, go back to when we created our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6115" target="_blank">01:41:55.100</a></span> | <span class="t">data loader from scratch and we actually created a sampler. And so here we're going to create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6121" target="_blank">01:42:01.420</a></span> | <span class="t">a different type of sampler and it is simply one that goes through our data, looks at how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6129" target="_blank">01:42:09.100</a></span> | <span class="t">many documents is in it, creates the range from zero to the number of documents, sorts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6135" target="_blank">01:42:15.900</a></span> | <span class="t">them by some key and returns that iterator, sorts them in reverse order. So we're going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6142" target="_blank">01:42:22.340</a></span> | <span class="t">to use sort sampler passing in the key, which is a lambda function that grabs the length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6149" target="_blank">01:42:29.300</a></span> | <span class="t">of the document. So that way our sampler is going to cause each mini batch to be documents</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6162" target="_blank">01:42:42.900</a></span> | <span class="t">of similar lengths. The problem is we can only do this for validation, not for training because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6168" target="_blank">01:42:48.700</a></span> | <span class="t">for training we want to shuffle and sorting would undo any shuffling because sorting is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6175" target="_blank">01:42:55.180</a></span> | <span class="t">deterministic. So that's why we create something called sort ish sampler. And the sort ish</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6181" target="_blank">01:43:01.860</a></span> | <span class="t">sampler approximately orders things by length. So every mini batch has things of similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6190" target="_blank">01:43:10.340</a></span> | <span class="t">lengths but with some randomness. And the way we do this, the details don't particularly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6195" target="_blank">01:43:15.580</a></span> | <span class="t">matter but basically I've created this idea of a mega batch, which is something that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6200" target="_blank">01:43:20.300</a></span> | <span class="t">50 times bigger than a batch and basically I sort those, okay? And so you end up with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6205" target="_blank">01:43:25.860</a></span> | <span class="t">these kind of like sorted mega batches and then I have random permutations within that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6212" target="_blank">01:43:32.500</a></span> | <span class="t">So you can see random permutations there and there. So you can look at the code if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6216" target="_blank">01:43:36.020</a></span> | <span class="t">care, the details don't matter. In the end, it's a random sort in which things of similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6223" target="_blank">01:43:43.020</a></span> | <span class="t">lengths tend to be next to each other and the biggest ones tend to be at the start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6227" target="_blank">01:43:47.960</a></span> | <span class="t">So now we've got a mini batch of numericalized, tokenized documents of similar lengths but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6234" target="_blank">01:43:54.060</a></span> | <span class="t">they're not identical lengths, right? And so you might remember the other thing when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6239" target="_blank">01:43:59.340</a></span> | <span class="t">we first created a data loader, we gave it two things, a sampler and a collate function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6245" target="_blank">01:44:05.860</a></span> | <span class="t">And the collate function that we wrote simply said torch.stack because all our images were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6251" target="_blank">01:44:11.180</a></span> | <span class="t">the same size because all our images were the same size so we could just literally just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6257" target="_blank">01:44:17.340</a></span> | <span class="t">stick them together. We can't do that for documents because they're different sizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6262" target="_blank">01:44:22.020</a></span> | <span class="t">So we've written something called pad collate. And what Sylvan did here was he basically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6268" target="_blank">01:44:28.100</a></span> | <span class="t">said let's create something that's big enough to handle the longest document in the mini</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6271" target="_blank">01:44:31.980</a></span> | <span class="t">batch and then go through every document and dump it into that big tensor either at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6279" target="_blank">01:44:39.180</a></span> | <span class="t">start or at the end depending on whether you said pad first. So now we can pass the sampler</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6288" target="_blank">01:44:48.660</a></span> | <span class="t">and the collate function to our data loader and that allows us to grab some mini batches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6295" target="_blank">01:44:55.940</a></span> | <span class="t">which as you can see contain padding at the end. And so here's our normal convenience</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6302" target="_blank">01:45:02.420</a></span> | <span class="t">functions that do all those things for us and that's that, okay. So that's quite a bit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6310" target="_blank">01:45:10.020</a></span> | <span class="t">of preprocessing and I guess the main tricky bit is this dealing with different lengths.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6317" target="_blank">01:45:17.100</a></span> | <span class="t">And at that point we can create our AWD LSTM. So these are just the steps we just did to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6327" target="_blank">01:45:27.300</a></span> | <span class="t">create our data loader. And now we're going to create an RNN. So an RNN remember is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6337" target="_blank">01:45:37.620</a></span> | <span class="t">a multi-layer network. But it's a multi-layer network that could be very, very, very many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6343" target="_blank">01:45:43.740</a></span> | <span class="t">layers. There could be like if it's a 2000 word document this is going to be 2000 layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6349" target="_blank">01:45:49.140</a></span> | <span class="t">So to avoid us having to write 2000 layers we used a for loop. And between every pair</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6356" target="_blank">01:45:56.500</a></span> | <span class="t">of hidden layers we use the same weight matrix. That's why they're the same color. And that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6361" target="_blank">01:46:01.060</a></span> | <span class="t">why we can use a for loop. Problem is as we've seen trying to handle 2000 layers of neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6368" target="_blank">01:46:08.860</a></span> | <span class="t">net we get vanishing gradients or exploding gradients it's really, really difficult to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6373" target="_blank">01:46:13.180</a></span> | <span class="t">get it to work. So what are we going to do? Because it's even worse than that because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6377" target="_blank">01:46:17.460</a></span> | <span class="t">often we have layers going into, RNNs going into other RNNs so we actually have stacked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6382" target="_blank">01:46:22.580</a></span> | <span class="t">RNNs which when we unstack them it's going to be even more thousands of layers effectively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6389" target="_blank">01:46:29.820</a></span> | <span class="t">So the trick is we create something called an LSTM cell. Rather than just doing a matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6394" target="_blank">01:46:34.980</a></span> | <span class="t">multiply as our layer we instead do this thing called an LSTM cell as our layer. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6402" target="_blank">01:46:42.620</a></span> | <span class="t">it here. So this is a sigmoid function and this is a tanh function. So the sigmoid function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6409" target="_blank">01:46:49.700</a></span> | <span class="t">remember goes from 0 to 1 and kind of nice and smooth between the two. And the tanh function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6415" target="_blank">01:46:55.120</a></span> | <span class="t">is identical to a sigmoid except it goes from minus 1 to 1 rather than 0 to 1. So sigmoid</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6420" target="_blank">01:47:00.660</a></span> | <span class="t">is 0 to 1, tanh is minus 1 to 1. So here's what we're going to do. We're going to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6425" target="_blank">01:47:05.580</a></span> | <span class="t">our input and we're going to have some hidden state as we've already always had in our RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6431" target="_blank">01:47:11.580</a></span> | <span class="t">This is just our usual hidden state. And we're going to multiply our input by some weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6438" target="_blank">01:47:18.300</a></span> | <span class="t">matrix in the usual way. Then we're going to multiply our hidden state by some weight</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6441" target="_blank">01:47:21.820</a></span> | <span class="t">matrix in the usual way and then we add the two together in the way we've done before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6445" target="_blank">01:47:25.660</a></span> | <span class="t">for RNNs. And then we're going to do something interesting. We're going to split the result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6451" target="_blank">01:47:31.500</a></span> | <span class="t">into four equal sized tensors. So the first one quarter of the activations will go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6458" target="_blank">01:47:38.060</a></span> | <span class="t">this path, the next will go through this path, the next will go through this path, the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6461" target="_blank">01:47:41.780</a></span> | <span class="t">will go through this path. So what this means is we kind of have like four little neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6468" target="_blank">01:47:48.460</a></span> | <span class="t">nets effectively, right? And so this path goes through a sigmoid and it hits this thing called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6477" target="_blank">01:47:57.200</a></span> | <span class="t">the cell. Now this is the new thing. So the cell, just like hidden state, is just a rank</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6482" target="_blank">01:48:02.900</a></span> | <span class="t">one tensor or for a mini batch, a rank two tensor. It's just some activations. And what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6489" target="_blank">01:48:09.660</a></span> | <span class="t">happens is we multiply it by the output of this sigmoid. So the sigmoid can go between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6495" target="_blank">01:48:15.760</a></span> | <span class="t">zero and one. So this, this gate has the ability to basically zero out bits of the cell state.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6505" target="_blank">01:48:25.100</a></span> | <span class="t">So we have the ability to basically take this state and say like delete some of it. So we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6508" target="_blank">01:48:28.900</a></span> | <span class="t">could look at some of these words or whatever in this LSTM and say based on looking at that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6513" target="_blank">01:48:33.660</a></span> | <span class="t">we think we should zero out some of our cell state. And so now the cell state has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6519" target="_blank">01:48:39.420</a></span> | <span class="t">selectively forgotten. So that's the forget gate. We then add it to the second chunk,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6526" target="_blank">01:48:46.640</a></span> | <span class="t">the second little mini neural net, which goes through sigmoid. So this is just our input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6533" target="_blank">01:48:53.660</a></span> | <span class="t">and we multiply it by the third one, which goes through a tench. So this basically allows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6541" target="_blank">01:49:01.100</a></span> | <span class="t">us to say, which bits of input do we care about? And then this gives us the numbers from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6546" target="_blank">01:49:06.740</a></span> | <span class="t">minus one to one, multiply them together. And this adds, so this is, how do we update our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6551" target="_blank">01:49:11.380</a></span> | <span class="t">cell state? So we add on some new state. And so now we take that cell state and we put</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6558" target="_blank">01:49:18.340</a></span> | <span class="t">it through another, well, one thing that happens is it goes through to the next time step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6563" target="_blank">01:49:23.960</a></span> | <span class="t">And the other thing that happens is it goes through one more tench to get multiplied by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6568" target="_blank">01:49:28.800</a></span> | <span class="t">the fourth little mini neural net, which is the output. So this is the actual, this actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6574" target="_blank">01:49:34.580</a></span> | <span class="t">creates the output hidden state. So it looks like there's a lot going on, but actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6583" target="_blank">01:49:43.100</a></span> | <span class="t">it's just this, right? So you've got one neural net that goes from input to hidden. It's a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6589" target="_blank">01:49:49.660</a></span> | <span class="t">linear layer. One that goes from hidden to hidden. Each one is going to be four times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6593" target="_blank">01:49:53.960</a></span> | <span class="t">the number of hidden because after we compute it and add them together, chunk splits it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6599" target="_blank">01:49:59.580</a></span> | <span class="t">up into four equal sized groups. Three of them go through a sigmoid. One of them goes through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6606" target="_blank">01:50:06.780</a></span> | <span class="t">a tench and then this is just the multiply and add that you saw. So there's kind of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6612" target="_blank">01:50:12.940</a></span> | <span class="t">conceptually a lot going on in LSTM and it's certainly worth doing some more reading about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6618" target="_blank">01:50:18.220</a></span> | <span class="t">why this particular architecture. But one thing I will say is there's lots of other ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6623" target="_blank">01:50:23.500</a></span> | <span class="t">you can set up a layer which has the ability to selectively update and selectively forget</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6629" target="_blank">01:50:29.620</a></span> | <span class="t">things. For example, there's something called a GIU, which has one less gate. The key thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6635" target="_blank">01:50:35.060</a></span> | <span class="t">seems to be giving it some way to make a decision to forget things. Cuz if you do that, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6642" target="_blank">01:50:42.620</a></span> | <span class="t">it has the ability to not push state through all thousand time steps or whatever. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6648" target="_blank">01:50:48.820</a></span> | <span class="t">our LSTM cell and so an LSTM layer, assuming we only have one layer, is just that for loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6657" target="_blank">01:50:57.900</a></span> | <span class="t">that we've seen before and we're just gonna call whatever cell we asked for. So we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6662" target="_blank">01:51:02.300</a></span> | <span class="t">gonna ask for an LSTM cell and it just loops through and see how the state, we can take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6670" target="_blank">01:51:10.140</a></span> | <span class="t">the state and we update the state. So you can see this is the classic deep learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6675" target="_blank">01:51:15.300</a></span> | <span class="t">it's like an NN.sequential, right? It's looping through a bunch of functions that are updating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6679" target="_blank">01:51:19.700</a></span> | <span class="t">itself. That's what makes it a deep learning network. So that's an LSTM. So that takes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6690" target="_blank">01:51:30.780</a></span> | <span class="t">105 milliseconds for a small net on the CPU. We could pop it onto CUDA, then it's 24 milliseconds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6704" target="_blank">01:51:44.060</a></span> | <span class="t">on GPU. It's not that much faster, because this loop, every time step, it's having to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6711" target="_blank">01:51:51.700</a></span> | <span class="t">push off another kernel launch off to the GPU and that's just slow, right? So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6720" target="_blank">01:52:00.380</a></span> | <span class="t">why we use the built in version. And the built in version behind the scenes calls a library</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6725" target="_blank">01:52:05.220</a></span> | <span class="t">from Nvidia called cuDNN, which has created a C++ version of this. It's about the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6732" target="_blank">01:52:12.300</a></span> | <span class="t">on the CPU, right? Not surprisingly, it's really not doing anything different, but on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6735" target="_blank">01:52:15.780</a></span> | <span class="t">the GPU goes from 24 milliseconds to 8 milliseconds. So it's dramatically faster. The good news</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6744" target="_blank">01:52:24.920</a></span> | <span class="t">is we can create a faster version by taking advantage of something in PyTorch called JIT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6751" target="_blank">01:52:31.380</a></span> | <span class="t">And what JIT does is it reads our Python and it converts it into C++ that does the same</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6758" target="_blank">01:52:38.140</a></span> | <span class="t">thing. It compiles it the first time you use it and then it uses that compiled code. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6765" target="_blank">01:52:45.860</a></span> | <span class="t">so that way it can create an on GPU loop. And so the result of that is, again, pretty similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6773" target="_blank">01:52:53.880</a></span> | <span class="t">on the CPU, but on the GPU, 12 milliseconds. So, you know, not as fast as the cuDNN version,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6783" target="_blank">01:53:03.300</a></span> | <span class="t">but certainly a lot better than our non-JIT version. So this seems like some kind of magic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6789" target="_blank">01:53:09.620</a></span> | <span class="t">thing that's going to save our lives and not require us to have to come to the Swift for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6793" target="_blank">01:53:13.860</a></span> | <span class="t">TensorFlow lectures. But I've got bad news for you. Trying to get JIT working has been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6807" target="_blank">01:53:27.460</a></span> | <span class="t">honestly a bit of a nightmare. This is the third time we've tried to introduce it in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6813" target="_blank">01:53:33.100</a></span> | <span class="t">this course. And the other two times we've just not gotten it working or we've gotten</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6823" target="_blank">01:53:43.580</a></span> | <span class="t">worse results. It doesn't work very well that often. And it's got a lot of weird things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6830" target="_blank">01:53:50.160</a></span> | <span class="t">going on. Like, for example, if you decide to comment out a line, right, and then run</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6837" target="_blank">01:53:57.940</a></span> | <span class="t">it and then run it, you'll get this error saying unexpected indent. Like, literally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6847" target="_blank">01:54:07.180</a></span> | <span class="t">it's not Python, right? So it doesn't even know how to comment out lines. It's this kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6853" target="_blank">01:54:13.480</a></span> | <span class="t">of weird thing where they try to -- it's heroic. It's amazing that it works at all. But the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6858" target="_blank">01:54:18.440</a></span> | <span class="t">idea that you could try and turn Python, which is so not C++, into C++ is really pushing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6864" target="_blank">01:54:24.580</a></span> | <span class="t">at what's possible. So it's astonishing this works at all. And occasionally it might be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6870" target="_blank">01:54:30.220</a></span> | <span class="t">useful, but it's very, very hard to use. And when something isn't as fast as you want,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6876" target="_blank">01:54:36.500</a></span> | <span class="t">it's very, very hard to -- you can't profile it, you can't debug it, not in the normal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6880" target="_blank">01:54:40.060</a></span> | <span class="t">ways. But, you know, obviously, it will improve. It's pretty early days. It will improve. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6889" target="_blank">01:54:49.540</a></span> | <span class="t">the idea of trying to parse Python and turn it into C++, literally, they're doing like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6894" target="_blank">01:54:54.020</a></span> | <span class="t">string interpolation behind the scenes, is kind of trying to reinvent all the stuff that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6900" target="_blank">01:55:00.420</a></span> | <span class="t">compilers already do, converting a language that was very explicitly not designed to do</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6906" target="_blank">01:55:06.540</a></span> | <span class="t">this kind of thing into one that does. And I just -- I don't think this is the future.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6912" target="_blank">01:55:12.540</a></span> | <span class="t">So I say for now, be aware that JIT exists. Be very careful in the short term. I found</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6918" target="_blank">01:55:18.820</a></span> | <span class="t">places where it literally gives the wrong gradients. So it goes down a totally different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6922" target="_blank">01:55:22.900</a></span> | <span class="t">auto grad path. And I've had models that trained incorrectly without any warnings. Because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6929" target="_blank">01:55:29.380</a></span> | <span class="t">was just wrong. So be very careful. But sometimes, like, for a researcher, if you want to play</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6935" target="_blank">01:55:35.860</a></span> | <span class="t">with different types of RNNs, this is your only option. Unless you, you know, write your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6943" target="_blank">01:55:43.820</a></span> | <span class="t">own C++. Or unless you try out Julia or Swift, I guess. Is there a question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6953" target="_blank">01:55:53.780</a></span> | <span class="t">>> Yeah. Why do we need -- why do we need torch.cuda.synchronize? Is it kind of a lock to synchronize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6962" target="_blank">01:56:02.100</a></span> | <span class="t">CUDA threads or something?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6964" target="_blank">01:56:04.220</a></span> | <span class="t">>> Yeah, this is something that, thanks to Tom on the forum for pointing this out, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6968" target="_blank">01:56:08.660</a></span> | <span class="t">just when we're timing, without the synchronize, it's -- let's find it. So I just created a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6980" target="_blank">01:56:20.900</a></span> | <span class="t">little timing function here. Without the synchronize, the CUDA thing will just keep on running things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6986" target="_blank">01:56:26.700</a></span> | <span class="t">in the background, but will return -- it will let your CPU thread keep going. So it could</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6993" target="_blank">01:56:33.540</a></span> | <span class="t">end up looking much faster than it actually is. So synchronize says, don't keep going in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=6999" target="_blank">01:56:39.140</a></span> | <span class="t">my Python world until my CUDA world is finished. Okay. So now we need dropout. And this is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7007" target="_blank">01:56:47.380</a></span> | <span class="t">bit that really is fantastic about AWDLSTM, is that Stephen Meredy thought about all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7014" target="_blank">01:56:54.900</a></span> | <span class="t">ways in which we can regularize a model. So basically, dropout is just Bernoulli random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7021" target="_blank">01:57:01.420</a></span> | <span class="t">noise. So Bernoulli random noise simply means create 1s and 0s, and it's 1 with this probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7028" target="_blank">01:57:08.340</a></span> | <span class="t">Right? So create a bunch of random 1s and 0s. And then divide by 1 minus P. So that makes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7034" target="_blank">01:57:14.620</a></span> | <span class="t">them, in this case, to 0.5, it's randomly 0s and 2s. And the reason they're 0s and 2s is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7040" target="_blank">01:57:20.180</a></span> | <span class="t">because that way the standard deviation doesn't change. So we can remove dropout for inference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7044" target="_blank">01:57:24.540</a></span> | <span class="t">time and the activations will be still scaled correctly. And we talked about that a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7050" target="_blank">01:57:30.020</a></span> | <span class="t">bit in part 1. And so now we can create our RNN dropout. And one of the nifty things here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7058" target="_blank">01:57:38.220</a></span> | <span class="t">is the way that Sylvain wrote this is you don't just pass in the thing to dropout, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7064" target="_blank">01:57:44.140</a></span> | <span class="t">you also pass in a size. Now, normally, you would just pass in the size of the thing to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7069" target="_blank">01:57:49.060</a></span> | <span class="t">dropout like this. But what he did here was he passed in, for the size, size 0, 1, size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7079" target="_blank">01:57:59.500</a></span> | <span class="t">2. And so if you remember back to broadcasting, this means that this is going to create something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7084" target="_blank">01:58:04.980</a></span> | <span class="t">with a unit axis in the middle. And so when we multiply that, so here's our matrix, when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7090" target="_blank">01:58:10.500</a></span> | <span class="t">we multiply the dropout by that, our 0s get broadcast. This is really important, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7098" target="_blank">01:58:18.340</a></span> | <span class="t">Because this is the sequence dimension. So every time step, if you drop out time step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7103" target="_blank">01:58:23.060</a></span> | <span class="t">number 3, but not time step 2 or 4, you've basically broken that whole sequence's ability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7110" target="_blank">01:58:30.500</a></span> | <span class="t">to calculate anything because you just killed it, right? So this is called RNN dropout or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7116" target="_blank">01:58:36.060</a></span> | <span class="t">also called variational dropout. There's a couple of different papers that introduce</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7119" target="_blank">01:58:39.380</a></span> | <span class="t">the same idea. And it's simply this that you do dropout on the entire sequence at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7126" target="_blank">01:58:46.460</a></span> | <span class="t">So there's RNN dropout. The second one that Stephen Meridy showed was something he called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7134" target="_blank">01:58:54.780</a></span> | <span class="t">weightdrop. It actually turns out that this already existed in the computer vision world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7140" target="_blank">01:59:00.380</a></span> | <span class="t">where it was called dropconnect. So there's now two things with different names but are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7146" target="_blank">01:59:06.780</a></span> | <span class="t">the same, weightdrop and dropconnect. And this is dropout not on the activations but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7153" target="_blank">01:59:13.580</a></span> | <span class="t">out on the weights themselves. So you can see here when we do the forward pass, we go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7157" target="_blank">01:59:17.900</a></span> | <span class="t">set weights that applies dropout to the actual weights. So that's our second type of dropout.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7168" target="_blank">01:59:28.380</a></span> | <span class="t">The next one is embedding dropout. And this one, as you can see, it drops out an entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7177" target="_blank">01:59:37.660</a></span> | <span class="t">row. This is actually a coincidence that all these rows are in order, but it drops out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7181" target="_blank">01:59:41.540</a></span> | <span class="t">an entire row. So by dropping it-- so what it does is it says, OK, you've got an embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7187" target="_blank">01:59:47.340</a></span> | <span class="t">And what I'm going to do is I'm going to drop out all of the embedding-- the entire embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7192" target="_blank">01:59:52.900</a></span> | <span class="t">vector for whatever word this is. So it's dropping out entire words at a time. So that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7202" target="_blank">02:00:02.060</a></span> | <span class="t">embedding dropout. So with all that in place, we can create an LSTM model. It can be a number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7210" target="_blank">02:00:10.520</a></span> | <span class="t">of layers. So we can create lots of LSTMs for however many layers you want. And we can loop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7215" target="_blank">02:00:15.340</a></span> | <span class="t">through them. And we can basically call each layer. And we've got all our different dropouts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7221" target="_blank">02:00:21.020</a></span> | <span class="t">And so basically this code is just calling all the different dropouts. So that is an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7227" target="_blank">02:00:27.700</a></span> | <span class="t">AWDLSTM. So then we can put on top of that a simple linear model with dropout. And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7241" target="_blank">02:00:41.660</a></span> | <span class="t">this simple linear model-- so it's literally just a linear model where we go dropout and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7247" target="_blank">02:00:47.820</a></span> | <span class="t">then call our linear model is-- we're going to create a sequential model which takes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7253" target="_blank">02:00:53.900</a></span> | <span class="t">RNN, so the AWDLSTM, and passes the result to a single linear layer with dropout. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7262" target="_blank">02:01:02.340</a></span> | <span class="t">that is our language model. Because that final linear layer is a thing which will figure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7267" target="_blank">02:01:07.060</a></span> | <span class="t">out what is the next word. So the size of that is the size of the vocab. It's good to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7275" target="_blank">02:01:15.860</a></span> | <span class="t">look at these little tests that we do along the way. These are the things we use to help</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7279" target="_blank">02:01:19.260</a></span> | <span class="t">pass check that everything looks sensible. And we found, yep, everything does look sensible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7286" target="_blank">02:01:26.020</a></span> | <span class="t">And then we added something that AWDLSTM did which is called gradient clipping, which is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7290" target="_blank">02:01:30.620</a></span> | <span class="t">a callback that just checks after the backward pass what are the gradients. And if the total</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7297" target="_blank">02:01:37.540</a></span> | <span class="t">norm of the gradients-- so the root sum of squares of gradients is bigger than some number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7303" target="_blank">02:01:43.380</a></span> | <span class="t">then we'll divide them all so that they're not bigger than that number anymore. So it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7307" target="_blank">02:01:47.060</a></span> | <span class="t">just clipping those gradients. So that's how easy it is to add gradient clipping. This is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7312" target="_blank">02:01:52.300</a></span> | <span class="t">a super good idea, not as used as much as it should be because it really lets you train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7317" target="_blank">02:01:57.220</a></span> | <span class="t">things at higher learning rates and avoid kind of gradients blowing out. Then there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7326" target="_blank">02:02:06.660</a></span> | <span class="t">two other kinds of regularization. This one here is called activation regularization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7334" target="_blank">02:02:14.740</a></span> | <span class="t">And it's actually just an L2 loss, an L2 penalty, just like weight decay. Except the L2 penalty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7344" target="_blank">02:02:24.540</a></span> | <span class="t">is not on the weights, it's on the activations. So this is going to make sure that our activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7351" target="_blank">02:02:31.820</a></span> | <span class="t">are never too high. And then this one's really interesting. This is called temporal activation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7356" target="_blank">02:02:36.540</a></span> | <span class="t">regularization. This checks how much does each activation change by from sequence step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7366" target="_blank">02:02:46.020</a></span> | <span class="t">to sequence step, and then take the square of that. So this is regularizing the RNN to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7376" target="_blank">02:02:56.300</a></span> | <span class="t">say try not to have things that massively change from time step to time step. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7382" target="_blank">02:03:02.900</a></span> | <span class="t">if it's doing that, that's probably not a good sign. So that's our RNN trainer callback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7390" target="_blank">02:03:10.740</a></span> | <span class="t">We set up our loss functions, which are just normal cross-entropy loss, and also a metric</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7394" target="_blank">02:03:14.340</a></span> | <span class="t">which is normal accuracy. But we just make sure that our batch and sequence length is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7399" target="_blank">02:03:19.140</a></span> | <span class="t">all flattened. So we can create our language model, add our callbacks, and fit. So once</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7407" target="_blank">02:03:27.620</a></span> | <span class="t">we've got all that, we can use it to train that language model on Wikitex 103. So I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7413" target="_blank">02:03:33.260</a></span> | <span class="t">not going to go through this, because it literally just uses what's in the previous notebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7417" target="_blank">02:03:37.260</a></span> | <span class="t">But this shows you here's how you can download Wikitex 103, split it into articles, create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7423" target="_blank">02:03:43.900</a></span> | <span class="t">the text lists, split into train and valid, tokenize, numericalize, data bunchify, create</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7432" target="_blank">02:03:52.980</a></span> | <span class="t">the model that we just saw and train it for, in this case, about five hours. Because it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7442" target="_blank">02:04:02.900</a></span> | <span class="t">quite a big model. So because we don't want you to have to train for five hours this RNN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7452" target="_blank">02:04:12.260</a></span> | <span class="t">you will find that you can download that small pre-trained model from this link. So you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7460" target="_blank">02:04:20.180</a></span> | <span class="t">now use that on IMDB. So you can, again, grab your IMDB data set, download that pre-trained</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7467" target="_blank">02:04:27.300</a></span> | <span class="t">model, load it in. And then we need to do one more step, which is that the embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7477" target="_blank">02:04:37.940</a></span> | <span class="t">matrix for the pre-trained Wikitex 103 model is for a different bunch of words to the IMDB</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7486" target="_blank">02:04:46.460</a></span> | <span class="t">version. So they've got different vocabs with some overlap. So I won't go through the code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7492" target="_blank">02:04:52.180</a></span> | <span class="t">but what we just do is we just go through each vocab item in the IMDB vocab, and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7500" target="_blank">02:05:00.180</a></span> | <span class="t">find out if it's in the Wikitex 103 vocab, and if it is, we copy Wikitex 103's vocab</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7506" target="_blank">02:05:06.860</a></span> | <span class="t">over. It's embedding over. So that way we'll end up with an embedding matrix for IMDB that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7514" target="_blank">02:05:14.220</a></span> | <span class="t">is the same as the Wikitex 103 embedding matrix. Any time there's a word that's the same, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7521" target="_blank">02:05:21.260</a></span> | <span class="t">any time there's a word that's missing, we're just going to use the mean bias and the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7526" target="_blank">02:05:26.140</a></span> | <span class="t">weights. So that's all that is. Okay, so once we've done that, we can then define a splitter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7535" target="_blank">02:05:35.780</a></span> | <span class="t">just like before to create our layer groups. We can set up our callbacks, our learner,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7541" target="_blank">02:05:41.500</a></span> | <span class="t">we can fit, and so then we'll train that for an hour or so, and at the end of that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7549" target="_blank">02:05:49.780</a></span> | <span class="t">a fine-tuned IMDB language model. So now we can load up our classifier data bunch, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7561" target="_blank">02:06:01.300</a></span> | <span class="t">we created earlier. That's exactly the same lines of code we had before. I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7568" target="_blank">02:06:08.420</a></span> | <span class="t">ignore this pack-padded-sequence stuff, but basically there's a neat little trick in PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7574" target="_blank">02:06:14.780</a></span> | <span class="t">where you can take data that's of different lengths and call pack-padded-sequence, pass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7581" target="_blank">02:06:21.420</a></span> | <span class="t">that to an RNN, and then call pad-sequence, and it basically takes things of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7588" target="_blank">02:06:28.700</a></span> | <span class="t">lengths and kind of optimally handles them in an RNN.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7595" target="_blank">02:06:35.460</a></span> | <span class="t">So we basically update our AWD LSTM to use that. You might remember that for ULM fit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7612" target="_blank">02:06:52.460</a></span> | <span class="t">we kind of create our hidden state in the LSTM for lots of time steps, and we want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7617" target="_blank">02:06:57.420</a></span> | <span class="t">say, "Oh, which bit of state do we actually want to use for classification?" People used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7622" target="_blank">02:07:02.780</a></span> | <span class="t">to basically use the final state. Something that I tried, and it turned out to work really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7627" target="_blank">02:07:07.900</a></span> | <span class="t">well, so it ended up in the paper, was that we actually do an average pool and a max pool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7636" target="_blank">02:07:16.380</a></span> | <span class="t">and use the final state, and we concatenate them all together. So this is like the concat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7642" target="_blank">02:07:22.940</a></span> | <span class="t">pooling we do for images. We do the same kind of thing for text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7650" target="_blank">02:07:30.140</a></span> | <span class="t">So we put all that together. This is just checking that everything looks sensible, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7656" target="_blank">02:07:36.540</a></span> | <span class="t">that gives us something that we call the pooling linear classifier, which is just a list of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7663" target="_blank">02:07:43.100</a></span> | <span class="t">batch norm dropout linear layers and our concat pooling, and that's about it. So we just go</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7673" target="_blank">02:07:53.660</a></span> | <span class="t">through our sentence, one BPTT at a time, and keep calling that thing and keep appending</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7681" target="_blank">02:08:01.020</a></span> | <span class="t">the results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7682" target="_blank">02:08:02.020</a></span> | <span class="t">So once we've done all that, we can train it. So here's our normal set of callbacks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7687" target="_blank">02:08:07.300</a></span> | <span class="t">We can load our fine-tuned encoder, and we can train, and 92% accuracy, which is pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7699" target="_blank">02:08:19.660</a></span> | <span class="t">close to where the state-of-the-art was a very small number of years ago, and this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7705" target="_blank">02:08:25.100</a></span> | <span class="t">not the same as we got about 94.5% or something like that, or 95% for the paper, because that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7710" target="_blank">02:08:30.460</a></span> | <span class="t">used a bigger model that we trained for longer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7716" target="_blank">02:08:36.740</a></span> | <span class="t">So that was a super-fast zip-through ULM fit, and plenty of stuff which is probably worth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7729" target="_blank">02:08:49.700</a></span> | <span class="t">reading in more detail, and we can answer questions on the forum as well. So let's spend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7734" target="_blank">02:08:54.380</a></span> | <span class="t">the last 10 minutes talking about Swift, because the next two classes are going to be about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7737" target="_blank">02:08:57.860</a></span> | <span class="t">Swift. So I think anybody who's got to lesson 12 in this course should be learning Swift</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7747" target="_blank">02:09:07.660</a></span> | <span class="t">for TensorFlow. The reason why is I think basically that Python stays a-numbered. That stuff I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7753" target="_blank">02:09:13.500</a></span> | <span class="t">showed you about JIT. The more I use JIT, the more I think about it, the more it looks like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7758" target="_blank">02:09:18.580</a></span> | <span class="t">failed examples of software development processes I've seen in the last 25 years. Whenever people</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7763" target="_blank">02:09:23.220</a></span> | <span class="t">try to convert one language into a different language, and then you're kind of using the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7766" target="_blank">02:09:26.740</a></span> | <span class="t">language that you're not really using, it requires brilliant, brilliant people like the PyTorch</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7773" target="_blank">02:09:33.420</a></span> | <span class="t">team years to make it almost kind of work. So I think Julia or Swift will eventually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7783" target="_blank">02:09:43.300</a></span> | <span class="t">in the coming years take over. I just don't think Python can survive, because we can't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7788" target="_blank">02:09:48.420</a></span> | <span class="t">write CUDA kernels in Python. We can't write RNN cells in Python and have them work reliably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7794" target="_blank">02:09:54.420</a></span> | <span class="t">and fast. DL libraries change all the time anyway, so if you're spending all your time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7799" target="_blank">02:09:59.220</a></span> | <span class="t">just studying one library and one language, then you're not going to be ready for that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7803" target="_blank">02:10:03.060</a></span> | <span class="t">change. So you'll need to learn something new anyway. It'll probably be Swift or Julia,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7811" target="_blank">02:10:11.940</a></span> | <span class="t">and I think they're both perfectly good things to look at. Regardless, I've spent time using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7818" target="_blank">02:10:18.780</a></span> | <span class="t">in real-world scenarios at least a couple of dozen languages, and every time I learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7823" target="_blank">02:10:23.980</a></span> | <span class="t">a new language, I become a better developer. So it's just a good idea to learn a new language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7831" target="_blank">02:10:31.340</a></span> | <span class="t">And like the for TensorFlow bit might put you off a bit, because I've complained a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7834" target="_blank">02:10:34.780</a></span> | <span class="t">about TensorFlow, but there's a flow in the future that's going to look almost totally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7841" target="_blank">02:10:41.220</a></span> | <span class="t">different to TensorFlow in the past. The things that are happening with Swift for TensorFlow</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7844" target="_blank">02:10:44.740</a></span> | <span class="t">are so exciting. So there's basically almost no data science ecosystem for Swift, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7853" target="_blank">02:10:53.260</a></span> | <span class="t">means the whole thing is open for you to contribute to. So you can make serious contributions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7858" target="_blank">02:10:58.740</a></span> | <span class="t">look at any Python little library or just one function that doesn't exist in Swift and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7864" target="_blank">02:11:04.980</a></span> | <span class="t">write it. The Swift community doesn't have people like us. They have people that understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7874" target="_blank">02:11:14.220</a></span> | <span class="t">deep learning. They're just not people who are generally in the Swift community right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7877" target="_blank">02:11:17.300</a></span> | <span class="t">now with some exceptions. So we are valued. And you'll be working on stuff that will look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7886" target="_blank">02:11:26.500</a></span> | <span class="t">pretty familiar, because we're building something a lot like fast AI, but hopefully much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7892" target="_blank">02:11:32.660</a></span> | <span class="t">So with that, I have here Chris Latner, who come on over, who started the Swift project</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7900" target="_blank">02:11:40.540</a></span> | <span class="t">and is now running Swift for TensorFlow team at Google. And we have time for I think three</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7906" target="_blank">02:11:46.200</a></span> | <span class="t">questions from the community for Chris and I.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7909" target="_blank">02:11:49.020</a></span> | <span class="t">>> Sure. Assuming someone has zero knowledge of Swift, what would be the most efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7913" target="_blank">02:11:53.260</a></span> | <span class="t">way to learn it and get up to speed with using Swift for TensorFlow?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7917" target="_blank">02:11:57.220</a></span> | <span class="t">>> Sure. So the courses we're teaching will assume they don't have prior Swift experience,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7923" target="_blank">02:12:03.220</a></span> | <span class="t">but if you're interested, you can go to Swift.org. In the documentation tab, there's a whole book</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7927" target="_blank">02:12:07.200</a></span> | <span class="t">online. The thing I recommend is there's a thing called A-Swift Tour. You can just Google</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7933" target="_blank">02:12:13.060</a></span> | <span class="t">for that. It gives you a really quick sense of what it looks like. And it explains the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7938" target="_blank">02:12:18.180</a></span> | <span class="t">basic concepts. It's super accessible. That's where I want to start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7941" target="_blank">02:12:21.420</a></span> | <span class="t">>> The best version of the Swift book is on the iPad. It uses something called Swift Playgrounds,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7946" target="_blank">02:12:26.260</a></span> | <span class="t">which is one of these amazing things that Chris built, which basically lets you go through</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7949" target="_blank">02:12:29.380</a></span> | <span class="t">the book in a very interactive way. It will feel a lot like the experience of using a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7953" target="_blank">02:12:33.900</a></span> | <span class="t">Jupyter notebook, but it's even more fancy in some ways. So you can read the book as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7958" target="_blank">02:12:38.660</a></span> | <span class="t">you experiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7959" target="_blank">02:12:39.660</a></span> | <span class="t">>> As Swift for TensorFlow evolves, what do you think will be the first kind of machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7965" target="_blank">02:12:45.100</a></span> | <span class="t">learning work accessible to people who don't have access to big corporate data centers where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7970" target="_blank">02:12:50.300</a></span> | <span class="t">Swift for TensorFlow's particular strengths will make it a better choice than the more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7973" target="_blank">02:12:53.740</a></span> | <span class="t">traditional Python frameworks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7976" target="_blank">02:12:56.180</a></span> | <span class="t">>> Sure. I don't know what that first thing will be. But I think you have to look at the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7982" target="_blank">02:13:02.100</a></span> | <span class="t">goals of the project. And I think there's two goals for this project overall. One is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7986" target="_blank">02:13:06.220</a></span> | <span class="t">be very subtractive. And subtractive of complexity. And I think that one of the things that Jeremy's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7992" target="_blank">02:13:12.220</a></span> | <span class="t">highlighting is that in practice, being effective in the machine learning field means you end</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=7997" target="_blank">02:13:17.080</a></span> | <span class="t">up doing a lot of weird things at different levels. And so you may be dropping down to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8001" target="_blank">02:13:21.020</a></span> | <span class="t">C++ or writing CUDA code, depending on what you're doing. Or playing with these other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8006" target="_blank">02:13:26.900</a></span> | <span class="t">systems or these other C libraries that get wrapped up with Python. But these become leaky</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8012" target="_blank">02:13:32.420</a></span> | <span class="t">abstractions you have to deal with. So we're trying to make it so you don't have to deal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8015" target="_blank">02:13:35.300</a></span> | <span class="t">with a lot of that complexity. So you can stay in one language. It works top to bottom.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8020" target="_blank">02:13:40.020</a></span> | <span class="t">It's fast, has lots of other good things to go with it. So that's one aspect of it. The</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8024" target="_blank">02:13:44.020</a></span> | <span class="t">other pieces, we're thinking about it from the bottom up, including the compiler bits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8028" target="_blank">02:13:48.580</a></span> | <span class="t">all the systems integration pieces, the application integration pieces. And I have a theory that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8033" target="_blank">02:13:53.900</a></span> | <span class="t">once we get past the world of Python here, that people are going to start doing a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8037" target="_blank">02:13:57.820</a></span> | <span class="t">of really interesting things where you integrate deep learning into applications. And right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8043" target="_blank">02:14:03.300</a></span> | <span class="t">now the application world and the ML world are different. I mean, people literally export</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8049" target="_blank">02:14:09.860</a></span> | <span class="t">their model into like an ONNX or TF serving or whatever, and dump it into some C++ thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8056" target="_blank">02:14:16.060</a></span> | <span class="t">where it's a whole new world. It's a completely different world. And so now you have this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8060" target="_blank">02:14:20.060</a></span> | <span class="t">barrier between the training, the learning and the ML pieces. And you have the application</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8066" target="_blank">02:14:26.700</a></span> | <span class="t">pieces. And often these are different teams or different people thinking about things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8070" target="_blank">02:14:30.060</a></span> | <span class="t">in different ways. And breaking down those kinds of barriers, I think is a really big</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8074" target="_blank">02:14:34.180</a></span> | <span class="t">opportunity that enables new kinds of work to be done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8077" target="_blank">02:14:37.060</a></span> | <span class="t">Very powerful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8078" target="_blank">02:14:38.060</a></span> | <span class="t">And that leads well into the next pair of questions. Does it make sense to spend efforts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8083" target="_blank">02:14:43.720</a></span> | <span class="t">learning and writing in Swift only, or is it worth to have some understanding of C++</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8088" target="_blank">02:14:48.180</a></span> | <span class="t">as well to be good in numerical computations? And then secondly, after going through some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8093" target="_blank">02:14:53.180</a></span> | <span class="t">of the Swift documentations, it seems like it's a very versatile language. If I understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8097" target="_blank">02:14:57.820</a></span> | <span class="t">correctly, deep learning, robotics, web development, and systems programming all seem well under</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8102" target="_blank">02:15:02.260</a></span> | <span class="t">its purview. Do you foresee Swift's influence flourishing in all these separate areas and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8107" target="_blank">02:15:07.080</a></span> | <span class="t">allowing for a tighter and more fluid development between disciplines?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8110" target="_blank">02:15:10.700</a></span> | <span class="t">Sure. So I think these are two sides of the same coin. I totally agree with Jeremy. Learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8116" target="_blank">02:15:16.500</a></span> | <span class="t">new programming languages is good just because often you learn to think about things in a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8123" target="_blank">02:15:23.060</a></span> | <span class="t">new way, or they open up new kinds of approaches, and having more different kinds of mental</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8128" target="_blank">02:15:28.900</a></span> | <span class="t">frameworks gives you the ability to solve problems that otherwise you might not be able</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8132" target="_blank">02:15:32.140</a></span> | <span class="t">to do. And so learning C++ in the abstract is a good thing. Having to use C++ is a little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8141" target="_blank">02:15:41.620</a></span> | <span class="t">bit of a different thing, in my opinion. And so C++ has lots of drawbacks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8145" target="_blank">02:15:45.220</a></span> | <span class="t">This is coming from somebody who's written a C++ compiler.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8147" target="_blank">02:15:47.780</a></span> | <span class="t">I've written way too much C++ myself, and maybe I'm a little bit damaged here, but C++</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8153" target="_blank">02:15:53.380</a></span> | <span class="t">is a super complicated language. It's also full of memory safety problems and security</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8158" target="_blank">02:15:58.460</a></span> | <span class="t">vulnerabilities and a lot of other things that are pretty well known. It's a great language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8163" target="_blank">02:16:03.100</a></span> | <span class="t">It supports tons of really important work, but one of the goals with Swift is to be a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8167" target="_blank">02:16:07.580</a></span> | <span class="t">full stack language and really span from the scripting all the way down to the things C++</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8172" target="_blank">02:16:12.460</a></span> | <span class="t">is good at, and getting C++-level performance in the same language that you can do high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8178" target="_blank">02:16:18.380</a></span> | <span class="t">machine learning frameworks in is pretty cool. I think that that's one of the really unique</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8185" target="_blank">02:16:25.380</a></span> | <span class="t">aspects of Swift, is it was designed for compilation, for usability, for accessibility, and I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8192" target="_blank">02:16:32.300</a></span> | <span class="t">not aware of a system that's similar in that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8194" target="_blank">02:16:34.660</a></span> | <span class="t">Great. I'm really looking forward to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8196" target="_blank">02:16:36.500</a></span> | <span class="t">Can we ask one more question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8197" target="_blank">02:16:37.500</a></span> | <span class="t">I think we're out of time. Sorry. Yeah. One more question next time. Thanks, everybody.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8205" target="_blank">02:16:45.020</a></span> | <span class="t">Thank you, Chris Latner, and we'll see you next week.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=vnOpEwmtFJ8&t=8206" target="_blank">02:16:46.620</a></span> | <span class="t">[APPLAUSE]</span></div></div></body></html>