<html><head><title>The spelled-out intro to language modeling: building makemore</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>The spelled-out intro to language modeling: building makemore</h2><a href="https://www.youtube.com/watch?v=PaCmpygFfXo"><img src="https://i.ytimg.com/vi_webp/PaCmpygFfXo/maxresdefault.webp" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=0">0:0</a> intro<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=183">3:3</a> reading and exploring the dataset<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=384">6:24</a> exploring the bigrams in the dataset<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=564">9:24</a> counting bigrams in a python dictionary<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=765">12:45</a> counting bigrams in a 2D torch tensor ("training the model")<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1099">18:19</a> visualizing the bigram tensor<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1254">20:54</a> deleting spurious (S) and (E) tokens in favor of a single . token<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1442">24:2</a> sampling from the model<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2177">36:17</a> efficiency! vectorized normalization of the rows, tensor broadcasting<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3014">50:14</a> loss function (the negative log likelihood of the data under our model)<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3650">60:50</a> model smoothing with fake counts<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3777">62:57</a> PART 2: the neural network approach: intro<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3926">65:26</a> creating the bigram dataset for the neural net<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4201">70:1</a> feeding integers into neural nets? one-hot encodings<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4433">73:53</a> the "neural net": one linear layer of neurons implemented with matrix multiplication<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4726">78:46</a> transforming neural net outputs into probabilities: the softmax<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5177">86:17</a> summary, preview to next steps, reference to micrograd<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5749">95:49</a> vectorized loss<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5916">98:36</a> backward and update, in PyTorch<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6175">102:55</a> putting everything together<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6469">107:49</a> note 1: one-hot encoding really just selects a row of the next Linear layer's weight matrix<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6618">110:18</a> note 2: model smoothing as regularization loss<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6871">114:31</a> sampling from the neural net<br><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6976">116:16</a> conclusion<br><br><div style="text-align: left;"><a href="./PaCmpygFfXo.html">Whisper Transcript</a> | <a href="./transcript_PaCmpygFfXo.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hi everyone, hope you're well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2" target="_blank">00:00:02.480</a></span> | <span class="t">And next up what I'd like to do is I'd like to build out Makemore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6" target="_blank">00:00:06.320</a></span> | <span class="t">Like micrograd before it, Makemore is a repository that I have on my GitHub web page.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=11" target="_blank">00:00:11.440</a></span> | <span class="t">You can look at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=12" target="_blank">00:00:12.760</a></span> | <span class="t">But just like with micrograd, I'm going to build it out step by step, and I'm going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=16" target="_blank">00:00:16.600</a></span> | <span class="t">spell everything out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=17" target="_blank">00:00:17.920</a></span> | <span class="t">So we're going to build it out slowly and together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=20" target="_blank">00:00:20.400</a></span> | <span class="t">Now what is Makemore?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=22" target="_blank">00:00:22.000</a></span> | <span class="t">Makemore, as the name suggests, makes more of things that you give it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=27" target="_blank">00:00:27.800</a></span> | <span class="t">So here's an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=29" target="_blank">00:00:29.320</a></span> | <span class="t">Names.txt is an example dataset to Makemore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=32" target="_blank">00:00:32.800</a></span> | <span class="t">And when you look at names.txt, you'll find that it's a very large dataset of names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=38" target="_blank">00:00:38.360</a></span> | <span class="t">So here's lots of different types of names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=41" target="_blank">00:00:41.800</a></span> | <span class="t">In fact, I believe there are 32,000 names that I've sort of found randomly on a government</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=46" target="_blank">00:00:46.520</a></span> | <span class="t">website.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=48" target="_blank">00:00:48.120</a></span> | <span class="t">And if you train Makemore on this dataset, it will learn to make more of things like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=53" target="_blank">00:00:53.360</a></span> | <span class="t">this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=54" target="_blank">00:00:54.360</a></span> | <span class="t">And in particular, in this case, that will mean more things that sound name-like, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=60" target="_blank">00:01:00.600</a></span> | <span class="t">are actually unique names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=62" target="_blank">00:01:02.560</a></span> | <span class="t">And maybe if you have a baby and you're trying to assign a name, maybe you're looking for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=65" target="_blank">00:01:05.680</a></span> | <span class="t">a cool new sounding unique name, Makemore might help you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=69" target="_blank">00:01:09.780</a></span> | <span class="t">So here are some example generations from the neural network once we train it on our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=74" target="_blank">00:01:14.720</a></span> | <span class="t">dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=76" target="_blank">00:01:16.440</a></span> | <span class="t">So here's some example unique names that it will generate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=80" target="_blank">00:01:20.000</a></span> | <span class="t">So, "Montel", "Irat", "Zendi", and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=85" target="_blank">00:01:25.840</a></span> | <span class="t">And so all these sort of sound name-like, but they're not, of course, names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=90" target="_blank">00:01:30.900</a></span> | <span class="t">So under the hood, Makemore is a character-level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=94" target="_blank">00:01:34.980</a></span> | <span class="t">So what that means is that it is treating every single line here as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=99" target="_blank">00:01:39.880</a></span> | <span class="t">And within each example, it's treating them all as sequences of individual characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=105" target="_blank">00:01:45.260</a></span> | <span class="t">So R-E-E-S-E is this example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=109" target="_blank">00:01:49.160</a></span> | <span class="t">And that's the sequence of characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=110" target="_blank">00:01:50.800</a></span> | <span class="t">And that's the level on which we are building out Makemore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=114" target="_blank">00:01:54.160</a></span> | <span class="t">And what it means to be a character-level language model then is that it's just sort</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=118" target="_blank">00:01:58.440</a></span> | <span class="t">of modeling those sequences of characters, and it knows how to predict the next character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=122" target="_blank">00:02:02.000</a></span> | <span class="t">in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=123" target="_blank">00:02:03.840</a></span> | <span class="t">Now we're actually going to implement a large number of character-level language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=128" target="_blank">00:02:08.160</a></span> | <span class="t">in terms of the neural networks that are involved in predicting the next character in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=132" target="_blank">00:02:12.520</a></span> | <span class="t">So very simple bigram and bag-of-word models, multilayered perceptrons, recurrent neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=137" target="_blank">00:02:17.720</a></span> | <span class="t">networks, all the way to modern transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=139" target="_blank">00:02:19.840</a></span> | <span class="t">In fact, the transformer that we will build will be basically the equivalent transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=144" target="_blank">00:02:24.880</a></span> | <span class="t">to GPT-2, if you have heard of GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=148" target="_blank">00:02:28.280</a></span> | <span class="t">So that's kind of a big deal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=149" target="_blank">00:02:29.480</a></span> | <span class="t">It's a modern network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=150" target="_blank">00:02:30.840</a></span> | <span class="t">And by the end of the series, you will actually understand how that works on the level of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=155" target="_blank">00:02:35.000</a></span> | <span class="t">characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=156" target="_blank">00:02:36.000</a></span> | <span class="t">Now, to give you a sense of the extensions here, after characters, we will probably spend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=161" target="_blank">00:02:41.280</a></span> | <span class="t">some time on the word level so that we can generate documents of words, not just little</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=165" target="_blank">00:02:45.640</a></span> | <span class="t">segments of characters, but we can generate entire large, much larger documents.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=171" target="_blank">00:02:51.280</a></span> | <span class="t">And then we're probably going to go into images and image-text networks, such as DALI, stable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=176" target="_blank">00:02:56.600</a></span> | <span class="t">diffusion, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=178" target="_blank">00:02:58.360</a></span> | <span class="t">But for now, we have to start here, character-level language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=182" target="_blank">00:03:02.520</a></span> | <span class="t">Let's go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=183" target="_blank">00:03:03.660</a></span> | <span class="t">So like before, we are starting with a completely blank Jupyter Notebook page.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=187" target="_blank">00:03:07.240</a></span> | <span class="t">The first thing is I would like to basically load up the dataset, names.txt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=192" target="_blank">00:03:12.040</a></span> | <span class="t">So we're going to open up names.txt for reading, and we're going to read in everything into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=197" target="_blank">00:03:17.680</a></span> | <span class="t">a massive string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=200" target="_blank">00:03:20.060</a></span> | <span class="t">And then because it's a massive string, we'd only like the individual words and put them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=203" target="_blank">00:03:23.520</a></span> | <span class="t">in the list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=204" target="_blank">00:03:24.760</a></span> | <span class="t">So let's call splitlines on that string to get all of our words as a Python list of strings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=212" target="_blank">00:03:32.320</a></span> | <span class="t">So basically, we can look at, for example, the first 10 words, and we have that it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=217" target="_blank">00:03:37.320</a></span> | <span class="t">a list of Emma, Olivia, Ava, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=221" target="_blank">00:03:41.800</a></span> | <span class="t">And if we look at the top of the page here, that is indeed what we see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=228" target="_blank">00:03:48.440</a></span> | <span class="t">So that's good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=229" target="_blank">00:03:49.980</a></span> | <span class="t">This list actually makes me feel that this is probably sorted by frequency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=235" target="_blank">00:03:55.880</a></span> | <span class="t">But okay, so these are the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=238" target="_blank">00:03:58.560</a></span> | <span class="t">Now we'd like to actually learn a little bit more about this dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=242" target="_blank">00:04:02.120</a></span> | <span class="t">Let's look at the total number of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=243" target="_blank">00:04:03.500</a></span> | <span class="t">We expect this to be roughly 32,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=246" target="_blank">00:04:06.760</a></span> | <span class="t">And then what is the, for example, shortest word?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=249" target="_blank">00:04:09.360</a></span> | <span class="t">So min of len of each word for w in words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=253" target="_blank">00:04:13.880</a></span> | <span class="t">So the shortest word will be length 2, and max of len w for w in words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=260" target="_blank">00:04:20.680</a></span> | <span class="t">So the longest word will be 15 characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=264" target="_blank">00:04:24.900</a></span> | <span class="t">So let's now think through our very first language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=267" target="_blank">00:04:27.600</a></span> | <span class="t">As I mentioned, a character-level language model is predicting the next character in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=271" target="_blank">00:04:31.280</a></span> | <span class="t">a sequence given already some concrete sequence of characters before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=276" target="_blank">00:04:36.800</a></span> | <span class="t">Now what we have to realize here is that every single word here, like Isabella, is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=281" target="_blank">00:04:41.560</a></span> | <span class="t">quite a few examples packed in to that single word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=285" target="_blank">00:04:45.880</a></span> | <span class="t">Because what is an existence of a word like Isabella in the dataset telling us, really?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=290" target="_blank">00:04:50.020</a></span> | <span class="t">It's saying that the character i is a very likely character to come first in a sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=296" target="_blank">00:04:56.540</a></span> | <span class="t">of a name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=298" target="_blank">00:04:58.780</a></span> | <span class="t">The character s is likely to come after i.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=304" target="_blank">00:05:04.560</a></span> | <span class="t">The character a is likely to come after is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=307" target="_blank">00:05:07.880</a></span> | <span class="t">The character b is very likely to come after isa.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=310" target="_blank">00:05:10.920</a></span> | <span class="t">And so on, all the way to a following Isabelle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=314" target="_blank">00:05:14.680</a></span> | <span class="t">And then there's one more example actually packed in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=317" target="_blank">00:05:17.540</a></span> | <span class="t">And that is that after there's Isabella, the word is very likely to end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=324" target="_blank">00:05:24.020</a></span> | <span class="t">So that's one more sort of explicit piece of information that we have here that we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=328" target="_blank">00:05:28.160</a></span> | <span class="t">to be careful with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=329" target="_blank">00:05:29.900</a></span> | <span class="t">And so there's a lot packed into a single individual word in terms of the statistical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=334" target="_blank">00:05:34.280</a></span> | <span class="t">structure of what's likely to follow in these character sequences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=338" target="_blank">00:05:38.340</a></span> | <span class="t">And then of course, we don't have just an individual word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=340" target="_blank">00:05:40.480</a></span> | <span class="t">We actually have 32,000 of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=342" target="_blank">00:05:42.360</a></span> | <span class="t">And so there's a lot of structure here to model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=345" target="_blank">00:05:45.080</a></span> | <span class="t">Now in the beginning, what I'd like to start with is I'd like to start with building a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=348" target="_blank">00:05:48.520</a></span> | <span class="t">bigram language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=351" target="_blank">00:05:51.520</a></span> | <span class="t">Now in a bigram language model, we're always working with just two characters at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=356" target="_blank">00:05:56.920</a></span> | <span class="t">So we're only looking at one character that we are given, and we're trying to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=361" target="_blank">00:06:01.240</a></span> | <span class="t">the next character in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=364" target="_blank">00:06:04.080</a></span> | <span class="t">So what characters are likely to follow are, what characters are likely to follow a, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=369" target="_blank">00:06:09.360</a></span> | <span class="t">so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=370" target="_blank">00:06:10.360</a></span> | <span class="t">And we're just modeling that kind of a little local structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=373" target="_blank">00:06:13.160</a></span> | <span class="t">And we're forgetting the fact that we may have a lot more information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=377" target="_blank">00:06:17.000</a></span> | <span class="t">We're always just looking at the previous character to predict the next one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=380" target="_blank">00:06:20.440</a></span> | <span class="t">So it's a very simple and weak language model, but I think it's a great place to start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=384" target="_blank">00:06:24.360</a></span> | <span class="t">So now let's begin by looking at these bigrams in our dataset and what they look like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=388" target="_blank">00:06:28.120</a></span> | <span class="t">And these bigrams again are just two characters in a row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=391" target="_blank">00:06:31.160</a></span> | <span class="t">So for w in words, each w here is an individual word, a string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=396" target="_blank">00:06:36.600</a></span> | <span class="t">We want to iterate this word with consecutive characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=403" target="_blank">00:06:43.880</a></span> | <span class="t">So two characters at a time, sliding it through the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=406" target="_blank">00:06:46.960</a></span> | <span class="t">Now a interesting, nice way, cute way to do this in Python, by the way, is doing something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=412" target="_blank">00:06:52.120</a></span> | <span class="t">like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=413" target="_blank">00:06:53.120</a></span> | <span class="t">Character one, character two in zip of w and w at one, one column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=422" target="_blank">00:07:02.080</a></span> | <span class="t">Print character one, character two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=424" target="_blank">00:07:04.880</a></span> | <span class="t">And let's not do all the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=425" target="_blank">00:07:05.880</a></span> | <span class="t">Let's just do the first three words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=427" target="_blank">00:07:07.200</a></span> | <span class="t">And I'm going to show you in a second how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=430" target="_blank">00:07:10.240</a></span> | <span class="t">But for now, basically as an example, let's just do the very first word alone, Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=435" target="_blank">00:07:15.560</a></span> | <span class="t">You see how we have a Emma and this will just print em, mm, ma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=441" target="_blank">00:07:21.200</a></span> | <span class="t">And the reason this works is because w is the string Emma, w at one column is the string</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=447" target="_blank">00:07:27.260</a></span> | <span class="t">mma, and zip takes two iterators and it pairs them up and then creates an iterator over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=454" target="_blank">00:07:34.940</a></span> | <span class="t">the tuples of their consecutive entries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=457" target="_blank">00:07:37.600</a></span> | <span class="t">And if any one of these lists is shorter than the other, then it will just halt and return.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=463" target="_blank">00:07:43.920</a></span> | <span class="t">So basically that's why we return em, mm, mm, ma, but then because this iterator, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=472" target="_blank">00:07:52.040</a></span> | <span class="t">second one here, runs out of elements, zip just ends.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=475" target="_blank">00:07:55.960</a></span> | <span class="t">And that's why we only get these tuples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=477" target="_blank">00:07:57.920</a></span> | <span class="t">So pretty cute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=480" target="_blank">00:08:00.000</a></span> | <span class="t">So these are the consecutive elements in the first word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=483" target="_blank">00:08:03.320</a></span> | <span class="t">Now we have to be careful because we actually have more information here than just these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=486" target="_blank">00:08:06.400</a></span> | <span class="t">three examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=488" target="_blank">00:08:08.080</a></span> | <span class="t">As I mentioned, we know that e is very likely to come first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=492" target="_blank">00:08:12.080</a></span> | <span class="t">And we know that a in this case is coming last.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=495" target="_blank">00:08:15.840</a></span> | <span class="t">So what I'm going to do this is basically we're going to create a special array here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=501" target="_blank">00:08:21.120</a></span> | <span class="t">our characters, and we're going to hallucinate a special start token here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=507" target="_blank">00:08:27.800</a></span> | <span class="t">I'm going to call it like special start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=512" target="_blank">00:08:32.680</a></span> | <span class="t">So this is a list of one element plus w and then plus a special end character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=521" target="_blank">00:08:41.520</a></span> | <span class="t">And the reason I'm wrapping the list of w here is because w is a string, Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=526" target="_blank">00:08:46.240</a></span> | <span class="t">List of w will just have the individual characters in the list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=531" target="_blank">00:08:51.080</a></span> | <span class="t">And then doing this again now, but not iterating over w's, but over the characters will give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=538" target="_blank">00:08:58.560</a></span> | <span class="t">us something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=540" target="_blank">00:09:00.360</a></span> | <span class="t">So e is likely, so this is a bigram of the start character and e, and this is a bigram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=545" target="_blank">00:09:05.560</a></span> | <span class="t">of the a and the special end character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=549" target="_blank">00:09:09.320</a></span> | <span class="t">And now we can look at, for example, what this looks like for Olivia or Eva.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=553" target="_blank">00:09:13.600</a></span> | <span class="t">And indeed, we can actually potentially do this for the entire dataset, but we won't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=558" target="_blank">00:09:18.800</a></span> | <span class="t">print that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=559" target="_blank">00:09:19.800</a></span> | <span class="t">That's going to be too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=560" target="_blank">00:09:20.800</a></span> | <span class="t">But these are the individual character bigrams and we can print them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=565" target="_blank">00:09:25.160</a></span> | <span class="t">Now in order to learn the statistics about which characters are likely to follow other</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=569" target="_blank">00:09:29.400</a></span> | <span class="t">characters, the simplest way in the bigram language models is to simply do it by counting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=574" target="_blank">00:09:34.680</a></span> | <span class="t">So we're basically just going to count how often any one of these combinations occurs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=579" target="_blank">00:09:39.060</a></span> | <span class="t">in the training set in these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=582" target="_blank">00:09:42.040</a></span> | <span class="t">So we're going to need some kind of a dictionary that's going to maintain some counts for every</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=586" target="_blank">00:09:46.000</a></span> | <span class="t">one of these bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=587" target="_blank">00:09:47.540</a></span> | <span class="t">So let's use a dictionary B and this will map these bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=592" target="_blank">00:09:52.920</a></span> | <span class="t">So bigram is a tuple of character one, character two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=596" target="_blank">00:09:56.440</a></span> | <span class="t">And then B at bigram will be B dot get of bigram, which is basically the same as B at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=603" target="_blank">00:10:03.120</a></span> | <span class="t">bigram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=604" target="_blank">00:10:04.960</a></span> | <span class="t">But in the case that bigram is not in the dictionary B, we would like to by default</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=609" target="_blank">00:10:09.880</a></span> | <span class="t">return a zero plus one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=613" target="_blank">00:10:13.320</a></span> | <span class="t">So this will basically add up all the bigrams and count how often they occur.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=618" target="_blank">00:10:18.540</a></span> | <span class="t">Let's get rid of printing or rather let's keep the printing and let's just inspect what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=624" target="_blank">00:10:24.800</a></span> | <span class="t">B is in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=627" target="_blank">00:10:27.400</a></span> | <span class="t">And we see that many bigrams occur just a single time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=630" target="_blank">00:10:30.500</a></span> | <span class="t">This one allegedly occurred three times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=633" target="_blank">00:10:33.240</a></span> | <span class="t">So A was an ending character three times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=635" target="_blank">00:10:35.720</a></span> | <span class="t">And that's true for all of these words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=638" target="_blank">00:10:38.160</a></span> | <span class="t">All of Emma, Olivia and Eva end with A. So that's why this occurred three times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=646" target="_blank">00:10:46.800</a></span> | <span class="t">Now let's do it for all the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=649" target="_blank">00:10:49.840</a></span> | <span class="t">Oops, I should not have printed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=653" target="_blank">00:10:53.800</a></span> | <span class="t">I meant to erase that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=657" target="_blank">00:10:57.000</a></span> | <span class="t">Let's kill this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=659" target="_blank">00:10:59.000</a></span> | <span class="t">Let's just run and now B will have the statistics of the entire dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=664" target="_blank">00:11:04.360</a></span> | <span class="t">So these are the counts across all the words of the individual bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=668" target="_blank">00:11:08.680</a></span> | <span class="t">And we could, for example, look at some of the most common ones and least common ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=673" target="_blank">00:11:13.640</a></span> | <span class="t">This kind of grows in Python, but the way to do this, the simplest way I like is we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=677" target="_blank">00:11:17.480</a></span> | <span class="t">just use B dot items.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=679" target="_blank">00:11:19.800</a></span> | <span class="t">B dot items returns the tuples of key value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=685" target="_blank">00:11:25.560</a></span> | <span class="t">In this case, the keys are the character bigrams and the values are the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=691" target="_blank">00:11:31.160</a></span> | <span class="t">And so then what we want to do is we want to do sorted of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=698" target="_blank">00:11:38.720</a></span> | <span class="t">But by default, sort is on the first item of a tuple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=705" target="_blank">00:11:45.800</a></span> | <span class="t">But we want to sort by the values, which are the second element of a tuple that is the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=709" target="_blank">00:11:49.320</a></span> | <span class="t">key value.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=710" target="_blank">00:11:50.860</a></span> | <span class="t">So we want to use the key equals lambda that takes the key value and returns the key value</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=720" target="_blank">00:12:00.080</a></span> | <span class="t">at one, not at zero, but at one, which is the count.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=724" target="_blank">00:12:04.160</a></span> | <span class="t">So we want to sort by the count of these elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=730" target="_blank">00:12:10.680</a></span> | <span class="t">And actually we want it to go backwards.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=732" target="_blank">00:12:12.860</a></span> | <span class="t">So here what we have is the bigram QNR occurs only a single time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=738" target="_blank">00:12:18.160</a></span> | <span class="t">DZ occurred only a single time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=740" target="_blank">00:12:20.920</a></span> | <span class="t">And when we sort this the other way around, we're going to see the most likely bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=746" target="_blank">00:12:26.520</a></span> | <span class="t">So we see that N was very often an ending character, many, many times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=751" target="_blank">00:12:31.960</a></span> | <span class="t">And apparently N almost always follows an A, and that's a very likely combination as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=756" target="_blank">00:12:36.160</a></span> | <span class="t">well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=758" target="_blank">00:12:38.880</a></span> | <span class="t">So this is kind of the individual counts that we achieve over the entire dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=765" target="_blank">00:12:45.160</a></span> | <span class="t">Now it's actually going to be significantly more convenient for us to keep this information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=769" target="_blank">00:12:49.520</a></span> | <span class="t">in a two-dimensional array instead of a Python dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=773" target="_blank">00:12:53.800</a></span> | <span class="t">So we're going to store this information in a 2D array, and the rows are going to be the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=781" target="_blank">00:13:01.160</a></span> | <span class="t">first character of the bigram, and the columns are going to be the second character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=785" target="_blank">00:13:05.280</a></span> | <span class="t">And each entry in this two-dimensional array will tell us how often that first character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=789" target="_blank">00:13:09.240</a></span> | <span class="t">follows the second character in the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=792" target="_blank">00:13:12.920</a></span> | <span class="t">So in particular, the array representation that we're going to use, or the library, is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=797" target="_blank">00:13:17.440</a></span> | <span class="t">that of PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=798" target="_blank">00:13:18.440</a></span> | <span class="t">And PyTorch is a deep learning neural network framework, but part of it is also this torch.tensor,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=805" target="_blank">00:13:25.440</a></span> | <span class="t">which allows us to create multidimensional arrays and manipulate them very efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=810" target="_blank">00:13:30.000</a></span> | <span class="t">So let's import PyTorch, which you can do by import torch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=814" target="_blank">00:13:34.960</a></span> | <span class="t">And then we can create arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=817" target="_blank">00:13:37.740</a></span> | <span class="t">So let's create an array of zeros, and we give it a size of this array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=824" target="_blank">00:13:44.100</a></span> | <span class="t">Let's create a 3x5 array as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=827" target="_blank">00:13:47.420</a></span> | <span class="t">And this is a 3x5 array of zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=831" target="_blank">00:13:51.860</a></span> | <span class="t">And by default, you'll notice a.dtype, which is short for datatype, is float32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=836" target="_blank">00:13:56.820</a></span> | <span class="t">So these are single precision floating point numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=839" target="_blank">00:13:59.820</a></span> | <span class="t">Because we are going to represent counts, let's actually use dtype as torch.int32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=846" target="_blank">00:14:06.220</a></span> | <span class="t">So these are 32-bit integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=850" target="_blank">00:14:10.340</a></span> | <span class="t">So now you see that we have integer data inside this tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=854" target="_blank">00:14:14.820</a></span> | <span class="t">Now tensors allow us to really manipulate all the individual entries and do it very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=859" target="_blank">00:14:19.700</a></span> | <span class="t">efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=860" target="_blank">00:14:20.960</a></span> | <span class="t">So for example, if we want to change this bit, we have to index into the tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=865" target="_blank">00:14:25.980</a></span> | <span class="t">And in particular, here, this is the first row, because it's zero-indexed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=873" target="_blank">00:14:33.000</a></span> | <span class="t">So this is row index 1, and column index 0, 1, 2, 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=878" target="_blank">00:14:38.980</a></span> | <span class="t">So a at 1, 3, we can set that to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=883" target="_blank">00:14:43.920</a></span> | <span class="t">And then a will have a 1 over there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=887" target="_blank">00:14:47.380</a></span> | <span class="t">We can of course also do things like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=889" target="_blank">00:14:49.460</a></span> | <span class="t">So now a will be 2 over there, or 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=894" target="_blank">00:14:54.140</a></span> | <span class="t">And also we can, for example, say a[0, 0] is 5, and then a will have a 5 over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=900" target="_blank">00:15:00.320</a></span> | <span class="t">So that's how we can index into the arrays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=903" target="_blank">00:15:03.480</a></span> | <span class="t">Now of course the array that we are interested in is much, much bigger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=906" target="_blank">00:15:06.420</a></span> | <span class="t">So for our purposes, we have 26 letters of the alphabet, and then we have two special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=911" target="_blank">00:15:11.560</a></span> | <span class="t">characters, s and e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=914" target="_blank">00:15:14.020</a></span> | <span class="t">So we want 26+2, or 28 by 28 array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=919" target="_blank">00:15:19.500</a></span> | <span class="t">And let's call it the capital N, because it's going to represent the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=924" target="_blank">00:15:24.700</a></span> | <span class="t">Let me erase this stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=927" target="_blank">00:15:27.020</a></span> | <span class="t">So that's the array that starts at 0s, 28 by 28.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=930" target="_blank">00:15:30.700</a></span> | <span class="t">And now let's copy-paste this here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=934" target="_blank">00:15:34.860</a></span> | <span class="t">But instead of having a dictionary b, which we're going to erase, we now have an n.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=941" target="_blank">00:15:41.260</a></span> | <span class="t">Now the problem here is that we have these characters, which are strings, but we have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=944" target="_blank">00:15:44.580</a></span> | <span class="t">to now basically index into an array, and we have to index using integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=951" target="_blank">00:15:51.580</a></span> | <span class="t">So we need some kind of a lookup table from characters to integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=955" target="_blank">00:15:55.540</a></span> | <span class="t">So let's construct such a character array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=958" target="_blank">00:15:58.300</a></span> | <span class="t">And the way we're going to do this is we're going to take all the words, which is a list</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=961" target="_blank">00:16:01.620</a></span> | <span class="t">of strings, we're going to concatenate all of it into a massive string, so this is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=966" target="_blank">00:16:06.060</a></span> | <span class="t">simply the entire dataset as a single string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=969" target="_blank">00:16:09.500</a></span> | <span class="t">We're going to pass this to the set constructor, which takes this massive string and throws</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=975" target="_blank">00:16:15.320</a></span> | <span class="t">out duplicates, because sets do not allow duplicates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=979" target="_blank">00:16:19.080</a></span> | <span class="t">So set of this will just be the set of all the lowercase characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=984" target="_blank">00:16:24.500</a></span> | <span class="t">And there should be a total of 26 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=988" target="_blank">00:16:28.860</a></span> | <span class="t">And now we actually don't want a set, we want a list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=992" target="_blank">00:16:32.900</a></span> | <span class="t">But we don't want a list sorted in some weird arbitrary way, we want it to be sorted from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=997" target="_blank">00:16:37.860</a></span> | <span class="t">A to Z.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1000" target="_blank">00:16:40.060</a></span> | <span class="t">So a sorted list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1002" target="_blank">00:16:42.100</a></span> | <span class="t">So those are our characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1005" target="_blank">00:16:45.780</a></span> | <span class="t">Now what we want is this lookup table, as I mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1007" target="_blank">00:16:47.940</a></span> | <span class="t">So let's create a special s to i, I will call it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1013" target="_blank">00:16:53.580</a></span> | <span class="t">s is string, or character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1015" target="_blank">00:16:55.740</a></span> | <span class="t">And this will be an s to i mapping for i, s in enumerate of these characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1024" target="_blank">00:17:04.460</a></span> | <span class="t">So enumerate basically gives us this iterator over the integer, index, and the actual element</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1031" target="_blank">00:17:11.100</a></span> | <span class="t">of the list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1032" target="_blank">00:17:12.100</a></span> | <span class="t">And then we are mapping the character to the integer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1035" target="_blank">00:17:15.380</a></span> | <span class="t">So s to i is a mapping from A to 0, B to 1, etc., all the way from Z to 25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1044" target="_blank">00:17:24.340</a></span> | <span class="t">And that's going to be useful here, but we actually also have to specifically set that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1047" target="_blank">00:17:27.760</a></span> | <span class="t">s will be 26, and s to i at E will be 27, because Z was 25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1056" target="_blank">00:17:36.140</a></span> | <span class="t">So those are the lookups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1057" target="_blank">00:17:37.780</a></span> | <span class="t">And now we can come here and we can map both character 1 and character 2 to their integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1062" target="_blank">00:17:42.960</a></span> | <span class="t">So this will be s to i of character 1, and i x 2 will be s to i of character 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1069" target="_blank">00:17:49.620</a></span> | <span class="t">And now we should be able to do this line, but using our array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1074" target="_blank">00:17:54.700</a></span> | <span class="t">So n at x1, i x2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1077" target="_blank">00:17:57.540</a></span> | <span class="t">This is the two-dimensional array indexing I've shown you before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1080" target="_blank">00:18:00.820</a></span> | <span class="t">And honestly, just plus equals 1, because everything starts at 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1086" target="_blank">00:18:06.380</a></span> | <span class="t">So this should work and give us a large 28 by 28 array of all these counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1094" target="_blank">00:18:14.480</a></span> | <span class="t">So if we print n, this is the array, but of course it looks ugly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1099" target="_blank">00:18:19.960</a></span> | <span class="t">So let's erase this ugly mess and let's try to visualize it a bit more nicer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1105" target="_blank">00:18:25.000</a></span> | <span class="t">So for that, we're going to use a library called matplotlib.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1109" target="_blank">00:18:29.020</a></span> | <span class="t">So matplotlib allows us to create figures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1111" target="_blank">00:18:31.200</a></span> | <span class="t">So we can do things like plt im show of the count array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1116" target="_blank">00:18:36.280</a></span> | <span class="t">So this is the 28 by 28 array, and this is the structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1121" target="_blank">00:18:41.140</a></span> | <span class="t">But even this, I would say, is still pretty ugly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1124" target="_blank">00:18:44.040</a></span> | <span class="t">So we're going to try to create a much nicer visualization of it, and I wrote a bunch of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1127" target="_blank">00:18:47.920</a></span> | <span class="t">code for that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1129" target="_blank">00:18:49.880</a></span> | <span class="t">The first thing we're going to need is we're going to need to invert this array here, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1135" target="_blank">00:18:55.240</a></span> | <span class="t">dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1136" target="_blank">00:18:56.760</a></span> | <span class="t">So s to i is a mapping from s to i, and in i to s, we're going to reverse this dictionary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1143" target="_blank">00:19:03.200</a></span> | <span class="t">So iterate over all the items and just reverse that array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1146" target="_blank">00:19:06.760</a></span> | <span class="t">So i to s maps inversely from 0 to a, 1 to b, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1152" target="_blank">00:19:12.840</a></span> | <span class="t">So we'll need that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1154" target="_blank">00:19:14.400</a></span> | <span class="t">And then here's the code that I came up with to try to make this a little bit nicer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1159" target="_blank">00:19:19.320</a></span> | <span class="t">We create a figure, we plot n, and then we visualize a bunch of things later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1167" target="_blank">00:19:27.480</a></span> | <span class="t">Let me just run it so you get a sense of what this is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1172" target="_blank">00:19:32.240</a></span> | <span class="t">So you see here that we have the array spaced out, and every one of these is basically b</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1180" target="_blank">00:19:40.040</a></span> | <span class="t">follows g zero times, b follows h 41 times, so a follows j 175 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1188" target="_blank">00:19:48.160</a></span> | <span class="t">And so what you can see that I'm doing here is first I show that entire array, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1193" target="_blank">00:19:53.240</a></span> | <span class="t">I iterate over all the individual little cells here, and I create a character string here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1199" target="_blank">00:19:59.440</a></span> | <span class="t">which is the inverse mapping i to s of the integer i and the integer j.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1204" target="_blank">00:20:04.680</a></span> | <span class="t">So those are the bigrams in a character representation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1208" target="_blank">00:20:08.800</a></span> | <span class="t">And then I plot just the bigram text, and then I plot the number of times that this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1214" target="_blank">00:20:14.400</a></span> | <span class="t">bigram occurs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1216" target="_blank">00:20:16.240</a></span> | <span class="t">Now the reason that there's a dot item here is because when you index into these arrays,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1221" target="_blank">00:20:21.120</a></span> | <span class="t">these are torch tensors, you see that we still get a tensor back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1226" target="_blank">00:20:26.160</a></span> | <span class="t">So the type of this thing, you'd think it would be just an integer, 149, but it's actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1230" target="_blank">00:20:30.140</a></span> | <span class="t">a torch dot tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1232" target="_blank">00:20:32.160</a></span> | <span class="t">And so if you do dot item, then it will pop out that individual integer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1238" target="_blank">00:20:38.640</a></span> | <span class="t">So it'll just be 149.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1240" target="_blank">00:20:40.840</a></span> | <span class="t">So that's what's happening there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1242" target="_blank">00:20:42.600</a></span> | <span class="t">And these are just some options to make it look nice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1245" target="_blank">00:20:45.440</a></span> | <span class="t">So what is the structure of this array?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1246" target="_blank">00:20:46.840</a></span> | <span class="t">We have all these counts, and we see that some of them occur often, and some of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1252" target="_blank">00:20:52.280</a></span> | <span class="t">do not occur often.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1254" target="_blank">00:20:54.120</a></span> | <span class="t">Now if you scrutinize this carefully, you will notice that we're not actually being</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1257" target="_blank">00:20:57.320</a></span> | <span class="t">very clever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1258" target="_blank">00:20:58.840</a></span> | <span class="t">That's because when you come over here, you'll notice that, for example, we have an entire</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1262" target="_blank">00:21:02.760</a></span> | <span class="t">row of completely zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1264" target="_blank">00:21:04.800</a></span> | <span class="t">And that's because the end character is never possibly going to be the first character of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1269" target="_blank">00:21:09.080</a></span> | <span class="t">a bigram, because we're always placing these end tokens at the end of the bigram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1274" target="_blank">00:21:14.360</a></span> | <span class="t">Similarly, we have entire columns of zeros here, because the s character will never possibly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1281" target="_blank">00:21:21.040</a></span> | <span class="t">be the second element of a bigram, because we always start with s and we end with e,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1285" target="_blank">00:21:25.880</a></span> | <span class="t">and we only have the words in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1287" target="_blank">00:21:27.840</a></span> | <span class="t">So we have an entire column of zeros, an entire row of zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1291" target="_blank">00:21:31.960</a></span> | <span class="t">And in this little two by two matrix here as well, the only one that can possibly happen</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1296" target="_blank">00:21:36.000</a></span> | <span class="t">is if s directly follows e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1298" target="_blank">00:21:38.580</a></span> | <span class="t">That can be non-zero if we have a word that has no letters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1303" target="_blank">00:21:43.240</a></span> | <span class="t">So in that case, there's no letters in the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1304" target="_blank">00:21:44.760</a></span> | <span class="t">It's an empty word, and we just have s follows e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1307" target="_blank">00:21:47.520</a></span> | <span class="t">But the other ones are just not possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1310" target="_blank">00:21:50.320</a></span> | <span class="t">And so we're basically wasting space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1311" target="_blank">00:21:51.820</a></span> | <span class="t">And not only that, but the s and the e are getting very crowded here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1315" target="_blank">00:21:55.600</a></span> | <span class="t">I was using these brackets because there's convention in natural language processing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1319" target="_blank">00:21:59.440</a></span> | <span class="t">to use these kinds of brackets to denote special tokens, but we're going to use something else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1325" target="_blank">00:22:05.360</a></span> | <span class="t">So let's fix all this and make it prettier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1328" target="_blank">00:22:08.380</a></span> | <span class="t">We're not actually going to have two special tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1330" target="_blank">00:22:10.440</a></span> | <span class="t">We're only going to have one special token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1333" target="_blank">00:22:13.140</a></span> | <span class="t">So we're going to have n by n array of 27 by set 27 instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1339" target="_blank">00:22:19.160</a></span> | <span class="t">Instead of having two, we will just have one, and I will call it a dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1347" target="_blank">00:22:27.520</a></span> | <span class="t">Let me swing this over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1350" target="_blank">00:22:30.600</a></span> | <span class="t">Now one more thing that I would like to do is I would actually like to make this special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1354" target="_blank">00:22:34.360</a></span> | <span class="t">character have position zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1356" target="_blank">00:22:36.480</a></span> | <span class="t">And I would like to offset all the other letters off.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1359" target="_blank">00:22:39.000</a></span> | <span class="t">I find that a little bit more pleasing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1362" target="_blank">00:22:42.800</a></span> | <span class="t">So we need a plus one here so that the first character, which is a, will start at one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1370" target="_blank">00:22:50.040</a></span> | <span class="t">So s to i will now be a starts at one and dot is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1376" target="_blank">00:22:56.120</a></span> | <span class="t">And i to s, of course, we're not changing this because i to s just creates a reverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1380" target="_blank">00:23:00.420</a></span> | <span class="t">mapping and this will work fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1382" target="_blank">00:23:02.400</a></span> | <span class="t">So one is a, two is b, zero is dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1386" target="_blank">00:23:06.760</a></span> | <span class="t">So we've reversed that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1388" target="_blank">00:23:08.160</a></span> | <span class="t">Here we have a dot and a dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1393" target="_blank">00:23:13.160</a></span> | <span class="t">This should work fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1394" target="_blank">00:23:14.960</a></span> | <span class="t">Make sure I start at zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1397" target="_blank">00:23:17.840</a></span> | <span class="t">Count.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1398" target="_blank">00:23:18.920</a></span> | <span class="t">And then here we don't go up to 28, we go up to 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1402" target="_blank">00:23:22.840</a></span> | <span class="t">And this should just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1406" target="_blank">00:23:26.960</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1411" target="_blank">00:23:31.960</a></span> | <span class="t">So we see that dot dot never happened.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1413" target="_blank">00:23:33.640</a></span> | <span class="t">It's at zero because we don't have empty words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1416" target="_blank">00:23:36.680</a></span> | <span class="t">And this row here now is just very simply the counts for all the first letters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1423" target="_blank">00:23:43.680</a></span> | <span class="t">So j starts a word, h starts a word, i starts a word, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1429" target="_blank">00:23:49.720</a></span> | <span class="t">And then these are all the ending characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1433" target="_blank">00:23:53.200</a></span> | <span class="t">And in between we have the structure of what characters follow each other.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1437" target="_blank">00:23:57.200</a></span> | <span class="t">So this is the counts array of our entire dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1441" target="_blank">00:24:01.840</a></span> | <span class="t">So this array actually has all the information necessary for us to actually sample from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1446" target="_blank">00:24:06.340</a></span> | <span class="t">bigram character level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1449" target="_blank">00:24:09.960</a></span> | <span class="t">And roughly speaking, what we're going to do is we're just going to start following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1453" target="_blank">00:24:13.640</a></span> | <span class="t">these probabilities and these counts, and we're going to start sampling from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1459" target="_blank">00:24:19.000</a></span> | <span class="t">So in the beginning, of course, we start with the dot, the start token dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1464" target="_blank">00:24:24.760</a></span> | <span class="t">So to sample the first character of a name, we're looking at this row here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1470" target="_blank">00:24:30.720</a></span> | <span class="t">So we see that we have the counts and those counts externally are telling us how often</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1475" target="_blank">00:24:35.720</a></span> | <span class="t">any one of these characters is to start a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1479" target="_blank">00:24:39.740</a></span> | <span class="t">So if we take this n and we grab the first row, we can do that by using just indexing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1487" target="_blank">00:24:47.340</a></span> | <span class="t">as zero and then using this notation, colon, for the rest of that row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1493" target="_blank">00:24:53.860</a></span> | <span class="t">So n zero colon is indexing into the zeroth row and then it's grabbing all the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1502" target="_blank">00:25:02.140</a></span> | <span class="t">And so this will give us a one dimensional array of the first row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1506" target="_blank">00:25:06.320</a></span> | <span class="t">So 0 4 4 10, you know, 0 4 4 10, 1 3 0 6, 1 5 4 2, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1513" target="_blank">00:25:13.020</a></span> | <span class="t">It's just the first row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1514" target="_blank">00:25:14.560</a></span> | <span class="t">The shape of this is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1517" target="_blank">00:25:17.420</a></span> | <span class="t">Just the row of 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1520" target="_blank">00:25:20.180</a></span> | <span class="t">And the other way that you can do this also is you just you don't actually give this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1523" target="_blank">00:25:23.920</a></span> | <span class="t">you just grab the zeroth row like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1526" target="_blank">00:25:26.420</a></span> | <span class="t">This is equivalent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1528" target="_blank">00:25:28.300</a></span> | <span class="t">Now these are the counts and now what we'd like to do is we'd like to basically sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1533" target="_blank">00:25:33.700</a></span> | <span class="t">from this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1534" target="_blank">00:25:34.700</a></span> | <span class="t">Since these are the raw counts, we actually have to convert this to probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1539" target="_blank">00:25:39.300</a></span> | <span class="t">So we create a probability vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1543" target="_blank">00:25:43.100</a></span> | <span class="t">So we'll take n of zero and we'll actually convert this to float first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1548" target="_blank">00:25:48.860</a></span> | <span class="t">OK, so these integers are converted to float, floating point numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1554" target="_blank">00:25:54.300</a></span> | <span class="t">And the reason we're creating floats is because we're about to normalize these counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1559" target="_blank">00:25:59.060</a></span> | <span class="t">So to create a probability distribution here, we want to divide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1564" target="_blank">00:26:04.020</a></span> | <span class="t">We basically want to do p, p, p divide p dot sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1569" target="_blank">00:26:09.860</a></span> | <span class="t">And now we get a vector of smaller numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1572" target="_blank">00:26:12.280</a></span> | <span class="t">And these are now probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1573" target="_blank">00:26:13.900</a></span> | <span class="t">So of course, because we divided by the sum, the sum of p now is one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1578" target="_blank">00:26:18.980</a></span> | <span class="t">So this is a nice proper probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1581" target="_blank">00:26:21.220</a></span> | <span class="t">It sums to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1582" target="_blank">00:26:22.220</a></span> | <span class="t">And this is giving us the probability for any single character to be the first character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1586" target="_blank">00:26:26.420</a></span> | <span class="t">of a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1588" target="_blank">00:26:28.140</a></span> | <span class="t">So now we can try to sample from this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1590" target="_blank">00:26:30.900</a></span> | <span class="t">To sample from these distributions, we're going to use tors dot multinomial, which I've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1594" target="_blank">00:26:34.580</a></span> | <span class="t">pulled up here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1596" target="_blank">00:26:36.400</a></span> | <span class="t">So tors dot multinomial returns samples from the multinomial probability distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1603" target="_blank">00:26:43.460</a></span> | <span class="t">which is a complicated way of saying you give me probabilities and I will give you integers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1608" target="_blank">00:26:48.220</a></span> | <span class="t">which are sampled according to the probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1611" target="_blank">00:26:51.820</a></span> | <span class="t">So this is the signature of the method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1613" target="_blank">00:26:53.420</a></span> | <span class="t">And to make everything deterministic, we're going to use a generator object in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1619" target="_blank">00:26:59.460</a></span> | <span class="t">So this makes everything deterministic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1621" target="_blank">00:27:01.160</a></span> | <span class="t">So when you run this on your computer, you're going to get the exact same results that I'm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1624" target="_blank">00:27:04.940</a></span> | <span class="t">getting here on my computer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1627" target="_blank">00:27:07.440</a></span> | <span class="t">So let me show you how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1632" target="_blank">00:27:12.980</a></span> | <span class="t">Here's the deterministic way of creating a torch generator object, seeding it with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1639" target="_blank">00:27:19.180</a></span> | <span class="t">number that we can agree on, so that seeds a generator, gives us an object g.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1644" target="_blank">00:27:24.940</a></span> | <span class="t">And then we can pass that g to a function that creates here random numbers, tors dot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1652" target="_blank">00:27:32.180</a></span> | <span class="t">rand creates random numbers, three of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1654" target="_blank">00:27:34.940</a></span> | <span class="t">And it's using this generator object as a source of randomness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1660" target="_blank">00:27:40.580</a></span> | <span class="t">So without normalizing it, I can just print.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1666" target="_blank">00:27:46.700</a></span> | <span class="t">This is sort of like numbers between zero and one that are random according to this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1670" target="_blank">00:27:50.500</a></span> | <span class="t">thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1671" target="_blank">00:27:51.500</a></span> | <span class="t">And whenever I run it again, I'm always going to get the same result because I keep using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1675" target="_blank">00:27:55.500</a></span> | <span class="t">the same generator object, which I'm seeding here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1679" target="_blank">00:27:59.000</a></span> | <span class="t">And then if I divide to normalize, I'm going to get a nice probability distribution of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1685" target="_blank">00:28:05.380</a></span> | <span class="t">just three elements.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1687" target="_blank">00:28:07.780</a></span> | <span class="t">And then we can use tors dot multinomial to draw samples from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1691" target="_blank">00:28:11.360</a></span> | <span class="t">So this is what that looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1693" target="_blank">00:28:13.900</a></span> | <span class="t">Tors dot multinomial will take the torch tensor of probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1701" target="_blank">00:28:21.220</a></span> | <span class="t">Then we can ask for a number of samples, let's say 20.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1705" target="_blank">00:28:25.140</a></span> | <span class="t">Replacement equals true means that when we draw an element, we can draw it and then we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1710" target="_blank">00:28:30.980</a></span> | <span class="t">can put it back into the list of eligible indices to draw again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1716" target="_blank">00:28:36.100</a></span> | <span class="t">And we have to specify replacement as true because by default, for some reason, it's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1721" target="_blank">00:28:41.260</a></span> | <span class="t">false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1722" target="_blank">00:28:42.260</a></span> | <span class="t">You know, it's just something to be careful with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1725" target="_blank">00:28:45.980</a></span> | <span class="t">And the generator is passed in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1727" target="_blank">00:28:47.580</a></span> | <span class="t">So we're going to always get deterministic results, the same results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1731" target="_blank">00:28:51.480</a></span> | <span class="t">So if I run these two, we're going to get a bunch of samples from this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1737" target="_blank">00:28:57.260</a></span> | <span class="t">Now you'll notice here that the probability for the first element in this tensor is 60%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1744" target="_blank">00:29:04.700</a></span> | <span class="t">So in these 20 samples, we'd expect 60% of them to be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1750" target="_blank">00:29:10.900</a></span> | <span class="t">We'd expect 30% of them to be one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1754" target="_blank">00:29:14.500</a></span> | <span class="t">And because the element index two has only 10% probability, very few of these samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1760" target="_blank">00:29:20.820</a></span> | <span class="t">should be two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1762" target="_blank">00:29:22.380</a></span> | <span class="t">And indeed, we only have a small number of twos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1765" target="_blank">00:29:25.660</a></span> | <span class="t">We can sample as many as we like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1769" target="_blank">00:29:29.220</a></span> | <span class="t">And the more we sample, the more these numbers should roughly have the distribution here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1776" target="_blank">00:29:36.080</a></span> | <span class="t">So we should have lots of zeros, half as many ones, and we should have three times as few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1785" target="_blank">00:29:45.340</a></span> | <span class="t">ones, and three times as few twos.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1791" target="_blank">00:29:51.940</a></span> | <span class="t">So you see that we have very few twos, we have some ones, and most of them are zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1795" target="_blank">00:29:55.940</a></span> | <span class="t">So that's what torsion multinomial is doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1799" target="_blank">00:29:59.060</a></span> | <span class="t">For us here, we are interested in this row, we've created this p here, and now we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1807" target="_blank">00:30:07.440</a></span> | <span class="t">sample from it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1809" target="_blank">00:30:09.840</a></span> | <span class="t">So if we use the same seed, and then we sample from this distribution, let's just get one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1815" target="_blank">00:30:15.800</a></span> | <span class="t">sample, then we see that the sample is, say, 13.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1822" target="_blank">00:30:22.900</a></span> | <span class="t">So this will be the index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1826" target="_blank">00:30:26.080</a></span> | <span class="t">You see how it's a tensor that wraps 13?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1828" target="_blank">00:30:28.940</a></span> | <span class="t">We again have to use .item to pop out that integer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1833" target="_blank">00:30:33.160</a></span> | <span class="t">And now index would be just the number 13.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1837" target="_blank">00:30:37.640</a></span> | <span class="t">And of course, we can map the I2S of IX to figure out exactly which character we're sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1845" target="_blank">00:30:45.720</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1846" target="_blank">00:30:46.720</a></span> | <span class="t">We're sampling M. So we're saying that the first character is M in our generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1853" target="_blank">00:30:53.380</a></span> | <span class="t">And just looking at the row here, M was drawn, and we can see that M actually starts a large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1858" target="_blank">00:30:58.560</a></span> | <span class="t">number of words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1860" target="_blank">00:31:00.320</a></span> | <span class="t">M started 2,500 words out of 32,000 words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1864" target="_blank">00:31:04.880</a></span> | <span class="t">So almost a bit less than 10% of the words start with M. So this is actually a fairly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1870" target="_blank">00:31:10.120</a></span> | <span class="t">likely character to draw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1875" target="_blank">00:31:15.440</a></span> | <span class="t">So that would be the first character of our word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1877" target="_blank">00:31:17.240</a></span> | <span class="t">And now we can continue to sample more characters, because now we know that M is already sampled.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1884" target="_blank">00:31:24.940</a></span> | <span class="t">So now to draw the next character, we will come back here, and we will look for the row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1890" target="_blank">00:31:30.920</a></span> | <span class="t">that starts with M. So you see M, and we have a row here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1896" target="_blank">00:31:36.900</a></span> | <span class="t">So we see that M. is 516, MA is this many, MB is this many, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1903" target="_blank">00:31:43.920</a></span> | <span class="t">So these are the counts for the next row, and that's the next character that we are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1906" target="_blank">00:31:46.840</a></span> | <span class="t">going to now generate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1908" target="_blank">00:31:48.840</a></span> | <span class="t">So I think we are ready to actually just write out the loop, because I think you're starting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1912" target="_blank">00:31:52.120</a></span> | <span class="t">to get a sense of how this is going to go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1914" target="_blank">00:31:54.720</a></span> | <span class="t">We always begin at index 0, because that's the start token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1922" target="_blank">00:32:02.560</a></span> | <span class="t">And then while true, we're going to grab the row corresponding to index that we're currently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1929" target="_blank">00:32:09.440</a></span> | <span class="t">on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1930" target="_blank">00:32:10.440</a></span> | <span class="t">So that's N array at ix, converted to float is our P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1939" target="_blank">00:32:19.400</a></span> | <span class="t">Then we normalize this P to sum to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1944" target="_blank">00:32:24.040</a></span> | <span class="t">I accidentally ran the infinite loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1948" target="_blank">00:32:28.360</a></span> | <span class="t">We normalize P to sum to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1950" target="_blank">00:32:30.960</a></span> | <span class="t">Then we need this generator object.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1952" target="_blank">00:32:32.960</a></span> | <span class="t">We're going to initialize up here, and we're going to draw a single sample from this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1961" target="_blank">00:32:41.040</a></span> | <span class="t">And then this is going to tell us what index is going to be next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1966" target="_blank">00:32:46.760</a></span> | <span class="t">If the index sampled is 0, then that's now the end token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1972" target="_blank">00:32:52.840</a></span> | <span class="t">So we will break.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1974" target="_blank">00:32:54.640</a></span> | <span class="t">Otherwise, we are going to print s2i of ix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1982" target="_blank">00:33:02.400</a></span> | <span class="t">i2s of ix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1985" target="_blank">00:33:05.680</a></span> | <span class="t">And that's pretty much it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1987" target="_blank">00:33:07.440</a></span> | <span class="t">We're just — this should work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1990" target="_blank">00:33:10.360</a></span> | <span class="t">Okay, more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1992" target="_blank">00:33:12.280</a></span> | <span class="t">So that's the name that we've sampled.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=1994" target="_blank">00:33:14.880</a></span> | <span class="t">We started with M, the next step was O, then R, and then dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2001" target="_blank">00:33:21.680</a></span> | <span class="t">And this dot, we printed here as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2005" target="_blank">00:33:25.040</a></span> | <span class="t">So let's now do this a few times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2010" target="_blank">00:33:30.200</a></span> | <span class="t">So let's actually create an out list here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2017" target="_blank">00:33:37.360</a></span> | <span class="t">And instead of printing, we're going to append.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2019" target="_blank">00:33:39.920</a></span> | <span class="t">So out.append this character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2024" target="_blank">00:33:44.640</a></span> | <span class="t">And then here, let's just print it at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2027" target="_blank">00:33:47.080</a></span> | <span class="t">So let's just join up all the outs, and we're just going to print more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2031" target="_blank">00:33:51.040</a></span> | <span class="t">Now, we're always getting the same result because of the generator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2035" target="_blank">00:33:55.360</a></span> | <span class="t">So if we want to do this a few times, we can go for i in range 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2040" target="_blank">00:34:00.840</a></span> | <span class="t">We can sample 10 names, and we can just do that 10 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2045" target="_blank">00:34:05.960</a></span> | <span class="t">And these are the names that we're getting out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2048" target="_blank">00:34:08.800</a></span> | <span class="t">Let's do 20.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2054" target="_blank">00:34:14.480</a></span> | <span class="t">I'll be honest with you, this doesn't look right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2056" target="_blank">00:34:16.720</a></span> | <span class="t">So I started a few minutes to convince myself that it actually is right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2060" target="_blank">00:34:20.680</a></span> | <span class="t">The reason these samples are so terrible is that bigram language model is actually just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2066" target="_blank">00:34:26.280</a></span> | <span class="t">really terrible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2067" target="_blank">00:34:27.280</a></span> | <span class="t">We can generate a few more here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2069" target="_blank">00:34:29.560</a></span> | <span class="t">And you can see that they're kind of like — they're name-like a little bit, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2073" target="_blank">00:34:33.200</a></span> | <span class="t">Ianu, O'Reilly, et cetera, but they're just totally messed up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2078" target="_blank">00:34:38.920</a></span> | <span class="t">And I mean, the reason that this is so bad — like, we're generating H as a name,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2083" target="_blank">00:34:43.240</a></span> | <span class="t">but you have to think through it from the model's eyes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2086" target="_blank">00:34:46.680</a></span> | <span class="t">It doesn't know that this H is the very first H.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2089" target="_blank">00:34:49.520</a></span> | <span class="t">All it knows is that H was previously, and now how likely is H the last character?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2095" target="_blank">00:34:55.040</a></span> | <span class="t">Well, it's somewhat likely, and so it just makes it last character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2099" target="_blank">00:34:59.240</a></span> | <span class="t">It doesn't know that there were other things before it or there were not other things before</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2103" target="_blank">00:35:03.200</a></span> | <span class="t">it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2104" target="_blank">00:35:04.200</a></span> | <span class="t">And so that's why it's generating all these, like, nonsense names.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2108" target="_blank">00:35:08.480</a></span> | <span class="t">Another way to do this is to convince yourself that this is actually doing something reasonable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2114" target="_blank">00:35:14.600</a></span> | <span class="t">even though it's so terrible, is these little p's here are 27, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2121" target="_blank">00:35:21.200</a></span> | <span class="t">Like 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2123" target="_blank">00:35:23.380</a></span> | <span class="t">So how about if we did something like this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2126" target="_blank">00:35:26.600</a></span> | <span class="t">Instead of p having any structure whatsoever, how about if p was just torch.once(27).</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2137" target="_blank">00:35:37.440</a></span> | <span class="t">By default, this is a float 32, so this is fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2140" target="_blank">00:35:40.480</a></span> | <span class="t">Divide 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2143" target="_blank">00:35:43.120</a></span> | <span class="t">So what I'm doing here is this is the uniform distribution, which will make everything equally</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2148" target="_blank">00:35:48.160</a></span> | <span class="t">likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2150" target="_blank">00:35:50.120</a></span> | <span class="t">And we can sample from that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2152" target="_blank">00:35:52.100</a></span> | <span class="t">So let's see if that does any better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2155" target="_blank">00:35:55.080</a></span> | <span class="t">So this is what you have from a model that is completely untrained, where everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2159" target="_blank">00:35:59.200</a></span> | <span class="t">is equally likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2160" target="_blank">00:36:00.820</a></span> | <span class="t">So it's obviously garbage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2162" target="_blank">00:36:02.560</a></span> | <span class="t">And then if we have a trained model, which is trained on just bigrams, this is what we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2167" target="_blank">00:36:07.680</a></span> | <span class="t">get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2168" target="_blank">00:36:08.680</a></span> | <span class="t">So you can see that it is more name-like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2170" target="_blank">00:36:10.600</a></span> | <span class="t">It is actually working.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2172" target="_blank">00:36:12.000</a></span> | <span class="t">It's just bigram is so terrible, and we have to do better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2176" target="_blank">00:36:16.720</a></span> | <span class="t">Now next, I would like to fix an inefficiency that we have going on here, because what we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2180" target="_blank">00:36:20.760</a></span> | <span class="t">doing here is we're always fetching a row of n from the counts matrix up ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2186" target="_blank">00:36:26.720</a></span> | <span class="t">And then we're always doing the same things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2188" target="_blank">00:36:28.080</a></span> | <span class="t">We're converting to float and we're dividing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2190" target="_blank">00:36:30.080</a></span> | <span class="t">And we're doing this every single iteration of this loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2192" target="_blank">00:36:32.960</a></span> | <span class="t">And we just keep renormalizing these rows over and over again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2195" target="_blank">00:36:35.080</a></span> | <span class="t">And it's extremely inefficient and wasteful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2197" target="_blank">00:36:37.580</a></span> | <span class="t">So what I'd like to do is I'd like to actually prepare a matrix, capital P, that will just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2202" target="_blank">00:36:42.120</a></span> | <span class="t">have the probabilities in it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2204" target="_blank">00:36:44.160</a></span> | <span class="t">So in other words, it's going to be the same as the capital N matrix here of counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2208" target="_blank">00:36:48.180</a></span> | <span class="t">But every single row will have the row of probabilities that is normalized to 1, indicating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2213" target="_blank">00:36:53.440</a></span> | <span class="t">the probability distribution for the next character given the character before it, as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2218" target="_blank">00:36:58.760</a></span> | <span class="t">defined by which row we're in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2221" target="_blank">00:37:01.760</a></span> | <span class="t">So basically what we'd like to do is we'd like to just do it up front here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2225" target="_blank">00:37:05.280</a></span> | <span class="t">And then we would like to just use that row here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2228" target="_blank">00:37:08.360</a></span> | <span class="t">So here, we would like to just do P equals P of IX instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2235" target="_blank">00:37:15.120</a></span> | <span class="t">The other reason I want to do this is not just for efficiency, but also I would like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2238" target="_blank">00:37:18.120</a></span> | <span class="t">us to practice these n-dimensional tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2241" target="_blank">00:37:21.360</a></span> | <span class="t">And I'd like us to practice their manipulation, and especially something that's called broadcasting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2245" target="_blank">00:37:25.120</a></span> | <span class="t">that we'll go into in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2247" target="_blank">00:37:27.160</a></span> | <span class="t">We're actually going to have to become very good at these tensor manipulations, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2251" target="_blank">00:37:31.000</a></span> | <span class="t">if we're going to build out all the way to transformers, we're going to be doing some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2254" target="_blank">00:37:34.040</a></span> | <span class="t">pretty complicated array operations for efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2257" target="_blank">00:37:37.600</a></span> | <span class="t">And we need to really understand that and be very good at it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2262" target="_blank">00:37:42.460</a></span> | <span class="t">So intuitively, what we want to do is we first want to grab the floating point copy of N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2268" target="_blank">00:37:48.440</a></span> | <span class="t">And I'm mimicking the line here, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2271" target="_blank">00:37:51.160</a></span> | <span class="t">And then we want to divide all the rows so that they sum to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2275" target="_blank">00:37:55.880</a></span> | <span class="t">So we'd like to do something like this, P divide P dot sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2280" target="_blank">00:38:00.840</a></span> | <span class="t">But now we have to be careful, because P dot sum actually produces a sum--sorry, P equals</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2289" target="_blank">00:38:09.120</a></span> | <span class="t">N dot float copy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2290" target="_blank">00:38:10.920</a></span> | <span class="t">P dot sum produces a--sums up all of the counts of this entire matrix N and gives us a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2299" target="_blank">00:38:19.060</a></span> | <span class="t">number of just the summation of everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2301" target="_blank">00:38:21.460</a></span> | <span class="t">So that's not the way we want to divide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2303" target="_blank">00:38:23.600</a></span> | <span class="t">We want to simultaneously and in parallel divide all the rows by their respective sums.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2310" target="_blank">00:38:30.800</a></span> | <span class="t">So what we have to do now is we have to go into documentation for torch dot sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2316" target="_blank">00:38:36.100</a></span> | <span class="t">And we can scroll down here to a definition that is relevant to us, which is where we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2320" target="_blank">00:38:40.120</a></span> | <span class="t">don't only provide an input array that we want to sum, but we also provide the dimension</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2325" target="_blank">00:38:45.240</a></span> | <span class="t">along which we want to sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2326" target="_blank">00:38:46.720</a></span> | <span class="t">And in particular, we want to sum up over rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2332" target="_blank">00:38:52.560</a></span> | <span class="t">Now one more argument that I want you to pay attention to here is the keep_dim is false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2338" target="_blank">00:38:58.080</a></span> | <span class="t">If keep_dim is true, then the output tensor is of the same size as input, except of course</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2343" target="_blank">00:39:03.100</a></span> | <span class="t">the dimension along which you summed, which will become just 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2347" target="_blank">00:39:07.600</a></span> | <span class="t">But if you pass in keep_dim as false, then this dimension is squeezed out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2354" target="_blank">00:39:14.580</a></span> | <span class="t">And so torch dot sum not only does the sum and collapses dimension to be of size 1, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2359" target="_blank">00:39:19.120</a></span> | <span class="t">in addition, it does what's called a squeeze, where it squeezes out that dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2364" target="_blank">00:39:24.780</a></span> | <span class="t">So basically what we want here is we instead want to do p dot sum of some axis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2371" target="_blank">00:39:31.040</a></span> | <span class="t">And in particular, notice that p dot shape is 27 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2375" target="_blank">00:39:35.820</a></span> | <span class="t">So when we sum up across axis 0, then we would be taking the 0th dimension and we would be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2380" target="_blank">00:39:40.580</a></span> | <span class="t">summing across it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2382" target="_blank">00:39:42.860</a></span> | <span class="t">So when keep_dim is true, then this thing will not only give us the counts along the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2390" target="_blank">00:39:50.500</a></span> | <span class="t">columns, but notice that basically the shape of this is 1 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2395" target="_blank">00:39:55.640</a></span> | <span class="t">We just get a row vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2397" target="_blank">00:39:57.700</a></span> | <span class="t">And the reason we get a row vector here, again, is because we pass in 0th dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2401" target="_blank">00:40:01.080</a></span> | <span class="t">So this 0th dimension becomes 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2403" target="_blank">00:40:03.180</a></span> | <span class="t">And we've done a sum, and we get a row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2406" target="_blank">00:40:06.060</a></span> | <span class="t">And so basically we've done the sum this way, vertically, and arrived at just a single 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2411" target="_blank">00:40:11.500</a></span> | <span class="t">by 27 vector of counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2415" target="_blank">00:40:15.540</a></span> | <span class="t">What happens when you take out keep_dim is that we just get 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2419" target="_blank">00:40:19.740</a></span> | <span class="t">So it squeezes out that dimension, and we just get a one-dimensional vector of size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2425" target="_blank">00:40:25.340</a></span> | <span class="t">27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2428" target="_blank">00:40:28.780</a></span> | <span class="t">Now we don't actually want 1 by 27 row vector, because that gives us the counts, or the sums,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2435" target="_blank">00:40:35.940</a></span> | <span class="t">across the columns.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2439" target="_blank">00:40:39.660</a></span> | <span class="t">We actually want to sum the other way, along dimension 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2442" target="_blank">00:40:42.940</a></span> | <span class="t">And you'll see that the shape of this is 27 by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2445" target="_blank">00:40:45.900</a></span> | <span class="t">So it's a column vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2447" target="_blank">00:40:47.620</a></span> | <span class="t">It's a 27 by 1 vector of counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2454" target="_blank">00:40:54.100</a></span> | <span class="t">And that's because what's happened here is that we're going horizontally, and this 27</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2457" target="_blank">00:40:57.820</a></span> | <span class="t">by 27 matrix becomes a 27 by 1 array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2463" target="_blank">00:41:03.740</a></span> | <span class="t">And you'll notice, by the way, that the actual numbers of these counts are identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2470" target="_blank">00:41:10.820</a></span> | <span class="t">And that's because this special array of counts here comes from bigram statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2475" target="_blank">00:41:15.020</a></span> | <span class="t">And actually, it just so happens by chance, or because of the way this array is constructed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2480" target="_blank">00:41:20.340</a></span> | <span class="t">that the sums along the columns, or along the rows, horizontally or vertically, is identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2486" target="_blank">00:41:26.380</a></span> | <span class="t">But actually what we want to do in this case is we want to sum across the rows, horizontally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2492" target="_blank">00:41:32.420</a></span> | <span class="t">So what we want here is p.sum of 1 with keep_dim true, 27 by 1 column vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2499" target="_blank">00:41:39.740</a></span> | <span class="t">And now what we want to do is we want to divide by that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2504" target="_blank">00:41:44.900</a></span> | <span class="t">Now we have to be careful here again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2506" target="_blank">00:41:46.400</a></span> | <span class="t">Is it possible to take what's a p.shape you see here, 27 by 27, is it possible to take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2513" target="_blank">00:41:53.500</a></span> | <span class="t">a 27 by 27 array and divide it by what is a 27 by 1 array?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2521" target="_blank">00:42:01.500</a></span> | <span class="t">Is that an operation that you can do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2524" target="_blank">00:42:04.140</a></span> | <span class="t">And whether or not you can perform this operation is determined by what's called broadcasting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2527" target="_blank">00:42:07.420</a></span> | <span class="t">rules.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2528" target="_blank">00:42:08.480</a></span> | <span class="t">So if you just search broadcasting semantics in Torch, you'll notice that there's a special</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2533" target="_blank">00:42:13.100</a></span> | <span class="t">definition for what's called broadcasting, that for whether or not these two arrays can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2540" target="_blank">00:42:20.380</a></span> | <span class="t">be combined in a binary operation like division.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2544" target="_blank">00:42:24.120</a></span> | <span class="t">So the first condition is each tensor has at least one dimension, which is the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2547" target="_blank">00:42:27.500</a></span> | <span class="t">for us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2548" target="_blank">00:42:28.780</a></span> | <span class="t">And then when iterating over the dimension sizes, starting at the trailing dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2552" target="_blank">00:42:32.580</a></span> | <span class="t">the dimension sizes must either be equal, one of them is one, or one of them does not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2556" target="_blank">00:42:36.420</a></span> | <span class="t">exist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2559" target="_blank">00:42:39.100</a></span> | <span class="t">So let's do that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2560" target="_blank">00:42:40.460</a></span> | <span class="t">We need to align the two arrays and their shapes, which is very easy because both of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2565" target="_blank">00:42:45.340</a></span> | <span class="t">these shapes have two elements, so they're aligned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2568" target="_blank">00:42:48.300</a></span> | <span class="t">Then we iterate over from the right and going to the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2572" target="_blank">00:42:52.560</a></span> | <span class="t">Each dimension must be either equal, one of them is a one, or one of them does not exist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2578" target="_blank">00:42:58.140</a></span> | <span class="t">So in this case, they're not equal, but one of them is a one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2580" target="_blank">00:43:00.620</a></span> | <span class="t">So this is fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2582" target="_blank">00:43:02.120</a></span> | <span class="t">And then this dimension, they're both equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2584" target="_blank">00:43:04.220</a></span> | <span class="t">So this is fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2586" target="_blank">00:43:06.100</a></span> | <span class="t">So all the dimensions are fine, and therefore this operation is broadcastable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2592" target="_blank">00:43:12.100</a></span> | <span class="t">So that means that this operation is allowed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2594" target="_blank">00:43:14.740</a></span> | <span class="t">And what is it that these arrays do when you divide 27 by 27 by 27 by one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2600" target="_blank">00:43:20.040</a></span> | <span class="t">What it does is that it takes this dimension one, and it stretches it out, it copies it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2606" target="_blank">00:43:26.100</a></span> | <span class="t">to match 27 here in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2609" target="_blank">00:43:29.220</a></span> | <span class="t">So in our case, it takes this column vector, which is 27 by one, and it copies it 27 times</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2616" target="_blank">00:43:36.800</a></span> | <span class="t">to make these both be 27 by 27 internally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2620" target="_blank">00:43:40.680</a></span> | <span class="t">You can think of it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2621" target="_blank">00:43:41.680</a></span> | <span class="t">And so it copies those counts, and then it does an element-wise division, which is what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2627" target="_blank">00:43:47.900</a></span> | <span class="t">we want because these counts, we want to divide by them on every single one of these columns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2633" target="_blank">00:43:53.060</a></span> | <span class="t">in this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2635" target="_blank">00:43:55.000</a></span> | <span class="t">So this actually, we expect, will normalize every single row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2639" target="_blank">00:43:59.960</a></span> | <span class="t">And we can check that this is true by taking the first row, for example, and taking its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2644" target="_blank">00:44:04.860</a></span> | <span class="t">sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2645" target="_blank">00:44:05.860</a></span> | <span class="t">We expect this to be one because it's now normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2650" target="_blank">00:44:10.580</a></span> | <span class="t">And then we expect this now because if we actually correctly normalize all the rows,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2655" target="_blank">00:44:15.740</a></span> | <span class="t">we expect to get the exact same result here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2657" target="_blank">00:44:17.900</a></span> | <span class="t">So let's run this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2658" target="_blank">00:44:18.900</a></span> | <span class="t">It's the exact same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2661" target="_blank">00:44:21.560</a></span> | <span class="t">So this is correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2663" target="_blank">00:44:23.160</a></span> | <span class="t">So now I would like to scare you a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2665" target="_blank">00:44:25.620</a></span> | <span class="t">You actually have to like, I basically encourage you very strongly to read through broadcasting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2669" target="_blank">00:44:29.320</a></span> | <span class="t">semantics and I encourage you to treat this with respect.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2672" target="_blank">00:44:32.960</a></span> | <span class="t">And it's not something to play fast and loose with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2675" target="_blank">00:44:35.340</a></span> | <span class="t">It's something to really respect, really understand, and look up maybe some tutorials for broadcasting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2679" target="_blank">00:44:39.580</a></span> | <span class="t">and practice it and be careful with it because you can very quickly run into bugs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2684" target="_blank">00:44:44.020</a></span> | <span class="t">Let me show you what I mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2687" target="_blank">00:44:47.380</a></span> | <span class="t">You see how here we have p.sum of one, keep them as true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2690" target="_blank">00:44:50.680</a></span> | <span class="t">The shape of this is 27 by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2693" target="_blank">00:44:53.200</a></span> | <span class="t">Let me take out this line just so we have the n and then we can see the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2698" target="_blank">00:44:58.640</a></span> | <span class="t">We can see that this is all the counts across all the rows and it's 27 by one column vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2706" target="_blank">00:45:06.280</a></span> | <span class="t">right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2707" target="_blank">00:45:07.280</a></span> | <span class="t">Now suppose that I tried to do the following, but I erase keep them as true here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2714" target="_blank">00:45:14.160</a></span> | <span class="t">What does that do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2715" target="_blank">00:45:15.160</a></span> | <span class="t">If keep them is not true, it's false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2717" target="_blank">00:45:17.360</a></span> | <span class="t">And remember, according to documentation, it gets rid of this dimension one, it squeezes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2721" target="_blank">00:45:21.920</a></span> | <span class="t">it out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2723" target="_blank">00:45:23.080</a></span> | <span class="t">So basically we just get all the same counts, the same result, except the shape of it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2727" target="_blank">00:45:27.520</a></span> | <span class="t">not 27 by one, it's just 27, the one that disappears.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2731" target="_blank">00:45:31.900</a></span> | <span class="t">But all the counts are the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2734" target="_blank">00:45:34.340</a></span> | <span class="t">So you'd think that this divide that would work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2740" target="_blank">00:45:40.280</a></span> | <span class="t">First of all, can we even write this and is it even expected to run?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2745" target="_blank">00:45:45.160</a></span> | <span class="t">Is it broadcastable?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2746" target="_blank">00:45:46.160</a></span> | <span class="t">Let's determine if this result is broadcastable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2749" target="_blank">00:45:49.320</a></span> | <span class="t">P dot sum at one is shape is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2753" target="_blank">00:45:53.120</a></span> | <span class="t">This is 27 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2754" target="_blank">00:45:54.600</a></span> | <span class="t">So 27 by 27 broadcasting into 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2760" target="_blank">00:46:00.460</a></span> | <span class="t">So now rules of broadcasting number one, align all the dimensions on the right, done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2766" target="_blank">00:46:06.440</a></span> | <span class="t">Now iteration over all the dimensions starting from the right, going to the left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2770" target="_blank">00:46:10.360</a></span> | <span class="t">All the dimensions must either be equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2773" target="_blank">00:46:13.200</a></span> | <span class="t">One of them must be one or one of them does not exist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2776" target="_blank">00:46:16.120</a></span> | <span class="t">So here they are all equal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2777" target="_blank">00:46:17.960</a></span> | <span class="t">Here the dimension does not exist.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2780" target="_blank">00:46:20.160</a></span> | <span class="t">So internally what broadcasting will do is it will create a one here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2784" target="_blank">00:46:24.560</a></span> | <span class="t">And then we see that one of them is a one and this will get copied and this will run,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2790" target="_blank">00:46:30.400</a></span> | <span class="t">this will broadcast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2791" target="_blank">00:46:31.960</a></span> | <span class="t">Okay, so you'd expect this to work because we are, this broadcasts and this, we can divide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2802" target="_blank">00:46:42.880</a></span> | <span class="t">this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2803" target="_blank">00:46:43.880</a></span> | <span class="t">Now if I run this, you'd expect it to work, but it doesn't.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2808" target="_blank">00:46:48.280</a></span> | <span class="t">You actually get garbage.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2809" target="_blank">00:46:49.280</a></span> | <span class="t">You get a wrong result because this is actually a bug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2812" target="_blank">00:46:52.640</a></span> | <span class="t">This keep dim equals true makes it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2820" target="_blank">00:47:00.840</a></span> | <span class="t">This is a bug.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2823" target="_blank">00:47:03.040</a></span> | <span class="t">In both cases, we are doing the correct counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2826" target="_blank">00:47:06.440</a></span> | <span class="t">We are summing up across the rows, but keep dim is saving us and making it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2831" target="_blank">00:47:11.720</a></span> | <span class="t">So in this case, I'd like to encourage you to potentially like pause this video at this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2835" target="_blank">00:47:15.320</a></span> | <span class="t">point and try to think about why this is buggy and why the keep dim was necessary here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2841" target="_blank">00:47:21.320</a></span> | <span class="t">Okay, so the reason to do for this is I'm trying to hint it here when I was sort of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2847" target="_blank">00:47:27.360</a></span> | <span class="t">giving you a bit of a hint on how this works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2849" target="_blank">00:47:29.720</a></span> | <span class="t">This 27 vector internally inside the broadcasting, this becomes a one by 27 and one by 27 is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2857" target="_blank">00:47:37.640</a></span> | <span class="t">a row vector, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2859" target="_blank">00:47:39.840</a></span> | <span class="t">And now we are dividing 27 by 27 by one by 27 and torch will replicate this dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2866" target="_blank">00:47:46.100</a></span> | <span class="t">So basically it will take, it will take this row vector and it will copy it vertically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2874" target="_blank">00:47:54.160</a></span> | <span class="t">now 27 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2876" target="_blank">00:47:56.380</a></span> | <span class="t">So the 27 by 27 lines exactly and element wise divides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2880" target="_blank">00:48:00.520</a></span> | <span class="t">And so basically what's happening here is we're actually normalizing the columns instead</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2886" target="_blank">00:48:06.420</a></span> | <span class="t">of normalizing the rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2889" target="_blank">00:48:09.640</a></span> | <span class="t">So you can check that what's happening here is that P at zero, which is the first row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2894" target="_blank">00:48:14.120</a></span> | <span class="t">of P dot sum is not one, it's seven.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2898" target="_blank">00:48:18.800</a></span> | <span class="t">It is the first column as an example that sums to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2903" target="_blank">00:48:23.840</a></span> | <span class="t">So to summarize, where does the issue come from?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2906" target="_blank">00:48:26.640</a></span> | <span class="t">The issue comes from the silent adding of a dimension here, because in broadcasting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2910" target="_blank">00:48:30.840</a></span> | <span class="t">rules, you align on the right and go from right to left.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2913" target="_blank">00:48:33.960</a></span> | <span class="t">And if dimension doesn't exist, you create it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2916" target="_blank">00:48:36.420</a></span> | <span class="t">So that's where the problem happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2918" target="_blank">00:48:38.080</a></span> | <span class="t">We still did the counts correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2919" target="_blank">00:48:39.520</a></span> | <span class="t">We did the counts across the rows and we got the counts on the right here as a column vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2925" target="_blank">00:48:45.800</a></span> | <span class="t">But because the keyptons was true, this dimension was discarded and now we just have a vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2930" target="_blank">00:48:50.360</a></span> | <span class="t">of 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2931" target="_blank">00:48:51.680</a></span> | <span class="t">And because of broadcasting the way it works, this vector of 27 suddenly becomes a row vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2937" target="_blank">00:48:57.240</a></span> | <span class="t">And then this row vector gets replicated vertically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2940" target="_blank">00:49:00.120</a></span> | <span class="t">And that every single point we are dividing by the count in the opposite direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2947" target="_blank">00:49:07.520</a></span> | <span class="t">So this thing just doesn't work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2951" target="_blank">00:49:11.640</a></span> | <span class="t">This needs to be keyptons equals true in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2954" target="_blank">00:49:14.380</a></span> | <span class="t">So then we have that P at zero is normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2960" target="_blank">00:49:20.120</a></span> | <span class="t">And conversely, the first column you'd expect to potentially not be normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2964" target="_blank">00:49:24.840</a></span> | <span class="t">And this is what makes it work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2967" target="_blank">00:49:27.840</a></span> | <span class="t">So pretty subtle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2969" target="_blank">00:49:29.600</a></span> | <span class="t">And hopefully this helps to scare you, that you should have respect for broadcasting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2974" target="_blank">00:49:34.400</a></span> | <span class="t">be careful, check your work, and understand how it works under the hood and make sure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2979" target="_blank">00:49:39.200</a></span> | <span class="t">that it's broadcasting in the direction that you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2981" target="_blank">00:49:41.160</a></span> | <span class="t">Otherwise, you're going to introduce very subtle bugs, very hard to find bugs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2985" target="_blank">00:49:45.360</a></span> | <span class="t">And just be careful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2986" target="_blank">00:49:46.840</a></span> | <span class="t">One more note on efficiency.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2988" target="_blank">00:49:48.440</a></span> | <span class="t">We don't want to be doing this here because this creates a completely new tensor that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2993" target="_blank">00:49:53.040</a></span> | <span class="t">we store into P. We prefer to use in-place operations if possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=2998" target="_blank">00:49:58.040</a></span> | <span class="t">So this would be an in-place operation, has the potential to be faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3001" target="_blank">00:50:01.960</a></span> | <span class="t">It doesn't create new memory under the hood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3004" target="_blank">00:50:04.680</a></span> | <span class="t">And then let's erase this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3006" target="_blank">00:50:06.420</a></span> | <span class="t">We don't need it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3008" target="_blank">00:50:08.280</a></span> | <span class="t">And let's also just do fewer, just so I'm not wasting space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3014" target="_blank">00:50:14.280</a></span> | <span class="t">Okay, so we're actually in a pretty good spot now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3017" target="_blank">00:50:17.160</a></span> | <span class="t">We trained a bigram language model, and we trained it really just by counting how frequently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3022" target="_blank">00:50:22.900</a></span> | <span class="t">any pairing occurs and then normalizing so that we get a nice property distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3028" target="_blank">00:50:28.120</a></span> | <span class="t">So really these elements of this array P are really the parameters of our bigram language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3033" target="_blank">00:50:33.080</a></span> | <span class="t">model, giving us and summarizing the statistics of these bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3037" target="_blank">00:50:37.140</a></span> | <span class="t">So we trained the model, and then we know how to sample from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3040" target="_blank">00:50:40.320</a></span> | <span class="t">We just iteratively sample the next character and feed it in each time and get the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3045" target="_blank">00:50:45.680</a></span> | <span class="t">character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3046" target="_blank">00:50:46.680</a></span> | <span class="t">Now, what I'd like to do is I'd like to somehow evaluate the quality of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3051" target="_blank">00:50:51.320</a></span> | <span class="t">We'd like to somehow summarize the quality of this model into a single number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3055" target="_blank">00:50:55.400</a></span> | <span class="t">How good is it at predicting the training set?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3059" target="_blank">00:50:59.280</a></span> | <span class="t">And as an example, so in the training set, we can evaluate now the training loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3064" target="_blank">00:51:04.480</a></span> | <span class="t">And this training loss is telling us about sort of the quality of this model in a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3068" target="_blank">00:51:08.520</a></span> | <span class="t">number, just like we saw in micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3072" target="_blank">00:51:12.120</a></span> | <span class="t">So let's try to think through the quality of the model and how we would evaluate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3076" target="_blank">00:51:16.160</a></span> | <span class="t">Basically, what we're going to do is we're going to copy paste this code that we previously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3081" target="_blank">00:51:21.440</a></span> | <span class="t">used for counting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3084" target="_blank">00:51:24.440</a></span> | <span class="t">And let me just print the bigrams first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3086" target="_blank">00:51:26.160</a></span> | <span class="t">We're going to use f strings, and I'm going to print character one followed by character</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3090" target="_blank">00:51:30.480</a></span> | <span class="t">two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3091" target="_blank">00:51:31.480</a></span> | <span class="t">These are the bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3092" target="_blank">00:51:32.480</a></span> | <span class="t">And then I don't want to do it for all the words, just do the first three words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3096" target="_blank">00:51:36.240</a></span> | <span class="t">So here we have Emma, Olivia, and Ava bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3100" target="_blank">00:51:40.360</a></span> | <span class="t">Now what we'd like to do is we'd like to basically look at the probability that the model assigns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3106" target="_blank">00:51:46.120</a></span> | <span class="t">to every one of these bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3108" target="_blank">00:51:48.400</a></span> | <span class="t">So in other words, we can look at the probability, which is summarized in the matrix P of IX1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3114" target="_blank">00:51:54.160</a></span> | <span class="t">and then we can print it here as probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3120" target="_blank">00:52:00.920</a></span> | <span class="t">And because these probabilities are way too large, let me percent or colon .4f to truncate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3127" target="_blank">00:52:07.560</a></span> | <span class="t">it a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3129" target="_blank">00:52:09.400</a></span> | <span class="t">So what do we have here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3130" target="_blank">00:52:10.400</a></span> | <span class="t">We're looking at the probabilities that the model assigns to every one of these bigrams</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3133" target="_blank">00:52:13.840</a></span> | <span class="t">in the dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3135" target="_blank">00:52:15.440</a></span> | <span class="t">And so we can see some of them are 4%, 3%, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3138" target="_blank">00:52:18.880</a></span> | <span class="t">Just to have a measuring stick in our mind, by the way, we have 27 possible characters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3143" target="_blank">00:52:23.880</a></span> | <span class="t">or tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3144" target="_blank">00:52:24.960</a></span> | <span class="t">And if everything was equally likely, then you'd expect all these probabilities to be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3149" target="_blank">00:52:29.160</a></span> | <span class="t">4% roughly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3152" target="_blank">00:52:32.640</a></span> | <span class="t">So anything above 4% means that we've learned something useful from these bigram statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3157" target="_blank">00:52:37.240</a></span> | <span class="t">And you see that roughly some of these are 4%, but some of them are as high as 40%, 35%,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3163" target="_blank">00:52:43.360</a></span> | <span class="t">and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3164" target="_blank">00:52:44.360</a></span> | <span class="t">So you see that the model actually assigned a pretty high probability to whatever's in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3167" target="_blank">00:52:47.440</a></span> | <span class="t">the training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3168" target="_blank">00:52:48.480</a></span> | <span class="t">And so that's a good thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3170" target="_blank">00:52:50.960</a></span> | <span class="t">Especially if you have a very good model, you'd expect that these probabilities should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3173" target="_blank">00:52:53.840</a></span> | <span class="t">be near 1, because that means that your model is correctly predicting what's going to come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3178" target="_blank">00:52:58.120</a></span> | <span class="t">next, especially on the training set where you trained your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3183" target="_blank">00:53:03.000</a></span> | <span class="t">So now we'd like to think about how can we summarize these probabilities into a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3187" target="_blank">00:53:07.900</a></span> | <span class="t">number that measures the quality of this model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3191" target="_blank">00:53:11.920</a></span> | <span class="t">Now when you look at the literature into maximum likelihood estimation and statistical modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3196" target="_blank">00:53:16.160</a></span> | <span class="t">and so on, you'll see that what's typically used here is something called the likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3201" target="_blank">00:53:21.720</a></span> | <span class="t">And the likelihood is the product of all of these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3206" target="_blank">00:53:26.120</a></span> | <span class="t">And so the product of all of these probabilities is the likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3209" target="_blank">00:53:29.460</a></span> | <span class="t">And it's really telling us about the probability of the entire data set assigned by the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3216" target="_blank">00:53:36.640</a></span> | <span class="t">that we've trained.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3217" target="_blank">00:53:37.880</a></span> | <span class="t">And that is a measure of quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3219" target="_blank">00:53:39.620</a></span> | <span class="t">So the product of these should be as high as possible when you are training the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3224" target="_blank">00:53:44.560</a></span> | <span class="t">and when you have a good model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3226" target="_blank">00:53:46.520</a></span> | <span class="t">Your product of these probabilities should be very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3230" target="_blank">00:53:50.560</a></span> | <span class="t">Now because the product of these probabilities is an unwieldy thing to work with, you can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3234" target="_blank">00:53:54.560</a></span> | <span class="t">see that all of them are between 0 and 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3236" target="_blank">00:53:56.480</a></span> | <span class="t">So your product of these probabilities will be a very tiny number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3241" target="_blank">00:54:01.100</a></span> | <span class="t">So for convenience, what people work with usually is not the likelihood, but they work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3245" target="_blank">00:54:05.240</a></span> | <span class="t">with what's called the log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3248" target="_blank">00:54:08.080</a></span> | <span class="t">So the product of these is the likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3251" target="_blank">00:54:11.040</a></span> | <span class="t">To get the log likelihood, we just have to take the log of the probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3255" target="_blank">00:54:15.240</a></span> | <span class="t">And so the log of the probability here, I have the log of x from 0 to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3259" target="_blank">00:54:19.900</a></span> | <span class="t">The log is a, you see here, monotonic transformation of the probability, where if you pass in 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3267" target="_blank">00:54:27.400</a></span> | <span class="t">you get 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3269" target="_blank">00:54:29.020</a></span> | <span class="t">So probability 1 gets you log probability of 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3272" target="_blank">00:54:32.440</a></span> | <span class="t">And then as you go lower and lower probability, the log will grow more and more negative until</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3277" target="_blank">00:54:37.040</a></span> | <span class="t">all the way to negative infinity at 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3282" target="_blank">00:54:42.080</a></span> | <span class="t">So here we have a log prob, which is really just a torsion log of probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3287" target="_blank">00:54:47.080</a></span> | <span class="t">Let's print it out to get a sense of what that looks like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3290" target="_blank">00:54:50.240</a></span> | <span class="t">Log prob also 0.4f.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3296" target="_blank">00:54:56.800</a></span> | <span class="t">So as you can see, when we plug in numbers that are very close, some of our higher numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3301" target="_blank">00:55:01.520</a></span> | <span class="t">we get closer and closer to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3303" target="_blank">00:55:03.660</a></span> | <span class="t">And then if we plug in very bad probabilities, we get more and more negative number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3308" target="_blank">00:55:08.120</a></span> | <span class="t">That's bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3309" target="_blank">00:55:09.760</a></span> | <span class="t">So and the reason we work with this is for large extent convenience, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3315" target="_blank">00:55:15.560</a></span> | <span class="t">Because we have mathematically that if you have some product, a times b times c, of all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3319" target="_blank">00:55:19.360</a></span> | <span class="t">these probabilities, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3321" target="_blank">00:55:21.360</a></span> | <span class="t">The likelihood is the product of all these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3325" target="_blank">00:55:25.620</a></span> | <span class="t">And the log of these is just log of a plus log of b plus log of c.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3335" target="_blank">00:55:35.260</a></span> | <span class="t">If you remember your logs from your high school or undergrad and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3339" target="_blank">00:55:39.960</a></span> | <span class="t">So we have that basically, the likelihood is the product of probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3343" target="_blank">00:55:43.420</a></span> | <span class="t">The log likelihood is just the sum of the logs of the individual probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3348" target="_blank">00:55:48.980</a></span> | <span class="t">So log likelihood starts at 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3354" target="_blank">00:55:54.860</a></span> | <span class="t">And then log likelihood here, we can just accumulate simply.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3360" target="_blank">00:56:00.580</a></span> | <span class="t">And then the end, we can print this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3365" target="_blank">00:56:05.660</a></span> | <span class="t">Print the log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3367" target="_blank">00:56:07.660</a></span> | <span class="t">F strings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3370" target="_blank">00:56:10.180</a></span> | <span class="t">Maybe you're familiar with this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3374" target="_blank">00:56:14.100</a></span> | <span class="t">So log likelihood is negative 38.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3378" target="_blank">00:56:18.540</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3381" target="_blank">00:56:21.420</a></span> | <span class="t">Now we actually want -- so how high can log likelihood get?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3387" target="_blank">00:56:27.900</a></span> | <span class="t">It can go to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3390" target="_blank">00:56:30.020</a></span> | <span class="t">So when all the probabilities are 1, log likelihood will be 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3393" target="_blank">00:56:33.020</a></span> | <span class="t">And then when all the probabilities are lower, this will grow more and more negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3397" target="_blank">00:56:37.580</a></span> | <span class="t">Now we don't actually like this because what we'd like is a loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3401" target="_blank">00:56:41.340</a></span> | <span class="t">And a loss function has the semantics that low is good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3406" target="_blank">00:56:46.500</a></span> | <span class="t">Because we're trying to minimize the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3408" target="_blank">00:56:48.340</a></span> | <span class="t">So we actually need to invert this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3410" target="_blank">00:56:50.540</a></span> | <span class="t">And that's what gives us something called the negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3416" target="_blank">00:56:56.300</a></span> | <span class="t">Negative log likelihood is just negative of the log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3424" target="_blank">00:57:04.060</a></span> | <span class="t">These are F strings, by the way, if you'd like to look this up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3426" target="_blank">00:57:06.780</a></span> | <span class="t">Negative log likelihood equals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3429" target="_blank">00:57:09.540</a></span> | <span class="t">So negative log likelihood now is just negative of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3432" target="_blank">00:57:12.240</a></span> | <span class="t">And so the negative log likelihood is a very nice loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3436" target="_blank">00:57:16.140</a></span> | <span class="t">Because the lowest it can get is 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3439" target="_blank">00:57:19.980</a></span> | <span class="t">And the higher it is, the worse off the predictions are that you're making.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3444" target="_blank">00:57:24.900</a></span> | <span class="t">And then one more modification to this that sometimes people do is that for convenience,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3449" target="_blank">00:57:29.300</a></span> | <span class="t">they actually like to normalize by -- they like to make it an average instead of a sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3454" target="_blank">00:57:34.620</a></span> | <span class="t">And so here, let's just keep some counts as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3459" target="_blank">00:57:39.480</a></span> | <span class="t">So n plus equals 1 starts at 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3463" target="_blank">00:57:43.020</a></span> | <span class="t">And then here, we can have sort of like a normalized log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3470" target="_blank">00:57:50.660</a></span> | <span class="t">If we just normalize it by the count, then we will sort of get the average log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3475" target="_blank">00:57:55.940</a></span> | <span class="t">So this would be usually our loss function here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3479" target="_blank">00:57:59.980</a></span> | <span class="t">This is what we would use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3482" target="_blank">00:58:02.480</a></span> | <span class="t">So our loss function for the training set assigned by the model is 2.4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3486" target="_blank">00:58:06.620</a></span> | <span class="t">That's the quality of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3488" target="_blank">00:58:08.740</a></span> | <span class="t">And the lower it is, the better off we are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3490" target="_blank">00:58:10.700</a></span> | <span class="t">And the higher it is, the worse off we are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3493" target="_blank">00:58:13.740</a></span> | <span class="t">And the job of our training is to find the parameters that minimize the negative log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3500" target="_blank">00:58:20.140</a></span> | <span class="t">likelihood loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3503" target="_blank">00:58:23.140</a></span> | <span class="t">And that would be like a high-quality model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3505" target="_blank">00:58:25.140</a></span> | <span class="t">Okay, so to summarize, I actually wrote it out here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3508" target="_blank">00:58:28.240</a></span> | <span class="t">So our goal is to maximize likelihood, which is the product of all the probabilities assigned</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3514" target="_blank">00:58:34.420</a></span> | <span class="t">by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3515" target="_blank">00:58:35.420</a></span> | <span class="t">And we want to maximize this likelihood with respect to the model parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3519" target="_blank">00:58:39.420</a></span> | <span class="t">And in our case, the model parameters here are defined in the table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3523" target="_blank">00:58:43.660</a></span> | <span class="t">These numbers, the probabilities, are the model parameters sort of in our bigram language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3528" target="_blank">00:58:48.580</a></span> | <span class="t">model so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3529" target="_blank">00:58:49.580</a></span> | <span class="t">But you have to keep in mind that here we are storing everything in a table format,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3533" target="_blank">00:58:53.820</a></span> | <span class="t">the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3534" target="_blank">00:58:54.820</a></span> | <span class="t">But what's coming up as a brief preview is that these numbers will not be kept explicitly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3540" target="_blank">00:59:00.340</a></span> | <span class="t">but these numbers will be calculated by a neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3543" target="_blank">00:59:03.340</a></span> | <span class="t">So that's coming up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3544" target="_blank">00:59:04.340</a></span> | <span class="t">And we want to change and tune the parameters of these neural networks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3548" target="_blank">00:59:08.260</a></span> | <span class="t">We want to change these parameters to maximize the likelihood, the product of the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3553" target="_blank">00:59:13.500</a></span> | <span class="t">Now maximizing the likelihood is equivalent to maximizing the log likelihood because log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3557" target="_blank">00:59:17.900</a></span> | <span class="t">is a monotonic function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3560" target="_blank">00:59:20.180</a></span> | <span class="t">Here's the graph of log.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3562" target="_blank">00:59:22.260</a></span> | <span class="t">And basically all it is doing is it's just scaling your, you can look at it as just a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3567" target="_blank">00:59:27.520</a></span> | <span class="t">scaling of the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3569" target="_blank">00:59:29.700</a></span> | <span class="t">And so the optimization problem here and here are actually equivalent because this is just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3575" target="_blank">00:59:35.020</a></span> | <span class="t">scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3576" target="_blank">00:59:36.020</a></span> | <span class="t">You can look at it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3577" target="_blank">00:59:37.240</a></span> | <span class="t">And so these are two identical optimization problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3582" target="_blank">00:59:42.460</a></span> | <span class="t">Maximizing the log likelihood is equivalent to minimizing the negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3586" target="_blank">00:59:46.420</a></span> | <span class="t">And then in practice, people actually minimize the average negative log likelihood to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3590" target="_blank">00:59:50.540</a></span> | <span class="t">numbers like 2.4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3593" target="_blank">00:59:53.180</a></span> | <span class="t">And then this summarizes the quality of your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3596" target="_blank">00:59:56.460</a></span> | <span class="t">And we'd like to minimize it and make it as small as possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3599" target="_blank">00:59:59.900</a></span> | <span class="t">And the lowest it can get is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3602" target="_blank">01:00:02.600</a></span> | <span class="t">And the lower it is, the better off your model is because it's assigning high probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3608" target="_blank">01:00:08.600</a></span> | <span class="t">to your data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3609" target="_blank">01:00:09.600</a></span> | <span class="t">Now let's estimate the probability over the entire training set just to make sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3612" target="_blank">01:00:12.680</a></span> | <span class="t">we get something around 2.4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3615" target="_blank">01:00:15.160</a></span> | <span class="t">Let's run this over the entire, oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3617" target="_blank">01:00:17.580</a></span> | <span class="t">Let's take out the print statement as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3619" target="_blank">01:00:19.600</a></span> | <span class="t">Okay, 2.45 over the entire training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3624" target="_blank">01:00:24.700</a></span> | <span class="t">Now what I'd like to show you is that you can actually evaluate the probability for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3627" target="_blank">01:00:27.260</a></span> | <span class="t">any word that you want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3628" target="_blank">01:00:28.520</a></span> | <span class="t">So for example, if we just test a single word, Andre, and bring back the print statement,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3636" target="_blank">01:00:36.040</a></span> | <span class="t">then you see that Andre is actually kind of like an unlikely word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3639" target="_blank">01:00:39.000</a></span> | <span class="t">Like on average, we take three log probability to represent it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3644" target="_blank">01:00:44.760</a></span> | <span class="t">And roughly that's because EJ apparently is very uncommon as an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3650" target="_blank">01:00:50.200</a></span> | <span class="t">Now think through this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3652" target="_blank">01:00:52.440</a></span> | <span class="t">When I take Andre and I append Q and I test the probability of it, Andre Q, we actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3660" target="_blank">01:01:00.680</a></span> | <span class="t">get infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3663" target="_blank">01:01:03.240</a></span> | <span class="t">And that's because JQ has a 0% probability according to our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3667" target="_blank">01:01:07.840</a></span> | <span class="t">So the log likelihood, so the log of zero will be negative infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3672" target="_blank">01:01:12.360</a></span> | <span class="t">We get infinite loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3674" target="_blank">01:01:14.760</a></span> | <span class="t">So this is kind of undesirable, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3675" target="_blank">01:01:15.980</a></span> | <span class="t">Because we plugged in a string that could be like a somewhat reasonable name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3679" target="_blank">01:01:19.400</a></span> | <span class="t">But basically what this is saying is that this model is exactly 0% likely to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3684" target="_blank">01:01:24.600</a></span> | <span class="t">this name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3686" target="_blank">01:01:26.760</a></span> | <span class="t">And our loss is infinity on this example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3690" target="_blank">01:01:30.000</a></span> | <span class="t">And really what the reason for that is that J is followed by Q zero times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3696" target="_blank">01:01:36.560</a></span> | <span class="t">Where's Q?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3697" target="_blank">01:01:37.960</a></span> | <span class="t">JQ is zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3699" target="_blank">01:01:39.520</a></span> | <span class="t">And so JQ is 0% likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3702" target="_blank">01:01:42.440</a></span> | <span class="t">So it's actually kind of gross and people don't like this too much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3705" target="_blank">01:01:45.420</a></span> | <span class="t">To fix this, there's a very simple fix that people like to do to sort of like smooth out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3709" target="_blank">01:01:49.500</a></span> | <span class="t">your model a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3710" target="_blank">01:01:50.500</a></span> | <span class="t">And it's called model smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3712" target="_blank">01:01:52.340</a></span> | <span class="t">And roughly what's happening is that we will add some fake counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3716" target="_blank">01:01:56.440</a></span> | <span class="t">So imagine adding a count of one to everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3721" target="_blank">01:02:01.160</a></span> | <span class="t">So we add a count of one like this, and then we recalculate the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3727" target="_blank">01:02:07.980</a></span> | <span class="t">And that's model smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3728" target="_blank">01:02:08.980</a></span> | <span class="t">And you can add as much as you like.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3730" target="_blank">01:02:10.260</a></span> | <span class="t">You can add five and that will give you a smoother model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3733" target="_blank">01:02:13.140</a></span> | <span class="t">And the more you add here, the more uniform model you're going to have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3737" target="_blank">01:02:17.940</a></span> | <span class="t">And the less you add, the more peaked model you're going to have, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3742" target="_blank">01:02:22.500</a></span> | <span class="t">So one is like a pretty decent count to add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3745" target="_blank">01:02:25.940</a></span> | <span class="t">And that will ensure that there will be no zeros in our probability matrix P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3751" target="_blank">01:02:31.120</a></span> | <span class="t">And so this will of course change the generations a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3753" target="_blank">01:02:33.800</a></span> | <span class="t">In this case, it didn't, but in principle it could.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3756" target="_blank">01:02:36.780</a></span> | <span class="t">But what that's going to do now is that nothing will be infinity unlikely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3761" target="_blank">01:02:41.340</a></span> | <span class="t">So now our model will predict some other probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3764" target="_blank">01:02:44.940</a></span> | <span class="t">And we see that JQ now has a very small probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3767" target="_blank">01:02:47.860</a></span> | <span class="t">So the model still finds it very surprising that this was a word or a bigram, but we don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3772" target="_blank">01:02:52.020</a></span> | <span class="t">get negative infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3773" target="_blank">01:02:53.020</a></span> | <span class="t">So it's kind of like a nice fix that people like to apply sometimes, and it's called model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3776" target="_blank">01:02:56.340</a></span> | <span class="t">smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3777" target="_blank">01:02:57.340</a></span> | <span class="t">Okay, so we've now trained a respectable bigram character level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3781" target="_blank">01:03:01.660</a></span> | <span class="t">And we saw that we both sort of trained the model by looking at the counts of all the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3786" target="_blank">01:03:06.780</a></span> | <span class="t">bigrams and normalizing the rows to get probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3791" target="_blank">01:03:11.660</a></span> | <span class="t">We saw that we can also then use those parameters of this model to perform sampling of new words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3799" target="_blank">01:03:19.700</a></span> | <span class="t">So we sample new names according to those distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3802" target="_blank">01:03:22.420</a></span> | <span class="t">And we also saw that we can evaluate the quality of this model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3805" target="_blank">01:03:25.640</a></span> | <span class="t">And the quality of this model is summarized in a single number, which is the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3808" target="_blank">01:03:28.820</a></span> | <span class="t">log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3810" target="_blank">01:03:30.140</a></span> | <span class="t">And the lower this number is, the better the model is, because it is giving high probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3815" target="_blank">01:03:35.620</a></span> | <span class="t">to the actual next characters in all the bigrams in our training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3820" target="_blank">01:03:40.340</a></span> | <span class="t">So that's all well and good, but we've arrived at this model explicitly by doing something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3824" target="_blank">01:03:44.940</a></span> | <span class="t">that felt sensible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3826" target="_blank">01:03:46.260</a></span> | <span class="t">We were just performing counts, and then we were normalizing those counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3831" target="_blank">01:03:51.180</a></span> | <span class="t">Now what I would like to do is I would like to take an alternative approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3834" target="_blank">01:03:54.260</a></span> | <span class="t">We will end up in a very, very similar position, but the approach will look very different,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3838" target="_blank">01:03:58.460</a></span> | <span class="t">because I would like to cast the problem of bigram character level language modeling into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3842" target="_blank">01:04:02.460</a></span> | <span class="t">the neural network framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3844" target="_blank">01:04:04.380</a></span> | <span class="t">And in the neural network framework, we're going to approach things slightly differently,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3848" target="_blank">01:04:08.300</a></span> | <span class="t">but again, end up in a very similar spot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3850" target="_blank">01:04:10.300</a></span> | <span class="t">I'll go into that later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3852" target="_blank">01:04:12.100</a></span> | <span class="t">Now our neural network is going to be still a bigram character level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3857" target="_blank">01:04:17.420</a></span> | <span class="t">So it receives a single character as an input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3860" target="_blank">01:04:20.540</a></span> | <span class="t">Then there's neural network with some weights or some parameters w.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3864" target="_blank">01:04:24.220</a></span> | <span class="t">And it's going to output the probability distribution over the next character in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3869" target="_blank">01:04:29.300</a></span> | <span class="t">And it's going to make guesses as to what is likely to follow this character that was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3873" target="_blank">01:04:33.740</a></span> | <span class="t">input to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3876" target="_blank">01:04:36.140</a></span> | <span class="t">And then in addition to that, we're going to be able to evaluate any setting of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3879" target="_blank">01:04:39.900</a></span> | <span class="t">parameters of the neural net, because we have the loss function, the negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3885" target="_blank">01:04:45.160</a></span> | <span class="t">So we're going to take a look at this probability distributions, and we're going to use the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3888" target="_blank">01:04:48.260</a></span> | <span class="t">labels, which are basically just the identity of the next character in that bigram, the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3893" target="_blank">01:04:53.460</a></span> | <span class="t">second character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3894" target="_blank">01:04:54.880</a></span> | <span class="t">So knowing what second character actually comes next in the bigram allows us to then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3899" target="_blank">01:04:59.140</a></span> | <span class="t">look at how high of a probability the model assigns to that character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3904" target="_blank">01:05:04.100</a></span> | <span class="t">And then we of course want the probability to be very high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3907" target="_blank">01:05:07.240</a></span> | <span class="t">And that is another way of saying that the loss is low.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3910" target="_blank">01:05:10.960</a></span> | <span class="t">So we're going to use gradient based optimization then to tune the parameters of this network,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3915" target="_blank">01:05:15.660</a></span> | <span class="t">because we have the loss function and we're going to minimize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3918" target="_blank">01:05:18.600</a></span> | <span class="t">So we're going to tune the weights so that the neural net is correctly predicting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3922" target="_blank">01:05:22.220</a></span> | <span class="t">probabilities for the next character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3924" target="_blank">01:05:24.540</a></span> | <span class="t">So let's get started.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3925" target="_blank">01:05:25.700</a></span> | <span class="t">The first thing I want to do is I want to compile the training set of this neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3929" target="_blank">01:05:29.660</a></span> | <span class="t">So create the training set of all the bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3937" target="_blank">01:05:37.940</a></span> | <span class="t">And here I'm going to copy/paste this code, because this code iterates over all the bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3947" target="_blank">01:05:47.580</a></span> | <span class="t">So here we start with the words, we iterate over all the bigrams, and previously, as you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3951" target="_blank">01:05:51.380</a></span> | <span class="t">recall, we did the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3953" target="_blank">01:05:53.220</a></span> | <span class="t">But now we're not going to do counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3954" target="_blank">01:05:54.640</a></span> | <span class="t">We're just creating a training set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3957" target="_blank">01:05:57.060</a></span> | <span class="t">Now this training set will be made up of two lists.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3962" target="_blank">01:06:02.220</a></span> | <span class="t">We have the inputs and the targets, the labels.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3969" target="_blank">01:06:09.720</a></span> | <span class="t">And these bigrams will denote x, y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3971" target="_blank">01:06:11.380</a></span> | <span class="t">Those are the characters, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3973" target="_blank">01:06:13.340</a></span> | <span class="t">And so we're given the first character of the bigram, and then we're trying to predict</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3976" target="_blank">01:06:16.420</a></span> | <span class="t">the next one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3977" target="_blank">01:06:17.980</a></span> | <span class="t">Both of these are going to be integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3979" target="_blank">01:06:19.520</a></span> | <span class="t">So here we'll take xs.append is just x1, ys.append is just x2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3987" target="_blank">01:06:27.840</a></span> | <span class="t">And then here, we actually don't want lists of integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3991" target="_blank">01:06:31.620</a></span> | <span class="t">We will create tensors out of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=3993" target="_blank">01:06:33.760</a></span> | <span class="t">So xs is tors.tensor of xs, and ys is tors.tensor of ys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4001" target="_blank">01:06:41.640</a></span> | <span class="t">And then we don't actually want to take all the words just yet, because I want everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4005" target="_blank">01:06:45.360</a></span> | <span class="t">to be manageable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4007" target="_blank">01:06:47.120</a></span> | <span class="t">So let's just do the first word, which is Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4011" target="_blank">01:06:51.400</a></span> | <span class="t">And then it's clear what these xs and ys would be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4015" target="_blank">01:06:55.600</a></span> | <span class="t">Here let me print character1, character2, just so you see what's going on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4021" target="_blank">01:07:01.760</a></span> | <span class="t">So the bigrams of these characters is .emmmma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4028" target="_blank">01:07:08.960</a></span> | <span class="t">So this single word, as I mentioned, has one, two, three, four, five examples for our neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4033" target="_blank">01:07:13.440</a></span> | <span class="t">network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4034" target="_blank">01:07:14.440</a></span> | <span class="t">There are five separate examples in Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4037" target="_blank">01:07:17.760</a></span> | <span class="t">And those examples are summarized here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4039" target="_blank">01:07:19.480</a></span> | <span class="t">When the input to the neural network is integer 0, the desired label is integer 5, which corresponds</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4046" target="_blank">01:07:26.520</a></span> | <span class="t">to e.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4047" target="_blank">01:07:27.920</a></span> | <span class="t">When the input to the neural network is 5, we want its weights to be arranged so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4052" target="_blank">01:07:32.480</a></span> | <span class="t">13 gets a very high probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4055" target="_blank">01:07:35.280</a></span> | <span class="t">When 13 is put in, we want 13 to have a high probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4059" target="_blank">01:07:39.360</a></span> | <span class="t">When 13 is put in, we also want 1 to have a high probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4063" target="_blank">01:07:43.840</a></span> | <span class="t">When 1 is input, we want 0 to have a very high probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4067" target="_blank">01:07:47.600</a></span> | <span class="t">So there are five separate input examples to a neural net in this dataset.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4075" target="_blank">01:07:55.120</a></span> | <span class="t">I wanted to add a tangent of a note of caution to be careful with a lot of the APIs of some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4079" target="_blank">01:07:59.920</a></span> | <span class="t">of these frameworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4081" target="_blank">01:08:01.600</a></span> | <span class="t">You saw me silently use torch.tensor with a lowercase t, and the output looked right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4088" target="_blank">01:08:08.040</a></span> | <span class="t">But you should be aware that there's actually two ways of constructing a tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4091" target="_blank">01:08:11.960</a></span> | <span class="t">There's a torch.lowercase tensor, and there's also a torch.capital tensor class, which you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4097" target="_blank">01:08:17.040</a></span> | <span class="t">can also construct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4098" target="_blank">01:08:18.840</a></span> | <span class="t">So you can actually call both.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4100" target="_blank">01:08:20.240</a></span> | <span class="t">You can also do torch.capital tensor, and you get an X as in Y as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4105" target="_blank">01:08:25.520</a></span> | <span class="t">So that's not confusing at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4109" target="_blank">01:08:29.080</a></span> | <span class="t">There are threads on what is the difference between these two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4111" target="_blank">01:08:31.840</a></span> | <span class="t">And unfortunately, the docs are just not clear on the difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4116" target="_blank">01:08:36.200</a></span> | <span class="t">And when you look at the docs of lowercase tensor, construct tensor with no autograd</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4120" target="_blank">01:08:40.880</a></span> | <span class="t">history by copying data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4123" target="_blank">01:08:43.760</a></span> | <span class="t">It's just like it doesn't make sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4126" target="_blank">01:08:46.840</a></span> | <span class="t">So the actual difference, as far as I can tell, is explained eventually in this random</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4130" target="_blank">01:08:50.080</a></span> | <span class="t">thread that you can Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4131" target="_blank">01:08:51.800</a></span> | <span class="t">And really it comes down to, I believe, that-- where is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4138" target="_blank">01:08:58.840</a></span> | <span class="t">Torch.tensor infers the D type, the data type, automatically, while torch.tensor just returns</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4143" target="_blank">01:09:03.080</a></span> | <span class="t">a float tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4144" target="_blank">01:09:04.480</a></span> | <span class="t">I would recommend to stick to torch.lowercase tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4147" target="_blank">01:09:07.960</a></span> | <span class="t">So indeed, we see that when I construct this with a capital T, the data type here of X's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4155" target="_blank">01:09:15.600</a></span> | <span class="t">is float32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4158" target="_blank">01:09:18.360</a></span> | <span class="t">But torch.lowercase tensor, you see how it's now X.Dtype is now integer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4166" target="_blank">01:09:26.920</a></span> | <span class="t">So it's advised that you use lowercase t, and you can read more about it if you like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4172" target="_blank">01:09:32.480</a></span> | <span class="t">in some of these threads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4174" target="_blank">01:09:34.440</a></span> | <span class="t">But basically, I'm pointing out some of these things because I want to caution you, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4179" target="_blank">01:09:39.480</a></span> | <span class="t">I want you to get used to reading a lot of documentation and reading through a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4183" target="_blank">01:09:43.800</a></span> | <span class="t">Q&As and threads like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4187" target="_blank">01:09:47.320</a></span> | <span class="t">And some of this stuff is unfortunately not easy and not very well documented, and you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4191" target="_blank">01:09:51.440</a></span> | <span class="t">have to be careful out there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4192" target="_blank">01:09:52.880</a></span> | <span class="t">What we want here is integers, because that's what makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4198" target="_blank">01:09:58.280</a></span> | <span class="t">And so lowercase tensor is what we are using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4201" target="_blank">01:10:01.240</a></span> | <span class="t">Okay, now we want to think through how we're going to feed in these examples into a neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4205" target="_blank">01:10:05.160</a></span> | <span class="t">network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4206" target="_blank">01:10:06.160</a></span> | <span class="t">Now, it's not quite as straightforward as plugging it in, because these examples right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4211" target="_blank">01:10:11.200</a></span> | <span class="t">now are integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4212" target="_blank">01:10:12.440</a></span> | <span class="t">So there's like a 0, 5, or 13.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4214" target="_blank">01:10:14.800</a></span> | <span class="t">It gives us the index of the character, and you can't just plug an integer index into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4218" target="_blank">01:10:18.560</a></span> | <span class="t">a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4220" target="_blank">01:10:20.180</a></span> | <span class="t">These neural nets are sort of made up of these neurons, and these neurons have weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4227" target="_blank">01:10:27.060</a></span> | <span class="t">And as you saw in micrograd, these weights act multiplicatively on the inputs, wx + b,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4232" target="_blank">01:10:32.760</a></span> | <span class="t">there's 10 h's and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4234" target="_blank">01:10:34.440</a></span> | <span class="t">And so it doesn't really make sense to make an input neuron take on integer values that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4237" target="_blank">01:10:37.900</a></span> | <span class="t">you feed in and then multiply on with weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4241" target="_blank">01:10:41.880</a></span> | <span class="t">So instead, a common way of encoding integers is what's called one-hot encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4247" target="_blank">01:10:47.160</a></span> | <span class="t">In one-hot encoding, we take an integer like 13, and we create a vector that is all zeros,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4253" target="_blank">01:10:53.760</a></span> | <span class="t">except for the 13th dimension, which we turn to a 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4257" target="_blank">01:10:57.640</a></span> | <span class="t">And then that vector can feed into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4261" target="_blank">01:11:01.280</a></span> | <span class="t">Now conveniently, PyTorch actually has something called the one-hot function inside Torch and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4269" target="_blank">01:11:09.200</a></span> | <span class="t">in Functional.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4270" target="_blank">01:11:10.480</a></span> | <span class="t">It takes a tensor made up of integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4275" target="_blank">01:11:15.280</a></span> | <span class="t">Long is an integer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4279" target="_blank">01:11:19.440</a></span> | <span class="t">And it also takes a number of classes, which is how large you want your vector to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4287" target="_blank">01:11:27.880</a></span> | <span class="t">So here, let's import Torch. and in .Functional.sf.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4291" target="_blank">01:11:31.900</a></span> | <span class="t">This is a common way of importing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4294" target="_blank">01:11:34.300</a></span> | <span class="t">And then let's do f.one-hot, and we feed in the integers that we want to encode.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4300" target="_blank">01:11:40.160</a></span> | <span class="t">So we can actually feed in the entire array of x's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4304" target="_blank">01:11:44.240</a></span> | <span class="t">And we can tell it that numClass is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4307" target="_blank">01:11:47.900</a></span> | <span class="t">So it doesn't have to try to guess it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4309" target="_blank">01:11:49.540</a></span> | <span class="t">It may have guessed that it's only 13 and would give us an incorrect result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4314" target="_blank">01:11:54.820</a></span> | <span class="t">So this is the one-hot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4316" target="_blank">01:11:56.020</a></span> | <span class="t">Let's call this xInc for xEncoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4322" target="_blank">01:12:02.240</a></span> | <span class="t">And then we see that xEncoded.shape is 5 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4327" target="_blank">01:12:07.360</a></span> | <span class="t">And we can also visualize it, plt.imshow(xInc) to make it a little bit more clear, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4333" target="_blank">01:12:13.760</a></span> | <span class="t">this is a little messy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4335" target="_blank">01:12:15.640</a></span> | <span class="t">So we see that we've encoded all the five examples into vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4340" target="_blank">01:12:20.680</a></span> | <span class="t">We have five examples, so we have five rows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4342" target="_blank">01:12:22.860</a></span> | <span class="t">And each row here is now an example into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4346" target="_blank">01:12:26.480</a></span> | <span class="t">And we see that the appropriate bit is turned on as a 1, and everything else is 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4352" target="_blank">01:12:32.080</a></span> | <span class="t">So here, for example, the 0th bit is turned on, the 5th bit is turned on, the 13th bits</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4359" target="_blank">01:12:39.080</a></span> | <span class="t">are turned on for both of these examples, and the first bit here is turned on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4364" target="_blank">01:12:44.800</a></span> | <span class="t">So that's how we can encode integers into vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4369" target="_blank">01:12:49.520</a></span> | <span class="t">And then these vectors can feed into neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4372" target="_blank">01:12:52.120</a></span> | <span class="t">One more issue to be careful with here, by the way, is let's look at the data type of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4376" target="_blank">01:12:56.800</a></span> | <span class="t">xEncoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4377" target="_blank">01:12:57.800</a></span> | <span class="t">We always want to be careful with data types.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4378" target="_blank">01:12:58.800</a></span> | <span class="t">What would you expect xEncoding's data type to be?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4383" target="_blank">01:13:03.000</a></span> | <span class="t">When we're plugging numbers into neural nets, we don't want them to be integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4386" target="_blank">01:13:06.340</a></span> | <span class="t">We want them to be floating-point numbers that can take on various values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4390" target="_blank">01:13:10.620</a></span> | <span class="t">But the Dtype here is actually a 64-bit integer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4394" target="_blank">01:13:14.480</a></span> | <span class="t">And the reason for that, I suspect, is that one hot received a 64-bit integer here, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4399" target="_blank">01:13:19.880</a></span> | <span class="t">it returned the same data type.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4402" target="_blank">01:13:22.120</a></span> | <span class="t">And when you look at the signature of one hot, it doesn't even take a Dtype, a desired</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4405" target="_blank">01:13:25.960</a></span> | <span class="t">data type, off the output tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4408" target="_blank">01:13:28.640</a></span> | <span class="t">And so we can't, in a lot of functions in Torch, we'd be able to do something like Dtype</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4412" target="_blank">01:13:32.400</a></span> | <span class="t">equals Torch.float32, which is what we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4415" target="_blank">01:13:35.840</a></span> | <span class="t">But one hot does not support that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4418" target="_blank">01:13:38.120</a></span> | <span class="t">So instead, we're going to want to cast this to float like this, so that these, everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4425" target="_blank">01:13:45.200</a></span> | <span class="t">is the same, everything looks the same, but the Dtype is float32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4430" target="_blank">01:13:50.160</a></span> | <span class="t">And floats can feed into neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4433" target="_blank">01:13:53.480</a></span> | <span class="t">So now let's construct our first neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4436" target="_blank">01:13:56.240</a></span> | <span class="t">This neuron will look at these input vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4440" target="_blank">01:14:00.320</a></span> | <span class="t">And as you remember from micrograd, these neurons basically perform a very simple function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4444" target="_blank">01:14:04.600</a></span> | <span class="t">wx plus b, where wx is a dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4449" target="_blank">01:14:09.760</a></span> | <span class="t">So we can achieve the same thing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4452" target="_blank">01:14:12.260</a></span> | <span class="t">Let's first define the weights of this neuron, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4454" target="_blank">01:14:14.640</a></span> | <span class="t">What are the initial weights at initialization for this neuron?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4459" target="_blank">01:14:19.040</a></span> | <span class="t">Let's initialize them with Torch.random.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4461" target="_blank">01:14:21.840</a></span> | <span class="t">Torch.random fills a tensor with random numbers drawn from a normal distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4469" target="_blank">01:14:29.480</a></span> | <span class="t">And a normal distribution has a probability density function like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4474" target="_blank">01:14:34.600</a></span> | <span class="t">And so most of the numbers drawn from this distribution will be around zero, but some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4479" target="_blank">01:14:39.560</a></span> | <span class="t">of them will be as high as almost three and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4482" target="_blank">01:14:42.120</a></span> | <span class="t">And very few numbers will be above three in magnitude.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4486" target="_blank">01:14:46.520</a></span> | <span class="t">So we need to take a size as an input here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4490" target="_blank">01:14:50.660</a></span> | <span class="t">And I'm going to use size as to be 27 by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4494" target="_blank">01:14:54.800</a></span> | <span class="t">So 27 by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4496" target="_blank">01:14:56.960</a></span> | <span class="t">And then let's visualize w.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4498" target="_blank">01:14:58.640</a></span> | <span class="t">So w is a column vector of 27 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4503" target="_blank">01:15:03.360</a></span> | <span class="t">And these weights are then multiplied by the inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4508" target="_blank">01:15:08.840</a></span> | <span class="t">So now to perform this multiplication, we can take x encoding and we can multiply it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4513" target="_blank">01:15:13.640</a></span> | <span class="t">with w.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4515" target="_blank">01:15:15.080</a></span> | <span class="t">This is a matrix multiplication operator in PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4520" target="_blank">01:15:20.180</a></span> | <span class="t">And the output of this operation is 5 by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4523" target="_blank">01:15:23.840</a></span> | <span class="t">The reason it's 5 by 5 is the following.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4526" target="_blank">01:15:26.080</a></span> | <span class="t">We took x encoding, which is 5 by 27, and we multiplied it by 27 by 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4533" target="_blank">01:15:33.840</a></span> | <span class="t">And in matrix multiplication, you see that the output will become 5 by 1 because these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4540" target="_blank">01:15:40.320</a></span> | <span class="t">27 will multiply and add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4545" target="_blank">01:15:45.100</a></span> | <span class="t">So basically what we're seeing here out of this operation is we are seeing the five activations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4553" target="_blank">01:15:53.720</a></span> | <span class="t">of this neuron on these five inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4558" target="_blank">01:15:58.480</a></span> | <span class="t">And we've evaluated all of them in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4560" target="_blank">01:16:00.680</a></span> | <span class="t">We didn't feed in just a single input to the single neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4563" target="_blank">01:16:03.600</a></span> | <span class="t">We fed in simultaneously all the five inputs into the same neuron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4568" target="_blank">01:16:08.320</a></span> | <span class="t">And in parallel, PyTorch has evaluated the wx plus b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4573" target="_blank">01:16:13.120</a></span> | <span class="t">But here it's just wx.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4574" target="_blank">01:16:14.600</a></span> | <span class="t">There's no bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4575" target="_blank">01:16:15.600</a></span> | <span class="t">It has evaluated w times x for all of them independently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4580" target="_blank">01:16:20.800</a></span> | <span class="t">Now instead of a single neuron, though, I would like to have 27 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4584" target="_blank">01:16:24.280</a></span> | <span class="t">And I'll show you in a second why I want 27 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4587" target="_blank">01:16:27.820</a></span> | <span class="t">So instead of having just a 1 here, which is indicating this presence of one single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4591" target="_blank">01:16:31.560</a></span> | <span class="t">neuron, we can use 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4594" target="_blank">01:16:34.940</a></span> | <span class="t">And then when w is 27 by 27, this will in parallel evaluate all the 27 neurons on all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4603" target="_blank">01:16:43.720</a></span> | <span class="t">the five inputs, giving us a much better, much, much bigger result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4609" target="_blank">01:16:49.740</a></span> | <span class="t">So now what we've done is 5 by 27 multiplied 27 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4614" target="_blank">01:16:54.340</a></span> | <span class="t">And the output of this is now 5 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4617" target="_blank">01:16:57.960</a></span> | <span class="t">So we can see that the shape of this is 5 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4624" target="_blank">01:17:04.080</a></span> | <span class="t">So what is every element here telling us?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4627" target="_blank">01:17:07.400</a></span> | <span class="t">It's telling us for every one of 27 neurons that we created, what is the firing rate of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4635" target="_blank">01:17:15.160</a></span> | <span class="t">those neurons on every one of those five examples?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4639" target="_blank">01:17:19.800</a></span> | <span class="t">So the element, for example, 3, 13 is giving us the firing rate of the 13th neuron looking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4649" target="_blank">01:17:29.600</a></span> | <span class="t">at the third input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4652" target="_blank">01:17:32.120</a></span> | <span class="t">And the way this was achieved is by a dot product between the third input and the 13th</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4660" target="_blank">01:17:40.200</a></span> | <span class="t">column of this w matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4665" target="_blank">01:17:45.800</a></span> | <span class="t">So using matrix multiplication, we can very efficiently evaluate the dot product between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4671" target="_blank">01:17:51.980</a></span> | <span class="t">lots of input examples in a batch and lots of neurons where all of those neurons have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4678" target="_blank">01:17:58.080</a></span> | <span class="t">weights in the columns of those w's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4681" target="_blank">01:18:01.320</a></span> | <span class="t">And in matrix multiplication, we're just doing those dot products in parallel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4685" target="_blank">01:18:05.720</a></span> | <span class="t">Just to show you that this is the case, we can take x_inc and we can take the third row.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4690" target="_blank">01:18:10.600</a></span> | <span class="t">And we can take the w and take its 13th column.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4697" target="_blank">01:18:17.600</a></span> | <span class="t">And then we can do x_inc at 3, element-wise multiply with w at 13, and sum that up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4707" target="_blank">01:18:27.840</a></span> | <span class="t">That's wx plus b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4708" target="_blank">01:18:28.840</a></span> | <span class="t">Well, there's no plus b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4710" target="_blank">01:18:30.840</a></span> | <span class="t">It's just wx dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4713" target="_blank">01:18:33.120</a></span> | <span class="t">And that's this number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4715" target="_blank">01:18:35.520</a></span> | <span class="t">So you see that this is just being done efficiently by the matrix multiplication operation for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4720" target="_blank">01:18:40.680</a></span> | <span class="t">all the input examples and for all the output neurons of this first layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4725" target="_blank">01:18:45.760</a></span> | <span class="t">Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4731" target="_blank">01:18:51.480</a></span> | <span class="t">27 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4733" target="_blank">01:18:53.040</a></span> | <span class="t">So we have 27 inputs and now we have 27 neurons.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4737" target="_blank">01:18:57.220</a></span> | <span class="t">These neurons perform w times x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4739" target="_blank">01:18:59.860</a></span> | <span class="t">They don't have a bias and they don't have a nonlinearity like tanh.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4743" target="_blank">01:19:03.360</a></span> | <span class="t">We're going to leave them to be a linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4746" target="_blank">01:19:06.740</a></span> | <span class="t">In addition to that, we're not going to have any other layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4749" target="_blank">01:19:09.260</a></span> | <span class="t">This is going to be it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4750" target="_blank">01:19:10.260</a></span> | <span class="t">It's just going to be the dumbest, smallest, simplest neural net, which is just a single</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4754" target="_blank">01:19:14.440</a></span> | <span class="t">linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4756" target="_blank">01:19:16.680</a></span> | <span class="t">And now I'd like to explain what I want those 27 outputs to be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4760" target="_blank">01:19:20.680</a></span> | <span class="t">Intuitively, what we're trying to produce here for every single input example is we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4764" target="_blank">01:19:24.880</a></span> | <span class="t">trying to produce some kind of a probability distribution for the next character in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4769" target="_blank">01:19:29.600</a></span> | <span class="t">And there's 27 of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4771" target="_blank">01:19:31.760</a></span> | <span class="t">But we have to come up with precise semantics for exactly how we're going to interpret these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4776" target="_blank">01:19:36.200</a></span> | <span class="t">27 numbers that these neurons take on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4779" target="_blank">01:19:39.880</a></span> | <span class="t">Now intuitively, you see here that these numbers are negative and some of them are positive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4784" target="_blank">01:19:44.760</a></span> | <span class="t">etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4785" target="_blank">01:19:45.760</a></span> | <span class="t">And that's because these are coming out of a neural net layer initialized with these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4791" target="_blank">01:19:51.640</a></span> | <span class="t">normal distribution parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4794" target="_blank">01:19:54.560</a></span> | <span class="t">But what we want is we want something like we had here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4797" target="_blank">01:19:57.460</a></span> | <span class="t">But each row here told us the counts, and then we normalized the counts to get probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4803" target="_blank">01:20:03.720</a></span> | <span class="t">And we want something similar to come out of a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4806" target="_blank">01:20:06.700</a></span> | <span class="t">But what we just have right now is just some negative and positive numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4810" target="_blank">01:20:10.720</a></span> | <span class="t">Now we want those numbers to somehow represent the probabilities for the next character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4815" target="_blank">01:20:15.560</a></span> | <span class="t">But you see that probabilities, they have a special structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4820" target="_blank">01:20:20.120</a></span> | <span class="t">They're positive numbers and they sum to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4823" target="_blank">01:20:23.060</a></span> | <span class="t">And so that doesn't just come out of a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4825" target="_blank">01:20:25.980</a></span> | <span class="t">And then they can't be counts because these counts are positive and counts are integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4832" target="_blank">01:20:32.960</a></span> | <span class="t">So counts are also not really a good thing to output from a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4836" target="_blank">01:20:36.920</a></span> | <span class="t">So instead, what the neural net is going to output and how we are going to interpret the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4840" target="_blank">01:20:40.880</a></span> | <span class="t">27 numbers is that these 27 numbers are giving us log counts, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4850" target="_blank">01:20:50.660</a></span> | <span class="t">So instead of giving us counts directly, like in this table, they're giving us log counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4856" target="_blank">01:20:56.300</a></span> | <span class="t">And to get the counts, we're going to take the log counts and we're going to exponentiate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4860" target="_blank">01:21:00.180</a></span> | <span class="t">them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4861" target="_blank">01:21:01.540</a></span> | <span class="t">Now exponentiation takes the following form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4867" target="_blank">01:21:07.360</a></span> | <span class="t">It takes numbers that are negative or they are positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4871" target="_blank">01:21:11.120</a></span> | <span class="t">It takes the entire real line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4873" target="_blank">01:21:13.120</a></span> | <span class="t">And then if you plug in negative numbers, you're going to get e to the x, which is always below</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4879" target="_blank">01:21:19.160</a></span> | <span class="t">1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4880" target="_blank">01:21:20.840</a></span> | <span class="t">So you're getting numbers lower than 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4883" target="_blank">01:21:23.680</a></span> | <span class="t">And if you plug in numbers greater than 0, you're getting numbers greater than 1, all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4888" target="_blank">01:21:28.480</a></span> | <span class="t">the way growing to the infinity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4891" target="_blank">01:21:31.080</a></span> | <span class="t">And this here grows to 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4893" target="_blank">01:21:33.480</a></span> | <span class="t">So basically, we're going to take these numbers here and instead of them being positive and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4904" target="_blank">01:21:44.640</a></span> | <span class="t">negative and all over the place, we're going to interpret them as log counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4908" target="_blank">01:21:48.980</a></span> | <span class="t">And then we're going to element-wise exponentiate these numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4913" target="_blank">01:21:53.480</a></span> | <span class="t">Exponentiating them now gives us something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4916" target="_blank">01:21:56.640</a></span> | <span class="t">And you see that these numbers now, because they went through an exponent, all the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4919" target="_blank">01:21:59.940</a></span> | <span class="t">numbers turned into numbers below 1, like 0.338.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4924" target="_blank">01:22:04.460</a></span> | <span class="t">And all the positive numbers originally turned into even more positive numbers, sort of greater</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4928" target="_blank">01:22:08.940</a></span> | <span class="t">than 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4931" target="_blank">01:22:11.000</a></span> | <span class="t">So like for example, 7 is some positive number over here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4937" target="_blank">01:22:17.560</a></span> | <span class="t">That is greater than 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4941" target="_blank">01:22:21.300</a></span> | <span class="t">But exponentiated outputs here basically give us something that we can use and interpret</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4947" target="_blank">01:22:27.800</a></span> | <span class="t">as the equivalent of counts originally.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4951" target="_blank">01:22:31.140</a></span> | <span class="t">So you see these counts here, 112, 751, 1, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4956" target="_blank">01:22:36.560</a></span> | <span class="t">The neural net is kind of now predicting counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4961" target="_blank">01:22:41.860</a></span> | <span class="t">And these counts are positive numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4964" target="_blank">01:22:44.060</a></span> | <span class="t">They can never be below 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4965" target="_blank">01:22:45.580</a></span> | <span class="t">So that makes sense.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4967" target="_blank">01:22:47.040</a></span> | <span class="t">And they can now take on various values depending on the settings of W.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4974" target="_blank">01:22:54.420</a></span> | <span class="t">So let me break this down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4976" target="_blank">01:22:56.400</a></span> | <span class="t">We're going to interpret these to be the log counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4981" target="_blank">01:23:01.420</a></span> | <span class="t">In other words for this, that is often used, is so-called logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4985" target="_blank">01:23:05.380</a></span> | <span class="t">These are logits, log counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4988" target="_blank">01:23:08.800</a></span> | <span class="t">And these will be sort of the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4991" target="_blank">01:23:11.620</a></span> | <span class="t">Logits exponentiate it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=4993" target="_blank">01:23:13.680</a></span> | <span class="t">And this is equivalent to the n matrix, sort of the n array that we used previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5000" target="_blank">01:23:20.100</a></span> | <span class="t">Remember this was the n?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5001" target="_blank">01:23:21.820</a></span> | <span class="t">This is the array of counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5004" target="_blank">01:23:24.260</a></span> | <span class="t">And each row here are the counts for the next character, sort of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5012" target="_blank">01:23:32.900</a></span> | <span class="t">So those are the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5014" target="_blank">01:23:34.520</a></span> | <span class="t">And now the probabilities are just the counts normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5019" target="_blank">01:23:39.860</a></span> | <span class="t">And so I'm not going to find the same, but basically I'm not going to scroll all over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5025" target="_blank">01:23:45.740</a></span> | <span class="t">the place.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5026" target="_blank">01:23:46.740</a></span> | <span class="t">We've already done this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5027" target="_blank">01:23:47.740</a></span> | <span class="t">We want to counts.sum along the first dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5031" target="_blank">01:23:51.940</a></span> | <span class="t">And we want to keep dims as true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5034" target="_blank">01:23:54.900</a></span> | <span class="t">We went over this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5036" target="_blank">01:23:56.400</a></span> | <span class="t">And this is how we normalize the rows of our counts matrix to get our probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5045" target="_blank">01:24:05.100</a></span> | <span class="t">So now these are the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5048" target="_blank">01:24:08.240</a></span> | <span class="t">And these are the counts that we have currently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5050" target="_blank">01:24:10.880</a></span> | <span class="t">And now when I show the probabilities, you see that every row here, of course, will sum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5059" target="_blank">01:24:19.880</a></span> | <span class="t">to one because they're normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5063" target="_blank">01:24:23.360</a></span> | <span class="t">And the shape of this is 5 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5067" target="_blank">01:24:27.640</a></span> | <span class="t">And so really what we've achieved is for every one of our five examples, we now have a row</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5073" target="_blank">01:24:33.180</a></span> | <span class="t">that came out of a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5075" target="_blank">01:24:35.360</a></span> | <span class="t">And because of the transformations here, we made sure that this output of this neural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5079" target="_blank">01:24:39.480</a></span> | <span class="t">net now are probabilities, or we can interpret to be probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5084" target="_blank">01:24:44.200</a></span> | <span class="t">So our WX here gave us logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5088" target="_blank">01:24:48.260</a></span> | <span class="t">And then we interpret those to be log counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5090" target="_blank">01:24:50.880</a></span> | <span class="t">We exponentiate to get something that looks like counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5094" target="_blank">01:24:54.200</a></span> | <span class="t">And then we normalize those counts to get a probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5097" target="_blank">01:24:57.700</a></span> | <span class="t">And all of these are differentiable operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5100" target="_blank">01:25:00.560</a></span> | <span class="t">So what we've done now is we are taking inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5103" target="_blank">01:25:03.400</a></span> | <span class="t">We have differentiable operations that we can back propagate through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5107" target="_blank">01:25:07.120</a></span> | <span class="t">And we're getting out probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5110" target="_blank">01:25:10.040</a></span> | <span class="t">So for example, for the zeroth example that fed in, which was the zeroth example here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5118" target="_blank">01:25:18.360</a></span> | <span class="t">was a 1/2 vector of 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5120" target="_blank">01:25:20.880</a></span> | <span class="t">And it basically corresponded to feeding in this example here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5128" target="_blank">01:25:28.000</a></span> | <span class="t">So we're feeding in a dot into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5130" target="_blank">01:25:30.560</a></span> | <span class="t">And the way we fed the dot into a neural net is that we first got its index.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5134" target="_blank">01:25:34.560</a></span> | <span class="t">Then we one-hot encoded it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5136" target="_blank">01:25:36.800</a></span> | <span class="t">Then it went into the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5138" target="_blank">01:25:38.680</a></span> | <span class="t">And out came this distribution of probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5143" target="_blank">01:25:43.600</a></span> | <span class="t">And its shape is 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5147" target="_blank">01:25:47.040</a></span> | <span class="t">There's 27 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5148" target="_blank">01:25:48.920</a></span> | <span class="t">And we're going to interpret this as the neural net's assignment for how likely every one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5154" target="_blank">01:25:54.400</a></span> | <span class="t">of these characters, the 27 characters, are to come next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5159" target="_blank">01:25:59.980</a></span> | <span class="t">And as we tune the weights w, we're going to be, of course, getting different probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5164" target="_blank">01:26:04.400</a></span> | <span class="t">out for any character that you input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5167" target="_blank">01:26:07.400</a></span> | <span class="t">And so now the question is just, can we optimize and find a good w such that the probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5172" target="_blank">01:26:12.120</a></span> | <span class="t">coming out are pretty good?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5174" target="_blank">01:26:14.720</a></span> | <span class="t">And the way we measure pretty good is by the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5176" target="_blank">01:26:16.960</a></span> | <span class="t">OK, so I organized everything into a single summary so that hopefully it's a bit more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5181" target="_blank">01:26:21.040</a></span> | <span class="t">clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5182" target="_blank">01:26:22.040</a></span> | <span class="t">So it starts here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5183" target="_blank">01:26:23.040</a></span> | <span class="t">We have an input data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5184" target="_blank">01:26:24.580</a></span> | <span class="t">We have some inputs to the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5186" target="_blank">01:26:26.600</a></span> | <span class="t">And we have some labels for the correct next character in a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5190" target="_blank">01:26:30.680</a></span> | <span class="t">These are integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5193" target="_blank">01:26:33.000</a></span> | <span class="t">Here I'm using torch generators now so that you see the same numbers that I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5198" target="_blank">01:26:38.800</a></span> | <span class="t">And I'm generating 27 neurons' weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5203" target="_blank">01:26:43.160</a></span> | <span class="t">And each neuron here receives 27 inputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5208" target="_blank">01:26:48.800</a></span> | <span class="t">Then here we're going to plug in all the input examples, x's, into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5212" target="_blank">01:26:52.780</a></span> | <span class="t">So here, this is a forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5215" target="_blank">01:26:55.200</a></span> | <span class="t">First, we have to encode all of the inputs into one-hot representations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5220" target="_blank">01:27:00.640</a></span> | <span class="t">So we have 27 classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5221" target="_blank">01:27:01.840</a></span> | <span class="t">We pass in these integers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5224" target="_blank">01:27:04.120</a></span> | <span class="t">And x_inc becomes an array that is 5 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5229" target="_blank">01:27:09.280</a></span> | <span class="t">0's, etc. a few ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5232" target="_blank">01:27:12.480</a></span> | <span class="t">We then multiply this in the first layer of a neural net to get logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5237" target="_blank">01:27:17.440</a></span> | <span class="t">Exponentiate the logits to get fake counts, sort of.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5240" target="_blank">01:27:20.920</a></span> | <span class="t">And normalize these counts to get probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5244" target="_blank">01:27:24.560</a></span> | <span class="t">So these last two lines, by the way, here, are called the softmax, which I pulled up</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5250" target="_blank">01:27:30.880</a></span> | <span class="t">here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5252" target="_blank">01:27:32.600</a></span> | <span class="t">Softmax is a very often used layer in a neural net that takes these z's, which are logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5259" target="_blank">01:27:39.640</a></span> | <span class="t">exponentiates them, and divides and normalizes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5263" target="_blank">01:27:43.320</a></span> | <span class="t">It's a way of taking outputs of a neural net layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5266" target="_blank">01:27:46.540</a></span> | <span class="t">And these outputs can be positive or negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5270" target="_blank">01:27:50.120</a></span> | <span class="t">And it outputs probability distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5272" target="_blank">01:27:52.600</a></span> | <span class="t">It outputs something that always sums to 1 and are positive numbers, just like probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5279" target="_blank">01:27:59.080</a></span> | <span class="t">So this is kind of like a normalization function, if you want to think of it that way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5282" target="_blank">01:28:02.400</a></span> | <span class="t">And you can put it on top of any other linear layer inside a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5285" target="_blank">01:28:05.840</a></span> | <span class="t">And it basically makes a neural net output probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5288" target="_blank">01:28:08.920</a></span> | <span class="t">That's very often used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5290" target="_blank">01:28:10.680</a></span> | <span class="t">And we used it as well here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5293" target="_blank">01:28:13.600</a></span> | <span class="t">So this is the forward pass, and that's how we made a neural net output probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5298" target="_blank">01:28:18.080</a></span> | <span class="t">Now, you'll notice that all of these, this entire forward pass is made up of differentiable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5306" target="_blank">01:28:26.600</a></span> | <span class="t">layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5308" target="_blank">01:28:28.200</a></span> | <span class="t">Everything here, we can backpropagate through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5310" target="_blank">01:28:30.400</a></span> | <span class="t">And we saw some of the backpropagation in micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5313" target="_blank">01:28:33.440</a></span> | <span class="t">This is just multiplication and addition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5316" target="_blank">01:28:36.700</a></span> | <span class="t">All that's happening here is just multiply and add.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5318" target="_blank">01:28:38.960</a></span> | <span class="t">And we know how to backpropagate through them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5321" target="_blank">01:28:41.000</a></span> | <span class="t">Exponentiation, we know how to backpropagate through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5324" target="_blank">01:28:44.160</a></span> | <span class="t">And then here, we are summing, and sum is easily backpropagatable as well, and division</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5331" target="_blank">01:28:51.200</a></span> | <span class="t">as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5332" target="_blank">01:28:52.200</a></span> | <span class="t">So everything here is a differentiable operation, and we can backpropagate through.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5337" target="_blank">01:28:57.080</a></span> | <span class="t">Now, we achieve these probabilities, which are 5 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5341" target="_blank">01:29:01.880</a></span> | <span class="t">For every single example, we have a vector of probabilities that sum to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5346" target="_blank">01:29:06.560</a></span> | <span class="t">And then here, I wrote a bunch of stuff to sort of like break down the examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5351" target="_blank">01:29:11.680</a></span> | <span class="t">So we have five examples making up Emma, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5356" target="_blank">01:29:16.640</a></span> | <span class="t">And there are five bigrams inside Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5360" target="_blank">01:29:20.240</a></span> | <span class="t">So bigram example 1 is that E is the beginning character, right after dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5368" target="_blank">01:29:28.600</a></span> | <span class="t">And the indexes for these are 0 and 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5371" target="_blank">01:29:31.540</a></span> | <span class="t">So then we feed in a 0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5374" target="_blank">01:29:34.440</a></span> | <span class="t">That's the input to the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5376" target="_blank">01:29:36.160</a></span> | <span class="t">We get probabilities from the neural net that are 27 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5381" target="_blank">01:29:41.540</a></span> | <span class="t">And then the label is 5, because E actually comes after dot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5386" target="_blank">01:29:46.100</a></span> | <span class="t">So that's the label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5388" target="_blank">01:29:48.140</a></span> | <span class="t">And then we use this label 5 to index into the probability distribution here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5394" target="_blank">01:29:54.560</a></span> | <span class="t">So this index 5 here is 0, 1, 2, 3, 4, 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5399" target="_blank">01:29:59.860</a></span> | <span class="t">It's this number here, which is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5404" target="_blank">01:30:04.320</a></span> | <span class="t">So that's basically the probability assigned by the neural net to the actual correct character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5408" target="_blank">01:30:08.960</a></span> | <span class="t">You see that the network currently thinks that this next character, that E following</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5412" target="_blank">01:30:12.960</a></span> | <span class="t">dot, is only 1% likely, which is, of course, not very good, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5417" target="_blank">01:30:17.360</a></span> | <span class="t">Because this actually is a training example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5419" target="_blank">01:30:19.980</a></span> | <span class="t">And the network thinks that this is currently very, very unlikely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5422" target="_blank">01:30:22.720</a></span> | <span class="t">But that's just because we didn't get very lucky in generating a good setting of W.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5427" target="_blank">01:30:27.140</a></span> | <span class="t">So right now, this network thinks this is unlikely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5429" target="_blank">01:30:29.620</a></span> | <span class="t">And 0.01 is not a good outcome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5432" target="_blank">01:30:32.180</a></span> | <span class="t">So the log likelihood, then, is very negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5436" target="_blank">01:30:36.100</a></span> | <span class="t">And the negative log likelihood is very positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5439" target="_blank">01:30:39.660</a></span> | <span class="t">And so 4 is a very high negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5443" target="_blank">01:30:43.420</a></span> | <span class="t">And that means we're going to have a high loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5445" target="_blank">01:30:45.620</a></span> | <span class="t">Because what is the loss?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5446" target="_blank">01:30:46.980</a></span> | <span class="t">The loss is just the average negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5451" target="_blank">01:30:51.880</a></span> | <span class="t">So the second character is EM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5453" target="_blank">01:30:53.860</a></span> | <span class="t">And you see here that also the network thought that M following E is very unlikely, 1%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5461" target="_blank">01:31:01.740</a></span> | <span class="t">For M following M, it thought it was 2%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5464" target="_blank">01:31:04.500</a></span> | <span class="t">And for A following M, it actually thought it was 7% likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5468" target="_blank">01:31:08.020</a></span> | <span class="t">So just by chance, this one actually has a pretty good probability and therefore a pretty</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5472" target="_blank">01:31:12.600</a></span> | <span class="t">low negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5475" target="_blank">01:31:15.580</a></span> | <span class="t">And finally here, it thought this was 1% likely.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5478" target="_blank">01:31:18.580</a></span> | <span class="t">So overall, our average negative log likelihood, which is the loss, the total loss that summarizes</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5484" target="_blank">01:31:24.740</a></span> | <span class="t">basically how well this network currently works, at least on this one word, not on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5489" target="_blank">01:31:29.220</a></span> | <span class="t">full data set, just the one word, is 3.76, which is actually a fairly high loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5494" target="_blank">01:31:34.340</a></span> | <span class="t">This is not a very good setting of W's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5496" target="_blank">01:31:36.300</a></span> | <span class="t">Now here's what we can do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5498" target="_blank">01:31:38.860</a></span> | <span class="t">We're currently getting 3.76.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5501" target="_blank">01:31:41.480</a></span> | <span class="t">We can actually come here and we can change our W. We can resample it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5505" target="_blank">01:31:45.860</a></span> | <span class="t">So let me just add one to have a different seed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5508" target="_blank">01:31:48.980</a></span> | <span class="t">And then we get a different W. And then we can rerun this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5513" target="_blank">01:31:53.100</a></span> | <span class="t">And with this different seed, with this different setting of W's, we now get 3.37.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5518" target="_blank">01:31:58.820</a></span> | <span class="t">So this is a much better W, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5520" target="_blank">01:32:00.940</a></span> | <span class="t">And it's better because the probabilities just happen to come out higher for the characters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5526" target="_blank">01:32:06.940</a></span> | <span class="t">that actually are next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5529" target="_blank">01:32:09.100</a></span> | <span class="t">And so you can imagine actually just resampling this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5531" target="_blank">01:32:11.820</a></span> | <span class="t">We can try 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5534" target="_blank">01:32:14.540</a></span> | <span class="t">So okay, this was not very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5537" target="_blank">01:32:17.440</a></span> | <span class="t">Let's try one more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5538" target="_blank">01:32:18.720</a></span> | <span class="t">We can try 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5540" target="_blank">01:32:20.180</a></span> | <span class="t">Okay, this was a terrible setting because we have a very high loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5544" target="_blank">01:32:24.940</a></span> | <span class="t">So anyway, I'm going to erase this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5548" target="_blank">01:32:28.020</a></span> | <span class="t">What I'm doing here, which is just guess and check of randomly assigning parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5553" target="_blank">01:32:33.460</a></span> | <span class="t">and seeing if the network is good, that is amateur hour.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5557" target="_blank">01:32:37.420</a></span> | <span class="t">That's not how you optimize in neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5559" target="_blank">01:32:39.300</a></span> | <span class="t">The way you optimize neural net is you start with some random guess, and we're going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5562" target="_blank">01:32:42.740</a></span> | <span class="t">commit to this one, even though it's not very good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5565" target="_blank">01:32:45.380</a></span> | <span class="t">But now the big deal is we have a loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5568" target="_blank">01:32:48.500</a></span> | <span class="t">So this loss is made up only of differentiable operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5574" target="_blank">01:32:54.500</a></span> | <span class="t">And we can minimize the loss by tuning Ws by computing the gradients of the loss with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5581" target="_blank">01:33:01.460</a></span> | <span class="t">respect to these W matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5585" target="_blank">01:33:05.320</a></span> | <span class="t">And so then we can tune W to minimize the loss and find a good setting of W using gradient-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5590" target="_blank">01:33:10.540</a></span> | <span class="t">optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5591" target="_blank">01:33:11.900</a></span> | <span class="t">So let's see how that will work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5593" target="_blank">01:33:13.260</a></span> | <span class="t">Now things are actually going to look almost identical to what we had with micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5597" target="_blank">01:33:17.300</a></span> | <span class="t">So here I pulled up the lecture from micrograd, the notebook that's from this repository.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5604" target="_blank">01:33:24.100</a></span> | <span class="t">And when I scroll all the way to the end where we left off with micrograd, we had something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5607" target="_blank">01:33:27.180</a></span> | <span class="t">very, very similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5608" target="_blank">01:33:28.740</a></span> | <span class="t">We had a number of input examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5611" target="_blank">01:33:31.020</a></span> | <span class="t">In this case, we had four input examples inside Xs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5614" target="_blank">01:33:34.420</a></span> | <span class="t">And we had their targets, desired targets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5617" target="_blank">01:33:37.920</a></span> | <span class="t">Just like here, we have our Xs now, but we have five of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5620" target="_blank">01:33:40.940</a></span> | <span class="t">And they're now integers instead of vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5624" target="_blank">01:33:44.380</a></span> | <span class="t">But we're going to convert our integers to vectors, except our vectors will be 27 large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5629" target="_blank">01:33:49.380</a></span> | <span class="t">instead of three large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5632" target="_blank">01:33:52.280</a></span> | <span class="t">And then here what we did is first we did a forward pass where we ran a neural net on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5636" target="_blank">01:33:56.340</a></span> | <span class="t">all of the inputs to get predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5640" target="_blank">01:34:00.500</a></span> | <span class="t">Our neural net at the time, this N of X, was a multilayer perceptron.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5645" target="_blank">01:34:05.340</a></span> | <span class="t">Our neural net is going to look different because our neural net is just a single layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5650" target="_blank">01:34:10.660</a></span> | <span class="t">single linear layer followed by a softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5654" target="_blank">01:34:14.060</a></span> | <span class="t">So that's our neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5656" target="_blank">01:34:16.100</a></span> | <span class="t">And the loss here was the mean squared error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5658" target="_blank">01:34:18.600</a></span> | <span class="t">So we simply subtracted the prediction from the ground truth and squared it and summed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5662" target="_blank">01:34:22.120</a></span> | <span class="t">it all up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5663" target="_blank">01:34:23.120</a></span> | <span class="t">And that was the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5664" target="_blank">01:34:24.380</a></span> | <span class="t">And loss was the single number that summarized the quality of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5668" target="_blank">01:34:28.700</a></span> | <span class="t">And when loss is low, like almost zero, that means the neural net is predicting correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5676" target="_blank">01:34:36.440</a></span> | <span class="t">So we had a single number that summarized the performance of the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5682" target="_blank">01:34:42.240</a></span> | <span class="t">And everything here was differentiable and was stored in a massive compute graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5687" target="_blank">01:34:47.040</a></span> | <span class="t">And then we iterated over all the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5689" target="_blank">01:34:49.320</a></span> | <span class="t">We made sure that the gradients are set to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5691" target="_blank">01:34:51.960</a></span> | <span class="t">And we called loss.backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5694" target="_blank">01:34:54.360</a></span> | <span class="t">And loss.backward initiated backpropagation at the final output node of loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5699" target="_blank">01:34:59.680</a></span> | <span class="t">So remember these expressions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5702" target="_blank">01:35:02.280</a></span> | <span class="t">We had loss all the way at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5703" target="_blank">01:35:03.720</a></span> | <span class="t">We start backpropagation and we went all the way back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5706" target="_blank">01:35:06.600</a></span> | <span class="t">And we made sure that we populated all the parameters dot grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5711" target="_blank">01:35:11.000</a></span> | <span class="t">So dot grad started at zero, but backpropagation filled it in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5714" target="_blank">01:35:14.600</a></span> | <span class="t">And then in the update, we iterated over all the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5717" target="_blank">01:35:17.680</a></span> | <span class="t">And we simply did a parameter update where every single element of our parameters was</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5723" target="_blank">01:35:23.800</a></span> | <span class="t">nudged in the opposite direction of the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5727" target="_blank">01:35:27.860</a></span> | <span class="t">And so we're going to do the exact same thing here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5731" target="_blank">01:35:31.960</a></span> | <span class="t">So I'm going to pull this up on the side here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5738" target="_blank">01:35:38.760</a></span> | <span class="t">So that we have it available.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5740" target="_blank">01:35:40.080</a></span> | <span class="t">And we're actually going to do the exact same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5742" target="_blank">01:35:42.320</a></span> | <span class="t">So this was the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5744" target="_blank">01:35:44.320</a></span> | <span class="t">So where we did this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5747" target="_blank">01:35:47.160</a></span> | <span class="t">And probs is our white bread.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5749" target="_blank">01:35:49.200</a></span> | <span class="t">So now we have to evaluate the loss, but we're not using the mean squared error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5752" target="_blank">01:35:52.600</a></span> | <span class="t">We're using the negative log likelihood because we are doing classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5755" target="_blank">01:35:55.760</a></span> | <span class="t">We're not doing regression as it's called.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5759" target="_blank">01:35:59.220</a></span> | <span class="t">So here we want to calculate loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5762" target="_blank">01:36:02.640</a></span> | <span class="t">Now the way we calculate it is just this average negative log likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5767" target="_blank">01:36:07.380</a></span> | <span class="t">Now this probs here has a shape of five by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5773" target="_blank">01:36:13.660</a></span> | <span class="t">And so to get all the, we basically want to pluck out the probabilities at the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5778" target="_blank">01:36:18.720</a></span> | <span class="t">indices here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5779" target="_blank">01:36:19.720</a></span> | <span class="t">So in particular, because the labels are stored here in the array y's, basically what we're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5785" target="_blank">01:36:25.120</a></span> | <span class="t">after is for the first example, we're looking at probability of five, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5789" target="_blank">01:36:29.800</a></span> | <span class="t">At index five.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5791" target="_blank">01:36:31.120</a></span> | <span class="t">For the second example, at the second row or row index one, we are interested in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5797" target="_blank">01:36:37.040</a></span> | <span class="t">probability assigned to index 13.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5800" target="_blank">01:36:40.520</a></span> | <span class="t">At the second example, we also have 13.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5803" target="_blank">01:36:43.640</a></span> | <span class="t">At the third row, we want one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5807" target="_blank">01:36:47.640</a></span> | <span class="t">And at the last row, which is four, we want zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5811" target="_blank">01:36:51.480</a></span> | <span class="t">So these are the probabilities we're interested in, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5814" target="_blank">01:36:54.280</a></span> | <span class="t">And you can see that they're not amazing as we saw above.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5818" target="_blank">01:36:58.880</a></span> | <span class="t">So these are the probabilities we want, but we want like a more efficient way to access</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5822" target="_blank">01:37:02.620</a></span> | <span class="t">these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5824" target="_blank">01:37:04.760</a></span> | <span class="t">Not just listing them out in a tuple like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5827" target="_blank">01:37:07.280</a></span> | <span class="t">So it turns out that the way to do this in PyTorch, one of the ways at least, is we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5831" target="_blank">01:37:11.340</a></span> | <span class="t">basically pass in all of these, sorry about that, all of these integers in a vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5842" target="_blank">01:37:22.280</a></span> | <span class="t">So these ones, you see how they're just 0, 1, 2, 3, 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5847" target="_blank">01:37:27.320</a></span> | <span class="t">We can actually create that using mp, not mp, sorry, torch.arrange(5), 0, 1, 2, 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5854" target="_blank">01:37:34.280</a></span> | <span class="t">4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5855" target="_blank">01:37:35.280</a></span> | <span class="t">So we index here with torch.arrange(5), and here we index with y's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5860" target="_blank">01:37:40.600</a></span> | <span class="t">And you see that that gives us exactly these numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5869" target="_blank">01:37:49.200</a></span> | <span class="t">So that plucks out the probabilities that the neural network assigns to the correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5874" target="_blank">01:37:54.640</a></span> | <span class="t">next character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5876" target="_blank">01:37:56.460</a></span> | <span class="t">Now we take those probabilities, and we actually look at the log probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5880" target="_blank">01:38:00.720</a></span> | <span class="t">So we want to dot log, and then we want to just average that up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5886" target="_blank">01:38:06.840</a></span> | <span class="t">So take the mean of all of that, and then it's the negative average log likelihood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5891" target="_blank">01:38:11.880</a></span> | <span class="t">that is the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5894" target="_blank">01:38:14.400</a></span> | <span class="t">So the loss here is 3.7 something, and you see that this loss, 3.76, 3.76 is exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5902" target="_blank">01:38:22.080</a></span> | <span class="t">as we've obtained before, but this is a vectorized form of that expression.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5906" target="_blank">01:38:26.640</a></span> | <span class="t">So we get the same loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5909" target="_blank">01:38:29.720</a></span> | <span class="t">And this same loss we can consider sort of as part of this forward pass, and we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5914" target="_blank">01:38:34.480</a></span> | <span class="t">achieved here now loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5915" target="_blank">01:38:35.960</a></span> | <span class="t">Okay, so we made our way all the way to loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5918" target="_blank">01:38:38.480</a></span> | <span class="t">We've defined the forward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5920" target="_blank">01:38:40.360</a></span> | <span class="t">We've forwarded the network and the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5922" target="_blank">01:38:42.280</a></span> | <span class="t">Now we're ready to do the backward pass.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5924" target="_blank">01:38:44.540</a></span> | <span class="t">So backward pass, we want to first make sure that all the gradients are reset, so they're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5931" target="_blank">01:38:51.360</a></span> | <span class="t">at zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5932" target="_blank">01:38:52.600</a></span> | <span class="t">Now in PyTorch, you can set the gradients to be zero, but you can also just set it to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5937" target="_blank">01:38:57.400</a></span> | <span class="t">none, and setting it to none is more efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5940" target="_blank">01:39:00.440</a></span> | <span class="t">And PyTorch will interpret none as like a lack of a gradient, and it's the same as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5944" target="_blank">01:39:04.840</a></span> | <span class="t">zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5946" target="_blank">01:39:06.000</a></span> | <span class="t">So this is a way to set to zero the gradient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5950" target="_blank">01:39:10.680</a></span> | <span class="t">And now we do loss.backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5955" target="_blank">01:39:15.080</a></span> | <span class="t">Before we do loss.backward, we need one more thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5957" target="_blank">01:39:17.080</a></span> | <span class="t">If you remember from micrograd, PyTorch actually requires that we pass in requires grad is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5963" target="_blank">01:39:23.480</a></span> | <span class="t">true so that we tell PyTorch that we are interested in calculating gradient for this leaf tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5971" target="_blank">01:39:31.560</a></span> | <span class="t">By default, this is false.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5973" target="_blank">01:39:33.720</a></span> | <span class="t">So let me recalculate with that, and then set to none and loss.backward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5980" target="_blank">01:39:40.920</a></span> | <span class="t">Now something magical happened when loss.backward was run, because PyTorch, just like micrograd,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5987" target="_blank">01:39:47.280</a></span> | <span class="t">when we did the forward pass here, it keeps track of all the operations under the hood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5992" target="_blank">01:39:52.520</a></span> | <span class="t">It builds a full computational graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=5995" target="_blank">01:39:55.080</a></span> | <span class="t">Just like the graphs we produced in micrograd, those graphs exist inside PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6001" target="_blank">01:40:01.020</a></span> | <span class="t">And so it knows all the dependencies and all the mathematical operations of everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6005" target="_blank">01:40:05.260</a></span> | <span class="t">And when you then calculate the loss, we can call a dot backward on it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6009" target="_blank">01:40:09.860</a></span> | <span class="t">And dot backward then fills in the gradients of all the intermediates all the way back</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6016" target="_blank">01:40:16.320</a></span> | <span class="t">to W's, which are the parameters of our neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6020" target="_blank">01:40:20.180</a></span> | <span class="t">So now we can do W.grad, and we see that it has structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6024" target="_blank">01:40:24.140</a></span> | <span class="t">There's stuff inside it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6029" target="_blank">01:40:29.400</a></span> | <span class="t">And these gradients, every single element here, so W.shape is 27 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6036" target="_blank">01:40:36.840</a></span> | <span class="t">W.grad's shape is the same, 27 by 27.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6040" target="_blank">01:40:40.920</a></span> | <span class="t">And every element of W.grad is telling us the influence of that weight on the loss function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6048" target="_blank">01:40:48.960</a></span> | <span class="t">So for example, this number all the way here, if this element, the 0,0 element of W, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6055" target="_blank">01:40:55.960</a></span> | <span class="t">the gradient is positive, it's telling us that this has a positive influence on the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6060" target="_blank">01:41:00.640</a></span> | <span class="t">loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6061" target="_blank">01:41:01.720</a></span> | <span class="t">Slightly nudging W, slightly taking W,0,0 and adding a small h to it would increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6071" target="_blank">01:41:11.240</a></span> | <span class="t">the loss mildly, because this gradient is positive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6075" target="_blank">01:41:15.940</a></span> | <span class="t">Some of these gradients are also negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6078" target="_blank">01:41:18.820</a></span> | <span class="t">So that's telling us about the gradient information, and we can use this gradient information to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6083" target="_blank">01:41:23.860</a></span> | <span class="t">update the weights of this neural network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6086" target="_blank">01:41:26.900</a></span> | <span class="t">So let's now do the update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6088" target="_blank">01:41:28.500</a></span> | <span class="t">It's going to be very similar to what we had in micrograd.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6090" target="_blank">01:41:30.960</a></span> | <span class="t">We need no loop over all the parameters, because we only have one parameter tensor, and that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6096" target="_blank">01:41:36.080</a></span> | <span class="t">is W. So we simply do W.data plus equals.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6102" target="_blank">01:41:42.360</a></span> | <span class="t">We can actually copy this almost exactly, -0.1 times W.grad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6109" target="_blank">01:41:49.780</a></span> | <span class="t">And that would be the update to the tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6114" target="_blank">01:41:54.740</a></span> | <span class="t">So that updates the tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6119" target="_blank">01:41:59.060</a></span> | <span class="t">And because the tensor is updated, we would expect that now the loss should decrease.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6124" target="_blank">01:42:04.520</a></span> | <span class="t">So here, if I print loss.item, it was 3.76, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6133" target="_blank">01:42:13.320</a></span> | <span class="t">So we've updated the W here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6136" target="_blank">01:42:16.180</a></span> | <span class="t">So if I recalculate forward pass, loss now should be slightly lower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6141" target="_blank">01:42:21.640</a></span> | <span class="t">So 3.76 goes to 3.74.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6146" target="_blank">01:42:26.040</a></span> | <span class="t">And then we can again set grad to none and backward, update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6152" target="_blank">01:42:32.820</a></span> | <span class="t">And now the parameters changed again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6155" target="_blank">01:42:35.080</a></span> | <span class="t">So if we recalculate the forward pass, we expect a lower loss again, 3.72.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6163" target="_blank">01:42:43.060</a></span> | <span class="t">And this is again doing the...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6164" target="_blank">01:42:44.580</a></span> | <span class="t">We're now doing gradient descent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6168" target="_blank">01:42:48.780</a></span> | <span class="t">And when we achieve a low loss, that will mean that the network is assigning high probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6173" target="_blank">01:42:53.720</a></span> | <span class="t">to the correct next characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6175" target="_blank">01:42:55.040</a></span> | <span class="t">Okay, so I rearranged everything and I put it all together from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6179" target="_blank">01:42:59.640</a></span> | <span class="t">So here is where we construct our data set of bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6183" target="_blank">01:43:03.480</a></span> | <span class="t">You see that we are still iterating only over the first word, Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6187" target="_blank">01:43:07.040</a></span> | <span class="t">I'm going to change that in a second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6189" target="_blank">01:43:09.120</a></span> | <span class="t">I added a number that counts the number of elements in X's so that we explicitly see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6194" target="_blank">01:43:14.800</a></span> | <span class="t">that number of examples is five, because currently we're just working with Emma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6198" target="_blank">01:43:18.800</a></span> | <span class="t">There's five bigrams there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6200" target="_blank">01:43:20.840</a></span> | <span class="t">And here I added a loop of exactly what we had before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6203" target="_blank">01:43:23.920</a></span> | <span class="t">So we had 10 iterations of gradient descent of forward pass, backward pass, and an update.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6209" target="_blank">01:43:29.180</a></span> | <span class="t">And so running these two cells, initialization and gradient descent, gives us some improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6215" target="_blank">01:43:35.360</a></span> | <span class="t">on the last function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6218" target="_blank">01:43:38.360</a></span> | <span class="t">But now I want to use all the words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6222" target="_blank">01:43:42.000</a></span> | <span class="t">And there's not five, but 228,000 bigrams now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6226" target="_blank">01:43:46.520</a></span> | <span class="t">However, this should require no modification whatsoever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6229" target="_blank">01:43:49.780</a></span> | <span class="t">Everything should just run, because all the code we wrote doesn't care if there's five</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6233" target="_blank">01:43:53.040</a></span> | <span class="t">bigrams or 228,000 bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6236" target="_blank">01:43:56.040</a></span> | <span class="t">And with everything, it should just work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6237" target="_blank">01:43:57.480</a></span> | <span class="t">So you see that this will just run.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6240" target="_blank">01:44:00.080</a></span> | <span class="t">But now we are optimizing over the entire training set of all the bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6244" target="_blank">01:44:04.920</a></span> | <span class="t">And you see now that we are decreasing very slightly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6247" target="_blank">01:44:07.700</a></span> | <span class="t">So actually, we can probably afford a larger learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6251" target="_blank">01:44:11.080</a></span> | <span class="t">We can probably afford an even larger learning rate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6260" target="_blank">01:44:20.980</a></span> | <span class="t">Even 50 seems to work on this very, very simple example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6264" target="_blank">01:44:24.140</a></span> | <span class="t">So let me reinitialize, and let's run 100 iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6269" target="_blank">01:44:29.520</a></span> | <span class="t">See what happens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6276" target="_blank">01:44:36.440</a></span> | <span class="t">We seem to be coming up to some pretty good losses here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6281" target="_blank">01:44:41.360</a></span> | <span class="t">2.47.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6282" target="_blank">01:44:42.360</a></span> | <span class="t">Let me run 100 more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6284" target="_blank">01:44:44.760</a></span> | <span class="t">What is the number that we expect, by the way, in the loss?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6287" target="_blank">01:44:47.440</a></span> | <span class="t">We expect to get something around what we had originally, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6292" target="_blank">01:44:52.200</a></span> | <span class="t">So all the way back, if you remember, in the beginning of this video, when we optimized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6297" target="_blank">01:44:57.480</a></span> | <span class="t">just by counting, our loss was roughly 2.47 after we added smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6303" target="_blank">01:45:03.720</a></span> | <span class="t">But before smoothing, we had roughly 2.45 likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6307" target="_blank">01:45:07.160</a></span> | <span class="t">Sorry, loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6309" target="_blank">01:45:09.920</a></span> | <span class="t">And so that's actually roughly the vicinity of what we expect to achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6314" target="_blank">01:45:14.000</a></span> | <span class="t">But before, we achieved it by counting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6316" target="_blank">01:45:16.060</a></span> | <span class="t">And here, we are achieving roughly the same result, but with gradient-based optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6321" target="_blank">01:45:21.160</a></span> | <span class="t">So we come to about 2.46, 2.45, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6326" target="_blank">01:45:26.500</a></span> | <span class="t">And that makes sense, because fundamentally, we're not taking in any additional information.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6330" target="_blank">01:45:30.080</a></span> | <span class="t">We're still just taking in the previous character and trying to predict the next one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6333" target="_blank">01:45:33.840</a></span> | <span class="t">But instead of doing it explicitly by counting and normalizing, we are doing it with gradient-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6339" target="_blank">01:45:39.360</a></span> | <span class="t">learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6340" target="_blank">01:45:40.360</a></span> | <span class="t">And it just so happens that the explicit approach happens to very well optimize the loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6345" target="_blank">01:45:45.620</a></span> | <span class="t">without any need for gradient-based optimization, because the setup for bigram language models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6350" target="_blank">01:45:50.520</a></span> | <span class="t">is so straightforward, it's so simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6353" target="_blank">01:45:53.040</a></span> | <span class="t">We can just afford to estimate those probabilities directly and maintain them in a table.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6359" target="_blank">01:45:59.080</a></span> | <span class="t">But the gradient-based approach is significantly more flexible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6363" target="_blank">01:46:03.080</a></span> | <span class="t">So we've actually gained a lot, because what we can do now is we can expand this approach</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6371" target="_blank">01:46:11.000</a></span> | <span class="t">and complexify the neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6373" target="_blank">01:46:13.000</a></span> | <span class="t">So currently, we're just taking a single character and feeding it into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6376" target="_blank">01:46:16.060</a></span> | <span class="t">And the neural net is extremely simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6377" target="_blank">01:46:17.980</a></span> | <span class="t">But we're about to iterate on this substantially.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6380" target="_blank">01:46:20.600</a></span> | <span class="t">We're going to be taking multiple previous characters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6383" target="_blank">01:46:23.520</a></span> | <span class="t">And we're going to be feeding them into increasingly more complex neural nets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6387" target="_blank">01:46:27.640</a></span> | <span class="t">But fundamentally, the output of the neural net will always just be logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6392" target="_blank">01:46:32.880</a></span> | <span class="t">And those logits will go through the exact same transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6395" target="_blank">01:46:35.760</a></span> | <span class="t">We're going to take them through a softmax, calculate the loss function and the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6399" target="_blank">01:46:39.920</a></span> | <span class="t">log likelihood, and do gradient-based optimization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6403" target="_blank">01:46:43.780</a></span> | <span class="t">And so actually, as we complexify the neural nets and work all the way up to transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6409" target="_blank">01:46:49.920</a></span> | <span class="t">none of this will really fundamentally change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6412" target="_blank">01:46:52.120</a></span> | <span class="t">None of this will fundamentally change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6413" target="_blank">01:46:53.760</a></span> | <span class="t">The only thing that will change is the way we do the forward pass, where we take in some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6418" target="_blank">01:46:58.200</a></span> | <span class="t">previous characters and calculate logits for the next character in the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6423" target="_blank">01:47:03.080</a></span> | <span class="t">That will become more complex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6425" target="_blank">01:47:05.440</a></span> | <span class="t">And we'll use the same machinery to optimize it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6429" target="_blank">01:47:09.160</a></span> | <span class="t">And it's not obvious how we would have extended this bigram approach into the case where there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6436" target="_blank">01:47:16.560</a></span> | <span class="t">are many more characters at the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6439" target="_blank">01:47:19.440</a></span> | <span class="t">Because eventually, these tables would get way too large, because there's way too many</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6443" target="_blank">01:47:23.120</a></span> | <span class="t">combinations of what previous characters could be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6448" target="_blank">01:47:28.000</a></span> | <span class="t">If you only have one previous character, we can just keep everything in a table that counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6452" target="_blank">01:47:32.240</a></span> | <span class="t">But if you have the last 10 characters that are input, we can't actually keep everything</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6456" target="_blank">01:47:36.320</a></span> | <span class="t">in the table anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6457" target="_blank">01:47:37.580</a></span> | <span class="t">So this is fundamentally an unscalable approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6460" target="_blank">01:47:40.020</a></span> | <span class="t">And the neural network approach is significantly more scalable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6463" target="_blank">01:47:43.320</a></span> | <span class="t">And it's something that actually we can improve on over time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6466" target="_blank">01:47:46.920</a></span> | <span class="t">So that's where we will be digging next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6468" target="_blank">01:47:48.560</a></span> | <span class="t">I wanted to point out two more things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6471" target="_blank">01:47:51.360</a></span> | <span class="t">Number one, I want you to notice that this xnk here, this is made up of one-hot vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6479" target="_blank">01:47:59.160</a></span> | <span class="t">And then those one-hot vectors are multiplied by this W matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6483" target="_blank">01:48:03.400</a></span> | <span class="t">And we think of this as multiple neurons being forwarded in a fully connected manner.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6488" target="_blank">01:48:08.880</a></span> | <span class="t">But actually what's happening here is that, for example, if you have a one-hot vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6493" target="_blank">01:48:13.600</a></span> | <span class="t">here that has a 1 at, say, the fifth dimension, then because of the way the matrix multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6499" target="_blank">01:48:19.800</a></span> | <span class="t">works, multiplying that one-hot vector with W actually ends up plucking out the fifth</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6505" target="_blank">01:48:25.560</a></span> | <span class="t">row of W. Logits would become just the fifth row of W. And that's because of the way the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6512" target="_blank">01:48:32.720</a></span> | <span class="t">matrix multiplication works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6517" target="_blank">01:48:37.080</a></span> | <span class="t">So that's actually what ends up happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6520" target="_blank">01:48:40.760</a></span> | <span class="t">But that's actually exactly what happened before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6523" target="_blank">01:48:43.540</a></span> | <span class="t">Because remember, all the way up here, we have a bigram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6527" target="_blank">01:48:47.720</a></span> | <span class="t">We took the first character, and then that first character indexed into a row of this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6533" target="_blank">01:48:53.460</a></span> | <span class="t">array here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6535" target="_blank">01:48:55.160</a></span> | <span class="t">And that row gave us the probability distribution for the next character.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6538" target="_blank">01:48:58.820</a></span> | <span class="t">So the first character was used as a lookup into a matrix here to get the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6545" target="_blank">01:49:05.120</a></span> | <span class="t">distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6546" target="_blank">01:49:06.120</a></span> | <span class="t">Well, that's actually exactly what's happening here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6548" target="_blank">01:49:08.800</a></span> | <span class="t">Because we're taking the index, we're encoding it as one-hot, and multiplying it by W. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6553" target="_blank">01:49:13.680</a></span> | <span class="t">Logits literally becomes the appropriate row of W. And that gets, just as before, exponentiated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6563" target="_blank">01:49:23.360</a></span> | <span class="t">to create the counts, and then normalized and becomes probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6567" target="_blank">01:49:27.640</a></span> | <span class="t">So this W here is literally the same as this array here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6575" target="_blank">01:49:35.320</a></span> | <span class="t">But W, remember, is the log counts, not the counts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6579" target="_blank">01:49:39.080</a></span> | <span class="t">So it's more precise to say that W exponentiated, W.exp, is this array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6586" target="_blank">01:49:46.440</a></span> | <span class="t">But this array was filled in by counting, and by basically populating the counts of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6593" target="_blank">01:49:53.480</a></span> | <span class="t">bigrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6594" target="_blank">01:49:54.480</a></span> | <span class="t">So in the gradient-based framework, we initialize it randomly, and then we let the loss guide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6599" target="_blank">01:49:59.640</a></span> | <span class="t">us to arrive at the exact same array.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6603" target="_blank">01:50:03.400</a></span> | <span class="t">So this array exactly here is basically the array W at the end of optimization, except</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6610" target="_blank">01:50:10.560</a></span> | <span class="t">we arrived at it piece by piece by following the loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6615" target="_blank">01:50:15.240</a></span> | <span class="t">And that's why we also obtain the same loss function at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6618" target="_blank">01:50:18.160</a></span> | <span class="t">And the second note is, if I come here, remember the smoothing where we added fake counts to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6623" target="_blank">01:50:23.840</a></span> | <span class="t">our counts in order to smooth out and make more uniform the distributions of these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6631" target="_blank">01:50:31.280</a></span> | <span class="t">And that prevented us from assigning zero probability to any one bigram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6637" target="_blank">01:50:37.360</a></span> | <span class="t">Now if I increase the count here, what's happening to the probability?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6643" target="_blank">01:50:43.040</a></span> | <span class="t">As I increase the count, probability becomes more and more uniform, because these counts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6649" target="_blank">01:50:49.520</a></span> | <span class="t">go only up to like 900 or whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6651" target="_blank">01:50:51.680</a></span> | <span class="t">So if I'm adding plus a million to every single number here, you can see how the row and its</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6657" target="_blank">01:50:57.480</a></span> | <span class="t">probability then, when we divide, is just going to become more and more close to exactly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6661" target="_blank">01:51:01.520</a></span> | <span class="t">even probability, uniform distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6665" target="_blank">01:51:05.400</a></span> | <span class="t">It turns out that the gradient-based framework has an equivalent to smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6670" target="_blank">01:51:10.960</a></span> | <span class="t">In particular, think through these W's here, which we initialized randomly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6678" target="_blank">01:51:18.760</a></span> | <span class="t">We could also think about initializing W's to be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6682" target="_blank">01:51:22.320</a></span> | <span class="t">If all the entries of W are zero, then you'll see that logits will become all zero, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6689" target="_blank">01:51:29.080</a></span> | <span class="t">then exponentiating those logits becomes all one, and then the probabilities turn out to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6693" target="_blank">01:51:33.920</a></span> | <span class="t">be exactly uniform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6695" target="_blank">01:51:35.980</a></span> | <span class="t">So basically when W's are all equal to each other, or say especially zero, then the probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6702" target="_blank">01:51:42.440</a></span> | <span class="t">come out completely uniform.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6704" target="_blank">01:51:44.640</a></span> | <span class="t">So trying to incentivize W to be near zero is basically equivalent to label smoothing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6713" target="_blank">01:51:53.120</a></span> | <span class="t">And the more you incentivize that in a loss function, the more smooth distribution you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6717" target="_blank">01:51:57.480</a></span> | <span class="t">going to achieve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6719" target="_blank">01:51:59.140</a></span> | <span class="t">So this brings us to something that's called regularization, where we can actually augment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6723" target="_blank">01:52:03.520</a></span> | <span class="t">the loss function to have a small component that we call a regularization loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6727" target="_blank">01:52:07.960</a></span> | <span class="t">In particular, what we're going to do is we can take W, and we can, for example, square</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6732" target="_blank">01:52:12.800</a></span> | <span class="t">all of its entries, and then we can take all the entries of W and we can sum them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6743" target="_blank">01:52:23.880</a></span> | <span class="t">And because we're squaring, there will be no signs anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6748" target="_blank">01:52:28.680</a></span> | <span class="t">Negatives and positives all get squashed to be positive numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6751" target="_blank">01:52:31.760</a></span> | <span class="t">And then the way this works is you achieve zero loss if W is exactly or zero, but if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6757" target="_blank">01:52:37.480</a></span> | <span class="t">W has non-zero numbers, you accumulate loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6761" target="_blank">01:52:41.420</a></span> | <span class="t">And so we can actually take this and we can add it on here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6765" target="_blank">01:52:45.080</a></span> | <span class="t">So we can do something like loss plus W square dot sum, or let's actually instead of sum,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6773" target="_blank">01:52:53.640</a></span> | <span class="t">let's take a mean, because otherwise the sum gets too large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6777" target="_blank">01:52:57.720</a></span> | <span class="t">So mean is like a little bit more manageable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6781" target="_blank">01:53:01.560</a></span> | <span class="t">And then we have a regularization loss here, like say 0.01 times or something like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6786" target="_blank">01:53:06.480</a></span> | <span class="t">You can choose the regularization strength.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6789" target="_blank">01:53:09.560</a></span> | <span class="t">And then we can just optimize this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6792" target="_blank">01:53:12.280</a></span> | <span class="t">And now this optimization actually has two components.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6795" target="_blank">01:53:15.480</a></span> | <span class="t">Not only is it trying to make all the probabilities work out, but in addition to that, there's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6799" target="_blank">01:53:19.440</a></span> | <span class="t">an additional component that simultaneously tries to make all W's be zero, because if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6804" target="_blank">01:53:24.240</a></span> | <span class="t">W's are non-zero, you feel a loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6806" target="_blank">01:53:26.400</a></span> | <span class="t">And so minimizing this, the only way to achieve that is for W to be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6810" target="_blank">01:53:30.840</a></span> | <span class="t">And so you can think of this as adding like a spring force or like a gravity force that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6816" target="_blank">01:53:36.080</a></span> | <span class="t">pushes W to be zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6818" target="_blank">01:53:38.000</a></span> | <span class="t">So W wants to be zero and the probabilities want to be uniform, but they also simultaneously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6822" target="_blank">01:53:42.720</a></span> | <span class="t">want to match up your probabilities as indicated by the data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6827" target="_blank">01:53:47.600</a></span> | <span class="t">And so the strength of this regularization is exactly controlling the amount of counts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6834" target="_blank">01:53:54.480</a></span> | <span class="t">that you add here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6837" target="_blank">01:53:57.520</a></span> | <span class="t">Adding a lot more counts here corresponds to increasing this number, because the more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6845" target="_blank">01:54:05.200</a></span> | <span class="t">you increase it, the more this part of the loss function dominates this part, and the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6849" target="_blank">01:54:09.840</a></span> | <span class="t">more these weights will be unable to grow, because as they grow, they accumulate way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6856" target="_blank">01:54:16.640</a></span> | <span class="t">too much loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6858" target="_blank">01:54:18.600</a></span> | <span class="t">And so if this is strong enough, then we are not able to overcome the force of this loss,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6865" target="_blank">01:54:25.040</a></span> | <span class="t">and we will never... and basically everything will be uniform predictions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6869" target="_blank">01:54:29.580</a></span> | <span class="t">So I thought that's kind of cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6870" target="_blank">01:54:30.880</a></span> | <span class="t">Okay, and lastly, before we wrap up, I wanted to show you how you would sample from this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6875" target="_blank">01:54:35.060</a></span> | <span class="t">neural net model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6877" target="_blank">01:54:37.160</a></span> | <span class="t">And I copy-pasted the sampling code from before, where remember that we sampled five times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6884" target="_blank">01:54:44.560</a></span> | <span class="t">and all we did is we started at zero, we grabbed the current ix row of p, and that was our</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6890" target="_blank">01:54:50.800</a></span> | <span class="t">probability row, from which we sampled the next index and just accumulated that and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6896" target="_blank">01:54:56.800</a></span> | <span class="t">break when zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6899" target="_blank">01:54:59.160</a></span> | <span class="t">And running this gave us these results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6903" target="_blank">01:55:03.960</a></span> | <span class="t">I still have the p in memory, so this is fine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6907" target="_blank">01:55:07.720</a></span> | <span class="t">Now this p doesn't come from the row of p, instead it comes from this neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6915" target="_blank">01:55:15.280</a></span> | <span class="t">First we take ix, and we encode it into a one-hot row of xnc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6922" target="_blank">01:55:22.640</a></span> | <span class="t">This xnc multiplies our w, which really just plucks out the row of w corresponding to ix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6929" target="_blank">01:55:29.240</a></span> | <span class="t">really that's what's happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6930" target="_blank">01:55:30.660</a></span> | <span class="t">And then that gets our logits, and then we normalize those logits, exponentiate to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6935" target="_blank">01:55:35.600</a></span> | <span class="t">counts, and then normalize to get the distribution, and then we can sample from the distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6941" target="_blank">01:55:41.560</a></span> | <span class="t">So if I run this, kind of anticlimactic or climatic, depending how you look at it, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6948" target="_blank">01:55:48.800</a></span> | <span class="t">we get the exact same result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6952" target="_blank">01:55:52.480</a></span> | <span class="t">And that's because this is the identical model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6954" target="_blank">01:55:54.880</a></span> | <span class="t">Not only does it achieve the same loss, but as I mentioned, these are identical models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6960" target="_blank">01:56:00.040</a></span> | <span class="t">and this w is the log counts of what we've estimated before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6964" target="_blank">01:56:04.440</a></span> | <span class="t">But we came to this answer in a very different way, and it's got a very different interpretation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6969" target="_blank">01:56:09.680</a></span> | <span class="t">But fundamentally this is basically the same model, and gives the same samples here, and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6973" target="_blank">01:56:13.440</a></span> | <span class="t">so that's kind of cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6975" target="_blank">01:56:15.640</a></span> | <span class="t">Okay, so we've actually covered a lot of ground.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6978" target="_blank">01:56:18.200</a></span> | <span class="t">We introduced the bigram character-level language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6982" target="_blank">01:56:22.160</a></span> | <span class="t">We saw how we can train the model, how we can sample from the model, and how we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6986" target="_blank">01:56:26.000</a></span> | <span class="t">evaluate the quality of the model using the negative log likelihood loss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6990" target="_blank">01:56:30.440</a></span> | <span class="t">And then we actually trained the model in two completely different ways that actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6993" target="_blank">01:56:33.840</a></span> | <span class="t">get the same result and the same model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=6996" target="_blank">01:56:36.480</a></span> | <span class="t">In the first way, we just counted up the frequency of all the bigrams and normalized.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7001" target="_blank">01:56:41.680</a></span> | <span class="t">In the second way, we used the negative log likelihood loss as a guide to optimizing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7008" target="_blank">01:56:48.920</a></span> | <span class="t">counts matrix, or the counts array, so that the loss is minimized in a gradient-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7015" target="_blank">01:56:55.200</a></span> | <span class="t">framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7016" target="_blank">01:56:56.200</a></span> | <span class="t">And we saw that both of them give the same result, and that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7021" target="_blank">01:57:01.600</a></span> | <span class="t">Now the second one of these, the gradient-based framework, is much more flexible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7025" target="_blank">01:57:05.160</a></span> | <span class="t">And right now our neural network is super simple.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7027" target="_blank">01:57:07.760</a></span> | <span class="t">We're taking a single previous character, and we're taking it through a single linear</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7031" target="_blank">01:57:11.880</a></span> | <span class="t">layer to calculate the logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7034" target="_blank">01:57:14.320</a></span> | <span class="t">This is about to complexify.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7036" target="_blank">01:57:16.080</a></span> | <span class="t">So in the follow-up videos, we're going to be taking more and more of these characters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7040" target="_blank">01:57:20.800</a></span> | <span class="t">and we're going to be feeding them into a neural net.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7043" target="_blank">01:57:23.140</a></span> | <span class="t">But this neural net will still output the exact same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7045" target="_blank">01:57:25.520</a></span> | <span class="t">The neural net will output logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7048" target="_blank">01:57:28.320</a></span> | <span class="t">And these logits will still be normalized in the exact same way, and all the loss and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7051" target="_blank">01:57:31.600</a></span> | <span class="t">everything else in the gradient-based framework, everything stays identical.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7055" target="_blank">01:57:35.640</a></span> | <span class="t">It's just that this neural net will now complexify all the way to transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7060" target="_blank">01:57:40.680</a></span> | <span class="t">So that's going to be pretty awesome, and I'm looking forward to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=PaCmpygFfXo&t=7063" target="_blank">01:57:43.600</a></span> | <span class="t">For now, bye.</span></div></div></body></html>