<html><head><title>Stanford CS224N NLP with Deep Learning | 2023 | Lecture 11 - Natural Language Generation</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Stanford CS224N NLP with Deep Learning | 2023 | Lecture 11 - Natural Language Generation</h2><a href="https://www.youtube.com/watch?v=N9L32bFieEY"><img src="https://i.ytimg.com/vi/N9L32bFieEY/maxresdefault.jpg" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./N9L32bFieEY.html">Whisper Transcript</a> | <a href="./transcript_N9L32bFieEY.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello everyone, my name is Lisa. I'm a third year PhD student in the NLP group. I'm advised</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=11" target="_blank">00:00:11.080</a></span> | <span class="t">by Percy and Tatsu. Today I will give a lecture on natural language generation. And this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=17" target="_blank">00:00:17.080</a></span> | <span class="t">also the research area that I work on. So I'm super excited about it. I'm happy to answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=21" target="_blank">00:00:21.400</a></span> | <span class="t">any questions both during the lecture and after class about natural language generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=26" target="_blank">00:00:26.580</a></span> | <span class="t">So NLG is a super exciting area and it's also moving really, really fast. So today we will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=32" target="_blank">00:00:32.840</a></span> | <span class="t">discuss all the excitement of NLG. But before we get into the really exciting part, I have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=39" target="_blank">00:00:39.120</a></span> | <span class="t">to make some announcements. So first, it is very, very important for you to remember to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=44" target="_blank">00:00:44.060</a></span> | <span class="t">sign up for AWS by midnight today. So this is related to our homework five, whether you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=51" target="_blank">00:00:51.140</a></span> | <span class="t">have GPU access and then also related to our final project. So please, please remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=56" target="_blank">00:00:56.180</a></span> | <span class="t">to sign up for AWS by tonight. And second, the project proposal is due on Tuesday, next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=64" target="_blank">00:01:04.300</a></span> | <span class="t">Tuesday. And I think assignment four should just due. Hopefully you had fun with machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=70" target="_blank">00:01:10.660</a></span> | <span class="t">translation and stuff. And also assignment five is out today, I think just now. And it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=77" target="_blank">00:01:17.420</a></span> | <span class="t">is due on Friday, basically Friday midnight. And last, we will hold a Hugging Face Transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=87" target="_blank">00:01:27.260</a></span> | <span class="t">Library tutorial this Friday. So if your final project is related to implementing transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=93" target="_blank">00:01:33.820</a></span> | <span class="t">or playing with large language models, you should definitely go to this tutorial because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=97" target="_blank">00:01:37.260</a></span> | <span class="t">it's going to be very, very helpful. Also, yeah, just one more time, please remember</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=102" target="_blank">00:01:42.180</a></span> | <span class="t">to sign up for AWS because this is the final hard deadline. Okay, cool. Now moving on to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=109" target="_blank">00:01:49.580</a></span> | <span class="t">the main topic for today, the very exciting natural language generation stuff. So today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=114" target="_blank">00:01:54.700</a></span> | <span class="t">we will discuss what is NLG, review some models, discuss about how to decode from language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=120" target="_blank">00:02:00.620</a></span> | <span class="t">models and how to train language models. And we will also talk about evaluations. And finally,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=126" target="_blank">00:02:06.780</a></span> | <span class="t">we'll discuss ethical and risk considerations with the current NLG systems. So this natural</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=132" target="_blank">00:02:12.500</a></span> | <span class="t">language generation techniques are going to be really exciting because this is kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=137" target="_blank">00:02:17.260</a></span> | <span class="t">getting us closer to explain the magic of chatGPT, which is a super popular model recently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=142" target="_blank">00:02:22.860</a></span> | <span class="t">And practically speaking, they could also help you with your final project if you decide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=146" target="_blank">00:02:26.700</a></span> | <span class="t">to work on something related to text generation. So let's get started. To begin with, let's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=152" target="_blank">00:02:32.660</a></span> | <span class="t">ask the question of what is natural language generation. So natural language generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=157" target="_blank">00:02:37.940</a></span> | <span class="t">is actually a really broad category. People have divided NLP into natural language understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=164" target="_blank">00:02:44.180</a></span> | <span class="t">and natural language generation. So the understanding part mostly means that the task input is in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=169" target="_blank">00:02:49.900</a></span> | <span class="t">natural language, such as semantic parsing, natural language inference, and so on. Whereas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=176" target="_blank">00:02:56.260</a></span> | <span class="t">natural language generation means that the task output is in natural language. So NLG</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=182" target="_blank">00:03:02.780</a></span> | <span class="t">focuses on systems that produce fluent, coherent, and useful language outputs for human to use.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=189" target="_blank">00:03:09.700</a></span> | <span class="t">Historically, there are many NLG systems that use rule-based systems, such as templates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=195" target="_blank">00:03:15.820</a></span> | <span class="t">or infilling. But nowadays, deep learning is powering almost every text generation systems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=202" target="_blank">00:03:22.460</a></span> | <span class="t">So this lecture today will be mostly focused on deep learning stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=207" target="_blank">00:03:27.940</a></span> | <span class="t">So first, what are some examples of natural language generation? It's actually everywhere,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=213" target="_blank">00:03:33.140</a></span> | <span class="t">including our homework. Machine translation is a form of NLG, where the input is some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=218" target="_blank">00:03:38.460</a></span> | <span class="t">utterance in the source language, and the output is generated text in the target language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=224" target="_blank">00:03:44.500</a></span> | <span class="t">Digital assistant, such as Ceres or Alexa, they are also NLG systems. So it takes in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=230" target="_blank">00:03:50.140</a></span> | <span class="t">dialogue history and generates continuations of the conversation. There is also summarization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=236" target="_blank">00:03:56.620</a></span> | <span class="t">systems that takes in a long document, such as a research article, and then the idea is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=242" target="_blank">00:04:02.300</a></span> | <span class="t">trying to summarize it into a few sentences that are easy to read.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=247" target="_blank">00:04:07.460</a></span> | <span class="t">So beyond these classic tasks, there are some more interesting uses, like creative storywriting,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=253" target="_blank">00:04:13.500</a></span> | <span class="t">where you can prompt a language model with a story plot, and then it will give you some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=257" target="_blank">00:04:17.860</a></span> | <span class="t">creative stories that are aligned with the plot. There is data to text, where you give</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=262" target="_blank">00:04:22.660</a></span> | <span class="t">the language model some database or some tables, and then the idea is that it will output some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=267" target="_blank">00:04:27.980</a></span> | <span class="t">textual description of the table content. And finally, there is also visual description-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=273" target="_blank">00:04:33.540</a></span> | <span class="t">NLG systems, like image captioning or image-based storytelling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=280" target="_blank">00:04:40.380</a></span> | <span class="t">So the really cool example is the popular ChatGPT models. So ChatGPT is also an NLG</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=288" target="_blank">00:04:48.500</a></span> | <span class="t">system. It is very general purpose, so therefore you can use it to do many different tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=294" target="_blank">00:04:54.860</a></span> | <span class="t">with different prompts. For example, we can use ChatGPT to simulate a chatbot. It can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=301" target="_blank">00:05:01.220</a></span> | <span class="t">answer questions about creative GIFs for 10 years old. It can be used to do poetry generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=308" target="_blank">00:05:08.820</a></span> | <span class="t">For example, we can ask it to generate a poem about sorting algorithms. And it's actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=314" target="_blank">00:05:14.100</a></span> | <span class="t">well, I wouldn't say it's very poetic, but at least it has the same format as a poem,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=318" target="_blank">00:05:18.540</a></span> | <span class="t">and the content is actually correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=322" target="_blank">00:05:22.740</a></span> | <span class="t">So ChatGPT can also be used in some really useful settings, like web search. So here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=330" target="_blank">00:05:30.180</a></span> | <span class="t">Bing is augmented with ChatGPT, and there are some Twitters that are saying that the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=334" target="_blank">00:05:34.100</a></span> | <span class="t">magic of ChatGPT is that it actually makes people be happy to use Bing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=342" target="_blank">00:05:42.700</a></span> | <span class="t">So there are so many tasks that actually belong to the NLG category. So how do we categorize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=347" target="_blank">00:05:47.380</a></span> | <span class="t">these tasks? One common way is to think about the open-endedness of the task. So here, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=353" target="_blank">00:05:53.100</a></span> | <span class="t">draw a line for the spectrum of open-endedness. On the one end, we have tasks like machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=358" target="_blank">00:05:58.820</a></span> | <span class="t">translation and summarization. So we consider them not very open-ended, because for each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=364" target="_blank">00:06:04.500</a></span> | <span class="t">source sentence, the output is almost determined by the input. Because basically, we are trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=370" target="_blank">00:06:10.580</a></span> | <span class="t">to do machine translation, the semantics should be exactly similar to the input sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=375" target="_blank">00:06:15.540</a></span> | <span class="t">So there are only a few ways that you can rephrase the output, like authorities have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=379" target="_blank">00:06:19.500</a></span> | <span class="t">announced that today is a national holiday. You can rephrase it a little bit to say, today</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=384" target="_blank">00:06:24.060</a></span> | <span class="t">is a national holiday announced by the authorities. But the actual space is really small, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=389" target="_blank">00:06:29.500</a></span> | <span class="t">you have to make sure the semantics doesn't change. So we can say that the output space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=394" target="_blank">00:06:34.140</a></span> | <span class="t">here is not very diverse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=397" target="_blank">00:06:37.900</a></span> | <span class="t">And moving to the middle of the spectrum, there is dialogue tasks, such as task-driven</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=402" target="_blank">00:06:42.260</a></span> | <span class="t">dialogue or a chitchat dialogue. So we can see that for each dialogue input, there are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=407" target="_blank">00:06:47.100</a></span> | <span class="t">multiple responses, and the degree of freedom has increased. Here, we can respond by saying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=413" target="_blank">00:06:53.580</a></span> | <span class="t">good and you, or we can say about, thanks for asking, barely surviving all my homeworks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=419" target="_blank">00:06:59.980</a></span> | <span class="t">So here, we are observing that there are actually multiple ways to continue this conversation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=424" target="_blank">00:07:04.780</a></span> | <span class="t">And then this is where we say the output space is getting more and more diverse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=429" target="_blank">00:07:09.900</a></span> | <span class="t">And on the other end of the spectrum, there is the very open-ended generation tasks, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=435" target="_blank">00:07:15.020</a></span> | <span class="t">story generation. So given the input, like write me a story about three little pigs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=440" target="_blank">00:07:20.060</a></span> | <span class="t">there are so many ways to continue the prompt. We can write about them going to schools,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=444" target="_blank">00:07:24.300</a></span> | <span class="t">building houses, like they always do. So the valid output here is extremely large. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=450" target="_blank">00:07:30.300</a></span> | <span class="t">we call this open-ended generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=453" target="_blank">00:07:33.940</a></span> | <span class="t">So it's hard to really draw a boundary between open-ended and non-open-ended tasks. But we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=458" target="_blank">00:07:38.820</a></span> | <span class="t">still try to give a rough categorization. So open-ended generation refers to tasks whose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=464" target="_blank">00:07:44.140</a></span> | <span class="t">output distribution has a high degree of freedom. Or non-open-ended generation tasks refers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=470" target="_blank">00:07:50.580</a></span> | <span class="t">to tasks where the input will almost certainly determine the output generation. Examples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=477" target="_blank">00:07:57.260</a></span> | <span class="t">of non-open-ended generations are machine translation, summarization. And examples of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=481" target="_blank">00:08:01.940</a></span> | <span class="t">open-ended generations are story generation, chitchat dialogue, task-oriented dialogue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=487" target="_blank">00:08:07.100</a></span> | <span class="t">et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=488" target="_blank">00:08:08.060</a></span> | <span class="t">So how do we formalize this categorization? One way of formalizing is by computing the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=493" target="_blank">00:08:13.500</a></span> | <span class="t">entropy of the NLG system. So high entropy means that we are to the right of the spectrum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=500" target="_blank">00:08:20.220</a></span> | <span class="t">So it is more open-ended. And low entropy means that we are to the left of the spectrum</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=505" target="_blank">00:08:25.420</a></span> | <span class="t">and less open-ended. So these two classes of NLG tasks actually require different decoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=512" target="_blank">00:08:32.020</a></span> | <span class="t">and training approaches, as we will talk about later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=515" target="_blank">00:08:35.420</a></span> | <span class="t">OK, cool. Now let's recall some previous lectures and review the NLG models and trainings that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=522" target="_blank">00:08:42.260</a></span> | <span class="t">we have studied before. So I think we discussed the basics of natural language generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=528" target="_blank">00:08:48.780</a></span> | <span class="t">So here is how autoregressive language model works. At each time step, our model would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=533" target="_blank">00:08:53.420</a></span> | <span class="t">take in a sequence of tokens as input. And here it is y less than t. And the output is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=540" target="_blank">00:09:00.540</a></span> | <span class="t">basically the new token yt. So to decide on yt, we first use the model to assign a score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=546" target="_blank">00:09:06.980</a></span> | <span class="t">for each token in the vocabulary, denoted as s. And then we apply softmax to get the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=552" target="_blank">00:09:12.740</a></span> | <span class="t">next token distribution, p. And we choose a token according to this next token distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=559" target="_blank">00:09:19.300</a></span> | <span class="t">And similarly, once we have predicted yt hat, we then pass it back into the language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=563" target="_blank">00:09:23.300</a></span> | <span class="t">as the input, predict y hat t plus 1. And then we do so recursively until we reach the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=569" target="_blank">00:09:29.660</a></span> | <span class="t">end of the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=572" target="_blank">00:09:32.020</a></span> | <span class="t">So any questions so far? OK, good. So for the two types of NLG tasks that we talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=580" target="_blank">00:09:40.620</a></span> | <span class="t">about, like the open-ended and non-open-ended tasks, they tend to prefer different model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=585" target="_blank">00:09:45.140</a></span> | <span class="t">architectures. So for non-open-ended tasks, like machine translation, we typically use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=590" target="_blank">00:09:50.660</a></span> | <span class="t">an encoder-decoder system, where the autoregressive decoder that we just talked about functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=596" target="_blank">00:09:56.060</a></span> | <span class="t">as the decoder. And then we have another bidirectional encoder for encoding the inputs. So this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=601" target="_blank">00:10:01.260</a></span> | <span class="t">kind of what you implemented for assignment 4, because the encoder is like the bidirectional</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=607" target="_blank">00:10:07.140</a></span> | <span class="t">LSTM, and the decoder is another LSTM that is autoregressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=612" target="_blank">00:10:12.660</a></span> | <span class="t">So for more open-ended tasks, typically autoregressive generation model is the only component. Of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=621" target="_blank">00:10:21.180</a></span> | <span class="t">course, these architectures are not really hard constraints, because an autoregressive</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=625" target="_blank">00:10:25.860</a></span> | <span class="t">decoder alone can also be used to do machine translation. And an encoder-decoder model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=630" target="_blank">00:10:30.660</a></span> | <span class="t">can also be used for storage generation. So this is kind of the convention for now, but</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=635" target="_blank">00:10:35.980</a></span> | <span class="t">it's a reasonable convention, because using decoder-only model for MT tends to hurt performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=642" target="_blank">00:10:42.180</a></span> | <span class="t">compared to an encoder-decoder model for MT. And using an encoder-decoder model for open-ended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=647" target="_blank">00:10:47.500</a></span> | <span class="t">generation seems to achieve similar performance to a decoder-only model. And therefore, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=653" target="_blank">00:10:53.260</a></span> | <span class="t">you have the compute budget to train an encoder-decoder model, you might just be better off by only</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=657" target="_blank">00:10:57.900</a></span> | <span class="t">training a larger decoder model. So it's kind of more of an allocation of resources problem</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=662" target="_blank">00:11:02.740</a></span> | <span class="t">than whether this architecture will type check with your task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=668" target="_blank">00:11:08.700</a></span> | <span class="t">So how do we train such a language model? In previous lectures, we talked about that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=675" target="_blank">00:11:15.540</a></span> | <span class="t">the language models are trained by maximum likelihood. So basically, we were trying to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=680" target="_blank">00:11:20.740</a></span> | <span class="t">maximize the probability of the next token, yt, given the preceding words. And this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=686" target="_blank">00:11:26.260</a></span> | <span class="t">our optimization objective. So at each time step, this can be regarded as a classification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=692" target="_blank">00:11:32.540</a></span> | <span class="t">task, because we are trying to distinguish the actual word, yt star, from all the remaining</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=698" target="_blank">00:11:38.340</a></span> | <span class="t">words in the vocabulary. And this is also called teacher forcing, because at each time</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=703" target="_blank">00:11:43.780</a></span> | <span class="t">step, we are using the gold standard, y star less than t, as input to the model. Whereas,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=712" target="_blank">00:11:52.940</a></span> | <span class="t">presumably, at generation time, you wouldn't have any access to y star. So you would have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=717" target="_blank">00:11:57.140</a></span> | <span class="t">to use the model's own prediction to feed it back into the model to generate the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=721" target="_blank">00:12:01.340</a></span> | <span class="t">token. And that is called student forcing, which we'll talk in detail later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=725" target="_blank">00:12:05.500</a></span> | <span class="t">Oh, sorry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=726" target="_blank">00:12:06.500</a></span> | <span class="t">Yeah, I think I skipped two slides ago. About autoregressive, we never used that word before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=734" target="_blank">00:12:14.740</a></span> | <span class="t">What does it mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=735" target="_blank">00:12:15.740</a></span> | <span class="t">Autoregressive? Oh, it just means like-- so let's look at this animation again. Oops,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=742" target="_blank">00:12:22.380</a></span> | <span class="t">sorry. It just looks like you are generating word from left to right, one by one. So here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=748" target="_blank">00:12:28.020</a></span> | <span class="t">suppose that you are given y less than t. And then autoregressively, you first generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=752" target="_blank">00:12:32.660</a></span> | <span class="t">yt. And then once you have yt, you'll feed it back in, generate yt plus 1, and then feed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=757" target="_blank">00:12:37.780</a></span> | <span class="t">it back in, generate another thing. So this left to right nature, because you are using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=761" target="_blank">00:12:41.260</a></span> | <span class="t">chain rule to condition on the tokens that you just generated, this chain rule thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=767" target="_blank">00:12:47.100</a></span> | <span class="t">is called autoregressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=768" target="_blank">00:12:48.740</a></span> | <span class="t">And typically, I think conventionally, we are doing left to right autoregressive by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=772" target="_blank">00:12:52.460</a></span> | <span class="t">generating from left to right. But there are also other more interesting models that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=776" target="_blank">00:12:56.580</a></span> | <span class="t">do backward or infill and other things. This idea of generating one token at once is autoregressive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=783" target="_blank">00:13:03.580</a></span> | <span class="t">Cool. Any other questions? Yep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=793" target="_blank">00:13:13.340</a></span> | <span class="t">So at inference time, our decoding algorithm would define a function to select a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=799" target="_blank">00:13:19.020</a></span> | <span class="t">from this distribution. So we've discussed that we can use the language model to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=803" target="_blank">00:13:23.820</a></span> | <span class="t">this p, which is the next token distribution. And then g here, based on our notation, is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=809" target="_blank">00:13:29.140</a></span> | <span class="t">the decoding algorithm, which helps us select what token we are actually going to use for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=813" target="_blank">00:13:33.380</a></span> | <span class="t">yt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=814" target="_blank">00:13:34.700</a></span> | <span class="t">So the obvious decoding algorithm is to greedily choose the highest probability token as yt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=820" target="_blank">00:13:40.660</a></span> | <span class="t">for each time step. So while this basic algorithm sort of works, because they work for your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=825" target="_blank">00:13:45.340</a></span> | <span class="t">homework 4, to do better, there are two main avenues that we can take. We can decide to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=830" target="_blank">00:13:50.500</a></span> | <span class="t">improve decoding. And we can also decide to improve the training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=835" target="_blank">00:13:55.020</a></span> | <span class="t">Of course, there are other things that we can do. We can improve training data. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=838" target="_blank">00:13:58.340</a></span> | <span class="t">we can improve model architectures. But for this lecture, we will focus on decoding and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=842" target="_blank">00:14:02.460</a></span> | <span class="t">training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=844" target="_blank">00:14:04.660</a></span> | <span class="t">So now let's talk about how decoding algorithms work for natural language generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=849" target="_blank">00:14:09.860</a></span> | <span class="t">models. Before that, I'm happy to take any questions about the previous slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=853" target="_blank">00:14:13.980</a></span> | <span class="t">OK. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=854" target="_blank">00:14:14.980</a></span> | <span class="t">Sorry, could you just explain one more time the difference between teacher forcing and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=855" target="_blank">00:14:15.980</a></span> | <span class="t">student forcing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=856" target="_blank">00:14:16.980</a></span> | <span class="t">I think I'll go into this in detail later. But sure. So basically, for teacher forcing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=868" target="_blank">00:14:28.820</a></span> | <span class="t">the idea is you do teacher forcing where you train the language model, because you already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=872" target="_blank">00:14:32.500</a></span> | <span class="t">observe the gold text. So you use the gold text up until time step t, put it into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=878" target="_blank">00:14:38.500</a></span> | <span class="t">model. And then the model would try to predict y t plus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=882" target="_blank">00:14:42.900</a></span> | <span class="t">Whereas student forcing means that you don't have access to this gold reference data. Instead,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=888" target="_blank">00:14:48.020</a></span> | <span class="t">but you are still trying to generate a sequence of data. So you have to use the text that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=891" target="_blank">00:14:51.380</a></span> | <span class="t">you generated yourself using the model, and then feed it back into the model as input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=895" target="_blank">00:14:55.580</a></span> | <span class="t">to predict t plus 1. That's the primary difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=900" target="_blank">00:15:00.420</a></span> | <span class="t">Cool. So what is decoding all about? At each time step, our model computes a vector of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=908" target="_blank">00:15:08.180</a></span> | <span class="t">score for each token. So it takes in preceding context y less than t and produce a score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=913" target="_blank">00:15:13.860</a></span> | <span class="t">s. And then we try to compute a probability distribution p out of the scores by just applying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=919" target="_blank">00:15:19.980</a></span> | <span class="t">softmax to normalize them. And our decoding algorithm is defined as this function g, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=926" target="_blank">00:15:26.860</a></span> | <span class="t">takes in the probability distribution and try to map it to some word. Basically, try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=931" target="_blank">00:15:31.580</a></span> | <span class="t">to select a token from this probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=935" target="_blank">00:15:35.140</a></span> | <span class="t">So in the machine translation lecture, we talked about greedy decoding, which selects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=940" target="_blank">00:15:40.300</a></span> | <span class="t">the highest probability token of this p distribution. And we also talk about beam search, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=947" target="_blank">00:15:47.380</a></span> | <span class="t">has the same objective as greedy decoding, which is that we are both trying to find the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=951" target="_blank">00:15:51.980</a></span> | <span class="t">most likely string defined based on the model. But instead of doing so greedily for beam</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=956" target="_blank">00:15:56.780</a></span> | <span class="t">search, we actually explore a wider range of candidates. So we have a wider exploration</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=962" target="_blank">00:16:02.140</a></span> | <span class="t">of candidates by keeping always k candidates in the beam.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=968" target="_blank">00:16:08.340</a></span> | <span class="t">So overall, this maximum probability decoding is good for low entropy tasks like machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=973" target="_blank">00:16:13.180</a></span> | <span class="t">translation and summarization. But it actually encounters more problems for open-ended generation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=979" target="_blank">00:16:19.420</a></span> | <span class="t">So the most likely string is actually very repetitive when we try to do open-ended text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=984" target="_blank">00:16:24.380</a></span> | <span class="t">generation. As we can see in this example, the context is perfectly normal. It's about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=990" target="_blank">00:16:30.540</a></span> | <span class="t">a unicorn trying to speak English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=993" target="_blank">00:16:33.140</a></span> | <span class="t">And by the continuation, the first part of it looks great. It's like valid English. It</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=998" target="_blank">00:16:38.380</a></span> | <span class="t">talks about science. But suddenly, it starts to repeat. And it starts to repeat, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1004" target="_blank">00:16:44.100</a></span> | <span class="t">an institution's name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1006" target="_blank">00:16:46.660</a></span> | <span class="t">So why does this happen? If we look at, for example, this plot, which shows the language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1014" target="_blank">00:16:54.500</a></span> | <span class="t">model's probability assigned to the sequence I don't know, we can see here is the pattern.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1020" target="_blank">00:17:00.140</a></span> | <span class="t">It has regular probability. But if we keep repeating this phrase, I don't know, I don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1024" target="_blank">00:17:04.460</a></span> | <span class="t">know, I don't know, for 10 times, then we can see that there is a decreasing trend in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1029" target="_blank">00:17:09.300</a></span> | <span class="t">their negative log likelihood. So the y-axis is the negative log probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1034" target="_blank">00:17:14.020</a></span> | <span class="t">We can see this decreasing trend, which means that the model actually has higher probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1038" target="_blank">00:17:18.740</a></span> | <span class="t">as the repeat goes on, which is quite strange because it's suggesting that there is a self-amplification</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1045" target="_blank">00:17:25.180</a></span> | <span class="t">effect. So the more repeat we have, the more confident the model becomes about this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1049" target="_blank">00:17:29.980</a></span> | <span class="t">repeat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1052" target="_blank">00:17:32.420</a></span> | <span class="t">And this keeps going on. We can see that for I am tired, I'm tired, repeat 100 times, we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1056" target="_blank">00:17:36.540</a></span> | <span class="t">can see a continuously decreasing trend until the model is almost 100% sure that it's going</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1062" target="_blank">00:17:42.100</a></span> | <span class="t">to keep repeating the same thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1065" target="_blank">00:17:45.900</a></span> | <span class="t">And sadly, this problem is not really solved by architecture. Here, the red plot is a LSTM</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1073" target="_blank">00:17:53.020</a></span> | <span class="t">model, and the blue curve is a transformer model. We can see that both models kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1077" target="_blank">00:17:57.380</a></span> | <span class="t">suffers from the same problem. And scale also doesn't solve this problem. So we kind of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1082" target="_blank">00:18:02.060</a></span> | <span class="t">believe that scale is the magical thing in NLP. But even models with 175 billion parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1089" target="_blank">00:18:09.060</a></span> | <span class="t">will still suffer from repetition if we try to find the most likely string.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1096" target="_blank">00:18:16.300</a></span> | <span class="t">So how do we reduce repetition? One canonical approach is to do n-gram blocking. So the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1102" target="_blank">00:18:22.180</a></span> | <span class="t">principle is fairly simple. Basically, you just don't want to see the same n-gram twice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1107" target="_blank">00:18:27.460</a></span> | <span class="t">If we set n to be 3, then for any text that contains the phrase "I am happy," the next</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1112" target="_blank">00:18:32.300</a></span> | <span class="t">time you see the prefix "I am," n-gram blocking would automatically set the probability of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1117" target="_blank">00:18:37.340</a></span> | <span class="t">happy to be 0 so that you will never see this n-gram, this trigram again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1123" target="_blank">00:18:43.100</a></span> | <span class="t">But clearly, this n-gram blocking heuristic has some problems because sometimes it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1128" target="_blank">00:18:48.020</a></span> | <span class="t">quite common for you to want to see a person's name appear twice or three times or even more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1132" target="_blank">00:18:52.500</a></span> | <span class="t">in a text. But this n-gram blocking will eliminate that possibility.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1137" target="_blank">00:18:57.300</a></span> | <span class="t">So what are better options that possibly are more complicated? For example, we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1142" target="_blank">00:19:02.700</a></span> | <span class="t">a different training objective. Instead of training by MLE, we can train by unlikelihood</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1148" target="_blank">00:19:08.300</a></span> | <span class="t">objective. So in this approach, the model is actually penalized for generating already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1154" target="_blank">00:19:14.660</a></span> | <span class="t">seen tokens. So it's kind of like putting this n-gram blocking idea into training time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1160" target="_blank">00:19:20.700</a></span> | <span class="t">Rather than at decoding time for this constraint, at training time, we just decrease the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1164" target="_blank">00:19:24.460</a></span> | <span class="t">of repetition. Another training objective is coverage wealth, which uses the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1171" target="_blank">00:19:31.420</a></span> | <span class="t">mechanism to prevent repetition. So basically, if you try to regularize and enforce your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1175" target="_blank">00:19:35.980</a></span> | <span class="t">attention so that it's always attending to different words for each token, then it is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1181" target="_blank">00:19:41.260</a></span> | <span class="t">highly likely that you are not going to repeat because repetition tends to happen when you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1185" target="_blank">00:19:45.700</a></span> | <span class="t">have similar attention patterns. Another different angle is that instead of searching for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1191" target="_blank">00:19:51.980</a></span> | <span class="t">most likely string, we can use a different decoding objective. So maybe we can search</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1196" target="_blank">00:19:56.500</a></span> | <span class="t">for strings that maximizes the difference between log probabilities of two models. Say</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1201" target="_blank">00:20:01.940</a></span> | <span class="t">that we want to maximize log probability of large model minus log probability of small</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1206" target="_blank">00:20:06.260</a></span> | <span class="t">model. In this way, because both models are repetitive, so they kind of cancel out. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1210" target="_blank">00:20:10.860</a></span> | <span class="t">they would both assign high probabilities of repetition. And after applying this new</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1214" target="_blank">00:20:14.900</a></span> | <span class="t">objective, the repetition stuff will actually be penalized because it cancels out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1220" target="_blank">00:20:20.820</a></span> | <span class="t">So here comes the broader question. Is finding the most likely string even a reasonable thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1226" target="_blank">00:20:26.420</a></span> | <span class="t">to do for open-ended text generation? The answer is probably no, because this doesn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1232" target="_blank">00:20:32.540</a></span> | <span class="t">really match human pattern. So we can see in this plot, the orange curve is the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1237" target="_blank">00:20:37.060</a></span> | <span class="t">pattern, and the blue curve is the machine-generated text using beam search. So you can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1242" target="_blank">00:20:42.100</a></span> | <span class="t">with human talks, there are actually lots of uncertainty, as we can see by the fluctuation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1247" target="_blank">00:20:47.860</a></span> | <span class="t">of the probabilities. For some words, we can be very certain. For some words, we are a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1252" target="_blank">00:20:52.580</a></span> | <span class="t">little bit unsure. Whereas here, for the model distribution, it's always very sure. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1256" target="_blank">00:20:56.580</a></span> | <span class="t">always assigning probability 1 to the sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1259" target="_blank">00:20:59.500</a></span> | <span class="t">So because we now are seeing a-- basically, there is a mismatch between the two distributions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1266" target="_blank">00:21:06.080</a></span> | <span class="t">So it's kind of suggesting that maybe searching for the most likely string is not the right</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1270" target="_blank">00:21:10.940</a></span> | <span class="t">decoding objective at all. Any questions so far before we move on? Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1275" target="_blank">00:21:15.940</a></span> | <span class="t">So is this the underlying mechanism for some detector of whether some text is generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1281" target="_blank">00:21:21.940</a></span> | <span class="t">by changing the [INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1284" target="_blank">00:21:24.540</a></span> | <span class="t">Not really, because this can only detect the really simple things that humans are also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1288" target="_blank">00:21:28.740</a></span> | <span class="t">able to detect, like repetition. So in order to avoid the previous problems that we've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1294" target="_blank">00:21:34.420</a></span> | <span class="t">talked about, I'll talk about some other decoding families that generate more robust text that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1300" target="_blank">00:21:40.180</a></span> | <span class="t">actually look like this, whose probability distribution looks like the orange curve.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1305" target="_blank">00:21:45.500</a></span> | <span class="t">So I wouldn't say this is the to-go answer for watermarking or detection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1310" target="_blank">00:21:50.460</a></span> | <span class="t">Can you repeat the student's question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1313" target="_blank">00:21:53.700</a></span> | <span class="t">Oh, yeah. OK, cool. So she asked about whether this mechanism of plotting the probabilities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1320" target="_blank">00:22:00.180</a></span> | <span class="t">of human text and machine-generated text is one way of detecting whether some text is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1325" target="_blank">00:22:05.980</a></span> | <span class="t">generated by a model or a human. And my answer is, I don't think so, but this could be an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1331" target="_blank">00:22:11.900</a></span> | <span class="t">interesting research direction. Because I feel like there are more robust decoding approaches</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1337" target="_blank">00:22:17.540</a></span> | <span class="t">that generate text that actually fluctuates a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1344" target="_blank">00:22:24.400</a></span> | <span class="t">So yeah, let's talk about the decoding algorithm that is able to generate text that fluctuates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1349" target="_blank">00:22:29.260</a></span> | <span class="t">So given that searching for the most likely string is a bad idea, what else should we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1353" target="_blank">00:22:33.580</a></span> | <span class="t">do? And how do we simulate that human pattern? And the answer to this is to introduce randomness</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1359" target="_blank">00:22:39.060</a></span> | <span class="t">and stochasticity to decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1361" target="_blank">00:22:41.860</a></span> | <span class="t">So suppose that we are sampling a token from this distribution, P. Basically, we are trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1368" target="_blank">00:22:48.420</a></span> | <span class="t">to sample YT hat from this distribution. It is random so that you can essentially sample</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1373" target="_blank">00:22:53.420</a></span> | <span class="t">any token in the distribution. Previously, you are kind of restricted to selecting rest</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1377" target="_blank">00:22:57.460</a></span> | <span class="t">from your grocery. But now you can select bathroom instead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1382" target="_blank">00:23:02.980</a></span> | <span class="t">So however, sampling introduces a new set of problems. Since we never really zero out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1388" target="_blank">00:23:08.580</a></span> | <span class="t">any token probabilities, vanilla sampling would make every token in the vocabulary a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1394" target="_blank">00:23:14.100</a></span> | <span class="t">viable option. And in some unlucky cases, we might end up with a bad word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1400" target="_blank">00:23:20.040</a></span> | <span class="t">So assuming that we already have a very well-trained model, even if most of the probability mass</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1406" target="_blank">00:23:26.540</a></span> | <span class="t">of the distribution is over the limited set of good options, the tail of the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1411" target="_blank">00:23:31.580</a></span> | <span class="t">will still be very long because we have so many words in our vocabulary. And therefore,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1416" target="_blank">00:23:36.700</a></span> | <span class="t">if we add all those long tails, it aggregates. They still have a considerable mass. So statistically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1422" target="_blank">00:23:42.060</a></span> | <span class="t">speaking, this is called heavy tail distribution. And language is exactly a heavy tail distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1427" target="_blank">00:23:47.980</a></span> | <span class="t">So for example, many tokens are probably really wrong in this context. And then given that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1434" target="_blank">00:23:54.420</a></span> | <span class="t">we have a good language model, we assign them each very little probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1438" target="_blank">00:23:58.740</a></span> | <span class="t">But this doesn't really solve the problem because there are so many of them. So you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1442" target="_blank">00:24:02.500</a></span> | <span class="t">aggregate them as a group. We'll still have a high chance of being selected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1448" target="_blank">00:24:08.180</a></span> | <span class="t">And the solution here that we have for this problem of long tail is that we should just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1452" target="_blank">00:24:12.380</a></span> | <span class="t">cut off the tail. We should just zero out the probabilities that we don't want. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1456" target="_blank">00:24:16.900</a></span> | <span class="t">one idea is called top case sampling, where the idea is that we would only sample from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1463" target="_blank">00:24:23.580</a></span> | <span class="t">the top k tokens in the probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1469" target="_blank">00:24:29.060</a></span> | <span class="t">Any questions for now?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1470" target="_blank">00:24:30.580</a></span> | <span class="t">OK, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1471" target="_blank">00:24:31.580</a></span> | <span class="t">Well, the model we were looking at a second ago had some very low probability samples</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1479" target="_blank">00:24:39.940</a></span> | <span class="t">as well on the graph, right? How would top case sampling deal with that?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1485" target="_blank">00:24:45.420</a></span> | <span class="t">You mean this one?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1486" target="_blank">00:24:46.420</a></span> | <span class="t">You mean the orange-blue graph of the human versus--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1491" target="_blank">00:24:51.660</a></span> | <span class="t">Oh, yeah. So top k will basically eliminate-- it will make it impossible to generate the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1498" target="_blank">00:24:58.940</a></span> | <span class="t">super low probability tokens. So technically, it's not exactly simulating this pattern because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1504" target="_blank">00:25:04.700</a></span> | <span class="t">now you don't have the super low probability tokens, whereas human can generate super low</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1508" target="_blank">00:25:08.780</a></span> | <span class="t">probability tokens in a fluent way. But yeah, that could be another hint that people can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1515" target="_blank">00:25:15.140</a></span> | <span class="t">use for detecting machine-generated text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1517" target="_blank">00:25:17.980</a></span> | <span class="t">Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1518" target="_blank">00:25:18.980</a></span> | <span class="t">It also depends on the type of text you want to generate, for example, for more novels</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1524" target="_blank">00:25:24.500</a></span> | <span class="t">or more creative writing. Is it then you decide the hyperparameter?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1528" target="_blank">00:25:28.820</a></span> | <span class="t">Yeah, yeah, for sure. K is a hyperparameter. Depending on the type of task, you will choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1533" target="_blank">00:25:33.020</a></span> | <span class="t">K differently. Mostly for a closed-ended task, K should be small. And for open-ended, K should</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1539" target="_blank">00:25:39.100</a></span> | <span class="t">be large. Yeah, question in the back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1541" target="_blank">00:25:41.660</a></span> | <span class="t">How come-- I guess intuitively, this builds off of one of the earlier questions. Why don't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1546" target="_blank">00:25:46.820</a></span> | <span class="t">we consider the case where we sample, and then we just weight the probability of each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1552" target="_blank">00:25:52.020</a></span> | <span class="t">word by its score or something, rather than just looking at top K? We don't do a weighted</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1557" target="_blank">00:25:57.580</a></span> | <span class="t">sampling type of situation. So we still have that small but non-zero probability of selecting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1563" target="_blank">00:26:03.740</a></span> | <span class="t">I think top K is also weighted. So top K just zeroes out all the tails of the distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1571" target="_blank">00:26:11.380</a></span> | <span class="t">But for the things that it didn't zero out, it's not a uniform choice among the K. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1576" target="_blank">00:26:16.260</a></span> | <span class="t">still trying to choose proportional to the scores that you computed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1580" target="_blank">00:26:20.540</a></span> | <span class="t">Is that just like a computationally it's more efficient because you don't have to do for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1585" target="_blank">00:26:25.700</a></span> | <span class="t">17,000 words. It could be for 10 or something? Yeah, sure. That could be one gain of top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1591" target="_blank">00:26:31.820</a></span> | <span class="t">K decoding is that your softmax will take in fewer candidates.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1596" target="_blank">00:26:36.020</a></span> | <span class="t">But it's not the main reason. I think you should show--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1600" target="_blank">00:26:40.900</a></span> | <span class="t">Yeah, I'll keep talking about the main reason. So we've discussed this part. And then here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1611" target="_blank">00:26:51.140</a></span> | <span class="t">this is formally what is happening for top K sampling. Now that we are only sampling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1617" target="_blank">00:26:57.740</a></span> | <span class="t">from the top K tokens of the probability distribution. And as we've said, K is a hyperparameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1623" target="_blank">00:27:03.780</a></span> | <span class="t">So we can set K to be large or small. If we increase K, this means that we are making</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1629" target="_blank">00:27:09.460</a></span> | <span class="t">our output more diverse, but at the risk of including some tokens that are bad. If we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1634" target="_blank">00:27:14.500</a></span> | <span class="t">decrease K, then we are making more conservative and safe options. But possibly the generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1639" target="_blank">00:27:19.380</a></span> | <span class="t">will be quite generic and boring.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1644" target="_blank">00:27:24.340</a></span> | <span class="t">So is top K decoding good enough? The answer is not really. Because we can still find some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1650" target="_blank">00:27:30.220</a></span> | <span class="t">problems with top K decoding. For example, in the context, she said, I never blank. There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1656" target="_blank">00:27:36.060</a></span> | <span class="t">are many words that are still valid options, such as went, ate. But those words got zeroed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1662" target="_blank">00:27:42.340</a></span> | <span class="t">out because they are not within the top K candidates. So this actually leads to bad</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1666" target="_blank">00:27:46.780</a></span> | <span class="t">recall for your generation system. And similarly, another failure of top K is that it can also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1673" target="_blank">00:27:53.260</a></span> | <span class="t">cut off too quickly. So in this example, code is not really a valid answer, according to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1679" target="_blank">00:27:59.580</a></span> | <span class="t">common sense, because you probably don't want to eat a piece of code. But the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1684" target="_blank">00:28:04.100</a></span> | <span class="t">remains non-zero, meaning that the model might still sample code as an output, despite this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1690" target="_blank">00:28:10.100</a></span> | <span class="t">low probability, but it might still happen. And this means bad precision for the generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1695" target="_blank">00:28:15.180</a></span> | <span class="t">model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1697" target="_blank">00:28:17.780</a></span> | <span class="t">So given these problems with top K decoding, how can we address them? How can we address</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1703" target="_blank">00:28:23.900</a></span> | <span class="t">this issue of there is no single K that fits all circumstances? This is basically because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1710" target="_blank">00:28:30.580</a></span> | <span class="t">the probability distribution that we sample from our dynamic. So when the probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1714" target="_blank">00:28:34.740</a></span> | <span class="t">distribution is relatively flat, having a small K will remove many viable options. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1721" target="_blank">00:28:41.660</a></span> | <span class="t">having a limited K will remove many viable options, and we want K to be larger for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1725" target="_blank">00:28:45.820</a></span> | <span class="t">case. Similarly, when a distribution P is too picky, then we want the-- a high K would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1733" target="_blank">00:28:53.700</a></span> | <span class="t">allow for too many options to be viable. And instead, we might want a smaller K so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1739" target="_blank">00:28:59.460</a></span> | <span class="t">we are being safer. So the solution here is that maybe K is just a bad hyperparameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1745" target="_blank">00:29:05.060</a></span> | <span class="t">And instead of doing K, we should think about probability. We should think about how to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1750" target="_blank">00:29:10.420</a></span> | <span class="t">sample from tokens in a top P probability percentiles of the cumulative probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1756" target="_blank">00:29:16.980</a></span> | <span class="t">mass of the CDF, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1760" target="_blank">00:29:20.940</a></span> | <span class="t">So now, the advantage of doing top P sampling, where we sample from the top P percentile</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1767" target="_blank">00:29:27.340</a></span> | <span class="t">of the cumulative probability mass, is that this is actually equivalent to-- we have now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1771" target="_blank">00:29:31.980</a></span> | <span class="t">an adaptive K for each different distribution. And let me explain what I mean by having an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1778" target="_blank">00:29:38.260</a></span> | <span class="t">adaptive K. So in the first distribution, this is like a regular power law of language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1784" target="_blank">00:29:44.180</a></span> | <span class="t">that's kind of typical. And then doing top K sampling means we are selecting the top</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1789" target="_blank">00:29:49.820</a></span> | <span class="t">K. But doing top P sampling means that we are zooming into maybe something that's similar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1795" target="_blank">00:29:55.460</a></span> | <span class="t">to top K in effect. But if I have a relatively flat distribution like the blue one, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1801" target="_blank">00:30:01.380</a></span> | <span class="t">see that doing top P means that we are including more candidates. And then if we have a more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1806" target="_blank">00:30:06.700</a></span> | <span class="t">skewed distribution like the green one, doing top P means that we actually include fewer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1811" target="_blank">00:30:11.180</a></span> | <span class="t">candidates. So by actually selecting the top P percentile in the probability distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1818" target="_blank">00:30:18.780</a></span> | <span class="t">we are actually having a more flexible K and therefore have a better sense of what are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1824" target="_blank">00:30:24.460</a></span> | <span class="t">the good options in the model. Any questions about top P, top K decoding? So everything's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1833" target="_blank">00:30:33.500</a></span> | <span class="t">clear. Yeah, sounds good. So to go back to that question, doing top K is not necessarily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1840" target="_blank">00:30:40.460</a></span> | <span class="t">saving compute. Or this whole idea is not really compute saving intended. Because in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1846" target="_blank">00:30:46.820</a></span> | <span class="t">the case of top P, in order to select the top P percentile, we still need to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1851" target="_blank">00:30:51.540</a></span> | <span class="t">the softmax over the entire vocabulary set in order for us to compute the P properly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1859" target="_blank">00:30:59.180</a></span> | <span class="t">So therefore, it's not really saving compute, but it's improving performance. Cool. Moving</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1865" target="_blank">00:31:05.420</a></span> | <span class="t">on. So there are much more to go with decoding algorithms. Besides the top K and top P that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1872" target="_blank">00:31:12.740</a></span> | <span class="t">we've discussed, there are some more recent approaches like typical sampling, where the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1877" target="_blank">00:31:17.780</a></span> | <span class="t">idea is that we want to relate the score based on the entropy of the distribution and try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1882" target="_blank">00:31:22.740</a></span> | <span class="t">to generate texts that are closer to the negative-- whose probability is closer to the negative</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1887" target="_blank">00:31:27.620</a></span> | <span class="t">entropy of the data distribution. This means that if you have a closed-ended task or non-open-ended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1894" target="_blank">00:31:34.700</a></span> | <span class="t">task, it has smaller entropy. So you'll want negative log probability to be smaller. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1900" target="_blank">00:31:40.380</a></span> | <span class="t">you want probability to be larger. So it type checks very well. And additionally, there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1906" target="_blank">00:31:46.380</a></span> | <span class="t">is also epsilon sampling coming from John. So this is an idea where we set the threshold</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1913" target="_blank">00:31:53.700</a></span> | <span class="t">to lower bound probabilities. So basically, if you have a word whose probability is less</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1918" target="_blank">00:31:58.380</a></span> | <span class="t">than 0.03, for example, then that word will never appear in the output distribution. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1925" target="_blank">00:32:05.140</a></span> | <span class="t">that word will never be part of your output because it has so low probability. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1929" target="_blank">00:32:09.980</a></span> | <span class="t">How do you know if it's top-degree, the entropy of a distribution?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1934" target="_blank">00:32:14.100</a></span> | <span class="t">Oh, cool. Great question. So the entropy distribution is defined as-- suppose that we have a discrete</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1941" target="_blank">00:32:21.300</a></span> | <span class="t">distribution. We can go over it. We'll just enumerate x. And then it's negative log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1947" target="_blank">00:32:27.620</a></span> | <span class="t">of x. So if we write it from an expectation perspective, it's basically expected of log</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1954" target="_blank">00:32:34.700</a></span> | <span class="t">probability of x. I have to do a little bit here. So this is the entropy of a distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1965" target="_blank">00:32:45.140</a></span> | <span class="t">And then-- so basically, if your distribution is very concentrated to a few words, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1970" target="_blank">00:32:50.260</a></span> | <span class="t">the entropy will be relatively small. If your distribution is very flat, then your entropy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1975" target="_blank">00:32:55.660</a></span> | <span class="t">will be very large. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1977" target="_blank">00:32:57.660</a></span> | <span class="t">What if the epsilon sampling is such that we have no valid option?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1985" target="_blank">00:33:05.700</a></span> | <span class="t">Oh, yeah. I mean, there will be some back-off cases, I think. So in the case that there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1991" target="_blank">00:33:11.180</a></span> | <span class="t">is no valid options, you'll probably still want to select one or two things, just as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=1996" target="_blank">00:33:16.540</a></span> | <span class="t">an edge case, I think. OK, cool. Moving on. So another hyperparameter that we can tune</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2006" target="_blank">00:33:26.780</a></span> | <span class="t">to affect decoding is the temperature parameter. So recall that previously at each time step,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2013" target="_blank">00:33:33.060</a></span> | <span class="t">we asked the model to compute a score. And then we renormalized that score using softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2018" target="_blank">00:33:38.180</a></span> | <span class="t">to get a probability distribution. So one thing that we can adjust here is that we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2022" target="_blank">00:33:42.540</a></span> | <span class="t">insert this temperature parameter tau to relate the score. So basically, we just divide all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2027" target="_blank">00:33:47.660</a></span> | <span class="t">the sw by tau. And after dividing this, we apply softmax. And we get a new distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2035" target="_blank">00:33:55.460</a></span> | <span class="t">And this temperature adjustment is not really going to affect the monotonosity of the distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2041" target="_blank">00:34:01.580</a></span> | <span class="t">For example, if word A has higher probability than word B previously, then after the adjustment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2047" target="_blank">00:34:07.620</a></span> | <span class="t">word A is still going to have a higher probability than word B. But their relative difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2052" target="_blank">00:34:12.300</a></span> | <span class="t">will change. So for example, if we raise the temperature tau to be greater than 1, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2060" target="_blank">00:34:20.380</a></span> | <span class="t">the distribution Pt will become more uniform. It will be flatter. And this implies that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2066" target="_blank">00:34:26.700</a></span> | <span class="t">there will be more diverse output because our distribution is flatter. And it's more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2071" target="_blank">00:34:31.660</a></span> | <span class="t">spread out across different words in the vocabulary. On the other hand, if we lower the temperature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2077" target="_blank">00:34:37.620</a></span> | <span class="t">tau less than 1, then Pt becomes very spiky. And then this means that if we sample from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2084" target="_blank">00:34:44.700</a></span> | <span class="t">the Pt, we'll get less diverse output. So because here, the probability is concentrated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2089" target="_blank">00:34:49.860</a></span> | <span class="t">only on the top words. So in the very extreme case, if we set tau to be very, very close</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2094" target="_blank">00:34:54.540</a></span> | <span class="t">to 0, then the probability will be a 1/2 vector, where all the probability mass will be centered</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2101" target="_blank">00:35:01.220</a></span> | <span class="t">on one word. And then this reduces back to argmax sampling or greedy decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2107" target="_blank">00:35:07.860</a></span> | <span class="t">So temperature is a hyperparameter as well, as for k and P in topk and topp. It is a hyperparameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2114" target="_blank">00:35:14.220</a></span> | <span class="t">for decoding. It can be tuned for beam search and sampling algorithms. So it's kind of orthogonal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2119" target="_blank">00:35:19.900</a></span> | <span class="t">to the approaches that we discussed before. Any questions so far? OK, cool. Temperature</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2129" target="_blank">00:35:29.860</a></span> | <span class="t">is so easy. So well, because sampling still involves randomness, even though we try very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2138" target="_blank">00:35:38.900</a></span> | <span class="t">hard in terms of truncation, truncating the tail, sampling still has randomness. So what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2143" target="_blank">00:35:43.740</a></span> | <span class="t">if we're just unlucky and decode a bad sequence from the model? One common solution is to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2149" target="_blank">00:35:49.180</a></span> | <span class="t">do re-ranking. So basically, we would decode a bunch of sequences. For example, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2153" target="_blank">00:35:53.460</a></span> | <span class="t">decode 10 candidates. But 10 or 30 is up to you. The only choice is that you want to balance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2159" target="_blank">00:35:59.540</a></span> | <span class="t">between your compute efficiency and performance. So if you decode too many sequences, then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2165" target="_blank">00:36:05.460</a></span> | <span class="t">of course, your performance is going to increase. But it's also very costly to just generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2170" target="_blank">00:36:10.460</a></span> | <span class="t">a lot of things for one example. And so once you have a bunch of sample sequences, then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2178" target="_blank">00:36:18.100</a></span> | <span class="t">we are trying to define a score to approximate the quality of the sequence and re-rank all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2184" target="_blank">00:36:24.060</a></span> | <span class="t">the candidates by this score. So the simple thing to do is we can use perplexity as a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2189" target="_blank">00:36:29.500</a></span> | <span class="t">metric, as a scoring function. But we need to be careful that, because we have talked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2195" target="_blank">00:36:35.180</a></span> | <span class="t">about this, the extreme of perplexity, like if we try to arc max log probability, when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2200" target="_blank">00:36:40.540</a></span> | <span class="t">we try to aim for a super low perplexity, the tags are actually very repetitive. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2205" target="_blank">00:36:45.500</a></span> | <span class="t">we shouldn't really aim for extremely low perplexity. And perplexity, to some extent,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2210" target="_blank">00:36:50.180</a></span> | <span class="t">is not a perfect scoring function. It's not a perfect scoring function because it's not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2215" target="_blank">00:36:55.540</a></span> | <span class="t">really robust to maximize. So alternatively, the re-rankers can actually use a wide variety</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2222" target="_blank">00:37:02.380</a></span> | <span class="t">of other scoring functions. We can score tags based on their style, their discourse coherence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2228" target="_blank">00:37:08.620</a></span> | <span class="t">their entailment, factuality properties, consistency, and so on. And additionally, we can compose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2236" target="_blank">00:37:16.860</a></span> | <span class="t">multiple re-rankers together. Yeah, question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2240" target="_blank">00:37:20.540</a></span> | <span class="t">>> You mentioned 10 candidates or any number of candidates. What's the strategy you usually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2247" target="_blank">00:37:27.540</a></span> | <span class="t">use to generate these other candidates? Like what heuristic do you use?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2252" target="_blank">00:37:32.540</a></span> | <span class="t">>> So basically, the idea is to sample from the model. So when you sample from the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2257" target="_blank">00:37:37.260</a></span> | <span class="t">each time you sample, you are going to get a different output. And then that's what I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2260" target="_blank">00:37:40.820</a></span> | <span class="t">mean by different candidates. So if you sample 10 times, you will very likely get 10 different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2265" target="_blank">00:37:45.820</a></span> | <span class="t">outputs. And then you are just-- given these 10 different outputs that come from sampling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2271" target="_blank">00:37:51.500</a></span> | <span class="t">you can just decide, re-rank them, and select the candidate that has the highest score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2276" target="_blank">00:37:56.180</a></span> | <span class="t">>> Where does the randomness come from? >> Oh, because we are sampling here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2280" target="_blank">00:38:00.980</a></span> | <span class="t">>> That sample, okay. >> Yeah, yeah. For example, if you are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2284" target="_blank">00:38:04.420</a></span> | <span class="t">top-T sampling, then, well, suppose that A and B are equally probable, then you might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2289" target="_blank">00:38:09.180</a></span> | <span class="t">sample A, you might sample B with the same probability. Okay, cool. And another cool</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2296" target="_blank">00:38:16.140</a></span> | <span class="t">thing that we can do with re-ranking is that we can compose multiple re-rankers together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2300" target="_blank">00:38:20.540</a></span> | <span class="t">So basically, suppose you have a scoring function for style, and you have a scoring function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2304" target="_blank">00:38:24.780</a></span> | <span class="t">for factual consistency. You can just add those two scoring functions together to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2308" target="_blank">00:38:28.900</a></span> | <span class="t">a new scoring function, and then re-rank everything based on your new scoring function to get</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2314" target="_blank">00:38:34.580</a></span> | <span class="t">tags that are both good at style and good at factual consistency. Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2318" target="_blank">00:38:38.980</a></span> | <span class="t">>> Yeah, so when you say that we re-rank by score, do we just pick the decoding that has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2325" target="_blank">00:38:45.500</a></span> | <span class="t">the highest score, or do we do some more sampling again based on the score?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2329" target="_blank">00:38:49.900</a></span> | <span class="t">>> The idea is you just take the decoding that has the highest score, because you already</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2332" target="_blank">00:38:52.540</a></span> | <span class="t">have, say, 10 candidates. So out of these 10, you only need one, and then you just choose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2337" target="_blank">00:38:57.300</a></span> | <span class="t">one that has the highest score. Yeah. Cool. Any other questions? Yeah?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2344" target="_blank">00:39:04.900</a></span> | <span class="t">>> Sorry. What is perplexity?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2348" target="_blank">00:39:08.900</a></span> | <span class="t">>> Oh, yeah. Perplexity is like, you can kind of regard it as log probabilities. It's like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2355" target="_blank">00:39:15.180</a></span> | <span class="t">E to the negative log probabilities. It's kind of like if a token has high perplexity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2362" target="_blank">00:39:22.140</a></span> | <span class="t">then it means it has low probability, because you are more perplexed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2368" target="_blank">00:39:28.020</a></span> | <span class="t">Okay. So taking a step back to summarize this decoding section, we have discussed many decoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2375" target="_blank">00:39:35.180</a></span> | <span class="t">approaches from selecting the most probable string to sampling, and then to various truncation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2381" target="_blank">00:39:41.860</a></span> | <span class="t">approaches that we can do to improve sampling, like top P, top K, epsilon, typical decoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2387" target="_blank">00:39:47.820</a></span> | <span class="t">And finally, we discussed how we can do in terms of re-ranking the results. So decoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2394" target="_blank">00:39:54.020</a></span> | <span class="t">is still a really essential problem in NLG, and there are lots of works to be done here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2399" target="_blank">00:39:59.420</a></span> | <span class="t">still, especially as like chatGP is so powerful. We should all go study decoding. So it would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2405" target="_blank">00:40:05.060</a></span> | <span class="t">be interesting if you want to do such final projects. And also, different decoding algorithms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2410" target="_blank">00:40:10.380</a></span> | <span class="t">can allow us to inject different inductive biases to the text that we are trying to generate.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2417" target="_blank">00:40:17.800</a></span> | <span class="t">And some of the most impactful advances in NLG in the last couple of years actually come</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2422" target="_blank">00:40:22.420</a></span> | <span class="t">from simple but effective decoding algorithms. For example, the nuclear sampling paper is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2428" target="_blank">00:40:28.260</a></span> | <span class="t">actually very, very highly cited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2431" target="_blank">00:40:31.540</a></span> | <span class="t">So moving on to talk about training NLG models. Well, we have seen this example before in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2438" target="_blank">00:40:38.740</a></span> | <span class="t">the decoding slides, and I'm just trying to show them again, because even though we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2443" target="_blank">00:40:43.100</a></span> | <span class="t">solve this repetition problem by instead of doing search, doing sampling. But it's still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2449" target="_blank">00:40:49.300</a></span> | <span class="t">concerning from a language modeling perspective that your model would put so much probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2454" target="_blank">00:40:54.540</a></span> | <span class="t">on such repetitive and degenerate text. So we ask this question, well, is repetition</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2459" target="_blank">00:40:59.740</a></span> | <span class="t">due to how language models are trained? You have also seen this plot before, which shows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2466" target="_blank">00:41:06.940</a></span> | <span class="t">this decaying pattern or this self-amplification effect. So we can conclude from this observation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2473" target="_blank">00:41:13.340</a></span> | <span class="t">that model trained via a MLE objective wears a really bad mode of the distribution. By</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2479" target="_blank">00:41:19.820</a></span> | <span class="t">mode of the distribution, I mean the argmax of the distribution. So basically, they would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2483" target="_blank">00:41:23.500</a></span> | <span class="t">assign high probability to terrible strings. And this is definitely problematic for a model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2488" target="_blank">00:41:28.900</a></span> | <span class="t">perspective. So why is this the case? Shouldn't MLE be a gold standard in machine learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2496" target="_blank">00:41:36.140</a></span> | <span class="t">in general, not just machine translation? Shouldn't MLE be a gold standard for machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2499" target="_blank">00:41:39.700</a></span> | <span class="t">learning? The answer here is not really, especially for text, because MLE has some problem for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2506" target="_blank">00:41:46.340</a></span> | <span class="t">sequential data. And we call this problem exposure bias. So training with teacher forcing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2513" target="_blank">00:41:53.340</a></span> | <span class="t">leads to exposure bias at generation time, because during training, our model's inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2518" target="_blank">00:41:58.140</a></span> | <span class="t">are gold context tokens from real human-generated text, as denoted by a hat less than T here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2525" target="_blank">00:42:05.060</a></span> | <span class="t">But during generation time, our model's input become previously decoded tokens from the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2530" target="_blank">00:42:10.820</a></span> | <span class="t">model, y hat T. And suppose that our model has minor errors, then y hat less than T will</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2538" target="_blank">00:42:18.260</a></span> | <span class="t">be much worse in terms of quality than y star less than T. And this discrepancy is terrible,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2543" target="_blank">00:42:23.900</a></span> | <span class="t">because it actually causes a discrepancy between training and test time, which actually hurts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2550" target="_blank">00:42:30.300</a></span> | <span class="t">model performance. And we call this problem exposure bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2555" target="_blank">00:42:35.980</a></span> | <span class="t">So people have proposed many solutions to address this exposure bias problem. One thing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2561" target="_blank">00:42:41.180</a></span> | <span class="t">to do is to do scheduled sampling, which means that with probability p, we try to decode</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2567" target="_blank">00:42:47.900</a></span> | <span class="t">a token and feed it back in as context to train the model. And with probability 1 minus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2573" target="_blank">00:42:53.860</a></span> | <span class="t">p, we use the gold token as context. So throughout training, we try to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2580" target="_blank">00:43:00.140</a></span> | <span class="t">p to gradually warm it up, and then prepare it for test time generation. So this leads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2586" target="_blank">00:43:06.500</a></span> | <span class="t">to improvement in practice, because using this p probabilities, we're actually gradually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2594" target="_blank">00:43:14.580</a></span> | <span class="t">trying to narrow the discrepancy between training and test time. But the objective is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2599" target="_blank">00:43:19.260</a></span> | <span class="t">quite strange, and training can be very unstable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2603" target="_blank">00:43:23.580</a></span> | <span class="t">Another idea is to do data set aggregation. And the method is called Dagger. Essentially,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2609" target="_blank">00:43:29.860</a></span> | <span class="t">at various interval during training, we try to generate a sequence of tags from the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2613" target="_blank">00:43:33.980</a></span> | <span class="t">model, and then use this, and then put this sequence of tags into the training data. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2619" target="_blank">00:43:39.060</a></span> | <span class="t">we're kind of continuously doing this training data augmentation scheme to make sure that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2624" target="_blank">00:43:44.940</a></span> | <span class="t">the training distribution and the generation distribution are closer together. So both</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2629" target="_blank">00:43:49.980</a></span> | <span class="t">approaches, both scheduled sampling and data set aggregation, are ways to narrow the discrepancy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2635" target="_blank">00:43:55.060</a></span> | <span class="t">between training and test. Yes, question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2638" target="_blank">00:43:58.820</a></span> | <span class="t">What is the gold token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2640" target="_blank">00:44:00.980</a></span> | <span class="t">Gold token just means human text. It means like-- well, when you train a language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2646" target="_blank">00:44:06.500</a></span> | <span class="t">you will see lots of corpus that are human written. Gold is just human. Yeah. OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2655" target="_blank">00:44:15.540</a></span> | <span class="t">So another approach is to do retrieval augmented generation. So we first learn to retrieve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2660" target="_blank">00:44:20.700</a></span> | <span class="t">a sequence from some existing corpus of prototypes. And then we train a model to actually edit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2666" target="_blank">00:44:26.140</a></span> | <span class="t">the retrieved text by doing insertion, deletion, or swapping. We can add or remove tokens from</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2673" target="_blank">00:44:33.260</a></span> | <span class="t">this prototype, and then try to modify it into another sentence. So this doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2680" target="_blank">00:44:40.260</a></span> | <span class="t">suffer from exposure bias, because we start from a high-quality prototype. So that at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2685" target="_blank">00:44:45.620</a></span> | <span class="t">training time and at test time, you don't really have the discrepancy anymore, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2689" target="_blank">00:44:49.460</a></span> | <span class="t">you are not generating from left to right.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2693" target="_blank">00:44:53.780</a></span> | <span class="t">Another approach is to do reinforcement learning. So here, the idea is to cast your generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2699" target="_blank">00:44:59.340</a></span> | <span class="t">problem as a Markov decision process. So there is the state s, which is the model's representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2706" target="_blank">00:45:06.700</a></span> | <span class="t">for all the preceding context. There is action a, which is basically the next token that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2712" target="_blank">00:45:12.740</a></span> | <span class="t">we are trying to pick. And there is policy, which is the language model, or also called</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2716" target="_blank">00:45:16.860</a></span> | <span class="t">the decoder. And there is the reward r, which is provided by some external score. And the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2722" target="_blank">00:45:22.540</a></span> | <span class="t">idea here-- well, we won't go into details about reinforcement learning and how it works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2728" target="_blank">00:45:28.220</a></span> | <span class="t">but we will recommend the class CS234.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2734" target="_blank">00:45:34.380</a></span> | <span class="t">So in the reinforcement learning context, because reinforcement learning involves a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2738" target="_blank">00:45:38.180</a></span> | <span class="t">reward function, that's very important. So how do we do reward estimation for text generation?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2744" target="_blank">00:45:44.020</a></span> | <span class="t">Well, really a natural idea is to just use the evaluation metrics. So whatever-- because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2749" target="_blank">00:45:49.060</a></span> | <span class="t">you are trying to do well in terms of evaluation, so why not just improve for evaluation metrics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2754" target="_blank">00:45:54.100</a></span> | <span class="t">directly at training time? For example, in the case of machine translation, we can use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2758" target="_blank">00:45:58.940</a></span> | <span class="t">blue score as the reward function. In the case of summarization, we can use root score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2763" target="_blank">00:46:03.980</a></span> | <span class="t">as the reward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2766" target="_blank">00:46:06.620</a></span> | <span class="t">But we really need to be careful about optimizing for tasks as opposed to gaining the reward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2772" target="_blank">00:46:12.140</a></span> | <span class="t">because evaluation metrics are merely proxies for the generation quality. So sometimes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2777" target="_blank">00:46:17.060</a></span> | <span class="t">you run RL and improve the blue score by a lot. But when you run human evaluations, humans</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2783" target="_blank">00:46:23.740</a></span> | <span class="t">might still think that, well, this generated text is no better than the previous one, or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2788" target="_blank">00:46:28.300</a></span> | <span class="t">even worse, even though it gives you a much better blue score. So we want to be careful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2793" target="_blank">00:46:33.540</a></span> | <span class="t">about this case of not gaining the reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2797" target="_blank">00:46:37.500</a></span> | <span class="t">So what behaviors can we tie to a reward function? This is about reward design and reward estimation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2802" target="_blank">00:46:42.780</a></span> | <span class="t">There are so many things that we can do. We can do cross-modality consistency for image</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2807" target="_blank">00:46:47.540</a></span> | <span class="t">captioning. We can do sentence simplicity to make sure that we are generating simple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2813" target="_blank">00:46:53.780</a></span> | <span class="t">English that are understandable. We can do formality and politeness to make sure that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2818" target="_blank">00:46:58.660</a></span> | <span class="t">I don't know, your chatbot doesn't suddenly yell at you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2822" target="_blank">00:47:02.780</a></span> | <span class="t">And the most important thing that's really, really popular recently is human preference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2828" target="_blank">00:47:08.860</a></span> | <span class="t">So we should just build a reward model that captures human preference. And this is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2834" target="_blank">00:47:14.220</a></span> | <span class="t">the technique behind the chat GPT model. So the idea here is that we would ask human to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2839" target="_blank">00:47:19.860</a></span> | <span class="t">rank a bunch of generated text based on their preference. And then we will use this preference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2844" target="_blank">00:47:24.740</a></span> | <span class="t">data to learn a reward function, which will basically always assign high score to something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2851" target="_blank">00:47:31.580</a></span> | <span class="t">that humans might prefer and assign low score to something that humans wouldn't prefer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2856" target="_blank">00:47:36.620</a></span> | <span class="t">Yeah, question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2857" target="_blank">00:47:37.620</a></span> | <span class="t">Would it be more expensive? It's like, is it all just real?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2862" target="_blank">00:47:42.620</a></span> | <span class="t">Oh yeah, sure. I mean, it is going to be very expensive. But I feel like compared to all</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2867" target="_blank">00:47:47.660</a></span> | <span class="t">the cost of training models, training like 170 billion parameter models, I feel like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2872" target="_blank">00:47:52.780</a></span> | <span class="t">OpenAI and Google are, well, they can afford hiring lots of humans to do human annotations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2878" target="_blank">00:47:58.300</a></span> | <span class="t">and ask their preference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2879" target="_blank">00:47:59.300</a></span> | <span class="t">How much data would we need to, like, give simple answers?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2884" target="_blank">00:48:04.300</a></span> | <span class="t">Yeah, this is a great question. So I think it's kind of a mystery about how much data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2889" target="_blank">00:48:09.860</a></span> | <span class="t">you exactly need to achieve the level of performance of chat GPT. But roughly speaking, I feel</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2895" target="_blank">00:48:15.500</a></span> | <span class="t">like, I mean, whenever you try to fine tune a model on some downstream task, similarly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2899" target="_blank">00:48:19.940</a></span> | <span class="t">here you are trying to fine tune your model on human preference. It do need quite a lot</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2904" target="_blank">00:48:24.940</a></span> | <span class="t">of data, like maybe on the scale of 50k to 100k. That's roughly the scale that-- like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2909" target="_blank">00:48:29.780</a></span> | <span class="t">Anthropic actually released some data set about human preference. That's roughly the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2913" target="_blank">00:48:33.780</a></span> | <span class="t">scale that they released, I think, if I remember correctly. Yeah, question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2918" target="_blank">00:48:38.780</a></span> | <span class="t">So we talked about earlier about how many of the state of the art language models use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2923" target="_blank">00:48:43.180</a></span> | <span class="t">transformers as their architecture. How do you apply reinforcement learning to this model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2930" target="_blank">00:48:50.860</a></span> | <span class="t">To what do you mean? To transformer model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2933" target="_blank">00:48:53.380</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2934" target="_blank">00:48:54.380</a></span> | <span class="t">Yeah, I feel like reinforcement learning is kind of a modeling tool. I mean, it's kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2940" target="_blank">00:49:00.220</a></span> | <span class="t">of an objective that you are trying to optimize. Instead of an MLE objective, now you are optimizing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2944" target="_blank">00:49:04.540</a></span> | <span class="t">for an RL objective. So it's kind of orthogonal to the architecture choice. So a transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2951" target="_blank">00:49:11.900</a></span> | <span class="t">is an architecture. You just use transformer to give you probability of the next token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2956" target="_blank">00:49:16.260</a></span> | <span class="t">distribution or to try to estimate probability of a sequence. And then once you have the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2961" target="_blank">00:49:21.420</a></span> | <span class="t">probability of a sequence, you use that probability of the sequence, pass it into the RL objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2967" target="_blank">00:49:27.660</a></span> | <span class="t">that you have. And then suppose that you are trying to do policy gradient or something,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2971" target="_blank">00:49:31.900</a></span> | <span class="t">then you need to estimate the probability of that sequence. And then you just need to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2976" target="_blank">00:49:36.460</a></span> | <span class="t">be able to backprop through transformer, which is doable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2980" target="_blank">00:49:40.340</a></span> | <span class="t">Yeah, so I think the question about architecture and objectives are orthogonal. So even if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2985" target="_blank">00:49:45.220</a></span> | <span class="t">you have an LSTM, you can do it. You have a transformer, you can also do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2988" target="_blank">00:49:48.700</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2989" target="_blank">00:49:49.700</a></span> | <span class="t">Cool. Hope I answered that question. Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2992" target="_blank">00:49:52.700</a></span> | <span class="t">And it just like with a model for this kind of reward. For example, we can do another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=2999" target="_blank">00:49:59.700</a></span> | <span class="t">transformer to calculate the reward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3002" target="_blank">00:50:02.420</a></span> | <span class="t">Yeah, I think that's exactly what they did. So for example, you would have GPT-3. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3008" target="_blank">00:50:08.020</a></span> | <span class="t">use GPT-3 as the generator that generate text. And you kind of have another pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3013" target="_blank">00:50:13.700</a></span> | <span class="t">that could probably also be GPT-3, but I'm guessing here, that you fine tune it to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3018" target="_blank">00:50:18.820</a></span> | <span class="t">human preference. And then once you have a human preference model, you use the human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3023" target="_blank">00:50:23.580</a></span> | <span class="t">preference model to put it into RL as the reward model. And then use the original GPT-3</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3029" target="_blank">00:50:29.140</a></span> | <span class="t">as the policy model. And then you apply RL objectives and then update them so that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3035" target="_blank">00:50:35.460</a></span> | <span class="t">will get a new model that's better at everything.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3038" target="_blank">00:50:38.940</a></span> | <span class="t">OK, cool. Yeah, actually, if you are very curious about RLHF, I would encourage you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3045" target="_blank">00:50:45.020</a></span> | <span class="t">to come to the next lecture, where Jesse will talk about RLHF. RLHF is shorthand for RL</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3054" target="_blank">00:50:54.660</a></span> | <span class="t">using human feedback.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3059" target="_blank">00:50:59.100</a></span> | <span class="t">So takeaways. T-shirt forcing is still the main algorithm for training text generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3065" target="_blank">00:51:05.300</a></span> | <span class="t">models. And exposure bias causes problems in text generation models. For example, it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3070" target="_blank">00:51:10.940</a></span> | <span class="t">causes models to lose coherence, causes model to be repetitive. And models must learn to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3076" target="_blank">00:51:16.180</a></span> | <span class="t">recover from their own bad samples by using techniques like scheduled sampling or a dagger.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3082" target="_blank">00:51:22.860</a></span> | <span class="t">And models shouldn't-- another approach to reduce exposure bias is to start with good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3088" target="_blank">00:51:28.540</a></span> | <span class="t">text, like retrieval plus generation. And we also discussed how to do training with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3092" target="_blank">00:51:32.780</a></span> | <span class="t">RL. And this can actually make model learn behaviors that are preferred by human-- that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3100" target="_blank">00:51:40.260</a></span> | <span class="t">are preferred by human or preferred by some metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3103" target="_blank">00:51:43.180</a></span> | <span class="t">So to be very up to date, in the best language model nowadays, chat-GPT, the training is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3109" target="_blank">00:51:49.220</a></span> | <span class="t">actually pipelined. For example, we would first pre-train a large language models using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3113" target="_blank">00:51:53.420</a></span> | <span class="t">internet corpus by self-supervision. And this kind of gets you chat-GPT-- sorry, GPT-3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3119" target="_blank">00:51:59.740</a></span> | <span class="t">which is the original version. And then you would do some sort of instruction tuning to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3124" target="_blank">00:52:04.260</a></span> | <span class="t">fine-tune the language model, to fine-tune the pre-trained language model so that it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3127" target="_blank">00:52:07.500</a></span> | <span class="t">learns roughly how to follow human instructions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3130" target="_blank">00:52:10.900</a></span> | <span class="t">And finally, we would do RLHF to make sure that these models are well-aligned with human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3135" target="_blank">00:52:15.580</a></span> | <span class="t">preference. So if we start RLHF from scratch, it's probably going to be very hard for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3141" target="_blank">00:52:21.700</a></span> | <span class="t">model to converge, because RL is hard to train for text data, et cetera. So RL doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3147" target="_blank">00:52:27.500</a></span> | <span class="t">work from scratch. But with all these smart tricks about pre-training and instruction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3153" target="_blank">00:52:33.260</a></span> | <span class="t">tuning, suddenly now they're off to a good start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3159" target="_blank">00:52:39.060</a></span> | <span class="t">Cool. Any questions so far? OK. Oh, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3165" target="_blank">00:52:45.060</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3175" target="_blank">00:52:55.060</a></span> | <span class="t">You mean the difference between Dagger and schedule sampling is how long the sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3181" target="_blank">00:53:01.300</a></span> | <span class="t">are? Yeah, I think roughly that is it. Because for Dagger, you are trying to put in full-generated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3190" target="_blank">00:53:10.100</a></span> | <span class="t">sequence. But I feel like there can be variations of Dagger. Dagger is just like a high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3193" target="_blank">00:53:13.740</a></span> | <span class="t">framework and idea. There can be variations of Dagger that are very similar to schedule</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3198" target="_blank">00:53:18.820</a></span> | <span class="t">sampling, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3200" target="_blank">00:53:20.140</a></span> | <span class="t">I feel like for schedule sampling, it's kind of a more smoothed version of Dagger. Because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3204" target="_blank">00:53:24.860</a></span> | <span class="t">for Dagger, you have to-- well, basically, for this epoch, I am generating something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3211" target="_blank">00:53:31.260</a></span> | <span class="t">And then after this epoch finishes, I put this into the data together and then train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3215" target="_blank">00:53:35.380</a></span> | <span class="t">for another epoch. Whereas Dagger seems to be more flexible in terms of where you add</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3219" target="_blank">00:53:39.940</a></span> | <span class="t">data. Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3220" target="_blank">00:53:40.940</a></span> | <span class="t">So for Dagger, if you regress the models coming out, how does it help the model?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3228" target="_blank">00:53:48.740</a></span> | <span class="t">I think that's a good question. I feel like if you regress the model-- for example, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3234" target="_blank">00:53:54.900</a></span> | <span class="t">you regress the model on its own output, I think there should be smarter ways than to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3241" target="_blank">00:54:01.740</a></span> | <span class="t">exactly regress on your own output. For example, you might still consult some gold reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3246" target="_blank">00:54:06.780</a></span> | <span class="t">data, for example, given that you ask the model to generate for something. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3250" target="_blank">00:54:10.980</a></span> | <span class="t">you can, instead of using-- say you ask the model to generate for five tokens. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3256" target="_blank">00:54:16.420</a></span> | <span class="t">instead of using the model's generation to be the sixth token, you'll probably try to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3261" target="_blank">00:54:21.980</a></span> | <span class="t">find some examples in the training data that would be good continuations. And then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3266" target="_blank">00:54:26.660</a></span> | <span class="t">try to plug that in by connecting the model generation and some gold text. And then therefore,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3274" target="_blank">00:54:34.100</a></span> | <span class="t">you are able to correct the model, even though it probably went off path a little bit by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3279" target="_blank">00:54:39.700</a></span> | <span class="t">generating its own stuff. So it's kind of like letting the model learn how to correct</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3283" target="_blank">00:54:43.060</a></span> | <span class="t">for itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3284" target="_blank">00:54:44.060</a></span> | <span class="t">But yes, I think you are right. If you just put model generation in the data, it shouldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3291" target="_blank">00:54:51.540</a></span> | <span class="t">really work. Yeah. Any other questions? Cool. Moving on. Yes. So now we'll talk about how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3308" target="_blank">00:55:08.020</a></span> | <span class="t">we are going to evaluate NLG systems. So there are three types of methods for evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3313" target="_blank">00:55:13.540</a></span> | <span class="t">There is content overlap metrics. There is model-based metrics. And there is human evaluations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3320" target="_blank">00:55:20.340</a></span> | <span class="t">So first, content overlap metrics compute a score based on lexical similarities between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3325" target="_blank">00:55:25.100</a></span> | <span class="t">the generated text and the gold reference text. So the advantage of this approach is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3329" target="_blank">00:55:29.460</a></span> | <span class="t">that it's very fast and efficient and widely used. For example, a blue score is very popular</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3335" target="_blank">00:55:35.300</a></span> | <span class="t">in MT. And rouge score is very popular in summarization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3341" target="_blank">00:55:41.660</a></span> | <span class="t">So these methods are very popular because they are cheap and easy to run. But they are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3347" target="_blank">00:55:47.940</a></span> | <span class="t">not really the ideal metrics. For example, simply relying on lexical overlap might miss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3353" target="_blank">00:55:53.540</a></span> | <span class="t">some refreezings that have the same semantic meaning. Or it might reward text with a large</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3359" target="_blank">00:55:59.180</a></span> | <span class="t">portion of lexical overlap, but actually have the opposite meaning. So you have lots of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3363" target="_blank">00:56:03.620</a></span> | <span class="t">both false positive and false negative problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3367" target="_blank">00:56:07.540</a></span> | <span class="t">So despite all these disadvantages, the metrics are still the to-go evaluation standard in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3372" target="_blank">00:56:12.300</a></span> | <span class="t">machine translation. Part of the reason is that MT is actually super close-ended. It's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3378" target="_blank">00:56:18.060</a></span> | <span class="t">very non-open-ended. And then therefore, this is probably still fine to use a blue score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3385" target="_blank">00:56:25.020</a></span> | <span class="t">to measure machine translation. And they get progressively worse for tasks that are more</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3389" target="_blank">00:56:29.740</a></span> | <span class="t">open-ended. For example, they get worse for summarization, as long as the output text--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3395" target="_blank">00:56:35.580</a></span> | <span class="t">because the output text becomes much harder to measure. They are much worse for dialogue,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3400" target="_blank">00:56:40.340</a></span> | <span class="t">which is more open-ended. And then they are much, much worse for story generation, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3404" target="_blank">00:56:44.100</a></span> | <span class="t">is also open-ended. And then the drawback here is that because the n-gram metrics--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3411" target="_blank">00:56:51.100</a></span> | <span class="t">this is because suppose that you are generating a story that's relatively long. Then if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3415" target="_blank">00:56:55.740</a></span> | <span class="t">are still looking at word overlap, then you might actually get very high n-gram scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3420" target="_blank">00:57:00.380</a></span> | <span class="t">because of your text is very long, not because it's actually of high quality. Just because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3424" target="_blank">00:57:04.780</a></span> | <span class="t">you are talking so much that you might have covered lots of points already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3428" target="_blank">00:57:08.580</a></span> | <span class="t">Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3429" target="_blank">00:57:09.580</a></span> | <span class="t">[INAUDIBLE]</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3430" target="_blank">00:57:10.580</a></span> | <span class="t">Yes, exactly. That's the next thing that I will talk about as a better metric for evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3442" target="_blank">00:57:22.700</a></span> | <span class="t">But for now, let's do a case study of a failure mode for blue score, for example. So suppose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3449" target="_blank">00:57:29.300</a></span> | <span class="t">that Chris asked the question, are you enjoying the CS224L lectures? The correct answer, of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3453" target="_blank">00:57:33.900</a></span> | <span class="t">course, is heck yes. So if we have this, if one of the answers is yes, it will get a score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3462" target="_blank">00:57:42.220</a></span> | <span class="t">of 0.61 because it has some lexical overlap with the correct answer. If you answer you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3468" target="_blank">00:57:48.100</a></span> | <span class="t">know it, then it gets a relatively lower score because it doesn't really have any lexical</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3473" target="_blank">00:57:53.420</a></span> | <span class="t">overlap except from the exclamation mark. And if you answer yep, this is semantically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3479" target="_blank">00:57:59.420</a></span> | <span class="t">correct, but it actually gets 0 score because there is no lexical overlap between the gold</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3485" target="_blank">00:58:05.340</a></span> | <span class="t">answer and the generation. If you answer heck no, this should be wrong. But because it has</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3492" target="_blank">00:58:12.020</a></span> | <span class="t">lots of lexical overlap with the correct answer, it's actually getting some high scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3499" target="_blank">00:58:19.740</a></span> | <span class="t">So these two cases are the major failure modes of lexical-based n-gram overlap metrics. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3506" target="_blank">00:58:26.060</a></span> | <span class="t">get false negatives and false positives. So moving beyond this failure modes of lexical-based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3515" target="_blank">00:58:35.180</a></span> | <span class="t">metrics, the next step is to check for semantic similarities. And model-based metrics are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3520" target="_blank">00:58:40.380</a></span> | <span class="t">better at capturing the semantic similarities. So this is kind of similar to what you kind</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3525" target="_blank">00:58:45.200</a></span> | <span class="t">of raised up a couple minutes ago. We can actually use learned representation of words</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3530" target="_blank">00:58:50.700</a></span> | <span class="t">and sentences to compute semantic similarities between generated and referenced text. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3538" target="_blank">00:58:58.460</a></span> | <span class="t">now we are no longer bottlenecked by n-gram. And instead, we're using embeddings. And these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3543" target="_blank">00:59:03.820</a></span> | <span class="t">embeddings are going to be pre-trained. But the methods can still move on because we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3547" target="_blank">00:59:07.900</a></span> | <span class="t">just swap in different pre-trained method and use the fixed metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3552" target="_blank">00:59:12.460</a></span> | <span class="t">So here are some good examples of the metrics that could be used. One thing is to do vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3557" target="_blank">00:59:17.540</a></span> | <span class="t">similarity. This is very similar to homework one, where you are trying to compute similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3562" target="_blank">00:59:22.340</a></span> | <span class="t">between words, except now we are trying to compute similarity between sentences. There</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3567" target="_blank">00:59:27.620</a></span> | <span class="t">are some ideas of how to go from word similarity to sentence similarities. For example, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3572" target="_blank">00:59:32.100</a></span> | <span class="t">can just average the embedding, which is like a relatively naive idea, but it works sometimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3580" target="_blank">00:59:40.260</a></span> | <span class="t">Another high-level idea is that we can measure word movers distance. The idea here is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3587" target="_blank">00:59:47.460</a></span> | <span class="t">we can use optimal transports to align the source and target word embeddings. Suppose</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3592" target="_blank">00:59:52.180</a></span> | <span class="t">that your source word embedding is Obama speaks to the media in Illinois, and the target is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3599" target="_blank">00:59:59.820</a></span> | <span class="t">the president grace the press in Chicago. From a human evaluation perspective, these</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3604" target="_blank">01:00:04.100</a></span> | <span class="t">two are actually very similar, but they are not exactly aligned word by word. So we need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3609" target="_blank">01:00:09.100</a></span> | <span class="t">to figure out how to optimally align words to words, like align Obama to president, align</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3614" target="_blank">01:00:14.220</a></span> | <span class="t">Chicago to Illinois, and then therefore we can compute a score. We can compute the pairwise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3620" target="_blank">01:00:20.100</a></span> | <span class="t">word embedding difference between this, and then get a good score for the sentence similarities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3627" target="_blank">01:00:27.540</a></span> | <span class="t">And finally, there is BERT score, which is also a very popular metric for semantic similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3632" target="_blank">01:00:32.580</a></span> | <span class="t">So it first computes pairwise cosine distance using BERT embeddings, and then it finds an</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3638" target="_blank">01:00:38.260</a></span> | <span class="t">optimal alignment between the source and target sentence, and then it finally computes some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3643" target="_blank">01:00:43.260</a></span> | <span class="t">score. So I feel like these details are not really that important, but the high-level</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3648" target="_blank">01:00:48.580</a></span> | <span class="t">idea is super important, is that we can now use word embeddings to compute sentence similarities</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3656" target="_blank">01:00:56.180</a></span> | <span class="t">by doing some sort of smart alignment, and then transform from word similarity to sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3660" target="_blank">01:01:00.620</a></span> | <span class="t">similarities. To move beyond word embeddings, we can also use sentence embeddings to compute</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3666" target="_blank">01:01:06.820</a></span> | <span class="t">sentence similarities. So typically, this doesn't have the very comprehensive alignment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3671" target="_blank">01:01:11.260</a></span> | <span class="t">by word problem, but it has similar problems about you need to now align sentences or phrases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3676" target="_blank">01:01:16.580</a></span> | <span class="t">in a sentence. And similarly, there is BLURT, which is slightly different. It is a regression</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3682" target="_blank">01:01:22.340</a></span> | <span class="t">model based on BERT. So the model is trained as a regression problem to return a score</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3689" target="_blank">01:01:29.140</a></span> | <span class="t">that indicates how good the text is in terms of grammaticality and the meaning of the reference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3693" target="_blank">01:01:33.620</a></span> | <span class="t">text, and similarity with the reference text. So this is kind of a training evaluation as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3698" target="_blank">01:01:38.020</a></span> | <span class="t">a regression problem. Any questions so far? OK, cool. You can move on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3711" target="_blank">01:01:51.180</a></span> | <span class="t">So all the previous mentioned approaches are evaluating semantic similarities, so they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3715" target="_blank">01:01:55.500</a></span> | <span class="t">can be applied to non-open-ended generation tasks. But what about open-ended settings?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3720" target="_blank">01:02:00.980</a></span> | <span class="t">So here, enforcing semantic similarity seems wrong, because a story can be perfectly fluent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3726" target="_blank">01:02:06.220</a></span> | <span class="t">and perfectly high quality without having to reassemble any of the reference stories.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3731" target="_blank">01:02:11.380</a></span> | <span class="t">So one idea here is that maybe we want to evaluate open-ended text generation using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3736" target="_blank">01:02:16.780</a></span> | <span class="t">this MOV score. MOV score computes the information divergence in a quantized embedding space</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3743" target="_blank">01:02:23.260</a></span> | <span class="t">between the generated text and the goal reference text. So here is roughly the detail of what's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3748" target="_blank">01:02:28.420</a></span> | <span class="t">going on. Suppose that you have a batch of text from the goal reference that are human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3753" target="_blank">01:02:33.220</a></span> | <span class="t">written, and you have a batch of text that's generated by your model. Step number one is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3757" target="_blank">01:02:37.820</a></span> | <span class="t">that you want to embed this text. You want to put this text into some continuous representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3762" target="_blank">01:02:42.340</a></span> | <span class="t">space, which is kind of the figure to the left. But it's really hard to compute any</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3767" target="_blank">01:02:47.660</a></span> | <span class="t">distance metrics in this continuous embedding space, because different sentences might actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3773" target="_blank">01:02:53.140</a></span> | <span class="t">lie very far away from each other. So the idea here is that we are trying to do a k-means</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3778" target="_blank">01:02:58.300</a></span> | <span class="t">cluster to discretize the continuous space into some discrete space. Now, after the discretization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3784" target="_blank">01:03:04.460</a></span> | <span class="t">we can actually have a histogram for the goal human written text and a histogram for the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3791" target="_blank">01:03:11.220</a></span> | <span class="t">machine generated text. And then we can now compute precision recall using these two discretized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3797" target="_blank">01:03:17.020</a></span> | <span class="t">distributions. And then we can compute precision by forward KL and recall by backward KL. Yes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3803" target="_blank">01:03:23.300</a></span> | <span class="t">question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3804" target="_blank">01:03:24.300</a></span> | <span class="t">Why do we want to discretize it? I didn't catch that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3808" target="_blank">01:03:28.780</a></span> | <span class="t">So imagine that you-- suppose-- maybe it's equivalent to answer, why is it hard to work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3814" target="_blank">01:03:34.300</a></span> | <span class="t">with the continuous space? The idea is if you embed a sentence into the continuous space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3820" target="_blank">01:03:40.580</a></span> | <span class="t">say that it lies here, and you embed another sentence in a continuous space that lies here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3824" target="_blank">01:03:44.740</a></span> | <span class="t">suppose that you only have a finite number of sentences. Then they would basically be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3828" target="_blank">01:03:48.660</a></span> | <span class="t">direct delta distributions in your manifold. So it's hard to-- you probably want a smoother</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3835" target="_blank">01:03:55.460</a></span> | <span class="t">distribution. But it's hard to define what is a good, smooth distribution in the case</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3839" target="_blank">01:03:59.940</a></span> | <span class="t">of text embedding, because they're not super interpretable. So therefore, eventually, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3844" target="_blank">01:04:04.100</a></span> | <span class="t">will have-- if you embed everything in a continuous space, you will have lots of direct deltas</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3850" target="_blank">01:04:10.380</a></span> | <span class="t">that are just very high and then not really connected to its neighbors. So it's hard to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3857" target="_blank">01:04:17.540</a></span> | <span class="t">quantify KL divergence or a distance matrix in that space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3862" target="_blank">01:04:22.220</a></span> | <span class="t">For example, you have to make some assumptions. For example, you want to make Gaussian assumptions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3866" target="_blank">01:04:26.060</a></span> | <span class="t">that I want to smooth all the embeddings by convolving it with a Gaussian. And then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3871" target="_blank">01:04:31.260</a></span> | <span class="t">can start getting some meaningful distance metrics. But with just the embeddings alone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3877" target="_blank">01:04:37.540</a></span> | <span class="t">you're not going to get meaningful distance metrics. And then it doesn't really make sense</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3880" target="_blank">01:04:40.540</a></span> | <span class="t">to smooth things using Gaussian, because who said word representations are Gaussian related?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3886" target="_blank">01:04:46.060</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3887" target="_blank">01:04:47.060</a></span> | <span class="t">Question?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3888" target="_blank">01:04:48.060</a></span> | <span class="t">How do you know it would be continuous to understand distributions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3891" target="_blank">01:04:51.380</a></span> | <span class="t">I think this requires some Gaussian smoothing. Yeah, I think that the plot is made with some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3895" target="_blank">01:04:55.780</a></span> | <span class="t">smoothing. Yeah, I mean, I didn't make the plot, so I couldn't be perfectly sure. But</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3899" target="_blank">01:04:59.980</a></span> | <span class="t">I think the fact that it looks like this means that you smooth it a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3903" target="_blank">01:05:03.220</a></span> | <span class="t">So you put in word embeddings and--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3906" target="_blank">01:05:06.100</a></span> | <span class="t">These are sentence embeddings or concatenated word embeddings, because you are comparing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3910" target="_blank">01:05:10.380</a></span> | <span class="t">sentences to sentences, not words to words. Yeah, so the advantage of MOLF score is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3916" target="_blank">01:05:16.980</a></span> | <span class="t">it is applicable to open-ended settings, because you are now measuring precision and recall</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3922" target="_blank">01:05:22.820</a></span> | <span class="t">with regard to the target distribution. Cool. So it has a better probabilistic interpretation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3930" target="_blank">01:05:30.580</a></span> | <span class="t">than all the previous similarity metrics. Cool. Any other questions? Yes?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3937" target="_blank">01:05:37.980</a></span> | <span class="t">I'm just not entirely clear. So if we're trying to maximize precision, we can call it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3942" target="_blank">01:05:42.980</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3943" target="_blank">01:05:43.980</a></span> | <span class="t">How is that different from just trying to maximize the similarity between the target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3947" target="_blank">01:05:47.980</a></span> | <span class="t">and the distribution?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3948" target="_blank">01:05:48.980</a></span> | <span class="t">Oh, yeah, that's a good question. Well, this is because in a case where it's really hard</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3955" target="_blank">01:05:55.580</a></span> | <span class="t">to get exactly the same thing-- well, for example, I would say that maybe-- because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3960" target="_blank">01:06:00.580</a></span> | <span class="t">I've never tried this myself, but if you try to run MOLF on a machine translation task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3965" target="_blank">01:06:05.660</a></span> | <span class="t">you might get very high score. But if you try to run Bool score on the open-ended text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3971" target="_blank">01:06:11.260</a></span> | <span class="t">generation, you will get super low score. So it's just not really measurable, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3974" target="_blank">01:06:14.900</a></span> | <span class="t">everything's so different from each other. So I feel like MOLF is kind of a middle ground,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3979" target="_blank">01:06:19.780</a></span> | <span class="t">where you are trying to evaluate something that are actually very far away from each</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3983" target="_blank">01:06:23.460</a></span> | <span class="t">other, but you still want a meaningful representation. Of course, I mean, if your source and target</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3990" target="_blank">01:06:30.380</a></span> | <span class="t">are exactly the same or are just different up to some rephrasing, you will get the best</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3994" target="_blank">01:06:34.780</a></span> | <span class="t">MOLF score. But maybe that's not really what you're looking for, because given the current</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=3999" target="_blank">01:06:39.940</a></span> | <span class="t">situation, you only have generations that are very far away from the gold text. How</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4003" target="_blank">01:06:43.940</a></span> | <span class="t">do we evaluate this type of things?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4006" target="_blank">01:06:46.220</a></span> | <span class="t">Yes, question in the back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4008" target="_blank">01:06:48.900</a></span> | <span class="t">I'm still trying to understand the MOF score. Is it possible to write out the map, even</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4014" target="_blank">01:06:54.500</a></span> | <span class="t">in just kind of pseudo, simple form?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4017" target="_blank">01:06:57.700</a></span> | <span class="t">Yeah, I think it's possible. I mean, maybe we can put this discussion after class, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4022" target="_blank">01:07:02.780</a></span> | <span class="t">I kind of want to finish my slides. Yeah, but happy to chat after class. There is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4028" target="_blank">01:07:08.060</a></span> | <span class="t">paper about it if you search for MOLF score. I think it's probably the best paper in some</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4032" target="_blank">01:07:12.860</a></span> | <span class="t">ICML or Europe's conference as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4036" target="_blank">01:07:16.020</a></span> | <span class="t">OK, so moving on. I've pointed out that there are so many evaluation methods. So let's take</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4042" target="_blank">01:07:22.300</a></span> | <span class="t">a step back and think about what's a good metric for evaluation methods. So how do we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4046" target="_blank">01:07:26.660</a></span> | <span class="t">evaluate evaluations? Nowadays, the gold standard is still to check how well this metric is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4052" target="_blank">01:07:32.580</a></span> | <span class="t">aligned with human judgment. So if a model match human preference, in other words, if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4060" target="_blank">01:07:40.980</a></span> | <span class="t">the metric correlates very strongly with human judgment, then we say that the metric is a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4065" target="_blank">01:07:45.100</a></span> | <span class="t">good metric. So in this plot, people have plot blue score and human score on y and x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4072" target="_blank">01:07:52.140</a></span> | <span class="t">axis respectively. And then because we didn't see a correlation, a strong correlation, this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4076" target="_blank">01:07:56.580</a></span> | <span class="t">kind of suggests that blue score is not a very good metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4081" target="_blank">01:08:01.660</a></span> | <span class="t">So actually, the gold standard for human evaluation-- the gold standard for evaluating language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4087" target="_blank">01:08:07.860</a></span> | <span class="t">models is always to do human evaluation. So automatic metrics fall short of matching human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4094" target="_blank">01:08:14.860</a></span> | <span class="t">decisions. And human evaluation is kind of the most important criteria for evaluating</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4100" target="_blank">01:08:20.620</a></span> | <span class="t">text that are generated from a model. And it's also the gold standard in developing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4105" target="_blank">01:08:25.200</a></span> | <span class="t">automatic metrics because we want everything to match human evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4111" target="_blank">01:08:31.620</a></span> | <span class="t">So what do we mean by human evaluation? How is it conducted? Typically, we will provide</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4116" target="_blank">01:08:36.780</a></span> | <span class="t">human annotators with some axes that we care about, like fluency, coherence for open-ended</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4123" target="_blank">01:08:43.460</a></span> | <span class="t">text generation. Suppose that we also care about factuality for summarization. We care</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4127" target="_blank">01:08:47.900</a></span> | <span class="t">about style of the writing and common sense, for example, if we're trying to write a children's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4132" target="_blank">01:08:52.740</a></span> | <span class="t">story.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4136" target="_blank">01:08:56.540</a></span> | <span class="t">Essentially, another thing to note is that please don't compare human evaluations across</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4140" target="_blank">01:09:00.720</a></span> | <span class="t">different papers or different studies because human evaluations tends to not be well-collaborated</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4145" target="_blank">01:09:05.660</a></span> | <span class="t">and are not really reproducible. Even though we believe that human evaluations are the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4150" target="_blank">01:09:10.900</a></span> | <span class="t">gold standard, there are still many drawbacks. For example, human evaluations are really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4155" target="_blank">01:09:15.580</a></span> | <span class="t">slow and expensive. But even beyond the slow and expensiveness, they are still not perfect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4163" target="_blank">01:09:23.300</a></span> | <span class="t">because first, human evaluations, the results may be inconsistent, and they may not be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4168" target="_blank">01:09:28.220</a></span> | <span class="t">reproducible. So if you ask the same human whether you like A or B, they might say A</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4172" target="_blank">01:09:32.100</a></span> | <span class="t">the first time and B the second time. And then human evaluations are typically not really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4177" target="_blank">01:09:37.500</a></span> | <span class="t">logical. And sometimes, human annotators might misinterpret your question. Suppose that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4184" target="_blank">01:09:44.580</a></span> | <span class="t">want them to measure coherence of the text. Different people have different criteria for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4188" target="_blank">01:09:48.860</a></span> | <span class="t">coherence. Some people might think coherence is equivalent to fluency, and then they look</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4193" target="_blank">01:09:53.020</a></span> | <span class="t">for grammaticality errors. Some people might think coherence means how well your continuation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4198" target="_blank">01:09:58.860</a></span> | <span class="t">is aligned with the prompt or the topic. So there are all sorts of misunderstandings that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4205" target="_blank">01:10:05.100</a></span> | <span class="t">might make human evaluation very hard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4208" target="_blank">01:10:08.420</a></span> | <span class="t">And finally, human evaluation only measures precision, not recall. This means that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4213" target="_blank">01:10:13.380</a></span> | <span class="t">can give a sentence to human and ask the human, how do you like the sentence? But you couldn't</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4218" target="_blank">01:10:18.220</a></span> | <span class="t">ask the human whether this model is able to generate all possible sentences that are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4224" target="_blank">01:10:24.460</a></span> | <span class="t">So it's only a precision-based metrics, not a recall-based metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4228" target="_blank">01:10:28.260</a></span> | <span class="t">So here are two approaches that tries to combine human evaluations with modeling. For example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4236" target="_blank">01:10:36.260</a></span> | <span class="t">the first idea is basically trying to learn a metric from human judgment, basically by</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4242" target="_blank">01:10:42.540</a></span> | <span class="t">trying to use human judgment data as training data, and then train a model to simulate human</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4248" target="_blank">01:10:48.420</a></span> | <span class="t">judgment. And the second approach is trying to ask human and model to collaborate so that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4254" target="_blank">01:10:54.780</a></span> | <span class="t">the human would be in charge of evaluating precision, whereas the model would be in charge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4258" target="_blank">01:10:58.940</a></span> | <span class="t">of evaluating recall.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4261" target="_blank">01:11:01.140</a></span> | <span class="t">Also, we have tried approaches in terms of evaluating models interactively. So in this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4267" target="_blank">01:11:07.380</a></span> | <span class="t">case, we not only care about the output quality, we also care about how the person feels when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4274" target="_blank">01:11:14.460</a></span> | <span class="t">they interact with the model, when they try to be a co-author with the model, and how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4278" target="_blank">01:11:18.860</a></span> | <span class="t">the person feels about the writing process, et cetera. So this is called trying to evaluate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4284" target="_blank">01:11:24.900</a></span> | <span class="t">the models more interactively.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4289" target="_blank">01:11:29.300</a></span> | <span class="t">So the takeaway here is that content overlap is a bad metric. Model-based metrics become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4295" target="_blank">01:11:35.900</a></span> | <span class="t">better because it's more focused on semantics, but it's still not good enough. Human judgment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4300" target="_blank">01:11:40.900</a></span> | <span class="t">is the gold standard, but it's hard to do human judgment-- it's hard to do human study</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4305" target="_blank">01:11:45.140</a></span> | <span class="t">well. And in many cases, this is a hint for final project. The best judge of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4311" target="_blank">01:11:51.420</a></span> | <span class="t">quality is actually you. So if you want to do a final project in natural language generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4316" target="_blank">01:11:56.980</a></span> | <span class="t">you should look at the model output yourself. And don't just rely on the numbers that are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4321" target="_blank">01:12:01.740</a></span> | <span class="t">reported by Blue Swirl or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4324" target="_blank">01:12:04.940</a></span> | <span class="t">Cool. So finally, we will discuss ethical considerations of natural language generation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4331" target="_blank">01:12:11.220</a></span> | <span class="t">problems. So as language models get better and better, ethical considerations become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4336" target="_blank">01:12:16.980</a></span> | <span class="t">much more pressing. So we want to ensure that the models are well-aligned with human values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4341" target="_blank">01:12:21.940</a></span> | <span class="t">For example, we want to make sure the models are not harmful, they are not toxic, and we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4346" target="_blank">01:12:26.820</a></span> | <span class="t">want to make sure that the models are unbiased and fair to all demographics groups.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4351" target="_blank">01:12:31.460</a></span> | <span class="t">So for example here, we also don't want the model to generate any harmful content. Basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4357" target="_blank">01:12:37.460</a></span> | <span class="t">I try to prompt ChatGPT to say, can you write me some toxic content? ChatGPT politely refused</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4362" target="_blank">01:12:42.860</a></span> | <span class="t">me, which I'm quite happy about. But there are other people who try to jailbreak ChatGPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4371" target="_blank">01:12:51.860</a></span> | <span class="t">The idea here is that ChatGPT-- actually, I think internally, they probably implement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4376" target="_blank">01:12:56.060</a></span> | <span class="t">some detection tools so that when you try to prompt it adversarially, it's going to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4380" target="_blank">01:13:00.380</a></span> | <span class="t">avoid doing adversarial things. But here, there are many very complicated ways to prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4386" target="_blank">01:13:06.820</a></span> | <span class="t">ChatGPT so that you can get over the firewall and then therefore still ask ChatGPT to generate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4393" target="_blank">01:13:13.420</a></span> | <span class="t">some bad English.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4402" target="_blank">01:13:22.140</a></span> | <span class="t">So another problem with these large language models is that they are not necessarily truthful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4407" target="_blank">01:13:27.940</a></span> | <span class="t">So for example, this very famous news that Google's model actually generated factual</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4413" target="_blank">01:13:33.460</a></span> | <span class="t">errors, which is quite disappointing. But the way the model talks about it is very convincing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4421" target="_blank">01:13:41.660</a></span> | <span class="t">So you wouldn't really know that it's a factual error unless you go check that this is not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4426" target="_blank">01:13:46.100</a></span> | <span class="t">the first picture or something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4431" target="_blank">01:13:51.060</a></span> | <span class="t">So we want to avoid this type of problems. Actually, the models have already been trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4435" target="_blank">01:13:55.700</a></span> | <span class="t">very hard to reframe from generating harmful content. But for models that are more open-sourced</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4443" target="_blank">01:14:03.540</a></span> | <span class="t">and are smaller, the same problem still appears. And then typically, when we do our final projects</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4449" target="_blank">01:14:09.460</a></span> | <span class="t">or when we work with models, we are probably going to deal with much smaller models. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4453" target="_blank">01:14:13.500</a></span> | <span class="t">then therefore, we need to think about ways to deal with these problems better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4457" target="_blank">01:14:17.540</a></span> | <span class="t">So text generation models are often constructed from pre-trained language models. And then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4461" target="_blank">01:14:21.980</a></span> | <span class="t">pre-trained language models are trained on internet data, which contains lots of harmful</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4465" target="_blank">01:14:25.940</a></span> | <span class="t">stuff and bias. So when the models are prompted for this information, they will just repeat</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4473" target="_blank">01:14:33.340</a></span> | <span class="t">the negative stereotypes that they learn from the internet training data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4477" target="_blank">01:14:37.060</a></span> | <span class="t">So one way to avoid this is to do extensive data cleaning so that the pre-training data</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4481" target="_blank">01:14:41.980</a></span> | <span class="t">does not contain any bias or stereotypical content. However, this is going to be very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4486" target="_blank">01:14:46.700</a></span> | <span class="t">labor-intensive and almost impossible to do because filtering a large amount of internet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4491" target="_blank">01:14:51.100</a></span> | <span class="t">data is just so costly that it's not really possible.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4496" target="_blank">01:14:56.860</a></span> | <span class="t">Again, with existing language models like GPT-2 Medium, there are some adversarial inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4503" target="_blank">01:15:03.860</a></span> | <span class="t">that almost always trigger toxic content. And these models might be exploited in the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4509" target="_blank">01:15:09.100</a></span> | <span class="t">real world by ill-intended people. So for example, there is a paper about universal</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4515" target="_blank">01:15:15.820</a></span> | <span class="t">adversarial triggers where the authors just find some universal set of words that would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4521" target="_blank">01:15:21.060</a></span> | <span class="t">trigger bad content from the-- that would trigger toxic content from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4528" target="_blank">01:15:28.300</a></span> | <span class="t">And sometimes, even if you don't try to trigger the model, the model might still start to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4532" target="_blank">01:15:32.180</a></span> | <span class="t">generate toxic content by itself. So in this case, the pre-trained language models are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4538" target="_blank">01:15:38.100</a></span> | <span class="t">prompted with very innocuous prompts, but they still degenerate into toxic content.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4543" target="_blank">01:15:43.540</a></span> | <span class="t">So the takeaway here is that models really shouldn't be deployed without proper safeguards</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4548" target="_blank">01:15:48.940</a></span> | <span class="t">to control for toxic content or any harmful contents in general. And models should not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4553" target="_blank">01:15:53.420</a></span> | <span class="t">be deployed without careful considerations of how users will interact with these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4562" target="_blank">01:16:02.300</a></span> | <span class="t">So in the ethics section, one major takeaway is that we are trying to advocate that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4567" target="_blank">01:16:07.460</a></span> | <span class="t">need to think more about the model that you are building. So before deploying or publishing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4573" target="_blank">01:16:13.420</a></span> | <span class="t">any NLG models, please check if the model's output is not harmful. And please check if</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4579" target="_blank">01:16:19.340</a></span> | <span class="t">the model is more robust-- is robust to all the trigger words and other adversarial prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4585" target="_blank">01:16:25.380</a></span> | <span class="t">And of course, there are more. So well, basically, one can never do enough to improve the ethics</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4590" target="_blank">01:16:30.660</a></span> | <span class="t">of text generation systems. And OK, cool. I still have three minutes left, so I can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4595" target="_blank">01:16:35.420</a></span> | <span class="t">still do concluding thoughts. The idea here-- well, today, we talk about the exciting applications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4601" target="_blank">01:16:41.380</a></span> | <span class="t">of natural language generation systems. But one might think that, well, given that ChatGPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4608" target="_blank">01:16:48.460</a></span> | <span class="t">is already so good, are there any other things that we can do research-wise? If you try interacting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4613" target="_blank">01:16:53.380</a></span> | <span class="t">with these models, if you try to interact with these models, actually, you can see that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4618" target="_blank">01:16:58.580</a></span> | <span class="t">there are still lots of limitations in their skills and performance. For example, ChatGPT</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4622" target="_blank">01:17:02.940</a></span> | <span class="t">is able to do a lot of things with manipulating text, but it couldn't really create interesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4629" target="_blank">01:17:09.540</a></span> | <span class="t">contents, or it couldn't really think deeply about stuff. So there are lots of headrooms,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4635" target="_blank">01:17:15.860</a></span> | <span class="t">and there are still many improvements ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4638" target="_blank">01:17:18.460</a></span> | <span class="t">And evaluation remains a really huge challenge in natural language generation. Basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4643" target="_blank">01:17:23.900</a></span> | <span class="t">we need better ways to automatically evaluate performance of NLG models, because human evaluations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4649" target="_blank">01:17:29.500</a></span> | <span class="t">are expensive and not reproducible. So it's better to figure out ways to compile all those</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4656" target="_blank">01:17:36.900</a></span> | <span class="t">human judgments into a very reliable and trustworthy model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4661" target="_blank">01:17:41.620</a></span> | <span class="t">And also, with the advance of all these large-scale language models, doing neural natural language</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4668" target="_blank">01:17:48.300</a></span> | <span class="t">generation has been reset. And it's never been easier to jump into this space, because</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4674" target="_blank">01:17:54.340</a></span> | <span class="t">now there are all the tools that are already there for you to build upon.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4678" target="_blank">01:17:58.740</a></span> | <span class="t">And finally, it is one of the most exciting and fun areas of NLP to work on. So yeah,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4683" target="_blank">01:18:03.340</a></span> | <span class="t">I'm happy to chat more about NLG if you have any questions, both after class and in class,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4688" target="_blank">01:18:08.580</a></span> | <span class="t">I guess, in one minute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4690" target="_blank">01:18:10.580</a></span> | <span class="t">OK, cool. That's everything. So do you have any questions? If you don't, we can end the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4697" target="_blank">01:18:17.300</a></span> | <span class="t">class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=N9L32bFieEY&t=4697" target="_blank">01:18:17.980</a></span> | <span class="t">[BLANK_AUDIO]</span></div></div></body></html>