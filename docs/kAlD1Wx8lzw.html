<html><head><title>Breaking down the OG GPT Paper by Alec Radford</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 80%;
        margin: auto;
        overflow: hidden;
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Breaking down the OG GPT Paper by Alec Radford</h2><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw"><img src="https://i.ytimg.com/vi_webp/kAlD1Wx8lzw/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./kAlD1Wx8lzw.html">Whisper Transcript</a> | <a href="./transcript_kAlD1Wx8lzw.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Okay. Sure. So, hey everyone. My name is Amged. I'm a machine learning engineer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=6" target="_blank">00:00:06.000</a></span> | <span class="t">I generally do ML consulting services to startups. I help them like ship AI-powered products,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=13" target="_blank">00:00:13.000</a></span> | <span class="t">especially in the field of NLP and speech-to-text applications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=17" target="_blank">00:00:17.000</a></span> | <span class="t">And I run a blog where I like publish posts about ML stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=23" target="_blank">00:00:23.000</a></span> | <span class="t">So feel free to check it out. I've done some posts about Whisper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=27" target="_blank">00:00:27.000</a></span> | <span class="t">Yeah. So with that out of the way, let's get directly to what we want to discuss today,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=33" target="_blank">00:00:33.000</a></span> | <span class="t">which is like the GPT-1 paper by the folks at OpenAI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=40" target="_blank">00:00:40.000</a></span> | <span class="t">So the paper is titled "Improving Language Understanding by Generative Pre-training".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=45" target="_blank">00:00:45.000</a></span> | <span class="t">It was published in June 2018. And these are like the authors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=50" target="_blank">00:00:50.000</a></span> | <span class="t">Let me switch to presentation mode. Yeah. And these are like the authors of the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=54" target="_blank">00:00:54.000</a></span> | <span class="t">So mainly Alec Radford and Elia Suskiver, very well-known publishers in the ML field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=62" target="_blank">00:01:02.000</a></span> | <span class="t">So let's get started. Back in 2018, deep learning was becoming very popular.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=70" target="_blank">00:01:10.000</a></span> | <span class="t">But the main thing with deep learning is like it is very, very data hungry.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=75" target="_blank">00:01:15.000</a></span> | <span class="t">There are like good news and bad news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=77" target="_blank">00:01:17.000</a></span> | <span class="t">The good news is like there is a ton of data available everywhere on the Internet and just online.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=84" target="_blank">00:01:24.000</a></span> | <span class="t">We have like tons and tons of data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=86" target="_blank">00:01:26.000</a></span> | <span class="t">The bad news is this data is not annotated and it's not curated and it's basically very, very messy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=93" target="_blank">00:01:33.000</a></span> | <span class="t">And if you want to train machine learning models back then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=96" target="_blank">00:01:36.000</a></span> | <span class="t">the only way to do it is just to annotate data yourself or hire like data annotators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=101" target="_blank">00:01:41.000</a></span> | <span class="t">And these tend to be very, very expensive and difficult to scale and hire.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=105" target="_blank">00:01:45.000</a></span> | <span class="t">Like if you think GPUs are expensive, you have not worked with data annotators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=109" target="_blank">00:01:49.000</a></span> | <span class="t">That's what I like to say. So this makes like deep learning very restricted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=114" target="_blank">00:01:54.000</a></span> | <span class="t">You cannot use it everywhere.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=116" target="_blank">00:01:56.000</a></span> | <span class="t">Like you're only restricted to fields that have good, high quality annotated data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=122" target="_blank">00:02:02.000</a></span> | <span class="t">And this is a very big bottleneck.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=125" target="_blank">00:02:05.000</a></span> | <span class="t">So people back then in 2018 are trying to solve this problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=129" target="_blank">00:02:09.000</a></span> | <span class="t">Like how do we get over the fact that we need labeled data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=135" target="_blank">00:02:15.000</a></span> | <span class="t">And one potential solution to this problem is like unsupervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=140" target="_blank">00:02:20.000</a></span> | <span class="t">So the question is, what if we can leverage like just the linguistic information from the unlabeled data?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=146" target="_blank">00:02:26.000</a></span> | <span class="t">So we have like a bunch of text, like novels, books, articles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=152" target="_blank">00:02:32.000</a></span> | <span class="t">How can we leverage like the linguistic information from this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=155" target="_blank">00:02:35.000</a></span> | <span class="t">And the answer to this could be using unsupervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=160" target="_blank">00:02:40.000</a></span> | <span class="t">And if you can do this successfully, this alleviates the need for large amounts of labeled data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=165" target="_blank">00:02:45.000</a></span> | <span class="t">Because basically you can utilize Wikipedia, which is very big, or even all the papers published on archive and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=175" target="_blank">00:02:55.000</a></span> | <span class="t">And even if you have like lots of labeled data, using unsupervised learning as a step one or step zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=181" target="_blank">00:03:01.000</a></span> | <span class="t">is going to make your model actually perform better than just training on this labeled data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=186" target="_blank">00:03:06.000</a></span> | <span class="t">Because of transfer learning, where you can transfer the things that your model has learned during pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=192" target="_blank">00:03:12.000</a></span> | <span class="t">into your actual objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=195" target="_blank">00:03:15.000</a></span> | <span class="t">And a good evidence of this is back in 2017 and 20, or even a bit before 2017,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=200" target="_blank">00:03:20.000</a></span> | <span class="t">is like people have been using pre-trained word embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=204" target="_blank">00:03:24.000</a></span> | <span class="t">Things like Wave2Vec, GloVe, FastText, to achieve very good performance on many tasks like classification and machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=212" target="_blank">00:03:32.000</a></span> | <span class="t">Sorry, machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=214" target="_blank">00:03:34.000</a></span> | <span class="t">So this is a good evidence that actually unsupervised learning works and it's a very good approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=220" target="_blank">00:03:40.000</a></span> | <span class="t">So this is the premise of the paper actually, like unsupervised learning is very promising.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=228" target="_blank">00:03:48.000</a></span> | <span class="t">So let's talk a bit about word embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=231" target="_blank">00:03:51.000</a></span> | <span class="t">So the idea behind word embedding is like you want to project words, so just text, into an n-dimensional space.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=238" target="_blank">00:03:58.000</a></span> | <span class="t">And n is usually 300 or 1,000 back then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=242" target="_blank">00:04:02.000</a></span> | <span class="t">And this space has a very special property that words that are similar in meaning have very similar products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=248" target="_blank">00:04:08.000</a></span> | <span class="t">Sorry, very similar vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=251" target="_blank">00:04:11.000</a></span> | <span class="t">And by similar vectors I mean like you can measure similarity by things like cosine similarity dot product, like L2 distance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=260" target="_blank">00:04:20.000</a></span> | <span class="t">So we can capture similarity between words that don't each other but have similar meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=267" target="_blank">00:04:27.000</a></span> | <span class="t">For example, the word "booking" and "reservation".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=270" target="_blank">00:04:30.000</a></span> | <span class="t">Just from the syntax, they are very different words, but they have very similar meanings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=274" target="_blank">00:04:34.000</a></span> | <span class="t">We're just booking something.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=276" target="_blank">00:04:36.000</a></span> | <span class="t">And similarly, the word "adam" and "sgd".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=279" target="_blank">00:04:39.000</a></span> | <span class="t">These are like completely different words, but they are both actually like optimizers and use in machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=284" target="_blank">00:04:44.000</a></span> | <span class="t">So they are very similar and their vectors are going to be probably similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=288" target="_blank">00:04:48.000</a></span> | <span class="t">The most common implementation of word embedding is like "wave2vec" by Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=293" target="_blank">00:04:53.000</a></span> | <span class="t">This is what popularized the usage of word embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=297" target="_blank">00:04:57.000</a></span> | <span class="t">And then "glove" by Stanford and "fasttext" by Facebook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=301" target="_blank">00:05:01.000</a></span> | <span class="t">And the way these word embedding models are trained is like leveraging co-occurrence between words that have similar contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=309" target="_blank">00:05:09.000</a></span> | <span class="t">For example, similar words tend to occur in similar contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=313" target="_blank">00:05:13.000</a></span> | <span class="t">Like you would generally find the word "adam" associated with learning rate or machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=319" target="_blank">00:05:19.000</a></span> | <span class="t">Similarly, "sgd" is associated with learning rate as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=322" target="_blank">00:05:22.000</a></span> | <span class="t">So you can conclude that these two words are kind of similar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=325" target="_blank">00:05:25.000</a></span> | <span class="t">And the way these word embedding are used is that they are like utilized by training a head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=331" target="_blank">00:05:31.000</a></span> | <span class="t">Like for example, if you want to classify a word as being positive or negative, having a positive sentiment or a negative sentiment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=338" target="_blank">00:05:38.000</a></span> | <span class="t">you can train a classification head on top of the frozen embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=342" target="_blank">00:05:42.000</a></span> | <span class="t">So you use the word embeddings like as a fixed feature input, input features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=347" target="_blank">00:05:47.000</a></span> | <span class="t">Just frozen things without training the word embeddings themselves.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=354" target="_blank">00:05:54.000</a></span> | <span class="t">And this has like a significant drawback or a bunch of significant drawbacks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=359" target="_blank">00:05:59.000</a></span> | <span class="t">The first thing is it doesn't utilize the context of the text, so you're just using the word itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=365" target="_blank">00:06:05.000</a></span> | <span class="t">And some words, even the same word, can have very different meanings depending on the context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=371" target="_blank">00:06:11.000</a></span> | <span class="t">For example, the river bank, like the Amazon bank or the oil bank, is different from the HSBC bank or like the JPMorgan bank,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=379" target="_blank">00:06:19.000</a></span> | <span class="t">even though these are the exact same words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=382" target="_blank">00:06:22.000</a></span> | <span class="t">So if you're going to use just wave2vec, these two words will have the same vector, even though they have very different contexts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=389" target="_blank">00:06:29.000</a></span> | <span class="t">And even beyond this, natural language has nuances that cannot just be captured by using words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=395" target="_blank">00:06:35.000</a></span> | <span class="t">Like the way you write things like "glamour3on" and "skillissue" on OpenAI, the way you write it this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=401" target="_blank">00:06:41.000</a></span> | <span class="t">you have a very specific intonation compared just to writing OpenAI in the normal way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=407" target="_blank">00:06:47.000</a></span> | <span class="t">So are there any questions about word embeddings so far?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=420" target="_blank">00:07:00.000</a></span> | <span class="t">Someone says in the chat, nope. I cannot read the chat, so if just someone can say this using the microphone, that'd be great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=428" target="_blank">00:07:08.000</a></span> | <span class="t">I don't think there are any questions. I think Sean said no.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=431" target="_blank">00:07:11.000</a></span> | <span class="t">But if anyone has any questions, you can maybe drop it in the chat, and then I can just surface them as and when.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=437" target="_blank">00:07:17.000</a></span> | <span class="t">But it seems like everyone's okay with it for now. I'll keep an eye on the chat for you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=441" target="_blank">00:07:21.000</a></span> | <span class="t">Yeah, great. Yeah, thank you. That's appreciated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=444" target="_blank">00:07:24.000</a></span> | <span class="t">So this was word embedding. The main limitation or drawback is you cannot use context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=450" target="_blank">00:07:30.000</a></span> | <span class="t">So can we go beyond word embedding?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=453" target="_blank">00:07:33.000</a></span> | <span class="t">Word embeddings are too local. We need something that's more global that can capture the higher-level semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=459" target="_blank">00:07:39.000</a></span> | <span class="t">But this also has its complications.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=463" target="_blank">00:07:43.000</a></span> | <span class="t">How do you leverage more than word-level information from unlabeled text?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=467" target="_blank">00:07:47.000</a></span> | <span class="t">You're going to have some questions that you need to answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=471" target="_blank">00:07:51.000</a></span> | <span class="t">First is, which objective should you use while training?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=474" target="_blank">00:07:54.000</a></span> | <span class="t">Do you want to use language modeling or machine translation or discourse coherence or something else?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=480" target="_blank">00:08:00.000</a></span> | <span class="t">And in 2024 right now, we definitely know the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=485" target="_blank">00:08:05.000</a></span> | <span class="t">Language modeling works very well because this is what we've been using for the past three years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=489" target="_blank">00:08:09.000</a></span> | <span class="t">But back then in 2017 and 2018, this was not very clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=493" target="_blank">00:08:13.000</a></span> | <span class="t">For example, this paper came out before BERT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=496" target="_blank">00:08:16.000</a></span> | <span class="t">And even the original machine learning, the original transformer paper, attention is all you need to use machine translation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=505" target="_blank">00:08:25.000</a></span> | <span class="t">So back then, this definitely was not very obvious to people.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=509" target="_blank">00:08:29.000</a></span> | <span class="t">The second question is, how should we transfer these embeddings to the target task?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=515" target="_blank">00:08:35.000</a></span> | <span class="t">There are a bunch of ways to do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=517" target="_blank">00:08:37.000</a></span> | <span class="t">Do you want to modify the model architecture?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=523" target="_blank">00:08:43.000</a></span> | <span class="t">Each task is going to require each specific modification to the model itself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=529" target="_blank">00:08:49.000</a></span> | <span class="t">This is one approach, and this requires very deep knowledge of model architecture and being just a wizard to modify the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=539" target="_blank">00:08:59.000</a></span> | <span class="t">The second approach is to use a specific recipe or schema to do transfer learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=545" target="_blank">00:09:05.000</a></span> | <span class="t">A very popular example of this is ULM Fit by Jeremy Howard and Sebastian Ruder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=551" target="_blank">00:09:11.000</a></span> | <span class="t">We're going to cover this in the next two slides, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=554" target="_blank">00:09:14.000</a></span> | <span class="t">The third option is also you can add an auxiliary learning objective during pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=559" target="_blank">00:09:19.000</a></span> | <span class="t">While you're pre-training on language modeling, you can have an auxiliary learning objective like machine translation or discourse coherence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=567" target="_blank">00:09:27.000</a></span> | <span class="t">These are some of the approaches you might want to take when you are deciding on the target task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=574" target="_blank">00:09:34.000</a></span> | <span class="t">All these questions made going beyond word embedding not so straightforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=582" target="_blank">00:09:42.000</a></span> | <span class="t">They made it difficult to utilize semi-supervised learning or unsupervised pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=589" target="_blank">00:09:49.000</a></span> | <span class="t">Let's take a look at ULM Fit and how they did this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=593" target="_blank">00:09:53.000</a></span> | <span class="t">This is a paper titled "Universal Language Model Fine-Tuning for Text Classification."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=600" target="_blank">00:10:00.000</a></span> | <span class="t">Their objective is text classification, but they want to also build a universal language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=606" target="_blank">00:10:06.000</a></span> | <span class="t">This is a very similar work in NLP. It's a very well-known paper and it has a big impact.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=612" target="_blank">00:10:12.000</a></span> | <span class="t">The question they are raising is instead of just utilizing the word embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=621" target="_blank">00:10:21.000</a></span> | <span class="t">like you're going to have a classifier, you're going to have an embedding layer and a classification layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=626" target="_blank">00:10:26.000</a></span> | <span class="t">The old way of doing this, you're going to use the embedding layer from Web2Vec and you are going to randomly initialize the classification head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=634" target="_blank">00:10:34.000</a></span> | <span class="t">They are asking why not just have a good initialization point for all the layers, not just the embedding layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=642" target="_blank">00:10:42.000</a></span> | <span class="t">Their answer to this question is an approach called ULM Fit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=647" target="_blank">00:10:47.000</a></span> | <span class="t">It is a three-step recipe for state-of-the-art text classification back then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=652" target="_blank">00:10:52.000</a></span> | <span class="t">We have three steps. The first step is to train the language model on general domain data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=658" target="_blank">00:10:58.000</a></span> | <span class="t">We call this pre-training on large corpus these days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=662" target="_blank">00:11:02.000</a></span> | <span class="t">You just train your language model on a very big corpus like Wikitext.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=667" target="_blank">00:11:07.000</a></span> | <span class="t">This was big back then, but it's probably small now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=670" target="_blank">00:11:10.000</a></span> | <span class="t">You can do pre-training on 15 trillion tokens of data if you are a big organization like Meta, for example, or even more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=679" target="_blank">00:11:19.000</a></span> | <span class="t">This is the first step. The second step is you do fine-tune the language model on your target data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=686" target="_blank">00:11:26.000</a></span> | <span class="t">You keep doing language modeling in this step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=690" target="_blank">00:11:30.000</a></span> | <span class="t">This is similar to what we call continued pre-training on target domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=695" target="_blank">00:11:35.000</a></span> | <span class="t">You just take your LLAMA270B base and you just do language modeling on, let's say, financial books to try to make your model like Bloomberg GPT, for example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=708" target="_blank">00:11:48.000</a></span> | <span class="t">But you're still doing language modeling. You're not doing any task-specific training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=714" target="_blank">00:11:54.000</a></span> | <span class="t">The third and final step is to train a classifier on your label data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=718" target="_blank">00:11:58.000</a></span> | <span class="t">This is the fine-tuning step that we are all familiar with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=722" target="_blank">00:12:02.000</a></span> | <span class="t">Let's say you want to classify Amazon reviews as being positive, neutral, or negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=727" target="_blank">00:12:07.000</a></span> | <span class="t">You're just going to get maybe 1,000 reviews on the label for each review and just train the model in a supervised fashion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=736" target="_blank">00:12:16.000</a></span> | <span class="t">This was a very good paper and it was very influential and made a big buzz in the ML field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=746" target="_blank">00:12:26.000</a></span> | <span class="t">This paper was released in 2018, but it did not mention the word "transformer" at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=752" target="_blank">00:12:32.000</a></span> | <span class="t">They did not even reference the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=755" target="_blank">00:12:35.000</a></span> | <span class="t">The architecture of the model used was an RNN-based model. I think it was an LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=760" target="_blank">00:12:40.000</a></span> | <span class="t">So, no transformers at all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=763" target="_blank">00:12:43.000</a></span> | <span class="t">And this is kind of a big gap in this work that GPT folks are going to fill.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=770" target="_blank">00:12:50.000</a></span> | <span class="t">Otherwise, this paper would have been a very, very good paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=775" target="_blank">00:12:55.000</a></span> | <span class="t">It still is a good paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=777" target="_blank">00:12:57.000</a></span> | <span class="t">So, let's talk a bit about GPT, the thing that we want to talk about today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=784" target="_blank">00:13:04.000</a></span> | <span class="t">GPT stands for Generative Pre-trained Transformer, the keyword is "transformer".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=790" target="_blank">00:13:10.000</a></span> | <span class="t">It was developed by OpenAI, by these awesome folks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=795" target="_blank">00:13:15.000</a></span> | <span class="t">It was actually one of the first things that made OpenAI a popular organization in the ML field.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=802" target="_blank">00:13:22.000</a></span> | <span class="t">The whole premise is to use a semi-supervised approach for NLU tasks, natural language understanding tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=812" target="_blank">00:13:32.000</a></span> | <span class="t">So, the goal is to learn a universal representation that transfers very well to other downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=821" target="_blank">00:13:41.000</a></span> | <span class="t">So, basically have a good starting point, instead of starting from scratch every single time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=827" target="_blank">00:13:47.000</a></span> | <span class="t">And their approach is basically two-step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=830" target="_blank">00:13:50.000</a></span> | <span class="t">The first step is to do unsupervised pre-training, and then supervised fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=835" target="_blank">00:13:55.000</a></span> | <span class="t">And this is kind of where the word "semi-supervised" comes from.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=839" target="_blank">00:13:59.000</a></span> | <span class="t">So, a mixture of unsupervised training and supervised training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=843" target="_blank">00:14:03.000</a></span> | <span class="t">And their architecture is transformers, of course, because we have GPUs, and GPUs love transformers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=849" target="_blank">00:14:09.000</a></span> | <span class="t">So, that's a good thing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=854" target="_blank">00:14:14.000</a></span> | <span class="t">So, to do this approach, you're going to need two things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=857" target="_blank">00:14:17.000</a></span> | <span class="t">The first thing is a very big corpus of unlabeled text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=860" target="_blank">00:14:20.000</a></span> | <span class="t">And then the second thing is a dataset that is annotated, that is ready to use for supervised fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=868" target="_blank">00:14:28.000</a></span> | <span class="t">So, you could have multiple datasets if you want to train your model for several tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=874" target="_blank">00:14:34.000</a></span> | <span class="t">But the good news is your target tasks do not need to be in the same domain as the unlabeled text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=880" target="_blank">00:14:40.000</a></span> | <span class="t">So, for example, let's say you want to train on financial tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=885" target="_blank">00:14:45.000</a></span> | <span class="t">You're going to give the model some information about a stock and ask it about how it performs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=890" target="_blank">00:14:50.000</a></span> | <span class="t">Or whether it should buy or sell.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=892" target="_blank">00:14:52.000</a></span> | <span class="t">So, like a financial classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=895" target="_blank">00:14:55.000</a></span> | <span class="t">Actually, you can pre-train on just like normal general data.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=899" target="_blank">00:14:59.000</a></span> | <span class="t">Like you can pre-train on a corpus from the Internet and then just fine-tune on your desired tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=905" target="_blank">00:15:05.000</a></span> | <span class="t">Like your unlabeled corpus does not need to be in the same domain as your objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=911" target="_blank">00:15:11.000</a></span> | <span class="t">And this is good news because we have a lot of general-purpose text that you can use for pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=917" target="_blank">00:15:17.000</a></span> | <span class="t">While obtaining very domain-specific corpus is more involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=922" target="_blank">00:15:22.000</a></span> | <span class="t">And a very minor note here is the name can be misleading in this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=927" target="_blank">00:15:27.000</a></span> | <span class="t">The word "generative" here mainly refers to the pre-training step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=931" target="_blank">00:15:31.000</a></span> | <span class="t">The actual tasks that they had in mind are more discriminative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=935" target="_blank">00:15:35.000</a></span> | <span class="t">So, like classification, question answering, and semantic similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=938" target="_blank">00:15:38.000</a></span> | <span class="t">That is, natural language understanding tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=941" target="_blank">00:15:41.000</a></span> | <span class="t">So, they did not discuss machine translation or just being a chatbot in this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=949" target="_blank">00:15:49.000</a></span> | <span class="t">And they actually released their blog post under a different, and I think more fitting title,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=954" target="_blank">00:15:54.000</a></span> | <span class="t">called "Improving Language Understanding with Unsupervised Learning".</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=958" target="_blank">00:15:58.000</a></span> | <span class="t">And this is like the key idea here, like unsupervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=963" target="_blank">00:16:03.000</a></span> | <span class="t">This is a very nice blog post on their blog.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=967" target="_blank">00:16:07.000</a></span> | <span class="t">So, now we have discussed EOLMFET and GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=974" target="_blank">00:16:14.000</a></span> | <span class="t">Let's also discuss some of the other related work in this domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=978" target="_blank">00:16:18.000</a></span> | <span class="t">So, let's first start with semi-supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=981" target="_blank">00:16:21.000</a></span> | <span class="t">The work GPT falls under this domain.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=985" target="_blank">00:16:25.000</a></span> | <span class="t">And back then this was becoming very popular, like sequence labeling, text classification tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=991" target="_blank">00:16:31.000</a></span> | <span class="t">People were doing semi-supervised learning for these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=994" target="_blank">00:16:34.000</a></span> | <span class="t">And you have different levels for this approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=998" target="_blank">00:16:38.000</a></span> | <span class="t">So, the first and basic level is just using language statistics to get the features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1006" target="_blank">00:16:46.000</a></span> | <span class="t">Like use bag of words, tf, idf, and all these classical machine learning stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1013" target="_blank">00:16:53.000</a></span> | <span class="t">You can use them as input features and just train a classifier on top of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1018" target="_blank">00:16:58.000</a></span> | <span class="t">But that's not very helpful, because two words that are different in syntax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1023" target="_blank">00:17:03.000</a></span> | <span class="t">but are similar in semantics, are going to have very different features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1028" target="_blank">00:17:08.000</a></span> | <span class="t">So, it makes the model a bit limited.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1033" target="_blank">00:17:13.000</a></span> | <span class="t">The second step is to use the word embedding, like we discussed previously.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1036" target="_blank">00:17:16.000</a></span> | <span class="t">And this approach allows you to capture the semantics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1040" target="_blank">00:17:20.000</a></span> | <span class="t">but it's also very limited in that it's based on words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1044" target="_blank">00:17:24.000</a></span> | <span class="t">We're not capturing the higher level semantics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1047" target="_blank">00:17:27.000</a></span> | <span class="t">The third level is like sequence embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1050" target="_blank">00:17:30.000</a></span> | <span class="t">So, instead of just using the word to get the embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1055" target="_blank">00:17:35.000</a></span> | <span class="t">you are going to utilize the entire sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1058" target="_blank">00:17:38.000</a></span> | <span class="t">So, like the sentence or the paragraph to get the embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1061" target="_blank">00:17:41.000</a></span> | <span class="t">And this actually allows us to utilize the context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1064" target="_blank">00:17:44.000</a></span> | <span class="t">to understand the high level semantics of the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1067" target="_blank">00:17:47.000</a></span> | <span class="t">And this is like level 3 is where GPT falls.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1070" target="_blank">00:17:50.000</a></span> | <span class="t">We are using the entire sequence to generate embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1074" target="_blank">00:17:54.000</a></span> | <span class="t">that we can use for classification or any other task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1079" target="_blank">00:17:59.000</a></span> | <span class="t">So, this is regarding semi-supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1082" target="_blank">00:18:02.000</a></span> | <span class="t">The second field, and the more specific field, is unsupervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1087" target="_blank">00:18:07.000</a></span> | <span class="t">So, it's a special case of semi-supervised learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1090" target="_blank">00:18:10.000</a></span> | <span class="t">These terminologies can be confusing, I know.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1094" target="_blank">00:18:14.000</a></span> | <span class="t">But the goal is to find a good initial starting point</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1098" target="_blank">00:18:18.000</a></span> | <span class="t">instead of just going directly to do unsupervised learning objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1107" target="_blank">00:18:27.000</a></span> | <span class="t">This is a typo, probably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1110" target="_blank">00:18:30.000</a></span> | <span class="t">So, the early works in unsupervised learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1116" target="_blank">00:18:36.000</a></span> | <span class="t">was actually used in vision, in image classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1119" target="_blank">00:18:39.000</a></span> | <span class="t">So, for example, you can use ResNet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1121" target="_blank">00:18:41.000</a></span> | <span class="t">that was trained to classify ImageNet.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1123" target="_blank">00:18:43.000</a></span> | <span class="t">You can take the backbone, the ResNet as a backbone,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1126" target="_blank">00:18:46.000</a></span> | <span class="t">and then do like segmentation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1129" target="_blank">00:18:49.000</a></span> | <span class="t">or just try to detect pneumonia in chest x-rays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1132" target="_blank">00:18:52.000</a></span> | <span class="t">And this is a very good starting point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1134" target="_blank">00:18:54.000</a></span> | <span class="t">Even though ResNet was...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1136" target="_blank">00:18:56.000</a></span> | <span class="t">Even though ImageNet is just classifying images</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1140" target="_blank">00:19:00.000</a></span> | <span class="t">as being cats or dogs or humans or horses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1144" target="_blank">00:19:04.000</a></span> | <span class="t">So, it's more of a general purpose dataset,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1146" target="_blank">00:19:06.000</a></span> | <span class="t">but you learn a very good representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1149" target="_blank">00:19:09.000</a></span> | <span class="t">that can be used in your downstream task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1153" target="_blank">00:19:13.000</a></span> | <span class="t">And in the field of vision,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1155" target="_blank">00:19:15.000</a></span> | <span class="t">this actually proved to be quite well</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1158" target="_blank">00:19:18.000</a></span> | <span class="t">because people have found out that pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1160" target="_blank">00:19:20.000</a></span> | <span class="t">acts as a regularization scheme.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1162" target="_blank">00:19:22.000</a></span> | <span class="t">Your model tends to have better generalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1165" target="_blank">00:19:25.000</a></span> | <span class="t">if you pre-train it on a very large corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1169" target="_blank">00:19:29.000</a></span> | <span class="t">And the authors mentioned in the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1171" target="_blank">00:19:31.000</a></span> | <span class="t">that the closest line of work to their work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1173" target="_blank">00:19:33.000</a></span> | <span class="t">is actually what we discovered...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1175" target="_blank">00:19:35.000</a></span> | <span class="t">What we covered so far,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1177" target="_blank">00:19:37.000</a></span> | <span class="t">the ULMFET by Howard and Ruder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1179" target="_blank">00:19:39.000</a></span> | <span class="t">and also another work by Dai et al.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1183" target="_blank">00:19:43.000</a></span> | <span class="t">But I did not go into...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1185" target="_blank">00:19:45.000</a></span> | <span class="t">I did not go in detail into this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1189" target="_blank">00:19:49.000</a></span> | <span class="t">However, the main drawback of these two works</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1192" target="_blank">00:19:52.000</a></span> | <span class="t">and everything else, almost everything else back then,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1195" target="_blank">00:19:55.000</a></span> | <span class="t">is like they are using RNNs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1197" target="_blank">00:19:57.000</a></span> | <span class="t">And we know that by 2018,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1199" target="_blank">00:19:59.000</a></span> | <span class="t">like, transformer reigns supreme</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1201" target="_blank">00:20:01.000</a></span> | <span class="t">because we have GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1204" target="_blank">00:20:04.000</a></span> | <span class="t">So we can train transformers more efficiently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1206" target="_blank">00:20:06.000</a></span> | <span class="t">And also RNNs have, like...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1208" target="_blank">00:20:08.000</a></span> | <span class="t">They are limited in their ability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1210" target="_blank">00:20:10.000</a></span> | <span class="t">to process large contexts</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1213" target="_blank">00:20:13.000</a></span> | <span class="t">because of, like, the gradient vanishing problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1216" target="_blank">00:20:16.000</a></span> | <span class="t">and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1219" target="_blank">00:20:19.000</a></span> | <span class="t">So this is, like, the most common approach back then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1221" target="_blank">00:20:21.000</a></span> | <span class="t">Another approach is also to use the head-end representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1224" target="_blank">00:20:24.000</a></span> | <span class="t">from a pre-trained language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1226" target="_blank">00:20:26.000</a></span> | <span class="t">or machine translation model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1227" target="_blank">00:20:27.000</a></span> | <span class="t">as auxiliary features while training your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1230" target="_blank">00:20:30.000</a></span> | <span class="t">So you can have...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1232" target="_blank">00:20:32.000</a></span> | <span class="t">Let's say you have a machine learning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1236" target="_blank">00:20:36.000</a></span> | <span class="t">You're going to get the representations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1238" target="_blank">00:20:38.000</a></span> | <span class="t">or, like, the hidden state of this model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1241" target="_blank">00:20:41.000</a></span> | <span class="t">and then just give it to your classifier</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1244" target="_blank">00:20:44.000</a></span> | <span class="t">as, like, additional features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1247" target="_blank">00:20:47.000</a></span> | <span class="t">And as you can imagine, this is, like...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1249" target="_blank">00:20:49.000</a></span> | <span class="t">This involves, like, a substantial amount of work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1251" target="_blank">00:20:51.000</a></span> | <span class="t">and new parameters for each task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1254" target="_blank">00:20:54.000</a></span> | <span class="t">So this is not very universal.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1257" target="_blank">00:20:57.000</a></span> | <span class="t">So any questions so far</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1259" target="_blank">00:20:59.000</a></span> | <span class="t">before we actually go in detail into the approach?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1264" target="_blank">00:21:04.000</a></span> | <span class="t">- I think there was a question from Sook.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1267" target="_blank">00:21:07.000</a></span> | <span class="t">Did AlexNet use GPUs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1270" target="_blank">00:21:10.000</a></span> | <span class="t">And were the transformers... - AlexNet?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1271" target="_blank">00:21:11.000</a></span> | <span class="t">- ...the first ones to use the GPUs?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1273" target="_blank">00:21:13.000</a></span> | <span class="t">I think he just put it in the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1275" target="_blank">00:21:15.000</a></span> | <span class="t">Yeah, but...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1278" target="_blank">00:21:18.000</a></span> | <span class="t">Were transformers, like, some of the...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1279" target="_blank">00:21:19.000</a></span> | <span class="t">- Yeah, AlexNet, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1282" target="_blank">00:21:22.000</a></span> | <span class="t">Yeah, AlexNet did definitely use GPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1284" target="_blank">00:21:24.000</a></span> | <span class="t">but back then, I think they were writing Qt code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1287" target="_blank">00:21:27.000</a></span> | <span class="t">I think Alex Kryzewski was the person</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1290" target="_blank">00:21:30.000</a></span> | <span class="t">doing the GPU programming stuff himself.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1296" target="_blank">00:21:36.000</a></span> | <span class="t">So, yeah, I think AlexNet did definitely use GPUs,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1299" target="_blank">00:21:39.000</a></span> | <span class="t">if I remember correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1301" target="_blank">00:21:41.000</a></span> | <span class="t">- Yes, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1303" target="_blank">00:21:43.000</a></span> | <span class="t">Cool, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1305" target="_blank">00:21:45.000</a></span> | <span class="t">Well, seems like they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1307" target="_blank">00:21:47.000</a></span> | <span class="t">Okay, I would just like to see</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1309" target="_blank">00:21:49.000</a></span> | <span class="t">if anyone else has any other questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1310" target="_blank">00:21:50.000</a></span> | <span class="t">I think it should be good for now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1313" target="_blank">00:21:53.000</a></span> | <span class="t">- Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1315" target="_blank">00:21:55.000</a></span> | <span class="t">Someone said, "According to papers with Qt, they did."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1318" target="_blank">00:21:58.000</a></span> | <span class="t">Yeah, I think they did use GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1321" target="_blank">00:22:01.000</a></span> | <span class="t">So let's go into details about the GPT approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1326" target="_blank">00:22:06.000</a></span> | <span class="t">So we have two steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1328" target="_blank">00:22:08.000</a></span> | <span class="t">The first step is unsupervised pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1330" target="_blank">00:22:10.000</a></span> | <span class="t">So basically, the goal is to train</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1332" target="_blank">00:22:12.000</a></span> | <span class="t">a high-capacity language model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1334" target="_blank">00:22:14.000</a></span> | <span class="t">on a large corpus of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1336" target="_blank">00:22:16.000</a></span> | <span class="t">The training objective here is language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1338" target="_blank">00:22:18.000</a></span> | <span class="t">That is, given a sequence of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1341" target="_blank">00:22:21.000</a></span> | <span class="t">try to correctly predict the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1344" target="_blank">00:22:24.000</a></span> | <span class="t">So let's say the cat sat on D.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1347" target="_blank">00:22:27.000</a></span> | <span class="t">You're trying to predict the next token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1349" target="_blank">00:22:29.000</a></span> | <span class="t">which I think should be the mat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1351" target="_blank">00:22:31.000</a></span> | <span class="t">So this is basically language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1354" target="_blank">00:22:34.000</a></span> | <span class="t">The loss function D used is negative log-likelihood.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1358" target="_blank">00:22:38.000</a></span> | <span class="t">It's also called cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1360" target="_blank">00:22:40.000</a></span> | <span class="t">Basically, this equation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1363" target="_blank">00:22:43.000</a></span> | <span class="t">the negative log-likelihood of the correct label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1367" target="_blank">00:22:47.000</a></span> | <span class="t">And you do this over--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1369" target="_blank">00:22:49.000</a></span> | <span class="t">a very, very important note here is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1371" target="_blank">00:22:51.000</a></span> | <span class="t">you do this over the entire sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1373" target="_blank">00:22:53.000</a></span> | <span class="t">So if you have a sentence that said,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1375" target="_blank">00:22:55.000</a></span> | <span class="t">"The cat sat on the mat,"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1377" target="_blank">00:22:57.000</a></span> | <span class="t">you do this over every single token in this sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1380" target="_blank">00:23:00.000</a></span> | <span class="t">And this is very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1382" target="_blank">00:23:02.000</a></span> | <span class="t">So you are training on your entire input--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1385" target="_blank">00:23:05.000</a></span> | <span class="t">your entire corpus,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1388" target="_blank">00:23:08.000</a></span> | <span class="t">not just the last token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1391" target="_blank">00:23:11.000</a></span> | <span class="t">So the architecture is like a transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1394" target="_blank">00:23:14.000</a></span> | <span class="t">a multi-layer transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1396" target="_blank">00:23:16.000</a></span> | <span class="t">but they are using the decoder only.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1398" target="_blank">00:23:18.000</a></span> | <span class="t">They are not using the encoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1400" target="_blank">00:23:20.000</a></span> | <span class="t">So--and the difference between the encoder and the decoder,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1404" target="_blank">00:23:24.000</a></span> | <span class="t">I think, can be summarized to, like, how you do attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1407" target="_blank">00:23:27.000</a></span> | <span class="t">So in encoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1409" target="_blank">00:23:29.000</a></span> | <span class="t">every token has access to every other token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1412" target="_blank">00:23:32.000</a></span> | <span class="t">But in decoders,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1414" target="_blank">00:23:34.000</a></span> | <span class="t">every token has access only to the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1416" target="_blank">00:23:36.000</a></span> | <span class="t">that came before it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1418" target="_blank">00:23:38.000</a></span> | <span class="t">So you only have one directional attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1420" target="_blank">00:23:40.000</a></span> | <span class="t">I think it's also called left-to-right attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1424" target="_blank">00:23:44.000</a></span> | <span class="t">or right-to-left, or, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1426" target="_blank">00:23:46.000</a></span> | <span class="t">you only have attention in one direction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1429" target="_blank">00:23:49.000</a></span> | <span class="t">And basically, the transformer architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1432" target="_blank">00:23:52.000</a></span> | <span class="t">applies, like, multi-headed mask itself attention operation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1435" target="_blank">00:23:55.000</a></span> | <span class="t">over the input tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1437" target="_blank">00:23:57.000</a></span> | <span class="t">and then this is followed by a position-wise feed-forward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1440" target="_blank">00:24:00.000</a></span> | <span class="t">And you keep doing this for, like, n,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1442" target="_blank">00:24:02.000</a></span> | <span class="t">where n is the number of transformer layers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1445" target="_blank">00:24:05.000</a></span> | <span class="t">and then you just generate an auto-distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1448" target="_blank">00:24:08.000</a></span> | <span class="t">over the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1450" target="_blank">00:24:10.000</a></span> | <span class="t">So let's take this into more detail.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1452" target="_blank">00:24:12.000</a></span> | <span class="t">You have your input text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1454" target="_blank">00:24:14.000</a></span> | <span class="t">that has been tokenized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1456" target="_blank">00:24:16.000</a></span> | <span class="t">into tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1458" target="_blank">00:24:18.000</a></span> | <span class="t">and these tokens have been encoded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1461" target="_blank">00:24:21.000</a></span> | <span class="t">So you have, like, the token integer or token ID.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1464" target="_blank">00:24:24.000</a></span> | <span class="t">So it's a number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1466" target="_blank">00:24:26.000</a></span> | <span class="t">So you take this number, and you pass it through the embedding layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1469" target="_blank">00:24:29.000</a></span> | <span class="t">the token embedding layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1471" target="_blank">00:24:31.000</a></span> | <span class="t">also called the semantic embedding layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1473" target="_blank">00:24:33.000</a></span> | <span class="t">and you get a vector for this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1476" target="_blank">00:24:36.000</a></span> | <span class="t">And you also get the positional embedding for this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1479" target="_blank">00:24:39.000</a></span> | <span class="t">You sum these up, like, just vector addition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1483" target="_blank">00:24:43.000</a></span> | <span class="t">and you get your input to the first transformer block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1487" target="_blank">00:24:47.000</a></span> | <span class="t">which is H0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1489" target="_blank">00:24:49.000</a></span> | <span class="t">So just the positional embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1493" target="_blank">00:24:53.000</a></span> | <span class="t">and the token embedding added together,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1496" target="_blank">00:24:56.000</a></span> | <span class="t">and you get your input, H0.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1498" target="_blank">00:24:58.000</a></span> | <span class="t">And then you take H0 and pass it to each block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1502" target="_blank">00:25:02.000</a></span> | <span class="t">each transformer block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1504" target="_blank">00:25:04.000</a></span> | <span class="t">and the output of the first block is going to be the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1506" target="_blank">00:25:06.000</a></span> | <span class="t">to the second block, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1508" target="_blank">00:25:08.000</a></span> | <span class="t">And this is what this equation says.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1511" target="_blank">00:25:11.000</a></span> | <span class="t">And at the end,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1513" target="_blank">00:25:13.000</a></span> | <span class="t">once you've done going through all these blocks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1515" target="_blank">00:25:15.000</a></span> | <span class="t">you're going to go to the output layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1518" target="_blank">00:25:18.000</a></span> | <span class="t">which is actually a reverse embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1521" target="_blank">00:25:21.000</a></span> | <span class="t">So you have a vector, but you want to go back to a token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1524" target="_blank">00:25:24.000</a></span> | <span class="t">or, like, I should say,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1526" target="_blank">00:25:26.000</a></span> | <span class="t">a probability distribution over all the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1530" target="_blank">00:25:30.000</a></span> | <span class="t">And once you have this distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1532" target="_blank">00:25:32.000</a></span> | <span class="t">you pass it through a softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1534" target="_blank">00:25:34.000</a></span> | <span class="t">to get actual probabilities that sum up to one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1537" target="_blank">00:25:37.000</a></span> | <span class="t">instead of just logits or scores.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1540" target="_blank">00:25:40.000</a></span> | <span class="t">And I think we've covered the transformer paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1543" target="_blank">00:25:43.000</a></span> | <span class="t">so I think people are familiar with this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1546" target="_blank">00:25:46.000</a></span> | <span class="t">but if you have other questions, please go ahead.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1549" target="_blank">00:25:49.000</a></span> | <span class="t">A small remark about this is that we use</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1551" target="_blank">00:25:51.000</a></span> | <span class="t">tied input and output token embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1553" target="_blank">00:25:53.000</a></span> | <span class="t">so the embedding layer in this step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1555" target="_blank">00:25:55.000</a></span> | <span class="t">is the same as the one used at the final step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1558" target="_blank">00:25:58.000</a></span> | <span class="t">So...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1560" target="_blank">00:26:00.000</a></span> | <span class="t">Yeah, someone says causal masking.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1568" target="_blank">00:26:08.000</a></span> | <span class="t">Yes, exactly, causal language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1571" target="_blank">00:26:11.000</a></span> | <span class="t">So the second step is supervised fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1574" target="_blank">00:26:14.000</a></span> | <span class="t">So the goal of this step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1576" target="_blank">00:26:16.000</a></span> | <span class="t">is to adapt the parameters of the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1579" target="_blank">00:26:19.000</a></span> | <span class="t">to your supervised target task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1582" target="_blank">00:26:22.000</a></span> | <span class="t">And for this, we need a label data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1584" target="_blank">00:26:24.000</a></span> | <span class="t">where each instance is a pair of, like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1586" target="_blank">00:26:26.000</a></span> | <span class="t">you have a sequence of input tokens and the label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1588" target="_blank">00:26:28.000</a></span> | <span class="t">So, for example, an input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1591" target="_blank">00:26:31.000</a></span> | <span class="t">could be this product is very bad,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1593" target="_blank">00:26:33.000</a></span> | <span class="t">and your label could be, like, a negative sentiment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1597" target="_blank">00:26:37.000</a></span> | <span class="t">So the inputs are passed through the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1600" target="_blank">00:26:40.000</a></span> | <span class="t">to obtain the final transformer blocks activation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1603" target="_blank">00:26:43.000</a></span> | <span class="t">So if you go back a bit,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1606" target="_blank">00:26:46.000</a></span> | <span class="t">you take your input sequence and pass it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1608" target="_blank">00:26:48.000</a></span> | <span class="t">through this same transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1611" target="_blank">00:26:51.000</a></span> | <span class="t">and get the output of the final token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1613" target="_blank">00:26:53.000</a></span> | <span class="t">and then you compare this to your label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1617" target="_blank">00:26:57.000</a></span> | <span class="t">So you're gonna get the hidden representation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1623" target="_blank">00:27:03.000</a></span> | <span class="t">of the last encoder layer at the M token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1626" target="_blank">00:27:06.000</a></span> | <span class="t">where M is, like, the final token, the input sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1628" target="_blank">00:27:08.000</a></span> | <span class="t">And you just pass this through your classification head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1631" target="_blank">00:27:11.000</a></span> | <span class="t">or whatever head you're using.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1633" target="_blank">00:27:13.000</a></span> | <span class="t">So, for example, in classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1635" target="_blank">00:27:15.000</a></span> | <span class="t">we're gonna use Softmax on top of a linear layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1638" target="_blank">00:27:18.000</a></span> | <span class="t">to get your output and compare it with the label.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1643" target="_blank">00:27:23.000</a></span> | <span class="t">And you are using roughly the same loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1646" target="_blank">00:27:26.000</a></span> | <span class="t">which is negative log-light-load estimation</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1648" target="_blank">00:27:28.000</a></span> | <span class="t">or cross-entropy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1652" target="_blank">00:27:32.000</a></span> | <span class="t">And a key distinction here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1655" target="_blank">00:27:35.000</a></span> | <span class="t">between this and the previous step</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1657" target="_blank">00:27:37.000</a></span> | <span class="t">is you are only calculating the loss</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1659" target="_blank">00:27:39.000</a></span> | <span class="t">over only the output token, not the entire sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1662" target="_blank">00:27:42.000</a></span> | <span class="t">So the loss is only on Y, not on X1 or XM and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1668" target="_blank">00:27:48.000</a></span> | <span class="t">So the only extra parameters you need for this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1671" target="_blank">00:27:51.000</a></span> | <span class="t">is your classification head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1674" target="_blank">00:27:54.000</a></span> | <span class="t">So, for example, W-Y,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1676" target="_blank">00:27:56.000</a></span> | <span class="t">if you're trying to do classification,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1678" target="_blank">00:27:58.000</a></span> | <span class="t">the parameter metrics of the--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1682" target="_blank">00:28:02.000</a></span> | <span class="t">the metrics of the parameters of the output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1686" target="_blank">00:28:06.000</a></span> | <span class="t">And also embeddings if you're adding new tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1689" target="_blank">00:28:09.000</a></span> | <span class="t">And we're gonna see this in a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1692" target="_blank">00:28:12.000</a></span> | <span class="t">Something they also experimented with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1694" target="_blank">00:28:14.000</a></span> | <span class="t">is auxiliary training objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1696" target="_blank">00:28:16.000</a></span> | <span class="t">So they also used language modeling</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1698" target="_blank">00:28:18.000</a></span> | <span class="t">as an auxiliary objective in fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1700" target="_blank">00:28:20.000</a></span> | <span class="t">So not just this--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1703" target="_blank">00:28:23.000</a></span> | <span class="t">not just this classification, but also language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1706" target="_blank">00:28:26.000</a></span> | <span class="t">And they say this helped them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1709" target="_blank">00:28:29.000</a></span> | <span class="t">by improving the generalization of the supervised model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1712" target="_blank">00:28:32.000</a></span> | <span class="t">and accelerating convergence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1714" target="_blank">00:28:34.000</a></span> | <span class="t">They also say this is in line with prior work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1717" target="_blank">00:28:37.000</a></span> | <span class="t">They also observed better performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1719" target="_blank">00:28:39.000</a></span> | <span class="t">when using it as an auxiliary objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1722" target="_blank">00:28:42.000</a></span> | <span class="t">And the way you do this is your loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1725" target="_blank">00:28:45.000</a></span> | <span class="t">is now a sum of multiple loss functions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1728" target="_blank">00:28:48.000</a></span> | <span class="t">where one loss function is this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1731" target="_blank">00:28:51.000</a></span> | <span class="t">the classification loss function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1733" target="_blank">00:28:53.000</a></span> | <span class="t">and also you have the language modeling loss function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1737" target="_blank">00:28:57.000</a></span> | <span class="t">with a certain weight, like lambda here is like a weight.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1740" target="_blank">00:29:00.000</a></span> | <span class="t">And lambda could be, for example, 0.5 or 0.3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1744" target="_blank">00:29:04.000</a></span> | <span class="t">So you have like a summation of multiple losses.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1749" target="_blank">00:29:09.000</a></span> | <span class="t">And a small note for myself here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1752" target="_blank">00:29:12.000</a></span> | <span class="t">I'm not sure if auxiliary language--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1754" target="_blank">00:29:14.000</a></span> | <span class="t">like auxiliary objectives are popular today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1757" target="_blank">00:29:17.000</a></span> | <span class="t">I think people just do supervised fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1760" target="_blank">00:29:20.000</a></span> | <span class="t">without an auxiliary objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1763" target="_blank">00:29:23.000</a></span> | <span class="t">That's just my take.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1766" target="_blank">00:29:26.000</a></span> | <span class="t">So any questions so far?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1770" target="_blank">00:29:30.000</a></span> | <span class="t">I think the chat seems to not have any questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1777" target="_blank">00:29:37.000</a></span> | <span class="t">So maybe you want to just-- you can just continue from now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1781" target="_blank">00:29:41.000</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1782" target="_blank">00:29:42.000</a></span> | <span class="t">So now we have discovered--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1787" target="_blank">00:29:47.000</a></span> | <span class="t">we have covered the approach, the basic two steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1791" target="_blank">00:29:51.000</a></span> | <span class="t">You will now get a very good, let's say, classifier,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1794" target="_blank">00:29:54.000</a></span> | <span class="t">because you have done unsupervised pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1797" target="_blank">00:29:57.000</a></span> | <span class="t">and supervised fine-tuning on classifiers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1799" target="_blank">00:29:59.000</a></span> | <span class="t">But GPT-1 is actually trying to be more--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1801" target="_blank">00:30:01.000</a></span> | <span class="t">it's trying to be more of a universal model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1803" target="_blank">00:30:03.000</a></span> | <span class="t">than just a classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1805" target="_blank">00:30:05.000</a></span> | <span class="t">So they are trying to handle multiple tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1807" target="_blank">00:30:07.000</a></span> | <span class="t">like classification, entailment, semantic similarity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1810" target="_blank">00:30:10.000</a></span> | <span class="t">and answering multiple choice questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1813" target="_blank">00:30:13.000</a></span> | <span class="t">And these are all, as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1815" target="_blank">00:30:15.000</a></span> | <span class="t">discriminative tasks other than generative tasks,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1818" target="_blank">00:30:18.000</a></span> | <span class="t">as we discussed before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1821" target="_blank">00:30:21.000</a></span> | <span class="t">So for tasks like classification, this is very easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1823" target="_blank">00:30:23.000</a></span> | <span class="t">You can do what we have covered so far.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1825" target="_blank">00:30:25.000</a></span> | <span class="t">Just add the head on top and do the classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1828" target="_blank">00:30:28.000</a></span> | <span class="t">But other tasks have different structured inputs and outputs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1833" target="_blank">00:30:33.000</a></span> | <span class="t">So for example, text entailment has ordered sentence pairs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1836" target="_blank">00:30:36.000</a></span> | <span class="t">MCQs have a question with multiple answers, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1841" target="_blank">00:30:41.000</a></span> | <span class="t">So each task has its own specific structure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1845" target="_blank">00:30:45.000</a></span> | <span class="t">And the way people have dealt with this previously</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1847" target="_blank">00:30:47.000</a></span> | <span class="t">is just learn a specific architecture for each task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1853" target="_blank">00:30:53.000</a></span> | <span class="t">on top of your model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1855" target="_blank">00:30:55.000</a></span> | <span class="t">And this defeats the whole purpose of the GPT work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1860" target="_blank">00:31:00.000</a></span> | <span class="t">We're trying to do something that's</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1862" target="_blank">00:31:02.000</a></span> | <span class="t">global, general, and a general-purpose model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1865" target="_blank">00:31:05.000</a></span> | <span class="t">rather than having multiple task-specific architectures.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1871" target="_blank">00:31:11.000</a></span> | <span class="t">So instead of using this approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1874" target="_blank">00:31:14.000</a></span> | <span class="t">they opted to use a different approach,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1877" target="_blank">00:31:17.000</a></span> | <span class="t">where they convert the structured inputs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1881" target="_blank">00:31:21.000</a></span> | <span class="t">into just tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1883" target="_blank">00:31:23.000</a></span> | <span class="t">So they are trying to create a multitask format.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1887" target="_blank">00:31:27.000</a></span> | <span class="t">And this is similar to what people have used in future work</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1890" target="_blank">00:31:30.000</a></span> | <span class="t">like T5 and Whisper, where basically you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1893" target="_blank">00:31:33.000</a></span> | <span class="t">trying to model different tasks just using</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1897" target="_blank">00:31:37.000</a></span> | <span class="t">tokens and special tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1900" target="_blank">00:31:40.000</a></span> | <span class="t">These input transformations allows</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1902" target="_blank">00:31:42.000</a></span> | <span class="t">us to use the same architecture for different tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1905" target="_blank">00:31:45.000</a></span> | <span class="t">So you don't need to do a lot of modification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1909" target="_blank">00:31:49.000</a></span> | <span class="t">And we're going to go into this in two details</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1911" target="_blank">00:31:51.000</a></span> | <span class="t">in the next slide.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1913" target="_blank">00:31:53.000</a></span> | <span class="t">So for example, let's take textual entailment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1916" target="_blank">00:31:56.000</a></span> | <span class="t">This task involves reading a pair of sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1919" target="_blank">00:31:59.000</a></span> | <span class="t">and judging the relationship between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1921" target="_blank">00:32:01.000</a></span> | <span class="t">So the relationship could be one of entailment, contradiction,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1923" target="_blank">00:32:03.000</a></span> | <span class="t">or neutral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1924" target="_blank">00:32:04.000</a></span> | <span class="t">And a small note is that this task is still challenging</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1930" target="_blank">00:32:10.000</a></span> | <span class="t">because your model needs to have good understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1933" target="_blank">00:32:13.000</a></span> | <span class="t">and reasoning of the language, because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1936" target="_blank">00:32:16.000</a></span> | <span class="t">can be confusing sometimes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1939" target="_blank">00:32:19.000</a></span> | <span class="t">So you have your premise, and you have your hypothesis,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1942" target="_blank">00:32:22.000</a></span> | <span class="t">and you're trying to classify or try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1944" target="_blank">00:32:24.000</a></span> | <span class="t">to predict the relationship as being entailment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1946" target="_blank">00:32:26.000</a></span> | <span class="t">contradiction, or neutral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1949" target="_blank">00:32:29.000</a></span> | <span class="t">So the way to do this is just to concatenate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1952" target="_blank">00:32:32.000</a></span> | <span class="t">the premise and the hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1954" target="_blank">00:32:34.000</a></span> | <span class="t">So this could be a sentence, and this could be a sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1957" target="_blank">00:32:37.000</a></span> | <span class="t">You just concatenate them and add a special delimiter token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1960" target="_blank">00:32:40.000</a></span> | <span class="t">in between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1961" target="_blank">00:32:41.000</a></span> | <span class="t">And obviously, you add your start token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1963" target="_blank">00:32:43.000</a></span> | <span class="t">at the beginning and your end token at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1966" target="_blank">00:32:46.000</a></span> | <span class="t">And just try to train a classifier on top of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1969" target="_blank">00:32:49.000</a></span> | <span class="t">And your classifier should classify</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1972" target="_blank">00:32:52.000</a></span> | <span class="t">this sequence of input tokens as one of entailment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1975" target="_blank">00:32:55.000</a></span> | <span class="t">contradiction, or neutral.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1977" target="_blank">00:32:57.000</a></span> | <span class="t">So this has become just a classification task</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1981" target="_blank">00:33:01.000</a></span> | <span class="t">by just doing transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1986" target="_blank">00:33:06.000</a></span> | <span class="t">The second task that we cover is semantic similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1989" target="_blank">00:33:09.000</a></span> | <span class="t">And I think this is very popular nowadays,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1992" target="_blank">00:33:12.000</a></span> | <span class="t">because of retrieval, augmented generation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1995" target="_blank">00:33:15.000</a></span> | <span class="t">and embeddings in general.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=1998" target="_blank">00:33:18.000</a></span> | <span class="t">So all this cool rag stuff.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2001" target="_blank">00:33:21.000</a></span> | <span class="t">So this task is about predicting how semantically</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2005" target="_blank">00:33:25.000</a></span> | <span class="t">similar two sentences are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2007" target="_blank">00:33:27.000</a></span> | <span class="t">And semantic similarity just means how close in meaning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2010" target="_blank">00:33:30.000</a></span> | <span class="t">they are.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2011" target="_blank">00:33:31.000</a></span> | <span class="t">Do they talk about the same thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2013" target="_blank">00:33:33.000</a></span> | <span class="t">Do they mean the same thing?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2015" target="_blank">00:33:35.000</a></span> | <span class="t">Do they have similar meaning or not?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2018" target="_blank">00:33:38.000</a></span> | <span class="t">And again, this can be challenging,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2020" target="_blank">00:33:40.000</a></span> | <span class="t">because you can have two paragraphs that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2023" target="_blank">00:33:43.000</a></span> | <span class="t">have very different usage of words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2025" target="_blank">00:33:45.000</a></span> | <span class="t">but they convey the same meaning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2027" target="_blank">00:33:47.000</a></span> | <span class="t">So this can be challenging if your model is not smart enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2030" target="_blank">00:33:50.000</a></span> | <span class="t">And one note about this task is there</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2032" target="_blank">00:33:52.000</a></span> | <span class="t">is no inherent ordering of the sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2034" target="_blank">00:33:54.000</a></span> | <span class="t">So you can just--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2035" target="_blank">00:33:55.000</a></span> | <span class="t">there is no sentence A and sentence B.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2037" target="_blank">00:33:57.000</a></span> | <span class="t">You just have two sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2039" target="_blank">00:33:59.000</a></span> | <span class="t">Unlike, for example, in the entailment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2041" target="_blank">00:34:01.000</a></span> | <span class="t">you have a very specific order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2042" target="_blank">00:34:02.000</a></span> | <span class="t">Like, you have a distinction between the premise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2045" target="_blank">00:34:05.000</a></span> | <span class="t">and the hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2046" target="_blank">00:34:06.000</a></span> | <span class="t">But here you have just two random sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2049" target="_blank">00:34:09.000</a></span> | <span class="t">And the way they approach this is using a Siamese architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2054" target="_blank">00:34:14.000</a></span> | <span class="t">where we have--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2055" target="_blank">00:34:15.000</a></span> | <span class="t">where we generate two input sequences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2058" target="_blank">00:34:18.000</a></span> | <span class="t">and pass them through the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2060" target="_blank">00:34:20.000</a></span> | <span class="t">and compare between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2062" target="_blank">00:34:22.000</a></span> | <span class="t">So basically, Siamese architecture</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2064" target="_blank">00:34:24.000</a></span> | <span class="t">is a fancy way of saying that you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2066" target="_blank">00:34:26.000</a></span> | <span class="t">are using the same model twice, or two identical versions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2070" target="_blank">00:34:30.000</a></span> | <span class="t">of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2071" target="_blank">00:34:31.000</a></span> | <span class="t">So you get your sentence-- first sentence and second sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2074" target="_blank">00:34:34.000</a></span> | <span class="t">and then concatenate them and add your special tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2077" target="_blank">00:34:37.000</a></span> | <span class="t">And you pass them through the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2079" target="_blank">00:34:39.000</a></span> | <span class="t">and you get some vector at the end.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2081" target="_blank">00:34:41.000</a></span> | <span class="t">And also, you do the same thing, but you reverse</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2084" target="_blank">00:34:44.000</a></span> | <span class="t">the order of the sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2087" target="_blank">00:34:47.000</a></span> | <span class="t">So you get the second input sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2089" target="_blank">00:34:49.000</a></span> | <span class="t">pass it to the transformer, and you get your vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2092" target="_blank">00:34:52.000</a></span> | <span class="t">So basically, at the end, you're going to have two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2095" target="_blank">00:34:55.000</a></span> | <span class="t">You just add them--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2097" target="_blank">00:34:57.000</a></span> | <span class="t">do vector addition on top of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2099" target="_blank">00:34:59.000</a></span> | <span class="t">And then you just add a head on top of the output vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2105" target="_blank">00:35:05.000</a></span> | <span class="t">So you just multiply the vector by some layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2109" target="_blank">00:35:09.000</a></span> | <span class="t">a linear layer, for example, that has parameters w,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2112" target="_blank">00:35:12.000</a></span> | <span class="t">and you get your output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2114" target="_blank">00:35:14.000</a></span> | <span class="t">And for example, if you're doing--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2116" target="_blank">00:35:16.000</a></span> | <span class="t">like, if you just have--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2118" target="_blank">00:35:18.000</a></span> | <span class="t">if you're just interested in knowing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2120" target="_blank">00:35:20.000</a></span> | <span class="t">whether these sentences are similar or not similar,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2123" target="_blank">00:35:23.000</a></span> | <span class="t">so you have only two labels, you can train a classifier.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2126" target="_blank">00:35:26.000</a></span> | <span class="t">If you're interested in having more of a scale of similarity,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2130" target="_blank">00:35:30.000</a></span> | <span class="t">like from 0 to 10, you can train a regressor on top of this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2133" target="_blank">00:35:33.000</a></span> | <span class="t">So that's very cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2137" target="_blank">00:35:37.000</a></span> | <span class="t">We're still using the same architecture, by the way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2139" target="_blank">00:35:39.000</a></span> | <span class="t">just the transformer here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2141" target="_blank">00:35:41.000</a></span> | <span class="t">We're not modifying it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2142" target="_blank">00:35:42.000</a></span> | <span class="t">We're just being smart about how to approach this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2148" target="_blank">00:35:48.000</a></span> | <span class="t">You can extend this to do question answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2151" target="_blank">00:35:51.000</a></span> | <span class="t">So let's say you have a question and you have four choices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2155" target="_blank">00:35:55.000</a></span> | <span class="t">Or let's say you have a document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2156" target="_blank">00:35:56.000</a></span> | <span class="t">You have a question about the document,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2158" target="_blank">00:35:58.000</a></span> | <span class="t">and you have four potential answers to this document.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2161" target="_blank">00:36:01.000</a></span> | <span class="t">And you want your model to pick one answer of these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2164" target="_blank">00:36:04.000</a></span> | <span class="t">So the way to do this is you take your document or context</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2168" target="_blank">00:36:08.000</a></span> | <span class="t">and add the question and then add the first answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2172" target="_blank">00:36:12.000</a></span> | <span class="t">And then you get your first input sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2175" target="_blank">00:36:15.000</a></span> | <span class="t">And you pass this to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2177" target="_blank">00:36:17.000</a></span> | <span class="t">And similarly, you get your context or document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2180" target="_blank">00:36:20.000</a></span> | <span class="t">and add the question and add the second answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2182" target="_blank">00:36:22.000</a></span> | <span class="t">and make this into a sequence and give it to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2185" target="_blank">00:36:25.000</a></span> | <span class="t">And you do this for all the answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2187" target="_blank">00:36:27.000</a></span> | <span class="t">And then you just compare between the scores</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2191" target="_blank">00:36:31.000</a></span> | <span class="t">the model gives to each of these potential answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2196" target="_blank">00:36:36.000</a></span> | <span class="t">So for example, we have a Wikipedia article,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2199" target="_blank">00:36:39.000</a></span> | <span class="t">and we have a question, and we have answer A.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2202" target="_blank">00:36:42.000</a></span> | <span class="t">We concatenate all of these with some special tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2206" target="_blank">00:36:46.000</a></span> | <span class="t">And we pass them to the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2209" target="_blank">00:36:49.000</a></span> | <span class="t">and to the final linear layer, and we get the score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2213" target="_blank">00:36:53.000</a></span> | <span class="t">So score for answer A and score for answer B</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2215" target="_blank">00:36:55.000</a></span> | <span class="t">and score for answer C. And just do a softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2217" target="_blank">00:36:57.000</a></span> | <span class="t">to get a proper probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2221" target="_blank">00:37:01.000</a></span> | <span class="t">And that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2222" target="_blank">00:37:02.000</a></span> | <span class="t">You've got your model to do question answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2225" target="_blank">00:37:05.000</a></span> | <span class="t">And you can do this also for other tasks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2227" target="_blank">00:37:07.000</a></span> | <span class="t">like common sense reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2230" target="_blank">00:37:10.000</a></span> | <span class="t">So any questions about these transformations for each task?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2238" target="_blank">00:37:18.000</a></span> | <span class="t">I think there was a question about modifying the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2242" target="_blank">00:37:22.000</a></span> | <span class="t">sequence to both possible orderings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2244" target="_blank">00:37:24.000</a></span> | <span class="t">at least when it comes to semantics in learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2247" target="_blank">00:37:27.000</a></span> | <span class="t">I think someone had a question about that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2251" target="_blank">00:37:31.000</a></span> | <span class="t">So he just said that he doesn't really</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2254" target="_blank">00:37:34.000</a></span> | <span class="t">understand what it means to modify the input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2257" target="_blank">00:37:37.000</a></span> | <span class="t">for both possible orderings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2261" target="_blank">00:37:41.000</a></span> | <span class="t">Yeah, sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2262" target="_blank">00:37:42.000</a></span> | <span class="t">So if you're doing textual entailment,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2265" target="_blank">00:37:45.000</a></span> | <span class="t">you have distinction between the premise and the hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2267" target="_blank">00:37:47.000</a></span> | <span class="t">Like you have a premise and a hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2272" target="_blank">00:37:52.000</a></span> | <span class="t">So you can just mix them up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2274" target="_blank">00:37:54.000</a></span> | <span class="t">You have a very specific premise and a very specific hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2279" target="_blank">00:37:59.000</a></span> | <span class="t">And the ordering here matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2280" target="_blank">00:38:00.000</a></span> | <span class="t">Like you should put the premise first, and then</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2282" target="_blank">00:38:02.000</a></span> | <span class="t">the special token, and then the hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2284" target="_blank">00:38:04.000</a></span> | <span class="t">You can put the hypothesis first, and then the delimiter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2287" target="_blank">00:38:07.000</a></span> | <span class="t">and then the premise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2288" target="_blank">00:38:08.000</a></span> | <span class="t">So this is what they mean by the ordering here matters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2292" target="_blank">00:38:12.000</a></span> | <span class="t">But for semantic similarity, you just have two sentences.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2296" target="_blank">00:38:16.000</a></span> | <span class="t">There is no inherent ordering for the semantic similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2300" target="_blank">00:38:20.000</a></span> | <span class="t">task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2303" target="_blank">00:38:23.000</a></span> | <span class="t">I hope this answers the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2307" target="_blank">00:38:27.000</a></span> | <span class="t">I think it did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2309" target="_blank">00:38:29.000</a></span> | <span class="t">He said, ah, I see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2310" target="_blank">00:38:30.000</a></span> | <span class="t">Thanks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2310" target="_blank">00:38:30.500</a></span> | <span class="t">I think probably that's about it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2313" target="_blank">00:38:33.000</a></span> | <span class="t">I don't see any other questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2316" target="_blank">00:38:36.000</a></span> | <span class="t">He said, does it also combat things like positional biases</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2319" target="_blank">00:38:39.000</a></span> | <span class="t">and transformers by switching the order of the sentences</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2322" target="_blank">00:38:42.000</a></span> | <span class="t">itself?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2323" target="_blank">00:38:43.000</a></span> | <span class="t">And you've added two quantities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2325" target="_blank">00:38:45.000</a></span> | <span class="t">Yeah, exactly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2326" target="_blank">00:38:46.000</a></span> | <span class="t">I think, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2327" target="_blank">00:38:47.000</a></span> | <span class="t">I think this is one of the motivations they did this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2330" target="_blank">00:38:50.000</a></span> | <span class="t">They want to say, there is no inherent order for this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2334" target="_blank">00:38:54.000</a></span> | <span class="t">So maybe the transformer will just have a bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2337" target="_blank">00:38:57.000</a></span> | <span class="t">So let's try both ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2338" target="_blank">00:38:58.000</a></span> | <span class="t">Like let's give it the first sentence, and then the second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2341" target="_blank">00:39:01.000</a></span> | <span class="t">And let's give it the other way around</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2345" target="_blank">00:39:05.000</a></span> | <span class="t">to get over the positional bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2348" target="_blank">00:39:08.000</a></span> | <span class="t">Because I think for some models, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2352" target="_blank">00:39:12.000</a></span> | <span class="t">pay more attention to the last few tokens in the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2357" target="_blank">00:39:17.000</a></span> | <span class="t">And they disregard the earlier tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2359" target="_blank">00:39:19.000</a></span> | <span class="t">This tends to happen sometimes, yes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2363" target="_blank">00:39:23.000</a></span> | <span class="t">I think, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2364" target="_blank">00:39:24.000</a></span> | <span class="t">I think this is a good way to think about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2369" target="_blank">00:39:29.000</a></span> | <span class="t">Awesome.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2370" target="_blank">00:39:30.000</a></span> | <span class="t">I think-- I don't see any other questions in the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2373" target="_blank">00:39:33.000</a></span> | <span class="t">So I can just service them as and when they come.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2375" target="_blank">00:39:35.000</a></span> | <span class="t">So I think we should be good for now by the looks of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2378" target="_blank">00:39:38.000</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2379" target="_blank">00:39:39.000</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2379" target="_blank">00:39:39.500</a></span> | <span class="t">So that was the whole approach.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2381" target="_blank">00:39:41.000</a></span> | <span class="t">We can now discuss some details about the training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2384" target="_blank">00:39:44.000</a></span> | <span class="t">And back then, OpenAI actually did release info</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2387" target="_blank">00:39:47.000</a></span> | <span class="t">about their training and models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2389" target="_blank">00:39:49.000</a></span> | <span class="t">They don't do this now, unfortunately.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2392" target="_blank">00:39:52.000</a></span> | <span class="t">But anyways, so the data set they use for training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2395" target="_blank">00:39:55.000</a></span> | <span class="t">is BookCorpus for training the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2398" target="_blank">00:39:58.000</a></span> | <span class="t">So this is in step one, which is unsupervised training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2401" target="_blank">00:40:01.000</a></span> | <span class="t">BookCorpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2403" target="_blank">00:40:03.000</a></span> | <span class="t">And back then, this was huge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2404" target="_blank">00:40:04.000</a></span> | <span class="t">It has 7,000 unique unpublished books from different genres.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2408" target="_blank">00:40:08.000</a></span> | <span class="t">So the variety here helps as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2411" target="_blank">00:40:11.000</a></span> | <span class="t">They have adventure, fantasy, and romance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2413" target="_blank">00:40:13.000</a></span> | <span class="t">So kind of like a very good, big, diverse data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2418" target="_blank">00:40:18.000</a></span> | <span class="t">And the main advantage of this data set,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2421" target="_blank">00:40:21.000</a></span> | <span class="t">and the reason they chose it, is because it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2423" target="_blank">00:40:23.000</a></span> | <span class="t">has long stretches of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2425" target="_blank">00:40:25.000</a></span> | <span class="t">So if you have a book or a novel,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2427" target="_blank">00:40:27.000</a></span> | <span class="t">you have a paragraph that's maybe 10 lines or more.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2431" target="_blank">00:40:31.000</a></span> | <span class="t">And this helps the model to learn</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2433" target="_blank">00:40:33.000</a></span> | <span class="t">how to handle long-range information</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2435" target="_blank">00:40:35.000</a></span> | <span class="t">and how to handle long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2437" target="_blank">00:40:37.000</a></span> | <span class="t">For example, there is also another data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2439" target="_blank">00:40:39.000</a></span> | <span class="t">that's called WordBenchmark, which is also big and diverse.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2443" target="_blank">00:40:43.000</a></span> | <span class="t">But it doesn't have this long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2446" target="_blank">00:40:46.000</a></span> | <span class="t">It's just a bunch of small sentences, I would say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2451" target="_blank">00:40:51.000</a></span> | <span class="t">And this way, your model will not</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2454" target="_blank">00:40:54.000</a></span> | <span class="t">learn how to handle long context.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2456" target="_blank">00:40:56.000</a></span> | <span class="t">And a side note here is like ELMo is also</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2460" target="_blank">00:41:00.000</a></span> | <span class="t">a very seminal work in NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2463" target="_blank">00:41:03.000</a></span> | <span class="t">And it falls under the same domain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2465" target="_blank">00:41:05.000</a></span> | <span class="t">of unsupervised pre-training and having good embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2470" target="_blank">00:41:10.000</a></span> | <span class="t">So it's one of the fundamental papers and works in NLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2475" target="_blank">00:41:15.000</a></span> | <span class="t">So they say their model achieves a very low token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2478" target="_blank">00:41:18.000</a></span> | <span class="t">level perplexity of 18.4 on this corpus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2482" target="_blank">00:41:22.000</a></span> | <span class="t">But I don't think this is actually low today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2485" target="_blank">00:41:25.000</a></span> | <span class="t">I think perplexity of 18 is a bit high.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2488" target="_blank">00:41:28.000</a></span> | <span class="t">But I'm not sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2490" target="_blank">00:41:30.000</a></span> | <span class="t">And their model is about--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2492" target="_blank">00:41:32.000</a></span> | <span class="t">their data set is about 5 gigabyte in size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2494" target="_blank">00:41:34.000</a></span> | <span class="t">So not quite big by today's standard, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2499" target="_blank">00:41:39.000</a></span> | <span class="t">The architecture, as we mentioned,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2501" target="_blank">00:41:41.000</a></span> | <span class="t">they have a transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2502" target="_blank">00:41:42.000</a></span> | <span class="t">They use a tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2503" target="_blank">00:41:43.000</a></span> | <span class="t">They use byte-pair encoding back then, which is also, I think,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2509" target="_blank">00:41:49.000</a></span> | <span class="t">what's used right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2510" target="_blank">00:41:50.000</a></span> | <span class="t">So this is still not changed from today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2513" target="_blank">00:41:53.000</a></span> | <span class="t">They have a surprisingly big vocab size</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2517" target="_blank">00:41:57.000</a></span> | <span class="t">by their time's standard.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2519" target="_blank">00:41:59.000</a></span> | <span class="t">Like, they have 40,000 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2521" target="_blank">00:42:01.000</a></span> | <span class="t">I think Llamatu had also something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2524" target="_blank">00:42:04.000</a></span> | <span class="t">that's in the same range, like 40,000, 50,000 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2527" target="_blank">00:42:07.000</a></span> | <span class="t">So back then, this was actually quite big, I would say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2531" target="_blank">00:42:11.000</a></span> | <span class="t">And they used the FTFY library to clean the raw text</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2534" target="_blank">00:42:14.000</a></span> | <span class="t">and then do some standardization using spaCy tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2539" target="_blank">00:42:19.000</a></span> | <span class="t">So this is good work in the tokenization area, I would say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2545" target="_blank">00:42:25.000</a></span> | <span class="t">And their model is just a typical transformer model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2549" target="_blank">00:42:29.000</a></span> | <span class="t">like typical by today's standard, just a transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2552" target="_blank">00:42:32.000</a></span> | <span class="t">decoder-only transformer with, like, 12 layers and mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2555" target="_blank">00:42:35.000</a></span> | <span class="t">self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2556" target="_blank">00:42:36.000</a></span> | <span class="t">And it has a very big size of 117 millions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2560" target="_blank">00:42:40.000</a></span> | <span class="t">And this was actually big back then, although this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2563" target="_blank">00:42:43.000</a></span> | <span class="t">is trivial now, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2565" target="_blank">00:42:45.000</a></span> | <span class="t">Their embedding, they used learned positional embeddings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2568" target="_blank">00:42:48.000</a></span> | <span class="t">compared to the sinusoidal embedding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2571" target="_blank">00:42:51.000</a></span> | <span class="t">in the original transformer paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2573" target="_blank">00:42:53.000</a></span> | <span class="t">So this was actually much simpler to implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2577" target="_blank">00:42:57.000</a></span> | <span class="t">And the model just has to learn the embeddings in training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2581" target="_blank">00:43:01.000</a></span> | <span class="t">They have a context size of 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2584" target="_blank">00:43:04.000</a></span> | <span class="t">Again, back then, that was very big.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2587" target="_blank">00:43:07.000</a></span> | <span class="t">And they also used tied input and output token embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2590" target="_blank">00:43:10.000</a></span> | <span class="t">as we mentioned.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2591" target="_blank">00:43:11.000</a></span> | <span class="t">And for the attention block, they have 12 attention heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2594" target="_blank">00:43:14.000</a></span> | <span class="t">Each head has 64--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2597" target="_blank">00:43:17.000</a></span> | <span class="t">each head has a dimension of 64 for a total of 768 dimensions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2604" target="_blank">00:43:24.000</a></span> | <span class="t">for the entire attention block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2607" target="_blank">00:43:27.000</a></span> | <span class="t">And after the attention, you have the MLP layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2609" target="_blank">00:43:29.000</a></span> | <span class="t">also known as position-wise feedforward network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2612" target="_blank">00:43:32.000</a></span> | <span class="t">And the size of the inner state of this network is 3,072--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2618" target="_blank">00:43:38.000</a></span> | <span class="t">the size is 3,072.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2621" target="_blank">00:43:41.000</a></span> | <span class="t">And this means that, actually, this is an expansion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2623" target="_blank">00:43:43.000</a></span> | <span class="t">and then contraction network.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2626" target="_blank">00:43:46.000</a></span> | <span class="t">So you go from 768 to 3K, and then you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2629" target="_blank">00:43:49.000</a></span> | <span class="t">go back from 3K to 768.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2631" target="_blank">00:43:51.000</a></span> | <span class="t">And all this is happening inside this MLP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2633" target="_blank">00:43:53.000</a></span> | <span class="t">And the activation layer they used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2635" target="_blank">00:43:55.000</a></span> | <span class="t">is the GLO activation layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2639" target="_blank">00:43:59.000</a></span> | <span class="t">The optimizer, they used Adam, which was becoming</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2643" target="_blank">00:44:03.000</a></span> | <span class="t">very popular back then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2644" target="_blank">00:44:04.000</a></span> | <span class="t">I think Adam was developed in 2014, 2015.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2649" target="_blank">00:44:09.000</a></span> | <span class="t">So it was getting a lot of traction back then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2651" target="_blank">00:44:11.000</a></span> | <span class="t">And the maximum learning rate is this learning rate,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2654" target="_blank">00:44:14.000</a></span> | <span class="t">which is also very popular nowadays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2656" target="_blank">00:44:16.000</a></span> | <span class="t">I think this was used in one of the Lama-Tung models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2660" target="_blank">00:44:20.000</a></span> | <span class="t">So this is very familiar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2663" target="_blank">00:44:23.000</a></span> | <span class="t">They do warm-up as well from 0 to the maximum learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2666" target="_blank">00:44:26.000</a></span> | <span class="t">rate over 2K steps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2669" target="_blank">00:44:29.000</a></span> | <span class="t">And then they just cosine annealing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2672" target="_blank">00:44:32.000</a></span> | <span class="t">to-- they do cosine annealing from the maximum learning rate</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2675" target="_blank">00:44:35.000</a></span> | <span class="t">to 0, so just learning rate decay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2679" target="_blank">00:44:39.000</a></span> | <span class="t">Their compute, they used one machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2683" target="_blank">00:44:43.000</a></span> | <span class="t">that has 8x P600 GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2687" target="_blank">00:44:47.000</a></span> | <span class="t">This is the same family of GPUs as P100, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2690" target="_blank">00:44:50.000</a></span> | <span class="t">But I don't have much information about this GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2693" target="_blank">00:44:53.000</a></span> | <span class="t">They train for 30 days, which actually is not bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2699" target="_blank">00:44:59.000</a></span> | <span class="t">That's not very long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2700" target="_blank">00:45:00.000</a></span> | <span class="t">But back then, this was very long, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2703" target="_blank">00:45:03.000</a></span> | <span class="t">Their utilization dimension is 0.33.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2706" target="_blank">00:45:06.000</a></span> | <span class="t">And the total flops is 0.6 petaflop,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2710" target="_blank">00:45:10.000</a></span> | <span class="t">so almost one petaflop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2712" target="_blank">00:45:12.000</a></span> | <span class="t">And for reference, I think the tiny grad machine</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2715" target="_blank">00:45:15.000</a></span> | <span class="t">is trying to give you one petaflop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2719" target="_blank">00:45:19.000</a></span> | <span class="t">Or I could be wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2721" target="_blank">00:45:21.000</a></span> | <span class="t">Yeah, I could be wrong about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2723" target="_blank">00:45:23.000</a></span> | <span class="t">So anyway, the compute they used is almost one petaflop days.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2728" target="_blank">00:45:28.000</a></span> | <span class="t">And this is the way they calculated this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2731" target="_blank">00:45:31.000</a></span> | <span class="t">And this is actually how the model looks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2733" target="_blank">00:45:33.000</a></span> | <span class="t">like if you try to do it in the transformers framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2736" target="_blank">00:45:36.000</a></span> | <span class="t">So you have token embedding, position embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2738" target="_blank">00:45:38.000</a></span> | <span class="t">and then some dropouts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2740" target="_blank">00:45:40.000</a></span> | <span class="t">And then you have your actual transformer blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2744" target="_blank">00:45:44.000</a></span> | <span class="t">And this is the architecture of the language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2748" target="_blank">00:45:48.000</a></span> | <span class="t">So there is no head in here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2750" target="_blank">00:45:50.000</a></span> | <span class="t">So the block is just attention, and then layer norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2753" target="_blank">00:45:53.000</a></span> | <span class="t">and then MLP, and then layer norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2755" target="_blank">00:45:55.000</a></span> | <span class="t">So if there are no questions, we can move on</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2762" target="_blank">00:46:02.000</a></span> | <span class="t">to the second step, which is supervised fine tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2767" target="_blank">00:46:07.000</a></span> | <span class="t">There was one question from Sean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2769" target="_blank">00:46:09.000</a></span> | <span class="t">which was, are perplexity numbers comparable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2771" target="_blank">00:46:11.000</a></span> | <span class="t">across different models?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2773" target="_blank">00:46:13.000</a></span> | <span class="t">And I think we discussed just now</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2775" target="_blank">00:46:15.000</a></span> | <span class="t">that the model itself has a perplexity of 18.4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2778" target="_blank">00:46:18.000</a></span> | <span class="t">And so in this case, would it be OK to compare--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2783" target="_blank">00:46:23.000</a></span> | <span class="t">is it a metric that's invariant across different models</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2788" target="_blank">00:46:28.000</a></span> | <span class="t">in this sense?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2791" target="_blank">00:46:31.000</a></span> | <span class="t">Yeah, so that's a good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2792" target="_blank">00:46:32.000</a></span> | <span class="t">I think perplexity is just a metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2794" target="_blank">00:46:34.000</a></span> | <span class="t">Like, you're going to measure perplexity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2796" target="_blank">00:46:36.000</a></span> | <span class="t">on a certain data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2797" target="_blank">00:46:37.000</a></span> | <span class="t">Like, let's say you have this data set that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2799" target="_blank">00:46:39.000</a></span> | <span class="t">is 1 trillion tokens, or 1 billion tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2802" target="_blank">00:46:42.000</a></span> | <span class="t">And you measure perplexity of GPT-1 on this data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2806" target="_blank">00:46:46.000</a></span> | <span class="t">And you can measure the perplexity of Lama2</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2808" target="_blank">00:46:48.000</a></span> | <span class="t">on this data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2809" target="_blank">00:46:49.000</a></span> | <span class="t">And I think you can compare this metric.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2812" target="_blank">00:46:52.000</a></span> | <span class="t">It's like saying Lama2 has a human eval of 70,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2817" target="_blank">00:46:57.000</a></span> | <span class="t">and GPT-4 has a human eval of maybe 90.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2820" target="_blank">00:47:00.000</a></span> | <span class="t">It's not totally fair, but we can do it, I guess.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2823" target="_blank">00:47:03.000</a></span> | <span class="t">We can get away with doing it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2825" target="_blank">00:47:05.000</a></span> | <span class="t">Yeah, but it might not be 100% fair.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2827" target="_blank">00:47:07.000</a></span> | <span class="t">Yeah, so that's a good point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2833" target="_blank">00:47:13.000</a></span> | <span class="t">OK, cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2834" target="_blank">00:47:14.000</a></span> | <span class="t">I think that's probably the only question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2836" target="_blank">00:47:16.000</a></span> | <span class="t">So I think we should move on to supervised fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2840" target="_blank">00:47:20.000</a></span> | <span class="t">So the second step is supervised fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2842" target="_blank">00:47:22.000</a></span> | <span class="t">And they use these data sets for this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2844" target="_blank">00:47:24.000</a></span> | <span class="t">So for sentence similarity, they use these data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2848" target="_blank">00:47:28.000</a></span> | <span class="t">And for classifications, they used COLA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2850" target="_blank">00:47:30.000</a></span> | <span class="t">which I think is popular now as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2854" target="_blank">00:47:34.000</a></span> | <span class="t">I'm not going to go into details about this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2856" target="_blank">00:47:36.000</a></span> | <span class="t">because I don't have much information about these.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2860" target="_blank">00:47:40.000</a></span> | <span class="t">And the architecture is just use the same backbone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2863" target="_blank">00:47:43.000</a></span> | <span class="t">as pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2864" target="_blank">00:47:44.000</a></span> | <span class="t">And you add your head, mostly classifier head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2868" target="_blank">00:47:48.000</a></span> | <span class="t">with a dropout of 0.1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2870" target="_blank">00:47:50.000</a></span> | <span class="t">They train for three epochs with a batch size of 32.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2873" target="_blank">00:47:53.000</a></span> | <span class="t">This is also still standard nowadays as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2877" target="_blank">00:47:57.000</a></span> | <span class="t">People usually train, do fine-tuning for three epochs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2880" target="_blank">00:48:00.000</a></span> | <span class="t">with a batch size of maybe 32 or 64.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2883" target="_blank">00:48:03.000</a></span> | <span class="t">So yeah, that's common nowadays as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2886" target="_blank">00:48:06.000</a></span> | <span class="t">The learning rate is 6.25, 8.25.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2890" target="_blank">00:48:10.000</a></span> | <span class="t">And also, people use very similar learning rates nowadays</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2895" target="_blank">00:48:15.000</a></span> | <span class="t">as well.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2897" target="_blank">00:48:17.000</a></span> | <span class="t">So this is very familiar.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2898" target="_blank">00:48:18.000</a></span> | <span class="t">This architecture for fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2901" target="_blank">00:48:21.000</a></span> | <span class="t">is very familiar, even today.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2904" target="_blank">00:48:24.000</a></span> | <span class="t">And they also use learning rate decay with a warm-up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2908" target="_blank">00:48:28.000</a></span> | <span class="t">And when they use the auxiliary language,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2912" target="_blank">00:48:32.000</a></span> | <span class="t">the auxiliary objective, they used a lambda of 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2916" target="_blank">00:48:36.000</a></span> | <span class="t">So the weight of the auxiliary language modeling objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2919" target="_blank">00:48:39.000</a></span> | <span class="t">was 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2921" target="_blank">00:48:41.000</a></span> | <span class="t">But this is not popular nowadays.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2923" target="_blank">00:48:43.000</a></span> | <span class="t">Most people don't do this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2924" target="_blank">00:48:44.000</a></span> | <span class="t">Any question about this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2933" target="_blank">00:48:53.000</a></span> | <span class="t">I think we should--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2934" target="_blank">00:48:54.000</a></span> | <span class="t">Before we go to the benchmarks?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2935" target="_blank">00:48:55.000</a></span> | <span class="t">Yeah, the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2936" target="_blank">00:48:56.000</a></span> | <span class="t">There's no questions in the chat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2938" target="_blank">00:48:58.000</a></span> | <span class="t">So I think we can just move on to the benchmarks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2941" target="_blank">00:49:01.000</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2942" target="_blank">00:49:02.000</a></span> | <span class="t">So the benchmark is like--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2944" target="_blank">00:49:04.000</a></span> | <span class="t">they do a lot of SOTA performance on many tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2947" target="_blank">00:49:07.000</a></span> | <span class="t">Like you can see here, almost every single task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2950" target="_blank">00:49:10.000</a></span> | <span class="t">except this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2951" target="_blank">00:49:11.000</a></span> | <span class="t">And there are ones where they have significant improvement,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2954" target="_blank">00:49:14.000</a></span> | <span class="t">like this in QNLI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2957" target="_blank">00:49:17.000</a></span> | <span class="t">They have like 6% absolute improvement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2961" target="_blank">00:49:21.000</a></span> | <span class="t">I think-- let's go back a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2963" target="_blank">00:49:23.000</a></span> | <span class="t">QNLI-- where is it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2967" target="_blank">00:49:27.000</a></span> | <span class="t">In here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2968" target="_blank">00:49:28.000</a></span> | <span class="t">I think the QNLI is more of a natural language understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2978" target="_blank">00:49:38.000</a></span> | <span class="t">where it requires reasoning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2979" target="_blank">00:49:39.000</a></span> | <span class="t">And this is where they make the most improvement, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2983" target="_blank">00:49:43.000</a></span> | <span class="t">But overall, they are doing very good performance on many tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2989" target="_blank">00:49:49.000</a></span> | <span class="t">And they are comparing to LSTMs and other models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2992" target="_blank">00:49:52.000</a></span> | <span class="t">So good news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2993" target="_blank">00:49:53.000</a></span> | <span class="t">GPT-1 tends to be a good model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2996" target="_blank">00:49:56.000</a></span> | <span class="t">Yeah, this is for natural language inference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=2999" target="_blank">00:49:59.000</a></span> | <span class="t">For question answering and common sense,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3001" target="_blank">00:50:01.000</a></span> | <span class="t">they also make very big gains, as you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3005" target="_blank">00:50:05.000</a></span> | <span class="t">So 76 compared to 86.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3010" target="_blank">00:50:10.000</a></span> | <span class="t">They compare also to multiple models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3013" target="_blank">00:50:13.000</a></span> | <span class="t">And one of them is actually an ensemble of nine models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3016" target="_blank">00:50:16.000</a></span> | <span class="t">as they denote here by 9x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3020" target="_blank">00:50:20.000</a></span> | <span class="t">And also, the same happens for semantic similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3023" target="_blank">00:50:23.000</a></span> | <span class="t">and classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3024" target="_blank">00:50:24.000</a></span> | <span class="t">Although, I would say the boost in performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3028" target="_blank">00:50:28.000</a></span> | <span class="t">is not as big as the previous ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3030" target="_blank">00:50:30.000</a></span> | <span class="t">They are good on some metrics and bad on other metrics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3039" target="_blank">00:50:39.000</a></span> | <span class="t">So this is my favorite part of the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3045" target="_blank">00:50:45.000</a></span> | <span class="t">They did-- after the benchmarks, they now have a good model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3048" target="_blank">00:50:48.000</a></span> | <span class="t">So they are trying to understand why their model is good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3051" target="_blank">00:50:51.000</a></span> | <span class="t">and why their GPT-1 is suddenly SOTA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3056" target="_blank">00:50:56.000</a></span> | <span class="t">So they do analysis and try to understand</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3060" target="_blank">00:51:00.000</a></span> | <span class="t">why this is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3062" target="_blank">00:51:02.000</a></span> | <span class="t">So the first step is they're trying</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3064" target="_blank">00:51:04.000</a></span> | <span class="t">to analyze the impact of the transfer learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3067" target="_blank">00:51:07.000</a></span> | <span class="t">the number of layers transferred from the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3070" target="_blank">00:51:10.000</a></span> | <span class="t">to the fine-tuned model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3073" target="_blank">00:51:13.000</a></span> | <span class="t">So they just take the pre-trained transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3076" target="_blank">00:51:16.000</a></span> | <span class="t">and they take all the layers and do fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3080" target="_blank">00:51:20.000</a></span> | <span class="t">And then they take 11 layers, and then they do fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3083" target="_blank">00:51:23.000</a></span> | <span class="t">And then they take 10 layers and do fine-tuning, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3086" target="_blank">00:51:26.000</a></span> | <span class="t">They compare the performance of these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3088" target="_blank">00:51:28.000</a></span> | <span class="t">And as you can see, the more layers you add,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3090" target="_blank">00:51:30.000</a></span> | <span class="t">the more performance you get.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3092" target="_blank">00:51:32.000</a></span> | <span class="t">And if you do zero layers transferred,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3095" target="_blank">00:51:35.000</a></span> | <span class="t">you're not taking any of the pre-trained weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3099" target="_blank">00:51:39.000</a></span> | <span class="t">You're just starting from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3101" target="_blank">00:51:41.000</a></span> | <span class="t">You get this performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3102" target="_blank">00:51:42.000</a></span> | <span class="t">But if you just add one layer, you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3104" target="_blank">00:51:44.000</a></span> | <span class="t">get a very significant boost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3106" target="_blank">00:51:46.000</a></span> | <span class="t">The solid lines here are the dev data sets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3110" target="_blank">00:51:50.000</a></span> | <span class="t">The dashed line is the training data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3112" target="_blank">00:51:52.000</a></span> | <span class="t">So you can see that a very big improvement in performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3114" target="_blank">00:51:54.000</a></span> | <span class="t">just by adding one layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3116" target="_blank">00:51:56.000</a></span> | <span class="t">And this is almost like a continuous trend.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3120" target="_blank">00:52:00.000</a></span> | <span class="t">So they observe the obvious fact that transferring more layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3126" target="_blank">00:52:06.000</a></span> | <span class="t">improves performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3127" target="_blank">00:52:07.000</a></span> | <span class="t">And just only transferring the embedding layers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3130" target="_blank">00:52:10.000</a></span> | <span class="t">also improves performance significantly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3133" target="_blank">00:52:13.000</a></span> | <span class="t">And they mentioned that each layer adds a boost of 9%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3137" target="_blank">00:52:17.000</a></span> | <span class="t">for each layer you add on this task, multi-NLI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3142" target="_blank">00:52:22.000</a></span> | <span class="t">And this analysis actually indicates</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3144" target="_blank">00:52:24.000</a></span> | <span class="t">that each layer in the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3146" target="_blank">00:52:26.000</a></span> | <span class="t">actually has a purpose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3147" target="_blank">00:52:27.000</a></span> | <span class="t">And it learns something that's useful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3150" target="_blank">00:52:30.000</a></span> | <span class="t">And it's very helpful in the fine-tuning model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3154" target="_blank">00:52:34.000</a></span> | <span class="t">So different layers learn different things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3157" target="_blank">00:52:37.000</a></span> | <span class="t">They learn different scales.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3158" target="_blank">00:52:38.000</a></span> | <span class="t">And they are all helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3160" target="_blank">00:52:40.000</a></span> | <span class="t">This is a very good finding of this work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3164" target="_blank">00:52:44.000</a></span> | <span class="t">So each layer in the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3166" target="_blank">00:52:46.000</a></span> | <span class="t">contains useful information on functionality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3168" target="_blank">00:52:48.000</a></span> | <span class="t">for solving for target tasks like classification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3171" target="_blank">00:52:51.000</a></span> | <span class="t">The second piece of analysis they did is zero-shot evaluation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3179" target="_blank">00:52:59.000</a></span> | <span class="t">And this was, I would say, radical back then.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3183" target="_blank">00:53:03.000</a></span> | <span class="t">They are trying to evaluate how good the pre-trained model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3188" target="_blank">00:53:08.000</a></span> | <span class="t">on these tasks without even fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3191" target="_blank">00:53:11.000</a></span> | <span class="t">And they want to answer the questions like,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3193" target="_blank">00:53:13.000</a></span> | <span class="t">why is the language model pre-training effective?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3196" target="_blank">00:53:16.000</a></span> | <span class="t">Why does pre-training a transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3200" target="_blank">00:53:20.000</a></span> | <span class="t">on language modeling help us when we are doing classification?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3204" target="_blank">00:53:24.000</a></span> | <span class="t">They have a hypothesis that the underlying generative model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3208" target="_blank">00:53:28.000</a></span> | <span class="t">learns actually multiple tasks in the pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3211" target="_blank">00:53:31.000</a></span> | <span class="t">So it's not just learning language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3213" target="_blank">00:53:33.000</a></span> | <span class="t">Because if you think about it, if you're</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3216" target="_blank">00:53:36.000</a></span> | <span class="t">learning how to model language properly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3220" target="_blank">00:53:40.000</a></span> | <span class="t">you're more likely to have a deeper understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3223" target="_blank">00:53:43.000</a></span> | <span class="t">of the language and just natural language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3227" target="_blank">00:53:47.000</a></span> | <span class="t">Like, for example, if you speak only English and French,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3233" target="_blank">00:53:53.000</a></span> | <span class="t">and you cannot speak, let's say, Spanish,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3235" target="_blank">00:53:55.000</a></span> | <span class="t">you won't be able to do classifications in Spanish.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3239" target="_blank">00:53:59.000</a></span> | <span class="t">But if you speak English and you speak French,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3241" target="_blank">00:54:01.000</a></span> | <span class="t">you probably have the understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3243" target="_blank">00:54:03.000</a></span> | <span class="t">to do these tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3245" target="_blank">00:54:05.000</a></span> | <span class="t">So this is something cool and something to think about a bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3250" target="_blank">00:54:10.000</a></span> | <span class="t">Another thing is attention is very helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3253" target="_blank">00:54:13.000</a></span> | <span class="t">Transformers show very big improvement compared to LSTMs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3258" target="_blank">00:54:18.000</a></span> | <span class="t">Or that's what they are hypothesizing about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3261" target="_blank">00:54:21.000</a></span> | <span class="t">and what they are trying to test.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3263" target="_blank">00:54:23.000</a></span> | <span class="t">So to test these two hypotheses, they</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3265" target="_blank">00:54:25.000</a></span> | <span class="t">designed a series of heuristics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3267" target="_blank">00:54:27.000</a></span> | <span class="t">They basically tried to evaluate the pre-trained model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3272" target="_blank">00:54:32.000</a></span> | <span class="t">on these tasks before doing the supervised fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3276" target="_blank">00:54:36.000</a></span> | <span class="t">And they have different modifications</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3278" target="_blank">00:54:38.000</a></span> | <span class="t">for each data set and each task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3282" target="_blank">00:54:42.000</a></span> | <span class="t">For example, for linguistic acceptability with the COLA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3286" target="_blank">00:54:46.000</a></span> | <span class="t">they used the examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3288" target="_blank">00:54:48.000</a></span> | <span class="t">And they take the average token log probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3291" target="_blank">00:54:51.000</a></span> | <span class="t">for each token and use this as a score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3294" target="_blank">00:54:54.000</a></span> | <span class="t">And they have a threshold.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3296" target="_blank">00:54:56.000</a></span> | <span class="t">And they just determine if the model got this right or wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3299" target="_blank">00:54:59.000</a></span> | <span class="t">Something that's very cool is how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3303" target="_blank">00:55:03.000</a></span> | <span class="t">they handle sentiment analysis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3305" target="_blank">00:55:05.000</a></span> | <span class="t">So you've got your, let's say, Amazon review.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3308" target="_blank">00:55:08.000</a></span> | <span class="t">And you append the token vary to the review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3311" target="_blank">00:55:11.000</a></span> | <span class="t">and restrict the language to generate only two tokens, one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3315" target="_blank">00:55:15.000</a></span> | <span class="t">of two tokens, positive or negative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3317" target="_blank">00:55:17.000</a></span> | <span class="t">And you see which token has the higher score or higher</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3320" target="_blank">00:55:20.000</a></span> | <span class="t">probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3321" target="_blank">00:55:21.000</a></span> | <span class="t">And this is the prediction of the model on this sentiment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3324" target="_blank">00:55:24.000</a></span> | <span class="t">or this paragraph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3326" target="_blank">00:55:26.000</a></span> | <span class="t">So just restrict the language model prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3329" target="_blank">00:55:29.000</a></span> | <span class="t">to these two tokens and see which one is higher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3334" target="_blank">00:55:34.000</a></span> | <span class="t">This is actually pretty cool.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3335" target="_blank">00:55:35.000</a></span> | <span class="t">It's kind of similar to constrained grammar</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3338" target="_blank">00:55:38.000</a></span> | <span class="t">and constrained output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3341" target="_blank">00:55:41.000</a></span> | <span class="t">Another example is question answering.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3343" target="_blank">00:55:43.000</a></span> | <span class="t">So for each answer, as you said, concatenate the document</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3346" target="_blank">00:55:46.000</a></span> | <span class="t">question and answer in the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3348" target="_blank">00:55:48.000</a></span> | <span class="t">And you average the token log probability of just the answer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3352" target="_blank">00:55:52.000</a></span> | <span class="t">and use this as the score for this answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3354" target="_blank">00:55:54.000</a></span> | <span class="t">And do this for multiple answers and just</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3356" target="_blank">00:55:56.000</a></span> | <span class="t">pick the one with the highest score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3358" target="_blank">00:55:58.000</a></span> | <span class="t">And for winning grad schemas, for this task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3366" target="_blank">00:56:06.000</a></span> | <span class="t">let's say you have a sentence that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3368" target="_blank">00:56:08.000</a></span> | <span class="t">says the city councilman refused the demonstrators a permit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3371" target="_blank">00:56:11.000</a></span> | <span class="t">because they advocated violence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3373" target="_blank">00:56:13.000</a></span> | <span class="t">So the goal here is to find this word "they."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3377" target="_blank">00:56:17.000</a></span> | <span class="t">What does it refer to?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3378" target="_blank">00:56:18.000</a></span> | <span class="t">Does it refer to the demonstrators</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3380" target="_blank">00:56:20.000</a></span> | <span class="t">or the city councilman?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3382" target="_blank">00:56:22.000</a></span> | <span class="t">This is what they mean by winning grad schema.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3384" target="_blank">00:56:24.000</a></span> | <span class="t">And I think it's a very popular task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3387" target="_blank">00:56:27.000</a></span> | <span class="t">So the way they do this is they take this word "they"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3390" target="_blank">00:56:30.000</a></span> | <span class="t">and replace it with the two possible answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3393" target="_blank">00:56:33.000</a></span> | <span class="t">In this case, councilman and demonstrators.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3395" target="_blank">00:56:35.000</a></span> | <span class="t">So you have example A, you use the word "councilman."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3400" target="_blank">00:56:40.000</a></span> | <span class="t">And example B, use the word "demonstrators."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3402" target="_blank">00:56:42.000</a></span> | <span class="t">And just average the token probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3407" target="_blank">00:56:47.000</a></span> | <span class="t">of what comes after this word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3409" target="_blank">00:56:49.000</a></span> | <span class="t">And then you just take the one with the higher probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3415" target="_blank">00:56:55.000</a></span> | <span class="t">score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3418" target="_blank">00:56:58.000</a></span> | <span class="t">So this is how they wanted to evaluate the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3421" target="_blank">00:57:01.000</a></span> | <span class="t">Let's see, actually, the evaluation results.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3424" target="_blank">00:57:04.000</a></span> | <span class="t">And surprise, surprise, the longer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3428" target="_blank">00:57:08.000</a></span> | <span class="t">you train the model and the more data you give the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3433" target="_blank">00:57:13.000</a></span> | <span class="t">the more it tends to perform on this task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3435" target="_blank">00:57:15.000</a></span> | <span class="t">So the better zero-shot performance it has, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3439" target="_blank">00:57:19.000</a></span> | <span class="t">This is a very cool graph.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3441" target="_blank">00:57:21.000</a></span> | <span class="t">So we have different tasks here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3443" target="_blank">00:57:23.000</a></span> | <span class="t">And the solid line is the GPT transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3446" target="_blank">00:57:26.000</a></span> | <span class="t">And you can see with more steps, more training steps,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3449" target="_blank">00:57:29.000</a></span> | <span class="t">the better performance you have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3451" target="_blank">00:57:31.000</a></span> | <span class="t">And they also did a very cool analysis</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3453" target="_blank">00:57:33.000</a></span> | <span class="t">where they trained an LSTM, which is the dashed line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3456" target="_blank">00:57:36.000</a></span> | <span class="t">And you can see that for almost every single task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3459" target="_blank">00:57:39.000</a></span> | <span class="t">the transformer is better than the LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3463" target="_blank">00:57:43.000</a></span> | <span class="t">So they observed the obvious that more training gives</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3467" target="_blank">00:57:47.000</a></span> | <span class="t">better performance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3469" target="_blank">00:57:49.000</a></span> | <span class="t">More pre-training gives better performance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3471" target="_blank">00:57:51.000</a></span> | <span class="t">even in zero-shot settings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3474" target="_blank">00:57:54.000</a></span> | <span class="t">And this suggests that generative pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3477" target="_blank">00:57:57.000</a></span> | <span class="t">actually-- in generative pre-training,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3480" target="_blank">00:58:00.000</a></span> | <span class="t">the model learns a lot of tasks, not just language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3483" target="_blank">00:58:03.000</a></span> | <span class="t">And they mentioned something here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3485" target="_blank">00:58:05.000</a></span> | <span class="t">They said they observed that the LSTM exhibits higher variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3488" target="_blank">00:58:08.000</a></span> | <span class="t">in its zero-shot performance, suggesting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3490" target="_blank">00:58:10.000</a></span> | <span class="t">that the inclusive bits of the transformer assist in transfer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3493" target="_blank">00:58:13.000</a></span> | <span class="t">But I'm not sure what they mean by high variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3496" target="_blank">00:58:16.000</a></span> | <span class="t">in this graph, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3498" target="_blank">00:58:18.000</a></span> | <span class="t">So if anyone has any clue about this, please share it with us.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3501" target="_blank">00:58:21.000</a></span> | <span class="t">Yeah, it's better less than D1, and GPUs are all we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3509" target="_blank">00:58:29.000</a></span> | <span class="t">Sadly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3516" target="_blank">00:58:36.000</a></span> | <span class="t">So the final section in the paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3520" target="_blank">00:58:40.000</a></span> | <span class="t">is the appellation studies they did.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3523" target="_blank">00:58:43.000</a></span> | <span class="t">They have three appellation studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3525" target="_blank">00:58:45.000</a></span> | <span class="t">They want to first see the effect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3527" target="_blank">00:58:47.000</a></span> | <span class="t">of the auxiliary language modeling objective</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3530" target="_blank">00:58:50.000</a></span> | <span class="t">during fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3532" target="_blank">00:58:52.000</a></span> | <span class="t">So in this case, we have the normal transformer training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3538" target="_blank">00:58:58.000</a></span> | <span class="t">with auxiliary language modeling, which is row 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3541" target="_blank">00:59:01.000</a></span> | <span class="t">And we have the one without auxiliary language modeling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3544" target="_blank">00:59:04.000</a></span> | <span class="t">which is row 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3546" target="_blank">00:59:06.000</a></span> | <span class="t">They say that the trend suggests that larger datasets benefit</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3552" target="_blank">00:59:12.000</a></span> | <span class="t">from auxiliary training compared to smaller datasets.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3555" target="_blank">00:59:15.000</a></span> | <span class="t">And this is kind of like actually probably</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3558" target="_blank">00:59:18.000</a></span> | <span class="t">why people stop doing auxiliary language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3560" target="_blank">00:59:20.000</a></span> | <span class="t">It doesn't seem to be quite important, probably.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3566" target="_blank">00:59:26.000</a></span> | <span class="t">The second appellation study is the effect of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3569" target="_blank">00:59:29.000</a></span> | <span class="t">So they just trained the usual transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3572" target="_blank">00:59:32.000</a></span> | <span class="t">and they compared it with an LSTM, so row 1 and row 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3578" target="_blank">00:59:38.000</a></span> | <span class="t">And you can see, yeah, the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3580" target="_blank">00:59:40.000</a></span> | <span class="t">has generally better performance than the LSTM on most tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3587" target="_blank">00:59:47.000</a></span> | <span class="t">So they mentioned that they observed a 5.6 average drop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3590" target="_blank">00:59:50.000</a></span> | <span class="t">when they used the LSTM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3594" target="_blank">00:59:54.000</a></span> | <span class="t">And it only outperforms the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3596" target="_blank">00:59:56.000</a></span> | <span class="t">on one dataset, which is MRPC.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3601" target="_blank">01:00:01.000</a></span> | <span class="t">I'm not sure what this is.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3603" target="_blank">01:00:03.000</a></span> | <span class="t">The second appellation study, and I</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3605" target="_blank">01:00:05.000</a></span> | <span class="t">would say the most important one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3606" target="_blank">01:00:06.000</a></span> | <span class="t">is the effect of pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3608" target="_blank">01:00:08.000</a></span> | <span class="t">So they compared the transformer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3610" target="_blank">01:00:10.000</a></span> | <span class="t">has been trained in the framework they proposed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3613" target="_blank">01:00:13.000</a></span> | <span class="t">the two-step framework.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3614" target="_blank">01:00:14.000</a></span> | <span class="t">And they also compared it to a transformer that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3617" target="_blank">01:00:17.000</a></span> | <span class="t">was directly trained on the supervised task, that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3620" target="_blank">01:00:20.000</a></span> | <span class="t">is, without any pre-training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3621" target="_blank">01:00:21.000</a></span> | <span class="t">So we have row 1 and row 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3623" target="_blank">01:00:23.000</a></span> | <span class="t">You can see that this is where the huge difference shows up,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3626" target="_blank">01:00:26.000</a></span> | <span class="t">like 74 compared to 59, 45 compared to 18, 88 compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3632" target="_blank">01:00:32.000</a></span> | <span class="t">to 71.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3633" target="_blank">01:00:33.000</a></span> | <span class="t">So this is a very big difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3636" target="_blank">01:00:36.000</a></span> | <span class="t">Yeah, and we observed that no pre-training actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3639" target="_blank">01:00:39.000</a></span> | <span class="t">hurts the performance quite a lot on almost all tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3644" target="_blank">01:00:44.000</a></span> | <span class="t">And this results in 15% decrease.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3648" target="_blank">01:00:48.000</a></span> | <span class="t">And this is actually the worst-performing model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3650" target="_blank">01:00:50.000</a></span> | <span class="t">out of all these models.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3652" target="_blank">01:00:52.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3653" target="_blank">01:00:53.000</a></span> | <span class="t">So I would say that the conclusion</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3655" target="_blank">01:00:55.000</a></span> | <span class="t">from this appellation study is that pre-training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3658" target="_blank">01:00:58.000</a></span> | <span class="t">is very, very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3659" target="_blank">01:00:59.000</a></span> | <span class="t">I think this is the final section before the future</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3667" target="_blank">01:01:07.000</a></span> | <span class="t">studies do you want to discuss.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3670" target="_blank">01:01:10.000</a></span> | <span class="t">Any questions so far?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3671" target="_blank">01:01:11.000</a></span> | <span class="t">I think chat seems good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3677" target="_blank">01:01:17.000</a></span> | <span class="t">Do you want to-- you have one more slide, I think.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3679" target="_blank">01:01:19.000</a></span> | <span class="t">You said future studies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3680" target="_blank">01:01:20.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3681" target="_blank">01:01:21.000</a></span> | <span class="t">Do you want to just finish it up?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3683" target="_blank">01:01:23.000</a></span> | <span class="t">Then we can open it up to questions if people--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3686" target="_blank">01:01:26.000</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3687" target="_blank">01:01:27.000</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3688" target="_blank">01:01:28.000</a></span> | <span class="t">Sure.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3689" target="_blank">01:01:29.000</a></span> | <span class="t">So this section, I don't think it was in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3691" target="_blank">01:01:31.000</a></span> | <span class="t">I think it was only on the blog post.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3693" target="_blank">01:01:33.000</a></span> | <span class="t">They discuss what future work could be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3697" target="_blank">01:01:37.000</a></span> | <span class="t">And the first approach is surprise, surprise,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3700" target="_blank">01:01:40.000</a></span> | <span class="t">just scale up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3702" target="_blank">01:01:42.000</a></span> | <span class="t">So they mentioned that they noticed improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3704" target="_blank">01:01:44.000</a></span> | <span class="t">in language modeling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3705" target="_blank">01:01:45.000</a></span> | <span class="t">And it correlates with downstream tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3708" target="_blank">01:01:48.000</a></span> | <span class="t">And they're only using very limited hardware,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3710" target="_blank">01:01:50.000</a></span> | <span class="t">many GPUs on a machine and training</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3712" target="_blank">01:01:52.000</a></span> | <span class="t">on a very small data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3714" target="_blank">01:01:54.000</a></span> | <span class="t">So maybe there is room for improvement</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3716" target="_blank">01:01:56.000</a></span> | <span class="t">if you scale the model, the training, and the data set.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3718" target="_blank">01:01:58.000</a></span> | <span class="t">And I think we know the answer to this question,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3721" target="_blank">01:02:01.000</a></span> | <span class="t">or the answer to this hypothesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3724" target="_blank">01:02:04.000</a></span> | <span class="t">The second approach is--</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3727" target="_blank">01:02:07.000</a></span> | <span class="t">the second futuristic approach they want to try</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3729" target="_blank">01:02:09.000</a></span> | <span class="t">is try to see if there is-- you can tweak the fine-tuning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3734" target="_blank">01:02:14.000</a></span> | <span class="t">approach instead of just doing vanilla fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3737" target="_blank">01:02:17.000</a></span> | <span class="t">You can use adaptation or one of the other fancy ways</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3741" target="_blank">01:02:21.000</a></span> | <span class="t">of doing fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3743" target="_blank">01:02:23.000</a></span> | <span class="t">I don't think this is as important as they might</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3748" target="_blank">01:02:28.000</a></span> | <span class="t">have thought about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3749" target="_blank">01:02:29.000</a></span> | <span class="t">Because people right now are doing just simple fine-tuning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3752" target="_blank">01:02:32.000</a></span> | <span class="t">and it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3754" target="_blank">01:02:34.000</a></span> | <span class="t">So yeah, not quite as promising as the first approach, which</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3757" target="_blank">01:02:37.000</a></span> | <span class="t">is just scaling up.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3759" target="_blank">01:02:39.000</a></span> | <span class="t">And the third one is understanding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3761" target="_blank">01:02:41.000</a></span> | <span class="t">of why generative pretraining helps.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3764" target="_blank">01:02:44.000</a></span> | <span class="t">And they've done some ablations and analysis about this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3768" target="_blank">01:02:48.000</a></span> | <span class="t">They want to do even more further targeted experiments</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3771" target="_blank">01:02:51.000</a></span> | <span class="t">and research to understand this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3773" target="_blank">01:02:53.000</a></span> | <span class="t">So basically, observability and explainability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3776" target="_blank">01:02:56.000</a></span> | <span class="t">in machine learning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3779" target="_blank">01:02:59.000</a></span> | <span class="t">And one very good question they ask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3781" target="_blank">01:03:01.000</a></span> | <span class="t">is, how much does longer context help compared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3786" target="_blank">01:03:06.000</a></span> | <span class="t">to just improved world knowledge when</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3789" target="_blank">01:03:09.000</a></span> | <span class="t">you are doing pretraining?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3791" target="_blank">01:03:11.000</a></span> | <span class="t">So for example, this GPT model is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3793" target="_blank">01:03:13.000</a></span> | <span class="t">able to process a longer context because it's a transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3798" target="_blank">01:03:18.000</a></span> | <span class="t">So is this the thing that makes it softer performance?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3801" target="_blank">01:03:21.000</a></span> | <span class="t">Or is it the fact that they train on a bigger data set</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3806" target="_blank">01:03:26.000</a></span> | <span class="t">with a longer training time, and it obtains world knowledge?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3810" target="_blank">01:03:30.000</a></span> | <span class="t">And I think both are important, I would say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3813" target="_blank">01:03:33.000</a></span> | <span class="t">But that's a very good question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3817" target="_blank">01:03:37.000</a></span> | <span class="t">So yeah, that was it in the GPT.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3819" target="_blank">01:03:39.000</a></span> | <span class="t">They introduce a framework for achieving SOTA performance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3822" target="_blank">01:03:42.000</a></span> | <span class="t">by doing two-step approach, do pretraining,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3824" target="_blank">01:03:44.000</a></span> | <span class="t">and then fine-tuning.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3825" target="_blank">01:03:45.000</a></span> | <span class="t">The goal of pretraining is to obtain a very good world</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3833" target="_blank">01:03:53.000</a></span> | <span class="t">knowledge and a very good starting point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3835" target="_blank">01:03:55.000</a></span> | <span class="t">And you do the pretraining on a diverse corpus</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3838" target="_blank">01:03:58.000</a></span> | <span class="t">with long stretches of text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3840" target="_blank">01:04:00.000</a></span> | <span class="t">So the model acquires, actually, world knowledge.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3842" target="_blank">01:04:02.000</a></span> | <span class="t">And then you actually do transfer learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3844" target="_blank">01:04:04.000</a></span> | <span class="t">by fine-tuning on tasks like question answering and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3848" target="_blank">01:04:08.000</a></span> | <span class="t">And the results is that they improve</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3851" target="_blank">01:04:11.000</a></span> | <span class="t">the state-of-the-art performance online data sets out of 12.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3855" target="_blank">01:04:15.000</a></span> | <span class="t">And they successfully have utilized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3858" target="_blank">01:04:18.000</a></span> | <span class="t">the unsupervised approach to do transfer learning</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3861" target="_blank">01:04:21.000</a></span> | <span class="t">to discriminative tasks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3863" target="_blank">01:04:23.000</a></span> | <span class="t">So now we have a clear way of how</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3864" target="_blank">01:04:24.000</a></span> | <span class="t">to do unsupervised training and semi-supervised training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3871" target="_blank">01:04:31.000</a></span> | <span class="t">And the work also highlights that transformers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3875" target="_blank">01:04:35.000</a></span> | <span class="t">is a very good architecture, and larger data sets are good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3879" target="_blank">01:04:39.000</a></span> | <span class="t">And this is a very important push</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3881" target="_blank">01:04:41.000</a></span> | <span class="t">in the direction of scaling up and pretraining.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3884" target="_blank">01:04:44.000</a></span> | <span class="t">And this is actually what people are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3886" target="_blank">01:04:46.000</a></span> | <span class="t">going to do for the next six years, from 2018 to 2024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3892" target="_blank">01:04:52.000</a></span> | <span class="t">Yeah, so very good paper, very good work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3896" target="_blank">01:04:56.000</a></span> | <span class="t">And with that, we've come to the end of this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=kAlD1Wx8lzw&t=3898" target="_blank">01:04:58.000</a></span> | <span class="t">So if you have any questions, I think we can take questions now.</span></div></div></body></html>