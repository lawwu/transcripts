<html><head><title>AI Engineer World’s Fair 2025 — GraphRAG</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>AI Engineer World’s Fair 2025 — GraphRAG</h2><a href="https://www.youtube.com/watch?v=RR5le0K4Wtw" target="_blank"><img src="https://i.ytimg.com/vi/RR5le0K4Wtw/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>- Quick question, everyone who attended the keynote this morning, we got a very good question from the organizer. Not a question, a statement. Is reg dead or is agents taking over reg? How many of you have implemented reg in production? Oh, plenty. So I can say with high confidence, reg is not dead.</p><p>My take on reg is, I think if reg can solve the problem that you're working on in production, you don't need agents, and vice versa. Why, wrong analogy, why build a network tower when you can get done with a smaller, minuscule version of it? So I think that is how I see reg being important.</p><p>And there are a lot of use cases where reg has found its application. So I'll stop there, we'll talk more in my presentation, but I'm waiting for 11:15 because this session is streamed live on YouTube for our friends who couldn't attend this conference. So when he gives me a heads up, I'll start.</p><p>Should we start? Okay. Oh, thank you. All right. So thanks for coming. To quickly introduce myself, my name is Mitesh. I lead the developer advocate team at NVIDIA. And the goal of my team is to create technical workflows, notebooks for different applications, and then we release that code base on GitHub.</p><p>So developers in general, which is me and you, all of us together, we can harness that knowledge and take it further for the application or use case that you're working on. So that is what my team does, including myself. In today's talk, I'm going to talk about this project that we did with one of our partners and some of my colleagues at NVIDIA and our partner about how can we create a GraphRack system, what are the advantages of it, and if we add the hybrid nature to it, how it is helpful.</p><p>So that's what my talk is going to be on. I will not be able to give you a 10-fit view where I can dive with you in the code base, but there is a GitHub link at the end of this talk which you can scan, and all these notebooks, whatever I'm going to talk about, is available for you to take home.</p><p>But I'll give you a 10,000-fit view if you are trying to build your own GraphRack system, how can you build it? So a quick refresher, what is knowledge graph, and why are they important? So it is a network that represents relationship between different entities, and those entities can be anything.</p><p>It can be people, places, concept events. A simple example would be me being here, what is my relationship to AI World Fair Conference, AI Engineers World Fair Conference, and my relationship is I'm a speaker at this conference. What is my relationship to anyone who's attending here? Well, our relationship is you attended my session.</p><p>So this edge of relationship between the two entities becomes very important to which only graph-based network can exploit, or knowledge graphs can exploit. And that is the reason why there's a lot of active research happening in this domain of how you can harness GraphRack, how you can harness knowledge graph and put it into a Rack-based system.</p><p>So the goal is three things, how can you create a triplet, which defines the relationship between these entities that GraphRack are a graph-based system or knowledge graph is really good at exploiting. And that's what is unique about this knowledge graph. So if you think about why can they work better than semantic Rack system, well, it captures the information between entities in much more detail.</p><p>So those connections can provide a very comprehensive view of the knowledge that you are creating in your Rack system, and that will become very important to exploit when you are retrieving some of that information and converting that into a response for the user who is asking that question. And it tells the ability to organize your data from multiple sources.</p><p>I mean, that's a given, no matter what kind of Rack system you're building. So how do we create a GraphRack or a hybrid system? So this is the high-level diagram of what it entails. So I broke it down into four components. The very first thing is your data, you need to process your data.</p><p>The better you process your data, the better is the knowledge graph. The better is the knowledge graph, the better is the retrieval. So four components: data, data processing, your graph creation, or your semantic embedding vector database creation. Those are the three steps, and then the last step is, of course, inferencing when you're asking questions to your Rack pipeline.</p><p>And at a higher level, this can be broken down into two big pieces: offline, online. So all your data processing work, which is a one-time process, is offline. And once you have created your knowledge graph, which is your triplet entity relationship, entity two, or your semantic vector database, once you have it, then it's all about querying it and converting that information into a response that is readable to the user.</p><p>It cannot be something that, here are the three relationships, and then we, as the user, have to go figure out what does this exactly mean. So the top part of this flow diagram is where you build your semantic vector database, which is you pick your documents, and then you convert them into vector embeddings, and you store them into a vector database.</p><p>So that piece is how you create your semantic vector database. And then the piece below is how you create your knowledge graph. And it is much more, there are much more steps that you have to follow, a care that you have to take when you're creating your knowledge graph.</p><p>So diving in the first step, creating your knowledge graph, how can you create those triplets out of documents that are not that structured? So creating triplets, which exposes the information between two entities, and picking up those entities so that that information becomes helpful is very important. Here's a simple example.</p><p>This document is of ExxonMobil's results, I think, their quarterly results. And we try to pick up the relationship or create the knowledge graph using an LLM. And if you see at the first line, it's ExxonMobil, which is a company, that's the entity. Cut is the feature of that entity, spending oil and gas exploration and activity.</p><p>My apologies. Cut is the relationship between ExxonMobil and spending on oil and gas exploration. And activity is the name of the entity, spending on oil and gas exploration. So this is how the relationship needs to be exploited. Now, the question that comes to our mind is that sounds very difficult to do.</p><p>And exactly, it is difficult to do. And that is the reason why we need to harness or we need to use LLMs to figure out a way to extract this information and structure it for us so that we can save it in a triplet format. And how can we do that?</p><p>Prompt engineering, but we need to be much more defined about it. So based on the use case that you are trying to work on, you can define your oncology. And once you have defined your oncology, you can put it in your prompt and then ask the LLM to go extract this information that is oncology specific from the documents and then structure it in that way so that that can be stored in the form of a triplet.</p><p>This step is very important. You might be spending a lot of time here to make sure your prompt is doing the right thing and it is creating the right oncology for you. If your oncology is not right, if your triplets are not right, if they are noisy, your retrieval will be noisy.</p><p>So this is where you will be going back and forth figuring out how to get a better oncology. So this is where you will spend, this is where you will spend 80% of your time to make sure you get the oncology right and you will be going back and forth in an intuitive manner to see how you can make it better over time.</p><p>And then the next vector database for a hybrid RAC system is to create the semantic vector database. And that is very reasonably straightforward or it is well studied. So you pick your document. This is the first page of attention is all you need research paper. And you break it into chunk sizes and you have another factor called overlap.</p><p>And chunk sizes are important because what semantic vector database does is it will pick up that chunk and convert that into, use the embedding model and convert that into a embedding vector and storing the vector database. And it will, if you don't have an overlap, then the context between the previous and the next chunk will be lost if there is any relationship.</p><p>So you try to be smart on how much overlap do I need between my previous chunk and the next chunk and what is the size of the chunk that I should use when I'm chunking my documents into different paragraphs. That is where the advantage of graph reg comes into play.</p><p>Because if you think about it, the important information, which is the relationship between different entities are not exploited by your semantic vector database, but they are exploited really well when you're trying to use a knowledge graph or create a knowledge graph based system. So once you have created this knowledge graph, what is the next step?</p><p>Now comes the retrieval piece, which is you ask a question. What is ExxonMobil's cut this quarter that it is looking like? And knowledge graph will help you figure out how to retrieve those nodes or those entities and the relationship between them. If you do a very flat retrieval, which is a single hop, you are missing the most important piece that graph allows you, which is exploitation through multiple nodes that you can think about.</p><p>That becomes very, very, very important. I cannot stress how important that becomes. So think of different strategies. Again, you would spend a lot of time to optimize this, whether you should look at single hop, double hop, how much deep you want to go so that nodes, the relationship between your first node to the second node, your second node to the third node is exploited pretty well.</p><p>And the more deeper you go, the better context you will get, but there's a disadvantage of that. The more deeper you go, the more time you're going to spend on retrieving that information. So then latency becomes a factor as well, especially when you're working in a production environment. So there is a sweet spot that you have to hit when you're trying to go, how deep you want to go, how many hops you want to go into your graph versus what is the latency that you can survive.</p><p>So that becomes very, very important. And some of those searches can be accelerated. So we created a library called cool graph, which is available or integrated in a lot of libraries out there, like network X and whatnot. But that acceleration becomes important so that it gives you the flexibility to get deeper into your graph, both through multiple hops, but at the same time, you can reduce the latency.</p><p>So your performance of your graph improves a lot. So this is where the retrieval piece comes into play, where you can have different strategies defined so that when you're querying your data and getting the responses, you can have better responses. And the other important piece, I personally worked on this piece, so I can talk at length on this, but I'm going to give you a very high level, is evaluating the performance.</p><p>And there are multiple factors that you can evaluate on faithfulness, answer relevancy, precision recall, if you try to use an LLM model, helpfulness, correctiveness, coherence, complexity, verbosity, all these factors becomes very important. So there's a library, so there's a library called RAGAS. It is meant to evaluate your RAG workflow end to end.</p><p>Anyone who used RAGAS for evaluating your graph? All right, a few of them, thank you. But it is an amazing library that you can use to evaluate your RAG pipeline end to end, because it evaluates the response, it evaluates the retrieval, and it evaluates what the query is. So it will evaluate your pipeline end to end, which becomes very handy when you're trying to test whether my retrieval is doing the right thing or whether the question that I'm asking is the LLM interpreting it in the right way or not.</p><p>So you can break down your responses in the RAGAS pipeline, we evaluate all those pieces and see what your eventual score is. So it is a pip install library. The other is LLM, and RAGAS under the hood uses an LLM, no surprises there. By default, it is integrated with GPT, but it provides you the flexibility that if you have your own model, you can bring it in as well, and you can wire it up with your API, and you can use that LLM to figure out on these four evaluation parameters that RAGAS offers.</p><p>So it's quite, I wouldn't say it's comprehensive, but it's really good in terms of giving you that flexibility. The other part is using a model that is meant to evaluate specifically the response coming out of the LLM. And that is where this model, Lama Nimotron 340 billion reward model that we released, I think, a few years ago.</p><p>At that time, it was a really good response model. It's a 340 billion parameter model, so reasonably big, but it evaluates, it's a reward model. So it will go and evaluate the response of another LLM and judge it in terms of how the responses are looking like on these five parameters.</p><p>But it is meant to go and judge other LLMs. That is how it was trained. So moving further, I would like to use this analogy that for, to create a graph right system, it will take you, which is 80% of the job, it will take you 20% of your time.</p><p>But then to make it better, which is the last 20%, sorry, which is the 80-20 rule, the last 20% will take 80% of your time. Because now you are in the process of optimizing it further to make sure it works for the use case good enough for the application that you're working on.</p><p>And there are some strategies there which I would like to walk you through. So one, as I said before, which I couldn't stress enough, the way you are creating your knowledge graph out of your unstructured data becomes very important. The better your knowledge graph, the better results you're going to get.</p><p>And something that we did as experimentation through this use case that we were exploring with one of our partners, was can we fine tune an LLM model to get the quality of the triplets that we are creating better? And does that improve results? Can we do a better job at data processing, like removing regex, apostrophes, brackets, words that, characters that don't matter?</p><p>If we remove them, does it give you better results? So these are like small things that you can think about, but it gives you, it improves the performance of your overall system. So that is where you, I'm talking about 80% of your time. Small nitty-gritty of the things that you are, the knobs that you are fine tuning slowly and steadily to make sure your performance gets better and better.</p><p>And I would like to share a few strategies that we did, which we got, which led us to, which led us to get better results. So the very first thing is Regex, or just cleaning out your data. We removed apostrophes, other characters that are not that important, if you think about triplet generation.</p><p>That led us to better results. We then implemented another strategy of reducing the, not missing out of longer output, making it smaller. That got us better results. And we also fine tuned the LAMA 3.3 model or 3.2 model, and that got us better results. So if you look at the last three columns, you'll see that by using LAMA 3.3 as is, we got 71% accuracy.</p><p>So this was tested on 100 triplets to see how it is performing. And as it, sorry, 100 documents. So as it got better, and as we introduced LARA, we fine tuned the LAMA 3.1 model, our accuracy or performance went up from 71% to 87%. And then we did those small tweaks, it improved the performance better.</p><p>Again, remember, there's 100 documents, so the accuracy is looking high, but if your document pool increases, that will come down a bit. But in comparison to where we were before, we saw improvement. And that is where the small tweaks come into play, which will be very, very, very helpful to you when you're putting a system, a graph reg or a reg system into production.</p><p>The other is from a latency standpoint. So if your graph gets bigger and bigger, now you're talking about a network which goes into millions or billions of parameters, or millions and billions of nodes. Now, how do you, how do you do search in those millions and billions, in the graph that has got millions or billions of nodes?</p><p>And that is where acceleration comes into play. So with Coograph, which is now available through NetworkX, so NetworkX is also a pip install library. Anyone who uses NetworkX here? All right, a few, okay. So NetworkX is also a pip install library. Under the hood, it uses acceleration. And if you see a few of the algorithms, we did a performance test on that.</p><p>And you can see the amount of latency in terms of overall execution and reducing drastically. So that is where you can, again, small tweaks, which will lead you to better results. So these are two things that we experimented, which led us to better results in terms of accuracy, as well as reducing the overall latency.</p><p>And these are small tweaks, and it leads us to better results. So then the question, obviously, is should I use graph, or should I use semantic base drag system, or should I use hybrid? And I'm going to give you the diplomatic answer. It depends. But there are a few things I would like you guys to take home, which will help you to come up to a decision.</p><p>So that you can make an educated guess that for this use case that I'm working on, a drag system would solve the problem, I don't need a graph, and vice versa. Or I need a hybrid approach. So it depends on two factors. One is your data. Traditionally, if you look at retail data, if you look at FSI data, if you look at employee database of companies, those have a really good structure defined.</p><p>So those kind of data set becomes really good use cases for graph-based system. And the other thing you think about is even if you have unstructured data, can you create a good graph, a knowledge graph out of it? If the answer is yes, then it's worthwhile experimenting to go the graph path.</p><p>And it will depend on the application and use case. So if your use case requires to understand the complex relationship and then extract that information for the questions that you are asking, only then it makes sense to use graph-based. Because remember, these are compute-heavy systems. So you need to make sure that these things are taken care of.</p><p>I am running out of time, I think. But as I said before, all these things that I talked about, I gave you a 10,000-feet view. If you want to get a 100-feet view where you are coding into things, all these things is available on GitHub. Even the fine-tuning of the LAMA 1.1 LoRa model.</p><p>And we had a workshop, a two-hour workshop. So I gave you a 20-minute talk, but this whole workshop is covered in two hours as well. And lastly, join our developer programs. We do release all these things on a regular basis. If you join the mailing list, you get this information based on your interest.</p><p>And as my colleague mentioned, I will be across the hall at Neo4j booth to answer questions if any. I would love to interact with you and see if you have any questions and I can answer those questions. Thank you for your time. Thank you. Thank you, Mattesh. That was fantastic.</p><p>We've got another great talk coming up here. Chen, you can come on up. And if I get this right, you're going to take a philosophical perspective on this. Yes, yes. Hello. Hello. Yeah, thanks. Five. Four. Three. Two. One. Hey, thanks. two one hey where's the note there oh um yeah okay so i don't know if there's a way to get speaker notes onto the screens at the bottom you guys know yes you have to go on full screen do you need the notes yeah i need the notes i see the mode you can also just walk through it on the side and just yeah sure like that can you collapse this side yeah yeah okay so hi hi everybody uh my name is chink young lamb um i'm a founder and CEO of patho.ai uh a bit of background by my company uh patho.ai started two years ago with an invitation from national science foundation from the spir grant funding investigating lm bird we did a lm bird driven discovery application uh since then we branch out to leverage what we learned about building ai system for large corporation we are currently building expert ai system for several clients currently the system we build goes beyond rack system um many of our clients is asking for ai system that perform tasks like a research and advisory role based on their area of interest uh today the talk is about sharing with our fellow ai engineer what we learned so far within this kind of system okay uh what is knowledge okay generally philosophically i say knowledge is the understanding and awareness gained through experience education and a comprehension of facts and principle and that lead to the next question is what is knowledge graph right so knowledge graph is a systematic method of preserving wisdom by connecting them and creating a network or interconnect relationship that's important the graph represents the thought process and comprehensive taxonomy of a specific domain of expertise that's why this is very important for people moving forward it's about ai system then think a lot and return advice instead of just retrieves you know data from your database right so that comes to the development of this uh k-a-g okay what is k-a-g k-a-g stands for knowledge augment generations and it's different from rack okay it is enhanced language model by integrating structure knowledge graph for more accurate and insightful response making it smarter more structure approach than a simple rack k-a-g doesn't just retrieve remember you understand this is different okay after interviewing a lot of my client okay so we also expert in a certain area of scale i found that there are common ways of their thinking decision making process the way that make them expert in their area knowledge graph seems to be a perfect repeat so here is the graph or state diagram if you are computer engineering grad like me so um it shows a wisdom the form the for the wisdom note as you can see is that it's a core right this wisdom it just isn't static it actively guide decision and views by other element the output from the wisdom actually goes to decision making in the blue right wisdom isn't passive it guide decision helping us choose wisely okay and then the decision making analyze the situation given in the circle in the circle in the green and decision on make you know in a vacuum okay they analyze real world situation that's the difference okay so look at the wisdom input okay look at the relationship feedback from the knowledge to wisdom in gold color example of that is knowledge to wisdom like all your book smart and encyclopedia wikipedia whatever you store plus once that data get absorbed by our whatever model you use up there it need to regurgitate that and understand that's why it's very important that wisdom is able to synthesize the data after you ingested knowledge you know that's the kind of abstract but i'll come to that later how about i'm talking about okay okay from inside example of that is wisdom derived pattern from chaos right some of my client has a lot of social media their product how do they you know track their product settlement from from social media right so it's very chaotic and from x tweet right so so from that you can see some pattern of their competitor versus uh current what my product is that that is like an example of that and i will go to that later okay when all these connected nodes matter together why do they matter all the nodes relate to one another to ever increase enriching our wisdom storing system okay this talk is about storing wisdom right so knowledge tells you what it is right and experience tell you what worked before inside invent what to try next right like a pizza knowledge is recipe experience is knowing your oven burnt crust inside is like hey it is adding you know honey to the crust you make caramelized perfectly right so the most important part of the knowledge graph is feedback loop okay feedback isn't one way street it learn from itself look at the feedback from the going back going back to all the note from inside to wisdom okay um situation informs future wisdom experience deepen it inside sharpen it like a tree growing roots the more effect the stronger it gets now now i want to ask you a question in general where do you see this circle in your life maybe a tough decision that you know taught you something so one practical application for leadership is wisdom avoid knee jack reaction by learning from feedback as for personal growth ever notice how past mistakes make you wiser that's the loop in the action all this so the takeaway from the slide in this is wisdom isn't a trophy you earn it is the muscle you exercise the more you feed knowledge experience insight the more they guide you now i will show you how it being mapped to my current client you know all this is like very abstract right so hi one of my clients actually doing a competitive analysis uh they used to have a marketing department doing that but they want ai to do that right they asked me to build the system this is exactly what i did with the same taxonomy of storing all this so this talk taxonomy will be later on i talk about how multi-agents going to handle all that here is one of the chatbot that i build for my client to do you know not just some we not just some cap okay it's our wisdom graph power ai designed to turn data in the strategy right dominant so what kind of question i talk about how do i win my competitor in this market space that's kind of a very sophisticated question right so without uh if you do simply just write my first speaker talk about right right so it's not going to cut it they're not going to able to answer that kind of question okay what i did is this uh we retained the same taxonomy and the wisdom is then mapped the same engine there the wisdom engine this engine is like an orchestration agent that does a lot of decision making including advising what the arms able to see based on the current situation what to do next right so um what i did is uh for the uh decision making i map it to a strategy generator so these customers are talking about a competitive analysis right so um i map the knowledge in term of knowledge what do they have they have market data right so i map this experience to hp is one of a kind past campaign so they have a lot of campaign doing a lot of marketing and then um the inside is actually mapped to uh in industrial insight so they have a database doing storing that and then of course the most important is is the situation the situation is how how am i doing how my product selling right so so that that is like a situation and then i map that to a a competitor witness that means they say if you make the lm aware of that you probably get a very good answer and then you know the chatbot will probably be doing the right thing advising so from here very high level you know state diagram or that how do i map it to a system that drive well here comes the trick so so anybody here heard of any all right all right all right so so so i i first encounter similar situation when i passed iot project which is not great developed by uh ibm right so it's the same kind of thing it's like no code but but underneath the hood there's a bunch of code okay it's all node.js code okay so uh but but for the for for proving your concept and all that it's very very very flexible and i right highly recommend that and and and here here you can take a look at the the workflow the break from i enable the implementation of this complicated state diagram with um what i say is there is a different community you know one of the very powerful node is the ai agent well previously nann is just workflow automation too i'm not selling for anything i'm just telling you i'm using it uh for prototyping uh further down the road maybe the client say us too like i i really need to you know go is now have the option to drive uh different model like open ai model and tropic model and even on-prem model and then that the key in making the stick the same machine work is that move out in a graph rag track why are we talking about fusion and decoder well i'm glad you asked because the next big breakthrough was knowledge graph with fusion and decoder so you can use knowledge graphs with fusion and decoder uh as a technique and this sort of improves upon the fusion and decoder paper by using knowledge graphs to understand the relationships between the retrieved passages and so it helps with this efficiency bottleneck and improves uh the process i'm not going to walk through this diagram step by step but this is the diagram in the paper of the architecture where it uses the graph and then does this kind of two-stage re-ranking of the passages and it helps with uh improving the efficiency while also lowering the cost and so the team took all this research and came to get came together to build their own implementation of fusion and decoder since we actually build our own models uh to make that kind of the final piece of the puzzle and it really helped our hallucination rate it really drove it down and then we published a white paper with our own findings of it and so then we kind of had that piece of the puzzle and there's a few other techniques that we don't have time to go over but point being we're assembling together multiple techniques based on research to get the best results we can for our customers so that's all well and good but like does it actually work like that's the important part right so we did some benchmarking last year we used amazon's robust qa data set and compared our retrieval system with knowledge graph and fusion decoder and everything uh with our with seven different vector search uh systems and we found that we had the the best accuracy and the fastest response time so encourage you to check that out and kind of check out this process benchmarks are really cool but what's even cooler is like what it unlocks for our customers which are various features in the product um for one because like most draft structures we can actually expose the thought process because we have that relationships and the additional context where you can show the snippets and the sub queries and the sources for how the rag system is actually getting the answers and we can expose this in the api to developers as well as in the product and then we're also to have able to have knowledge graphics sell at multi-hop questions where we can reason across multiple documents and multiple topics without any struggles and then lastly it can handle complex data formats where vector retrieval struggles where an answer might be split into multiple pages or maybe there's a similar term that doesn't quite match what the user is looking for but because we have that graph structure and and the and fusion and decoder with the additional context and relationships we're able to uh formulate these correct answers so again my main takeaway here is that there are many ways that you can get the benefits of knowledge graphs in rag that could be through a graph database it could be through doing something creative with posters it could be through a search engine uh but you can you take advantage of the relationships that you can build with knowledge graphs uh in your rag system and as you get there you can challenge your assumptions and focus on the customers to be able to get to the end result to to make the team successful and so for our team it was focusing on the customer needs instead of what was hyped staying flexible based on the expertise of the team and letting research challenge their assumptions um so if you want to join this amazing team we're hiring across research engineering and products uh we would love to talk to you about any of our open roles and i'm available for questions you can come find me in the hallway or reach out to me on twitter or linkedin and that's all i've got for you thank you so much you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you can you hear me now there i am lies on okay in the giant umbrella Can you hear me now?</p><p>There I am. Liza on. Okay. In the giant umbrella that is Graphrag, there are many techniques, many approaches, many ways that get things done, there's knowledge graph construction, there's retrieval, but then there's the notion of going post-rag and thinking about different ways of thinking about what knowledge is, what we're actually doing in the first place.</p><p>So next up is my good friend Daniel from Zapp to lead us through that. Daniel. Not yet? Oh, here we go. Five, four, three, two, one. Great. Well, welcome, everybody. Thank you, Andrea, Andreas, as well, for the intro. I'm Daniel, the founder of Zapp AI, and we build memory infrastructure for AI agents.</p><p>And I'm going to tell you that you're doing memory all wrong. Well, it may not be you directly, but it may be the framework that you're using to build your agents. I also think that knowledge graphs are awesome. Otherwise, why would we be here, right? And you should be using them for agent memory, not just for Graphrag.</p><p>So before I dive into expanding on my hot takes, I want to touch on why memory is so important. So we're routinely building agents that forget important context about our users. All that dynamic data that we're gathering from conversations between the agent and the user, all the data, business data from our applications, line of business applications, et cetera, there's so much richness about who the user is, and yet we're not enabling our agents with that data.</p><p>And our agents respond as a result generically or hallucinate even worse. And this definitely isn't the path to AGI, or more concretely, retaining our customers. So memory isn't about semantically similar content. RAG does that really well. And when I talk about RAG here, I'm primarily talking about vector database-based RAG, not necessarily GraphRAG.</p><p>But consider the stylized example where we have learned a brand preference for Adidas shoes. And unfortunately, Robbie's Adidas shoes fall apart, so he's rather unhappy. So the preference changes. However, Robbie's follow-up question to the agent, where he asked what sneakers he should purchase, is most similar to the first Adidas fact.</p><p>And so if we're using a vector database, that fact may be at the top of the search results, and the agent responds incorrectly. So when using RAG, each fact is an isolated and immutable piece of content. And this is a real problem. The three facts on the left exist with no understanding of causality.</p><p>Semantic search can't reason with the why things change over time. And this is why RAG approaches fail as memory. RAG lacks a native temporal and relational reasoning. And none of this should be a surprise under the hood. We're just working with similarity in an abstract space. There's no explicit relationships between these embeddings, these vector representations of the facts that we've generated for our memory.</p><p>However, when we look at knowledge graphs, we can define explicit relationships. Graphs can model the why, and at Zepp, we've got them to model the when as well, behind the preference change, which adds a temporal dimension that your agent can reason over. And this structural difference is fundamental to how memory should work.</p><p>So which is a good segue to Graffiti. Graffiti is Zepp's open source framework for building real-time, dynamic temporal graphs. And it addresses these exact problems. Graffiti is temporally aware and graph relational. You can find it on GitHub, go to git.new forward slash graffiti. It has 10,000 plus stars, almost 11,000 quadrupled within the last six weeks.</p><p>So thank you, everybody, who's dried out graffiti and loved it. So let's dive into how each of these attributes of graffiti works. So this is the secret source. Graffiti extracts and tracks multiple temporal dimensions for each fact. It identified when a fact is valid and becomes invalid. On the right-hand side, you can see how, when we're using the example that I illustrated a few slides back, how graffiti would pause those different time frames.</p><p>And this enables temporal reasoning. What did the user prefer in February? And it can answer questions that RAG simply cannot handle. Or it enables your agent to answer questions that RAG simply cannot handle. And so when we look at what RAG can do, we actually sit with a bunch of contradictory embeddings with no resolution in the vector database.</p><p>So if we're updating the brand preference, we'll have a new brand preference fact in the vector database. However, graffiti understands that broken shoes invalidate the love relationship, which creates a causal relationship between those three events in the previous slide. Broken shoes result in disappointment, which results in a brand preference change.</p><p>Graffiti doesn't delete the history of facts as they change, as they're invalidated, but marks them invalid, rather. And so we store a sequence of state changes on the graph, which allows your agent to then reason with those state changes over time. So for example, the next time I come back to the e-commerce agent to purchase shoes is not going to recommend the Adidas shoes to me.</p><p>And here's the resulting graph, a closer approximation to how humans might process and recall changing state over time. On the graph, we can see that the existing Adidas brand preference is still there, but it hasn't expired that date. We also see that there's a new brand preference for Puma shoes, which is valid, and it doesn't have an invalid that date.</p><p>So graffiti doesn't abandon embeddings. They're still incredibly useful. Graffiti uses semantic search and BM25 full-text retrieval to identify sub-graphs within the broader graffiti graph. And these can be traversed using graph traversal techniques to develop a richer understanding of memory. So we can find adjacent facts that might fill out the agent's understanding of memory.</p><p>And the results are then each fused together. And so this offers a very fast, accurate retrieval approach. And graffiti has a number of different search recipes built into it. So you can really explore how to take different approaches to retrieving data for your particular agent. So a little bit of a bonus.</p><p>When we look at recent changes that we've added to graffiti, we allow developers now to model their business domain on the graph. Because a mental health application will have very different types of things it needs to store and recall from memory to an e-commerce agent. And so graffiti allows you to build constructs, custom entities, and edges that represent the business objects within your particular business or application.</p><p>And so here we have an example of a media preference where we've been learning all about a user's preferred podcasts and music. And we have defined an actual structure here for media preference. And what this does is it allows us to then also retrieve, explicitly retrieve, media preferences from the graph, rather than a bunch of other noise that we might have added to memory.</p><p>And this ontology really enables you to bring a lot of depth to how memory operates. So I'm not advocating that you replace RAG everywhere. RAG, graph RAG, the various forms of graph RAG, and graffiti, each has its strengths and ideal use cases. The key is recognizing when you need each.</p><p>Graffiti is really strong when you're wanting to integrate incrementally dynamic data into a graph without significant recomputation. It's really strong when you want to model your business domain. It's strong where it has very low latency retrieval. There's no LLM in the path. If you've tried graph RAGs, they often have an LLM in the path incrementally summarizing the output from the graph.</p><p>It can take tens of seconds. Graffiti operates in under hundreds of milliseconds. And so the key is recognizing which solution offers to your business, what it offers to your business. And most agentic applications could use RAG, a RAG approach, and graffiti. So just summing it all up, agent memory is not about knowledge retrieval.</p><p>Temporal and relational reasoning is so critical to coherent memory. We need to track state changes over time. We need to understand how something like preferences or user traits might change over time. And that's something that contemporary RAG solutions lack. So we published a paper earlier this year describing ZEPP's use of graffiti.</p><p>And it's a deep dive into the graffiti architecture and how ZEPP performs as a consequence of using graffiti under the hood. So you can follow the link below to land at the archive preprint if you'd like to take a look. And I'm sure the slides will be available after the talk, so you can go to the paper.</p><p>So a quick plug for ZEPP. ZEPP goes beyond simple agent memory to build a unified customer record derived from both chat history and business data. So you can stream in user chat conversations, but also stream in business data from your SaaS application, from line of business applications like CRM or billing systems.</p><p>And it builds this unified, holistic view of the user, really enabling your agent to have an accurate and very comprehensive real-time understanding of the user so it can solve complex problems for that user. So stick around for the agent memory lunch and learn, which is the next session. It's being led by Mark Bain.</p><p>And in it, amongst other folks, I'll be demoing ZEPP's approach to domain-specific memory, built on graffiti's custom entities and edges. So thanks for listening to me. We have a few minutes, so I'm happy to answer questions. And I will, yeah, if there are any. No questions. Oh, there's one over there.</p><p>The question was, do you need to use ZEPP to use graffiti? No. Graffiti is open source. It's available on GitHub. You can go to the link, git.new, graffiti. And all you'll need today is Neo4j. So our partners, Neo4j, can assist you with a community edition install. And strongly recommend their desktop product.</p><p>It's wonderful. And you can get going very easily. Another question here? So underneath the hood, how do you invalidate graph edges? Are we using LLMs? So graffiti makes extensive use of LLMs to intelligently parse incoming data, which could be unstructured or structured. So the unstructured conversation, unstructured emails, structured data in JSON format, and fuse it together on the graph.</p><p>And as part of integrating, we're using LLMs to identify, in a pipeline, identify conflicting facts. And so that's where we get this ability to go from broken shoes to disappointment to a switch in brand preferences. So the LLMs is able to understand emotional valence of the events that it is seeing.</p><p>One more question? How do we handle invalidation of facts? Yeah, depending on the context. So the question was, how do we handle revalidation of facts if a state flips back to a prior state? And so it depends on the context. A new edge might be created that represents a success of fact.</p><p>Or the invalid update might be nullified. Yeah, that's a really good question. So why can graffiti do real-time updates but Microsoft GraphRag cannot? So micro GraphRag is, an oversimplification, is summarizing document chunks or documents at many different levels. And creating repeated summarizations at different levels. So a summary of the summaries of the summaries, et cetera.</p><p>And that's very computationally expensive. So if any of the underlying data changes, you're ending up with a cascading number of summarizations. It's expensive and complicated. Graffiti is designed to identify specific nodes and edges that are implicated in an update. And then is able to invalidate with a surgical precision the edges that are implicated in the conflict.</p><p>Or we just add new edges or nodes into the graph where it's relevant. And so we're able to use a variety of search pipelines as well as a number of different heuristics to really make very focused changes in the graph, which are lightweight and cheap. Here we go. How does it get for the normalized ontology?</p><p>I'm guessing that data is very dynamic. How do you know what that is? Yeah. That's a good question as well. So there are two ways that graffiti operates. The first, last question, the first one is that graffiti will build the ontology for you and will very carefully try to deduplicate edge and node types.</p><p>Secondly, as I mentioned a little bit earlier, we allow you to define an ontology using Pydantic, Zod, Go structs, et cetera. All right. I think we're at time. So thank you, everybody. And as Daniel mentioned, there's lunch being served outside right now, actually. So I encourage you all to go get a bite to eat.</p><p>At 1 o'clock, come back into the room. We're going to have a panel discussion about overall agentic memory, different implementations of it. And you're going to be part of that panel as well, right? Yes. So it'll be a great session. So come back at 1 o'clock for a longer session of day, gentle memory.</p><p>Hi. I have a question. Why don't I hop down? Yeah. Are there any limitations when it comes to the data set, graffiti can handle? Because I have... Thank you. I will come back about 10 minutes before 1. Okay. Yeah. Minutes. Yeah. There you go. Minutes. Minutes. And... And... ...</p><p>... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... All right, now we have a lunch break.</p><p>Yeah, okay, cool. What's going on in this room? That's why I love our room. She's overdoing that, and I'm overdoing that, and I'm overdoing that situation. I'm overdoing that situation. I'm overdoing that situation. I'm overdoing that situation. I'm overdoing that situation. I'm overdoing that situation. Thank you. Thank you.</p><p>Check, check, check, check, check, check, check, check, check, one, two. Okay. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Okay. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. You, thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. You can wear this on your belt. You can wear it on your belt. You can do whatever you want. You can put it in your pocket.</p><p>Could you help me? Yeah. I think pocket's the best. I usually put it in pockets. You gotta hide it. There's a camera on it. You'd like the cable to be invisible too. So, could you help me? Yeah, sure. Just tuck it in. Of course. There you go. Feel free.</p><p>You're welcome. All set? All right. You're welcome. Thanks. Can we do a test for all of us before we head to the stage? Sure. Thanks. Is the computer all set? Everything fine? Perfect. I think I need to mirror, right? Mirror the screens. Yeah. Okay. I'll do that. Okay. Okay.</p><p>Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. And the ethernet is already here. Okay. I think I need like five minutes if we have three minutes. Yes. That's the right amount. All right. Okay. All right. All right. All right. Can we do a test? He's going to do that?</p><p>I see. I'm video. I see. He's audio. I see. He said something about mirroring your screen up there, so we have to, like, set that up. Yeah. Yeah. You can practice that. You could actually tell that to Daniel and Vasilija just in case. Okay. So that there is no, I mean, there are always glitches, but less of them.</p><p>Yeah. Oh, can I do this test? You need a moment. You need a moment. Okay. Great. Great. Thank you. Okay. Okay. Let me straighten that out. Yeah. Come on back to the tent. Okay. All right. Okay. Okay. Underneath. Beautiful. All right. All right. Thank you. Thank you. All right.</p><p>Once you get up there, you're going to get there. Okay. cut that from five. Yep. Okay. Yeah. Okay. Okay. Can I do this test or? You need a moment. You need a moment. Okay. Okay. Nope. Hello. Hello. Hello. Hello. Hello. All right. Okay. Okay. Thank you. Let me straighten that out.</p><p>Come on back to the tent. All right. Okay. Underneath. Beautiful. All right. All right. Once you get up there, you're going to cut that from five. Yep. All right. Okay. Are you done? Nope. Hello. Hello. Hello. That's right. This is the musical section of the afternoon. Hi, everyone. Welcome to the graph rag track.</p><p>And we're having a lunch and learn. And I should remember myself, but I've told the other speakers to stay here in the middle of the stage. For the lunch and learn, we have the great treat that my good friend, Mark Bain, Mark, who's over there, Mark, who's going to be taking us through agentic memory, doing a kind of a broad sweep, like, you know, dive into agentic memory, but then we're also going to have a panel discussion around it and a couple of demos.</p><p>This is going to be a longer session than the rest of the graph rag track. It's about 45 minutes. Totally worth staying for the entire time. It should be amazing. Mark, are you ready to talk? Mark, are you ready to talk? Of course. My friend, Mark Bain, please. All right.</p><p>One, one, one. All right. How is everyone doing here? Woo-hoo! I'm super excited to be here with you. This is my first time speaking at AI Engineer. And we have an amazing group of speakers, guest speakers. Vasilija Markovic from Cogni. Vasilija. Oh, there is Vasilija. Daniel Chalev from Graffiti and Zepp AI.</p><p>And Alex Gilmore from Neo4j. The plan looks like this. I will do a very quick power talk about the topic that I'm super passionate. The AI memory. Next, we'll have four live demos. And we'll move on to some new solution that we are proposing. A GraphRack chat arena that I will be able to demonstrate.</p><p>And I would like you to follow along once it's being demonstrated. And at the very end, we'll have a very short Q&A session. There is a Slack channel that I would like you to join. So, please scan the QR code right now before we begin. And let's make sure that everyone has access to this material.</p><p>There is a walkthrough shirt on the channel that will go through closer to the end of our workshop. But I would like you to start setting it up, if you may, on your laptops if you want to follow along. All right. It's workshop-graphrackchat. You can also find it on Slack.</p><p>And you can join the channel. So, a little bit about myself. So, hi, everyone again. I'm Mark Bain. And I'm very passionate about the memory. What is memory? The deep physics. And applications of memory across different technologies. You can find me at Mark and Bain on social media or on my website.</p><p>And let me tell you a little bit of a story about myself. So, when I was 16 years old, I was very good at maths. And I did math olympiads with many brilliant minds, including Wojciech Zaremba, the co-founder of OpenAI. And thanks to that deep understanding of maths and physics, I did have many great opportunities to be exposed to the problem of AI memory.</p><p>So, first of all, I would like to recall two conversations that I had with Wojciech and Ilya in 2014 in September. When I came here to study at Stanford, at one party, we met with Ilya and Wojciech, who back then worked at Google. And they were kind of trying to pitch me that there will be a huge revolution in AI.</p><p>And I kind of like followed that. I was a little bit unimpressed back then. Right now, I probably kind of take it as a very big excitement when I look back to the times. And I was really wishing good luck to the guys who were doing deep learning. Because back then, I didn't really see this prospect of GPUs giving that huge edge in compute.</p><p>And so, during that conversation, it was like 20 minutes. At the very end, I asked Ilya, all right, so there is going to be a big AI revolution. But how will these AI systems communicate with each other? And the answer was very perplexing and kind of sets the stage to what's happening right now.</p><p>Ilya simply answered, I don't know. I think they will invent their own language. So, that was 11 years ago. Fast forward to now. The last two years, I've spent doing very deep research on physics of AI. And kind of like delve into all of these most modern AI architectures.</p><p>Including attention, diffusion models, VAEs, and many other ones. And I realized that there is something critical. Something missing. And this power talk is about this missing thing. So, over the last two years, I kind of followed on on my last years of doing a lot of research in physics, computer science, information science.</p><p>And I came to this conclusion that memory, AI memory, in fact, is any data in any format. And this is important. Including code, algorithms, and hardware, and any causal changes that affect them. That was something very mind-blowing to reach that conclusion. And that conclusion sets the tone to this whole track.</p><p>The GraphRack track. In fact, I was also perplexed by how biological systems use memory. And how different cosmological structures or quantum structures, they, in fact, have a memory. They kind of remember. And... Let's get back to maths and to physics. And geometry. When I was doing science olympiads, I was really focused on two, three things.</p><p>Geometry, trigonometry, and algebra. And I realized in the last year that more or less the volume of loss in physics perfectly matches the volume of loss in mathematics. And also the constants in mathematics. If you really think deeply through geometry, they match the constants both in mathematics and in physics.</p><p>And if you really think even deeper, they kind of like transcend over all the other disciplines. So that way we think a lot. And I found out that the principles that govern LLMs are the exact same principles that govern neuroscience. And they are the exact same principles that govern mathematics.</p><p>I studied... I studied papers of... Perlman. I don't know if you've heard who is Perlman. Perlman is this mathematician who refused to take a $1 million award for proving the... one of the most important conjectures. About symmetries of three spheres. And once I realized that this deep math of...</p><p>spheres and circles is very much linked with how attention and diffusion models work. Basically the formulas that Perlman reached are linking entropy with curvature. And curvature, basically if you think of curvature, it's attention. It's gravity. So in a sense there are multiple disciplines where the same things are appearing multiple times.</p><p>And I will be publishing a series of papers with some amazing supervisors who are co-authors of two of these methods, methodologies. The transformers and VAEs. And I came to this realization that this equation governs everything. Governs mass. Governs physics. Governs our AI memory. Governs neuroscience. Biology. Physics. Chemistry. And so on and so forth.</p><p>So... I came to this equation that memory times compute would like to be a squared imaginary unit circle. If that existed ever, we would have perfect symmetries and we would kind of not exist. because for us to exist, this asymmetries needs to show up. And in a sense, every single LLM through weights and biases, the weights are giving the structure.</p><p>The compute that comes and transforms the data in sort of the raw format, the compute turns it into weights. The weights are basically, if you take these billions of parameters, the weights are the sort of like matrix structure of how this data looks like when you really find relationships in the raw data.</p><p>So... All right. And then there are these biases, these tiny shifts that are kind of like trying to like in a robust way adapt to this model so that it doesn't break apart but still is... Still is very well reflecting the reality. So something is missing. So when we take weights and biases and we apply scaling laws and we keep adding more data, more compute, we kind of get a better and better and better understanding of the reality.</p><p>In a sense, if we had infinite data, we wouldn't have any biases. And this understanding is, again, the principle of this track, of GraphRack. The disappearance of biases is what we are looking for when we are scaling our models. So in a sense, the amount of memory and compute should be exactly the same.</p><p>It's just slightly expressed in a different way. But if there are some... There are any imbalances, then something important happens. And I came to another conclusion that our universe is basically a network database. It has a graph structure and it's a temporal structure. So it keeps on moving, following some certain principles and rules.</p><p>And these principles and rules are not necessarily fuzzy. They have to be fuzzy because otherwise everything would be completely predictable. But if it would be completely predictable, it means that me, myself, would know everything about every single of you, about myself from the past and myself from the future.</p><p>So in a sense, it's impossible. And that's why we have this sort of like heat diffusion entropy models. They allow us to exist. But something is preserved. Any single asymmetry that happens at the quantum level, any single tiny asymmetry that happens preserves causal links. And these causal links are the exact thing that I would like you to have as a takeaway from this workshop.</p><p>The difference between simple RAC, hybrid RAC, any types of RAC, and graph RAC, is that we are having the ability to keep these causal links in our memory systems. Basically, the relationships are what preserves causality. That's why we can solve hallucinations. That's why we can optimize hypothesis generation and testing.</p><p>So we will be able to do amazing research in biosciences, chemical sciences, just because of understanding that this causality is preserved within the relationships. And these relationships, when there are these asymmetries that are needed, they kind of create this curvature, I would say. So we intuitively feel every single of you is choosing some specific workshops and talks that you guys go to.</p><p>Right now, all of you are attending to the talk and workshop that we are giving. It means that it matters to you. And it means that potentially you see value. And this value, this information, is transcended through space and time. It's very subjective to you or any other object.</p><p>And I think we really need to understand this. So LLMs are basically these weights and biases or correlations. They give us this opportunity to be fuzzy. You know, actually one thing that I learned from Wojciech 10, 8, 11 years ago was that hallucinations are the exact necessary thing to be able to solve a problem where you have too little memory or too little compute for the combinatorial space of the problem you are solving.</p><p>So you're basically imagining. We are taking some hypothesis based on your history and you are kind of trying to project it into the future. But you have too little memory, too little compute to do that. So you can be as good as the amount of memory and compute you have.</p><p>So it means that the missing part is something that you kind of can curve thanks to all of these causal relationships and this fuzziness. And reasoning is reading of these asymmetries and the causal links. Hence, I really believe that agentic systems are sort of the next big thing right now because they are following the network database principle.</p><p>But to be causal, to recover this causality from our fuzziness, we need graph databases. We need causal relationships. And that's the major thing in this emerging trend of GraphRack that we are here to talk about. And I would like to, at this moment, invite on stage our three amazing guest speakers.</p><p>And I would like to start with Vasilje. Vasilje, please come over to the stage. Next will be Alex and Daniel. And I will present something myself. All right. So Vasilje will show us how to search and optimize memory based on certain use case at hand. All right. All right.</p><p>All right. All right. All right. All right. All right. All right. All right. All right. So let's just make sure this works. All right. All right. Let's just make sure this works. All right. All right. All right. All right. All right. All right. All right. All right. So let's just make sure this works.</p><p>Nice to meet you all. And I'm Vasilje. I'm originally from Montenegro, a small country in the Balkans. Beautiful. So if you want to go there, my cousins Igor and Miloš are going to welcome you. Everyone knows everyone. So, you know, if in case you're just curious about memory, I'm building a memory tool on top of the graph and vector databases.</p><p>My background's in business, big data engineering and clinical psychology. So a lot what Mark talked about kind of connects to that. I'm going to show you a small demo here. The demo is to do a Mexican standoff between two developers where we are analyzing their GitHub repositories. And these data from the GitHub depositories is in the graph.</p><p>And these Mexican standoff means that we will let the crew of agents go analyze, look at their data and try to compare them against each other and give us a result that should represent who should we hire, let's say, ideally out of these two people. So what we're seeing here currently is how Cognify works in the background.</p><p>So Cognify is working by adding some data, turning that into a semantic graph, and then we can search it with wide variety of options. We plugged in crew AI on top of it so we can pretty much do this on the fly. So here in the background I have a client running.</p><p>This client is connected to the system. So it's now currently searching the data sets and starting to build the graphs. So let's see. It takes a couple of seconds. But in the background we are effectively ingesting the GitHub data from the GitHub API, building the semantic structure, and then letting the agents actually search it and make decisions on top of it.</p><p>So every time with live demos things might go wrong. So I have a video version in case this does. Let's see. And I'll switch to the video. Oh, here we go. So the semantic graph starts generating. And as you can see we have activity log where the graph is being continuously updated on the fly.</p><p>Data is being stored in memory. And then data is being enriched and the agents are going and making decisions on top. So what you can see here on the side is effectively the agentic logic that is reading, writing, analyzing, and using all of this, let's say, preconfigured set of weights and benchmarks to analyze any person here.</p><p>So Cogni is a framework that's modular. You can build these tasks. You can ingest from any type of a data source. 30 plus data sources supported now. You can build any type of a custom graph. You can build graphs from relational databases, semi-structured data. And we also have this memory association layers inspired by the cognitive science approach.</p><p>And then effectively, as we kind of build and enrich this graph on the fly, we see that, you know, it's getting bigger. It's getting more popular. And then we're storing the data back into the graph. So this is the stateful temporal aspect of it. We kind of build the graph in a way that we can add the data back, that we can analyze these reports, that we can search them, and that we can let other agents access them on the fly.</p><p>The idea for us was let's have a place where agents can write and continuously add the data in. So I'll have a look at the graph now so we can inspect the bits. So if we click on any node, we can see the details about the commits, about the information from the developers, the PRs, whatever they did in the past, and which report they contributed to.</p><p>And then at the end, as the graph is pretty much filled, we will see the final report kind of starting to come in. So let's see how far we got with this. So it's taking, it's preparing now the final output for the hiring decision task. So let's have a look at that when it gets loaded.</p><p>We just finished this this morning. I hope to have a hosted version for you all today, but it didn't work. because AI is causing some trouble. So let's, we have to resolve this one. So let's see. Yes, so I will just show you the video with the end so we don't wait for it.</p><p>So here you can see that towards the end, we can see the graph. And we can see the final decision, which is a green node. And in the green node, we can see that we decided to hire Laszlo, our developer, who has a PhD in graphs. So it's not really difficult to make that call.</p><p>And we see why and we see the numbers and the benchmarks. So thank you. This has been very fast three minute demo. So hope you enjoyed. And if you have some questions, I'm here afterwards. We have, we are open source. So happy to see new users. And if you're interested, try it.</p><p>Thanks. Woohoo! Thank you. Thank you, Vasilija. Next up is Alex. So Vasilija showed us something I call semantic memory. So basically you take your data, you load it and cognify it, as they like to say. Come on, come on up, Alex. And that's the base. That's something we already are doing.</p><p>I think. And next up is Alex will show us Neo4j MCP server. The stage is yours. Okay. Yeah. I'm going to go ahead. Okay. Okay. So, hi everyone. My name's Alex. I'm an AI architect at Neo4j. I'm going to demo the memory MCP server that we have available. So there is this walkthrough document that I have.</p><p>We'll make this available in the Slack or by some means so that you can do this on your own. But it's pretty simple to set up. And what we're going to showcase today is really like the foundational functionality that we would like to see in a agentic memory sort of application.</p><p>Primarily we're going to take a look at semantic memory in this MCP server. But we are currently currently going to have a lot of time. We're going to look at semantic memory in this MCP server. But we are currently going to have a lot of time to see. I'm going to look at semantic memory in this MCP server.</p><p>And we're going to add additional memory types as well, which we'll discuss probably later on in the presentation. So, in order to do this, we will need a Neo4j database. Neo4j is a graph native database that we'll be using to store our knowledge graph that we're creating. They have a Aura option which is hosted in the cloud.</p><p>Or we can just do this locally with the Neo4j desktop app. Additionally, we're going to do this via cloud desktop. And so, we just need to download that. And then we can just add this config to the MCP configuration file in Cloud. And this will just connect to the Neo4j instance that you create.</p><p>And what's happening here is we're going to, Cloud will pull down the memory server from PyPy. And it will host it in the back end for us. And then it will be able to use the tools that are accessible via the MCP server. And the final thing that we're going to do before we can actually have the conversation is we're just going to use this brief system prompt.</p><p>And what this does is just ensure that we are properly recalling and then logging memories after each interaction that we have. So, with that, we can take a look at a conversation that I had in Qua desktop using this memory server. And so, this is a conversation about starting an agentic AI memory company.</p><p>And so, we can see all these tool calls here. And so, initially, we have nothing in our memory store, which is as expected. But as we kind of progress through this conversation, we can see that at each interaction, it tries to recall memories that are related to the user prompt.</p><p>And then, at the end of this interaction, it will create new entities in our knowledge graph and relationships. And so, in this case, an entity is going to have a name, a type, and then a list of observations. And these are just facts that we know about this entity.</p><p>And this is what is going to be updated as we learn more. In terms of the relationships, these are just identifying how these entities relate to one another. And this is really the core piece of why using a graph database as sort of the context layer here is so important.</p><p>Because we can identify how these entities are actually related to each other. It provides a very rich context. And so, as this goes on, we can see that we have quite a few interactions. We are adding observations, creating more entities. And at the very end here, we can see we have quite a lengthy conversation.</p><p>We can say, you know, let's review what we have so far. And so, we can read the entire knowledge graph back as context. And Claude can then summarize that for us. And so, we have all the entities we've found, all the relationships that we've identified, and all the facts that we know about these entities based on our conversation.</p><p>And so, this provides a nice review of what we've discussed about this company and our ideas about how to create it. Now, we can also go into Neo4j browser. This is available both in Aura and local. And we can actually visualize this knowledge graph. And we can see that we discussed Neo4j.</p><p>We discussed MCP and line graph. And if we click on one of these nodes, we can see that there is a list of observations that we have. And this is all the information that we've tracked throughout that conversation. And so, it's important to know that, like, even though this knowledge graph was created with a single conversation, we can also take this and use it in additional conversations.</p><p>We can use this knowledge graph with other clients such as cursor IDE or Windsurf. And so, this is really a powerful way to create a, like, memory layer for all of your applications. And so, with that, I'll pass it on. Thank you. All right. Give a round of applause to Alex.</p><p>Thank you, Alex. The next up is Daniel. I'll just assure personal beliefs about MCPs. I was testing MCPs of Neo4j, Graffiti, Cogni, Mem0, just before the workshop. And I'm a strong believer that this is our future. We'll have to work on that. And in a second, I will be showing a mini GraphRack chat arena.</p><p>And next up, something very, very important that Daniel does is temporal graphs. Daniel is co-founder of Graffiti and Zepp. They have 10,000 stars on GitHub and growing very fast. The stage is here, Daniel. Please show us what you do. Thank you. So, five, four, three, two, one. Did that work?</p><p>It seems to have, right? So, I'm here today to tell you that there's no one-size-fits-all memory. And why you need to model your memory after your business domain. So, if you saw me a little bit earlier, and I was talking about Graffiti, Zepp's open-source temporal graph framework, you might have seen me just speak to how you can build custom entities and edges in the graffiti graph for your particular business domain.</p><p>So, business objects from your business domain. So, business objects from your business domain. What I'm going to demo today is actually how Zepp implements that and how easy it is to use from Python, TypeScript, or Go. And what we've done here is we've solved the fundamental problem plaguing memory.</p><p>And we're enabling developers to build out memory that is far more cogent and capable for many different use cases. So, I'm going to just show you a quick example of where things go really wrong. So, many of you might have used ChatGPT before. It generates facts about you in memory.</p><p>And you might have noticed that it really struggles with relevance. Sometimes it just pulls out all sorts of arbitrary facts about you. And, unfortunately, when you store arbitrary facts and retrieve them as memory, you get inaccurate responses or hallucinations. And the same problem happens when you're building your own agents.</p><p>So, here we go. We have an example media assistant. And it should remember things about jazz music, NPR, podcasts, the daily, et cetera. All the things that I like to listen to. But, unfortunately, because I'm in conversation with the agent or it's picking up my voice when I'm, you know, it's a voice agent.</p><p>It's learning all sorts of irrelevant things. Like, I wake up at 7:00 a.m., my dog's name is Melody, et cetera. And the point here is that irrelevant facts pollute memory. They're not specific to the media player business domain. And so, the technical reality here is, as well, that many frameworks take this really simplistic approach to generating facts.</p><p>If you're using a framework that has memory capabilities, agent framework, it's generating facts and throwing it into a vector database. And, unfortunately, the facts dumped into the vector database or Redis mean that when you're recalling that memory, it's difficult to differentiate what should be returned. We're going to return what is semantically similar.</p><p>And here we have a bunch of facts that are semantically similar to my request for my favorite tunes. We have some good things and, unfortunately, Melody is there as well because Melody is a dog named Melody. And that might be something to do with tunes. And so, a bunch of irrelevant stuff.</p><p>So, basically, semantic similarity is not business relevance. And this is not unexpected. I was speaking a little bit earlier about how vectors are just basically projections into an embedding space. There's no causal or relational relations between them. And so, we need a solution. We need domain aware memory, not better semantic search.</p><p>So, with that, I am going to, unfortunately, be showing you a video because the Wi-Fi has been absolutely terrible. And let me bring up the video. Okay. So, I built a little application here. And it is a finance coach. And I've told it I want to buy a house.</p><p>And it's asking me, well, how much do I earn a year? It's asking me about what student loan debt I might have. And we'll see that on the right-hand side, what is stored in Zepp's memory are some very explicit business objects. We have financial goals, debts, income sources, et cetera.</p><p>These are defined by the developer. And they're defined in a way which is really simple to understand. We can use Pydantic or Zod or Go structs. And we can apply business rules. So, let's go take a look at some of the code here. We have a TypeScript financial goal schema using Zepp's underlying SDK.</p><p>We can define these entity types. We can give a description to the entity type. We can even define fields, the business rules for those fields and the values that they take on. And then we can build tools for our agent to retrieve a financial snapshot which runs multiple Zepp searches at the same time concurrently and filters by specific node types.</p><p>And when we start our Zepp application, what we're going to do is we're going to register these particular goals, sorry, objects with Zepp. So, it knows to build this ontology in the graph. So, let's do a quick little addition here. I'm going to say that I have $5,000 a month rent.</p><p>I think it's rent. And in a few seconds, we see that Zepp's already parsed that new message and has captured that $5,000. So, we can go look at the chart, the graph. This is the Zepp front end. And we can see the knowledge graph for this user has got a debt account entity.</p><p>It's got fields on it that we've defined as a developer. And so, again, we can really get really tight about what we retrieve from Zepp by filtering. Okay. So, we're at time. So, just very quickly, we wrote a paper about how all of this works. You can get to it by that link below and appreciate your time today.</p><p>You can look me up afterwards. Great paper, really. Thank you. All right. So, once I'm getting ready, I would appreciate if you confirm with me whether you have access to Slack. Is the Slack working for you? The Slack channel? All right. I think we are slowly running out of time.</p><p>So, I'd appreciate if you have any questions to any of the speakers. Please write these questions on Slack. And we will be outside of this room. And we are happy to answer more of these questions just after the workshop. I, right now, move on with a use case that I developed and to this GraphRack chat arena.</p><p>To be specific, before delving into agentic memory, into knowledge graphs, I led a private cyber security lab. And worked for defense clients. A very big client with very serious problems on the security side. And I used to, in one project, I had to navigate between something like a 27, 29 different terminals And I realized that LLMs are not only amazing to translate these languages, but they are also very good to kind of create a new type of shell.</p><p>A human language shell. A human language shell. There are such shells. But such shells, they would really be excellent if they have episodes. episodic memory. The sort of temporal memory. The sort of temporal memory. Of what we were doing. Of what we were doing. The sort of temporal memory.</p><p>Of what we were doing. Of what we were doing. Of what we were doing. We were doing. So, what we were doing. Of what we were doing. We were doing. Of what we were doing. Of what we were doing. of create a new type of shell, a human language shell.</p><p>There are such shells. But such shells, they would really be excellent if they have episodic memory, the sort of temporal memory of what was happening in this shell historically. And if we have access to this temporal history, the events, we kind of know what the users were doing, what their behaviors are.</p><p>We kind of can control every single code execution function that's running, including the ones of agents. So I spotted with some investors and advisors of mine, I spotted a niche, something we call agentic firewall. And I wanted to do a super quick demo of how it would work. So basically you would run commands and type pwd, and in a sense, I suppose lots of us had computer science classes or we worked in shell.</p><p>And we have to remember all of these commands. Like, show me running Docker containers. Like, it's Docker PS, right? But if you go for more advanced commands... I think it's for a reason, yeah. I think it's for a reason. Oh, let me see, one second. Sorry about that. All right, it's there.</p><p>Okay. Thank you. In general, I would need to know right now some command that can extract me, for instance, the name of the container that's running and its status. Show me just image and status. I can make mistakes, like human language, fuzzy mistakes. Show if Apache is running. All right, show the command we did three commands ago.</p><p>So basically if you plug in the agentic memory to things like that, I think it got it wrong, but you get me right. So if I get through, like, different shells and terminals, and I have this textual context of what was done, and the context of the certain machine of what is happening here, and it kind of spans across all the machines, all the users, and all the sessions in PTYs, TTYs, I think that we can really have a very good context also for security.</p><p>So that space, the temporal logs, the episodic logs, is something that I see will boom and emerge. So I believe that all of our agents that will be executing codes in terminals will be executing it through, maybe not all, but the ones that are running on the enterprise gate.</p><p>They will be going through agentic firewalls. I'm close to sure about that. So that's my use case. And now let's move on to GraphRack chat arena. So you have on Slack a link to this doc. And this doc is allowing you to set up a repo that we've created for this workshop.</p><p>And we'll be promoting it afterwards. So about a year ago, I met with Jerry Liu from LamaLindex, and we were chatting quite a while about how to evolve this conversational memory. And he gave me two pieces of advice. One of them, think about data abstractions. The other, think about evals.</p><p>Data abstractions, I kind of quickly solved within, like, two months. Evals, I realized that there wouldn't be any evals in form of a benchmark. This, all of these hot potatoes and all of that, it's fun. I know that there are great papers written by our guest speakers and other folks about hot potatoes.</p><p>But it's not the thing. You can't do a benchmark for a thing that doesn't exist. Basically, the agentic GraphRack memory will be this type of memory that evolves. So you don't know what will evolve. So if you don't know what will evolve, you will need a simulation arena. And that will be the only right eval.</p><p>So one year, fast forward, and we've created a prototype of such agentic memory arena. Think about it like web arena, but for memory. And let me quickly show you that. You can go to this repository. I did a fork of that. There is Memzero. There is Graffiti. There is Cogni.</p><p>And there will be two approaches. One approach will be sort of the repo, the library itself, and the other is through MCPs. Because we don't really know what will work out better. So whether repos or the MCPs will work out better. So we need to test these different approaches.</p><p>We need to create this arena for that. So we basically cloned that repo. And we use ADK for that. So we get this nice chat where you can talk to these agents. And you can switch between agents. So I want to talk with Neo. And there is a Neo4j agent running behind the scenes.</p><p>There is a Cypher graph agent running behind the scenes. And I can kind of, for now, switch between these agents. Maybe I'll increase the font size a little bit. So the Neo agents basically answering the questions about this amazing technology, the graphs, specifically Neo4j. And I can switch to Cypher.</p><p>And then an agent that is excellent at running Cypher queries talks with me. And I'm writing add to graph that mark. And I'm passionate about memory architectures. And basically, what it does is it runs these layers that are created by Cogni, by Memzero, by Graffiti, and all the other vendors of semantic and temporal memory solutions.</p><p>Or, specifically, created by an MCP server that Alex was demonstrating, the Neo4j MCP server. So I'm really looking forward to how this technology evolves. But what I really -- what I quickly wanted to show you is that it already works. It has this science of being this agentic memory arena.</p><p>So I can ask my graph through questions, and the agent goes to the connection. This is just one -- you know what's amazing? It's just one Neo4j graph. It's just one Neo4j graph on the backend, and all of these technologies that can be tested. How the graphs are being created and retrieved.</p><p>It's like -- when I think of that, it's like the most brilliant idea that we can do with agentic memory simulations. So I get answers from the graph. Here is the graph. I can basically rerun the commands to see what's happening on this graph. And let me just move on.</p><p>And next thing is I would like to add to the graph that Vasilio will show how to integrate Cogni and ta-da-da-da, so I add new information. And the cipher writes it to the graph. And then I want to do something else. It's super early stage still. But then I transfer it to graffiti, and I can repeat the exact same process.</p><p>So I can right now, using graffiti, search what I just added. And I can switch between these different memory solutions. So that's why I'm so excited about that. And we do not have time to, like, practice it together, do the workshop, but I'm sure we'll write some articles. So please follow us.</p><p>And I would appreciate, if you have any questions, pass them on to Slack. I will ask Andreas whether we have time for a short Q&A or do we need to move it to, like, breakout or outside of the room. It could take, like, five minutes. Five minutes. All right.</p><p>So that's all for now for today. I really would like Vasilya, Daniel, and Alex to come back to stage so you can ask any of us. Please direct the questions to any of us. And we'll try to answer them. Yeah. Let's go. Hi. I'm Lucas. I want to ask a fundamental question.</p><p>How do you decide what is a bad memory over time? Because you could, like, as a developer and as a person, we evolve the line of thought, right? So one thing that you thought was good, like, three years, ten years ago may not be good right today. So how do you decide?</p><p>Sure. A very good question. So I will answer in -- maybe you guys can help. I will answer in a very scientific way. So basically the one that causes a lot of noise. The noisy one doesn't make a lot of sense. So you decrease noise by redundancy and by relationships.</p><p>So the less relationships and the more noisiness, the -- so in a sense, a not well-connected node has the potential of not being correct. But there are other ways to validate that. Would you like to follow on? Yeah. Sure. A practical way. We will let you model the data with Pydantix so you can kind of load the data you need and add weights to the edges and nodes so you can do something like temporal weighting.</p><p>You can add your custom logic and then effectively you would know how your data is kind of evolving in time and then how it's becoming less or more relevant and what is the set of algorithms you would need to apply. So this is the idea. Not solve it for you, but let's help you solve the tooling.</p><p>But, yeah, there is -- depends on the use case, I would say. Yeah. I have nothing to add. I think that's a great explanation. I think what I would add is that there is missing causal links. Missing causal links is what is most probably a good indicator of fuzzy nice.</p><p>Yeah. Next question. Can you hear me? How would you embed in the security or privacy into the network or the application layer? If there's a corporate, they have top secret data, or I have personal data that is a graph. I want to share that, but not all of it.</p><p>Oh, that's a really good one. I think I'll answer that very briefly. So basically, you do have to have that context. You do have to have these decisions, intentions of colonels, of majors, and anyone like in the enterprise -- like CISOs and anyone in the enterprise stack. And in a sense, it also gets kind of like fuzzy and complex, so I expect this to be a very big challenge.</p><p>That's why I want to work on that. But I'm sure that applying ontologies, the right ontologies, first of all, to this enterprise cyber security stack and really kind of provides these guardrails for navigating this challenging problem and decreasing this fuzziness and errors. Thank you. Yeah. I would also just add, like, all these applications are built on Neo4j.</p><p>And so in Neo4j, you can, like, do role-based access controls, and so you can prevent users from accessing data that they're not allowed to see. So it's something that you can configure with that. And one more thing. Hi. This question is for Mark. Yeah. Yeah. Go on. Go on.</p><p>Go on. You were about to say something? Please go ahead first. Yeah. Just one thing. Kind of keep it, like, very physically separate. For us, it really works well. People react to that really well. So that's one way. Yes. Independent graphs. Personal graphs. Yeah. Mark, in your earlier presentation, you mentioned this equation related gravity, entropy and something, and also memory and compute.</p><p>Yes. Could you show those two again and explain them again? Of course. Yeah. If we have time. Other than that, it's probably for a series of papers to properly explain that. So that's one. Memory times compute. Equal size square. The other one is that if you take all the attention, diffusion, and VAs which are doing the smoothing, it preserves the sort of asymmetries.</p><p>So very briefly speaking, let's set up the vocabulary. So first of all, curvature equals attention equals gravity. This is the very simple, most important principle here. I will need to, when writing these papers, we are really tightly trying to define these three. Next. Inclusion, heat, entropy. It's the exact same thing.</p><p>We just need to align definitions. And if it's not the exact same thing, if there are other definitions, we need to show what's really different. And now, if you think about attention, it kind of shows the sort of like pathways towards certain asymmetries. If you take a sphere, if you start bending that sphere and make it like, you know, like you kind of try to extend it, two things happen.</p><p>Entropy increases and curvature increases, in a sense. And Perelman, what he did, he proved that you can like bend these spheres in any way, 3D spheres, 4D and 5D and higher level spheres were already solved. So he solved for 3D sphere. And these equations are proving that basically there won't be any other architectures for LLMs.</p><p>It will be just attention diffusion models and VAs. Maybe not just VAs, but like kind of like something that smooths, leaves room for biases. All right. Thank you all. I really appreciate you coming. I hope it was helpful. Thank you, the guest speakers. And we'll answer the questions outside of the room.</p><p>I appreciate that. Thank you. We've got about maybe a 10-minute break before the next speaker is up. But we've got a bit of setup to do. So this is a great time to grab a coffee. Michael is going to be talking to us next. A practical graph rack, right?</p><p>Yep. Yeah. Hi, if you are staying for the next session, I believe you have to go out and get your badge scanned because that's how they keep track of how many people are at each session. So that was the directive. Thank you. Thank you. Thank you. We'll hack later.</p><p>Thanks. Thanks. Thank you. Thank you. Yeah. Thank you, everyone. We're closing out this room for a turnover. I'll be around. Appreciate you. You have a very nice presentation. I like your way of presenting from another perspective. Please go out and get your battery scanned if you're going to stay in the room.</p><p>Let's head out. Thank you. Yeah. They need to get ready. So I will be there. Let's talk. I will be there in 30 seconds. Don't worry. in these directions. Am I a robot? Of course. Let's go out. Yes, we will. Go on. Love the physics connection you built there.</p><p>Can I, do you write blogs about this? Can I discover some of your content? I will be writing such like deep science, like theoretical physics papers. First theoretical physics, then I mean, I have drafts that are being like reviewed. It's, it's, it's, it's, it's really like, one second. So it's really challenging to kind of question general relativity and it's like built on Perlman and all of these like quantum physics.</p><p>It's just like, I, I only feel comfortable doing that when I have very good supervisor. So it takes time. I basically, so the way it was, is I was starting with like Cogni and graffiti and Memzero. We were like kind of like building things, but I was like, hi, I want to get into science.</p><p>So follow me on LinkedIn and on the website. Okay. I can probably write some posts about it briefly. But what we are trying to do is like do this deep theoretical physics papers first. And after that, so, so papers like one, two, three will be about that. Papers like three, five will be about relating that to transformers, diffusion models, heat transfer, and all of these other things.</p><p>And in a sense, I, I, I feel like doing popular science, someone will take care of that. So we are trying to do like real research. It's more campaign. It's more campaign.com. I can show it to you. One second. Um, could take a picture. Of course. Yeah. Of course.</p><p>Can you, can you add my, one second. Can you take it? Yeah. Thanks. Yeah. Can you end mic? One second. Can you take it? Yeah. Thanks, Mike. Yeah. Yeah. Yeah, we'll take care of it. Okay. Did you get mic two back? He's ready for your... Oh, okay. Cool. Okay, so, he's gonna be on one, and he's on two.</p><p>Okay, that's good. Okay, go ahead and do the count back from five. Hello, hello? Say five, four, three, two, one. Michael! Count back from five, please. A little slower. All good? Give me a countdown back from ten? Ten, nine, eight, seven, five, six, five, four, three, two, one. All right, you're good.</p><p>All right, you're good. Thanks, Mike. So, go ahead and take this down. Oh, I'm sorry. How's that? This part, I'll fix this one. because we have to have to raise a certain way. so it doesn't sound off my phone. Sure. Okay, count back from five for me. Five, four, three, two, one, zero.</p><p>You're good. Is that okay? Yeah, yeah, that's fine. Is that okay? Yeah, that's fine. Is that okay? Yeah, that's fine. Is that okay? Yeah, that's fine. Is that okay? Yeah, that's fine. Okay. Okay, can I add some five for me? Five, four, three, two, one, zero. Okay, you know best.</p><p>All right, thank you very much. Thank you. Thank you. At the beginning you have all these slides about like the papers. Do we want to skip some of them? Because I had this like summary slide that had all the search stuff on one slide. How do you want to do it?</p><p>Because it's like six slides. Yes. I mean, I'm just going to click through them. Okay. So I'll go here, right? So from here, switch. She said we need to stay at the podium. So you have to step back and then I go here because of the live streaming. Okay.</p><p>Okay, so I'll just intro. I think we should be up here together for the intro. Yeah. But then if you don't want to stand here and watch me, then you can sit down. Yeah. Do you have a clicker? No, I forgot mine at home. Do you have one? No, I got one.</p><p>Yeah, I got one. I mean, if the... Yeah, that's true. Yeah, that's true. Oh, I think. Just one USB stick. One. Oh, USB A. We're always ready. Good. Oh, yeah, Oregon. That's good. Hello, everyone. Hope you had some good coffee. Please come in. We are talking about graph rec today.</p><p>That's the graph rec trick. The graph rec trick, of course. And we want to look at patterns for successful graphic applications for making LLMs a little bit smarter by putting knowledge graph into the picture. My name is Michael Hunger. I'm VP of product innovation at Neo4j. My name is Steven Shin.</p><p>I lead the developer relations at Neo4j. And actually, we're both co-authoring. This is fun because we're both already authors and finally we've been friends for years and we finally get to co-author a book. We're also authoring graph rec, the definitive guide for O'Reilly. So basically, we didn't sleep this past weekend because we had a book deadline.</p><p>Yep. So I'm going to talk a little bit about kind of how to high level what graph rec is, why it's important, what we're seeing in the media. And then Michael's going to drill down into all of the details and patterns and give you a bunch of takeaways and things you can do.</p><p>This is probably, if you want to know how to do graph rec, Michael's quick deep dive on this is the best introduction you can get. So I'm also excited. Awesome. That's good going. Okay. So the case for graph reg is where we're going to start. And the challenge with using LLMs and using other patterns for this is basically they don't have the enterprise domain knowledge.</p><p>They don't verify or explain the answers. They're subject to hallucinations. And they've ethical and data bias concerns. And you can see that very much like our friendly parrot here, they are all the things which parrots behave and act like, except a cute bird. So we want to do better than this with graph reg and figure out how we can use domain specific knowledge, accurate, contextual and explainable answers.</p><p>And really, I think like what a lot of companies and what the industry is figuring out is it's really a data problem. You need good data. You need good data. You need to have data you can power your system with. One of the patterns you can do this with is RAG.</p><p>So you can stick your external data into a RAG system. You can get stuff back from a database for the pattern. But vector databases and RAG fall short because it's lacking kind of your full data set. It's only pulling back a fraction of the information by vector simulating algorithms.</p><p>Typically, a lot of the especially modern vector databases, which everyone's using, they're easy to start with, but they're not robust. They're not mature. They're not something which has scalability and fallback and gives you that what you need to get into build a strong, robust enterprise system. And vector similarity is not the same as relevance.</p><p>So results you get back from using a basic RAG system. They give you back things which are related to the topic, but it's not complete. And it's typically also not very relevant. And then it's very hard to explain what's coming out of the system. So we need to answer.</p><p>Lifeline. Graphrag. And what Graphrag is, is we're bringing the knowledge and the context and the environment to what LMs are good at. So you can think of this kind of like the human brain. Our left brain is, our right brain is more creative. It does more like building things.</p><p>It does more extrapolation of information. Whereas our left brain is the logical part. That's what actually has reasoning, has facts, and can enrich data. And it's built off of knowledge graphs. So a knowledge graph is a collection of nodes, relationships, and properties. Here's a really simple example of a knowledge graph where you have two people.</p><p>They live together. You have a car. But when you look into the details, it's actually like a little bit more complex than it seems at first. Because they both have a car, but the owner of the car is not the person who drives it. This is kind of like my family.</p><p>My wife does all the bills, but then she hands me the keys whenever we get on the freeway. She hates driving. So knowledge graphs also are a great way of getting really rich data. Here's an example of the Stack Overflow graph built into a knowledge graph where you can see all of the rich metadata and the complexity of the results.</p><p>And we can use this to evolve RAG into a more complex system, basically graph RAG, where we get better relevancy. We're getting more relevant results. We get more context because now we can actually pull back all of the related information by graph closeness algorithms. We can explain what's going on because it's no longer just vectors.</p><p>It's no longer statistical probabilities coming out of a vector database. We actually have nodes. We have structure. We have semantics we can look at. And we can add in security and role-based access on top of this. So it's context-rich. It's grounded. This gives us a lot of power. And it gives us the ability to start explaining what we're doing, where now we can visualize it, we can analyze it, and we can log all of this.</p><p>Now, this is one of the initial papers, the graph RAG paper from Microsoft Research, where they went through this and they showed that you could actually get not only better results, but less token costs. It was actually less expensive to do a graph RAG algorithm. There have been a lot of papers since then which show all of the different research and interesting work which is going on in the graph RAG area.</p><p>And this is just a quick view of the different studies and results which are coming out. But even from the early Data.World study where they showed a three times improvement in graph RAG capabilities. And the analysts are even showing how graph RAG is trending up. So this is the Gartner kind of hype cycle from 2024.</p><p>And you can see generic AI is kind of, you know, on the downtrends. RAG is getting over the hump. But graph RAG and a bunch of these things actually are providing and breathing more life into the AI ecosystem. So a lot of great reports from Gartner showing that it's grounded in facts.</p><p>It resolves hallucinations. Together knowledge graphs and AI are solving these problems. And it's getting a lot of adoption by different industry leaders. By big organizations who are taking advantage of this and actually producing production applications. And making it work like LinkedIn customer support where they actually wrote this great research paper.</p><p>Where they showed that using a knowledge graph for customer support scenarios actually gave them better results. And allowed them to improve the quality and reduce the response time for getting back to customers. Median per issue resolution time was reduced by 28.6%. I mentioned the data.world study which basically was a comparison of doing RAG on SQL versus RAG on graph databases.</p><p>And they showed a three times improvement in accuracy of LLM responses. And let's chat about patterns, Michael. Because I think everyone's here to learn how to do this. Exactly. So let's look at how to do this actually. Right? So and if you look at graph RAG, that's actually two sides to the coin.</p><p>So one, of course, you don't start in a vacuum. You have to create your knowledge graph, right? And we see basically multiple steps to get there. Initially, you get unstructured information. You will substructure it. You put it into a lexical graph, which represents documents, chunks, and their relationships. And the second step, you can then extract entities using, for instance, LLMs with this graph schema to extract entities and the relationships from that graph.</p><p>And in the third phase, you would enrich this graph, for instance, with graph algorithms, doing things like, you know, page rank, community summarization, and so on. And then when you have this build-up knowledge graph, then you do graph REC as the search mechanism. Either with local search or global search and other ways.</p><p>Right? So let's first look at the first phase of, like, knowledge graph construction a little bit. So like, always in data engineering, there is, if you want to have higher quality outputs, you have to put in more effort at the beginning. Right? So it's been nothing comes for free.</p><p>There's no free lunch after all. But what you do at the beginning is basically paying off multiple times because what you get out of your unstructured documents is actually high quality, high structured information, which you then can use to extract contextual information for your queries, which allows to reach retrieval at the end.</p><p>Okay. And so after seeing graph REC being used by a number of users customers, we've seen -- we looked at research papers. we saw that a number of patterns emerging in terms of like how we structure our graphs, how we query these graphs, and so on. And so we started to collect these patterns and put them on graph REC.com.</p><p>And we want to -- I wanted to show what this looks like. So we have basically example graphs in the pattern. The pattern has a name, description, context, and we see also queries that are used for extracting this information. Right? Let's look at the three steps in a little bit more detail on the graph model side.</p><p>So on one side, we have for lexical graph, you represent documents and the elements. So that could be something simple as a chunk. But if you have structured elements and documents, you can also do something like, okay, I have a book which has chapters, which have sections, which have paragraphs, where the paragraph is the semantically cohesive unit that you would use to, for instance, create a vector embedding that you can use later for vector search.</p><p>But what's really interesting in the graph is that you can connect these things all up, right? So you know exactly who is the predecessor, who is the successor to a chunk, who is the parent of an element. And using something like vector or text similarity, you can also connect these chunks as well by a the other way.</p><p>And then you can use all these relationships when you extract the context and the retrieval phase to find what are related chunks by document, by temporal sequence, by similarity and other things. Right? So that's only on the lexical side. This looks like this. So for instance, you have an RFP and you want to break it up in a structured way.</p><p>Then you basically create the relationships between these chunks or these subsections at the text to the vector embeddings and then you do it at scale and then you get a full lexical graph out of that. Next phase is entity extraction, which is also something that has been around for quite some time with NLP.</p><p>But LLMs actually take this to the next level with the multi-language understanding, with their high flexibility, good language skills for extraction. So you basically provide a graph schema and an instruction prompt to the LLM plus your pieces of information, pieces of text. Now with large context windows, you can even put in $10,000, $100,000 tokens for extraction.</p><p>If you have, you can also put in already existing ground pools. So for instance, if you have existing structure data where your entities, let's say products or genes or partners or clients are already existing, then you can also put this in as part of the prompt. So that the LLM doesn't do an extraction, but more an recognition and finding approach, where you find the entities and then you extract the relationships from them and then you can store additional facts and additional information that you store as part of relationships and entities as well.</p><p>So basically in the first part you have the lexical graph, which is scripted on a document structure, but in the second part you extract the relevant entities and their relationships. If you have already an existing knowledge graph, you can also connect this to an existing knowledge graph. So imagine you have a CRM where you already have customer clients and leads in your knowledge graph, but then you want to enrich this with, for instance, protocol from core transcripts and then you basically connect this to your existing structure data as well.</p><p>So that's also a possibility. And then in the next phase what you can do is you can run graph algorithms for enrichment, which then, for instance, can do clustering on the entity graph and then you generate something like communities where an LLM can generate summaries across them and such.</p><p>And for especially the last one, it's interesting because what you identify is actually cross document topics, right? So because it's basically each document is in a temporal vertical representation of information, but what this is is actually it looks at which topics are here occurring across many different documents. So you find these kind of topic clusters across documents as well.</p><p>Cool. So if you look at the second phase, the search phase, which is basically the retrieval part of red, what we see here is basically that in a graphic retriever, you don't just do a simple vector lookup to get the results return, but what you do, you do an initial index search.</p><p>It could be vector search, full text search, hybrid search, spatial search, other kinds of searches to find the entry points in your graph. And then you basically can take, as you can see here, starting from these entry points, you then follow the relationships up to a certain degree or up to a certain relevancy to fetch in additional context.</p><p>And this context can be coming from user question. It can be external user context that comes in, for instance, when someone from, let's say, your finance department is looking at your data, you return different information than someone from the, let's say, engineering department is looking at your data, right?</p><p>So you also take this external context into a column, how much in which context you retrieve. And then you return to the LLM to generate the answer, not just basically text fragments like you would do in vector search, but you also create the, return these more complete subset of the, of the contextual graph to the LLM as well.</p><p>And modern LLMs are actually more trained on graph processing as well. So they can actually deal with these additional pattern structures where you have node relationship, node patterns that you provide as additional context to the LLM. and then of course I mentioned that you can enrich it using graph algorithms so that you can do things like clustering, link prediction, page rank and other things to enrich your data.</p><p>Cool. Let's look at some practical examples. We don't have too much time left. So one is knowledge graph construction from unstructured sources. So there's a number of libraries. You've already heard some today from people that do these kind of things. So one thing that you build is a tool that allows you to take PDFs, YouTube transcripts, local documents, web articles, Wikipedia articles, and it extracts your data into a graph.</p><p>And let me just switch over to the demo here. So this is the tool. So I uploaded information from different Wikipedia pages, YouTube videos, articles, and so on. And here is, for instance, a Google DeepMind extraction. So you can use a lot of different LLMs here. And then you can also, if you want to, in graph enhancement, provide graph schema as well.</p><p>So you can, for instance, say a person works for a company and add these patterns to your schema. And then the LLM is using this information to drive the extraction as well. And so if you look at the data that has been extracted from DeepMind, it's this one here.</p><p>We can actually see from the Wikipedia article two aspects. One is the document with the chunks, which is this part of the graph, right? And then the second part is the entities that have been extracted from this article as well. So you see actually the connected knowledge graph of entities, which are companies, locations, people, and technologies.</p><p>So it followed our, followed our schema to extract this. And then if I want to run graph rec, you have here a number of different retrievers. So we have vector retriever, graph and full text, entity retrievers, and others that you can select. All of this is also an open source project.</p><p>So you can just go to GitHub and have a look at this. And so I just ran this before because internet is not so reliable here. So what has DeepMind worked on? And I get a detailed explanation. And then if I want to, I can here look at details.</p><p>So it shows me which sources that it used, alpha4, Google, Wikipedia, another PDF. I see which chunks have been used, which is the full text and hybrid search. But then I also see which entities have been used from the graph. So I can actually really see from an explainability perspective, these are the entities that have been retrieved by the graph rec retriever, passed to the LLM, in addition to the text that's connected to these entities.</p><p>So it gets a richer response as such. And then you can also do eval on that with as well. So while I'm on the screen, let me just show you another thing that we worked on, which is more like an energetic approach where you basically put these individual retrievers into a configuration and configuration, where you have basically domain-specific retrievers that are running individual suffocates.</p><p>So for instance, if you look at, let's say, this one, it has the query here. And basically a tool with inputs and a description. And then you can have an agentic loop using these tools, basically doing graphics with each individual tool, taking the responses and then doing deeper tool calls.</p><p>I'll show you a deeper example in a minute. So this is basically what I showed you. This is all available as open source libraries. You can use it yourself from Python as well. Or it showed Neoconverse, which was able not just output text, but also charts and other visualizations, networks, visualizations as well.</p><p>And what's interesting here in the agentic approach, you don't just use vector search to retrieve your data, but you basically break down user question into individual tasks and extract parameters and run these individual tools, which then are either run in sequence or in a loop to return the data, and then you get basically these outputs back.</p><p>And then for each of these things, individual tools are called and used here. And the last thing that I want to show is the graphic Python package, which is basically also encapsulating all of the construction and the retrieval into one package. So you can build a large graph, you can implement the retrieval and create the pipelines here.</p><p>And here's an example of where I pass in PDFs plus a graph schema and then basically it runs the import into Neo4j and then I can, in the Python notebook, visualize the data later on. And with that I leave you with, one second, the takeaway, which is on graphrack.com you find all of these resources, a lot of the patterns and we'd love to have contributions and love to talk more.</p><p>I'm outside at the booth if you have more questions. Yeah, so that was great and I think you're getting it all from the expert with all the tooling. Actually, Michael's team builds a lot of the tools like Knowledge Graph Builder. I'm very excited you all came to the graphrack track and hope to chat with you all more.</p><p>If you have questions for me and Michael, just meet us in the Neo4j booth across the way. Thank you. Thank you. Thank you. Thank you, Michael and Steven. That was fantastic. My big takeaway was that there is so much to look at. It's amazing. Is this one for power?</p><p>That's the Wi-Fi. Wi-Fi. Okay. Okay. Thank you. Thank you. Thank you. Thank you, Michael and Steven. That was fantastic. My big takeaway was that there is so much to look at. It's amazing. So in this next talk, who's going to be taking us through a multi-agent framework for network analysis, is this right?</p><p>Correct. Fantastic. Correct. Thank you. Thank you. Thank you. All right. One, two, three, four, five. Five, four, three, two, one. Microphone check. One, two, one, two. All right. Good afternoon, everyone. My name is Ola Mabadeje. I'm a product guy from Cisco. So my presentation is going to be a little more producty than techy, but I think you're going to enjoy it.</p><p>And so I've been at Cisco working on AI for the last three years, and I work in this group called OutShift. So OutShift is Cisco's incubation group. Our charter is to help Cisco look at emerging technologies and see how these emerging technologies can help us accelerate the roadmaps of our traditional business units.</p><p>And so by training, I'm an electrical engineer, doubled into network engineering, enjoyed it, and I've been doing that for a while. But over the last three years, focused on AI. Our group also focuses on quantum technology, so quantum networking is something that we're focused on. And if you want to learn more about what we do with OutShift at Cisco, you can learn more about that.</p><p>So for today, we're going to dive into this real quick. And like I said, I'm a product guy, so I usually start with my customers' problems, trying to understand what are they trying to solve for, and then from that, walk backwards towards creating a solution for that. So as part of the process for us, we usually go through this incubation phase where we ask customers a lot of questions, and then we come up with prototypes.</p><p>We do A testing, B testing, and then we kind of deliver an MVP into a production environment. And once we get product market fit, that product graduates into the Cisco's businesses. So this customer had this issue. They said, when we do change management, we have a lot of challenges with failures in production.</p><p>How can we reduce that? Can we use AI to reduce that problem? So we double-clicked on that problem statement, and we realized it was a major problem across the industry. I won't go into the details here, but it's a big problem. Now, for us to solve the problem, we need to understand, does AI really have a place here, or it's just going to be rule-based automation to solve this problem?</p><p>And when we looked at the workflow, we realized that there are specific spots in the workflow where AI agents can actually help address a problem. And so we kind of highlighted three, four, and five, where we believe that AI agents can help increase the value for customers and reduce the pain points that they were describing.</p><p>And so we sat down together with the teams. We said, let's figure out a solution for this. And so this solution consists of three big buckets. The first one is the fact that it has to be a natural language interface where network operations teams can actually interact with the system.</p><p>So that's the first thing. And not just engineers, but also systems. So for example, in our case, we built this system to talk to an ITSM tool such as ServiceNow. So we actually have agents on the ServiceNow side talking to agents on our side. The second piece of this is the multi-agent system that sits within this application.</p><p>So we have agents that are tasked at doing specific things. So an agent that is tasked as doing impact assessments, doing testing, doing reasoning around potential failures that could happen in the network. And then the third piece of this is where we're going to spend some of the time today, which is a network knowledge graph.</p><p>So we're having the concept of a digital twin in this case. So what we're trying to do here is to build a twin of the actual production network. And that twin includes a knowledge graph plus a set of tools to execute testing. And so we're going to dive into that in a little bit.</p><p>But before we go into that, we had this challenge of, okay, we want to build a representation of the actual network. How are we going to do this? Because if you know networking pretty well, networking is a very complex technology. You have a variety of vendors in a customized environment, a variety of devices, firewalls, switches, routers, and so on.</p><p>And all of these different devices are spitting out data in different formats. So the challenge for us is how can we create a representation of this real-world network using knowledge graphs in a data schema that can be understood by agents. And so the goal is for us to create this ingestion pipeline that can represent the network in such a way that agents can take the right actions in a meaningful way and predictive way.</p><p>And so for us to kind of proceed with that, we had these three big buckets of things to consider. So we had to think about what are the data sources going to be. So if you, again, in networking, their controller systems, their devices themselves, their agents in the devices, their configuration management systems, all of these things are all collecting data from the network, or they all have data about the network.</p><p>Now, when they spit out their data, they're spitting it out in different languages, Yang, Jason, and so on. Another set of considerations to have. And then in terms of how the data is actually coming out, it could be coming out in terms of streaming telemetry, it could be configuration files in Jason, it could be some other form of data.</p><p>How can we look at all of these three different considerations and be able to come up with a set of requirements that allows us to actually build a system that addresses the customer's pain point again? And so the team, from the product side, we had a set of requirements.</p><p>We wanted a system that, a knowledge graph that can have multimodal flexibility, that means it can talk key value pairs, you understand JSON files, you understand relationships across different entities in the network. Second thing is performance. If an engineer is querying a knowledge graph, we want to have instant access to the node, information about the node, no matter where the location of that node is.</p><p>That was important for our customers. The second thing was operational flexibility. So the schema has to be such that we can consolidate into one schema framework. The fourth piece here is where the RAG piece comes into play. So we've been hearing a little about graph RAG for a little bit today.</p><p>We wanted this to be a system that has ability to have vector indexing in it so that when you want to do semantic searches, at some point, you can do that as well. And then in terms of just ecosystem stability, we want to make sure that when we put this in the customer's environment, there's not going to be a lot of heavy lifting that's going to be done by the customer to integrate with their systems.</p><p>And again, he has to support multiple vendors. So these were the requirements from a product side. And then our engineering teams, kind of we started to consider some of the options on the table. Neo4j, obviously, market leader, and the various other open source tools. At the end of the day, the engineering teams decided to kind of do some analysis around this.</p><p>So I'm showing the table on the right-hand side. It's not an exhaustive list of things that they considered, but these were the things that they looked at that they wanted to see, okay, what is the right solution to address the requirements coming from product? And we kind of all centered around the first two here, Neo4j and ArangoDB.</p><p>But for historical reasons, the team decided to go with ArangoDB because we had some use cases that were in the security space. There was kind of a recommendation system type of use cases that we wanted to kind of continue using. But we are still exploring the use of Neo4j for some of the use cases that are coming up as part of this project.</p><p>So we settled on ArangoDB for this and we eventually came up with a solution that looks like this. So we have this knowledge graph solution. This is an overview of it. On the left-hand side, we have all of the production environment. We have the controllers, the Splunk, which is the same system, traffic telemetry coming in.</p><p>All of them are coming into this ingestion service, which is doing an ETL, transforming all of this information into one schema, open config. So open config schema is a schema that is designed around networking primarily and it helps us to, because there's a lot of documentation about it on the internet, so LLMs understand this very well.</p><p>So this setup is primarily a database of networking information that has open config schema as a primary way for us to communicate with it. So natural language communication through an individual engineer or the agents that are actually interacting with that system. And so we built this in the form of layers.</p><p>So if you're into networking again, there is a set of entities in the network that you want to be able to interact with. And so we have layered this up in this way such that if there's a tool call or there's a decision to be made about a test, for example, let's say you want to do a test about configuration drift as an example.</p><p>You don't need to go to all of the layers of the graph. You just go straight down into the raw configuration file and be able to do your comparisons there. If you're trying to do, like, a test around reachability, for example, then you need a couple of layers, maybe you need raw configuration layers, data plane layers, and control plane layers.</p><p>So it's structured in a way that when the agents are making their calls to this system, they understand what the request is from the system and they're able to actually go to the right layer to pick up the information that they need to execute on it. So this is kind of a high-level view of what the graph system looks like in layers.</p><p>Now, I'm going to kind of switch gears and go back to the system. Remember I described a system that had agents, a knowledge graph, a digital twin, as well as natural language interface. So let's talk about the agentic layer. And before I kind of talk about the specific agents in this system, on this application, we are looking at how we are going to build a system that is based on open standards for all of the internet.</p><p>And this is one of the challenges we have within Cisco. We are looking at a system, a set of a collective, open source collective that includes all of the partners we see down here. So we have OutShift by Cisco, we have Langchain, Galileo, we have all of these members who are supporters of this collective.</p><p>And what we are trying to do is to set up a system that allows agents from across the world, so it's a big vision that they can talk to each other without having to do heavy lifting of reconstructing your agents every time you want to integrate them with another agent.</p><p>So it consists of identity, schema framework for defining an agent's skills and capabilities, the directory where you actually store these agents, and then how you actually compose the agents, both of the semantic layer and the synthetic layer, and then how do you observe the agents in process. All of these are part of this collective's vision as a group.</p><p>And if you want to learn more about this, it's on agency.org, and I also have a slide here that kind of talks about there's real code, actually, that you can leverage today. Or if you want to contribute to the code, you can actually go there. There's a GitHub repo here that you can go to and you can start to contribute or use the data.</p><p>There's documentation available as well, and there's Apple applications that allows you to actually see how this works in real life. And we know that there's MCP, there's A2A, all of these protocols are becoming very popular. We also integrate all of these protocols because the goal, again, is not to create something that is bespoke.</p><p>We want to make it open to everyone to be able to create agents and be able to make these agents work in production environments. So, back to the specific application we're talking about. Based on this framework, we delivered this set of agents. We built this set of agents as a group.</p><p>So, we have five agents right now as part of this application. There's an assistant agent that's kind of the planner that kind of orchestrates things across the globe, across all of these agents. And then we have other agents that are all based on React reasoning loops. There's one particular agent I want to call out here, the query agent.</p><p>This query agent is the one that actually interacts directly with the knowledge graph on a regular basis. We have to fine-tune these agents because we initially started by doing a... Attempting to use RAG to do some querying of the knowledge graph but that was not working out well so we decided that for immediate results we're going to fine-tune it and so we did some fine-tuning of this agent with some schema information as well as example queries.</p><p>And so, that helped us to actually reduce two things. The number of tokens we were born in because every time we were... Before that, the AQL queries were going through all of the layers of the knowledge graph and in a reasoning loop was consuming lots of tokens and taking a lot of time for it to result...</p><p>to return results. After fine-tuning we saw a drastic reduction in number of tokens consumed as well as the amount of time it took to actually come back with the results. So, that kind of helped us there. So, I'm going to kind of pause here. I'm talking a lot about...</p><p>There's a lot of slide wear here. I want to show a quick demo of what this actually looks like. So, tying together everything from the natural language interface interaction with an ITSM system to how the agents interact to how that collects information from knowledge graph and delivers results to the customer.</p><p>So, the scenario we have here is a network engineer wants to make a change to a firewall rule. They have to do that to accommodate a new server into the network. And so, what they need to do is to, first of all, start from ITSM, so to submit a ticket in the service now.</p><p>Now, our system here, the UI I'm showing right here is the UI of the actual system we've built, the application we've built. We have ingested information about the tickets here in natural language. And so, the agents here are able to actually start to work on this. So, I'm going to play video here just to make it more relatable.</p><p>So, the first thing that's happening here is that these agents, the first agent is asking that for the information to be synthesized in a summarized way so that they can understand what to quickly do. The next action that has been asked here is for you to create an impact assessment.</p><p>So, impact assessment here just means that I have to understand will this change have any implications for me beyond the immediate target area? And that's going to be summarized and we are now going to ask the agents that is responsible for this particular task to go and attach this information into the ITSM ticket.</p><p>So, I'm going to say attach this information about the impact assessment into the ITSM ticket. So, that's been done. Now, the next step is to actually create a test plan. So, test plan is one of the biggest problems that our customers are facing. They run a lot of tests but they miss out on the right test to run.</p><p>So, these agents are actually able to reason through a lot of information about test plans across the internet and based on the intent that was collected from the ServiceNow ticket is going to come up with a list of tests that you have to run to be able to make sure that this firewall rule change doesn't make a big impact or create problems in production environments.</p><p>So, as you can see here, this agent has gone ahead and actually listed all of the test cases that need to be run and the expected results for each of the tests. So, we are going to ask this agent to attach this information again back to the ITSM ticket because that's where the approval board needs to see this information before they approve the implementation of this change in production environment.</p><p>So, we can see here that that information has now been attached back by this agent to the ITSM ticket. So, two separate systems but agents talking to each other. Now, the next step is actually run a test on all of these test cases. So, in this case, the configuration file that is going to be used to make the change in the firewall is sitting in the GitHub repo and so, we are going to do a pull request of that config file and going to take that information.</p><p>So, this is the GitHub repo where we're going to do a pull request. We're going to take the link for that pull request and paste it in the ticket, the ITSM ticket and so that when the execution agent starts doing his job, he's actually going to pull from that and use it to run his test.</p><p>So, at this moment, we are going to start running the test. We're going to ask this agent to go ahead and actually run the test and execute on this test and so, I have attached the change, sorry, I don't have my glasses, I've attached my change candidates to the ticket.</p><p>Can you go ahead and run the test? So, what is going to happen here is if you look on the right hand side of this screen here, a series of things are happening. The first thing is that this agent called the executor agent goes, looks at the test cases and then it goes into the knowledge graph and it's going to go ahead and actually do a snapshot of the most recent visual or most recent information about the network.</p><p>It's now going to take the pull request that it pulled from GitHub, the snapshot it just took from the knowledge graph, it's going to compute it together and then run all of the individual tests one at a time. So, we can see that it's running the test, one test, test one test, two test, three test, four.</p><p>So, all of this is happening in what we call a digital twin. So, a digital twin, again, is a combination of the knowledge graph instead of tools that you can use to run a test. So, an example of a tool here could be Batfish or could be RouteNet or some other tools that you use for network engineering purposes.</p><p>So, once all of these tests are completed, this tool actually is going to, this agent is going to now generate a report about the test results. So, we give you some time to run through this, it's still running the tests, but once it concludes all of the tests, it's going to report what actually the test results are.</p><p>So, which results, which tests actually passed, which ones failed. for the ones that have failed, it's going to make some recommendations on what you can do to go in and fix the problem. I'm going to skip to the front here to just quickly get this done quickly because of time.</p><p>So, it's attached the results to the ticket, and this is the report that it's spitting out in terms of, this is the report for the tests that were run. So, this execution agent actually created a report about all of the different test cases that were run by the system.</p><p>So, very quick short demo here. There's a lot of detail behind the scenes, but I can ask some questions offline. The couple of things I want to leave us with is that, before I go to the end of this, is that evaluation is very critical here for us to be able to understand how this delivers value to customers.</p><p>We're looking at a variety of things here. So, the agents themselves, the knowledge graph, slash digital twin, and we're looking at what can we actually measure quantifiably. Now, for the knowledge graph, we're looking at extrinsic metrics, particularly not intrinsic ones because we want to map this back to the customer's use case.</p><p>So, this is the summary of what we see in terms of evaluation metrics. We are still learning. This is, for now, it's an MVP, but what we are learning so far is that those two key building blocks, the knowledge graph, and the open framework for building agents is very critical for us to actually build a scalable system for our customers.</p><p>And so, I'm going to stop. It's eight seconds to go. Thank you for listening to me, and then if you have questions, I'll be out there. Thank you so much, Ola. That was fantastic. I love getting a deep dive, and always a perspective from a product guy. It's always good to hear.</p><p>Keep us set in reality. Thank you. So, for the GraphRack track, closing out the day on this track, it's going to be my friend, Tom Smoker, from Why Howl, who's going to be talking about legal documents, and how to turn those into knowledge graphs, right? Part of it, yeah.</p><p>Awesome. Quick check on your audio. Five, four, three, two, one. Still up? Oh, there you are. Oh, yeah. Beautiful. Great. Okay. Cool. Thanks, everyone. When we're ready, ABK, you just let me know. Standby. Change the records. Good. Cool. I can't see a whole lot. Thank you. I'll get started.</p><p>Okay. Yes. Oh, no. Standby. One moment. We have to change the records. No worries. The drive, I'm sorry. Thank you. I have bad eyesight and an Australian accent, so this is not a great combination, so I appreciate you working with me. Thank you. Hello, everyone. I am here to talk about Graph Rag, as we're here for the track, but I'm talking about what to do in the legal industry and what we do in the legal industry, and what does it look like to turn documents into graphs and use those graphs in the age of AI.</p><p>I tend to have to qualify why I'm at places. There's various reasons why I could be talking today. You choose the one that you want to, but generally, I've been working in graphs for about a decade. I have a good relationship with the Neo4j team, and I've been doing graphs for a long time, but primarily, I am the technical founder of a company called whyhow.ai, and we find cases first before lawyers do and then give them to lawyers.</p><p>Now, how we find these cases is a process that I'll go through, but we use a variation of graphs, multi-agent systems, signals, etc., and I'll detail through today how we do that at a high level and a low level, and I'm happy to answer questions at any point. This is broadly what we do.</p><p>We work in law. This is an example. We find class action, mass tort cases before other people do. We have agents. We have graphs. We store that information. We scrape the web. We qualify that with a proprietary process, and we deal with lawyers every day and understand exactly how they think and build these cases, and the cases I'm referring to would be like many people used a pharmaceutical product.</p><p>That product has caused them harm. Science has proved that harm, and we can collect those people and collectively sue the pharmaceutical company. So we support the law firms that do that. And as I'm talking, and everyone here for a graph rad track can start to imagine that I'm starting to develop a bit of a schema there.</p><p>I'm describing individuals. I'm describing products. Those products have ingredients. Those ingredients have concentrations. Those concentrations may have an ID number, and all of a sudden, you can start to imagine there is this large, networked, schematized bit of data that has particular points in it that are very valuable and very visual and very useful to domain experts.</p><p>So I'm going to start to use some definitions because knowledge graphs have been around for a long time, and ABK would know that more than I would, but I started my PhD and won my master's in graphs in 2016, and it was not nearly as popular as it is now, and it's fascinating to see how far it's come, but I do think it's important for me to define how we use them and how we think about them.</p><p>Broadly, to me, graphs are relations. That's part of the visual element, and there's a back-end element as well, but it's the benefit of using graphs is that I can see what is connected to something else. I can be explicit about what is connected to something else, and I can do mass analytics on what is connected to something else.</p><p>All the way from I can see it down to I can do large-scale analytics on it is the value of the relations. And when I use relations, it's not necessarily node-to-node. It can be node-to-node-to-node. It can be multi-hop. It can be as varied and as forked and as distributed as you want.</p><p>This is why we use graphs in our process. Broadly, throughout the process of running this company and previously as an academic, this is what I think is easy about graphs. People look at them and go, well, that's fantastic. I have a great understanding of what this is. And someone else says, me too.</p><p>And there isn't necessarily a consistency in what those two people just said. They may have a different understanding of what is represented. Broadly, throughout my career, these are the things that are difficult about graphs, right? And you can say that they're nodes connected to edges. You can say they're distributed.</p><p>You can say they're backed up. There's a variety of ways in which people use the data that they have, the way they store it and the way they talk about it. And now, as graphs have become very necessary and consistent for things like graph rag, for things like structured data, et cetera, more and more people are coming to this relatively niche area previously that even at the time wasn't necessarily agreed upon what it was.</p><p>So I do like to define what it is we're using. So graphs and multi-agent systems. These are the two things that I want to define as there's a variety of ways that people use them. This is how we use multi-agent systems, right? So now, multi-agent systems are all the way from very specifically define what you're dealing with and chain those together and use an LLM to glue it all together.</p><p>Or it is, in our case, break down a complicated white-collar workflow down into a specific set of steps that I can I-O test, right? And each of those steps have different requirements, different frequencies, different state. and that state can be controlled often, in our case, by a graph. This is why we like to use them.</p><p>When we're building an application for the legal industry, I'm not sure if you guys know this, but lawyers don't really like when things are incorrect, right? It is basically the whole industry is make this very specifically correct and proper and definitely in the right language. So when it comes to building applications, probabilistic large language models don't necessarily work for that just in isolation.</p><p>I need to have a very specific control and structure and schema for the way that we build these systems and I need to be able to test and be able to pinpoint exactly what is going right and wrong at any point in time. Here's some of the issues with that, right?</p><p>And we've heard about multi-agent systems a lot. I'm sure other people have as well. Sometimes the part in the workflow is much more important than the other part. Sometimes there's parts in the workflow I don't particularly care about. There are also agents in the world. Agents imply that these things are very capable, but I can write a bad prompt very easily and all of a sudden I have a bad agent.</p><p>So when it comes to what is the agent that I trust, very few. We spend a lot of time guard railing as much as we possibly can. We spend time making so that the memory is not just immediate but it's episodic. We spend time capturing the information state over time and then pruning that state.</p><p>And again, to bring it back, capturing, expanding, pruning, structuring, and then querying state for us happens in a graphical format because the necessity of having the structure, having the extendability, and then having the ability to remove that extension is really important for us. And then finally, I'm trying not to make this too deep depth and too many numbers, but 95% accuracy for a single agent I think is a tall order at this point.</p><p>Maybe people have entirely accurate agents. I'm very happy for you. I don't have that exactly right now. I have systems that I can put in place like guardrails and humans in the loop that can bring these agents to a point that it is accurate enough that people are willing to use them.</p><p>However, five 95% accurate agents chain together sequentially that 77% expected accuracy. That's not that many agents in a row. If you think about a workflow, that's five steps. And if I'm basically saying that if each of those five steps are 95% accurate, already quite a hard thing to ask, especially if there's an LLM involved.</p><p>Now we're at 77% of the time it gets to the end of that workflow in the way that I want. That is part of, probably if I was to summarize my main problem, it would be that. It would be decision making under uncertainty throughout the process of building these systems.</p><p>That's the background. That's how we understand these systems. We use multi-agent systems and we're naturally skeptical. We use graphs every day and we have a natural skepticism of exactly how these things are stored and structured. But we use them specifically and consistently in the way that we like. So, I am using the term agent because everyone's using the term agent.</p><p>We build litigation agents. Litigation is the process of, well, I'm going to summarize, but we work with class action slash mass tort law. As I said before, get everyone together. They were harmed. Put that harm all in place and then sue a pharmaceutical company. Now, we don't do any of the litigating as a company or the suing, but we do support the lawyers who do that.</p><p>We do that in a few different ways. Here is one of the ways that we look at the legal industry, right? Without exception, everything needs to be perfect. It needs to be accurate. It needs to be written in the correct way, right? There's also, once you have that correct format, creative arguments.</p><p>The best lawyers are very, very, very detail-oriented and then very, very creative in the way that they can apply those details to a case. For example, there was an issue with Netflix and they were capturing data from their users as they do and they should and I'm a Netflix user and they capture my data and I appreciate it because they give me the better shows that I'd like to watch.</p><p>However, there is a legal limit as to how much information they can capture from me, right? And you cannot surpass that legal limit or you can, but then you can go into the process of litigation. Now, if you surpass that, there needs to be a precedent as to why someone could say you cannot capture this much information and the particular precinct I'm referring to is many years ago, Blockbuster was sued by keeping too many details about the literal physical DVDs that people rented.</p><p>That is a reasonably creative way to say, look, I remember that Blockbuster happened and what Netflix is doing isn't that different. It may be in a digital format, it may be at a larger scale, it may be into an algorithm instead of someone who's recommending it. However, that is an interesting application of what I'm doing.</p><p>so these problems then which is necessary accuracy and then creativity on top of that accuracy and then all of that information is kept in separate places and a lot of that creativity comes from the latent knowledge in the expert's head starts to come to a bit of a fall when you say, well, I have these probabilistic agents that you could argue aren't that creative, right?</p><p>I have these agents that most of the time do a pretty good job and can be creative in a way that frankly can be quite frustrating especially to a lawyer. So this butts heads in terms of exactly how lawyers want to deal with this information and again, I'm painting a very broad brush.</p><p>I'm not a lawyer, my co-founder is. If anyone is a lawyer in the audience who's offended, I do apologize but this is broadly what I've seen to be accurate. we help with legal discovery as well, right? Like I described before there could be an unnamed pharmaceutical company and a pharmaceutical company is great but they happen to have done some harm, right?</p><p>And it is in their best interest to give all of the information to the law firm and describe exactly well, not exactly describe in as many ways as possible here is 500 gigabytes of emails that don't matter go nuts, right? Figure out exactly what happened at what point and bring out the information.</p><p>Now, that is a challenge at the moment. A lot of the time it's manually reviewed. There are shortcuts and processes by necessity because a lot of these lawsuits are on a particular timeline. It is physically impossible to read all of the information that is given in the discovery of the processing of a lawsuit.</p><p>However, and this is just a generic graph I used because I'm not allowed to use the ones that I'm currently working on. However, if you can take all of that information, you can extract the information and structure it in such a way that it is consistent, all of a sudden that mountain of emails becomes a lot of information I can immediately dismiss and a bunch of genuinely useful information that I can look at.</p><p>And not just that, when it comes to a graph, I can actually augment the information from discovery and then I can give that visual to the expert who can make an immediate decision. I'm going to loop back to the example I was describing before, which is the pharmaceutical example.</p><p>So again, if ingredients are a certain concentration, that concentration is at a problem, that problem happened at a certain time, there is only going to be a few people in that graph of potentially millions of nodes that are a problem, right? In the same way that there are only a few people in that mountain of documents that were a problem.</p><p>However, now I've changed the form factor such that I can specifically hone in on what matters and not just hone in in a data-driven way, I can hone in in a visual way in natural language such that the lawyer who knows exactly what that natural language means or the expert who knows exactly what that natural language means can make a decision that's data-driven.</p><p>There's also a process if we can build this information exactly and I'm giving the fundamentals, this is a graph rag talk, we want to bring this graph in. The graph I just described is not that large, the graph I just described has a consistent schema and the graph I just described can be relatively easily retrieved.</p><p>I'm not going to say that retrieval is completely solved, I am going to say we have agents in production right now that lawyers can in natural language query and further understand the lawsuit and the individuals that they're representing. Now we get to case research. So that was more discovery, right, mountain of documents.</p><p>Case research would be a lot of people used said product and they're complaining about it online and this is a lot of the value of our company and what we do. People can complain all the time. They can shout into the void of a niche subreddit or they can go on Twitter or they can be on a forum that they're used to, they can be in IRC, they can be wherever they want, right, but they're using similar language about a specific thing and so when it comes to traditional case research, that information isn't really discovered.</p><p>A lot of the time it happens through talking to another individual, subscribing to a newsletter, et cetera. How do people find the information? So this is a graphic I've taken from our website which I promise looks significantly better than the slides that I make but I tend to try and talk to them.</p><p>Here is how case research in our case for our business works and that is we start and scrape the entire web. Now anyone can scrape the entire web. It's doable. It's a technical challenge but it's doable and you can scrape it at a frequency in the services, et cetera.</p><p>what we do is scrape the web and then qualify the leads of that scraping. We filter down all of the information down to specifically what the individuals want. We have schemas that we work with particular law firms and lawyers and those schemas get us down to just the information that they care about and look, maybe there is but right now at least for me there's no such thing as a perfect case.</p><p>There's no such thing as a perfect lawsuit. It depends on the lawyer or the partner or the firm who's willing to take that on. So it is not a problem of best. It's a problem of specific and personalized and that is where things like LLMs are particularly useful at the moment.</p><p>That's where things like multi-agent systems are fantastic. That's where things like structured information and graphs all of a sudden a different lawyer can have a different multi-agent system and a different graph that backs up their specific way that they like to work as opposed to having a compromise previously on the way that everyone else liked to work or maybe hear something if they can.</p><p>And from there once we've honed down just to the signals that they care about the qualified signals that are specific to them that signal can then further generate a report and that report can be entirely specific to the lawyer as well. So when it comes to report generation again multi-agent system that's backed up by a schema and that schema is consistent and pruned and that schema looks like controlled state with a graph that can build a report that the lawyer wants.</p><p>And every report is going to be different but the structure is going to be the same for each lawyer and each lawyer has a different process. What I'm broadly describing is mass scraping the web down to a specific signal generated just for the lawyer. It's an entirely personalized service that's been automated.</p><p>And that is the process of what we do and this is part of how we are able to manage and use state and graphs and multi-agent systems to bring the information together. Cool. I'm going to go through I think I have one case study that I want to describe just conscious of time.</p><p>This happens. It's not great. No one really wants it to. there may be situations in which there's a bunch of people who bought a car who really wanted it to catch fire. We don't necessarily deal with them. What we do find is that there are people who are driving their car and it starts to smoke and then it catches fire and that is not the behavior that they intended to happen.</p><p>It was not on the brochure when they bought it. It's not what they want. Those people immediately go and complain as they should. They go to government website. They go to carcomplaints.com. They're on a specific subreddit or forum. And once we can start to track that, which we can, and once we can start to scrape and then structure and then schematize and then analyze, we can start to basically build a density of complaints for a specific vehicle, for a specific year, for a specific problem.</p><p>And that density is a combination of how many complaints multiplied by the velocity of complaints, so a certain amount per month over a number of months. Well, all of a sudden, we get to the point where we're finding these leads particularly early. And now, as we're building models, we're starting to find these leads early and earlier and that we don't necessarily need the velocity straight away.</p><p>We can start to figure out what are the previous lawsuits which were all public and very well documented and exactly what happened in that process. And so, for a large law firm, maybe eight or nine months post people starting to complain, they can take that lawsuit on if they want to.</p><p>For us, we can find it within about 15 minutes and then generally it takes probably a month for you to be confident that this is the signal that you want. and so we can find things significantly earlier. That process, again, scraping the web, filtering down, producing the specific report.</p><p>This is an example that we did. And again, we deal with what the lawyers want. So this lawyer, again, he made the case that people's cars are catching fires. They don't really want them to. Those are the cases that he would like to take on. It's of a certain amount of money.</p><p>It's of a certain maker model. It's in a certain jurisdiction, et cetera. Those specific filters, that schema, can be applied throughout the entire process. That's basically the graph. Each of these lawyers have a specific graph that they want. And not just that, they can filter and feedback that information.</p><p>So it's not just a static graph. I mean, the benefit of a graph structure, at least, well, one of the benefits of a graph structure, I should say, is that it's an extensible schema and that I can update and I can query across and I can understand that information.</p><p>So while we are dealing with RAG, I would say we have less of a chat RAG interface. Well, the lawyers definitely do appreciate that. A lot of what we have when it comes to RAG or retrieval augmented generation would be generating these reports. because as much as a lawyer does want an answer, what they also want is the form factor they're used to.</p><p>And so all of these graphs are consistently made and built each day and then some subgraph from that broader monolithic structure is then brought in and composed into a report that a lawyer can action. kind of what's next and I'll talk about the future a little bit. I mean, what I described is kind of what we're doing but this is what we're doing.</p><p>Final lawsuit's early. Compensate harm and then people can have that information if they want to. We're able to do this entirely technically. We're able to scrape the web structure, etc. We're able to iteratively build up a schema as we want to. This is not just a Gen AI problem and I think this is an important thing that I've seen around this conference and people may be seeing is that Gen AI is not better than machine learning and LLMs are not better than traditional ML systems but there are situations in which one is fantastic and one is not.</p><p>If you look at multi-agent systems and again, I was previously an academic in multi-agent systems and no one ever listened to me so this is a bizarre situation but when you used to structure the multi-agent systems together somewhere along that workflow you would have to stop or say this is not doable because I cannot plug these two bits of information together.</p><p>It's too probabilistic or it's too random or it's too inconsistent or the way to describe it is not a binary feature, right? It is I really just want to kind of type what I want, right? Now with LLMs you can but it's very much for us not an LLM filtered system it's an ML filtered system that LLMs have allowed us to pipe together such that you can actually provide value completely end-to-end which I think was previously not doable and for us, again, we've been using graphs for a long time for us, the ability to iteratively build that graph prune that graph and every single report gets better because we're able to manage the state is why people like working with us because we can consistently follow and track exactly what they want specifically.</p><p>Cool. I think I'm just about at time kind of got in early but that's been the talk specifically around I'm happy to talk to anyone about the specifics graph rag, et cetera multi-agent systems but that's how we use the process. Thank you very much. I don't want to touch you if I don't need to.</p><p>What's the best way? Oh, sorry. Let me come down. Can I unplug this? Is that okay? Thank you. That's a wrap for our day. Okay. Is that ready? Thank you so much. Thank you very much. I'll talk to you soon. Wow, your information is so awesome. Do you have a business?</p><p>I don't know. I'm going to shoot my. I don't know, I think. Is it the specific lawsuit or the following? The following. It's not. Thank you. the sessions in this room. We do have to clear this room out to set it for tomorrow so we thank you and there's plenty of work space out there.</p><p>I promise, they still have Wi-Fi out there. It's probably even better. They might even have coffee and lemon bars. thank you. Thank you. Thank you. We're going to have our questions outside in the track. Thank you. He's going to meet you outside. I promise he's not going to go anywhere.</p><p>be glad I don't have music. What's that? We usually just turn the music up but they didn't clear any copyright. They didn't clear any music. No music for us. They didn't clear flash. Right. Yeah, yeah. Can you tell I'm a producer? Yes. Thank you. Awesome, thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p><p>Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you.</p></div></div></body></html>