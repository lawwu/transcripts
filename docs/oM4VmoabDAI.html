<html><head><title>Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>Coding LLaMA 2 from scratch in PyTorch - KV Cache, Grouped Query Attention, Rotary PE, RMSNorm</h2><a href="https://www.youtube.com/watch?v=oM4VmoabDAI"><img src="https://i.ytimg.com/vi/oM4VmoabDAI/maxresdefault.jpg" style="width:50%;"></a><div><br></div><h3>Chapters</h3><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=0">0:0</a> Introduction<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=80">1:20</a> LLaMA Architecture<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=194">3:14</a> Embeddings<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=322">5:22</a> Coding the Transformer<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1195">19:55</a> Rotary Positional Embedding<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3830">63:50</a> RMS Normalization<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4273">71:13</a> Encoder Layer<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4610">76:50</a> Self Attention with KV Cache<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5352">89:12</a> Grouped Query Attention<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5654">94:14</a> Coding the Self Attention<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7300">121:40</a> Feed Forward Layer with SwiGLU<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7730">128:50</a> Model weights loading<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8486">141:26</a> Inference strategies<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8715">145:15</a> Greedy Strategy<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8848">147:28</a> Beam Search<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9073">151:13</a> Temperature<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9172">152:52</a> Random Sampling<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9267">154:27</a> Top K<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9423">157:3</a> Top P<br><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9539">158:59</a> Coding the Inference<br><br><div style="text-align: left;"><a href="./oM4VmoabDAI.html">Whisper Transcript</a> | <a href="./transcript_oM4VmoabDAI.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Hello guys, welcome to my new coding video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2" target="_blank">00:00:02.640</a></span> | <span class="t">In this video we will be coding Lama 2 from scratch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5" target="_blank">00:00:05.640</a></span> | <span class="t">And just like the previous video in which I was coding the transformer model from zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11" target="_blank">00:00:11.640</a></span> | <span class="t">while coding I will also explain all the aspects of Lama, so each building block of the Lama architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=19" target="_blank">00:00:19.400</a></span> | <span class="t">And I will also explain the math behind the rotary positional encoding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=23" target="_blank">00:00:23.360</a></span> | <span class="t">I will also explain grouped query attention, the KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=27" target="_blank">00:00:27.280</a></span> | <span class="t">So we will not only have a theoretical view of these concepts, but also a practical one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=33" target="_blank">00:00:33.120</a></span> | <span class="t">If you are not familiar with the transformer model, I highly recommend you watch my previous video on the transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=38" target="_blank">00:00:38.640</a></span> | <span class="t">And then if you want, you can also watch the previous video on how to code a transformer model from zero,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=43" target="_blank">00:00:43.440</a></span> | <span class="t">because it will really help you a lot in understanding what we are doing in this current video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=49" target="_blank">00:00:49.120</a></span> | <span class="t">If you already watched my previous video on the architecture of Lama, in which I explained the concepts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=55" target="_blank">00:00:55.080</a></span> | <span class="t">it will be also really helpful if you didn't, I will try to explain all the concepts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=61" target="_blank">00:01:01.880</a></span> | <span class="t">but not as much in detail as in the last video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=66" target="_blank">00:01:06.960</a></span> | <span class="t">So please, if you have time, if you want, please watch my previous video on the Lama architecture,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=71" target="_blank">00:01:11.520</a></span> | <span class="t">and then watch this video, because this will really give you a deeper understanding of what is happening.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=76" target="_blank">00:01:16.920</a></span> | <span class="t">And let's review the Lama architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=81" target="_blank">00:01:21.280</a></span> | <span class="t">Here we have a comparison between the architecture of the standard transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=85" target="_blank">00:01:25.040</a></span> | <span class="t">as introduced in the "Attention is all you need" paper, and the architecture of Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=90" target="_blank">00:01:30.320</a></span> | <span class="t">The first thing we notice is that the transformer was an encoder-decoder model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=96" target="_blank">00:01:36.000</a></span> | <span class="t">And in the previous video, we actually trained it on a translation task,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=100" target="_blank">00:01:40.120</a></span> | <span class="t">on how to translate, for example, from English to Italian, while Lama is a large language model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=106" target="_blank">00:01:46.160</a></span> | <span class="t">So the goal of a large language model is actually to work with what is called the next token prediction task.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=113" target="_blank">00:01:53.720</a></span> | <span class="t">So given a prompt, the model tries to come up with the next token that completes this prompt in the most coherent way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=122" target="_blank">00:02:02.080</a></span> | <span class="t">So in a way that it makes sense, the answer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=125" target="_blank">00:02:05.720</a></span> | <span class="t">And we keep asking the model for the successive tokens based on the previous tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=132" target="_blank">00:02:12.040</a></span> | <span class="t">So this is why it's called a causal model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=134" target="_blank">00:02:14.560</a></span> | <span class="t">So each output depends on the previous tokens, which is also called the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=140" target="_blank">00:02:20.800</a></span> | <span class="t">Contrary to what I have done in my previous video on coding the transformer model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=145" target="_blank">00:02:25.840</a></span> | <span class="t">in this video, we will not start by coding the single building blocks of Lama and then come up with a bigger picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=154" target="_blank">00:02:34.120</a></span> | <span class="t">But we will start from the bigger picture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=155" target="_blank">00:02:35.840</a></span> | <span class="t">So we will first make the skeleton of the architecture and then we will build each block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=162" target="_blank">00:02:42.240</a></span> | <span class="t">I find that this is a better way for explaining Lama also because it's the model is simpler,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=168" target="_blank">00:02:48.040</a></span> | <span class="t">even if the single building blocks are much more complex.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=171" target="_blank">00:02:51.320</a></span> | <span class="t">And this is why it's better to first look at how they interact with each other and then zoom in into their inner workings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=179" target="_blank">00:02:59.600</a></span> | <span class="t">Let's start our journey with the embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=183" target="_blank">00:03:03.160</a></span> | <span class="t">So this block here, let me use the laser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=186" target="_blank">00:03:06.200</a></span> | <span class="t">So this block here, so we are given an input and we want to convert it into embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=192" target="_blank">00:03:12.680</a></span> | <span class="t">Let's also review what are embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=194" target="_blank">00:03:14.760</a></span> | <span class="t">These are my slides from my previous video on the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=199" target="_blank">00:03:19.560</a></span> | <span class="t">And as you can see, we start with an input sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=203" target="_blank">00:03:23.200</a></span> | <span class="t">So we are given, for example, the sentence, your cat is a lovely cat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=206" target="_blank">00:03:26.640</a></span> | <span class="t">We tokenize it, so we split into single tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=209" target="_blank">00:03:29.480</a></span> | <span class="t">We match, we map each token into its position in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=213" target="_blank">00:03:33.880</a></span> | <span class="t">The vocabulary is the list of all the words that our model can recognize.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=219" target="_blank">00:03:39.680</a></span> | <span class="t">These tokens actually most of the time are not single words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=224" target="_blank">00:03:44.240</a></span> | <span class="t">What I mean is that the model doesn't just split the word by whitespace into single words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=230" target="_blank">00:03:50.200</a></span> | <span class="t">Usually the most commonly used tokenizer is the BPE tokenizer, which means byte pair encoding tokenizer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=236" target="_blank">00:03:56.960</a></span> | <span class="t">in which the single tokens can also be a sequence of letters that are not necessarily mapped to a single word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=244" target="_blank">00:04:04.440</a></span> | <span class="t">It may be a part of a word or maybe it may be a whitespace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=248" target="_blank">00:04:08.840</a></span> | <span class="t">It may be multiple words or it may be a single digit, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=253" target="_blank">00:04:13.440</a></span> | <span class="t">And the embedding is a mapping between the number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=257" target="_blank">00:04:17.840</a></span> | <span class="t">So the input IDs that represent the position of the token inside of the vocabulary to a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=264" target="_blank">00:04:24.400</a></span> | <span class="t">This vector in the original transformer was of size 512, while in Lama, in the base model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=271" target="_blank">00:04:31.720</a></span> | <span class="t">so the 7 billion model, it's 4096.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=276" target="_blank">00:04:36.600</a></span> | <span class="t">The dimension is 4096, which means this vector will contain 4096 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=283" target="_blank">00:04:43.200</a></span> | <span class="t">Each of these vectors represents somehow the meaning of the word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=288" target="_blank">00:04:48.400</a></span> | <span class="t">because each of these vectors are actually parameter vectors that are trained along the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=294" target="_blank">00:04:54.400</a></span> | <span class="t">and somehow they capture the meaning of each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=297" target="_blank">00:04:57.680</a></span> | <span class="t">So for example, if we take the word "cat" and "dog",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=301" target="_blank">00:05:01.560</a></span> | <span class="t">they will have an embedding that is more similar compared to "cat" and "tree",</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=306" target="_blank">00:05:06.880</a></span> | <span class="t">if we compare the distance of these two vectors, so we could say the Euclidean distance of these two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=314" target="_blank">00:05:14.960</a></span> | <span class="t">And this is why they are called embedding, this is why we say they capture the meaning of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=321" target="_blank">00:05:21.640</a></span> | <span class="t">So let's start coding the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=324" target="_blank">00:05:24.600</a></span> | <span class="t">I will open Visual Studio Code and first of all, we make sure that we have the necessary libraries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=332" target="_blank">00:05:32.800</a></span> | <span class="t">I will also share the repository of this project.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=335" target="_blank">00:05:35.800</a></span> | <span class="t">So the only thing we need is Torch, Sentence Piece, which is the tokenizer that we use in Lama, and TKODM.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=343" target="_blank">00:05:43.200</a></span> | <span class="t">The second thing is from the official repository of Lama, you should download the download script.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=347" target="_blank">00:05:47.440</a></span> | <span class="t">It's this file, download.sh, that allows you to download the weights of the Lama model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=352" target="_blank">00:05:52.400</a></span> | <span class="t">In my case, I have downloaded the Lama 2 7 billion, along with the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=357" target="_blank">00:05:57.920</a></span> | <span class="t">And this is the smallest model, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=361" target="_blank">00:06:01.120</a></span> | <span class="t">And I will not even be able to use the GPU because my GPU is not powerful enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=366" target="_blank">00:06:06.000</a></span> | <span class="t">And so I will run the model on the CPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=368" target="_blank">00:06:08.400</a></span> | <span class="t">I think most of you guys will do the same because it's actually a little big for a normal computer unless you have a powerful GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=375" target="_blank">00:06:15.840</a></span> | <span class="t">So let's start coding it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=377" target="_blank">00:06:17.920</a></span> | <span class="t">Let's create a new file, model.py, and let's start our journey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=386" target="_blank">00:06:26.080</a></span> | <span class="t">Import the necessary things. So we import Torch, import and also Torch.nn.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=400" target="_blank">00:06:40.880</a></span> | <span class="t">Okay, these are the basic things that we always import. I remember we also need math and then also data classes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=417" target="_blank">00:06:57.840</a></span> | <span class="t">And this is all the imports we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=420" target="_blank">00:07:00.240</a></span> | <span class="t">Most of the code is actually based on the original Lama code, so you don't be surprised if you see a lot of similarities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=426" target="_blank">00:07:06.480</a></span> | <span class="t">But I simplified a lot of parts to remove things that we don't need, especially, for example, the parallelization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=432" target="_blank">00:07:12.320</a></span> | <span class="t">And I also tried to add a lot of comments to show you all the shapes changed in each tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=440" target="_blank">00:07:20.560</a></span> | <span class="t">And that's it. So let's start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=442" target="_blank">00:07:22.800</a></span> | <span class="t">So the first thing I want to create is the class that represents the parameters of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=449" target="_blank">00:07:29.360</a></span> | <span class="t">So...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=477" target="_blank">00:07:57.600</a></span> | <span class="t">Here we already see that we have two type of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=480" target="_blank">00:08:00.160</a></span> | <span class="t">One is the number of heads for the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=482" target="_blank">00:08:02.960</a></span> | <span class="t">So number of heads for the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=486" target="_blank">00:08:06.320</a></span> | <span class="t">And here we have the number of heads for the k and the v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=491" target="_blank">00:08:11.520</a></span> | <span class="t">So the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=493" target="_blank">00:08:13.600</a></span> | <span class="t">Because we will see later, with the grouped query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=497" target="_blank">00:08:17.040</a></span> | <span class="t">we don't have to necessarily have the same number of heads for the query key and values like in the original transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=502" target="_blank">00:08:22.720</a></span> | <span class="t">But we can have multiple number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=505" target="_blank">00:08:25.440</a></span> | <span class="t">And we will see why and how they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=516" target="_blank">00:08:36.160</a></span> | <span class="t">This will be set when we load the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=543" target="_blank">00:09:03.680</a></span> | <span class="t">These two parameters indicate the dimension, the hidden dimension of the ffnlayer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=547" target="_blank">00:09:07.600</a></span> | <span class="t">So the feedforward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=549" target="_blank">00:09:09.360</a></span> | <span class="t">The basic idea is that they try to, when they introduce the grouped query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=553" target="_blank">00:09:13.680</a></span> | <span class="t">they try to keep the number of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=556" target="_blank">00:09:16.240</a></span> | <span class="t">Because with the grouped query attention, we reduce the number of heads of the k and v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=560" target="_blank">00:09:20.160</a></span> | <span class="t">But they incremented the number of parameters of the feedforward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=563" target="_blank">00:09:23.680</a></span> | <span class="t">So as the number of the total parameters of the model remains the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=567" target="_blank">00:09:27.440</a></span> | <span class="t">This allows to compare the full, the base transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=572" target="_blank">00:09:32.480</a></span> | <span class="t">So with all the heads for the query, the key and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=575" target="_blank">00:09:35.520</a></span> | <span class="t">With the one they use in llama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=577" target="_blank">00:09:37.600</a></span> | <span class="t">Which has a reduced number of heads for the k and v.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=580" target="_blank">00:09:40.160</a></span> | <span class="t">But this is just a decision, an architectural decision.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=583" target="_blank">00:09:43.360</a></span> | <span class="t">Then we have some EPS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=586" target="_blank">00:09:46.560</a></span> | <span class="t">This is a number that is very small.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=589" target="_blank">00:09:49.200</a></span> | <span class="t">And we will see why we need it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=601" target="_blank">00:10:01.600</a></span> | <span class="t">Oh my god.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=602" target="_blank">00:10:02.160</a></span> | <span class="t">And these are all the parameters we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=614" target="_blank">00:10:14.800</a></span> | <span class="t">Also here we have two parameters that we will later use for the kv cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=618" target="_blank">00:10:18.400</a></span> | <span class="t">And I will explain you later what is it and how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=620" target="_blank">00:10:20.720</a></span> | <span class="t">Let's start, as I said before, let's start with implementing the skeleton of the entire model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=626" target="_blank">00:10:26.880</a></span> | <span class="t">And then we implement each single part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=629" target="_blank">00:10:29.440</a></span> | <span class="t">And while implementing each single part, I will also review the background.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=633" target="_blank">00:10:33.760</a></span> | <span class="t">And how it works and the maths behind it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=635" target="_blank">00:10:35.680</a></span> | <span class="t">This is the main class that will represent the entire model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=644" target="_blank">00:10:44.640</a></span> | <span class="t">So all the model we can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=648" target="_blank">00:10:48.400</a></span> | <span class="t">So all this model here, except for the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=651" target="_blank">00:10:51.600</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=668" target="_blank">00:11:08.560</a></span> | <span class="t">we make sure that we have set the vocabulary size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=678" target="_blank">00:11:18.720</a></span> | <span class="t">We save the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=697" target="_blank">00:11:37.680</a></span> | <span class="t">This is the number of layers of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=701" target="_blank">00:11:41.920</a></span> | <span class="t">So this represents this block here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=706" target="_blank">00:11:46.640</a></span> | <span class="t">It's repeated many times, one after another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=709" target="_blank">00:11:49.040</a></span> | <span class="t">Just like in the transformer, the base transformer, if you remember correctly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=712" target="_blank">00:11:52.560</a></span> | <span class="t">This block here and this block here were repeated one after another many times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=716" target="_blank">00:11:56.080</a></span> | <span class="t">And here it's repeated 32 times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=719" target="_blank">00:11:59.600</a></span> | <span class="t">And the output of the last layer is then sent to this rms norm, then to the linear, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=734" target="_blank">00:12:14.880</a></span> | <span class="t">So I'm using the same names as used in the transformer in the code from the Lama repository.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=748" target="_blank">00:12:28.400</a></span> | <span class="t">Because when we will load the weights of the model, the names must match.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=752" target="_blank">00:12:32.400</a></span> | <span class="t">Otherwise, the torch doesn't know where to load the weights in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=756" target="_blank">00:12:36.240</a></span> | <span class="t">And this is the reason I'm trying to keep the same name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=760" target="_blank">00:12:40.080</a></span> | <span class="t">I only changed some names to make them more clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=763" target="_blank">00:12:43.520</a></span> | <span class="t">But most of the other names are the same.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=765" target="_blank">00:12:45.200</a></span> | <span class="t">This is the model list.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=775" target="_blank">00:12:55.360</a></span> | <span class="t">So this is the list of the layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=776" target="_blank">00:12:56.960</a></span> | <span class="t">We will create later the encoder block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=793" target="_blank">00:13:13.680</a></span> | <span class="t">Which is each of these blocks here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=797" target="_blank">00:13:17.760</a></span> | <span class="t">This is the encoder block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=799" target="_blank">00:13:19.120</a></span> | <span class="t">For now, we just create the skeleton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=801" target="_blank">00:13:21.760</a></span> | <span class="t">So we have a list of these blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=803" target="_blank">00:13:23.440</a></span> | <span class="t">Then we have a normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=804" target="_blank">00:13:24.720</a></span> | <span class="t">And the normalization is the rms normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=809" target="_blank">00:13:29.760</a></span> | <span class="t">We will implement it later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=810" target="_blank">00:13:30.960</a></span> | <span class="t">We need to tell him the size of the features.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=815" target="_blank">00:13:35.040</a></span> | <span class="t">And the EPS is a very small number that is needed for the normalization calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=821" target="_blank">00:13:41.120</a></span> | <span class="t">So that we never divide it by zero, basically.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=824" target="_blank">00:13:44.160</a></span> | <span class="t">And then we have the output layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=838" target="_blank">00:13:58.160</a></span> | <span class="t">Okay, then we need to pre-compute the frequencies of the rotary positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=849" target="_blank">00:14:09.280</a></span> | <span class="t">So let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=850" target="_blank">00:14:10.640</a></span> | <span class="t">I created this method and then we go to implement it and I will show you how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=869" target="_blank">00:14:29.840</a></span> | <span class="t">So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=878" target="_blank">00:14:38.160</a></span> | <span class="t">let me check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=895" target="_blank">00:14:55.520</a></span> | <span class="t">I think we have a parenthesis.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=897" target="_blank">00:14:57.440</a></span> | <span class="t">Okay, this is the base transformer model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=900" target="_blank">00:15:00.880</a></span> | <span class="t">So first of all, we have n layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=903" target="_blank">00:15:03.360</a></span> | <span class="t">We have, first of all, the input embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=905" target="_blank">00:15:05.920</a></span> | <span class="t">So we convert the input into embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=909" target="_blank">00:15:09.920</a></span> | <span class="t">Then we pass it through a list of layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=911" target="_blank">00:15:11.920</a></span> | <span class="t">The last layer output is sent to the normalization, then to the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=915" target="_blank">00:15:15.840</a></span> | <span class="t">So this logic will be more clear in the forward method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=922" target="_blank">00:15:22.720</a></span> | <span class="t">Okay, here you will see one thing that is different from the previous transformers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=937" target="_blank">00:15:37.840</a></span> | <span class="t">is that the sequence length that we want is always one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=940" target="_blank">00:15:40.880</a></span> | <span class="t">And this is because we are using the KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=943" target="_blank">00:15:43.920</a></span> | <span class="t">And we will see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=946" target="_blank">00:15:46.480</a></span> | <span class="t">So in the previous, let's review here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=951" target="_blank">00:15:51.360</a></span> | <span class="t">Here, when we give the input, we want to give the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=955" target="_blank">00:15:55.680</a></span> | <span class="t">and the model will give us a softmax of the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=959" target="_blank">00:15:59.200</a></span> | <span class="t">But with the KV cache, we don't need to give all the previous tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=963" target="_blank">00:16:03.360</a></span> | <span class="t">because maybe we already computed them in the previous iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=966" target="_blank">00:16:06.960</a></span> | <span class="t">So we only need to give the latest token and then the model will output the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=971" target="_blank">00:16:11.440</a></span> | <span class="t">While the intermediate tokens, so the cache of the previous tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=974" target="_blank">00:16:14.640</a></span> | <span class="t">will be kept by the model in its cache because here we have a KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=978" target="_blank">00:16:18.000</a></span> | <span class="t">But we will see this mechanism later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=979" target="_blank">00:16:19.520</a></span> | <span class="t">So for now, just remember that the input here that we will get is one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=984" target="_blank">00:16:24.640</a></span> | <span class="t">So we will get a batch with sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=992" target="_blank">00:16:32.800</a></span> | <span class="t">Okay, so batch size, sequence length is tokens.shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1002" target="_blank">00:16:42.560</a></span> | <span class="t">And we make sure that the sequence length is actually one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1005" target="_blank">00:16:45.840</a></span> | <span class="t">And second thing is this model is only good for inferencing, not for training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1019" target="_blank">00:16:59.120</a></span> | <span class="t">Because for training, of course, we need to not have the KV cache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1022" target="_blank">00:17:02.160</a></span> | <span class="t">and we need to be able to process multiple tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1026" target="_blank">00:17:06.000</a></span> | <span class="t">But our goal is actually to use the pre-trained LAMA weights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1039" target="_blank">00:17:19.360</a></span> | <span class="t">So we convert the tokens into embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1043" target="_blank">00:17:23.760</a></span> | <span class="t">As you can see, we add the dim dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1050" target="_blank">00:17:30.160</a></span> | <span class="t">So the dimension of the embeddings, which is 4096 for the base model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1055" target="_blank">00:17:35.520</a></span> | <span class="t">But depending on the model size, it can be different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1068" target="_blank">00:17:48.000</a></span> | <span class="t">Okay, I promise I will explain this line here and this line here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1094" target="_blank">00:18:14.480</a></span> | <span class="t">much more in detail just in two minutes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1096" target="_blank">00:18:16.720</a></span> | <span class="t">Let me finish it and I will explain everything together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1099" target="_blank">00:18:19.360</a></span> | <span class="t">So this is basically, we pre-compute something about the positional encoding</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1104" target="_blank">00:18:24.400</a></span> | <span class="t">that we then give to the successive layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1106" target="_blank">00:18:26.960</a></span> | <span class="t">But let's finish writing it and then I explain this method and this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1113" target="_blank">00:18:33.680</a></span> | <span class="t">And everything will be clear.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1115" target="_blank">00:18:35.040</a></span> | <span class="t">So suppose with this one, we retrieve something that is needed</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1119" target="_blank">00:18:39.440</a></span> | <span class="t">for computing the positional encoding, which then we feed to the next layers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1123" target="_blank">00:18:43.120</a></span> | <span class="t">So we consecutively apply all the encoder blocks.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1141" target="_blank">00:19:01.360</a></span> | <span class="t">And finally, we apply the normalization, just like here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1149" target="_blank">00:19:09.920</a></span> | <span class="t">So we apply these blocks one after another many times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1152" target="_blank">00:19:12.960</a></span> | <span class="t">Then we apply the normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1154" target="_blank">00:19:14.800</a></span> | <span class="t">And then we calculate the output using the linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1158" target="_blank">00:19:18.560</a></span> | <span class="t">And finally, we return the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1163" target="_blank">00:19:23.440</a></span> | <span class="t">So this is the skeleton of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1166" target="_blank">00:19:26.720</a></span> | <span class="t">So we take the input, we convert it into embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1169" target="_blank">00:19:29.600</a></span> | <span class="t">This part I will explain later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1171" target="_blank">00:19:31.920</a></span> | <span class="t">We give this input embeddings with something about the positional encodings</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1176" target="_blank">00:19:36.640</a></span> | <span class="t">to these blocks one after another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1178" target="_blank">00:19:38.400</a></span> | <span class="t">We take the output of the last one and give it to the RMS norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1181" target="_blank">00:19:41.440</a></span> | <span class="t">We take the output of the RMS norm and give it to the linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1184" target="_blank">00:19:44.320</a></span> | <span class="t">And then during the inference, we will apply the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1187" target="_blank">00:19:47.680</a></span> | <span class="t">Now, let's concentrate on the positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1191" target="_blank">00:19:51.760</a></span> | <span class="t">Let's first review how they worked in the original transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1194" target="_blank">00:19:54.800</a></span> | <span class="t">As you remember, in the original transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1197" target="_blank">00:19:57.520</a></span> | <span class="t">so the transformer in the attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1199" target="_blank">00:19:59.920</a></span> | <span class="t">We first take the sentence, we convert into embedding vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1204" target="_blank">00:20:04.320</a></span> | <span class="t">So vectors of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1207" target="_blank">00:20:07.200</a></span> | <span class="t">We then add another vector which has the same size, so 512,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1211" target="_blank">00:20:11.440</a></span> | <span class="t">that represents the position of that token inside the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1215" target="_blank">00:20:15.760</a></span> | <span class="t">So every sentence, every token in the first position of a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1220" target="_blank">00:20:20.400</a></span> | <span class="t">will receive this vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1222" target="_blank">00:20:22.480</a></span> | <span class="t">Every token in the second position of a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1225" target="_blank">00:20:25.600</a></span> | <span class="t">will have this vector added to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1229" target="_blank">00:20:29.200</a></span> | <span class="t">And every token in the third position of a sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1232" target="_blank">00:20:32.160</a></span> | <span class="t">will have this vector added to it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1234" target="_blank">00:20:34.640</a></span> | <span class="t">These vectors are pre-computed because they only depend on the position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1239" target="_blank">00:20:39.440</a></span> | <span class="t">not on the word they are applied to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1242" target="_blank">00:20:42.080</a></span> | <span class="t">And this is why they are called absolute positional encoding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1244" target="_blank">00:20:44.640</a></span> | <span class="t">because they only strictly depend on the position of the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1247" target="_blank">00:20:47.840</a></span> | <span class="t">inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1249" target="_blank">00:20:49.040</a></span> | <span class="t">While in the contrary, in the rotary positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1253" target="_blank">00:20:53.360</a></span> | <span class="t">they are a little different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1254" target="_blank">00:20:54.320</a></span> | <span class="t">Let's go have a look.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1255" target="_blank">00:20:55.440</a></span> | <span class="t">First of all, the rotary positional encodings or embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1259" target="_blank">00:20:59.440</a></span> | <span class="t">they are computed right before the calculation of the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1263" target="_blank">00:21:03.440</a></span> | <span class="t">And they are only applied to the Q and the K matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1267" target="_blank">00:21:07.200</a></span> | <span class="t">not to the V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1267" target="_blank">00:21:07.920</a></span> | <span class="t">Let's see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1269" target="_blank">00:21:09.840</a></span> | <span class="t">The first thing we need to understand is the difference between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1273" target="_blank">00:21:13.120</a></span> | <span class="t">absolute positional encodings and relative ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1275" target="_blank">00:21:15.280</a></span> | <span class="t">The absolute positional encodings are the one we just saw,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1277" target="_blank">00:21:17.680</a></span> | <span class="t">so they deal with one token at a time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1280" target="_blank">00:21:20.240</a></span> | <span class="t">and each token gets its own embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1282" target="_blank">00:21:22.720</a></span> | <span class="t">While the relative positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1284" target="_blank">00:21:24.800</a></span> | <span class="t">they come into play during the calculation of the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1288" target="_blank">00:21:28.240</a></span> | <span class="t">And the calculation of the attention is basically done using the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1293" target="_blank">00:21:33.600</a></span> | <span class="t">Because it's query multiplied by the transpose of the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1296" target="_blank">00:21:36.560</a></span> | <span class="t">divided by the square root of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1298" target="_blank">00:21:38.640</a></span> | <span class="t">So there is a dot product in between.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1300" target="_blank">00:21:40.640</a></span> | <span class="t">And that we want with the relative positional encodings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1303" target="_blank">00:21:43.600</a></span> | <span class="t">we change this dot product so that we introduce a new vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1307" target="_blank">00:21:47.280</a></span> | <span class="t">that indicates the distance between the two tokens involved in the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1312" target="_blank">00:21:52.160</a></span> | <span class="t">So for example, in the original transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1314" target="_blank">00:21:54.720</a></span> | <span class="t">we have this formula here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1317" target="_blank">00:21:57.760</a></span> | <span class="t">So query multiplied by the transpose of the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1320" target="_blank">00:22:00.240</a></span> | <span class="t">divided by the square root of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1322" target="_blank">00:22:02.720</a></span> | <span class="t">While in the relative positional encodings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1324" target="_blank">00:22:04.640</a></span> | <span class="t">which are not the one used in Lama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1326" target="_blank">00:22:06.720</a></span> | <span class="t">so this is just an introduction.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1328" target="_blank">00:22:08.400</a></span> | <span class="t">The relative positional encodings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1330" target="_blank">00:22:10.960</a></span> | <span class="t">we have another vector here that represents the distance between these two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1335" target="_blank">00:22:15.200</a></span> | <span class="t">And we compute the attention mechanism like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1338" target="_blank">00:22:18.240</a></span> | <span class="t">The rotary positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1340" target="_blank">00:22:20.720</a></span> | <span class="t">the one that are used in Lama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1342" target="_blank">00:22:22.560</a></span> | <span class="t">are something in between the absolute and the relative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1346" target="_blank">00:22:26.080</a></span> | <span class="t">Absolute because each token will get its own embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1351" target="_blank">00:22:31.680</a></span> | <span class="t">but relative because the attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1355" target="_blank">00:22:35.360</a></span> | <span class="t">will be evaluated using the relative distance between two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1359" target="_blank">00:22:39.600</a></span> | <span class="t">Let's see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1360" target="_blank">00:22:40.100</a></span> | <span class="t">The rotary positional embeddings were introduced in the paper from this company, JUE.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1370" target="_blank">00:22:50.160</a></span> | <span class="t">And the authors of this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1374" target="_blank">00:22:54.400</a></span> | <span class="t">they wanted to find an inner product that works like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1379" target="_blank">00:22:59.440</a></span> | <span class="t">So first of all, what is an inner product?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1381" target="_blank">00:23:01.920</a></span> | <span class="t">We are all familiar with the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1383" target="_blank">00:23:03.760</a></span> | <span class="t">The inner product can be thought of as a generalization of the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1388" target="_blank">00:23:08.720</a></span> | <span class="t">So it's an operation that has some properties that reflect what is the dot product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1394" target="_blank">00:23:14.960</a></span> | <span class="t">So the authors of this paper wanted to find an inner product</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1399" target="_blank">00:23:19.520</a></span> | <span class="t">between the two vectors, query and key,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1403" target="_blank">00:23:23.040</a></span> | <span class="t">such that they only depend on the...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1407" target="_blank">00:23:27.840</a></span> | <span class="t">So this is the symbol for inner product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1409" target="_blank">00:23:29.680</a></span> | <span class="t">So this inner product only depends on the embedding of the two tokens involved,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1414" target="_blank">00:23:34.240</a></span> | <span class="t">so XM and XN,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1416" target="_blank">00:23:36.560</a></span> | <span class="t">and the relative distance of these two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1420" target="_blank">00:23:40.000</a></span> | <span class="t">So the distance between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1422" target="_blank">00:23:42.320</a></span> | <span class="t">For example, if the first token is in position two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1426" target="_blank">00:23:46.320</a></span> | <span class="t">and the second token is in position five,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1429" target="_blank">00:23:49.120</a></span> | <span class="t">so M equal two, N equal five,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1431" target="_blank">00:23:51.440</a></span> | <span class="t">the distance between two will be three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1433" target="_blank">00:23:53.440</a></span> | <span class="t">or minus three according to the order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1438" target="_blank">00:23:58.000</a></span> | <span class="t">And they wanted to find a dot product that has this property,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1441" target="_blank">00:24:01.680</a></span> | <span class="t">that only depends on the embedding of the first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1443" target="_blank">00:24:03.920</a></span> | <span class="t">on the embedding of the second token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1446" target="_blank">00:24:06.480</a></span> | <span class="t">and the relative distance between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1448" target="_blank">00:24:08.720</a></span> | <span class="t">Then they saw that if this function G is built in this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1453" target="_blank">00:24:13.760</a></span> | <span class="t">then we achieve that objective.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1455" target="_blank">00:24:15.920</a></span> | <span class="t">That is, we take the first token, so the query for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1460" target="_blank">00:24:20.000</a></span> | <span class="t">we multiply it by the W matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1462" target="_blank">00:24:22.880</a></span> | <span class="t">This is actually done also in the vanilla transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1464" target="_blank">00:24:24.960</a></span> | <span class="t">but okay, suppose there is no W matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1467" target="_blank">00:24:27.920</a></span> | <span class="t">We convert it into a complex number in this form,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1471" target="_blank">00:24:31.840</a></span> | <span class="t">we take the key vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1475" target="_blank">00:24:35.360</a></span> | <span class="t">we transform into a complex number into this form,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1478" target="_blank">00:24:38.640</a></span> | <span class="t">and we define the inner product in this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1481" target="_blank">00:24:41.840</a></span> | <span class="t">This inner product will basically depend only on the distance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1487" target="_blank">00:24:47.600</a></span> | <span class="t">between these two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1489" target="_blank">00:24:49.440</a></span> | <span class="t">So they wanted to find an encoding mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1492" target="_blank">00:24:52.560</a></span> | <span class="t">such that the attention mechanism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1494" target="_blank">00:24:54.480</a></span> | <span class="t">which is based on a dot product,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1496" target="_blank">00:24:56.080</a></span> | <span class="t">which is an inner product, behaves like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1499" target="_blank">00:24:59.840</a></span> | <span class="t">So it only depends on the embeddings of the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1502" target="_blank">00:25:02.160</a></span> | <span class="t">and the distance between them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1503" target="_blank">00:25:03.840</a></span> | <span class="t">And if we, for example, this formulation here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1508" target="_blank">00:25:08.640</a></span> | <span class="t">if we apply it on a vector of dimension two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1511" target="_blank">00:25:11.200</a></span> | <span class="t">so we think of embedding with only two dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1514" target="_blank">00:25:14.640</a></span> | <span class="t">it becomes in this form here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1517" target="_blank">00:25:17.200</a></span> | <span class="t">This is due to the Euler's formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1519" target="_blank">00:25:19.920</a></span> | <span class="t">So each complex number, thanks to the Euler's formula,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1522" target="_blank">00:25:22.640</a></span> | <span class="t">can be written as the cosine plus a sine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1528" target="_blank">00:25:28.160</a></span> | <span class="t">And this matrix here reminds us of the rotation matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1535" target="_blank">00:25:35.600</a></span> | <span class="t">Let me give you an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1537" target="_blank">00:25:37.040</a></span> | <span class="t">Suppose our original vector is here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1539" target="_blank">00:25:39.520</a></span> | <span class="t">and if we multiply this vector v zero by this matrix here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1546" target="_blank">00:25:46.480</a></span> | <span class="t">the resulting vector will be rotated by the angle theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1551" target="_blank">00:25:51.040</a></span> | <span class="t">So this is why they're called rotary positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1553" target="_blank">00:25:53.680</a></span> | <span class="t">because the matrix here represents a rotation of the vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1558" target="_blank">00:25:58.320</a></span> | <span class="t">So when we want to visualize</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1561" target="_blank">00:26:01.440</a></span> | <span class="t">how the rotary positional embedding work,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1563" target="_blank">00:26:03.280</a></span> | <span class="t">we have to think that they will map it into a vector space,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1566" target="_blank">00:26:06.640</a></span> | <span class="t">and they will rotate each word to an angle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1570" target="_blank">00:26:10.320</a></span> | <span class="t">that is a multiple of a base angle, so theta,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1573" target="_blank">00:26:13.440</a></span> | <span class="t">and proportional to the theta angle,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1576" target="_blank">00:26:16.800</a></span> | <span class="t">proportional according to its position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1579" target="_blank">00:26:19.120</a></span> | <span class="t">So that two tokens that occupy similar positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1583" target="_blank">00:26:23.680</a></span> | <span class="t">will have similar inclination,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1585" target="_blank">00:26:25.920</a></span> | <span class="t">and the two tokens have different positions</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1588" target="_blank">00:26:28.560</a></span> | <span class="t">will have different inclinations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1590" target="_blank">00:26:30.160</a></span> | <span class="t">And this is the idea behind the rotary positional embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1594" target="_blank">00:26:34.080</a></span> | <span class="t">But how do we actually compute them in the code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1598" target="_blank">00:26:38.640</a></span> | <span class="t">in the PyTorch?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1600" target="_blank">00:26:40.000</a></span> | <span class="t">Well, to compute them, we need to build a matrix like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1604" target="_blank">00:26:44.000</a></span> | <span class="t">And as you can see, this matrix is actually full of zeros.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1609" target="_blank">00:26:49.120</a></span> | <span class="t">And so when we calculate the embedding in this way,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1612" target="_blank">00:26:52.080</a></span> | <span class="t">we will do, if we do a matrix multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1614" target="_blank">00:26:54.400</a></span> | <span class="t">we will be doing a lot of operations that are useless,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1617" target="_blank">00:26:57.120</a></span> | <span class="t">because most of these items are zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1619" target="_blank">00:26:59.680</a></span> | <span class="t">So the authors of the paper proposed another form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1622" target="_blank">00:27:02.480</a></span> | <span class="t">that is more computationally efficient.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1624" target="_blank">00:27:04.400</a></span> | <span class="t">And this form basically says that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1626" target="_blank">00:27:06.960</a></span> | <span class="t">we take the embedding of the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1629" target="_blank">00:27:09.360</a></span> | <span class="t">to which we want to apply the positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1632" target="_blank">00:27:12.000</a></span> | <span class="t">So for example, this one, this is a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1634" target="_blank">00:27:14.400</a></span> | <span class="t">So the first dimension, the second dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1636" target="_blank">00:27:16.160</a></span> | <span class="t">the third dimension, and the last dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1638" target="_blank">00:27:18.480</a></span> | <span class="t">So if this is, for example, the vanilla transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1640" target="_blank">00:27:20.560</a></span> | <span class="t">this would be XD, which should be 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1644" target="_blank">00:27:24.320</a></span> | <span class="t">We multiply it element-wise by this matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1649" target="_blank">00:27:29.040</a></span> | <span class="t">plus another vector that is actually based on this vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1654" target="_blank">00:27:34.160</a></span> | <span class="t">but with the positions and the designs changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1657" target="_blank">00:27:37.040</a></span> | <span class="t">So this is actually in the first position,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1658" target="_blank">00:27:38.800</a></span> | <span class="t">we have the second dimension with its sign change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1661" target="_blank">00:27:41.680</a></span> | <span class="t">The second position, we have actually the first dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1664" target="_blank">00:27:44.560</a></span> | <span class="t">In the third position, we have the fourth dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1667" target="_blank">00:27:47.120</a></span> | <span class="t">but with sign change, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1668" target="_blank">00:27:48.400</a></span> | <span class="t">So it depends only on the embedding of the word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1671" target="_blank">00:27:51.760</a></span> | <span class="t">but it changes with the signs and position change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1675" target="_blank">00:27:55.840</a></span> | <span class="t">And then we multiply this element-wise</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1678" target="_blank">00:27:58.720</a></span> | <span class="t">with another matrix that you can see here, this vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1682" target="_blank">00:28:02.160</a></span> | <span class="t">Then this will be the encoding of the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1686" target="_blank">00:28:06.320</a></span> | <span class="t">we are talking about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1687" target="_blank">00:28:07.920</a></span> | <span class="t">And now what we can pre-compute is this matrix here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1694" target="_blank">00:28:14.000</a></span> | <span class="t">because it doesn't depend on the token to which we apply it to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1697" target="_blank">00:28:17.040</a></span> | <span class="t">and this matrix here, because it doesn't depend</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1699" target="_blank">00:28:19.040</a></span> | <span class="t">on the token we apply it to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1700" target="_blank">00:28:20.480</a></span> | <span class="t">And they depend on m, so it's the position of the word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1705" target="_blank">00:28:25.360</a></span> | <span class="t">and theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1706" target="_blank">00:28:26.400</a></span> | <span class="t">What is theta?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1707" target="_blank">00:28:27.520</a></span> | <span class="t">Theta is a series of numbers defined like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1711" target="_blank">00:28:31.280</a></span> | <span class="t">And so let's first build the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1715" target="_blank">00:28:35.840</a></span> | <span class="t">to pre-compute this and this here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1719" target="_blank">00:28:39.200</a></span> | <span class="t">Let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1720" target="_blank">00:28:40.400</a></span> | <span class="t">I will first write the code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1722" target="_blank">00:28:42.480</a></span> | <span class="t">and later I will show you how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1752" target="_blank">00:29:12.000</a></span> | <span class="t">This theta parameter, 10,000, comes from the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1754" target="_blank">00:29:14.640</a></span> | <span class="t">It's written here, 10,000.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1757" target="_blank">00:29:17.200</a></span> | <span class="t">We first need to make sure that the dimension of the word</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1765" target="_blank">00:29:25.280</a></span> | <span class="t">to which we are applying the embedding is actually even,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1768" target="_blank">00:29:28.240</a></span> | <span class="t">because in the paper it's written</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1770" target="_blank">00:29:30.000</a></span> | <span class="t">that this rotary positional encoding cannot be applied</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1773" target="_blank">00:29:33.520</a></span> | <span class="t">to an embedding which has an odd dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1778" target="_blank">00:29:38.320</a></span> | <span class="t">So it cannot be 513, it must be 512 or 514</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1782" target="_blank">00:29:42.400</a></span> | <span class="t">or any other even number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1783" target="_blank">00:29:43.760</a></span> | <span class="t">And this is as written in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1794" target="_blank">00:29:54.240</a></span> | <span class="t">Even, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1802" target="_blank">00:30:02.960</a></span> | <span class="t">Now we build the theta parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1804" target="_blank">00:30:04.800</a></span> | <span class="t">which is a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1813" target="_blank">00:30:13.120</a></span> | <span class="t">And the shape of this theta will be head dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1824" target="_blank">00:30:24.080</a></span> | <span class="t">Because we will apply these embeddings to each head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1827" target="_blank">00:30:27.840</a></span> | <span class="t">so not right after the embedding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1830" target="_blank">00:30:30.000</a></span> | <span class="t">but after we have split them into multi-head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1832" target="_blank">00:30:32.880</a></span> | <span class="t">so each token, the token of each head,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1835" target="_blank">00:30:35.600</a></span> | <span class="t">we check the size of the dimension of each head</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1840" target="_blank">00:30:40.880</a></span> | <span class="t">and we divide it by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1842" target="_blank">00:30:42.000</a></span> | <span class="t">Because, why divide it by 2?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1843" target="_blank">00:30:43.680</a></span> | <span class="t">Because in the paper they also divide it by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1846" target="_blank">00:30:46.480</a></span> | <span class="t">So D divided by 2 here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1853" target="_blank">00:30:53.200</a></span> | <span class="t">Okay, so what's the formula here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1861" target="_blank">00:31:01.760</a></span> | <span class="t">The formula is theta of i is equal to 10,000</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1867" target="_blank">00:31:07.360</a></span> | <span class="t">to the power of minus 2 multiplied by i minus 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1873" target="_blank">00:31:13.520</a></span> | <span class="t">divided by dimension for i equal to 1, 2, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1880" target="_blank">00:31:20.400</a></span> | <span class="t">up to dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1883" target="_blank">00:31:23.200</a></span> | <span class="t">So now we are computing this part here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1887" target="_blank">00:31:27.280</a></span> | <span class="t">So this part here, which is a series.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1891" target="_blank">00:31:31.760</a></span> | <span class="t">So i, here it starts from 1, we will start from 0,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1894" target="_blank">00:31:34.800</a></span> | <span class="t">so we don't have to do i minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1896" target="_blank">00:31:36.880</a></span> | <span class="t">And theta is equal to 1 over the theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1904" target="_blank">00:31:44.640</a></span> | <span class="t">So 10,000 to the power of theta numerator</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1909" target="_blank">00:31:49.440</a></span> | <span class="t">divided by head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1916" target="_blank">00:31:56.080</a></span> | <span class="t">Why do we do 1 over theta?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1918" target="_blank">00:31:58.080</a></span> | <span class="t">Well, because this is to the power of minus 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1921" target="_blank">00:32:01.600</a></span> | <span class="t">So something to the power of a negative number is 1 over</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1925" target="_blank">00:32:05.920</a></span> | <span class="t">that something to the power of the positive exponent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1929" target="_blank">00:32:09.680</a></span> | <span class="t">And then this will result in a matrix with shape,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1937" target="_blank">00:32:17.680</a></span> | <span class="t">head dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1940" target="_blank">00:32:20.480</a></span> | <span class="t">So shape is head dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1946" target="_blank">00:32:26.240</a></span> | <span class="t">Now we construct the positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1948" target="_blank">00:32:28.000</a></span> | <span class="t">So what are the positions?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1949" target="_blank">00:32:29.360</a></span> | <span class="t">Because we want to build these two matrices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1951" target="_blank">00:32:31.600</a></span> | <span class="t">they depend on theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1952" target="_blank">00:32:32.960</a></span> | <span class="t">So the series of theta that goes from theta 1</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1955" target="_blank">00:32:35.520</a></span> | <span class="t">to theta dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1957" target="_blank">00:32:37.840</a></span> | <span class="t">And that we already have.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1959" target="_blank">00:32:39.440</a></span> | <span class="t">Now we need to build the m's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1961" target="_blank">00:32:41.040</a></span> | <span class="t">Because the m's, the possible positions of a token can be many.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1964" target="_blank">00:32:44.720</a></span> | <span class="t">We basically give as input to this function</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1968" target="_blank">00:32:48.640</a></span> | <span class="t">the maximum sequence length that we can afford</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1972" target="_blank">00:32:52.880</a></span> | <span class="t">multiplied by 2, because we have also the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1976" target="_blank">00:32:56.480</a></span> | <span class="t">which may be long.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1977" target="_blank">00:32:57.600</a></span> | <span class="t">So we say, OK, let's pre-compute all the possible theta and m</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1982" target="_blank">00:33:02.560</a></span> | <span class="t">for all the possible positions that our model will see.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1985" target="_blank">00:33:05.840</a></span> | <span class="t">And all the possible positions is given by this parameter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1989" target="_blank">00:33:09.040</a></span> | <span class="t">sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=1989" target="_blank">00:33:09.840</a></span> | <span class="t">So now construct the positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2000" target="_blank">00:33:20.480</a></span> | <span class="t">And the shape is sequence length, which is m.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2004" target="_blank">00:33:24.880</a></span> | <span class="t">Now we need to multiply m by all the sequence of thetas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2017" target="_blank">00:33:37.760</a></span> | <span class="t">But each m with all the thetas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2021" target="_blank">00:33:41.120</a></span> | <span class="t">So for example, if we have m equal to 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2023" target="_blank">00:33:43.200</a></span> | <span class="t">we need m1 theta 1, m1 theta 2, m1 theta d divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2029" target="_blank">00:33:49.360</a></span> | <span class="t">Then we need m2 theta 1, m2 theta 2, m2 theta 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2032" target="_blank">00:33:52.800</a></span> | <span class="t">So for that, we will use a outer product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2035" target="_blank">00:33:55.440</a></span> | <span class="t">The outer product, I will show you later,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2038" target="_blank">00:33:58.400</a></span> | <span class="t">basically means multiply all the elements of the first vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2043" target="_blank">00:34:03.680</a></span> | <span class="t">with all the elements of the second vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2045" target="_blank">00:34:05.680</a></span> | <span class="t">all the possible combinations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2048" target="_blank">00:34:08.080</a></span> | <span class="t">So for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2059" target="_blank">00:34:19.120</a></span> | <span class="t">here we have a frequency is equal to torch outer product m and theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2069" target="_blank">00:34:29.680</a></span> | <span class="t">OK, so what we are doing is we are doing the outer product within m,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2076" target="_blank">00:34:36.080</a></span> | <span class="t">which is the positions, multiplied by theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2078" target="_blank">00:34:38.400</a></span> | <span class="t">This will basically take the first element of the first vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2082" target="_blank">00:34:42.080</a></span> | <span class="t">and multiply with all the elements of the second vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2085" target="_blank">00:34:45.280</a></span> | <span class="t">Then take the second element of the first vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2087" target="_blank">00:34:47.520</a></span> | <span class="t">and multiply it with all the elements of the second vector, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2090" target="_blank">00:34:50.720</a></span> | <span class="t">So if we start with a shape, let me say shape of m is sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2098" target="_blank">00:34:58.960</a></span> | <span class="t">Let's say outer product with head dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2108" target="_blank">00:35:08.560</a></span> | <span class="t">This will result in a tensor of sequence length by head dimension divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2116" target="_blank">00:35:16.000</a></span> | <span class="t">So for each position, we will have all the theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2118" target="_blank">00:35:18.160</a></span> | <span class="t">Then for the second position, we will have all the theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2120" target="_blank">00:35:20.400</a></span> | <span class="t">For the third position, we will have all the theta, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2123" target="_blank">00:35:23.840</a></span> | <span class="t">Now, we want to write these numbers into a complex form, and I will show you why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2144" target="_blank">00:35:44.640</a></span> | <span class="t">I multiplied by m multiplied by theta, where r is equal to 1, as follows.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2156" target="_blank">00:35:56.720</a></span> | <span class="t">So we compute.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2165" target="_blank">00:36:05.280</a></span> | <span class="t">Let me also write the shape, and then I'll explain to you how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2180" target="_blank">00:36:20.320</a></span> | <span class="t">This is here, too.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2183" target="_blank">00:36:23.360</a></span> | <span class="t">Okay, let's write some formulas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2190" target="_blank">00:36:30.880</a></span> | <span class="t">I could also, you know, not explain all the proofs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2194" target="_blank">00:36:34.320</a></span> | <span class="t">So I know the next few minutes will be a little boring</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2197" target="_blank">00:36:37.280</a></span> | <span class="t">because I will be explaining all the math behind it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2199" target="_blank">00:36:39.280</a></span> | <span class="t">But of course, I don't think you, just like me,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2202" target="_blank">00:36:42.640</a></span> | <span class="t">you like to watch just some code and say, "Okay, this is how it's done."</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2207" target="_blank">00:36:47.280</a></span> | <span class="t">No, I like to actually give a motivation behind every operation we do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2211" target="_blank">00:36:51.840</a></span> | <span class="t">and that's, I think, one of the reasons you are watching this video</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2214" target="_blank">00:36:54.080</a></span> | <span class="t">and not just reading the code from the beta repository.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2217" target="_blank">00:36:57.200</a></span> | <span class="t">So let's do some math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2220" target="_blank">00:37:00.880</a></span> | <span class="t">The first thing we need to review is how complex numbers work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2226" target="_blank">00:37:06.720</a></span> | <span class="t">Okay, a complex number is a number in the form a plus i multiplied by b,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2234" target="_blank">00:37:14.240</a></span> | <span class="t">where a is called the real part and b is called the imaginary part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2239" target="_blank">00:37:19.040</a></span> | <span class="t">And i is a number such that i to the power of 2 is equal to minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2245" target="_blank">00:37:25.680</a></span> | <span class="t">So the complex numbers were introduced to represent all the numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2250" target="_blank">00:37:30.080</a></span> | <span class="t">that involve somehow the square root of a negative number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2253" target="_blank">00:37:33.200</a></span> | <span class="t">As you know from school, the square root of a negative number cannot be calculated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2257" target="_blank">00:37:37.280</a></span> | <span class="t">but so that's why we introduce this constant i,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2259" target="_blank">00:37:39.920</a></span> | <span class="t">which is the negative number, which is the square root of minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2263" target="_blank">00:37:43.520</a></span> | <span class="t">And so we can represent the square root of negative numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2266" target="_blank">00:37:46.160</a></span> | <span class="t">And they can also be helpful in vector calculations, and we will see how.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2272" target="_blank">00:37:52.400</a></span> | <span class="t">Because we have the Euler's formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2277" target="_blank">00:37:57.280</a></span> | <span class="t">The Euler's formula says that e to the power of i multiplied by x</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2282" target="_blank">00:38:02.480</a></span> | <span class="t">is equal to cosine of x plus i multiplied by sine of x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2291" target="_blank">00:38:11.600</a></span> | <span class="t">So it allows us to represent a complex number in the exponential form</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2296" target="_blank">00:38:16.960</a></span> | <span class="t">into a sum of two trigonometric functions, the cosine and the sine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2302" target="_blank">00:38:22.000</a></span> | <span class="t">And this will be very helpful later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2304" target="_blank">00:38:24.160</a></span> | <span class="t">Because our goal is to calculate these matrices here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2311" target="_blank">00:38:31.920</a></span> | <span class="t">the cosine of m theta and the sine of m theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2315" target="_blank">00:38:35.520</a></span> | <span class="t">And the first thing we did is we calculated all the theta one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2320" target="_blank">00:38:40.400</a></span> | <span class="t">then we calculated all the positions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2322" target="_blank">00:38:42.160</a></span> | <span class="t">then we calculated all the possible combinations of positions and thetas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2327" target="_blank">00:38:47.360</a></span> | <span class="t">So what we did is we calculated a vector that represents the theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2333" target="_blank">00:38:53.760</a></span> | <span class="t">So theta 1, theta 2, up to theta d divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2342" target="_blank">00:39:02.320</a></span> | <span class="t">Then we calculated all the possible m's.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2344" target="_blank">00:39:04.720</a></span> | <span class="t">m can be 1, can be 2, can be whatever.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2349" target="_blank">00:39:09.360</a></span> | <span class="t">So sequence length, let's say.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2350" target="_blank">00:39:10.800</a></span> | <span class="t">Then we calculated the product of each of them for all the possible thetas.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2359" target="_blank">00:39:19.360</a></span> | <span class="t">So for example, we created a new matrix that has m1 theta 1, m1 theta 2, m1 theta 3,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2370" target="_blank">00:39:30.880</a></span> | <span class="t">up to m1 theta d divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2379" target="_blank">00:39:39.360</a></span> | <span class="t">And then m2 theta 1, m2 theta 2, m2 theta 3, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2389" target="_blank">00:39:49.920</a></span> | <span class="t">Until m2 theta d divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2394" target="_blank">00:39:54.640</a></span> | <span class="t">These numbers are still not complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2399" target="_blank">00:39:59.120</a></span> | <span class="t">They're just real numbers because theta is a real number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2401" target="_blank">00:40:01.760</a></span> | <span class="t">m is a real number, but they are not complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2404" target="_blank">00:40:04.000</a></span> | <span class="t">Then we convert them into complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2406" target="_blank">00:40:06.160</a></span> | <span class="t">So what we do with the last operation here, this one here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2409" target="_blank">00:40:09.680</a></span> | <span class="t">we convert each of these numbers into polar, into its polar form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2414" target="_blank">00:40:14.560</a></span> | <span class="t">A number in polar form is a number that can be written as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2418" target="_blank">00:40:18.080</a></span> | <span class="t">r multiplied by e to the power of i theta,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2422" target="_blank">00:40:22.480</a></span> | <span class="t">which can be written as r cosine of theta plus i sine of theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2429" target="_blank">00:40:29.360</a></span> | <span class="t">Why? Because it can be represented in the graphical, let's say, graphical plane xy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2438" target="_blank">00:40:38.640</a></span> | <span class="t">As you know, complex numbers can be represented into the 2D plane xy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2445" target="_blank">00:40:45.120</a></span> | <span class="t">where the real part is on the x and the imaginary part is on the y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2450" target="_blank">00:40:50.640</a></span> | <span class="t">So we are actually representing a vector of size, let's say, r with an inclination of theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2458" target="_blank">00:40:58.640</a></span> | <span class="t">Because, as you know, the projection of this vector on the real part is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2463" target="_blank">00:41:03.200</a></span> | <span class="t">r cos theta plus i, the projection on the y-axis is sine of theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2471" target="_blank">00:41:11.200</a></span> | <span class="t">And here I forgot r, yeah, I've forgotten r here, r sine of theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2478" target="_blank">00:41:18.240</a></span> | <span class="t">So this is another way of representing complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2481" target="_blank">00:41:21.760</a></span> | <span class="t">And what we are doing is we are calculating this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2485" target="_blank">00:41:25.120</a></span> | <span class="t">and then converting all these numbers into their complex form.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2488" target="_blank">00:41:28.800</a></span> | <span class="t">So we are converting it into another matrix that has r equal to 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2494" target="_blank">00:41:34.000</a></span> | <span class="t">And this number here, for example, this item here will become</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2500" target="_blank">00:41:40.480</a></span> | <span class="t">cosine of m1 theta 1 plus i sine of m1 theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2511" target="_blank">00:41:51.440</a></span> | <span class="t">This number here will become another number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2515" target="_blank">00:41:55.280</a></span> | <span class="t">So this is only one number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2517" target="_blank">00:41:57.920</a></span> | <span class="t">This has become another complex number that is the cosine of m1 theta 2 plus i sine of m1 theta 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2531" target="_blank">00:42:11.040</a></span> | <span class="t">Etc, etc, for all that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2532" target="_blank">00:42:12.560</a></span> | <span class="t">Because we are not increasing the numbers, the total numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2537" target="_blank">00:42:17.040</a></span> | <span class="t">this shape of the tensor also doesn't change.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2539" target="_blank">00:42:19.440</a></span> | <span class="t">It just becomes a more complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2541" target="_blank">00:42:21.200</a></span> | <span class="t">So instead of having m theta 1, it becomes cosine of m theta 1 plus i m theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2547" target="_blank">00:42:27.440</a></span> | <span class="t">Why do we need this form here?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2549" target="_blank">00:42:29.200</a></span> | <span class="t">Because we need sines and cosines.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2553" target="_blank">00:42:33.520</a></span> | <span class="t">And later we will see how we will use them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2556" target="_blank">00:42:36.320</a></span> | <span class="t">Now, the point is, imagine we are given a vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2560" target="_blank">00:42:40.560</a></span> | <span class="t">because we want to apply these positional encodings to a vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2563" target="_blank">00:42:43.600</a></span> | <span class="t">So how to apply them?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2565" target="_blank">00:42:45.600</a></span> | <span class="t">Because the vector will be given us as a list of dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2570" target="_blank">00:42:50.080</a></span> | <span class="t">from x1 to the last dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2572" target="_blank">00:42:52.080</a></span> | <span class="t">Just like in the original transformer, we have a vector of size 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2576" target="_blank">00:42:56.560</a></span> | <span class="t">In this case, it will be much smaller because it's the dimension of each head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2580" target="_blank">00:43:00.640</a></span> | <span class="t">And as you remember, each head doesn't watch the full dimension of the embedding vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2586" target="_blank">00:43:06.080</a></span> | <span class="t">but a part of it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2587" target="_blank">00:43:07.840</a></span> | <span class="t">So, but for us, okay, imagine it's only one head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2590" target="_blank">00:43:10.480</a></span> | <span class="t">So if it's only one head, we will watch the full dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2593" target="_blank">00:43:13.040</a></span> | <span class="t">So for now, don't consider the multi-head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2595" target="_blank">00:43:15.840</a></span> | <span class="t">Just suppose that we only are working with one head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2598" target="_blank">00:43:18.240</a></span> | <span class="t">So imagine we have a token with its full dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2601" target="_blank">00:43:21.520</a></span> | <span class="t">So in the case of Lama, 4096 dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2605" target="_blank">00:43:25.360</a></span> | <span class="t">And in the case of the vanilla transformer, 512.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2608" target="_blank">00:43:28.560</a></span> | <span class="t">How to apply it?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2609" target="_blank">00:43:29.440</a></span> | <span class="t">Let's do some math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2611" target="_blank">00:43:31.200</a></span> | <span class="t">Actually, let's do some more math.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2614" target="_blank">00:43:34.560</a></span> | <span class="t">So we are given,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2615" target="_blank">00:43:35.920</a></span> | <span class="t">suppose a smaller embedding vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2622" target="_blank">00:43:42.720</a></span> | <span class="t">because we want to do the calculation and not go crazy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2626" target="_blank">00:43:46.880</a></span> | <span class="t">Otherwise, 4096 is a little difficult to prove.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2630" target="_blank">00:43:50.800</a></span> | <span class="t">I want to make a list of operations on this vector until we arrive to this form here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2639" target="_blank">00:43:59.840</a></span> | <span class="t">So let's start.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2641" target="_blank">00:44:01.440</a></span> | <span class="t">Suppose our embedding vector is only made of four dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2644" target="_blank">00:44:04.640</a></span> | <span class="t">X1, X2, X3, and X4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2650" target="_blank">00:44:10.880</a></span> | <span class="t">Okay, the first thing we do is I will do some transformations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2659" target="_blank">00:44:19.360</a></span> | <span class="t">and I will later translate them into code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2661" target="_blank">00:44:21.680</a></span> | <span class="t">So for now, just follow the transformations I'm doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2664" target="_blank">00:44:24.000</a></span> | <span class="t">This is the transformation number one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2667" target="_blank">00:44:27.760</a></span> | <span class="t">I want to group successive tokens, successive dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2672" target="_blank">00:44:32.640</a></span> | <span class="t">So into another dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2675" target="_blank">00:44:35.920</a></span> | <span class="t">So X1 and X2 become another dimension in this tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2681" target="_blank">00:44:41.040</a></span> | <span class="t">And X3 and X4 become, oops, very badly written.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2686" target="_blank">00:44:46.880</a></span> | <span class="t">X4 become another dimension in this tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2692" target="_blank">00:44:52.480</a></span> | <span class="t">The total number of items is still four, but I added another dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2697" target="_blank">00:44:57.360</a></span> | <span class="t">And this has size four by one, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2704" target="_blank">00:45:04.240</a></span> | <span class="t">This one has two by two by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2708" target="_blank">00:45:08.000</a></span> | <span class="t">So I split it into multiple tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2711" target="_blank">00:45:11.440</a></span> | <span class="t">And okay, now this next thing I do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2716" target="_blank">00:45:16.080</a></span> | <span class="t">I consider this first number of this part to be the real part of the complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2722" target="_blank">00:45:22.640</a></span> | <span class="t">And this one to be the imaginary part of the complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2726" target="_blank">00:45:26.000</a></span> | <span class="t">And the same for the second vector here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2728" target="_blank">00:45:28.080</a></span> | <span class="t">So I do another transformation that we will call two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2732" target="_blank">00:45:32.480</a></span> | <span class="t">in which X1 plus IX2, and then X3 plus IX2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2745" target="_blank">00:45:45.520</a></span> | <span class="t">This vector has less items because now two numbers became one complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2754" target="_blank">00:45:54.160</a></span> | <span class="t">Now I multiply this element wise with the vector that we pre-computed before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2761" target="_blank">00:46:01.360</a></span> | <span class="t">As you remember before, we pre-computed this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2765" target="_blank">00:46:05.440</a></span> | <span class="t">Cosine of M1 theta 1 plus I of M1 theta 1, cosine of M1 theta 2 plus I.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2772" target="_blank">00:46:12.560</a></span> | <span class="t">Because they suppose this position, this token here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2776" target="_blank">00:46:16.960</a></span> | <span class="t">suppose his position is M1, because we need also the M.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2781" target="_blank">00:46:21.520</a></span> | <span class="t">So suppose this token here, his position is M1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2785" target="_blank">00:46:25.600</a></span> | <span class="t">So we take all this row here, M1, and this will become our new matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2794" target="_blank">00:46:34.160</a></span> | <span class="t">We only have four dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2797" target="_blank">00:46:37.680</a></span> | <span class="t">So four dimensions means we have a theta 1 and theta 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2802" target="_blank">00:46:42.000</a></span> | <span class="t">Because D divided by 2 until D divided by 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2804" target="_blank">00:46:44.960</a></span> | <span class="t">So element wise with the cosine of M1 theta 1 plus I sine of M1 theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2827" target="_blank">00:47:07.840</a></span> | <span class="t">And then we have cosine of M1 theta 2 plus I of sine of M1 theta 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2843" target="_blank">00:47:23.520</a></span> | <span class="t">Now we have an element wise product between the first item of this matrix</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2851" target="_blank">00:47:31.200</a></span> | <span class="t">and the first item of this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2853" target="_blank">00:47:33.440</a></span> | <span class="t">Actually they are two vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2855" target="_blank">00:47:35.200</a></span> | <span class="t">And then we have the product of two complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2859" target="_blank">00:47:39.360</a></span> | <span class="t">This complex number here and this complex number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2862" target="_blank">00:47:42.960</a></span> | <span class="t">So let's see how to compute the product of two complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2867" target="_blank">00:47:47.520</a></span> | <span class="t">Because I don't want to write very long expressions, I will call this one F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2874" target="_blank">00:47:54.320</a></span> | <span class="t">So F1 is the cosine of M1 theta 1 and F2 is the sine of M1 theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2884" target="_blank">00:48:04.240</a></span> | <span class="t">And for the same reason I will call this one F3 and F4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2888" target="_blank">00:48:08.720</a></span> | <span class="t">Now let's compute the product of the first item of this vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2897" target="_blank">00:48:17.840</a></span> | <span class="t">and the first item of this vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2899" target="_blank">00:48:19.440</a></span> | <span class="t">So X1 plus IX2 multiplied by F1 plus IF2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2913" target="_blank">00:48:33.840</a></span> | <span class="t">This is equal to X1 F1 plus IX1 F2 plus IX2 F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2940" target="_blank">00:49:00.800</a></span> | <span class="t">Then we have this product IX2 multiplied by IX2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2945" target="_blank">00:49:05.040</a></span> | <span class="t">But it will become I squared X2 F2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2948" target="_blank">00:49:08.720</a></span> | <span class="t">I squared we know it's equal to minus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2951" target="_blank">00:49:11.040</a></span> | <span class="t">So it will become minus X2 F2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2956" target="_blank">00:49:16.240</a></span> | <span class="t">This one can then be written as real part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2960" target="_blank">00:49:20.160</a></span> | <span class="t">So all the terms that don't have I, X1 F1 minus X2 F2 plus I that multiplies X1 F2 plus X2 F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2985" target="_blank">00:49:45.760</a></span> | <span class="t">Okay, this is how to compute the product of two complex numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2990" target="_blank">00:49:50.160</a></span> | <span class="t">Let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2991" target="_blank">00:49:51.120</a></span> | <span class="t">So the first number here in the resulting matrix from this element-wise multiplication will be</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=2998" target="_blank">00:49:58.400</a></span> | <span class="t">X1 F1 minus X2 F2 plus I of X1 F2 plus X2 F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3025" target="_blank">00:50:25.680</a></span> | <span class="t">The second element, we don't need to do this multiplication</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3029" target="_blank">00:50:29.120</a></span> | <span class="t">because they have the similar structure as the first one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3031" target="_blank">00:50:31.600</a></span> | <span class="t">So we just change the X1 with X3, X2 with X4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3035" target="_blank">00:50:35.280</a></span> | <span class="t">This is X4 and F1 with F3 and F2 with F4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3043" target="_blank">00:50:43.120</a></span> | <span class="t">So the resulting matrix will be X3 F3 minus X4 F4 plus I X3 F4 plus X4 F3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3071" target="_blank">00:51:11.840</a></span> | <span class="t">This one can then be split back.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3075" target="_blank">00:51:15.920</a></span> | <span class="t">So this complex number, we can split the real part and the imaginary part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3079" target="_blank">00:51:19.920</a></span> | <span class="t">And this we will call it transformation number three.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3083" target="_blank">00:51:23.680</a></span> | <span class="t">So we can split it in a tensor of two dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3090" target="_blank">00:51:30.400</a></span> | <span class="t">One is the real part and one is the complex part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3093" target="_blank">00:51:33.200</a></span> | <span class="t">Where is X1 F1 minus X2 F2 then X1 F2 plus X2 F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3116" target="_blank">00:51:56.240</a></span> | <span class="t">This is the first tensor. The second tensor will be X3 F3 minus X4 F4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3128" target="_blank">00:52:08.400</a></span> | <span class="t">And the second will be X3 F4 plus X4 F3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3140" target="_blank">00:52:20.960</a></span> | <span class="t">I'm really sorry for the bad handwriting, but it's my touchpad is not so good.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3146" target="_blank">00:52:26.320</a></span> | <span class="t">Then we do another transformation in which we flatten all these values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3152" target="_blank">00:52:32.480</a></span> | <span class="t">So this will be the first item.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3155" target="_blank">00:52:35.600</a></span> | <span class="t">This is the second, the third and the fourth.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3157" target="_blank">00:52:37.440</a></span> | <span class="t">So we remove this dimension, the inner dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3160" target="_blank">00:52:40.720</a></span> | <span class="t">So we flatten this matrix and it will become X1 F1 minus X2 F2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3171" target="_blank">00:52:51.760</a></span> | <span class="t">The second items will be X1 F2 plus X2 F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3191" target="_blank">00:53:11.200</a></span> | <span class="t">Then we have X3 F3 minus X4 F4 then we have X3 F4 plus X4 F3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3212" target="_blank">00:53:32.240</a></span> | <span class="t">Let's compare this resulting matrix with what is in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3217" target="_blank">00:53:37.840</a></span> | <span class="t">So let's compare it with this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3222" target="_blank">00:53:42.800</a></span> | <span class="t">Let me zoom a little bit.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3226" target="_blank">00:53:46.160</a></span> | <span class="t">OK, the resulting matrix is exactly the same as what is in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3233" target="_blank">00:53:53.040</a></span> | <span class="t">So X1 multiplied by F1, which is the cosine, as you remember, M1 theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3240" target="_blank">00:54:00.160</a></span> | <span class="t">So X1 multiplied by F1 plus minus X2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3246" target="_blank">00:54:06.160</a></span> | <span class="t">So minus X2 multiplied by F2, which is the sine, as you can see here, sine of M1 theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3253" target="_blank">00:54:13.120</a></span> | <span class="t">Here it's not M1 because it's for the generic M, but we set M equal to M1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3258" target="_blank">00:54:18.080</a></span> | <span class="t">And the second dimension is also correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3261" target="_blank">00:54:21.600</a></span> | <span class="t">So it's X1 F2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3263" target="_blank">00:54:23.200</a></span> | <span class="t">So X1 with X1 here, because here is the sum, so the order doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3269" target="_blank">00:54:29.520</a></span> | <span class="t">So X1 F2, so X1 multiplied by the sine plus X2 F1, X2 F1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3275" target="_blank">00:54:35.680</a></span> | <span class="t">And let me check if we can use...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3279" target="_blank">00:54:39.840</a></span> | <span class="t">OK, the third dimension is X3 F3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3284" target="_blank">00:54:44.800</a></span> | <span class="t">So X3 multiplied by the cosine minus X4 sine minus X4 sine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3290" target="_blank">00:54:50.960</a></span> | <span class="t">Then we have X3 F4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3293" target="_blank">00:54:53.520</a></span> | <span class="t">So X3 F4, F4 is the sine of theta 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3297" target="_blank">00:54:57.360</a></span> | <span class="t">And plus X4 F3, X4 F3, F3 is the cosine of theta 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3304" target="_blank">00:55:04.160</a></span> | <span class="t">Also in this case, because we have the sum here inside, the order doesn't matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3310" target="_blank">00:55:10.160</a></span> | <span class="t">So as you can see, we started with a vector of dimension 4, but it could be of dimension N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3317" target="_blank">00:55:17.280</a></span> | <span class="t">And we did some transformation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3320" target="_blank">00:55:20.480</a></span> | <span class="t">We then multiplied with the matrix that we pre-computed here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3323" target="_blank">00:55:23.760</a></span> | <span class="t">Then we did some other transformation, and the end result is exactly as doing this operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3329" target="_blank">00:55:29.920</a></span> | <span class="t">So now let's translate this into code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3332" target="_blank">00:55:32.720</a></span> | <span class="t">Because this is actually what we need to apply the embedding vector to this vector here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3338" target="_blank">00:55:38.560</a></span> | <span class="t">So to this token, how to apply the embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3340" target="_blank">00:55:40.960</a></span> | <span class="t">the rotary position embeddings through this series of transformations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3344" target="_blank">00:55:44.320</a></span> | <span class="t">So I could have also written the code and not tell you anything,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3347" target="_blank">00:55:47.600</a></span> | <span class="t">but I like to give proof to what I do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3350" target="_blank">00:55:50.720</a></span> | <span class="t">So that you know that what I'm doing is actually described in the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3356" target="_blank">00:55:56.480</a></span> | <span class="t">and we are actually doing it according to the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3359" target="_blank">00:55:59.280</a></span> | <span class="t">There is also a visualization in the paper that is really helpful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3362" target="_blank">00:56:02.560</a></span> | <span class="t">So what we did here, for example, is we transform the embedding vector into,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3369" target="_blank">00:56:09.200</a></span> | <span class="t">split it into a new tensor, which has half dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3375" target="_blank">00:56:15.040</a></span> | <span class="t">But by grouping two consecutive dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3378" target="_blank">00:56:18.160</a></span> | <span class="t">So the two consecutive dimension x1 and x2 and x3 and x4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3382" target="_blank">00:56:22.160</a></span> | <span class="t">Then we multiply, we transform it with a complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3387" target="_blank">00:56:27.120</a></span> | <span class="t">We multiply it with M theta that we pre-computed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3390" target="_blank">00:56:30.960</a></span> | <span class="t">And this visualization of why we do this is present in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3395" target="_blank">00:56:35.920</a></span> | <span class="t">It's in particular at this figure here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3401" target="_blank">00:56:41.760</a></span> | <span class="t">So here they say, if you have a word with n dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3407" target="_blank">00:56:47.120</a></span> | <span class="t">you need, of course, d dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3411" target="_blank">00:56:51.360</a></span> | <span class="t">Then, of course, you will have theta of d half theta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3415" target="_blank">00:56:55.440</a></span> | <span class="t">Because we have theta 1, theta 2, up to theta d half.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3419" target="_blank">00:56:59.840</a></span> | <span class="t">We group successive dimensions into a new complex number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3426" target="_blank">00:57:06.880</a></span> | <span class="t">that if we project it on the complex plane, it will result into this vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3431" target="_blank">00:57:11.200</a></span> | <span class="t">So the x1, x2 vector you can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3433" target="_blank">00:57:13.760</a></span> | <span class="t">And then we multiply it with the complex number M theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3438" target="_blank">00:57:18.320</a></span> | <span class="t">This will result in the number being rotated by the angle indicated by M theta 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3445" target="_blank">00:57:25.360</a></span> | <span class="t">And this is the encoded number, is the encoded token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3449" target="_blank">00:57:29.840</a></span> | <span class="t">And this is exactly what we are doing with our matrix transformations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3453" target="_blank">00:57:33.520</a></span> | <span class="t">that I show you right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3455" target="_blank">00:57:35.200</a></span> | <span class="t">Now let's translate this into code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3458" target="_blank">00:57:38.400</a></span> | <span class="t">Apply Rotary Embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3461" target="_blank">00:57:41.200</a></span> | <span class="t">X is the token to which we want to apply the Rotary Embeddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3469" target="_blank">00:57:49.200</a></span> | <span class="t">FreqsComplex is the output of this function, but only for the position of this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3480" target="_blank">00:58:00.000</a></span> | <span class="t">So only for all the positions of this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3482" target="_blank">00:58:02.880</a></span> | <span class="t">Because this will have all the theta for all the possible positions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3486" target="_blank">00:58:06.320</a></span> | <span class="t">but we only need the positions for this particular token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3489" target="_blank">00:58:09.600</a></span> | <span class="t">And then we need the device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3492" target="_blank">00:58:12.320</a></span> | <span class="t">The first thing we do is the transformation number 1, I think I call it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3502" target="_blank">00:58:22.800</a></span> | <span class="t">Yeah, this one here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3504" target="_blank">00:58:24.080</a></span> | <span class="t">And number 1 and number 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3506" target="_blank">00:58:26.960</a></span> | <span class="t">So the first thing we do is we call the transformation number 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3510" target="_blank">00:58:30.960</a></span> | <span class="t">And number 2. So the first thing we do is we transform</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3514" target="_blank">00:58:34.320</a></span> | <span class="t">the two consecutive dimensions into a new tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3518" target="_blank">00:58:38.320</a></span> | <span class="t">And then we visualize it as a complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3521" target="_blank">00:58:41.200</a></span> | <span class="t">These operations are supported by PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3524" target="_blank">00:58:44.480</a></span> | <span class="t">So we do them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3527" target="_blank">00:58:47.360</a></span> | <span class="t">So we create XComplex is equal to.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3543" target="_blank">00:59:03.680</a></span> | <span class="t">Okay, this operation here is basically saying take two consecutive dimensions and group them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3555" target="_blank">00:59:15.920</a></span> | <span class="t">And then we transform this intermediate tensor into a complex tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3559" target="_blank">00:59:19.920</a></span> | <span class="t">by using the view as complex operation from Torch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3563" target="_blank">00:59:23.600</a></span> | <span class="t">Let me write some comments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3566" target="_blank">00:59:26.800</a></span> | <span class="t">So we are starting from B, sequence length, H, head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3573" target="_blank">00:59:33.680</a></span> | <span class="t">Because I saw before this X is actually not the original vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3578" target="_blank">00:59:38.160</a></span> | <span class="t">but it's already the one divided with its head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3582" target="_blank">00:59:42.640</a></span> | <span class="t">Because we will have a multi head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3587" target="_blank">00:59:47.600</a></span> | <span class="t">But if there is no multi head attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3589" target="_blank">00:59:49.360</a></span> | <span class="t">then this head dimension is actually the full dimension of the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3592" target="_blank">00:59:52.640</a></span> | <span class="t">So 4096.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3593" target="_blank">00:59:53.920</a></span> | <span class="t">Then we have this tensor here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3601" target="_blank">01:00:01.360</a></span> | <span class="t">But this tensor has two dimensions less than this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3604" target="_blank">01:00:04.480</a></span> | <span class="t">It doesn't have the batch dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3606" target="_blank">01:00:06.400</a></span> | <span class="t">And it doesn't have the head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3608" target="_blank">01:00:08.160</a></span> | <span class="t">So we need to add it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3609" target="_blank">01:00:09.120</a></span> | <span class="t">So take the XComplex and we add the two dimensions that it's missing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3613" target="_blank">01:00:13.840</a></span> | <span class="t">And here we are doing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3623" target="_blank">01:00:23.440</a></span> | <span class="t">Okay, let me write all the transformations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3625" target="_blank">01:00:25.360</a></span> | <span class="t">Here we are going from here to divide by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3635" target="_blank">01:00:35.280</a></span> | <span class="t">Why divide by two?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3636" target="_blank">01:00:36.080</a></span> | <span class="t">Because every two consecutive pairs are becoming one complex number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3640" target="_blank">01:00:40.160</a></span> | <span class="t">And here we go from sequence length to head dimension divide by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3646" target="_blank">01:00:46.560</a></span> | <span class="t">We are mapping it to one because this is the batch dimension sequence length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3653" target="_blank">01:00:53.920</a></span> | <span class="t">then the head dimension one, and then head dimension divide by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3659" target="_blank">01:00:59.600</a></span> | <span class="t">Now we multiply them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3662" target="_blank">01:01:02.160</a></span> | <span class="t">So we do this operation here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3663" target="_blank">01:01:03.920</a></span> | <span class="t">So element wise multiplication,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3666" target="_blank">01:01:06.080</a></span> | <span class="t">which will result in a rotation as we saw in the figure before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3668" target="_blank">01:01:08.880</a></span> | <span class="t">So that's why I call it X rotated is equal to X complex</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3673" target="_blank">01:01:13.120</a></span> | <span class="t">multiplied by the complex number of the frequencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3677" target="_blank">01:01:17.040</a></span> | <span class="t">In this case, we are doing sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3682" target="_blank">01:01:22.720</a></span> | <span class="t">H dimension divide by two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3696" target="_blank">01:01:36.000</a></span> | <span class="t">Then we multiply it, we obtain this result.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3705" target="_blank">01:01:45.600</a></span> | <span class="t">And then we first transform it into...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3712" target="_blank">01:01:52.080</a></span> | <span class="t">We transform the complex number into a tensor</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3715" target="_blank">01:01:55.520</a></span> | <span class="t">in which the first item is the real part of the complex number</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3718" target="_blank">01:01:58.400</a></span> | <span class="t">and then the complex part, the imaginary part, and then we flatten it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3724" target="_blank">01:02:04.480</a></span> | <span class="t">So let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3734" target="_blank">01:02:14.640</a></span> | <span class="t">This operation view as real will transform the tensor like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3741" target="_blank">01:02:21.040</a></span> | <span class="t">So it's this transformation here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3760" target="_blank">01:02:40.960</a></span> | <span class="t">in which we transform the complex number into a tensor of two dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3765" target="_blank">01:02:45.280</a></span> | <span class="t">Because that's why you can see this additional dimension here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3768" target="_blank">01:02:48.080</a></span> | <span class="t">And then we flatten it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3770" target="_blank">01:02:50.000</a></span> | <span class="t">You can just say to flatten it with the shape of the original...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3780" target="_blank">01:03:00.160</a></span> | <span class="t">with the original tensor we gave it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3785" target="_blank">01:03:05.360</a></span> | <span class="t">So become...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3795" target="_blank">01:03:15.200</a></span> | <span class="t">And this is how we calculate the embedding.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3810" target="_blank">01:03:30.240</a></span> | <span class="t">So given a tensor of representing a token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3814" target="_blank">01:03:34.800</a></span> | <span class="t">or a list of tokens, because we have the batch dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3817" target="_blank">01:03:37.920</a></span> | <span class="t">we can apply the embeddings like this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3821" target="_blank">01:03:41.040</a></span> | <span class="t">doing all these transformations that we have done here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3824" target="_blank">01:03:44.160</a></span> | <span class="t">and that are represented in this code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3826" target="_blank">01:03:46.080</a></span> | <span class="t">And they are all equivalent to doing this operation as written on the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3830" target="_blank">01:03:50.000</a></span> | <span class="t">Now we need to go forward with our transformer by implementing the rest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3838" target="_blank">01:03:58.000</a></span> | <span class="t">The next thing that we can implement is this RMS norm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3842" target="_blank">01:04:02.000</a></span> | <span class="t">because it's present at the output of the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3846" target="_blank">01:04:06.880</a></span> | <span class="t">but also at the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3848" target="_blank">01:04:08.480</a></span> | <span class="t">So let's go review again the architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3851" target="_blank">01:04:11.360</a></span> | <span class="t">We can see that we have the normalization, the RMS normalization here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3857" target="_blank">01:04:17.360</a></span> | <span class="t">but we also have it here and here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3859" target="_blank">01:04:19.520</a></span> | <span class="t">So let's implement it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3860" target="_blank">01:04:20.720</a></span> | <span class="t">Let's also visualize how the RMS norm works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3864" target="_blank">01:04:24.640</a></span> | <span class="t">If you want to have a deep understanding of how normalization works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3868" target="_blank">01:04:28.400</a></span> | <span class="t">in my previous video about Llama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3870" target="_blank">01:04:30.160</a></span> | <span class="t">I actually described why we need normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3872" target="_blank">01:04:32.720</a></span> | <span class="t">how it was historically done and how it works,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3875" target="_blank">01:04:35.280</a></span> | <span class="t">also at the autograd level.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3877" target="_blank">01:04:37.840</a></span> | <span class="t">So I will not repeat the same lecture here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3881" target="_blank">01:04:41.280</a></span> | <span class="t">I will just briefly introduce how it works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3884" target="_blank">01:04:44.000</a></span> | <span class="t">But if you want to have a better understanding,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3885" target="_blank">01:04:45.760</a></span> | <span class="t">please watch my previous video.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3887" target="_blank">01:04:47.760</a></span> | <span class="t">So as you remember, in the original transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3890" target="_blank">01:04:50.000</a></span> | <span class="t">we used layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3891" target="_blank">01:04:51.680</a></span> | <span class="t">And layer normalization worked like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3894" target="_blank">01:04:54.000</a></span> | <span class="t">We have an input where we have some items,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3897" target="_blank">01:04:57.280</a></span> | <span class="t">suppose item 1, item 2, up to item 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3900" target="_blank">01:05:00.160</a></span> | <span class="t">Each item has three features, so A1, A2, A3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3904" target="_blank">01:05:04.480</a></span> | <span class="t">What we did with layer normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3906" target="_blank">01:05:06.400</a></span> | <span class="t">we computed two statistics, one for each item,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3910" target="_blank">01:05:10.080</a></span> | <span class="t">so mu and sigma, so the mean and the sigma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3913" target="_blank">01:05:13.680</a></span> | <span class="t">And we standardize each item,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3916" target="_blank">01:05:16.320</a></span> | <span class="t">normalize each element of this input matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3919" target="_blank">01:05:19.760</a></span> | <span class="t">using this formula here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3921" target="_blank">01:05:21.760</a></span> | <span class="t">which transforms it into a distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3924" target="_blank">01:05:24.960</a></span> | <span class="t">with zero mean and variance of one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3927" target="_blank">01:05:27.680</a></span> | <span class="t">And this formula comes from probability statistics.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3931" target="_blank">01:05:31.520</a></span> | <span class="t">So as you know, if you have any random variable</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3934" target="_blank">01:05:34.240</a></span> | <span class="t">with its mu and sigma,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3936" target="_blank">01:05:36.160</a></span> | <span class="t">if you do the variable minus its mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3939" target="_blank">01:05:39.600</a></span> | <span class="t">divided by the standard deviation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3942" target="_blank">01:05:42.160</a></span> | <span class="t">so the square root of the variance,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3943" target="_blank">01:05:43.760</a></span> | <span class="t">it will result into a Gaussian of mean zero</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3947" target="_blank">01:05:47.520</a></span> | <span class="t">and the standard and the variance of one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3950" target="_blank">01:05:50.320</a></span> | <span class="t">We then multiply this with the gamma parameter</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3954" target="_blank">01:05:54.080</a></span> | <span class="t">and we also add a beta parameter here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3956" target="_blank">01:05:56.800</a></span> | <span class="t">But this was done in the layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3958" target="_blank">01:05:58.960</a></span> | <span class="t">In LLAMA, we use RMS normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3965" target="_blank">01:06:05.120</a></span> | <span class="t">and let's see the difference.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3966" target="_blank">01:06:06.320</a></span> | <span class="t">In RMS normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3969" target="_blank">01:06:09.040</a></span> | <span class="t">the paper of the RMS normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3971" target="_blank">01:06:11.120</a></span> | <span class="t">claims that we don't need to obtain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3974" target="_blank">01:06:14.160</a></span> | <span class="t">the effect of layer normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3976" target="_blank">01:06:16.240</a></span> | <span class="t">we don't need to compute two statistics,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3978" target="_blank">01:06:18.480</a></span> | <span class="t">that is the mean and the variance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3980" target="_blank">01:06:20.560</a></span> | <span class="t">And actually they claim that the normal,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3982" target="_blank">01:06:22.800</a></span> | <span class="t">the effect given by layer normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3985" target="_blank">01:06:25.760</a></span> | <span class="t">can be obtained without recentering the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3988" target="_blank">01:06:28.720</a></span> | <span class="t">So without recentering them around the mean of zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3992" target="_blank">01:06:32.480</a></span> | <span class="t">But just by scaling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3995" target="_blank">01:06:35.040</a></span> | <span class="t">However, the variance in the layer normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3997" target="_blank">01:06:37.840</a></span> | <span class="t">was computed using the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=3999" target="_blank">01:06:39.440</a></span> | <span class="t">because if you remember the formula of the variance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4001" target="_blank">01:06:41.680</a></span> | <span class="t">is X minus the mean of the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4006" target="_blank">01:06:46.000</a></span> | <span class="t">to the power of two divided by N.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4008" target="_blank">01:06:48.800</a></span> | <span class="t">So to compute the variance, we needed the mean,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4013" target="_blank">01:06:53.600</a></span> | <span class="t">but we wanted to avoid computing the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4015" target="_blank">01:06:55.920</a></span> | <span class="t">because we don't need it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4016" target="_blank">01:06:56.960</a></span> | <span class="t">This is not what the RMS paper claims.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4019" target="_blank">01:06:59.600</a></span> | <span class="t">RMS paper claims that we don't need the mean</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4021" target="_blank">01:07:01.760</a></span> | <span class="t">and we don't need to recenter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4023" target="_blank">01:07:03.680</a></span> | <span class="t">So we need to compute a statistic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4025" target="_blank">01:07:05.760</a></span> | <span class="t">that doesn't depend on the mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4027" target="_blank">01:07:07.360</a></span> | <span class="t">That's why they introduced these statistics here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4030" target="_blank">01:07:10.160</a></span> | <span class="t">which is the root mean squared</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4031" target="_blank">01:07:11.520</a></span> | <span class="t">that doesn't depend on the mean.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4033" target="_blank">01:07:13.360</a></span> | <span class="t">And in practice gives the same normalization effect</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4037" target="_blank">01:07:17.040</a></span> | <span class="t">as the layer normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4039" target="_blank">01:07:19.040</a></span> | <span class="t">And we also have a gamma parameter also here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4041" target="_blank">01:07:21.840</a></span> | <span class="t">that is learnable and that's multiplied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4044" target="_blank">01:07:24.240</a></span> | <span class="t">So as you can see, the only difference</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4045" target="_blank">01:07:25.760</a></span> | <span class="t">between layer normalization and RMS normalization</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4048" target="_blank">01:07:28.640</a></span> | <span class="t">is that we don't recenter the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4050" target="_blank">01:07:30.400</a></span> | <span class="t">And it looks like that recentering was not necessary</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4054" target="_blank">01:07:34.480</a></span> | <span class="t">as written in the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4055" target="_blank">01:07:35.440</a></span> | <span class="t">because they say in this paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4056" target="_blank">01:07:36.640</a></span> | <span class="t">we hypothesize that the rescaling invariance</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4059" target="_blank">01:07:39.760</a></span> | <span class="t">is the reason for the success of layer norm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4062" target="_blank">01:07:42.560</a></span> | <span class="t">rather than the recentering invariance.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4065" target="_blank">01:07:45.280</a></span> | <span class="t">So they just rescale the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4068" target="_blank">01:07:48.320</a></span> | <span class="t">according to the RMS statistic.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4070" target="_blank">01:07:50.560</a></span> | <span class="t">And this is what we will do in our code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4073" target="_blank">01:07:53.120</a></span> | <span class="t">So let's build this block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4077" target="_blank">01:07:57.120</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4099" target="_blank">01:08:19.840</a></span> | <span class="t">So the APS value you can see here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4102" target="_blank">01:08:22.160</a></span> | <span class="t">is used as a denominator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4107" target="_blank">01:08:27.600</a></span> | <span class="t">Let me go back here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4109" target="_blank">01:08:29.280</a></span> | <span class="t">It's used here as the added to the denominator.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4112" target="_blank">01:08:32.160</a></span> | <span class="t">So to avoid a division by zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4114" target="_blank">01:08:34.160</a></span> | <span class="t">And then we have the gamma parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4122" target="_blank">01:08:42.080</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4132" target="_blank">01:08:52.640</a></span> | <span class="t">And this is it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4133" target="_blank">01:08:53.920</a></span> | <span class="t">Then we define the function norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4136" target="_blank">01:08:56.480</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4143" target="_blank">01:09:03.840</a></span> | <span class="t">Where x is batch sequence length dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4150" target="_blank">01:09:10.880</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4152" target="_blank">01:09:12.820</a></span> | <span class="t">So we return x multiplied by torch dot r sqrt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4158" target="_blank">01:09:18.160</a></span> | <span class="t">r sqrt stands for the one over the square root.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4165" target="_blank">01:09:25.040</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4184" target="_blank">01:09:44.960</a></span> | <span class="t">And that's it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4185" target="_blank">01:09:45.760</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4204" target="_blank">01:10:04.240</a></span> | <span class="t">We multiply by gamma.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4205" target="_blank">01:10:05.520</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4216" target="_blank">01:10:16.480</a></span> | <span class="t">So we have, as you can see,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4218" target="_blank">01:10:18.720</a></span> | <span class="t">weight is actually is a number,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4220" target="_blank">01:10:20.480</a></span> | <span class="t">a list of ones with the dimension dim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4222" target="_blank">01:10:22.480</a></span> | <span class="t">So dim multiplied by b sequence length dim</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4228" target="_blank">01:10:28.800</a></span> | <span class="t">results in b sequence length dim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4232" target="_blank">01:10:32.720</a></span> | <span class="t">Where b is the batch dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4235" target="_blank">01:10:35.120</a></span> | <span class="t">And here what we are doing is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4236" target="_blank">01:10:36.960</a></span> | <span class="t">this is r sqrt is equal to one over sqrt of x,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4243" target="_blank">01:10:43.440</a></span> | <span class="t">just as a reminder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4244" target="_blank">01:10:44.400</a></span> | <span class="t">And the dimensions here are multiplied by b sequence length one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4254" target="_blank">01:10:54.080</a></span> | <span class="t">which results in b sequence length dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4258" target="_blank">01:10:58.880</a></span> | <span class="t">So what we are doing is exactly just this formula here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4263" target="_blank">01:11:03.360</a></span> | <span class="t">So just multiply it by one over rms</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4267" target="_blank">01:11:07.760</a></span> | <span class="t">and then multiply it with gamma here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4271" target="_blank">01:11:11.200</a></span> | <span class="t">Now that we have also built the rms norm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4275" target="_blank">01:11:15.760</a></span> | <span class="t">let's go check our next building block,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4278" target="_blank">01:11:18.080</a></span> | <span class="t">which is this encoder block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4279" target="_blank">01:11:19.600</a></span> | <span class="t">So what is the encoder block?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4280" target="_blank">01:11:20.880</a></span> | <span class="t">Let's go back to the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4283" target="_blank">01:11:23.360</a></span> | <span class="t">Here we have the encoder block is all this block here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4288" target="_blank">01:11:28.160</a></span> | <span class="t">that contains a normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4291" target="_blank">01:11:31.200</a></span> | <span class="t">It contains a self-attention here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4294" target="_blank">01:11:34.000</a></span> | <span class="t">It contains skip connections.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4295" target="_blank">01:11:35.920</a></span> | <span class="t">You can see here another normalization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4298" target="_blank">01:11:38.560</a></span> | <span class="t">another skip connection and a feed forward layer here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4302" target="_blank">01:11:42.160</a></span> | <span class="t">I think the easiest one to start with is the feed forward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4309" target="_blank">01:11:49.040</a></span> | <span class="t">but we can also, we can also, okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4312" target="_blank">01:11:52.160</a></span> | <span class="t">let's start first build the encoder block</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4314" target="_blank">01:11:54.800</a></span> | <span class="t">and then we will build the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4317" target="_blank">01:11:57.760</a></span> | <span class="t">and finally the feed forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4319" target="_blank">01:11:59.440</a></span> | <span class="t">So we first build the skeleton of this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4321" target="_blank">01:12:01.840</a></span> | <span class="t">then the attention and then this, let's go.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4334" target="_blank">01:12:14.960</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4358" target="_blank">01:12:38.960</a></span> | <span class="t">I received some parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4361" target="_blank">01:12:41.280</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4372" target="_blank">01:12:52.080</a></span> | <span class="t">What is the head dimension is the dimension of the vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4375" target="_blank">01:12:55.360</a></span> | <span class="t">divided by the number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4376" target="_blank">01:12:56.720</a></span> | <span class="t">So 4,096 divided by, here is the divide by 32,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4381" target="_blank">01:13:01.920</a></span> | <span class="t">because as we can see here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4384" target="_blank">01:13:04.960</a></span> | <span class="t">we have the dimension of the vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4386" target="_blank">01:13:06.640</a></span> | <span class="t">of the embedding vector is 4,096,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4388" target="_blank">01:13:08.400</a></span> | <span class="t">but we have 32 heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4389" target="_blank">01:13:09.600</a></span> | <span class="t">So each head will see 4,096 divided by 32 items</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4393" target="_blank">01:13:13.680</a></span> | <span class="t">from each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4394" target="_blank">01:13:14.960</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4404" target="_blank">01:13:24.880</a></span> | <span class="t">Then we have a self-attention block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4407" target="_blank">01:13:27.120</a></span> | <span class="t">I define it, but don't build it right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4409" target="_blank">01:13:29.520</a></span> | <span class="t">Just define the skeleton.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4410" target="_blank">01:13:30.960</a></span> | <span class="t">Then we have the feed forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4417" target="_blank">01:13:37.040</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4419" target="_blank">01:13:39.760</a></span> | <span class="t">Then we have the normalization before the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4424" target="_blank">01:13:44.720</a></span> | <span class="t">So self-attention, this is our RMS norm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4432" target="_blank">01:13:52.160</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4437" target="_blank">01:13:57.120</a></span> | <span class="t">And this is the motivation behind this argument norm abs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4440" target="_blank">01:14:00.560</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4444" target="_blank">01:14:04.160</a></span> | <span class="t">Then we have an after the feed forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4446" target="_blank">01:14:06.480</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4452" target="_blank">01:14:12.800</a></span> | <span class="t">Is it after?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4453" target="_blank">01:14:13.680</a></span> | <span class="t">It's after the attention, not after the feed forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4457" target="_blank">01:14:17.680</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4459" target="_blank">01:14:19.200</a></span> | <span class="t">So before the feed forward block.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4461" target="_blank">01:14:21.760</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4475" target="_blank">01:14:35.840</a></span> | <span class="t">And then we have norm abs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4478" target="_blank">01:14:38.000</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4479" target="_blank">01:14:39.600</a></span> | <span class="t">Okay, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4481" target="_blank">01:14:41.040</a></span> | <span class="t">Now let's implement the forward method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4483" target="_blank">01:14:43.440</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4492" target="_blank">01:14:52.320</a></span> | <span class="t">StartPause indicates the position of the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4494" target="_blank">01:14:54.960</a></span> | <span class="t">I kept the same variable number as in the original code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4498" target="_blank">01:14:58.240</a></span> | <span class="t">It's actually the position of the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4499" target="_blank">01:14:59.920</a></span> | <span class="t">Because as you remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4501" target="_blank">01:15:01.200</a></span> | <span class="t">we will be dealing with only one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4503" target="_blank">01:15:03.760</a></span> | <span class="t">So StartPause indicates the position of the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4506" target="_blank">01:15:06.720</a></span> | <span class="t">we are dealing with.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4507" target="_blank">01:15:07.520</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4511" target="_blank">01:15:11.200</a></span> | <span class="t">These are the pre-computed frequencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4514" target="_blank">01:15:14.160</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4525" target="_blank">01:15:25.280</a></span> | <span class="t">So we need to the skip connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4527" target="_blank">01:15:27.840</a></span> | <span class="t">And yeah, okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4529" target="_blank">01:15:29.920</a></span> | <span class="t">The hidden is equal to x plus the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4536" target="_blank">01:15:36.800</a></span> | <span class="t">So we calculated the attention of what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4539" target="_blank">01:15:39.840</a></span> | <span class="t">Of the normalized version of this input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4543" target="_blank">01:15:43.040</a></span> | <span class="t">So we first apply the normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4545" target="_blank">01:15:45.440</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4549" target="_blank">01:15:49.840</a></span> | <span class="t">And then we calculate this attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4552" target="_blank">01:15:52.080</a></span> | <span class="t">And to the attention, we also give the frequencies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4555" target="_blank">01:15:55.360</a></span> | <span class="t">Because as you remember,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4556" target="_blank">01:15:56.640</a></span> | <span class="t">the rotary positional encodings are kind of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4562" target="_blank">01:16:02.080</a></span> | <span class="t">they come into play when we calculate the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4564" target="_blank">01:16:04.720</a></span> | <span class="t">And these operations involve tensors of size B,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4568" target="_blank">01:16:08.960</a></span> | <span class="t">sequence length dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4570" target="_blank">01:16:10.320</a></span> | <span class="t">which is x plus the skip connection,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4572" target="_blank">01:16:12.880</a></span> | <span class="t">plus the output of the attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4575" target="_blank">01:16:15.040</a></span> | <span class="t">B sequence length dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4577" target="_blank">01:16:17.920</a></span> | <span class="t">which results in B sequence length dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4583" target="_blank">01:16:23.280</a></span> | <span class="t">Then we have another,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4586" target="_blank">01:16:26.880</a></span> | <span class="t">we have the application of the feedforward</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4588" target="_blank">01:16:28.720</a></span> | <span class="t">with its skip connection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4590" target="_blank">01:16:30.080</a></span> | <span class="t">So out is equal to h plus.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4596" target="_blank">01:16:36.240</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4600" target="_blank">01:16:40.400</a></span> | <span class="t">And before we send it to the feedforward,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4602" target="_blank">01:16:42.320</a></span> | <span class="t">before we applied the normalization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4605" target="_blank">01:16:45.920</a></span> | <span class="t">(keyboard clicking)</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4609" target="_blank">01:16:49.280</a></span> | <span class="t">And this is the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4610" target="_blank">01:16:50.320</a></span> | <span class="t">Now we need to build the self-attention and the feedforward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4614" target="_blank">01:16:54.240</a></span> | <span class="t">Let's start with the harder part first.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4616" target="_blank">01:16:56.640</a></span> | <span class="t">So the self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4617" target="_blank">01:16:57.840</a></span> | <span class="t">because I think it's more interesting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4619" target="_blank">01:16:59.760</a></span> | <span class="t">Before we build the self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4621" target="_blank">01:17:01.920</a></span> | <span class="t">let's review how self-attention worked</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4624" target="_blank">01:17:04.480</a></span> | <span class="t">in the original transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4626" target="_blank">01:17:06.000</a></span> | <span class="t">and how it will work here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4627" target="_blank">01:17:07.920</a></span> | <span class="t">So, okay, this is the original paper</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4631" target="_blank">01:17:11.840</a></span> | <span class="t">from the original paper of the transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4634" target="_blank">01:17:14.560</a></span> | <span class="t">So attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4636" target="_blank">01:17:16.000</a></span> | <span class="t">Let's review the self-attention mechanism</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4637" target="_blank">01:17:17.840</a></span> | <span class="t">in the original transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4639" target="_blank">01:17:19.040</a></span> | <span class="t">And then we will see how it works in Llama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4641" target="_blank">01:17:21.120</a></span> | <span class="t">In the attention is all you need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4644" target="_blank">01:17:24.240</a></span> | <span class="t">We have an input, which is sequenced by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4646" target="_blank">01:17:26.560</a></span> | <span class="t">So a sequence of tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4647" target="_blank">01:17:27.920</a></span> | <span class="t">each token modeled by a vector of size T model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4651" target="_blank">01:17:31.040</a></span> | <span class="t">We transform them into query key and values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4654" target="_blank">01:17:34.400</a></span> | <span class="t">which are the same input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4655" target="_blank">01:17:35.920</a></span> | <span class="t">We multiply by a W matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4658" target="_blank">01:17:38.560</a></span> | <span class="t">which is a parameter matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4660" target="_blank">01:17:40.080</a></span> | <span class="t">which results in a new matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4662" target="_blank">01:17:42.240</a></span> | <span class="t">which has this dimension of sequence by D model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4664" target="_blank">01:17:44.640</a></span> | <span class="t">And we then split them into the number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4669" target="_blank">01:17:49.280</a></span> | <span class="t">that we have,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4670" target="_blank">01:17:50.400</a></span> | <span class="t">such that each vector that represents the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4674" target="_blank">01:17:54.000</a></span> | <span class="t">is split into, suppose we have four heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4676" target="_blank">01:17:56.320</a></span> | <span class="t">So each vector, each head will see a part</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4680" target="_blank">01:18:00.400</a></span> | <span class="t">of the embedding of each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4682" target="_blank">01:18:02.320</a></span> | <span class="t">So if the token was 512 in size, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4685" target="_blank">01:18:05.840</a></span> | <span class="t">the embedding vector,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4687" target="_blank">01:18:07.280</a></span> | <span class="t">the first head will watch 128 dimensions of this vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4692" target="_blank">01:18:12.880</a></span> | <span class="t">The second head will watch the next 108 dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4696" target="_blank">01:18:16.880</a></span> | <span class="t">The next head will watch the next 108 dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4699" target="_blank">01:18:19.760</a></span> | <span class="t">et cetera, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4700" target="_blank">01:18:20.960</a></span> | <span class="t">We then calculate the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4703" target="_blank">01:18:23.280</a></span> | <span class="t">between all these smaller matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4705" target="_blank">01:18:25.360</a></span> | <span class="t">So Q, K and V.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4706" target="_blank">01:18:26.640</a></span> | <span class="t">This results in head 1, head 2, head 3 and head 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4710" target="_blank">01:18:30.800</a></span> | <span class="t">We then concatenate them together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4713" target="_blank">01:18:33.120</a></span> | <span class="t">We multiply with the W matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4716" target="_blank">01:18:36.640</a></span> | <span class="t">And this is the output of the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4721" target="_blank">01:18:41.680</a></span> | <span class="t">In this case, it's called self-attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4724" target="_blank">01:18:44.320</a></span> | <span class="t">because it's the same input that acts as a query,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4727" target="_blank">01:18:47.920</a></span> | <span class="t">as key and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4729" target="_blank">01:18:49.680</a></span> | <span class="t">In case the query comes from one place</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4733" target="_blank">01:18:53.280</a></span> | <span class="t">and the key and the values come from another place,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4735" target="_blank">01:18:55.360</a></span> | <span class="t">in that case, it's called cross-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4737" target="_blank">01:18:57.440</a></span> | <span class="t">And that kind of attention is used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4739" target="_blank">01:18:59.200</a></span> | <span class="t">in multi-modal architectures, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4743" target="_blank">01:19:03.440</a></span> | <span class="t">when you want to combine, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4745" target="_blank">01:19:05.040</a></span> | <span class="t">pictures with captions or music with text,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4749" target="_blank">01:19:09.280</a></span> | <span class="t">or you want to translate from one language to another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4751" target="_blank">01:19:11.840</a></span> | <span class="t">So you have kind of multi-modality</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4753" target="_blank">01:19:13.600</a></span> | <span class="t">and you want to connect the two together.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4755" target="_blank">01:19:15.520</a></span> | <span class="t">But in our case, we are modeling a language.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4758" target="_blank">01:19:18.560</a></span> | <span class="t">So self-attention is what we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4761" target="_blank">01:19:21.920</a></span> | <span class="t">Actually, attention is all we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4763" target="_blank">01:19:23.440</a></span> | <span class="t">So let's watch how it works in LLAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4768" target="_blank">01:19:28.480</a></span> | <span class="t">Okay, in LLAMA, we need to talk about a lot of things</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4772" target="_blank">01:19:32.880</a></span> | <span class="t">before we build the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4774" target="_blank">01:19:34.640</a></span> | <span class="t">We need to review how the self-attention works in LLAMA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4777" target="_blank">01:19:37.600</a></span> | <span class="t">how is the key, what is the KV cache,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4779" target="_blank">01:19:39.840</a></span> | <span class="t">what is the grouped query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4781" target="_blank">01:19:41.680</a></span> | <span class="t">and actually how the inference works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4784" target="_blank">01:19:44.640</a></span> | <span class="t">So we need to review all this stuff</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4785" target="_blank">01:19:45.840</a></span> | <span class="t">before we proceed with the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4787" target="_blank">01:19:47.040</a></span> | <span class="t">Otherwise, it will be very hard to follow the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4790" target="_blank">01:19:50.000</a></span> | <span class="t">So let's first talk about the inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4794" target="_blank">01:19:54.240</a></span> | <span class="t">Given, suppose we have a model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4797" target="_blank">01:19:57.440</a></span> | <span class="t">so that has been trained on this particular line.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4801" target="_blank">01:20:01.520</a></span> | <span class="t">So the line is love that can quickly seize the gentle heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4805" target="_blank">01:20:05.840</a></span> | <span class="t">And this is a line from Dante Alighieri.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4809" target="_blank">01:20:09.120</a></span> | <span class="t">You can see this from the epistle</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4811" target="_blank">01:20:11.120</a></span> | <span class="t">from the Inferno, Fifth Canto.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4814" target="_blank">01:20:14.000</a></span> | <span class="t">It's not the first line actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4815" target="_blank">01:20:15.920</a></span> | <span class="t">but this is Paolo and Francesca, by the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4818" target="_blank">01:20:18.800</a></span> | <span class="t">And we have a model that has been trained on this line,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4823" target="_blank">01:20:23.280</a></span> | <span class="t">love that can quickly seize the gentle heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4825" target="_blank">01:20:25.120</a></span> | <span class="t">Now, a model that has been trained on this particular line</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4830" target="_blank">01:20:30.480</a></span> | <span class="t">using the next token prediction</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4832" target="_blank">01:20:32.080</a></span> | <span class="t">should have an input that is built in this way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4834" target="_blank">01:20:34.320</a></span> | <span class="t">So the start of sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4836" target="_blank">01:20:36.400</a></span> | <span class="t">and then the tokens that represented the sentence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4840" target="_blank">01:20:40.000</a></span> | <span class="t">then the target should be the same sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4842" target="_blank">01:20:42.320</a></span> | <span class="t">with the end of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4843" target="_blank">01:20:43.600</a></span> | <span class="t">Because the transformer is a sequence-to-sequence model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4847" target="_blank">01:20:47.440</a></span> | <span class="t">it maps one input sequence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4849" target="_blank">01:20:49.280</a></span> | <span class="t">into an output sequence of the same size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4851" target="_blank">01:20:51.680</a></span> | <span class="t">This means that the first token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4854" target="_blank">01:20:54.320</a></span> | <span class="t">will be mapped to the first token of the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4856" target="_blank">01:20:56.720</a></span> | <span class="t">The second token of the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4858" target="_blank">01:20:58.080</a></span> | <span class="t">will be mapped to the second token of the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4862" target="_blank">01:21:02.080</a></span> | <span class="t">The third token of the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4864" target="_blank">01:21:04.720</a></span> | <span class="t">will be mapped to the third token of the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4868" target="_blank">01:21:08.560</a></span> | <span class="t">But it's not a one-to-one correspondence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4872" target="_blank">01:21:12.160</a></span> | <span class="t">Because of the mask, of the causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4874" target="_blank">01:21:14.880</a></span> | <span class="t">that we apply during the self-attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4877" target="_blank">01:21:17.360</a></span> | <span class="t">to predict this particular token can, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4882" target="_blank">01:21:22.160</a></span> | <span class="t">the model doesn't only watch the same token in the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4886" target="_blank">01:21:26.480</a></span> | <span class="t">so that, but also watch all the previous tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4889" target="_blank">01:21:29.520</a></span> | <span class="t">So the model to predict can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4892" target="_blank">01:21:32.480</a></span> | <span class="t">needs to access not only that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4894" target="_blank">01:21:34.800</a></span> | <span class="t">but also SOS, love, that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4897" target="_blank">01:21:37.760</a></span> | <span class="t">And the self-attention mechanism with its causal mask</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4900" target="_blank">01:21:40.880</a></span> | <span class="t">will access all the previous tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4903" target="_blank">01:21:43.280</a></span> | <span class="t">but not the next ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4904" target="_blank">01:21:44.640</a></span> | <span class="t">This means that when we do the inferencing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4907" target="_blank">01:21:47.600</a></span> | <span class="t">we should do it like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4908" target="_blank">01:21:48.720</a></span> | <span class="t">We start with the start of sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4910" target="_blank">01:21:50.320</a></span> | <span class="t">and the model will output the first word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4912" target="_blank">01:21:52.640</a></span> | <span class="t">To output the next token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4914" target="_blank">01:21:54.240</a></span> | <span class="t">we need to give the previous output token as input also.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4918" target="_blank">01:21:58.000</a></span> | <span class="t">So we always append the last token of the output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4922" target="_blank">01:22:02.080</a></span> | <span class="t">to the input to predict the successive tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4924" target="_blank">01:22:04.960</a></span> | <span class="t">So for example, to output that,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4926" target="_blank">01:22:06.640</a></span> | <span class="t">we need to give SOS, love.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4928" target="_blank">01:22:08.560</a></span> | <span class="t">To output the next token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4929" target="_blank">01:22:09.760</a></span> | <span class="t">we take this that and we put it in the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4933" target="_blank">01:22:13.440</a></span> | <span class="t">so that we can get the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4935" target="_blank">01:22:15.520</a></span> | <span class="t">To output the next token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4938" target="_blank">01:22:18.320</a></span> | <span class="t">we need to append the previous output to the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4941" target="_blank">01:22:21.440</a></span> | <span class="t">to get the new output quickly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4943" target="_blank">01:22:23.200</a></span> | <span class="t">Now, when we do this job,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4946" target="_blank">01:22:26.000</a></span> | <span class="t">the model is actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4948" target="_blank">01:22:28.480</a></span> | <span class="t">we are giving this input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4950" target="_blank">01:22:30.880</a></span> | <span class="t">which is a sequence of four tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4952" target="_blank">01:22:32.640</a></span> | <span class="t">and the model will produce a sequence of four tokens as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4957" target="_blank">01:22:37.600</a></span> | <span class="t">But this is not really convenient when we do the inferencing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4960" target="_blank">01:22:40.720</a></span> | <span class="t">because the model is doing a lot of dot products</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4964" target="_blank">01:22:44.080</a></span> | <span class="t">that are not necessary,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4966" target="_blank">01:22:46.240</a></span> | <span class="t">that have already been built.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4967" target="_blank">01:22:47.760</a></span> | <span class="t">For example, what I want to say is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4971" target="_blank">01:22:51.360</a></span> | <span class="t">in order to get this last token quickly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4973" target="_blank">01:22:53.760</a></span> | <span class="t">we need to access all the previous context here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4978" target="_blank">01:22:58.720</a></span> | <span class="t">But we don't need to output love that can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4981" target="_blank">01:23:01.600</a></span> | <span class="t">because we don't care.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4982" target="_blank">01:23:02.560</a></span> | <span class="t">We already have these tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4983" target="_blank">01:23:03.920</a></span> | <span class="t">We only care about the last one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4985" target="_blank">01:23:05.760</a></span> | <span class="t">However, we can't just tell the transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4988" target="_blank">01:23:08.640</a></span> | <span class="t">to not output the previous tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4990" target="_blank">01:23:10.640</a></span> | <span class="t">We need to change the calculations in such a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4993" target="_blank">01:23:13.520</a></span> | <span class="t">that we only receive at the output of the transformer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4996" target="_blank">01:23:16.800</a></span> | <span class="t">only one token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=4997" target="_blank">01:23:17.840</a></span> | <span class="t">so that all the other tokens are not even calculated.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5000" target="_blank">01:23:20.480</a></span> | <span class="t">And this will make the inferencing fast.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5002" target="_blank">01:23:22.960</a></span> | <span class="t">And this is the job of the KVCache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5005" target="_blank">01:23:25.200</a></span> | <span class="t">Let me show you with some diagrams.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5006" target="_blank">01:23:26.960</a></span> | <span class="t">As you can see, at every step of the token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5013" target="_blank">01:23:33.600</a></span> | <span class="t">we are only interested in the last token output by the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5016" target="_blank">01:23:36.720</a></span> | <span class="t">because we already have the previous ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5018" target="_blank">01:23:38.640</a></span> | <span class="t">However, the model needs to access all the previous tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5022" target="_blank">01:23:42.240</a></span> | <span class="t">to decide which token to output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5024" target="_blank">01:23:44.800</a></span> | <span class="t">because the model needs to access all the prompt</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5028" target="_blank">01:23:48.320</a></span> | <span class="t">to output the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5030" target="_blank">01:23:50.400</a></span> | <span class="t">And we do this using the KVCache</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5032" target="_blank">01:23:52.480</a></span> | <span class="t">to reduce the amount of computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5034" target="_blank">01:23:54.320</a></span> | <span class="t">So let's do with some examples.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5037" target="_blank">01:23:57.040</a></span> | <span class="t">Suppose we do the same job that we did before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5039" target="_blank">01:23:59.680</a></span> | <span class="t">So the inferencing of that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5042" target="_blank">01:24:02.560</a></span> | <span class="t">We give the first token, so the SOS.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5044" target="_blank">01:24:04.800</a></span> | <span class="t">This will be multiplied.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5046" target="_blank">01:24:06.560</a></span> | <span class="t">This is the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5047" target="_blank">01:24:07.600</a></span> | <span class="t">So it will be multiplied by the transposed of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5050" target="_blank">01:24:10.400</a></span> | <span class="t">This will produce this matrix here, which is one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5053" target="_blank">01:24:13.200</a></span> | <span class="t">So you can check the dimensions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5054" target="_blank">01:24:14.480</a></span> | <span class="t">One by 4,096 multiplied by 4,006 by one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5059" target="_blank">01:24:19.120</a></span> | <span class="t">will output a matrix that is one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5061" target="_blank">01:24:21.360</a></span> | <span class="t">This will be multiplied by the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5065" target="_blank">01:24:25.040</a></span> | <span class="t">and this will result in the output token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5067" target="_blank">01:24:27.120</a></span> | <span class="t">So this is the inferencing step one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5069" target="_blank">01:24:29.200</a></span> | <span class="t">in which the only token we give is the start of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5072" target="_blank">01:24:32.320</a></span> | <span class="t">Then we take this token output, the token at the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5077" target="_blank">01:24:37.040</a></span> | <span class="t">This is actually not the token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5079" target="_blank">01:24:39.840</a></span> | <span class="t">because this has to be mapped to the linear layer, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5082" target="_blank">01:24:42.480</a></span> | <span class="t">But suppose this is already the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5084" target="_blank">01:24:44.400</a></span> | <span class="t">And we append it to the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5088" target="_blank">01:24:48.880</a></span> | <span class="t">So it becomes the second input of the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5092" target="_blank">01:24:52.880</a></span> | <span class="t">So this is SOS and this is the last output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5095" target="_blank">01:24:55.920</a></span> | <span class="t">We multiply it by the transposed of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5098" target="_blank">01:24:58.640</a></span> | <span class="t">We get this matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5100" target="_blank">01:25:00.960</a></span> | <span class="t">We multiply it by the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5102" target="_blank">01:25:02.480</a></span> | <span class="t">and we get two output tokens as output</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5105" target="_blank">01:25:05.200</a></span> | <span class="t">because it's a sequence to sequence model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5109" target="_blank">01:25:09.280</a></span> | <span class="t">Then we append the output of the previous as the input</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5114" target="_blank">01:25:14.160</a></span> | <span class="t">and we multiply it by the transposed of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5116" target="_blank">01:25:16.800</a></span> | <span class="t">We get this matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5118" target="_blank">01:25:18.160</a></span> | <span class="t">We then multiply it by the Vs</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5119" target="_blank">01:25:19.600</a></span> | <span class="t">and we get three tokens as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5121" target="_blank">01:25:21.440</a></span> | <span class="t">We then append the output of the last one at the Q.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5125" target="_blank">01:25:25.280</a></span> | <span class="t">We multiply it by the transposed of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5127" target="_blank">01:25:27.440</a></span> | <span class="t">We get this matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5128" target="_blank">01:25:28.720</a></span> | <span class="t">We multiply it by the V and we get this sequence as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5132" target="_blank">01:25:32.000</a></span> | <span class="t">But we see some problems.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5133" target="_blank">01:25:33.600</a></span> | <span class="t">And the one that I told you before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5135" target="_blank">01:25:35.120</a></span> | <span class="t">We are doing a lot of computations that we don't need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5137" target="_blank">01:25:37.680</a></span> | <span class="t">First of all, these dot products that we are computing here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5144" target="_blank">01:25:44.080</a></span> | <span class="t">because this is the self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5145" target="_blank">01:25:45.520</a></span> | <span class="t">So Q multiplied by the transposed of the keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5148" target="_blank">01:25:48.320</a></span> | <span class="t">will result in a lot of dot products that result in this matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5151" target="_blank">01:25:51.600</a></span> | <span class="t">These dot products that you see here highlighted in violet</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5155" target="_blank">01:25:55.520</a></span> | <span class="t">have been already computed at the previous steps</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5157" target="_blank">01:25:57.920</a></span> | <span class="t">because we are at the step number four</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5159" target="_blank">01:25:59.760</a></span> | <span class="t">but these have already been computed at the previous step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5162" target="_blank">01:26:02.240</a></span> | <span class="t">Plus, not only they have been computed already,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5165" target="_blank">01:26:05.120</a></span> | <span class="t">we don't need them because we only are interested in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5168" target="_blank">01:26:08.720</a></span> | <span class="t">what the latest token that we added as the input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5171" target="_blank">01:26:11.680</a></span> | <span class="t">so to the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5172" target="_blank">01:26:12.880</a></span> | <span class="t">what is this tokens dot product with all the other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5179" target="_blank">01:26:19.120</a></span> | <span class="t">because this tokens dot product with all the other tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5182" target="_blank">01:26:22.080</a></span> | <span class="t">will result in the output of the last token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5185" target="_blank">01:26:25.920</a></span> | <span class="t">the one we are interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5187" target="_blank">01:26:27.760</a></span> | <span class="t">So if there is a way to not do all these computations again</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5193" target="_blank">01:26:33.120</a></span> | <span class="t">and also to not output all the previous tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5196" target="_blank">01:26:36.160</a></span> | <span class="t">that we actually don't need</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5197" target="_blank">01:26:37.440</a></span> | <span class="t">because we always access the latest token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5200" target="_blank">01:26:40.320</a></span> | <span class="t">yes, we just use the KVCache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5202" target="_blank">01:26:42.720</a></span> | <span class="t">In the KVCache, what we do is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5206" target="_blank">01:26:46.160</a></span> | <span class="t">we always take the last token and we use it as input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5212" target="_blank">01:26:52.080</a></span> | <span class="t">So we don't append it to the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5214" target="_blank">01:26:54.160</a></span> | <span class="t">We just use it directly as query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5216" target="_blank">01:26:56.480</a></span> | <span class="t">But because the query needs to access all the previous tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5221" target="_blank">01:27:01.600</a></span> | <span class="t">we keep the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5223" target="_blank">01:27:03.600</a></span> | <span class="t">So we append the last input to the keys and the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5227" target="_blank">01:27:07.280</a></span> | <span class="t">but we don't append it to the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5229" target="_blank">01:27:09.360</a></span> | <span class="t">We replace it entirely with the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5232" target="_blank">01:27:12.800</a></span> | <span class="t">Let's see with an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5234" target="_blank">01:27:14.800</a></span> | <span class="t">For example, this is our first step of inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5238" target="_blank">01:27:18.320</a></span> | <span class="t">So this is just the start of sentence token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5241" target="_blank">01:27:21.280</a></span> | <span class="t">So we just have one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5242" target="_blank">01:27:22.720</a></span> | <span class="t">We multiply it by the transpose of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5245" target="_blank">01:27:25.520</a></span> | <span class="t">It will result in one by one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5246" target="_blank">01:27:26.960</a></span> | <span class="t">So we only have one token as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5248" target="_blank">01:27:28.480</a></span> | <span class="t">This token, in the previous case, was appended to the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5253" target="_blank">01:27:33.520</a></span> | <span class="t">So in the next step, it became a matrix of dimension 2 by 4096.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5259" target="_blank">01:27:39.680</a></span> | <span class="t">But in our case, at the time step 2, we don't append it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5263" target="_blank">01:27:43.360</a></span> | <span class="t">We only append it to the end of the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5266" target="_blank">01:27:46.560</a></span> | <span class="t">And we only keep the queries here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5270" target="_blank">01:27:50.240</a></span> | <span class="t">If we do this product again now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5272" target="_blank">01:27:52.080</a></span> | <span class="t">we will see that this row here is the only one we are interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5275" target="_blank">01:27:55.520</a></span> | <span class="t">So the one that was not violet in the previous diagram.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5278" target="_blank">01:27:58.720</a></span> | <span class="t">And if we do this dot product,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5280" target="_blank">01:28:00.960</a></span> | <span class="t">it will result in only the last token, the one we are interested in.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5284" target="_blank">01:28:04.560</a></span> | <span class="t">And every time we keep doing this job,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5286" target="_blank">01:28:06.400</a></span> | <span class="t">we will see the key and the values grow.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5290" target="_blank">01:28:10.160</a></span> | <span class="t">The queries will be always the last token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5294" target="_blank">01:28:14.880</a></span> | <span class="t">But the number of dot products that we are doing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5297" target="_blank">01:28:17.920</a></span> | <span class="t">during the inferencing is much less.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5299" target="_blank">01:28:19.680</a></span> | <span class="t">We don't need to do all those dot products that we did before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5302" target="_blank">01:28:22.560</a></span> | <span class="t">So compare this is time step 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5304" target="_blank">01:28:24.400</a></span> | <span class="t">This is 4 dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5307" target="_blank">01:28:27.120</a></span> | <span class="t">Compare it with the previous time step 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5309" target="_blank">01:28:29.360</a></span> | <span class="t">So here we have 16 dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5313" target="_blank">01:28:33.200</a></span> | <span class="t">So we reduce it by a factor of 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5315" target="_blank">01:28:35.520</a></span> | <span class="t">And so that's why it's much faster to do inferencing with the KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5322" target="_blank">01:28:42.160</a></span> | <span class="t">And let's review again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5324" target="_blank">01:28:44.480</a></span> | <span class="t">So here, as you can see, the matrix QK.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5327" target="_blank">01:28:47.280</a></span> | <span class="t">And every time we add the token to the queue grows, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5330" target="_blank">01:28:50.720</a></span> | <span class="t">But all these previous values with the KV cache, we are not computing it again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5334" target="_blank">01:28:54.640</a></span> | <span class="t">So that's why this is much faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5336" target="_blank">01:28:56.240</a></span> | <span class="t">We only compute the one we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5338" target="_blank">01:28:58.240</a></span> | <span class="t">And we only get one token as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5341" target="_blank">01:29:01.600</a></span> | <span class="t">If this mechanism is not clear, please watch my previous video about LAMA,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5344" target="_blank">01:29:04.880</a></span> | <span class="t">in which I describe it in much more detail and also with much more visualizations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5350" target="_blank">01:29:10.960</a></span> | <span class="t">Now let's go build this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5355" target="_blank">01:29:15.440</a></span> | <span class="t">There is another thing actually I want to show you before we go to build it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5358" target="_blank">01:29:18.800</a></span> | <span class="t">which is the grouped query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5361" target="_blank">01:29:21.040</a></span> | <span class="t">This one here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5363" target="_blank">01:29:23.120</a></span> | <span class="t">So I call it grouped multi-query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5366" target="_blank">01:29:26.400</a></span> | <span class="t">because it's actually the successive version of the multi-query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5370" target="_blank">01:29:30.240</a></span> | <span class="t">It's something in between the multi-head attention and the multi-query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5373" target="_blank">01:29:33.520</a></span> | <span class="t">But actually, the real name is grouped query attention in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5376" target="_blank">01:29:36.880</a></span> | <span class="t">Also, it's called grouped query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5378" target="_blank">01:29:38.560</a></span> | <span class="t">Now, the reason we introduced the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5384" target="_blank">01:29:44.400</a></span> | <span class="t">is first of all, we had the multi-query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5387" target="_blank">01:29:47.680</a></span> | <span class="t">The multi-query attention basically were introduced to solve one problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5391" target="_blank">01:29:51.840</a></span> | <span class="t">That is, we first had the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5395" target="_blank">01:29:55.920</a></span> | <span class="t">We introduced the KV cache with the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5399" target="_blank">01:29:59.920</a></span> | <span class="t">Just the one we just saw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5402" target="_blank">01:30:02.240</a></span> | <span class="t">The problem was that with the multi-head attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5404" target="_blank">01:30:04.480</a></span> | <span class="t">we were doing too many dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5406" target="_blank">01:30:06.400</a></span> | <span class="t">With the multi-head with the KV cache, we do less dot products.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5409" target="_blank">01:30:09.600</a></span> | <span class="t">This resulted in a lot less computation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5413" target="_blank">01:30:13.120</a></span> | <span class="t">But it also resulted in a new bottleneck for the algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5416" target="_blank">01:30:16.560</a></span> | <span class="t">So the bottleneck was not longer the number of computations,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5419" target="_blank">01:30:19.680</a></span> | <span class="t">but how many memory access we were performing to access these tensors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5424" target="_blank">01:30:24.560</a></span> | <span class="t">Because in the GPU,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5426" target="_blank">01:30:26.480</a></span> | <span class="t">the GPU is much faster at doing computations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5432" target="_blank">01:30:32.800</a></span> | <span class="t">than it is at moving tensors around in its memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5436" target="_blank">01:30:36.320</a></span> | <span class="t">So when we optimize an algorithm,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5438" target="_blank">01:30:38.480</a></span> | <span class="t">we not only need to consider how many operations we are doing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5442" target="_blank">01:30:42.560</a></span> | <span class="t">but also how many tensors we are accessing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5444" target="_blank">01:30:44.960</a></span> | <span class="t">and where are these tensors located.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5446" target="_blank">01:30:46.880</a></span> | <span class="t">So it's not a good idea to keep copying tensor from one place to another,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5451" target="_blank">01:30:51.280</a></span> | <span class="t">because the GPU is much slower at copying memory from one place to another</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5454" target="_blank">01:30:54.960</a></span> | <span class="t">than it is at computing operations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5457" target="_blank">01:30:57.040</a></span> | <span class="t">And this can be visualized on the datasheet of the GPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5462" target="_blank">01:31:02.560</a></span> | <span class="t">You can see, for example, that computing operations</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5464" target="_blank">01:31:04.640</a></span> | <span class="t">is 19.5 Tera floating point operations per second.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5468" target="_blank">01:31:08.640</a></span> | <span class="t">And while the memory bandwidth, so how fast it can move memory,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5472" target="_blank">01:31:12.320</a></span> | <span class="t">is 40 times slower.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5474" target="_blank">01:31:14.000</a></span> | <span class="t">So we need to optimize algorithms also for managing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5480" target="_blank">01:31:20.480</a></span> | <span class="t">how many tensors we access and how we move them around the memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5484" target="_blank">01:31:24.480</a></span> | <span class="t">This is why we introduced the multi-query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5488" target="_blank">01:31:28.000</a></span> | <span class="t">The multi-query attention basically means that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5489" target="_blank">01:31:29.920</a></span> | <span class="t">we have many heads for the queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5494" target="_blank">01:31:34.080</a></span> | <span class="t">but we only have one head for the key and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5498" target="_blank">01:31:38.000</a></span> | <span class="t">This resulted in a new algorithm that was much more efficient</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5502" target="_blank">01:31:42.960</a></span> | <span class="t">than the algorithm just with the KVCache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5507" target="_blank">01:31:47.120</a></span> | <span class="t">Because the KVCache, yeah, it reduced the number of dot products,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5510" target="_blank">01:31:50.480</a></span> | <span class="t">but it had a new bottleneck, that is the number of memory access.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5513" target="_blank">01:31:53.600</a></span> | <span class="t">With this algorithm, we also may optimize the memory access,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5516" target="_blank">01:31:56.560</a></span> | <span class="t">but we lose some quality,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5519" target="_blank">01:31:59.440</a></span> | <span class="t">because we are reducing the number of heads for the key and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5522" target="_blank">01:32:02.960</a></span> | <span class="t">So we are reducing the number of parameters in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5526" target="_blank">01:32:06.400</a></span> | <span class="t">And this way, the model, because we are reducing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5531" target="_blank">01:32:11.440</a></span> | <span class="t">the number of parameters involved in the attention mechanism,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5534" target="_blank">01:32:14.240</a></span> | <span class="t">of course the model will degrade in quality.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5537" target="_blank">01:32:17.680</a></span> | <span class="t">But we saw that practically it degraded the quality not so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5542" target="_blank">01:32:22.560</a></span> | <span class="t">So actually the quality was not bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5544" target="_blank">01:32:24.560</a></span> | <span class="t">And this was in this paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5546" target="_blank">01:32:26.800</a></span> | <span class="t">So they show that the quality degradation was very little,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5550" target="_blank">01:32:30.400</a></span> | <span class="t">so from 26.7 to 26.5,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5553" target="_blank">01:32:33.920</a></span> | <span class="t">but the performance gains were very important.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5557" target="_blank">01:32:37.200</a></span> | <span class="t">We went from 48 microseconds per token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5561" target="_blank">01:32:41.440</a></span> | <span class="t">to 5 microseconds or 6 microseconds per token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5564" target="_blank">01:32:44.560</a></span> | <span class="t">so a lot faster.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5565" target="_blank">01:32:45.600</a></span> | <span class="t">Now, let's introduce the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5571" target="_blank">01:32:51.520</a></span> | <span class="t">or the grouped multi-query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5572" target="_blank">01:32:52.960</a></span> | <span class="t">In the multi-head attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5575" target="_blank">01:32:55.360</a></span> | <span class="t">we had n heads for the queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5579" target="_blank">01:32:59.520</a></span> | <span class="t">n heads for the keys and n heads for the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5582" target="_blank">01:33:02.720</a></span> | <span class="t">In the multi-query attention, we have n heads for the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5587" target="_blank">01:33:07.520</a></span> | <span class="t">but only one head for the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5590" target="_blank">01:33:10.000</a></span> | <span class="t">In the grouped multi-query attention or the grouped query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5593" target="_blank">01:33:13.920</a></span> | <span class="t">we have less number of heads for the keys and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5600" target="_blank">01:33:20.720</a></span> | <span class="t">So every two heads for the queries, in this case, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5604" target="_blank">01:33:24.160</a></span> | <span class="t">we will have one head for the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5608" target="_blank">01:33:28.960</a></span> | <span class="t">And this is a good balance between quality and speeds,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5613" target="_blank">01:33:33.120</a></span> | <span class="t">because, of course, the fastest one is this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5615" target="_blank">01:33:35.920</a></span> | <span class="t">because you have less heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5617" target="_blank">01:33:37.280</a></span> | <span class="t">But, of course, the best one from a quality point of view is this one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5622" target="_blank">01:33:42.560</a></span> | <span class="t">but this is a good compromise between the two.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5624" target="_blank">01:33:44.720</a></span> | <span class="t">So you don't lose quality, but at the same time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5627" target="_blank">01:33:47.520</a></span> | <span class="t">you also optimize the speed compared to the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5630" target="_blank">01:33:50.880</a></span> | <span class="t">So now that we have reviewed all this concept, let's go build it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5634" target="_blank">01:33:54.960</a></span> | <span class="t">So please, again, if you didn't understand very much in detail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5639" target="_blank">01:33:59.280</a></span> | <span class="t">this part is better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5640" target="_blank">01:34:00.640</a></span> | <span class="t">You go to review my other video about Llama,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5642" target="_blank">01:34:02.960</a></span> | <span class="t">in which I explain all this part much better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5644" target="_blank">01:34:04.960</a></span> | <span class="t">Otherwise, if I have to repeat the same content of the previous video,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5648" target="_blank">01:34:08.960</a></span> | <span class="t">this would be the current video would become 10 hours.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5652" target="_blank">01:34:12.400</a></span> | <span class="t">So let's go build it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5661" target="_blank">01:34:21.680</a></span> | <span class="t">Okay, we need to save some things.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5676" target="_blank">01:34:36.480</a></span> | <span class="t">Compared to the original code from Facebook, from Meta,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5682" target="_blank">01:34:42.000</a></span> | <span class="t">I actually removed the parallelization.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5684" target="_blank">01:34:44.320</a></span> | <span class="t">First of all, because I cannot test it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5686" target="_blank">01:34:46.080</a></span> | <span class="t">I don't have multiple GPUs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5687" target="_blank">01:34:47.520</a></span> | <span class="t">I don't have a very powerful GPU, actually.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5689" target="_blank">01:34:49.520</a></span> | <span class="t">And so I simplified the code a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5703" target="_blank">01:35:03.280</a></span> | <span class="t">And KVHeads indicates the number of heads for the keys and the values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5717" target="_blank">01:35:17.200</a></span> | <span class="t">because they can be different than the number of heads for the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5721" target="_blank">01:35:21.680</a></span> | <span class="t">And this is why we also have an headsQueue.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5749" target="_blank">01:35:49.600</a></span> | <span class="t">This value here represents the ratio between the number of heads for the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5754" target="_blank">01:35:54.560</a></span> | <span class="t">and the number of heads for the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5757" target="_blank">01:35:57.520</a></span> | <span class="t">We will use it later when we calculate the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5759" target="_blank">01:35:59.760</a></span> | <span class="t">So let me write some comments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5762" target="_blank">01:36:02.000</a></span> | <span class="t">So this is…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5781" target="_blank">01:36:21.360</a></span> | <span class="t">So…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5782" target="_blank">01:36:22.800</a></span> | <span class="t">And then we have a self.headDimension, which is…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5809" target="_blank">01:36:49.120</a></span> | <span class="t">Which is…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5814" target="_blank">01:36:54.720</a></span> | <span class="t">This indicates the part of the embedding that will be visualized by each head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5819" target="_blank">01:36:59.920</a></span> | <span class="t">Because, as you know, the embedding is split into multiple heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5822" target="_blank">01:37:02.960</a></span> | <span class="t">So each head will watch the full sentence, but a part of the embedding of each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5837" target="_blank">01:37:17.760</a></span> | <span class="t">Then we have the W matrices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5839" target="_blank">01:37:19.520</a></span> | <span class="t">WQ, WK, WV, and WO, just like in the normal vanilla transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5844" target="_blank">01:37:24.880</a></span> | <span class="t">And they don't have any bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5861" target="_blank">01:37:41.680</a></span> | <span class="t">Oops, why did I write true?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5891" target="_blank">01:38:11.520</a></span> | <span class="t">And then we create a cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5910" target="_blank">01:38:30.800</a></span> | <span class="t">We will see later how it's used.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5912" target="_blank">01:38:32.560</a></span> | <span class="t">I just now created one for the keys and one for the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5919" target="_blank">01:38:39.600</a></span> | <span class="t">So…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5930" target="_blank">01:38:50.560</a></span> | <span class="t">Okay, finally, we implement the forward method, which is the salient part here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5959" target="_blank">01:39:19.520</a></span> | <span class="t">So, self x is…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5964" target="_blank">01:39:24.640</a></span> | <span class="t">To simplify the code for you, I will write the…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5970" target="_blank">01:39:30.000</a></span> | <span class="t">For each operation, I will write the dimensions of the tensor that is involved in the operation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5975" target="_blank">01:39:35.040</a></span> | <span class="t">and also the resulting tensor from each operation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5978" target="_blank">01:39:38.000</a></span> | <span class="t">The start position indicates just the position of the token inside of the sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5988" target="_blank">01:39:48.000</a></span> | <span class="t">And these are the frequencies that we have computed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=5990" target="_blank">01:39:50.640</a></span> | <span class="t">Okay, let's start by extracting pipe size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6002" target="_blank">01:40:02.400</a></span> | <span class="t">B, sequence length, and dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6016" target="_blank">01:40:16.320</a></span> | <span class="t">But the sequence length, we know it's one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6018" target="_blank">01:40:18.320</a></span> | <span class="t">So, dimension, yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6021" target="_blank">01:40:21.360</a></span> | <span class="t">Then what we do is we multiply, just like in the original transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6026" target="_blank">01:40:26.960</a></span> | <span class="t">we take the query, the key, and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6028" target="_blank">01:40:28.640</a></span> | <span class="t">We multiply it by then the WQ, WK, and WK matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6032" target="_blank">01:40:32.320</a></span> | <span class="t">So, xq is equal to self.wq.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6039" target="_blank">01:40:39.680</a></span> | <span class="t">This means going from B, one dimension, to B, one head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6049" target="_blank">01:40:49.120</a></span> | <span class="t">So, the number of heads for the query multiplied by the dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6052" target="_blank">01:40:52.640</a></span> | <span class="t">because we are…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6053" target="_blank">01:40:53.280</a></span> | <span class="t">In this case, we are…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6056" target="_blank">01:40:56.560</a></span> | <span class="t">This is actually equal to dim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6058" target="_blank">01:40:58.080</a></span> | <span class="t">So, the number of heads multiply the head dimension, as you can see from here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6065" target="_blank">01:41:05.600</a></span> | <span class="t">So, we are not changing the shape.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6075" target="_blank">01:41:15.040</a></span> | <span class="t">In this case, however, we may change the shape of the…</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6088" target="_blank">01:41:28.560</a></span> | <span class="t">Because the number of heads for the kv may be smaller than q.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6094" target="_blank">01:41:34.080</a></span> | <span class="t">So, this matrix may have a last dimension that is smaller than xq.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6098" target="_blank">01:41:38.960</a></span> | <span class="t">And the same is for xv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6102" target="_blank">01:41:42.640</a></span> | <span class="t">So, here, let me write some comment.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6108" target="_blank">01:41:48.640</a></span> | <span class="t">Apply the WQ, WK, and WV matrix to queries, keys, and values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6122" target="_blank">01:42:02.320</a></span> | <span class="t">which are the same, because it's a self-attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6124" target="_blank">01:42:04.320</a></span> | <span class="t">So, the query, key, and value is always x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6127" target="_blank">01:42:07.440</a></span> | <span class="t">We then divide them into their corresponding number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6133" target="_blank">01:42:13.840</a></span> | <span class="t">So, xq is equal to xq.q.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6138" target="_blank">01:42:18.480</a></span> | <span class="t">Batch size, we keep it like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6141" target="_blank">01:42:21.200</a></span> | <span class="t">Sequence length is one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6150" target="_blank">01:42:30.000</a></span> | <span class="t">So, we divide b1, h, q multiplied by head dimension into b1, head, h, q, and head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6166" target="_blank">01:42:46.160</a></span> | <span class="t">So, we divide them into the h heads for the query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6172" target="_blank">01:42:52.720</a></span> | <span class="t">And then we do the same for the key and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6180" target="_blank">01:43:00.240</a></span> | <span class="t">And the same for the b.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6195" target="_blank">01:43:15.200</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6201" target="_blank">01:43:21.760</a></span> | <span class="t">so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6229" target="_blank">01:43:49.360</a></span> | <span class="t">now, we have multiplied, okay, we have the x input,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6234" target="_blank">01:43:54.000</a></span> | <span class="t">we multiply it by the WQ, WK, and WK, y.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6237" target="_blank">01:43:57.280</a></span> | <span class="t">Let's go check the code here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6239" target="_blank">01:43:59.040</a></span> | <span class="t">As you remember, we take the input, we multiply it by WQ, WK, and WV.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6244" target="_blank">01:44:04.560</a></span> | <span class="t">This will result in these matrices here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6246" target="_blank">01:44:06.880</a></span> | <span class="t">We then divide them into the number of heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6249" target="_blank">01:44:09.440</a></span> | <span class="t">But in the case of grouped query attention, they may be different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6252" target="_blank">01:44:12.160</a></span> | <span class="t">So, this may be four heads, and this may be two heads, and this may be two heads.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6256" target="_blank">01:44:16.800</a></span> | <span class="t">So, they are not the same number.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6258" target="_blank">01:44:18.960</a></span> | <span class="t">The next thing we are going to do, and this is present in the here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6264" target="_blank">01:44:24.400</a></span> | <span class="t">we need to apply the rotary positional encodings to the query</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6270" target="_blank">01:44:30.320</a></span> | <span class="t">and the keys, but not the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6272" target="_blank">01:44:32.240</a></span> | <span class="t">Let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6277" target="_blank">01:44:37.680</a></span> | <span class="t">And this is how we apply the positional encodings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6289" target="_blank">01:44:49.440</a></span> | <span class="t">This will not change the size of the vectors.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6291" target="_blank">01:44:51.680</a></span> | <span class="t">You can see that here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6293" target="_blank">01:44:53.760</a></span> | <span class="t">Because at the end, we have the same shape as the original input vector.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6311" target="_blank">01:45:11.360</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6326" target="_blank">01:45:26.480</a></span> | <span class="t">Now, now comes the KVCache part.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6330" target="_blank">01:45:30.960</a></span> | <span class="t">Let's watch again the slides.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6332" target="_blank">01:45:32.640</a></span> | <span class="t">As we can see here, every time we have an input-output token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6340" target="_blank">01:45:40.160</a></span> | <span class="t">so for example, the attention 2 here, it supposes the token number 2,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6344" target="_blank">01:45:44.320</a></span> | <span class="t">we append it at the end of the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6348" target="_blank">01:45:48.320</a></span> | <span class="t">And this is exactly what we are going to do.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6351" target="_blank">01:45:51.200</a></span> | <span class="t">So, what we do here, we keep a cache of the keys and the values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6356" target="_blank">01:45:56.800</a></span> | <span class="t">because they will be used for the next iterations.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6359" target="_blank">01:45:59.840</a></span> | <span class="t">Because at every iteration, in X, we only receive the latest token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6364" target="_blank">01:46:04.640</a></span> | <span class="t">that was output from the previous iteration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6367" target="_blank">01:46:07.840</a></span> | <span class="t">We append it to the K and the V, and then we compute the attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6374" target="_blank">01:46:14.080</a></span> | <span class="t">between all the K, all the V, but only the single token as query.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6378" target="_blank">01:46:18.800</a></span> | <span class="t">So, let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6382" target="_blank">01:46:22.320</a></span> | <span class="t">So, first, replace.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6396" target="_blank">01:46:36.640</a></span> | <span class="t">This is the position of the token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6398" target="_blank">01:46:38.080</a></span> | <span class="t">This should be 1, because sequence length is actually 1, always.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6404" target="_blank">01:46:44.080</a></span> | <span class="t">But I try to keep this code the same as the one from Lama, from Meta.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6411" target="_blank">01:46:51.840</a></span> | <span class="t">This is, basically, it means that if we have one token from many batches,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6421" target="_blank">01:47:01.920</a></span> | <span class="t">I mean, we have one token for every batch, we replace them,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6425" target="_blank">01:47:05.600</a></span> | <span class="t">because we can process multiple batches.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6430" target="_blank">01:47:10.720</a></span> | <span class="t">So, we replace the entry for this particular position for every batch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6435" target="_blank">01:47:15.520</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6445" target="_blank">01:47:25.860</a></span> | <span class="t">Now, we replace it only for this position here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6451" target="_blank">01:47:31.360</a></span> | <span class="t">But when we compute the attention using the KVCache, let's go watch again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6456" target="_blank">01:47:36.320</a></span> | <span class="t">we need to calculate the dot product between the only one token but all the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6466" target="_blank">01:47:46.960</a></span> | <span class="t">And then we will need to multiply with all the values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6469" target="_blank">01:47:49.840</a></span> | <span class="t">and this will result in only one token as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6472" target="_blank">01:47:52.320</a></span> | <span class="t">So, we need to extract from this cache all the tokens as keys</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6476" target="_blank">01:47:56.800</a></span> | <span class="t">and all the tokens as values up to this position here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6481" target="_blank">01:48:01.200</a></span> | <span class="t">The one we are passing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6482" target="_blank">01:48:02.480</a></span> | <span class="t">So, keys is equal to all.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6501" target="_blank">01:48:21.040</a></span> | <span class="t">So, starting from 0 up to startPos plus sequenceLength,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6507" target="_blank">01:48:27.840</a></span> | <span class="t">and the values are length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6526" target="_blank">01:48:46.800</a></span> | <span class="t">Now, what happens is that, let me write also some sizes here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6537" target="_blank">01:48:57.600</a></span> | <span class="t">We have b, sequenceLength of K and V,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6541" target="_blank">01:49:01.760</a></span> | <span class="t">because the sequenceLength of the input is always 1, we know that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6545" target="_blank">01:49:05.280</a></span> | <span class="t">But the sequenceLength of the cache means all the cached keys and values,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6554" target="_blank">01:49:14.000</a></span> | <span class="t">which are up to startPosition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6556" target="_blank">01:49:16.320</a></span> | <span class="t">So, this sequenceLength is actually equal to startPosition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6561" target="_blank">01:49:21.760</a></span> | <span class="t">And actually, startPosition plus 1.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6564" target="_blank">01:49:24.800</a></span> | <span class="t">My next dimension is the number of heads for the K and V,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6572" target="_blank">01:49:32.880</a></span> | <span class="t">and then the dimension of each head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6574" target="_blank">01:49:34.640</a></span> | <span class="t">Now, the number of heads for the keys and values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6581" target="_blank">01:49:41.200</a></span> | <span class="t">may not correspond to the number of heads of the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6586" target="_blank">01:49:46.400</a></span> | <span class="t">So, how do we compute?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6588" target="_blank">01:49:48.000</a></span> | <span class="t">In the original code from Lama, what they did was basically,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6591" target="_blank">01:49:51.120</a></span> | <span class="t">let's go check the code for here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6594" target="_blank">01:49:54.640</a></span> | <span class="t">So, in the grouped query, attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6598" target="_blank">01:49:58.720</a></span> | <span class="t">we have that the number of heads for the keys and the values</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6601" target="_blank">01:50:01.840</a></span> | <span class="t">is not the same as the number of heads for the queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6604" target="_blank">01:50:04.240</a></span> | <span class="t">So, there are two ways.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6606" target="_blank">01:50:06.080</a></span> | <span class="t">One is to make an optimized algorithm</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6608" target="_blank">01:50:08.080</a></span> | <span class="t">that actually takes this into consideration.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6610" target="_blank">01:50:10.160</a></span> | <span class="t">The other way is to just copy this single head into multiple heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6619" target="_blank">01:50:19.040</a></span> | <span class="t">such that we arrive to this situation here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6621" target="_blank">01:50:21.520</a></span> | <span class="t">and then we just compute it just like a multi-head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6624" target="_blank">01:50:24.320</a></span> | <span class="t">This is not an optimized solution, but it's the one used by the code by Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6628" target="_blank">01:50:28.720</a></span> | <span class="t">And it's also the one I will be sticking to,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6631" target="_blank">01:50:31.680</a></span> | <span class="t">because I don't have any way of testing other codes,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6635" target="_blank">01:50:35.760</a></span> | <span class="t">because the only model that supports the grouped query attention</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6640" target="_blank">01:50:40.480</a></span> | <span class="t">is the biggest one from Lama, so with 70 billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6644" target="_blank">01:50:44.240</a></span> | <span class="t">but my computer will never be able to load that model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6647" target="_blank">01:50:47.600</a></span> | <span class="t">And so, I don't have any way of testing it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6649" target="_blank">01:50:49.600</a></span> | <span class="t">so that's why I also didn't optimize the code</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6652" target="_blank">01:50:52.240</a></span> | <span class="t">for actually computing the grouped query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6654" target="_blank">01:50:54.160</a></span> | <span class="t">but I will just replicate this single head multiple times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6657" target="_blank">01:50:57.680</a></span> | <span class="t">such that we arrive to this situation here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6661" target="_blank">01:51:01.520</a></span> | <span class="t">So, I will also repeat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6667" target="_blank">01:51:07.920</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6676" target="_blank">01:51:16.320</a></span> | <span class="t">okay, this function here, repeat_kv,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6698" target="_blank">01:51:38.000</a></span> | <span class="t">just repeats the keys until we reach the number of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6703" target="_blank">01:51:43.280</a></span> | <span class="t">for this number of times, so nrep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6705" target="_blank">01:51:45.920</a></span> | <span class="t">What is this?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6706" target="_blank">01:51:46.720</a></span> | <span class="t">It's the ratio of the number of heads of the queries</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6709" target="_blank">01:51:49.600</a></span> | <span class="t">by the number of heads of the keys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6711" target="_blank">01:51:51.600</a></span> | <span class="t">So, if the number of heads of the keys is four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6715" target="_blank">01:51:55.040</a></span> | <span class="t">and the number of heads for the queries is eight,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6717" target="_blank">01:51:57.200</a></span> | <span class="t">that means we need to repeat twice each head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6719" target="_blank">01:51:59.840</a></span> | <span class="t">So, let's build also this method, since we are here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6723" target="_blank">01:52:03.200</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6737" target="_blank">01:52:17.200</a></span> | <span class="t">okay, we don't need to repeat it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6751" target="_blank">01:52:31.040</a></span> | <span class="t">so there is only one repetition.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6752" target="_blank">01:52:32.480</a></span> | <span class="t">We just return the basic tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6756" target="_blank">01:52:36.080</a></span> | <span class="t">Otherwise, we repeat it n times.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6757" target="_blank">01:52:37.680</a></span> | <span class="t">So, the first thing we do is we add a new dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6777" target="_blank">01:52:57.360</a></span> | <span class="t">and we can do like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6778" target="_blank">01:52:58.640</a></span> | <span class="t">part_sequence_length, number_of_heads, then nothing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6788" target="_blank">01:53:08.880</a></span> | <span class="t">and then this will add this new dimension in this position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6794" target="_blank">01:53:14.640</a></span> | <span class="t">Then we expand it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6803" target="_blank">01:53:23.360</a></span> | <span class="t">And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6813" target="_blank">01:53:33.360</a></span> | <span class="t">then we reshape it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6826" target="_blank">01:53:46.880</a></span> | <span class="t">Basically, we introduce a new dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6831" target="_blank">01:53:51.760</a></span> | <span class="t">We repeat all the sequence this dimension number of times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6836" target="_blank">01:53:56.240</a></span> | <span class="t">along this dimension n-wrap number of times,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6840" target="_blank">01:54:00.560</a></span> | <span class="t">and then we just flatten it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6842" target="_blank">01:54:02.640</a></span> | <span class="t">So, we remove again this dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6847" target="_blank">01:54:07.840</a></span> | <span class="t">And this is how we repeat the keys and also the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6854" target="_blank">01:54:14.240</a></span> | <span class="t">Now we can repeat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6863" target="_blank">01:54:23.600</a></span> | <span class="t">Okay, now we just proceed just like with the standard,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6869" target="_blank">01:54:29.120</a></span> | <span class="t">the standard calculation for the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6874" target="_blank">01:54:34.400</a></span> | <span class="t">That is, we first move the head dimension before the sequence dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6882" target="_blank">01:54:42.080</a></span> | <span class="t">because each head will watch all the sequence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6884" target="_blank">01:54:44.720</a></span> | <span class="t">but a part of the embedding of each token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6887" target="_blank">01:54:47.440</a></span> | <span class="t">So, what we are doing is batch 1,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6895" target="_blank">01:54:55.680</a></span> | <span class="t">because 1 is the sequence length of the queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6898" target="_blank">01:54:58.000</a></span> | <span class="t">the number of heads of the queries, and head dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6905" target="_blank">01:55:05.680</a></span> | <span class="t">batch head sequence length and head dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6910" target="_blank">01:55:10.720</a></span> | <span class="t">We do the same for the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6922" target="_blank">01:55:22.000</a></span> | <span class="t">Then we do the standard formula for queries multiplied by the transpose of the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6934" target="_blank">01:55:34.160</a></span> | <span class="t">divided by the square root of the dimension of each head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6937" target="_blank">01:55:37.040</a></span> | <span class="t">So, xq, so the queries multiplied by the transpose of the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6948" target="_blank">01:55:48.960</a></span> | <span class="t">all of this divided by the square root of the dimension of each head.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6957" target="_blank">01:55:57.200</a></span> | <span class="t">Then we apply the softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6963" target="_blank">01:56:03.360</a></span> | <span class="t">and this one will result in a shape of queries,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6975" target="_blank">01:56:15.520</a></span> | <span class="t">one head dimension multiplied by qv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6994" target="_blank">01:56:34.720</a></span> | <span class="t">The softmax doesn't change the dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=6998" target="_blank">01:56:38.800</a></span> | <span class="t">Then we multiply it by the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7004" target="_blank">01:56:44.960</a></span> | <span class="t">So, the formula is queries multiplied by the transpose of the keys,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7010" target="_blank">01:56:50.960</a></span> | <span class="t">and then we do the softmax, then the output is multiplied by the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7020" target="_blank">01:57:00.400</a></span> | <span class="t">So, this will result in b,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7048" target="_blank">01:57:28.160</a></span> | <span class="t">and then we multiply it by the output matrix,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7052" target="_blank">01:57:32.800</a></span> | <span class="t">but before we remove all the heads, so we concatenate again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7056" target="_blank">01:57:36.160</a></span> | <span class="t">This is what we did also here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7059" target="_blank">01:57:39.040</a></span> | <span class="t">So, here we take the output of all the heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7062" target="_blank">01:57:42.000</a></span> | <span class="t">then we concatenate them together, and then we multiply it by the wo matrix.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7072" target="_blank">01:57:52.320</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7080" target="_blank">01:58:00.480</a></span> | <span class="t">and this will result in a b1 dim,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7097" target="_blank">01:58:17.200</a></span> | <span class="t">b1 dim, this one is bhq one head dimension into b1 hq head dimension,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7114" target="_blank">01:58:34.720</a></span> | <span class="t">because of the transposition,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7118" target="_blank">01:58:38.480</a></span> | <span class="t">and then we remove the dimension for the head, so b1 dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7127" target="_blank">01:58:47.200</a></span> | <span class="t">And this is our self-attention with kvcache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7131" target="_blank">01:58:51.280</a></span> | <span class="t">So, let's review what we have done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7132" target="_blank">01:58:52.720</a></span> | <span class="t">Here, I think I made some mistake, because self, that's why it's colored differently.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7140" target="_blank">01:59:00.240</a></span> | <span class="t">Okay, let's review what we have done.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7142" target="_blank">01:59:02.080</a></span> | <span class="t">When we calculated the self-attention, because we are inferencing,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7145" target="_blank">01:59:05.760</a></span> | <span class="t">so this code will only work for inferencing, we can use the kvcache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7149" target="_blank">01:59:09.840</a></span> | <span class="t">The kvcache allow us to save a number of dot products that we don't need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7155" target="_blank">01:59:15.120</a></span> | <span class="t">Why? Because every time we are in the original transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7158" target="_blank">01:59:18.880</a></span> | <span class="t">we were computing a lot of dot products for tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7162" target="_blank">01:59:22.480</a></span> | <span class="t">output tokens that we don't care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7164" target="_blank">01:59:24.800</a></span> | <span class="t">In this case, we simplified the mechanism to output only one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7169" target="_blank">01:59:29.600</a></span> | <span class="t">As you can see, the output of the self-attention is b, so batch,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7173" target="_blank">01:59:33.440</a></span> | <span class="t">one token only with its embedding size, which is 4096.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7180" target="_blank">01:59:40.160</a></span> | <span class="t">So, we are only outputting one token, not many tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7184" target="_blank">01:59:44.880</a></span> | <span class="t">We input only one token, and we output one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7187" target="_blank">01:59:47.680</a></span> | <span class="t">But because we need to relate that single token with all the previous tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7193" target="_blank">01:59:53.040</a></span> | <span class="t">we keep a cache of the keys and the values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7195" target="_blank">01:59:55.680</a></span> | <span class="t">Every time we have a token, we put it into the cache, like here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7200" target="_blank">02:00:00.400</a></span> | <span class="t">then we retrieve all the previous saved tokens from the cache,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7204" target="_blank">02:00:04.480</a></span> | <span class="t">and then we calculate the attention between all the previous tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7207" target="_blank">02:00:07.600</a></span> | <span class="t">so the keys and the values, and the single token as input of, as queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7212" target="_blank">02:00:12.960</a></span> | <span class="t">The output is the only token we care about.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7216" target="_blank">02:00:16.480</a></span> | <span class="t">This is the idea behind kvcache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7220" target="_blank">02:00:20.880</a></span> | <span class="t">And the grouped query attention is the fact that we have a different number of heads</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7226" target="_blank">02:00:26.240</a></span> | <span class="t">for the keys and values, but in our case,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7229" target="_blank">02:00:29.680</a></span> | <span class="t">we do have a different number of heads for the keys and queries.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7237" target="_blank">02:00:37.680</a></span> | <span class="t">But we just repeat the one that we are missing to calculate the attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7242" target="_blank">02:00:42.080</a></span> | <span class="t">So the attention is calculated just like the previous transformer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7246" target="_blank">02:00:46.480</a></span> | <span class="t">like a normal multi-head attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7248" target="_blank">02:00:48.560</a></span> | <span class="t">but by repeating the missing keys and values heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7252" target="_blank">02:00:52.880</a></span> | <span class="t">instead of actually optimizing the algorithm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7255" target="_blank">02:00:55.360</a></span> | <span class="t">This has also been done by Meta in its official implementation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7259" target="_blank">02:00:59.920</a></span> | <span class="t">and I also did it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7261" target="_blank">02:01:01.040</a></span> | <span class="t">The biggest reason is because I cannot test any other modification.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7265" target="_blank">02:01:05.120</a></span> | <span class="t">I cannot test another algorithm that actually tries to optimize this calculation.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7270" target="_blank">02:01:10.800</a></span> | <span class="t">So if I find another implementation that I know is working,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7275" target="_blank">02:01:15.440</a></span> | <span class="t">I will share it with you guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7276" target="_blank">02:01:16.640</a></span> | <span class="t">Otherwise, I will try to run it on Colab and see if I can come up with a better solution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7281" target="_blank">02:01:21.840</a></span> | <span class="t">But for now, we just repeat it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7283" target="_blank">02:01:23.440</a></span> | <span class="t">But at least we got the concept of the grouped query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7287" target="_blank">02:01:27.200</a></span> | <span class="t">That is, we have less number of heads,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7289" target="_blank">02:01:29.440</a></span> | <span class="t">and it's something that is in between the multi-query attention and the multi-head attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7294" target="_blank">02:01:34.880</a></span> | <span class="t">That doesn't sacrifice quality, but improves speed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7298" target="_blank">02:01:38.080</a></span> | <span class="t">Now, the last thing that we didn't implement is the feedforward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7303" target="_blank">02:01:43.120</a></span> | <span class="t">For the feedforward layer, the only thing that we need to review</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7305" target="_blank">02:01:45.680</a></span> | <span class="t">is the ZWIGGLU activation function that we can see here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7308" target="_blank">02:01:48.320</a></span> | <span class="t">And this activation function has been changed compared to the previous</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7313" target="_blank">02:01:53.360</a></span> | <span class="t">activation function used in the vanilla transformer, which was the RELU function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7317" target="_blank">02:01:57.680</a></span> | <span class="t">And the only reason we replaced it is because this one performs better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7322" target="_blank">02:02:02.480</a></span> | <span class="t">And as I showed in my previous video, we cannot prove why it works better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7328" target="_blank">02:02:08.400</a></span> | <span class="t">Because in such a big model with 70 billion parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7331" target="_blank">02:02:11.520</a></span> | <span class="t">it's difficult to explain why a little modification works better than another.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7335" target="_blank">02:02:15.840</a></span> | <span class="t">We just know that some things work better in practice for that kind of model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7339" target="_blank">02:02:19.440</a></span> | <span class="t">or for that kind of application.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7341" target="_blank">02:02:21.040</a></span> | <span class="t">And this is actually not my opinion.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7343" target="_blank">02:02:23.120</a></span> | <span class="t">This is actually written in the paper.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7344" target="_blank">02:02:24.480</a></span> | <span class="t">So as you can see here in the conclusion of the paper,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7347" target="_blank">02:02:27.520</a></span> | <span class="t">they say that we offer no explanation as to why this architecture seems to work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7351" target="_blank">02:02:31.280</a></span> | <span class="t">We attribute their success as all else to divine benevolence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7354" target="_blank">02:02:34.720</a></span> | <span class="t">So it means that when you have such a big model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7356" target="_blank">02:02:36.960</a></span> | <span class="t">and you change a little thing and it works better,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7359" target="_blank">02:02:39.120</a></span> | <span class="t">you cannot always come up with a pattern to describe why it is working better.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7363" target="_blank">02:02:43.120</a></span> | <span class="t">You just take it for granted that it works better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7366" target="_blank">02:02:46.560</a></span> | <span class="t">and you use it because it works better in practice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7369" target="_blank">02:02:49.520</a></span> | <span class="t">So to implement the ZWIGGLU function, we need to apply...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7373" target="_blank">02:02:53.760</a></span> | <span class="t">This is the formula from the original transformer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7377" target="_blank">02:02:57.040</a></span> | <span class="t">So we have two matrices here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7380" target="_blank">02:03:00.880</a></span> | <span class="t">So this is the RELU function of the first linear layer and the second linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7385" target="_blank">02:03:05.520</a></span> | <span class="t">In LAMA, we use the ZWIGGLU function which involves the three matrices here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7390" target="_blank">02:03:10.800</a></span> | <span class="t">Because they incremented the number of parameters here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7393" target="_blank">02:03:13.600</a></span> | <span class="t">and also they were experimenting with the grouped query attention,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7397" target="_blank">02:03:17.840</a></span> | <span class="t">the architecture of LAMA has some more parameters</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7403" target="_blank">02:03:23.760</a></span> | <span class="t">to adjust the number of parameters of this feedforward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7407" target="_blank">02:03:27.920</a></span> | <span class="t">So as it respects some constraints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7410" target="_blank">02:03:30.720</a></span> | <span class="t">And this is actually used in deep learning research.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7415" target="_blank">02:03:35.360</a></span> | <span class="t">Whenever we modify the transformer model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7417" target="_blank">02:03:37.360</a></span> | <span class="t">and this reduces the number of parameters or increases the number of parameters,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7421" target="_blank">02:03:41.280</a></span> | <span class="t">the first thing the researchers do,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7422" target="_blank">02:03:42.880</a></span> | <span class="t">they adjust the numbers of parameters of the feedforward layer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7426" target="_blank">02:03:46.720</a></span> | <span class="t">so that when they make comparison between two models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7429" target="_blank">02:03:49.760</a></span> | <span class="t">they have the same number of parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7431" target="_blank">02:03:51.920</a></span> | <span class="t">So I will also, of course, use the same structure</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7436" target="_blank">02:03:56.160</a></span> | <span class="t">because otherwise I cannot load the weight from the pre-trained model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7440" target="_blank">02:04:00.560</a></span> | <span class="t">So let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7442" target="_blank">02:04:02.240</a></span> | <span class="t">The hidden size is calculated like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7446" target="_blank">02:04:06.240</a></span> | <span class="t">So four times the dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7447" target="_blank">02:04:07.680</a></span> | <span class="t">Then they do the two-third of this dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7450" target="_blank">02:04:10.880</a></span> | <span class="t">And then they also have a multiplier if it's specified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7461" target="_blank">02:04:21.760</a></span> | <span class="t">Then they say round the hidden... oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7476" target="_blank">02:04:36.800</a></span> | <span class="t">By using this modification to calculating the hidden dimension like this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7481" target="_blank">02:04:41.600</a></span> | <span class="t">it may not be the case that this hidden dimension is a multiple of this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7488" target="_blank">02:04:48.720</a></span> | <span class="t">So maybe they want the size of the hidden dimension to be multiple of this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7494" target="_blank">02:04:54.480</a></span> | <span class="t">So maybe they want the size of the hidden dimension to be multiple of this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7497" target="_blank">02:04:57.280</a></span> | <span class="t">So maybe they want the size of the hidden dimension to be multiple of this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7499" target="_blank">02:04:59.760</a></span> | <span class="t">So maybe they want the size of the hidden dimension to be multiple of this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7502" target="_blank">02:05:02.560</a></span> | <span class="t">So maybe they want the size of the hidden dimension to be multiple of this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7503" target="_blank">02:05:03.440</a></span> | <span class="t">So maybe they want the size of the hidden layer to be a multiple of 256.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7508" target="_blank">02:05:08.880</a></span> | <span class="t">But by calculating it like this, it may not be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7511" target="_blank">02:05:11.680</a></span> | <span class="t">So what they do is they make it round up to the next multiple of the multiple of parameter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7530" target="_blank">02:05:30.960</a></span> | <span class="t">So this is a way to do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7536" target="_blank">02:05:36.720</a></span> | <span class="t">Okay, let me give you an example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7538" target="_blank">02:05:38.640</a></span> | <span class="t">It's easier to show with an example than to actually write it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7542" target="_blank">02:05:42.400</a></span> | <span class="t">So suppose you have the hidden size is equal to, let's say, 7.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7548" target="_blank">02:05:48.800</a></span> | <span class="t">But you want it to multiple of is equal to 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7552" target="_blank">02:05:52.240</a></span> | <span class="t">So you want the hidden size to be a multiple of 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7554" target="_blank">02:05:54.800</a></span> | <span class="t">So how do we do?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7556" target="_blank">02:05:56.240</a></span> | <span class="t">Well, what we do is, basically, we do hidden plus 4 in this case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7561" target="_blank">02:06:01.840</a></span> | <span class="t">So we do 7 plus 4, which is 11.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7564" target="_blank">02:06:04.880</a></span> | <span class="t">We divide it by 5, which is equal to 2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7570" target="_blank">02:06:10.800</a></span> | <span class="t">And then we multiply this 2 by 5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7575" target="_blank">02:06:15.200</a></span> | <span class="t">So it will result in 2 by 5 is equal to 10.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7578" target="_blank">02:06:18.320</a></span> | <span class="t">It will result in the first multiple that is bigger or equal to this number here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7583" target="_blank">02:06:23.280</a></span> | <span class="t">That's the idea.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7585" target="_blank">02:06:25.680</a></span> | <span class="t">And then we have these matrices for the Zwiglu function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7590" target="_blank">02:06:30.720</a></span> | <span class="t">It's very easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7591" target="_blank">02:06:31.440</a></span> | <span class="t">We just follow the formula for the Zwiglu function, which is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7598" target="_blank">02:06:38.080</a></span> | <span class="t">So w, the Zwish of, what is Zwish?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7601" target="_blank">02:06:41.920</a></span> | <span class="t">The Zwish is the Sillu function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7605" target="_blank">02:06:45.520</a></span> | <span class="t">Because the Zwish with the beta is equal to 1 is actually the Sillu function,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7610" target="_blank">02:06:50.480</a></span> | <span class="t">which has this graph here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7612" target="_blank">02:06:52.800</a></span> | <span class="t">And then we multiply it with another parameter matrix here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7616" target="_blank">02:06:56.800</a></span> | <span class="t">And then we apply it to another linear layer, w2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7619" target="_blank">02:06:59.920</a></span> | <span class="t">So in total, we have three matrices, w1, we call it w2, and w3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7633" target="_blank">02:07:13.920</a></span> | <span class="t">And they don't have bias.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7638" target="_blank">02:07:18.880</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7639" target="_blank">02:07:19.920</a></span> | <span class="t">This is the hidden dimension.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7651" target="_blank">02:07:31.040</a></span> | <span class="t">Okay, now we implement the forward method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7670" target="_blank">02:07:50.400</a></span> | <span class="t">The first thing we do is we calculate the Zwish function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7680" target="_blank">02:08:00.400</a></span> | <span class="t">Then we calculate, so we are calculating, let me show you.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7692" target="_blank">02:08:12.320</a></span> | <span class="t">We are calculating this one, xw, Zwish of xw.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7698" target="_blank">02:08:18.800</a></span> | <span class="t">Then we calculate this xv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7700" target="_blank">02:08:20.640</a></span> | <span class="t">Then we multiply them together, just like in the formula.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7712" target="_blank">02:08:32.000</a></span> | <span class="t">So Zwish multiplied by xv.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7714" target="_blank">02:08:34.720</a></span> | <span class="t">And then we apply the last linear layer, which is w2.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7720" target="_blank">02:08:40.240</a></span> | <span class="t">Which results in a multiplication by the w2 matrix, by the way.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7727" target="_blank">02:08:47.840</a></span> | <span class="t">And then we return x.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7728" target="_blank">02:08:48.800</a></span> | <span class="t">And this is the field forward layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7733" target="_blank">02:08:53.200</a></span> | <span class="t">Now that we have all the building blocks, we need to go to the inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7740" target="_blank">02:09:00.560</a></span> | <span class="t">Let's start building the inference code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7744" target="_blank">02:09:04.000</a></span> | <span class="t">So inference.py, the first code we will be, first we will build a code to load the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7750" target="_blank">02:09:10.960</a></span> | <span class="t">And then we will build a code to inference the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7754" target="_blank">02:09:14.640</a></span> | <span class="t">I will actually also show all the inference techniques that are out there,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7758" target="_blank">02:09:18.640</a></span> | <span class="t">and which one we will apply and why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7760" target="_blank">02:09:20.560</a></span> | <span class="t">So let's start by building first the code for loading the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7765" target="_blank">02:09:25.440</a></span> | <span class="t">So first we import the stuff we need.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7782" target="_blank">02:09:42.240</a></span> | <span class="t">We need the JSON to load the parameters.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7784" target="_blank">02:09:44.640</a></span> | <span class="t">And then we need the sentence piece to load the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7789" target="_blank">02:09:49.040</a></span> | <span class="t">Because the sentence piece is the tokenizer that has been used,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7792" target="_blank">02:09:52.320</a></span> | <span class="t">and it's a library from Google.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7793" target="_blank">02:09:53.600</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7800" target="_blank">02:10:00.100</a></span> | <span class="t">From model import model-args and the transformer class.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7805" target="_blank">02:10:05.920</a></span> | <span class="t">We define the class Lama, which is our model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7811" target="_blank">02:10:11.840</a></span> | <span class="t">It takes a transformer, a tokenizer, which is a sentence piece processor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7818" target="_blank">02:10:18.640</a></span> | <span class="t">And then the model arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7832" target="_blank">02:10:32.640</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7839" target="_blank">02:10:39.060</a></span> | <span class="t">Oops.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7839" target="_blank">02:10:39.560</a></span> | <span class="t">Args.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7844" target="_blank">02:10:44.120</a></span> | <span class="t">Model-args.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7846" target="_blank">02:10:46.980</a></span> | <span class="t">Yeah.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7847" target="_blank">02:10:47.700</a></span> | <span class="t">Model-args.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7848" target="_blank">02:10:48.200</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7850" target="_blank">02:10:50.680</a></span> | <span class="t">Now we build a static method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7854" target="_blank">02:10:54.740</a></span> | <span class="t">Static method.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7856" target="_blank">02:10:56.020</a></span> | <span class="t">And we call it build, just like in the original code from Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7859" target="_blank">02:10:59.060</a></span> | <span class="t">In which we pass the directory where the checkpoints are saved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7865" target="_blank">02:11:05.540</a></span> | <span class="t">In this case, the directory name is lama27b, in my case.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7868" target="_blank">02:11:08.740</a></span> | <span class="t">But it depends on which size of the model you have downloaded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7872" target="_blank">02:11:12.260</a></span> | <span class="t">Then the tokenizer path, which is the path to the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7877" target="_blank">02:11:17.460</a></span> | <span class="t">This is the file of the tokenizer that I downloaded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7879" target="_blank">02:11:19.780</a></span> | <span class="t">Then we have a load model layer, max sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7892" target="_blank">02:11:32.260</a></span> | <span class="t">Max patch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7893" target="_blank">02:11:33.620</a></span> | <span class="t">And we have device.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7897" target="_blank">02:11:37.460</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7902" target="_blank">02:11:42.100</a></span> | <span class="t">This is only for displaying how much time it takes to load the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7905" target="_blank">02:11:45.620</a></span> | <span class="t">If we want to load the model, we will also load the checkpoints.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7913" target="_blank">02:11:53.140</a></span> | <span class="t">So checkpoints is equal to sorted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7919" target="_blank">02:11:59.780</a></span> | <span class="t">The glob method allows you to find all the files that match this filter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7933" target="_blank">02:12:13.780</a></span> | <span class="t">Okay, we see that we are loading checkpoint this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7955" target="_blank">02:12:35.700</a></span> | <span class="t">And then we actually load it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7963" target="_blank">02:12:43.540</a></span> | <span class="t">And we save it on the CPU.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7973" target="_blank">02:12:53.140</a></span> | <span class="t">We can show how much time it takes to load the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7981" target="_blank">02:13:01.060</a></span> | <span class="t">In my computer, usually it takes 10 to 20 seconds.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=7993" target="_blank">02:13:13.140</a></span> | <span class="t">Then previous time we rewrite it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8003" target="_blank">02:13:23.620</a></span> | <span class="t">So we can also show how much time it takes to load all the parameters of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8009" target="_blank">02:13:29.380</a></span> | <span class="t">Then we load the parameters, so the JSON file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8019" target="_blank">02:13:39.460</a></span> | <span class="t">We read it, open it as read-only file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8034" target="_blank">02:13:54.500</a></span> | <span class="t">And okay, then we build the arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8044" target="_blank">02:14:04.580</a></span> | <span class="t">Maximum sequence length is the one we have specified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8052" target="_blank">02:14:12.580</a></span> | <span class="t">And then we have the max patch size is the max patch size.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8064" target="_blank">02:14:24.180</a></span> | <span class="t">The device is the one we have specified.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8067" target="_blank">02:14:27.060</a></span> | <span class="t">And then all the parameters loaded from the JSON file.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8070" target="_blank">02:14:30.820</a></span> | <span class="t">Then we loaded the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8075" target="_blank">02:14:35.620</a></span> | <span class="t">Then we, by using the tokenizer, we can populate the vocab size of the model args.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8090" target="_blank">02:14:50.740</a></span> | <span class="t">The vocabulary size is actually the number of tokens inside the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8095" target="_blank">02:14:55.060</a></span> | <span class="t">Now this is also the default tensor for PyTorch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8112" target="_blank">02:15:12.500</a></span> | <span class="t">So whenever PyTorch wants to create a new tensor, what kind of type it should use,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8117" target="_blank">02:15:17.140</a></span> | <span class="t">it's defined, this is by meta, so they want for CUDA to use this type that I show you here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8122" target="_blank">02:15:22.980</a></span> | <span class="t">Default tensor type torch.cuda half tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8128" target="_blank">02:15:28.980</a></span> | <span class="t">This changes the precision that the tensor supports.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8134" target="_blank">02:15:34.100</a></span> | <span class="t">So how much space it occupies in memory.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8139" target="_blank">02:15:39.060</a></span> | <span class="t">Otherwise, then we created the actual model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8152" target="_blank">02:15:52.020</a></span> | <span class="t">Okay, when we load a checkpoint,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8167" target="_blank">02:16:07.620</a></span> | <span class="t">actually the checkpoint is a list of key and values.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8172" target="_blank">02:16:12.020</a></span> | <span class="t">Each key is a matrix in the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8175" target="_blank">02:16:15.540</a></span> | <span class="t">So the weight, for example, of a linear layer,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8177" target="_blank">02:16:17.780</a></span> | <span class="t">or the bias of a linear layer, or something like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8180" target="_blank">02:16:20.420</a></span> | <span class="t">And the names that we have used for the variable names and the matrices here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8189" target="_blank">02:16:29.460</a></span> | <span class="t">for example, wqwk, match actually the name that are present in the checkpoint here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8195" target="_blank">02:16:35.140</a></span> | <span class="t">except for one name.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8196" target="_blank">02:16:36.740</a></span> | <span class="t">So to make sure that I have used the right names,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8199" target="_blank">02:16:39.300</a></span> | <span class="t">I will load the checkpoint with strict equal true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8205" target="_blank">02:16:45.140</a></span> | <span class="t">Strict equal true means that if there is at least one name that doesn't match,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8209" target="_blank">02:16:49.300</a></span> | <span class="t">it will throw an error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8210" target="_blank">02:16:50.340</a></span> | <span class="t">So if load model, model.loadState ticked, strict equal true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8223" target="_blank">02:17:03.460</a></span> | <span class="t">So if there is at least one name in the loaded file that doesn't match the name</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8228" target="_blank">02:17:08.340</a></span> | <span class="t">in the classes that I have created here in the model, it will throw an error.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8231" target="_blank">02:17:11.940</a></span> | <span class="t">But I know that there is one key that we don't need,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8235" target="_blank">02:17:15.620</a></span> | <span class="t">which are the frequencies for the rotary positional embeddings,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8239" target="_blank">02:17:19.540</a></span> | <span class="t">which we actually are computing every time we create the tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8243" target="_blank">02:17:23.140</a></span> | <span class="t">So we are creating them here by using this function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8246" target="_blank">02:17:26.660</a></span> | <span class="t">So we don't need to load them from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8249" target="_blank">02:17:29.540</a></span> | <span class="t">So we can remove it from the model, from the checkpoint.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8253" target="_blank">02:17:33.780</a></span> | <span class="t">So because the checkpoint is a dictionary, we can just remove this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8257" target="_blank">02:17:37.460</a></span> | <span class="t">It's called rope.freqs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8260" target="_blank">02:17:40.100</a></span> | <span class="t">And then we can print how much time it took to load the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8277" target="_blank">02:17:57.860</a></span> | <span class="t">And then we return llama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8282" target="_blank">02:18:02.820</a></span> | <span class="t">Model tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8286" target="_blank">02:18:06.260</a></span> | <span class="t">And model args.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8289" target="_blank">02:18:09.380</a></span> | <span class="t">Now, before we proceed further, let me test if the model can be successfully loaded.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8295" target="_blank">02:18:15.140</a></span> | <span class="t">So let's do it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8298" target="_blank">02:18:18.660</a></span> | <span class="t">If name...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8304" target="_blank">02:18:24.340</a></span> | <span class="t">First, I will set the manual seed to zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8307" target="_blank">02:18:27.540</a></span> | <span class="t">So later we use it for inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8310" target="_blank">02:18:30.340</a></span> | <span class="t">Then I don't want to use CUDA because my GPU doesn't support it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8315" target="_blank">02:18:35.540</a></span> | <span class="t">So I say allow_cuda = 4.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8317" target="_blank">02:18:37.220</a></span> | <span class="t">Then device is equal to storage.cuda.is_available and allow_cuda else cpu.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8327" target="_blank">02:18:47.620</a></span> | <span class="t">Next time if you want to load the model with CUDA, just set this variable to true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8332" target="_blank">02:18:52.900</a></span> | <span class="t">But in my case, I will always leave it to false because I don't want to load CUDA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8346" target="_blank">02:19:06.500</a></span> | <span class="t">Sequence length, I set it to 1024.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8365" target="_blank">02:19:25.060</a></span> | <span class="t">Max batch size, let's say 3.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8371" target="_blank">02:19:31.300</a></span> | <span class="t">And device now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8377" target="_blank">02:19:37.940</a></span> | <span class="t">Let's run it and hopefully it will not crash.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8388" target="_blank">02:19:48.580</a></span> | <span class="t">Wow, already.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8391" target="_blank">02:19:51.300</a></span> | <span class="t">Not tensore, but tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8395" target="_blank">02:19:55.220</a></span> | <span class="t">So let's run it again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8400" target="_blank">02:20:00.100</a></span> | <span class="t">There is always a lot of typos when you write code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8402" target="_blank">02:20:02.580</a></span> | <span class="t">Another problem here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8405" target="_blank">02:20:05.700</a></span> | <span class="t">Ah, not storage, but tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8411" target="_blank">02:20:11.220</a></span> | <span class="t">This should be tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8413" target="_blank">02:20:13.460</a></span> | <span class="t">bfloat16 tensor.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8417" target="_blank">02:20:17.460</a></span> | <span class="t">Yeah, let's try again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8419" target="_blank">02:20:19.300</a></span> | <span class="t">Hidden... hidden what?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8425" target="_blank">02:20:25.540</a></span> | <span class="t">Hidden dimension, of course.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8427" target="_blank">02:20:27.060</a></span> | <span class="t">And let's try again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8439" target="_blank">02:20:39.060</a></span> | <span class="t">Yeah, all okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8441" target="_blank">02:20:41.140</a></span> | <span class="t">Okay, wonderful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8441" target="_blank">02:20:41.940</a></span> | <span class="t">It means that at least it's doing something and it's not crashing, which is always a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8446" target="_blank">02:20:46.900</a></span> | <span class="t">news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8447" target="_blank">02:20:47.460</a></span> | <span class="t">So let's run it again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8449" target="_blank">02:20:49.540</a></span> | <span class="t">Okay, wonderful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8450" target="_blank">02:20:50.420</a></span> | <span class="t">It means that at least it's doing something and it's not crashing, which is always a good</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8455" target="_blank">02:20:55.620</a></span> | <span class="t">news.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8455" target="_blank">02:20:55.940</a></span> | <span class="t">So our next step is actually to build the inferencing code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8459" target="_blank">02:20:59.460</a></span> | <span class="t">So what we want to do is actually we want to be able to give some prompts to the model</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8465" target="_blank">02:21:05.620</a></span> | <span class="t">and then check the output for this prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8467" target="_blank">02:21:07.540</a></span> | <span class="t">So let's define some prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8469" target="_blank">02:21:09.460</a></span> | <span class="t">We will define some prompts here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8473" target="_blank">02:21:13.380</a></span> | <span class="t">And here we pass, for example, the size of the prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8477" target="_blank">02:21:17.940</a></span> | <span class="t">And then we want to, you know, we want to inference the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8486" target="_blank">02:21:26.100</a></span> | <span class="t">So before we start inferencing the model, we need to build the code for inferencing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8490" target="_blank">02:21:30.500</a></span> | <span class="t">the model, because we need to find a strategy for selecting the next token, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8494" target="_blank">02:21:34.980</a></span> | <span class="t">So let's review how the inferencing works and what are the various strategies for inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8499" target="_blank">02:21:39.700</a></span> | <span class="t">Okay, so when we are dealing with the next token prediction task, when we want to inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8505" target="_blank">02:21:45.460</a></span> | <span class="t">we usually give the prompt and then we want to predict the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8509" target="_blank">02:21:49.860</a></span> | <span class="t">But we give one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8512" target="_blank">02:21:52.260</a></span> | <span class="t">And every time we give one more token, the model will output one more token as output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8516" target="_blank">02:21:56.820</a></span> | <span class="t">And we only keep the last one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8518" target="_blank">02:21:58.340</a></span> | <span class="t">But with the KVCache, actually, we always give one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8522" target="_blank">02:22:02.660</a></span> | <span class="t">The KVCache will keep the cache for the keys and the values and with only output one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8528" target="_blank">02:22:08.820</a></span> | <span class="t">Okay, the point is, we need to find strategies for selecting this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8534" target="_blank">02:22:14.020</a></span> | <span class="t">Among all the tokens that we have in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8537" target="_blank">02:22:17.060</a></span> | <span class="t">And this is the job of the logits and the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8540" target="_blank">02:22:20.340</a></span> | <span class="t">So let's review how they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8542" target="_blank">02:22:22.180</a></span> | <span class="t">Now imagine I give you the following task as human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8547" target="_blank">02:22:27.140</a></span> | <span class="t">So complete the following sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8549" target="_blank">02:22:29.700</a></span> | <span class="t">I think nuclear power is and then you have to choose a word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8553" target="_blank">02:22:33.860</a></span> | <span class="t">Now you as human may have thought of the possible next tokens, which may be clean, dangerous,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8560" target="_blank">02:22:40.100</a></span> | <span class="t">cheap, expensive, safe, difficult, or something else.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8563" target="_blank">02:22:43.540</a></span> | <span class="t">The choice of the next token in your head depends on your education,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8567" target="_blank">02:22:47.780</a></span> | <span class="t">on your experience with nuclear power, and your opinion on the matter.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8571" target="_blank">02:22:51.460</a></span> | <span class="t">Large language models also face the same problem.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8575" target="_blank">02:22:55.220</a></span> | <span class="t">When we give them a prompt, then the model has to choose the next word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8579" target="_blank">02:22:59.300</a></span> | <span class="t">The model, the uncertainty of the choice derives entirely from their training process</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8585" target="_blank">02:23:05.220</a></span> | <span class="t">and the strategy that we use to select the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8589" target="_blank">02:23:09.380</a></span> | <span class="t">There are many strategies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8590" target="_blank">02:23:10.820</a></span> | <span class="t">For example, we have the greedy strategy, the beam search, temperature is a parameter,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8595" target="_blank">02:23:15.140</a></span> | <span class="t">random sampling, top k, top p.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8597" target="_blank">02:23:17.060</a></span> | <span class="t">In this video, we will review all these strategies and how they work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8601" target="_blank">02:23:21.140</a></span> | <span class="t">But first, we need to understand what are the logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8603" target="_blank">02:23:23.700</a></span> | <span class="t">Let's look at the transformer model from Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8608" target="_blank">02:23:28.260</a></span> | <span class="t">So the output of the self-attention is a sequence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8613" target="_blank">02:23:33.540</a></span> | <span class="t">In the case of the KVCache is only one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8616" target="_blank">02:23:36.900</a></span> | <span class="t">We then run it through a linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8618" target="_blank">02:23:38.980</a></span> | <span class="t">So after normalization, we run it through a linear layer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8621" target="_blank">02:23:41.780</a></span> | <span class="t">The linear layer will transform the embedding that is output from the self-attention here</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8627" target="_blank">02:23:47.060</a></span> | <span class="t">into a list of numbers that represent the kind of the probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8634" target="_blank">02:23:54.820</a></span> | <span class="t">they are not really a probability, but we can think of it as a probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8638" target="_blank">02:23:58.500</a></span> | <span class="t">of that token in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8641" target="_blank">02:24:01.540</a></span> | <span class="t">So if our vocabulary is made of, let's say, 100 tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8646" target="_blank">02:24:06.100</a></span> | <span class="t">this linear layer will output 100 numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8648" target="_blank">02:24:08.820</a></span> | <span class="t">And after we apply the softmax,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8651" target="_blank">02:24:11.940</a></span> | <span class="t">these 100 numbers will become the probability of that token being the next more probable token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8659" target="_blank">02:24:19.860</a></span> | <span class="t">for the prompt given to the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8662" target="_blank">02:24:22.500</a></span> | <span class="t">So given an input, a prompt, the model comes up with probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8667" target="_blank">02:24:27.620</a></span> | <span class="t">Probabilities for which token to choose next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8670" target="_blank">02:24:30.660</a></span> | <span class="t">And so what is the job of the linear layer and what is the job of the softmax?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8675" target="_blank">02:24:35.700</a></span> | <span class="t">The linear layer converts the embedding of a token into a list of numbers</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8681" target="_blank">02:24:41.060</a></span> | <span class="t">such that each number represents a score that later with the softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8686" target="_blank">02:24:46.020</a></span> | <span class="t">represents the probability of that particular token in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8690" target="_blank">02:24:50.260</a></span> | <span class="t">The softmax job is just to scale the logits in such a way that they sum up to one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8695" target="_blank">02:24:55.380</a></span> | <span class="t">So that's why we can talk about probabilities with the softmax, but not with the logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8700" target="_blank">02:25:00.100</a></span> | <span class="t">So the output of the softmax is thus a probability distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8704" target="_blank">02:25:04.340</a></span> | <span class="t">over all the words in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8706" target="_blank">02:25:06.580</a></span> | <span class="t">That is, each word in the vocabulary will have a probability associated with it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8710" target="_blank">02:25:10.580</a></span> | <span class="t">But now, given these words, each one with their probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8714" target="_blank">02:25:14.340</a></span> | <span class="t">how do we choose the next token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8716" target="_blank">02:25:16.260</a></span> | <span class="t">There are many strategies.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8717" target="_blank">02:25:17.300</a></span> | <span class="t">The easiest one is the greedy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8720" target="_blank">02:25:20.420</a></span> | <span class="t">The greedy strategy basically says we just select the token with the maximum probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8725" target="_blank">02:25:25.540</a></span> | <span class="t">So imagine we are inferencing and the time step is the first time step in the greedy strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8731" target="_blank">02:25:31.380</a></span> | <span class="t">The prompt is Celia, you're breaking my heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8734" target="_blank">02:25:34.420</a></span> | <span class="t">You're shaking my.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8735" target="_blank">02:25:35.620</a></span> | <span class="t">OK, this is a line from a very famous song from Simone Ergampfunkel.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8741" target="_blank">02:25:41.220</a></span> | <span class="t">And the next word, for those who know, will be confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8747" target="_blank">02:25:47.220</a></span> | <span class="t">So Celia, you're breaking my heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8748" target="_blank">02:25:48.980</a></span> | <span class="t">You are shaking my confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8750" target="_blank">02:25:50.500</a></span> | <span class="t">Suppose the output of the softmax is this distribution here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8755" target="_blank">02:25:55.620</a></span> | <span class="t">So we have 40% probability for this word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8758" target="_blank">02:25:58.660</a></span> | <span class="t">20% for this word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8760" target="_blank">02:26:00.260</a></span> | <span class="t">15% for this word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8761" target="_blank">02:26:01.700</a></span> | <span class="t">And 10% for this word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8763" target="_blank">02:26:03.220</a></span> | <span class="t">With a greedy strategy, we always choose the token with the maximum probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8769" target="_blank">02:26:09.300</a></span> | <span class="t">Then we append it to the input.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8771" target="_blank">02:26:11.780</a></span> | <span class="t">So the input at the next inference step becomes Celia, you're breaking my heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8776" target="_blank">02:26:16.260</a></span> | <span class="t">You're shaking my confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8777" target="_blank">02:26:17.620</a></span> | <span class="t">And then the model has to come up with the next word, which, if you know the song, is daily.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8782" target="_blank">02:26:22.420</a></span> | <span class="t">If we use the greedy strategy, we select the one with the highest probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8787" target="_blank">02:26:27.860</a></span> | <span class="t">So in this case, it's daily, and it's also the correct one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8790" target="_blank">02:26:30.980</a></span> | <span class="t">So this is how the greedy strategy works.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8793" target="_blank">02:26:33.380</a></span> | <span class="t">At every step, we choose the token with the maximum probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8796" target="_blank">02:26:36.900</a></span> | <span class="t">which is then appended to the input to generate the next token, and so on.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8800" target="_blank">02:26:40.900</a></span> | <span class="t">But if the initial token happens to be the wrong one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8805" target="_blank">02:26:45.140</a></span> | <span class="t">so not only the initial, but the initial two, three tokens happen to be the wrong ones,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8809" target="_blank">02:26:49.300</a></span> | <span class="t">it's very likely that all the next tokens will also be wrong,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8812" target="_blank">02:26:52.740</a></span> | <span class="t">because we are giving a wrong prompt to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8815" target="_blank">02:26:55.220</a></span> | <span class="t">So imagine at the time step one, we don't choose confidence,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8819" target="_blank">02:26:59.860</a></span> | <span class="t">but somehow the model came up with a high score for liver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8823" target="_blank">02:27:03.540</a></span> | <span class="t">So you're shaking my liver, but then the next word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8826" target="_blank">02:27:06.660</a></span> | <span class="t">the model will not be able to come up with a reasonable next word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8830" target="_blank">02:27:10.420</a></span> | <span class="t">because there is no song that says you're shaking my liver.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8834" target="_blank">02:27:14.580</a></span> | <span class="t">So if we make a mistake in the early stage of the greedy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8837" target="_blank">02:27:17.780</a></span> | <span class="t">all the next token very probably will also be wrong.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8840" target="_blank">02:27:20.500</a></span> | <span class="t">But it's very easy to implement.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8843" target="_blank">02:27:23.060</a></span> | <span class="t">And however, it performs poorly in practice, that it's very, it's not used so much.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8847" target="_blank">02:27:27.940</a></span> | <span class="t">Another strategy is the BeamSearch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8850" target="_blank">02:27:30.980</a></span> | <span class="t">In BeamSearch, we have a parameter, which is called K,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8855" target="_blank">02:27:35.060</a></span> | <span class="t">which means that at every step, we not only choose the top ones,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8858" target="_blank">02:27:38.660</a></span> | <span class="t">but the top K at every step.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8861" target="_blank">02:27:41.060</a></span> | <span class="t">And we always keep the top two best performing tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8865" target="_blank">02:27:45.380</a></span> | <span class="t">So in this case, for example, imagine we are time step one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8869" target="_blank">02:27:49.300</a></span> | <span class="t">So Celia, you're breaking my heart.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8870" target="_blank">02:27:50.980</a></span> | <span class="t">You are shaking mine.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8871" target="_blank">02:27:51.940</a></span> | <span class="t">And the top two words are pizza and confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8875" target="_blank">02:27:55.620</a></span> | <span class="t">Pizza somehow has a higher, has a higher probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8880" target="_blank">02:28:00.260</a></span> | <span class="t">because maybe the model has never seen this song before.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8883" target="_blank">02:28:03.700</a></span> | <span class="t">So it doesn't know that the next word is confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8887" target="_blank">02:28:07.060</a></span> | <span class="t">So maybe the model outputs these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8890" target="_blank">02:28:10.180</a></span> | <span class="t">But we choose the two top most, the two tokens with the highest probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8897" target="_blank">02:28:17.460</a></span> | <span class="t">At the next time step, we make two prompts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8902" target="_blank">02:28:22.020</a></span> | <span class="t">one in case we choose the first one, so the first token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8905" target="_blank">02:28:25.700</a></span> | <span class="t">and one in case we choose the second token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8907" target="_blank">02:28:27.780</a></span> | <span class="t">And then we see what are the next possible choices if we use the first token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8912" target="_blank">02:28:32.500</a></span> | <span class="t">And what are the next choices if we use the second token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8915" target="_blank">02:28:35.540</a></span> | <span class="t">So we check the model output for the first prompt and for the second prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8919" target="_blank">02:28:39.940</a></span> | <span class="t">And in case we use, for example, the first prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8923" target="_blank">02:28:43.380</a></span> | <span class="t">the model will output these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8927" target="_blank">02:28:47.380</a></span> | <span class="t">And if we use the second prompt, the model will output these probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8931" target="_blank">02:28:51.700</a></span> | <span class="t">What we do then is we calculate the cumulative score for each possible path.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8937" target="_blank">02:28:57.460</a></span> | <span class="t">So for pizza, for example, the probability was 40%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8940" target="_blank">02:29:00.340</a></span> | <span class="t">But after pizza, the model produced the probability for the margarita,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8946" target="_blank">02:29:06.100</a></span> | <span class="t">for example, is 0.01%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8947" target="_blank">02:29:07.700</a></span> | <span class="t">So for this path, pizza, margarita, it's 0.004.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8952" target="_blank">02:29:12.660</a></span> | <span class="t">The probability is 0.4%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8954" target="_blank">02:29:14.580</a></span> | <span class="t">Pizza, anchovies, it's going to be 0.2% or 0.002.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8961" target="_blank">02:29:21.860</a></span> | <span class="t">However, with confidence, we get a new next token that can be either daily or monthly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8969" target="_blank">02:29:29.380</a></span> | <span class="t">With daily, we get a cumulative score of 0.16 and with monthly of 0.02.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8975" target="_blank">02:29:35.620</a></span> | <span class="t">So as we can see, at the time step two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8978" target="_blank">02:29:38.260</a></span> | <span class="t">even if at the time step one, pizza was the most probable word,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8983" target="_blank">02:29:43.060</a></span> | <span class="t">because we kept the second choice alive,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8985" target="_blank">02:29:45.860</a></span> | <span class="t">so we didn't kill it, just like we did it with greedy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8988" target="_blank">02:29:48.820</a></span> | <span class="t">Let me use the laser.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8991" target="_blank">02:29:51.300</a></span> | <span class="t">We can see that the confidence then produces a next token that is very probable,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=8997" target="_blank">02:29:57.380</a></span> | <span class="t">because now the model has more prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9000" target="_blank">02:30:00.260</a></span> | <span class="t">And so it can come up with more specific choices for the next tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9005" target="_blank">02:30:05.220</a></span> | <span class="t">with a very high confidence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9007" target="_blank">02:30:07.780</a></span> | <span class="t">So we compute the cumulative score of all these paths,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9014" target="_blank">02:30:14.180</a></span> | <span class="t">and we keep the two paths that have the top choices.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9016" target="_blank">02:30:16.820</a></span> | <span class="t">So now the pizza path has been killed,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9019" target="_blank">02:30:19.700</a></span> | <span class="t">because it's later we chose pizza at the beginning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9022" target="_blank">02:30:22.420</a></span> | <span class="t">because somehow the model thought it was pizza,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9024" target="_blank">02:30:24.420</a></span> | <span class="t">but then it couldn't find it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9025" target="_blank">02:30:25.860</a></span> | <span class="t">The model was not so confident about the next words.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9028" target="_blank">02:30:28.420</a></span> | <span class="t">But in the case of this token here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9031" target="_blank">02:30:31.300</a></span> | <span class="t">the model was very confident about the second score.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9033" target="_blank">02:30:33.540</a></span> | <span class="t">So we killed all this path here, and we kept this one</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9036" target="_blank">02:30:36.980</a></span> | <span class="t">until we arrived to the last token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9038" target="_blank">02:30:38.980</a></span> | <span class="t">in which we just selected the path with the highest score,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9042" target="_blank">02:30:42.180</a></span> | <span class="t">and that's the output of our inferencing strategy with BeamSearch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9045" target="_blank">02:30:45.860</a></span> | <span class="t">And repeat the steps of the last slide for all the successive tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9052" target="_blank">02:30:52.180</a></span> | <span class="t">until we arrive to the last one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9053" target="_blank">02:30:53.620</a></span> | <span class="t">And with BeamSearch, at every step we keep alive the top k paths,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9060" target="_blank">02:31:00.980</a></span> | <span class="t">and all the others are killed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9062" target="_blank">02:31:02.420</a></span> | <span class="t">It increases inferencing time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9064" target="_blank">02:31:04.820</a></span> | <span class="t">because at every step we must explore k possible options,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9068" target="_blank">02:31:08.100</a></span> | <span class="t">but generally it performs better than the greedy strategy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9071" target="_blank">02:31:11.060</a></span> | <span class="t">for the reason that I have just shown.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9072" target="_blank">02:31:12.580</a></span> | <span class="t">Another thing that is interesting in inferencing is the temperature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9078" target="_blank">02:31:18.180</a></span> | <span class="t">because the idea of the temperature is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9080" target="_blank">02:31:20.740</a></span> | <span class="t">we can make the model more confident or less confident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9086" target="_blank">02:31:26.100</a></span> | <span class="t">So for example, when we compute the logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9089" target="_blank">02:31:29.540</a></span> | <span class="t">which are not the probabilities,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9091" target="_blank">02:31:31.620</a></span> | <span class="t">so they are what will become the probabilities after we apply the Softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9096" target="_blank">02:31:36.100</a></span> | <span class="t">So before we apply the Softmax, we can scale the logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9099" target="_blank">02:31:39.860</a></span> | <span class="t">so that if we use, for example, like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9103" target="_blank">02:31:43.860</a></span> | <span class="t">So for example, we have these logits here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9105" target="_blank">02:31:45.700</a></span> | <span class="t">I choose the negative numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9108" target="_blank">02:31:48.020</a></span> | <span class="t">so the Softmax probabilities are reasonable numbers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9110" target="_blank">02:31:50.580</a></span> | <span class="t">And so these are the logits.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9113" target="_blank">02:31:53.700</a></span> | <span class="t">And if we divide these logits before applying the Softmax</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9118" target="_blank">02:31:58.260</a></span> | <span class="t">by a number that is low, so low temperature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9121" target="_blank">02:32:01.060</a></span> | <span class="t">it's called, this number is called the temperature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9123" target="_blank">02:32:03.220</a></span> | <span class="t">it will make the model more confident,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9126" target="_blank">02:32:06.100</a></span> | <span class="t">because it will make bigger probabilities bigger</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9129" target="_blank">02:32:09.060</a></span> | <span class="t">and smaller probabilities smaller.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9130" target="_blank">02:32:10.900</a></span> | <span class="t">So the gap between the low and high probability increases.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9135" target="_blank">02:32:15.220</a></span> | <span class="t">So for example, you can see that without applying any temperature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9138" target="_blank">02:32:18.900</a></span> | <span class="t">the highest logit gets 80% probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9142" target="_blank">02:32:22.980</a></span> | <span class="t">But applying a 0.4 temperature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9145" target="_blank">02:32:25.380</a></span> | <span class="t">the highest logit becomes 98% probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9149" target="_blank">02:32:29.300</a></span> | <span class="t">And if we apply a high temperature,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9151" target="_blank">02:32:31.140</a></span> | <span class="t">it makes the model less confident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9152" target="_blank">02:32:32.980</a></span> | <span class="t">So the gap between the low and high probability reduces.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9155" target="_blank">02:32:35.860</a></span> | <span class="t">The temperature is important if we want to increase</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9160" target="_blank">02:32:40.740</a></span> | <span class="t">the confidence of the model or not,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9162" target="_blank">02:32:42.820</a></span> | <span class="t">because it can be used in conjunction with other strategies,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9166" target="_blank">02:32:46.260</a></span> | <span class="t">like, for example, the greedy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9167" target="_blank">02:32:47.780</a></span> | <span class="t">or the top k or the top v that we will see later.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9170" target="_blank">02:32:50.580</a></span> | <span class="t">Another strategy is the random sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9175" target="_blank">02:32:55.620</a></span> | <span class="t">So as we saw, the logits are not a probability distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9179" target="_blank">02:32:59.060</a></span> | <span class="t">but after we apply the softmax, they become a distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9182" target="_blank">02:33:02.100</a></span> | <span class="t">So what we do, because it's a distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9185" target="_blank">02:33:05.140</a></span> | <span class="t">we can also sample from this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9187" target="_blank">02:33:07.700</a></span> | <span class="t">For example, in this distribution here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9190" target="_blank">02:33:10.260</a></span> | <span class="t">that comes from these logits here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9192" target="_blank">02:33:12.580</a></span> | <span class="t">we have one token that can be chosen with a 12% probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9196" target="_blank">02:33:16.020</a></span> | <span class="t">one can be chosen with 7% probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9198" target="_blank">02:33:18.420</a></span> | <span class="t">and one that can be chosen with 80% probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9201" target="_blank">02:33:21.060</a></span> | <span class="t">If we flip a coin, by 80% of the time,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9203" target="_blank">02:33:23.460</a></span> | <span class="t">we will choose this token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9204" target="_blank">02:33:24.980</a></span> | <span class="t">and 12% of the time we will choose this token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9207" target="_blank">02:33:27.380</a></span> | <span class="t">and 7% of the time we will choose this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9209" target="_blank">02:33:29.460</a></span> | <span class="t">So this means sample from this distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9212" target="_blank">02:33:32.500</a></span> | <span class="t">It means take a number from this distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9216" target="_blank">02:33:36.100</a></span> | <span class="t">according to its weight, to its probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9219" target="_blank">02:33:39.620</a></span> | <span class="t">Now, there is a problem with this sampling strategy here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9224" target="_blank">02:33:44.820</a></span> | <span class="t">that with very little probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9227" target="_blank">02:33:47.380</a></span> | <span class="t">it may happen that we choose tokens that are total crap.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9231" target="_blank">02:33:51.220</a></span> | <span class="t">For example, in this scenario here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9236" target="_blank">02:33:56.020</a></span> | <span class="t">for example, before, with the greedy strategy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9239" target="_blank">02:33:59.380</a></span> | <span class="t">or with Bream search, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9240" target="_blank">02:34:00.740</a></span> | <span class="t">this token here, if we use a random sampling,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9243" target="_blank">02:34:03.700</a></span> | <span class="t">we will choose the word pizza with 40% probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9247" target="_blank">02:34:07.220</a></span> | <span class="t">the word confidence with 20% probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9249" target="_blank">02:34:09.940</a></span> | <span class="t">but with a very little probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9251" target="_blank">02:34:11.620</a></span> | <span class="t">it may happen that we will choose the word Pokemon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9254" target="_blank">02:34:14.180</a></span> | <span class="t">with 10% probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9255" target="_blank">02:34:15.540</a></span> | <span class="t">Of course, the probability is low,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9258" target="_blank">02:34:18.100</a></span> | <span class="t">so the probability of us making a bad choice is low,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9261" target="_blank">02:34:21.540</a></span> | <span class="t">but there is this probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9264" target="_blank">02:34:24.020</a></span> | <span class="t">So this is a problem with random sampling.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9266" target="_blank">02:34:26.020</a></span> | <span class="t">The next strategy is TopKey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9269" target="_blank">02:34:29.540</a></span> | <span class="t">In TopKey, what we do is,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9271" target="_blank">02:34:31.780</a></span> | <span class="t">to avoid selecting the crappy tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9274" target="_blank">02:34:34.420</a></span> | <span class="t">we just remove them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9275" target="_blank">02:34:35.700</a></span> | <span class="t">So we take all the logits,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9278" target="_blank">02:34:38.260</a></span> | <span class="t">we sort them, and then we just keep the highest key,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9282" target="_blank">02:34:42.180</a></span> | <span class="t">so that the crappy one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9283" target="_blank">02:34:43.620</a></span> | <span class="t">we just remove them from this distribution,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9288" target="_blank">02:34:48.260</a></span> | <span class="t">and then we calculate the distribution for the rest.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9291" target="_blank">02:34:51.220</a></span> | <span class="t">So we apply the softmax only to the ones that survive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9295" target="_blank">02:34:55.060</a></span> | <span class="t">The problem is also here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9298" target="_blank">02:34:58.420</a></span> | <span class="t">Given the following, these two distributions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9300" target="_blank">02:35:00.740</a></span> | <span class="t">the low probability tokens can still make their way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9304" target="_blank">02:35:04.020</a></span> | <span class="t">into the TopKey,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9305" target="_blank">02:35:05.220</a></span> | <span class="t">because it all depends on the distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9307" target="_blank">02:35:07.780</a></span> | <span class="t">to which we apply the TopKey.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9309" target="_blank">02:35:09.140</a></span> | <span class="t">Let me give you a graphical example.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9311" target="_blank">02:35:11.380</a></span> | <span class="t">Imagine we have a distribution that is very flat.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9315" target="_blank">02:35:15.940</a></span> | <span class="t">Suppose this distribution here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9320" target="_blank">02:35:20.420</a></span> | <span class="t">so some words, this is our vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9325" target="_blank">02:35:25.540</a></span> | <span class="t">This is the probability of each word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9328" target="_blank">02:35:28.500</a></span> | <span class="t">So the word number one, word number two,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9330" target="_blank">02:35:30.260</a></span> | <span class="t">word number three, word number four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9331" target="_blank">02:35:31.620</a></span> | <span class="t">et cetera, et cetera, et cetera.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9332" target="_blank">02:35:32.740</a></span> | <span class="t">But more or less, all the words have the same probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9336" target="_blank">02:35:36.740</a></span> | <span class="t">So imagine we take the top 10 words,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9339" target="_blank">02:35:39.220</a></span> | <span class="t">so it will select all these tokens, right?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9341" target="_blank">02:35:41.860</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9343" target="_blank">02:35:43.380</a></span> | <span class="t">So it will select the token number one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9344" target="_blank">02:35:44.820</a></span> | <span class="t">token number two, token number three,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9346" target="_blank">02:35:46.020</a></span> | <span class="t">token number four,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9346" target="_blank">02:35:46.980</a></span> | <span class="t">up to whatever token here is here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9349" target="_blank">02:35:49.940</a></span> | <span class="t">Imagine we have another distribution</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9351" target="_blank">02:35:51.700</a></span> | <span class="t">that is made like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9352" target="_blank">02:35:52.900</a></span> | <span class="t">So we still have a vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9354" target="_blank">02:35:54.420</a></span> | <span class="t">Vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9356" target="_blank">02:35:56.500</a></span> | <span class="t">We still have a probability distribution.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9358" target="_blank">02:35:58.340</a></span> | <span class="t">And the distribution is made like this.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9362" target="_blank">02:36:02.500</a></span> | <span class="t">So because it's sorted,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9363" target="_blank">02:36:03.620</a></span> | <span class="t">we have a distribution that is very skewed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9369" target="_blank">02:36:09.060</a></span> | <span class="t">Because we still keep the top 10,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9372" target="_blank">02:36:12.500</a></span> | <span class="t">as you can see, we will select this token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9375" target="_blank">02:36:15.060</a></span> | <span class="t">this token, this token, this token, this token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9377" target="_blank">02:36:17.140</a></span> | <span class="t">But these tokens here are very crappy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9379" target="_blank">02:36:19.620</a></span> | <span class="t">compared to this one here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9381" target="_blank">02:36:21.780</a></span> | <span class="t">So they will still make their way into our selection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9385" target="_blank">02:36:25.460</a></span> | <span class="t">And this is not something that we want.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9387" target="_blank">02:36:27.300</a></span> | <span class="t">We want to avoid selecting crappy tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9389" target="_blank">02:36:29.620</a></span> | <span class="t">but we still want to have some randomness.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9391" target="_blank">02:36:31.780</a></span> | <span class="t">So we don't want to be totally greedy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9393" target="_blank">02:36:33.380</a></span> | <span class="t">because sometimes the tokens that are in the top N tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9398" target="_blank">02:36:38.980</a></span> | <span class="t">maybe they are all reasonable.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9401" target="_blank">02:36:41.460</a></span> | <span class="t">Also, sometimes the prompt can be quite ambiguous.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9405" target="_blank">02:36:45.060</a></span> | <span class="t">So we don't know which, even as humans,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9407" target="_blank">02:36:47.140</a></span> | <span class="t">we may not know what is the next word to be chosen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9409" target="_blank">02:36:49.540</a></span> | <span class="t">So we want some randomness,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9411" target="_blank">02:36:51.620</a></span> | <span class="t">but we also don't want the very low probability tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9414" target="_blank">02:36:54.980</a></span> | <span class="t">But with this top case strategy,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9416" target="_blank">02:36:56.820</a></span> | <span class="t">the low probability tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9419" target="_blank">02:36:59.140</a></span> | <span class="t">can still make their way into our selection.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9421" target="_blank">02:37:01.140</a></span> | <span class="t">And this problem is solved with top P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9424" target="_blank">02:37:04.580</a></span> | <span class="t">With top P, we only keep the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9427" target="_blank">02:37:07.940</a></span> | <span class="t">with the highest probability,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9429" target="_blank">02:37:09.620</a></span> | <span class="t">such that the cumulative probability</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9432" target="_blank">02:37:12.020</a></span> | <span class="t">is greater than or equal to the parameter P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9435" target="_blank">02:37:15.540</a></span> | <span class="t">What does this mean?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9436" target="_blank">02:37:16.660</a></span> | <span class="t">It means that if we have the previous distributions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9441" target="_blank">02:37:21.060</a></span> | <span class="t">so one that is quite flat, for example,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9444" target="_blank">02:37:24.020</a></span> | <span class="t">and the one that has a mode,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9448" target="_blank">02:37:28.900</a></span> | <span class="t">so for example, this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9450" target="_blank">02:37:30.340</a></span> | <span class="t">So this one is nearly 90% and the other one are 0.000%,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9456" target="_blank">02:37:36.580</a></span> | <span class="t">but this more or less all of them are like 0.2%</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9460" target="_blank">02:37:40.580</a></span> | <span class="t">and then they go down.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9463" target="_blank">02:37:43.940</a></span> | <span class="t">In the case, imagine P is equal to, let's say, 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9468" target="_blank">02:37:48.340</a></span> | <span class="t">In this case, we will select all the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9471" target="_blank">02:37:51.540</a></span> | <span class="t">such that the area under the curve is equal to 0.5.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9477" target="_blank">02:37:57.140</a></span> | <span class="t">But here, because this first token is already 0.9,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9481" target="_blank">02:38:01.540</a></span> | <span class="t">we will actually only select one token</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9483" target="_blank">02:38:03.700</a></span> | <span class="t">and all the crappy ones will not be selected</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9485" target="_blank">02:38:05.700</a></span> | <span class="t">because this area under the curve is already 0.9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9489" target="_blank">02:38:09.060</a></span> | <span class="t">And this is the idea behind the top P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9492" target="_blank">02:38:12.340</a></span> | <span class="t">So when the model, when the distribution is more flat,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9497" target="_blank">02:38:17.220</a></span> | <span class="t">we select more tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9498" target="_blank">02:38:18.580</a></span> | <span class="t">because it means that we are more uncertain</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9501" target="_blank">02:38:21.460</a></span> | <span class="t">about which token to choose.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9502" target="_blank">02:38:22.740</a></span> | <span class="t">But when we have a big mode, we select fewer tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9507" target="_blank">02:38:27.300</a></span> | <span class="t">and this way we avoid getting the low probability ones.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9513" target="_blank">02:38:33.540</a></span> | <span class="t">So now that we reviewed all the strategies</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9516" target="_blank">02:38:36.180</a></span> | <span class="t">for selecting the token, we will implement it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9519" target="_blank">02:38:39.860</a></span> | <span class="t">And in the case of Lama, also in the official code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9523" target="_blank">02:38:43.140</a></span> | <span class="t">they actually implement the top P strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9525" target="_blank">02:38:45.540</a></span> | <span class="t">In my case, I think that the BeamSearch is a reasonable choice.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9531" target="_blank">02:38:51.540</a></span> | <span class="t">So in another video, maybe I will make</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9534" target="_blank">02:38:54.180</a></span> | <span class="t">how to implement the BeamSearch.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9535" target="_blank">02:38:55.940</a></span> | <span class="t">But for now, I will implement the top P.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9537" target="_blank">02:38:57.620</a></span> | <span class="t">So let's go build it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9539" target="_blank">02:38:59.860</a></span> | <span class="t">So we implement the method, let's call it TextCompilation,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9543" target="_blank">02:39:03.620</a></span> | <span class="t">which is the same name that's used</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9545" target="_blank">02:39:05.860</a></span> | <span class="t">in the original code from Lama.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9547" target="_blank">02:39:07.940</a></span> | <span class="t">Given prompts a temperature that is 0.6.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9558" target="_blank">02:39:18.900</a></span> | <span class="t">And so 0.6 means that we want to make the model more confident.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9565" target="_blank">02:39:25.060</a></span> | <span class="t">Top P means that we want all the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9573" target="_blank">02:39:33.300</a></span> | <span class="t">such that their cumulative probability is at least 0.9.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9578" target="_blank">02:39:38.020</a></span> | <span class="t">So 90%.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9578" target="_blank">02:39:38.500</a></span> | <span class="t">Okay, I think here should be lowercase.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9596" target="_blank">02:39:56.020</a></span> | <span class="t">Okay, so if we didn't specify the max generation length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9601" target="_blank">02:40:01.940</a></span> | <span class="t">then we just generate the maximum token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9604" target="_blank">02:40:04.980</a></span> | <span class="t">Args.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9608" target="_blank">02:40:08.040</a></span> | <span class="t">Just generate as much token as we can up to the sequence length.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9614" target="_blank">02:40:14.100</a></span> | <span class="t">And then we, first of all, convert each token of the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9620" target="_blank">02:40:20.980</a></span> | <span class="t">So each prompt, actually, into tokens using the tokenizer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9628" target="_blank">02:40:28.740</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9629" target="_blank">02:40:29.240</a></span> | <span class="t">Then, as we saw before, we need to add the beginning of sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9647" target="_blank">02:40:47.060</a></span> | <span class="t">when we pass the input to the model for inferencing.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9656" target="_blank">02:40:56.260</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9656" target="_blank">02:40:56.760</a></span> | <span class="t">But not the end of sentence.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9660" target="_blank">02:41:00.020</a></span> | <span class="t">Okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9669" target="_blank">02:41:09.240</a></span> | <span class="t">Because we specified the max batch also for the model when we built it for the KVCache,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9680" target="_blank">02:41:20.020</a></span> | <span class="t">so we need to make sure that the batch size of the prompts is not too large.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9683" target="_blank">02:41:23.860</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9687" target="_blank">02:41:27.860</a></span> | <span class="t">And then max prompt length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9702" target="_blank">02:41:42.340</a></span> | <span class="t">is the maximum prompt length that we have in the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9707" target="_blank">02:41:47.700</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9736" target="_blank">02:42:16.020</a></span> | <span class="t">I'm not writing any message, even if you should, but okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9739" target="_blank">02:42:19.220</a></span> | <span class="t">for us it's just basically debugging.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9741" target="_blank">02:42:21.380</a></span> | <span class="t">Then the total length</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9744" target="_blank">02:42:24.980</a></span> | <span class="t">is how many tokens we want to get from the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9760" target="_blank">02:42:40.420</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9765" target="_blank">02:42:45.620</a></span> | <span class="t">Okay, now we create the list that will contain the generated token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9780" target="_blank">02:43:00.020</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9785" target="_blank">02:43:05.300</a></span> | <span class="t">so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9811" target="_blank">02:43:31.700</a></span> | <span class="t">this means create a tensor of shape batch size by total length,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9818" target="_blank">02:43:38.260</a></span> | <span class="t">in which each item is actually the padding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9820" target="_blank">02:43:40.820</a></span> | <span class="t">And then we fill the initial tokens with the prompt tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9837" target="_blank">02:43:57.380</a></span> | <span class="t">Mm-hmm.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9849" target="_blank">02:44:09.060</a></span> | <span class="t">Okay, we also need this variable that tells if we reach the end of sentence</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9867" target="_blank">02:44:27.460</a></span> | <span class="t">in any of the prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9878" target="_blank">02:44:38.980</a></span> | <span class="t">This indicates if the token in this position is a padding token or not, so true.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9896" target="_blank">02:44:56.100</a></span> | <span class="t">If the token is a prompt token, false, otherwise.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9900" target="_blank">02:45:00.660</a></span> | <span class="t">And then we can finally make the for loop to generate the tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9912" target="_blank">02:45:12.900</a></span> | <span class="t">DamageRTQDM, range from one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9918" target="_blank">02:45:18.580</a></span> | <span class="t">Okay, now we generate one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9929" target="_blank">02:45:29.700</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9932" target="_blank">02:45:32.580</a></span> | <span class="t">the logits come from the model, so set.model.forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9942" target="_blank">02:45:42.180</a></span> | <span class="t">We need to pass one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9944" target="_blank">02:45:44.100</a></span> | <span class="t">So, which token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9946" target="_blank">02:45:46.500</a></span> | <span class="t">The one currently we want to output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9949" target="_blank">02:45:49.140</a></span> | <span class="t">So, current minus one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9952" target="_blank">02:45:52.580</a></span> | <span class="t">Pause.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9954" target="_blank">02:45:54.980</a></span> | <span class="t">So, only one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9956" target="_blank">02:45:56.100</a></span> | <span class="t">And we also tell the model what is the position of this token, because for the KVCache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9960" target="_blank">02:46:00.900</a></span> | <span class="t">And if we use the temperature, we apply it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9966" target="_blank">02:46:06.500</a></span> | <span class="t">As you can see, every time when we want to inference, we always select the last token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9991" target="_blank">02:46:31.460</a></span> | <span class="t">But because we are using the KVCache, actually our model will only output one token at a time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=9995" target="_blank">02:46:35.780</a></span> | <span class="t">So, the next token will be selected according to our topP strategy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10008" target="_blank">02:46:48.820</a></span> | <span class="t">So, we have the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10010" target="_blank">02:46:50.180</a></span> | <span class="t">Now we apply the topP.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10012" target="_blank">02:46:52.580</a></span> | <span class="t">I just define it here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10014" target="_blank">02:46:54.580</a></span> | <span class="t">So, sample topP, and then we implement it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10018" target="_blank">02:46:58.420</a></span> | <span class="t">If we didn't specify any temperature, we just use the greedy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10026" target="_blank">02:47:06.340</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10038" target="_blank">02:47:18.420</a></span> | <span class="t">so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10045" target="_blank">02:47:25.380</a></span> | <span class="t">okay.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10061" target="_blank">02:47:41.620</a></span> | <span class="t">Now we have the next token according to this strategy or this greedy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10067" target="_blank">02:47:47.060</a></span> | <span class="t">Then we only replace the token if it is a padding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10074" target="_blank">02:47:54.260</a></span> | <span class="t">So, the problem is, we already have some tokens that come from the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10078" target="_blank">02:47:58.500</a></span> | <span class="t">But we don't want to, but we still need to give the prompt to the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10083" target="_blank">02:48:03.060</a></span> | <span class="t">But we are only giving one token at a time to the model to build the initial cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10087" target="_blank">02:48:07.060</a></span> | <span class="t">So, we will give, the first prompt tokens will be given to the model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10091" target="_blank">02:48:11.060</a></span> | <span class="t">not because we care about what the model will output for those tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10094" target="_blank">02:48:14.900</a></span> | <span class="t">But only because we want the KV cache to be built for those positions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10101" target="_blank">02:48:21.780</a></span> | <span class="t">And after we give the last token of the prompt,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10105" target="_blank">02:48:25.140</a></span> | <span class="t">then we care about what is the model outputting.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10108" target="_blank">02:48:28.340</a></span> | <span class="t">So, only replace the next token if it is a padding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10113" target="_blank">02:48:33.300</a></span> | <span class="t">And which one is a padding token?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10114" target="_blank">02:48:34.820</a></span> | <span class="t">The one that was not an initial prompt token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10118" target="_blank">02:48:38.020</a></span> | <span class="t">Because here we build tokens full of paddings.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10121" target="_blank">02:48:41.060</a></span> | <span class="t">But then, we replace the prompt tokens, the padding tokens with the prompt tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10127" target="_blank">02:48:47.860</a></span> | <span class="t">for the one with the initial tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10132" target="_blank">02:48:52.900</a></span> | <span class="t">All the others have to be inferred by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10135" target="_blank">02:48:55.220</a></span> | <span class="t">So, token...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10149" target="_blank">02:49:09.060</a></span> | <span class="t">This means, basically, check this mask.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10163" target="_blank">02:49:23.780</a></span> | <span class="t">What is this mask?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10164" target="_blank">02:49:24.740</a></span> | <span class="t">If it's true, if the token is a prompt token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10166" target="_blank">02:49:26.740</a></span> | <span class="t">So, if it is a prompt token, replace it with this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10170" target="_blank">02:49:30.740</a></span> | <span class="t">And if it's not a prompt token, just keep it the current one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10184" target="_blank">02:49:44.020</a></span> | <span class="t">Okay, then...</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10193" target="_blank">02:49:53.220</a></span> | <span class="t">Since we do not care about what the model outputs for the initial prompt tokens,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10202" target="_blank">02:50:02.020</a></span> | <span class="t">but only for the last prompt token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10204" target="_blank">02:50:04.180</a></span> | <span class="t">we don't care if we find an end-of-sentence position for those tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10209" target="_blank">02:50:09.380</a></span> | <span class="t">So, end-of-sentence is only reached if we find it for one of the tokens</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10213" target="_blank">02:50:13.220</a></span> | <span class="t">that we actually want to inference,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10214" target="_blank">02:50:14.820</a></span> | <span class="t">not the one that we send to the model just to build a KV cache.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10228" target="_blank">02:50:28.020</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10240" target="_blank">02:50:40.820</a></span> | <span class="t">okay,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10245" target="_blank">02:50:45.620</a></span> | <span class="t">this basically means the end-of-sentence for a particular prompt is reached</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10266" target="_blank">02:51:06.900</a></span> | <span class="t">only if it was a padding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10270" target="_blank">02:51:10.180</a></span> | <span class="t">So, only if it was a padding token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10271" target="_blank">02:51:11.860</a></span> | <span class="t">So, it was not a prompt token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10274" target="_blank">02:51:14.180</a></span> | <span class="t">This means not.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10275" target="_blank">02:51:15.140</a></span> | <span class="t">And we actually found an end-of-sentence token from the model output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10282" target="_blank">02:51:22.580</a></span> | <span class="t">If all of the prompts have reached the end-of-sentence token,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10286" target="_blank">02:51:26.900</a></span> | <span class="t">then we stop this for loop.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10288" target="_blank">02:51:28.180</a></span> | <span class="t">We don't need to inference anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10289" target="_blank">02:51:29.540</a></span> | <span class="t">Now, we prepare the output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10296" target="_blank">02:51:36.020</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10305" target="_blank">02:51:45.860</a></span> | <span class="t">so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10317" target="_blank">02:51:57.700</a></span> | <span class="t">so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10331" target="_blank">02:52:11.700</a></span> | <span class="t">so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10341" target="_blank">02:52:21.380</a></span> | <span class="t">this means that if we found an end-of-sentence token for one of the prompts,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10356" target="_blank">02:52:36.820</a></span> | <span class="t">we just cut the prompt output there.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10359" target="_blank">02:52:39.380</a></span> | <span class="t">The model output at that particular token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10362" target="_blank">02:52:42.100</a></span> | <span class="t">We don't care about what it outputs next.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10368" target="_blank">02:52:48.660</a></span> | <span class="t">This is the output text and then we output the tokens and the text.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10385" target="_blank">02:53:05.540</a></span> | <span class="t">Hopefully, I didn't make too many typos and mistakes.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10393" target="_blank">02:53:13.540</a></span> | <span class="t">So, now we need to build the sample_top_p.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10397" target="_blank">02:53:17.700</a></span> | <span class="t">So, we have the logits that are the output of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10400" target="_blank">02:53:20.820</a></span> | <span class="t">We transform them into probabilities by using the softmax.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10404" target="_blank">02:53:24.180</a></span> | <span class="t">But given these probabilities, we need to use the sample_top_p strategy</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10408" target="_blank">02:53:28.260</a></span> | <span class="t">to select all the tokens such that their cumulative probability is equal to top_p,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10414" target="_blank">02:53:34.900</a></span> | <span class="t">which in our case is 0.9, so 90 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10418" target="_blank">02:53:38.500</a></span> | <span class="t">Okay, the first thing we do is we sort these probabilities in descending order.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10434" target="_blank">02:53:54.100</a></span> | <span class="t">So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10444" target="_blank">02:54:04.100</a></span> | <span class="t">we then calculate the cumulative sum.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10462" target="_blank">02:54:22.820</a></span> | <span class="t">Then we create the mask that says which tokens we want to keep</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10466" target="_blank">02:54:26.740</a></span> | <span class="t">and which one we don't want to keep.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10468" target="_blank">02:54:28.260</a></span> | <span class="t">So, mask is equal to probability_sum minus probability_sort more than p.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10476" target="_blank">02:54:36.420</a></span> | <span class="t">Why do we do a minus probability_sort?</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10479" target="_blank">02:54:39.300</a></span> | <span class="t">Because we want to shift.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10480" target="_blank">02:54:40.740</a></span> | <span class="t">Let me show you on the slides here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10486" target="_blank">02:54:46.740</a></span> | <span class="t">You can see here, for example, the cumulative probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10492" target="_blank">02:54:52.500</a></span> | <span class="t">So, the probabilities are this one, 44 percent, 40 percent, 6 percent, 4 percent, and 3 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10499" target="_blank">02:54:59.780</a></span> | <span class="t">Then we calculated the cumulative.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10501" target="_blank">02:55:01.300</a></span> | <span class="t">That means up to here it's 44 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10505" target="_blank">02:55:05.140</a></span> | <span class="t">Then this one plus this one is 85 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10507" target="_blank">02:55:07.540</a></span> | <span class="t">Then this one plus this one plus this one is 91 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10510" target="_blank">02:55:10.500</a></span> | <span class="t">This one plus this one plus this one plus this one is 96 percent, etc, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10515" target="_blank">02:55:15.300</a></span> | <span class="t">But imagine we have a 0.90 percent probability or 0.5 percent probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10524" target="_blank">02:55:24.660</a></span> | <span class="t">We need to keep up to this token here because this one is not enough.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10531" target="_blank">02:55:31.140</a></span> | <span class="t">It's zero point.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10531" target="_blank">02:55:31.780</a></span> | <span class="t">So, we need to up to this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10533" target="_blank">02:55:33.300</a></span> | <span class="t">So, the first number that is less than or equal to p.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10536" target="_blank">02:55:36.820</a></span> | <span class="t">And it's this in case it's this one.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10539" target="_blank">02:55:39.700</a></span> | <span class="t">So, that's why we shift it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10540" target="_blank">02:55:40.820</a></span> | <span class="t">We want also this token inclusive.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10542" target="_blank">02:55:42.900</a></span> | <span class="t">And this is why we do this minus probability sort.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10548" target="_blank">02:55:48.900</a></span> | <span class="t">So, all the ones that we didn't select, we zero them out.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10553" target="_blank">02:55:53.860</a></span> | <span class="t">Zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10558" target="_blank">02:55:58.900</a></span> | <span class="t">And then we redistribute the probabilities because, of course,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10564" target="_blank">02:56:04.980</a></span> | <span class="t">if we remove some items from here, they don't sum up to one anymore.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10568" target="_blank">02:56:08.820</a></span> | <span class="t">So, we need to redistribute the probabilities.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10570" target="_blank">02:56:10.900</a></span> | <span class="t">And this is very easy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10595" target="_blank">02:56:35.300</a></span> | <span class="t">Okay, then the next token is basically, suppose we keep the first two tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10601" target="_blank">02:56:41.700</a></span> | <span class="t">And then what we do is we want to sample from them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10604" target="_blank">02:56:44.500</a></span> | <span class="t">So, the first token is 0.44 percent probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10607" target="_blank">02:56:47.460</a></span> | <span class="t">The second token is 0.40 percent probability.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10610" target="_blank">02:56:50.420</a></span> | <span class="t">But after we redistribute their probabilities, actually,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10613" target="_blank">02:56:53.380</a></span> | <span class="t">this one will be a little higher.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10614" target="_blank">02:56:54.740</a></span> | <span class="t">And this one will be a little higher than 40 percent.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10616" target="_blank">02:56:56.900</a></span> | <span class="t">And then we sample.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10620" target="_blank">02:57:00.820</a></span> | <span class="t">It means that the first token will have a slightly better chances of being chosen.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10624" target="_blank">02:57:04.660</a></span> | <span class="t">And the second token will have slightly less chance of being selected.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10627" target="_blank">02:57:07.860</a></span> | <span class="t">And we want one sample because we want one token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10641" target="_blank">02:57:21.540</a></span> | <span class="t">And it's not next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10644" target="_blank">02:57:24.500</a></span> | <span class="t">And then next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10646" target="_blank">02:57:26.420</a></span> | <span class="t">Because this indicates which index to select,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10651" target="_blank">02:57:31.140</a></span> | <span class="t">then we need to map that index to the actual number in the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10656" target="_blank">02:57:36.500</a></span> | <span class="t">But because we already changed the order of these numbers,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10660" target="_blank">02:57:40.180</a></span> | <span class="t">because we sorted it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10661" target="_blank">02:57:41.540</a></span> | <span class="t">So, initially, the logits were built in such a way</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10665" target="_blank">02:57:45.700</a></span> | <span class="t">that the first logit corresponded to the first number of the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10669" target="_blank">02:57:49.220</a></span> | <span class="t">The second logit corresponded to the second number of the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10672" target="_blank">02:57:52.900</a></span> | <span class="t">But because we sorted it by descending order, this order has been gone.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10676" target="_blank">02:57:56.820</a></span> | <span class="t">So, we don't know now, just given the token selected,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10680" target="_blank">02:58:00.420</a></span> | <span class="t">we don't know which number it maps back into the vocabulary.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10684" target="_blank">02:58:04.580</a></span> | <span class="t">That's why the sort method returns two arguments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10687" target="_blank">02:58:07.380</a></span> | <span class="t">One is the sorted numbers and one is the indexes that it changed.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10691" target="_blank">02:58:11.140</a></span> | <span class="t">So, it will tell you for each position what was the original item in that position.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10696" target="_blank">02:58:16.500</a></span> | <span class="t">So, this is why we actually query using gather.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10699" target="_blank">02:58:19.620</a></span> | <span class="t">Gather allows us to retrieve from an element what was the original one,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10709" target="_blank">02:58:29.380</a></span> | <span class="t">given this index here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10710" target="_blank">02:58:30.740</a></span> | <span class="t">And then we return the next token.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10713" target="_blank">02:58:33.380</a></span> | <span class="t">And this will map back into the vocabulary directly.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10719" target="_blank">02:58:39.700</a></span> | <span class="t">And this should be it.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10723" target="_blank">02:58:43.460</a></span> | <span class="t">So, now let's create some prompts and let's run the code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10727" target="_blank">02:58:47.540</a></span> | <span class="t">I have some prompts here that I copied and pasted.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10730" target="_blank">02:58:50.580</a></span> | <span class="t">So, now let's build the inference code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10734" target="_blank">02:58:54.660</a></span> | <span class="t">So, out_tokens, out_text, we want to generate maximum 64 tokens.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10749" target="_blank">02:59:09.460</a></span> | <span class="t">We assert that the len of the output text is actually equal equal to len of prompts.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10757" target="_blank">02:59:17.540</a></span> | <span class="t">It should be.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10758" target="_blank">02:59:18.900</a></span> | <span class="t">So, for i in range, hopefully the model will work.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10766" target="_blank">02:59:26.660</a></span> | <span class="t">And then we print the output text for each prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10782" target="_blank">02:59:42.900</a></span> | <span class="t">So, let's run the code and let's hope for the best.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10786" target="_blank">02:59:46.580</a></span> | <span class="t">Okay, self-attention is missing the required forward function.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10792" target="_blank">02:59:52.260</a></span> | <span class="t">Let's see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10798" target="_blank">02:59:58.820</a></span> | <span class="t">Oops, it's forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10804" target="_blank">03:00:04.820</a></span> | <span class="t">It should be forward.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10806" target="_blank">03:00:06.340</a></span> | <span class="t">Let's run again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10811" target="_blank">03:00:11.700</a></span> | <span class="t">Sum_received, this is wrong because it should be dimension, not div, but should be dim.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10823" target="_blank">03:00:23.460</a></span> | <span class="t">Let's run again.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10826" target="_blank">03:00:26.660</a></span> | <span class="t">For bfloat, 16.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10834" target="_blank">03:00:34.020</a></span> | <span class="t">So, let's see why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10841" target="_blank">03:00:41.140</a></span> | <span class="t">eos token, let me check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10845" target="_blank">03:00:45.300</a></span> | <span class="t">Okay, now it's training.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10859" target="_blank">03:00:59.940</a></span> | <span class="t">I just changed this tensor from capital T to small t.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10864" target="_blank">03:01:04.260</a></span> | <span class="t">I will investigate why.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10869" target="_blank">03:01:09.860</a></span> | <span class="t">Wow, we have an output.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10871" target="_blank">03:01:11.140</a></span> | <span class="t">So, let's check.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10872" target="_blank">03:01:12.420</a></span> | <span class="t">First of all, let's check the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10873" target="_blank">03:01:13.780</a></span> | <span class="t">Simply put, the theory of relativity states that time is relative to the observer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10882" target="_blank">03:01:22.020</a></span> | <span class="t">Mass is relative to the observer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10883" target="_blank">03:01:23.620</a></span> | <span class="t">Speed is relative to the observer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10885" target="_blank">03:01:25.220</a></span> | <span class="t">Energy is relative to the observer.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10887" target="_blank">03:01:27.380</a></span> | <span class="t">So, it looks like it's not bad.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10889" target="_blank">03:01:29.140</a></span> | <span class="t">Suppose the second prompt says if Google was an Italian company founded in Milan,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10895" target="_blank">03:01:35.940</a></span> | <span class="t">it would be listed on the Milan Stock Exchange,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10898" target="_blank">03:01:38.500</a></span> | <span class="t">as the Milan Stock Exchange is the largest in Italy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10901" target="_blank">03:01:41.060</a></span> | <span class="t">But since Google is a US company, it is listed on the Nasdaq Stock Exchange.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10904" target="_blank">03:01:44.740</a></span> | <span class="t">So, it avoided actually answering the question.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10906" target="_blank">03:01:46.740</a></span> | <span class="t">Let's try the few-shot prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10908" target="_blank">03:01:48.820</a></span> | <span class="t">So, this is how you copy it actually from the LAMA code.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10912" target="_blank">03:01:52.180</a></span> | <span class="t">So, they ask to translate from English to French.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10914" target="_blank">03:01:54.420</a></span> | <span class="t">And after cheese, we expect to find fromage, onion, etc.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10921" target="_blank">03:02:01.220</a></span> | <span class="t">So, it looks correct.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10922" target="_blank">03:02:02.660</a></span> | <span class="t">And we can also see that the spaces have been kept.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10926" target="_blank">03:02:06.420</a></span> | <span class="t">So, these spaces were not added by me, but actually by the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10929" target="_blank">03:02:09.700</a></span> | <span class="t">So, it keeps the output aligned with what was the prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10932" target="_blank">03:02:12.900</a></span> | <span class="t">And then I created a zero-shot prompt.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10936" target="_blank">03:02:16.340</a></span> | <span class="t">So, tell me if the following person is actually Doraemon disguised as human.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10940" target="_blank">03:02:20.100</a></span> | <span class="t">So, the name is Umar Jameel, and the decision is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10942" target="_blank">03:02:22.180</a></span> | <span class="t">he's a hero in every sense of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10945" target="_blank">03:02:25.300</a></span> | <span class="t">He's a hero in every sense of the word.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10946" target="_blank">03:02:26.900</a></span> | <span class="t">I'm very happy, LAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10949" target="_blank">03:02:29.140</a></span> | <span class="t">Actually, okay, this is the output of the model.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10952" target="_blank">03:02:32.420</a></span> | <span class="t">With manual seed zero.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10953" target="_blank">03:02:33.700</a></span> | <span class="t">If I think I change the seed to some other number and run the model again,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10958" target="_blank">03:02:38.500</a></span> | <span class="t">the output will be totally different or maybe slightly different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10962" target="_blank">03:02:42.260</a></span> | <span class="t">I hope not, but it may be different.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10964" target="_blank">03:02:44.420</a></span> | <span class="t">Anyway, thanks for watching my video, guys.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10966" target="_blank">03:02:46.740</a></span> | <span class="t">I tried to convey the idea of what is the architecture inside LAMA.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10974" target="_blank">03:02:54.420</a></span> | <span class="t">And even if I didn't build the training code,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10976" target="_blank">03:02:56.980</a></span> | <span class="t">because actually to build the training code is rather complicated,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10980" target="_blank">03:03:00.420</a></span> | <span class="t">we need a big corpus of text, we need to tokenize it,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10985" target="_blank">03:03:05.220</a></span> | <span class="t">and it's going to take a long time.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10986" target="_blank">03:03:06.900</a></span> | <span class="t">But I hope to make another video in the future on how to train a language model,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10993" target="_blank">03:03:13.460</a></span> | <span class="t">maybe with a smaller dataset and with a lighter architecture.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=10996" target="_blank">03:03:16.420</a></span> | <span class="t">And I tried to convey all the math behind all the choices,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11001" target="_blank">03:03:21.300</a></span> | <span class="t">and also how the inner workings of the KV cache and the grouped query attention.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11008" target="_blank">03:03:28.980</a></span> | <span class="t">If you have any questions, please write in the comments.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11011" target="_blank">03:03:31.380</a></span> | <span class="t">I also will share the repository with the code that I have previously built for this,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11016" target="_blank">03:03:36.500</a></span> | <span class="t">and which has much more comments than the one I have written here.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11020" target="_blank">03:03:40.500</a></span> | <span class="t">It's much more in detail,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11022" target="_blank">03:03:42.660</a></span> | <span class="t">so everyone can understand step by step all the dimensions involved.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11026" target="_blank">03:03:46.420</a></span> | <span class="t">Here I tried to write the most important dimensions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11029" target="_blank">03:03:49.300</a></span> | <span class="t">but because of time, I didn't write all of them.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11031" target="_blank">03:03:51.380</a></span> | <span class="t">So thank you again, guys, for watching.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11034" target="_blank">03:03:54.580</a></span> | <span class="t">It was a long journey, but I can assure you that you learned a lot.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11038" target="_blank">03:03:58.260</a></span> | <span class="t">Hopefully.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11039" target="_blank">03:03:59.060</a></span> | <span class="t">And I hope you will visit again my channel for more videos about deep learning,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11043" target="_blank">03:04:03.620</a></span> | <span class="t">about PyTorch, about coding, and about everything that we love in AI.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=oM4VmoabDAI&t=11048" target="_blank">03:04:08.580</a></span> | <span class="t">Thank you for watching, guys.</span></div></div></body></html>