<html><head><title>When Vectors Break Down: Graph-Based RAG for Dense Enterprise Knowledge - Sam Julien, Writer</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">back to index</a><h2>When Vectors Break Down: Graph-Based RAG for Dense Enterprise Knowledge - Sam Julien, Writer</h2><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow"><img src="https://i.ytimg.com/vi_webp/XlAIgmi_Vow/maxresdefault.webp" style="width:50%;"></a><div><br></div><div style="text-align: left;"><a href="./XlAIgmi_Vow.html">Whisper Transcript</a> | <a href="./transcript_XlAIgmi_Vow.html">Transcript Only Page</a></div><br><div style="max-width: 800px;"><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=0" target="_blank">00:00:00.000</a></span> | <span class="t">Welcome! So glad to see you all here. Welcome to When Vectors Break Down: Graph-based RAG for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=21" target="_blank">00:00:21.600</a></span> | <span class="t">dense enterprise knowledge. And big thank you to SWIX and Ben for putting on yet another amazing</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=27" target="_blank">00:00:27.360</a></span> | <span class="t">event. So it's a pretty interesting signal that we have an entire track dedicated to graph-based RAG.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=34" target="_blank">00:00:34.720</a></span> | <span class="t">And I think in addition to all of the agentic promise of graph-based RAG, we're also seeing that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=40" target="_blank">00:00:40.800</a></span> | <span class="t">the market is starting to catch up, that vector search is just not enough for RAG at scale. You</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=46" target="_blank">00:00:46.000</a></span> | <span class="t">may have seen this really interesting article by Joe Christian Burgum, who is around here somewhere</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=50" target="_blank">00:00:50.720</a></span> | <span class="t">on the rise and fall of the vector database infrastructure category and his subsequent</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=56" target="_blank">00:00:56.080</a></span> | <span class="t">interview on Latentspace, where he talked about how vector databases have experienced this gold rush</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=61" target="_blank">00:01:01.360</a></span> | <span class="t">after ChatGPT's launch, but that the industry is starting to recognize that vector search alone is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=67" target="_blank">00:01:07.120</a></span> | <span class="t">just insufficient for sophisticated retrieval and that we're going to need multiple strategies beyond</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=73" target="_blank">00:01:13.760</a></span> | <span class="t">simple vector similarity.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=75" target="_blank">00:01:15.440</a></span> | <span class="t">This is music to our ears at Rider because we've actually been talking about this for a long time. We've</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=80" target="_blank">00:01:20.560</a></span> | <span class="t">been talking about the benefits of graph-based RAG for a couple of years now. In fact, if you look at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=85" target="_blank">00:01:25.200</a></span> | <span class="t">this article from November 2023, which in AI time is like prehistoric times, we actually talk about the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=92" target="_blank">00:01:32.880</a></span> | <span class="t">benefits of knowledge graphs and the shortcomings of vector databases and simple similarity search for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=99" target="_blank">00:01:39.040</a></span> | <span class="t">enterprise RAG at scale. And if you're not familiar with Rider, we're this end-to-end agentic platform for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=105" target="_blank">00:01:45.920</a></span> | <span class="t">enterprises where we build our own models, we build our own graph-based RAG system and have this suite of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=110" target="_blank">00:01:50.720</a></span> | <span class="t">software tools on top of that for enterprises to be able to build agents and AI applications. And so,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=116" target="_blank">00:01:56.880</a></span> | <span class="t">as we've been building Knowledge Graph over the years, it's been an interesting journey as we've been</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=121" target="_blank">00:02:01.600</a></span> | <span class="t">working with these Fortune 500 and Global 2000 companies at scale. Most of them or many of them</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=128" target="_blank">00:02:08.400</a></span> | <span class="t">are in highly regulated industries like healthcare and finance, where accuracy and low hallucinations are</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=134" target="_blank">00:02:14.880</a></span> | <span class="t">super important. And so, our team has been putting together this system over the years of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=140" target="_blank">00:02:20.720</a></span> | <span class="t">components put together and different techniques that we could really drive our accuracy rate up high and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=146" target="_blank">00:02:26.800</a></span> | <span class="t">reduce our hallucinations. And so, what I wanted to share in this talk was kind of the journey</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=151" target="_blank">00:02:31.520</a></span> | <span class="t">of how we got there. And the main takeaway being, as you're seeing in several of these talks, like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=155" target="_blank">00:02:35.920</a></span> | <span class="t">the first talk about hybrid search, there are many different ways that you can get the benefits of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=160" target="_blank">00:02:40.000</a></span> | <span class="t">knowledge graphs in RAG. And also, how you get there and what you learn along the way is actually</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=166" target="_blank">00:02:46.640</a></span> | <span class="t">often very valuable as you're building out your retrieval system, almost just as valuable as the end result</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=172" target="_blank">00:02:52.720</a></span> | <span class="t">itself. So, I'm going to weave together these two stories of our journey to graph-based RAG and sort of the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=179" target="_blank">00:02:59.200</a></span> | <span class="t">first principles thinking that I think has made our team successful in putting together this system as</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=184" target="_blank">00:03:04.240</a></span> | <span class="t">we continue to iterate and improve on it. So, I'm Sam Julien. I'm the director of developer relations at</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=189" target="_blank">00:03:09.200</a></span> | <span class="t">Rider, and you can find most of my writing and books and newsletters and all of those things at samjulien.com.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=194" target="_blank">00:03:14.560</a></span> | <span class="t">So, I talked about this system composed of multiple pieces put together over a couple of different</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=200" target="_blank">00:03:20.720</a></span> | <span class="t">years. And I want to talk about sort of how we got to this point and where we are now. And I'm just going to put a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=207" target="_blank">00:03:27.360</a></span> | <span class="t">blanket caveat on here that please consider this a sketch and not a blueprint of what is currently</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=212" target="_blank">00:03:32.560</a></span> | <span class="t">in production. Of course, there are like many moving pieces and many layers to this, but I want to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=217" target="_blank">00:03:37.360</a></span> | <span class="t">abstract it enough to make it something that is practical and usable for people. So, our research</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=223" target="_blank">00:03:43.840</a></span> | <span class="t">team, we have a cracked research team at Rider, and they have four main areas of focus. Enterprise models,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=230" target="_blank">00:03:50.400</a></span> | <span class="t">like our Palmyra X5 model, that's the one powering the chat on the AI engineer website right now.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=235" target="_blank">00:03:55.600</a></span> | <span class="t">Practical evaluations, like our finance benchmark called Failsafe QA. Domain specific specialization,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=242" target="_blank">00:04:02.960</a></span> | <span class="t">these are our domain specific models like Palmyra Med and Palmyra Finn. And then what our focus is here,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=247" target="_blank">00:04:07.760</a></span> | <span class="t">retrieval and knowledge integration. So, bringing enterprise data to work with our models in a secure,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=255" target="_blank">00:04:15.040</a></span> | <span class="t">reliable way. And I think what's really cool about the way our research team works is that</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=259" target="_blank">00:04:19.520</a></span> | <span class="t">they're very focused on solving practical problems for our customers. They're not just sort of like</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=265" target="_blank">00:04:25.440</a></span> | <span class="t">working in isolation, working on theoretical things. They're actually driven by customer insights.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=270" target="_blank">00:04:30.720</a></span> | <span class="t">And that's really what I would consider like sort of the first meta lesson of why I think this is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=276" target="_blank">00:04:36.560</a></span> | <span class="t">working so well for Rider right now. We're really focused on solving the customer problems rather than</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=281" target="_blank">00:04:41.360</a></span> | <span class="t">implementing specific solutions. So, the problem that we are trying to solve kind of constantly,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=287" target="_blank">00:04:47.680</a></span> | <span class="t">as most of us are here, is that enterprise data is really dense, specialized, and massive. So,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=293" target="_blank">00:04:53.360</a></span> | <span class="t">we're often dealing with terabytes of data, and it uses very specific language, and it's often very</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=299" target="_blank">00:04:59.600</a></span> | <span class="t">clustered together. There's not a lot of diversity in the language used in these documents. And that's what</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=304" target="_blank">00:05:04.640</a></span> | <span class="t">our research and engineering teams have been focused on these last few years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=307" target="_blank">00:05:07.680</a></span> | <span class="t">So, like most, we kind of started out with a regular search of querying a knowledge base,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=314" target="_blank">00:05:14.800</a></span> | <span class="t">using an algorithm, and passing that to the LLM. But that quickly sort of like ran out because of,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=320" target="_blank">00:05:20.880</a></span> | <span class="t">you know, it was good for basic keyword searches, but not really great for that advanced similarity</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=324" target="_blank">00:05:24.800</a></span> | <span class="t">search that we needed. So, then, again, like most, we went to vector embeddings and did chunking and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=331" target="_blank">00:05:31.680</a></span> | <span class="t">embeddings and put it in a database and then similarity search and passing it to the LLM for the end user to</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=338" target="_blank">00:05:38.000</a></span> | <span class="t">query. But we ran into two major problems with this. The first is that with vector retrieval, chunking and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=346" target="_blank">00:05:46.880</a></span> | <span class="t">nearest neighbors can give inaccurate answers. So, if you look at this example of kind of this text about</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=353" target="_blank">00:05:53.200</a></span> | <span class="t">the founding of Apple and the timeline, it's very easy for us as humans to look at these text chunks</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=358" target="_blank">00:05:58.640</a></span> | <span class="t">and pick out the fact that the Macintosh was created in 1984. But when you chunk this text naively and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=364" target="_blank">00:06:04.720</a></span> | <span class="t">you just give it to a nearest neighbor search, it can get confused. And it thinks that it was actually in</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=369" target="_blank">00:06:09.280</a></span> | <span class="t">1983 instead of 1984 because it's in the same chunk as the introduction of the Lisa. Side note, I'm a huge</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=375" target="_blank">00:06:15.440</a></span> | <span class="t">vintage Apple nerd, and so I liked this example. The other big problem that we ran into with vector</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=382" target="_blank">00:06:22.480</a></span> | <span class="t">retrieval was that it was failing with really concentrated data. So, if you think about a lot of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=386" target="_blank">00:06:26.800</a></span> | <span class="t">large enterprises, it's not like they're dealing with documents where, like, some of them are talking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=390" target="_blank">00:06:30.480</a></span> | <span class="t">about animals and some of them are talking about fruit, right? So, if you have a mobile phone company,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=395" target="_blank">00:06:35.680</a></span> | <span class="t">for example, and they have thousands and thousands of documents that all use megapixels and cameras and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=401" target="_blank">00:06:41.120</a></span> | <span class="t">battery life and things like that, and you ask the RAG system and the LLM to compare two different phone</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=406" target="_blank">00:06:46.640</a></span> | <span class="t">models, it's going to really struggle with that because it's going to find all these answers and have</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=410" target="_blank">00:06:50.160</a></span> | <span class="t">no idea how to make sense of them. And so that's what took us to graph-based RAG, where instead we would</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=418" target="_blank">00:06:58.080</a></span> | <span class="t">query a graph database and get back the relevant documents using keys and generate an answer. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=424" target="_blank">00:07:04.400</a></span> | <span class="t">especially powerful if you combine that with, like, full text and similarity search and things like that.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=430" target="_blank">00:07:10.080</a></span> | <span class="t">And so this really helped us with our accuracy because we were able to preserve the relationships</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=436" target="_blank">00:07:16.880</a></span> | <span class="t">with the text and provide more context to the model. And this was really interesting because,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=442" target="_blank">00:07:22.160</a></span> | <span class="t">at the time, there actually weren't that many people doing graph-based RAG over the last couple of years.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=447" target="_blank">00:07:27.360</a></span> | <span class="t">And that's why I think the focus of the team on really trying to solve the problem of the customer</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=452" target="_blank">00:07:32.320</a></span> | <span class="t">rather than chase whatever was being hyped up at the time was really important. So that was really great.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=459" target="_blank">00:07:39.040</a></span> | <span class="t">But we did run into some challenges back then with using graph databases. Now this is not an indictment</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=464" target="_blank">00:07:44.720</a></span> | <span class="t">of any graph database technology. It's just that we were running into these issues at the time a couple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=470" target="_blank">00:07:50.240</a></span> | <span class="t">of years ago. And so there were four things that we ran into. First, that converting the data into the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=476" target="_blank">00:07:56.960</a></span> | <span class="t">structured graph was getting really challenging and costly at scale. As the graph database scaled, we were</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=483" target="_blank">00:08:03.760</a></span> | <span class="t">hitting the limits of our team's expertise as well as hitting some cost issues. And then we were running</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=489" target="_blank">00:08:09.280</a></span> | <span class="t">into some problems where Cypher was struggling with the advanced similarity matching that we needed. And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=494" target="_blank">00:08:14.320</a></span> | <span class="t">were noticing that LLMs were doing better with text-based queries rather than complex graph structures. Now,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=499" target="_blank">00:08:19.360</a></span> | <span class="t">again, if you were to do this now, you might not run into those problems, but this is what we ran into</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=503" target="_blank">00:08:23.440</a></span> | <span class="t">historically. And so I think the way that the team approached this is also very interesting, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=508" target="_blank">00:08:28.720</a></span> | <span class="t">they decided to stay flexible based on their expertise. So they were running into these problems</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=514" target="_blank">00:08:34.240</a></span> | <span class="t">that I think were not necessarily fundamental to the technology itself, but more like, okay, how can we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=518" target="_blank">00:08:38.880</a></span> | <span class="t">solve the problems for our customers using the expertise that we have on the team? And so they came up with a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=524" target="_blank">00:08:44.080</a></span> | <span class="t">few really interesting solutions to this problem, to these problems. So first, when it came to converting</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=530" target="_blank">00:08:50.240</a></span> | <span class="t">the data into the graph structure, the team went back to their expertise and they say, what do we know</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=534" target="_blank">00:08:54.320</a></span> | <span class="t">how to do? We know how to build models. So let's build a specialized model that can scale and run on CPUs or</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=541" target="_blank">00:09:01.760</a></span> | <span class="t">smaller GPUs, which I think is a really clever solution. Now, if you were to do this now, there's probably enough</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=547" target="_blank">00:09:07.840</a></span> | <span class="t">fast, small models out there that you could fine tune something like that. You wouldn't have to build it</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=551" target="_blank">00:09:11.440</a></span> | <span class="t">yourself. But at the time, we didn't really have any options like that. So the team built it themselves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=555" target="_blank">00:09:15.680</a></span> | <span class="t">and fine tuned a model that was trained to map this data into graph structures of nodes and edges. And</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=561" target="_blank">00:09:21.200</a></span> | <span class="t">we did some better context-aware splitting and chunking to preserve the context and the semantic</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=567" target="_blank">00:09:27.040</a></span> | <span class="t">relationships. And this really helped preserve the reliability. Okay. And so then the issues with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=574" target="_blank">00:09:34.720</a></span> | <span class="t">scaling of the graph databases and the limitations of the expertise on the team with the cost at scale.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=580" target="_blank">00:09:40.240</a></span> | <span class="t">So again, we went back and thought about what is our team's expertise in and what can we do? And so</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=585" target="_blank">00:09:45.120</a></span> | <span class="t">what we did was instead, we stored the data points as JSON in a Lucene-based search engine. So we take the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=591" target="_blank">00:09:51.040</a></span> | <span class="t">graph structure, we convert it into JSON, we put it in the search engine. And this allowed us to easily</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=596" target="_blank">00:09:56.720</a></span> | <span class="t">handle the large amounts of data without any performance or speed degradation at scale, while still</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=603" target="_blank">00:10:03.520</a></span> | <span class="t">being something that the team was really good at. And so the team had started to assemble this concept</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=609" target="_blank">00:10:09.680</a></span> | <span class="t">of what our RAG system was looking like. And again, this is kind of more of a historical snapshot and a</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=615" target="_blank">00:10:15.680</a></span> | <span class="t">sketch over time. But where we do the context-aware splitting and text-to-graph with this specialized</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=621" target="_blank">00:10:21.040</a></span> | <span class="t">model and then pass it to a search engine. And we were really starting to drive up our accuracy.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=626" target="_blank">00:10:26.560</a></span> | <span class="t">But we still have those problems with the similarity matching and the text-based queries doing better</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=633" target="_blank">00:10:33.760</a></span> | <span class="t">than the complex graph structures. And so again, the team sort of went back to first principles and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=639" target="_blank">00:10:39.520</a></span> | <span class="t">thought, okay, what is it that we're trying to solve here? And let's go back to the research and figure out</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=644" target="_blank">00:10:44.720</a></span> | <span class="t">what we can build on to build a solution that's best for our customers and our specific needs.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=649" target="_blank">00:10:49.440</a></span> | <span class="t">And I think this is kind of the final meta point of letting research challenge your assumptions. So</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=655" target="_blank">00:10:55.840</a></span> | <span class="t">rather than staying focused on the solution, step back, look at the research, and figure out what you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=661" target="_blank">00:11:01.520</a></span> | <span class="t">can do to solve the challenges for your customers. So they went back to the original RAG paper. And if you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=666" target="_blank">00:11:06.000</a></span> | <span class="t">go back to the original RAG paper, it doesn't actually ever talk about using prompt context and questions,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=670" target="_blank">00:11:10.880</a></span> | <span class="t">which is super interesting. It's sort of like the de facto way of doing RAG now. But the original RAG</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=676" target="_blank">00:11:16.080</a></span> | <span class="t">paper actually proposed this whole two-component architecture with a retriever and a generator with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=682" target="_blank">00:11:22.000</a></span> | <span class="t">a pre-trained sequence-to-sequence model. It never actually talks about prompt and context and questions.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=687" target="_blank">00:11:27.760</a></span> | <span class="t">And so that's where they came across Fusion and Decoder, which I kind of think of as an alternate timeline</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=693" target="_blank">00:11:33.120</a></span> | <span class="t">for RAG, like if we didn't go down the road of prompt and context and questions. And so Fusion and Decoder is</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=699" target="_blank">00:11:39.600</a></span> | <span class="t">this technique that kind of builds upon the original proposal of the original RAG paper, where it processes the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=705" target="_blank">00:11:45.200</a></span> | <span class="t">passages independently in the encoder to get linear scaling instead of quadratic scaling, but then jointly</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=710" target="_blank">00:11:50.880</a></span> | <span class="t">in the decoder for better evidence aggregation. So big efficiency breakthrough and lots of state-of-the-art</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=716" target="_blank">00:11:56.400</a></span> | <span class="t">performance. I know there's a super abstract. So if you go to Facebook, they actually have a Fusion and Decoder</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=721" target="_blank">00:12:01.760</a></span> | <span class="t">library that you can play around with and actually do the steps of Fusion and Decoder. I also know that at this point you're going,</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=728" target="_blank">00:12:08.320</a></span> | <span class="t">"What the heck is this guy talking about in a graph RAG track? Why are we talking about Fusion and Decoder?"</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=732" target="_blank">00:12:12.400</a></span> | <span class="t">Well, I'm glad you asked, because the next big breakthrough was knowledge graph with Fusion and Decoder.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=737" target="_blank">00:12:17.120</a></span> | <span class="t">So you can use knowledge graphs with Fusion and Decoder as a technique. And this sort of improves upon</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=743" target="_blank">00:12:23.360</a></span> | <span class="t">the Fusion and Decoder paper by using knowledge graphs to understand the relationships between</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=748" target="_blank">00:12:28.480</a></span> | <span class="t">the retrieved passages. And so it helps with this efficiency bottleneck and improves</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=752" target="_blank">00:12:32.880</a></span> | <span class="t">the process. I'm not going to walk through this diagram step by step, but this is the diagram</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=757" target="_blank">00:12:37.680</a></span> | <span class="t">in the paper of the architecture where it uses the graph and then does this kind of two-stage re-ranking</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=763" target="_blank">00:12:43.680</a></span> | <span class="t">of the passages. And it helps with improving the efficiency while also lowering the cost.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=768" target="_blank">00:12:48.960</a></span> | <span class="t">And so the team took all this research and came together to build their own implementation of Fusion and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=774" target="_blank">00:12:54.000</a></span> | <span class="t">Decoder, since we actually build our own models, to make that kind of the final piece of the puzzle.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=778" target="_blank">00:12:58.880</a></span> | <span class="t">And it really helped our hallucination rate. It really drove it down. And then we published a white</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=783" target="_blank">00:13:03.680</a></span> | <span class="t">paper with our own findings of it. And so then we kind of had that piece of the puzzle. And there's a few</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=789" target="_blank">00:13:09.120</a></span> | <span class="t">other techniques that we don't have time to go over. But point being, we're assembling together multiple</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=794" target="_blank">00:13:14.160</a></span> | <span class="t">techniques based on research to get the best results we can for our customers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=798" target="_blank">00:13:18.800</a></span> | <span class="t">So that's all well and good, but does it actually work? That's the important part, right? So we did</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=803" target="_blank">00:13:23.040</a></span> | <span class="t">some benchmarking last year. We used Amazon's robust QA data set and compared our retrieval system with</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=808" target="_blank">00:13:28.960</a></span> | <span class="t">Knowledge Graph and Fusion to Decoder and everything with seven different vector search systems. And we</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=816" target="_blank">00:13:36.720</a></span> | <span class="t">found that we had the best accuracy and the fastest response time. So I encourage you to check that out and</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=822" target="_blank">00:13:42.080</a></span> | <span class="t">kind of check out this process. Benchmarks are really cool. But what's even cooler is what it unlocks for</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=828" target="_blank">00:13:48.080</a></span> | <span class="t">our customers, which are various features in the product. For one, like most graph structures, we can</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=835" target="_blank">00:13:55.680</a></span> | <span class="t">actually expose the thought process because we have that relationships and the additional context where you</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=841" target="_blank">00:14:01.680</a></span> | <span class="t">can show the snippets and the sub queries and the sources for how the RAG system is actually getting the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=846" target="_blank">00:14:06.880</a></span> | <span class="t">answers. And we can expose this in the API to developers as well as in the product.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=851" target="_blank">00:14:11.120</a></span> | <span class="t">Benchmarks: Yes. Benchmarks: And then we're also able to have knowledge graphics sell it multi-hop</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=855" target="_blank">00:14:15.440</a></span> | <span class="t">questions where we can reason across multiple documents and multiple topics without any struggles.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=863" target="_blank">00:14:23.200</a></span> | <span class="t">Benchmarks: And then lastly, it can handle complex data formats where vector retrieval struggles, where</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=867" target="_blank">00:14:27.920</a></span> | <span class="t">an answer might be split into multiple pages or maybe there's a similar term that doesn't quite match</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=873" target="_blank">00:14:33.920</a></span> | <span class="t">what the user is looking for. But because we have that graph structure and Fusion and Decoder with the</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=879" target="_blank">00:14:39.120</a></span> | <span class="t">additional context and relationships, we're able to formulate these correct answers.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=884" target="_blank">00:14:44.240</a></span> | <span class="t">Benchmarks: So again, my main takeaway here is that there are many ways that you can get the benefits of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=890" target="_blank">00:14:50.800</a></span> | <span class="t">knowledge graphs in RAG. That could be through a graph database. It could be through doing something</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=895" target="_blank">00:14:55.360</a></span> | <span class="t">creative with Postgres. It could be through a search engine. But you can take advantage of the relationships</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=901" target="_blank">00:15:01.920</a></span> | <span class="t">that you can build with knowledge graphs in your RAG system. And as you get there, you can challenge your</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=906" target="_blank">00:15:06.720</a></span> | <span class="t">assumptions and focus on the customers to be able to get to the end result to make the team successful.</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=912" target="_blank">00:15:12.400</a></span> | <span class="t">Benchmarks: And so for our team, it was focusing on the customer needs instead of what was hyped, staying flexible based</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=917" target="_blank">00:15:17.680</a></span> | <span class="t">on the expertise of the team, and letting research challenge their assumptions. So if you want to join this</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=924" target="_blank">00:15:24.400</a></span> | <span class="t">amazing team, we're hiring across research engineering and products. We would love to talk to you about any of</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=929" target="_blank">00:15:29.520</a></span> | <span class="t">our open roles. And I'm available for questions. You can come find me in the hallway or reach out to me</span></div><div class="c"><span class="s"><a href="https://www.youtube.com/watch?v=XlAIgmi_Vow&t=934" target="_blank">00:15:34.880</a></span> | <span class="t">on Twitter or LinkedIn. And that's all I've got for you. Thank you so much.</span></div></div></body></html>