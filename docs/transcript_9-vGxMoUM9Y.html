<html><head><title>Trust, but Verify: Shreya Rajpal</title>
<style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
        color: #333;
    }
    .container {
        width: 95%;  /* Increased width to use more space */
        margin: auto;
        overflow: auto;  /* Added to handle overflow by adding a scrollbar if necessary */
    }
    h2, h3 {
        color: #333;
        text-align: center;
    }
    a {
        color: #0000FF;  /* Traditional blue color for links */
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    img {
        display: block;
        margin: auto;
        max-width: 100%;
    }
    .c {
        margin: 10px 0;
    }
    .s, .t {
        display: inline-block;
        margin-right: 5px;
    }
    .max-width {
        max-width: 800px;
        margin: auto;
        padding-left: 20px;
    }
    table {
        width: 100%;
        border-collapse: collapse;
    }
    th, td {
        border: 1px solid #ddd;
        padding: 8px;
        text-align: left;  /* Ensure text alignment is consistent */
    }
    tr:nth-child(even) {
        background-color: #f2f2f2;
    }
    tr:nth-child(odd) {
        background-color: #e6e6e6;
    }
</style>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-69VLBMTTP0"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-69VLBMTTP0');
    </script>
    </head><body><div class='container'><a href="index.html">Back to Index</a><h2>Trust, but Verify: Shreya Rajpal</h2><a href="https://www.youtube.com/watch?v=9-vGxMoUM9Y" target="_blank"><img src="https://i.ytimg.com/vi/9-vGxMoUM9Y/sddefault.jpg" style="width:50%;"></a><div><br></div><h3>Transcript</h3><div class='max-width'><p>Shreya Rajpal: Hey everyone, thank you for coming. I am Shreya Rajpal, I am one of the co-founders and the CEO of Godrails.ai. And today we are going to be talking about trust but verify, which is a new programming paradigms that we need as we're entering Gen.ai native application development.</p><p>Before we get started, a little bit about me. As I mentioned, I'm currently at Godrails.ai. In the past, I've spent about a decade or so working in machine learning. Previously, I was the machine learning infrastructure lead at Predibase, which is a machine learning infrastructure company. I spent a number of years in the self-driving car space, working across the stack of self-driving, and before that did research in classical AI and deep learning.</p><p>Shreya Rajpal: Awesome, so we're seeing this massive explosion in AI applications over the last year. There's a lot of excitement. And that's also why so many of you guys are here attending this. We have folks from Auto GPT, which really took the world by storm and opened up the possibility, and all of our minds with what AI can do.</p><p>We've seen a lot of really awesome applications in mental illness, sales, even software engineering. This is a relevant graph. This is basically search interest for artificial intelligence over time. You can really see that peak around where ChatGPT came out. But if you think about where a lot of the reality is, or a lot of where the value lies today, even though generative AI applications have seen fastest adoption compared to a lot of these other consumer applications, their retention right now tends to be lower.</p><p>So these are some graphs I borrowed from this really fantastic article by Sequoia. And you can really see that retention for AI-first companies versus the one-month retention for non-AI-first traditional software companies. So why is this the case? A common symptom that a lot of people experience as they're working with generative AI applications is, my app worked while prototyping, but it failed the moment I tried shipping it out.</p><p>Or even the moment someone else tried testing this, it just behaved very unreliably. But the root cause of this symptom is that machine learning is fundamentally non-deterministic. For those of you, we're going to dig deeper into what that really means. So I'm guessing that a lot of you here have worked with traditional software systems before.</p><p>So if you think about a database and querying a database to get a question about how much was the spend of X user over the last month. Every single time you hit that database API, you are going to get what is the correct response. Right? And correct really means representative of whatever your true data actually is.</p><p>So this is completely irrespective of uptime and availability, et cetera. This fundamental property allows you to really build these really complex software systems which are in the same way for our world today. But if you think about machine learning model APIs, this is not really the case. Because if you know fundamental stochasticity that is part of machine learning systems, for a lot of you that have worked with generative AI systems and LLMs in the past, you'll see that even if you ask the same question across multiple times in a row, you're going to end up seeing different responses.</p><p>And because of this, being able to build these really complex systems that talk to each other, that rely on previous outputs, et cetera, becomes harder. Because you have this issue of compounding errors that really kind of explodes. This is just diving deeper into the problem a little bit. A lot of common issues as you work with these problems, hallucinations.</p><p>That's a very buzzwordy thing that a lot of us here are familiar with. But there's a lot of other issues like correct structure, their vulnerability to prompt injections. And all of this is exacerbated by the fact that unlike all other previous generations of programming, the only tool that is really available to you is English.</p><p>It's just the prompt that you can really work with. So we end up in the scenario right now and in the current time that we're in where use of LLMs is limited wherever correctness is really critical. I love GitHub Copilot, it's on my badge as my favorite tool, but if GitHub Copilot is wrong, you just ignore it and move on.</p><p>Same as ChatGPT, the chat interface is really, really great because it's iterative, and you can give it feedback, and if it's incorrect, you can tell it why it's incorrect, and it can maybe give you something that's more appropriate. But this is not the use case for a lot of really high value critical applications.</p><p>And so how do we add correctness guarantees to LLMs while still retaining their flexible nature that really allows them to adapt so well to so many tasks? So I'm going to add this quick quote here by Alex Gravely, who is the creator of GitHub Copilot. It's a very simple idea, which is that add a constraint checker to check for valid generation.</p><p>On violation, inject what was generated and the rule violation and regenerate. So once again, we're trying to think about how programming paradigms change as we're working with this fundamentally non-deterministic technology. So this is something that wasn't needed for the longest time because we're working with deterministic systems, but becomes very relevant now.</p><p>So interestingly, this tweet was actually pretty recent. But Guardrails AI, the open source framework that implements this and kind of like builds a framework around this strategy, has existed for a little while longer from the beginning of this year. So Guardrails acts as a safety firewall around your LLMs, and this kind of fundamentally introduces a novel paradigm that, once again, wasn't as necessary in the previous generations of software development.</p><p>So this is what a lot of the software development architectures for applications that you might build may look like, where you have some application, and then in that application you have a prompt that gets sent to an LLM, and then you end up getting some output or some response back.</p><p>So this is the new paradigm that we propose, and that Guardrails kind of implements as a framework, wherein every output that you get back passes through a verification suite. And that verification suite looks at all of the functional areas of, you know, inconsistencies or risks that you are really sensitive to as an application builder, which may be very, very different from, you know, if you're building a code generation application, whereas if you're building, like, a healthcare chatbot.</p><p>So maybe, like, containing PII or PHI, like, sensitive information might be something you want to check against, or profanity filtering that out. If you're building a commercial application, you might really care about the fact that there's no mention of any competitors, like, if you're building a McDonald's chatbot, like, nobody should be able to get your chatbot to say that Burger King's the best burger in town.</p><p>So, making sure that any code that you generate is executable within your environment, as well as, you know, summarization or free-form text generation is true and grounded in the source that you know to be, you know, correct, and not just hallucinated from the model. So, each of these ends up being an independent check that runs as part of this, like, comprehensive verification suite that allows you to build trust in the models and the ML applications that you're building.</p><p>So, the paradigm that we propose is that only use large language model outputs. If your verification suite passes on failure, you can really hook into this very powerful capability that LLMs unleash, which is, you know, their ability to, like, self-heal, which is that if you tell them why they're wrong, they can often correct themselves.</p><p>And you can kind of go through this loop again if you have the, you know, latency budget or even the dollar budget or the token budget to implement this. I'm going to, like, go over this very briefly, but under the hood how Guardrails does this is that it allows you to create what we call guards from, you know, different inputs.</p><p>So, you can use, like, either a declarative model spec, such as, like, you know, like XML or rail. You can use pydantic models that implement, like, specific validation criteria and structure. Or you can use string implementation. You can create a guard from all of these components. If you want, you can add information about, you know, your prompt as well as the LLMs you want to use.</p><p>And then you create this at initialization. But at runtime, this guard will basically surround your LLM callable and then make sure that everything that you're sending in or getting out of the LLM is valid and correct for you. Right? So, for example, if your output is valid, you end up sending the output back to your application.</p><p>But if it's invalid, you go through this loop of looking at which constraint is violated or which check is violated. And then if on violation, you have a set of these policies including, like, re-asking, which we touched on earlier. Filtering or fixing, which is programmatically trying to correct output.</p><p>Falling back on some other system. So refraining from answering. Or, you know, just no-op where you don't actively take an action, but you log and store what the outputs of those checks or verification was and, like, why that particular check failed. And then you only do this, like, on -- you only return the output once you know you can trust whatever came out of the LLM.</p><p>So within this framework, what Guardrails.ai does is it's a fully open source library that allows you to, A, create custom validators. It orchestrates the whole validation and verification process for you to make sure that, you know, you're not taking on this, like, really kind of, like, often latency-intensive task of doing validation and make sure that it's done as efficiently as possible.</p><p>It's a library and a catalog of many, many commonly used validators across a bunch of use cases. And it's a specification language that allows you to compile your requirements into a prompt. So that, like, whatever specific validators you want to use are automatically turned into a prompt so that you know that, you know, those requirements are also being communicated to the LLM.</p><p>All right, so a common question, why do I need this? Why can't I just use prompt engineering or, you know, a better fine-tuned model? So, okay, so for some reason my rendering here is weird. But controlling the outputs with prompts, including using retrieval augmented generation, which basically injects specific context into your prompt, doesn't act as a guarantee, right?</p><p>LLMs are stochastic. Even if you do all the prompt engineering in the world, there's nothing guaranteeing that those instructions will be followed. We actually did this as an experiment for an unrelated thing where we used LLMs as evaluators. We ran the exact same experiment five different times, changing, like, absolutely zero parameters with zero temperature and saw, like, different numbers across our benchmark, which is, you know, really fascinating and wouldn't really fly in, like, previous generations of machine learning.</p><p>And then second, prompts don't offer any guarantees. LLMs don't, you know, always follow instructions. The alternative is also, like, controlling the outputs with models. So, first of all, it is very expensive and time-consuming to train a model. In my past life, this was basically what I've done my whole life.</p><p>And I was so frustrated with this whole process as I joined a startup where my job was to make this, you know, this process easier, like, as a function. But it still requires, like, you know, compiling a lot of data set, which is expensive, training a model over a bunch of hyperparameters, and then serving it.</p><p>And then if you aren't doing that and you're using, like, an LLM that's hidden behind a commercial API, you typically don't have any control over model version updates. So I've kind of seen this where, you know, I mentioned, like, validations get compiled into prompts. So I've kind of, like, observed where commercial models will get updated under the hood.</p><p>And so prompts that might have worked for you in the past will stop working just over time. So how do these guardrails work under the hood, right? There's no, like, one-stop-shop solution for a guardrail here. It really depends on the type of problem that you're solving. So a very reliable way, if possible, for implementing a guardrail is to ground it in an external system.</p><p>So let's say you're working in a code generation app. A really good way to generate more reliable code is to actually hook up the output of the LLM into a runtime that basically contains application-specific data. So we tried it for a lot of text-to-SQL applications, which is something that is supported as a first-class citizen in guardrails.</p><p>And we found that this re-asking framework, where you hook it up to, you know, a sandbox that contains your database and your schema, really substantially improved the correctness of the SQL queries that you got. You can also use rule-based heuristics. So really looking into, like, OK, if I'm, let's say, trying to extract an interest rate from a really long document, I always must know that interest rates, you know, end with, like, percentage signs.</p><p>And so that can be a clue that I must always be retrieving. You can try to use, like, traditional machine learning methods or high-precision deep learning classifiers. So really you don't need the full power of an LLM to solve, you know, really basic constraints. So trying to find, like, is there some type of toxicity in this output?</p><p>Does some type of output contain, you know, advice that is harmful for my users or is misleading my users in some way? You don't need-- my favorite analogy to use is you don't need, like, a jackhammer to crack open a walnut. So if possible, you know, some of the guardrails should use, like, smaller classifiers that are much more reliable and deterministic instead of, you know, using LLMs.</p><p>And then finally you can also use LLM self-reflection. All right. So we're going to walk through this example of how this works in practice for building a chatbot where you want to generate correct responses always. So let's say you're an organization that has certain help center articles and you want to make sure that you always generate-- you know, your users can ask questions over those help center articles in a chatbot.</p><p>And you always generate, like, correct responses where correctness means no hallucinations, not using any foul language, so don't swear at your customers, and never mention any competitors. Now, how do you really prevent hallucinations? Like, that's a very fundamental question, right? Providence guardrails. Providence guardrails essentially mean that every LLM utterance should have some leaning in a source of truth, right?</p><p>Especially if you're building, like, retrieval augmented generation applications. You make the assumption that, okay, I gave it this context. I hope it's using the context. What you want to make sure is that every output that is generated, you're able to pinpoint to where in the context, you know, your response kind of came from.</p><p>So this is one of the guardrails that, you know, exists in our catalog of guardrails. Under the hood, there's a few different techniques that we employ. We use embedding similarity. We also have, like, classifiers that are built on traditional NLI, like, natural language inference models. And we use LLM self-reflection.</p><p>This is a very brief, you know, snippet of, like, how to configure a guard, where you can essentially, like, select from this catalog which guardrails you want to use. So we've used provenance, profanity, no references to peer or competitor institutions. And then you essentially wrap your LLM call with, you know, the guard that you've created.</p><p>So very briefly, let's say you get some question which is, like, how do I change my password on your application? You have, like, some prompt that, you know, is constructed from your retrieval augmented generation application. But because LLMs are very, very prone to hallucinating, there's, like, it hallucinates where the setting exists for you in your, you know, in the response.</p><p>When this passes through your verification suite, the provenance guardrail will essentially spike and will cause the LLM to, you know, like, go through this, like, re-asking loop. Where a re-ask prompt will automatically be constructed for you via guardrails, which will, like, pinpoint which part is hallucinated, give it the context again, and ask it to correct itself.</p><p>And then finally, the re-ask output, you know, it tends to be more correct. And so we can kind of see here in this toy example that the output is, you know, corrected for you. And finally, verification passes, and you can send this back to the output. Very briefly, more examples of validators that you can create or that exist.</p><p>Never giving any financial or healthcare advice. Making sure that any code that you generate is usable. Never asking any private questions from your customers or mentioning competitors. No profanity, prompt injection, et cetera. And then just to summarize what guardrails does for you, custom validations, orchestration of verification, a catalog of commonly used guardrails, as well as automatic prompt compilation from your verification checks.</p><p>To follow along, you can look at the GitHub project, which is at Shreya r/guardrails. Our website with our documentation is guardrailsai.com. Or you can follow me or the project on Twitter. And that's for my LinkedIn. Awesome. Thank you so much, everyone. Thank you. Thank you. Thank you. Thank you.</p><p>you you</p></div></div></body></html>