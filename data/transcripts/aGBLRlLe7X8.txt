
[00:00:00.000 --> 00:00:05.000]   "At which point is the neural network a being versus a tool?"
[00:00:05.000 --> 00:00:11.400]   The following is a conversation with Aurel Vinales,
[00:00:11.400 --> 00:00:13.480]   his second time in the podcast.
[00:00:13.480 --> 00:00:15.960]   Aurel is the research director
[00:00:15.960 --> 00:00:18.040]   and deep learning lead at DeepMind,
[00:00:18.040 --> 00:00:20.980]   and one of the most brilliant thinkers and researchers
[00:00:20.980 --> 00:00:24.360]   in the history of artificial intelligence.
[00:00:24.360 --> 00:00:26.680]   This is the Lex Friedman Podcast.
[00:00:26.680 --> 00:00:28.880]   To support it, please check out our sponsors
[00:00:28.880 --> 00:00:30.200]   in the description.
[00:00:30.200 --> 00:00:33.600]   And now, dear friends, here's Aurel Vinales.
[00:00:33.600 --> 00:00:37.060]   You are one of the most brilliant researchers
[00:00:37.060 --> 00:00:38.480]   in the history of AI,
[00:00:38.480 --> 00:00:40.600]   working across all kinds of modalities.
[00:00:40.600 --> 00:00:42.720]   Probably the one common theme is
[00:00:42.720 --> 00:00:45.040]   it's always sequences of data.
[00:00:45.040 --> 00:00:48.020]   So we're talking about languages, images, even biology,
[00:00:48.020 --> 00:00:50.280]   and games, as we talked about last time.
[00:00:50.280 --> 00:00:53.400]   So you're a good person to ask this.
[00:00:53.400 --> 00:00:57.360]   In your lifetime, will we be able to build an AI system
[00:00:57.360 --> 00:01:00.760]   that's able to replace me as the interviewer
[00:01:00.760 --> 00:01:02.600]   in this conversation,
[00:01:02.600 --> 00:01:04.480]   in terms of ability to ask questions
[00:01:04.480 --> 00:01:06.600]   that are compelling to somebody listening?
[00:01:06.600 --> 00:01:09.400]   And then further question is,
[00:01:09.400 --> 00:01:10.640]   are we close,
[00:01:10.640 --> 00:01:13.880]   will we be able to build a system that replaces you
[00:01:13.880 --> 00:01:16.080]   as the interviewee
[00:01:16.080 --> 00:01:18.120]   in order to create a compelling conversation?
[00:01:18.120 --> 00:01:20.040]   How far away are we, do you think?
[00:01:20.040 --> 00:01:21.800]   - It's a good question.
[00:01:21.800 --> 00:01:24.680]   I think partly I would say, do we want that?
[00:01:24.680 --> 00:01:29.360]   I really like when we start now with very powerful models,
[00:01:29.360 --> 00:01:30.960]   interacting with them
[00:01:30.960 --> 00:01:34.040]   and thinking of them more closer to us.
[00:01:34.040 --> 00:01:34.880]   The question is,
[00:01:34.880 --> 00:01:38.320]   if you remove the human side of the conversation,
[00:01:38.320 --> 00:01:40.200]   is that an interesting,
[00:01:40.200 --> 00:01:42.320]   is that an interesting artifact?
[00:01:42.320 --> 00:01:44.440]   And I would say probably not.
[00:01:44.440 --> 00:01:47.400]   I've seen, for instance, last time we spoke,
[00:01:47.400 --> 00:01:50.280]   like we were talking about StarCraft
[00:01:50.280 --> 00:01:53.480]   and creating agents that play games
[00:01:53.480 --> 00:01:54.880]   involves self-play,
[00:01:54.880 --> 00:01:57.600]   but ultimately what people care about was,
[00:01:57.600 --> 00:01:59.080]   how does this agent behave
[00:01:59.080 --> 00:02:02.680]   when the opposite side is a human?
[00:02:02.680 --> 00:02:04.720]   So without a doubt,
[00:02:04.720 --> 00:02:08.520]   we will probably be more empowered by AI.
[00:02:08.520 --> 00:02:12.480]   Maybe you can source some questions from an AI system.
[00:02:12.480 --> 00:02:13.960]   I mean, that even today, I would say,
[00:02:13.960 --> 00:02:17.040]   it's quite plausible that with your creativity,
[00:02:17.040 --> 00:02:19.400]   you might actually find very interesting questions
[00:02:19.400 --> 00:02:20.720]   that you can filter.
[00:02:20.720 --> 00:02:22.400]   We call this cherry picking sometimes
[00:02:22.400 --> 00:02:24.040]   in the field of language.
[00:02:24.040 --> 00:02:27.520]   And likewise, if I had now the tools on my side,
[00:02:27.520 --> 00:02:28.520]   I could say, look,
[00:02:28.520 --> 00:02:30.640]   you're asking this interesting question.
[00:02:30.640 --> 00:02:31.600]   From this answer,
[00:02:31.600 --> 00:02:34.760]   I like the words chosen by this particular system
[00:02:34.760 --> 00:02:36.600]   that created a few words.
[00:02:36.600 --> 00:02:41.280]   Completely replacing it feels not exactly exciting to me.
[00:02:41.280 --> 00:02:43.760]   Although in my lifetime, I think way,
[00:02:43.760 --> 00:02:45.520]   I mean, given the trajectory,
[00:02:45.520 --> 00:02:48.000]   I think it's possible that perhaps
[00:02:48.000 --> 00:02:49.880]   there could be interesting,
[00:02:49.880 --> 00:02:53.040]   maybe self-play interviews as you're suggesting
[00:02:53.040 --> 00:02:56.160]   that would look or sound quite interesting
[00:02:56.160 --> 00:02:57.720]   and probably would educate,
[00:02:57.720 --> 00:02:59.160]   or you could learn a topic
[00:02:59.160 --> 00:03:01.600]   through listening to one of these interviews
[00:03:01.600 --> 00:03:03.200]   at a basic level, at least.
[00:03:03.200 --> 00:03:04.800]   - So you said it doesn't seem exciting to you,
[00:03:04.800 --> 00:03:07.520]   but what if exciting is part of the objective function
[00:03:07.520 --> 00:03:09.120]   the thing is optimized over?
[00:03:09.120 --> 00:03:12.840]   So there's probably a huge amount of data of humans,
[00:03:12.840 --> 00:03:14.120]   if you look correctly,
[00:03:14.120 --> 00:03:16.080]   of humans communicating online,
[00:03:16.080 --> 00:03:19.560]   and there's probably ways to measure the degree of,
[00:03:19.560 --> 00:03:21.920]   as they talk about engagement.
[00:03:21.920 --> 00:03:24.120]   So you can probably optimize the question
[00:03:24.120 --> 00:03:28.680]   that's most created an engaging conversation in the past.
[00:03:28.680 --> 00:03:31.560]   So actually, if you strictly use the word exciting,
[00:03:31.560 --> 00:03:36.520]   there is probably a way
[00:03:36.520 --> 00:03:40.320]   to create a optimally exciting conversations
[00:03:40.320 --> 00:03:42.160]   that involve AI systems.
[00:03:42.160 --> 00:03:44.600]   At least one side is AI.
[00:03:44.600 --> 00:03:45.640]   - Yeah, that makes sense.
[00:03:45.640 --> 00:03:48.880]   I think maybe looping back a bit to games
[00:03:48.880 --> 00:03:50.240]   and the game industry,
[00:03:50.240 --> 00:03:53.040]   when you design algorithms,
[00:03:53.040 --> 00:03:55.800]   you're thinking about winning as the objective, right?
[00:03:55.800 --> 00:03:57.320]   Or the reward function.
[00:03:57.320 --> 00:04:00.080]   But in fact, when we discuss this with Blizzard,
[00:04:00.080 --> 00:04:02.320]   the creators of StarCraft in this case,
[00:04:02.320 --> 00:04:05.360]   I think what's exciting, fun,
[00:04:05.360 --> 00:04:09.160]   if you could measure that and optimize for that,
[00:04:09.160 --> 00:04:11.720]   that's probably why we play video games
[00:04:11.720 --> 00:04:14.600]   or why we interact or listen or look at cat videos
[00:04:14.600 --> 00:04:16.440]   or whatever on the internet.
[00:04:16.440 --> 00:04:19.480]   So it's true that modeling reward
[00:04:19.480 --> 00:04:21.320]   beyond the obvious reward functions
[00:04:21.320 --> 00:04:23.720]   we've used to in reinforcement learning
[00:04:23.720 --> 00:04:25.560]   is definitely very exciting.
[00:04:25.560 --> 00:04:28.240]   And again, there is some progress actually
[00:04:28.240 --> 00:04:32.160]   into a particular aspect of AI, which is quite critical,
[00:04:32.160 --> 00:04:36.120]   which is, for instance, is a conversation
[00:04:36.120 --> 00:04:38.200]   or is the information truthful, right?
[00:04:38.200 --> 00:04:41.640]   So you could start trying to evaluate these
[00:04:41.640 --> 00:04:44.400]   from except from the internet, right?
[00:04:44.400 --> 00:04:45.800]   That has lots of information.
[00:04:45.800 --> 00:04:50.160]   And then if you can learn a function automated ideally,
[00:04:50.160 --> 00:04:52.880]   so you can also optimize it more easily,
[00:04:52.880 --> 00:04:54.840]   then you could actually have conversations
[00:04:54.840 --> 00:04:59.360]   that optimize for non-obvious things such as excitement.
[00:04:59.360 --> 00:05:01.040]   So yeah, that's quite possible.
[00:05:01.040 --> 00:05:03.560]   And then I would say in that case,
[00:05:03.560 --> 00:05:05.880]   it would definitely be fun exercise
[00:05:05.880 --> 00:05:08.040]   and quite unique to have at least one side
[00:05:08.040 --> 00:05:12.800]   that is fully driven by an excitement reward function.
[00:05:12.800 --> 00:05:16.920]   But obviously there would be still quite a lot of humanity
[00:05:16.920 --> 00:05:20.760]   in the system, both from who is building the system,
[00:05:20.760 --> 00:05:23.560]   of course, and also ultimately,
[00:05:23.560 --> 00:05:26.000]   if we think of labeling for excitement,
[00:05:26.000 --> 00:05:28.440]   that those labels must come from us
[00:05:28.440 --> 00:05:32.480]   because it's just hard to have a computational measure
[00:05:32.480 --> 00:05:34.560]   of excitement as far as I understand,
[00:05:34.560 --> 00:05:36.120]   there's no such thing.
[00:05:36.120 --> 00:05:39.240]   - Wow, as you mentioned truth also,
[00:05:39.240 --> 00:05:41.800]   I would actually venture to say that excitement
[00:05:41.800 --> 00:05:44.160]   is easier to label than truth,
[00:05:44.160 --> 00:05:49.000]   or is perhaps has lower consequences of failure.
[00:05:49.000 --> 00:05:54.920]   But there is perhaps the humanness that you mentioned,
[00:05:54.920 --> 00:05:58.240]   that's perhaps part of a thing that could be labeled.
[00:05:58.240 --> 00:06:02.480]   And that could mean an AI system that's doing dialogue,
[00:06:02.480 --> 00:06:07.480]   that's doing conversations should be flawed, for example.
[00:06:07.480 --> 00:06:09.440]   Like that's the thing you optimize for,
[00:06:09.440 --> 00:06:13.280]   which is have inherent contradictions by design,
[00:06:13.280 --> 00:06:15.080]   have flaws by design.
[00:06:15.080 --> 00:06:18.760]   Maybe it also needs to have a strong sense of identity.
[00:06:18.760 --> 00:06:22.680]   So it has a backstory, it told itself that it sticks to,
[00:06:22.680 --> 00:06:26.880]   it has memories, not in terms of how the system is designed,
[00:06:26.880 --> 00:06:30.360]   but it's able to tell stories about its past.
[00:06:30.360 --> 00:06:35.360]   It's able to have mortality and fear of mortality
[00:06:35.360 --> 00:06:39.120]   in the following way that it has an identity
[00:06:39.120 --> 00:06:41.240]   and like if it says something stupid
[00:06:41.240 --> 00:06:44.720]   and gets canceled on Twitter, that's the end of that system.
[00:06:44.720 --> 00:06:47.360]   So it's not like you get to rebrand yourself,
[00:06:47.360 --> 00:06:49.360]   that system is, that's it.
[00:06:49.360 --> 00:06:52.120]   So maybe that the high stakes nature of it,
[00:06:52.120 --> 00:06:54.560]   because like you can't say anything stupid now,
[00:06:54.560 --> 00:06:57.720]   or because you'd be canceled on Twitter.
[00:06:57.720 --> 00:06:59.760]   And that there's stakes to that.
[00:06:59.760 --> 00:07:01.160]   And that I think part of the reason
[00:07:01.160 --> 00:07:03.520]   that makes it interesting.
[00:07:03.520 --> 00:07:04.720]   And then you have a perspective
[00:07:04.720 --> 00:07:07.720]   like you've built up over time that you stick with,
[00:07:07.720 --> 00:07:09.120]   and then people can disagree with you.
[00:07:09.120 --> 00:07:11.800]   So holding that perspective strongly,
[00:07:11.800 --> 00:07:14.040]   holding sort of maybe a controversial,
[00:07:14.040 --> 00:07:16.300]   at least a strong opinion.
[00:07:16.300 --> 00:07:18.840]   All of those elements, it feels like they can be learned
[00:07:18.840 --> 00:07:21.760]   because it feels like there's a lot of data
[00:07:21.760 --> 00:07:24.520]   on the internet of people having an opinion.
[00:07:24.520 --> 00:07:25.400]   (laughs)
[00:07:25.400 --> 00:07:27.840]   And then combine that with a metric of excitement,
[00:07:27.840 --> 00:07:30.000]   you can start to create something that,
[00:07:30.000 --> 00:07:34.480]   as opposed to trying to optimize for sort of
[00:07:34.480 --> 00:07:38.120]   grammatical clarity and truthfulness,
[00:07:38.120 --> 00:07:42.000]   the factual consistency over many sentences,
[00:07:42.000 --> 00:07:45.320]   you optimize for the humanness.
[00:07:45.320 --> 00:07:48.880]   And there's obviously data for humanness on the internet.
[00:07:48.880 --> 00:07:53.040]   So I wonder if there's a future where that's part,
[00:07:53.040 --> 00:07:56.400]   I mean I sometimes wonder that about myself,
[00:07:56.400 --> 00:07:58.120]   I'm a huge fan of podcasts,
[00:07:58.120 --> 00:08:00.760]   and I listen to some podcasts,
[00:08:00.760 --> 00:08:03.240]   and I think like what is interesting about this,
[00:08:03.240 --> 00:08:04.280]   what is compelling?
[00:08:04.280 --> 00:08:07.440]   The same way you watch other games,
[00:08:07.440 --> 00:08:09.160]   like you said, watch, play StarCraft,
[00:08:09.160 --> 00:08:13.040]   or have Magnus Carlsen play chess.
[00:08:13.040 --> 00:08:14.920]   So I'm not a chess player,
[00:08:14.920 --> 00:08:16.120]   but it's still interesting to me,
[00:08:16.120 --> 00:08:16.960]   and what is that?
[00:08:16.960 --> 00:08:19.440]   That's the stakes of it,
[00:08:19.440 --> 00:08:23.400]   maybe the end of a domination of a series of wins.
[00:08:23.400 --> 00:08:25.440]   I don't know, there's all those elements
[00:08:25.440 --> 00:08:28.000]   somehow connect to a compelling conversation,
[00:08:28.000 --> 00:08:30.200]   and I wonder how hard is that to replace?
[00:08:30.200 --> 00:08:31.840]   'Cause ultimately all of that connects
[00:08:31.840 --> 00:08:34.600]   to the initial proposition of how to test
[00:08:34.600 --> 00:08:38.640]   whether an AI is intelligent or not with the Turing test.
[00:08:38.640 --> 00:08:41.760]   Which I guess, my question comes from a place
[00:08:41.760 --> 00:08:43.680]   of the spirit of that test.
[00:08:43.680 --> 00:08:45.440]   - Yes, I actually recall,
[00:08:45.440 --> 00:08:47.920]   I was just listening to our first podcast
[00:08:47.920 --> 00:08:50.360]   where we discussed Turing test.
[00:08:50.360 --> 00:08:54.760]   So I would say from a neural network,
[00:08:54.760 --> 00:08:57.120]   AI builder perspective,
[00:08:59.160 --> 00:09:03.160]   usually you try to map many of these interesting topics
[00:09:03.160 --> 00:09:05.200]   you discuss to benchmarks,
[00:09:05.200 --> 00:09:08.120]   and then also to actual architectures
[00:09:08.120 --> 00:09:10.640]   on how these systems are currently built,
[00:09:10.640 --> 00:09:13.080]   how they learn, what data they learn from,
[00:09:13.080 --> 00:09:14.280]   what are they learning, right?
[00:09:14.280 --> 00:09:17.800]   We're talking about weights of a mathematical function,
[00:09:17.800 --> 00:09:21.560]   and then looking at the current state of the game,
[00:09:21.560 --> 00:09:26.000]   maybe what do we need leaps forward
[00:09:26.000 --> 00:09:30.640]   to get to the ultimate stage of all these experiences,
[00:09:30.640 --> 00:09:32.840]   lifetime experience, fears,
[00:09:32.840 --> 00:09:37.840]   like words that currently barely we're seeing progress,
[00:09:37.840 --> 00:09:40.040]   just because what's happening today
[00:09:40.040 --> 00:09:43.960]   is you take all these human interactions,
[00:09:43.960 --> 00:09:47.920]   it's a large vast variety of human interactions online,
[00:09:47.920 --> 00:09:51.600]   and then you're distilling these sequences, right?
[00:09:51.600 --> 00:09:54.680]   Going back to my passion, like sequences of words,
[00:09:54.680 --> 00:09:56.920]   letters, images, sound,
[00:09:56.920 --> 00:09:59.840]   there's more modalities here to be at play.
[00:09:59.840 --> 00:10:03.360]   And then you're trying to just learn a function
[00:10:03.360 --> 00:10:04.400]   that will be happy,
[00:10:04.400 --> 00:10:08.840]   that maximizes the likelihood of seeing all these
[00:10:08.840 --> 00:10:10.880]   through a neural network.
[00:10:10.880 --> 00:10:14.200]   Now, I think there's a few places
[00:10:14.200 --> 00:10:17.240]   where the way currently we train these models
[00:10:17.240 --> 00:10:20.000]   would clearly like to be able to develop
[00:10:20.000 --> 00:10:22.120]   the kinds of capabilities you say.
[00:10:22.120 --> 00:10:23.520]   I'll tell you maybe a couple.
[00:10:23.520 --> 00:10:27.640]   One is the lifetime of an agent or a model.
[00:10:27.640 --> 00:10:30.840]   So you learn from these data offline, right?
[00:10:30.840 --> 00:10:33.560]   So you're just passively observing and maximizing these,
[00:10:33.560 --> 00:10:37.760]   you know, it's almost like a landscape of mountains.
[00:10:37.760 --> 00:10:39.120]   And then everywhere there's data
[00:10:39.120 --> 00:10:41.040]   that humans interacted in this way,
[00:10:41.040 --> 00:10:43.000]   you're trying to make that higher
[00:10:43.000 --> 00:10:45.720]   and then lower where there's no data.
[00:10:45.720 --> 00:10:48.480]   And then these models generally
[00:10:48.480 --> 00:10:51.160]   don't then experience themselves.
[00:10:51.160 --> 00:10:52.520]   They just are observers, right?
[00:10:52.520 --> 00:10:54.600]   They're passive observers of the data.
[00:10:54.600 --> 00:10:57.440]   And then we're putting them to then generate data
[00:10:57.440 --> 00:10:59.200]   when we interact with them.
[00:10:59.200 --> 00:11:00.920]   But that's very limiting.
[00:11:00.920 --> 00:11:03.480]   The experience they actually experience
[00:11:03.480 --> 00:11:05.680]   when they could maybe be optimizing
[00:11:05.680 --> 00:11:07.440]   or further optimizing the weights,
[00:11:07.440 --> 00:11:08.640]   we're not even doing that.
[00:11:08.640 --> 00:11:13.640]   So to be clear, and again, mapping to AlphaGo, AlphaStar,
[00:11:13.640 --> 00:11:15.280]   we train the model.
[00:11:15.280 --> 00:11:18.280]   And when we deploy it to play against humans,
[00:11:18.280 --> 00:11:20.320]   or in this case, interact with humans,
[00:11:20.320 --> 00:11:23.480]   like language models, they don't even keep training, right?
[00:11:23.480 --> 00:11:26.160]   They're not learning in the sense of the weights
[00:11:26.160 --> 00:11:28.200]   that you've learned from the data.
[00:11:28.200 --> 00:11:29.760]   They don't keep changing.
[00:11:29.760 --> 00:11:33.480]   Now there's something a bit more, feels magical,
[00:11:33.480 --> 00:11:36.200]   but it's understandable if you're into neural net,
[00:11:36.200 --> 00:11:39.120]   which is, well, they might not learn
[00:11:39.120 --> 00:11:41.480]   in the strict sense of the words, the weights changing.
[00:11:41.480 --> 00:11:44.360]   Maybe that's mapping to how neurons interconnect
[00:11:44.360 --> 00:11:46.640]   and how we learn over our lifetime.
[00:11:46.640 --> 00:11:50.280]   But it's true that the context of the conversation
[00:11:50.280 --> 00:11:54.960]   that takes place when you talk to these systems,
[00:11:54.960 --> 00:11:57.200]   it's held in their working memory, right?
[00:11:57.200 --> 00:12:00.120]   It's almost like you start a computer,
[00:12:00.120 --> 00:12:02.840]   it has a hard drive that has a lot of information.
[00:12:02.840 --> 00:12:04.000]   You have access to the internet,
[00:12:04.000 --> 00:12:06.320]   which has probably all the information,
[00:12:06.320 --> 00:12:08.440]   but there's also a working memory
[00:12:08.440 --> 00:12:11.080]   where these agents, as we call them,
[00:12:11.080 --> 00:12:13.840]   or start calling them, build upon.
[00:12:13.840 --> 00:12:16.560]   Now, this memory is very limited.
[00:12:16.560 --> 00:12:19.200]   I mean, right now we're talking, to be concrete,
[00:12:19.200 --> 00:12:21.760]   about 2000 words that we hold,
[00:12:21.760 --> 00:12:24.840]   and then beyond that, we start forgetting what we've seen.
[00:12:24.840 --> 00:12:28.040]   So you can see that there's some short-term coherence
[00:12:28.040 --> 00:12:29.880]   already, right, with when you said,
[00:12:29.880 --> 00:12:32.280]   I mean, it's a very interesting topic,
[00:12:32.280 --> 00:12:37.280]   having sort of a mapping, an agent to have consistency.
[00:12:37.280 --> 00:12:40.760]   Then if you say, "Oh, what's your name?"
[00:12:40.760 --> 00:12:42.240]   It could remember that,
[00:12:42.240 --> 00:12:44.960]   but then it might forget beyond 2000 words,
[00:12:44.960 --> 00:12:47.480]   which is not that long of context,
[00:12:47.480 --> 00:12:51.760]   if we think even of these podcast books are much longer.
[00:12:51.760 --> 00:12:55.120]   So technically speaking, there's a limitation there.
[00:12:55.120 --> 00:12:58.160]   Super exciting from people that work on deep learning
[00:12:58.160 --> 00:12:59.960]   to be working on,
[00:12:59.960 --> 00:13:03.040]   but I would say we lack maybe benchmarks
[00:13:03.040 --> 00:13:07.840]   and the technology to have this lifetime-like experience
[00:13:07.840 --> 00:13:10.840]   of memory that keeps building up.
[00:13:10.840 --> 00:13:13.160]   However, the way it learns offline
[00:13:13.160 --> 00:13:14.880]   is clearly very powerful, right?
[00:13:14.880 --> 00:13:17.400]   So you asked me three years ago,
[00:13:17.400 --> 00:13:18.640]   I would say, "Oh, we're very far."
[00:13:18.640 --> 00:13:22.240]   I think we've seen the power of this imitation,
[00:13:22.240 --> 00:13:26.240]   again, on the internet scale that has enabled this
[00:13:26.240 --> 00:13:28.760]   to feel like at least the knowledge,
[00:13:28.760 --> 00:13:30.160]   the basic knowledge about the world
[00:13:30.160 --> 00:13:33.120]   now is incorporated into the weights,
[00:13:33.120 --> 00:13:36.560]   but then this experience is lacking.
[00:13:36.560 --> 00:13:39.320]   And in fact, as I said, we don't even train them
[00:13:39.320 --> 00:13:41.160]   when we're talking to them,
[00:13:41.160 --> 00:13:44.760]   other than their working memory, of course, is affected.
[00:13:44.760 --> 00:13:46.560]   So that's the dynamic part,
[00:13:46.560 --> 00:13:48.240]   but they don't learn in the same way
[00:13:48.240 --> 00:13:49.720]   that you and I have learned, right?
[00:13:49.720 --> 00:13:54.040]   When, from basically when we were born and probably before.
[00:13:54.040 --> 00:13:56.480]   So lots of fascinating, interesting questions
[00:13:56.480 --> 00:13:57.400]   you asked there.
[00:13:57.400 --> 00:14:01.680]   I think the one I mentioned is this idea of memory
[00:14:01.680 --> 00:14:05.480]   and experience versus just kind of observe the world
[00:14:05.480 --> 00:14:06.720]   and learn its knowledge,
[00:14:06.720 --> 00:14:08.880]   which I think for that, I would argue,
[00:14:08.880 --> 00:14:10.320]   lots of recent advancements
[00:14:10.320 --> 00:14:13.400]   that make me very excited about the field.
[00:14:13.400 --> 00:14:18.160]   And then the second maybe issue that I see is
[00:14:18.160 --> 00:14:21.240]   all these models, we train them from scratch.
[00:14:21.240 --> 00:14:24.000]   That's something I would have complained three years ago
[00:14:24.000 --> 00:14:26.400]   or six years ago or 10 years ago.
[00:14:26.400 --> 00:14:31.360]   And it feels, if we take inspiration from how we got here,
[00:14:31.360 --> 00:14:35.240]   how the universe evolved us and we keep evolving,
[00:14:35.240 --> 00:14:37.840]   it feels that is a missing piece,
[00:14:37.840 --> 00:14:41.320]   that we should not be training models from scratch
[00:14:41.320 --> 00:14:45.280]   every few months, that there should be some sort of way
[00:14:45.280 --> 00:14:49.000]   in which we can grow models much like as a species
[00:14:49.000 --> 00:14:51.520]   and many other elements in the universe
[00:14:51.520 --> 00:14:55.000]   is building from the previous sort of iterations.
[00:14:55.000 --> 00:14:59.520]   And that from a just purely neural network perspective,
[00:14:59.520 --> 00:15:02.280]   even though we would like to make it work,
[00:15:02.280 --> 00:15:05.600]   it's proven very hard to not, you know,
[00:15:05.600 --> 00:15:07.680]   throw away the previous weights, right?
[00:15:07.680 --> 00:15:10.280]   This landscape we learn from the data and, you know,
[00:15:10.280 --> 00:15:13.360]   refresh it with a brand new set of weights,
[00:15:13.360 --> 00:15:16.960]   given maybe a recent snapshot of these datasets
[00:15:16.960 --> 00:15:19.960]   we train on, et cetera, or even a new game we're learning.
[00:15:19.960 --> 00:15:24.160]   So that feels like something is missing fundamentally.
[00:15:24.160 --> 00:15:27.440]   We might find it, but it's not very clear
[00:15:27.440 --> 00:15:28.400]   how it will look like.
[00:15:28.400 --> 00:15:30.800]   There's many ideas and it's super exciting as well.
[00:15:30.800 --> 00:15:32.440]   - Yes, just for people who don't know,
[00:15:32.440 --> 00:15:35.720]   when you're approaching new problem in machine learning,
[00:15:35.720 --> 00:15:38.200]   you're going to come up with an architecture
[00:15:38.200 --> 00:15:40.960]   that has a bunch of weights
[00:15:40.960 --> 00:15:43.360]   and then you initialize them somehow,
[00:15:43.360 --> 00:15:47.280]   which in most cases is some version of random.
[00:15:47.280 --> 00:15:48.960]   So that's what you mean by starting from scratch.
[00:15:48.960 --> 00:15:52.880]   And it seems like it's a waste every time you solve
[00:15:52.880 --> 00:15:59.440]   the game of Go and chess, StarCraft, protein folding,
[00:15:59.440 --> 00:16:03.160]   like surely there's some way to reuse the weights
[00:16:03.160 --> 00:16:08.160]   as we grow this giant database of neural networks.
[00:16:08.400 --> 00:16:10.000]   - That has solved some of the toughest problems
[00:16:10.000 --> 00:16:10.840]   in the world.
[00:16:10.840 --> 00:16:15.240]   And so some of that is, what is that?
[00:16:15.240 --> 00:16:19.080]   Methods, how to reuse weights,
[00:16:19.080 --> 00:16:22.480]   how to learn extract was generalizable,
[00:16:22.480 --> 00:16:25.160]   or at least has a chance to be
[00:16:25.160 --> 00:16:26.900]   and throw away the other stuff.
[00:16:26.900 --> 00:16:29.560]   And maybe the neural network itself
[00:16:29.560 --> 00:16:31.640]   should be able to tell you that.
[00:16:31.640 --> 00:16:34.640]   Like what, yeah, how do you,
[00:16:34.640 --> 00:16:37.520]   what ideas do you have for better initialization of weights?
[00:16:37.520 --> 00:16:38.720]   Maybe stepping back,
[00:16:38.720 --> 00:16:41.720]   if we look at the field of machine learning,
[00:16:41.720 --> 00:16:44.040]   but especially deep learning, right?
[00:16:44.040 --> 00:16:45.240]   At the core of deep learning,
[00:16:45.240 --> 00:16:49.240]   there's this beautiful idea that is a single algorithm
[00:16:49.240 --> 00:16:50.920]   can solve any task, right?
[00:16:50.920 --> 00:16:54.400]   So it's been proven over and over
[00:16:54.400 --> 00:16:56.440]   with more increasing set of benchmarks
[00:16:56.440 --> 00:16:58.580]   and things that were thought impossible
[00:16:58.580 --> 00:17:01.960]   that are being cracked by this basic principle.
[00:17:01.960 --> 00:17:05.800]   That is you take a neural network of uninitialized weights.
[00:17:05.800 --> 00:17:09.640]   So like a blank computational brain,
[00:17:09.640 --> 00:17:12.600]   then you give it in the case of supervised learning,
[00:17:12.600 --> 00:17:14.960]   a lot ideally of examples of,
[00:17:14.960 --> 00:17:17.120]   hey, here is what the input looks like
[00:17:17.120 --> 00:17:19.560]   and the desired output should look like this.
[00:17:19.560 --> 00:17:22.360]   I mean, image classification is very clear example,
[00:17:22.360 --> 00:17:25.560]   images to maybe one of a thousand categories.
[00:17:25.560 --> 00:17:26.840]   That's what ImageNet is like,
[00:17:26.840 --> 00:17:30.720]   but many, many, if not all problems can be mapped this way.
[00:17:30.720 --> 00:17:33.840]   And then there's a generic recipe, right?
[00:17:33.840 --> 00:17:35.240]   That you can use.
[00:17:35.240 --> 00:17:38.600]   And this recipe with very little change.
[00:17:38.600 --> 00:17:40.920]   And I think that's the core of deep learning research,
[00:17:40.920 --> 00:17:41.760]   right?
[00:17:41.760 --> 00:17:44.400]   That what is the recipe that is universal
[00:17:44.400 --> 00:17:46.400]   that for any new given task,
[00:17:46.400 --> 00:17:48.440]   I'll be able to use without thinking,
[00:17:48.440 --> 00:17:51.740]   without having to work very hard on the problem at stake.
[00:17:51.740 --> 00:17:54.400]   We have not found this recipe,
[00:17:54.400 --> 00:17:59.400]   but I think the field is excited to find less tweaks
[00:17:59.400 --> 00:18:02.000]   or tricks that people find
[00:18:02.000 --> 00:18:05.280]   when they work on important problems specific to those
[00:18:05.280 --> 00:18:07.540]   and more of a general algorithm, right?
[00:18:07.540 --> 00:18:09.300]   So at an algorithmic level,
[00:18:09.300 --> 00:18:11.760]   I would say we have something general already,
[00:18:11.760 --> 00:18:14.520]   which is this formula of training a very powerful model
[00:18:14.520 --> 00:18:17.000]   and neural network on a lot of data.
[00:18:17.000 --> 00:18:19.400]   And in many cases,
[00:18:19.400 --> 00:18:21.200]   you need some specificity
[00:18:21.200 --> 00:18:23.400]   to the actual problem you're solving.
[00:18:23.400 --> 00:18:26.060]   Protein folding being such an important problem
[00:18:26.060 --> 00:18:30.780]   has some basic recipe that is learned from before, right?
[00:18:30.780 --> 00:18:34.120]   Like transformer models, graph neural networks,
[00:18:34.120 --> 00:18:35.720]   ideas coming from NLP,
[00:18:35.720 --> 00:18:38.580]   like something called BERT,
[00:18:38.580 --> 00:18:41.280]   that is a kind of loss that you can emplace
[00:18:41.280 --> 00:18:42.420]   to help the model.
[00:18:42.420 --> 00:18:45.680]   Knowledge distillation is another technique, right?
[00:18:45.680 --> 00:18:47.080]   So this is the formula.
[00:18:47.080 --> 00:18:50.560]   We still had to find some particular things
[00:18:50.560 --> 00:18:53.600]   that were specific to alpha fold, right?
[00:18:53.600 --> 00:18:55.880]   That's very important because protein folding
[00:18:55.880 --> 00:18:59.120]   is such a high value problem that as humans,
[00:18:59.120 --> 00:19:02.860]   we should solve it no matter if we need to be a bit specific.
[00:19:02.860 --> 00:19:04.940]   And it's possible that some of these learnings
[00:19:04.940 --> 00:19:07.380]   will apply then to the next iteration of this recipe
[00:19:07.380 --> 00:19:09.340]   that deep learners are about.
[00:19:09.340 --> 00:19:11.820]   But it is true that so far,
[00:19:11.820 --> 00:19:13.180]   the recipe is what's common,
[00:19:13.180 --> 00:19:15.860]   but the weights you generally throw away,
[00:19:15.860 --> 00:19:17.780]   which feels very sad.
[00:19:17.780 --> 00:19:21.380]   Although maybe in the last,
[00:19:21.380 --> 00:19:23.360]   especially in the last two, three years,
[00:19:23.360 --> 00:19:24.620]   and when we last spoke,
[00:19:24.620 --> 00:19:26.600]   I mentioned these area of meta-learning,
[00:19:26.600 --> 00:19:28.580]   which is the idea of learning to learn.
[00:19:29.540 --> 00:19:33.100]   That idea and some progress has been had starting,
[00:19:33.100 --> 00:19:37.140]   I would say, mostly from GPT-3 on the language domain only,
[00:19:37.140 --> 00:19:42.060]   in which you could conceive a model that is trained once,
[00:19:42.060 --> 00:19:44.680]   and then this model is not narrow in that
[00:19:44.680 --> 00:19:47.640]   it only knows how to translate a pair of languages,
[00:19:47.640 --> 00:19:51.480]   or it only knows how to assign sentiment to a sentence.
[00:19:51.480 --> 00:19:54.100]   These actually, you could teach it
[00:19:54.100 --> 00:19:55.460]   by a prompting, it's called.
[00:19:55.460 --> 00:19:58.060]   And this prompting is essentially just showing it
[00:19:58.060 --> 00:19:59.860]   a few more examples,
[00:19:59.860 --> 00:20:02.980]   almost like you do show examples, input-output examples,
[00:20:02.980 --> 00:20:04.080]   algorithmically speaking,
[00:20:04.080 --> 00:20:06.280]   to the process of creating this model.
[00:20:06.280 --> 00:20:07.820]   But now you're doing it through language,
[00:20:07.820 --> 00:20:11.040]   which is very natural way for us to learn from one another.
[00:20:11.040 --> 00:20:13.080]   I tell you, "Hey, you should do this new task.
[00:20:13.080 --> 00:20:14.500]   "I'll tell you a bit more.
[00:20:14.500 --> 00:20:16.040]   "Maybe you ask me some questions."
[00:20:16.040 --> 00:20:17.800]   And now you know the task, right?
[00:20:17.800 --> 00:20:20.300]   You didn't need to retrain it from scratch.
[00:20:20.300 --> 00:20:23.180]   And we've seen these magical moments almost
[00:20:23.180 --> 00:20:26.940]   in this way to do few-shot prompting through language
[00:20:26.940 --> 00:20:28.520]   on language-only domain.
[00:20:28.520 --> 00:20:30.940]   And then in the last two years,
[00:20:30.940 --> 00:20:34.620]   we've seen these expanded to beyond language,
[00:20:34.620 --> 00:20:38.040]   adding vision, adding actions and games,
[00:20:38.040 --> 00:20:39.460]   lots of progress to be had.
[00:20:39.460 --> 00:20:42.120]   But this is maybe, if you ask me about
[00:20:42.120 --> 00:20:43.700]   how are we gonna crack this problem,
[00:20:43.700 --> 00:20:47.760]   this is perhaps one way in which you have a single model.
[00:20:47.760 --> 00:20:52.140]   The problem of this model is it's hard to grow
[00:20:52.140 --> 00:20:54.260]   in weights or capacity,
[00:20:54.260 --> 00:20:56.380]   but the model is certainly so powerful
[00:20:56.380 --> 00:20:58.920]   that you can teach it some tasks, right?
[00:20:58.920 --> 00:21:01.940]   In this way that I could teach you a new task now
[00:21:01.940 --> 00:21:05.060]   if we were all at a text-based task
[00:21:05.060 --> 00:21:08.400]   or a classification, a vision-style task.
[00:21:08.400 --> 00:21:12.820]   But it still feels like more breakthroughs should be had,
[00:21:12.820 --> 00:21:13.980]   but it's a great beginning, right?
[00:21:13.980 --> 00:21:15.400]   We have a good baseline.
[00:21:15.400 --> 00:21:17.740]   We have an idea that this maybe is the way
[00:21:17.740 --> 00:21:20.740]   we want to benchmark progress towards AGI.
[00:21:20.740 --> 00:21:22.820]   And I think in my view, that's critical
[00:21:22.820 --> 00:21:25.000]   to always have a way to benchmark
[00:21:25.000 --> 00:21:27.780]   the community converging to this overall,
[00:21:27.780 --> 00:21:29.200]   which is good to see.
[00:21:29.200 --> 00:21:33.500]   And then this is actually what excites me
[00:21:33.500 --> 00:21:36.580]   in terms of also next steps for deep learning
[00:21:36.580 --> 00:21:39.040]   is how to make these models more powerful.
[00:21:39.040 --> 00:21:40.460]   How do you train them?
[00:21:40.460 --> 00:21:43.080]   How to grow them if they must grow?
[00:21:43.080 --> 00:21:44.500]   Should they change their weights
[00:21:44.500 --> 00:21:46.060]   as you teach it the task or not?
[00:21:46.060 --> 00:21:48.520]   There's some interesting questions, many to be answered.
[00:21:48.520 --> 00:21:49.760]   - Yeah, you've opened the door
[00:21:49.760 --> 00:21:52.260]   to a bunch of questions I wanna ask,
[00:21:52.260 --> 00:21:55.660]   but let's first return to your tweet
[00:21:55.660 --> 00:21:57.120]   and read it like a Shakespeare.
[00:21:57.120 --> 00:22:01.220]   You wrote, "Gato is not the end, it's the beginning."
[00:22:01.220 --> 00:22:04.960]   And then you wrote, "Meow," and then an emoji of a cat.
[00:22:04.960 --> 00:22:07.700]   So first, two questions.
[00:22:07.700 --> 00:22:10.020]   First, can you explain the meow and the cat emoji?
[00:22:10.020 --> 00:22:13.620]   And second, can you explain what Gato is and how it works?
[00:22:13.620 --> 00:22:14.580]   - Right, indeed.
[00:22:14.580 --> 00:22:16.500]   I mean, thanks for reminding me
[00:22:16.500 --> 00:22:19.900]   that we're all exposing on Twitter and-
[00:22:19.900 --> 00:22:20.900]   - Permanently there.
[00:22:20.900 --> 00:22:21.900]   - Yes, permanently there.
[00:22:21.900 --> 00:22:25.100]   - One of the greatest AI researchers of all time,
[00:22:25.100 --> 00:22:27.220]   meow and cat emoji.
[00:22:27.220 --> 00:22:28.260]   - Yes. - There you go.
[00:22:28.260 --> 00:22:29.100]   - Right, so-
[00:22:29.100 --> 00:22:31.940]   - Can you imagine like touring, tweeting,
[00:22:31.940 --> 00:22:34.340]   meow and cat, probably he would, probably would.
[00:22:34.340 --> 00:22:35.180]   - Probably.
[00:22:35.180 --> 00:22:38.020]   So yeah, the tweet is important, actually.
[00:22:38.020 --> 00:22:39.800]   You know, I put thought on the tweets.
[00:22:39.800 --> 00:22:40.780]   I hope people-
[00:22:40.780 --> 00:22:43.060]   - Which part did you think, okay.
[00:22:43.060 --> 00:22:44.900]   So there's three sentences.
[00:22:44.900 --> 00:22:48.700]   Gato's not the end, Gato's the beginning.
[00:22:48.700 --> 00:22:50.140]   Meow, cat emoji.
[00:22:50.140 --> 00:22:51.740]   Okay, which is the important part?
[00:22:51.740 --> 00:22:53.140]   - It's the meow, no, no.
[00:22:53.140 --> 00:22:56.060]   Definitely that it is the beginning.
[00:22:56.060 --> 00:23:00.340]   I mean, I probably was just explaining a bit
[00:23:00.340 --> 00:23:03.740]   where the field is going, but let me tell you about Gato.
[00:23:03.740 --> 00:23:08.100]   So first, the name Gato comes from maybe a sequence
[00:23:08.100 --> 00:23:11.820]   of releases that DeepMind had that named,
[00:23:11.820 --> 00:23:15.100]   like used animal names to name some of their models
[00:23:15.100 --> 00:23:19.100]   that are based on this idea of large sequence models.
[00:23:19.100 --> 00:23:20.620]   Initially, they're only language,
[00:23:20.620 --> 00:23:23.180]   but we are expanding to other modalities.
[00:23:23.180 --> 00:23:28.180]   So we had, you know, we had gopher, chinchilla,
[00:23:28.180 --> 00:23:29.940]   these were language only.
[00:23:29.940 --> 00:23:32.700]   And then more recently we released flamingo,
[00:23:32.700 --> 00:23:35.420]   which adds vision to the equation.
[00:23:35.420 --> 00:23:38.140]   And then Gato, which adds vision
[00:23:38.140 --> 00:23:41.620]   and then also actions in the mix, right?
[00:23:41.620 --> 00:23:44.500]   As we discuss actually actions,
[00:23:44.500 --> 00:23:47.540]   especially discrete actions like up, down, left, right.
[00:23:47.540 --> 00:23:49.460]   I just told you the actions, but they're words.
[00:23:49.460 --> 00:23:52.740]   So you can kind of see how actions naturally map
[00:23:52.740 --> 00:23:54.500]   to sequence modeling of words,
[00:23:54.500 --> 00:23:57.020]   which these models are very powerful.
[00:23:57.020 --> 00:24:01.660]   So Gato was named after, I believe,
[00:24:01.660 --> 00:24:03.580]   I can only from memory, right?
[00:24:03.580 --> 00:24:06.020]   These, you know, these things always happen
[00:24:06.020 --> 00:24:08.500]   with an amazing team of researchers behind.
[00:24:08.500 --> 00:24:12.180]   So before the release, we had the discussion
[00:24:12.180 --> 00:24:14.220]   about which animal would we pick, right?
[00:24:14.220 --> 00:24:18.340]   And I think because of the word general agent, right?
[00:24:18.340 --> 00:24:21.860]   And this is a property quite unique to Gato.
[00:24:21.860 --> 00:24:24.700]   We kind of were playing with the GA words
[00:24:24.700 --> 00:24:25.980]   and then, you know, Gato is-
[00:24:25.980 --> 00:24:26.900]   - Rhymes with cat.
[00:24:26.900 --> 00:24:27.740]   - Yes.
[00:24:27.740 --> 00:24:30.220]   And Gato is obviously a Spanish version of cat.
[00:24:30.220 --> 00:24:32.220]   I had nothing to do with it, although I'm from Spain.
[00:24:32.220 --> 00:24:33.260]   - Oh, how do you, wait, sorry.
[00:24:33.260 --> 00:24:34.620]   How do you say cat in Spanish?
[00:24:34.620 --> 00:24:35.460]   - Gato.
[00:24:35.460 --> 00:24:36.300]   - Oh, Gato.
[00:24:36.300 --> 00:24:37.140]   - Yeah.
[00:24:37.140 --> 00:24:37.980]   - Now it all makes sense. - Okay, okay, I see, I see.
[00:24:37.980 --> 00:24:39.060]   - Now it all makes sense.
[00:24:39.060 --> 00:24:39.900]   - Okay, so-
[00:24:39.900 --> 00:24:40.780]   - How do you say meow in Spanish?
[00:24:40.780 --> 00:24:41.900]   No, that's probably the same.
[00:24:41.900 --> 00:24:44.380]   - I think you say it the same way,
[00:24:44.380 --> 00:24:48.060]   but you write it as M-I-A-U.
[00:24:48.060 --> 00:24:49.220]   - Okay, it's universal.
[00:24:49.220 --> 00:24:50.060]   - Yeah.
[00:24:50.060 --> 00:24:51.660]   - All right, so then how does the thing work?
[00:24:51.660 --> 00:24:56.660]   So you said general is, so you said language, vision-
[00:24:56.660 --> 00:24:58.380]   - And action.
[00:24:58.380 --> 00:24:59.220]   - Action.
[00:24:59.220 --> 00:25:01.820]   How does this, can you explain
[00:25:01.820 --> 00:25:04.220]   what kind of neural networks are involved?
[00:25:04.220 --> 00:25:06.340]   What does the training look like?
[00:25:06.340 --> 00:25:10.900]   And maybe what to you are some beautiful ideas
[00:25:10.900 --> 00:25:11.860]   within this system?
[00:25:11.860 --> 00:25:16.060]   - Yeah, so maybe the basics of Gato
[00:25:16.060 --> 00:25:19.940]   are not that dissimilar from many, many work that comes.
[00:25:19.940 --> 00:25:22.900]   So here is where the sort of the recipe,
[00:25:22.900 --> 00:25:24.220]   I mean, hasn't changed too much.
[00:25:24.220 --> 00:25:25.580]   There is a transformer model
[00:25:25.580 --> 00:25:28.620]   that's the kind of recurrent neural network
[00:25:28.620 --> 00:25:33.300]   that essentially takes a sequence of modalities,
[00:25:33.300 --> 00:25:36.380]   observations that could be words,
[00:25:36.380 --> 00:25:38.820]   could be vision, or could be actions.
[00:25:38.820 --> 00:25:42.140]   And then its own objective that you train it to do
[00:25:42.140 --> 00:25:44.060]   when you train it is to predict
[00:25:44.060 --> 00:25:46.380]   what the next anything is.
[00:25:46.380 --> 00:25:48.780]   And anything means what's the next action.
[00:25:48.780 --> 00:25:51.220]   If this sequence that I'm showing you to train
[00:25:51.220 --> 00:25:53.500]   is a sequence of actions and observations,
[00:25:53.500 --> 00:25:55.620]   then you're predicting what's the next action
[00:25:55.620 --> 00:25:57.100]   and the next observation, right?
[00:25:57.100 --> 00:26:00.900]   So you think of these really as a sequence of bytes, right?
[00:26:00.900 --> 00:26:04.220]   So take any sequence of words,
[00:26:04.220 --> 00:26:06.980]   a sequence of interleaved words and images,
[00:26:06.980 --> 00:26:11.260]   a sequence of maybe observations that are images
[00:26:11.260 --> 00:26:14.260]   and moves in a tarry up, down, left, right.
[00:26:14.260 --> 00:26:17.620]   And these, you just think of them as bytes
[00:26:17.620 --> 00:26:20.540]   and you're modeling what's the next byte gonna be like.
[00:26:20.540 --> 00:26:23.380]   And you might interpret that as an action
[00:26:23.380 --> 00:26:25.820]   and then play it in a game,
[00:26:25.820 --> 00:26:27.660]   or you could interpret it as a word
[00:26:27.660 --> 00:26:29.060]   and then write it down
[00:26:29.060 --> 00:26:31.340]   if you're chatting with the system and so on.
[00:26:31.340 --> 00:26:36.580]   So Gato basically can be thought as inputs,
[00:26:36.620 --> 00:26:41.500]   images, text, video, actions.
[00:26:41.500 --> 00:26:45.780]   It also actually inputs some sort of proprioception sensors
[00:26:45.780 --> 00:26:48.260]   from robotics because robotics is one of the tasks
[00:26:48.260 --> 00:26:49.860]   that it's been trained to do.
[00:26:49.860 --> 00:26:51.900]   And then at the output, similarly,
[00:26:51.900 --> 00:26:53.700]   it outputs words, actions.
[00:26:53.700 --> 00:26:55.700]   It does not output images.
[00:26:55.700 --> 00:26:57.420]   That's just by design,
[00:26:57.420 --> 00:26:59.900]   we decided not to go that way for now.
[00:26:59.900 --> 00:27:02.740]   That's also in part why it's the beginning
[00:27:02.740 --> 00:27:04.900]   because there's more to do clearly.
[00:27:04.900 --> 00:27:06.420]   But that's kind of what Gato is.
[00:27:06.420 --> 00:27:09.220]   It's this brain that essentially you give it any sequence
[00:27:09.220 --> 00:27:11.940]   of these observations and modalities
[00:27:11.940 --> 00:27:13.780]   and it outputs the next step.
[00:27:13.780 --> 00:27:15.340]   And then off you go,
[00:27:15.340 --> 00:27:17.380]   you feed the next step into
[00:27:17.380 --> 00:27:20.060]   and predict the next one and so on.
[00:27:20.060 --> 00:27:24.140]   Now, it is more than a language model
[00:27:24.140 --> 00:27:26.780]   because even though you can chat with Gato,
[00:27:26.780 --> 00:27:29.540]   like you can chat with Chinchilla or Flamingo,
[00:27:29.540 --> 00:27:33.220]   it also is an agent, right?
[00:27:33.220 --> 00:27:37.220]   So that's why we call it A of Gato,
[00:27:37.220 --> 00:27:41.380]   like the letter A and also it's general.
[00:27:41.380 --> 00:27:43.260]   It's not an agent that's been trained
[00:27:43.260 --> 00:27:47.900]   to be good at only StarCraft or only Atari or only Go.
[00:27:47.900 --> 00:27:51.660]   It's been trained on a vast variety of datasets.
[00:27:51.660 --> 00:27:53.860]   - What makes it an agent, if I may interrupt?
[00:27:53.860 --> 00:27:56.020]   The fact that it can generate actions?
[00:27:56.020 --> 00:27:58.180]   - Yes, so when we call it,
[00:27:58.180 --> 00:28:00.100]   I mean, it's a good question, right?
[00:28:00.100 --> 00:28:02.780]   When do we call a model?
[00:28:02.780 --> 00:28:03.860]   I mean, everything is a model,
[00:28:03.860 --> 00:28:06.740]   but what is an agent in my view is indeed
[00:28:06.740 --> 00:28:09.700]   the capacity to take actions in an environment
[00:28:09.700 --> 00:28:11.660]   that you then send to it
[00:28:11.660 --> 00:28:13.500]   and then the environment might return
[00:28:13.500 --> 00:28:15.040]   with a new observation
[00:28:15.040 --> 00:28:17.660]   and then you generate the next action and so on.
[00:28:17.660 --> 00:28:20.420]   - This actually, this reminds me of the question
[00:28:20.420 --> 00:28:23.000]   from the side of biology, what is life?
[00:28:23.000 --> 00:28:25.380]   Which is actually a very difficult question as well.
[00:28:25.380 --> 00:28:26.780]   What is living?
[00:28:26.780 --> 00:28:29.460]   What is living when you think about life here
[00:28:29.460 --> 00:28:31.000]   on this planet Earth?
[00:28:31.000 --> 00:28:33.420]   And a question interesting to me about aliens,
[00:28:33.420 --> 00:28:35.720]   what is life when we visit another planet?
[00:28:35.720 --> 00:28:37.220]   Would we be able to recognize it?
[00:28:37.220 --> 00:28:40.220]   And this feels like, it sounds perhaps silly,
[00:28:40.220 --> 00:28:41.380]   but I don't think it is.
[00:28:41.380 --> 00:28:46.380]   At which point is the neural network a being versus a tool?
[00:28:46.380 --> 00:28:52.400]   And it feels like action, ability to modify its environment,
[00:28:52.400 --> 00:28:54.540]   is that fundamental leap.
[00:28:54.540 --> 00:28:57.420]   - Yeah, I think it certainly feels like action
[00:28:57.420 --> 00:29:01.920]   is a necessary condition to be more alive,
[00:29:01.920 --> 00:29:04.380]   but probably not sufficient either.
[00:29:04.380 --> 00:29:05.220]   So sadly--
[00:29:05.220 --> 00:29:06.880]   - It's a soul consciousness thing, whatever.
[00:29:06.880 --> 00:29:09.060]   - Yeah, yeah, we can get back to that later.
[00:29:09.060 --> 00:29:12.300]   But anyways, going back to the meow and the Gato, right?
[00:29:12.300 --> 00:29:16.100]   So one of the leaps forward
[00:29:16.100 --> 00:29:19.100]   and what took the team a lot of effort and time was,
[00:29:19.100 --> 00:29:23.100]   as you were asking, how has Gato been trained?
[00:29:23.100 --> 00:29:26.060]   So I told you Gato is this transformer neural network,
[00:29:26.060 --> 00:29:30.580]   models actions, sequences of actions, words, et cetera.
[00:29:30.580 --> 00:29:34.820]   And then the way we train it is by essentially
[00:29:34.820 --> 00:29:39.380]   pulling data sets of observations, right?
[00:29:39.380 --> 00:29:42.620]   So it's a massive imitation learning algorithm
[00:29:42.620 --> 00:29:46.300]   that it imitates obviously to what is the next word
[00:29:46.300 --> 00:29:49.860]   that comes next from the usual data sets we use before,
[00:29:49.860 --> 00:29:50.700]   right?
[00:29:50.700 --> 00:29:52.980]   So these are these web scale style data sets
[00:29:52.980 --> 00:29:57.980]   of people writing on webs or chatting or whatnot, right?
[00:29:57.980 --> 00:30:00.480]   So that's an obvious source that we use
[00:30:00.480 --> 00:30:02.020]   on all language work.
[00:30:02.020 --> 00:30:05.620]   But then we also took a lot of agents
[00:30:05.620 --> 00:30:06.700]   that we have at DeepMind.
[00:30:06.700 --> 00:30:08.160]   I mean, as you know, DeepMind,
[00:30:08.160 --> 00:30:13.580]   we're quite interested in learning reinforcement learning
[00:30:13.580 --> 00:30:16.940]   and learning agents that play in different environments.
[00:30:16.940 --> 00:30:20.740]   So we kind of created a data set of these trajectories
[00:30:20.740 --> 00:30:23.020]   as we call them or agent experiences.
[00:30:23.020 --> 00:30:25.660]   So in a way, there are other agents we train
[00:30:25.660 --> 00:30:28.420]   for a single mind purpose to, let's say,
[00:30:28.420 --> 00:30:33.340]   control a 3D game environment and navigate a maze.
[00:30:33.340 --> 00:30:36.060]   So we had all the experience that was created
[00:30:36.060 --> 00:30:39.560]   through the one agent interacting with that environment.
[00:30:39.560 --> 00:30:41.860]   And we added these to the data sets, right?
[00:30:41.860 --> 00:30:44.380]   And as I said, we just see all the data,
[00:30:44.380 --> 00:30:47.500]   all these sequences of words or sequences of these agent
[00:30:47.500 --> 00:30:49.700]   interacting with that environment
[00:30:49.700 --> 00:30:52.180]   or agents playing Atari and so on.
[00:30:52.180 --> 00:30:54.860]   We see this as the same kind of data.
[00:30:54.860 --> 00:30:59.220]   And so we mix these data sets together and we train Gato.
[00:30:59.220 --> 00:31:01.580]   That's the G part, right?
[00:31:01.580 --> 00:31:05.220]   It's general because it really has mixed,
[00:31:05.220 --> 00:31:07.520]   it doesn't have different brains for each modality
[00:31:07.520 --> 00:31:09.060]   or each narrow task.
[00:31:09.060 --> 00:31:10.500]   It has a single brain.
[00:31:10.500 --> 00:31:12.700]   It's not that big of a brain compared to most
[00:31:12.700 --> 00:31:14.780]   of the neural networks we see these days.
[00:31:14.780 --> 00:31:17.140]   It has 1 billion parameters.
[00:31:17.140 --> 00:31:21.100]   Some models we're seeing getting the trillions these days
[00:31:21.100 --> 00:31:25.060]   and certainly 100 billion feels like a size
[00:31:25.060 --> 00:31:28.980]   that is very common from when you train these jobs.
[00:31:28.980 --> 00:31:32.660]   So the actual agent is relatively small,
[00:31:32.660 --> 00:31:35.020]   but it's been trained on a very challenging,
[00:31:35.020 --> 00:31:37.980]   diverse data set, not only containing all of internet,
[00:31:37.980 --> 00:31:40.380]   but containing all these agent experience
[00:31:40.380 --> 00:31:43.140]   playing very different distinct environments.
[00:31:43.140 --> 00:31:46.420]   So this brings us to the part of the tweet of,
[00:31:46.420 --> 00:31:48.900]   this is not the end, it's the beginning.
[00:31:48.900 --> 00:31:53.100]   It feels very cool to see Gato in principle
[00:31:53.100 --> 00:31:56.620]   is able to control any sort of environments
[00:31:56.620 --> 00:31:59.140]   that especially the ones that it's been trained to do,
[00:31:59.140 --> 00:32:01.100]   these 3D games, Atari games,
[00:32:01.100 --> 00:32:04.620]   all sorts of robotics tasks and so on.
[00:32:04.620 --> 00:32:08.960]   But obviously it's not as proficient as the teachers
[00:32:08.960 --> 00:32:09.800]   it learned from on these environments.
[00:32:09.800 --> 00:32:11.740]   - Is that why it's not obvious?
[00:32:11.740 --> 00:32:15.100]   It's not obvious that it wouldn't be more proficient.
[00:32:15.100 --> 00:32:18.040]   It's just the current beginning part
[00:32:18.040 --> 00:32:21.780]   is that the performance is such that it's not as good
[00:32:21.780 --> 00:32:23.460]   as if it's specialized to that task.
[00:32:23.460 --> 00:32:25.820]   - Right, so it's not as good,
[00:32:25.820 --> 00:32:28.060]   although I would argue size matters here.
[00:32:28.060 --> 00:32:29.180]   So the fact that--
[00:32:29.180 --> 00:32:31.220]   - I would argue size always matters.
[00:32:31.220 --> 00:32:33.420]   - Yeah, okay. - That's a different conversation.
[00:32:33.420 --> 00:32:36.260]   - But for neural networks, certainly size does matter.
[00:32:36.260 --> 00:32:39.660]   So it's the beginning because it's relatively small.
[00:32:39.660 --> 00:32:42.620]   So obviously scaling this idea up
[00:32:42.620 --> 00:32:46.540]   might make the connections that exist
[00:32:46.540 --> 00:32:50.740]   between text on the internet and playing Atari and so on
[00:32:50.740 --> 00:32:53.340]   more synergistic with one another.
[00:32:53.340 --> 00:32:54.260]   And you might gain.
[00:32:54.260 --> 00:32:56.360]   And that moment we didn't quite see,
[00:32:56.360 --> 00:32:58.660]   but obviously that's why it's the beginning.
[00:32:58.660 --> 00:33:00.980]   - That synergy might emerge with scale.
[00:33:00.980 --> 00:33:02.140]   - Right, might emerge with scale.
[00:33:02.140 --> 00:33:04.420]   And also I believe there's some new research
[00:33:04.420 --> 00:33:07.620]   or ways in which you prepare the data
[00:33:07.620 --> 00:33:10.940]   that you might need to sort of make it more clear
[00:33:10.940 --> 00:33:14.180]   to the model that you're not only playing Atari
[00:33:14.180 --> 00:33:16.360]   and it's just, you start from a screen
[00:33:16.360 --> 00:33:18.400]   and here is up and a screen and down.
[00:33:18.400 --> 00:33:20.660]   Maybe you can think of playing Atari
[00:33:20.660 --> 00:33:23.900]   as there's some sort of context that is needed for the agent
[00:33:23.900 --> 00:33:26.900]   before it starts seeing, oh, this is an Atari screen,
[00:33:26.900 --> 00:33:28.640]   I'm gonna start playing.
[00:33:28.640 --> 00:33:33.420]   You might require, for instance, to be told in words,
[00:33:33.420 --> 00:33:36.860]   hey, in this sequence that I'm showing,
[00:33:36.860 --> 00:33:39.100]   you're gonna be playing an Atari game.
[00:33:39.100 --> 00:33:41.980]   So text might actually be a good driver
[00:33:41.980 --> 00:33:44.460]   to enhance the data.
[00:33:44.460 --> 00:33:47.220]   So then these connections might be made more easily.
[00:33:47.220 --> 00:33:51.240]   That's an idea that we start seeing in language,
[00:33:51.240 --> 00:33:55.180]   but obviously beyond this is gonna be effective.
[00:33:55.180 --> 00:33:57.460]   It's not like I don't show you a screen
[00:33:57.460 --> 00:34:01.000]   and you from scratch, you're supposed to learn a game.
[00:34:01.000 --> 00:34:03.380]   There is a lot of context we might set.
[00:34:03.380 --> 00:34:05.860]   So there might be some work needed as well
[00:34:05.860 --> 00:34:07.780]   to set that context.
[00:34:07.780 --> 00:34:10.420]   But anyways, there's a lot of work.
[00:34:10.420 --> 00:34:13.540]   - So that context puts all the different modalities
[00:34:13.540 --> 00:34:14.980]   on the same level ground.
[00:34:14.980 --> 00:34:15.820]   - Exactly. - If you provide
[00:34:15.820 --> 00:34:16.660]   the context best.
[00:34:16.660 --> 00:34:18.980]   So maybe on that point,
[00:34:18.980 --> 00:34:23.100]   so there's this task which may not seem trivial
[00:34:23.100 --> 00:34:28.100]   of tokenizing the data, of converting the data into pieces,
[00:34:28.100 --> 00:34:31.300]   into basic atomic elements
[00:34:31.300 --> 00:34:35.300]   that then could cross modalities somehow.
[00:34:35.300 --> 00:34:37.900]   So what's tokenization?
[00:34:37.900 --> 00:34:39.680]   How do you tokenize text?
[00:34:39.680 --> 00:34:42.180]   How do you tokenize images?
[00:34:42.180 --> 00:34:47.060]   How do you tokenize games and actions and robotics tasks?
[00:34:47.060 --> 00:34:48.220]   - Yeah, that's a great question.
[00:34:48.220 --> 00:34:52.820]   So tokenization is the entry point
[00:34:52.820 --> 00:34:55.580]   to actually make all the data look like a sequence
[00:34:55.580 --> 00:34:57.660]   because tokens then are just kind of
[00:34:57.660 --> 00:34:59.500]   these little puzzle pieces.
[00:34:59.500 --> 00:35:01.740]   We break down anything into these puzzle pieces
[00:35:01.740 --> 00:35:03.460]   and then we just model,
[00:35:03.460 --> 00:35:05.340]   what's this puzzle look like, right?
[00:35:05.340 --> 00:35:07.700]   When you make it lay down in a line,
[00:35:07.700 --> 00:35:09.500]   so to speak, in a sequence.
[00:35:09.500 --> 00:35:14.500]   So in Gato, the text, there's a lot of work.
[00:35:14.500 --> 00:35:17.340]   You tokenize text usually by looking
[00:35:17.340 --> 00:35:20.020]   at commonly used substrings, right?
[00:35:20.020 --> 00:35:22.500]   So there's, you know, ing in English
[00:35:22.500 --> 00:35:23.660]   is a very common substring.
[00:35:23.660 --> 00:35:25.500]   So that becomes a token.
[00:35:25.500 --> 00:35:29.060]   There's quite well studied problem on tokenizing text
[00:35:29.060 --> 00:35:31.580]   and Gato just use the standard techniques
[00:35:31.580 --> 00:35:34.300]   that have been developed from many years,
[00:35:34.300 --> 00:35:37.940]   even starting from Ngram models in the 1950s and so on.
[00:35:37.940 --> 00:35:40.180]   - Just for context, how many tokens,
[00:35:40.180 --> 00:35:41.780]   like what order of magnitude,
[00:35:41.780 --> 00:35:44.460]   number of tokens is required for a word?
[00:35:44.460 --> 00:35:45.300]   - Yeah. - Usually.
[00:35:45.300 --> 00:35:46.180]   What are we talking about?
[00:35:46.180 --> 00:35:48.620]   - Yeah, for a word in English, right?
[00:35:48.620 --> 00:35:51.100]   I mean, every language is very different.
[00:35:51.100 --> 00:35:53.900]   The current level or granularity of tokenization
[00:35:53.900 --> 00:35:57.780]   generally means it's maybe two to five.
[00:35:57.780 --> 00:36:00.140]   I mean, I don't know the statistics exactly,
[00:36:00.140 --> 00:36:02.100]   but to give you an idea,
[00:36:02.100 --> 00:36:04.100]   we don't tokenize at the level of letters,
[00:36:04.100 --> 00:36:05.460]   then it would probably be like,
[00:36:05.460 --> 00:36:07.500]   I don't know what the average length of a word
[00:36:07.500 --> 00:36:09.220]   is in English, but that would be, you know,
[00:36:09.220 --> 00:36:11.380]   the minimum set of tokens you could use.
[00:36:11.380 --> 00:36:13.180]   - So it's bigger than letters, smaller than words.
[00:36:13.180 --> 00:36:14.020]   - Yes, yes.
[00:36:14.020 --> 00:36:16.860]   And you could think of very, very common words like the,
[00:36:16.860 --> 00:36:18.780]   I mean, that would be a single token,
[00:36:18.780 --> 00:36:21.500]   but very quickly you're talking two, three, four,
[00:36:21.500 --> 00:36:22.340]   four tokens or so.
[00:36:22.340 --> 00:36:24.740]   - Have you ever tried to tokenize emojis?
[00:36:24.740 --> 00:36:29.420]   - Emojis are actually just sequences of letters.
[00:36:29.420 --> 00:36:30.940]   So- - Maybe to you,
[00:36:30.940 --> 00:36:32.980]   but to me, they mean so much more.
[00:36:32.980 --> 00:36:34.380]   - Yeah, you can render the emoji,
[00:36:34.380 --> 00:36:36.780]   but you might, if you actually just-
[00:36:36.780 --> 00:36:38.940]   - Yeah, this is a philosophical question.
[00:36:38.940 --> 00:36:43.300]   Is emojis an image or a text?
[00:36:43.300 --> 00:36:46.900]   - The way we do these things is they're actually mapped
[00:36:46.900 --> 00:36:49.540]   to small sequences of characters.
[00:36:49.540 --> 00:36:52.580]   So you can actually play with these models
[00:36:52.580 --> 00:36:55.780]   and input emojis, it will output emojis back,
[00:36:55.780 --> 00:36:57.900]   which is actually quite a fun exercise.
[00:36:57.900 --> 00:37:02.300]   You probably can find other tweets about these out there.
[00:37:02.300 --> 00:37:04.460]   But yeah, so anyways, text, there's like,
[00:37:04.460 --> 00:37:06.780]   it's very clear how this is done.
[00:37:06.780 --> 00:37:10.620]   And then in Gato, what we did for images
[00:37:10.620 --> 00:37:13.780]   is we map images to essentially,
[00:37:13.780 --> 00:37:15.460]   we compressed images, so to speak,
[00:37:15.460 --> 00:37:17.460]   into something that looks more like,
[00:37:17.460 --> 00:37:21.300]   less like every pixel with every intensity
[00:37:21.300 --> 00:37:23.820]   that would mean we have a very long sequence, right?
[00:37:23.820 --> 00:37:27.300]   Like if we were talking about 100 by 100 pixel images,
[00:37:27.300 --> 00:37:29.940]   that would make the sequences far too long.
[00:37:29.940 --> 00:37:33.340]   So what was done there is you just use a technique
[00:37:33.340 --> 00:37:35.860]   that essentially compresses an image
[00:37:35.860 --> 00:37:40.180]   into maybe 16 by 16 patches of pixels.
[00:37:40.180 --> 00:37:42.740]   And then that is mapped, again, tokenized.
[00:37:42.740 --> 00:37:45.380]   You just essentially quantize this space
[00:37:45.380 --> 00:37:49.020]   into a special word that actually maps
[00:37:49.020 --> 00:37:51.820]   to these little sequence of pixels.
[00:37:51.820 --> 00:37:55.140]   And then you put the pixels together in some raster order,
[00:37:55.140 --> 00:37:57.820]   and then that's how you get out
[00:37:57.820 --> 00:38:00.820]   or in the image that you're processing.
[00:38:00.820 --> 00:38:04.060]   - But there's no semantic aspect to that.
[00:38:04.060 --> 00:38:05.860]   So you're doing some kind of,
[00:38:05.860 --> 00:38:07.780]   you don't need to understand anything about the image
[00:38:07.780 --> 00:38:09.660]   in order to tokenize it currently.
[00:38:09.660 --> 00:38:12.620]   - No, you're only using this notion of compression.
[00:38:12.620 --> 00:38:15.100]   So you're trying to find common,
[00:38:15.100 --> 00:38:17.660]   it's like JPG or all these algorithms,
[00:38:17.660 --> 00:38:20.540]   it's actually very similar at the tokenization level.
[00:38:20.540 --> 00:38:23.340]   All we're doing is finding common patterns
[00:38:23.340 --> 00:38:25.860]   and then making sure in a lossy way,
[00:38:25.860 --> 00:38:27.260]   we compress these images,
[00:38:27.260 --> 00:38:29.540]   given the statistics of the images
[00:38:29.540 --> 00:38:31.860]   that are contained in all the data we deal with.
[00:38:31.860 --> 00:38:34.220]   - Although you could probably argue that JPG
[00:38:34.220 --> 00:38:36.660]   does have some understanding of images.
[00:38:36.660 --> 00:38:42.940]   Because visual information, maybe color,
[00:38:42.940 --> 00:38:46.980]   compressing crudely based on color
[00:38:46.980 --> 00:38:51.180]   does capture something important about an image
[00:38:51.180 --> 00:38:54.620]   that's about its meaning, not just about some statistics.
[00:38:54.620 --> 00:38:56.660]   - Yeah, I mean, JP, as I said,
[00:38:56.660 --> 00:38:59.420]   the algorithms look actually very similar to,
[00:38:59.420 --> 00:39:02.820]   they use the cosine transform in JPG.
[00:39:02.820 --> 00:39:07.100]   The approach we usually do in machine learning
[00:39:07.100 --> 00:39:10.140]   when we deal with images and we do this quantization step
[00:39:10.140 --> 00:39:11.380]   is a bit more data-driven.
[00:39:11.380 --> 00:39:14.140]   So rather than have some sort of Fourier basis
[00:39:14.140 --> 00:39:18.900]   for how frequencies appear in the natural world,
[00:39:18.900 --> 00:39:23.820]   we actually just use the statistics of the images
[00:39:23.820 --> 00:39:26.980]   and then quantize them based on the statistics,
[00:39:26.980 --> 00:39:28.300]   much like you do in words, right?
[00:39:28.300 --> 00:39:32.420]   So common substrings are allocated a token,
[00:39:32.420 --> 00:39:34.420]   and images is very similar.
[00:39:34.420 --> 00:39:38.260]   But there's no connection, the token space,
[00:39:38.260 --> 00:39:41.060]   if you think of, oh, like the tokens are an integer
[00:39:41.060 --> 00:39:42.420]   and in the end of the day.
[00:39:42.420 --> 00:39:46.180]   So now like we work on, maybe we have about,
[00:39:46.180 --> 00:39:47.980]   let's say, I don't know the exact numbers,
[00:39:47.980 --> 00:39:51.180]   but let's say 10,000 tokens for text, right?
[00:39:51.180 --> 00:39:52.820]   Certainly more than characters
[00:39:52.820 --> 00:39:55.340]   because we have groups of characters and so on.
[00:39:55.340 --> 00:39:58.300]   So from one to 10,000, those are representing
[00:39:58.300 --> 00:40:00.980]   all the language and the words we'll see.
[00:40:00.980 --> 00:40:04.180]   And then images occupy the next set of integers.
[00:40:04.180 --> 00:40:05.820]   So they're completely independent, right?
[00:40:05.820 --> 00:40:09.860]   So from 10,001 to 20,000, those are the tokens
[00:40:09.860 --> 00:40:12.780]   that represent these other modality images.
[00:40:12.780 --> 00:40:16.940]   And that is an interesting aspect
[00:40:16.940 --> 00:40:18.660]   that makes it orthogonal.
[00:40:18.660 --> 00:40:21.620]   So what connects these concepts is the data, right?
[00:40:21.620 --> 00:40:24.460]   Once you have a data set, for instance,
[00:40:24.460 --> 00:40:26.900]   that captions images, that tells you,
[00:40:26.900 --> 00:40:30.500]   oh, this is someone playing a Frisbee on a green field.
[00:40:30.500 --> 00:40:34.580]   Now the model will need to predict the tokens
[00:40:34.580 --> 00:40:37.780]   from the text green field to then the pixels,
[00:40:37.780 --> 00:40:39.740]   and that will start making the connections
[00:40:39.740 --> 00:40:40.580]   between the tokens.
[00:40:40.580 --> 00:40:43.620]   So these connections happen as the algorithm learns.
[00:40:43.620 --> 00:40:45.820]   And then the last, if we think of these integers,
[00:40:45.820 --> 00:40:48.740]   the first few are words, the next few are images.
[00:40:48.740 --> 00:40:53.740]   In Gato, we also allocated the highest order of integers
[00:40:53.740 --> 00:40:56.260]   to actions, right?
[00:40:56.260 --> 00:40:59.940]   Which we discretize and actions are very diverse, right?
[00:40:59.940 --> 00:41:04.100]   In Atari, there's, I don't know if 17 discrete actions.
[00:41:04.100 --> 00:41:06.940]   In robotics, actions might be torques
[00:41:06.940 --> 00:41:08.220]   and forces that we apply.
[00:41:08.220 --> 00:41:11.180]   So we just use kind of similar ideas
[00:41:11.180 --> 00:41:14.300]   to compress these actions into tokens.
[00:41:14.300 --> 00:41:18.660]   And then we just, that's how we map now all the space
[00:41:18.660 --> 00:41:20.780]   to these sequence of integers.
[00:41:20.780 --> 00:41:22.420]   But they occupy different space,
[00:41:22.420 --> 00:41:24.820]   and what connects them is then the learning algorithm.
[00:41:24.820 --> 00:41:26.260]   That's where the magic happens.
[00:41:26.260 --> 00:41:28.780]   - So the modalities are orthogonal
[00:41:28.780 --> 00:41:30.300]   to each other in token space.
[00:41:30.300 --> 00:41:31.140]   - Right, right.
[00:41:31.140 --> 00:41:33.620]   - So in the input, everything you add,
[00:41:33.620 --> 00:41:35.220]   you add extra tokens.
[00:41:35.220 --> 00:41:36.060]   - Right.
[00:41:36.060 --> 00:41:40.420]   - And then you're shoving all of that into one place.
[00:41:40.420 --> 00:41:41.620]   - Yes, the transformer.
[00:41:41.620 --> 00:41:42.740]   - And that transformer,
[00:41:42.740 --> 00:41:47.740]   that transformer tries to look at this gigantic token space
[00:41:47.740 --> 00:41:52.220]   and tries to form some kind of representation,
[00:41:52.220 --> 00:41:56.740]   some kind of unique wisdom
[00:41:56.740 --> 00:41:59.220]   about all of these different modalities.
[00:41:59.220 --> 00:42:02.100]   How's that possible?
[00:42:02.100 --> 00:42:06.500]   If you were to sort of put your psychoanalysis hat on
[00:42:06.500 --> 00:42:09.380]   and try to psychoanalyze this neural network,
[00:42:09.380 --> 00:42:11.740]   is it schizophrenic?
[00:42:11.740 --> 00:42:16.740]   Does it try to, given this very few weights,
[00:42:16.740 --> 00:42:19.540]   represent multiple disjoint things
[00:42:19.540 --> 00:42:22.780]   and somehow have them not interfere with each other?
[00:42:22.780 --> 00:42:27.780]   Or is it somehow building on the joint strength,
[00:42:27.780 --> 00:42:31.700]   on whatever is common to all the different modalities?
[00:42:31.700 --> 00:42:35.580]   If you were to ask a question, is it schizophrenic
[00:42:35.580 --> 00:42:38.660]   or is it of one mind?
[00:42:38.660 --> 00:42:41.020]   - I mean, it is one mind,
[00:42:41.020 --> 00:42:44.340]   and it's actually the simplest algorithm,
[00:42:44.340 --> 00:42:47.420]   which that's kind of in a way how it feels
[00:42:47.420 --> 00:42:51.660]   like the field hasn't changed since backpropagation
[00:42:51.660 --> 00:42:53.620]   and gradient descent was purpose
[00:42:53.620 --> 00:42:55.700]   for learning neural networks.
[00:42:55.700 --> 00:42:58.660]   So there is obviously details on the architecture.
[00:42:58.660 --> 00:42:59.580]   This has evolved.
[00:42:59.580 --> 00:43:03.020]   The current iteration is still the transformer,
[00:43:03.020 --> 00:43:07.380]   which is a powerful sequence modeling architecture.
[00:43:07.380 --> 00:43:12.220]   But then the goal of setting these weights
[00:43:12.220 --> 00:43:15.460]   to predict the data is essentially the same
[00:43:15.460 --> 00:43:17.180]   as basically I could describe,
[00:43:17.180 --> 00:43:18.620]   I mean, we described a few years ago,
[00:43:18.620 --> 00:43:21.540]   AlphaStar, language modeling, and so on, right?
[00:43:21.540 --> 00:43:24.540]   We take, let's say, an Atari game.
[00:43:24.540 --> 00:43:27.580]   We map it to a string of numbers
[00:43:27.580 --> 00:43:30.300]   that will all be probably image space
[00:43:30.300 --> 00:43:32.380]   and action space interleaved.
[00:43:32.380 --> 00:43:34.020]   And all we're gonna do is say,
[00:43:34.060 --> 00:43:37.260]   okay, given the numbers,
[00:43:37.260 --> 00:43:40.380]   you know, 10,001, 10,004, 10,005,
[00:43:40.380 --> 00:43:43.220]   the next number that comes is 20,006,
[00:43:43.220 --> 00:43:45.380]   which is in the action space.
[00:43:45.380 --> 00:43:48.820]   And you're just optimizing these weights
[00:43:48.820 --> 00:43:51.660]   via very simple gradients,
[00:43:51.660 --> 00:43:53.500]   like, you know, mathematical is almost
[00:43:53.500 --> 00:43:55.860]   the most boring algorithm you could imagine.
[00:43:55.860 --> 00:43:57.780]   We set all the weights so that
[00:43:57.780 --> 00:44:00.180]   given this particular instance,
[00:44:00.180 --> 00:44:03.180]   these weights are set to maximize
[00:44:03.180 --> 00:44:05.020]   the probability of having seen
[00:44:05.020 --> 00:44:07.260]   this particular sequence of integers
[00:44:07.260 --> 00:44:09.100]   for this particular game.
[00:44:09.100 --> 00:44:11.620]   And then the algorithm does this
[00:44:11.620 --> 00:44:14.740]   for many, many, many iterations,
[00:44:14.740 --> 00:44:16.860]   looking at different modalities,
[00:44:16.860 --> 00:44:17.860]   different games, right?
[00:44:17.860 --> 00:44:20.460]   That's the mixture of the dataset we discussed.
[00:44:20.460 --> 00:44:24.020]   So in a way, it's a very simple algorithm.
[00:44:24.020 --> 00:44:27.540]   And the weights, right, they're all shared, right?
[00:44:27.540 --> 00:44:30.900]   So in terms of, is it focusing on one modality or not,
[00:44:30.900 --> 00:44:33.180]   the intermediate weights that are converting
[00:44:33.180 --> 00:44:35.140]   from these input of integers
[00:44:35.140 --> 00:44:37.660]   to the target integer you're predicting next,
[00:44:37.660 --> 00:44:40.300]   those weights certainly are common.
[00:44:40.300 --> 00:44:43.380]   And then the way the tokenization happens,
[00:44:43.380 --> 00:44:45.820]   there is a special place in the neural network,
[00:44:45.820 --> 00:44:49.780]   which is we map these integer, like number 10,001,
[00:44:49.780 --> 00:44:53.700]   to a vector of real numbers, like real numbers.
[00:44:53.700 --> 00:44:56.100]   We can optimize them with gradient descent, right?
[00:44:56.100 --> 00:44:58.260]   The functions we learn are actually
[00:44:58.260 --> 00:44:59.700]   surprisingly differentiable.
[00:44:59.700 --> 00:45:01.700]   That's why we compute gradients.
[00:45:01.700 --> 00:45:03.900]   So this step is the only one
[00:45:03.900 --> 00:45:06.540]   that this orthogonality dimension applies.
[00:45:06.540 --> 00:45:11.540]   So mapping a certain token for text or image or actions,
[00:45:11.540 --> 00:45:15.020]   each of these tokens gets its own little vector
[00:45:15.020 --> 00:45:17.180]   of real numbers that represents this.
[00:45:17.180 --> 00:45:19.540]   If you look at the field back many years ago,
[00:45:19.540 --> 00:45:23.460]   people were talking about word vectors or word embeddings.
[00:45:23.460 --> 00:45:24.300]   These are the same.
[00:45:24.300 --> 00:45:25.980]   We have word vectors or embeddings.
[00:45:25.980 --> 00:45:28.860]   We have image vector or embeddings
[00:45:28.860 --> 00:45:30.900]   and action vector of embeddings.
[00:45:30.900 --> 00:45:33.900]   And the beauty here is that as you train this model,
[00:45:33.900 --> 00:45:36.660]   if you visualize these little vectors,
[00:45:36.660 --> 00:45:38.460]   it might be that they start aligning
[00:45:38.460 --> 00:45:41.100]   even though they're independent parameters.
[00:45:41.100 --> 00:45:42.860]   There could be anything,
[00:45:42.860 --> 00:45:47.460]   but then it might be that you take the word gato or cat,
[00:45:47.460 --> 00:45:49.020]   which maybe is common enough that it actually
[00:45:49.020 --> 00:45:50.220]   has its own token.
[00:45:50.220 --> 00:45:52.380]   And then you take pixels that have a cat,
[00:45:52.380 --> 00:45:55.300]   and you might start seeing that these vectors
[00:45:55.300 --> 00:45:57.420]   look like they align, right?
[00:45:57.420 --> 00:46:00.660]   So by learning from this vast amount of data,
[00:46:00.660 --> 00:46:03.940]   the model is realizing the potential connections
[00:46:03.940 --> 00:46:05.660]   between these modalities.
[00:46:05.660 --> 00:46:07.860]   Now I will say there will be another way,
[00:46:07.860 --> 00:46:12.860]   at least in part, to not have these different vectors
[00:46:12.860 --> 00:46:15.500]   for each different modality.
[00:46:15.500 --> 00:46:20.220]   For instance, when I tell you about actions in certain space,
[00:46:20.220 --> 00:46:22.820]   I'm defining actions by words, right?
[00:46:22.820 --> 00:46:26.500]   So you could imagine a world in which I'm not learning
[00:46:26.500 --> 00:46:31.180]   that the action app in Atari is its own number.
[00:46:31.180 --> 00:46:34.380]   The action app in Atari maybe is literally the word
[00:46:34.380 --> 00:46:37.300]   or the sentence app in Atari, right?
[00:46:37.300 --> 00:46:39.380]   And that would mean we now leverage
[00:46:39.380 --> 00:46:41.020]   much more from the language.
[00:46:41.020 --> 00:46:42.500]   This is not what we did here,
[00:46:42.500 --> 00:46:45.660]   but certainly it might make these connections
[00:46:45.660 --> 00:46:49.060]   much easier to learn and also to teach the model
[00:46:49.060 --> 00:46:51.260]   to correct its own actions and so on, right?
[00:46:51.260 --> 00:46:55.860]   So all this to say that gato is indeed the beginning,
[00:46:55.860 --> 00:46:59.420]   that it is a radical idea to do this this way,
[00:46:59.420 --> 00:47:02.340]   but there's probably a lot more to be done
[00:47:02.340 --> 00:47:04.420]   and the results to be more impressive,
[00:47:04.420 --> 00:47:07.940]   not only through scale, but also through some new research
[00:47:07.940 --> 00:47:10.460]   that will come hopefully in the years to come.
[00:47:10.460 --> 00:47:12.260]   - So just to elaborate quickly,
[00:47:12.260 --> 00:47:16.660]   you mean one possible next step
[00:47:16.660 --> 00:47:20.180]   or one of the paths that you might take next
[00:47:20.180 --> 00:47:25.180]   is doing the tokenization fundamentally
[00:47:25.180 --> 00:47:28.260]   as a kind of linguistic communication.
[00:47:28.260 --> 00:47:31.340]   So like you convert even images into language.
[00:47:31.340 --> 00:47:35.540]   So doing something like a crude semantic segmentation,
[00:47:35.540 --> 00:47:38.340]   trying to just assign a bunch of words to an image
[00:47:38.340 --> 00:47:42.300]   that like have almost like a dumb entity
[00:47:42.300 --> 00:47:45.300]   explaining as much as it can about the image.
[00:47:45.300 --> 00:47:46.900]   And so you convert that into words
[00:47:46.900 --> 00:47:49.260]   and then you convert games into words
[00:47:49.260 --> 00:47:52.100]   and then you provide the context in words and all of it.
[00:47:53.500 --> 00:47:56.300]   And eventually getting to a point
[00:47:56.300 --> 00:47:58.100]   where everybody agrees with Noam Chomsky
[00:47:58.100 --> 00:48:00.940]   that language is actually at the core of everything.
[00:48:00.940 --> 00:48:04.980]   That it's the base layer of intelligence and consciousness
[00:48:04.980 --> 00:48:07.500]   and all that kind of stuff, okay.
[00:48:07.500 --> 00:48:11.260]   You mentioned early on like it's hard to grow.
[00:48:11.260 --> 00:48:12.780]   What did you mean by that?
[00:48:12.780 --> 00:48:15.700]   'Cause we're talking about scale might change.
[00:48:15.700 --> 00:48:18.980]   There might be, and we'll talk about this too,
[00:48:18.980 --> 00:48:22.060]   like there's a emergent,
[00:48:22.940 --> 00:48:25.020]   there's certain things about these neural networks
[00:48:25.020 --> 00:48:25.860]   that are emergent.
[00:48:25.860 --> 00:48:28.980]   So certain like performance we can see only with scale
[00:48:28.980 --> 00:48:30.980]   and there's some kind of threshold of scale.
[00:48:30.980 --> 00:48:35.980]   So why is it hard to grow something like this Meow Network?
[00:48:35.980 --> 00:48:41.140]   - So the Meow Network, it's not hard to grow
[00:48:41.140 --> 00:48:42.620]   if you retrain it.
[00:48:42.620 --> 00:48:46.860]   What's hard is, well, we have now 1 billion parameters.
[00:48:46.860 --> 00:48:48.140]   We train them for a while.
[00:48:48.140 --> 00:48:50.740]   We spend some amount of work
[00:48:50.740 --> 00:48:53.140]   towards building these weights
[00:48:53.140 --> 00:48:55.900]   that are an amazing initial brain
[00:48:55.900 --> 00:48:58.860]   for doing these kind of tasks we care about.
[00:48:58.860 --> 00:49:03.860]   Could we reuse the weights and expand to a larger brain?
[00:49:03.860 --> 00:49:06.700]   And that is extraordinarily hard,
[00:49:06.700 --> 00:49:10.100]   but also exciting from a research perspective
[00:49:10.100 --> 00:49:12.580]   and a practical perspective point of view, right?
[00:49:12.580 --> 00:49:17.580]   So there's this notion of modularity in software engineering
[00:49:17.580 --> 00:49:20.500]   and we starting to see some examples
[00:49:20.500 --> 00:49:23.340]   and work that leverages modularity.
[00:49:23.340 --> 00:49:26.340]   In fact, if we go back one step from Gato
[00:49:26.340 --> 00:49:29.700]   to a work that I would say train much larger,
[00:49:29.700 --> 00:49:32.580]   much more capable network called Flamingo.
[00:49:32.580 --> 00:49:34.340]   Flamingo did not deal with actions,
[00:49:34.340 --> 00:49:38.460]   but it definitely dealt with images in an interesting way,
[00:49:38.460 --> 00:49:40.300]   kind of akin to what Gato did,
[00:49:40.300 --> 00:49:43.020]   but slightly different technique for tokenizing,
[00:49:43.020 --> 00:49:45.420]   but we don't need to go into that detail.
[00:49:45.420 --> 00:49:49.380]   But what Flamingo also did, which Gato didn't do,
[00:49:49.380 --> 00:49:51.620]   and that just happens because these projects,
[00:49:51.620 --> 00:49:53.580]   you know, they're different.
[00:49:53.580 --> 00:49:55.900]   You know, it's a bit of like the exploratory nature
[00:49:55.900 --> 00:49:57.260]   of research, which is great.
[00:49:57.260 --> 00:50:00.620]   - The research behind these projects is also modular.
[00:50:00.620 --> 00:50:01.860]   - Yes, exactly.
[00:50:01.860 --> 00:50:02.780]   And it has to be, right?
[00:50:02.780 --> 00:50:05.620]   We need to have creativity
[00:50:05.620 --> 00:50:08.860]   and sometimes you need to protect pockets of, you know,
[00:50:08.860 --> 00:50:10.340]   people, researchers, and so on.
[00:50:10.340 --> 00:50:11.860]   - By we, you mean humans.
[00:50:11.860 --> 00:50:12.860]   - Yes. - Okay.
[00:50:12.860 --> 00:50:14.620]   - And also in particular researchers
[00:50:14.620 --> 00:50:16.780]   and maybe even further, you know,
[00:50:16.780 --> 00:50:18.860]   DeepMind or other such labs.
[00:50:18.860 --> 00:50:21.020]   - And then the neural networks themselves.
[00:50:21.020 --> 00:50:23.380]   So it's modularity all the way down.
[00:50:23.380 --> 00:50:24.260]   - All the way down.
[00:50:24.260 --> 00:50:27.540]   So the way that we did modularity very beautifully
[00:50:27.540 --> 00:50:30.140]   in Flamingo is we took Chinchilla,
[00:50:30.140 --> 00:50:32.860]   which is a language only model,
[00:50:32.860 --> 00:50:34.700]   not an agent if we think of actions
[00:50:34.700 --> 00:50:36.740]   being necessary for agency.
[00:50:36.740 --> 00:50:40.980]   So we took Chinchilla, we took the weights of Chinchilla,
[00:50:40.980 --> 00:50:42.820]   and then we froze them.
[00:50:42.820 --> 00:50:44.820]   We said, "These don't change."
[00:50:44.820 --> 00:50:47.580]   We trained them to be very good at predicting the next word.
[00:50:47.580 --> 00:50:49.460]   It's a very good language model,
[00:50:49.460 --> 00:50:51.260]   state of the art at the time you release it,
[00:50:51.260 --> 00:50:52.980]   et cetera, et cetera.
[00:50:52.980 --> 00:50:55.540]   We're gonna add a capability to see, right?
[00:50:55.540 --> 00:50:56.980]   We are gonna add the ability to see
[00:50:56.980 --> 00:50:58.340]   to this language model.
[00:50:58.340 --> 00:51:01.980]   So we're gonna attach small pieces of neural networks
[00:51:01.980 --> 00:51:03.900]   at the right places in the model.
[00:51:03.900 --> 00:51:07.940]   It's almost like injecting the network
[00:51:07.940 --> 00:51:10.780]   with some weights and some substructures
[00:51:10.780 --> 00:51:12.860]   in the ways, in a good way, right?
[00:51:12.860 --> 00:51:15.300]   So you need the research to say what is effective,
[00:51:15.300 --> 00:51:16.740]   how do you add this capability
[00:51:16.740 --> 00:51:18.860]   without destroying others, et cetera.
[00:51:18.860 --> 00:51:23.500]   So we created a small sub-network,
[00:51:23.500 --> 00:51:25.420]   initialized not from random,
[00:51:25.420 --> 00:51:28.820]   but actually from self-supervised learning,
[00:51:28.820 --> 00:51:32.900]   that, you know, a model that understands vision in general.
[00:51:32.900 --> 00:51:37.340]   And then we took datasets that connect the two modalities,
[00:51:37.340 --> 00:51:38.820]   vision and language.
[00:51:38.820 --> 00:51:41.260]   And then we froze the main part,
[00:51:41.260 --> 00:51:43.780]   the largest portion of the network, which was Chinchilla,
[00:51:43.780 --> 00:51:46.020]   that is 70 billion parameters.
[00:51:46.020 --> 00:51:49.300]   And then we added a few more parameters on top,
[00:51:49.300 --> 00:51:50.580]   trained from scratch,
[00:51:50.580 --> 00:51:52.700]   and then some others that were pre-trained
[00:51:52.700 --> 00:51:55.340]   from like, with the capacity to see.
[00:51:55.340 --> 00:51:58.900]   Like it was not tokenization in the way I described for Gato,
[00:51:58.900 --> 00:52:01.500]   but it's a similar idea.
[00:52:01.500 --> 00:52:03.700]   And then we trained the whole system.
[00:52:03.700 --> 00:52:06.700]   Parts of it were frozen, parts of it were new.
[00:52:06.700 --> 00:52:09.780]   And all of a sudden we developed Flamingo,
[00:52:09.780 --> 00:52:12.700]   which is an amazing model that is essentially,
[00:52:12.700 --> 00:52:15.140]   I mean, describing it is a chatbot
[00:52:15.140 --> 00:52:17.100]   where you can also upload images
[00:52:17.100 --> 00:52:20.060]   and start conversing about images,
[00:52:20.060 --> 00:52:23.860]   but it's also kind of a dialogue style chatbot.
[00:52:23.860 --> 00:52:25.900]   - So the input is images and text,
[00:52:25.900 --> 00:52:28.060]   and the output is text. - Yes, exactly.
[00:52:28.060 --> 00:52:29.500]   And- - How many parameters?
[00:52:29.500 --> 00:52:31.940]   You said 70 billion for Chinchilla.
[00:52:31.940 --> 00:52:33.380]   - Yeah, Chinchilla is 70 billion.
[00:52:33.380 --> 00:52:34.780]   And then the ones we add on top,
[00:52:34.780 --> 00:52:39.340]   which kind of almost is almost like a way to overwrite
[00:52:39.340 --> 00:52:42.540]   its little activations so that when it sees vision,
[00:52:42.540 --> 00:52:44.700]   it does kind of a correct computation
[00:52:44.700 --> 00:52:48.100]   of what it's seeing, mapping it back to words, so to speak.
[00:52:48.100 --> 00:52:50.980]   That adds an extra 10 billion parameters, right?
[00:52:50.980 --> 00:52:54.100]   So it's total 80 billion, the largest one we released.
[00:52:54.100 --> 00:52:57.460]   And then you train it on a few data sets
[00:52:57.460 --> 00:52:59.460]   that contain vision and language.
[00:52:59.460 --> 00:53:01.260]   And once you interact with the model,
[00:53:01.260 --> 00:53:04.340]   you start seeing that you can upload an image
[00:53:04.340 --> 00:53:08.100]   and start sort of having a dialogue about the image,
[00:53:08.100 --> 00:53:09.580]   which is actually not something,
[00:53:09.580 --> 00:53:11.900]   it's very similar and akin to what we saw
[00:53:11.900 --> 00:53:15.380]   in language only, these prompting abilities that it has.
[00:53:15.380 --> 00:53:17.860]   You can teach it a new vision task, right?
[00:53:17.860 --> 00:53:21.620]   It does things beyond the capabilities that, in theory,
[00:53:21.620 --> 00:53:24.660]   the data sets provided in themselves,
[00:53:24.660 --> 00:53:27.260]   but because it leverages a lot of the language knowledge
[00:53:27.260 --> 00:53:29.020]   acquired from Chinchilla,
[00:53:29.020 --> 00:53:31.900]   it actually has this few-shot learning ability
[00:53:31.900 --> 00:53:34.780]   and these emerging abilities that we didn't even measure
[00:53:34.780 --> 00:53:36.580]   once we were developing the model.
[00:53:36.580 --> 00:53:40.220]   But once developed, then as you play with the interface,
[00:53:40.220 --> 00:53:41.820]   you can start seeing, wow, okay, yeah,
[00:53:41.820 --> 00:53:44.300]   it's cool, we can upload, I think,
[00:53:44.300 --> 00:53:45.940]   one of the tweets talking about Twitter
[00:53:45.940 --> 00:53:49.980]   was this image from Obama that is placing a weight
[00:53:49.980 --> 00:53:52.540]   and someone is kind of weighting themselves
[00:53:52.540 --> 00:53:55.060]   and it's kind of a joke-style image.
[00:53:55.060 --> 00:53:58.020]   And it's notable because I think Andriy Karpathy
[00:53:58.020 --> 00:54:00.860]   a few years ago said, "No computer vision system
[00:54:00.860 --> 00:54:04.780]   "can understand the subtlety of this joke in this image,
[00:54:04.780 --> 00:54:06.500]   "all the things that go on."
[00:54:06.500 --> 00:54:09.740]   And so what we try to do, and it's very anecdotally,
[00:54:09.740 --> 00:54:12.300]   I mean, this is not a proof that we solved this issue,
[00:54:12.300 --> 00:54:15.860]   but it just shows that you can upload now this image
[00:54:15.860 --> 00:54:17.700]   and start conversing with the model,
[00:54:17.700 --> 00:54:21.500]   trying to make out if it gets that there's a joke
[00:54:21.500 --> 00:54:23.100]   because the person weighting themselves
[00:54:23.100 --> 00:54:26.820]   doesn't see that someone behind is making the weight higher
[00:54:26.820 --> 00:54:27.980]   and so on and so forth.
[00:54:27.980 --> 00:54:30.020]   So it's a fascinating capability
[00:54:30.020 --> 00:54:33.380]   and it comes from this key idea of modularity
[00:54:33.380 --> 00:54:34.940]   where we took a frozen brain
[00:54:34.940 --> 00:54:37.900]   and we just added a new capability.
[00:54:37.900 --> 00:54:40.740]   So the question is, should we,
[00:54:40.740 --> 00:54:42.860]   so in a way you can see even from DeepMind,
[00:54:42.860 --> 00:54:46.420]   we have Flamingo that this moderate approach
[00:54:46.420 --> 00:54:49.180]   and thus could leverage a scale a bit more reasonably
[00:54:49.180 --> 00:54:52.340]   because we didn't need to retrain a system from scratch.
[00:54:52.340 --> 00:54:54.180]   And on the other hand, we had Gato,
[00:54:54.180 --> 00:54:55.940]   which used the same datasets,
[00:54:55.940 --> 00:54:57.500]   but then it trained it from scratch, right?
[00:54:57.500 --> 00:55:01.660]   And so I guess big question for the community is,
[00:55:01.660 --> 00:55:04.700]   should we train from scratch or should we embrace modularity?
[00:55:04.780 --> 00:55:09.780]   And this goes back to modularity as a way to grow,
[00:55:09.780 --> 00:55:12.140]   but reuse seems like natural
[00:55:12.140 --> 00:55:15.020]   and it was very effective, certainly.
[00:55:15.020 --> 00:55:19.060]   - The next question is, if you go the way of modularity,
[00:55:19.060 --> 00:55:22.780]   is there a systematic way of freezing weights
[00:55:22.780 --> 00:55:27.100]   and joining different modalities across,
[00:55:27.100 --> 00:55:29.300]   you know, not just two or three or four networks,
[00:55:29.300 --> 00:55:30.620]   but hundreds of networks
[00:55:30.620 --> 00:55:32.420]   from all different kinds of places,
[00:55:32.420 --> 00:55:36.420]   maybe open source network that looks at weather patterns
[00:55:36.420 --> 00:55:38.020]   and you shove that in somehow,
[00:55:38.020 --> 00:55:40.500]   and then you have networks that, I don't know,
[00:55:40.500 --> 00:55:42.140]   do all kinds of, play StarCraft
[00:55:42.140 --> 00:55:44.100]   and play all the other video games,
[00:55:44.100 --> 00:55:49.100]   and you can keep adding them in without significant effort,
[00:55:49.100 --> 00:55:52.540]   like maybe the effort scales linearly
[00:55:52.540 --> 00:55:53.380]   or something like that,
[00:55:53.380 --> 00:55:55.020]   as opposed to like the more network you add,
[00:55:55.020 --> 00:55:57.980]   the more you have to worry about the instabilities created.
[00:55:57.980 --> 00:56:00.020]   - Yeah, so that vision is beautiful.
[00:56:00.020 --> 00:56:03.580]   I think there's still the question
[00:56:03.580 --> 00:56:06.900]   about within single modalities, like Chinchilla was reused,
[00:56:06.900 --> 00:56:10.260]   but now if we train a next iteration of language models,
[00:56:10.260 --> 00:56:11.900]   are we gonna use Chinchilla or not?
[00:56:11.900 --> 00:56:13.220]   - Yeah, how do you swap out Chinchilla?
[00:56:13.220 --> 00:56:15.980]   - Right, so there's still big questions,
[00:56:15.980 --> 00:56:18.420]   but that idea is actually really akin
[00:56:18.420 --> 00:56:19.420]   to software engineering,
[00:56:19.420 --> 00:56:21.140]   which we're not re-implementing,
[00:56:21.140 --> 00:56:22.420]   you know, libraries from scratch,
[00:56:22.420 --> 00:56:25.460]   we're reusing and then building ever more amazing things,
[00:56:25.460 --> 00:56:29.060]   including neural networks with software that we're reusing.
[00:56:29.060 --> 00:56:32.260]   So I think this idea of modularity, I like it,
[00:56:32.260 --> 00:56:33.980]   I think it's here to stay,
[00:56:33.980 --> 00:56:35.980]   and that's also why I mentioned
[00:56:35.980 --> 00:56:38.300]   it's just the beginning, not the end.
[00:56:38.300 --> 00:56:39.500]   - You've mentioned meta-learning,
[00:56:39.500 --> 00:56:42.900]   so given this promise of Gato,
[00:56:42.900 --> 00:56:46.100]   can we try to redefine this term
[00:56:46.100 --> 00:56:47.700]   that's almost akin to consciousness,
[00:56:47.700 --> 00:56:50.260]   because it means different things to different people
[00:56:50.260 --> 00:56:52.500]   throughout the history of artificial intelligence,
[00:56:52.500 --> 00:56:57.500]   but what do you think meta-learning is and looks like
[00:56:58.220 --> 00:57:00.140]   now in the five years, 10 years,
[00:57:00.140 --> 00:57:03.300]   will it look like system like Gato, but scaled?
[00:57:03.300 --> 00:57:04.260]   What's your sense of,
[00:57:04.260 --> 00:57:08.380]   what does meta-learning look like, do you think,
[00:57:08.380 --> 00:57:10.580]   with all the wisdom we've learned so far?
[00:57:10.580 --> 00:57:11.660]   - Yeah, great question,
[00:57:11.660 --> 00:57:14.620]   maybe it's good to give another data point
[00:57:14.620 --> 00:57:16.300]   looking backwards rather than forward.
[00:57:16.300 --> 00:57:20.660]   So when we talk in 2019,
[00:57:20.660 --> 00:57:26.620]   meta-learning meant something that has changed
[00:57:26.620 --> 00:57:31.260]   mostly through the revolution of GPT-3 and beyond.
[00:57:31.260 --> 00:57:34.060]   So what meta-learning meant at the time
[00:57:34.060 --> 00:57:37.780]   was driven by what benchmarks people care about
[00:57:37.780 --> 00:57:38.940]   in meta-learning,
[00:57:38.940 --> 00:57:40.740]   and the benchmarks were about
[00:57:40.740 --> 00:57:45.100]   a capability to learn about object identities,
[00:57:45.100 --> 00:57:47.500]   so it was very much over-fitted
[00:57:47.500 --> 00:57:50.460]   to vision and object classification,
[00:57:50.460 --> 00:57:53.020]   and the part that was meta about that was that,
[00:57:53.020 --> 00:57:55.420]   oh, we're not just learning a thousand categories
[00:57:55.420 --> 00:57:57.140]   that ImageNet tells us to learn,
[00:57:57.140 --> 00:58:00.580]   we're gonna learn object categories that can be defined
[00:58:00.580 --> 00:58:03.380]   when we interact with the model.
[00:58:03.380 --> 00:58:06.740]   So it's interesting to see the evolution, right?
[00:58:06.740 --> 00:58:10.860]   The way this started was we have a special language
[00:58:10.860 --> 00:58:13.340]   that was a data set, a small data set
[00:58:13.340 --> 00:58:15.380]   that we prompted the model with,
[00:58:15.380 --> 00:58:18.900]   saying, hey, here is a new classification task,
[00:58:18.900 --> 00:58:21.860]   I'll give you one image and the name,
[00:58:21.860 --> 00:58:24.460]   which was an integer at the time of the image,
[00:58:24.460 --> 00:58:26.060]   and a different image, and so on.
[00:58:26.060 --> 00:58:30.100]   So you have a small prompt in the form of a data set,
[00:58:30.100 --> 00:58:31.700]   a machine learning data set,
[00:58:31.700 --> 00:58:35.580]   and then you got then a system that could then predict
[00:58:35.580 --> 00:58:37.020]   or classify these objects
[00:58:37.020 --> 00:58:39.420]   that you just defined kind of on the fly.
[00:58:39.420 --> 00:58:43.220]   So fast forward,
[00:58:43.220 --> 00:58:47.500]   it was revealed that language models are future learners,
[00:58:47.500 --> 00:58:50.140]   that's the title of the paper, so very good title.
[00:58:50.140 --> 00:58:51.580]   Sometimes titles are really good,
[00:58:51.580 --> 00:58:53.580]   so this one is really, really good,
[00:58:53.580 --> 00:58:56.220]   because that's the point of GPT-3,
[00:58:56.220 --> 00:58:58.820]   that showed that, look, sure,
[00:58:58.820 --> 00:59:00.980]   we can focus on object classification
[00:59:00.980 --> 00:59:02.580]   and what meta-learning means
[00:59:02.580 --> 00:59:05.460]   within the space of learning object categories,
[00:59:05.460 --> 00:59:07.460]   this goes beyond, or before, rather,
[00:59:07.460 --> 00:59:10.060]   to also Omniglot, before ImageNet, and so on.
[00:59:10.060 --> 00:59:11.500]   So there's a few benchmarks.
[00:59:11.500 --> 00:59:13.020]   To now, all of a sudden,
[00:59:13.020 --> 00:59:15.220]   we're a bit unlocked from benchmarks,
[00:59:15.220 --> 00:59:17.900]   and through language, we can define tasks, right?
[00:59:17.900 --> 00:59:21.580]   So we're literally telling the model some logical task
[00:59:21.580 --> 00:59:23.860]   or little thing that we wanted to do.
[00:59:23.860 --> 00:59:25.900]   We prompt it much like we did before,
[00:59:25.900 --> 00:59:28.460]   but now we prompt it through natural language.
[00:59:28.460 --> 00:59:30.420]   And then, not perfectly,
[00:59:30.420 --> 00:59:33.180]   I mean, these models have failure modes, and that's fine,
[00:59:33.180 --> 00:59:37.140]   but these models then are now doing a new task, right?
[00:59:37.140 --> 00:59:40.460]   So they meta-learn this new capability.
[00:59:40.460 --> 00:59:43.380]   Now, that's where we are now.
[00:59:43.380 --> 00:59:47.220]   Flamingo expanded this to visual and language,
[00:59:47.220 --> 00:59:49.300]   but it basically has the same abilities.
[00:59:49.300 --> 00:59:51.540]   You can teach it, for instance,
[00:59:51.540 --> 00:59:53.260]   an emergent property was that
[00:59:53.260 --> 00:59:55.260]   you can take pictures of numbers
[00:59:55.260 --> 00:59:57.780]   and then do arithmetic with the numbers
[00:59:57.780 --> 00:59:59.900]   just by teaching it, "Oh, that's,
[00:59:59.900 --> 01:00:01.980]   "when I show you three plus six,
[01:00:01.980 --> 01:00:03.620]   "I want you to output nine,
[01:00:03.620 --> 01:00:06.660]   "and you show it a few examples, and now it does that."
[01:00:06.660 --> 01:00:10.180]   So it went way beyond this ImageNet
[01:00:10.180 --> 01:00:12.620]   sort of categorization of images
[01:00:12.620 --> 01:00:14.140]   that we were a bit stuck, maybe,
[01:00:14.140 --> 01:00:19.020]   before this revelation moment that happened in 2000.
[01:00:19.020 --> 01:00:21.860]   I believe it was '19, but it was after we chatted.
[01:00:21.860 --> 01:00:24.260]   - And that way it has solved meta-learning
[01:00:24.260 --> 01:00:26.020]   as was previously defined.
[01:00:26.020 --> 01:00:27.700]   - Yes, it expanded what it meant.
[01:00:27.700 --> 01:00:29.460]   So that's what you say, what does it mean?
[01:00:29.460 --> 01:00:31.300]   So it's an evolving term.
[01:00:31.300 --> 01:00:35.140]   But here is maybe now looking forward,
[01:00:35.140 --> 01:00:37.540]   looking at what's happening,
[01:00:37.540 --> 01:00:41.340]   obviously in the community with more modalities,
[01:00:41.340 --> 01:00:42.420]   what we can expect.
[01:00:42.420 --> 01:00:44.900]   And I would certainly hope to see the following,
[01:00:44.900 --> 01:00:48.340]   and this is a pretty drastic hope,
[01:00:48.340 --> 01:00:51.140]   but in five years, maybe we chat again.
[01:00:51.140 --> 01:00:55.860]   And we have a system, right, a set of weights
[01:00:55.860 --> 01:00:59.780]   that we can teach it to play StarCraft.
[01:00:59.780 --> 01:01:01.420]   Maybe not at the level of AlphaStar,
[01:01:01.420 --> 01:01:03.620]   but play StarCraft, a complex game.
[01:01:03.620 --> 01:01:06.860]   We teach it through interactions to prompting.
[01:01:06.860 --> 01:01:08.460]   You can certainly prompt a system,
[01:01:08.460 --> 01:01:11.700]   that's what Gato shows, to play some simple Atari games.
[01:01:11.700 --> 01:01:15.300]   So imagine if you start talking to a system,
[01:01:15.300 --> 01:01:16.660]   teaching it a new game,
[01:01:16.780 --> 01:01:20.940]   showing it examples of, in this particular game,
[01:01:20.940 --> 01:01:22.740]   this user did something good.
[01:01:22.740 --> 01:01:25.420]   Maybe the system can even play and ask you questions,
[01:01:25.420 --> 01:01:26.940]   say, "Hey, I played this game.
[01:01:26.940 --> 01:01:27.860]   I just played this game.
[01:01:27.860 --> 01:01:29.060]   Did I do well?
[01:01:29.060 --> 01:01:30.420]   Can you teach me more?"
[01:01:30.420 --> 01:01:34.780]   So five, maybe to 10 years, these capabilities,
[01:01:34.780 --> 01:01:36.180]   or what meta-learning means,
[01:01:36.180 --> 01:01:38.860]   will be much more interactive, much more rich,
[01:01:38.860 --> 01:01:41.620]   and through domains that we were specializing, right?
[01:01:41.620 --> 01:01:42.900]   So you see the difference, right?
[01:01:42.900 --> 01:01:46.980]   We built AlphaStar specialized to play StarCraft.
[01:01:46.980 --> 01:01:48.220]   The algorithms were general,
[01:01:48.220 --> 01:01:50.420]   but the weights were specialized.
[01:01:50.420 --> 01:01:54.180]   And what we're hoping is that we can teach a network
[01:01:54.180 --> 01:01:56.580]   to play games, to play any game,
[01:01:56.580 --> 01:01:58.580]   just using games as an example,
[01:01:58.580 --> 01:02:01.500]   through interacting with it, teaching it,
[01:02:01.500 --> 01:02:03.740]   uploading the Wikipedia page of StarCraft.
[01:02:03.740 --> 01:02:06.100]   Like this is in the horizon,
[01:02:06.100 --> 01:02:09.340]   and obviously there are details need to be filled,
[01:02:09.340 --> 01:02:10.940]   and research need to be done.
[01:02:10.940 --> 01:02:13.220]   But that's how I see meta-learning above,
[01:02:13.220 --> 01:02:15.380]   which is gonna be beyond prompting.
[01:02:15.380 --> 01:02:17.060]   It's gonna be a bit more interactive.
[01:02:17.060 --> 01:02:19.820]   It's gonna, you know, the system might tell us
[01:02:19.820 --> 01:02:22.340]   to give it feedback after it maybe makes mistakes
[01:02:22.340 --> 01:02:26.260]   or it loses a game, but it's nonetheless very exciting
[01:02:26.260 --> 01:02:29.020]   because if you think about this this way,
[01:02:29.020 --> 01:02:30.620]   the benchmarks are already there.
[01:02:30.620 --> 01:02:33.180]   We just repurpose the benchmarks, right?
[01:02:33.180 --> 01:02:36.980]   So in a way, I like to map the space
[01:02:36.980 --> 01:02:40.340]   of what maybe AGI means to say,
[01:02:40.340 --> 01:02:45.340]   okay, like we went 101% performance in Go,
[01:02:45.340 --> 01:02:47.860]   in Chess, in StarCraft.
[01:02:47.860 --> 01:02:51.900]   The next iteration might be 20% performance
[01:02:51.900 --> 01:02:54.700]   across quote unquote all tasks, right?
[01:02:54.700 --> 01:02:56.300]   And even if it's not as good, it's fine.
[01:02:56.300 --> 01:02:59.940]   We actually, we have ways to also measure progress
[01:02:59.940 --> 01:03:01.620]   because we have those special agents,
[01:03:01.620 --> 01:03:04.180]   specialized agents, and so on.
[01:03:04.180 --> 01:03:06.220]   So this is to me very exciting.
[01:03:06.220 --> 01:03:09.260]   And these next iteration models
[01:03:09.260 --> 01:03:13.380]   are definitely hinting at that direction of progress,
[01:03:13.380 --> 01:03:14.700]   which hopefully we can have.
[01:03:14.700 --> 01:03:17.580]   There are obviously some things that could go wrong
[01:03:17.580 --> 01:03:20.100]   in terms of we might not have the tools,
[01:03:20.100 --> 01:03:22.540]   maybe transformers are not enough, then we must,
[01:03:22.540 --> 01:03:24.300]   there's some breakthroughs to come,
[01:03:24.300 --> 01:03:26.300]   which makes the field more exciting
[01:03:26.300 --> 01:03:28.620]   to people like me as well, of course.
[01:03:28.620 --> 01:03:32.100]   But that's, if you ask me five to 10 years,
[01:03:32.100 --> 01:03:34.300]   you might see these models that start to look more
[01:03:34.300 --> 01:03:36.860]   like weights that are already trained.
[01:03:36.860 --> 01:03:39.540]   And then it's more about teaching
[01:03:39.540 --> 01:03:42.540]   or make their meta learn what you're trying to induce
[01:03:42.540 --> 01:03:46.940]   in terms of tasks and so on.
[01:03:46.940 --> 01:03:50.980]   Well beyond the simple now tasks we're starting to see emerge
[01:03:50.980 --> 01:03:54.140]   like small arithmetic tasks and so on.
[01:03:54.140 --> 01:03:55.700]   - So a few questions around that.
[01:03:55.700 --> 01:03:57.180]   This is fascinating.
[01:03:57.180 --> 01:04:01.420]   So that kind of teaching, interactive,
[01:04:01.420 --> 01:04:02.740]   so it's beyond prompting,
[01:04:02.740 --> 01:04:05.180]   so it's interacting with the neural network,
[01:04:05.180 --> 01:04:08.380]   that's different than the training process.
[01:04:08.380 --> 01:04:12.420]   So it's different than the optimization
[01:04:12.420 --> 01:04:15.900]   over differentiable functions.
[01:04:15.900 --> 01:04:18.620]   This is already trained and now you're teaching,
[01:04:18.620 --> 01:04:24.180]   I mean, it's almost like akin to the brain,
[01:04:24.180 --> 01:04:26.900]   the neurons are already set with their connections.
[01:04:26.900 --> 01:04:29.980]   On top of that, you're now using that infrastructure
[01:04:29.980 --> 01:04:31.740]   to build up further knowledge.
[01:04:32.620 --> 01:04:36.700]   - Okay, so that's a really interesting distinction
[01:04:36.700 --> 01:04:38.060]   that's actually not obvious
[01:04:38.060 --> 01:04:40.340]   from a software engineering perspective,
[01:04:40.340 --> 01:04:42.820]   that there's a line to be drawn.
[01:04:42.820 --> 01:04:44.900]   'Cause you always think for neural network to learn,
[01:04:44.900 --> 01:04:48.340]   it has to be retrained, trained and retrained.
[01:04:48.340 --> 01:04:53.220]   But maybe, and prompting is a way of teaching
[01:04:53.220 --> 01:04:55.980]   a neural network a little bit of context
[01:04:55.980 --> 01:04:58.020]   about whatever the heck you're trying to do.
[01:04:58.020 --> 01:05:00.460]   So you can maybe expand this prompting capability
[01:05:00.460 --> 01:05:04.220]   by making it interact, that's really, really interesting.
[01:05:04.220 --> 01:05:06.380]   - Yeah, by the way, this is not,
[01:05:06.380 --> 01:05:09.220]   if you look at way back at different ways
[01:05:09.220 --> 01:05:11.820]   to tackle even classification tasks,
[01:05:11.820 --> 01:05:16.460]   so this comes from like long standing literature
[01:05:16.460 --> 01:05:18.260]   in machine learning.
[01:05:18.260 --> 01:05:20.780]   What I'm suggesting could sound to some
[01:05:20.780 --> 01:05:23.420]   like a bit like nearest neighbor.
[01:05:23.420 --> 01:05:26.100]   So nearest neighbor is almost the simplest algorithm
[01:05:26.100 --> 01:05:30.060]   that does not require learning.
[01:05:30.060 --> 01:05:31.740]   So it has this interesting,
[01:05:31.740 --> 01:05:34.340]   like you don't need to compute gradients.
[01:05:34.340 --> 01:05:37.500]   And what nearest neighbor does is you quote unquote,
[01:05:37.500 --> 01:05:39.980]   have a data set or upload a data set.
[01:05:39.980 --> 01:05:43.060]   And then all you need to do is a way to measure distance
[01:05:43.060 --> 01:05:44.780]   between points.
[01:05:44.780 --> 01:05:46.660]   And then to classify a new point,
[01:05:46.660 --> 01:05:49.220]   you're just simply computing what's the closest point
[01:05:49.220 --> 01:05:51.260]   in this massive amount of data.
[01:05:51.260 --> 01:05:52.700]   And that's my answer.
[01:05:52.700 --> 01:05:55.500]   So you can think of prompting in a way
[01:05:55.500 --> 01:05:58.620]   as you're uploading, not just simple points
[01:05:58.620 --> 01:06:02.420]   and the metric is not the distance between the images
[01:06:02.420 --> 01:06:03.260]   or something simple,
[01:06:03.260 --> 01:06:06.020]   it's something that you compute that's much more advanced,
[01:06:06.020 --> 01:06:08.380]   but in a way, it's very similar, right?
[01:06:08.380 --> 01:06:12.620]   You simply are uploading some knowledge
[01:06:12.620 --> 01:06:15.060]   to this pre-trained system in nearest neighbor,
[01:06:15.060 --> 01:06:17.260]   maybe the metric is learned or not,
[01:06:17.260 --> 01:06:19.460]   but you don't need to further train it.
[01:06:19.460 --> 01:06:23.700]   And then now you immediately get a classifier out of this.
[01:06:23.700 --> 01:06:25.820]   Now it's just an evolution of that concept,
[01:06:25.820 --> 01:06:27.820]   very classical concept in machine learning,
[01:06:27.820 --> 01:06:32.180]   which is just learning through what's the closest point,
[01:06:32.180 --> 01:06:34.540]   closest by some distance and that's it.
[01:06:34.540 --> 01:06:36.100]   It's an evolution of that.
[01:06:36.100 --> 01:06:39.020]   And I will say how I saw meta-learning
[01:06:39.020 --> 01:06:43.900]   when we worked on a few ideas in 2016,
[01:06:43.900 --> 01:06:47.220]   was precisely through the lens of nearest neighbor,
[01:06:47.220 --> 01:06:49.940]   which is very common in computer vision community, right?
[01:06:49.940 --> 01:06:52.140]   There's a very active area of research
[01:06:52.140 --> 01:06:55.460]   about how do you compute the distance between two images,
[01:06:55.460 --> 01:06:57.580]   but if you have a good distance metric,
[01:06:57.580 --> 01:06:59.940]   you also have a good classifier, right?
[01:06:59.940 --> 01:07:01.740]   All I'm saying is now these distances
[01:07:01.740 --> 01:07:03.780]   and the points are not just images,
[01:07:03.780 --> 01:07:08.540]   they're like words or sequences of words and images
[01:07:08.540 --> 01:07:10.380]   and actions that teach you something new,
[01:07:10.380 --> 01:07:14.740]   but it might be that technique-wise, those come back.
[01:07:14.740 --> 01:07:18.180]   And I will say that it's not necessarily true
[01:07:18.180 --> 01:07:21.780]   that you might not ever train the weights a bit further.
[01:07:21.780 --> 01:07:23.900]   Some aspect of meta-learning,
[01:07:23.900 --> 01:07:26.020]   some techniques in meta-learning
[01:07:26.020 --> 01:07:28.900]   do actually do a bit of fine tuning, as it's called, right?
[01:07:28.900 --> 01:07:31.100]   They train the weights a little bit
[01:07:31.100 --> 01:07:32.820]   when they get a new task.
[01:07:32.820 --> 01:07:36.940]   So as I call the how, or how we're gonna achieve this,
[01:07:36.940 --> 01:07:39.820]   as a deep learner, I'm very skeptic.
[01:07:39.820 --> 01:07:41.220]   We're gonna try a few things,
[01:07:41.220 --> 01:07:44.180]   whether it's a bit of training, adding a few parameters,
[01:07:44.180 --> 01:07:45.940]   thinking of these as nearest neighbor,
[01:07:45.940 --> 01:07:49.180]   or just simply thinking of there's a sequence of words,
[01:07:49.180 --> 01:07:52.980]   it's a prefix, and that's the new classifier.
[01:07:52.980 --> 01:07:53.820]   We'll see, right?
[01:07:53.820 --> 01:07:55.420]   There's the beauty of research,
[01:07:55.420 --> 01:08:00.140]   but what's important is that is a good goal in itself
[01:08:00.140 --> 01:08:02.740]   that I see as very worthwhile pursuing
[01:08:02.740 --> 01:08:05.700]   for the next stages of not only meta-learning.
[01:08:05.700 --> 01:08:08.460]   I think this is basically what's exciting
[01:08:08.460 --> 01:08:11.380]   about machine learning, period, to me.
[01:08:11.380 --> 01:08:13.740]   - Well, and then the interactive aspect of that
[01:08:13.740 --> 01:08:15.140]   is also very interesting.
[01:08:15.140 --> 01:08:16.380]   - Yes. - The interactive version
[01:08:16.380 --> 01:08:18.420]   of nearest neighbor. (laughs)
[01:08:18.420 --> 01:08:20.620]   - Yeah. - To help you pull out
[01:08:20.620 --> 01:08:23.740]   the classifier from this giant thing.
[01:08:23.740 --> 01:08:28.740]   Okay, is this the way we can go in five, 10 plus years
[01:08:28.740 --> 01:08:36.100]   from any task, sorry, from many tasks to any task?
[01:08:36.100 --> 01:08:39.420]   So, and what does that mean?
[01:08:39.420 --> 01:08:41.620]   What does it need to be actually trained on?
[01:08:41.620 --> 01:08:45.460]   Which point is the network had enough?
[01:08:45.460 --> 01:08:50.460]   So what does a network need to learn about this world
[01:08:50.460 --> 01:08:52.460]   in order to be able to perform any task?
[01:08:52.460 --> 01:08:57.460]   Is it just as simple as language, image, and action?
[01:08:57.460 --> 01:09:01.820]   Or do you need some set of representative images?
[01:09:01.820 --> 01:09:05.180]   Like if you only see land images,
[01:09:05.180 --> 01:09:06.700]   will you know anything about underwater?
[01:09:06.700 --> 01:09:08.740]   Is that somehow fundamentally different?
[01:09:08.740 --> 01:09:09.580]   I don't know.
[01:09:09.580 --> 01:09:12.060]   - Those, I mean, those are awkward questions, I would say.
[01:09:12.060 --> 01:09:15.020]   I mean, the way you put, let me maybe further your example.
[01:09:15.020 --> 01:09:18.400]   Right, if all you see is land images,
[01:09:18.400 --> 01:09:21.540]   but you're reading all about land and water worlds,
[01:09:21.540 --> 01:09:23.900]   but in books, right, imagine.
[01:09:23.900 --> 01:09:25.380]   Would that be enough?
[01:09:25.380 --> 01:09:26.460]   Good question.
[01:09:26.460 --> 01:09:30.380]   We don't know, but I guess maybe you can join us
[01:09:30.380 --> 01:09:32.100]   if you want in our quest to find this.
[01:09:32.100 --> 01:09:33.420]   That's precisely--
[01:09:33.420 --> 01:09:34.340]   - Water world, yeah.
[01:09:34.340 --> 01:09:37.620]   - Yes, that's precisely, I mean, the beauty of research
[01:09:37.620 --> 01:09:42.620]   and that's the research business we're in, I guess,
[01:09:42.620 --> 01:09:46.220]   is to figure these out and ask the right questions
[01:09:46.220 --> 01:09:49.540]   and then iterate with the whole community,
[01:09:49.540 --> 01:09:52.420]   publishing findings and so on.
[01:09:52.420 --> 01:09:55.100]   But yeah, this is a question.
[01:09:55.100 --> 01:09:57.540]   It's not the only question, but it's certainly, as you ask,
[01:09:57.540 --> 01:10:00.020]   is on my mind constantly, right?
[01:10:00.020 --> 01:10:03.260]   And so we'll need to wait for maybe the,
[01:10:03.260 --> 01:10:05.940]   let's say five years, let's hope it's not 10,
[01:10:05.940 --> 01:10:08.380]   to see what are the answers.
[01:10:08.380 --> 01:10:12.660]   Some people will largely believe in unsupervised
[01:10:12.660 --> 01:10:15.460]   or self-supervised learning of single modalities
[01:10:15.460 --> 01:10:17.040]   and then crossing them.
[01:10:18.000 --> 01:10:21.680]   Some people might think end-to-end learning is the answer.
[01:10:21.680 --> 01:10:23.780]   Modularity is maybe the answer.
[01:10:23.780 --> 01:10:24.960]   So we don't know,
[01:10:24.960 --> 01:10:27.520]   but we're just definitely excited to find out.
[01:10:27.520 --> 01:10:29.280]   - But it feels like this is the right time
[01:10:29.280 --> 01:10:31.720]   and we're at the beginning of this journey.
[01:10:31.720 --> 01:10:34.640]   We're finally ready to do these kind of general,
[01:10:34.640 --> 01:10:37.600]   big models and agents.
[01:10:37.600 --> 01:10:42.480]   What do you sort of specific technical thing
[01:10:42.480 --> 01:10:47.360]   about Gato, Flamingo, Chinchilla, Gopher,
[01:10:47.360 --> 01:10:49.520]   any of these that is especially beautiful,
[01:10:49.520 --> 01:10:51.640]   that was surprising maybe?
[01:10:51.640 --> 01:10:54.220]   Is there something that just jumps out at you?
[01:10:54.220 --> 01:10:57.560]   Of course, there's the general thing of like,
[01:10:57.560 --> 01:10:58.900]   you didn't think it was possible
[01:10:58.900 --> 01:11:01.700]   and then you realize it's possible
[01:11:01.700 --> 01:11:04.480]   in terms of the generalizability across modalities
[01:11:04.480 --> 01:11:05.560]   and all that kind of stuff.
[01:11:05.560 --> 01:11:08.920]   Or maybe how small of a network, relatively speaking,
[01:11:08.920 --> 01:11:10.440]   Gato is, all that kind of stuff.
[01:11:10.440 --> 01:11:15.200]   But is there some weird little things that were surprising?
[01:11:15.200 --> 01:11:18.240]   - Look, I'll give you an answer that's very important
[01:11:18.240 --> 01:11:22.600]   because maybe people don't quite realize this,
[01:11:22.600 --> 01:11:27.240]   but the teams behind these efforts, the actual humans,
[01:11:27.240 --> 01:11:31.720]   that's maybe the surprising, obviously positive way.
[01:11:31.720 --> 01:11:34.580]   So anytime you see these breakthroughs,
[01:11:34.580 --> 01:11:37.160]   I mean, it's easy to map it to a few people.
[01:11:37.160 --> 01:11:39.220]   There's people that are great at explaining things
[01:11:39.220 --> 01:11:40.720]   and so on, that's very nice.
[01:11:40.720 --> 01:11:44.680]   But maybe the learnings or the meta learnings
[01:11:44.680 --> 01:11:47.400]   that I get as a human about this is,
[01:11:47.400 --> 01:11:49.060]   sure, we can move forward,
[01:11:49.060 --> 01:11:55.480]   but the surprising bit is how important
[01:11:55.480 --> 01:11:58.720]   are all the pieces of these projects,
[01:11:58.720 --> 01:12:00.040]   how do they come together?
[01:12:00.040 --> 01:12:04.440]   So I'll give you maybe some of the ingredients of success
[01:12:04.440 --> 01:12:06.440]   that are common across these,
[01:12:06.440 --> 01:12:08.480]   but not the obvious ones in machine learning.
[01:12:08.480 --> 01:12:11.320]   I can always also give you those.
[01:12:11.320 --> 01:12:16.320]   But basically, engineering is critical.
[01:12:16.320 --> 01:12:19.600]   So very good engineering,
[01:12:19.600 --> 01:12:23.760]   because ultimately we're collecting data sets, right?
[01:12:23.760 --> 01:12:26.160]   So the engineering of data
[01:12:26.160 --> 01:12:29.740]   and then of deploying the models at scale
[01:12:29.740 --> 01:12:32.840]   into some compute cluster that cannot go understated,
[01:12:32.840 --> 01:12:35.940]   that is a huge factor of success.
[01:12:36.880 --> 01:12:41.560]   And it's hard to believe that details matter so much.
[01:12:41.560 --> 01:12:44.040]   We would like to believe that it's true
[01:12:44.040 --> 01:12:47.440]   that there is more and more of a standard formula,
[01:12:47.440 --> 01:12:50.560]   as I was saying, like this recipe that works for everything.
[01:12:50.560 --> 01:12:53.680]   But then when you zoom in into each of these projects,
[01:12:53.680 --> 01:12:57.840]   then you realize the devil is indeed in the details.
[01:12:57.840 --> 01:13:01.520]   And then the teams have to work kind of together
[01:13:01.520 --> 01:13:03.040]   towards these goals.
[01:13:03.040 --> 01:13:07.520]   So engineering of data and obviously clusters
[01:13:07.520 --> 01:13:09.280]   and large scale is very important.
[01:13:09.280 --> 01:13:13.120]   And then one that is often not,
[01:13:13.120 --> 01:13:15.080]   maybe nowadays it is more clear,
[01:13:15.080 --> 01:13:17.160]   is benchmark progress, right?
[01:13:17.160 --> 01:13:19.860]   So we're talking here about multiple months
[01:13:19.860 --> 01:13:22.120]   of tens of researchers
[01:13:22.120 --> 01:13:26.160]   and people that are trying to organize the research
[01:13:26.160 --> 01:13:28.080]   and so on, working together.
[01:13:28.080 --> 01:13:32.120]   And you don't know that you can get there.
[01:13:32.120 --> 01:13:34.360]   I mean, this is the beauty.
[01:13:34.360 --> 01:13:37.320]   Like if you're not risking to trying to do something
[01:13:37.320 --> 01:13:40.540]   that feels impossible, you're not gonna get there,
[01:13:40.540 --> 01:13:43.960]   but you need a way to measure progress.
[01:13:43.960 --> 01:13:47.740]   So the benchmarks that you build are critical.
[01:13:47.740 --> 01:13:50.520]   I've seen this beautifully play out in many projects.
[01:13:50.520 --> 01:13:53.880]   I mean, maybe the one I've seen it more consistently,
[01:13:53.880 --> 01:13:56.840]   which means we establish the metric,
[01:13:56.840 --> 01:13:58.320]   actually the community did,
[01:13:58.320 --> 01:14:01.560]   and then we leverage that massively is AlphaFold.
[01:14:01.560 --> 01:14:04.520]   This is a project where the data,
[01:14:04.520 --> 01:14:06.120]   the metrics were all there.
[01:14:06.120 --> 01:14:09.120]   And all it took was, and it's easier said than done,
[01:14:09.120 --> 01:14:11.640]   an amazing team working,
[01:14:11.640 --> 01:14:14.760]   not to try to find some incremental improvement
[01:14:14.760 --> 01:14:17.940]   and publish, which is one way to do research that is valid,
[01:14:17.940 --> 01:14:22.520]   but aim very high and work literally for years
[01:14:22.520 --> 01:14:24.120]   to iterate over that process.
[01:14:24.120 --> 01:14:25.660]   And working for years with the team,
[01:14:25.660 --> 01:14:29.800]   I mean, it is tricky that also happened to happen
[01:14:29.800 --> 01:14:32.200]   partly during a pandemic and so on.
[01:14:32.200 --> 01:14:35.280]   So I think my meta learning from all this is,
[01:14:35.280 --> 01:14:37.960]   the teams are critical to the success.
[01:14:37.960 --> 01:14:40.200]   And then if now going to the machine learning,
[01:14:40.200 --> 01:14:42.880]   the part that's surprising is,
[01:14:42.880 --> 01:14:48.720]   so we like architectures like neural networks,
[01:14:48.720 --> 01:14:53.120]   and I would say this was a very rapidly evolving field
[01:14:53.120 --> 01:14:54.960]   until the transformer came.
[01:14:54.960 --> 01:14:58.160]   So attention might indeed be all you need,
[01:14:58.160 --> 01:15:00.280]   which is the title, also a good title,
[01:15:00.280 --> 01:15:02.280]   although in hindsight is good.
[01:15:02.280 --> 01:15:03.440]   I don't think at the time I thought
[01:15:03.440 --> 01:15:05.040]   this is a great title for a paper,
[01:15:05.040 --> 01:15:08.960]   but that architecture is proving
[01:15:08.960 --> 01:15:12.540]   that the dream of modeling sequences of any bytes,
[01:15:12.540 --> 01:15:15.360]   there is something there that will stick.
[01:15:15.360 --> 01:15:18.280]   And I think these advance in architectures,
[01:15:18.280 --> 01:15:21.040]   in kind of how neural networks are architectured
[01:15:21.040 --> 01:15:23.120]   to do what they do.
[01:15:23.120 --> 01:15:26.080]   It's been hard to find one that has been so stable
[01:15:26.080 --> 01:15:28.920]   and relatively has changed very little
[01:15:28.920 --> 01:15:33.040]   since it was invented five or so years ago.
[01:15:33.040 --> 01:15:35.200]   So that is a surprising,
[01:15:35.200 --> 01:15:38.320]   is a surprise that keeps recurring to other projects.
[01:15:38.320 --> 01:15:42.440]   - Try to, on a philosophical or technical level,
[01:15:42.440 --> 01:15:45.480]   introspect what is the magic of attention?
[01:15:45.480 --> 01:15:47.320]   What is attention?
[01:15:47.320 --> 01:15:50.120]   That's attention in people that study cognition,
[01:15:50.120 --> 01:15:52.080]   so human attention.
[01:15:52.080 --> 01:15:55.780]   I think there's giant wars over what attention means,
[01:15:55.780 --> 01:15:57.440]   how it works in the human mind.
[01:15:57.440 --> 01:16:00.200]   So what, there's very simple looks
[01:16:00.200 --> 01:16:02.600]   at what attention is in neural network
[01:16:02.600 --> 01:16:04.440]   from the days of attention is all you need,
[01:16:04.440 --> 01:16:06.840]   but do you think there's a general principle
[01:16:06.840 --> 01:16:08.780]   that's really powerful here?
[01:16:08.780 --> 01:16:13.360]   - Yeah, so a distinction between transformers and LSTMs,
[01:16:13.360 --> 01:16:15.360]   which were what came before,
[01:16:15.360 --> 01:16:17.840]   and there was a transitional period
[01:16:17.840 --> 01:16:19.680]   where you could use both.
[01:16:19.680 --> 01:16:22.000]   In fact, when we talked about AlphaStar,
[01:16:22.000 --> 01:16:24.280]   we used transformers and LSTMs.
[01:16:24.280 --> 01:16:26.380]   So it was still the beginning of transformers.
[01:16:26.380 --> 01:16:27.400]   They were very powerful,
[01:16:27.400 --> 01:16:31.520]   but LSTMs were still also very powerful sequence models.
[01:16:31.520 --> 01:16:36.520]   So the power of the transformer is that it has built in
[01:16:36.520 --> 01:16:41.140]   what we call an inductive bias of attention
[01:16:41.140 --> 01:16:43.040]   that makes the model,
[01:16:43.040 --> 01:16:45.700]   when you think of a sequence of integers, right?
[01:16:45.700 --> 01:16:47.440]   Like we discussed this before, right?
[01:16:47.440 --> 01:16:48.960]   This is a sequence of words.
[01:16:50.420 --> 01:16:54.780]   When you have to do very hard tasks over these words,
[01:16:54.780 --> 01:16:57.900]   this could be, we're gonna translate a whole paragraph
[01:16:57.900 --> 01:16:59.780]   or we're gonna predict the next paragraph
[01:16:59.780 --> 01:17:01.740]   given 10 paragraphs before.
[01:17:01.740 --> 01:17:09.260]   There's some loose intuition from how we do it as a human
[01:17:09.260 --> 01:17:14.780]   that is very nicely mimicked and replicated,
[01:17:14.780 --> 01:17:16.540]   structurally speaking in the transformer,
[01:17:16.540 --> 01:17:21.160]   which is this idea of you're looking for something, right?
[01:17:21.160 --> 01:17:23.900]   So you're sort of, when you're,
[01:17:23.900 --> 01:17:25.740]   you just read a piece of text,
[01:17:25.740 --> 01:17:27.920]   now you're thinking what comes next.
[01:17:27.920 --> 01:17:31.780]   You might wanna relook at the text or look it from scratch.
[01:17:31.780 --> 01:17:35.080]   I mean, literally is because there's no recurrence.
[01:17:35.080 --> 01:17:37.300]   You're just thinking what comes next.
[01:17:37.300 --> 01:17:40.020]   And it's almost hypothesis driven, right?
[01:17:40.020 --> 01:17:43.380]   So if I'm thinking the next word that I'll write
[01:17:43.380 --> 01:17:45.560]   is cat or dog, okay?
[01:17:46.560 --> 01:17:49.840]   The way the transformer works almost philosophically
[01:17:49.840 --> 01:17:52.840]   is it has these two hypotheses.
[01:17:52.840 --> 01:17:55.640]   Is it gonna be cat or is it gonna be dog?
[01:17:55.640 --> 01:17:58.360]   And then it thinks, okay, if it's cat,
[01:17:58.360 --> 01:18:00.680]   I'm gonna look for certain words, not necessarily cat,
[01:18:00.680 --> 01:18:02.920]   although cat is an obvious word you would look in the past
[01:18:02.920 --> 01:18:05.960]   to see whether it makes more sense to output cat or dog.
[01:18:05.960 --> 01:18:09.400]   And then it does some very deep computation
[01:18:09.400 --> 01:18:11.400]   over the words and beyond, right?
[01:18:11.400 --> 01:18:14.100]   So it combines the words and,
[01:18:14.100 --> 01:18:18.440]   but it has the query as we call it, that is cat.
[01:18:18.440 --> 01:18:20.600]   And then similarly for dog, right?
[01:18:20.600 --> 01:18:24.360]   And so it's a very computational way to think about,
[01:18:24.360 --> 01:18:26.980]   look, if I'm thinking deeply about text,
[01:18:26.980 --> 01:18:29.560]   I need to go back to look at all of the text,
[01:18:29.560 --> 01:18:31.860]   attend over it, but it's not just attention.
[01:18:31.860 --> 01:18:33.920]   Like what is guiding the attention?
[01:18:33.920 --> 01:18:36.660]   And that was the key insight from an earlier paper
[01:18:36.660 --> 01:18:39.100]   is not how far away is it?
[01:18:39.100 --> 01:18:40.760]   I mean, how far away is it is important?
[01:18:40.760 --> 01:18:42.680]   What did I just write about?
[01:18:42.680 --> 01:18:44.100]   That's critical.
[01:18:44.100 --> 01:18:48.360]   But what you wrote about 10 pages ago might also be critical.
[01:18:48.360 --> 01:18:53.160]   So you're looking not positionally, but content-wise, right?
[01:18:53.160 --> 01:18:56.040]   And transformers have this beautiful way
[01:18:56.040 --> 01:18:59.420]   to query for certain content and pull it out
[01:18:59.420 --> 01:19:00.280]   in a compressed way.
[01:19:00.280 --> 01:19:02.960]   So then you can make a more informed decision.
[01:19:02.960 --> 01:19:05.920]   I mean, that's one way to explain transformers.
[01:19:05.920 --> 01:19:10.000]   But I think it's a very powerful inductive bias.
[01:19:10.000 --> 01:19:12.480]   There might be some details that might change over time,
[01:19:12.480 --> 01:19:16.400]   but I think that is what makes transformers
[01:19:16.400 --> 01:19:19.880]   so much more powerful than the recurrent networks
[01:19:19.880 --> 01:19:22.420]   that were more recency bias-based,
[01:19:22.420 --> 01:19:24.300]   which obviously works in some tasks,
[01:19:24.300 --> 01:19:26.680]   but it has major flaws.
[01:19:26.680 --> 01:19:29.280]   Transformer itself has flaws.
[01:19:29.280 --> 01:19:32.160]   And I think the main one, the main challenge is
[01:19:32.160 --> 01:19:35.720]   these prompts that we just were talking about,
[01:19:35.720 --> 01:19:38.040]   they can be a thousand words long.
[01:19:38.040 --> 01:19:39.880]   But if I'm teaching you StarCraft,
[01:19:39.880 --> 01:19:41.840]   I mean, I'll have to show you videos.
[01:19:41.840 --> 01:19:44.600]   I'll have to point you to whole Wikipedia articles
[01:19:44.600 --> 01:19:46.120]   about the game.
[01:19:46.120 --> 01:19:48.000]   We'll have to interact probably as you play,
[01:19:48.000 --> 01:19:49.480]   you'll ask me questions.
[01:19:49.480 --> 01:19:52.340]   The context required for us to achieve
[01:19:52.340 --> 01:19:54.760]   me being a good teacher to you on the game,
[01:19:54.760 --> 01:19:56.960]   as you would want to do it with a model,
[01:19:56.960 --> 01:20:01.600]   I think goes well beyond the current capabilities.
[01:20:01.600 --> 01:20:03.900]   So the question is, how do we benchmark this?
[01:20:03.900 --> 01:20:06.400]   And then how do we change the structure
[01:20:06.400 --> 01:20:07.280]   of the architectures?
[01:20:07.280 --> 01:20:08.820]   I think there's ideas on both sides,
[01:20:08.820 --> 01:20:11.280]   but we'll have to see empirically, right?
[01:20:11.280 --> 01:20:13.360]   Obviously what ends up working in the--
[01:20:13.360 --> 01:20:15.880]   - And as you talked about, some of the ideas could be,
[01:20:15.880 --> 01:20:19.480]   keeping the constraint of that length in place,
[01:20:19.480 --> 01:20:23.060]   but then forming hierarchical representations
[01:20:23.060 --> 01:20:26.240]   to where you can start being much clever
[01:20:26.240 --> 01:20:28.840]   in how you use those thousand tokens.
[01:20:28.840 --> 01:20:29.680]   - Indeed.
[01:20:29.680 --> 01:20:32.240]   - Yeah, that's really interesting.
[01:20:32.240 --> 01:20:34.840]   But it also is possible that this attentional mechanism
[01:20:34.840 --> 01:20:37.560]   where you basically, you don't have a recency bias,
[01:20:37.560 --> 01:20:40.300]   but you look more generally,
[01:20:40.300 --> 01:20:42.000]   you make it learnable.
[01:20:42.000 --> 01:20:45.280]   The mechanism in which way you look back into the past,
[01:20:45.280 --> 01:20:46.800]   you make that learnable.
[01:20:46.800 --> 01:20:50.200]   It's also possible where at the very beginning of that,
[01:20:50.200 --> 01:20:54.380]   because that, you might become smarter and smarter
[01:20:54.380 --> 01:20:56.920]   in the way you query the past.
[01:20:56.920 --> 01:21:00.600]   So recent past and distant past,
[01:21:00.600 --> 01:21:02.360]   and maybe very, very distant past.
[01:21:02.360 --> 01:21:04.980]   So almost like the attention mechanism
[01:21:04.980 --> 01:21:08.680]   will have to improve and evolve as good as the,
[01:21:09.620 --> 01:21:11.980]   the tokenization mechanism,
[01:21:11.980 --> 01:21:14.980]   where so you can represent long-term memory somehow.
[01:21:14.980 --> 01:21:16.140]   - Yes.
[01:21:16.140 --> 01:21:18.220]   And I mean, hierarchies are very,
[01:21:18.220 --> 01:21:22.180]   I mean, it's a very nice word that sounds appealing.
[01:21:22.180 --> 01:21:25.900]   There's lots of work adding hierarchy to the memories.
[01:21:25.900 --> 01:21:29.460]   In practice, it does seem like we keep coming back
[01:21:29.460 --> 01:21:33.000]   to the main formula or main architecture.
[01:21:33.000 --> 01:21:35.300]   That sometimes tells us something.
[01:21:35.300 --> 01:21:38.540]   There is such a sentence that a friend of mine told me,
[01:21:38.540 --> 01:21:41.040]   like whether it wants to work or not.
[01:21:41.040 --> 01:21:45.000]   So Transformer was clearly an idea that wanted to work.
[01:21:45.000 --> 01:21:47.540]   And then I think there's some principles
[01:21:47.540 --> 01:21:51.040]   we believe will be needed, but finding the exact details,
[01:21:51.040 --> 01:21:52.920]   details matter so much, right?
[01:21:52.920 --> 01:21:54.280]   That's gonna be tricky.
[01:21:54.280 --> 01:21:56.800]   - I love the idea that there's like,
[01:21:56.800 --> 01:22:01.320]   you as a human being, you want some ideas to work,
[01:22:01.320 --> 01:22:04.520]   and then there's the model that wants some ideas to work,
[01:22:04.520 --> 01:22:07.400]   and you get to have a conversation to see which,
[01:22:07.400 --> 01:22:09.600]   - More likely the model will win in the end.
[01:22:09.600 --> 01:22:12.860]   Because it's the one, you don't have to do any work.
[01:22:12.860 --> 01:22:14.380]   The model's the one that has to do the work,
[01:22:14.380 --> 01:22:15.900]   so you should listen to the model.
[01:22:15.900 --> 01:22:17.900]   And I really love this idea that you talked about,
[01:22:17.900 --> 01:22:21.200]   the humans in this picture, if I could just briefly ask.
[01:22:21.200 --> 01:22:25.700]   One is you're saying the benchmarks about,
[01:22:25.700 --> 01:22:27.980]   so the modular humans working on this,
[01:22:27.980 --> 01:22:31.700]   the benchmarks providing a sturdy ground
[01:22:31.700 --> 01:22:34.700]   of a wish to do these things that seem impossible.
[01:22:34.700 --> 01:22:39.140]   They give you, in the darkest of times, give you hope,
[01:22:39.140 --> 01:22:40.940]   because little signs of improvement.
[01:22:40.940 --> 01:22:46.560]   Somehow you're not lost if you have metrics
[01:22:46.560 --> 01:22:48.680]   to measure your improvement.
[01:22:48.680 --> 01:22:52.260]   And then there's other aspect, you said elsewhere,
[01:22:52.260 --> 01:22:56.600]   and here today, titles matter.
[01:22:56.600 --> 01:23:00.520]   I wonder how much humans matter
[01:23:00.520 --> 01:23:02.360]   in the evolution of all of this,
[01:23:02.360 --> 01:23:04.300]   meaning individual humans.
[01:23:04.300 --> 01:23:08.140]   Something about their interaction,
[01:23:08.140 --> 01:23:09.200]   something about their ideas,
[01:23:09.200 --> 01:23:13.180]   how much they change the direction of all of this.
[01:23:13.180 --> 01:23:15.680]   If you change the humans in this picture,
[01:23:15.680 --> 01:23:18.240]   is it that the model is sitting there,
[01:23:18.240 --> 01:23:22.520]   and it wants some idea to work,
[01:23:22.520 --> 01:23:25.600]   or is it the humans, or maybe the model's providing
[01:23:25.600 --> 01:23:27.020]   20 ideas that could work,
[01:23:27.020 --> 01:23:29.100]   and depending on the humans you pick,
[01:23:29.100 --> 01:23:31.800]   they're going to be able to hear some of those ideas.
[01:23:31.800 --> 01:23:34.600]   - In all the, because you're now directing
[01:23:34.600 --> 01:23:35.920]   all of deep learning at DeepMind,
[01:23:35.920 --> 01:23:37.440]   you get to interact with a lot of projects,
[01:23:37.440 --> 01:23:39.000]   a lot of brilliant researchers.
[01:23:39.000 --> 01:23:43.100]   How much variability is created by the humans
[01:23:43.100 --> 01:23:44.160]   in all of this?
[01:23:44.160 --> 01:23:47.380]   - Yeah, I mean, I do believe humans matter a lot,
[01:23:47.380 --> 01:23:52.380]   at the very least, at the time scale of years
[01:23:52.380 --> 01:23:54.880]   on when things are happening,
[01:23:54.880 --> 01:23:56.940]   and what's the sequencing of it, right?
[01:23:56.940 --> 01:24:00.560]   So you get to interact with people that,
[01:24:00.560 --> 01:24:02.240]   I mean, you mentioned this,
[01:24:02.240 --> 01:24:05.160]   some people really want some idea to work,
[01:24:05.160 --> 01:24:06.720]   and they'll persist,
[01:24:06.720 --> 01:24:09.400]   and then some other people might be more practical,
[01:24:09.400 --> 01:24:12.880]   like, I don't care what idea works,
[01:24:12.880 --> 01:24:15.920]   I care about cracking protein folding.
[01:24:15.920 --> 01:24:21.240]   And these, at least these two kind of seem opposite sides,
[01:24:21.240 --> 01:24:25.680]   we need both, and we've clearly had both historically,
[01:24:25.680 --> 01:24:29.000]   and that made certain things happen earlier or later,
[01:24:29.000 --> 01:24:33.480]   so definitely humans involved in all of these endeavor
[01:24:33.480 --> 01:24:38.480]   have had, I would say, years of change or of ordering,
[01:24:38.480 --> 01:24:40.480]   how things have happened,
[01:24:40.480 --> 01:24:41.840]   which breakthroughs came before
[01:24:41.840 --> 01:24:43.300]   which other breakthroughs, and so on,
[01:24:43.300 --> 01:24:45.800]   so certainly that does happen,
[01:24:45.800 --> 01:24:50.600]   and so one other, maybe one other axis of distinction
[01:24:50.600 --> 01:24:52.040]   is what I called,
[01:24:52.040 --> 01:24:54.860]   and this is most commonly used in reinforcement learning,
[01:24:54.860 --> 01:24:57.800]   is the exploration-exploitation trade-off as well,
[01:24:57.800 --> 01:25:00.920]   it's not exactly what I meant, although quite related.
[01:25:00.920 --> 01:25:05.920]   So when you start trying to help others,
[01:25:05.920 --> 01:25:11.480]   like you become a bit more of a mentor
[01:25:11.480 --> 01:25:13.100]   to a large group of people,
[01:25:13.100 --> 01:25:16.380]   be it a project or the deep learning team or something,
[01:25:16.380 --> 01:25:17.460]   or even in the community
[01:25:17.460 --> 01:25:20.800]   when you interact with people in conferences and so on,
[01:25:20.800 --> 01:25:24.920]   you're identifying quickly some things
[01:25:24.920 --> 01:25:27.080]   that are explorative or exploitative,
[01:25:27.080 --> 01:25:30.720]   and it's tempting to try to guide people, obviously,
[01:25:30.720 --> 01:25:33.200]   I mean, that's what makes our experience,
[01:25:33.200 --> 01:25:36.760]   we bring it and we try to shape things, sometimes wrongly,
[01:25:36.760 --> 01:25:39.600]   and there's many times that I've been wrong in the past,
[01:25:39.600 --> 01:25:43.720]   that's great, but it would be wrong
[01:25:43.720 --> 01:25:48.160]   to dismiss any sort of the research styles
[01:25:48.160 --> 01:25:51.280]   that I'm observing, and I often get asked,
[01:25:51.280 --> 01:25:52.720]   "Well, you're in industry, right,
[01:25:52.720 --> 01:25:55.580]   "so we do have access to large compute scale and so on,
[01:25:55.580 --> 01:25:57.380]   "so there's certain kinds of research
[01:25:57.380 --> 01:26:01.680]   "I almost feel like we need to do responsibly and so on,"
[01:26:01.680 --> 01:26:05.200]   but it is, Carmos, we have the particle accelerator here,
[01:26:05.200 --> 01:26:07.520]   so to speak, in physics, so we need to use it,
[01:26:07.520 --> 01:26:08.840]   we need to answer the questions
[01:26:08.840 --> 01:26:10.440]   that we should be answering right now
[01:26:10.440 --> 01:26:12.400]   for the scientific progress.
[01:26:12.400 --> 01:26:15.240]   But then at the same time, I look at many advances,
[01:26:15.240 --> 01:26:19.360]   including attention, which was discovered in Montreal
[01:26:19.360 --> 01:26:22.440]   initially because of lack of compute, right?
[01:26:22.440 --> 01:26:24.960]   So we were working on sequence to sequence
[01:26:24.960 --> 01:26:27.920]   with my friends over at Google Brain at the time,
[01:26:27.920 --> 01:26:30.400]   and we were using, I think, eight GPUs,
[01:26:30.400 --> 01:26:32.480]   which was somehow a lot at the time,
[01:26:32.480 --> 01:26:35.240]   and then I think Montreal was a bit more limited
[01:26:35.240 --> 01:26:37.320]   in the scale, but then they discovered
[01:26:37.320 --> 01:26:39.240]   this content-based attention concept
[01:26:39.240 --> 01:26:43.400]   that then has obviously triggered things like Transformer.
[01:26:43.400 --> 01:26:46.320]   Not everything obviously starts Transformer.
[01:26:46.320 --> 01:26:49.920]   There's always a history that is important to recognize
[01:26:49.920 --> 01:26:53.040]   because then you can make sure that then those
[01:26:53.040 --> 01:26:56.360]   who might feel now, "Well, we don't have so much compute,"
[01:26:56.360 --> 01:27:01.360]   you need to then help them optimize that kind of research
[01:27:01.360 --> 01:27:04.240]   that might actually produce amazing change.
[01:27:04.240 --> 01:27:07.920]   Perhaps it's not as short-term as some of these advancements
[01:27:07.920 --> 01:27:09.720]   or perhaps it's a different timescale,
[01:27:09.720 --> 01:27:13.040]   but the people and the diversity of the field
[01:27:13.040 --> 01:27:15.720]   is quite critical that we maintain it,
[01:27:15.720 --> 01:27:19.040]   and at times, especially mixed a bit with hype
[01:27:19.040 --> 01:27:21.520]   or other things, it's a bit tricky
[01:27:21.520 --> 01:27:24.160]   to be observing maybe too much
[01:27:24.160 --> 01:27:27.760]   of the same thinking across the board,
[01:27:27.760 --> 01:27:30.480]   but the humans definitely are critical,
[01:27:30.480 --> 01:27:33.880]   and I can think of quite a few personal examples
[01:27:33.880 --> 01:27:36.560]   where also someone told me something
[01:27:36.560 --> 01:27:40.240]   that had a huge effect onto some idea,
[01:27:40.240 --> 01:27:43.280]   and then that's why I'm saying at least in terms of ears,
[01:27:43.280 --> 01:27:44.880]   probably some things do happen.
[01:27:44.880 --> 01:27:45.720]   - Yeah, it's fascinating.
[01:27:45.720 --> 01:27:46.560]   - Yeah.
[01:27:46.560 --> 01:27:48.200]   - And it's also fascinating how constraints somehow
[01:27:48.200 --> 01:27:49.840]   are essential for innovation,
[01:27:51.040 --> 01:27:53.400]   and the other thing you mentioned about engineering,
[01:27:53.400 --> 01:27:56.640]   I have a sneaking suspicion, maybe I over,
[01:27:56.640 --> 01:27:59.960]   my love is with engineering,
[01:27:59.960 --> 01:28:04.480]   so I have a sneaking suspicion that all the genius,
[01:28:04.480 --> 01:28:06.280]   a large percentage of the genius
[01:28:06.280 --> 01:28:09.280]   is in the tiny details of engineering,
[01:28:09.280 --> 01:28:14.280]   so I think we like to think the genius is in the big ideas.
[01:28:14.280 --> 01:28:20.160]   I have a sneaking suspicion that,
[01:28:20.160 --> 01:28:22.600]   because I've seen the genius of details,
[01:28:22.600 --> 01:28:24.120]   of engineering details,
[01:28:24.120 --> 01:28:28.760]   make the night and day difference,
[01:28:28.760 --> 01:28:32.120]   and I wonder if those kind of have a ripple effect over time.
[01:28:32.120 --> 01:28:35.520]   So that too, so that's sort of,
[01:28:35.520 --> 01:28:36.840]   taking the engineering perspective,
[01:28:36.840 --> 01:28:39.360]   that sometimes that quiet innovation
[01:28:39.360 --> 01:28:41.720]   at the level of an individual engineer,
[01:28:41.720 --> 01:28:44.600]   or maybe at the small scale of a few engineers,
[01:28:44.600 --> 01:28:46.760]   can make all the difference, that scales,
[01:28:46.760 --> 01:28:49.680]   because we're doing, we're working on computers
[01:28:49.680 --> 01:28:53.440]   that are scaled across large groups,
[01:28:53.440 --> 01:28:56.960]   that one engineering decision can lead to ripple effects.
[01:28:56.960 --> 01:28:57.800]   - Yes. - Which is interesting
[01:28:57.800 --> 01:28:58.920]   to think about.
[01:28:58.920 --> 01:29:00.760]   - Yeah, I mean, engineering,
[01:29:00.760 --> 01:29:04.160]   there's also kind of a historical,
[01:29:04.160 --> 01:29:06.280]   it might be a bit random,
[01:29:06.280 --> 01:29:09.760]   because if you think of the history of how,
[01:29:09.760 --> 01:29:12.320]   especially deep learning and neural networks took off,
[01:29:12.320 --> 01:29:15.000]   feels like a bit random,
[01:29:15.000 --> 01:29:17.800]   because GPUs happen to be there at the right time
[01:29:17.800 --> 01:29:20.640]   for a different purpose, which was to play video games.
[01:29:20.640 --> 01:29:24.600]   So even the engineering that goes into the hardware,
[01:29:24.600 --> 01:29:26.320]   and it might have a time,
[01:29:26.320 --> 01:29:28.000]   like the timeframe might be very different.
[01:29:28.000 --> 01:29:31.560]   I mean, the GPUs were evolved throughout many years,
[01:29:31.560 --> 01:29:33.840]   where we didn't even, we're looking at that, right?
[01:29:33.840 --> 01:29:37.480]   So even at that level, right, that revolution, so to speak,
[01:29:37.480 --> 01:29:42.160]   the ripples are like, we'll see when they stop, right?
[01:29:42.160 --> 01:29:45.920]   But in terms of thinking of why is this happening, right?
[01:29:45.920 --> 01:29:49.760]   There's, I think that when I try to categorize it
[01:29:49.760 --> 01:29:52.720]   in sort of things that might not be so obvious,
[01:29:52.720 --> 01:29:54.960]   I mean, clearly there's a hardware revolution.
[01:29:54.960 --> 01:29:58.360]   We are surfing thanks to that.
[01:29:58.360 --> 01:29:59.760]   Data centers as well.
[01:29:59.760 --> 01:30:01.840]   I mean, data centers are where,
[01:30:01.840 --> 01:30:03.200]   like, I mean, at Google, for instance,
[01:30:03.200 --> 01:30:04.800]   obviously they're serving Google,
[01:30:04.800 --> 01:30:06.920]   but there's also now, thanks to that,
[01:30:06.920 --> 01:30:09.640]   and to have built such amazing data centers,
[01:30:09.640 --> 01:30:11.720]   we can train these models.
[01:30:11.720 --> 01:30:13.400]   Software is an important one.
[01:30:13.400 --> 01:30:18.280]   I think if I look at the state of how I had to implement
[01:30:18.280 --> 01:30:20.040]   things to implement my ideas,
[01:30:20.040 --> 01:30:22.120]   how I discarded ideas because they were too hard
[01:30:22.120 --> 01:30:25.280]   to implement, yeah, clearly the times have changed,
[01:30:25.280 --> 01:30:27.600]   and thankfully we are in a much better
[01:30:27.600 --> 01:30:29.400]   software position as well.
[01:30:29.400 --> 01:30:32.240]   And then, I mean, obviously there's research
[01:30:32.240 --> 01:30:35.160]   that happens at scale and more people enter the field.
[01:30:35.160 --> 01:30:36.000]   That's great to see,
[01:30:36.000 --> 01:30:38.280]   but it's almost enabled by these other things.
[01:30:38.280 --> 01:30:40.600]   And last but not least is also data, right?
[01:30:40.600 --> 01:30:43.120]   Curating data sets, labeling data sets,
[01:30:43.120 --> 01:30:44.960]   these benchmarks we think about,
[01:30:44.960 --> 01:30:48.920]   maybe we'll want to have all the benchmarks in one system,
[01:30:48.920 --> 01:30:51.320]   but it's still very valuable that someone
[01:30:51.320 --> 01:30:53.600]   put the thought and the time and the vision
[01:30:53.600 --> 01:30:54.880]   to build certain benchmarks.
[01:30:54.880 --> 01:30:56.640]   We've seen progress thanks to,
[01:30:56.640 --> 01:30:59.280]   but we're gonna repurpose the benchmarks.
[01:30:59.280 --> 01:31:01.640]   That's the beauty of Atari,
[01:31:01.640 --> 01:31:04.240]   is like we solved it in a way,
[01:31:04.240 --> 01:31:06.000]   but we use it in Gato.
[01:31:06.000 --> 01:31:09.120]   It was critical, and I'm sure there's still a lot more
[01:31:09.120 --> 01:31:10.960]   to do thanks to that amazing benchmark
[01:31:10.960 --> 01:31:13.120]   that someone took the time to put,
[01:31:13.120 --> 01:31:15.160]   even though at the time maybe,
[01:31:15.160 --> 01:31:17.360]   oh, you have to think what's the next,
[01:31:17.360 --> 01:31:19.440]   you know, iteration of architectures.
[01:31:19.440 --> 01:31:21.400]   That's what maybe the field recognizes,
[01:31:21.400 --> 01:31:24.000]   but we need to, that's another thing we need to balance
[01:31:24.000 --> 01:31:25.800]   in terms of a human's behind.
[01:31:25.800 --> 01:31:27.960]   We need to recognize all these aspects
[01:31:27.960 --> 01:31:29.480]   because they're all critical.
[01:31:29.480 --> 01:31:32.800]   And we tend to, yeah, we tend to think of the genius,
[01:31:32.800 --> 01:31:35.680]   the scientist and so on, but I'm glad you're,
[01:31:35.680 --> 01:31:38.000]   I know you have a strong engineering background, so.
[01:31:38.000 --> 01:31:40.040]   - But also I'm a lover of data,
[01:31:40.040 --> 01:31:43.240]   and it gives us a pushback on the engineering comment,
[01:31:43.240 --> 01:31:46.080]   ultimately could be the creators of benchmarks
[01:31:46.080 --> 01:31:47.440]   who have the most impact.
[01:31:47.440 --> 01:31:49.200]   Andrej Karpathy, who you mentioned,
[01:31:49.200 --> 01:31:52.000]   has recently been talking a lot of trash about ImageNet,
[01:31:52.000 --> 01:31:53.200]   which he has the right to do
[01:31:53.200 --> 01:31:55.480]   because of how critical he is about,
[01:31:55.480 --> 01:31:57.760]   how essential he is to the development
[01:31:57.760 --> 01:32:01.520]   and the success of deep learning around ImageNet.
[01:32:01.520 --> 01:32:02.920]   And you're saying that that's actually,
[01:32:02.920 --> 01:32:05.480]   that benchmark is holding back the field
[01:32:05.480 --> 01:32:07.680]   because, I mean, especially in his context
[01:32:07.680 --> 01:32:11.080]   on Tesla Autopilot, that's looking at real world behavior
[01:32:11.080 --> 01:32:16.080]   of a system, it's, there's something fundamentally missing
[01:32:16.080 --> 01:32:17.960]   about ImageNet that doesn't capture
[01:32:17.960 --> 01:32:20.440]   the real worldness of things,
[01:32:20.440 --> 01:32:22.640]   that we need to have data sets, benchmarks
[01:32:22.640 --> 01:32:27.080]   that have the unpredictability, the edge cases,
[01:32:27.080 --> 01:32:29.680]   the whatever the heck it is that makes the real world
[01:32:29.680 --> 01:32:32.280]   so difficult to operate in,
[01:32:32.280 --> 01:32:34.680]   we need to have benchmarks with that, so.
[01:32:34.680 --> 01:32:37.760]   But just to think about the impact of ImageNet
[01:32:37.760 --> 01:32:42.120]   as a benchmark, and that really puts a lot of emphasis
[01:32:42.120 --> 01:32:43.720]   on the importance of a benchmark,
[01:32:43.720 --> 01:32:46.680]   both sort of internally a deep mind and as a community.
[01:32:46.680 --> 01:32:48.960]   So one is coming in from within,
[01:32:48.960 --> 01:32:52.520]   like how do I create a benchmark for me
[01:32:52.520 --> 01:32:57.280]   to mark and make progress, and how do I make benchmark
[01:32:57.280 --> 01:33:02.280]   for the community to mark and push progress?
[01:33:02.520 --> 01:33:05.880]   - You have this amazing paper you co-authored,
[01:33:05.880 --> 01:33:08.600]   a survey paper called Emergent Abilities
[01:33:08.600 --> 01:33:11.440]   of Large Language Models, it has, again,
[01:33:11.440 --> 01:33:14.480]   the philosophy here that I'd love to ask you about.
[01:33:14.480 --> 01:33:16.680]   What's the intuition about the phenomena
[01:33:16.680 --> 01:33:18.480]   of emergence in neural networks,
[01:33:18.480 --> 01:33:20.660]   transform as language models?
[01:33:20.660 --> 01:33:24.160]   Is there a magic threshold beyond which
[01:33:24.160 --> 01:33:27.160]   we start to see certain performance?
[01:33:27.160 --> 01:33:29.960]   And is that different from task to task?
[01:33:29.960 --> 01:33:32.640]   Is that us humans just being poetic and romantic,
[01:33:32.640 --> 01:33:35.440]   or is there literally some level
[01:33:35.440 --> 01:33:38.200]   at which we start to see breakthrough performance?
[01:33:38.200 --> 01:33:41.520]   - Yeah, I mean, this is a property that we start seeing
[01:33:41.520 --> 01:33:46.880]   in systems that actually tend to be,
[01:33:46.880 --> 01:33:49.280]   so in machine learning, traditionally,
[01:33:49.280 --> 01:33:51.960]   again, going to benchmarks, I mean,
[01:33:51.960 --> 01:33:54.860]   if you have some input-output, right,
[01:33:54.860 --> 01:33:58.280]   like that is just a single input and a single output,
[01:33:58.280 --> 01:34:01.200]   you generally, when you train these systems,
[01:34:01.200 --> 01:34:04.420]   you see reasonably smooth curves
[01:34:04.420 --> 01:34:09.420]   when you analyze how much the data set size
[01:34:09.420 --> 01:34:12.020]   affects the performance, or how the model size
[01:34:12.020 --> 01:34:15.080]   affects the performance, or how much you long train,
[01:34:15.080 --> 01:34:17.920]   how long you train the system for
[01:34:17.920 --> 01:34:19.360]   affects the performance, right?
[01:34:19.360 --> 01:34:22.080]   So, you know, if we think of ImageNet,
[01:34:22.080 --> 01:34:25.080]   like the training curves look fairly smooth
[01:34:25.080 --> 01:34:26.660]   and predictable in a way,
[01:34:28.160 --> 01:34:31.360]   and I would say that's probably because of the,
[01:34:31.360 --> 01:34:36.360]   it's kind of a one-hop reasoning task, right?
[01:34:36.360 --> 01:34:38.240]   It's like, here is an input,
[01:34:38.240 --> 01:34:40.800]   and you think for a few milliseconds,
[01:34:40.800 --> 01:34:43.760]   or 100 milliseconds, 300, as a human,
[01:34:43.760 --> 01:34:44.840]   and then you tell me, yeah,
[01:34:44.840 --> 01:34:47.880]   there's an alpaca in this image.
[01:34:47.880 --> 01:34:52.800]   So, in language, we are seeing benchmarks
[01:34:52.800 --> 01:34:57.800]   that require more pondering and more thought in a way, right?
[01:34:58.240 --> 01:35:01.960]   This is just kind of, you need to look for some subtleties,
[01:35:01.960 --> 01:35:05.400]   that it involves inputs that you might think of,
[01:35:05.400 --> 01:35:07.860]   or even if the input is a sentence
[01:35:07.860 --> 01:35:09.800]   describing a mathematical problem,
[01:35:09.800 --> 01:35:14.180]   there is a bit more processing required as a human
[01:35:14.180 --> 01:35:15.700]   and more introspection.
[01:35:15.700 --> 01:35:20.520]   So, I think that how these benchmarks work
[01:35:20.520 --> 01:35:23.520]   means that there is actually a threshold,
[01:35:23.520 --> 01:35:26.760]   just going back to how transformers work
[01:35:26.760 --> 01:35:29.560]   in this way of querying for the right questions
[01:35:29.560 --> 01:35:31.160]   to get the right answers,
[01:35:31.160 --> 01:35:35.520]   that might mean that performance becomes random
[01:35:35.520 --> 01:35:37.800]   until the right question is asked
[01:35:37.800 --> 01:35:40.080]   by the querying system of a transformer
[01:35:40.080 --> 01:35:42.880]   or of a language model like a transformer,
[01:35:42.880 --> 01:35:47.720]   and then, only then, you might start seeing performance
[01:35:47.720 --> 01:35:50.120]   going from random to non-random,
[01:35:50.120 --> 01:35:52.720]   and this is more empirical.
[01:35:52.720 --> 01:35:56.320]   There's no formalism or theory behind this yet,
[01:35:56.320 --> 01:35:57.800]   although it might be quite important,
[01:35:57.800 --> 01:36:00.360]   but we're seeing these phase transitions
[01:36:00.360 --> 01:36:03.680]   of random performance until some, let's say,
[01:36:03.680 --> 01:36:06.800]   scale of a model, and then it goes beyond that.
[01:36:06.800 --> 01:36:10.560]   And it might be that you need to fit
[01:36:10.560 --> 01:36:14.040]   a few low-order bits of thought
[01:36:14.040 --> 01:36:17.200]   before you can make progress on the whole task.
[01:36:17.200 --> 01:36:19.720]   And if you could measure, actually,
[01:36:19.720 --> 01:36:21.880]   those breakdown of the task,
[01:36:21.880 --> 01:36:23.480]   maybe you would see more smooth,
[01:36:23.480 --> 01:36:24.960]   oh, like, yeah, this, you know,
[01:36:24.960 --> 01:36:27.760]   once you get this and this and this and this and this,
[01:36:27.760 --> 01:36:30.320]   then you start making progress in the task.
[01:36:30.320 --> 01:36:33.520]   But it's somehow a bit annoying
[01:36:33.520 --> 01:36:37.480]   because then it means that certain questions
[01:36:37.480 --> 01:36:40.320]   we might ask about architectures
[01:36:40.320 --> 01:36:43.040]   possibly can only be done at certain scale.
[01:36:43.040 --> 01:36:46.120]   And one thing that, conversely,
[01:36:46.120 --> 01:36:49.200]   I've seen great progress on in the last couple of years
[01:36:49.200 --> 01:36:52.480]   is this notion of science of deep learning
[01:36:52.480 --> 01:36:55.040]   and science of scale in particular, right?
[01:36:55.040 --> 01:36:58.680]   So, on the negative is that there's some benchmarks
[01:36:58.680 --> 01:37:01.800]   for which progress might need to be measured
[01:37:01.800 --> 01:37:04.000]   at minimum and at certain scale
[01:37:04.000 --> 01:37:07.560]   until you see then what details of the model matter
[01:37:07.560 --> 01:37:10.000]   to make that performance better, right?
[01:37:10.000 --> 01:37:11.920]   So that's a bit of a con.
[01:37:11.920 --> 01:37:14.720]   But what we've also seen is that
[01:37:14.720 --> 01:37:18.600]   you can sort of empirically analyze
[01:37:18.600 --> 01:37:22.880]   behavior of models at scales that are smaller, right?
[01:37:22.880 --> 01:37:25.680]   So let's say, to put an example,
[01:37:25.680 --> 01:37:27.840]   we had this chinchilla paper
[01:37:27.840 --> 01:37:31.360]   that revised the so-called scaling laws of models.
[01:37:31.360 --> 01:37:34.720]   And that whole study is done at a reasonably small scale,
[01:37:34.720 --> 01:37:36.520]   right, that may be hundreds of millions
[01:37:36.520 --> 01:37:38.680]   up to 1 billion parameters.
[01:37:38.680 --> 01:37:41.880]   And then the cool thing is that you create some loss, right?
[01:37:41.880 --> 01:37:43.640]   Some loss that, some trends, right?
[01:37:43.640 --> 01:37:46.600]   You extract trends from data that you see, okay,
[01:37:46.600 --> 01:37:49.400]   like it looks like the amount of data required
[01:37:49.400 --> 01:37:52.120]   to train now a 10X larger model would be this.
[01:37:52.120 --> 01:37:53.960]   And these loss so far,
[01:37:53.960 --> 01:37:57.480]   these extrapolations have helped us safe compute
[01:37:57.480 --> 01:38:00.920]   and just get to a better place in terms of the science
[01:38:00.920 --> 01:38:03.800]   of how should we run these models at scale,
[01:38:03.800 --> 01:38:05.600]   how much data, how much depth,
[01:38:05.600 --> 01:38:08.480]   and all sorts of questions we start asking,
[01:38:08.480 --> 01:38:10.600]   extrapolating from a small scale.
[01:38:10.600 --> 01:38:12.720]   But then this emergence is sadly
[01:38:12.720 --> 01:38:15.680]   that not everything can be extrapolated from scale
[01:38:15.680 --> 01:38:16.880]   depending on the benchmark.
[01:38:16.880 --> 01:38:20.240]   And maybe the harder benchmarks are not so good
[01:38:20.240 --> 01:38:21.960]   for extracting these loss.
[01:38:21.960 --> 01:38:24.160]   But we have a variety of benchmarks at least.
[01:38:24.160 --> 01:38:28.000]   - So I wonder to which degree the threshold,
[01:38:28.000 --> 01:38:31.680]   the phase shift scale is a function of the benchmark.
[01:38:31.680 --> 01:38:37.840]   Some of the science of scale might be engineering benchmarks
[01:38:37.840 --> 01:38:40.400]   where that threshold is low.
[01:38:40.400 --> 01:38:43.840]   Sort of taking a main benchmark
[01:38:43.840 --> 01:38:46.120]   and reducing it somehow
[01:38:46.120 --> 01:38:48.480]   where the essential difficulty is left,
[01:38:48.480 --> 01:38:52.600]   but the scale at which the emergence happens is lower.
[01:38:52.600 --> 01:38:54.280]   Just for the science aspect of it
[01:38:54.280 --> 01:38:56.960]   versus the actual real world aspect.
[01:38:56.960 --> 01:38:59.280]   - Yeah, so luckily we have quite a few benchmarks,
[01:38:59.280 --> 01:39:00.560]   some of which are simpler,
[01:39:00.560 --> 01:39:01.880]   or maybe they're more like,
[01:39:01.880 --> 01:39:03.840]   I think people might call these systems one
[01:39:03.840 --> 01:39:05.920]   versus systems two style.
[01:39:05.920 --> 01:39:09.880]   So I think what we're not seeing luckily
[01:39:09.880 --> 01:39:14.040]   is that extrapolations from maybe slightly more smooth
[01:39:14.040 --> 01:39:18.560]   or simpler benchmarks are translating to the harder ones.
[01:39:18.560 --> 01:39:19.640]   But that is not to say
[01:39:19.640 --> 01:39:22.600]   that this extrapolation will hit its limits.
[01:39:22.600 --> 01:39:24.200]   And when it does,
[01:39:24.200 --> 01:39:27.560]   then how much we scale or how we scale
[01:39:27.560 --> 01:39:29.440]   will sadly be a bit suboptimal
[01:39:29.440 --> 01:39:31.800]   until we find better loss, right?
[01:39:31.800 --> 01:39:33.800]   And these laws, again, are very empirical laws.
[01:39:33.800 --> 01:39:35.920]   They're not like physical laws of models.
[01:39:35.920 --> 01:39:38.680]   Although I wish there would be better theory
[01:39:38.680 --> 01:39:40.120]   about these things as well,
[01:39:40.120 --> 01:39:43.000]   but so far I would say empirical theory,
[01:39:43.000 --> 01:39:44.520]   as I call it, is way ahead
[01:39:44.520 --> 01:39:47.000]   than actual theory of machine learning.
[01:39:47.000 --> 01:39:50.480]   - Let me ask you almost for fun.
[01:39:50.480 --> 01:39:52.080]   So this is not, Oriol,
[01:39:52.080 --> 01:39:54.640]   as a deep mind person
[01:39:54.640 --> 01:39:57.280]   or anything to do with deep mind or Google,
[01:39:57.280 --> 01:39:58.840]   just as a human being,
[01:39:58.840 --> 01:40:01.760]   and looking at these news of a Google engineer
[01:40:01.760 --> 01:40:05.800]   who claimed that,
[01:40:05.800 --> 01:40:08.360]   I guess the Lambda language model
[01:40:08.360 --> 01:40:11.120]   was sentient or had the,
[01:40:11.120 --> 01:40:14.080]   and you still need to look into the details of this,
[01:40:14.080 --> 01:40:18.680]   but sort of making an official report
[01:40:18.680 --> 01:40:21.740]   and a claim that he believes there's evidence
[01:40:21.740 --> 01:40:25.120]   that this system has achieved sentience.
[01:40:25.120 --> 01:40:29.560]   And I think this is a really interesting case
[01:40:29.560 --> 01:40:31.760]   on a human level, on a psychological level,
[01:40:31.760 --> 01:40:35.920]   on a technical machine learning level
[01:40:35.920 --> 01:40:38.360]   of how language models transform our world,
[01:40:38.360 --> 01:40:39.880]   and also just philosophical level
[01:40:39.880 --> 01:40:44.120]   of the role of AI systems in a human world.
[01:40:44.120 --> 01:40:48.120]   So what do you find interesting?
[01:40:48.120 --> 01:40:49.720]   What's your take on all of this
[01:40:49.720 --> 01:40:52.440]   as a machine learning engineer and a researcher
[01:40:52.440 --> 01:40:54.320]   and also as a human being?
[01:40:54.320 --> 01:40:56.400]   - Yeah, I mean, a few reactions.
[01:40:56.400 --> 01:40:58.760]   Quite a few, actually.
[01:40:58.760 --> 01:41:01.640]   - Have you ever briefly thought,
[01:41:01.640 --> 01:41:02.560]   is this thing sentient?
[01:41:02.560 --> 01:41:04.320]   - Right, so never.
[01:41:04.320 --> 01:41:05.160]   Absolutely never.
[01:41:05.160 --> 01:41:06.280]   - You mean with like AlphaStar?
[01:41:06.280 --> 01:41:07.120]   Wait a minute.
[01:41:07.120 --> 01:41:11.960]   - Sadly, though, I think, yeah, sadly I have not.
[01:41:11.960 --> 01:41:15.320]   Yeah, I think the current, any of the current models,
[01:41:15.320 --> 01:41:17.560]   although very useful and very good,
[01:41:17.560 --> 01:41:21.200]   yeah, I think we're quite far from that.
[01:41:21.200 --> 01:41:25.360]   And there's kind of a converse side story.
[01:41:25.360 --> 01:41:30.360]   So one of my passions is about science in general.
[01:41:30.360 --> 01:41:34.540]   And I think I feel I'm a bit of a failed scientist.
[01:41:34.540 --> 01:41:36.560]   That's why I came to machine learning,
[01:41:36.560 --> 01:41:40.160]   because you always feel, and you start seeing this,
[01:41:40.160 --> 01:41:43.200]   that machine learning is maybe the science
[01:41:43.200 --> 01:41:45.440]   that can help other sciences, as we've seen, right?
[01:41:45.440 --> 01:41:48.620]   Like you, you know, it's such a powerful tool.
[01:41:48.620 --> 01:41:52.520]   So thanks to that angle, right, that, okay, I love science.
[01:41:52.520 --> 01:41:54.960]   I love, I mean, I love astronomy, I love biology,
[01:41:54.960 --> 01:41:56.960]   but I'm not an expert and I decided,
[01:41:56.960 --> 01:42:00.040]   well, the thing I can do better at is computers.
[01:42:00.040 --> 01:42:02.960]   But having, especially with,
[01:42:02.960 --> 01:42:05.560]   when I was a bit more involved in AlphaFold,
[01:42:05.560 --> 01:42:08.800]   learning a bit about proteins and about biology
[01:42:08.800 --> 01:42:13.120]   and about life, the complexity,
[01:42:13.120 --> 01:42:15.040]   it feels like it really is, like, I mean,
[01:42:15.040 --> 01:42:18.160]   if you start looking at the things that are going on
[01:42:18.160 --> 01:42:23.880]   at the atomic level, and also, I mean,
[01:42:23.880 --> 01:42:27.720]   there's obviously the, we are maybe inclined
[01:42:27.720 --> 01:42:30.440]   to try to think of neural networks as like the brain,
[01:42:30.440 --> 01:42:35.080]   but the complexities and the amount of magic that it feels
[01:42:35.080 --> 01:42:37.120]   when, I mean, I don't, I'm not an expert,
[01:42:37.120 --> 01:42:38.600]   so it naturally feels more magic,
[01:42:38.600 --> 01:42:40.920]   but looking at biological systems,
[01:42:40.920 --> 01:42:45.540]   as opposed to these computer computational brains,
[01:42:45.540 --> 01:42:49.600]   just makes me like, wow, there's such level
[01:42:49.600 --> 01:42:51.480]   of complexity difference still, right?
[01:42:51.480 --> 01:42:53.820]   Like orders of magnitude complexity that,
[01:42:53.820 --> 01:42:56.680]   sure, these weights, I mean, we train them
[01:42:56.680 --> 01:43:00.160]   and they do nice things, but they're not at the level
[01:43:00.160 --> 01:43:05.160]   of biological entities, brains, cells.
[01:43:05.160 --> 01:43:08.960]   It just feels like it's just not possible
[01:43:08.960 --> 01:43:12.360]   to achieve the same level of complexity behavior,
[01:43:12.360 --> 01:43:16.280]   and my belief, when I talk to other beings,
[01:43:16.280 --> 01:43:20.340]   is certainly shaped by this amazement of biology
[01:43:20.340 --> 01:43:22.340]   that maybe because I know too much,
[01:43:22.340 --> 01:43:23.760]   I don't have about machine learning,
[01:43:23.760 --> 01:43:27.600]   but I certainly feel it's very far-fetched
[01:43:27.600 --> 01:43:29.780]   and far in the future to be calling,
[01:43:29.780 --> 01:43:34.560]   or to be thinking, well, this mathematical function
[01:43:34.560 --> 01:43:39.200]   that is differentiable is in fact sentient and so on.
[01:43:39.200 --> 01:43:41.980]   - There's something on that point, it's very interesting.
[01:43:41.980 --> 01:43:46.980]   So you know enough about machines and enough about biology
[01:43:46.980 --> 01:43:49.040]   to know that there's many orders of magnitude
[01:43:49.040 --> 01:43:50.620]   of difference and complexity,
[01:43:50.620 --> 01:43:56.060]   but you know how machine learning works.
[01:43:56.060 --> 01:43:58.140]   So the interesting question for human beings
[01:43:58.140 --> 01:43:59.400]   that are interacting with a system
[01:43:59.400 --> 01:44:02.240]   that don't know about the underlying complexity,
[01:44:02.240 --> 01:44:05.240]   and I've seen people, probably including myself,
[01:44:05.240 --> 01:44:07.920]   that have fallen in love with things that are quite simple.
[01:44:07.920 --> 01:44:09.440]   - Yeah, so-- - And so maybe
[01:44:09.440 --> 01:44:11.500]   the complexity is one part of the picture,
[01:44:11.500 --> 01:44:16.500]   but maybe that's not a necessary condition for sentience,
[01:44:16.500 --> 01:44:23.840]   for perception or emulation of sentience.
[01:44:25.000 --> 01:44:28.180]   - Right, so I mean, I guess the other side of this is,
[01:44:28.180 --> 01:44:29.560]   that's how I feel personally,
[01:44:29.560 --> 01:44:32.360]   I mean, you asked me about the person, right?
[01:44:32.360 --> 01:44:33.980]   Now it's very interesting to see
[01:44:33.980 --> 01:44:36.360]   how other humans feel about things, right?
[01:44:36.360 --> 01:44:40.800]   This is, we are like, again, like I'm not as amazed
[01:44:40.800 --> 01:44:42.320]   about things that I feel,
[01:44:42.320 --> 01:44:44.560]   this is not as magical as this other thing,
[01:44:44.560 --> 01:44:48.000]   because of maybe how I got to learn about it
[01:44:48.000 --> 01:44:50.480]   and how I see the curve a bit more smooth,
[01:44:50.480 --> 01:44:53.080]   because I, you know, like just seeing the progress
[01:44:53.080 --> 01:44:56.000]   of language models since Shannon in the '50s,
[01:44:56.000 --> 01:44:58.900]   and actually looking at that timescale,
[01:44:58.900 --> 01:45:00.840]   we're not that fast progress, right?
[01:45:00.840 --> 01:45:03.460]   I mean, what we were thinking at the time,
[01:45:03.460 --> 01:45:05.960]   like almost 100 years ago,
[01:45:05.960 --> 01:45:08.920]   is not that dissimilar to what we're doing now,
[01:45:08.920 --> 01:45:11.440]   but at the same time, yeah, obviously others,
[01:45:11.440 --> 01:45:14.500]   my experience, right, the personal experience,
[01:45:14.500 --> 01:45:17.360]   I think no one should, you know,
[01:45:17.360 --> 01:45:20.680]   I think no one should tell others how they should feel,
[01:45:20.680 --> 01:45:22.940]   I mean, the feelings are very personal, right?
[01:45:22.940 --> 01:45:26.120]   So how others might feel about the models and so on,
[01:45:26.120 --> 01:45:28.480]   that's one part of the story that is important
[01:45:28.480 --> 01:45:32.040]   to understand for me personally as a researcher,
[01:45:32.040 --> 01:45:36.160]   and then when I maybe disagree or I don't understand
[01:45:36.160 --> 01:45:38.560]   or see that, yeah, maybe this is not something
[01:45:38.560 --> 01:45:41.580]   I think right now is reasonable, knowing all that I know,
[01:45:41.580 --> 01:45:44.320]   one of the other things, and perhaps partly
[01:45:44.320 --> 01:45:46.600]   why it's great to be talking to you
[01:45:46.600 --> 01:45:49.860]   and reaching out to the world about machine learning is,
[01:45:49.860 --> 01:45:53.480]   hey, let's demystify a bit the magic
[01:45:53.480 --> 01:45:56.280]   and try to see a bit more of the math
[01:45:56.280 --> 01:45:59.920]   and the fact that literally to create these models,
[01:45:59.920 --> 01:46:03.160]   if we had the right software, it would be 10 lines of code
[01:46:03.160 --> 01:46:06.160]   and then just a dump of the internet,
[01:46:06.160 --> 01:46:10.320]   so versus like then the complexity of like the creation
[01:46:10.320 --> 01:46:13.640]   of humans from their inception, right,
[01:46:13.640 --> 01:46:15.820]   and also the complexity of evolution
[01:46:15.820 --> 01:46:19.240]   of the whole universe to where we are
[01:46:19.240 --> 01:46:21.960]   that feels orders of magnitude more complex
[01:46:21.960 --> 01:46:23.500]   and fascinating to me.
[01:46:23.500 --> 01:46:26.040]   So I think, yeah, maybe part of,
[01:46:26.040 --> 01:46:29.300]   the only thing I'm thinking about trying to tell you is,
[01:46:29.300 --> 01:46:32.640]   yeah, I think explaining a bit of the magic,
[01:46:32.640 --> 01:46:34.840]   there is a bit of magic, it's good to be in love,
[01:46:34.840 --> 01:46:37.040]   obviously, with what you do at work,
[01:46:37.040 --> 01:46:39.440]   and I'm certainly fascinated and surprised
[01:46:39.440 --> 01:46:43.200]   quite often as well, but I think hopefully,
[01:46:43.200 --> 01:46:45.900]   as experts in biology, hopefully you will tell me
[01:46:45.900 --> 01:46:48.720]   this is not as magic, and I'm happy to learn that.
[01:46:49.440 --> 01:46:52.280]   Through interactions with the larger community,
[01:46:52.280 --> 01:46:56.020]   we can also have a certain level of education
[01:46:56.020 --> 01:46:58.360]   that in practice also will matter,
[01:46:58.360 --> 01:47:00.800]   because I mean, one question is how you feel about this,
[01:47:00.800 --> 01:47:03.080]   but then the other very important is,
[01:47:03.080 --> 01:47:06.960]   you starting to interact with these in products and so on,
[01:47:06.960 --> 01:47:09.160]   it's good to understand a bit what's going on,
[01:47:09.160 --> 01:47:12.280]   what's not going on, what's safe, what's not safe,
[01:47:12.280 --> 01:47:14.840]   and so on, right, otherwise, the technology
[01:47:14.840 --> 01:47:17.040]   will not be used properly for good,
[01:47:17.040 --> 01:47:20.540]   which is obviously the goal of all of us, I hope.
[01:47:20.540 --> 01:47:22.940]   - So let me then ask the next question,
[01:47:22.940 --> 01:47:25.800]   do you think in order to solve intelligence,
[01:47:25.800 --> 01:47:29.560]   or to replace the Lexbot that does interviews,
[01:47:29.560 --> 01:47:31.440]   as we started this conversation with,
[01:47:31.440 --> 01:47:34.840]   do you think the system needs to be sentient?
[01:47:34.840 --> 01:47:37.260]   Do you think it needs to achieve something
[01:47:37.260 --> 01:47:39.780]   like consciousness, and do you think about
[01:47:39.780 --> 01:47:43.260]   what consciousness is in the human mind
[01:47:43.260 --> 01:47:46.760]   that could be instructive for creating AI systems?
[01:47:46.760 --> 01:47:51.040]   - Yeah, honestly, I think probably not
[01:47:51.040 --> 01:47:56.040]   to the degree of intelligence that there's this brain
[01:47:56.040 --> 01:48:00.320]   that can learn, can be extremely useful,
[01:48:00.320 --> 01:48:02.960]   can challenge you, can teach you,
[01:48:02.960 --> 01:48:05.640]   conversely, you can teach it to do things.
[01:48:05.640 --> 01:48:09.120]   I'm not sure it's necessary, personally speaking,
[01:48:09.120 --> 01:48:14.080]   but if consciousness or any other biological
[01:48:14.080 --> 01:48:19.080]   or evolutionary lesson can be repurposed
[01:48:19.080 --> 01:48:22.600]   to then influence our next set of algorithms,
[01:48:22.600 --> 01:48:25.680]   that is a great way to actually make progress, right?
[01:48:25.680 --> 01:48:28.220]   And the same way I tried to explain Transformers a bit,
[01:48:28.220 --> 01:48:33.220]   how it feels we operate when we look at text specifically,
[01:48:33.220 --> 01:48:36.000]   these insights are very important, right?
[01:48:36.000 --> 01:48:40.320]   So there's a distinction between details
[01:48:40.320 --> 01:48:43.260]   of how the brain might be doing computation.
[01:48:43.260 --> 01:48:46.560]   I think my understanding is, sure, there's neurons
[01:48:46.560 --> 01:48:48.520]   and there's some resemblance to neural networks,
[01:48:48.520 --> 01:48:51.440]   but we don't quite understand enough of the brain
[01:48:51.440 --> 01:48:55.320]   in detail, right, to be able to replicate it.
[01:48:55.320 --> 01:48:58.840]   But then more, if you zoom out a bit,
[01:48:58.840 --> 01:49:03.400]   how we then, our thought process, how memory works,
[01:49:03.400 --> 01:49:05.640]   maybe even how evolution got us here,
[01:49:05.640 --> 01:49:07.320]   what's exploration, exploitation,
[01:49:07.320 --> 01:49:08.800]   like how these things happen,
[01:49:08.800 --> 01:49:13.080]   I think these clearly can inform algorithmic level research.
[01:49:13.080 --> 01:49:18.080]   And I've seen some examples of this being quite useful
[01:49:18.080 --> 01:49:19.740]   to then guide the research,
[01:49:19.740 --> 01:49:21.660]   even it might be for the wrong reasons, right?
[01:49:21.660 --> 01:49:26.100]   So I think biology and what we know about ourselves
[01:49:26.100 --> 01:49:29.980]   can help a whole lot to build essentially
[01:49:29.980 --> 01:49:34.140]   what we call AGI, this general, the real gato, right?
[01:49:34.140 --> 01:49:36.540]   The last step of the chain, hopefully.
[01:49:36.540 --> 01:49:39.180]   But consciousness in particular,
[01:49:39.180 --> 01:49:42.060]   I don't myself at least think too hard
[01:49:42.060 --> 01:49:44.800]   about how to add that to the system,
[01:49:44.800 --> 01:49:47.840]   but maybe my understanding is also very personal
[01:49:47.840 --> 01:49:48.840]   about what it means, right?
[01:49:48.840 --> 01:49:51.760]   I think this, even that in itself is a long debate
[01:49:51.760 --> 01:49:55.300]   that I know people have often,
[01:49:55.300 --> 01:49:57.780]   and maybe I should learn more about this.
[01:49:57.780 --> 01:50:01.740]   - Yeah, and I personally, I notice the magic often
[01:50:01.740 --> 01:50:04.940]   on a personal level, especially with physical systems,
[01:50:04.940 --> 01:50:06.160]   like robots.
[01:50:06.160 --> 01:50:10.460]   I have a lot of legged robots now in Austin
[01:50:10.460 --> 01:50:11.700]   that I play with.
[01:50:11.700 --> 01:50:13.260]   And even when you program them,
[01:50:13.260 --> 01:50:15.580]   when they do things you didn't expect,
[01:50:15.580 --> 01:50:18.620]   there's an immediate anthropomorphization,
[01:50:18.620 --> 01:50:19.820]   and you notice the magic,
[01:50:19.820 --> 01:50:22.620]   and you start to think about things like sentience
[01:50:22.620 --> 01:50:26.020]   that has to do more with effective communication
[01:50:26.020 --> 01:50:28.580]   and less with any of these kind of dramatic things.
[01:50:28.580 --> 01:50:32.600]   It seems like a useful part of communication.
[01:50:32.600 --> 01:50:36.580]   Having the perception of consciousness
[01:50:36.580 --> 01:50:38.860]   seems like useful for us humans.
[01:50:38.860 --> 01:50:40.860]   We treat each other more seriously.
[01:50:40.860 --> 01:50:45.060]   We are able to do a nearest neighbor shoving
[01:50:45.060 --> 01:50:47.700]   of that entity into your memory correctly,
[01:50:47.700 --> 01:50:48.700]   all that kind of stuff.
[01:50:48.700 --> 01:50:50.860]   Seems useful, at least to fake it,
[01:50:50.860 --> 01:50:52.500]   even if you never make it.
[01:50:52.500 --> 01:50:55.660]   - So maybe, like, yeah, mirroring the question,
[01:50:55.660 --> 01:50:57.460]   and since you talked to a few people,
[01:50:57.460 --> 01:51:01.780]   then you do think that we'll need to figure something out
[01:51:01.780 --> 01:51:04.580]   in order to achieve intelligence
[01:51:04.580 --> 01:51:06.540]   in a grander sense of the word.
[01:51:06.540 --> 01:51:08.180]   - Yeah, I personally believe yes,
[01:51:08.220 --> 01:51:12.620]   but I don't even think it'll be like a separate island
[01:51:12.620 --> 01:51:14.140]   we'll have to travel to.
[01:51:14.140 --> 01:51:16.420]   I think it'll emerge quite naturally.
[01:51:16.420 --> 01:51:20.140]   - Okay, that's easier for us then, thank you.
[01:51:20.140 --> 01:51:22.820]   - But the reason I think it's important to think about
[01:51:22.820 --> 01:51:26.340]   is you will start, I believe, like with this Google Engineer,
[01:51:26.340 --> 01:51:28.780]   you will start seeing this a lot more,
[01:51:28.780 --> 01:51:30.540]   especially when you have AI systems
[01:51:30.540 --> 01:51:32.980]   that are actually interacting with human beings
[01:51:32.980 --> 01:51:35.180]   that don't have an engineering background,
[01:51:35.180 --> 01:51:37.860]   and we have to prepare for that.
[01:51:38.580 --> 01:51:40.100]   Because there'll be, I do believe
[01:51:40.100 --> 01:51:42.300]   there'll be a civil rights movement for robots,
[01:51:42.300 --> 01:51:44.580]   as silly as it is to say.
[01:51:44.580 --> 01:51:46.780]   There's going to be a large number of people
[01:51:46.780 --> 01:51:48.980]   that realize there's these intelligent entities
[01:51:48.980 --> 01:51:51.620]   with whom I have a deep relationship,
[01:51:51.620 --> 01:51:53.220]   and I don't wanna lose them.
[01:51:53.220 --> 01:51:54.780]   They've come to be a part of my life,
[01:51:54.780 --> 01:51:55.980]   and they mean a lot.
[01:51:55.980 --> 01:51:59.020]   They have a name, they have a story, they have a memory,
[01:51:59.020 --> 01:52:01.340]   and we start to ask questions about ourselves.
[01:52:01.340 --> 01:52:04.940]   Well, what, this thing sure seems like
[01:52:04.940 --> 01:52:07.600]   it's capable of suffering,
[01:52:07.600 --> 01:52:09.860]   because it tells all these stories of suffering.
[01:52:09.860 --> 01:52:11.700]   It doesn't wanna die and all those kinds of things,
[01:52:11.700 --> 01:52:14.460]   and we have to start to ask ourselves questions.
[01:52:14.460 --> 01:52:15.460]   Well, what is the difference
[01:52:15.460 --> 01:52:16.980]   between a human being and this thing?
[01:52:16.980 --> 01:52:18.580]   And so when you engineer,
[01:52:18.580 --> 01:52:21.500]   I believe from an engineering perspective,
[01:52:21.500 --> 01:52:24.980]   from like a deep mind, or anybody that builds systems,
[01:52:24.980 --> 01:52:26.500]   there might be laws in the future
[01:52:26.500 --> 01:52:29.140]   where you're not allowed to engineer systems
[01:52:29.140 --> 01:52:31.240]   with displays of sentience,
[01:52:31.240 --> 01:52:36.020]   unless they're explicitly designed to be that,
[01:52:36.020 --> 01:52:37.380]   unless it's a pet.
[01:52:37.380 --> 01:52:41.260]   So if you have a system that's just doing customer support,
[01:52:41.260 --> 01:52:44.180]   you're legally not allowed to display sentience.
[01:52:44.180 --> 01:52:47.300]   We'll start to ask ourselves that question,
[01:52:47.300 --> 01:52:49.500]   and then so that's going to be part
[01:52:49.500 --> 01:52:51.260]   of the software engineering process.
[01:52:51.260 --> 01:52:53.360]   Which features do we have,
[01:52:53.360 --> 01:52:56.820]   and one of them is communications of sentience.
[01:52:56.820 --> 01:52:58.700]   But it's important to start thinking about that stuff,
[01:52:58.700 --> 01:53:01.740]   especially how much it captivates public attention.
[01:53:01.740 --> 01:53:03.180]   - Yeah, absolutely.
[01:53:03.180 --> 01:53:06.420]   It's definitely a topic that is important.
[01:53:06.420 --> 01:53:09.540]   We think about, and I think in a way,
[01:53:09.540 --> 01:53:14.540]   I always see, not every movie is equally on point
[01:53:14.540 --> 01:53:16.100]   with certain things,
[01:53:16.100 --> 01:53:19.100]   but certainly science fiction in this sense,
[01:53:19.100 --> 01:53:20.740]   at least has prepared society
[01:53:20.740 --> 01:53:24.060]   to start thinking about certain topics
[01:53:24.060 --> 01:53:26.460]   that even if it's too early to talk about,
[01:53:26.460 --> 01:53:29.480]   as long as we are reasonable,
[01:53:29.480 --> 01:53:31.300]   it's certainly gonna prepare us
[01:53:31.300 --> 01:53:34.980]   for both the research to come and how to,
[01:53:34.980 --> 01:53:37.060]   I mean, there's many important challenges
[01:53:37.060 --> 01:53:42.060]   and topics that come with building an intelligent system,
[01:53:42.060 --> 01:53:44.660]   many of which you just mentioned, right?
[01:53:44.660 --> 01:53:49.660]   So I think we're never gonna be fully ready
[01:53:49.660 --> 01:53:51.420]   unless we talk about this,
[01:53:51.420 --> 01:53:54.140]   and we start also, as I said,
[01:53:54.140 --> 01:53:59.140]   just kind of expanding the people we talk to,
[01:53:59.140 --> 01:54:03.180]   to not include only our own researchers and so on.
[01:54:03.180 --> 01:54:06.540]   And in fact, places like DeepMind, but elsewhere,
[01:54:06.540 --> 01:54:10.380]   there's more interdisciplinary groups forming up
[01:54:10.380 --> 01:54:13.260]   to start asking and really working with us
[01:54:13.260 --> 01:54:14.980]   on these questions,
[01:54:14.980 --> 01:54:17.420]   because obviously this is not initially
[01:54:17.420 --> 01:54:19.380]   what your passion is when you do your PhD,
[01:54:19.380 --> 01:54:21.460]   but certainly it is coming, right?
[01:54:21.460 --> 01:54:23.140]   So it's fascinating, kind of.
[01:54:23.140 --> 01:54:27.180]   It's the thing that brings me to one of my passions
[01:54:27.180 --> 01:54:28.020]   that is learning.
[01:54:28.020 --> 01:54:31.740]   So in this sense, this is kind of a new area
[01:54:31.740 --> 01:54:36.660]   that as a learning system myself, I want to keep exploring.
[01:54:36.660 --> 01:54:41.060]   And I think it's great to see parts of the debate,
[01:54:41.060 --> 01:54:43.780]   and even I've seen a level of maturity
[01:54:43.780 --> 01:54:46.500]   in the conferences that deal with AI.
[01:54:46.500 --> 01:54:49.940]   If you look five years ago to now,
[01:54:49.940 --> 01:54:53.100]   just the amount of workshops and so on has changed so much.
[01:54:53.100 --> 01:54:58.100]   It's impressive to see how much topics of safety ethics
[01:54:58.100 --> 01:55:01.700]   and so on come to the surface, which is great.
[01:55:01.700 --> 01:55:03.860]   And if we were too early, clearly it's fine.
[01:55:03.860 --> 01:55:07.300]   I mean, it's a big field and there's lots of people
[01:55:07.300 --> 01:55:10.300]   with lots of interests that will do progress
[01:55:10.300 --> 01:55:11.940]   or make progress.
[01:55:11.940 --> 01:55:14.100]   And obviously I don't believe we're too late.
[01:55:14.100 --> 01:55:16.460]   So in that sense, I think it's great
[01:55:16.460 --> 01:55:18.180]   that we're doing this already.
[01:55:18.180 --> 01:55:20.220]   - It's better to be too early than too late
[01:55:20.220 --> 01:55:22.780]   when it comes to super intelligent AI systems.
[01:55:22.780 --> 01:55:25.500]   Let me ask, speaking of sentient AIs,
[01:55:25.500 --> 01:55:28.700]   you gave props to your friend, Ilyas Eskiver,
[01:55:28.700 --> 01:55:31.980]   for being elected the Fellow of the Royal Society.
[01:55:31.980 --> 01:55:35.140]   So just as a shout out to a fellow researcher and a friend,
[01:55:35.140 --> 01:55:39.420]   what's the secret to the genius of Ilyas Eskiver?
[01:55:39.420 --> 01:55:42.660]   And also, do you believe that his tweets,
[01:55:42.660 --> 01:55:46.020]   as you've hypothesized and Andrei Karpathy did as well,
[01:55:46.020 --> 01:55:48.660]   are generated by a language model?
[01:55:48.660 --> 01:55:49.500]   - Yeah.
[01:55:49.500 --> 01:55:53.820]   So I strongly believe Ilyas is gonna be visiting
[01:55:53.820 --> 01:55:57.580]   in a few weeks actually, so I'll ask him in person.
[01:55:57.580 --> 01:55:58.420]   But-
[01:55:58.420 --> 01:55:59.260]   - Will he tell you the truth?
[01:55:59.260 --> 01:56:00.100]   - Yes, of course.
[01:56:00.100 --> 01:56:00.940]   - Okay, sure. - Hopefully.
[01:56:00.940 --> 01:56:04.060]   I mean, ultimately we all have shared paths
[01:56:04.060 --> 01:56:06.940]   and there's friendships that go beyond,
[01:56:06.940 --> 01:56:09.860]   obviously, institutions and so on.
[01:56:09.860 --> 01:56:11.780]   So I hope he tells me the truth.
[01:56:11.780 --> 01:56:14.420]   - Or maybe the AI system is holding him hostage somehow.
[01:56:14.420 --> 01:56:16.980]   Maybe he has some videos that he doesn't wanna release.
[01:56:16.980 --> 01:56:19.740]   So maybe it has taken control over him,
[01:56:19.740 --> 01:56:20.580]   so he can't tell the truth.
[01:56:20.580 --> 01:56:22.300]   - Well, if I see him in person, then I think I'll-
[01:56:22.300 --> 01:56:23.220]   - He will know.
[01:56:23.220 --> 01:56:27.620]   - Yeah, but I think it's a good,
[01:56:27.620 --> 01:56:30.940]   I think Ilyas' personality, just knowing him for a while,
[01:56:30.940 --> 01:56:35.260]   yeah, he's, everyone in Twitter, I guess,
[01:56:35.260 --> 01:56:36.580]   gets a different persona.
[01:56:36.580 --> 01:56:40.860]   And I think Ilyas' one does not surprise me, right?
[01:56:40.860 --> 01:56:43.540]   So I think knowing Ilyas from before social media
[01:56:43.540 --> 01:56:45.740]   and before AI was so prevalent,
[01:56:45.740 --> 01:56:47.460]   I recognize a lot of his character.
[01:56:47.460 --> 01:56:50.460]   So that's something for me that I feel good about,
[01:56:50.460 --> 01:56:52.420]   a friend that hasn't changed
[01:56:52.420 --> 01:56:55.940]   or like is still true to himself, right?
[01:56:55.940 --> 01:56:58.900]   Obviously, there is though a fact
[01:56:58.900 --> 01:57:02.100]   that your field becomes more popular
[01:57:02.100 --> 01:57:05.420]   and he is obviously one of the main figures in the field,
[01:57:05.420 --> 01:57:06.860]   having done a lot of advancement.
[01:57:06.860 --> 01:57:08.980]   So I think that the tricky bit here
[01:57:08.980 --> 01:57:11.060]   is how to balance your true self
[01:57:11.060 --> 01:57:13.540]   with the responsibility that your words carry.
[01:57:13.540 --> 01:57:16.100]   So in this sense, I think, yeah,
[01:57:16.100 --> 01:57:19.300]   like I appreciate the style and I understand it,
[01:57:19.300 --> 01:57:24.100]   but it created debates on like some of his tweets, right?
[01:57:24.100 --> 01:57:26.780]   That maybe it's good we have them early anyways, right?
[01:57:26.780 --> 01:57:30.980]   But yeah, then the reactions are usually polarizing.
[01:57:30.980 --> 01:57:32.980]   I think we're just seeing kind of the reality
[01:57:32.980 --> 01:57:34.900]   of social media a bit there as well,
[01:57:34.900 --> 01:57:38.060]   reflected on that particular topic
[01:57:38.060 --> 01:57:40.220]   or set of topics he's tweeting about.
[01:57:40.220 --> 01:57:42.860]   - Yeah, I mean, it's funny that you speak to this tension.
[01:57:42.860 --> 01:57:46.100]   He was one of the early seminal figures
[01:57:46.100 --> 01:57:47.260]   in the field of deep learning.
[01:57:47.260 --> 01:57:48.900]   And so there's a responsibility with that,
[01:57:48.900 --> 01:57:53.100]   but he's also, from having interacted with him quite a bit,
[01:57:53.100 --> 01:57:57.380]   he's just a brilliant thinker about ideas.
[01:57:57.380 --> 01:58:01.180]   And which, as are you,
[01:58:01.180 --> 01:58:03.700]   and there's a tension between becoming the manager
[01:58:03.700 --> 01:58:08.700]   versus like the actual thinking through very novel ideas.
[01:58:08.700 --> 01:58:13.540]   The, yeah, the scientist versus the manager.
[01:58:13.540 --> 01:58:17.620]   And he's one of the great scientists of our time.
[01:58:17.620 --> 01:58:18.740]   This was quite interesting.
[01:58:18.740 --> 01:58:20.740]   And also people tell me quite silly,
[01:58:20.740 --> 01:58:23.180]   which I haven't quite detected yet,
[01:58:23.180 --> 01:58:25.940]   but in private, we'll have to see about that.
[01:58:25.940 --> 01:58:27.380]   - Yeah, yeah.
[01:58:27.380 --> 01:58:29.580]   I mean, just on the point of,
[01:58:29.580 --> 01:58:33.260]   I mean, Ilya has been an inspiration.
[01:58:33.260 --> 01:58:36.300]   I mean, quite a few colleagues I can think shaped,
[01:58:36.300 --> 01:58:37.980]   you know, the person you are.
[01:58:37.980 --> 01:58:42.220]   Like Ilya certainly gets probably the top spot,
[01:58:42.220 --> 01:58:43.700]   if not close to the top.
[01:58:43.700 --> 01:58:47.900]   And if we go back to the question about people in the field,
[01:58:47.900 --> 01:58:51.660]   like how their role would have changed the field or not,
[01:58:51.660 --> 01:58:53.900]   I think Ilya's case is interesting
[01:58:53.900 --> 01:58:56.740]   because he really has a deep belief
[01:58:56.740 --> 01:58:59.540]   in the scaling up of neural networks.
[01:58:59.540 --> 01:59:03.620]   There was a talk that is still famous to this day
[01:59:03.620 --> 01:59:06.100]   from the "Sequence to Sequence" paper,
[01:59:06.100 --> 01:59:08.340]   where he was just claiming,
[01:59:08.340 --> 01:59:11.700]   just give me supervised data and a large neural network,
[01:59:11.700 --> 01:59:13.140]   and then, you know, you'll solve
[01:59:13.140 --> 01:59:14.580]   basically all the problems, right?
[01:59:14.580 --> 01:59:19.580]   That vision, right, was already there many years ago.
[01:59:19.580 --> 01:59:22.820]   So it's good to see like someone who is, in this case,
[01:59:22.820 --> 01:59:27.140]   very deeply into this style of research
[01:59:27.140 --> 01:59:31.980]   and clearly has had a tremendous track record
[01:59:31.980 --> 01:59:34.100]   of successes and so on.
[01:59:34.100 --> 01:59:36.300]   The funny bit about that talk is that
[01:59:36.300 --> 01:59:39.020]   we rehearsed the talk in a hotel room before,
[01:59:39.020 --> 01:59:41.980]   and the original version of that talk
[01:59:41.980 --> 01:59:43.980]   would have been even more controversial.
[01:59:43.980 --> 01:59:46.540]   So maybe I'm the only person
[01:59:46.540 --> 01:59:49.180]   that has seen the unfiltered version of the talk.
[01:59:49.180 --> 01:59:51.660]   And, you know, maybe when the time comes,
[01:59:51.660 --> 01:59:55.100]   maybe we should revisit some of the skip slides
[01:59:55.100 --> 01:59:57.580]   from the talk from Ilya.
[01:59:57.580 --> 02:00:01.020]   But I really think the deep belief
[02:00:01.020 --> 02:00:03.900]   into some certain style of research pays out, right?
[02:00:03.900 --> 02:00:06.380]   It's good to be practical sometimes.
[02:00:06.380 --> 02:00:09.380]   And I actually think Ilya and myself are like practical,
[02:00:09.380 --> 02:00:13.260]   but it's also good there's some sort of long-term belief
[02:00:13.260 --> 02:00:14.820]   and trajectory.
[02:00:14.820 --> 02:00:16.700]   Obviously, there's a bit of luck involved,
[02:00:16.700 --> 02:00:18.820]   but it might be that that's the right path,
[02:00:18.820 --> 02:00:19.980]   then you clearly are ahead
[02:00:19.980 --> 02:00:23.540]   and hugely influential to the field, as he has been.
[02:00:23.540 --> 02:00:25.100]   - Do you agree with that intuition
[02:00:25.100 --> 02:00:29.660]   that maybe was written about by Rich Sutton
[02:00:29.660 --> 02:00:33.580]   in "The Bitter Lesson,"
[02:00:33.580 --> 02:00:35.260]   that the biggest lesson that can be read
[02:00:35.260 --> 02:00:38.620]   from 70 years of AI research is that general methods
[02:00:38.620 --> 02:00:42.780]   that leverage computation are ultimately the most effective.
[02:00:42.780 --> 02:00:47.780]   Do you think that intuition is ultimately correct?
[02:00:47.780 --> 02:00:52.220]   General methods that leverage computation,
[02:00:52.220 --> 02:00:56.140]   allowing the scaling of computation to do a lot of the work,
[02:00:56.140 --> 02:01:00.900]   and so the basic task of us humans is to design methods
[02:01:00.900 --> 02:01:02.580]   that are more and more general
[02:01:02.580 --> 02:01:07.060]   versus more and more specific to the tasks at hand?
[02:01:07.060 --> 02:01:10.300]   - I certainly think this essentially mimics
[02:01:10.380 --> 02:01:13.540]   a bit of the deep learning research,
[02:01:13.540 --> 02:01:16.980]   almost like philosophy,
[02:01:16.980 --> 02:01:20.460]   that on the one hand, we want to be data agnostic,
[02:01:20.460 --> 02:01:22.100]   we don't wanna pre-process datasets,
[02:01:22.100 --> 02:01:23.420]   we wanna see the bytes, right?
[02:01:23.420 --> 02:01:25.540]   Like the true data as it is,
[02:01:25.540 --> 02:01:27.340]   and then learn everything on top.
[02:01:27.340 --> 02:01:29.780]   So very much agree with that.
[02:01:29.780 --> 02:01:32.860]   And I think scaling up feels at the very least,
[02:01:32.860 --> 02:01:37.860]   again, necessary for building incredible complex systems.
[02:01:39.020 --> 02:01:42.140]   It's possibly not sufficient,
[02:01:42.140 --> 02:01:45.060]   barring that we need a couple of breakthroughs.
[02:01:45.060 --> 02:01:48.580]   I think Rich Sutton mentioned search being part
[02:01:48.580 --> 02:01:52.260]   of the equation of scale and search.
[02:01:52.260 --> 02:01:55.420]   I think search, I've seen it,
[02:01:55.420 --> 02:01:57.300]   that's been more mixed in my experience,
[02:01:57.300 --> 02:01:59.340]   or from that lesson in particular,
[02:01:59.340 --> 02:02:01.180]   search is a bit more tricky
[02:02:01.180 --> 02:02:05.340]   because it is very appealing to search in domains like Go,
[02:02:05.340 --> 02:02:07.460]   where you have a clear reward function
[02:02:07.460 --> 02:02:10.620]   that you can then discard some search traces.
[02:02:10.620 --> 02:02:12.940]   But then in some other tasks,
[02:02:12.940 --> 02:02:15.260]   it's not very clear how you would do that.
[02:02:15.260 --> 02:02:18.620]   Although recently, one of our recent works,
[02:02:18.620 --> 02:02:22.140]   which actually was mostly mimicking or a continuation,
[02:02:22.140 --> 02:02:23.700]   and even the team and the people involved
[02:02:23.700 --> 02:02:27.220]   were pretty much very, like intersecting with AlphaStar,
[02:02:27.220 --> 02:02:30.980]   was AlphaCode, in which we actually saw the bitter lesson,
[02:02:30.980 --> 02:02:32.620]   how scale of the models,
[02:02:32.620 --> 02:02:34.260]   and then a massive amount of search,
[02:02:34.260 --> 02:02:36.780]   yielded this kind of very interesting result
[02:02:36.780 --> 02:02:41.340]   of being able to have human level code competition.
[02:02:41.340 --> 02:02:43.660]   So I've seen examples of it being
[02:02:43.660 --> 02:02:46.380]   literally mapped to search and scale.
[02:02:46.380 --> 02:02:48.140]   I'm not so convinced about the search bit,
[02:02:48.140 --> 02:02:50.900]   but certainly I'm convinced scale will be needed.
[02:02:50.900 --> 02:02:52.660]   So we need general methods.
[02:02:52.660 --> 02:02:53.500]   We need to test them,
[02:02:53.500 --> 02:02:56.140]   and maybe we need to make sure that we can scale them,
[02:02:56.140 --> 02:02:59.100]   given the hardware that we have in practice,
[02:02:59.100 --> 02:03:00.940]   but then maybe we should also shape
[02:03:00.940 --> 02:03:02.860]   how the hardware looks like,
[02:03:02.860 --> 02:03:05.620]   based on which methods might be needed to scale.
[02:03:05.620 --> 02:03:10.620]   And that's an interesting contrast of this GPU comment,
[02:03:10.620 --> 02:03:13.380]   that is, we got it for free almost,
[02:03:13.380 --> 02:03:15.060]   because games were using this,
[02:03:15.060 --> 02:03:19.500]   but maybe now if sparsity is required,
[02:03:19.500 --> 02:03:21.860]   we don't have the hardware, although in theory,
[02:03:21.860 --> 02:03:23.180]   I mean, many people are building
[02:03:23.180 --> 02:03:24.660]   different kinds of hardware these days,
[02:03:24.660 --> 02:03:27.780]   but there's a bit of this notion of hardware lottery
[02:03:27.780 --> 02:03:31.260]   for scale that might actually have an impact,
[02:03:31.260 --> 02:03:33.420]   at least on the year, again, scale of years,
[02:03:33.420 --> 02:03:35.180]   on how fast we'll make progress
[02:03:35.180 --> 02:03:39.420]   to maybe a version of neural nets or whatever comes next
[02:03:39.420 --> 02:03:44.420]   that might enable truly intelligent agents.
[02:03:44.420 --> 02:03:46.100]   - Do you think in your lifetime,
[02:03:46.100 --> 02:03:49.500]   we will build an AGI system
[02:03:49.500 --> 02:03:54.020]   that would undeniably be a thing
[02:03:54.020 --> 02:03:57.460]   that achieves human level intelligence and goes far beyond?
[02:03:57.460 --> 02:04:01.100]   - I definitely think it's possible
[02:04:02.340 --> 02:04:03.700]   that it will go far beyond,
[02:04:03.700 --> 02:04:04.860]   but I'm definitely convinced
[02:04:04.860 --> 02:04:08.060]   that it will be human level intelligence.
[02:04:08.060 --> 02:04:10.940]   And I'm hypothesizing about the beyond
[02:04:10.940 --> 02:04:15.940]   because the beyond bit is a bit tricky to define,
[02:04:15.940 --> 02:04:19.980]   especially when we look at the current formula
[02:04:19.980 --> 02:04:23.700]   of starting from this imitation learning standpoint, right?
[02:04:23.700 --> 02:04:28.700]   So we can certainly imitate humans at language and beyond.
[02:04:30.660 --> 02:04:33.340]   So getting at human level through imitation
[02:04:33.340 --> 02:04:34.860]   feels very possible.
[02:04:34.860 --> 02:04:38.980]   Going beyond will require reinforcement learning
[02:04:38.980 --> 02:04:39.820]   and other things.
[02:04:39.820 --> 02:04:41.620]   And I think in some areas
[02:04:41.620 --> 02:04:43.500]   that certainly already has paid out.
[02:04:43.500 --> 02:04:47.220]   I mean, Go being an example that's my favorite so far
[02:04:47.220 --> 02:04:50.340]   in terms of going beyond human capabilities.
[02:04:50.340 --> 02:04:55.340]   But in general, I'm not sure we can define reward functions
[02:04:55.340 --> 02:04:59.940]   that from a seat of imitating human level intelligence
[02:04:59.940 --> 02:05:02.820]   that is general and then going beyond.
[02:05:02.820 --> 02:05:05.140]   That bit is not so clear in my lifetime,
[02:05:05.140 --> 02:05:08.100]   but certainly human level, yes.
[02:05:08.100 --> 02:05:10.860]   And I mean, that in itself is already quite powerful,
[02:05:10.860 --> 02:05:11.700]   I think.
[02:05:11.700 --> 02:05:14.420]   So going beyond, I think it's obviously not,
[02:05:14.420 --> 02:05:16.060]   we're not gonna not try that
[02:05:16.060 --> 02:05:19.860]   if then we get to superhuman scientist
[02:05:19.860 --> 02:05:22.060]   and discovery and advancing the world.
[02:05:22.060 --> 02:05:24.660]   But at least human level is also,
[02:05:24.660 --> 02:05:27.460]   in general, is also very, very powerful.
[02:05:27.460 --> 02:05:31.500]   - Well, especially if human level or slightly beyond
[02:05:31.500 --> 02:05:33.740]   is integrated deeply with human society
[02:05:33.740 --> 02:05:36.460]   and there's billions of agents like that,
[02:05:36.460 --> 02:05:38.460]   do you think there's a singularity moment
[02:05:38.460 --> 02:05:43.460]   beyond which our world will be just very deeply transformed
[02:05:43.460 --> 02:05:45.620]   by these kinds of systems?
[02:05:45.620 --> 02:05:47.780]   Because now you're talking about intelligence systems
[02:05:47.780 --> 02:05:52.780]   that are just, I mean, this is no longer just going
[02:05:52.780 --> 02:05:56.420]   from horse and buggy to the car.
[02:05:56.420 --> 02:05:59.780]   It feels like a very different kind of shift
[02:05:59.780 --> 02:06:03.300]   in what it means to be a living entity on earth.
[02:06:03.300 --> 02:06:04.180]   Are you afraid?
[02:06:04.180 --> 02:06:06.300]   Are you excited of this world?
[02:06:06.300 --> 02:06:09.340]   - I'm afraid if there's a lot more.
[02:06:09.340 --> 02:06:13.020]   So I think maybe we'll need to think about
[02:06:13.020 --> 02:06:14.940]   if we truly get there,
[02:06:14.940 --> 02:06:18.340]   just thinking of limited resources,
[02:06:18.340 --> 02:06:21.420]   like humanity clearly hits some limits
[02:06:21.420 --> 02:06:23.420]   and then there's some balance, hopefully,
[02:06:23.420 --> 02:06:26.260]   that biologically the planet is imposing
[02:06:26.260 --> 02:06:28.500]   and we should actually try to get better at this.
[02:06:28.500 --> 02:06:31.500]   As we know, there's quite a few issues
[02:06:31.500 --> 02:06:35.740]   with having too many people coexisting
[02:06:35.740 --> 02:06:37.580]   in a resource-limited way.
[02:06:37.580 --> 02:06:40.300]   So for digital entities, it's an interesting question.
[02:06:40.300 --> 02:06:43.540]   I think such a limit maybe should exist,
[02:06:43.540 --> 02:06:47.620]   but maybe it's gonna be imposed by energy availability
[02:06:47.620 --> 02:06:49.700]   because this also consumes energy.
[02:06:49.700 --> 02:06:53.500]   In fact, most systems are more inefficient
[02:06:53.500 --> 02:06:55.980]   than we are in terms of energy required.
[02:06:55.980 --> 02:06:56.820]   - Correct, yeah.
[02:06:56.820 --> 02:06:59.500]   - But definitely, I think as a society,
[02:06:59.500 --> 02:07:02.220]   we'll need to just work together
[02:07:02.220 --> 02:07:06.380]   to find what would be reasonable in terms of growth
[02:07:06.380 --> 02:07:11.380]   or how we coexist if that is to happen.
[02:07:11.380 --> 02:07:14.660]   I am very excited about, obviously,
[02:07:14.660 --> 02:07:17.700]   the aspects of automation that make people
[02:07:17.700 --> 02:07:19.020]   that obviously don't have access
[02:07:19.020 --> 02:07:20.980]   to certain resources or knowledge,
[02:07:20.980 --> 02:07:23.900]   for them to have that access.
[02:07:23.900 --> 02:07:26.260]   I think those are the applications in a way
[02:07:26.260 --> 02:07:30.940]   that I'm most excited to see and to personally work towards.
[02:07:30.940 --> 02:07:32.660]   - Yeah, there's going to be significant improvements
[02:07:32.660 --> 02:07:34.340]   in productivity and the quality of life
[02:07:34.340 --> 02:07:36.980]   across the whole population, which is very interesting.
[02:07:36.980 --> 02:07:39.180]   But I'm looking even far beyond
[02:07:39.180 --> 02:07:42.660]   us becoming a multi-planetary species.
[02:07:42.660 --> 02:07:45.340]   And just as a quick bet, last question,
[02:07:45.340 --> 02:07:49.180]   do you think as humans become multi-planetary species,
[02:07:49.180 --> 02:07:52.460]   go outside our solar system, all that kind of stuff,
[02:07:52.460 --> 02:07:54.420]   do you think there'll be more humans
[02:07:54.420 --> 02:07:57.180]   or more robots in that future world?
[02:07:57.180 --> 02:08:02.180]   So will humans be the quirky,
[02:08:02.180 --> 02:08:04.460]   intelligent being of the past,
[02:08:04.460 --> 02:08:06.980]   or is there something deeply fundamental
[02:08:06.980 --> 02:08:09.580]   to human intelligence that's truly special,
[02:08:09.580 --> 02:08:12.100]   where we will be part of those other planets,
[02:08:12.100 --> 02:08:13.900]   not just AI systems?
[02:08:13.900 --> 02:08:18.660]   - I think we're all excited to build AGI
[02:08:18.660 --> 02:08:23.660]   to empower or make us more powerful as human species.
[02:08:23.660 --> 02:08:27.580]   Not to say there might be some hybridization.
[02:08:27.580 --> 02:08:29.700]   I mean, this is obviously speculation,
[02:08:29.700 --> 02:08:32.500]   but there are companies also trying to,
[02:08:32.500 --> 02:08:35.660]   the same way medicine is making us better.
[02:08:35.660 --> 02:08:39.100]   Maybe there are other things that are yet to happen on that.
[02:08:39.100 --> 02:08:43.340]   But if the ratio is not at most one-to-one,
[02:08:43.340 --> 02:08:44.540]   I would not be happy.
[02:08:44.580 --> 02:08:49.220]   So I would hope that we are part of the equation,
[02:08:49.220 --> 02:08:52.780]   but maybe there's, maybe a one-to-one ratio
[02:08:52.780 --> 02:08:56.220]   feels like possible, constructive and so on,
[02:08:56.220 --> 02:08:59.620]   but it would not be good to have a misbalance,
[02:08:59.620 --> 02:09:01.420]   at least from my core beliefs
[02:09:01.420 --> 02:09:05.180]   and the why I'm doing what I'm doing when I go to work
[02:09:05.180 --> 02:09:07.100]   and I research what I research.
[02:09:07.100 --> 02:09:09.500]   - Well, this is how I know you're human,
[02:09:09.500 --> 02:09:12.700]   and this is how you've passed the Turing test.
[02:09:12.700 --> 02:09:14.940]   And you are one of the special humans, Ariel.
[02:09:14.940 --> 02:09:17.060]   It's a huge honor that you would talk with me,
[02:09:17.060 --> 02:09:19.900]   and I hope we get the chance to speak again,
[02:09:19.900 --> 02:09:23.020]   maybe once before the singularity, once after,
[02:09:23.020 --> 02:09:25.420]   and see how our view of the world changes.
[02:09:25.420 --> 02:09:26.540]   Thank you again for talking today.
[02:09:26.540 --> 02:09:28.140]   Thank you for the amazing work you do.
[02:09:28.140 --> 02:09:31.300]   You're a shining example of a researcher
[02:09:31.300 --> 02:09:32.900]   and a human being in this community.
[02:09:32.900 --> 02:09:34.020]   - Thanks a lot, Lex.
[02:09:34.020 --> 02:09:36.780]   Yeah, looking forward to before the singularity, certainly.
[02:09:36.780 --> 02:09:37.820]   (Lex laughs)
[02:09:37.820 --> 02:09:38.980]   - And maybe after.
[02:09:38.980 --> 02:09:41.460]   Thanks for listening to this conversation
[02:09:41.460 --> 02:09:43.100]   with Ariel Vinales.
[02:09:43.100 --> 02:09:44.260]   To support this podcast,
[02:09:44.260 --> 02:09:46.940]   please check out our sponsors in the description.
[02:09:46.940 --> 02:09:50.060]   And now, let me leave you with some words from Alan Turing.
[02:09:50.060 --> 02:09:55.140]   "Those who can imagine anything can create the impossible."
[02:09:55.140 --> 02:09:59.180]   Thank you for listening, and hope to see you next time.
[02:09:59.180 --> 02:10:01.780]   (upbeat music)
[02:10:01.780 --> 02:10:04.380]   (upbeat music)
[02:10:04.380 --> 02:10:14.380]   [BLANK_AUDIO]

