
[00:00:00.000 --> 00:00:03.320]   [MUSIC PLAYING]
[00:00:03.320 --> 00:00:04.520]   Well, hello, everyone.
[00:00:04.520 --> 00:00:07.760]   I'm really excited to be moderating today's panel
[00:00:07.760 --> 00:00:10.240]   on large language models.
[00:00:10.240 --> 00:00:14.520]   I'm Chris Van Pelt, co-founder of Weights and Biases.
[00:00:14.520 --> 00:00:17.000]   And I'd like to start by just going around the room
[00:00:17.000 --> 00:00:19.840]   and having the panelists introduce themselves
[00:00:19.840 --> 00:00:22.840]   and share a little bit about what they're working on.
[00:00:22.840 --> 00:00:24.840]   Yes, so I am Jay Alamar.
[00:00:24.840 --> 00:00:29.100]   I work at Cohere, which is a provider of large language
[00:00:29.100 --> 00:00:33.480]   models and NLP tools for developers.
[00:00:33.480 --> 00:00:36.120]   I am director and engineering fellow.
[00:00:36.120 --> 00:00:38.840]   And I help the developer community and enterprises
[00:00:38.840 --> 00:00:42.600]   figure out the best use cases for these models
[00:00:42.600 --> 00:00:45.040]   and just connect the dots between model capabilities
[00:00:45.040 --> 00:00:47.400]   and, let's say, business use cases.
[00:00:47.400 --> 00:00:50.320]   I'm also just fascinated about machine learning and NLP
[00:00:50.320 --> 00:00:50.880]   as a whole.
[00:00:50.880 --> 00:00:53.680]   So I maintain an active blog that
[00:00:53.680 --> 00:00:57.320]   explains a lot of these concepts between transformers, BERT,
[00:00:57.320 --> 00:00:59.680]   GPT models.
[00:00:59.680 --> 00:01:02.120]   My latest write-up was on stable diffusion.
[00:01:02.120 --> 00:01:04.160]   Excited to be here.
[00:01:04.160 --> 00:01:04.720]   Very cool.
[00:01:04.720 --> 00:01:07.160]   Excited to have you, Jay.
[00:01:07.160 --> 00:01:08.320]   Hanlin.
[00:01:08.320 --> 00:01:09.800]   Yeah, great to meet you all.
[00:01:09.800 --> 00:01:11.360]   My name's Hanlin Tang.
[00:01:11.360 --> 00:01:14.200]   I'm the CTO and co-founder of Mosaic ML.
[00:01:14.200 --> 00:01:16.160]   We build software and tooling that
[00:01:16.160 --> 00:01:19.600]   enables companies to easily train their own large AI
[00:01:19.600 --> 00:01:22.040]   models, such as LLMs, on their own data
[00:01:22.040 --> 00:01:24.480]   and inside their secure environments.
[00:01:24.480 --> 00:01:26.240]   We built a stack that makes training
[00:01:26.240 --> 00:01:29.000]   these large models for quote, unquote, "just work."
[00:01:29.000 --> 00:01:31.680]   I'm a computational neuroscientist by training.
[00:01:31.680 --> 00:01:33.520]   So I have been working with neural networks,
[00:01:33.520 --> 00:01:37.480]   both artificial and live, for many years now.
[00:01:37.480 --> 00:01:38.840]   Very cool.
[00:01:38.840 --> 00:01:41.640]   Last but not least, Boris.
[00:01:41.640 --> 00:01:43.320]   Yeah, I'm Boris Tehima.
[00:01:43.320 --> 00:01:45.600]   So what I do, I'm continuously learning
[00:01:45.600 --> 00:01:49.840]   about how to use those language models and training new models.
[00:01:49.840 --> 00:01:52.760]   I learn a lot with your blogs, Jay, by the way,
[00:01:52.760 --> 00:01:55.880]   but the illustrated transformer and all that,
[00:01:55.880 --> 00:01:58.280]   I don't know how many times I opened it
[00:01:58.280 --> 00:02:01.560]   until I actually understood what it really worked.
[00:02:01.560 --> 00:02:02.680]   But I really like that.
[00:02:02.680 --> 00:02:07.440]   And at the moment, I work a lot on generative AI,
[00:02:07.440 --> 00:02:11.040]   in particular images, like the alumini and now crayon.
[00:02:11.040 --> 00:02:14.040]   I have a lot of fun writing dumb things
[00:02:14.040 --> 00:02:16.320]   and have some image coming up.
[00:02:16.320 --> 00:02:20.360]   So that's what I do now.
[00:02:20.360 --> 00:02:20.960]   Very cool.
[00:02:20.960 --> 00:02:25.480]   It's been a heck of a year for AI/ML,
[00:02:25.480 --> 00:02:27.800]   and especially these large language models.
[00:02:27.800 --> 00:02:30.080]   Boris, I remember when you released crayon.
[00:02:30.080 --> 00:02:33.720]   This was before stable diffusion, before chat GPT,
[00:02:33.720 --> 00:02:38.120]   and all of this p-type.
[00:02:38.120 --> 00:02:41.720]   I guess a question for the group.
[00:02:41.720 --> 00:02:44.800]   Clearly, there's a lot of hype around AI/ML
[00:02:44.800 --> 00:02:47.360]   in general in these large language models.
[00:02:47.360 --> 00:02:48.720]   Is it overhyped?
[00:02:48.720 --> 00:02:49.640]   How do we feel?
[00:02:49.640 --> 00:02:53.600]   What's the temperature on the general sentiment
[00:02:53.600 --> 00:02:56.800]   here towards what we've all been doing for many, many years,
[00:02:56.800 --> 00:03:00.080]   but now the rest of the world is really excited about?
[00:03:00.080 --> 00:03:01.440]   I guess I can kick it off.
[00:03:01.440 --> 00:03:03.920]   I usually try to avoid the hype as much as possible
[00:03:03.920 --> 00:03:06.360]   and focus on the business and the use cases.
[00:03:06.360 --> 00:03:10.200]   But it's hard to not get excited.
[00:03:10.200 --> 00:03:16.400]   We work with a lot of startups and tinkers out there.
[00:03:16.400 --> 00:03:18.320]   And yeah, it's hard not to get excited
[00:03:18.320 --> 00:03:21.360]   by the absolute creativity that these folks are now building.
[00:03:21.360 --> 00:03:24.760]   And we have almost the privilege of observing.
[00:03:24.760 --> 00:03:28.360]   Folks bring language models to motion,
[00:03:28.360 --> 00:03:31.680]   AI-powered 3D design tools, financial customers
[00:03:31.680 --> 00:03:32.960]   deploying them today.
[00:03:32.960 --> 00:03:34.960]   Even though I try to stay grounded in use cases,
[00:03:34.960 --> 00:03:37.320]   it's really hard to actually not feel
[00:03:37.320 --> 00:03:41.000]   excited with everything that's going on now.
[00:03:41.000 --> 00:03:44.000]   Yeah, I think to me, it's fun.
[00:03:44.000 --> 00:03:45.840]   Because for example, on the Alimini,
[00:03:45.840 --> 00:03:50.120]   I was having fun just training it and sharing the training,
[00:03:50.120 --> 00:03:51.400]   the results going on and all.
[00:03:51.400 --> 00:03:55.160]   And there was a few tiny user base still playing with it.
[00:03:55.160 --> 00:03:57.560]   There was the Discord of DalipyTorch.
[00:03:57.560 --> 00:03:59.080]   People would play with it.
[00:03:59.080 --> 00:04:00.640]   And suddenly, it exploded.
[00:04:00.640 --> 00:04:04.440]   And I had that tiny demo that people could just toy with.
[00:04:04.440 --> 00:04:10.480]   And there was maybe 10 users on it, me included and my friends.
[00:04:10.480 --> 00:04:13.920]   And suddenly, it became so big that the demo couldn't handle
[00:04:13.920 --> 00:04:14.880]   the traffic anymore.
[00:04:14.880 --> 00:04:18.480]   And the focus moved from training these models
[00:04:18.480 --> 00:04:19.920]   to serving them.
[00:04:19.920 --> 00:04:22.640]   And I think it was cool to see that.
[00:04:22.640 --> 00:04:25.960]   I was excited on it by myself before.
[00:04:25.960 --> 00:04:29.560]   And seeing that other people find that it's cool,
[00:04:29.560 --> 00:04:30.960]   it's always nice.
[00:04:30.960 --> 00:04:35.840]   It motivates you more into building these models.
[00:04:35.840 --> 00:04:38.480]   Yeah, I would echo that by saying there's something
[00:04:38.480 --> 00:04:40.280]   really special happening here.
[00:04:40.280 --> 00:04:43.560]   In what's happening in AI, the developments
[00:04:43.560 --> 00:04:46.880]   in the last few years, it's really astounding.
[00:04:46.880 --> 00:04:48.880]   And it's really pushing software to do things
[00:04:48.880 --> 00:04:51.280]   that we never thought were possible.
[00:04:51.280 --> 00:04:53.560]   So the idea that you can write a sentence
[00:04:53.560 --> 00:04:55.800]   and have a model create an image for it,
[00:04:55.800 --> 00:04:59.680]   or write a full essay, or understand things
[00:04:59.680 --> 00:05:01.400]   is something--
[00:05:01.400 --> 00:05:03.520]   if you were building software five years ago,
[00:05:03.520 --> 00:05:06.840]   you'd think this is the closest thing to science fiction.
[00:05:06.840 --> 00:05:10.120]   So there is definitely a lot to be excited about.
[00:05:10.120 --> 00:05:13.800]   A lot of that is very applicable and, let's say,
[00:05:13.800 --> 00:05:17.600]   directly related to products that
[00:05:17.600 --> 00:05:19.680]   can be improved in the market, but also
[00:05:19.680 --> 00:05:21.960]   completely new categories that are now just
[00:05:21.960 --> 00:05:25.440]   arising on top of these models.
[00:05:25.440 --> 00:05:27.960]   We can get into some of these features,
[00:05:27.960 --> 00:05:30.840]   but also new sub-industries based
[00:05:30.840 --> 00:05:33.200]   on large language models.
[00:05:33.200 --> 00:05:37.400]   But there's a lot happening on the text side.
[00:05:37.400 --> 00:05:41.680]   Search is, for example, one area where that's directly
[00:05:41.680 --> 00:05:44.600]   feeding into products.
[00:05:44.600 --> 00:05:46.440]   A lot of excitement around text generation.
[00:05:46.440 --> 00:05:49.200]   But if you're in a media company,
[00:05:49.200 --> 00:05:53.040]   if you're in Disney or Pixar and you see things like Dali
[00:05:53.040 --> 00:05:56.320]   Mini or Mid Journey, it's hard to think that this is not
[00:05:56.320 --> 00:05:58.000]   going to change your business.
[00:05:58.000 --> 00:05:59.360]   And machine learning is not going
[00:05:59.360 --> 00:06:01.680]   to be at the center of media production in the next two
[00:06:01.680 --> 00:06:02.720]   years.
[00:06:02.720 --> 00:06:05.520]   Yeah, I mean, we obviously all have a vested interest
[00:06:05.520 --> 00:06:08.080]   in the hype playing out.
[00:06:08.080 --> 00:06:09.560]   But I would echo you all.
[00:06:09.560 --> 00:06:11.920]   It does feel very special.
[00:06:11.920 --> 00:06:14.760]   And I've been in this space for 15 years.
[00:06:14.760 --> 00:06:16.560]   So that's saying something.
[00:06:16.560 --> 00:06:20.840]   Jay, you mentioned novel use cases and things
[00:06:20.840 --> 00:06:26.640]   that are now starting to happen in this very new world of LLMs.
[00:06:26.640 --> 00:06:31.200]   I think it'd be helpful to hear maybe some specific industries,
[00:06:31.200 --> 00:06:35.920]   such as, say, marketing folks is an obvious one.
[00:06:35.920 --> 00:06:40.560]   But I think we'd be interested to hear maybe
[00:06:40.560 --> 00:06:43.680]   more interesting use cases or how people are using
[00:06:43.680 --> 00:06:45.840]   these models in ways that maybe we didn't originally
[00:06:45.840 --> 00:06:47.400]   expect them to.
[00:06:47.400 --> 00:06:48.680]   Sure, yes, absolutely.
[00:06:48.680 --> 00:06:52.760]   So anywhere, any pipe where data flows and software
[00:06:52.760 --> 00:06:55.720]   does things with text, these models
[00:06:55.720 --> 00:06:58.040]   are enabling the software to do things
[00:06:58.040 --> 00:06:59.760]   that it wasn't able to do before.
[00:06:59.760 --> 00:07:03.320]   A lot of that is categorizing and classifying text.
[00:07:03.320 --> 00:07:05.720]   A lot of it is grouping and allowing people
[00:07:05.720 --> 00:07:08.160]   to explore vast amounts of text.
[00:07:08.160 --> 00:07:10.160]   And that applies across industries.
[00:07:10.160 --> 00:07:14.880]   So if you're in marketing and you have 100,000 tweets coming
[00:07:14.880 --> 00:07:17.440]   at your brand and you want to cluster and see
[00:07:17.440 --> 00:07:21.080]   what are the sentiments of people, what are common themes,
[00:07:21.080 --> 00:07:25.480]   that's a topic modeling exercise that language models improve.
[00:07:25.480 --> 00:07:30.320]   You can apply that to any collection of texts, let's say.
[00:07:30.320 --> 00:07:32.360]   So that's a little bit more on, let's say,
[00:07:32.360 --> 00:07:33.640]   language understanding.
[00:07:33.640 --> 00:07:36.080]   And there are things in text generation
[00:07:36.080 --> 00:07:38.560]   where it's where these new industries are coming.
[00:07:38.560 --> 00:07:42.320]   So a lot of these AI writing assistants,
[00:07:42.320 --> 00:07:44.120]   that's an industry that did not really exist
[00:07:44.120 --> 00:07:45.800]   before generative text models.
[00:07:45.800 --> 00:07:51.520]   So companies like Jasper and CopyAI and HyperWrite,
[00:07:51.520 --> 00:07:54.000]   for example, that's an entire new industry that's
[00:07:54.000 --> 00:07:58.680]   based on these generative models.
[00:07:58.680 --> 00:08:05.240]   And so you'll see anywhere where there's an abundance of text,
[00:08:05.240 --> 00:08:08.000]   a lot more tasks can be automated there.
[00:08:08.000 --> 00:08:11.320]   A lot of it can be just by taking the software
[00:08:11.320 --> 00:08:12.240]   to the next level.
[00:08:12.240 --> 00:08:15.440]   So things that are routine that humans do,
[00:08:15.440 --> 00:08:20.000]   software would be able to assign a tag towards on, let's say,
[00:08:20.000 --> 00:08:22.040]   tickets based on their content, assigning them
[00:08:22.040 --> 00:08:23.920]   to the right person or to the right department
[00:08:23.920 --> 00:08:26.600]   or forwarding them to the right--
[00:08:26.600 --> 00:08:30.720]   so there's a lot of interesting future potential
[00:08:30.720 --> 00:08:31.960]   on the text generation side.
[00:08:31.960 --> 00:08:33.480]   And new industry is happening there.
[00:08:33.480 --> 00:08:35.720]   But on the text understanding side,
[00:08:35.720 --> 00:08:39.160]   that's where also a lot of the more reliable, robust use
[00:08:39.160 --> 00:08:46.440]   cases that are proliferating in industry at the moment.
[00:08:46.440 --> 00:08:48.760]   Yeah, I, for one, hope Siri gets better.
[00:08:48.760 --> 00:08:52.120]   It seems like it should be better.
[00:08:52.120 --> 00:08:53.600]   You would think so, yeah.
[00:08:53.600 --> 00:08:55.280]   Yeah.
[00:08:55.280 --> 00:08:59.760]   All right, Boris, what are some of the biggest challenges
[00:08:59.760 --> 00:09:04.000]   that you faced in developing these much larger models?
[00:09:04.000 --> 00:09:07.120]   And how have you addressed those challenges?
[00:09:07.120 --> 00:09:10.520]   So the first one I had is when I trained Alimony,
[00:09:10.520 --> 00:09:12.720]   actually I was surprised that it worked.
[00:09:12.720 --> 00:09:15.400]   And I got very excited seeing that it was not
[00:09:15.400 --> 00:09:17.480]   making really nice images, but it
[00:09:17.480 --> 00:09:20.680]   was starting to make something that resembled images.
[00:09:20.680 --> 00:09:23.080]   And it would work in some cases, like landscape and all.
[00:09:23.080 --> 00:09:25.040]   So I was like, OK, that's going to be very easy.
[00:09:25.040 --> 00:09:28.040]   I just have to make the model two or three times bigger
[00:09:28.040 --> 00:09:32.520]   and train it and it becomes two or three times better.
[00:09:32.520 --> 00:09:34.680]   And so I was really, really excited.
[00:09:34.680 --> 00:09:37.000]   I was like, it's going to take a month or two.
[00:09:37.000 --> 00:09:39.440]   And what happens, like we didn't train,
[00:09:39.440 --> 00:09:43.680]   or it's hard when you use different more machines.
[00:09:43.680 --> 00:09:46.400]   But also you start having a lot of instabilities
[00:09:46.400 --> 00:09:48.160]   that you didn't have before.
[00:09:48.160 --> 00:09:50.240]   And you're like, you have to solve them.
[00:09:50.240 --> 00:09:53.600]   And somehow, just putting more parameters,
[00:09:53.600 --> 00:09:55.440]   which is you have those memes, right?
[00:09:55.440 --> 00:09:56.880]   Add more layers.
[00:09:56.880 --> 00:10:00.320]   And you solve all the issues with that.
[00:10:00.320 --> 00:10:02.880]   It works, but at some point, it doesn't work.
[00:10:02.880 --> 00:10:04.720]   Because your model becomes unstable.
[00:10:04.720 --> 00:10:07.360]   So to solve it, I had to kind of go back
[00:10:07.360 --> 00:10:11.240]   on doing research on how do people make stable training.
[00:10:11.240 --> 00:10:15.040]   And there's not much literature about training large models.
[00:10:15.040 --> 00:10:17.720]   You have all these things about, oh, our large model
[00:10:17.720 --> 00:10:22.480]   of 50 billion parameters, or 100 billion, or more,
[00:10:22.480 --> 00:10:23.880]   has that type of performance.
[00:10:23.880 --> 00:10:25.480]   But you don't know that along the way,
[00:10:25.480 --> 00:10:28.240]   they had a lot of issues.
[00:10:28.240 --> 00:10:29.520]   And they had to overcome them.
[00:10:29.520 --> 00:10:33.160]   And you cannot evaluate your model during training,
[00:10:33.160 --> 00:10:37.240]   because it's so heavy that you need to develop
[00:10:37.240 --> 00:10:39.560]   another infrastructure in parallel,
[00:10:39.560 --> 00:10:43.160]   to restore your checkpoint, to test the performance,
[00:10:43.160 --> 00:10:47.200]   and be able to track any problem that happened.
[00:10:47.200 --> 00:10:48.880]   And I think one of the biggest problems,
[00:10:48.880 --> 00:10:51.240]   like when you have a small model, it fails.
[00:10:51.240 --> 00:10:53.840]   You just restart training, and you do it again,
[00:10:53.840 --> 00:10:55.320]   and you change the parameter.
[00:10:55.320 --> 00:10:58.040]   But if you train a large model for maybe 10 days,
[00:10:58.040 --> 00:10:59.600]   you cannot just restart and let me
[00:10:59.600 --> 00:11:02.360]   try to adjust the weight decay and see what happens.
[00:11:02.360 --> 00:11:06.960]   You kind of want to fix the problem, like where I'm at,
[00:11:06.960 --> 00:11:08.560]   and can I continue the training?
[00:11:08.560 --> 00:11:11.840]   But somehow, that instability won't happen.
[00:11:11.840 --> 00:11:14.360]   That becomes a new problem that before you didn't care about.
[00:11:14.360 --> 00:11:16.520]   You would just restart from scratch.
[00:11:16.520 --> 00:11:20.120]   And when I redo everything with a small learning rate,
[00:11:20.120 --> 00:11:21.880]   so all these challenges arise.
[00:11:21.880 --> 00:11:26.840]   So it's fun when it works, after a while.
[00:11:26.840 --> 00:11:28.520]   Yeah, if I could just add there, I
[00:11:28.520 --> 00:11:31.760]   think a lot of the challenge that we have is that not only
[00:11:31.760 --> 00:11:33.720]   do we need to train these large models ourselves,
[00:11:33.720 --> 00:11:36.720]   but we need to build tooling to help customers reliably
[00:11:36.720 --> 00:11:37.760]   do it for themselves.
[00:11:37.760 --> 00:11:39.520]   So we've learned kind of the hard way.
[00:11:39.520 --> 00:11:42.080]   Like, it's one thing to shoestring a bunch of stuff
[00:11:42.080 --> 00:11:45.680]   together and just get that darn large model trained.
[00:11:45.680 --> 00:11:48.160]   It's another one to try to build customer-facing tooling
[00:11:48.160 --> 00:11:49.360]   to make it easy.
[00:11:49.360 --> 00:11:50.960]   And that's really sharpened our focus
[00:11:50.960 --> 00:11:53.240]   on maybe the less exciting parts of training a large language
[00:11:53.240 --> 00:11:55.480]   model, like how do you just make sure the data
[00:11:55.480 --> 00:11:58.280]   sample is deterministic to your point
[00:11:58.280 --> 00:12:01.200]   about restarting and testing, right?
[00:12:01.200 --> 00:12:02.960]   Even if you're changing GPU sizes
[00:12:02.960 --> 00:12:05.280]   because you're testing your debug in a smaller scale
[00:12:05.280 --> 00:12:08.360]   system, how do you make sure all that is deterministic?
[00:12:08.360 --> 00:12:10.480]   How do you make sure evaluation is really fast?
[00:12:10.480 --> 00:12:12.480]   These are the kind of less exciting
[00:12:12.480 --> 00:12:14.940]   than talking about a mixture of experts and the latest modeling
[00:12:14.940 --> 00:12:15.480]   techniques.
[00:12:15.480 --> 00:12:19.160]   But that's the nitty gritty that gets the job done
[00:12:19.160 --> 00:12:20.800]   for our customers, at least.
[00:12:20.800 --> 00:12:22.680]   It's really cool you guys are making a product
[00:12:22.680 --> 00:12:24.520]   to simplify a lot of these things.
[00:12:24.520 --> 00:12:28.200]   It's you need a degree in DevOps and networking
[00:12:28.200 --> 00:12:30.640]   and all of this stuff to get these big models going.
[00:12:30.640 --> 00:12:36.600]   So making it more easier is going to accelerate innovation.
[00:12:36.600 --> 00:12:39.880]   Jay, what are some of the current limitations of LLMs?
[00:12:39.880 --> 00:12:46.800]   And how, as people making tools and offerings in this area,
[00:12:46.800 --> 00:12:48.720]   what things can we do to help address
[00:12:48.720 --> 00:12:52.920]   some of these limitations or what's happening there?
[00:12:52.920 --> 00:12:55.280]   Yeah, so it depends on the domain.
[00:12:55.280 --> 00:12:57.080]   There are a bunch of different domains
[00:12:57.080 --> 00:12:59.560]   that have, let's say, different challenges.
[00:12:59.560 --> 00:13:05.000]   So it's kind of surprising and also wonderful
[00:13:05.000 --> 00:13:11.000]   in the same time that we went from not having software that
[00:13:11.000 --> 00:13:12.920]   is able to speak or write language
[00:13:12.920 --> 00:13:16.440]   to be in the space where we're saying, oh, yes,
[00:13:16.440 --> 00:13:18.760]   we have software that can write amazing articles,
[00:13:18.760 --> 00:13:19.720]   but it hallucinates.
[00:13:19.720 --> 00:13:21.240]   So how can we solve hallucinations?
[00:13:21.240 --> 00:13:25.760]   So we've moved so fast beyond taking that one capability
[00:13:25.760 --> 00:13:29.480]   for granted that machines or objects didn't really
[00:13:29.480 --> 00:13:32.040]   have for thousands of years in human history
[00:13:32.040 --> 00:13:35.160]   to take the next step of how can we
[00:13:35.160 --> 00:13:38.080]   make this more reliable for different use cases,
[00:13:38.080 --> 00:13:40.760]   reducing things like hallucinations,
[00:13:40.760 --> 00:13:44.120]   relying on things like retrieval augmentation.
[00:13:44.120 --> 00:13:45.880]   So let's say there's a frontier that's
[00:13:45.880 --> 00:13:50.880]   happening with what is the edge of where these models can
[00:13:50.880 --> 00:13:52.920]   be taken.
[00:13:52.920 --> 00:13:56.080]   There are obviously issues with the safe deployment
[00:13:56.080 --> 00:13:56.960]   of these models.
[00:13:56.960 --> 00:13:58.540]   There are a lot of discussions that we
[00:13:58.540 --> 00:14:02.880]   need to have about biases, about safe release of these models,
[00:14:02.880 --> 00:14:08.080]   about usage guidelines of the providers of these models.
[00:14:08.080 --> 00:14:10.880]   So there's a lot that sort of needs to be figured out.
[00:14:10.880 --> 00:14:14.360]   And on the image side, there's a lot of, let's say,
[00:14:14.360 --> 00:14:17.640]   discussions to be had about the rights of the training sets
[00:14:17.640 --> 00:14:25.400]   and how artists can opt in or opt out in that, let's say,
[00:14:25.400 --> 00:14:26.840]   framing.
[00:14:26.840 --> 00:14:29.480]   So I think these would be some of the top ones
[00:14:29.480 --> 00:14:36.520]   that come to mind if I scan the domain generally at the moment.
[00:14:36.520 --> 00:14:41.520]   Yeah, I've heard the example of having an LLM generate code
[00:14:41.520 --> 00:14:43.560]   and then have it run that code.
[00:14:43.560 --> 00:14:46.040]   And if that code generates errors, feed that back in
[00:14:46.040 --> 00:14:47.880]   and have it fix the code.
[00:14:47.880 --> 00:14:50.400]   As a developer, it just makes me a little nervous
[00:14:50.400 --> 00:14:52.120]   when we're letting these things run code.
[00:14:52.120 --> 00:14:55.800]   But I guess that's neither here nor there.
[00:14:55.800 --> 00:14:57.480]   Hanlon, I'll throw a question over to you.
[00:14:57.480 --> 00:15:04.680]   How do you think about the trade-offs between model size
[00:15:04.680 --> 00:15:06.680]   and efficiency?
[00:15:06.680 --> 00:15:11.000]   It seems like we could throw just more and more parameters
[00:15:11.000 --> 00:15:12.480]   at these things, but obviously, that
[00:15:12.480 --> 00:15:17.040]   has a big cost and complexity and ability to operate.
[00:15:17.040 --> 00:15:18.600]   Curious what your thoughts are here
[00:15:18.600 --> 00:15:20.560]   is finding that kind of sweet spot.
[00:15:20.560 --> 00:15:22.640]   Yeah, we address it all the time, I think.
[00:15:22.640 --> 00:15:24.520]   The failure mode for our entire field
[00:15:24.520 --> 00:15:28.440]   is if we develop AGI, but one forward pass through that
[00:15:28.440 --> 00:15:31.360]   costs as much as a year's salary,
[00:15:31.360 --> 00:15:34.040]   so it's not actually deployable.
[00:15:34.040 --> 00:15:36.440]   I think what we've learned working with our customers
[00:15:36.440 --> 00:15:40.680]   is that pre-training smaller domain-specific models really
[00:15:40.680 --> 00:15:43.280]   make a difference in changing the unit economics
[00:15:43.280 --> 00:15:45.360]   of inference.
[00:15:45.360 --> 00:15:47.080]   Working with Stanford, we trained a model
[00:15:47.080 --> 00:15:50.680]   that's on biomedical data, and a smallish $3 billion
[00:15:50.680 --> 00:15:53.400]   product model that's very specific for the use case
[00:15:53.400 --> 00:15:58.440]   can do just as well as a $175 billion general purpose model.
[00:15:58.440 --> 00:16:01.080]   And that really changes how you think about model size
[00:16:01.080 --> 00:16:01.880]   and efficiency.
[00:16:01.880 --> 00:16:03.920]   And that's what's really opened the eyes for a lot
[00:16:03.920 --> 00:16:05.300]   of our customers, like, oh, this is actually
[00:16:05.300 --> 00:16:07.480]   possible in production.
[00:16:07.480 --> 00:16:09.440]   You don't need a lot of expertise
[00:16:09.440 --> 00:16:11.800]   in model optimization to be able to leverage
[00:16:11.800 --> 00:16:15.720]   some of these very exciting new techniques.
[00:16:15.720 --> 00:16:18.680]   Boris, you had to scale a service that
[00:16:18.680 --> 00:16:22.560]   got very popular on the internet.
[00:16:22.560 --> 00:16:27.040]   Do you have thoughts on this or any tricks for our listeners
[00:16:27.040 --> 00:16:29.760]   on how to make these things more efficient?
[00:16:29.760 --> 00:16:33.120]   Yeah, it's actually tricky, because initially, it's
[00:16:33.120 --> 00:16:35.480]   not our background providing a service.
[00:16:35.480 --> 00:16:37.400]   Of most of the machine learning researchers,
[00:16:37.400 --> 00:16:40.320]   it's mainly training a model and maybe sometimes
[00:16:40.320 --> 00:16:42.200]   using Jupyter or Netbook.
[00:16:42.200 --> 00:16:43.680]   And so it's not necessarily providing
[00:16:43.680 --> 00:16:48.320]   a user-friendly interface that can handle a big batch.
[00:16:48.320 --> 00:16:51.960]   But I think what's important when we design a model--
[00:16:51.960 --> 00:16:54.080]   you have the goal that, for example, for image,
[00:16:54.080 --> 00:16:56.480]   you want the prettiest image possible.
[00:16:56.480 --> 00:16:59.720]   But what's important is that those models can be used, too.
[00:16:59.720 --> 00:17:02.520]   So even before training a model, I
[00:17:02.520 --> 00:17:05.960]   think it's important to see what will be your inference
[00:17:05.960 --> 00:17:07.000]   requirements.
[00:17:07.000 --> 00:17:09.160]   How fast it's going to need to be,
[00:17:09.160 --> 00:17:11.040]   how much is it going to cost, for example,
[00:17:11.040 --> 00:17:12.640]   for us to generate an image.
[00:17:12.640 --> 00:17:14.240]   And you size your model accordingly,
[00:17:14.240 --> 00:17:16.560]   because you can have the best model of the world.
[00:17:16.560 --> 00:17:19.760]   But if it costs $100 to make an image, it's not worth it.
[00:17:19.760 --> 00:17:24.800]   It's the same about solving AGI, right?
[00:17:24.800 --> 00:17:26.480]   If it costs a year's salary.
[00:17:26.480 --> 00:17:28.720]   I like a lot that example.
[00:17:28.720 --> 00:17:31.440]   You need to definitely size the model according
[00:17:31.440 --> 00:17:34.480]   to what your budget is for inference.
[00:17:34.480 --> 00:17:38.280]   Not just for training, but also for inference.
[00:17:38.280 --> 00:17:42.120]   Thank god for Moore's law.
[00:17:42.120 --> 00:17:44.600]   Boris, how do you see LLMs impacting
[00:17:44.600 --> 00:17:52.120]   the field of creative arts, such as gaming, music, literature?
[00:17:52.120 --> 00:17:56.600]   I think crayon is probably most close to that world.
[00:17:56.600 --> 00:18:00.840]   Yeah, so it was fun, because initially, the users--
[00:18:00.840 --> 00:18:04.440]   there was users a long time ago when the model was still
[00:18:04.440 --> 00:18:06.640]   not really good.
[00:18:06.640 --> 00:18:09.120]   But it was fun, because people were
[00:18:09.120 --> 00:18:11.520]   excited that it was just drawing an image.
[00:18:11.520 --> 00:18:13.840]   And maybe the fact that it was bad
[00:18:13.840 --> 00:18:17.520]   was considered as, oh, it's just doing some abstract art.
[00:18:17.520 --> 00:18:19.720]   But no, it's just the model is bad, right?
[00:18:19.720 --> 00:18:21.840]   You ask for a picture of a cat, and maybe the cat
[00:18:21.840 --> 00:18:22.800]   is completely deformed.
[00:18:22.800 --> 00:18:25.080]   And like, I guess it's some kind of abstract art.
[00:18:25.080 --> 00:18:27.400]   And you know, we fixed it with a better model.
[00:18:27.400 --> 00:18:30.800]   And people were like, oh, I prefer the image from before.
[00:18:30.800 --> 00:18:31.400]   We're better.
[00:18:31.400 --> 00:18:33.200]   You know, I like the interpretation of it.
[00:18:33.200 --> 00:18:35.000]   But it was probably not interpretation.
[00:18:35.000 --> 00:18:38.320]   It was mainly the quality of the model.
[00:18:38.320 --> 00:18:42.440]   But what's interesting is it impacts a bit
[00:18:42.440 --> 00:18:44.160]   on the entire spectrum.
[00:18:44.160 --> 00:18:46.200]   You have people who cannot draw like me.
[00:18:46.200 --> 00:18:47.840]   I'm going to use these models, and they
[00:18:47.840 --> 00:18:50.560]   are going to make something very quickly that I can immediately
[00:18:50.560 --> 00:18:52.600]   use, which is really nice.
[00:18:52.600 --> 00:18:54.920]   Same when you need to write some article or find
[00:18:54.920 --> 00:18:58.160]   a cool title or something quickly,
[00:18:58.160 --> 00:19:01.280]   you can use these models of something very quickly.
[00:19:01.280 --> 00:19:03.480]   But on the other end, people who are extremely
[00:19:03.480 --> 00:19:05.720]   skilled and talented, they also benefit from it
[00:19:05.720 --> 00:19:07.960]   because they get a lot of ideas.
[00:19:07.960 --> 00:19:10.680]   And maybe they start to use it as a starting point,
[00:19:10.680 --> 00:19:12.840]   and then they iterate over it.
[00:19:12.840 --> 00:19:16.080]   And you can see that for editing content, like text content.
[00:19:16.080 --> 00:19:19.960]   But even with images, people create an image with crayon.
[00:19:19.960 --> 00:19:22.640]   Then the ones that are actually skilled,
[00:19:22.640 --> 00:19:25.400]   they put it into Photoshop, and they add lightning.
[00:19:25.400 --> 00:19:29.960]   They add random effects that bring it to another level.
[00:19:29.960 --> 00:19:32.720]   So the people who are skilled, everybody progresses a lot.
[00:19:32.720 --> 00:19:36.880]   People who are not skilled progress to an average level
[00:19:36.880 --> 00:19:37.800]   or maybe good level.
[00:19:37.800 --> 00:19:41.000]   And people who are good become extremely good and faster at it.
[00:19:41.000 --> 00:19:44.200]   So it's kind of nice to see that impact.
[00:19:44.200 --> 00:19:46.120]   Just a little bit on the text side.
[00:19:46.120 --> 00:19:50.600]   So some of the initial killer apps for text generation models
[00:19:50.600 --> 00:19:53.000]   were games, were interactive text games,
[00:19:53.000 --> 00:19:55.440]   like AI Dungeon, for example.
[00:19:55.440 --> 00:20:01.520]   And there's a lot of exploration about how can characters
[00:20:01.520 --> 00:20:03.280]   rely on generative models and how
[00:20:03.280 --> 00:20:07.800]   to establish their characters and their speaking behaviors
[00:20:07.800 --> 00:20:10.880]   via text generation models.
[00:20:10.880 --> 00:20:15.680]   But I do echo Boris's sentiment that, yes,
[00:20:15.680 --> 00:20:19.960]   while the models can create an entire thing on their own,
[00:20:19.960 --> 00:20:22.920]   I'm most excited about them as brainstorming tools
[00:20:22.920 --> 00:20:28.200]   and tools that aid creatives and help them take the next level.
[00:20:28.200 --> 00:20:31.640]   And if you're stuck with a blank canvas,
[00:20:31.640 --> 00:20:34.560]   maybe you can have four different possible starting
[00:20:34.560 --> 00:20:35.520]   points to start from.
[00:20:35.520 --> 00:20:38.160]   And then the human sort of guides that creative process.
[00:20:38.160 --> 00:20:42.480]   That, to me, is an exciting area for this.
[00:20:42.480 --> 00:20:43.480]   Yeah, for sure.
[00:20:43.480 --> 00:20:45.560]   There's also a lot of very cool stuff
[00:20:45.560 --> 00:20:52.120]   happening with music lately, which has been very cool to see.
[00:20:52.120 --> 00:20:57.240]   Hanlon, as this space continues to evolve, specifically
[00:20:57.240 --> 00:21:03.320]   these large language models, the way I kind of think about it
[00:21:03.320 --> 00:21:05.280]   is if I were a company that wanted
[00:21:05.280 --> 00:21:08.360]   to incorporate this technology into my organization,
[00:21:08.360 --> 00:21:10.080]   I kind of have three options.
[00:21:10.080 --> 00:21:15.960]   I can use an API and just give it
[00:21:15.960 --> 00:21:19.240]   kind of one shot or zero shot learning prompts
[00:21:19.240 --> 00:21:21.240]   to have it do what I want to do.
[00:21:21.240 --> 00:21:25.480]   I could use an API and fine tune that model,
[00:21:25.480 --> 00:21:29.680]   or I could manage the whole stack myself
[00:21:29.680 --> 00:21:32.000]   or with the help of companies like Mosaic.
[00:21:32.000 --> 00:21:35.240]   Where do you kind of see this space going towards?
[00:21:35.240 --> 00:21:38.840]   Or what are your thoughts as to kind of where the enterprise is
[00:21:38.840 --> 00:21:40.720]   going to put their money?
[00:21:40.720 --> 00:21:44.040]   Yeah, I think eventually all three might coexist
[00:21:44.040 --> 00:21:45.480]   inside the same--
[00:21:45.480 --> 00:21:47.400]   inside these same enterprises.
[00:21:47.400 --> 00:21:50.480]   Many of them, maybe for data privacy reasons,
[00:21:50.480 --> 00:21:53.040]   or model ownership, or they're working with unique data,
[00:21:53.040 --> 00:21:56.880]   like tax compliance data or genomics,
[00:21:56.880 --> 00:22:00.520]   they're going to want to train their own models and own them.
[00:22:00.520 --> 00:22:02.680]   But then for other same use cases
[00:22:02.680 --> 00:22:05.360]   inside the same organization, it would
[00:22:05.360 --> 00:22:08.560]   be advantageous for them to be fine tuning
[00:22:08.560 --> 00:22:12.640]   via amazing APIs like Cohere.
[00:22:12.640 --> 00:22:15.640]   I think for us, at least, we see a lot of enterprises
[00:22:15.640 --> 00:22:18.680]   gravitate towards sort of the model ownership
[00:22:18.680 --> 00:22:20.840]   and the unique data that they have
[00:22:20.840 --> 00:22:23.680]   and wanting to be able to pre-train their own models
[00:22:23.680 --> 00:22:26.720]   there and seeing the ROI for that.
[00:22:26.720 --> 00:22:30.920]   Yeah, I'd say in our experience, selling to enterprises,
[00:22:30.920 --> 00:22:34.000]   the data kind of leaving their networks or silos
[00:22:34.000 --> 00:22:35.920]   is often a non-starter.
[00:22:35.920 --> 00:22:39.960]   So having ways to do this on-premises
[00:22:39.960 --> 00:22:44.120]   is definitely something the enterprise wants badly.
[00:22:44.120 --> 00:22:46.200]   Yeah, absolutely.
[00:22:46.200 --> 00:22:48.520]   Any other folks wager a prediction
[00:22:48.520 --> 00:22:51.920]   on which of the three methods will turn out
[00:22:51.920 --> 00:22:54.560]   to be the most popular or how it's going to pan out?
[00:22:54.560 --> 00:22:56.920]   Yeah, I do echo the same sentiment.
[00:22:56.920 --> 00:22:58.960]   With these models, domain adaptation
[00:22:58.960 --> 00:23:01.840]   is one thing that's, for example, needed.
[00:23:01.840 --> 00:23:07.720]   So if you're a company in, let's say, the legal space,
[00:23:07.720 --> 00:23:11.320]   your models would benefit by being optimized
[00:23:11.320 --> 00:23:14.720]   for the language that is used in that use case.
[00:23:14.720 --> 00:23:19.360]   So that has been a common method of boosting
[00:23:19.360 --> 00:23:21.520]   the performance of these models.
[00:23:21.520 --> 00:23:22.920]   And the more tooling that you have
[00:23:22.920 --> 00:23:28.040]   to be able to experiment quickly and the easier fine-tuning
[00:23:28.040 --> 00:23:32.920]   is the more ability to run a lot of experiments.
[00:23:32.920 --> 00:23:35.120]   Because at the end of the time, at the end of the day,
[00:23:35.120 --> 00:23:38.200]   you're not just going to have one or two or three models.
[00:23:38.200 --> 00:23:42.200]   Systems might end up having thousands of AI touchpoints
[00:23:42.200 --> 00:23:45.320]   that would need to be tweaked one way or another.
[00:23:45.320 --> 00:23:46.480]   Cool.
[00:23:46.480 --> 00:23:52.360]   All right, this is a big, juicy question.
[00:23:52.360 --> 00:23:54.560]   I'll open it up to the group.
[00:23:54.560 --> 00:23:56.720]   With all the hype around LLMs and what
[00:23:56.720 --> 00:23:59.640]   they're able to do through this statistical modeling
[00:23:59.640 --> 00:24:05.200]   of predicting, say, the next word,
[00:24:05.200 --> 00:24:09.840]   are we just scaling these up to get to human-level reasoning
[00:24:09.840 --> 00:24:12.560]   or the ability to think about past events
[00:24:12.560 --> 00:24:13.800]   and predict future events?
[00:24:13.800 --> 00:24:19.240]   Or is there a fundamental architecture shift?
[00:24:19.240 --> 00:24:21.680]   I know this is very broad and speculative,
[00:24:21.680 --> 00:24:24.440]   but I'm curious if folks have thoughts as to,
[00:24:24.440 --> 00:24:28.320]   is the transformer the way to AGI?
[00:24:28.320 --> 00:24:32.840]   Or do there need to be meaningful advancements
[00:24:32.840 --> 00:24:38.000]   to get to human-level reasoning?
[00:24:38.000 --> 00:24:41.400]   The one thing I'd say is that we are
[00:24:41.400 --> 00:24:44.680]   able to get this far using predict the next word is
[00:24:44.680 --> 00:24:46.680]   absolutely surprising.
[00:24:46.680 --> 00:24:49.080]   And the closest or the most impressive
[00:24:49.080 --> 00:24:52.240]   I've seen it deal with reasoning is something
[00:24:52.240 --> 00:24:55.560]   like Google's Minerva that is able to solve
[00:24:55.560 --> 00:24:59.000]   mathematical problems using a generative model that's
[00:24:59.000 --> 00:25:02.280]   fine-tuned on that data set and then using advanced prompting
[00:25:02.280 --> 00:25:05.640]   techniques like chain of thought, scratchpad.
[00:25:05.640 --> 00:25:08.520]   So that we have these methods that
[00:25:08.520 --> 00:25:13.280]   are able to do that just based on these base models,
[00:25:13.280 --> 00:25:15.200]   I think nobody expected that we'd
[00:25:15.200 --> 00:25:20.040]   get this far using these models.
[00:25:20.040 --> 00:25:22.200]   Is just the transformer, is it just scaling
[00:25:22.200 --> 00:25:26.360]   the models and the data sets?
[00:25:26.360 --> 00:25:28.680]   Most likely, I would say there's probably
[00:25:28.680 --> 00:25:32.440]   a need for more components.
[00:25:32.440 --> 00:25:36.160]   But that can be one of the components that
[00:25:36.160 --> 00:25:42.320]   creates or makes these future models that reason better.
[00:25:42.320 --> 00:25:45.080]   But definitely things that would help
[00:25:45.080 --> 00:25:47.960]   are things like embodiment and the ability
[00:25:47.960 --> 00:25:52.280]   to interact with the outside world
[00:25:52.280 --> 00:25:54.040]   and learn from these interactions
[00:25:54.040 --> 00:25:57.480]   and their consequences.
[00:25:57.480 --> 00:26:01.680]   And then potentially also social interaction with people.
[00:26:01.680 --> 00:26:04.840]   That's a more complex horizon.
[00:26:04.840 --> 00:26:08.960]   There's a great paper that deals about these,
[00:26:08.960 --> 00:26:12.080]   and calls them world scopes.
[00:26:12.080 --> 00:26:14.400]   That includes embodiment, multimodality, and then
[00:26:14.400 --> 00:26:18.320]   social interaction.
[00:26:18.320 --> 00:26:19.400]   I think as a--
[00:26:19.400 --> 00:26:21.440]   I'll just say, I think as a former computational
[00:26:21.440 --> 00:26:24.480]   neuroscience, I would be very disappointed
[00:26:24.480 --> 00:26:28.160]   if the transformer architecture is
[00:26:28.160 --> 00:26:30.920]   the equivalent of, what is it, like Fukuyama's
[00:26:30.920 --> 00:26:37.040]   end of political governance?
[00:26:37.040 --> 00:26:41.120]   If transformers is that equivalent,
[00:26:41.120 --> 00:26:44.760]   there's so much intricate structure in the human brain
[00:26:44.760 --> 00:26:47.960]   that requires to deliver this type of intelligence.
[00:26:47.960 --> 00:26:49.800]   It would be disappointing for me.
[00:26:49.800 --> 00:26:52.920]   On the other hand, there are theories that say,
[00:26:52.920 --> 00:26:55.400]   like the retina, the sophisticated visual processing
[00:26:55.400 --> 00:26:58.080]   we do, was evolved on the basis of being
[00:26:58.080 --> 00:26:59.400]   able to predict the next pixel.
[00:26:59.400 --> 00:27:03.560]   Because that is essential to human, to animal behaviors,
[00:27:03.560 --> 00:27:05.560]   to be able to predict what's going to happen next
[00:27:05.560 --> 00:27:06.400]   in the pixel space.
[00:27:06.400 --> 00:27:11.320]   So both sides are coming at a very exciting
[00:27:11.320 --> 00:27:14.640]   to see how this evolves over the next 5, 10 years.
[00:27:14.640 --> 00:27:17.160]   Yeah, I feel very grateful to be living in this time.
[00:27:17.160 --> 00:27:18.520]   I can't wait to see what's next.
[00:27:18.520 --> 00:27:20.800]   I think there's actually still probably a lot of tricks
[00:27:20.800 --> 00:27:22.160]   to be found.
[00:27:22.160 --> 00:27:26.200]   We have very basic building blocks in the end,
[00:27:26.200 --> 00:27:30.080]   like linear layers and a few non-linear functions.
[00:27:30.080 --> 00:27:33.240]   It's very, very simple overall what we use.
[00:27:33.240 --> 00:27:35.560]   And attention changed a lot of things.
[00:27:35.560 --> 00:27:37.880]   And I think there's some other things magical
[00:27:37.880 --> 00:27:40.640]   that are going to happen, like whether on the architecture,
[00:27:40.640 --> 00:27:43.240]   maybe all of the optimizers are still
[00:27:43.240 --> 00:27:45.720]   maybe a bit rudimentary.
[00:27:45.720 --> 00:27:48.600]   And some things that can make a big difference.
[00:27:48.600 --> 00:27:51.200]   What's fun a bit and a bit sad at the same time
[00:27:51.200 --> 00:27:55.320]   is people try to optimize a lot the architecture.
[00:27:55.320 --> 00:28:00.320]   And as we get more compute power with the evolution of GPUs
[00:28:00.320 --> 00:28:03.040]   and all, we think, oh, OK, training becomes cheaper.
[00:28:03.040 --> 00:28:04.440]   But it's not really what happens.
[00:28:04.440 --> 00:28:06.480]   What happens is people use immediately
[00:28:06.480 --> 00:28:07.880]   that to make bigger models.
[00:28:07.880 --> 00:28:11.160]   As a GPU becomes twice cheaper, nice,
[00:28:11.160 --> 00:28:13.160]   I can make models twice bigger.
[00:28:13.160 --> 00:28:15.680]   So it actually evolved a bit in that direction.
[00:28:15.680 --> 00:28:18.320]   And somehow that's what works the best,
[00:28:18.320 --> 00:28:22.320]   just scaling up the model has the biggest impact of fun.
[00:28:22.320 --> 00:28:24.880]   Like the open-end models often are still
[00:28:24.880 --> 00:28:28.200]   like a simple GPT old architecture.
[00:28:28.200 --> 00:28:29.960]   And they work perfectly well.
[00:28:29.960 --> 00:28:31.920]   And so it's almost a bit sad.
[00:28:31.920 --> 00:28:34.120]   It's impressive and sad at the same time
[00:28:34.120 --> 00:28:38.840]   that it works so well with no magic trick behind it.
[00:28:38.840 --> 00:28:41.760]   But I think some other tricks will make an impact,
[00:28:41.760 --> 00:28:43.960]   like attention did.
[00:28:43.960 --> 00:28:44.760]   Very cool.
[00:28:44.760 --> 00:28:45.600]   All right.
[00:28:45.600 --> 00:28:46.840]   Final question, team.
[00:28:46.840 --> 00:28:48.360]   I'll start with you, Hanlon.
[00:28:48.360 --> 00:28:50.080]   But we can go around.
[00:28:50.080 --> 00:28:53.720]   For the folks watching today's panel,
[00:28:53.720 --> 00:28:57.280]   what advice would you give to someone
[00:28:57.280 --> 00:29:00.360]   who's potentially looking to get started in AI or LLMs?
[00:29:00.360 --> 00:29:03.680]   And what skills do you think are going
[00:29:03.680 --> 00:29:05.600]   to be most important to be successful
[00:29:05.600 --> 00:29:08.800]   in this new and emerging field in the years to come?
[00:29:08.800 --> 00:29:11.320]   Yeah, when I started first in the ML space,
[00:29:11.320 --> 00:29:13.280]   I knew no Python.
[00:29:13.280 --> 00:29:18.280]   I had done my PhD in MATLAB, of all languages.
[00:29:18.280 --> 00:29:21.640]   So I think what I've learned is that skill set-wise,
[00:29:21.640 --> 00:29:25.160]   just good software engineering skill and dogged persistence
[00:29:25.160 --> 00:29:30.040]   is much more important than being a genius researcher
[00:29:30.040 --> 00:29:33.920]   or coming up with some fancy new theoretical technique.
[00:29:33.920 --> 00:29:36.880]   Training these models can be extraordinarily difficult.
[00:29:36.880 --> 00:29:40.240]   And a good software engineering base is what I would advise.
[00:29:40.240 --> 00:29:44.160]   I think what's amazing about this field is that, especially
[00:29:44.160 --> 00:29:47.200]   with this panel assembled, is that there's
[00:29:47.200 --> 00:29:51.360]   so much amazing open source tooling and educational
[00:29:51.360 --> 00:29:54.480]   material out there that I've talked
[00:29:54.480 --> 00:29:57.680]   to folks who come from non-traditional backgrounds
[00:29:57.680 --> 00:30:01.120]   who have made very immense impacts in ML just
[00:30:01.120 --> 00:30:03.320]   because of the accessibility of that.
[00:30:03.320 --> 00:30:06.800]   So just get started and dig in.
[00:30:06.800 --> 00:30:09.920]   There's no coursework you have to take to hold you back.
[00:30:09.920 --> 00:30:12.920]   It's get in, tinker, learn, and build.
[00:30:12.920 --> 00:30:14.480]   That's really the best way to do it.
[00:30:14.480 --> 00:30:16.720]   And that's very exciting to me.
[00:30:16.720 --> 00:30:19.840]   Jay, any advice for the folks?
[00:30:19.840 --> 00:30:24.480]   I think that aligns with exactly what I was going to say.
[00:30:24.480 --> 00:30:27.200]   So definitely find something that excites you.
[00:30:27.200 --> 00:30:28.280]   Get curious.
[00:30:28.280 --> 00:30:30.080]   Follow your curiosity.
[00:30:30.080 --> 00:30:32.440]   And then build in public and share what you know.
[00:30:32.440 --> 00:30:35.680]   So Boris is definitely one of my heroes
[00:30:35.680 --> 00:30:38.800]   for building all the incredible things that he's built,
[00:30:38.800 --> 00:30:42.440]   but also sharing them publicly and sharing the learnings
[00:30:42.440 --> 00:30:48.600]   publicly and giving ability to thousands or millions of people
[00:30:48.600 --> 00:30:52.400]   to try tech that was not accessible outside a couple
[00:30:52.400 --> 00:30:56.960]   of labs when some of the models that he released were outside.
[00:30:56.960 --> 00:30:59.120]   So definitely learn in public.
[00:30:59.120 --> 00:30:59.640]   Share.
[00:30:59.640 --> 00:31:03.480]   Don't be held back by imposter syndrome.
[00:31:03.480 --> 00:31:06.480]   There's value in a lot of the content and educational.
[00:31:06.480 --> 00:31:09.520]   And even if you just curate the resources that
[00:31:09.520 --> 00:31:13.360]   work for you to learn from, that's a valuable value add.
[00:31:13.360 --> 00:31:15.840]   And we're all learning from each other.
[00:31:15.840 --> 00:31:19.960]   And machine learning is absolutely an exciting field
[00:31:19.960 --> 00:31:22.160]   that everybody collaborates this way.
[00:31:22.160 --> 00:31:24.360]   And it's been absolutely inspiring
[00:31:24.360 --> 00:31:26.320]   to see all of this collaboration.
[00:31:26.320 --> 00:31:28.200]   Yeah, I would agree with that.
[00:31:28.200 --> 00:31:31.520]   There's so much good content nowadays online that--
[00:31:31.520 --> 00:31:34.320]   often, I guess if you want to start in machine learning,
[00:31:34.320 --> 00:31:36.440]   people will probably start looking at--
[00:31:36.440 --> 00:31:37.600]   and I probably did that--
[00:31:37.640 --> 00:31:39.720]   what's the best ML course?
[00:31:39.720 --> 00:31:41.560]   What's the best ML framework?
[00:31:41.560 --> 00:31:43.400]   Honestly, it doesn't really matter.
[00:31:43.400 --> 00:31:45.640]   They're all really, really good.
[00:31:45.640 --> 00:31:47.200]   What matters is just starting.
[00:31:47.200 --> 00:31:50.760]   Find some subject that interests you and just try it out.
[00:31:50.760 --> 00:31:51.920]   Try to run it.
[00:31:51.920 --> 00:31:53.920]   Try to tweak it based on what you want to do.
[00:31:53.920 --> 00:31:55.640]   And they're all great.
[00:31:55.640 --> 00:31:58.920]   There's no bad answers.
[00:31:58.920 --> 00:32:01.360]   The only thing I'd add is I would say use weights
[00:32:01.360 --> 00:32:05.640]   and biases, but I'm a little biased.
[00:32:05.640 --> 00:32:06.400]   Well, cool, guys.
[00:32:06.400 --> 00:32:09.240]   It's been lovely chatting with you all today.
[00:32:09.240 --> 00:32:14.320]   Also, I did want to give a shout out to ChatGPT
[00:32:14.320 --> 00:32:17.560]   for providing some of today's questions, actually.
[00:32:17.560 --> 00:32:20.800]   It was very good at generating questions for the panel.
[00:32:20.800 --> 00:32:24.840]   So if anyone out there needs to moderate a panel, use an LLM.
[00:32:24.840 --> 00:32:28.200]   And with that, we'll say goodbye.
[00:32:28.200 --> 00:32:29.000]   Thanks, everyone.
[00:32:29.000 --> 00:32:32.360]   [MUSIC PLAYING]
[00:32:32.360 --> 00:32:34.940]   (upbeat music)
[00:32:34.940 --> 00:32:37.700]   (logo whooshing)

