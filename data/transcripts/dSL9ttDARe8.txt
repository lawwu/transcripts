
[00:00:00.000 --> 00:00:06.480]   We have this 7-Eleven that gets burglarized like literally once a week.
[00:00:06.480 --> 00:00:10.000]   Guy walks up with a crowbar and he swings at the door.
[00:00:10.000 --> 00:00:13.520]   And that's the end of our video because our guards get on and say, "Hey, jerk, get out
[00:00:13.520 --> 00:00:14.520]   of here.
[00:00:14.520 --> 00:00:15.520]   The police are on their way."
[00:00:15.520 --> 00:00:16.520]   And the guy walks away.
[00:00:16.520 --> 00:00:20.520]   Whereas if you had a dumb camera, you get this really cool video that for the next 45
[00:00:20.520 --> 00:00:23.240]   seconds, you see this guy banging on a window.
[00:00:23.240 --> 00:00:27.360]   And so what we've had to do is we've had to train the market that like, "Hey, prevention
[00:00:27.360 --> 00:00:28.360]   is possible."
[00:00:28.360 --> 00:00:32.480]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:32.480 --> 00:00:34.160]   models work in the real world.
[00:00:34.160 --> 00:00:36.240]   I'm your host, Lukas Biewald.
[00:00:36.240 --> 00:00:40.200]   Dave Selinger is the co-founder and CEO of Deep Sentinel, an intelligent crime prediction
[00:00:40.200 --> 00:00:46.160]   and prevention system that stops crime before it happens using deep learning vision techniques.
[00:00:46.160 --> 00:00:51.480]   Prior to founding Deep Sentinel, Dave co-founded Rich Relevance, an AI recommendation company.
[00:00:51.480 --> 00:00:53.760]   Super excited to talk to him.
[00:00:53.760 --> 00:00:57.720]   Maybe you could actually start by just describing what your company does and how it uses deep
[00:00:57.720 --> 00:00:58.720]   learning.
[00:00:58.720 --> 00:00:59.720]   Sure.
[00:00:59.720 --> 00:01:01.520]   So Deep Sentinel is a physical security company.
[00:01:01.520 --> 00:01:05.400]   So we have cameras and we actually protect facilities.
[00:01:05.400 --> 00:01:07.880]   It's not cybersecurity protecting computers.
[00:01:07.880 --> 00:01:13.200]   We are kind of a competitor to ADT, which I'm sure you've seen the lawn signs for.
[00:01:13.200 --> 00:01:18.000]   And the whole premise behind that business was, "Hey, ADT doesn't work."
[00:01:18.000 --> 00:01:21.560]   A lot of people don't know this, but like police departments across the country basically
[00:01:21.560 --> 00:01:26.420]   don't respond to burglar alarms because they're 99% false alarms.
[00:01:26.420 --> 00:01:29.400]   And so they just really don't help people protect their families.
[00:01:29.400 --> 00:01:32.960]   Maybe it makes you feel better for a little while, but it doesn't actually solve the problem.
[00:01:32.960 --> 00:01:35.480]   And so I started looking at what really wealthy people do.
[00:01:35.480 --> 00:01:40.000]   And they have people that sit in a guard shack and they watch cameras all day.
[00:01:40.000 --> 00:01:45.880]   And they pay tens of thousands of dollars a month for that service because it works.
[00:01:45.880 --> 00:01:51.040]   And the problem is obviously if you got a bill for $25,000 a month, I would say there's
[00:01:51.040 --> 00:01:55.900]   a very small population that's going to write a check for that every single month.
[00:01:55.900 --> 00:02:01.920]   And so what we realized was that if we use AI to both make the guards more effective,
[00:02:01.920 --> 00:02:05.800]   more efficient, and able to do their job, and then we use all the technologies that
[00:02:05.800 --> 00:02:09.320]   are available now to kind of drive that into unit economics, where instead of having one
[00:02:09.320 --> 00:02:14.440]   guard or 10 guards protecting one house, we have one guard protecting 10 homes, then protecting
[00:02:14.440 --> 00:02:17.480]   100 homes and now protecting 500 homes.
[00:02:17.480 --> 00:02:20.040]   And that's really what our business is all about.
[00:02:20.040 --> 00:02:21.040]   Gotcha.
[00:02:21.180 --> 00:02:26.140]   So the vision systems are sort of deciding which cameras are interesting to look at.
[00:02:26.140 --> 00:02:29.100]   And then a human actually kind of verifies that there's something going on.
[00:02:29.100 --> 00:02:30.100]   Is that right?
[00:02:30.100 --> 00:02:31.100]   Yeah.
[00:02:31.100 --> 00:02:32.100]   I mean, at its rawest, that's exactly right.
[00:02:32.100 --> 00:02:35.100]   We use vision systems, which is where deep neural networks have made a lot of progress
[00:02:35.100 --> 00:02:36.980]   to choose what to look at.
[00:02:36.980 --> 00:02:41.060]   And then we're getting increasingly sophisticated with that to not only choose which cameras
[00:02:41.060 --> 00:02:43.100]   to look at, but where to look in the cameras.
[00:02:43.100 --> 00:02:44.100]   What are the areas of interest?
[00:02:44.100 --> 00:02:48.420]   Hey, if something happened five seconds ago, here are the references that kind of wrap
[00:02:48.420 --> 00:02:49.420]   around this event.
[00:02:49.420 --> 00:02:54.560]   So that you as a human being have the full context, even in the course of like half a
[00:02:54.560 --> 00:02:55.560]   second.
[00:02:55.560 --> 00:02:59.800]   So how much of that is sort of interface for the operator and how much of it is like intelligence?
[00:02:59.800 --> 00:03:03.200]   Like where do you sort of invest your effort?
[00:03:03.200 --> 00:03:04.200]   That's a great question, right?
[00:03:04.200 --> 00:03:08.320]   So because nothing exists like this, we had to build the company in a vertically integrated
[00:03:08.320 --> 00:03:09.320]   fashion.
[00:03:09.320 --> 00:03:10.880]   So we actually build our own hardware.
[00:03:10.880 --> 00:03:14.360]   Behind me, we have our hub, which we actually built ourselves.
[00:03:14.360 --> 00:03:17.640]   And that runs the AI in the customer's home.
[00:03:17.640 --> 00:03:19.680]   And so we've had to invest a lot of effort.
[00:03:19.680 --> 00:03:22.920]   Vertically integrated business is like by far the most complex way to do it.
[00:03:22.920 --> 00:03:25.160]   So we have a lot of investments that's gone into AI.
[00:03:25.160 --> 00:03:27.640]   We've got a lot of investment that's gone into operations.
[00:03:27.640 --> 00:03:31.120]   But really the thesis of the business is, hey, AI can change all of those things.
[00:03:31.120 --> 00:03:35.400]   If you use AI correctly, and this is an AI driven business, because of the nature of
[00:03:35.400 --> 00:03:38.960]   what we do, which is video oriented, we can make our operations team smarter.
[00:03:38.960 --> 00:03:40.640]   We can make our customer care team smarter.
[00:03:40.640 --> 00:03:43.880]   We can make our engineering team smarter by integrating that throughout.
[00:03:43.880 --> 00:03:44.880]   Interesting.
[00:03:44.880 --> 00:03:47.040]   So what's the part that runs in the customer's home?
[00:03:47.040 --> 00:03:49.760]   You actually do some processing before it even goes to you.
[00:03:49.760 --> 00:03:50.760]   Yeah.
[00:03:50.760 --> 00:03:51.960]   I'll tilt this to the side here.
[00:03:51.960 --> 00:03:54.520]   And so, so right there is our, our hub.
[00:03:54.520 --> 00:03:56.360]   It's right here in my office.
[00:03:56.360 --> 00:04:00.760]   Down here are the next generation versions of the hub, which we're working on right now.
[00:04:00.760 --> 00:04:05.960]   This is both my home office and our R&D lab, seeing as, as COVID has helped us really concentrate
[00:04:05.960 --> 00:04:06.960]   our real estate effort.
[00:04:06.960 --> 00:04:11.040]   But yeah, so what we run in the home actually is almost everything.
[00:04:11.040 --> 00:04:14.240]   So we've really focused on moving everything to the edge.
[00:04:14.240 --> 00:04:18.360]   And there's a lot of kind of work that's being done right now on edge processing.
[00:04:18.360 --> 00:04:24.200]   From what we're seeing and what we're building on, that curve is just astronomically improving
[00:04:24.200 --> 00:04:25.680]   year over year.
[00:04:25.680 --> 00:04:32.040]   What we currently do is we run a version of Linux in the home that has basically all of
[00:04:32.040 --> 00:04:33.280]   our BI stack.
[00:04:33.280 --> 00:04:38.600]   It's all encrypted and whatnot, but we run almost all of the business logic in the home
[00:04:38.600 --> 00:04:41.520]   so that the decisions can be made in real time with the camera.
[00:04:41.520 --> 00:04:46.720]   And as you might guess, in a security context, the difference between real time and 500 millisecond
[00:04:46.720 --> 00:04:50.140]   latency in communication is everything.
[00:04:50.140 --> 00:04:54.800]   And so we're able to do stuff at just very, very rapid speeds by doing that.
[00:04:54.800 --> 00:04:56.800]   And so what exactly is the business decision?
[00:04:56.800 --> 00:04:59.280]   Is it like, is there like a possible issue?
[00:04:59.280 --> 00:05:00.560]   Is that, is that right?
[00:05:00.560 --> 00:05:01.560]   Yeah.
[00:05:01.560 --> 00:05:05.260]   So, so if you think about if you've got a ring at your house or a Nest camera and you
[00:05:05.260 --> 00:05:10.480]   actually turn on alerts, which I highly recommend you don't do, you're going to get like a thousand
[00:05:10.480 --> 00:05:11.920]   different alerts.
[00:05:11.920 --> 00:05:17.400]   And so the first decision that the AI has to make is, is this alert worth reviewing
[00:05:17.400 --> 00:05:20.040]   in further detail and to what, to what detail?
[00:05:20.040 --> 00:05:25.160]   Then the second thing it needs to determine is what's in the field of view and is that
[00:05:25.160 --> 00:05:28.920]   worthy of taking the time of a human guard to review this?
[00:05:28.920 --> 00:05:32.200]   And then the third decision is when the guard is reviewing this, what are all the other
[00:05:32.200 --> 00:05:35.960]   pieces of information I need to share with that guard to make sure they can make an informed
[00:05:35.960 --> 00:05:38.980]   decision in the course of a couple of seconds?
[00:05:38.980 --> 00:05:43.640]   And so again, all of that runs in the home on that hub.
[00:05:43.640 --> 00:05:49.840]   And then by doing it in that local situation, all that kind of back and forth communication
[00:05:49.840 --> 00:05:56.560]   happens in sub 10 millisecond type timeframes versus 20 to 500 milliseconds back and forth
[00:05:56.560 --> 00:05:57.560]   to the cloud.
[00:05:57.560 --> 00:05:59.080]   Well, that's really interesting.
[00:05:59.080 --> 00:06:00.480]   Can you talk about your hardware?
[00:06:00.480 --> 00:06:01.480]   Yeah, sure.
[00:06:01.480 --> 00:06:05.520]   So the current version of hardware that we're running is a Qualcomm Snapdragon 820.
[00:06:05.520 --> 00:06:09.020]   So it's what you'd find in a Samsung Galaxy S6.
[00:06:09.020 --> 00:06:11.660]   And so we literally actually, if you open up our hub and you like peel back all the
[00:06:11.660 --> 00:06:16.060]   boards, you're going to see something that is exactly shaped like this at the bottom
[00:06:16.060 --> 00:06:17.140]   of the hub.
[00:06:17.140 --> 00:06:20.200]   And there's a bunch of circuitry around it, but it's literally kind of the reference design
[00:06:20.200 --> 00:06:23.220]   for the Samsung Galaxy S6 sitting in the middle.
[00:06:23.220 --> 00:06:27.060]   And then we put all of our electronics around the outside of that.
[00:06:27.060 --> 00:06:32.900]   And that was a quite an adventure for me because, you know, going into this, my experience with
[00:06:32.900 --> 00:06:38.420]   hardware had been the robotics that I built in a lab at Stanford when I was there.
[00:06:38.420 --> 00:06:42.500]   And that was, you know, build it once, use it once and, you know, make it do everything
[00:06:42.500 --> 00:06:43.500]   that it can.
[00:06:43.500 --> 00:06:46.780]   And then, you know, a bunch of Raspberry Pis and the types of stuff that you build in your
[00:06:46.780 --> 00:06:47.780]   house, right?
[00:06:47.780 --> 00:06:52.300]   Like that's a robot that I built of a BB-8, you know, using a bunch of Raspberry Pis and
[00:06:52.300 --> 00:06:53.960]   Arduinos and stuff like that.
[00:06:53.960 --> 00:06:59.340]   And so you think you know about hardware, just very, very briefly, I will summarize
[00:06:59.340 --> 00:07:01.780]   all of my learnings in the last four years.
[00:07:01.780 --> 00:07:03.780]   You don't know diddly squat.
[00:07:03.780 --> 00:07:06.620]   If that's what you know, you don't know anything about hardware.
[00:07:06.620 --> 00:07:11.740]   And so we really had to take the time to learn about how do we design that and manufacture
[00:07:11.740 --> 00:07:15.180]   that with high quality and solve a bunch of the problems.
[00:07:15.180 --> 00:07:18.380]   That last mile of real human beings is really important.
[00:07:18.380 --> 00:07:23.540]   Yeah, I feel like all of my friends that do hardware, I'm a little jealous because it
[00:07:23.540 --> 00:07:29.240]   seems so cool and they absolutely hate it and always complain about it.
[00:07:29.240 --> 00:07:30.580]   So I don't hate it.
[00:07:30.580 --> 00:07:36.340]   I just I would say, though, that the amount of learning that I experienced versus the
[00:07:36.340 --> 00:07:40.060]   amount that I expected is a ratio of about 10 to 1.
[00:07:40.060 --> 00:07:43.180]   Whereas, you know, for most smart people, like you go into something, you kind of get
[00:07:43.180 --> 00:07:47.500]   a sense of it and you're like, you know, maybe I'm off by 40 percent or 50 percent or maybe,
[00:07:47.500 --> 00:07:48.820]   you know, it's double.
[00:07:48.820 --> 00:07:52.700]   Hardware was definitely a 10 to 1 ratio of how much I had to learn in order to get into
[00:07:52.700 --> 00:07:54.260]   market and be productive.
[00:07:54.260 --> 00:07:55.260]   Interesting.
[00:07:55.260 --> 00:07:56.260]   I wanted to ask you.
[00:07:56.260 --> 00:07:57.260]   It's cool stuff, though.
[00:07:57.260 --> 00:08:00.180]   I mean, it's amazing what's happening in hardware right now.
[00:08:00.180 --> 00:08:01.180]   I will say it.
[00:08:01.180 --> 00:08:02.180]   Sorry to interrupt you.
[00:08:02.180 --> 00:08:03.180]   Oh, yeah.
[00:08:03.180 --> 00:08:04.180]   Like what's going on?
[00:08:04.180 --> 00:08:06.860]   I mean, the Google Coral board is really phenomenal.
[00:08:06.860 --> 00:08:09.140]   The work that NVIDIA has been doing is really great.
[00:08:09.140 --> 00:08:12.900]   I think my favorite thing, though, is just that it's not all about NVIDIA.
[00:08:12.900 --> 00:08:18.500]   So like on the training side, it's still really like an NVIDIA dominated world.
[00:08:18.500 --> 00:08:23.100]   You can get into China where in China I see a lot of new R&D happening outside of the
[00:08:23.100 --> 00:08:24.180]   NVIDIA world.
[00:08:24.180 --> 00:08:28.420]   But a lot of that's not really readily available to us here.
[00:08:28.420 --> 00:08:33.780]   On the mobile and edge side of the world, you've got everything from like Rockchip has
[00:08:33.780 --> 00:08:39.460]   what they're calling an NPU that is a neural processing unit and it's an accelerator.
[00:08:39.460 --> 00:08:43.020]   You've got the Google Coral board, which is driving quantization, which I think is super
[00:08:43.020 --> 00:08:44.020]   sweet.
[00:08:44.020 --> 00:08:48.180]   That makes things much simpler from a mathematical perspective, but way, way, way, way faster
[00:08:48.180 --> 00:08:50.060]   and way lower power.
[00:08:50.060 --> 00:08:53.060]   You've got NVIDIA doing their Jetson series.
[00:08:53.060 --> 00:08:57.020]   But I think overall, what I would say is that in the training side of the world, NVIDIA
[00:08:57.020 --> 00:08:59.380]   is here and everybody else is here.
[00:08:59.380 --> 00:09:02.780]   On the runtime side of the world, NVIDIA is here and there are people that I think that
[00:09:02.780 --> 00:09:08.700]   are better, significantly better on a cost for performance and an overall performance
[00:09:08.700 --> 00:09:09.860]   basis.
[00:09:09.860 --> 00:09:13.420]   And so we're seeing a lot of innovation happening there.
[00:09:13.420 --> 00:09:20.500]   NXP, which is another chip manufacturer, they're launching their own NPU in Q1 of 2021.
[00:09:20.500 --> 00:09:23.780]   And there's just a lot of promise to see a lot of competition, which I think is going
[00:09:23.780 --> 00:09:25.700]   to drive a lot of improvement.
[00:09:25.700 --> 00:09:28.660]   And actually, how do you evaluate?
[00:09:28.660 --> 00:09:31.620]   Why did you decide to go with the Qualcomm board or something else?
[00:09:31.620 --> 00:09:32.620]   How do you think about that?
[00:09:32.620 --> 00:09:36.100]   So we chose the Qualcomm board, great question, in 2018.
[00:09:36.100 --> 00:09:38.380]   And there were three primary factors.
[00:09:38.380 --> 00:09:41.220]   The first one was, does it do what we need it to do?
[00:09:41.220 --> 00:09:45.780]   Which by the way, at the time, there were a lot of things that would say, we do AI and
[00:09:45.780 --> 00:09:48.700]   it would come with a pre-trained inception model.
[00:09:48.700 --> 00:09:52.020]   And if you changed anything on it, it would break.
[00:09:52.020 --> 00:09:57.340]   The early days of runtime AI were pretty limited in terms of the scope of what things could
[00:09:57.340 --> 00:09:58.340]   do.
[00:09:58.340 --> 00:09:59.540]   So that was number one.
[00:09:59.540 --> 00:10:02.060]   Number two was, it's a consumer price point.
[00:10:02.060 --> 00:10:03.780]   So we're selling direct to consumer.
[00:10:03.780 --> 00:10:10.720]   And so while my hub is, that's a $250 COGS piece of equipment, that costs me 250 bucks.
[00:10:10.720 --> 00:10:14.300]   If you buy an Arlo or a Nest or any of that stuff, you know this, right?
[00:10:14.300 --> 00:10:16.460]   The end user price is lower than that.
[00:10:16.460 --> 00:10:21.800]   So I had to really make sure that it fit within the constraints of something that consumers
[00:10:21.800 --> 00:10:22.800]   could afford.
[00:10:22.800 --> 00:10:28.340]   And then the third thing was, it needed to be size-wise and power-wise kind of containable.
[00:10:28.340 --> 00:10:36.060]   So we couldn't just ship people an i7 with an NVIDIA GTX card in it.
[00:10:36.060 --> 00:10:37.540]   That wasn't going to work either.
[00:10:37.540 --> 00:10:38.540]   Cool.
[00:10:38.540 --> 00:10:40.620]   One question I really am dying to ask you.
[00:10:40.620 --> 00:10:43.180]   So I thought of you as a very successful entrepreneur.
[00:10:43.180 --> 00:10:49.660]   I actually have this memory of watching you pitch the CTO of eBay many years ago and just
[00:10:49.660 --> 00:10:53.740]   thinking like, wow, your presentation is so much better than my presentation.
[00:10:53.740 --> 00:10:58.380]   Just wondering, where did you hire this mark on team that's making this stuff?
[00:10:58.380 --> 00:11:00.100]   And that's kind of how I always thought about you.
[00:11:00.100 --> 00:11:05.040]   So I was really surprised when I was kind of running into some open CV bugs.
[00:11:05.040 --> 00:11:07.540]   And then I think I found you had like forked eBay.
[00:11:07.540 --> 00:11:10.900]   Did you really find that?
[00:11:10.900 --> 00:11:12.580]   That's so funny.
[00:11:12.580 --> 00:11:15.900]   I was like, what do you like, do you like forked open CV or something?
[00:11:15.900 --> 00:11:19.060]   I mean, I remember just thinking like, is this the same guy that I know?
[00:11:19.060 --> 00:11:20.060]   That's not possible.
[00:11:20.060 --> 00:11:21.060]   Like, what's he doing?
[00:11:21.060 --> 00:11:27.660]   I just was wondering, did you stay technical the whole time or was there like a moment
[00:11:27.660 --> 00:11:30.100]   where you got back into this stuff?
[00:11:30.100 --> 00:11:34.340]   Well, first of all, thank you for the shameful comment.
[00:11:34.340 --> 00:11:37.100]   I'm like literally red in the face here.
[00:11:37.100 --> 00:11:38.740]   I appreciate the compliment.
[00:11:38.740 --> 00:11:40.020]   I love being technical, right?
[00:11:40.020 --> 00:11:44.420]   Like, I mean, I have always had this office in my home, which now is where I live, obviously
[00:11:44.420 --> 00:11:47.060]   a hundred percent of my time, but I've always kept this.
[00:11:47.060 --> 00:11:49.060]   And my wife has always been super supportive of me.
[00:11:49.060 --> 00:11:51.700]   And it looks like you have something kind of similar in your garage and all the videos
[00:11:51.700 --> 00:11:56.920]   I've seen from you of just, you know, stay tinkering, stay busy, keep my hands dirty
[00:11:56.920 --> 00:11:57.980]   a little bit.
[00:11:57.980 --> 00:12:02.340]   There was a period of about, you know, three, four years at my last company, which was an
[00:12:02.340 --> 00:12:07.700]   enterprise sales company where I was traveling so much, I couldn't be super technical, but
[00:12:07.700 --> 00:12:11.500]   I took about a year and a half gap and started getting really, really technical again.
[00:12:11.500 --> 00:12:16.340]   And what I found, what you found actually is that there was a version of CUDA, which
[00:12:16.340 --> 00:12:20.500]   is the NVIDIA library that wasn't compatible with OpenCV.
[00:12:20.500 --> 00:12:22.140]   I ended up forking it.
[00:12:22.140 --> 00:12:26.540]   I ended up fixing the bug so that you could run AI and OpenCV at the same time.
[00:12:26.540 --> 00:12:31.480]   Because if you were trying to run AI with OpenCV, you couldn't do it for like a year.
[00:12:31.480 --> 00:12:36.740]   And so I forked it, I patched it and I figured out how to make it so that you could run AI
[00:12:36.740 --> 00:12:40.820]   with OpenCV on Linux on the most recent version of CUDA.
[00:12:40.820 --> 00:12:41.820]   And I was blown away.
[00:12:41.820 --> 00:12:46.620]   Like I just, I did it myself and I was getting like hundreds and hundreds and hundreds of
[00:12:46.620 --> 00:12:48.180]   downloads every single day.
[00:12:48.180 --> 00:12:51.980]   I was getting like bugs submitted to me on the main OpenCV branch.
[00:12:51.980 --> 00:12:55.700]   And I was like, all right, well, I guess that's kind of cool.
[00:12:55.700 --> 00:12:57.800]   It's funny that you ran into it though.
[00:12:57.800 --> 00:13:02.860]   But for like a year and a half, I was the primary maintainer of like OpenCV for all
[00:13:02.860 --> 00:13:04.780]   AI researchers around the US.
[00:13:04.780 --> 00:13:05.780]   It was pretty sweet.
[00:13:05.780 --> 00:13:06.780]   That's so awesome.
[00:13:06.780 --> 00:13:12.820]   I remember looking at your patch and just thinking, man, like this is like really deep.
[00:13:12.820 --> 00:13:19.420]   It was, yeah, it was like a, there was a bug in the compiled C code that made it incompatible
[00:13:19.420 --> 00:13:24.200]   in terms of the data structure and some of the libraries with the most recent version
[00:13:24.200 --> 00:13:25.780]   of CUDA.
[00:13:25.780 --> 00:13:29.300]   And at that point, were you already working on this new company?
[00:13:29.300 --> 00:13:31.100]   Was that what was happening or what?
[00:13:31.100 --> 00:13:32.700]   Well, I wasn't really working on the company.
[00:13:32.700 --> 00:13:33.700]   I was working on AI.
[00:13:33.700 --> 00:13:38.820]   In fact, I was really fortunate that I got to work with the guys over at Lux Capital.
[00:13:38.820 --> 00:13:42.140]   I went to them and just said, hey, I'm going to take a couple of years off.
[00:13:42.140 --> 00:13:46.460]   I want somebody to bounce ideas off of as I explore deep learning, because I think it's
[00:13:46.460 --> 00:13:48.180]   real.
[00:13:48.180 --> 00:13:54.300]   And within like a month I had built a, I was using genetic learning, but using a deep learning
[00:13:54.300 --> 00:13:58.260]   algorithm and some of the new stuff kind of coming out in terms of vision.
[00:13:58.260 --> 00:14:04.900]   So I used a vision feature set using some of the ImageNet competition vision libraries
[00:14:04.900 --> 00:14:08.580]   and then running just a stupid genetic algorithm to play Mario world.
[00:14:08.580 --> 00:14:14.820]   And I built the world's best Mario world playing system in like three weeks.
[00:14:14.820 --> 00:14:20.060]   And I was like, whoa, here I am in my little garage and I can beat the entire game of Super
[00:14:20.060 --> 00:14:21.980]   Mario Brothers.
[00:14:21.980 --> 00:14:22.980]   This has to be something.
[00:14:22.980 --> 00:14:28.780]   I mean, I haven't coded in four years and here I am kind of with the state of the art.
[00:14:28.780 --> 00:14:33.380]   And so I went and I worked with the guys over at Lux and I said, hey, can I spend a year
[00:14:33.380 --> 00:14:37.780]   just kind of coming in and out of your office and bounce ideas off of you as to what we
[00:14:37.780 --> 00:14:38.780]   can do with this technology?
[00:14:38.780 --> 00:14:43.860]   Because I want to make sure that we're thinking about the specific application in a way that
[00:14:43.860 --> 00:14:48.580]   can build a business that for me, what was most important is that it really had an impact
[00:14:48.580 --> 00:14:49.580]   on the world.
[00:14:49.580 --> 00:14:54.700]   And it meant in the world around us, because as you said, I'm fortunate enough that I can
[00:14:54.700 --> 00:14:57.740]   kind of choose the business problem I want to solve.
[00:14:57.740 --> 00:15:03.100]   And while making great returns for investors is absolutely a necessary requirement, I had
[00:15:03.100 --> 00:15:07.300]   the ability to take an extra 18 months and make sure that I also did something that when
[00:15:07.300 --> 00:15:12.300]   I look back and I talk to my kids about it, that I'm really proud of and that they are
[00:15:12.300 --> 00:15:13.860]   proud of and they can be a part of.
[00:15:13.860 --> 00:15:19.180]   And so we ended up spending that time trying to figure out, is there really a business
[00:15:19.180 --> 00:15:20.180]   model here?
[00:15:20.180 --> 00:15:23.140]   I mean, a lot of people said security is a bad industry.
[00:15:23.140 --> 00:15:25.380]   It's one that's dominated by ADT.
[00:15:25.380 --> 00:15:29.460]   It's got all these old players and people buy based on brand and they don't buy based
[00:15:29.460 --> 00:15:31.420]   on capability.
[00:15:31.420 --> 00:15:36.540]   And I was able to convince them and myself that if you could do a step function in terms
[00:15:36.540 --> 00:15:40.860]   of capability, just completely change the game, make something available, going from
[00:15:40.860 --> 00:15:46.700]   reactive to proactive, entirely based on the use of this new AI technology, that we can
[00:15:46.700 --> 00:15:48.340]   change the way people buy.
[00:15:48.340 --> 00:15:51.820]   And that's the entire thesis of Deep Sentinel is screw ADT.
[00:15:51.820 --> 00:15:54.020]   That just doesn't matter.
[00:15:54.020 --> 00:15:56.140]   Okay, whoever wants to buy that can buy it.
[00:15:56.140 --> 00:16:01.500]   But if I can say, look, you can go from having the police show up less than 5% of the time,
[00:16:01.500 --> 00:16:07.500]   eight hours after your house is burglarized to preventing crimes 95% of the time, isn't
[00:16:07.500 --> 00:16:10.040]   that a different reason to buy?
[00:16:10.040 --> 00:16:14.220]   And so that was when I was getting technical again to make sure that I understood what
[00:16:14.220 --> 00:16:16.460]   the underlying technology was about.
[00:16:16.460 --> 00:16:21.020]   And if I can add one more thing to that answer, my favorite thing I did in that whole period
[00:16:21.020 --> 00:16:25.900]   was I started a journal club because I wasn't technical enough to understand the state of
[00:16:25.900 --> 00:16:27.180]   the art at that point.
[00:16:27.180 --> 00:16:34.620]   I'd been so long since I'd been in AI, like 12 years, 13 years since I was at Stanford.
[00:16:34.620 --> 00:16:39.260]   And so I started a journal club and just by offering free pizza, all of a sudden I had
[00:16:39.260 --> 00:16:44.140]   these 20 people showing up together and just saying, "Hey, that's a topic I want to talk
[00:16:44.140 --> 00:16:45.580]   about too."
[00:16:45.580 --> 00:16:49.900]   And we ended up building this little kernel of about nine, 10 people that are still going
[00:16:49.900 --> 00:16:52.340]   today five years later.
[00:16:52.340 --> 00:16:56.540]   Every single week we meet and we read papers together and we debate things.
[00:16:56.540 --> 00:17:01.420]   And because they're all smarter than I am, for literally the cost of one pizza every
[00:17:01.420 --> 00:17:05.940]   single week, I was able to get them to read this paper with me and then explain it to
[00:17:05.940 --> 00:17:06.940]   me.
[00:17:06.940 --> 00:17:10.420]   I just pretended that I knew what it was about and then they corrected all of the underlying
[00:17:10.420 --> 00:17:11.420]   nature of it.
[00:17:11.420 --> 00:17:15.980]   It really kind of reminded me of the power to just bring people together and learn from
[00:17:15.980 --> 00:17:16.980]   each other.
[00:17:16.980 --> 00:17:20.540]   And again, I think it's been five years and the same group of people, we still meet every
[00:17:20.540 --> 00:17:21.540]   single week.
[00:17:21.540 --> 00:17:26.260]   We met last night and we read a paper on Facebook's new object detection algorithm.
[00:17:26.260 --> 00:17:28.940]   That sounds so fun.
[00:17:28.940 --> 00:17:31.020]   Where did you find these people that are smarter than you?
[00:17:31.020 --> 00:17:33.260]   Dude, I literally just posted on Meetup.
[00:17:33.260 --> 00:17:35.500]   I made this thing called East Bay ML.
[00:17:35.500 --> 00:17:37.580]   I posted it on Meetup and I started broadcasting.
[00:17:37.580 --> 00:17:42.460]   Actually, I did a little bit of Facebook advertising too, for people that had Python as an interest
[00:17:42.460 --> 00:17:43.980]   in their Facebook profile.
[00:17:43.980 --> 00:17:50.860]   But I only spent $300 or $400 and I got hundreds of people to sign up for this Meetup.
[00:17:50.860 --> 00:17:55.460]   And we now are at the point where every other week we do a paper and then in the alternate
[00:17:55.460 --> 00:17:56.580]   weeks we do code.
[00:17:56.580 --> 00:18:01.740]   And we literally load up Colab and code together as a group.
[00:18:01.740 --> 00:18:06.420]   And these are now, because we're in COVID, about 20% of the people, we've never even
[00:18:06.420 --> 00:18:07.420]   met them.
[00:18:07.420 --> 00:18:11.220]   We don't even know who these people are, but they just show up and we broadcast it.
[00:18:11.220 --> 00:18:16.420]   And for me, it allows me to stay technical while I'm the CEO of this company without
[00:18:16.420 --> 00:18:18.900]   having to distract or bug my engineers.
[00:18:18.900 --> 00:18:20.500]   That's so cool.
[00:18:20.500 --> 00:18:23.020]   What kinds of coding do you do?
[00:18:23.020 --> 00:18:26.180]   Mostly Python and Colab right now at this point.
[00:18:26.180 --> 00:18:28.700]   I'd say I do a little tiny bit on chips.
[00:18:28.700 --> 00:18:33.860]   So I have a Coral board and an NVIDIA board and a bunch of CPU GPUs in here.
[00:18:33.860 --> 00:18:34.980]   And so I do a little bit of that.
[00:18:34.980 --> 00:18:39.540]   But most of my work now is really in Colab just to make sure I kind of stay fresh on
[00:18:39.540 --> 00:18:43.180]   where are we at with kind of neural network architectures and how good do they really
[00:18:43.180 --> 00:18:46.980]   perform in reality.
[00:18:46.980 --> 00:18:51.980]   The improvements in ML over the last few years, does it affect your business?
[00:18:51.980 --> 00:18:58.780]   Is it important to use the very latest stuff to make your company work?
[00:18:58.780 --> 00:19:01.280]   It's not important to make it work.
[00:19:01.280 --> 00:19:03.500]   It's important to make it better.
[00:19:03.500 --> 00:19:07.380]   In the words of somebody that I think is again, way smarter than me, Kai-Fu Lee said that
[00:19:07.380 --> 00:19:09.900]   AI has moved into the age of implementation.
[00:19:09.900 --> 00:19:11.380]   It's no longer in the age of science.
[00:19:11.380 --> 00:19:12.540]   It's in the age of implementation.
[00:19:12.540 --> 00:19:14.500]   I would disagree with it a little bit.
[00:19:14.500 --> 00:19:19.780]   If you kind of look at the S curve of innovation, where you've kind of plateaued out in terms
[00:19:19.780 --> 00:19:27.240]   of the actual innovation, I think the neatest thing about AI is that we're at both ends
[00:19:27.240 --> 00:19:29.180]   of that curve simultaneously.
[00:19:29.180 --> 00:19:34.180]   We have implementations that are at massive scale, like Facebook doing facial recognition
[00:19:34.180 --> 00:19:35.180]   on all of your photos.
[00:19:35.180 --> 00:19:41.860]   At the same time, in the last year, we have seen BERT, which from a language processing
[00:19:41.860 --> 00:19:46.720]   perspective is a step function better than all the NLP that came before it.
[00:19:46.720 --> 00:19:50.860]   We saw AlphaZero come out, which I think is just a phenomenal leap forward in terms of
[00:19:50.860 --> 00:19:52.380]   reinforcement learning.
[00:19:52.380 --> 00:19:58.100]   And then just in the last four months, we saw the new DIRT paper come out of Facebook,
[00:19:58.100 --> 00:20:03.740]   which is an object detection algorithm that is also, I think it's like 25% better than
[00:20:03.740 --> 00:20:06.980]   the predecessor at mean average precision.
[00:20:06.980 --> 00:20:09.700]   And then Google had a similar one called Efficient Debt.
[00:20:09.700 --> 00:20:17.260]   But to see 20% improvements still in the course of a year, I mean, that's pretty amazing that
[00:20:17.260 --> 00:20:20.300]   we're both in implementation and in the research phases.
[00:20:20.300 --> 00:20:25.380]   So for us, what that means is that we have to continually be climbing that ladder because
[00:20:25.380 --> 00:20:26.880]   we get operational leverage.
[00:20:26.880 --> 00:20:32.300]   We get margin and we get operational effectiveness by continually researching how each of those
[00:20:32.300 --> 00:20:34.700]   different new technologies affect our business.
[00:20:34.700 --> 00:20:35.700]   Got it.
[00:20:35.700 --> 00:20:36.700]   That makes sense.
[00:20:36.700 --> 00:20:45.860]   I mean, has it been an adjustment to go from working on maybe not safety critical applications
[00:20:45.860 --> 00:20:49.340]   to an application where you really can't afford to make mistakes?
[00:20:49.340 --> 00:20:51.140]   Or do I even have that right?
[00:20:51.140 --> 00:20:55.420]   I would assume that in your world now, the cost of an error is much higher than with
[00:20:55.420 --> 00:20:58.780]   a recommendation system, stuff you've worked on in the past.
[00:20:58.780 --> 00:21:00.540]   Yeah, I mean, it is.
[00:21:00.540 --> 00:21:06.980]   And I think though, at the same time, I took a card from Elon Musk's deck, which is, if
[00:21:06.980 --> 00:21:12.860]   you're going to fail, fail early, fail fast, fail hard, and then get to the next plateau.
[00:21:12.860 --> 00:21:18.540]   Because my advantage in being a startup, again, versus kind of the behemoth of giant slow
[00:21:18.540 --> 00:21:22.940]   moving idiots of the world, not that ADT are idiots, but let's just call them idiots for
[00:21:22.940 --> 00:21:31.900]   the purpose of this conversation, that you only have really one advantage.
[00:21:31.900 --> 00:21:37.140]   And that is your willingness to just look the monster in the eye and fail fast and fail
[00:21:37.140 --> 00:21:38.140]   early.
[00:21:38.140 --> 00:21:42.500]   So yes, it is a mission critical product, and we absolutely strive to be better than
[00:21:42.500 --> 00:21:43.500]   everyone else.
[00:21:43.500 --> 00:21:47.220]   But we also recognize that if we're going to make these mistakes and do this learning,
[00:21:47.220 --> 00:21:51.900]   we've got to do it in this phase of the business so that when we get to the hundreds of thousands
[00:21:51.900 --> 00:21:55.700]   of customers, we're at another plateau.
[00:21:55.700 --> 00:22:00.860]   And I guess if ADT, if it's true, I wasn't aware of this, if ADT actually, when the alarm
[00:22:00.860 --> 00:22:07.220]   goes off, nothing typically happens, then maybe it's okay if occasionally your system
[00:22:07.220 --> 00:22:08.540]   has an alarm go off and nothing happens.
[00:22:08.540 --> 00:22:09.540]   Is that right?
[00:22:09.540 --> 00:22:12.380]   Or that's probably not what you put in your marketing content.
[00:22:12.380 --> 00:22:18.020]   I would not say that it's all right if we don't succeed, but I will say that even in
[00:22:18.020 --> 00:22:24.340]   the really, really rare cases where we don't operate as quickly as we would like, we're
[00:22:24.340 --> 00:22:29.180]   still a hundred times better than the next best alternative.
[00:22:29.180 --> 00:22:34.380]   We have customers where we've messed up and we have, and that happens.
[00:22:34.380 --> 00:22:40.540]   But on average, and in fact, to the 99th percentile, they understand because they understand that
[00:22:40.540 --> 00:22:43.580]   when they went to market and said, "Hey, who are Deep Sentinel's competitors?"
[00:22:43.580 --> 00:22:45.660]   The Google result list is zero.
[00:22:45.660 --> 00:22:50.020]   There's literally nobody else that can do what we do at a consumer affordable price
[00:22:50.020 --> 00:22:55.460]   point and nobody else that does it as effective and at much scale as we do.
[00:22:55.460 --> 00:22:59.540]   And so they are generally pretty understanding that I get that you're learning.
[00:22:59.540 --> 00:23:00.740]   I want you to protect my family.
[00:23:00.740 --> 00:23:01.740]   I rely on you.
[00:23:01.740 --> 00:23:04.700]   I can't put up with this, but I understand.
[00:23:04.700 --> 00:23:05.700]   Interesting.
[00:23:05.700 --> 00:23:06.700]   Yeah, it's funny.
[00:23:06.700 --> 00:23:10.220]   I've tried to ask this question to a whole bunch of people and I think I've kind of stopped.
[00:23:10.220 --> 00:23:15.860]   But I always wonder with people working on autonomous vehicles or applications where
[00:23:15.860 --> 00:23:18.420]   there's really safety is a huge issue.
[00:23:18.420 --> 00:23:21.420]   It just seems incredibly hard to know.
[00:23:21.420 --> 00:23:26.900]   I've looked at so many ROC-ers in my life and picked where you go in the precision recall
[00:23:26.900 --> 00:23:31.740]   trade-off, but I'm so glad that I'm not in your shoes trying to pick the precision recall
[00:23:31.740 --> 00:23:32.740]   trade-off.
[00:23:32.740 --> 00:23:33.740]   Do you have anything to say on how you-
[00:23:33.740 --> 00:23:35.340]   So that for me is a little bit easier.
[00:23:35.340 --> 00:23:40.500]   So on autonomous driving, you have to choose left or right, fast or slow, which is actually
[00:23:40.500 --> 00:23:46.900]   a more hard question than ours, which is show this to a guard and allow the guard to review
[00:23:46.900 --> 00:23:47.900]   it.
[00:23:47.900 --> 00:23:48.900]   Because I have a trade-off.
[00:23:48.900 --> 00:23:53.580]   I can have more guards than I need and I can show them more video.
[00:23:53.580 --> 00:23:58.900]   And I can even show them trash videos for a period of time because I can afford to do
[00:23:58.900 --> 00:23:59.900]   that.
[00:23:59.900 --> 00:24:03.380]   As long as I can maintain the fidelity of their operational behavior, I can actually
[00:24:03.380 --> 00:24:06.420]   just solve that problem with money.
[00:24:06.420 --> 00:24:10.740]   Whereas if you're choosing left or right, you got to choose one.
[00:24:10.740 --> 00:24:12.220]   You can't choose both.
[00:24:12.220 --> 00:24:15.860]   And that's, I think, a much harder decision that you really can't solve with money.
[00:24:15.860 --> 00:24:17.340]   So they have to go slower.
[00:24:17.340 --> 00:24:22.300]   I can go faster and just lose a little bit more money for a short period of time.
[00:24:22.300 --> 00:24:23.820]   Although you can't do that infinitely.
[00:24:23.820 --> 00:24:26.460]   So I feel like that logic would apply anywhere.
[00:24:26.460 --> 00:24:29.780]   So at some point you have to tolerate some amount of error.
[00:24:29.780 --> 00:24:33.380]   There's some amount of error you always have to tolerate with these systems.
[00:24:33.380 --> 00:24:36.020]   And so how do you even think about that?
[00:24:36.020 --> 00:24:37.020]   There is.
[00:24:37.020 --> 00:24:40.300]   The way that I think about it, again, because our decisions, the core decision, the most
[00:24:40.300 --> 00:24:43.200]   dangerous decision you make is binary.
[00:24:43.200 --> 00:24:45.900]   What you can do is you just perform it as an experiment.
[00:24:45.900 --> 00:24:51.700]   You operate at the lower threshold, but then you measure at a higher threshold and you
[00:24:51.700 --> 00:24:54.680]   just measure the gap here.
[00:24:54.680 --> 00:24:57.540]   And then you have a level of acceptable gap.
[00:24:57.540 --> 00:25:03.620]   And then the second thing that we do is we also measure that across the life cycle of
[00:25:03.620 --> 00:25:04.620]   an event.
[00:25:04.620 --> 00:25:12.260]   So because we're looking at events that are, let's say, 25 seconds long, that's 500 frames.
[00:25:12.260 --> 00:25:19.140]   And so I actually have 500 opportunities to make the decision to go wild with that event.
[00:25:19.140 --> 00:25:22.220]   And so I don't have to make that decision at this point.
[00:25:22.220 --> 00:25:27.380]   Now I can't tolerate it if it's 20 seconds late, but can I tolerate it if it's 300 milliseconds
[00:25:27.380 --> 00:25:28.380]   later?
[00:25:28.380 --> 00:25:30.260]   I generally can't.
[00:25:30.260 --> 00:25:35.380]   And so what we've done is we've been able to drive this threshold where we keep that
[00:25:35.380 --> 00:25:38.660]   number pretty much zero, by the way, to be honest with you.
[00:25:38.660 --> 00:25:43.900]   But then we drive other dimensions of flexibility in our decision making instead of driving
[00:25:43.900 --> 00:25:46.100]   the decision of I'm going to let this event go.
[00:25:46.100 --> 00:25:50.700]   Because we don't consider that to be acceptable and we keep that number pretty much zero.
[00:25:50.700 --> 00:25:55.020]   Do you pass along your confidence value to the operator?
[00:25:55.020 --> 00:25:56.660]   Great question.
[00:25:56.660 --> 00:26:02.820]   No, because in general, what we have seen is that the confidence values, and you see
[00:26:02.820 --> 00:26:04.300]   this all the time, right?
[00:26:04.300 --> 00:26:08.180]   I'm 99% sure that my cat is a dog, right?
[00:26:08.180 --> 00:26:09.900]   It's not real information.
[00:26:09.900 --> 00:26:14.820]   I don't find in the shape of those softmax curves are so sensitive.
[00:26:14.820 --> 00:26:19.900]   And the number that comes out is not a true confidence number in the sense that humans
[00:26:19.900 --> 00:26:20.900]   interpret them.
[00:26:20.900 --> 00:26:25.420]   Like 0.6, which would turn into 60%, really doesn't mean 0.6.
[00:26:25.420 --> 00:26:29.980]   In fact, if you look at the shape of most of the softmax curves, they're very heavily
[00:26:29.980 --> 00:26:35.700]   weighted on the zero, between zero and 5% and between 90 and 100%.
[00:26:35.700 --> 00:26:39.700]   There's almost nothing that's kind of in that useful range in the middle.
[00:26:39.700 --> 00:26:43.820]   And so we tend to just trim it off because then if you try to normalize it so that it's
[00:26:43.820 --> 00:26:48.660]   distributed evenly between zero and 100, what you're doing is you're taking basically 0.9
[00:26:48.660 --> 00:26:56.460]   to 0.91 and making that this huge section between 50% and 75% confident.
[00:26:56.460 --> 00:27:00.260]   And we find that it's not actually that representative.
[00:27:00.260 --> 00:27:04.620]   The granularity doesn't really exist in the confidence, at least in the most of the curves
[00:27:04.620 --> 00:27:06.620]   that we've seen.
[00:27:06.620 --> 00:27:08.140]   Interesting.
[00:27:08.140 --> 00:27:12.260]   Do you ever go back and deploy new models to your customers?
[00:27:12.260 --> 00:27:13.900]   Like existing customers?
[00:27:13.900 --> 00:27:14.900]   All the time.
[00:27:14.900 --> 00:27:17.940]   We deploy new models on a weekly, monthly basis.
[00:27:17.940 --> 00:27:18.940]   Interesting.
[00:27:18.940 --> 00:27:23.420]   And will you train them on the customer's data?
[00:27:23.420 --> 00:27:24.420]   Yeah.
[00:27:24.420 --> 00:27:28.260]   So we train them both on the customer's data in aggregate.
[00:27:28.260 --> 00:27:33.100]   And then we actually have a patent on personalized training at the edge or patent pending that
[00:27:33.100 --> 00:27:38.020]   we use the individual customer's data to fine tune the model in their home.
[00:27:38.020 --> 00:27:41.380]   And that's one of the neatest things about having the hardware in the home is we have
[00:27:41.380 --> 00:27:45.340]   a huge chunk of data in the home and we also have a model in the home.
[00:27:45.340 --> 00:27:49.420]   And so there's a bunch of advancements that have been made in like semi-supervised learning
[00:27:49.420 --> 00:27:52.260]   where you can refine these models at the edge.
[00:27:52.260 --> 00:27:56.980]   And so we can do that both in terms of how we interpret the final coefficients coming
[00:27:56.980 --> 00:28:01.940]   out the fully connected layer, as well as do that on actually retraining some of the
[00:28:01.940 --> 00:28:04.220]   weights in the model in the home.
[00:28:04.220 --> 00:28:08.220]   So it lets you retrain on the edge and not phone home?
[00:28:08.220 --> 00:28:09.220]   Yeah.
[00:28:09.220 --> 00:28:10.860]   So we can do some amount of that.
[00:28:10.860 --> 00:28:14.820]   We have tended to do that only on the fully connected layer or like maybe one or two layers.
[00:28:14.820 --> 00:28:18.700]   And we've done a bunch of tests to see like, where does that matter?
[00:28:18.700 --> 00:28:23.260]   In general, we found that even just the fully connected layer rebalancing the weights of
[00:28:23.260 --> 00:28:28.660]   the fully connected layer is pretty effective at driving a massive shift in mean average
[00:28:28.660 --> 00:28:35.220]   precision, especially when it comes down to our problem very specifically as we have stationary
[00:28:35.220 --> 00:28:36.460]   cameras.
[00:28:36.460 --> 00:28:40.380]   And so a big part of stationary cameras in terms of a vision problem is identifying the
[00:28:40.380 --> 00:28:41.420]   background.
[00:28:41.420 --> 00:28:46.780]   And so when you have hundreds and thousands and millions of images of what a camera looks
[00:28:46.780 --> 00:28:52.180]   like on average, you can develop a very clean sense of what the features are of the background
[00:28:52.180 --> 00:28:56.420]   so that your background subtraction becomes even more effective kind of across the board.
[00:28:56.420 --> 00:29:01.660]   Do you have any examples of customers that have kind of taken a camera and deployed it
[00:29:01.660 --> 00:29:06.660]   in some way that you didn't expect that caused the system to struggle?
[00:29:06.660 --> 00:29:12.420]   I mean, we have tons of examples where the interface with the wet part of the world,
[00:29:12.420 --> 00:29:15.500]   the human being part of the world is whackadoo.
[00:29:15.500 --> 00:29:20.260]   We have to have a very specific policy about what you do when you see naked people, for
[00:29:20.260 --> 00:29:21.260]   example.
[00:29:21.260 --> 00:29:25.820]   So we have customers that we have discovered that, and you know him as well, right?
[00:29:25.820 --> 00:29:30.740]   So Sean Parker used to always say the world is broken up into two categories of people,
[00:29:30.740 --> 00:29:33.340]   exhibitionists and voyeurs.
[00:29:33.340 --> 00:29:37.420]   And I'll tell you a product that has people watching your cameras, you definitely find
[00:29:37.420 --> 00:29:39.740]   people in that first category.
[00:29:39.740 --> 00:29:46.780]   And so we have customers that will put their camera in their house and then proceed to
[00:29:46.780 --> 00:29:50.700]   treat it as if it were an adult rated YouTube channel.
[00:29:50.700 --> 00:29:59.260]   I have to ask, and maybe we should take this out, but what do you do?
[00:29:59.260 --> 00:30:03.960]   We turn the camera on and we apologize to the guard that had to watch it.
[00:30:03.960 --> 00:30:07.140]   So we have special features where that escalates immediately.
[00:30:07.140 --> 00:30:08.140]   We disable it.
[00:30:08.140 --> 00:30:11.180]   We send them a very nice letter that says, we noticed that you've installed your camera
[00:30:11.180 --> 00:30:12.600]   indoors.
[00:30:12.600 --> 00:30:14.260]   We have gone ahead and disabled that.
[00:30:14.260 --> 00:30:17.860]   Please verify when you've moved that outdoors and let us know.
[00:30:17.860 --> 00:30:21.220]   But I mean, it's a product that touches the real world.
[00:30:21.220 --> 00:30:26.580]   This isn't a Nest or an Arlo where it's a dumb device just recording.
[00:30:26.580 --> 00:30:29.260]   This is a device that is a live ecosystem.
[00:30:29.260 --> 00:30:32.300]   And so we find all kinds of stuff.
[00:30:32.300 --> 00:30:35.300]   And this is one of the things I really love about the business that we've started is that
[00:30:35.300 --> 00:30:39.420]   we have this huge, deep technical investment in AI.
[00:30:39.420 --> 00:30:44.420]   And then we have all of these different real scenarios that it's being trained against
[00:30:44.420 --> 00:30:49.860]   that when you compare our AI against what you would get from a horizontal vendor that
[00:30:49.860 --> 00:30:54.540]   does image recognition and classification, there's no way that they've spent the time
[00:30:54.540 --> 00:30:58.100]   to say, "Hey, what does it look like when you're inside and there's a guy dancing in
[00:30:58.100 --> 00:31:00.620]   his underwear in front of your camera?"
[00:31:00.620 --> 00:31:02.820]   The Nest doesn't have that.
[00:31:02.820 --> 00:31:04.740]   They don't do that.
[00:31:04.740 --> 00:31:10.420]   And so we've just solved a lot of these really neat technical problems that are at this interface
[00:31:10.420 --> 00:31:14.980]   between cameras and human beings that I think is really interesting.
[00:31:14.980 --> 00:31:19.660]   I guess this is outside of AI maybe, but I'm kind of curious as another entrepreneur to
[00:31:19.660 --> 00:31:20.660]   entrepreneur.
[00:31:20.660 --> 00:31:24.940]   I feel like you have this interesting challenge of proving to your consumers that your camera
[00:31:24.940 --> 00:31:25.940]   is actually better.
[00:31:25.940 --> 00:31:29.780]   I feel like a lot of things it's obviously better, but I guess in security, it's sort
[00:31:29.780 --> 00:31:33.140]   of better is the absence of issues.
[00:31:33.140 --> 00:31:37.100]   How do you demonstrate that your camera is better?
[00:31:37.100 --> 00:31:38.660]   It's more of a marketing question, right?
[00:31:38.660 --> 00:31:39.660]   In some senses, right?
[00:31:39.660 --> 00:31:46.220]   So one of the challenges that we have is that for the last 30 years, the definition of better
[00:31:46.220 --> 00:31:53.980]   for cameras is pixels per inch or color resolution where what we did is we changed the game.
[00:31:53.980 --> 00:31:55.980]   The camera really is a dumb camera.
[00:31:55.980 --> 00:32:01.100]   Our camera is great and it's as good as anybody else's, but it's not better as a camera.
[00:32:01.100 --> 00:32:06.020]   In fact, there are areas where the new version of Nest and the new version of Ring or whatever
[00:32:06.020 --> 00:32:10.740]   are technically better than our camera, but none of them do what our camera does.
[00:32:10.740 --> 00:32:12.560]   It's more about the capability.
[00:32:12.560 --> 00:32:15.900]   And so what we found is that we break that problem down into two challenges.
[00:32:15.900 --> 00:32:21.020]   We have to handle the matrix of like, are you better and are you worse?
[00:32:21.020 --> 00:32:24.360]   And then the second piece of it's really what you said, which is how do you deliver peace
[00:32:24.360 --> 00:32:28.340]   of mind, which is such a dangerous word from a marketing perspective.
[00:32:28.340 --> 00:32:32.780]   And so what we really focused on, we have this series of videos called the stopped videos
[00:32:32.780 --> 00:32:37.060]   that show us just stopping crimes, boom, repeatedly over and over.
[00:32:37.060 --> 00:32:43.060]   They're kind of individually boring because the point where the guy walks up to the front
[00:32:43.060 --> 00:32:47.940]   of a 7-Eleven, we have this 7-Eleven that gets burglarized like literally once a week.
[00:32:47.940 --> 00:32:52.500]   Guy walks up with a crowbar and he swings at the door and that's the end of our video
[00:32:52.500 --> 00:32:55.260]   because our guards get on and say, "Hey, jerk, get out of here.
[00:32:55.260 --> 00:32:56.700]   The police are on their way."
[00:32:56.700 --> 00:32:57.700]   And the guy walks away.
[00:32:57.700 --> 00:33:01.940]   Whereas if you had a dumb camera, you get this really cool video that for the next 45
[00:33:01.940 --> 00:33:04.700]   seconds, you see this guy banging on a window.
[00:33:04.700 --> 00:33:08.820]   And so what we've had to do is we've had to train the market that like, hey, prevention
[00:33:08.820 --> 00:33:10.140]   is possible.
[00:33:10.140 --> 00:33:13.000]   Proactive is, it's much harder to sell proactive.
[00:33:13.000 --> 00:33:14.000]   In some senses.
[00:33:14.000 --> 00:33:17.860]   And so we've really kind of spiced it up and made it exciting and made these like video
[00:33:17.860 --> 00:33:19.660]   series about it.
[00:33:19.660 --> 00:33:23.000]   And that leads to the question, hey, how do you guys do that?
[00:33:23.000 --> 00:33:29.300]   We now have an AI, an autonomous AI called AI deterrent that triggers within 200 milliseconds
[00:33:29.300 --> 00:33:33.300]   of the AI detecting somebody suspicious at night.
[00:33:33.300 --> 00:33:36.960]   And that's pretty sweet too, because we're literally kind of intervening in these crimes
[00:33:36.960 --> 00:33:39.020]   even before it would get to a guard.
[00:33:39.020 --> 00:33:43.160]   And that typically buys the guard another three to five seconds where the person's talking
[00:33:43.160 --> 00:33:47.680]   to the computer and then the guard's like, "Nah, man, I really need you to go."
[00:33:47.680 --> 00:33:50.000]   Do you worry about adversarial attacks?
[00:33:50.000 --> 00:33:53.200]   Like someone kind of back, like figuring out your algorithm and then like finding ways
[00:33:53.200 --> 00:33:55.080]   to go in undetected.
[00:33:55.080 --> 00:33:57.960]   Is that, I mean, I would think you might be too small for that to really be an issue,
[00:33:57.960 --> 00:33:59.600]   but maybe that is.
[00:33:59.600 --> 00:34:01.720]   I mean, I think, yes, we think about it, right?
[00:34:01.720 --> 00:34:04.560]   I think about that all the time because again, my business is security.
[00:34:04.560 --> 00:34:08.640]   So that's the way that you have to view the world when you're in security.
[00:34:08.640 --> 00:34:10.600]   But from a business perspective, right?
[00:34:10.600 --> 00:34:14.640]   Like at the point that we have people really developing adversarial attacks, we're in at
[00:34:14.640 --> 00:34:16.760]   another stage in our development.
[00:34:16.760 --> 00:34:17.760]   Got it.
[00:34:17.760 --> 00:34:18.760]   Okay.
[00:34:18.760 --> 00:34:22.320]   Well, another totally different thread that I wanted to ask you about, because people
[00:34:22.320 --> 00:34:23.800]   ask me about this all the time.
[00:34:23.800 --> 00:34:28.440]   I mean, you're a fairly active angel investor and long time, like super successful entrepreneur.
[00:34:28.440 --> 00:34:33.400]   I was kind of wondering how you think about investing in AI companies, like what you look
[00:34:33.400 --> 00:34:34.400]   for there.
[00:34:34.400 --> 00:34:35.400]   Yeah.
[00:34:35.400 --> 00:34:39.400]   So I have actually made the decision about four or five years ago to stop angel investing
[00:34:39.400 --> 00:34:45.040]   for the most part, because I did okay on some of them, but I just, I found that the people
[00:34:45.040 --> 00:34:49.840]   that are really good at angel investing do that full time and that I didn't want to do
[00:34:49.840 --> 00:34:50.840]   it full time.
[00:34:50.840 --> 00:34:54.760]   I don't, I find it really neat to meet with entrepreneurs and hear their story and like
[00:34:54.760 --> 00:34:55.760]   you, right?
[00:34:55.760 --> 00:34:57.920]   Like I like helping, I like kind of being there for them.
[00:34:57.920 --> 00:35:03.320]   And the investment piece was in some senses clouding that interaction just because I don't
[00:35:03.320 --> 00:35:07.800]   have enough money to invest in every one of the cool entrepreneurs that I see.
[00:35:07.800 --> 00:35:10.840]   And at the same time, I don't have enough time to spend time with a bunch of them.
[00:35:10.840 --> 00:35:15.600]   And so what I ended up doing was I ended up joining a venture firm as a venture partner
[00:35:15.600 --> 00:35:18.120]   so that they could kind of deal with the investment side of things.
[00:35:18.120 --> 00:35:22.760]   And I could just really focus on the like advising and talking to them.
[00:35:22.760 --> 00:35:27.600]   And what I've, what I've generally seen are kind of two camps of AI companies.
[00:35:27.600 --> 00:35:32.260]   And I think this is what you always see with emerging technologies is you see the kind
[00:35:32.260 --> 00:35:37.920]   of geeky technology oriented founder who really doesn't understand the business that they're
[00:35:37.920 --> 00:35:40.860]   in and they're just like super smart and they're endearing.
[00:35:40.860 --> 00:35:42.520]   Like you want to help them.
[00:35:42.520 --> 00:35:46.340]   And then you see the shmarmy like, sorry, sorry for all the guys out there or gals out
[00:35:46.340 --> 00:35:49.520]   there that are this, but like the sales person, right?
[00:35:49.520 --> 00:35:54.080]   That shows up and they're like, and the AI is just amazing.
[00:35:54.080 --> 00:36:00.680]   You'd be, I mean, you would not believe it's using this thing called stochastic gradient
[00:36:00.680 --> 00:36:02.240]   descent.
[00:36:02.240 --> 00:36:04.280]   And you're just like, oh my God.
[00:36:04.280 --> 00:36:09.200]   And so after I pulled a knife out of my eye, I don't advise those companies, but I entirely
[00:36:09.200 --> 00:36:13.360]   focus on those founders that have that chutzpah.
[00:36:13.360 --> 00:36:18.200]   They've got the, they've taken the risk to do something that they are not good at, right?
[00:36:18.200 --> 00:36:21.880]   I am going to go and start a business as a tech person.
[00:36:21.880 --> 00:36:24.940]   And I've got this crazy great insight.
[00:36:24.940 --> 00:36:28.880]   And I find that to be much more compelling and interesting and fun for me versus, you
[00:36:28.880 --> 00:36:36.040]   know, trying to educate some sales schmo on what's really happening under the covers.
[00:36:36.040 --> 00:36:39.600]   I guess you've watched a lot of technical founders become wildly successful.
[00:36:39.600 --> 00:36:43.360]   I mean, you've been doing this a long time and if I'd really front row seats to Silicon
[00:36:43.360 --> 00:36:47.040]   Valley, we should put your bio in the show notes, but it's impressive.
[00:36:47.040 --> 00:36:50.640]   Do you have any, are there any patterns that you've noticed over the years of like, you
[00:36:50.640 --> 00:36:54.600]   know, who you meet in that mode as like a technical founder?
[00:36:54.600 --> 00:36:58.600]   Because I'm sure that's a lot of the people, you know, watching this, listening to this
[00:36:58.600 --> 00:37:03.240]   and how these people, how they actually succeed, like kind of which people are likely to succeed
[00:37:03.240 --> 00:37:07.080]   and then what they do to kind of make themselves successful.
[00:37:07.080 --> 00:37:11.560]   So the two things that I think are pretty consistent is embrace crazy, right?
[00:37:11.560 --> 00:37:12.560]   Consistently, right?
[00:37:12.560 --> 00:37:17.920]   Like the things that other people aren't doing, that's exactly the world that you have to
[00:37:17.920 --> 00:37:18.920]   live in.
[00:37:18.920 --> 00:37:20.640]   You can't do all of crazy, right?
[00:37:20.640 --> 00:37:25.280]   But if you live in just not crazy, then you're competing with them on their terms.
[00:37:25.280 --> 00:37:28.680]   And that's, that is consistently a path to failure.
[00:37:28.680 --> 00:37:31.920]   So you have to embrace crazy.
[00:37:31.920 --> 00:37:39.760]   And then number two is the technical founders that I have seen get, get wildly successful.
[00:37:39.760 --> 00:37:44.080]   They aren't necessarily really self-aware, but they're sufficiently self-aware to hire
[00:37:44.080 --> 00:37:45.480]   their compliment.
[00:37:45.480 --> 00:37:51.640]   It's either in like one really amazing person or in, you know, two or three or a whole team,
[00:37:51.640 --> 00:37:57.320]   but they find some way to remain the crazy person and have a team that embraces that
[00:37:57.320 --> 00:38:01.840]   and supports them and gets the other stuff done.
[00:38:01.840 --> 00:38:07.160]   So I want to talk about social responsibility because I've seen just this market shift in
[00:38:07.160 --> 00:38:11.840]   the role of data in our society over the last six months.
[00:38:11.840 --> 00:38:17.360]   And not necessarily in a positive way or a negative way, just a kind of a standout observation
[00:38:17.360 --> 00:38:22.760]   that if you look at the two or three big things that have happened to the United States and
[00:38:22.760 --> 00:38:31.120]   the world in the last six months, the COVID obviously and Black Lives Matter, both of
[00:38:31.120 --> 00:38:37.080]   those have been really hard for our society to get ahold of because we have built a society
[00:38:37.080 --> 00:38:41.000]   that is headline based, whether you blame it on Twitter or whatever, I don't care.
[00:38:41.000 --> 00:38:42.960]   I'm not going to blame anybody.
[00:38:42.960 --> 00:38:51.360]   It's just that we are a headline based society and the problems of COVID are so incredibly
[00:38:51.360 --> 00:38:52.360]   complex.
[00:38:52.360 --> 00:38:54.480]   They are deep statistics.
[00:38:54.480 --> 00:38:58.200]   They are statistics that don't lend themselves and they're so data oriented.
[00:38:58.200 --> 00:39:04.120]   They don't lend themselves to a quick, you know, 40 word summary as to what's going on.
[00:39:04.120 --> 00:39:06.600]   And so, and same thing with Black Lives Matter, right?
[00:39:06.600 --> 00:39:11.840]   Like you could find a black person that makes more than a white person doing the same job.
[00:39:11.840 --> 00:39:12.840]   Absolutely.
[00:39:12.840 --> 00:39:16.240]   And if I wanted to, I could, you know, write that article up, ship it off probably to Fox
[00:39:16.240 --> 00:39:18.000]   News and they would run it.
[00:39:18.000 --> 00:39:23.100]   I could also find a black man that makes a hundred dollars per hour less than a white
[00:39:23.100 --> 00:39:28.280]   person in the same job, ship that off to MSNBC and they would run with it.
[00:39:28.280 --> 00:39:36.120]   And that instantiation based, existence based proofs are exactly the opposite of what statistical
[00:39:36.120 --> 00:39:37.840]   distributions are all about, right?
[00:39:37.840 --> 00:39:43.120]   A statistical distribution is about capturing the totality of a problem.
[00:39:43.120 --> 00:39:49.480]   And COVID, I think brought the concept of an exponential curve to the masses in a way
[00:39:49.480 --> 00:39:55.940]   that, you know, I think 16% of Americans get through calculus and statistics in high school.
[00:39:55.940 --> 00:39:59.840]   But now a hundred percent of Americans have been exposed to the concept of flattening
[00:39:59.840 --> 00:40:02.460]   the curve and why an exponential curve is important.
[00:40:02.460 --> 00:40:07.380]   And then we got exposed to unintentionally what happens when you intervene in an exponential
[00:40:07.380 --> 00:40:08.380]   curve.
[00:40:08.380 --> 00:40:09.740]   Oh, well, but that never was going to happen.
[00:40:09.740 --> 00:40:15.100]   Well, now we have intervention based statistics that now, you know, I'd say less than 16%
[00:40:15.100 --> 00:40:19.740]   of Americans are aware of, but statistics have moved from being kind of a fringe thing
[00:40:19.740 --> 00:40:24.740]   that affected insurance and financial markets to being something that impacts all of our
[00:40:24.740 --> 00:40:25.740]   lives.
[00:40:25.740 --> 00:40:32.020]   And I think that's something that is both exciting to me and scary because we as data
[00:40:32.020 --> 00:40:35.700]   scientists have generally lived in this world of like, I'm just living in the numbers.
[00:40:35.700 --> 00:40:39.940]   So whatever I say is just the numbers and I have no social responsibility.
[00:40:39.940 --> 00:40:43.380]   And I think that's absolutely wrong.
[00:40:43.380 --> 00:40:46.180]   That is categorically an incorrect statement.
[00:40:46.180 --> 00:40:47.980]   And in fact, I think the opposite is true.
[00:40:47.980 --> 00:40:52.460]   It's that numbers have become so important to our society that it's our job to go to
[00:40:52.460 --> 00:40:53.500]   primary sources.
[00:40:53.500 --> 00:40:54.920]   It's our job to educate our friends.
[00:40:54.920 --> 00:41:01.900]   It's our job to read past the headlines and play an active role of helping people understand
[00:41:01.900 --> 00:41:04.100]   what a distribution means to those things.
[00:41:04.100 --> 00:41:08.460]   And I had a couple of examples for me that really stood out.
[00:41:08.460 --> 00:41:12.220]   At the beginning of COVID, there was a great blog called Towards Data Science.
[00:41:12.220 --> 00:41:14.260]   I don't know if you read that one, but I like it.
[00:41:14.260 --> 00:41:15.260]   I think it's pretty good.
[00:41:15.260 --> 00:41:16.260]   Totally.
[00:41:16.260 --> 00:41:23.300]   And there was an article written in Towards Data Science that said the cancellation of
[00:41:23.300 --> 00:41:27.780]   these conferences is absolutely the wrong thing to do.
[00:41:27.780 --> 00:41:31.100]   In fact, I'll just share it for those of you that are watching.
[00:41:31.100 --> 00:41:32.100]   It's this article here.
[00:41:32.100 --> 00:41:37.340]   He invented this term called "coronia," or like paranoia, right?
[00:41:37.340 --> 00:41:40.580]   And no slight to the author here, because I think we were all learning.
[00:41:40.580 --> 00:41:44.780]   So I don't want to slight this author, but I hated the article.
[00:41:44.780 --> 00:41:50.860]   And the reason I hated the article was because the paper, the article states, "Here's the
[00:41:50.860 --> 00:41:52.380]   population of Spain.
[00:41:52.380 --> 00:41:54.900]   It's 46.66 million people.
[00:41:54.900 --> 00:41:58.020]   The total cases of coronavirus in Spain is two.
[00:41:58.020 --> 00:42:06.420]   So therefore, the number of people that would get coronavirus at this conference is .0046."
[00:42:06.420 --> 00:42:12.460]   And what I really hated about this was that we had an opportunity to kind of have an educational
[00:42:12.460 --> 00:42:15.620]   moment and have a conversation.
[00:42:15.620 --> 00:42:17.860]   And instead what we did is we chose a false population, right?
[00:42:17.860 --> 00:42:21.580]   The population of Spain and the number of cases in Spain is not the problem.
[00:42:21.580 --> 00:42:26.780]   We simplified the problem in a way that abstracted out the exponential growth component and the
[00:42:26.780 --> 00:42:30.580]   exposure component, and we stopped and didn't believe in intervention, right?
[00:42:30.580 --> 00:42:32.140]   We didn't analyze the actual data.
[00:42:32.140 --> 00:42:37.220]   And this is the danger of statistics to me, is that you can use statistics to justify
[00:42:37.220 --> 00:42:42.120]   something if you just trick people by choosing the wrong population, you trick people by
[00:42:42.120 --> 00:42:46.420]   choosing the wrong coefficients, you trick people by using the wrong underlying model,
[00:42:46.420 --> 00:42:47.420]   right?
[00:42:47.420 --> 00:42:51.820]   And in this case, he used a linear model instead of an exponential exposure model.
[00:42:51.820 --> 00:42:58.860]   And that to me, I think is the quintessence of what we as data scientists must tackle,
[00:42:58.860 --> 00:43:03.300]   is to be, sure, we can be opinionated, but to be balanced.
[00:43:03.300 --> 00:43:10.580]   And I think it's just so much more important than it ever has been because we as a population
[00:43:10.580 --> 00:43:16.300]   don't have the educational system so that 100% of Americans understand what statistics
[00:43:16.300 --> 00:43:17.300]   mean.
[00:43:17.300 --> 00:43:21.260]   That reminds me that there's been, I think, another observation that a lot of folks have
[00:43:21.260 --> 00:43:26.500]   made about machine learning algorithms that when they're based on training data that could
[00:43:26.500 --> 00:43:33.140]   have underlying bias in that data, you can easily end up with models that reinforce society's
[00:43:33.140 --> 00:43:34.460]   biases and make it worse.
[00:43:34.460 --> 00:43:37.900]   And actually, you make a device that calls the police, right?
[00:43:37.900 --> 00:43:43.820]   And so, I mean, how do you think about that in your device, in your training data, and
[00:43:43.820 --> 00:43:44.820]   what you do?
[00:43:44.820 --> 00:43:47.940]   Dude, such an important question.
[00:43:47.940 --> 00:43:51.620]   And I just made this big statement that we have to be proactive.
[00:43:51.620 --> 00:43:55.180]   We have to recognize that if we just say, "I'm agnostic because I live in the data,
[00:43:55.180 --> 00:43:59.140]   so therefore I'm not biased," is a falsehood.
[00:43:59.140 --> 00:44:02.380]   And Mark Zuckerberg said that in front of Congress.
[00:44:02.380 --> 00:44:06.380]   And I love that he did that because it brought, shone a bright light on the fact that that
[00:44:06.380 --> 00:44:07.380]   is a lie.
[00:44:07.380 --> 00:44:11.260]   And I don't know that Mark did that intentionally because I think we all believed that right
[00:44:11.260 --> 00:44:13.020]   up until very recently.
[00:44:13.020 --> 00:44:17.900]   I don't think we were exposed to our own biases and our own potential for bias until that
[00:44:17.900 --> 00:44:18.900]   happened.
[00:44:18.900 --> 00:44:23.260]   What's interesting about that moment is I happened to be in Washington, DC on that exact
[00:44:23.260 --> 00:44:28.900]   same day, and I had called a meeting with the ACLU and the NAACP and a number of other
[00:44:28.900 --> 00:44:32.220]   civil rights organizations, and I presented my business to them.
[00:44:32.220 --> 00:44:33.220]   And they all said the same thing.
[00:44:33.220 --> 00:44:34.540]   And I said, "No, no, no, no, no.
[00:44:34.540 --> 00:44:37.780]   We can't be biased because we're based on the data."
[00:44:37.780 --> 00:44:41.340]   And what it did is it really forced me to take a step back and say, "Actually, could
[00:44:41.340 --> 00:44:42.340]   I?
[00:44:42.340 --> 00:44:43.340]   Let me ask the question.
[00:44:43.340 --> 00:44:47.300]   Instead of making the statement that because I'm focused on the data, I can't be biased,
[00:44:47.300 --> 00:44:49.260]   let me make the hypothesis.
[00:44:49.260 --> 00:44:55.620]   Let me make the null hypothesis that states because I'm based in data, I can't be biased
[00:44:55.620 --> 00:44:57.140]   and then disprove it."
[00:44:57.140 --> 00:45:00.420]   And I found that there were just hundreds and hundreds of ways to disprove it.
[00:45:00.420 --> 00:45:05.140]   And so we actually designed our system specifically to be race blind.
[00:45:05.140 --> 00:45:07.940]   We intentionally designed it to be race blind.
[00:45:07.940 --> 00:45:11.820]   We managed the training data coming in so that we can identify people.
[00:45:11.820 --> 00:45:19.140]   And the second thing we did was we used the data in a way that cannot be abused from a
[00:45:19.140 --> 00:45:21.940]   race perspective and an ethnicity perspective.
[00:45:21.940 --> 00:45:25.040]   And then the third thing we did, which I did not expect to do, in fact, I came into that
[00:45:25.040 --> 00:45:28.140]   meeting with the NAACP saying, "We're not going to do this."
[00:45:28.140 --> 00:45:33.700]   I said, "We're going to not track the race of the people that we call the police on."
[00:45:33.700 --> 00:45:35.780]   And they said, "In fact, I want you to.
[00:45:35.780 --> 00:45:39.180]   I want you to do that, but I want you to store it in a system that's not being used for machine
[00:45:39.180 --> 00:45:40.180]   learning.
[00:45:40.180 --> 00:45:43.220]   I want you to store it in a system that you use for auditing your employees and you use
[00:45:43.220 --> 00:45:48.020]   it for auditing your own business to make sure that you are doing the right things."
[00:45:48.020 --> 00:45:49.540]   And so we did those three things.
[00:45:49.540 --> 00:45:54.940]   And I feel wildly good about what we did because one, we were proactive.
[00:45:54.940 --> 00:45:56.300]   We did it before we got in trouble.
[00:45:56.300 --> 00:45:59.020]   We did it before we called the police on the wrong person.
[00:45:59.020 --> 00:46:01.620]   Two, it was an educational experience for me.
[00:46:01.620 --> 00:46:04.140]   Again, I was very strongly in the camp.
[00:46:04.140 --> 00:46:05.900]   I was in a recommendation system business.
[00:46:05.900 --> 00:46:07.760]   I built the recommender at Amazon.
[00:46:07.760 --> 00:46:11.580]   Those systems are entirely based on, "Hey, you just use the data and whatever the data
[00:46:11.580 --> 00:46:14.100]   say, that's what you optimize to."
[00:46:14.100 --> 00:46:18.460]   And I was surprised and pleasantly pleased with the results of taking that step back
[00:46:18.460 --> 00:46:22.420]   and saying, "Let's treat that instead of as a conclusion, as a null hypothesis."
[00:46:22.420 --> 00:46:24.660]   And I learned a lot.
[00:46:24.660 --> 00:46:29.260]   It's a strong claim though that your system can't use race.
[00:46:29.260 --> 00:46:31.900]   How do you know that's the case?
[00:46:31.900 --> 00:46:33.220]   Not that it can't use race.
[00:46:33.220 --> 00:46:39.320]   It's that we make sure that the distribution coming in on the training side is designed
[00:46:39.320 --> 00:46:41.520]   to be race agnostic.
[00:46:41.520 --> 00:46:46.960]   It's a fair distribution on the input side, which requires tweaking the distribution on
[00:46:46.960 --> 00:46:47.960]   the input side.
[00:46:47.960 --> 00:46:53.920]   Then the second thing is we don't allow the system to perform specific activities.
[00:46:53.920 --> 00:46:57.520]   We don't do facial recognition at this point because we recognize there are some issues
[00:46:57.520 --> 00:46:59.520]   with facial recognition.
[00:46:59.520 --> 00:47:01.760]   And we don't allow it to call the police.
[00:47:01.760 --> 00:47:06.260]   We focus on classifying something that is itself race agnostic.
[00:47:06.260 --> 00:47:12.140]   We focus on behavior and we focus on classifying people versus cars.
[00:47:12.140 --> 00:47:15.820]   We do not focus on classifying suspicious looking person.
[00:47:15.820 --> 00:47:21.740]   We specifically designed the system to not be able to do that, if that makes sense.
[00:47:21.740 --> 00:47:25.580]   The system looks for people, not suspicious people.
[00:47:25.580 --> 00:47:26.580]   That's right.
[00:47:26.580 --> 00:47:27.580]   And then if it sees a person-
[00:47:27.580 --> 00:47:33.080]   Then it identifies the behavior and it separates the behavior identification from the person
[00:47:33.080 --> 00:47:34.840]   identification.
[00:47:34.840 --> 00:47:38.320]   This is what I mean by actually designing it to not be able to do that.
[00:47:38.320 --> 00:47:43.560]   We do not allow the behavior identification portion, the suspicious identification portion,
[00:47:43.560 --> 00:47:47.240]   to know what the person looks like.
[00:47:47.240 --> 00:47:49.560]   It does not have access to the pixels.
[00:47:49.560 --> 00:47:56.440]   It only has access to a completely removed representation of the behavior.
[00:47:56.440 --> 00:48:01.300]   So it's a set of vectors and features that have been completely, have entirely removed
[00:48:01.300 --> 00:48:02.940]   all the pixels.
[00:48:02.940 --> 00:48:07.020]   So it sort of like synthesizes the pixels into some information that then-
[00:48:07.020 --> 00:48:08.020]   That's right.
[00:48:08.020 --> 00:48:12.060]   So we created an intermediary data structure, which is, here is this object, here's its
[00:48:12.060 --> 00:48:16.360]   classification and here are the dimensions of its motion.
[00:48:16.360 --> 00:48:17.660]   And then we drew a hard line.
[00:48:17.660 --> 00:48:23.140]   That system does not talk to the other system that says, "Based on these motion vectors
[00:48:23.140 --> 00:48:27.540]   and this description of behavior, this is suspicious or not suspicious."
[00:48:27.540 --> 00:48:28.540]   Interesting.
[00:48:28.540 --> 00:48:34.660]   Again, it was kind of non-intuitive to do that because as a data scientist, I would
[00:48:34.660 --> 00:48:36.820]   say, "I just want to go from here to here."
[00:48:36.820 --> 00:48:38.940]   That's going to be much more accurate.
[00:48:38.940 --> 00:48:45.520]   And from a mean average precision perspective, it might be, but it also is exposed in a real
[00:48:45.520 --> 00:48:49.480]   world context to having bias in the end to end.
[00:48:49.480 --> 00:48:53.880]   But by enforcing this intermediary that gets rid of the pixels that might contain anything
[00:48:53.880 --> 00:48:59.960]   having to do with race, it makes sure that it can't propagate through.
[00:48:59.960 --> 00:49:05.200]   And then you collect data that tells you that it's not using race, you're saying?
[00:49:05.200 --> 00:49:11.040]   So what we do is we then collect data in an independent system that says, "These guards
[00:49:11.040 --> 00:49:17.620]   were calling the police on only Latina people or only on Asian people or only on black people."
[00:49:17.620 --> 00:49:23.720]   And making sure that we are auditing all chunks of the system against the racial distribution
[00:49:23.720 --> 00:49:27.740]   so that they're acting in a way that is consistent.
[00:49:27.740 --> 00:49:29.820]   And is there any other groups that you worry about?
[00:49:29.820 --> 00:49:34.960]   Do you look at gender too or other aspects of appearance or anything like that?
[00:49:34.960 --> 00:49:35.960]   We do.
[00:49:35.960 --> 00:49:42.760]   So we track the race, the gender, and then generally the age as well, because age is
[00:49:42.760 --> 00:49:43.760]   a really important one.
[00:49:43.760 --> 00:49:48.980]   One of the things that I learned in my meeting with the NAACP is that black males under 18
[00:49:48.980 --> 00:49:54.320]   are frequently classified both by police and by witnesses as being adults.
[00:49:54.320 --> 00:49:56.700]   And you need to treat minors differently.
[00:49:56.700 --> 00:50:02.820]   We have a social responsibility to say, "Hey, man, I see you T.P.ing that house.
[00:50:02.820 --> 00:50:04.300]   Could you get out of here?"
[00:50:04.300 --> 00:50:06.980]   And we call the homeowner and say, "Hey, there's somebody T.P.ing your house."
[00:50:06.980 --> 00:50:10.560]   Instead of, "Hey, asshole, I'm calling the police.
[00:50:10.560 --> 00:50:12.780]   You need to stop right now."
[00:50:12.780 --> 00:50:18.340]   That creates, as we're seeing, the intervention of escalation creates a different outcome.
[00:50:18.340 --> 00:50:20.380]   The intervention creates the outcome.
[00:50:20.380 --> 00:50:25.820]   And so you better darn well choose that intervention based on real data.
[00:50:25.820 --> 00:50:32.540]   And if at the end of the day, black males are specifically classified by people frequently
[00:50:32.540 --> 00:50:35.420]   as being older, then we got to train for that.
[00:50:35.420 --> 00:50:36.740]   We got to compensate for that.
[00:50:36.740 --> 00:50:42.740]   We got to treat minors as minors because their brain development is the same regardless of
[00:50:42.740 --> 00:50:43.740]   race.
[00:50:43.740 --> 00:50:45.540]   We have to enable them to develop their brains.
[00:50:45.540 --> 00:50:47.140]   Have you written about this at all?
[00:50:47.140 --> 00:50:49.700]   Is there some place we could point people that wanted to learn more about this?
[00:50:49.700 --> 00:50:51.820]   I'm just not sure we can cover all the questions that-
[00:50:51.820 --> 00:50:54.700]   You know, I haven't written about it as much as I probably should have.
[00:50:54.700 --> 00:50:55.700]   I actually just did that.
[00:50:55.700 --> 00:51:00.580]   I had that meeting with those groups and I just did that for the purposes of ourselves.
[00:51:00.580 --> 00:51:05.780]   I probably should at some point write about why I did that and what it means and what
[00:51:05.780 --> 00:51:07.180]   the social impact is of that.
[00:51:07.180 --> 00:51:12.300]   I did do a little letter to our customer base when the Black Lives Matter movement was really
[00:51:12.300 --> 00:51:13.300]   kind of taking hold.
[00:51:13.300 --> 00:51:16.740]   And I said, "Look, I want you to know what we've done because I'm really proud of it
[00:51:16.740 --> 00:51:20.380]   and I don't want racism to exist in our business because it's so dangerous."
[00:51:20.380 --> 00:51:23.700]   Because we live at this intersection between human beings and the police.
[00:51:23.700 --> 00:51:27.060]   We have a real responsibility to take that at a high level.
[00:51:27.060 --> 00:51:31.820]   And I'd say 99% of our customers were like, "That's awesome that you've done that.
[00:51:31.820 --> 00:51:35.880]   You don't have to pick a political side to say you've done their socially and business
[00:51:35.880 --> 00:51:36.880]   responsible thing."
[00:51:36.880 --> 00:51:40.540]   Interestingly enough, we did get two or three customers that came back and were like, "F
[00:51:40.540 --> 00:51:43.780]   so, you're making this up."
[00:51:43.780 --> 00:51:46.460]   Which was surprising to me.
[00:51:46.460 --> 00:51:50.260]   So the hostility was like, "You're making this up or we don't like that you're doing
[00:51:50.260 --> 00:51:51.260]   this."
[00:51:51.260 --> 00:51:52.340]   You're making up that you did this.
[00:51:52.340 --> 00:51:54.020]   You're making up that there's a real problem.
[00:51:54.020 --> 00:51:55.020]   There is no problem.
[00:51:55.020 --> 00:51:56.020]   Oh, I see.
[00:51:56.020 --> 00:51:58.020]   Ben Shapiro told me that there's no problem.
[00:51:58.020 --> 00:51:59.020]   Right.
[00:51:59.020 --> 00:52:00.020]   Got it.
[00:52:00.020 --> 00:52:04.140]   At the end of the day, I very much strongly believe in the first amendment and I love
[00:52:04.140 --> 00:52:05.340]   that people have their opinions.
[00:52:05.340 --> 00:52:10.980]   And again, this is where statistics come into play.
[00:52:10.980 --> 00:52:15.140]   There are no real statistics that say that our country is not racially biased.
[00:52:15.140 --> 00:52:16.140]   Zero.
[00:52:16.140 --> 00:52:22.220]   Well, let me ask you one final question, maybe a little less intense than that one.
[00:52:22.220 --> 00:52:29.500]   In your process of going from a model to a deployed system, where have been the surprising
[00:52:29.500 --> 00:52:30.500]   bottlenecks?
[00:52:30.500 --> 00:52:35.100]   I think everybody senses that it's hard, but what were the points where this took a lot
[00:52:35.100 --> 00:52:36.100]   longer?
[00:52:36.100 --> 00:52:38.500]   It was a lot harder than you were imagining that it would be.
[00:52:38.500 --> 00:52:39.500]   Yeah.
[00:52:39.500 --> 00:52:47.940]   So number one, first and foremost, is specific operands on specific chipsets.
[00:52:47.940 --> 00:52:54.740]   So for example, Qualcomm implements this huge stack of operands and implements them in a
[00:52:54.740 --> 00:52:56.780]   way that you can accelerate.
[00:52:56.780 --> 00:53:01.220]   And so as long as you kind of stay in that little pool, you're okay.
[00:53:01.220 --> 00:53:04.500]   But all the new architectures typically are using new operands.
[00:53:04.500 --> 00:53:09.020]   That's a big piece of the new papers is they're getting the state of the art by using this
[00:53:09.020 --> 00:53:16.820]   new version of ReLU or this new version of a generalization operand between layers.
[00:53:16.820 --> 00:53:23.200]   And more activation functions, like other activation functions that are non-ReLU based.
[00:53:23.200 --> 00:53:29.940]   And what we have found is that in the last three years in general, that that problem
[00:53:29.940 --> 00:53:33.300]   is being solved with like a big baseball bat.
[00:53:33.300 --> 00:53:37.620]   Instead of being solved like with a surgical tool and being precise, it's being solved
[00:53:37.620 --> 00:53:38.620]   with broad swing.
[00:53:38.620 --> 00:53:42.980]   So for example, Qualcomm has the problem that if you don't support it, it just crashes.
[00:53:42.980 --> 00:53:43.980]   That's sweet.
[00:53:43.980 --> 00:53:44.980]   Good to know.
[00:53:44.980 --> 00:53:48.700]   Then you've got TF Lite, which I think is a much more robust architecture.
[00:53:48.700 --> 00:53:52.260]   And I like TF Lite a lot, but what does TF Lite do?
[00:53:52.260 --> 00:53:56.580]   Well, if you're using an operand that isn't supported on your particular architecture,
[00:53:56.580 --> 00:53:59.580]   you just move everything from that point forward into your CPU.
[00:53:59.580 --> 00:54:07.360]   So you go from a model, you make this tiny tweak to a model and it runs in 25 milliseconds.
[00:54:07.360 --> 00:54:11.940]   And then you make a tiny tweak and now it takes 750 milliseconds.
[00:54:11.940 --> 00:54:15.580]   It's not this kind of like nice smooth curve where things kind of get reconnected in the
[00:54:15.580 --> 00:54:16.580]   middle.
[00:54:16.580 --> 00:54:19.500]   It's just, and it blows up.
[00:54:19.500 --> 00:54:23.540]   And as you might guess, systems that are based on an assumption of performance between 25
[00:54:23.540 --> 00:54:28.420]   and 750 milliseconds do not perform well when performance goes to 750 milliseconds.
[00:54:28.420 --> 00:54:33.700]   So I think we've improved in that TF Lite doesn't crash and totally die, but actually
[00:54:33.700 --> 00:54:36.500]   at the end of the day, your system crashes and totally dies.
[00:54:36.500 --> 00:54:41.640]   So you get kind of the same outcome without having a system error, which is maybe better
[00:54:41.640 --> 00:54:44.580]   and moving in the right direction, but certainly not there.
[00:54:44.580 --> 00:54:48.740]   It's definitely still a lot harder than I would like.
[00:54:48.740 --> 00:54:56.140]   I would say that the second thing that I've learned is how incredibly distinct the world
[00:54:56.140 --> 00:55:00.040]   of training is from the world of runtime operations.
[00:55:00.040 --> 00:55:03.300]   If you're running it in the cloud and you're willing to pay the GPU prices on the cloud
[00:55:03.300 --> 00:55:06.020]   providers, then it's not that big a deal.
[00:55:06.020 --> 00:55:11.340]   Any of the more precise architectures, the level of kind of OS tuning that you have to
[00:55:11.340 --> 00:55:17.120]   do, the level of firmware driver management that you have to do, it's a lot more than
[00:55:17.120 --> 00:55:20.980]   I thought it was again, kind of coming from the Raspberry Pi tinkering type part of the
[00:55:20.980 --> 00:55:27.100]   world to actually implementing it and having it run a thousand times an hour, every single
[00:55:27.100 --> 00:55:32.820]   hour, 24 hours a day for 365 days a year in all 50 states.
[00:55:32.820 --> 00:55:36.660]   That last mile was much, much more complex than I expected.
[00:55:36.660 --> 00:55:37.660]   Interesting.
[00:55:37.660 --> 00:55:38.660]   Cool.
[00:55:38.660 --> 00:55:41.580]   Well, thanks so much for your time, I really appreciate it.
[00:55:41.580 --> 00:55:42.940]   It was great to catch up with you.
[00:55:42.940 --> 00:55:45.620]   I wish I could see you face to face.
[00:55:45.620 --> 00:55:48.760]   Thanks for listening to another episode of Gradient Dissent.
[00:55:48.760 --> 00:55:53.080]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:55:53.080 --> 00:55:55.840]   hear from the people that are listening to these episodes.
[00:55:55.840 --> 00:55:59.920]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:55:59.920 --> 00:56:03.880]   a conversation, that would make me inspired to do more of these episodes.
[00:56:03.880 --> 00:56:07.440]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

