
[00:00:00.000 --> 00:00:10.160]   Awesome. Cool. So, this is the starting of Fastbook session. And this is the second session.
[00:00:10.160 --> 00:00:15.160]   We had a last we had a first one last week. This is the second session. And we're going
[00:00:15.160 --> 00:00:20.720]   to start looking at Chapter 1. I'm excited to have you I'm excited to have everybody
[00:00:20.720 --> 00:00:27.240]   here. And I mean, this is this is a journey where as Jeremy mentioned last week is like
[00:00:27.240 --> 00:00:30.920]   we want to life will happen along the side. And what we want to do is we want to commit
[00:00:30.920 --> 00:00:38.200]   ourselves for the next 20 weeks. And conscious of time as well. Like we have I'm going to
[00:00:38.200 --> 00:00:45.440]   try and cover a chapter every week. Spending 60 minutes per chapter. But sometimes if you
[00:00:45.440 --> 00:00:50.960]   when we go deeper and deeper into the book, the book increases in complexity very fast.
[00:00:50.960 --> 00:00:57.560]   It's like an exponential curve. And something what we want to do is we want to stay as a
[00:00:57.560 --> 00:01:03.200]   group. Like we want to come back every week. We want to be there for each other when the
[00:01:03.200 --> 00:01:09.040]   complexity increases. Because I think that's where the reward is. The reward comes when
[00:01:09.040 --> 00:01:14.520]   we face a challenge and then we don't give up and we keep asking questions. We keep answering
[00:01:14.520 --> 00:01:19.960]   questions. And that's where the that's where the major benefit comes. So, that's one thing
[00:01:19.960 --> 00:01:25.560]   that I want to start with saying is like we want to come back every week and we we want
[00:01:25.560 --> 00:01:29.240]   to make sure that we finish and we finish strong.
[00:01:29.240 --> 00:01:37.320]   Okay. So, for the agenda for today, we're going to start by looking at the first chapter.
[00:01:37.320 --> 00:01:41.440]   We're going to start with a deep learning journey. It's it is a few topics that we will
[00:01:41.440 --> 00:01:47.560]   look into today. I'll spend the most time towards the later end. Like 1.6 is where I'll
[00:01:47.560 --> 00:01:52.000]   spend the most time, your first model. And if we get to 1.8, that's where I'll spend
[00:01:52.000 --> 00:01:58.600]   my most time as well. The other things are the idea for me is to give enough information
[00:01:58.600 --> 00:02:03.360]   that you can then that you can then go back and like give you enough information for you
[00:02:03.360 --> 00:02:09.200]   to go back and read the chapter. And if you have any questions and if you have, you know,
[00:02:09.200 --> 00:02:14.960]   you have any problems, you can you can reach out to us. And I'll be there to answer. So,
[00:02:14.960 --> 00:02:19.000]   one thing I did say last week as well, I do take it as my personal responsibility to make
[00:02:19.000 --> 00:02:25.400]   sure that we all get past the line and to make sure that we all learn and we all benefit
[00:02:25.400 --> 00:02:30.800]   from from from these Fastbook reading sessions. And I still live by it. So, you know, don't
[00:02:30.800 --> 00:02:36.640]   be afraid to reach out to me. Don't be afraid. I'll be investing a lot of my time in doing
[00:02:36.640 --> 00:02:41.840]   this and hopefully we can all benefit towards the end. So, yeah. So, that's the that's the
[00:02:41.840 --> 00:02:49.920]   idea. That's the agenda for today. If you haven't joined the 1DB.me/slack, now is a
[00:02:49.920 --> 00:02:56.840]   good time to do so. So, I'm just going to paste this in. I've just shared this I've
[00:02:56.840 --> 00:03:02.400]   just shared this link in Zoom chat as well. We're going to do something very similar as
[00:03:02.400 --> 00:03:08.200]   last time. We're going to use the report. We're going to for for questions. But if you
[00:03:08.200 --> 00:03:11.840]   haven't joined our Wits and Biases Slack, I think this is a good time because you go
[00:03:11.840 --> 00:03:19.240]   join in there and you go to the Fastbook channel. That's where we can ask each other quick questions.
[00:03:19.240 --> 00:03:33.440]   I'm sorry. I just saw somebody mentioned that my mic could be closed. Is this better? Okay.
[00:03:33.440 --> 00:03:41.320]   Awesome. Apologies about that. Cool. I'm not constantly monitoring the chat, though.
[00:03:41.320 --> 00:03:45.760]   So, if there's anything, you can just mention the chat and Angelica, you can reach out to
[00:03:45.760 --> 00:03:52.360]   me. I think that will be easier for us in the next 60 minutes.
[00:03:52.360 --> 00:03:59.600]   One thing I do want to stress a lot when we're beginning this journey is I want you to ask
[00:03:59.600 --> 00:04:06.400]   a lot of questions. Now it's just now it's just between us. And I want you to think of
[00:04:06.400 --> 00:04:13.920]   us as everybody in this group as a safe group. I want you to not be afraid of asking any
[00:04:13.920 --> 00:04:18.920]   question. There's no question as a silly question. And going forward, what we want to do is we
[00:04:18.920 --> 00:04:24.960]   want to ask lots and lots of questions. Because I've seen from personal experience that sometimes
[00:04:24.960 --> 00:04:31.560]   what we do is when we're stuck, we give up. And one of the easiest ways to get unstuck
[00:04:31.560 --> 00:04:37.040]   is to ask the question. Because somebody else might have the answer to a question that we're
[00:04:37.040 --> 00:04:44.000]   stuck at. And if you spend enough time on it when I say enough time, I think if you
[00:04:44.000 --> 00:04:48.480]   believe that this is something that you need help with, don't be afraid. And ask your question
[00:04:48.480 --> 00:04:53.680]   right away. And something you'll notice with the Fast.ai community is everybody here is
[00:04:53.680 --> 00:04:59.960]   very helpful. And you know, you'll people will make a genuine effort to try and find
[00:04:59.960 --> 00:05:07.280]   answers for the questions that you do ask. So please ask lots of questions. The next
[00:05:07.280 --> 00:05:11.960]   thing I want to say is answer questions, answer questions and lots of questions. So if you're
[00:05:11.960 --> 00:05:17.520]   somebody who has an answer to a question that somebody has already asked, be there to answer
[00:05:17.520 --> 00:05:22.880]   it for them. I might not be there or might not be around. It might be my night time or
[00:05:22.880 --> 00:05:26.360]   it might be very early morning. But if you're there and you see someone asking a question
[00:05:26.360 --> 00:05:31.440]   and you have the answer to it, I want you to think that you want to spend that time
[00:05:31.440 --> 00:05:35.440]   on in answering that question. And the reason and one example that I like to give here is
[00:05:35.440 --> 00:05:43.840]   that of Zach. Zach has spent Zach has spent over 33 days of reading time in the Fast.ai
[00:05:43.840 --> 00:05:54.080]   forums. If you haven't looked at the Fast.ai forums, let me bring them up quickly. So forums
[00:05:54.080 --> 00:05:58.760]   that Fast.ai, we briefly mentioned this and shared this last time as well. But forums
[00:05:58.760 --> 00:06:04.240]   that Fast.ai are a place where there's lots of activities happening. There's lots of threads
[00:06:04.240 --> 00:06:11.160]   in forums that Fast.ai. And something that Zach is really good at is like he spent a
[00:06:11.160 --> 00:06:17.800]   lot of time answering questions and discussing questions. And it's really helpful when you
[00:06:17.800 --> 00:06:22.360]   answer questions because you get a deeper understanding of things as you go. So something
[00:06:22.360 --> 00:06:29.560]   I really want to stress as well is if you try and answer somebody else's question, in
[00:06:29.560 --> 00:06:33.960]   the end, you might benefit with a deeper understanding of the thing that you're answering question
[00:06:33.960 --> 00:06:43.720]   of. Let me also quickly show up Slack where we're going to be discussing these questions.
[00:06:43.720 --> 00:06:48.600]   So if you go in Slack, it's called Weights and Biases Forum. To join it, the easiest
[00:06:48.600 --> 00:06:56.200]   way is to go to 1db.me/slack. Sorry, can you guys see my screen? And you can see Slack,
[00:06:56.200 --> 00:07:03.680]   right? Awesome. Cool. So then this is where in the Weights and Biases Slack, this is where
[00:07:03.680 --> 00:07:07.560]   you want to go and you want to ask your questions or answer your questions. If you go head over
[00:07:07.560 --> 00:07:12.960]   to the Fastbook channel, I've seen that there's already a question here and I'm making an
[00:07:12.960 --> 00:07:16.640]   effort to answer all of them. There's lots of other things. There's sometimes announcements
[00:07:16.640 --> 00:07:22.960]   and there's lots of activity happening in this Fastbook. You want to join this channel
[00:07:22.960 --> 00:07:33.360]   and you want to be a part of it. Next thing I do want to mention, if you go to 1db.me/fc,
[00:07:33.360 --> 00:07:44.160]   I'll place this in the channel as well. So if you go to 1db.me/fc and if you have a quick
[00:07:44.160 --> 00:07:50.720]   question on this, where is he going to reach out to me? If you don't know of this place,
[00:07:50.720 --> 00:07:56.960]   then go to 1db.me/fc and then head over to the forums. In the forums, you'll see lots
[00:07:56.960 --> 00:08:02.240]   of other cool stuff. So we'll be using a mix, like we'll be using a mix for Weights and
[00:08:02.240 --> 00:08:08.240]   Biases Slack. We'll be using a mix of Weights and Biases forums or even the Fast.ai forums.
[00:08:08.240 --> 00:08:14.600]   For me, either/or is equally good as long as I get tagged or as long as I know that
[00:08:14.600 --> 00:08:19.600]   you're asking a question. So we can get there and answer those questions for you. Today
[00:08:19.600 --> 00:08:24.120]   specifically we are going to be going to this, like we did last time, we're going to be going
[00:08:24.120 --> 00:08:30.120]   to this Weights and Biases report because we're also live on YouTube. So it's very hard
[00:08:30.120 --> 00:08:34.840]   for me to look at the Zoom chat. It's very hard for me to look at the YouTube chat. So
[00:08:34.840 --> 00:08:38.560]   this is a central place where if you write a comment and you just, which I'm writing
[00:08:38.560 --> 00:08:45.680]   test, it is live. So towards the end, we're going to try and answer these questions, whichever
[00:08:45.680 --> 00:08:51.880]   ones they may be over here. So I'll paste this. Oh, so by the way, Angelica has already
[00:08:51.880 --> 00:08:59.640]   shared in the Zoom chat, the easiest way to get here is to type this 1db.me/fastbook1.
[00:08:59.640 --> 00:09:05.040]   And every week going forward, this will go from Fastbook Week 1 to Fastbook Week 2, Week
[00:09:05.040 --> 00:09:15.960]   3 and so on. Cool. So we want to ask lots of questions. We want to answer lots of questions.
[00:09:15.960 --> 00:09:21.280]   We make progress as a group. What I mean by that is we're not here to judge anybody. We're
[00:09:21.280 --> 00:09:26.520]   not here to laugh at anybody. We're not here to feel like, oh, this guy doesn't know this
[00:09:26.520 --> 00:09:30.960]   or this person doesn't know that. No, instead we're going to be, okay, well, if you don't
[00:09:30.960 --> 00:09:36.000]   know something, I have two years of experience in doing this and I'm going to make my best
[00:09:36.000 --> 00:09:42.280]   ever to help you. Because the reason why I say this and the reason why this is crucial,
[00:09:42.280 --> 00:09:48.640]   in my opinion, to make progress as a group, most of the people in Fastbook that are here
[00:09:48.640 --> 00:09:54.640]   today have told me that they started their journey by themselves alone and they gave
[00:09:54.640 --> 00:09:59.720]   up on that journey. And the sole reason for that was they hit a roadblock that they could
[00:09:59.720 --> 00:10:05.080]   not cross by themselves. But that's the difference in doing this alone at home. And this is the
[00:10:05.080 --> 00:10:10.360]   difference of doing this as a group. So if we meet as a group for the next 20 weeks,
[00:10:10.360 --> 00:10:15.160]   then we make sure that if you have a question, there might be somebody who's really good
[00:10:15.160 --> 00:10:19.920]   at stats who will be able to answer that question. And that stats person who answered your question
[00:10:19.920 --> 00:10:23.800]   might have a question about programming, which you're good at, so you can answer that question.
[00:10:23.800 --> 00:10:30.440]   So see, that's the whole point of making progress as a group. One thing I really want to try
[00:10:30.440 --> 00:10:36.680]   and one thing I make my best efforts towards is we don't leave anybody behind. So as in
[00:10:36.680 --> 00:10:40.720]   what I mean by that is we make sure that we finish and we make, because that's where the
[00:10:40.720 --> 00:10:47.200]   progress will be. Like the progress, you won't notice the progress in the first session today,
[00:10:47.200 --> 00:10:51.280]   but you'll notice the progress after 10 weeks. Because that's where week on week and week
[00:10:51.280 --> 00:10:55.920]   by week, we learn new and new things. And that's where the progress will come. And also
[00:10:55.920 --> 00:11:01.960]   week by week, things will get more and more difficult. So we want to stick to people around
[00:11:01.960 --> 00:11:06.200]   us and we want to have this really good community and we want to make sure that we ask questions
[00:11:06.200 --> 00:11:17.120]   and we answer questions. With that being said, from this point on, you will be required to
[00:11:17.120 --> 00:11:23.120]   write and read code. If you haven't already seen this fastbook set of instructions forum
[00:11:23.120 --> 00:11:34.480]   post that I created, the easiest way to get there would be, again, if you go to 1db.me/fc
[00:11:34.480 --> 00:11:41.160]   and you go to the, and you head over to the forum. Very soon we're going to have a fastbook
[00:11:41.160 --> 00:11:49.800]   tag here, so it will make things easier, but you can search fastbook. That will bring up
[00:11:49.800 --> 00:11:55.500]   everything that you need. And here it is, a fastbook set of instructions. This is something
[00:11:55.500 --> 00:12:02.240]   where I have pointed out the list. There's a reading plan on what we're going to spend
[00:12:02.240 --> 00:12:07.600]   and how much time we're going to spend every week, which topics we're going to cover. We'll
[00:12:07.600 --> 00:12:12.360]   be joined by Zach, we'll be joined by Tanishq, we'll be joined by Sanyam, I believe they're
[00:12:12.360 --> 00:12:20.160]   here today as well. And this is how roughly the plan looks like. For me, the stress and
[00:12:20.160 --> 00:12:24.120]   the one thing that I really care about is making sure that we understand things. Even
[00:12:24.120 --> 00:12:31.080]   if we take, instead of 25 weeks, if we take 30 weeks, that's fine with me. But for me,
[00:12:31.080 --> 00:12:36.320]   what's not fine is that we leave a topic. So we don't want to leave something that could
[00:12:36.320 --> 00:12:41.360]   be important and then still try to catch up with the timeline. So if instead of 25 weeks,
[00:12:41.360 --> 00:12:48.240]   it takes 27, that's fine. And I'm going to try and make the best effort to complete one
[00:12:48.240 --> 00:12:55.800]   chapter a week. So we'll see how that goes. So here it is. This is the important bit.
[00:12:55.800 --> 00:13:00.320]   When you go back today, you will be required to do this. So how do you set up? Do I need
[00:13:00.320 --> 00:13:07.760]   a GPU? Recently, Jeremy on his YouTube channel, so if I go to YouTube and I search for Jeremy,
[00:13:07.760 --> 00:13:13.520]   you will be able to find that he has a lesson zero practical deep learning for coders, fast.ai.
[00:13:13.520 --> 00:13:18.000]   And I've posted that link in this forum post as well. So let me quickly post this in the
[00:13:18.000 --> 00:13:26.040]   chat as well. So if you can go to this page, you will be able to find that there is this
[00:13:26.040 --> 00:13:32.120]   YouTube video in this video. Jeremy shares information on how you can set up for doing
[00:13:32.120 --> 00:13:38.600]   the fast.ai course. You might have a question about do I need a GPU? There's lots of free
[00:13:38.600 --> 00:13:44.200]   options. One of them that are found to be the most important to be the easiest is using
[00:13:44.200 --> 00:13:49.680]   Google Colab. So if I just click here, there's a just click here and you can select any chapter.
[00:13:49.680 --> 00:13:54.920]   If I click here, as long as you have a Google account, this is really easy to start. So
[00:13:54.920 --> 00:14:01.160]   I just clicked on it. It opens something like this and I can go to it because we're looking
[00:14:01.160 --> 00:14:11.480]   at the first chapter today. So I can click on zero one intro. I can go to run time. I
[00:14:11.480 --> 00:14:17.160]   click on change runtime type. I select GPU. I click save and that's it. I'm good to go.
[00:14:17.160 --> 00:14:38.120]   Now I can actually run my cell. That's it. And then I can import everything and you'll
[00:14:38.120 --> 00:14:43.200]   be able to run code in here. I just wanted to share that. So that's one of the easiest
[00:14:43.200 --> 00:14:48.760]   and fastest ways to get started. It requires me to authenticate, which I can do later.
[00:14:48.760 --> 00:14:53.440]   But that's one of the easiest ways to get started. In that video and on the fast.ai
[00:14:53.440 --> 00:14:58.440]   website, there's also lots of other options. So if you're somebody who has a background
[00:14:58.440 --> 00:15:04.720]   in using AWS, Azure, or Jarvis Labs, or even Data Crunch. So these are other options that
[00:15:04.720 --> 00:15:11.280]   are mentioned here. And I've already have this link posted in that forum post. Let me
[00:15:11.280 --> 00:15:18.920]   quickly share it here as well. Okay. So there's lots of options that are already there on
[00:15:18.920 --> 00:15:26.920]   the fast.ai website. I do recommend that if you're new to this, don't be scared. Go back,
[00:15:26.920 --> 00:15:32.240]   take your time, watch Jeremy's video, and then set up, make sure that you can run the
[00:15:32.240 --> 00:15:36.680]   first notebook. If you can't, reach out to us on the Weights and Biases forums, the forums
[00:15:36.680 --> 00:15:50.400]   at fast.ai or Slack, anywhere is fine. Okay. With that being said, that was the early instructions.
[00:15:50.400 --> 00:15:56.880]   Now we're going to start with all the topics that are covered in the first chapter. Most
[00:15:56.880 --> 00:16:02.240]   particularly, we're going to be spending time in training our first model today. You heard
[00:16:02.240 --> 00:16:07.280]   it right. We're going to, within the next 30 minutes, we will have trained our first
[00:16:07.280 --> 00:16:14.160]   image classification model. And we'll also spend some time looking at the validation
[00:16:14.160 --> 00:16:27.720]   sets and the test sets. So I'm actually using AWS to set things up, and I'm running things
[00:16:27.720 --> 00:16:33.020]   on sorry, I'm using GCP and I'm running things in Jupyter. If you haven't seen something
[00:16:33.020 --> 00:16:39.380]   like this, this environment is called a Jupyter notebook. Again, in that video that Jeremy
[00:16:39.380 --> 00:16:46.080]   has shared already, you'll find information about what a Jupyter notebook is. So we'll
[00:16:46.080 --> 00:16:56.960]   get started. But very quickly, if anybody has any questions, let me just go to that.
[00:16:56.960 --> 00:17:00.400]   Let me just go here and let me just double check if anybody has any questions so far.
[00:17:00.400 --> 00:17:06.040]   Oh, there's a few questions. Okay. Let's take what time is it now? Let's take three minutes
[00:17:06.040 --> 00:17:16.160]   and let me see how many efforts I can. How important is understanding of ML in order
[00:17:16.160 --> 00:17:20.160]   to get started with deep learning? It's not important at all. Like everything we're going
[00:17:20.160 --> 00:17:26.080]   to do in this book, like you're not required, the expectation is that you can understand
[00:17:26.080 --> 00:17:31.720]   code. Apart from that, we're here and that's the journey. We're going to start with exactly
[00:17:31.720 --> 00:17:34.680]   like one of the things that we're going to look at today is what is machine learning
[00:17:34.680 --> 00:17:42.600]   and what is deep learning. So it's not a requirement at all. Thanks, Kevin. Put effort in your
[00:17:42.600 --> 00:17:47.240]   questions to providing source code that reproduces errors is the top way to get an answer. That's
[00:17:47.240 --> 00:17:52.120]   really helpful. I think what Kevin is mentioning here, like if you're trying to run code and
[00:17:52.120 --> 00:17:58.440]   you get stuck, please copy paste the whole thing and don't like just go, Oh, something
[00:17:58.440 --> 00:18:02.360]   like this is not working because then it's easier for us or for the other person on the
[00:18:02.360 --> 00:18:07.560]   other side to let you know exactly how to solve that problem. And something I want to
[00:18:07.560 --> 00:18:13.760]   say is like something that has really helped me in my career has been to be visible in
[00:18:13.760 --> 00:18:20.160]   the public eye. So I think not being shy of asking questions, not being shy of writing
[00:18:20.160 --> 00:18:26.600]   blog posts, not being shy of being active on Twitter is something that's rather a bigger
[00:18:26.600 --> 00:18:34.720]   benefit than not asking questions. Because if I'm not embarrassed to ask a question,
[00:18:34.720 --> 00:18:39.320]   but I'm stuck at that question and I ask it on Twitter publicly, then somebody is going
[00:18:39.320 --> 00:18:45.320]   to give me the answer. And that's a bigger benefit than not asking a question. And one
[00:18:45.320 --> 00:18:51.000]   thing I do want to stress as well from this point on is what we want to do is we also
[00:18:51.000 --> 00:18:54.680]   want to stress and focus on writing lots of blog posts, because that's something that
[00:18:54.680 --> 00:19:01.280]   Jeremy has always said that writing blog posts is really, really helpful. And I can say that
[00:19:01.280 --> 00:19:05.360]   from my personal experience as well. When you share your resume with people, only some
[00:19:05.360 --> 00:19:09.440]   people are going to see it when you apply for a job. But your blog post actually gets
[00:19:09.440 --> 00:19:13.760]   seen by a lot more people. It could be cattle kernels. It could be lots of things. So feel
[00:19:13.760 --> 00:19:20.200]   free to ask questions openly and don't be afraid at all. Nobody's going to judge you.
[00:19:20.200 --> 00:19:24.840]   And I promise that any question that I can answer, I will find the time to answer those
[00:19:24.840 --> 00:19:30.480]   questions. Why have you chosen to make the sessions without audio or video from the user?
[00:19:30.480 --> 00:19:34.360]   It makes me think that this group is more focused on the content versus is that a fair
[00:19:34.360 --> 00:19:40.200]   statement? Do you think there's value? Oh, absolutely. There's no such thing as there's
[00:19:40.200 --> 00:19:47.040]   no point in like having you guys without audio or video. It's just a number. In Zoom, you
[00:19:47.040 --> 00:19:50.160]   have other things like in Zoom, you can have a meeting to only a certain number of people
[00:19:50.160 --> 00:19:54.680]   and then you have to go switch to a webinar. So that's only the technical detail. For me,
[00:19:54.680 --> 00:19:59.560]   there's no such thing as we can't have audio from you guys. In fact, I would love to meet
[00:19:59.560 --> 00:20:06.920]   you guys. So it's just let me go back and bring this up with the events team. And if
[00:20:06.920 --> 00:20:13.160]   it is possible for 200 or 300 or 500 members to have audio and video on at the same time,
[00:20:13.160 --> 00:20:16.920]   then let's see. But otherwise, one thing that's going to happen is that going forward, we're
[00:20:16.920 --> 00:20:21.960]   going to form lots of small study groups, which are more concentrated and where we can
[00:20:21.960 --> 00:20:26.400]   have audio and video on at the same time. But if you have a question, just raise your
[00:20:26.400 --> 00:20:35.160]   hand and we can allow you to ask that question. How can I set up notifications for the fast
[00:20:35.160 --> 00:20:42.800]   up tag? That's something I'm not sure if that's a feature in yet. But actually, if there's
[00:20:42.800 --> 00:20:46.520]   something that you might know about this, you can please feel free to answer this one.
[00:20:46.520 --> 00:20:53.640]   Okay. Yes, there's going to be recordings about this session and Discord. We're not
[00:20:53.640 --> 00:20:59.880]   using Discord. We're using Slack. Is it only for discussions? It's either or. There's no
[00:20:59.880 --> 00:21:05.920]   point of saying Slack is only for this or the posts aren't. Do we really need a GPU?
[00:21:05.920 --> 00:21:10.320]   No, Google Cloud Platform provides it for free, so you don't need it. Will you recommend
[00:21:10.320 --> 00:21:14.400]   setting up locally? These are all questions I will answer now because I'm also conscious
[00:21:14.400 --> 00:21:33.800]   of time. So let's kick off. Okay. First things first, I want to give you enough context that
[00:21:33.800 --> 00:21:41.480]   you can then go back and you can read the whole Lesson 1 intro and you'll get all of
[00:21:41.480 --> 00:21:48.840]   it. That's my aim today. So the first thing is deep learning is for everyone. If you're
[00:21:48.840 --> 00:21:53.440]   someone who comes from a background who feels like, oh, I don't know math, I don't have
[00:21:53.440 --> 00:21:58.560]   lots of data, or I don't have expensive computers, I don't have GPUs, something that this book
[00:21:58.560 --> 00:22:04.720]   starts with saying is that deep learning is for everyone. And when they say deep learning
[00:22:04.720 --> 00:22:13.360]   is for everyone, deep learning is also around everywhere. So deep learning has a list of
[00:22:13.360 --> 00:22:17.400]   many different tasks that deep learning that you'll see deep learning today. So if you're
[00:22:17.400 --> 00:22:23.400]   writing a Gmail or your, I mean, deep learning is everywhere now. If you're facial recognition,
[00:22:23.400 --> 00:22:29.040]   self-driving cars, using when you're typing in email, spam or not, there's all these different
[00:22:29.040 --> 00:22:34.520]   things, even in medicine, you can have like x-ray CDs. So I used to, my previous workplace
[00:22:34.520 --> 00:22:39.240]   was exactly this, like we were working on x-ray and CD scans and trying to classify
[00:22:39.240 --> 00:22:44.240]   them different things. You can even generate images. So there's examples that I will showcase
[00:22:44.240 --> 00:22:50.240]   later down as we go into the fast book that you can actually ask a deep learning model
[00:22:50.240 --> 00:22:55.880]   to then, if you say that you create an image of something of an avocado that looks like
[00:22:55.880 --> 00:23:00.800]   a chair, it is actually going to take that input and can generate that image for you.
[00:23:00.800 --> 00:23:07.240]   It can be used to play games. We've seen AlphaGo that sort of, that deep learning is used these
[00:23:07.240 --> 00:23:12.680]   days to do all of these things that weren't possible 10 years ago, but now deep learning
[00:23:12.680 --> 00:23:19.280]   is everywhere. I'm going to skip over this part, which provides a brief history about
[00:23:19.280 --> 00:23:27.840]   neural networks. It's basically about Warren McCullough. He was a neurophysiologist and
[00:23:27.840 --> 00:23:33.520]   it just shares like this story of how neural networks came about, like the name and how
[00:23:33.520 --> 00:23:38.880]   it is related to neurons, which should be okay for you guys to go back and read. It's
[00:23:38.880 --> 00:23:48.800]   not a lot. It's only like three, four paragraphs. The main thing that's important here is like
[00:23:48.800 --> 00:23:54.520]   this book was written by Sylvain and Jeremy Howard. They are going to be our guides on
[00:23:54.520 --> 00:24:00.360]   this journey. Who am I? I'm someone who's on this journey with you guys as well, but
[00:24:00.360 --> 00:24:05.680]   as someone who's spent like two or three years just spending time with the Fast AI courses.
[00:24:05.680 --> 00:24:12.840]   What we're going to do is we're going to go through this book as a group. Jeremy, I mean,
[00:24:12.840 --> 00:24:17.640]   it would be really helpful to read about all of this, about Jeremy. I think we all met
[00:24:17.640 --> 00:24:25.520]   him last week. Jeremy, he's someone who doesn't come from a PhD background, but he's someone
[00:24:25.520 --> 00:24:30.280]   who's been using and teaching machine learning for around 30 years. While he doesn't have
[00:24:30.280 --> 00:24:37.040]   a PhD, he's someone who comes from a very practical and a coding oriented mindset. He
[00:24:37.040 --> 00:24:42.200]   started using neural networks 25 years ago. The other thing, Sylvain, on the other hand,
[00:24:42.200 --> 00:24:48.040]   has a formal technical education and he's in fact written 10 math textbooks. Sylvain
[00:24:48.040 --> 00:24:53.520]   is now working at Hugging Face. Unlike Jeremy, he hasn't spent many years coding and applying
[00:24:53.520 --> 00:25:02.440]   machine learning algorithms. He only recently came to the machine learning world. You get
[00:25:02.440 --> 00:25:07.160]   a mixed batch of experience and math between Jeremy and Sylvain, and that's why this book
[00:25:07.160 --> 00:25:12.440]   has been so popular and so helpful with people starting out. Because if you're someone who
[00:25:12.440 --> 00:25:16.240]   doesn't have a PhD, who doesn't come from a math background, then it's still really
[00:25:16.240 --> 00:25:20.040]   easy for you to understand. And if you are someone who's coming from a mathematics or
[00:25:20.040 --> 00:25:25.160]   statistical background, then again, Sylvain takes care of, you'll understand a lot of
[00:25:25.160 --> 00:25:30.640]   things that where Sylvain comes from. In this way, this book is really balanced and definitely
[00:25:30.640 --> 00:25:36.160]   one of my favorite ones to get started with deep learning.
[00:25:36.160 --> 00:25:39.240]   Next thing I want to stress out is how to learn deep learning. This is something really
[00:25:39.240 --> 00:25:46.040]   crucial between us. So one thing you want to do is you want to, the way this book is
[00:25:46.040 --> 00:25:57.280]   designed is on a Harvard professor's theory by David Perkins. Basically what he says is,
[00:25:57.280 --> 00:26:02.760]   when children go and they try to learn football, you don't actually teach them what pressure
[00:26:02.760 --> 00:26:07.240]   is. You don't teach them what a parabola is. You don't teach them, oh, this is the force
[00:26:07.240 --> 00:26:10.600]   at which if you hit this football, that's when and how it's going to go. You give them
[00:26:10.600 --> 00:26:14.280]   the football and you let people play and you let children play. And that's how they learn
[00:26:14.280 --> 00:26:20.640]   how to play football. Same goes with other sports. And it's completely the opposite of
[00:26:20.640 --> 00:26:26.840]   what's been happening in the academic world is, when we, at least when I started, we would
[00:26:26.840 --> 00:26:32.600]   first thought matrices, we would first thought linear algebra, we would first thought arrays,
[00:26:32.600 --> 00:26:37.920]   we would first thought vectors. And we would first thought all these crowned, like all
[00:26:37.920 --> 00:26:44.080]   these base things that I didn't know how to apply. And the reason why I didn't do very
[00:26:44.080 --> 00:26:48.200]   well at that was because I didn't have a motivation to like, okay, how are matrices going to be
[00:26:48.200 --> 00:26:52.320]   important in my life? Like, why do I even need to learn these things? And this is how
[00:26:52.320 --> 00:26:56.760]   this book is different from a lot of other ones is the way they've designed this book
[00:26:56.760 --> 00:27:02.560]   is, you'll start with training an image classifier first. In fact, we're going to do it today.
[00:27:02.560 --> 00:27:05.680]   And then you're going to learn slowly and slowly about things like what exactly is an
[00:27:05.680 --> 00:27:12.000]   image classifier? What exactly is a deep learning model or what exactly are the other things?
[00:27:12.000 --> 00:27:18.200]   So you go, you'll be going down that football approach where we as people are going to learn
[00:27:18.200 --> 00:27:21.800]   how to play football first, and then we're going to get better and better at all these
[00:27:21.800 --> 00:27:25.320]   other things. And we're going to go deeper and deeper into the subject. So it's a very
[00:27:25.320 --> 00:27:31.200]   top-down approach. And this is one that's definitely recommended in this book is like,
[00:27:31.200 --> 00:27:36.000]   you want to do lots of experiments. You want to do lots of code. You want to try and go
[00:27:36.000 --> 00:27:40.800]   back and like first fiddle with things of, okay, this is the input to this thing. This
[00:27:40.800 --> 00:27:44.800]   is the output of this thing. So this is how we're going to try is like, we're going to
[00:27:44.800 --> 00:27:50.480]   do lots and lots of experiments. And this is the commitment of this book is like, they're
[00:27:50.480 --> 00:27:54.280]   going to teach us the whole game as in not just give us the football, but actually they're
[00:27:54.280 --> 00:27:59.080]   going to teach us the whole set. Like when I started with this book, the first thing
[00:27:59.080 --> 00:28:04.560]   you learn is, okay, you learn how to do an image classifier using fast.ai. But after
[00:28:04.560 --> 00:28:08.000]   some time you actually learn what a ResNet is. You actually learn what a batch number
[00:28:08.000 --> 00:28:11.920]   is. You actually learn all these different concepts that you wouldn't have imagined that,
[00:28:11.920 --> 00:28:16.800]   okay, by the 20th chapter, you're learning all of these things. And today I also started
[00:28:16.800 --> 00:28:22.840]   with the same fast book, lesson one. And after some time when you actually get all these
[00:28:22.840 --> 00:28:26.600]   pieces together and you go deeper and deeper into subjects, you can then pick up a research
[00:28:26.600 --> 00:28:33.320]   paper and you can still understand it very well. So that's the commitment here. And always
[00:28:33.320 --> 00:28:37.600]   teaching through examples. So examples are going to be key. It's going to be simple and
[00:28:37.600 --> 00:28:43.280]   it's going to remove barriers. Deep learning has no barriers. It's going to be, in fact,
[00:28:43.280 --> 00:28:52.200]   what we're going to see in this book is like anybody can do deep learning.
[00:28:52.200 --> 00:28:57.360]   The next thing is fast.ai. I'm going to touch on the software side of things like PyTorch,
[00:28:57.360 --> 00:29:02.960]   fast.ai and Jupyter. So PyTorch is a library. If you don't know what a library is or you're
[00:29:02.960 --> 00:29:11.120]   new to coding, a library is basically something that has a set of, like you can, it's basically
[00:29:11.120 --> 00:29:16.000]   something that's more lower level. So it has, if you want to do something like say you want
[00:29:16.000 --> 00:29:21.520]   to add two matrices together, NumPy is a library that you can call a lower level function on
[00:29:21.520 --> 00:29:27.320]   and you can then call that specific thing, that specific part in that library to do those
[00:29:27.320 --> 00:29:34.800]   things. So PyTorch, because we do expect that you have one year of coding and you're good
[00:29:34.800 --> 00:29:42.160]   at Python. So then PyTorch came out in 2017. And before actually using PyTorch as the base
[00:29:42.160 --> 00:29:48.160]   library for fast.ai, Jeremy and Silvay actually spent thousands of hours trying different
[00:29:48.160 --> 00:29:52.280]   things, experimenting with different things, and then finally making sure that they're
[00:29:52.280 --> 00:29:59.080]   using this library for fast.ai. So PyTorch is the underneath layer on top of which fast.ai
[00:29:59.080 --> 00:30:03.200]   is built. And PyTorch has since then becomes the world's fastest growing deep learning
[00:30:03.200 --> 00:30:07.440]   library and it's already used in most research papers. In fact, lots of research papers that
[00:30:07.440 --> 00:30:12.920]   are read today have already have code provided in PyTorch. So it's really easy. It's really
[00:30:12.920 --> 00:30:19.600]   helpful that if you know PyTorch and if you know fast.ai, so as we go deeper and deeper,
[00:30:19.600 --> 00:30:23.120]   not only are we going to pick up more and more fast.ai, we're actually going to also
[00:30:23.120 --> 00:30:28.480]   learn more and more about PyTorch. And that's a skill that I find really, really helpful
[00:30:28.480 --> 00:30:33.440]   because in today's world, if I'm actually reading a research paper and there's code
[00:30:33.440 --> 00:30:39.040]   in PyTorch, then all these things that I've learned over time really come in helpful. And
[00:30:39.040 --> 00:30:49.280]   this book will cover the second version of the fast.ai library.
[00:30:49.280 --> 00:30:54.520]   Something I do want to stress, like we don't want to -- like something that's important
[00:30:54.520 --> 00:30:59.240]   is the ability to learn new things. And I definitely can say that from my personal experience
[00:30:59.240 --> 00:31:03.800]   as well. PyTorch may be relevant today, but in ten years down the line, you never know
[00:31:03.800 --> 00:31:08.680]   if Jax will come up or Julia will take over. There's all these different libraries that
[00:31:08.680 --> 00:31:13.880]   keep coming over the years. And it's only going to get more and more. So people are
[00:31:13.880 --> 00:31:17.560]   only -- as more and more people are getting into deep learning, they're going to bring
[00:31:17.560 --> 00:31:20.680]   their ideas and they're going to bring their libraries and they're going to bring all these
[00:31:20.680 --> 00:31:26.480]   different ways of coding over time. So something that you should also have this
[00:31:26.480 --> 00:31:32.360]   mindset is that I don't want to commit myself to just the one library because it might be
[00:31:32.360 --> 00:31:37.840]   outdated ten years from now. But rather what I want to focus on is learning the basics.
[00:31:37.840 --> 00:31:41.920]   Like learning -- this book will teach us the basics. Like, okay, what is deep learning?
[00:31:41.920 --> 00:31:45.840]   What is a deep learning model? What is a ResNet? What is a BatchNorm? Because if you learn
[00:31:45.840 --> 00:31:50.480]   the basics, then you can apply it to any library, be it TensorFlow, be it PyTorch or any other
[00:31:50.480 --> 00:31:55.680]   library. So that's the most important thing. Another thing we want to be really good at
[00:31:55.680 --> 00:31:59.760]   is writing code and experimenting. And we're going to do lots and lots of that. Over time
[00:31:59.760 --> 00:32:03.440]   we're going to spend lots of time writing code and we're going to spend lots of time
[00:32:03.440 --> 00:32:08.280]   writing experiments. In fact, I do plan that we form groups and we form and we work on
[00:32:08.280 --> 00:32:12.640]   projects, interesting projects that could then go on your resume or, you know, just
[00:32:12.640 --> 00:32:17.960]   these experiments that we do between us as a group. Or even research. But that all comes
[00:32:17.960 --> 00:32:23.440]   later. So as I've said, it's a marathon and it's not a sprint. So we're going to take
[00:32:23.440 --> 00:32:27.680]   things slowly, but we're going to take things consistently and we're going to improve over
[00:32:27.680 --> 00:32:36.080]   time. Cool. That was it. Like, that's just the basic introduction. And now we're ready
[00:32:36.080 --> 00:32:41.960]   to start with training our first image classifier that can recognize dogs and cats with almost
[00:32:41.960 --> 00:32:47.760]   100% accuracy. So when you get to this point, the first thing is getting a deep GPU, deep
[00:32:47.760 --> 00:32:52.140]   learning server. So you should be able to run this code in your own machines. I've already
[00:32:52.140 --> 00:32:59.000]   mentioned a few in this fast AI setup, but you can do it here and also watch Jeremy's
[00:32:59.000 --> 00:33:08.400]   lesson zero. But if you have any questions, feel free to reach out. One thing I also do
[00:33:08.400 --> 00:33:21.080]   want to mention is that the fast book has two versions. So there's all of these notebooks
[00:33:21.080 --> 00:33:26.600]   over here, but there's also a clean version. The clean version is just headings and it's
[00:33:26.600 --> 00:33:32.920]   just code. So it's not there. It doesn't have any of the pros like this version does. So
[00:33:32.920 --> 00:33:37.360]   there's only these two different versions. So something that I found really helpful and
[00:33:37.360 --> 00:33:41.880]   is a really good way to learn deep learning using this book is you spend time, you go
[00:33:41.880 --> 00:33:46.400]   back, I'm giving you enough context, I'm giving you enough things for you to spend time and
[00:33:46.400 --> 00:33:50.460]   go back and read about this whole chapter. And then once you read this whole chapter
[00:33:50.460 --> 00:33:54.760]   of the book this week, if you have any questions, you keep asking those questions. But once
[00:33:54.760 --> 00:34:00.600]   you've read the whole chapter, go back here, go to the clean version, and then see if this
[00:34:00.600 --> 00:34:05.040]   code and you can actually make sense of, okay, this is why we're running this code or this
[00:34:05.040 --> 00:34:13.640]   is what's happening in this. So it makes sense if you can summarize that whole thing.
[00:34:13.640 --> 00:34:18.760]   And towards the end, something that you will find really helpful is towards the end, you
[00:34:18.760 --> 00:34:25.480]   will see this questionnaire. So this questionnaire is just 20 questions, about 15, 20 questions
[00:34:25.480 --> 00:34:30.080]   on the whole chapter that we will read. And those 15, 20 questions is like, what is a
[00:34:30.080 --> 00:34:35.440]   GPU or just basic questions that are there that are going to be there in this whole chapter.
[00:34:35.440 --> 00:34:40.040]   And if there's any question that you ever get stuck with, we can actually discuss these
[00:34:40.040 --> 00:34:44.800]   questions on these going forward like this. We can have Fastbook week one question discussion,
[00:34:44.800 --> 00:34:49.520]   we can have Fastbook week two question discussion, and we can discuss all these different questions,
[00:34:49.520 --> 00:34:56.520]   but feel free to ask me anytime. So as I said, there's two folders, one is the clean and
[00:34:56.520 --> 00:35:02.560]   one is the unclean. Jupyter Notebooks has two modes. There's an edit mode and there's
[00:35:02.560 --> 00:35:07.000]   a command mode. So if I double click on this, I'm in the edit mode because I can then type
[00:35:07.000 --> 00:35:12.160]   things. And if I press escape, I'm in the command mode. So pressing control enter, and
[00:35:12.160 --> 00:35:16.280]   then I'm pressing up and down is something I'm in the command mode. If I press escape
[00:35:16.280 --> 00:35:20.600]   and I press B, because I'm in the command mode, it will start, it will create a new
[00:35:20.600 --> 00:35:25.400]   cell. So there's like, there's these small things. Oh, I just pressed escape and DD to
[00:35:25.400 --> 00:35:33.200]   actually delete this line of Jupyter Notebook. So it will really help if you spend time just
[00:35:33.200 --> 00:35:37.160]   this week getting to know yourselves of the Jupyter Notebook environment and understanding
[00:35:37.160 --> 00:35:41.000]   like these different things of edit mode and command mode, but I won't be spending lots
[00:35:41.000 --> 00:35:46.080]   of time on this because these are things, so my focus is to spend time on things that
[00:35:46.080 --> 00:35:49.600]   you could actually be stuck at when you go back and you're reading this chapter. But
[00:35:49.600 --> 00:35:54.780]   I feel like these are things that you can learn at your own pace. So now we're getting
[00:35:54.780 --> 00:36:02.080]   into the meat of the chapter.
[00:36:02.080 --> 00:36:08.640]   So let's look at what is machine learning. Cool. So this is the key bit. This is what
[00:36:08.640 --> 00:36:13.440]   we're all here for. We're all here to learn machine learning and deep learning. Somebody
[00:36:13.440 --> 00:36:18.640]   asked like, do I need to know machine learning to be able to start with deep learning? Well,
[00:36:18.640 --> 00:36:23.480]   deep learning is just a modern area in the general discipline of machine learning. So
[00:36:23.480 --> 00:36:27.840]   machine learning is this whole broader thing. Like you have all these different, like this
[00:36:27.840 --> 00:36:32.600]   whole practice is called this machine learning and deep learning is just a modern way of
[00:36:32.600 --> 00:36:36.160]   doing these things like neural networks and all this stuff that we call deep learning
[00:36:36.160 --> 00:36:42.200]   is just actually a part and a modern way of doing things using machine learning. But in
[00:36:42.200 --> 00:36:46.520]   this section, let's see what machine learning is. Something that the book says is machine
[00:36:46.520 --> 00:36:51.440]   learning is like regular programming and it's a way to get computers to complete a specific
[00:36:51.440 --> 00:36:56.840]   task. So in your heads, I want you to think of, I don't want you to think of machine learning
[00:36:56.840 --> 00:37:01.120]   as, or it's something that's so complex. It's something that's so different from regular
[00:37:01.120 --> 00:37:06.080]   programming. It's actually not, if you think of it, for those of you that have tried machine
[00:37:06.080 --> 00:37:11.400]   learning or for those of you that are new to machine learning, it's only a set of code
[00:37:11.400 --> 00:37:15.920]   and it's only a set of instructions that you run and you complete your task. So if your
[00:37:15.920 --> 00:37:22.520]   task is prediction, you actually create a program that then learns how to do those predictions.
[00:37:22.520 --> 00:37:35.680]   It's not very different from general and regular programming. So something that this point
[00:37:35.680 --> 00:37:42.280]   forward that goes is, you know, like machine learning, if you think it's still like a little
[00:37:42.280 --> 00:37:49.000]   bit different in the sense that when you go, like when you're trying to do, say, the main
[00:37:49.000 --> 00:37:52.720]   difference that I find is like when you're trying to do, say, build a website or you're
[00:37:52.720 --> 00:37:57.400]   a web developer, or you're trying to do software testing, then something that you do is, okay,
[00:37:57.400 --> 00:38:01.760]   I'm trying to do this particular thing. Then what you do is, okay, I think of these tasks,
[00:38:01.760 --> 00:38:06.920]   like the first thing I need to do is write a test for this function that can one plus
[00:38:06.920 --> 00:38:11.280]   one equals two. The second thing that I need to do is be able to run this test. The third
[00:38:11.280 --> 00:38:15.720]   thing I need to be able to do is create another test or whatever. There's this set of steps
[00:38:15.720 --> 00:38:24.800]   that you have. In machine learning, it's not generally like that. You don't have like this
[00:38:24.800 --> 00:38:30.160]   when you're creating a model, you're actually getting to a point where you teach the model
[00:38:30.160 --> 00:38:36.680]   to learn itself. But we'll get into the details of it very quickly. There's more on this.
[00:38:36.680 --> 00:38:43.400]   It's okay if you don't get this part, but bear with me. So a deep learning or a machine
[00:38:43.400 --> 00:38:48.120]   learning or a general program in that manner can be thought of like this. So if you have
[00:38:48.120 --> 00:38:53.160]   in, say, when you want to, like a general program, if you want to go from an unsorted
[00:38:53.160 --> 00:38:59.760]   list to a sorted list, then what do you do? Your input is an unsorted list. By unsorted
[00:38:59.760 --> 00:39:05.520]   list, what I mean is something like one, nine, two, four, something like that. Like it's
[00:39:05.520 --> 00:39:11.840]   unsorted and you want to sort it to something like one, two, four, nine. Then a general
[00:39:11.840 --> 00:39:17.920]   program is nothing but you'd get some inputs, then the program does its thing and you get
[00:39:17.920 --> 00:39:25.920]   a result, which is the sorted sequence. So this is what general programming is. But Arthur
[00:39:25.920 --> 00:39:34.120]   Samuel in basically in 1949, he was a researcher at IBM. He thought, okay, maybe there's a
[00:39:34.120 --> 00:39:39.680]   different way to get computers to do things. And that different way is what is called machine
[00:39:39.680 --> 00:39:45.080]   learning. So he thought, oh, maybe there's a different way of like, I don't have to write
[00:39:45.080 --> 00:39:52.400]   the program. I don't have to define the steps for the program to actually go from an unsorted
[00:39:52.400 --> 00:39:58.400]   list to a sorted list. But actually there's a way that the program can learn on its own.
[00:39:58.400 --> 00:40:03.320]   And that's what is machine learning. So instead of telling the computer the exact steps required
[00:40:03.320 --> 00:40:08.280]   to solve a problem, show it examples of the problem to solve and let it figure out on
[00:40:08.280 --> 00:40:13.040]   itself. That's the key bit. That's the key difference. We're going to let the computer
[00:40:13.040 --> 00:40:19.960]   figure it out on its own. It's like a baby. When you teach, a baby learns from its environment.
[00:40:19.960 --> 00:40:25.840]   The baby sees a hundred thousand different examples of using a spoon to eat food. And
[00:40:25.840 --> 00:40:29.720]   then you don't actually give a set of steps to the baby. You don't say, okay, you hold
[00:40:29.720 --> 00:40:33.200]   a spoon and then you pick it up by this and then you put it in your mouth. There's no
[00:40:33.200 --> 00:40:37.480]   set of steps. It's actually like the baby learns by looking at examples from adults
[00:40:37.480 --> 00:40:45.440]   around. And this is machine learning in a way is very similar. Like it learns from examples.
[00:40:45.440 --> 00:40:48.840]   So that's the main thing. Like that's the one thing that you should be very careful
[00:40:48.840 --> 00:40:56.800]   of like when you're thinking of machine learning. One thing that he did say in his, in his,
[00:40:56.800 --> 00:41:03.040]   one thing that he did say is, here's how he described his idea of machine learning is
[00:41:03.040 --> 00:41:09.640]   like this. Suppose we arrange for some automatic means of testing the effectiveness of any
[00:41:09.640 --> 00:41:15.080]   current weight assignment in terms of actual performance and provide a mechanism for altering
[00:41:15.080 --> 00:41:19.600]   the weight assignment so as to maximize the performance. We need not go into the details
[00:41:19.600 --> 00:41:24.920]   of such a procedure to see that it could be made entirely automatic and to see that a
[00:41:24.920 --> 00:41:31.560]   machine so programmed would learn from its experience. So in this text, Arthur Samuel
[00:41:31.560 --> 00:41:37.680]   is actually trying to describe what machine learning means in his own mind. And there's
[00:41:37.680 --> 00:41:43.520]   lots of powerful concepts in just this amount of text. So what he's saying is basically
[00:41:43.520 --> 00:41:47.160]   there's a, there's an, there's something called weight assessment. Like he said, what is weight
[00:41:47.160 --> 00:41:51.440]   assessment? He's, he's mentioned something like actual performance. He's mentioned something
[00:41:51.440 --> 00:41:57.240]   like automatic means of testing. He's, he's mentioned something of a need of a mechanism.
[00:41:57.240 --> 00:42:02.520]   But what do these things actually mean? So now let's spend time and we're going to dive,
[00:42:02.520 --> 00:42:07.040]   as we say, deeper and deeper into these things of like, what exactly is weight assignment?
[00:42:07.040 --> 00:42:12.040]   What exactly does he mean by actual performance? And what he means by weight assignment is
[00:42:12.040 --> 00:42:18.320]   something like this. So, so remember how I told you, you have an, you have an input that's
[00:42:18.320 --> 00:42:23.220]   an unsorted list. It goes into the program and then it gives its results. So we had something
[00:42:23.220 --> 00:42:29.560]   like this, an input program and results. The way he summarized this is that instead of
[00:42:29.560 --> 00:42:34.920]   calling this a program, let's call it a model and let's say it has some weights. So you
[00:42:34.920 --> 00:42:40.520]   have your inputs which are the, which go into the model and the model also has some weights
[00:42:40.520 --> 00:42:45.520]   and then you get some results. So let's think of it this way. And these weights, so these,
[00:42:45.520 --> 00:42:50.260]   think of these weights as like levers or think of these weights as, think of these weights
[00:42:50.260 --> 00:42:56.120]   as basically just things that can alter the results. So if the weights are different,
[00:42:56.120 --> 00:43:00.820]   the result from the model is going to be different, even if it's the same input. So think of these
[00:43:00.820 --> 00:43:06.580]   weights as things that control the model. So we've changed the name of a box from program
[00:43:06.580 --> 00:43:11.280]   to model. This is only to follow the modern terminology. Like there's no difference in
[00:43:11.280 --> 00:43:15.360]   Arthur Samuel's way of thinking of what a program is, but we just call these things
[00:43:15.360 --> 00:43:19.840]   as deep learning models today. And that's why we've just called it model. And that's
[00:43:19.840 --> 00:43:24.560]   why we've just called it weights. We could have even called it parameters. So what's
[00:43:24.560 --> 00:43:28.480]   the next thing? So this is just what he meant by weight assignment. So that's the first
[00:43:28.480 --> 00:43:33.420]   thing. He said this idea of weight assignment, which is just this, that something has some
[00:43:33.420 --> 00:43:40.040]   weights that can then be changed to alter the results. That's a weight assignment.
[00:43:40.040 --> 00:43:44.560]   Next thing that he says is what we need is an automatic means of testing the effectiveness.
[00:43:44.560 --> 00:43:57.520]   So this can be summarized by this image, this idea of having-- it can be summarized like
[00:43:57.520 --> 00:44:05.160]   this. So right now, we saw a model has some inputs, which is here. A model has some weights.
[00:44:05.160 --> 00:44:10.280]   And based on that, it gives results. But if you think of it, what machine learning does
[00:44:10.280 --> 00:44:18.720]   today is it learns from examples. And if you have-- it's different from general programming.
[00:44:18.720 --> 00:44:25.640]   If you want to teach computers to learn by themselves, then you need to give computers
[00:44:25.640 --> 00:44:30.360]   a way to be able to update those weights. Because something I've mentioned so far is
[00:44:30.360 --> 00:44:35.600]   if the weights are different, the results are going to be different based on the inputs.
[00:44:35.600 --> 00:44:43.840]   So think of it like this. If you have an unsorted list, like 1, 9, 4, 2, and I have some weights
[00:44:43.840 --> 00:44:50.440]   W. And based on this, it gives me an output like 1, 9, 2, 4. But this is incorrect. We
[00:44:50.440 --> 00:44:55.280]   know that this is these weights, whatever these weights may be, without telling you
[00:44:55.280 --> 00:44:59.560]   what weights are. Based on these weights, the output is incorrect. So something that
[00:44:59.560 --> 00:45:05.240]   we want to do is we want to be able to alter these weights so it gets to a point where
[00:45:05.240 --> 00:45:11.560]   it goes, OK, well, now that my weights are updated, so I'm just going to call them WU,
[00:45:11.560 --> 00:45:16.440]   updated weights. And based on this, it gives me the output of 1, 2, 4, 9. So that's what
[00:45:16.440 --> 00:45:24.080]   this idea is. But how does a computer know how to update the weights? This is the idea.
[00:45:24.080 --> 00:45:28.720]   You have your inputs. You have your weights to the model. You get some results. Based
[00:45:28.720 --> 00:45:35.160]   on that, you calculate the performance. Basically, what that means is you calculate how
[00:45:35.160 --> 00:45:41.440]   right or how wrong your model is or whether it's performing based on your expectation.
[00:45:41.440 --> 00:45:46.520]   If the performance is low, then you update the weights until to a point that the performance
[00:45:46.520 --> 00:45:52.440]   is high. So that's the main idea. This is the main idea of having an automatic means
[00:45:52.440 --> 00:46:02.160]   of testing the effectiveness. So once this model-- so as I said, you have this unsorted
[00:46:02.160 --> 00:46:14.320]   list. You have some weights updated. And you have a sorted list. These weights are updated
[00:46:14.320 --> 00:46:21.160]   and my performance is high. Then I could also provide this with a different-- I could also
[00:46:21.160 --> 00:46:29.840]   provide this with a different sequence. And I could use the same weights because right
[00:46:29.840 --> 00:46:37.280]   by now, the model knows how to sort things. And it will sort a different sequence.
[00:46:37.280 --> 00:46:43.520]   So we've gone from a model that didn't know how to sort things to a model that now does
[00:46:43.520 --> 00:46:48.040]   know how to sort things. And this is what is called that the model is trained. So when
[00:46:48.040 --> 00:46:52.680]   we say that I have a trained model, that just basically means that it knows how to do the
[00:46:52.680 --> 00:46:58.160]   task that I'm trying to teach the model to do the task. And this is what it means that
[00:46:58.160 --> 00:47:02.600]   it's my favorite weight assignment. Basically, these weights are the perfect weights for
[00:47:02.600 --> 00:47:09.720]   me to be able to solve this task. And that's why this weight assignment is good.
[00:47:09.720 --> 00:47:13.960]   So by the end, when you have this model, it just behaves like this. You can provide it
[00:47:13.960 --> 00:47:19.040]   with any inputs like I have, 2953. You have any model. And then it will give you these
[00:47:19.040 --> 00:47:25.940]   results. So by this time, a trained model is just like a computer program. We started
[00:47:25.940 --> 00:47:30.420]   with the definition of computer program right at the top, that a computer program is a set
[00:47:30.420 --> 00:47:37.920]   of steps that when you give it an input, it runs that program and gives you these results.
[00:47:37.920 --> 00:47:41.840]   This is what a trained deep learning model looks like. Once it's trained, you give it
[00:47:41.840 --> 00:47:47.080]   some inputs. The model does its thing, and you get some results. So this is an important
[00:47:47.080 --> 00:47:53.080]   insight. A trained model can be treated just like a regular computer program.
[00:47:53.080 --> 00:47:59.380]   So that's what, generally, machine learning is. That's what, generally, you mean by--
[00:47:59.380 --> 00:48:04.340]   I mean, this is just all very generic. And we haven't really gone into the details of
[00:48:04.340 --> 00:48:08.780]   it yet. But this is what generally means-- this is what generally the concept of performance
[00:48:08.780 --> 00:48:12.900]   means. This is what generally the concept of model training means. What model training
[00:48:12.900 --> 00:48:17.460]   means is we're updating the weights of the model to the point that it can actually do
[00:48:17.460 --> 00:48:21.440]   the task that we want it to do.
[00:48:21.440 --> 00:48:25.620]   Now what we're going to do is we're going to spend time in looking at what is a neural
[00:48:25.620 --> 00:48:38.540]   network. We're going to go into the deep learning jargon. And finally, what I did was-- I missed
[00:48:38.540 --> 00:48:46.220]   this bit. This is where we train our first deep learning model. So all I do, I do this.
[00:48:46.220 --> 00:48:58.700]   I just press Shift-Enter. And there we go. The model is training. So it looks a lot like--
[00:48:58.700 --> 00:49:05.260]   it looks a lot like this. That thing that we're doing at the top, that looks a lot like
[00:49:05.260 --> 00:49:11.740]   this. We're giving the model-- the model has some weights. We're giving the model some
[00:49:11.740 --> 00:49:18.580]   inputs. In turn, the model gives us some results. We calculate the performance of that model.
[00:49:18.580 --> 00:49:23.300]   And we update the model weights. We keep doing this over and over and over and over and over
[00:49:23.300 --> 00:49:28.620]   again until the point that we get our favorite weight assignment. And we're happy with the
[00:49:28.620 --> 00:49:32.580]   weights. And then we know that this model is able to do the thing that we want it to
[00:49:32.580 --> 00:49:35.460]   do.
[00:49:35.460 --> 00:49:41.380]   So let's have a look. So that's the first thing, right? The error rate is something--
[00:49:41.380 --> 00:49:48.340]   it's like the-- that's how we calculate the performance. So let me actually quickly take
[00:49:48.340 --> 00:50:00.340]   this and paste this here. So just so you guys understand exactly some idea of what it means
[00:50:00.340 --> 00:50:05.100]   by model training and you guys understand some idea of what we're doing, we're following
[00:50:05.100 --> 00:50:10.020]   this exact same process. We have a model, which is-- we just created this model. It's
[00:50:10.020 --> 00:50:15.660]   called a learner in this case. So a model is just a learner. And we say, OK, model,
[00:50:15.660 --> 00:50:21.340]   you start to train. It's seeing some inputs. These inputs are coming from here. They're
[00:50:21.340 --> 00:50:27.820]   just pet images. And this model has some weights. It gives us some results. By results, I mean
[00:50:27.820 --> 00:50:34.220]   it's giving us an error rate of 0.009%. So it's actually really good. And we're checking
[00:50:34.220 --> 00:50:38.220]   the performance of it. And we're updating the weights. So the first time, it had an
[00:50:38.220 --> 00:50:41.780]   error rate of this. But because we're doing this task over and over again and we're updating
[00:50:41.780 --> 00:50:46.220]   the weights to the point that its performance gets better, the second time, we have an error
[00:50:46.220 --> 00:50:54.780]   rate of 0.004. So that's the main things. Let me quickly check if there's been lots
[00:50:54.780 --> 00:51:18.340]   of questions. How would I handle inputs with variable lengths? I think that's a good question.
[00:51:18.340 --> 00:51:24.180]   But you can still pass inputs to the same model and it will still do things. This sorting
[00:51:24.180 --> 00:51:30.300]   of a list was just an example.
[00:51:30.300 --> 00:51:41.700]   Cool. Let's get into this idea of-- you would have heard this word-- I'm just conscious
[00:51:41.700 --> 00:51:46.460]   of time because I said 60 minutes and we're already at 51. We just trained the first model
[00:51:46.460 --> 00:51:51.940]   and we haven't really gotten to the point where I thought we could. But let me just
[00:51:51.940 --> 00:51:58.420]   quickly spend some time on going forward with this chapter. And then after that, I'll answer
[00:51:58.420 --> 00:52:00.660]   all the questions.
[00:52:00.660 --> 00:52:06.420]   So what is a neural network? This is a word that most of you, if not all of you, would
[00:52:06.420 --> 00:52:11.820]   have heard in your lifetime is a neural network. And then what exactly is it? How do I even
[00:52:11.820 --> 00:52:20.460]   define something like a neural network? Well, a neural network is nothing but a function.
[00:52:20.460 --> 00:52:26.180]   We can define it mathematically as a function that can be used to solve any given problem.
[00:52:26.180 --> 00:52:32.020]   So right now, what I mean by that is if neural network is a function that can be used to
[00:52:32.020 --> 00:52:38.620]   solve any problem, the neural network can be used to solve the sorting of the list problem.
[00:52:38.620 --> 00:52:44.260]   A neural network can be used to classify pets and images. It can be used to classify dogs
[00:52:44.260 --> 00:52:49.900]   from cats. A neural network can be used to predict the next words of a sentence. A neural
[00:52:49.900 --> 00:52:55.060]   network can be used to classify x-ray images. A neural network can be used to classify CT
[00:52:55.060 --> 00:52:58.620]   scans. Or it can be used to solve pretty much any problem that you think.
[00:52:58.620 --> 00:53:05.740]   So a neural network, what is it? It's just a mathematical function that is capable of
[00:53:05.740 --> 00:53:12.100]   solving any problem. And this is called something like the uniform-- I think it was called uniform
[00:53:12.100 --> 00:53:18.700]   solution, something like that. But what do we need? So we've looked at this. Like this
[00:53:18.700 --> 00:53:23.060]   model, if it's a neural network, then the one thing that we need is we need this way
[00:53:23.060 --> 00:53:29.500]   to be able to update the weights of this model. And that's something-- one could imagine that
[00:53:29.500 --> 00:53:33.940]   we need a new way or find a mechanism for automatically updating the weights of every
[00:53:33.940 --> 00:53:34.940]   problem.
[00:53:34.940 --> 00:53:42.820]   So the question is, if a neural network is something that can be used to solve any problem,
[00:53:42.820 --> 00:53:49.940]   then something you might ask, OK, if we're trying to do image classification, then do
[00:53:49.940 --> 00:53:55.380]   I need to update the weights of my model? Is it a different algorithm? Is it a different
[00:53:55.380 --> 00:54:01.140]   way to update the weights of my model? Or if I'm trying to do CT scans, is that a different
[00:54:01.140 --> 00:54:06.260]   way of updating the models? Or if I'm trying to do text classification, then is that a
[00:54:06.260 --> 00:54:09.980]   different way of trying to update my model?
[00:54:09.980 --> 00:54:15.860]   Obviously, the answer is no. So what we need by now, if everything was a different way,
[00:54:15.860 --> 00:54:21.020]   then what we need is we need one way. We need a conventional-- we need a generic way that
[00:54:21.020 --> 00:54:26.420]   we can update the weights of the model. And that generic thing is called stochastic gradient
[00:54:26.420 --> 00:54:28.460]   descent.
[00:54:28.460 --> 00:54:41.540]   So this idea of being able to update the weights-- so this model, if it's a neural network,
[00:54:41.540 --> 00:54:47.180]   it can be used for any task. But this update, this update point of being able to update
[00:54:47.180 --> 00:54:52.820]   the weights no matter what the task, and being able to still achieve good performance, this
[00:54:52.820 --> 00:54:56.940]   idea of being able to update the weights of this model is called a stochastic gradient
[00:54:56.940 --> 00:54:58.140]   descent.
[00:54:58.140 --> 00:55:04.940]   So right now, the first chapter-- in the first chapter, we pretty much have spent much time
[00:55:04.940 --> 00:55:10.780]   just understanding just these basic concepts of what is a deep learning model, or what
[00:55:10.780 --> 00:55:15.660]   is machine learning, or what is performance, or what is stochastic gradient descent. So
[00:55:15.660 --> 00:55:21.780]   this is just laying the groundwork of what's coming in the next weeks.
[00:55:21.780 --> 00:55:28.540]   I think we're out of time. It's just five minutes left. I will stop here today. But
[00:55:28.540 --> 00:55:38.740]   what you will do in going back is you'll take this-- of what we've learned so far, 1.6.7--
[00:55:38.740 --> 00:55:45.260]   go back, make sure that you have a GPU running, and make sure that you read every point of
[00:55:45.260 --> 00:55:52.460]   line of the book to this point. And we're going to pick up things from here after that.
[00:55:52.460 --> 00:55:57.140]   One thing I do want to-- I've already said that deep learning models are not just for
[00:55:57.140 --> 00:56:03.820]   image classification. The one example that we saw of that code was actually doing pets.
[00:56:03.820 --> 00:56:08.700]   But actually, deep learning can be used for a lot of other things. So it can actually
[00:56:08.700 --> 00:56:14.840]   be used for something like segmentation. And what segmentation means is it looks like this.
[00:56:14.840 --> 00:56:20.480]   When you're doing self-driving cars, when you're driving self-driving cars, then if
[00:56:20.480 --> 00:56:25.700]   you see an environment, then you need to be able to classify a bicycle from the road.
[00:56:25.700 --> 00:56:29.700]   And these are all these things that we're going to be learning over time. It can also
[00:56:29.700 --> 00:56:41.740]   do things like text classification. It can also do other things like-- it can also do
[00:56:41.740 --> 00:56:46.860]   other things like audio classification. It can also do things like time series analysis.
[00:56:46.860 --> 00:56:51.160]   It can also do things like fraud detection. So there's all these different tasks that
[00:56:51.160 --> 00:56:55.260]   deep learning can do. And over time, we're going to be spending time learning all of
[00:56:55.260 --> 00:57:01.620]   this. But go back and spend some time just reading so far. And let me just go have a
[00:57:01.620 --> 00:57:17.860]   look at the questions that we have. I must say one thing, though. I thought that we will
[00:57:17.860 --> 00:57:24.500]   be able to make more progress than imagined. But something I've realized is it's going
[00:57:24.500 --> 00:57:29.100]   to take more time. So I think this is open for discussion. And this is something let's
[00:57:29.100 --> 00:57:36.780]   discuss if you guys have time. Let's see if we can increase the time from 60 minutes to
[00:57:36.780 --> 00:57:42.300]   90 minutes. And this is an open question. But feel free to reply in the chat. Or let
[00:57:42.300 --> 00:58:07.260]   me just put this comment here. And you can feel free to reply with a yes or no, if that's
[00:58:07.260 --> 00:58:11.940]   something that will suit you.
[00:58:11.940 --> 00:58:15.780]   So the difference between several use cases of universal-- oh, that's what it's called.
[00:58:15.780 --> 00:58:21.020]   Yeah, universal approximation theorem is the SGD. So the difference between several use
[00:58:21.020 --> 00:58:27.140]   cases-- that's correct, pretty much. We'll go deeper into this. I haven't been able to
[00:58:27.140 --> 00:58:32.580]   touch and cover as much ground as I wanted to. But we will touch base into exactly what
[00:58:32.580 --> 00:58:37.980]   universal approximation function is and go deeper into what SGD does. Although I've read
[00:58:37.980 --> 00:58:43.420]   SGD and backpropagation, I still don't understand how they are related. OK, that's OK. This
[00:58:43.420 --> 00:58:52.020]   is one thing I can quickly look at and help you with.
[00:58:52.020 --> 00:58:57.500]   So backpropagation basically means-- and this is something that's OK for anybody else if
[00:58:57.500 --> 00:59:05.660]   you don't understand. But backpropagation basically means that you start with your input,
[00:59:05.660 --> 00:59:10.180]   you have your model, and you get your results. This, of going in this direction, is called
[00:59:10.180 --> 00:59:15.900]   a forward pass. Then you calculate the performance. And then you go back and you update the weights.
[00:59:15.900 --> 00:59:23.700]   So this process of going back into the model and updating the weights is called backpropagation.
[00:59:23.700 --> 00:59:28.820]   And the theorem, or the thing that's responsible for updating the weights in the correct manner
[00:59:28.820 --> 00:59:34.460]   that your performance goes up, is stochastic gradient descent. So stochastic gradient descent
[00:59:34.460 --> 00:59:39.940]   is the method by which you update the weights of the model. But the process of updating
[00:59:39.940 --> 00:59:45.740]   the weights of the model is called backpropagation.
[00:59:45.740 --> 00:59:51.620]   If anyone needs any help with setup, please let me know. Thanks, Sam. Why do we do fine
[00:59:51.620 --> 00:59:55.780]   tune? OK, this is something I did want to cover. I didn't get enough chance. So let's
[00:59:55.780 --> 01:00:03.700]   leave this as something that we cover next week or over the week. I would also appreciate
[01:00:03.700 --> 01:00:09.220]   if it started one hour later. Oh, everybody's saying yes. Awesome. Cool. So let's do this.
[01:00:09.220 --> 01:00:15.740]   So let's increase-- next week, let's go and make it 90 minutes. Oh, now I'm getting greedy.
[01:00:15.740 --> 01:00:23.300]   So how about 120 minutes? This is the question. I'll be investing lots of my time. But would
[01:00:23.300 --> 01:00:46.900]   120 minutes be a lot of time?
[01:00:46.900 --> 01:01:09.860]   OK, a lot of people are saying 90 minutes. So that's fine. We'll stick with 90 minutes.
[01:01:09.860 --> 01:01:14.620]   But go back today. Your homework for the time being and what you have to do over this week
[01:01:14.620 --> 01:01:24.260]   is go back and make sure there's a Fastbook. If you search Fastbook, it opens up this thing
[01:01:24.260 --> 01:01:30.340]   called the GitHub repository. Make sure that you go to these setup instructions, which
[01:01:30.340 --> 01:01:34.660]   I've mentioned here. I've already shared this with the group, but I'll be sending a message
[01:01:34.660 --> 01:01:40.540]   on the Slack. So one thing you do want to do is come here and join this Weights and
[01:01:40.540 --> 01:01:47.060]   Biases Slack. So basically, join here, and I'll be sharing the set of steps and everything
[01:01:47.060 --> 01:01:51.380]   that you have to do for this whole week. And we'll be discussing lots of things over this
[01:01:51.380 --> 01:01:56.900]   one week. But make sure that-- the one thing I really want to stress upon is make sure
[01:01:56.900 --> 01:02:00.740]   that you go back and you have your system running. Because every week, this is going
[01:02:00.740 --> 01:02:07.700]   to become increasingly difficult. And we're going to run more and more code. So we want
[01:02:07.700 --> 01:02:12.100]   to be set up and we want to be ready. And then once you have your setup ready-- so have
[01:02:12.100 --> 01:02:17.260]   a look at this video from Jeremy Howard, which is the introduction and shares how to do setup.
[01:02:17.260 --> 01:02:21.700]   So that's the first step. The second step is then you go and you actually try and run
[01:02:21.700 --> 01:02:28.300]   this notebook and you actually read this notebook to the point of 1.6.7 is where we left. So
[01:02:28.300 --> 01:02:32.940]   next week when we come back, I will come back and we will look at this and we will read
[01:02:32.940 --> 01:02:42.820]   from 1.6.7 from here on. And if you have any questions, head over to this point, 1db.me/fc,
[01:02:42.820 --> 01:02:48.100]   and you go to forums. You could also start a discussion here if you have anything that's
[01:02:48.100 --> 01:02:52.420]   not already mentioned. You could start a discussion and you could just say, hey, I have a new
[01:02:52.420 --> 01:02:59.460]   question. And you could ask a question here and then we as a group will try and find this.
[01:02:59.460 --> 01:03:05.140]   Cool. With that being said, thanks, everybody. Please-- I'm going to be still around for
[01:03:05.140 --> 01:03:11.060]   questions on Slack and with my forums or even forums at Fast.ai. But please, please do ask
[01:03:11.060 --> 01:03:12.860]   your questions. Thanks for coming.
[01:03:12.860 --> 01:03:18.860]   [END]
[01:03:18.860 --> 01:03:20.920]   you

