
[00:00:00.000 --> 00:00:01.500]   That's not even a question for me,
[00:00:01.500 --> 00:00:02.960]   whether we're going to go take a swing
[00:00:02.960 --> 00:00:04.160]   at building the next thing.
[00:00:04.160 --> 00:00:06.720]   I'm just incapable of not doing that.
[00:00:06.720 --> 00:00:09.920]   There's a bunch of times when we wanted to launch features,
[00:00:09.920 --> 00:00:12.640]   and then Apple's just like, "Nope, you're not launching that."
[00:00:12.640 --> 00:00:14.220]   I was like, "That sucks."
[00:00:14.220 --> 00:00:16.840]   Are we set up for that with AI,
[00:00:16.840 --> 00:00:19.060]   where you're going to get a handful of companies
[00:00:19.060 --> 00:00:20.400]   that run these closed models
[00:00:20.400 --> 00:00:22.320]   that are going to be in control of the APIs,
[00:00:22.320 --> 00:00:23.440]   and therefore are going to be able to tell you
[00:00:23.440 --> 00:00:24.600]   what you can build?
[00:00:24.600 --> 00:00:27.780]   Then when you start getting into building a data center
[00:00:27.780 --> 00:00:32.040]   that's like 300 megawatts or 500 megawatts or a gigawatt,
[00:00:32.040 --> 00:00:34.580]   just no one has built a single gigawatt data center yet.
[00:00:34.580 --> 00:00:35.420]   From wherever you sit,
[00:00:35.420 --> 00:00:37.340]   there's going to be some actor who you don't trust.
[00:00:37.340 --> 00:00:39.720]   If they're the ones who have the super strong AI,
[00:00:39.720 --> 00:00:44.000]   I think that that's potentially a much bigger risk.
[00:00:44.000 --> 00:00:45.380]   - Mark, welcome to the podcast.
[00:00:45.380 --> 00:00:46.340]   - Hey, thanks for having me.
[00:00:46.340 --> 00:00:47.380]   Big fan of your podcast.
[00:00:47.380 --> 00:00:48.220]   - Oh, thank you.
[00:00:48.220 --> 00:00:50.060]   That's very nice of you to say.
[00:00:50.060 --> 00:00:52.900]   Okay, so let's start by talking about the releases
[00:00:52.900 --> 00:00:56.020]   that will go out when this interview goes out.
[00:00:56.020 --> 00:00:57.060]   Tell me about the models.
[00:00:57.060 --> 00:00:58.020]   Tell me about Meta AI.
[00:00:58.020 --> 00:00:59.480]   What's new, what's exciting about them?
[00:00:59.480 --> 00:01:00.320]   - Yeah, sure.
[00:01:00.320 --> 00:01:02.700]   So, I think the main thing that most people in the world
[00:01:02.700 --> 00:01:04.780]   are going to see is the new version of Meta AI, right?
[00:01:04.780 --> 00:01:07.620]   So it's, and the most important thing
[00:01:07.620 --> 00:01:10.540]   about what we're doing is the upgrade to the model.
[00:01:10.540 --> 00:01:11.700]   We're rolling out LLAMA 3.
[00:01:11.700 --> 00:01:14.960]   We're doing it both as open source for the dev community,
[00:01:14.960 --> 00:01:17.840]   and it is now going to be powering Meta AI.
[00:01:17.840 --> 00:01:20.220]   So, there's a lot that I'm sure we'll go into
[00:01:20.220 --> 00:01:22.760]   around LLAMA 3, but I think the bottom line on this
[00:01:22.760 --> 00:01:24.260]   is that with LLAMA 3,
[00:01:24.300 --> 00:01:27.500]   we now think that Meta AI is the most intelligent
[00:01:27.500 --> 00:01:30.920]   AI assistant that people can use that's freely available.
[00:01:30.920 --> 00:01:32.720]   We're also integrating Google and Bing
[00:01:32.720 --> 00:01:34.680]   for real-time knowledge.
[00:01:34.680 --> 00:01:37.300]   We're going to make it a lot more prominent across our apps.
[00:01:37.300 --> 00:01:41.100]   So, basically, at the top of WhatsApp and Instagram
[00:01:41.100 --> 00:01:42.980]   and Facebook and Messenger,
[00:01:42.980 --> 00:01:46.180]   you'll just be able to use the search box right there
[00:01:46.180 --> 00:01:47.980]   to ask an any question.
[00:01:47.980 --> 00:01:49.940]   And there's a bunch of new creation features
[00:01:49.940 --> 00:01:51.860]   that we added that I think are pretty cool,
[00:01:51.860 --> 00:01:53.160]   that I think people enjoy.
[00:01:54.140 --> 00:01:57.020]   And I think animations is a good one.
[00:01:57.020 --> 00:01:59.100]   You can basically just take any image and animate it.
[00:01:59.100 --> 00:02:02.900]   But I think one that people are going to find pretty wild
[00:02:02.900 --> 00:02:07.440]   is it now generates high-quality images so quickly.
[00:02:07.440 --> 00:02:09.120]   I don't know if you've gotten a chance to play with this,
[00:02:09.120 --> 00:02:11.500]   that it actually generates it as you're typing
[00:02:11.500 --> 00:02:12.660]   and updates it in real time.
[00:02:12.660 --> 00:02:13.880]   So, you're like typing your query
[00:02:13.880 --> 00:02:16.620]   and it's kind of like honing in on,
[00:02:16.620 --> 00:02:21.260]   and it's like, okay, here, show me a picture of a cow,
[00:02:21.260 --> 00:02:23.820]   okay, in a field with mountains in the background.
[00:02:23.820 --> 00:02:24.660]   It's just like everything's popular.
[00:02:24.660 --> 00:02:25.480]   - Eating macadamia nuts.
[00:02:25.480 --> 00:02:26.740]   - Yeah, eating macadamia nuts, drinking beer,
[00:02:26.740 --> 00:02:30.900]   and it's just like it's updating the image in real time.
[00:02:30.900 --> 00:02:31.740]   It's pretty wild.
[00:02:31.740 --> 00:02:34.000]   I think people are going to enjoy that.
[00:02:34.000 --> 00:02:34.840]   So, yeah.
[00:02:34.840 --> 00:02:35.660]   So, that I think is,
[00:02:35.660 --> 00:02:37.300]   that's what most people are going to see in the world, right?
[00:02:37.300 --> 00:02:39.540]   We're rolling that out, you know, not everywhere,
[00:02:39.540 --> 00:02:41.900]   but we're starting in a handful of countries,
[00:02:41.900 --> 00:02:44.460]   and we'll do more over the coming weeks and months.
[00:02:44.460 --> 00:02:48.140]   So, that I think is going to be a pretty big deal.
[00:02:49.100 --> 00:02:51.340]   And I'm really excited to get that in people's hands.
[00:02:51.340 --> 00:02:53.940]   It's a big step forward for Meta AI.
[00:02:53.940 --> 00:02:56.540]   But I think, you know,
[00:02:56.540 --> 00:02:58.660]   if you want to get under the hood a bit,
[00:02:58.660 --> 00:03:00.340]   the Llama 3 stuff is obviously
[00:03:00.340 --> 00:03:01.580]   the most technically interesting.
[00:03:01.580 --> 00:03:03.620]   So, you know, we're basically,
[00:03:03.620 --> 00:03:06.840]   for the first version, we're training three versions,
[00:03:06.840 --> 00:03:08.940]   you know, an 8 billion and a 70 billion,
[00:03:08.940 --> 00:03:10.500]   which we're releasing today,
[00:03:10.500 --> 00:03:14.620]   and a 405 billion dense model, which is still training.
[00:03:14.620 --> 00:03:16.440]   So, we're not releasing that today.
[00:03:17.620 --> 00:03:19.820]   But, you know, the 8 and 70,
[00:03:19.820 --> 00:03:23.260]   I mean, I'm pretty excited about how they turned out.
[00:03:23.260 --> 00:03:27.300]   I mean, it's, you know, they're leading for their scale.
[00:03:27.300 --> 00:03:30.180]   You know, it's, I mean,
[00:03:30.180 --> 00:03:33.060]   we'll release a blog post with all the benchmarks
[00:03:33.060 --> 00:03:34.220]   so people can check it out themselves.
[00:03:34.220 --> 00:03:35.340]   And obviously it's open source,
[00:03:35.340 --> 00:03:37.980]   so people get a chance to play with it.
[00:03:37.980 --> 00:03:41.180]   We have a roadmap of new releases coming
[00:03:41.180 --> 00:03:43.300]   that are going to bring multi-modality,
[00:03:43.300 --> 00:03:45.860]   more multi-linguality,
[00:03:45.860 --> 00:03:48.340]   bigger context windows to those as well.
[00:03:48.340 --> 00:03:51.660]   And then, you know, hopefully sometime later in the year,
[00:03:51.660 --> 00:03:53.620]   we'll get to roll out the 405,
[00:03:53.620 --> 00:03:56.500]   which I think is, you know, in training,
[00:03:56.500 --> 00:03:57.540]   it's still training,
[00:03:57.540 --> 00:04:01.180]   but for where it is right now in training,
[00:04:01.180 --> 00:04:05.980]   it is already at around 85 MMLU.
[00:04:05.980 --> 00:04:10.780]   And just, we expect that it's gonna have leading benchmarks
[00:04:10.780 --> 00:04:12.980]   on a bunch of the benchmarks.
[00:04:12.980 --> 00:04:14.740]   So, I'm pretty excited about all of that.
[00:04:14.740 --> 00:04:18.620]   I mean, the 70 billion is great too.
[00:04:18.620 --> 00:04:19.580]   I mean, we're releasing that today.
[00:04:19.580 --> 00:04:20.940]   It's around 82 MMLU
[00:04:20.940 --> 00:04:23.300]   and has leading scores on math and reasoning.
[00:04:23.300 --> 00:04:24.140]   So, I mean, it's,
[00:04:24.140 --> 00:04:25.380]   I think just getting this in people's hands
[00:04:25.380 --> 00:04:26.660]   is gonna be pretty wild.
[00:04:26.660 --> 00:04:27.500]   - Oh, interesting.
[00:04:27.500 --> 00:04:28.420]   Yeah, that's the first I'm hearing this benchmark.
[00:04:28.420 --> 00:04:29.260]   That's super impressive.
[00:04:29.260 --> 00:04:34.260]   - Yeah, and the 8 billion is nearly as powerful
[00:04:34.260 --> 00:04:38.340]   as the biggest version of LLAMA 2 that we released.
[00:04:38.340 --> 00:04:39.940]   So, it's like the smallest LLAMA 3
[00:04:39.940 --> 00:04:43.500]   is basically as powerful as the biggest LLAMA 2.
[00:04:43.500 --> 00:04:45.820]   - Okay, so before we dig into these models,
[00:04:45.820 --> 00:04:47.900]   I actually wanna go back in time.
[00:04:47.900 --> 00:04:50.700]   2022 is I'm assuming when you started acquiring
[00:04:50.700 --> 00:04:54.700]   these H100s or you can tell me when.
[00:04:54.700 --> 00:04:56.500]   Where you're like stock price is getting hammered.
[00:04:56.500 --> 00:04:58.460]   People are like, what's happening with all this CapEx?
[00:04:58.460 --> 00:05:00.100]   People aren't buying the metaverse.
[00:05:00.100 --> 00:05:01.460]   And presumably you're spending that CapEx
[00:05:01.460 --> 00:05:03.140]   to get these H100s.
[00:05:03.140 --> 00:05:05.140]   Back then, how did you know to get the H100s?
[00:05:05.140 --> 00:05:07.900]   How did you know we'll need the GPUs?
[00:05:07.900 --> 00:05:10.140]   - I think it was because we were working on reels.
[00:05:10.140 --> 00:05:15.140]   So, we got into this situation where we always wanna have
[00:05:15.140 --> 00:05:21.340]   enough capacity to build something that we can't quite see
[00:05:21.340 --> 00:05:24.260]   that we're on the horizon yet.
[00:05:24.260 --> 00:05:26.740]   And we got into this position with reels
[00:05:26.740 --> 00:05:30.980]   where we needed more GPUs to train the models, right?
[00:05:30.980 --> 00:05:34.100]   It was this big evolution for our services
[00:05:34.100 --> 00:05:35.500]   where instead of just ranking content
[00:05:35.500 --> 00:05:37.700]   from people who you follow or your friends
[00:05:37.700 --> 00:05:39.620]   and whatever pages you follow,
[00:05:39.620 --> 00:05:45.740]   we made this big push to basically start recommending
[00:05:45.740 --> 00:05:47.500]   what we call unconnected content.
[00:05:47.500 --> 00:05:50.260]   Basically, content from people or pages
[00:05:50.260 --> 00:05:51.100]   that you're not following.
[00:05:51.100 --> 00:05:55.940]   So now, the corpus of content candidates
[00:05:55.940 --> 00:05:57.580]   that we could potentially show you expanded
[00:05:57.580 --> 00:05:59.860]   from on the order of thousands
[00:05:59.860 --> 00:06:02.580]   to on the order of hundreds of millions.
[00:06:02.580 --> 00:06:05.020]   So, completely different infrastructure.
[00:06:05.020 --> 00:06:08.860]   And we started working on doing that
[00:06:08.860 --> 00:06:13.100]   and we were constrained on basically the infrastructure
[00:06:13.100 --> 00:06:15.700]   that we had to catch up to what TikTok was doing
[00:06:15.700 --> 00:06:17.900]   as quickly as we would have wanted to.
[00:06:17.900 --> 00:06:19.020]   So, I basically looked at that and I was like,
[00:06:19.020 --> 00:06:20.860]   "Hey, we have to make sure
[00:06:20.860 --> 00:06:22.580]   "that we're never in this situation again.
[00:06:22.580 --> 00:06:26.380]   "So, let's order enough GPUs to do what we need to do
[00:06:26.380 --> 00:06:28.780]   "on reels and ranking content and feed.
[00:06:28.780 --> 00:06:30.620]   "But let's also, let's double that, right?"
[00:06:30.620 --> 00:06:32.940]   'Cause again, like our normal principle is
[00:06:32.940 --> 00:06:34.380]   there's gonna be something on the horizon
[00:06:34.380 --> 00:06:35.220]   that we can't see yet.
[00:06:35.220 --> 00:06:36.620]   - Did you know it would be AI?
[00:06:36.620 --> 00:06:40.460]   - Well, we thought it was gonna be something
[00:06:40.460 --> 00:06:42.500]   that had to do with training large models, right?
[00:06:42.500 --> 00:06:44.100]   I mean, but at the time, I thought it was probably
[00:06:44.100 --> 00:06:46.180]   gonna be more something that had to do with content.
[00:06:46.180 --> 00:06:47.020]   But I don't know.
[00:06:47.020 --> 00:06:49.620]   I mean, it's almost just the pattern matching
[00:06:49.620 --> 00:06:53.620]   and running the company is there's always another thing,
[00:06:53.620 --> 00:06:54.460]   right?
[00:06:54.460 --> 00:06:56.060]   So, I'm not even sure I had, at that time,
[00:06:56.060 --> 00:06:59.660]   I was so deep and just trying to get the recommendations
[00:06:59.660 --> 00:07:02.460]   working for reels and other content.
[00:07:02.460 --> 00:07:04.060]   'Cause I mean, that's just such a big unlock
[00:07:04.060 --> 00:07:06.540]   for Instagram and Facebook to now being able to show people
[00:07:06.540 --> 00:07:08.420]   content that's interesting to them that they're from people
[00:07:08.420 --> 00:07:09.860]   that they're not even following.
[00:07:09.860 --> 00:07:14.860]   But yeah, that ended up being a very good decision
[00:07:14.860 --> 00:07:16.260]   in retrospect.
[00:07:16.260 --> 00:07:17.100]   - Yeah, yeah.
[00:07:17.100 --> 00:07:18.460]   - Okay, and it came from being behind.
[00:07:18.460 --> 00:07:20.700]   So, then it wasn't like I was, you know,
[00:07:20.700 --> 00:07:22.260]   it wasn't like, oh, I was so far ahead.
[00:07:22.260 --> 00:07:24.380]   Actually, most of the times I think where we
[00:07:24.380 --> 00:07:27.460]   kind of make some decision that ends up seeming good
[00:07:27.460 --> 00:07:29.380]   is because we messed something up before
[00:07:29.380 --> 00:07:31.460]   and just didn't wanna repeat the mistake.
[00:07:31.460 --> 00:07:32.300]   - This is a total detour,
[00:07:32.300 --> 00:07:33.980]   but I actually wanna ask about this while we're on this.
[00:07:33.980 --> 00:07:36.540]   We'll get back to AI in a second.
[00:07:36.540 --> 00:07:38.420]   So, you didn't suffer one billion,
[00:07:38.420 --> 00:07:39.660]   but presumably there's some amount
[00:07:39.660 --> 00:07:40.900]   you would have sold for, right?
[00:07:40.900 --> 00:07:42.580]   Did you write down in your head, like,
[00:07:42.580 --> 00:07:45.220]   I think the actual valuation of Facebook at the time is this
[00:07:45.220 --> 00:07:47.060]   and they're not actually getting the valuation right?
[00:07:47.060 --> 00:07:48.300]   I mean, they're already $5 trillion.
[00:07:48.300 --> 00:07:49.100]   Of course you would have sold.
[00:07:49.100 --> 00:07:52.860]   So, how did you think about that choice?
[00:07:52.860 --> 00:07:53.700]   - Yeah, I don't know.
[00:07:53.700 --> 00:07:55.260]   I mean, look, I think some of these things
[00:07:55.260 --> 00:07:56.500]   are just personal.
[00:07:56.500 --> 00:08:01.140]   I don't know at the time that I was sophisticated enough
[00:08:01.140 --> 00:08:02.260]   to do that analysis,
[00:08:02.260 --> 00:08:04.100]   but I had all these people around me
[00:08:04.100 --> 00:08:06.260]   who were making all these arguments
[00:08:06.260 --> 00:08:09.380]   for how like a billion dollars was,
[00:08:09.380 --> 00:08:11.340]   you know, it's like, here's the revenue that we need to make
[00:08:11.340 --> 00:08:12.740]   and here's how big we need to be.
[00:08:12.740 --> 00:08:14.740]   And like, it's clearly so many years in the future.
[00:08:14.740 --> 00:08:18.180]   Like, it was very far ahead of where we were at the time.
[00:08:18.180 --> 00:08:21.420]   And I don't know, I didn't really
[00:08:21.420 --> 00:08:23.140]   have the financial sophistication
[00:08:23.140 --> 00:08:26.300]   to really even engage with that kind of debate.
[00:08:26.300 --> 00:08:29.140]   I just, I think I sort of deep down
[00:08:29.140 --> 00:08:30.740]   believed in what we were doing.
[00:08:30.740 --> 00:08:32.540]   And I did some analysis.
[00:08:32.540 --> 00:08:35.900]   I was like, okay, well,
[00:08:35.900 --> 00:08:38.820]   what would I go do if I wasn't doing this?
[00:08:38.820 --> 00:08:41.540]   It's like, well, I really like building things
[00:08:41.540 --> 00:08:43.300]   and I like helping people communicate
[00:08:43.300 --> 00:08:46.500]   and I like understanding what's going on with people
[00:08:46.500 --> 00:08:48.060]   and the dynamics between people.
[00:08:48.060 --> 00:08:50.220]   So, I think if I sold this company,
[00:08:50.220 --> 00:08:52.060]   I'd just go build another company like this.
[00:08:52.060 --> 00:08:54.540]   And I kind of like the one I have.
[00:08:54.540 --> 00:08:58.660]   So, I mean, you know, why, right?
[00:08:58.660 --> 00:09:01.100]   But I don't know.
[00:09:01.100 --> 00:09:04.340]   I think a lot of the biggest bets that people make
[00:09:04.340 --> 00:09:08.620]   are often just based on conviction and values.
[00:09:08.620 --> 00:09:13.340]   It's actually usually very hard to do the analyses
[00:09:13.340 --> 00:09:14.660]   trying to connect the dots forward.
[00:09:14.660 --> 00:09:15.500]   - Yeah.
[00:09:15.500 --> 00:09:18.540]   So, you've had Facebook AI research for a long time.
[00:09:18.540 --> 00:09:22.140]   Now it's become seemingly central to your company.
[00:09:22.140 --> 00:09:27.180]   At what point did making AGI or whatever,
[00:09:27.180 --> 00:09:28.580]   however you consider that mission,
[00:09:28.580 --> 00:09:29.620]   at what point is that like,
[00:09:29.620 --> 00:09:31.900]   this is a key priority of what Meta is doing?
[00:09:31.900 --> 00:09:35.100]   - Yeah, I mean, it's been a big deal for a while.
[00:09:35.100 --> 00:09:38.860]   So, we started FAIR about 10 years ago.
[00:09:38.860 --> 00:09:43.860]   And the idea was that along the way to general intelligence
[00:09:43.860 --> 00:09:47.620]   or AI, like full AI, whatever you wanna call it,
[00:09:47.620 --> 00:09:49.420]   there can be all these different innovations
[00:09:49.420 --> 00:09:51.820]   and that's gonna just improve everything that we do.
[00:09:51.820 --> 00:09:55.580]   So, we didn't kind of conceive it as a product.
[00:09:55.580 --> 00:09:58.100]   It was more kind of a research group.
[00:09:58.100 --> 00:10:01.340]   And over the last 10 years,
[00:10:01.340 --> 00:10:03.500]   it has created a lot of different things
[00:10:03.500 --> 00:10:07.180]   that have basically improved all of our products
[00:10:07.180 --> 00:10:09.940]   and advanced the field and allowed other people in the field
[00:10:09.940 --> 00:10:11.820]   to create things that have improved our products too.
[00:10:11.820 --> 00:10:13.660]   So, I think that that's been great.
[00:10:13.660 --> 00:10:18.500]   But there's obviously a big change in the last few years
[00:10:18.500 --> 00:10:21.380]   when chat GPT comes out,
[00:10:21.380 --> 00:10:23.620]   the diffusion models around image creation come out.
[00:10:23.620 --> 00:10:26.020]   And like, I mean, this is some pretty wild stuff, right?
[00:10:26.020 --> 00:10:27.980]   That I think is like pretty clearly gonna affect
[00:10:27.980 --> 00:10:32.500]   how people interact with like every app that's out there.
[00:10:32.500 --> 00:10:37.300]   So, at that point, we started a second group,
[00:10:37.300 --> 00:10:42.460]   the Gen AI group with the goal of basically
[00:10:42.460 --> 00:10:44.420]   bringing that stuff into our product.
[00:10:44.420 --> 00:10:46.620]   So, building leading foundation models
[00:10:46.620 --> 00:10:49.500]   that would sort of power all these different products.
[00:10:49.500 --> 00:10:52.260]   And initially when we started doing that,
[00:10:53.820 --> 00:10:54.940]   the theory at first was,
[00:10:54.940 --> 00:10:56.820]   hey, a lot of the stuff that we're doing
[00:10:56.820 --> 00:10:58.660]   is pretty social, right?
[00:10:58.660 --> 00:11:01.980]   So, it's helping people interact with creators,
[00:11:01.980 --> 00:11:04.980]   helping people interact with businesses
[00:11:04.980 --> 00:11:08.260]   so the businesses can sell things or do customer support
[00:11:08.260 --> 00:11:12.460]   or basic assistant functionality for,
[00:11:12.460 --> 00:11:14.980]   whether it's for our apps or the smart glasses
[00:11:14.980 --> 00:11:17.620]   or VR, like all these different things.
[00:11:17.620 --> 00:11:21.620]   So, initially it wasn't completely clear
[00:11:21.620 --> 00:11:26.220]   that you were gonna need kind of full AGI
[00:11:26.220 --> 00:11:27.780]   to be able to support those use cases.
[00:11:27.780 --> 00:11:29.660]   But then through working on them,
[00:11:29.660 --> 00:11:31.660]   I think it's actually become clear that you do, right?
[00:11:31.660 --> 00:11:32.620]   In all these subtle ways.
[00:11:32.620 --> 00:11:35.780]   So, for example, for Llama 2 when we were working on it,
[00:11:35.780 --> 00:11:37.300]   we didn't prioritize coding.
[00:11:37.300 --> 00:11:39.380]   And the reason why we didn't prioritize coding
[00:11:39.380 --> 00:11:41.620]   is because people aren't gonna ask Meta AI
[00:11:41.620 --> 00:11:43.420]   a lot of coding questions in WhatsApp.
[00:11:43.420 --> 00:11:44.580]   - Now they will. - Right?
[00:11:44.580 --> 00:11:45.420]   Well, I don't know.
[00:11:45.420 --> 00:11:47.100]   I'm not sure that WhatsApp is like the UI
[00:11:47.100 --> 00:11:49.100]   that people are gonna be doing a lot of coding questions.
[00:11:49.100 --> 00:11:49.980]   So, we're like, all right, look,
[00:11:49.980 --> 00:11:52.900]   in terms of the things that, or Facebook or Instagram
[00:11:52.900 --> 00:11:55.620]   or those different services, maybe the website, right?
[00:11:55.620 --> 00:11:58.180]   Meta.ai that we're launching, I think.
[00:11:58.180 --> 00:12:01.260]   But the thing that was sort of,
[00:12:01.260 --> 00:12:03.860]   I think has been a somewhat surprising result
[00:12:03.860 --> 00:12:08.780]   over the last 18 months is that it turns out
[00:12:08.780 --> 00:12:11.740]   that coding is important for a lot of domains,
[00:12:11.740 --> 00:12:12.580]   not just coding, right?
[00:12:12.580 --> 00:12:14.940]   So, even if people aren't asking coding questions
[00:12:14.940 --> 00:12:17.420]   to the models, training the models on coding
[00:12:17.420 --> 00:12:21.420]   helps them just be more rigorous and answer the question
[00:12:21.420 --> 00:12:23.980]   and kind of help reason across a lot
[00:12:23.980 --> 00:12:25.460]   of different types of domains.
[00:12:25.460 --> 00:12:26.740]   Okay, so that's one example where it's like, all right,
[00:12:26.740 --> 00:12:29.020]   so for Llama 3, we like really focused on training it
[00:12:29.020 --> 00:12:30.460]   with a lot of coding because it's like, all right,
[00:12:30.460 --> 00:12:32.220]   that's gonna make it better on all these things,
[00:12:32.220 --> 00:12:36.100]   even if people aren't asking primarily coding questions.
[00:12:36.100 --> 00:12:38.460]   Reasoning, I think, is another example.
[00:12:38.460 --> 00:12:41.660]   It's like, okay, yeah, maybe you wanna chat with a creator
[00:12:41.660 --> 00:12:44.220]   or you're a business and you're trying to interact
[00:12:44.220 --> 00:12:45.540]   with a customer.
[00:12:45.540 --> 00:12:47.380]   That interaction is not just like, okay,
[00:12:47.380 --> 00:12:50.740]   the person sends you a message and you just reply, right?
[00:12:50.740 --> 00:12:53.140]   It's like a multi-step interaction
[00:12:53.140 --> 00:12:54.660]   where you're trying to think through
[00:12:54.660 --> 00:12:56.540]   how do I accomplish the person's goals?
[00:12:56.540 --> 00:12:58.900]   And a lot of times when a customer comes,
[00:12:58.900 --> 00:13:00.780]   they don't necessarily know exactly
[00:13:00.780 --> 00:13:02.820]   what they're looking for or how to ask their questions.
[00:13:02.820 --> 00:13:05.060]   So, it's not really the job of the AI
[00:13:05.060 --> 00:13:06.580]   to just respond to the question.
[00:13:06.580 --> 00:13:08.060]   It's like, you need to kind of think about it
[00:13:08.060 --> 00:13:08.900]   more holistically.
[00:13:08.900 --> 00:13:10.540]   It really becomes a reasoning problem, right?
[00:13:10.540 --> 00:13:12.340]   So, if someone else solves reasoning
[00:13:12.340 --> 00:13:14.340]   or makes good advances on reasoning
[00:13:14.340 --> 00:13:17.180]   and we're sitting here with a basic chatbot,
[00:13:17.180 --> 00:13:18.620]   then our product is lame
[00:13:18.620 --> 00:13:20.300]   compared to what other people are building.
[00:13:20.300 --> 00:13:21.660]   So, it's like, so, okay.
[00:13:21.660 --> 00:13:25.660]   So, at the end of the day, we basically realized
[00:13:25.660 --> 00:13:28.380]   we've got to solve general intelligence
[00:13:28.380 --> 00:13:31.180]   and we just kind of upped the ante and the investment
[00:13:31.180 --> 00:13:32.580]   to make sure that we could do that.
[00:13:32.580 --> 00:13:34.540]   - So, the version of Llama
[00:13:34.540 --> 00:13:41.020]   that's going to solve all these use cases for users,
[00:13:41.020 --> 00:13:43.180]   is that the version that will be powerful enough
[00:13:43.180 --> 00:13:46.940]   to replace a programmer you might have in this building?
[00:13:46.940 --> 00:13:48.060]   - I mean, I just think that all this stuff
[00:13:48.060 --> 00:13:49.540]   is going to be progressive over time.
[00:13:49.540 --> 00:13:51.340]   - But in case, Llama 10.
[00:13:51.340 --> 00:13:55.700]   - I mean, I think that there's a lot
[00:13:55.700 --> 00:13:56.540]   baked into that question.
[00:13:56.540 --> 00:13:58.900]   I'm not sure that we're replacing people
[00:13:58.900 --> 00:14:02.180]   as much as giving people tools to do more stuff.
[00:14:02.180 --> 00:14:04.020]   - Is the programmer in this building 10x more productive
[00:14:04.020 --> 00:14:05.300]   after Llama 10? - I would hope more.
[00:14:05.300 --> 00:14:08.460]   But no, I mean, look, I'm not,
[00:14:08.460 --> 00:14:10.620]   I don't believe that there's like a single threshold
[00:14:10.620 --> 00:14:12.620]   of intelligence for humanity
[00:14:12.620 --> 00:14:14.700]   because I mean, people have different skills.
[00:14:14.700 --> 00:14:17.540]   And at some point, I think that AI is going to be,
[00:14:17.540 --> 00:14:21.540]   is probably going to surpass people at most of those things,
[00:14:21.540 --> 00:14:23.100]   depending on how powerful the models are.
[00:14:23.100 --> 00:14:27.060]   But I think it's progressive.
[00:14:27.060 --> 00:14:28.500]   And I don't think AGI is one thing.
[00:14:28.500 --> 00:14:31.060]   I think it's, you're basically adding different capabilities.
[00:14:31.060 --> 00:14:33.940]   So, multimodality is kind of a key one
[00:14:33.940 --> 00:14:35.420]   that we're focused on now,
[00:14:35.420 --> 00:14:38.140]   initially with photos and images and text,
[00:14:38.140 --> 00:14:39.460]   but eventually with videos.
[00:14:39.460 --> 00:14:41.380]   And then because we're so focused on the metaverse,
[00:14:41.380 --> 00:14:43.700]   kind of 3D type stuff is important.
[00:14:43.700 --> 00:14:46.980]   One modality that I'm pretty focused on
[00:14:46.980 --> 00:14:49.860]   that I haven't seen as many other people in the industry
[00:14:49.860 --> 00:14:53.740]   focus on this is sort of like emotional understanding.
[00:14:53.740 --> 00:14:56.340]   Like, I mean, so much of the human brain
[00:14:56.340 --> 00:14:59.180]   is just dedicated to understanding people
[00:14:59.180 --> 00:15:02.300]   and kind of like understanding your expressions and emotions
[00:15:02.300 --> 00:15:04.860]   and I think that that's like its own whole modality, right?
[00:15:04.860 --> 00:15:06.420]   That, I mean, you could say, okay,
[00:15:06.420 --> 00:15:07.940]   maybe it's just video or image,
[00:15:07.940 --> 00:15:09.900]   but it's like clearly a very specialized version
[00:15:09.900 --> 00:15:10.900]   of those two.
[00:15:10.900 --> 00:15:12.980]   So, there's all these different capabilities
[00:15:12.980 --> 00:15:16.020]   that I think you wanna basically train
[00:15:16.020 --> 00:15:17.700]   the models to focus on,
[00:15:17.700 --> 00:15:19.940]   as well as getting a lot better at reasoning,
[00:15:19.940 --> 00:15:21.100]   getting a lot better at memory,
[00:15:21.100 --> 00:15:23.180]   which I think is kind of its own whole thing.
[00:15:23.180 --> 00:15:24.620]   It's, I mean, I don't think we're gonna be,
[00:15:24.620 --> 00:15:26.740]   you know, primarily shoving context
[00:15:26.740 --> 00:15:30.260]   or kind of things into a query context window
[00:15:30.260 --> 00:15:32.940]   in the future to ask more complicated questions.
[00:15:32.940 --> 00:15:35.420]   I think that there'll be kind of different stores of memory
[00:15:35.420 --> 00:15:36.580]   or different custom models
[00:15:36.580 --> 00:15:39.780]   that are maybe more personalized to people.
[00:15:39.780 --> 00:15:40.820]   But, I don't know,
[00:15:40.820 --> 00:15:42.700]   I think that these are all just different capabilities.
[00:15:42.700 --> 00:15:44.420]   And then obviously making them big and small,
[00:15:44.420 --> 00:15:46.860]   we care about both because, you know, we wanna,
[00:15:46.860 --> 00:15:49.260]   you know, if you're running something like meta AI,
[00:15:49.260 --> 00:15:52.500]   then we have the ability to, that's pretty server-based,
[00:15:52.500 --> 00:15:54.340]   but we also want it running on smart glasses
[00:15:54.340 --> 00:15:56.740]   and, you know, there's not a lot of space in smart glasses.
[00:15:56.740 --> 00:16:00.140]   So, you wanna have something that's very efficient for that.
[00:16:00.140 --> 00:16:02.580]   What is the use case that if you're doing
[00:16:02.580 --> 00:16:04.060]   tens of billions of dollars worth of inference
[00:16:04.060 --> 00:16:05.660]   or even eventually hundreds of billions of dollars
[00:16:05.660 --> 00:16:06.820]   worth of inference,
[00:16:06.820 --> 00:16:09.220]   using intelligence in an industrial scale,
[00:16:09.220 --> 00:16:10.260]   what is the use case?
[00:16:10.260 --> 00:16:11.420]   Is it simulations?
[00:16:11.420 --> 00:16:13.220]   Is it the AIs that will be in the metaverse?
[00:16:13.220 --> 00:16:15.580]   What will we be using the data centers for?
[00:16:15.580 --> 00:16:21.260]   I mean, our bet is that it's gonna,
[00:16:21.260 --> 00:16:23.460]   this is basically gonna change all of the products, right?
[00:16:23.460 --> 00:16:25.500]   So, I think that there's gonna be
[00:16:25.500 --> 00:16:29.580]   a kind of meta AI general assistant product.
[00:16:29.580 --> 00:16:31.540]   And I think that that will shift
[00:16:31.540 --> 00:16:34.180]   from something that feels more like a chat bot,
[00:16:34.180 --> 00:16:35.700]   where it's like you just ask a question
[00:16:35.700 --> 00:16:37.540]   and it kind of formulates an answer,
[00:16:37.540 --> 00:16:39.100]   to things where you're increasingly giving it
[00:16:39.100 --> 00:16:40.420]   more complicated tasks,
[00:16:40.420 --> 00:16:42.420]   and then it goes away and does them.
[00:16:42.420 --> 00:16:44.620]   So, that's gonna take a lot of inference.
[00:16:44.620 --> 00:16:47.060]   It's gonna take a lot of compute and other ways too.
[00:16:47.060 --> 00:16:49.620]   Then I think that there's a big part
[00:16:49.620 --> 00:16:53.420]   of what we're gonna do that is like interacting
[00:16:53.420 --> 00:16:56.700]   with other agents for other people.
[00:16:56.700 --> 00:16:58.780]   So, whether it's businesses or creators,
[00:16:58.780 --> 00:17:01.820]   I guess a big part of my theory on this
[00:17:01.820 --> 00:17:04.180]   is that there's not just gonna be like one singular AI
[00:17:04.180 --> 00:17:05.340]   that you interact with,
[00:17:05.460 --> 00:17:09.100]   because I think every business is gonna want an AI
[00:17:09.100 --> 00:17:10.500]   that represents their interests.
[00:17:10.500 --> 00:17:12.900]   They're not gonna wanna primarily interact with you
[00:17:12.900 --> 00:17:14.700]   through an AI that is gonna sell
[00:17:14.700 --> 00:17:16.340]   their competitors' customers.
[00:17:16.340 --> 00:17:18.540]   So, sorry, their competitors' products.
[00:17:18.540 --> 00:17:25.380]   So, yeah, so I think creators is gonna be a big one.
[00:17:25.380 --> 00:17:27.420]   I mean, there are about 200 million creators
[00:17:27.420 --> 00:17:28.820]   on our platforms.
[00:17:28.820 --> 00:17:30.380]   They all basically have the pattern
[00:17:30.380 --> 00:17:33.300]   where they want to engage their community,
[00:17:33.300 --> 00:17:34.740]   but they're limited by hours in the day
[00:17:34.740 --> 00:17:36.980]   and their community generally wants to engage them,
[00:17:36.980 --> 00:17:39.700]   but they don't have, they're limited by hours in the day.
[00:17:39.700 --> 00:17:41.580]   So, if you could create something
[00:17:41.580 --> 00:17:44.180]   where an AI could basically,
[00:17:44.180 --> 00:17:46.820]   where that creator can basically own the AI
[00:17:46.820 --> 00:17:48.700]   and train it in the way that they want
[00:17:48.700 --> 00:17:51.740]   and can engage their community,
[00:17:51.740 --> 00:17:53.860]   I think that that's gonna be super powerful too.
[00:17:53.860 --> 00:17:56.380]   So, I think that there's gonna be a ton of engagement
[00:17:56.380 --> 00:17:57.580]   across all these things,
[00:17:57.580 --> 00:18:01.340]   but these are just the consumer use cases.
[00:18:01.340 --> 00:18:03.860]   I mean, I think when you think about stuff like,
[00:18:03.860 --> 00:18:07.220]   I mean, I run like our foundation, right,
[00:18:07.220 --> 00:18:09.260]   a Chan Zuckerberg initiative with my wife,
[00:18:09.260 --> 00:18:11.340]   and we're doing a bunch of stuff on science,
[00:18:11.340 --> 00:18:13.900]   and there's obviously a lot of AI work
[00:18:13.900 --> 00:18:17.020]   that I think is gonna advance science and healthcare
[00:18:17.020 --> 00:18:17.860]   and all these things too.
[00:18:17.860 --> 00:18:19.380]   So, I think that it's like,
[00:18:19.380 --> 00:18:21.140]   this is, I think, gonna end up affecting
[00:18:21.140 --> 00:18:25.420]   basically every area of the products and the economy.
[00:18:25.420 --> 00:18:27.340]   - The thing you mentioned about an AI
[00:18:27.340 --> 00:18:29.180]   that can just go out and do something for you
[00:18:29.180 --> 00:18:31.580]   that's multi-step, is that a bigger model?
[00:18:31.580 --> 00:18:34.420]   Is that you'll make, like Llama 4 will still,
[00:18:34.420 --> 00:18:35.940]   there'll be a version that's still 70B,
[00:18:35.940 --> 00:18:38.100]   but you'll just train it on the right data
[00:18:38.100 --> 00:18:40.220]   and that will be super powerful.
[00:18:40.220 --> 00:18:41.420]   Like, what does the progression look like?
[00:18:41.420 --> 00:18:42.260]   Is it scaling?
[00:18:42.260 --> 00:18:43.740]   Is it just same size,
[00:18:43.740 --> 00:18:46.140]   but different banks like you were talking about?
[00:18:46.140 --> 00:18:51.780]   - I don't know that we know the answer to that.
[00:18:51.780 --> 00:18:55.940]   So, I think one thing that seems to be a pattern
[00:18:55.940 --> 00:18:59.740]   is that you have the llama, sorry, the llama model,
[00:18:59.740 --> 00:19:04.740]   and then you build some kind of other application
[00:19:04.740 --> 00:19:06.420]   specific code around it, right?
[00:19:06.420 --> 00:19:08.780]   So, some of it is the fine tuning for the use case,
[00:19:08.780 --> 00:19:12.380]   but some of it is just like logic for, okay,
[00:19:12.380 --> 00:19:17.180]   how MetAI should integrate,
[00:19:17.180 --> 00:19:19.420]   like it should work with tools like Google or Bing
[00:19:19.420 --> 00:19:20.700]   to bring in real-time knowledge.
[00:19:20.700 --> 00:19:22.260]   I mean, that's not part of the base llama model.
[00:19:22.260 --> 00:19:23.260]   That's like part of a, okay.
[00:19:23.260 --> 00:19:26.860]   So, for Llama 2, we had some of that
[00:19:26.860 --> 00:19:29.580]   and it was a little more kind of hand engineered.
[00:19:29.580 --> 00:19:32.180]   And then part of our goal for Llama 3
[00:19:32.180 --> 00:19:35.260]   was to bring more of that into the model itself.
[00:19:35.260 --> 00:19:36.460]   And, but for Llama 3,
[00:19:36.460 --> 00:19:40.180]   as we start getting into more of these agent-like behaviors,
[00:19:40.180 --> 00:19:43.020]   I think some of that is gonna be more hand engineered.
[00:19:43.020 --> 00:19:45.100]   And then I think our goal for Llama 4
[00:19:45.100 --> 00:19:46.900]   will be to bring more of that into the model.
[00:19:46.900 --> 00:19:50.260]   So, I think at each point, like at each step along the way,
[00:19:50.260 --> 00:19:51.820]   you kind of have a sense
[00:19:51.820 --> 00:19:54.100]   of what's gonna be possible on the horizon.
[00:19:54.100 --> 00:19:56.700]   You start messing with it and hacking around it.
[00:19:56.700 --> 00:19:59.860]   And then I think that that helps you hone your intuition
[00:19:59.860 --> 00:20:01.660]   for what you wanna try to train
[00:20:01.660 --> 00:20:03.580]   into the next version of the model itself.
[00:20:03.580 --> 00:20:04.420]   - Interesting.
[00:20:04.420 --> 00:20:05.260]   - Which makes it more general,
[00:20:05.260 --> 00:20:08.620]   because obviously anything that you're hand coding is,
[00:20:08.620 --> 00:20:10.460]   you know, you can unlock some use cases,
[00:20:10.460 --> 00:20:13.660]   but it's just inherently brittle and non-general.
[00:20:13.660 --> 00:20:15.140]   - Hey everybody, real quick,
[00:20:15.140 --> 00:20:16.940]   I wanna tell you about a tool
[00:20:16.940 --> 00:20:19.220]   that I wish more applications used.
[00:20:19.220 --> 00:20:22.060]   So, obviously you've noticed every single company
[00:20:22.060 --> 00:20:25.260]   is trying to add an AI chat bot to their website.
[00:20:25.260 --> 00:20:28.660]   But as a user, I usually find them really annoying
[00:20:28.660 --> 00:20:30.500]   'cause they give these long, generic,
[00:20:30.500 --> 00:20:32.500]   often useless answers.
[00:20:32.500 --> 00:20:34.460]   Command Bar is a user assistant
[00:20:34.460 --> 00:20:37.580]   that you can just embed into your website or application.
[00:20:37.580 --> 00:20:39.340]   And it feels like you're talking
[00:20:39.340 --> 00:20:41.300]   to a friendly human support agent
[00:20:41.300 --> 00:20:44.220]   who is browsing with you and for you.
[00:20:44.220 --> 00:20:47.700]   And it's much more personalized than a regular chat bot.
[00:20:47.700 --> 00:20:49.620]   It can actually look up users history
[00:20:49.620 --> 00:20:51.740]   and respond differently based on that.
[00:20:51.740 --> 00:20:54.740]   It can use APIs to perform actions.
[00:20:54.740 --> 00:20:58.740]   It can even practically nudge users to explore new features.
[00:20:58.740 --> 00:21:00.260]   One thing that I think is really cool
[00:21:00.260 --> 00:21:02.740]   is that instead of just outputting text,
[00:21:02.740 --> 00:21:04.220]   Command Bar can kind of just say,
[00:21:04.220 --> 00:21:05.820]   "Here, let me show you,"
[00:21:05.820 --> 00:21:08.700]   and start browsing alongside the user.
[00:21:08.700 --> 00:21:11.320]   Anyways, they're in a bunch of great products already.
[00:21:11.320 --> 00:21:15.660]   You can learn more about them at commandbar.com.
[00:21:15.660 --> 00:21:17.700]   Thanks to them for sponsoring this episode.
[00:21:17.700 --> 00:21:19.380]   And now back to Mark.
[00:21:19.380 --> 00:21:20.740]   When you say into the model itself,
[00:21:20.740 --> 00:21:24.220]   you train it on the thing that you want in the model itself,
[00:21:24.220 --> 00:21:26.060]   but what do you mean by into the model itself?
[00:21:26.060 --> 00:21:28.340]   - Well, I mean, I think like the example
[00:21:28.340 --> 00:21:31.020]   that I gave for Llama 2, where, you know,
[00:21:31.020 --> 00:21:34.020]   we really, I mean, for Llama 2,
[00:21:34.020 --> 00:21:38.100]   the tool use was very, very specific.
[00:21:38.100 --> 00:21:39.620]   Whereas Llama 3 has the ability to,
[00:21:39.620 --> 00:21:41.020]   has much better tool use, right?
[00:21:41.020 --> 00:21:44.260]   So we don't have to like hand code all the stuff
[00:21:44.260 --> 00:21:48.300]   to have it use Google to go do a search.
[00:21:48.300 --> 00:21:50.060]   It just kind of can do that.
[00:21:51.100 --> 00:21:54.940]   So, and similarly for coding and kind of running code
[00:21:54.940 --> 00:21:56.940]   and just a bunch of stuff like that.
[00:21:56.940 --> 00:22:02.040]   But I think once you kind of get that capability,
[00:22:02.040 --> 00:22:03.860]   then you get a peak of, okay, well,
[00:22:03.860 --> 00:22:05.180]   what can we start doing next?
[00:22:05.180 --> 00:22:06.560]   Okay, well, I don't necessarily want to wait
[00:22:06.560 --> 00:22:09.460]   until Llama 4 is around to start building those capabilities.
[00:22:09.460 --> 00:22:10.720]   So let's start hacking around it.
[00:22:10.720 --> 00:22:13.140]   And so you do a bunch of hand coding
[00:22:13.140 --> 00:22:16.460]   and that makes the products better for the interim,
[00:22:16.460 --> 00:22:18.380]   but then that also helps show the way
[00:22:18.380 --> 00:22:19.800]   of what we want to try to build
[00:22:19.800 --> 00:22:21.340]   into the next version of the model.
[00:22:21.340 --> 00:22:23.260]   - What is the community fine tune
[00:22:23.260 --> 00:22:24.800]   of Llama 3 you're most excited by?
[00:22:24.800 --> 00:22:26.540]   Maybe not the one that will be most useful to you,
[00:22:26.540 --> 00:22:29.980]   but I guess you'll just enjoy playing it with the most.
[00:22:29.980 --> 00:22:31.380]   They like fine tune it on antiquity
[00:22:31.380 --> 00:22:33.140]   and you'll just be like talking to Virgil or something.
[00:22:33.140 --> 00:22:34.440]   What are you excited about?
[00:22:34.440 --> 00:22:35.280]   - I don't know.
[00:22:35.280 --> 00:22:39.780]   I mean, I think the nature of the stuff is it's like,
[00:22:39.780 --> 00:22:41.340]   you get surprised, right?
[00:22:41.340 --> 00:22:44.380]   So I think like any specific thing
[00:22:44.380 --> 00:22:47.740]   that I sort of thought would be valuable,
[00:22:47.740 --> 00:22:49.620]   we'd probably be building, right?
[00:22:49.620 --> 00:22:54.620]   So, but I think you'll get distilled versions.
[00:22:54.620 --> 00:22:57.300]   I think you'll get kind of smaller versions.
[00:22:57.300 --> 00:23:01.940]   I mean, one thing that I think is 8 billion,
[00:23:01.940 --> 00:23:03.700]   I don't think is quite small enough
[00:23:03.700 --> 00:23:05.620]   for a bunch of use cases, right?
[00:23:05.620 --> 00:23:06.900]   I think like over time,
[00:23:06.900 --> 00:23:09.820]   I'd love to get a billion parameter model
[00:23:09.820 --> 00:23:11.840]   or a 2 billion parameter model,
[00:23:11.840 --> 00:23:12.900]   or even like a, I don't know,
[00:23:12.900 --> 00:23:14.920]   maybe like a 500 million parameter model
[00:23:14.920 --> 00:23:15.840]   and see what you can do with that.
[00:23:15.840 --> 00:23:17.980]   Because I mean, as they start getting,
[00:23:17.980 --> 00:23:19.660]   if with 8 billion parameters,
[00:23:19.660 --> 00:23:21.820]   we're basically nearly as powerful
[00:23:21.820 --> 00:23:23.500]   as the largest Llama 2 model,
[00:23:23.500 --> 00:23:25.500]   then with a billion parameters,
[00:23:25.500 --> 00:23:26.340]   you should be able to do something
[00:23:26.340 --> 00:23:27.340]   that's interesting, right?
[00:23:27.340 --> 00:23:30.860]   And faster, good for classification
[00:23:30.860 --> 00:23:32.420]   or a lot of kind of like basic things
[00:23:32.420 --> 00:23:35.940]   that people do before kind of understanding
[00:23:35.940 --> 00:23:38.100]   the intent of a user query
[00:23:38.100 --> 00:23:39.740]   and feeding it to the most powerful model
[00:23:39.740 --> 00:23:42.700]   to kind of hone what the prompt should be.
[00:23:42.700 --> 00:23:45.020]   So I don't know.
[00:23:45.020 --> 00:23:45.840]   I think that's one thing
[00:23:45.840 --> 00:23:46.800]   that maybe the community can help fill in.
[00:23:46.800 --> 00:23:48.620]   But I mean, we're also thinking
[00:23:48.620 --> 00:23:50.700]   about getting around to distilling
[00:23:50.700 --> 00:23:51.660]   some of these ourselves,
[00:23:51.660 --> 00:23:56.660]   but right now the GPUs are training the 405, so.
[00:23:56.660 --> 00:23:58.460]   - Okay, so you have all these GPUs.
[00:23:58.460 --> 00:24:02.340]   I think you said 350,000 by the end of the year.
[00:24:02.340 --> 00:24:03.180]   - That's the whole fleet.
[00:24:03.180 --> 00:24:06.580]   I mean, we built two,
[00:24:06.580 --> 00:24:10.140]   I think it's like 22, 24,000 clusters
[00:24:10.140 --> 00:24:11.960]   that are kind of the single clusters
[00:24:11.960 --> 00:24:14.100]   that we have for training the big models.
[00:24:14.100 --> 00:24:16.180]   I mean, obviously across a lot of the stuff that we do,
[00:24:16.180 --> 00:24:18.040]   a lot of our stuff goes towards training
[00:24:18.040 --> 00:24:21.460]   like reels models and like Facebook news feed
[00:24:21.460 --> 00:24:22.300]   and Instagram feed.
[00:24:22.300 --> 00:24:24.020]   And then inference is a huge thing for us
[00:24:24.020 --> 00:24:25.500]   'cause we serve a ton of people, right?
[00:24:25.500 --> 00:24:30.260]   So our ratio of inference compute required
[00:24:30.260 --> 00:24:33.580]   to training is probably much higher
[00:24:33.580 --> 00:24:35.540]   than most other companies that are doing this stuff
[00:24:35.540 --> 00:24:36.940]   just because of the sheer volume
[00:24:36.940 --> 00:24:38.980]   of the community that we're serving.
[00:24:38.980 --> 00:24:39.900]   - Yeah. - Yeah.
[00:24:39.900 --> 00:24:40.740]   - Yeah, that was really interesting
[00:24:40.740 --> 00:24:42.300]   in the material they shared with me before
[00:24:42.300 --> 00:24:44.100]   that you trained it on more data
[00:24:44.100 --> 00:24:46.340]   than is compute optimal just for training
[00:24:46.340 --> 00:24:48.280]   because the inference is such a big deal for you guys
[00:24:48.280 --> 00:24:49.580]   and also for the community
[00:24:49.580 --> 00:24:51.180]   that it makes sense to just have this thing
[00:24:51.180 --> 00:24:52.860]   and have trillions of tokens in there.
[00:24:52.860 --> 00:24:53.700]   - Yeah, yeah.
[00:24:53.700 --> 00:24:56.140]   Although, and one of the interesting things about it
[00:24:56.140 --> 00:24:57.420]   that we saw even with the 70 billion
[00:24:57.420 --> 00:25:00.660]   is we thought it would get more saturated at,
[00:25:00.660 --> 00:25:03.060]   you know, it's like we trained it
[00:25:03.060 --> 00:25:05.780]   on around 15 trillion tokens.
[00:25:05.780 --> 00:25:07.580]   I guess our prediction going in
[00:25:07.580 --> 00:25:10.860]   was that it was gonna asymptote more,
[00:25:10.860 --> 00:25:14.080]   but even by the end it was still learning, right?
[00:25:14.080 --> 00:25:17.340]   It's like we probably could have fed it more tokens
[00:25:17.340 --> 00:25:19.260]   and it would have gotten somewhat better.
[00:25:19.260 --> 00:25:20.540]   But I mean, at some point, you know,
[00:25:20.540 --> 00:25:21.380]   you're running a company,
[00:25:21.380 --> 00:25:24.420]   you need to do these meta reasoning questions of like,
[00:25:24.420 --> 00:25:26.500]   all right, how do I wanna spend our GPUs
[00:25:26.500 --> 00:25:29.220]   on like training the 70 billion model further?
[00:25:29.220 --> 00:25:31.460]   Do we wanna kind of get on with it
[00:25:31.460 --> 00:25:33.940]   so we can start testing hypotheses for LLAMA4?
[00:25:33.940 --> 00:25:36.980]   So we kind of needed to make that call.
[00:25:36.980 --> 00:25:38.180]   And I think we got it.
[00:25:38.180 --> 00:25:39.420]   I think we got to a reasonable balance
[00:25:39.420 --> 00:25:41.520]   for this version of the 70 billion.
[00:25:43.100 --> 00:25:44.200]   There'll be others in the future
[00:25:44.200 --> 00:25:45.960]   where, you know, the 70 billion multimodal one
[00:25:45.960 --> 00:25:47.880]   that'll come over the next period.
[00:25:47.880 --> 00:25:51.440]   But yeah, I mean, that was fascinating
[00:25:51.440 --> 00:25:53.400]   that you could just, that it's the architectures
[00:25:53.400 --> 00:25:55.720]   at this point can just take so much data.
[00:25:55.720 --> 00:25:56.600]   - Yeah, that's really interesting.
[00:25:56.600 --> 00:25:59.000]   So what does this imply about future models?
[00:25:59.000 --> 00:26:02.160]   You mentioned that the LLAMA3 8B
[00:26:02.160 --> 00:26:03.760]   is better than the LLAMA2 70B, right?
[00:26:03.760 --> 00:26:04.880]   - No, no, no, it's nearly as good.
[00:26:04.880 --> 00:26:06.440]   - Okay, I don't understand.
[00:26:06.440 --> 00:26:07.280]   - But does that mean like the LLAMA4-
[00:26:07.280 --> 00:26:08.540]   - In the same order of magnitude.
[00:26:08.540 --> 00:26:09.800]   - Does that mean like the LLAMA4 70B
[00:26:09.800 --> 00:26:11.880]   will be as good as the LLAMA3 4 or 5B?
[00:26:11.880 --> 00:26:13.820]   - I mean, what is the future of this look like?
[00:26:13.820 --> 00:26:15.340]   - This is one of the great questions, right?
[00:26:15.340 --> 00:26:18.420]   That I think no one knows is basically,
[00:26:18.420 --> 00:26:22.900]   you know, it's one of the trickiest things in the world
[00:26:22.900 --> 00:26:25.460]   to plan around is when you have an exponential curve,
[00:26:25.460 --> 00:26:27.580]   how long does it keep going for?
[00:26:27.580 --> 00:26:32.260]   And I think it's likely enough that it will keep going,
[00:26:32.260 --> 00:26:36.420]   that it is worth investing the 10s or, you know,
[00:26:36.420 --> 00:26:39.460]   a hundred billion plus in building the infrastructure
[00:26:39.460 --> 00:26:42.640]   to assume that if that kind of keeps going,
[00:26:42.640 --> 00:26:44.880]   you're gonna get some really amazing things
[00:26:44.880 --> 00:26:47.160]   that are just gonna make amazing products.
[00:26:47.160 --> 00:26:51.660]   But I don't think anyone in the industry can really tell you
[00:26:51.660 --> 00:26:55.360]   that it will continue scaling at that rate for sure, right?
[00:26:55.360 --> 00:26:56.920]   In general, you know, in history,
[00:26:56.920 --> 00:26:58.760]   you hit bottlenecks at certain points.
[00:26:58.760 --> 00:27:00.920]   And now there's so much energy on this
[00:27:00.920 --> 00:27:03.400]   that maybe those bottlenecks get knocked over
[00:27:03.400 --> 00:27:06.520]   pretty quickly, but I don't know.
[00:27:06.520 --> 00:27:08.240]   I think that's an interesting question.
[00:27:08.240 --> 00:27:09.260]   - What does the world look like
[00:27:09.260 --> 00:27:11.060]   where there aren't these bottlenecks?
[00:27:11.060 --> 00:27:14.420]   You know, suppose like progress just continues at this pace,
[00:27:14.420 --> 00:27:17.740]   which seems like plausible, like zooming out.
[00:27:17.740 --> 00:27:20.700]   - Well, there are gonna be different bottlenecks.
[00:27:20.700 --> 00:27:23.820]   - Right, so if not training, then, like, oh yeah, go ahead.
[00:27:23.820 --> 00:27:26.620]   - Well, I think at some point, you know,
[00:27:26.620 --> 00:27:27.700]   over the last few years,
[00:27:27.700 --> 00:27:31.740]   I think there was this issue of GPU production, right?
[00:27:31.740 --> 00:27:34.200]   So even companies that had the models,
[00:27:34.200 --> 00:27:37.700]   sorry, that had the money to pay for the GPUs,
[00:27:38.520 --> 00:27:40.800]   couldn't necessarily get as many as they wanted
[00:27:40.800 --> 00:27:43.200]   because there were all these supply constraints.
[00:27:43.200 --> 00:27:46.320]   Now, I think that's sort of getting less.
[00:27:46.320 --> 00:27:49.480]   So now I think you're seeing a bunch of companies
[00:27:49.480 --> 00:27:52.120]   think about, wow, we should just like really invest
[00:27:52.120 --> 00:27:54.040]   a lot of money in building out these things.
[00:27:54.040 --> 00:27:57.400]   And I think that that will go for some period of time.
[00:27:57.400 --> 00:28:03.400]   I think there is a capital question of like, okay,
[00:28:03.400 --> 00:28:05.720]   at what point does it stop being worth it
[00:28:05.720 --> 00:28:06.780]   to put the capital in?
[00:28:06.780 --> 00:28:09.240]   But I actually think before we hit that,
[00:28:09.240 --> 00:28:11.440]   you're gonna run into energy constraints, right?
[00:28:11.440 --> 00:28:14.200]   Because I just, I mean,
[00:28:14.200 --> 00:28:16.880]   I don't think anyone's built a gigawatt
[00:28:16.880 --> 00:28:19.400]   single training cluster yet, right?
[00:28:19.400 --> 00:28:21.800]   And then you run into these things
[00:28:21.800 --> 00:28:23.340]   that just end up being slower in the world.
[00:28:23.340 --> 00:28:26.400]   Like getting energy permitted
[00:28:26.400 --> 00:28:31.400]   is like a very heavily regulated government function, right?
[00:28:31.400 --> 00:28:34.440]   So you're going from, on the one hand, software,
[00:28:34.440 --> 00:28:36.820]   which is somewhat regulated.
[00:28:36.820 --> 00:28:38.460]   I'd argue that it is more regulated
[00:28:38.460 --> 00:28:41.540]   than I think a lot of people in the tech community feel,
[00:28:41.540 --> 00:28:42.660]   although it's obviously different.
[00:28:42.660 --> 00:28:43.860]   If you're starting a small company,
[00:28:43.860 --> 00:28:44.740]   maybe you feel that less.
[00:28:44.740 --> 00:28:46.060]   If you're a big company,
[00:28:46.060 --> 00:28:47.680]   we just interact with people,
[00:28:47.680 --> 00:28:49.860]   different governments and regulators are,
[00:28:49.860 --> 00:28:52.660]   we have kind of lots of rules
[00:28:52.660 --> 00:28:53.740]   that we need to kind of follow
[00:28:53.740 --> 00:28:56.820]   and make sure we do a good job with around the world.
[00:28:56.820 --> 00:28:59.020]   But I think that there's no doubt that like energy,
[00:28:59.020 --> 00:29:02.820]   and if you're talking about building large new power plants
[00:29:02.820 --> 00:29:05.660]   or large build outs and then building transmission lines
[00:29:05.660 --> 00:29:10.060]   that cross other private or public land,
[00:29:10.060 --> 00:29:11.900]   that is just a heavily regulated thing.
[00:29:11.900 --> 00:29:14.520]   So you're talking about many years of lead time.
[00:29:14.520 --> 00:29:17.120]   So if we wanted to stand up
[00:29:17.120 --> 00:29:21.500]   just some like massive facility to power that,
[00:29:21.500 --> 00:29:26.500]   I think that that's a very long-term project, right?
[00:29:26.500 --> 00:29:31.060]   And so, I don't know, I think people will do it,
[00:29:31.060 --> 00:29:32.980]   but I don't think that this is like
[00:29:32.980 --> 00:29:35.100]   something that can be quite as magical
[00:29:35.100 --> 00:29:36.820]   as just like, okay, you get a level of AI
[00:29:36.820 --> 00:29:38.540]   and you get a bunch of capital and you put it in,
[00:29:38.540 --> 00:29:39.380]   and then like all of a sudden,
[00:29:39.380 --> 00:29:41.660]   the models are just gonna kind of like,
[00:29:41.660 --> 00:29:43.900]   I think you do hit different bottlenecks along the way.
[00:29:43.900 --> 00:29:46.360]   - Yeah, is there something, a project,
[00:29:46.360 --> 00:29:48.180]   maybe AI-related, maybe not,
[00:29:48.180 --> 00:29:50.340]   that even a company like Meta
[00:29:50.340 --> 00:29:51.540]   doesn't have the resources for?
[00:29:51.540 --> 00:29:53.420]   Like if your R&D budget or your CapEx budget
[00:29:53.420 --> 00:29:55.820]   was 10X what it is now, then you could pursue it.
[00:29:55.820 --> 00:29:56.940]   Like it's in the back of your mind,
[00:29:56.940 --> 00:30:00.060]   but Meta today, maybe you could like,
[00:30:00.060 --> 00:30:01.780]   even you can't even issue a stock or bond for it,
[00:30:01.780 --> 00:30:03.660]   it's like just 10X bigger than your budget.
[00:30:03.660 --> 00:30:06.540]   - Well, I think energy is one piece, right?
[00:30:06.540 --> 00:30:10.260]   I think we would probably build out bigger clusters
[00:30:10.260 --> 00:30:13.420]   than we currently can
[00:30:13.420 --> 00:30:15.940]   if we could get the energy to do it.
[00:30:15.940 --> 00:30:19.980]   So I think that that's fundamentally
[00:30:19.980 --> 00:30:22.060]   money bottlenecked in the limit,
[00:30:22.060 --> 00:30:22.900]   like if you had a trillion dollars.
[00:30:22.900 --> 00:30:24.960]   - I think it's time, right?
[00:30:24.960 --> 00:30:28.900]   Well, if you look at it in terms of,
[00:30:28.900 --> 00:30:31.620]   but it depends on how far the exponential curves go, right?
[00:30:31.620 --> 00:30:34.380]   Like I think a number of companies are working on,
[00:30:34.380 --> 00:30:36.980]   you know, right now, I think a lot of data centers
[00:30:36.980 --> 00:30:39.340]   are on the order of 50 megawatts or 100 megawatts,
[00:30:39.340 --> 00:30:41.500]   or like a big one might be 150 megawatts.
[00:30:41.500 --> 00:30:43.060]   Okay, so you take a whole data center
[00:30:43.060 --> 00:30:45.160]   and you fill it up with just all the stuff
[00:30:45.160 --> 00:30:46.300]   that you need to do for training
[00:30:46.300 --> 00:30:47.620]   and you build the biggest cluster you can.
[00:30:47.620 --> 00:30:49.900]   I think that's kind of,
[00:30:49.900 --> 00:30:53.260]   I think a bunch of companies are running at stuff like that.
[00:30:53.260 --> 00:30:57.220]   But then when you start getting into building
[00:30:57.220 --> 00:31:00.300]   a data center that's like 300 megawatts
[00:31:00.300 --> 00:31:02.860]   or 500 megawatts or a gigawatt,
[00:31:02.860 --> 00:31:04.220]   I just, I mean,
[00:31:04.220 --> 00:31:06.420]   just no one has built a single gigawatt data center yet.
[00:31:06.420 --> 00:31:07.780]   So I think it will happen, right?
[00:31:07.780 --> 00:31:08.980]   I mean, this is only a matter of time,
[00:31:08.980 --> 00:31:12.540]   but it's not gonna be like next year, right?
[00:31:12.540 --> 00:31:15.900]   I think that some of these things will take,
[00:31:15.900 --> 00:31:19.220]   I don't know, some number of years to build out.
[00:31:19.220 --> 00:31:21.520]   And then the question is, okay, well, if you,
[00:31:21.520 --> 00:31:25.580]   I mean, just to, I guess, put this in perspective,
[00:31:25.580 --> 00:31:29.260]   I think a gigawatt, it's like around the size
[00:31:29.260 --> 00:31:32.460]   of like a meaningful nuclear power plant
[00:31:32.460 --> 00:31:34.980]   only going towards training a model.
[00:31:34.980 --> 00:31:36.540]   - Didn't Amazon do this?
[00:31:36.540 --> 00:31:39.180]   There's like, they have a 950 gig, a megawatt.
[00:31:39.180 --> 00:31:40.700]   - Yeah, I'm not exactly sure what you did.
[00:31:40.700 --> 00:31:44.060]   You'd have to, what they did, you'd have to ask them.
[00:31:44.060 --> 00:31:45.460]   - But it doesn't have to be in the same place, right?
[00:31:45.460 --> 00:31:47.380]   If distributed training works, it can be distributed.
[00:31:47.380 --> 00:31:48.260]   - That I think is a big question.
[00:31:48.260 --> 00:31:49.100]   - Yeah.
[00:31:49.100 --> 00:31:50.060]   - Right, is basically how that's gonna work.
[00:31:50.060 --> 00:31:52.320]   And I do think in the future,
[00:31:52.320 --> 00:31:55.460]   it seems quite possible that more of
[00:31:55.460 --> 00:31:58.100]   what we call training for these big models
[00:31:58.100 --> 00:32:02.420]   is actually more along the lines
[00:32:02.420 --> 00:32:05.060]   of inference generating synthetic data
[00:32:05.060 --> 00:32:06.700]   to then go feed into the model.
[00:32:06.700 --> 00:32:08.700]   So I don't know what that ratio is gonna be,
[00:32:08.700 --> 00:32:12.540]   but I consider the generation of synthetic data
[00:32:12.540 --> 00:32:14.620]   to be more inference than training today.
[00:32:14.620 --> 00:32:16.940]   But obviously if you're doing it in order to train a model,
[00:32:16.940 --> 00:32:19.380]   it's part of the broader training process.
[00:32:19.380 --> 00:32:23.580]   So I don't know, that's an open question
[00:32:23.580 --> 00:32:25.660]   as to kind of where, what the balance of that
[00:32:25.660 --> 00:32:26.620]   and how that plays out.
[00:32:26.620 --> 00:32:28.700]   - If that's the case,
[00:32:28.700 --> 00:32:31.380]   would that potentially also be the case with LLAMA-3?
[00:32:31.380 --> 00:32:32.820]   And maybe like LLAMA-4 onwards,
[00:32:32.820 --> 00:32:36.060]   where you put this out and if somebody has a ton of compute,
[00:32:36.060 --> 00:32:37.860]   then using the models that you've put out,
[00:32:37.860 --> 00:32:40.740]   you can just keep making these things arbitrarily smarter.
[00:32:40.740 --> 00:32:43.900]   Like some Kuwait or UAE or some random country
[00:32:43.900 --> 00:32:47.460]   has a ton of compute and they can just actually
[00:32:47.460 --> 00:32:50.060]   just use LLAMA-4 to just make something much smarter.
[00:32:52.340 --> 00:32:55.700]   - I do think that there are gonna be dynamics like that.
[00:32:55.700 --> 00:33:00.700]   But I also think that there is a fundamental limitation
[00:33:00.700 --> 00:33:06.060]   on kind of the network architecture, right?
[00:33:06.060 --> 00:33:08.180]   Or the kind of model architecture, right?
[00:33:08.180 --> 00:33:12.380]   So I think like a 70 billion model
[00:33:12.380 --> 00:33:14.660]   that kind of we trained with the LLAMA-3 architecture
[00:33:14.660 --> 00:33:15.780]   can get better, right?
[00:33:15.780 --> 00:33:16.940]   It can keep going.
[00:33:16.940 --> 00:33:18.300]   Like I was saying, it's, you know,
[00:33:18.300 --> 00:33:21.060]   we felt like if we kept on feeding it more data
[00:33:21.060 --> 00:33:24.100]   or rotated the high value tokens through again,
[00:33:24.100 --> 00:33:26.860]   then it would continue getting better.
[00:33:26.860 --> 00:33:30.980]   But, and then we've seen a bunch of other people
[00:33:30.980 --> 00:33:32.860]   around the world, you know,
[00:33:32.860 --> 00:33:35.900]   different companies basically take the LLAMA-2
[00:33:35.900 --> 00:33:38.340]   70 billion base, like take that model architecture
[00:33:38.340 --> 00:33:39.700]   and then build a new model.
[00:33:39.700 --> 00:33:43.100]   It's still the case that when you make
[00:33:43.100 --> 00:33:45.500]   a generational improvement to the kind of LLAMA-3
[00:33:45.500 --> 00:33:47.340]   70 billion or the LLAMA-3 405,
[00:33:47.340 --> 00:33:49.940]   there's nothing open source, anything like that today.
[00:33:49.940 --> 00:33:53.180]   Right, like it's not, I think that that's like,
[00:33:53.180 --> 00:33:55.100]   it's a big step function
[00:33:55.100 --> 00:33:57.180]   and what people are gonna be able to build on top of
[00:33:57.180 --> 00:33:59.940]   that I don't think can go infinitely from there.
[00:33:59.940 --> 00:34:02.580]   I think it can, there can be some optimization in that
[00:34:02.580 --> 00:34:04.780]   until you get to the next step function.
[00:34:04.780 --> 00:34:05.780]   - Yeah, okay.
[00:34:05.780 --> 00:34:08.780]   So let's zoom out a little bit from specific models
[00:34:08.780 --> 00:34:12.140]   and even the many years lead times you would need
[00:34:12.140 --> 00:34:14.100]   to get energy approvals and so on.
[00:34:14.100 --> 00:34:16.380]   Like big picture, these next couple of decades,
[00:34:16.380 --> 00:34:18.380]   what's happening with AI?
[00:34:18.380 --> 00:34:20.900]   Does it feel like another technology like metaverse
[00:34:20.900 --> 00:34:23.100]   or social, or does it feel like a fundamentally
[00:34:23.100 --> 00:34:25.380]   different thing in the course of human history?
[00:34:25.380 --> 00:34:31.420]   - I think it's gonna be pretty fundamental.
[00:34:31.420 --> 00:34:35.740]   I think it's gonna be more like the creation of computing
[00:34:35.740 --> 00:34:37.460]   in the first place, right?
[00:34:37.460 --> 00:34:42.460]   So you'll get all these new apps in the same way
[00:34:42.460 --> 00:34:46.940]   that when you got the web or you got mobile phones,
[00:34:46.940 --> 00:34:49.220]   you got like people basically rethought
[00:34:49.220 --> 00:34:50.980]   all these experiences and a lot of things
[00:34:50.980 --> 00:34:53.420]   that weren't possible before now became possible.
[00:34:53.420 --> 00:34:56.060]   So I think that will happen,
[00:34:56.060 --> 00:34:58.860]   but I think it's a much lower level innovation.
[00:34:58.860 --> 00:35:02.220]   It's gonna be more like going from people
[00:35:02.220 --> 00:35:04.860]   didn't have computers to people have computers
[00:35:04.860 --> 00:35:06.020]   is my sense.
[00:35:06.020 --> 00:35:12.500]   But it's also, it's, I don't know,
[00:35:12.500 --> 00:35:17.100]   it's very hard to reason about exactly how this goes.
[00:35:17.100 --> 00:35:21.340]   I tend to think that, you know, in like the cosmic scale,
[00:35:21.340 --> 00:35:24.940]   obviously it'll happen quickly over a couple of decades
[00:35:24.940 --> 00:35:28.460]   or something, but I do think that there is some set
[00:35:28.460 --> 00:35:30.780]   of people who are afraid of like,
[00:35:30.780 --> 00:35:32.500]   you know, it really just kind of spins
[00:35:32.500 --> 00:35:34.420]   and goes from being like somewhat intelligent
[00:35:34.420 --> 00:35:36.140]   to extremely intelligent overnight.
[00:35:36.140 --> 00:35:37.980]   And I just think that there's all these physical constraints
[00:35:37.980 --> 00:35:41.260]   that make that, so that's unlikely to happen.
[00:35:41.260 --> 00:35:44.460]   I just don't, I don't really see that playing out.
[00:35:44.460 --> 00:35:46.500]   So I think you'll have, I think we'll have time
[00:35:46.500 --> 00:35:47.860]   to kind of acclimate a bit,
[00:35:47.860 --> 00:35:50.460]   but it will really change the way that we work
[00:35:50.460 --> 00:35:52.980]   and give people all these creative tools
[00:35:52.980 --> 00:35:56.220]   to do different things that they, yeah.
[00:35:56.220 --> 00:35:59.740]   I think it's gonna be, it's gonna really enable people
[00:35:59.740 --> 00:36:02.500]   to do the things that they want a lot more is my view.
[00:36:02.500 --> 00:36:04.940]   - Okay, so maybe not overnight,
[00:36:04.940 --> 00:36:07.740]   but is it your view that like on a cosmic scale,
[00:36:07.740 --> 00:36:09.780]   if you think like humans evolved
[00:36:09.780 --> 00:36:12.180]   and then like AI happened and then they like went out
[00:36:12.180 --> 00:36:15.260]   through the galaxy or maybe it takes many decades,
[00:36:15.260 --> 00:36:17.940]   maybe it takes a century, but like,
[00:36:17.940 --> 00:36:19.500]   is that like the grand scheme
[00:36:19.500 --> 00:36:21.460]   of what's happening right now in history?
[00:36:21.460 --> 00:36:23.780]   - Sorry, in what sense?
[00:36:23.780 --> 00:36:24.860]   I mean- - In the sense that
[00:36:24.860 --> 00:36:26.500]   there were other technologies like computers
[00:36:26.500 --> 00:36:29.060]   and even like fire, but like the AI happening
[00:36:29.060 --> 00:36:30.580]   is as significant as like humans evolving
[00:36:30.580 --> 00:36:31.540]   in the first place.
[00:36:31.540 --> 00:36:34.380]   - I think that's tricky.
[00:36:34.380 --> 00:36:39.380]   I think people like to, I mean, the history of humanity,
[00:36:39.540 --> 00:36:43.540]   I think has been people basically,
[00:36:43.540 --> 00:36:47.900]   thinking that certain aspects of humanity
[00:36:47.900 --> 00:36:51.900]   are like really unique in different ways.
[00:36:51.900 --> 00:36:56.620]   And then coming to grips with the fact that that's not true,
[00:36:56.620 --> 00:36:59.380]   but humanity is actually still super special, right?
[00:36:59.380 --> 00:37:04.020]   So it's like, we thought that the earth
[00:37:04.020 --> 00:37:06.060]   was the center of the universe and it's like, it's not,
[00:37:06.060 --> 00:37:09.900]   but like it's like humans are still pretty awesome, right?
[00:37:09.900 --> 00:37:11.620]   And pretty unique.
[00:37:11.620 --> 00:37:15.580]   I think that another bias that people tend to have
[00:37:15.580 --> 00:37:18.420]   is thinking that intelligence is somehow
[00:37:18.420 --> 00:37:24.060]   kind of fundamentally connected to life.
[00:37:24.060 --> 00:37:27.020]   And it's not actually clear that it is, right?
[00:37:27.020 --> 00:37:29.340]   I think like people think that,
[00:37:29.340 --> 00:37:34.020]   I mean, I don't know that we have a clear enough definition
[00:37:34.020 --> 00:37:39.020]   of consciousness or life to kind of fully interrogate this,
[00:37:39.020 --> 00:37:43.580]   but I know there's all this science fiction about, okay,
[00:37:43.580 --> 00:37:46.900]   you create intelligence and now it like starts taking on
[00:37:46.900 --> 00:37:50.420]   all these human-like behaviors and things like that.
[00:37:50.420 --> 00:37:52.660]   But I actually think that the current incarnation
[00:37:52.660 --> 00:37:54.020]   of all this stuff, at least,
[00:37:54.020 --> 00:37:55.500]   kind of feels like it's going in a direction
[00:37:55.500 --> 00:37:57.860]   where intelligence can be pretty separated
[00:37:57.860 --> 00:38:01.300]   from consciousness and agency and things like that,
[00:38:01.300 --> 00:38:05.100]   that I think just makes it a super valuable tool.
[00:38:05.100 --> 00:38:06.100]   So I don't know.
[00:38:06.100 --> 00:38:08.260]   I mean, obviously it's very difficult to predict
[00:38:08.260 --> 00:38:10.420]   what direction this stuff goes in over time,
[00:38:10.420 --> 00:38:14.220]   which is why I don't think anyone should be dogmatic
[00:38:14.220 --> 00:38:17.460]   about how they plan to develop it or what they plan to do.
[00:38:17.460 --> 00:38:19.780]   I think you want to kind of look at like each release.
[00:38:19.780 --> 00:38:22.100]   You know, it's like, we're obviously very pro open source,
[00:38:22.100 --> 00:38:23.620]   but I haven't committed that we're going to like release
[00:38:23.620 --> 00:38:25.060]   every single thing that we do,
[00:38:25.060 --> 00:38:29.260]   but it's basically, I'm just generally very inclined
[00:38:29.260 --> 00:38:31.620]   to thinking that open sourcing it is going to be good
[00:38:31.620 --> 00:38:33.980]   for the community and also good for us, right?
[00:38:33.980 --> 00:38:36.660]   'Cause we'll benefit from the innovations.
[00:38:36.660 --> 00:38:40.580]   But if at some point, like there's some qualitative change
[00:38:40.580 --> 00:38:42.540]   in what the thing is capable of,
[00:38:42.540 --> 00:38:44.540]   and we feel like it's just not responsible
[00:38:44.540 --> 00:38:46.100]   to open source it, then we won't.
[00:38:46.100 --> 00:38:49.100]   But, so I don't know.
[00:38:49.100 --> 00:38:51.140]   It's all very difficult to predict.
[00:38:51.140 --> 00:38:52.140]   - Yeah.
[00:38:52.140 --> 00:38:55.340]   What is a kind of qualitative change, like a specific thing?
[00:38:55.340 --> 00:38:57.340]   You're training Llama Phi, Llama Four,
[00:38:57.340 --> 00:38:59.940]   and you've seen this and like, you know what?
[00:38:59.940 --> 00:39:01.580]   I'm not sure about open sourcing it.
[00:39:01.580 --> 00:39:06.980]   - I think that that, it's a little hard to answer that
[00:39:06.980 --> 00:39:11.020]   in the abstract because there are negative behaviors
[00:39:11.020 --> 00:39:13.540]   that any product can exhibit that as long
[00:39:13.540 --> 00:39:17.620]   as you can mitigate it, it's like, it's okay, right?
[00:39:17.620 --> 00:39:20.820]   So, I mean, there's bad things about social media
[00:39:20.820 --> 00:39:22.260]   that we work to mitigate, right?
[00:39:22.260 --> 00:39:24.180]   There's bad things about Llama Two
[00:39:24.180 --> 00:39:26.220]   that we spend a lot of time trying to make sure
[00:39:26.220 --> 00:39:28.540]   that it's not like, you know, helping people
[00:39:28.540 --> 00:39:30.540]   commit violent acts or things like that, right?
[00:39:30.540 --> 00:39:32.540]   I mean, that doesn't mean that it's like a,
[00:39:32.540 --> 00:39:35.860]   a kind of autonomous or intelligent agent.
[00:39:35.860 --> 00:39:37.940]   It just means that it's learned a lot about the world
[00:39:37.940 --> 00:39:39.340]   and it can answer a set of questions
[00:39:39.340 --> 00:39:42.100]   that we think it would be unhelpful for it to answer.
[00:39:42.100 --> 00:39:47.260]   So, I don't know.
[00:39:47.260 --> 00:39:48.740]   I think the question isn't really
[00:39:48.740 --> 00:39:51.740]   what behaviors would it show,
[00:39:51.740 --> 00:39:54.100]   it's what things would we not be able to mitigate
[00:39:54.100 --> 00:39:55.460]   after it shows that?
[00:39:55.460 --> 00:40:00.460]   And I don't know, I think that there's so many ways
[00:40:00.460 --> 00:40:03.620]   in which something can be good or bad
[00:40:03.620 --> 00:40:05.500]   that it's hard to actually enumerate them all up front.
[00:40:05.500 --> 00:40:07.540]   If you even look at like what we've had to deal with
[00:40:07.540 --> 00:40:12.060]   in social media and like the different types of harms,
[00:40:12.060 --> 00:40:13.340]   we've basically gotten to, it's like,
[00:40:13.340 --> 00:40:16.380]   there's like 18 or 19 categories of harmful things
[00:40:16.380 --> 00:40:19.980]   that people do and we've basically built AI systems
[00:40:19.980 --> 00:40:22.220]   to try to go identify what those things are
[00:40:22.220 --> 00:40:23.660]   that people are doing and try to make sure
[00:40:23.660 --> 00:40:26.540]   that that doesn't happen on our network as much as possible.
[00:40:26.540 --> 00:40:28.980]   So, yeah, I think you can, over time,
[00:40:28.980 --> 00:40:30.780]   I think you'll be able to break down
[00:40:30.780 --> 00:40:33.300]   this into more of a taxonomy too.
[00:40:33.300 --> 00:40:34.940]   And I think that this is a thing
[00:40:34.940 --> 00:40:36.180]   that we spend time researching too
[00:40:36.180 --> 00:40:38.340]   'cause we wanna make sure that we understand that.
[00:40:38.340 --> 00:40:40.140]   - So, one of the things I asked Mark
[00:40:40.140 --> 00:40:44.220]   is what industrial scale use of LLMs would look like.
[00:40:44.220 --> 00:40:46.140]   You see this in previous technological revolutions
[00:40:46.140 --> 00:40:48.500]   where at first they're thinking in a very small scale way
[00:40:48.500 --> 00:40:49.660]   about what's enabled.
[00:40:49.660 --> 00:40:52.260]   And I think that's what chatbots might be for LLMs.
[00:40:52.260 --> 00:40:54.020]   And I think the large scale use case
[00:40:54.020 --> 00:40:56.580]   might look something like what V7 GO is.
[00:40:56.580 --> 00:40:58.540]   And by the way, it's made by V7 Labs
[00:40:58.540 --> 00:41:00.300]   who's sponsoring this episode.
[00:41:00.300 --> 00:41:02.300]   So, it's like a spreadsheet.
[00:41:02.300 --> 00:41:06.260]   You put in raw information like documents, images, whatever,
[00:41:06.260 --> 00:41:09.660]   and they become rows and the columns are populated
[00:41:09.660 --> 00:41:11.260]   by an LLM of your choice.
[00:41:11.260 --> 00:41:13.780]   And in fact, I used it to prepare for Mark.
[00:41:13.780 --> 00:41:16.340]   So, I fed in a bunch of blog posts and papers
[00:41:16.340 --> 00:41:18.060]   from Meta's AI research.
[00:41:18.060 --> 00:41:20.220]   And as you can see, if you're on YouTube,
[00:41:20.220 --> 00:41:23.460]   it summarizes and extracts exactly the information I want
[00:41:23.460 --> 00:41:24.460]   as columns.
[00:41:24.460 --> 00:41:26.700]   And obviously mine is a small use case.
[00:41:26.700 --> 00:41:28.340]   But you can imagine, for example,
[00:41:28.340 --> 00:41:29.620]   a company like FedEx
[00:41:29.620 --> 00:41:32.180]   has to process half a million documents a day.
[00:41:32.180 --> 00:41:34.260]   Obviously a chatbot can't do that.
[00:41:34.260 --> 00:41:35.420]   A spreadsheet can,
[00:41:35.420 --> 00:41:37.580]   because this is just like a fire hose of intelligence
[00:41:37.580 --> 00:41:38.620]   in there, right?
[00:41:38.620 --> 00:41:40.260]   Anyways, you can learn more about them
[00:41:40.260 --> 00:41:44.260]   at v7labs.com/go or the link in the description.
[00:41:44.260 --> 00:41:45.340]   Back to Mark.
[00:41:45.340 --> 00:41:48.660]   - Yeah, like it seems to me it would be a good idea.
[00:41:48.660 --> 00:41:50.100]   I would be disappointed in a future
[00:41:50.100 --> 00:41:51.860]   where AI systems aren't broadly deployed
[00:41:51.860 --> 00:41:54.220]   and everybody doesn't have access to them.
[00:41:54.220 --> 00:41:55.100]   At the same time,
[00:41:55.100 --> 00:41:58.140]   I wanna better understand the mitigations.
[00:41:58.140 --> 00:42:00.660]   'Cause if the mitigation is the fine tuning,
[00:42:00.660 --> 00:42:02.380]   well, the whole thing about open weights
[00:42:02.380 --> 00:42:05.780]   is that you can then remove the fine tuning,
[00:42:05.780 --> 00:42:08.020]   which is often superficial on top of these capabilities.
[00:42:08.020 --> 00:42:10.420]   Like, if it's like talking on Slack
[00:42:10.420 --> 00:42:12.140]   with a biology researcher.
[00:42:12.140 --> 00:42:14.060]   And again, I think like models are very far from this.
[00:42:14.060 --> 00:42:16.220]   They're right now, they're like Google search.
[00:42:16.220 --> 00:42:18.100]   But it's like, I can show them my Petri dish
[00:42:18.100 --> 00:42:18.940]   and they can explain like,
[00:42:18.940 --> 00:42:21.860]   here's why you're a smallpox sample didn't grow.
[00:42:21.860 --> 00:42:23.540]   Here's what to change.
[00:42:23.540 --> 00:42:24.820]   How do you mitigate that?
[00:42:24.820 --> 00:42:27.940]   'Cause somebody can just like fine tune that in there, right?
[00:42:27.940 --> 00:42:30.740]   - Yeah, I mean, that's true.
[00:42:30.740 --> 00:42:32.900]   I think a lot of people will basically use
[00:42:32.900 --> 00:42:34.940]   the off the shelf model
[00:42:34.940 --> 00:42:38.380]   and some people who have basically bad faith
[00:42:38.380 --> 00:42:40.700]   are going to try to strip out all the bad stuff.
[00:42:40.700 --> 00:42:42.460]   So I do think that that's an issue.
[00:42:42.460 --> 00:42:46.820]   The flip side of this is that,
[00:42:46.820 --> 00:42:48.060]   and this is one of the reasons
[00:42:48.060 --> 00:42:52.260]   why I'm kind of philosophically so pro open source
[00:42:52.260 --> 00:42:57.260]   is I do think that a concentration of AI in the future
[00:42:57.260 --> 00:43:01.020]   has the potential to be as dangerous
[00:43:01.020 --> 00:43:03.900]   as kind of it being widespread.
[00:43:03.900 --> 00:43:05.740]   So I think a lot of people are,
[00:43:05.740 --> 00:43:07.340]   they think about the questions of,
[00:43:07.340 --> 00:43:08.620]   okay, well, if we can do this stuff,
[00:43:08.620 --> 00:43:10.180]   is it bad for it to be out wild?
[00:43:10.180 --> 00:43:12.980]   Like just in kind of widely available.
[00:43:15.100 --> 00:43:16.940]   I think another version of this is like,
[00:43:16.940 --> 00:43:20.060]   okay, well, it's probably also pretty bad
[00:43:20.060 --> 00:43:24.980]   for one institution to have an AI
[00:43:24.980 --> 00:43:28.060]   that is way more powerful than everyone else's AI, right?
[00:43:28.060 --> 00:43:29.500]   So if you look at like,
[00:43:29.500 --> 00:43:32.620]   like I guess one security analogy that I think of is,
[00:43:32.620 --> 00:43:38.420]   you know, it doesn't take AI to basically,
[00:43:38.420 --> 00:43:41.340]   okay, there's security holes in so many different things.
[00:43:41.340 --> 00:43:44.940]   And if you could travel back in time a year or two years,
[00:43:44.940 --> 00:43:47.100]   right, it's like, that's not AI.
[00:43:47.100 --> 00:43:49.020]   It's like, you just, let's say you just have like one year
[00:43:49.020 --> 00:43:51.740]   or two years more knowledge of the security holes.
[00:43:51.740 --> 00:43:53.860]   It's pretty much hack into like any system, right?
[00:43:53.860 --> 00:43:56.900]   So it's not that far fetched to believe
[00:43:56.900 --> 00:43:59.700]   that a very intelligent AI
[00:43:59.700 --> 00:44:02.460]   would probably be able to identify some holes
[00:44:02.460 --> 00:44:04.260]   and basically be like a human
[00:44:04.260 --> 00:44:06.140]   who could potentially go back in time a year or two
[00:44:06.140 --> 00:44:07.340]   and compromise all these systems.
[00:44:07.340 --> 00:44:09.980]   Okay, so how have we dealt with that as a society?
[00:44:09.980 --> 00:44:13.420]   Well, one big part is open source software
[00:44:13.420 --> 00:44:14.940]   that makes it so that when improvements
[00:44:14.940 --> 00:44:16.260]   are made to the software,
[00:44:16.260 --> 00:44:17.820]   it doesn't just kind of get stuck
[00:44:17.820 --> 00:44:19.620]   in one company's products,
[00:44:19.620 --> 00:44:22.220]   but it can kind of be broadly deployed
[00:44:22.220 --> 00:44:23.620]   to a lot of different systems,
[00:44:23.620 --> 00:44:26.300]   whether it's banks or hospitals or government stuff.
[00:44:26.300 --> 00:44:28.260]   And like, just everyone can kind of,
[00:44:28.260 --> 00:44:30.340]   like as the software gets hardened,
[00:44:30.340 --> 00:44:31.980]   which happens because more people can see it
[00:44:31.980 --> 00:44:33.540]   and more people can bang on it.
[00:44:33.540 --> 00:44:36.100]   And there are standards on how this stuff works.
[00:44:36.100 --> 00:44:40.100]   The world can kind of get upgraded together pretty quickly.
[00:44:40.100 --> 00:44:43.780]   And I kind of think that a world where AI
[00:44:43.780 --> 00:44:46.380]   is very widely deployed in a way
[00:44:46.380 --> 00:44:50.220]   where it's gotten hardened progressively over time
[00:44:50.220 --> 00:44:54.460]   and is one where all the different systems will be in check
[00:44:54.460 --> 00:44:56.980]   in a way that seems like it is fundamentally
[00:44:56.980 --> 00:45:00.300]   more healthy to me than one where this is more concentrated.
[00:45:00.300 --> 00:45:03.060]   So there are risks on all sides,
[00:45:03.060 --> 00:45:07.660]   but I think that that's one risk that I think people,
[00:45:07.660 --> 00:45:09.340]   I don't hear them talking about quite as much.
[00:45:09.340 --> 00:45:11.740]   I think like there's sort of the risk of like,
[00:45:11.740 --> 00:45:14.140]   okay, well, what if the AI system does something bad?
[00:45:14.140 --> 00:45:18.540]   I am more like, you know, I stay up at night more worrying,
[00:45:18.540 --> 00:45:22.380]   well, what if like some actor that, whatever,
[00:45:22.380 --> 00:45:23.580]   it's like from wherever you sit,
[00:45:23.580 --> 00:45:25.500]   there's gonna be some actor who you don't trust.
[00:45:25.500 --> 00:45:27.700]   If they're the ones who have like the super strong AI,
[00:45:27.700 --> 00:45:29.580]   whether it's some like other government
[00:45:29.580 --> 00:45:33.060]   that is sort of like an opponent of our country
[00:45:33.060 --> 00:45:36.100]   or some company that you don't trust or whatever it is.
[00:45:37.780 --> 00:45:42.780]   Like, I think that that's potentially a much bigger risk.
[00:45:42.780 --> 00:45:47.500]   - As in, they could like overthrow our government
[00:45:47.500 --> 00:45:50.220]   because they have a weapon that like nobody else has?
[00:45:50.220 --> 00:45:52.100]   - Cause a lot of mayhem, right?
[00:45:52.100 --> 00:45:56.220]   I think it's like, I mean, I think the intuition
[00:45:56.220 --> 00:45:59.020]   is that this stuff ends up being pretty kind of important
[00:45:59.020 --> 00:46:02.540]   and valuable for both kind of economic
[00:46:02.540 --> 00:46:04.740]   and kind of security and other things.
[00:46:04.740 --> 00:46:07.460]   And I don't know, I just think, yeah,
[00:46:07.460 --> 00:46:09.620]   if like if someone who you don't trust
[00:46:09.620 --> 00:46:11.060]   or is an adversary of you,
[00:46:11.060 --> 00:46:13.020]   get something that is more powerful,
[00:46:13.020 --> 00:46:15.260]   then I think that that could be an issue.
[00:46:15.260 --> 00:46:17.500]   And I think probably the best way to mitigate that
[00:46:17.500 --> 00:46:20.500]   is to have good open source AI
[00:46:20.500 --> 00:46:23.580]   that basically becomes the standard
[00:46:23.580 --> 00:46:26.220]   and in a lot of ways kind of can become the leader.
[00:46:26.220 --> 00:46:30.180]   And in that way, it just ensures that it's a much more
[00:46:30.180 --> 00:46:33.020]   kind of even and balanced playing field.
[00:46:33.020 --> 00:46:34.460]   - Yeah, that seems plausible to me.
[00:46:34.500 --> 00:46:38.060]   And if that works out, that would be the future I prefer.
[00:46:38.060 --> 00:46:40.620]   I guess I want to understand like mechanistically
[00:46:40.620 --> 00:46:44.220]   how if somebody was gonna cause mayhem with AI systems,
[00:46:44.220 --> 00:46:46.540]   how the fact that there are other open source systems
[00:46:46.540 --> 00:46:48.180]   in the world prevents that.
[00:46:48.180 --> 00:46:50.340]   Like the specific example of like somebody
[00:46:50.340 --> 00:46:51.980]   coming with a bioweapon,
[00:46:51.980 --> 00:46:53.980]   is it just that we'll do a bunch of like R&D
[00:46:53.980 --> 00:46:54.820]   in the rest of the world
[00:46:54.820 --> 00:46:56.580]   to like figure out vaccines really fast?
[00:46:56.580 --> 00:46:57.420]   Like what's happening?
[00:46:57.420 --> 00:46:58.620]   - If you take like the computer,
[00:46:58.620 --> 00:47:00.780]   the security one that I was talking about,
[00:47:00.780 --> 00:47:03.100]   I think someone with a weaker AI trying to hack
[00:47:03.100 --> 00:47:06.180]   into a system that is like protected by a stronger AI
[00:47:06.180 --> 00:47:07.420]   will succeed less.
[00:47:07.420 --> 00:47:10.620]   Right, so I think that that's,
[00:47:10.620 --> 00:47:11.940]   I mean, that's like in terms of software security.
[00:47:11.940 --> 00:47:13.620]   - How do you know everything in the world is like that?
[00:47:13.620 --> 00:47:16.260]   Like what if bioweapons aren't like that?
[00:47:16.260 --> 00:47:18.140]   - No, I mean, I don't know that everything
[00:47:18.140 --> 00:47:19.420]   in the world is like that.
[00:47:19.420 --> 00:47:25.020]   I think that that's, I guess one of the,
[00:47:25.020 --> 00:47:26.300]   bioweapons are one of the areas
[00:47:26.300 --> 00:47:28.500]   where I think the people who are most worried
[00:47:28.500 --> 00:47:30.060]   about this stuff are focused.
[00:47:30.060 --> 00:47:32.580]   And I think that that's,
[00:47:32.580 --> 00:47:35.100]   I think it makes a lot of sense to think about that.
[00:47:35.100 --> 00:47:39.580]   And I think that there are certain mitigations
[00:47:39.580 --> 00:47:42.980]   you can try to not train certain knowledge into the model,
[00:47:42.980 --> 00:47:44.500]   right, there's different things,
[00:47:44.500 --> 00:47:48.860]   but yeah, I mean, it's some level,
[00:47:48.860 --> 00:47:51.540]   I mean, if you get a sufficiently bad actor
[00:47:51.540 --> 00:47:55.340]   and you don't have other AI that can sort of balance them
[00:47:55.340 --> 00:47:58.540]   and understand what's going on and what the threats are,
[00:47:58.540 --> 00:48:01.060]   then that could be a risk.
[00:48:01.060 --> 00:48:02.060]   So I think that that's one of the things
[00:48:02.060 --> 00:48:03.580]   that we need to watch out for.
[00:48:03.580 --> 00:48:06.740]   - Is there something you could see
[00:48:06.740 --> 00:48:08.020]   in the deployment of these systems
[00:48:08.020 --> 00:48:12.220]   where you observe like you're training a llama for,
[00:48:12.220 --> 00:48:13.740]   and it's like, it lied to you
[00:48:13.740 --> 00:48:15.620]   because it thought you weren't noticing or something.
[00:48:15.620 --> 00:48:18.740]   And you're like, whoa, what's going on here?
[00:48:18.740 --> 00:48:20.580]   Not that you, this is probably not likely
[00:48:20.580 --> 00:48:21.500]   with a llama for test system,
[00:48:21.500 --> 00:48:23.420]   but is there something you can imagine like that
[00:48:23.420 --> 00:48:26.460]   where you'd like be really concerned about deceptiveness?
[00:48:26.460 --> 00:48:29.460]   And if like billions of copies of things are out in the wild?
[00:48:29.460 --> 00:48:34.700]   - Yeah, I mean, I think that that's not necessarily,
[00:48:34.700 --> 00:48:37.740]   I mean, right now it's, we see a lot of hallucinations,
[00:48:37.740 --> 00:48:39.820]   right, so I think it's more that.
[00:48:39.820 --> 00:48:42.860]   I think it's an interesting question
[00:48:42.860 --> 00:48:43.860]   how you would tell the difference
[00:48:43.860 --> 00:48:46.500]   between a hallucination and deception.
[00:48:46.500 --> 00:48:48.820]   But yeah, look, I mean, I think that there's a lot of risks
[00:48:48.820 --> 00:48:50.140]   and things to think about.
[00:48:50.140 --> 00:48:55.140]   The flip side of all this is that there are also a lot of,
[00:48:56.260 --> 00:49:00.020]   I try to, in running our company at least,
[00:49:00.020 --> 00:49:02.580]   balance what I think of as these
[00:49:02.580 --> 00:49:05.500]   longer term theoretical risks
[00:49:05.500 --> 00:49:09.580]   with what I actually think are quite real risks
[00:49:09.580 --> 00:49:10.740]   that exist today.
[00:49:10.740 --> 00:49:14.500]   So like when you talk about deception,
[00:49:14.500 --> 00:49:16.220]   the form of that that I worry about most
[00:49:16.220 --> 00:49:18.740]   is people using this to generate misinformation
[00:49:18.740 --> 00:49:20.100]   and then like pump that through,
[00:49:20.100 --> 00:49:21.940]   whether it's our networks or others.
[00:49:21.940 --> 00:49:25.740]   So the way that we've basically combated
[00:49:25.740 --> 00:49:28.460]   a lot of this type of harmful content
[00:49:28.460 --> 00:49:30.140]   is by building AI systems
[00:49:30.140 --> 00:49:32.380]   that are smarter than the adversarial ones.
[00:49:32.380 --> 00:49:33.980]   And I guess this is part of,
[00:49:33.980 --> 00:49:36.020]   this kind of informs part of my theory on this, right,
[00:49:36.020 --> 00:49:38.140]   is if you look at like the different types of harm
[00:49:38.140 --> 00:49:42.020]   that people do or try to do through social networks,
[00:49:42.020 --> 00:49:46.820]   there are ones that are not very adversarial.
[00:49:46.820 --> 00:49:51.660]   So for example, like hate speech,
[00:49:51.660 --> 00:49:54.020]   I would say is not super adversarial
[00:49:54.020 --> 00:49:57.220]   in the sense that like people aren't getting
[00:49:57.220 --> 00:49:59.540]   better at being racist, right?
[00:49:59.540 --> 00:50:02.060]   They're just like, it's, you just like, okay,
[00:50:02.060 --> 00:50:04.660]   if you kind of, that's one where I think the AIs
[00:50:04.660 --> 00:50:08.180]   are generally just getting way more sophisticated faster
[00:50:08.180 --> 00:50:09.700]   than people are at those issues.
[00:50:09.700 --> 00:50:11.420]   So we have, and we have issues both ways.
[00:50:11.420 --> 00:50:14.340]   It's like people do bad things
[00:50:14.340 --> 00:50:17.340]   that whether they're trying to incite violence or something,
[00:50:17.340 --> 00:50:20.020]   but we also have a lot of false positives, right?
[00:50:20.020 --> 00:50:22.340]   So where we basically censor stuff that we shouldn't
[00:50:22.340 --> 00:50:25.340]   and I think understandably make a lot of people annoyed.
[00:50:25.340 --> 00:50:26.820]   So I think having an AI
[00:50:26.820 --> 00:50:29.580]   that just gets increasingly precise on that,
[00:50:29.580 --> 00:50:30.780]   that's gonna be good over time.
[00:50:30.780 --> 00:50:31.780]   But let me give you another example,
[00:50:31.780 --> 00:50:33.380]   which is like nation states trying
[00:50:33.380 --> 00:50:34.980]   to interfere in elections.
[00:50:34.980 --> 00:50:37.340]   That's an example where they're absolutely,
[00:50:37.340 --> 00:50:38.820]   they have cutting edge technology
[00:50:38.820 --> 00:50:41.500]   and absolutely get better each year.
[00:50:41.500 --> 00:50:44.740]   So we block some technique, they learn what we did,
[00:50:44.740 --> 00:50:46.780]   they come at us with a different technique, right?
[00:50:46.780 --> 00:50:50.700]   It's not like a person trying to, you know,
[00:50:50.700 --> 00:50:52.380]   say mean things, right?
[00:50:52.380 --> 00:50:55.340]   It's like, they're basically, they have a goal,
[00:50:55.340 --> 00:50:58.460]   they're sophisticated, they have a lot of technology.
[00:50:58.460 --> 00:51:00.460]   In those cases, I still think the ability
[00:51:00.460 --> 00:51:04.980]   to kind of have our AI systems grow in sophistication
[00:51:04.980 --> 00:51:07.820]   at a faster rate than theirs have, it's an arms race,
[00:51:07.820 --> 00:51:11.020]   but I think we're at least currently winning that arms race.
[00:51:11.020 --> 00:51:14.100]   So I don't know, I think that that's,
[00:51:14.100 --> 00:51:15.180]   but this is like a lot of the stuff
[00:51:15.180 --> 00:51:18.340]   that I spend time thinking about is like, okay,
[00:51:18.340 --> 00:51:22.380]   yes, it is possible that whether it's LLAMA-4
[00:51:22.380 --> 00:51:24.380]   or LLAMA-5 or LLAMA-6, yeah,
[00:51:24.380 --> 00:51:26.660]   we need to think about like what behaviors we're observing,
[00:51:26.660 --> 00:51:27.500]   and it's not just us.
[00:51:27.500 --> 00:51:28.940]   I think part of the reason why you make this open source
[00:51:28.940 --> 00:51:31.020]   is that there are a lot of other people who study this too.
[00:51:31.020 --> 00:51:34.340]   So yeah, we wanna see what other people are observing,
[00:51:34.340 --> 00:51:36.860]   what we're observing, what we can mitigate,
[00:51:36.860 --> 00:51:38.620]   and then we'll make our assessment
[00:51:38.620 --> 00:51:41.740]   on whether we can make it open source.
[00:51:41.740 --> 00:51:45.020]   But I think for the foreseeable future,
[00:51:45.020 --> 00:51:47.300]   I'm optimistic we will be able to.
[00:51:47.300 --> 00:51:51.100]   And in the near term, I don't wanna take our eye off the ball
[00:51:51.100 --> 00:51:53.060]   of what are actual bad things
[00:51:53.060 --> 00:51:55.020]   that people are trying to use the models for today,
[00:51:55.020 --> 00:51:56.260]   even if they're not existential,
[00:51:56.260 --> 00:52:00.700]   but they're like pretty bad kind of day-to-day harms
[00:52:00.700 --> 00:52:03.480]   that we're familiar with and running our services.
[00:52:03.480 --> 00:52:06.540]   That's actually a lot of what we have to,
[00:52:06.540 --> 00:52:07.820]   I think, spend our time on as well.
[00:52:07.820 --> 00:52:08.820]   - Yeah, yeah.
[00:52:08.820 --> 00:52:11.880]   Actually, I found the synthetic data thing really curious.
[00:52:11.880 --> 00:52:15.620]   I'm actually interested in why you don't think,
[00:52:15.620 --> 00:52:16.860]   like current models, it makes sense
[00:52:16.860 --> 00:52:18.220]   why there might be an asymptote
[00:52:18.220 --> 00:52:20.580]   with just doing the synthetic data again and again.
[00:52:20.580 --> 00:52:21.500]   - Yeah. - If they get smarter
[00:52:21.500 --> 00:52:23.100]   and you use the kind of techniques you talk about
[00:52:23.100 --> 00:52:25.900]   in the paper or the blog post that's coming out
[00:52:25.900 --> 00:52:26.980]   on the day this will be released,
[00:52:26.980 --> 00:52:30.220]   where it goes to the thought chain
[00:52:30.220 --> 00:52:33.180]   that is the most correct,
[00:52:33.180 --> 00:52:36.500]   why this wouldn't lead to a loop that,
[00:52:36.500 --> 00:52:37.420]   of course, it wouldn't be overnight,
[00:52:37.420 --> 00:52:38.940]   but over many months or years of training,
[00:52:38.940 --> 00:52:40.460]   potentially, with a smarter model,
[00:52:40.460 --> 00:52:42.060]   it gets smarter, makes better output,
[00:52:42.060 --> 00:52:43.420]   gets smarter, and so forth.
[00:52:45.500 --> 00:52:47.340]   - Well, I think it could within the parameter
[00:52:47.340 --> 00:52:49.660]   of whatever the model architecture is.
[00:52:49.660 --> 00:52:51.540]   It's just that at some level,
[00:52:51.540 --> 00:52:58.660]   I don't know, I think today's 8 billion parameter models,
[00:52:58.660 --> 00:53:01.780]   I just don't think you're gonna be able to get
[00:53:01.780 --> 00:53:05.020]   to be as good as the state-of-the-art
[00:53:05.020 --> 00:53:07.260]   multi-hundred billion parameter models
[00:53:07.260 --> 00:53:08.780]   that are incorporating new research
[00:53:08.780 --> 00:53:10.480]   into the architecture itself.
[00:53:10.480 --> 00:53:14.960]   - But those will be open source as well, right?
[00:53:14.960 --> 00:53:17.200]   - Well, yeah, but I think that that's,
[00:53:17.200 --> 00:53:21.020]   I mean, subject to all the questions
[00:53:21.020 --> 00:53:21.860]   that we just talked about.
[00:53:21.860 --> 00:53:23.980]   But yes, I mean, we would hope that that'll be the case.
[00:53:23.980 --> 00:53:25.500]   But I think that at each point,
[00:53:25.500 --> 00:53:29.460]   I don't know, it's like when you're building software,
[00:53:29.460 --> 00:53:32.100]   there's a ton of stuff that you can do with software,
[00:53:32.100 --> 00:53:33.860]   but then at some level, you're constrained
[00:53:33.860 --> 00:53:36.700]   by the chips that it's running on, right?
[00:53:36.700 --> 00:53:41.700]   So there are always gonna be different physical constraints,
[00:53:41.700 --> 00:53:44.140]   and it's like how big are the models
[00:53:44.140 --> 00:53:46.740]   is gonna be constrained by how much energy you can get
[00:53:46.740 --> 00:53:50.880]   and use for inference.
[00:53:50.880 --> 00:53:55.880]   So I guess I'm simultaneously very optimistic
[00:53:55.880 --> 00:53:59.060]   that this stuff will continue to improve quickly
[00:53:59.060 --> 00:54:02.060]   and also a little more measured
[00:54:02.060 --> 00:54:09.660]   than I think some people are about kind of it's,
[00:54:09.660 --> 00:54:12.840]   I just don't think the runaway case
[00:54:12.840 --> 00:54:15.260]   is like a particularly likely one.
[00:54:15.260 --> 00:54:18.440]   - I think it makes sense to keep your options open.
[00:54:18.440 --> 00:54:20.640]   Like there's so much we don't know.
[00:54:20.640 --> 00:54:21.520]   There's a case in which like,
[00:54:21.520 --> 00:54:23.240]   it's really important to keep the balance of power.
[00:54:23.240 --> 00:54:25.320]   So when nobody becomes like a totalitarian dictator,
[00:54:25.320 --> 00:54:26.680]   there's a case in which like,
[00:54:26.680 --> 00:54:28.720]   you don't wanna open source the architecture
[00:54:28.720 --> 00:54:32.360]   because like China can use it to catch up to America's AIs.
[00:54:32.360 --> 00:54:34.040]   And like there is an intelligence explosion
[00:54:34.040 --> 00:54:35.880]   and they like win that.
[00:54:35.880 --> 00:54:37.240]   Yeah, a lot of things seem possible,
[00:54:37.240 --> 00:54:38.720]   just like keeping your options open,
[00:54:38.720 --> 00:54:40.880]   considering all of them seems reasonable.
[00:54:40.880 --> 00:54:41.720]   - Yeah.
[00:54:42.680 --> 00:54:44.160]   Let's talk about some other things.
[00:54:44.160 --> 00:54:48.680]   Okay, metaverse, what time period in human history
[00:54:48.680 --> 00:54:50.720]   would you be most interested in going into?
[00:54:50.720 --> 00:54:52.800]   A 100,000 BCE to now.
[00:54:52.800 --> 00:54:53.640]   You just wanna see what it was like.
[00:54:53.640 --> 00:54:54.800]   - It has to be the past?
[00:54:54.800 --> 00:54:55.620]   - Huh?
[00:54:55.620 --> 00:54:56.460]   - It has to be the past?
[00:54:56.460 --> 00:54:57.520]   - Oh yeah, it has to be the past.
[00:54:57.520 --> 00:55:02.720]   - I don't know.
[00:55:02.720 --> 00:55:04.520]   I mean, I have the periods of time that I'm interested.
[00:55:04.520 --> 00:55:06.320]   I mean, I'm really interested in American history
[00:55:06.320 --> 00:55:07.560]   and classical history.
[00:55:07.560 --> 00:55:11.660]   I'm really interested in the history of science too.
[00:55:11.660 --> 00:55:16.440]   So I actually think seeing and trying to understand more
[00:55:16.440 --> 00:55:21.120]   about how some of the big advances came about.
[00:55:21.120 --> 00:55:24.520]   I mean, all we have are like somewhat limited writings
[00:55:24.520 --> 00:55:26.160]   about some of that stuff.
[00:55:26.160 --> 00:55:27.720]   I'm not sure the metaverse is gonna let you do that.
[00:55:27.720 --> 00:55:32.400]   'Cause I mean, it's gonna be hard to kind of go back
[00:55:32.400 --> 00:55:34.640]   in time for things that we don't have records of.
[00:55:34.640 --> 00:55:39.200]   But I'm actually not sure that going back in time
[00:55:39.200 --> 00:55:41.660]   is gonna be that important of a thing for them.
[00:55:41.660 --> 00:55:42.660]   I mean, I think it's gonna be cool
[00:55:42.660 --> 00:55:44.100]   for like history classes and stuff,
[00:55:44.100 --> 00:55:47.220]   but that's probably not the use case
[00:55:47.220 --> 00:55:49.820]   that I'm most excited about for the metaverse overall.
[00:55:49.820 --> 00:55:53.220]   I mean, I think the main thing is just the ability
[00:55:53.220 --> 00:55:55.540]   to feel present with people no matter where you are.
[00:55:55.540 --> 00:55:56.860]   I think that that's gonna be killer.
[00:55:56.860 --> 00:56:01.860]   I mean, in the AI conversation that we're having,
[00:56:01.860 --> 00:56:06.820]   I mean, so much of it is about physical constraints
[00:56:06.820 --> 00:56:09.220]   that kind of underlie all of this, right?
[00:56:09.220 --> 00:56:12.460]   And you wanna move, I mean, one lesson of technology
[00:56:12.460 --> 00:56:15.740]   is you wanna move things from the physical constraint realm
[00:56:15.740 --> 00:56:17.420]   into software as much as possible,
[00:56:17.420 --> 00:56:21.300]   because software is so much easier to build and evolve.
[00:56:21.300 --> 00:56:22.940]   And like, you can democratize it more
[00:56:22.940 --> 00:56:25.700]   because like not everyone is gonna have a data center,
[00:56:25.700 --> 00:56:29.020]   but like a lot of people can kind of write code
[00:56:29.020 --> 00:56:31.200]   and take open source code and modify it.
[00:56:31.200 --> 00:56:35.420]   The metaverse version of this
[00:56:35.420 --> 00:56:39.660]   is I think enabling realistic digital presence
[00:56:39.660 --> 00:56:43.860]   is going to be just an absolutely huge difference
[00:56:43.860 --> 00:56:48.340]   for making it so that people don't feel like
[00:56:48.340 --> 00:56:51.700]   they have to physically be together for as many things.
[00:56:51.700 --> 00:56:52.940]   Now, I mean, I think that there are gonna be things
[00:56:52.940 --> 00:56:55.220]   that are better about being physically together.
[00:56:55.220 --> 00:56:58.220]   So it's not, I mean, these things aren't binary.
[00:56:58.220 --> 00:56:59.140]   It's not gonna be like, okay,
[00:56:59.140 --> 00:57:01.040]   now you don't need to do that anymore.
[00:57:01.040 --> 00:57:06.040]   But overall, I mean, I think that this,
[00:57:06.040 --> 00:57:09.880]   it's just gonna be really powerful for socializing,
[00:57:09.880 --> 00:57:13.720]   for feeling connected with people, for working,
[00:57:13.720 --> 00:57:17.240]   for, I don't know, parts of industry,
[00:57:17.240 --> 00:57:20.160]   for medicine, for like so many things.
[00:57:20.160 --> 00:57:21.280]   I wanna go back to something you said
[00:57:21.280 --> 00:57:22.360]   at the beginning of the conversation
[00:57:22.360 --> 00:57:25.160]   where you didn't sell the company for a billion dollars
[00:57:25.160 --> 00:57:26.800]   and like the metaverse, you knew we were gonna do this
[00:57:26.800 --> 00:57:29.680]   even though the market was hammering you for it.
[00:57:29.680 --> 00:57:31.120]   And then I'm actually curious,
[00:57:31.120 --> 00:57:32.960]   like what is the source of that edge?
[00:57:32.960 --> 00:57:35.540]   And you said like, oh, values, I have this intuition,
[00:57:35.540 --> 00:57:37.080]   but like everybody says that, right?
[00:57:37.080 --> 00:57:39.360]   Like if you had to say something that's specific to you,
[00:57:39.360 --> 00:57:41.440]   what is, how would you express what that is?
[00:57:41.440 --> 00:57:44.000]   Like why were you so convinced about the metaverse?
[00:57:44.000 --> 00:57:51.560]   - Well, I think that those are different questions.
[00:57:51.560 --> 00:57:56.560]   So, I mean, what are the things that kind of power me?
[00:57:58.900 --> 00:58:00.100]   I think we've talked about a bunch of the things.
[00:58:00.100 --> 00:58:03.600]   So it's, I mean, I just really like building things.
[00:58:03.600 --> 00:58:07.180]   I specifically like building things
[00:58:07.180 --> 00:58:09.420]   around how people communicate
[00:58:09.420 --> 00:58:11.820]   and sort of understanding how people express themselves
[00:58:11.820 --> 00:58:12.660]   and how people work, right?
[00:58:12.660 --> 00:58:13.860]   And so when I was in college,
[00:58:13.860 --> 00:58:16.780]   I studied computer science and psychology.
[00:58:16.780 --> 00:58:17.960]   I think a lot of other people in the industry
[00:58:17.960 --> 00:58:19.620]   studied computer science, right?
[00:58:19.620 --> 00:58:23.780]   So it's always been sort of the intersection
[00:58:23.780 --> 00:58:25.300]   of those two things for me.
[00:58:25.300 --> 00:58:30.300]   But I think it's also sort of this like really deep drive.
[00:58:30.300 --> 00:58:33.300]   I don't know how to explain it,
[00:58:33.300 --> 00:58:37.300]   but I just feel like in the constitutionally,
[00:58:37.300 --> 00:58:39.420]   like I'm doing something wrong
[00:58:39.420 --> 00:58:41.980]   if I'm not building something new, right?
[00:58:41.980 --> 00:58:46.980]   And so I think that there's like,
[00:58:46.980 --> 00:58:52.340]   you know, even when we're putting together the business case
[00:58:52.340 --> 00:58:56.460]   for, you know, investing like a hundred billion dollars
[00:58:56.460 --> 00:58:59.540]   in AI or some huge amount in the metaverse,
[00:58:59.540 --> 00:59:01.740]   it's like, yeah, I mean, we have plans
[00:59:01.740 --> 00:59:03.020]   that I think make it pretty clear
[00:59:03.020 --> 00:59:05.580]   that if our stuff works, it'll be a good investment.
[00:59:05.580 --> 00:59:09.100]   But like, you can't know for certain from the outset.
[00:59:09.100 --> 00:59:12.220]   And so there's all these arguments that people have,
[00:59:12.220 --> 00:59:14.100]   you know, whether it's like, you know,
[00:59:14.100 --> 00:59:16.340]   with advisors or different folks,
[00:59:16.340 --> 00:59:17.900]   it's like, well, how could you,
[00:59:17.900 --> 00:59:20.500]   like how are you confident enough to do this?
[00:59:20.500 --> 00:59:23.900]   And it's like, well, the day I stop trying
[00:59:23.900 --> 00:59:26.420]   to build new things, I'm just done.
[00:59:26.420 --> 00:59:28.380]   I'm gonna go build new things somewhere else, right?
[00:59:28.380 --> 00:59:31.020]   It's like, it's like it is,
[00:59:31.020 --> 00:59:36.020]   I'm fundamentally incapable of running something
[00:59:36.020 --> 00:59:41.700]   or in my own life and like not trying to build new things
[00:59:41.700 --> 00:59:43.020]   that I think are interesting.
[00:59:43.020 --> 00:59:45.140]   It's like, that's not even a question for me, right?
[00:59:45.140 --> 00:59:47.820]   It's like whether we're gonna go take a swing
[00:59:47.820 --> 00:59:49.180]   at like building the next thing.
[00:59:49.180 --> 00:59:53.300]   It's like, it's like, I'm just incapable of not doing that.
[00:59:53.300 --> 00:59:59.900]   And I don't know, and I'm kind of like this
[00:59:59.900 --> 01:00:01.940]   in like all the different aspects of my life, right?
[01:00:01.940 --> 01:00:06.220]   It's like, we built this like family built this ranch
[01:00:06.220 --> 01:00:10.700]   in Kauai and like, I just like worked,
[01:00:10.700 --> 01:00:11.980]   like design all these buildings.
[01:00:11.980 --> 01:00:15.420]   I'm like kind of trying to like, we started raising cattle
[01:00:15.420 --> 01:00:16.740]   and I'm like, all right, well, I wanna make
[01:00:16.740 --> 01:00:18.340]   like the best cattle in the world, right?
[01:00:18.340 --> 01:00:20.460]   So it's like, how do we like, how do we architect this
[01:00:20.460 --> 01:00:22.260]   so that way we can figure this out and like,
[01:00:22.260 --> 01:00:24.140]   and build and call the stuff up
[01:00:24.140 --> 01:00:25.700]   that we need to try to do that.
[01:00:25.700 --> 01:00:28.220]   So I don't know, that's me.
[01:00:28.220 --> 01:00:32.180]   What was the other part of the question?
[01:00:32.180 --> 01:00:35.140]   - Look, Meta is just a really amazing tech company, right?
[01:00:35.140 --> 01:00:37.100]   They have all these great software engineers
[01:00:37.100 --> 01:00:40.020]   and even they work with Stripe to handle payments.
[01:00:40.020 --> 01:00:41.900]   And I think that's just a really notable fact
[01:00:41.900 --> 01:00:43.980]   that Stripe's ability to engineer
[01:00:43.980 --> 01:00:46.340]   these checkout experiences is so good
[01:00:46.340 --> 01:00:49.980]   that big companies like Ford, Zoom, Meta, even OpenAI,
[01:00:49.980 --> 01:00:52.100]   they work with Stripe to handle payments.
[01:00:52.100 --> 01:00:52.940]   Because just think about
[01:00:52.940 --> 01:00:54.980]   how many different possibilities you have to handle.
[01:00:54.980 --> 01:00:56.060]   If you're in a different country,
[01:00:56.060 --> 01:00:56.980]   you'll pay a different way.
[01:00:56.980 --> 01:00:58.580]   And if you're buying a certain kind of item,
[01:00:58.580 --> 01:01:00.540]   that might affect how you decide to pay.
[01:01:00.540 --> 01:01:03.980]   And Stripe is able to test these fine-grained optimizations
[01:01:03.980 --> 01:01:06.500]   across tens of billions of transactions a day
[01:01:06.500 --> 01:01:08.660]   to figure out what will convert people.
[01:01:08.660 --> 01:01:11.420]   And obviously conversion means more revenue for you.
[01:01:11.420 --> 01:01:13.880]   And look, I'm not a big company like Meta or anything,
[01:01:13.880 --> 01:01:15.460]   but I've been using Stripe since long
[01:01:15.460 --> 01:01:16.940]   before they were advertisers.
[01:01:16.940 --> 01:01:18.780]   Stripe Atlas was just the easiest way
[01:01:18.780 --> 01:01:20.260]   for me to set up an LLC,
[01:01:20.260 --> 01:01:22.360]   and they have these payments and invoicing features
[01:01:22.360 --> 01:01:24.020]   that make it super convenient for me
[01:01:24.020 --> 01:01:26.180]   to get money from advertisers.
[01:01:26.180 --> 01:01:27.180]   And obviously without that,
[01:01:27.180 --> 01:01:28.240]   it would have been much harder
[01:01:28.240 --> 01:01:30.180]   for me to earn money from the podcast.
[01:01:30.180 --> 01:01:31.700]   And so it's been great for me.
[01:01:31.700 --> 01:01:33.900]   Go to stripe.com to learn more.
[01:01:33.900 --> 01:01:35.620]   Thanks to them for sponsoring the episode.
[01:01:35.620 --> 01:01:37.320]   Now back to Mark.
[01:01:37.320 --> 01:01:38.160]   - I'm not sure,
[01:01:38.160 --> 01:01:39.460]   but I'm actually curious about something else,
[01:01:39.460 --> 01:01:43.940]   which is, so the 19-year-old Mark
[01:01:43.940 --> 01:01:46.540]   reads a bunch of antiquity and classics,
[01:01:46.540 --> 01:01:48.060]   high school, college.
[01:01:48.060 --> 01:01:49.620]   What important lesson did you learn from it?
[01:01:49.620 --> 01:01:50.700]   Not just interesting things you found,
[01:01:50.700 --> 01:01:52.980]   but there aren't that many tokens
[01:01:52.980 --> 01:01:54.300]   you've consumed by the time you're 19.
[01:01:54.300 --> 01:01:55.860]   A bunch of them were about the classics.
[01:01:55.860 --> 01:01:57.060]   Clearly that was important in some way.
[01:01:57.060 --> 01:01:58.480]   - There aren't that many tokens you've consumed.
[01:01:58.480 --> 01:02:00.560]   (laughs)
[01:02:00.560 --> 01:02:07.340]   - I don't know, that's a good question.
[01:02:07.340 --> 01:02:08.180]   I mean, one of the things
[01:02:08.180 --> 01:02:09.980]   that I thought was really fascinating
[01:02:11.500 --> 01:02:15.300]   is so when Augustus was first,
[01:02:15.300 --> 01:02:20.500]   so he became emperor
[01:02:20.500 --> 01:02:24.580]   and he was trying to establish peace.
[01:02:24.580 --> 01:02:29.580]   And there was no real conception of peace at the time.
[01:02:29.580 --> 01:02:32.940]   Like the people's understanding of peace
[01:02:32.940 --> 01:02:35.600]   was it is the temporary time
[01:02:35.600 --> 01:02:39.060]   between when your enemies will inevitably attack you again.
[01:02:39.060 --> 01:02:40.960]   So you get like a short rest.
[01:02:40.960 --> 01:02:43.740]   And he had this view, which is like, look,
[01:02:43.740 --> 01:02:45.900]   we want to change the economy
[01:02:45.900 --> 01:02:48.220]   from instead of being so mercenary
[01:02:48.220 --> 01:02:51.040]   and kind of militaristic
[01:02:51.040 --> 01:02:54.860]   to actually this positive something.
[01:02:54.860 --> 01:02:58.200]   It's like a very novel idea at the time.
[01:02:58.200 --> 01:03:03.380]   I don't know.
[01:03:03.380 --> 01:03:04.220]   I think that there's something
[01:03:04.220 --> 01:03:06.200]   that's just really fundamental about that.
[01:03:06.200 --> 01:03:08.140]   It's like in terms of the bounds
[01:03:08.140 --> 01:03:10.900]   on what people can conceive at the time
[01:03:10.900 --> 01:03:13.740]   of like what are rational ways to work.
[01:03:13.740 --> 01:03:17.460]   And I don't know, I'm going back to like,
[01:03:17.460 --> 01:03:19.700]   I mean, this applies to both the metaverse and the AI stuff,
[01:03:19.700 --> 01:03:21.580]   but like a lot of investors
[01:03:21.580 --> 01:03:23.660]   and just different people just can't wrap their head around
[01:03:23.660 --> 01:03:25.540]   why we would open source this.
[01:03:25.540 --> 01:03:29.540]   And it's like, I don't understand.
[01:03:29.540 --> 01:03:30.460]   It's like open source,
[01:03:30.460 --> 01:03:32.260]   that must just be like the temporary time
[01:03:32.260 --> 01:03:35.300]   between which you're making things proprietary, right?
[01:03:35.300 --> 01:03:38.540]   And it's, but I actually think it's like
[01:03:38.540 --> 01:03:42.380]   this very profound thing in tech
[01:03:42.380 --> 01:03:46.660]   that has actually, it creates a lot of winners, right?
[01:03:46.660 --> 01:03:49.420]   And it's, and so I don't know,
[01:03:49.420 --> 01:03:50.900]   I don't want to strain the analogy too much,
[01:03:50.900 --> 01:03:55.100]   but I do think that there's a lot of times,
[01:03:55.100 --> 01:03:57.040]   I think ways where you can,
[01:03:57.040 --> 01:04:02.220]   that are just like models for building things
[01:04:02.220 --> 01:04:05.100]   that people can't even like,
[01:04:05.100 --> 01:04:06.980]   they just like often can't wrap their head around
[01:04:06.980 --> 01:04:10.780]   how that would be a valuable thing for people to go do
[01:04:10.780 --> 01:04:14.660]   or like a reasonable state of the world that it's,
[01:04:14.660 --> 01:04:18.900]   I mean, it's, I think that there's more reasonable things
[01:04:18.900 --> 01:04:20.260]   than people think.
[01:04:20.260 --> 01:04:22.300]   - That's super fascinating.
[01:04:22.300 --> 01:04:24.020]   Can I give you my answer, what I was thinking?
[01:04:24.020 --> 01:04:24.860]   - Sure.
[01:04:24.860 --> 01:04:26.060]   - You might've gotten from it.
[01:04:26.060 --> 01:04:27.220]   This is probably totally off,
[01:04:27.220 --> 01:04:30.220]   but just how young some of these people are
[01:04:30.220 --> 01:04:32.660]   who have very important roles in the empire,
[01:04:32.660 --> 01:04:34.820]   like Caesar Augustus, like by the time he's 19,
[01:04:34.820 --> 01:04:36.180]   he's actually incredibly,
[01:04:36.180 --> 01:04:38.560]   one of the most prominent people in Roman politics.
[01:04:38.560 --> 01:04:39.580]   And he's like leading battles
[01:04:39.580 --> 01:04:41.260]   and forming the second triumvirate.
[01:04:41.260 --> 01:04:42.900]   I wonder if you were like the 19 year old is like,
[01:04:42.900 --> 01:04:45.100]   I can actually do this because like Caesar Augustus did this.
[01:04:45.100 --> 01:04:46.860]   - I think that's an interesting example,
[01:04:46.860 --> 01:04:50.900]   both from a lot of history and American history too.
[01:04:50.900 --> 01:04:54.860]   I mean, it's, I mean, one of my favorite quotes
[01:04:54.860 --> 01:04:57.820]   is it's this Picasso quote that all children are artists
[01:04:57.820 --> 01:04:59.660]   and the challenge is how do you remain an artist
[01:04:59.660 --> 01:05:00.700]   when you grow up?
[01:05:00.700 --> 01:05:01.900]   And it's like, basically I think,
[01:05:01.900 --> 01:05:04.260]   'cause when you're younger,
[01:05:04.260 --> 01:05:09.260]   I think it's just easier to have kind of wild ideas
[01:05:09.260 --> 01:05:11.760]   and you're not, you know, you have no,
[01:05:11.760 --> 01:05:15.300]   there are all these analogies to the innovators dilemma
[01:05:15.300 --> 01:05:18.820]   that exist in your life as well as your company
[01:05:18.820 --> 01:05:19.960]   or whatever you've built, right?
[01:05:19.960 --> 01:05:22.140]   So, you know, you're kind of earlier on your trajectory,
[01:05:22.140 --> 01:05:25.100]   it's easier to pivot and take in new ideas
[01:05:25.100 --> 01:05:26.900]   without it disrupting other commitments
[01:05:26.900 --> 01:05:29.100]   that you've made to different things.
[01:05:29.100 --> 01:05:32.060]   And so I don't know.
[01:05:32.060 --> 01:05:34.620]   I think that that's an interesting part of running a company
[01:05:34.620 --> 01:05:37.260]   is like, how do you kind of stay dynamic?
[01:05:37.260 --> 01:05:41.540]   - Going back to the investors in open source,
[01:05:41.540 --> 01:05:42.940]   the $10 billion model,
[01:05:42.940 --> 01:05:45.900]   suppose it's totally safe, you've done these evaluations
[01:05:45.900 --> 01:05:47.420]   and unlike in this case,
[01:05:47.420 --> 01:05:49.860]   the evaluators can also fine tune the model,
[01:05:49.860 --> 01:05:52.620]   which hopefully will be the case in future models.
[01:05:52.620 --> 01:05:55.540]   Would you open source that, the $10 billion model?
[01:05:55.540 --> 01:05:57.420]   - Well, I mean, as long as it's helping us, then yeah.
[01:05:57.420 --> 01:05:59.500]   - But would it, like the $10 billion of R&D
[01:05:59.500 --> 01:06:01.020]   and then now it's like open source for anybody?
[01:06:01.020 --> 01:06:03.020]   - Well, I think here's, I think a question,
[01:06:03.020 --> 01:06:06.540]   which we'll have to evaluate this as time goes on too,
[01:06:06.540 --> 01:06:11.540]   but we have a long history of open sourcing software, right?
[01:06:11.540 --> 01:06:15.720]   We don't tend to open source our product, right?
[01:06:15.720 --> 01:06:19.020]   So it's not like we don't take like the code for Instagram
[01:06:19.020 --> 01:06:19.940]   and make it open source,
[01:06:19.940 --> 01:06:23.580]   but we take like a lot of the low level infrastructure
[01:06:23.580 --> 01:06:25.700]   and we make that open source, right?
[01:06:25.700 --> 01:06:28.460]   The probably the biggest one in our history
[01:06:28.460 --> 01:06:31.360]   was Open Compute Project where we took the designs
[01:06:31.360 --> 01:06:35.740]   for kind of all of our servers and network switches
[01:06:35.740 --> 01:06:37.260]   and data centers and made it open source
[01:06:37.260 --> 01:06:38.980]   and ended up being super helpful
[01:06:38.980 --> 01:06:41.700]   because I mean, a lot of people can design servers,
[01:06:41.700 --> 01:06:43.700]   but now like the industry standardized on our design,
[01:06:43.700 --> 01:06:46.260]   which meant that the supply chains
[01:06:46.260 --> 01:06:48.060]   basically all got built out around our design,
[01:06:48.060 --> 01:06:50.060]   the volumes went up, so it got cheaper for everyone
[01:06:50.060 --> 01:06:51.700]   and saved us billions of dollars.
[01:06:51.700 --> 01:06:53.580]   So awesome, right?
[01:06:53.580 --> 01:06:56.320]   Okay, so there's multiple ways where open source
[01:06:56.320 --> 01:06:57.940]   I think could be helpful for us.
[01:06:57.940 --> 01:06:59.900]   One is if people figure out
[01:06:59.900 --> 01:07:01.500]   how to run the models more cheaply,
[01:07:01.500 --> 01:07:03.180]   well, we're gonna be spending tens
[01:07:03.180 --> 01:07:06.420]   or like a hundred billion dollars or more over time
[01:07:06.420 --> 01:07:07.700]   on all this stuff.
[01:07:07.700 --> 01:07:10.280]   So if we can do that 10% more effectively,
[01:07:10.280 --> 01:07:12.820]   we're saving billions or tens of billions of dollars.
[01:07:12.820 --> 01:07:15.020]   Okay, that's probably worth a lot by itself,
[01:07:15.020 --> 01:07:17.980]   especially if there's other competitive models out there.
[01:07:17.980 --> 01:07:19.860]   It's not like our thing is like
[01:07:19.860 --> 01:07:22.900]   be giving away some kind of crazy advantage.
[01:07:22.900 --> 01:07:26.020]   So is your view that the training will be commodified?
[01:07:27.020 --> 01:07:31.020]   - I think there's a bunch of ways that this could play out.
[01:07:31.020 --> 01:07:31.940]   That's one.
[01:07:31.940 --> 01:07:36.700]   The other is that,
[01:07:36.700 --> 01:07:39.300]   so commodity kind of implies
[01:07:39.300 --> 01:07:40.580]   that it's gonna get very cheap
[01:07:40.580 --> 01:07:43.200]   because there's lots of options.
[01:07:43.200 --> 01:07:45.500]   The other direction that this could go in
[01:07:45.500 --> 01:07:47.140]   is qualitative improvements.
[01:07:47.140 --> 01:07:49.500]   So you mentioned fine tuning, right?
[01:07:49.500 --> 01:07:51.940]   It's like right now it's pretty limited
[01:07:51.940 --> 01:07:53.660]   what you can do with fine tuning
[01:07:53.660 --> 01:07:55.380]   major other models out there.
[01:07:55.380 --> 01:07:56.540]   And there are some options,
[01:07:56.540 --> 01:07:58.740]   but generally not for the biggest models.
[01:07:58.740 --> 01:08:01.380]   So I think being able to do that
[01:08:01.380 --> 01:08:06.300]   and be able to kind of do different app specific things
[01:08:06.300 --> 01:08:07.580]   or use case specific things
[01:08:07.580 --> 01:08:09.580]   or build them into specific tool chains,
[01:08:09.580 --> 01:08:13.180]   I think will not only enable
[01:08:13.180 --> 01:08:15.740]   kind of more efficient development,
[01:08:15.740 --> 01:08:18.820]   it could enable qualitatively different things.
[01:08:18.820 --> 01:08:20.820]   Here's one analogy on this is,
[01:08:20.820 --> 01:08:24.400]   so one thing that I think generally sucks
[01:08:24.400 --> 01:08:26.180]   about the mobile ecosystem
[01:08:26.180 --> 01:08:30.260]   is that like you have these two gatekeeper companies,
[01:08:30.260 --> 01:08:32.260]   Apple and Google that can tell you
[01:08:32.260 --> 01:08:33.360]   what you're allowed to build.
[01:08:33.360 --> 01:08:35.680]   And there are lots of times in our history.
[01:08:35.680 --> 01:08:37.000]   So there's the economic version of that,
[01:08:37.000 --> 01:08:38.100]   which is like, all right, we build something
[01:08:38.100 --> 01:08:39.780]   and they're just like, I'm gonna take a bunch of your money.
[01:08:39.780 --> 01:08:43.700]   But then there's the qualitative version,
[01:08:43.700 --> 01:08:46.300]   which is actually what kind of upsets me more,
[01:08:46.300 --> 01:08:47.660]   which is there's a bunch of times
[01:08:47.660 --> 01:08:50.860]   when we've launched or wanted to launch features
[01:08:50.860 --> 01:08:53.540]   and then Apple's just like, nope, you're not launching that.
[01:08:53.540 --> 01:08:55.540]   I was like, that sucks, right?
[01:08:55.540 --> 01:09:00.100]   And so the question is what is like,
[01:09:00.100 --> 01:09:04.580]   are we kind of set up for a world like that with AI
[01:09:04.580 --> 01:09:07.580]   where like you're gonna get a handful of companies
[01:09:07.580 --> 01:09:08.920]   that run these closed models
[01:09:08.920 --> 01:09:10.820]   that are gonna be in control of the APIs
[01:09:10.820 --> 01:09:11.940]   and therefore are gonna be able to tell you
[01:09:11.940 --> 01:09:13.620]   what you can build?
[01:09:13.620 --> 01:09:16.540]   Well, for one, I can say for us,
[01:09:16.540 --> 01:09:18.900]   it is worth it to go build a model ourselves
[01:09:18.900 --> 01:09:21.100]   to make sure that we're not in that position, right?
[01:09:21.100 --> 01:09:23.780]   Like I don't want any of those other companies
[01:09:23.780 --> 01:09:26.020]   telling us what we can build.
[01:09:26.020 --> 01:09:28.180]   But from an open source perspective,
[01:09:28.180 --> 01:09:30.000]   I think a lot of developers don't want those companies
[01:09:30.000 --> 01:09:32.580]   telling them what they can build either.
[01:09:32.580 --> 01:09:35.860]   So the question is what is the ecosystem
[01:09:35.860 --> 01:09:37.020]   that gets built out around that?
[01:09:37.020 --> 01:09:38.560]   What are interesting new things?
[01:09:38.560 --> 01:09:40.560]   How much does that improve our products?
[01:09:40.560 --> 01:09:44.060]   I think that there's a lot of cases
[01:09:44.060 --> 01:09:47.420]   where if this ends up being like our databases
[01:09:47.420 --> 01:09:50.880]   or caching systems or architecture,
[01:09:50.880 --> 01:09:52.980]   we'll get valuable contributions from the community
[01:09:52.980 --> 01:09:54.180]   that'll make our stuff better.
[01:09:54.180 --> 01:09:56.500]   And then our app specific work that we do
[01:09:56.500 --> 01:09:58.080]   will still be so differentiated
[01:09:58.080 --> 01:09:59.460]   that it won't really matter, right?
[01:09:59.460 --> 01:10:01.560]   It's like, well, we'll be able to do what we do.
[01:10:01.560 --> 01:10:02.940]   We'll benefit and all the systems,
[01:10:02.940 --> 01:10:04.460]   ours and the community's will be better
[01:10:04.460 --> 01:10:06.220]   because it's open source.
[01:10:06.220 --> 01:10:10.500]   There is one world where maybe it's not that.
[01:10:10.500 --> 01:10:11.860]   I mean, maybe the model just ends up being
[01:10:11.860 --> 01:10:13.340]   more of the product itself.
[01:10:13.340 --> 01:10:17.140]   In that case, then I think it's a trickier
[01:10:17.140 --> 01:10:20.060]   economic calculation about whether you open source that
[01:10:20.060 --> 01:10:23.460]   because then you are kind of commoditizing yourself a lot.
[01:10:23.460 --> 01:10:24.900]   But I don't, from what I can see so far,
[01:10:24.900 --> 01:10:26.720]   it doesn't seem like we're in that zone.
[01:10:26.720 --> 01:10:28.540]   - Do you expect to earn significant revenue
[01:10:28.540 --> 01:10:30.740]   from licensing your model to the cloud providers?
[01:10:30.740 --> 01:10:33.540]   So they have to pay you a fee to actually serve the model?
[01:10:33.540 --> 01:10:38.680]   - We wanna have an arrangement like that,
[01:10:38.680 --> 01:10:40.900]   but I don't know how significant it'll be.
[01:10:40.900 --> 01:10:44.980]   And we have this, this is basically our license for Llama.
[01:10:44.980 --> 01:10:49.180]   In a lot of ways, it's like a very permissive
[01:10:49.180 --> 01:10:51.780]   open source license, except that we have a limit
[01:10:51.780 --> 01:10:53.540]   for the largest companies using it.
[01:10:53.540 --> 01:10:56.340]   And this is why we put that limit in
[01:10:56.340 --> 01:10:58.780]   is we're not trying to prevent them from using it.
[01:10:58.780 --> 01:11:00.060]   We just want them to come talk to us
[01:11:00.060 --> 01:11:01.220]   because if they're gonna just basically
[01:11:01.220 --> 01:11:03.860]   take what we built and resell it and make money off of it,
[01:11:03.860 --> 01:11:07.220]   then it's like, okay, well, if you're like,
[01:11:07.220 --> 01:11:09.300]   you know, Microsoft Azure or Amazon,
[01:11:09.300 --> 01:11:11.320]   then yeah, if you're gonna be reselling the model,
[01:11:11.320 --> 01:11:12.860]   then we should have some revenue share on that.
[01:11:12.860 --> 01:11:14.940]   So just come talk to us before you go do that.
[01:11:14.940 --> 01:11:15.940]   And that's how that's played out.
[01:11:15.940 --> 01:11:19.620]   So for Llama 2, it's, I mean, we basically just have deals
[01:11:19.620 --> 01:11:22.420]   with all these major cloud companies
[01:11:22.420 --> 01:11:24.980]   and Llama 2 is available as a hosted service
[01:11:24.980 --> 01:11:26.560]   on all those clouds.
[01:11:26.560 --> 01:11:31.340]   And I assume that as we release bigger and bigger models,
[01:11:31.340 --> 01:11:32.380]   that'll become a bigger thing.
[01:11:32.380 --> 01:11:33.580]   It's not the main thing that we're doing,
[01:11:33.580 --> 01:11:35.780]   but I just think if those companies
[01:11:35.780 --> 01:11:36.780]   are gonna be selling our models,
[01:11:36.780 --> 01:11:38.360]   it makes sense that we should, you know,
[01:11:38.360 --> 01:11:39.700]   share the upside of that somehow.
[01:11:39.700 --> 01:11:42.580]   - Yeah, with regards to the other open source dangers,
[01:11:42.580 --> 01:11:44.880]   I think I have like genuinely legitimate points
[01:11:44.880 --> 01:11:47.060]   about the balance of power stuff
[01:11:47.060 --> 01:11:49.420]   and potentially like the harms you can get rid of
[01:11:49.420 --> 01:11:52.460]   because we have better alignment techniques or something.
[01:11:52.460 --> 01:11:54.500]   I wish there was some sort of framework that Meta had,
[01:11:54.500 --> 01:11:56.440]   like other labs have this where they say like,
[01:11:56.440 --> 01:11:58.460]   if we see this concrete thing,
[01:11:58.460 --> 01:12:00.860]   then that's a no-go on the open source
[01:12:00.860 --> 01:12:03.220]   or like even potentially on deployment.
[01:12:03.220 --> 01:12:04.220]   I'm just like writing it down.
[01:12:04.220 --> 01:12:06.820]   So like the company is ready for it.
[01:12:06.820 --> 01:12:08.980]   People have expectations around it and so forth.
[01:12:08.980 --> 01:12:10.780]   - Yeah, no, I think that that's a fair point
[01:12:10.780 --> 01:12:12.140]   on the existential risk side.
[01:12:12.140 --> 01:12:14.920]   Right now we focus more on the types of risks
[01:12:14.920 --> 01:12:17.840]   that we see today, which are more of these content risks.
[01:12:17.840 --> 01:12:20.480]   So, you know, we have lines on,
[01:12:20.480 --> 01:12:25.120]   we don't want the model to be basically doing things
[01:12:25.120 --> 01:12:27.880]   that are helping people commit violence or fraud
[01:12:27.880 --> 01:12:30.440]   or, you know, just harming people in different ways.
[01:12:30.440 --> 01:12:35.320]   So in practice for today's models,
[01:12:35.320 --> 01:12:38.080]   and I would guess the next generation
[01:12:38.080 --> 01:12:40.440]   and maybe even the generation after that,
[01:12:40.440 --> 01:12:43.500]   I think while it is somewhat more
[01:12:43.500 --> 01:12:44.940]   maybe intellectually interesting
[01:12:44.940 --> 01:12:47.740]   to talk about the existential risks,
[01:12:47.740 --> 01:12:51.340]   I actually think the real harms
[01:12:51.340 --> 01:12:53.940]   that need more energy being mitigated
[01:12:53.940 --> 01:12:57.980]   are things that are going to like have someone take a model
[01:12:57.980 --> 01:13:01.780]   and do something to hurt a person with today's parameters
[01:13:01.780 --> 01:13:04.420]   and kind of the types of kind of more mundane harms
[01:13:04.420 --> 01:13:05.260]   that we see today,
[01:13:05.260 --> 01:13:08.100]   like people kind of committing fraud against each other
[01:13:08.100 --> 01:13:08.940]   or things like that.
[01:13:08.940 --> 01:13:13.480]   So that, I just don't want to short change that.
[01:13:13.480 --> 01:13:15.360]   I think we have a responsibility
[01:13:15.360 --> 01:13:17.000]   to make sure we do a good job on that.
[01:13:17.000 --> 01:13:18.960]   - Yeah, Meta's a big company, you can handle both.
[01:13:18.960 --> 01:13:19.800]   - Yeah.
[01:13:19.800 --> 01:13:23.080]   - Okay, so as far as the open source goes,
[01:13:23.080 --> 01:13:24.960]   I'm actually curious if you think the impact
[01:13:24.960 --> 01:13:26.920]   of the open source from PyTorch, React,
[01:13:26.920 --> 01:13:29.920]   Open Compute, these things has been bigger for the world
[01:13:29.920 --> 01:13:32.160]   than even the social media aspects of Meta.
[01:13:32.160 --> 01:13:33.760]   'Cause I've like talked to people who use these services
[01:13:33.760 --> 01:13:35.000]   who think like it's plausible.
[01:13:35.000 --> 01:13:37.480]   'Cause a big part of the internet runs on these things.
[01:13:38.480 --> 01:13:40.260]   - It's an interesting question.
[01:13:40.260 --> 01:13:43.500]   I mean, I think almost half the world uses our--
[01:13:43.500 --> 01:13:45.500]   - Yeah, that's a good point.
[01:13:45.500 --> 01:13:47.420]   (laughs)
[01:13:47.420 --> 01:13:49.860]   - So I think it's hard to beat that.
[01:13:49.860 --> 01:13:53.420]   But no, I think open sources,
[01:13:53.420 --> 01:13:57.460]   it's really powerful as a new way of building things.
[01:13:57.460 --> 01:14:01.780]   And yeah, I mean, it's possible.
[01:14:01.780 --> 01:14:03.820]   I mean, it's, you know, it may be one of these things
[01:14:03.820 --> 01:14:08.620]   where, I don't know, like Bell Labs, right?
[01:14:08.620 --> 01:14:12.840]   Where they, it's like they were working on the transistor
[01:14:12.840 --> 01:14:15.920]   because they wanted to enable long distance calling.
[01:14:15.920 --> 01:14:17.360]   And they did.
[01:14:17.360 --> 01:14:19.240]   And it ended up being really profitable for them
[01:14:19.240 --> 01:14:21.200]   that they were able to enable long distance calling.
[01:14:21.200 --> 01:14:25.780]   And if you ask them five to 10 years out from that,
[01:14:25.780 --> 01:14:29.760]   what was the most useful thing that they invented?
[01:14:29.760 --> 01:14:31.640]   It's like, okay, well, we enable long distance calling
[01:14:31.640 --> 01:14:33.240]   and now all these people are long distance calling.
[01:14:33.240 --> 01:14:34.600]   But if you ask a hundred years later,
[01:14:34.600 --> 01:14:35.800]   maybe it's a different question.
[01:14:35.800 --> 01:14:40.500]   So I think that that's true of a lot of the things
[01:14:40.500 --> 01:14:41.340]   that we're building, right?
[01:14:41.340 --> 01:14:44.540]   Reality labs, some of the AI stuff,
[01:14:44.540 --> 01:14:45.440]   some of the open source stuff.
[01:14:45.440 --> 01:14:48.600]   I think it's like the specific products evolve
[01:14:48.600 --> 01:14:50.640]   and to some degree come and go.
[01:14:50.640 --> 01:14:54.240]   But I think like the advances for humanity persist.
[01:14:54.240 --> 01:14:56.280]   And that's like a, I don't know,
[01:14:56.280 --> 01:14:58.200]   cool part of what we all get to do.
[01:14:58.200 --> 01:15:00.040]   - By when will the Lama models be trained
[01:15:00.040 --> 01:15:01.400]   on your own custom silicon?
[01:15:02.400 --> 01:15:07.400]   - Um, soon, not Lama 4.
[01:15:07.400 --> 01:15:11.400]   The approach that we took is first,
[01:15:11.400 --> 01:15:16.400]   we basically built custom silicon that could handle inference
[01:15:16.400 --> 01:15:19.840]   for our ranking and recommendation type stuff.
[01:15:19.840 --> 01:15:22.960]   So reels, newsfeed, ads.
[01:15:22.960 --> 01:15:27.720]   And that was consuming a lot of GPUs.
[01:15:27.720 --> 01:15:30.560]   But when we were able to move that to our own silicon,
[01:15:30.560 --> 01:15:35.480]   we now were able to use the more expensive NVIDIA GPUs
[01:15:35.480 --> 01:15:37.160]   only for training.
[01:15:37.160 --> 01:15:42.160]   So at some point we will hopefully have silicon ourselves
[01:15:42.160 --> 01:15:47.680]   that we can be using for probably first training
[01:15:47.680 --> 01:15:48.760]   some of the simpler things,
[01:15:48.760 --> 01:15:53.600]   then eventually training these like really large models.
[01:15:53.600 --> 01:15:56.580]   But in the meantime,
[01:15:56.580 --> 01:15:58.320]   I'd say the program is going quite well
[01:15:58.320 --> 01:16:00.040]   and we're just rolling it out methodically
[01:16:00.040 --> 01:16:01.880]   and have a long-term roadmap for it.
[01:16:01.880 --> 01:16:03.960]   - Final question.
[01:16:03.960 --> 01:16:05.240]   This is totally out of left field,
[01:16:05.240 --> 01:16:07.880]   but if you were made CEO of Google+,
[01:16:07.880 --> 01:16:08.800]   could you have made it work?
[01:16:08.800 --> 01:16:10.000]   - Google+?
[01:16:10.000 --> 01:16:11.560]   Oof.
[01:16:11.560 --> 01:16:13.160]   Well, I don't know.
[01:16:13.160 --> 01:16:15.160]   I don't know.
[01:16:15.160 --> 01:16:19.680]   That's a very difficult, very difficult counterfactual.
[01:16:19.680 --> 01:16:21.160]   - Okay, then the real final question will be,
[01:16:21.160 --> 01:16:22.640]   when Gemini was launched,
[01:16:22.640 --> 01:16:24.880]   did you, was there any chance that somebody in the office
[01:16:24.880 --> 01:16:26.520]   uttered Karthika Dalinda-esque?
[01:16:26.520 --> 01:16:29.840]   - No, I think we're tamer now.
[01:16:30.840 --> 01:16:31.680]   - Cool, cool.
[01:16:31.680 --> 01:16:32.520]   Awesome, Mark.
[01:16:32.520 --> 01:16:36.600]   - Yeah.
[01:16:36.600 --> 01:16:37.440]   I don't know.
[01:16:37.440 --> 01:16:38.280]   It's a good question.
[01:16:38.280 --> 01:16:39.120]   I don't know.
[01:16:39.120 --> 01:16:41.280]   The problem is there was no CEO of Google+.
[01:16:41.280 --> 01:16:43.960]   It was just like a division within a company.
[01:16:43.960 --> 01:16:45.440]   I think it's like,
[01:16:45.440 --> 01:16:46.280]   and you asked before about
[01:16:46.280 --> 01:16:48.680]   what are the kind of scarcest commodities,
[01:16:48.680 --> 01:16:50.920]   but you asked about it in terms of dollars.
[01:16:50.920 --> 01:16:53.760]   And I actually think for most companies,
[01:16:53.760 --> 01:16:57.400]   it's, of this scale at least, it's focus, right?
[01:16:57.400 --> 01:16:58.520]   It's like when you're a startup,
[01:16:58.520 --> 01:17:01.280]   maybe you're more constrained on capital.
[01:17:01.280 --> 01:17:03.280]   You just are working on one idea
[01:17:03.280 --> 01:17:05.440]   and you might not have all the resources.
[01:17:05.440 --> 01:17:07.480]   I think you cross some threshold at some point
[01:17:07.480 --> 01:17:09.680]   where the nature of what you're doing,
[01:17:09.680 --> 01:17:11.200]   you're building multiple things
[01:17:11.200 --> 01:17:13.240]   and you're creating more value across them,
[01:17:13.240 --> 01:17:17.360]   but you become more constrained on what can you direct
[01:17:17.360 --> 01:17:19.320]   and to go well.
[01:17:19.320 --> 01:17:22.120]   And like, there's always the cases
[01:17:22.120 --> 01:17:24.160]   where something just random awesome happens
[01:17:24.160 --> 01:17:25.000]   in the organization.
[01:17:25.000 --> 01:17:25.920]   I don't even know about it.
[01:17:25.920 --> 01:17:28.360]   And those are, that's great.
[01:17:28.360 --> 01:17:30.760]   But like, but I think in general,
[01:17:30.760 --> 01:17:33.840]   the organization's capacity is largely limited
[01:17:33.840 --> 01:17:38.640]   by what like the CEO and the management team
[01:17:38.640 --> 01:17:42.120]   are able to kind of oversee and kind of manage.
[01:17:42.120 --> 01:17:45.480]   It's, I think that that's just been a big focus for us.
[01:17:45.480 --> 01:17:48.080]   It's like, all right, keep the,
[01:17:48.080 --> 01:17:49.640]   as I guess Ben Horowitz says,
[01:17:49.640 --> 01:17:52.000]   keep the main thing, the main thing, right?
[01:17:52.000 --> 01:17:57.000]   And try to kind of stay focused on your key priorities.
[01:17:58.120 --> 01:17:58.960]   - Yeah.
[01:17:58.960 --> 01:17:59.800]   - All right.
[01:17:59.800 --> 01:18:00.640]   - Awesome.
[01:18:00.640 --> 01:18:01.480]   That was excellent, Mark.
[01:18:01.480 --> 01:18:02.320]   Thanks so much.
[01:18:02.320 --> 01:18:03.160]   That was a lot of fun.
[01:18:03.160 --> 01:18:04.000]   - Yeah, really fun.
[01:18:04.000 --> 01:18:04.840]   Thanks for having me.
[01:18:04.840 --> 01:18:05.680]   - Absolutely.
[01:18:05.680 --> 01:18:06.520]   Hey everybody.
[01:18:06.520 --> 01:18:08.040]   I hope you enjoyed that episode with Mark.
[01:18:08.040 --> 01:18:09.960]   As you can see, I'm now doing ads.
[01:18:09.960 --> 01:18:12.920]   So if you're interested in advertising on the podcast,
[01:18:12.920 --> 01:18:14.880]   go to the link in the description.
[01:18:14.880 --> 01:18:16.400]   Otherwise, as you know,
[01:18:16.400 --> 01:18:19.080]   the most helpful thing you can do is just share the podcast
[01:18:19.080 --> 01:18:21.160]   with people who you think might enjoy it.
[01:18:21.160 --> 01:18:23.840]   You know, your friends, group chats, Twitter,
[01:18:23.840 --> 01:18:25.240]   I guess threads.
[01:18:25.240 --> 01:18:28.000]   Yeah, hope you enjoyed and I'll see you on the next one.
[01:18:28.800 --> 01:18:31.400]   (upbeat music)
[01:18:31.400 --> 01:18:34.000]   (upbeat music)
[01:18:34.000 --> 01:18:38.000]   [Music]

