
[00:00:00.000 --> 00:00:09.280]   You'll come to someone and they say, "Okay, well, I want to figure out customer churn."
[00:00:09.280 --> 00:00:13.000]   And you're like, "Okay, well, I'll build this model, but I can't guarantee that it's going
[00:00:13.000 --> 00:00:14.580]   to be good.
[00:00:14.580 --> 00:00:17.640]   I can't guarantee it's going to be accurate in the first pass.
[00:00:17.640 --> 00:00:22.260]   But in the meantime, you have to figure out how long you're going to be at the client,
[00:00:22.260 --> 00:00:24.080]   how much value you're going to add."
[00:00:24.080 --> 00:00:26.880]   So it's very, very hazy.
[00:00:26.880 --> 00:00:30.720]   You're listening to Gradient Dissent, a show where we learn about making machine learning
[00:00:30.720 --> 00:00:32.380]   models work in the real world.
[00:00:32.380 --> 00:00:34.520]   I'm your host, Lukas Biewald.
[00:00:34.520 --> 00:00:38.660]   Vicky Boykus is a senior consultant in machine learning and engineering, and she works with
[00:00:38.660 --> 00:00:43.760]   very large clients and brings a really interesting perspective on how they think about data science
[00:00:43.760 --> 00:00:45.120]   and machine learning.
[00:00:45.120 --> 00:00:49.960]   She also has a hilarious Twitter account and a really fascinating newsletter that we'll
[00:00:49.960 --> 00:00:51.320]   link to.
[00:00:51.320 --> 00:00:53.120]   And I'm really excited to talk to her today.
[00:00:53.120 --> 00:00:56.280]   So it's really nice to talk to you, Vicky.
[00:00:56.280 --> 00:01:01.080]   Could you tell me a little bit about your career and how you got into data science and
[00:01:01.080 --> 00:01:02.080]   where that began?
[00:01:02.080 --> 00:01:05.880]   Yeah, it's been a really interesting career, I think.
[00:01:05.880 --> 00:01:09.840]   Not unlike a lot of other people, but a little bit different.
[00:01:09.840 --> 00:01:11.440]   So I don't have a comp sci background.
[00:01:11.440 --> 00:01:18.440]   I did an undergrad in economics at Penn State, and then I went into economic consulting,
[00:01:18.440 --> 00:01:19.440]   which is pretty unusual.
[00:01:19.440 --> 00:01:22.360]   And it was right around the time of the recession in 2007.
[00:01:22.360 --> 00:01:27.040]   So I was happy to find a job, actually, and even more so to find one in my industry.
[00:01:27.040 --> 00:01:33.380]   But that involved doing a lot of spreadsheets, tracking global trade movements, tracking
[00:01:33.380 --> 00:01:36.420]   internal projects, all that kind of stuff.
[00:01:36.420 --> 00:01:38.840]   So I started working with data analytics there.
[00:01:38.840 --> 00:01:41.860]   And then for my next couple of jobs, I worked in data analytics.
[00:01:41.860 --> 00:01:49.040]   And then for a job that I had in Philadelphia, Comcast, I started working with big data.
[00:01:49.040 --> 00:01:53.200]   So we had big data available, and there I started working with Hadoop.
[00:01:53.200 --> 00:01:55.600]   And looking at big data and more of...
[00:01:55.600 --> 00:01:58.440]   That was 2012, I want to say.
[00:01:58.440 --> 00:02:02.880]   Yeah, so right around the time that it was starting to get really big, I started working
[00:02:02.880 --> 00:02:04.560]   with that tool stack.
[00:02:04.560 --> 00:02:07.920]   And that involved me having to get a lot more technical.
[00:02:07.920 --> 00:02:13.560]   So at the time, I was doing primarily SQL, had some frustrations with just doing SQL
[00:02:13.560 --> 00:02:14.560]   with Hadoop.
[00:02:14.560 --> 00:02:16.200]   Hive was still relatively new.
[00:02:16.200 --> 00:02:18.020]   A lot of growing pains there.
[00:02:18.020 --> 00:02:20.800]   So I started working with that stack.
[00:02:20.800 --> 00:02:26.440]   And then from there, I started doing more large scale data science, sampling, programming,
[00:02:26.440 --> 00:02:30.600]   all of that, and then went on to my next job as a data science job.
[00:02:30.600 --> 00:02:32.640]   And I've been doing data science ever since.
[00:02:32.640 --> 00:02:36.920]   But ironically enough, I'm moving kind of away from data science now.
[00:02:36.920 --> 00:02:38.560]   And I actually wrote a post about this.
[00:02:38.560 --> 00:02:42.200]   I think the entire industry is moving a little bit in that direction.
[00:02:42.200 --> 00:02:47.840]   So not every job, but the industry on average, and more towards instrumenting the processes
[00:02:47.840 --> 00:02:49.780]   around data science.
[00:02:49.780 --> 00:02:54.320]   So creating machine learning pipelines, creating foundations and structures that are really
[00:02:54.320 --> 00:02:57.520]   solid, and kind of go end to end.
[00:02:57.520 --> 00:03:01.040]   So I think there's still a ton of jobs that are just pure analysis.
[00:03:01.040 --> 00:03:05.840]   But as the industry grows, as the amount of data that we work with grows, I think the
[00:03:05.840 --> 00:03:10.320]   whole industry as a whole is trying to get smarter about replicability.
[00:03:10.320 --> 00:03:14.280]   And that's where I'm working in now, more in the machine learning engineering space.
[00:03:14.280 --> 00:03:19.720]   So you think it's actually the bigger challenges now are becoming more engineering issues than
[00:03:19.720 --> 00:03:21.080]   analysis issues?
[00:03:21.080 --> 00:03:24.760]   Yeah, I think I agree with that, at least from my perspective.
[00:03:24.760 --> 00:03:26.440]   So again, I'm a consultant.
[00:03:26.440 --> 00:03:32.180]   So I come into companies that kind of want to build out data science or data engineering
[00:03:32.180 --> 00:03:33.240]   platforms.
[00:03:33.240 --> 00:03:38.760]   And usually they're starting from, so usually the question will be, do we have data or,
[00:03:38.760 --> 00:03:40.960]   I'm sorry, let me rephrase that.
[00:03:40.960 --> 00:03:45.120]   They have a question about, let's say, are sales going up or down and why?
[00:03:45.120 --> 00:03:49.920]   And then we work backwards and say, okay, well, you actually don't have the data yet
[00:03:49.920 --> 00:03:51.120]   to do this.
[00:03:51.120 --> 00:03:55.480]   And you don't have a platform set up where you can reliably look at this stuff on a month
[00:03:55.480 --> 00:03:56.840]   to month basis.
[00:03:56.840 --> 00:03:59.760]   So that's where a lot of the challenges that I see are now.
[00:03:59.760 --> 00:04:00.760]   Interesting.
[00:04:00.760 --> 00:04:05.960]   Is it maybe because you're going to companies now that are like a little bit further behind?
[00:04:05.960 --> 00:04:07.400]   Is that possible?
[00:04:07.400 --> 00:04:11.800]   Or like kind of starting from scratch or do you think like something's changing where
[00:04:11.800 --> 00:04:15.560]   people sort of expect to have more built out processes and tools?
[00:04:15.560 --> 00:04:17.320]   Yeah, I think it's the second one.
[00:04:17.320 --> 00:04:24.600]   I think if you look at the landscape of whatever the data tools landscape map that Matt Turk
[00:04:24.600 --> 00:04:26.000]   puts out every year.
[00:04:26.000 --> 00:04:31.800]   So in 2011 and 2012, it was like a quarter of a page and it was just like a dupe and
[00:04:31.800 --> 00:04:33.240]   that was it.
[00:04:33.240 --> 00:04:39.240]   And now I think poor Matt has to put together like 500 logos into a single page and there's
[00:04:39.240 --> 00:04:44.760]   an orchestration area and there's a tools area and there's an area just for tools around
[00:04:44.760 --> 00:04:46.840]   Spark and all that stuff.
[00:04:46.840 --> 00:04:53.920]   So I think people also, there's the expectation that if you have something, it should be productionizable.
[00:04:53.920 --> 00:04:59.560]   And even to the point where we now have notebooks, which are generally seen as like an exploration
[00:04:59.560 --> 00:05:00.560]   tool.
[00:05:00.560 --> 00:05:04.520]   There's also some movement, for example, Netflix recently has had around productionizing
[00:05:04.520 --> 00:05:05.520]   notebooks.
[00:05:05.520 --> 00:05:09.360]   So whatever workflow you're looking at, I think there's the expectation that in the
[00:05:09.360 --> 00:05:12.360]   end it be reproducible to be valuable.
[00:05:12.360 --> 00:05:13.360]   I see.
[00:05:13.360 --> 00:05:18.720]   It's funny, you know, my last company, we sold into a lot of the kind of Fortune 500,
[00:05:18.720 --> 00:05:20.200]   you know, not necessarily Silicon Valley.
[00:05:20.200 --> 00:05:23.640]   And I always kind of really enjoyed seeing the different perspectives and like all the
[00:05:23.640 --> 00:05:24.640]   different applications.
[00:05:24.640 --> 00:05:27.600]   How has it been for you as a consultant?
[00:05:27.600 --> 00:05:31.920]   Does it feel more like frustrating or exciting or I guess what is it like to kind of go into
[00:05:31.920 --> 00:05:36.320]   an organization and try to teach them how to build up these processes?
[00:05:36.320 --> 00:05:38.520]   I think it's a little bit of both.
[00:05:38.520 --> 00:05:44.900]   So it's interesting because consulting as a data scientist involves both.
[00:05:44.900 --> 00:05:48.280]   And I think this is actually true of all data science, but even more so with consulting.
[00:05:48.280 --> 00:05:52.760]   It involves both like the people piece and the technical piece.
[00:05:52.760 --> 00:05:56.520]   So you have to know what you're doing technically because you're the expert when you come into
[00:05:56.520 --> 00:06:00.160]   the company and you have to say, okay, this is how we want to do the architecture.
[00:06:00.160 --> 00:06:06.080]   But you're also going to be talking to people who maybe don't want this process at all.
[00:06:06.080 --> 00:06:08.480]   You're going to be talking to people who are disorganized.
[00:06:08.480 --> 00:06:12.640]   You're going to be talking to people who are for it, but don't necessarily understand it.
[00:06:12.640 --> 00:06:18.000]   And so a lot of that work is actually talking to people and building the case for this stuff
[00:06:18.000 --> 00:06:19.000]   as well.
[00:06:19.000 --> 00:06:24.880]   What is like, what is a typical stack end up looking like these days?
[00:06:24.880 --> 00:06:29.120]   It's hard to say, so I've dealt with both companies, small and large.
[00:06:29.120 --> 00:06:33.300]   A lot of companies are increasingly in the cloud.
[00:06:33.300 --> 00:06:34.400]   So it's interesting.
[00:06:34.400 --> 00:06:39.200]   I don't think I have any GCP clients that I've dealt with.
[00:06:39.200 --> 00:06:42.080]   AWS is of course probably the lead.
[00:06:42.080 --> 00:06:46.520]   And I did a Twitter question about this a couple of months ago, like who's using what?
[00:06:46.520 --> 00:06:50.160]   AWS came out something like 60 to 70%.
[00:06:50.160 --> 00:06:53.520]   Azure, I'm surprised is really catching up.
[00:06:53.520 --> 00:06:59.760]   I think even as little as two to three years ago, they were squarely in third place.
[00:06:59.760 --> 00:07:03.400]   No one was even considering them, but now it's really growing.
[00:07:03.400 --> 00:07:06.400]   And I think part of that is Microsoft's leadership.
[00:07:06.400 --> 00:07:12.240]   Plus the fact that a lot of companies in the retail space are not allowed to use AWS because
[00:07:12.240 --> 00:07:14.640]   they see them as a competitor.
[00:07:14.640 --> 00:07:19.400]   And partially because they are stepping up their game in the tools that they're providing.
[00:07:19.400 --> 00:07:23.600]   Is there a particular offering from Azure that you like or you think is kind of driving
[00:07:23.600 --> 00:07:24.600]   some of this growth?
[00:07:24.600 --> 00:07:28.800]   I actually, ironically, I haven't used Azure a lot.
[00:07:28.800 --> 00:07:33.120]   Most of my work has been in AWS, but now that I'm seeing that people are more interested
[00:07:33.120 --> 00:07:36.000]   in it, I'm definitely going to have to start looking into it.
[00:07:36.000 --> 00:07:37.000]   Interesting.
[00:07:37.000 --> 00:07:42.080]   What about, like, is there any tool that you think is sort of like underrated that people
[00:07:42.080 --> 00:07:45.280]   like probably should be using or you recommend but people aren't using yet?
[00:07:45.280 --> 00:07:48.840]   I want to say Bash.
[00:07:48.840 --> 00:08:00.560]   That's kind of a glib answer, but it's really true because a lot of the times when you come
[00:08:00.560 --> 00:08:07.480]   into these big, huge projects, you have five or six different AWS services spun up.
[00:08:07.480 --> 00:08:10.220]   You have GPUs, you have monitoring, you have all this stuff.
[00:08:10.220 --> 00:08:12.920]   And then you start thinking, okay, well, I have all this stuff.
[00:08:12.920 --> 00:08:14.640]   How am I going to use it for X?
[00:08:14.640 --> 00:08:17.960]   Well, oh, I can't test this locally.
[00:08:17.960 --> 00:08:19.280]   I can't do this locally.
[00:08:19.280 --> 00:08:20.800]   I can't sample the data.
[00:08:20.800 --> 00:08:22.240]   What am I going to do with it?
[00:08:22.240 --> 00:08:27.800]   So I really do think, and I find myself falling into this pattern too, where you use all this
[00:08:27.800 --> 00:08:32.120]   big data stuff, but then you don't use the stuff that you have available to you.
[00:08:32.120 --> 00:08:36.920]   And it's even easier these days when a lot of us are working with pretty high powered
[00:08:36.920 --> 00:08:41.160]   machines that you can do a lot locally as well.
[00:08:41.160 --> 00:08:42.160]   Interesting.
[00:08:42.160 --> 00:08:44.160]   So you run a lot of stuff locally?
[00:08:44.160 --> 00:08:46.280]   Some, yeah.
[00:08:46.280 --> 00:08:49.320]   Mostly to test stuff, to prototype.
[00:08:49.320 --> 00:08:53.760]   And in cloud environments, it's really hard to spin up those local environments.
[00:08:53.760 --> 00:08:59.920]   So just even to look at the data, to examine what you're dealing with, all of that stuff
[00:08:59.920 --> 00:09:04.760]   you can do locally and Bash goes a long way towards that.
[00:09:04.760 --> 00:09:09.440]   So I know that you can't talk about your individual customers, but can you kind of talk broadly
[00:09:09.440 --> 00:09:17.080]   about the questions that are driving the interest in more interest in data science right now?
[00:09:17.080 --> 00:09:18.080]   What's kind of top of mind?
[00:09:18.080 --> 00:09:24.360]   What would you expect going into another Fortune 500 company that the executives want out of
[00:09:24.360 --> 00:09:27.920]   their data science platform that they're not getting right now?
[00:09:27.920 --> 00:09:28.920]   Yeah.
[00:09:28.920 --> 00:09:34.920]   So the number one question is always to understand customers and understand what they're doing
[00:09:34.920 --> 00:09:39.840]   and understand how what the customers are doing ties directly to the bottom line.
[00:09:39.840 --> 00:09:42.960]   And that manifests itself in a number of different ways.
[00:09:42.960 --> 00:09:47.980]   The one that I usually talk about, which I've also written a blog post about is churn.
[00:09:47.980 --> 00:09:50.080]   Everybody always wants to know churn.
[00:09:50.080 --> 00:09:55.720]   So how many people are leaving and why they're leaving your platform and how much money it's
[00:09:55.720 --> 00:09:58.660]   going to cost you on a month to month basis.
[00:09:58.660 --> 00:10:02.480]   Everybody always wants to know that and I can guarantee in any given project, I'll see
[00:10:02.480 --> 00:10:03.960]   it.
[00:10:03.960 --> 00:10:09.880]   And then the second one is better understanding operational metrics.
[00:10:09.880 --> 00:10:13.280]   There's sometimes not a lot of insight into that.
[00:10:13.280 --> 00:10:19.480]   And the third one would probably be classifying customers into different types of customers.
[00:10:19.480 --> 00:10:20.480]   Interesting.
[00:10:20.480 --> 00:10:26.040]   What's like a deliverable that you would give a company around churn that they would be
[00:10:26.040 --> 00:10:27.040]   like excited about?
[00:10:27.040 --> 00:10:29.880]   Like, can you literally say I can predict churn like X percent better?
[00:10:29.880 --> 00:10:33.600]   Or is it more like, you know, if you see this signal, then that kind of means churn.
[00:10:33.600 --> 00:10:36.200]   Like how do you how do you actually like present some kind of analysis?
[00:10:36.200 --> 00:10:37.200]   Yes.
[00:10:37.200 --> 00:10:46.400]   So lever, it would be literally a platform that has information to be able to predict
[00:10:46.400 --> 00:10:51.080]   what what churn is going to be, for example, for next month.
[00:10:51.080 --> 00:10:55.960]   Usually what ends up happening is a lot of the things that I'll deliver are the data
[00:10:55.960 --> 00:11:00.160]   engineering piece around getting all the data all together in one place so we can have a
[00:11:00.160 --> 00:11:03.440]   data lake so we can actually deliver that churn piece.
[00:11:03.440 --> 00:11:09.560]   Hi, we'd love to take a moment to tell you guys about Weights and Biases.
[00:11:09.560 --> 00:11:15.040]   Weights and Biases is a tool that helps you track and visualize every detail of your machine
[00:11:15.040 --> 00:11:16.040]   learning models.
[00:11:16.040 --> 00:11:21.960]   We help you debug your machine learning models in real time, collaborate easily and advance
[00:11:21.960 --> 00:11:25.080]   the state of the art in machine learning.
[00:11:25.080 --> 00:11:30.280]   You can integrate Weights and Biases into your models with just a few lines of code.
[00:11:30.280 --> 00:11:35.040]   With hyperparameter sweeps, you can find the best set of hyperparameters for your models
[00:11:35.040 --> 00:11:36.960]   automatically.
[00:11:36.960 --> 00:11:42.360]   You can also track and compare how many GPU resources your models are using.
[00:11:42.360 --> 00:11:49.080]   With one line of code, you can visualize model predictions in form of images, videos, audio,
[00:11:49.080 --> 00:11:54.800]   plotly charts, molecular data, segmentation maps and 3D point clouds.
[00:11:54.800 --> 00:12:00.640]   You can save everything you need to reproduce your models days, weeks or even months after
[00:12:00.640 --> 00:12:01.640]   training.
[00:12:01.640 --> 00:12:06.760]   Finally, with reports, you can make your models come alive.
[00:12:06.760 --> 00:12:11.760]   Reports are like blog posts in which your readers can interact with your model metrics
[00:12:11.760 --> 00:12:13.560]   and predictions.
[00:12:13.560 --> 00:12:19.620]   Reports serve as a centralized repository of metrics, predictions, hyperparameter stride
[00:12:19.620 --> 00:12:21.520]   and accompanying nodes.
[00:12:21.520 --> 00:12:27.200]   All of this together gives you a bird's eye view of your machine learning workflow.
[00:12:27.200 --> 00:12:32.680]   You can use reports to share your model insights, keep your team on the same page and collaborate
[00:12:32.680 --> 00:12:34.760]   effectively remotely.
[00:12:34.760 --> 00:12:38.840]   I'll leave a link in the show notes below to help you get started.
[00:12:38.840 --> 00:12:41.640]   And now let's get back to the episode.
[00:12:41.640 --> 00:12:46.320]   How sophisticated would a churn prediction model be today?
[00:12:46.320 --> 00:12:53.280]   Are we talking like, are people using like deep learning for this or like what's how
[00:12:53.280 --> 00:12:55.840]   complicated these models get?
[00:12:55.840 --> 00:13:01.480]   I don't think there, I think a lot of times in companies and even before I was doing consulting,
[00:13:01.480 --> 00:13:07.920]   like in all my previous jobs, people are just impressed if you can get a model out the door
[00:13:07.920 --> 00:13:12.680]   a lot of the times in an overall industry.
[00:13:12.680 --> 00:13:18.120]   So if you have something that you can benchmark against, it's seen as good, especially because
[00:13:18.120 --> 00:13:20.640]   there's so many steps in doing it.
[00:13:20.640 --> 00:13:24.100]   So first you have to collect the data, then you have to clean the data.
[00:13:24.100 --> 00:13:29.160]   Then you have to go to the customer support team and say, does this thing, like does someone
[00:13:29.160 --> 00:13:32.520]   calling in mean that the person might churn or not?
[00:13:32.520 --> 00:13:36.800]   And then you have to collect all the manual data that they keep and keep track of that.
[00:13:36.800 --> 00:13:38.380]   Then you have to build the model.
[00:13:38.380 --> 00:13:40.040]   Then you have to do a prediction.
[00:13:40.040 --> 00:13:44.760]   Then you have to meet with the people who are in charge of this and explain your data
[00:13:44.760 --> 00:13:45.760]   to them.
[00:13:45.760 --> 00:13:47.480]   And then there's going to be a back and forth there.
[00:13:47.480 --> 00:13:50.560]   And then you have to productionalize all of that.
[00:13:50.560 --> 00:13:54.880]   So if you can get a model end to end going, and I've come into companies where there was
[00:13:54.880 --> 00:13:56.360]   zero data science before.
[00:13:56.360 --> 00:13:59.900]   And so that's why I'm saying that you have to build it from the ground up.
[00:13:59.900 --> 00:14:02.200]   Having that is fantastic.
[00:14:02.200 --> 00:14:06.240]   And just having metrics where there were no metrics before is a huge step up.
[00:14:06.240 --> 00:14:10.760]   And then the next step up is, of course, okay, well, why is this metric different this month?
[00:14:10.760 --> 00:14:12.720]   Why is this metric different that month?
[00:14:12.720 --> 00:14:18.400]   So a lot of the churn models I've built have been with pretty basic stuff like logistic
[00:14:18.400 --> 00:14:21.320]   regression and decision trees.
[00:14:21.320 --> 00:14:25.840]   I haven't seen any deep learning used for churn yet, but I'm sure that use case is around
[00:14:25.840 --> 00:14:26.840]   the corner.
[00:14:26.840 --> 00:14:28.680]   Well, I mean, okay, here's a specific question.
[00:14:28.680 --> 00:14:32.720]   I mean, like decision trees versus logistic regression, they do kind of different things.
[00:14:32.720 --> 00:14:37.760]   Do you have a particular one you start with or do you try both or some kind of hybrid?
[00:14:37.760 --> 00:14:39.200]   How do you think about that?
[00:14:39.200 --> 00:14:40.200]   Yeah.
[00:14:40.200 --> 00:14:44.000]   So again, depends on the data available.
[00:14:44.000 --> 00:14:48.400]   Usually if also depends on who's going to be looking at it.
[00:14:48.400 --> 00:14:53.400]   Usually if it's people at a higher level, like executives that need to briefly glance
[00:14:53.400 --> 00:14:58.280]   and understand something immediately, the decision tree is very intuitive and very easy
[00:14:58.280 --> 00:15:01.200]   to explain.
[00:15:01.200 --> 00:15:05.080]   And it can offer a number of different pathways for discussion.
[00:15:05.080 --> 00:15:10.200]   If you just need some sort of model that spits out, is this person going to churn?
[00:15:10.200 --> 00:15:11.640]   Yes or no.
[00:15:11.640 --> 00:15:14.560]   Logistic regression is a little bit better for that.
[00:15:14.560 --> 00:15:18.920]   But again, depends on the stack that they have.
[00:15:18.920 --> 00:15:23.640]   There's different software packages that are better or worse for logistic regression.
[00:15:23.640 --> 00:15:28.960]   So for example, surprisingly, Python, as far as I know, does not have very good decision
[00:15:28.960 --> 00:15:30.320]   tree support.
[00:15:30.320 --> 00:15:34.280]   You could do XGBoost, which is not quite the same.
[00:15:34.280 --> 00:15:37.320]   But it's like multiple decision trees.
[00:15:37.320 --> 00:15:38.320]   Right.
[00:15:38.320 --> 00:15:39.320]   That's the decision trees.
[00:15:39.320 --> 00:15:40.320]   You're doing boosted trees or something like that?
[00:15:40.320 --> 00:15:41.840]   Yeah, boosted trees.
[00:15:41.840 --> 00:15:50.600]   But it doesn't offer the nice visual interpretation, I guess, as much as the R part package.
[00:15:50.600 --> 00:15:55.720]   So yeah, it really depends on what you have available, what you can do, all that kind
[00:15:55.720 --> 00:15:56.720]   of stuff.
[00:15:56.720 --> 00:16:00.080]   But I would say all three of those are my go-to's for that.
[00:16:00.080 --> 00:16:01.080]   Interesting.
[00:16:01.080 --> 00:16:05.800]   So you'll build a stable pipeline that includes R in it?
[00:16:05.800 --> 00:16:11.680]   I've done it for stuff where I've had to prototype and kind of throw it out.
[00:16:11.680 --> 00:16:16.560]   I actually have not built an R pipeline in production, although I know it's very possible
[00:16:16.560 --> 00:16:19.440]   and increasingly becoming more and more possible.
[00:16:19.440 --> 00:16:20.440]   Interesting.
[00:16:20.440 --> 00:16:26.880]   So do you feel like R is kind of here to stay, or do you feel like it's getting replaced
[00:16:26.880 --> 00:16:27.880]   by Python?
[00:16:27.880 --> 00:16:33.320]   I mean, I think they're two different tools for two different things.
[00:16:33.320 --> 00:16:40.000]   So I think R is fantastic for statistics, for stuff that you're working on in probably
[00:16:40.000 --> 00:16:42.680]   smaller teams.
[00:16:42.680 --> 00:16:48.360]   And Python is more like of a general, like if you need to glue stuff together, and if
[00:16:48.360 --> 00:16:53.160]   you need to do deep learning, and if you need to have stuff end to end, you'll use Python.
[00:16:53.160 --> 00:17:02.200]   But its basic statistical capabilities are not as good as a lot of the R packages.
[00:17:02.200 --> 00:17:08.000]   How do you think about leaving your work in a state where another person can update it?
[00:17:08.000 --> 00:17:10.800]   How does that happen?
[00:17:10.800 --> 00:17:15.080]   Do you ever check back in with a client and see if anyone's touched your model and it's
[00:17:15.080 --> 00:17:17.680]   still useful for them?
[00:17:17.680 --> 00:17:19.200]   That seems like it must be really hard.
[00:17:19.240 --> 00:17:24.160]   Yeah, so what we usually do is we work side by side with the client.
[00:17:24.160 --> 00:17:27.920]   So we'll have a person on the client side who is a data scientist.
[00:17:27.920 --> 00:17:31.440]   So we can hand it off, or we'll have teams.
[00:17:31.440 --> 00:17:33.280]   And so we do education throughout the process.
[00:17:33.280 --> 00:17:35.360]   So we can hand it off and not just be like, "See ya."
[00:17:35.360 --> 00:17:39.920]   Like this person knows how to pick it up and knows how it was being built.
[00:17:39.920 --> 00:17:40.920]   I see.
[00:17:40.920 --> 00:17:41.920]   I see.
[00:17:41.920 --> 00:17:42.920]   That makes sense.
[00:17:42.920 --> 00:17:44.960]   And you probably like pick the technologies they're familiar with?
[00:17:44.960 --> 00:17:47.200]   Yeah, for sure.
[00:17:47.200 --> 00:17:51.800]   Yeah, so we try to pick technologies that are not foreign to the client.
[00:17:51.800 --> 00:17:57.640]   So it's not like they're completely floundering and gone when we hand over PyTorch or something.
[00:17:57.640 --> 00:18:04.120]   So what's the biggest frustration in this whole process?
[00:18:04.120 --> 00:18:07.720]   Where do you see the biggest room for potential improvement?
[00:18:07.720 --> 00:18:11.520]   I think we've both kind of sold into big companies and it's challenging.
[00:18:11.520 --> 00:18:15.520]   You don't want to say bad things about your clients, but also you do feel some like, "Come
[00:18:15.520 --> 00:18:16.880]   on, this is ridiculous, guys."
[00:18:16.880 --> 00:18:22.400]   Do you have any patterns there?
[00:18:22.400 --> 00:18:34.800]   I think the biggest issue is trying to explain the benefit of machine learning in a way where
[00:18:34.800 --> 00:18:37.600]   it's not always exactly clear.
[00:18:37.600 --> 00:18:42.040]   So for example, you'll come to someone and they say, "Okay, well, I want to figure out
[00:18:42.040 --> 00:18:43.040]   customer churn."
[00:18:43.040 --> 00:18:46.720]   And you're like, "Okay, well, I'll build this model, but I can't guarantee that it's going
[00:18:46.720 --> 00:18:48.280]   to be good.
[00:18:48.280 --> 00:18:51.360]   I can't guarantee it's going to be accurate in the first pass."
[00:18:51.360 --> 00:18:55.960]   But in the meantime, you have to figure out how long you're going to be at the client,
[00:18:55.960 --> 00:18:57.800]   how much value you're going to add.
[00:18:57.800 --> 00:19:00.800]   So it's very, very hazy.
[00:19:00.800 --> 00:19:05.320]   And I think that's more of a frustration for me, but it's also like an educational issue
[00:19:05.320 --> 00:19:11.880]   where you're not going to always get to a right answer, like the first sprint or the
[00:19:11.880 --> 00:19:13.640]   second sprint.
[00:19:13.640 --> 00:19:15.240]   It's going to be an iterative process.
[00:19:15.240 --> 00:19:18.040]   And sometimes if you add stuff, the model will get worse.
[00:19:18.040 --> 00:19:20.520]   If you take stuff away, the model will get better.
[00:19:20.520 --> 00:19:26.600]   So it's kind of hard because data science is always sold, or I see it being sold as
[00:19:26.600 --> 00:19:31.880]   this exact thing, but it's very much like an art process.
[00:19:31.880 --> 00:19:34.200]   So I think that's where some of the frustration is.
[00:19:34.200 --> 00:19:37.040]   It's not an exact thing and people expect it to be.
[00:19:37.040 --> 00:19:41.600]   And I can imagine it's probably really hard going in, like a priori, not knowing the amount
[00:19:41.600 --> 00:19:42.600]   of lift.
[00:19:42.600 --> 00:19:44.600]   It's not like, "I for sure want this to get better."
[00:19:44.600 --> 00:19:47.600]   It's like, "Well, how would you know?"
[00:19:47.600 --> 00:19:49.960]   How do you articulate that?
[00:19:49.960 --> 00:19:57.760]   So if someone's like, "Hey, tell me how much you're going to improve my churn prediction."
[00:19:57.760 --> 00:20:00.520]   What would you say to that?
[00:20:00.520 --> 00:20:02.640]   Yeah, that's a good one.
[00:20:02.640 --> 00:20:09.240]   I think first, I don't know, I've actually never had it happen that someone was like,
[00:20:09.240 --> 00:20:12.640]   "You have to improve my model by this much."
[00:20:12.640 --> 00:20:18.520]   It's usually like, "Let's create a model to do X, Y, or Z."
[00:20:18.520 --> 00:20:24.000]   But what we usually do is benchmark against previous metrics that they have.
[00:20:24.000 --> 00:20:28.600]   And so the goal there is to say, "Look, we're not sure how much we can improve your model,
[00:20:28.600 --> 00:20:34.360]   but we can improve the process around the model so that it can be a little clearer."
[00:20:34.360 --> 00:20:40.320]   When you look at the successful engagements where you feel like you really made a difference
[00:20:40.320 --> 00:20:45.440]   versus the ones that are more frustrating, are there patterns?
[00:20:45.440 --> 00:20:51.440]   Are there things that your more successful clients are doing around data science that
[00:20:51.440 --> 00:20:54.040]   sets them up for success?
[00:20:54.040 --> 00:20:57.200]   Usually working in a tight loop with me.
[00:20:57.200 --> 00:21:00.960]   So a lot of the times, the companies I work at will be bigger.
[00:21:00.960 --> 00:21:05.200]   And so the data science team will be on one side, the data engineering team will be on
[00:21:05.200 --> 00:21:09.080]   one side, the project management team will be somewhere over there.
[00:21:09.080 --> 00:21:16.160]   And so I'll talk to all of them, but they don't talk to each other necessarily.
[00:21:16.160 --> 00:21:22.960]   And so what I've seen work best is when I'm embedded with a developer, a data scientist,
[00:21:22.960 --> 00:21:27.400]   a project manager, they're all kind of working together towards the same thing, because there's
[00:21:27.400 --> 00:21:29.960]   a big tendency to get siloed.
[00:21:29.960 --> 00:21:36.360]   I think companies sometimes debate internally about, "Should we have a single data science
[00:21:36.360 --> 00:21:43.120]   function or should we embed the data scientists and have the different functional teams hire
[00:21:43.120 --> 00:21:45.560]   data scientists for the individual products that they're working on?"
[00:21:45.560 --> 00:21:49.920]   Do you have an opinion that it sounds like you might prefer data scientists being embedded
[00:21:49.920 --> 00:21:53.560]   in specific products or specific outcomes?
[00:21:53.560 --> 00:21:57.400]   Or do you think that it's better to keep it all as a single function so you can maybe
[00:21:57.400 --> 00:22:00.080]   hire better people or create a better culture?
[00:22:00.080 --> 00:22:03.040]   Yeah, I'm not really sure I have an opinion on that.
[00:22:03.040 --> 00:22:07.680]   I've seen it work well, different ways in different companies.
[00:22:07.680 --> 00:22:12.040]   I think probably for smaller companies, I would say like less than a thousand people
[00:22:12.040 --> 00:22:17.480]   or so, you probably want to have a centralized team.
[00:22:17.480 --> 00:22:22.040]   For much, much larger companies, you probably want to have embedded data science teams.
[00:22:22.040 --> 00:22:26.320]   But then the danger is if you don't manage them centrally, then you have five or six
[00:22:26.320 --> 00:22:29.240]   data science teams working on the same questions.
[00:22:29.240 --> 00:22:33.400]   And I've definitely seen this at companies where it's just replicated work and they're
[00:22:33.400 --> 00:22:35.880]   just approaching it in different ways.
[00:22:35.880 --> 00:22:41.560]   So you've seen success in both?
[00:22:41.560 --> 00:22:46.840]   Yeah, for sure.
[00:22:46.840 --> 00:22:56.280]   Do you see specific stages where you're prototyping something and then deploy it into production?
[00:22:56.280 --> 00:23:01.480]   It sounds like you're really focused on getting things stable and in production, but do you
[00:23:01.480 --> 00:23:04.320]   sort of prototype the steps first and then solidify them?
[00:23:04.320 --> 00:23:05.640]   How do you think about that?
[00:23:05.640 --> 00:23:09.400]   Do you specifically say to a client, we're going to prototype and then we're going to
[00:23:09.400 --> 00:23:10.400]   deploy?
[00:23:10.400 --> 00:23:13.360]   Yeah, that's usually what we do.
[00:23:13.360 --> 00:23:18.440]   So usually I come into an environment and you're not really clear on what's going on
[00:23:18.440 --> 00:23:21.000]   in the environment at all.
[00:23:21.000 --> 00:23:24.000]   You're just kind of thrown in and say, okay, go.
[00:23:24.000 --> 00:23:28.600]   So the first step is to gather and assess what's going on.
[00:23:28.600 --> 00:23:29.760]   What tools are they using?
[00:23:29.760 --> 00:23:33.320]   Who are the key people involved in this?
[00:23:33.320 --> 00:23:39.960]   Gather all of that out and then start to create a model from whatever data that you have available.
[00:23:39.960 --> 00:23:42.480]   See if you can actually create that model.
[00:23:42.480 --> 00:23:45.880]   And then many sprints later, take that model to production.
[00:23:45.880 --> 00:23:50.920]   It's usually never, you come in, you create something and then it's already running.
[00:23:50.920 --> 00:23:56.640]   It's usually a lot of human steps in the middle to get it to that point.
[00:23:56.640 --> 00:24:00.040]   And so what's the biggest, when you think about taking, I mean, I feel like everyone
[00:24:00.040 --> 00:24:05.840]   always underestimates the pain of taking a prototype into production.
[00:24:05.840 --> 00:24:09.560]   What are the biggest challenges that people might not expect or don't usually expect going
[00:24:09.560 --> 00:24:12.800]   into that process?
[00:24:12.800 --> 00:24:15.400]   Packaging the model is always a big one.
[00:24:15.400 --> 00:24:16.400]   How do you package it?
[00:24:16.400 --> 00:24:18.720]   How do you typically package it?
[00:24:18.720 --> 00:24:20.680]   What are the options?
[00:24:20.680 --> 00:24:22.560]   You could pickle it.
[00:24:22.560 --> 00:24:25.520]   You could create a REST endpoint from it.
[00:24:25.520 --> 00:24:29.760]   You could put the model in a Docker container and expose endpoints from it.
[00:24:29.760 --> 00:24:34.520]   I think that's something that I've seen happen more and more frequently where the resulting
[00:24:34.520 --> 00:24:41.240]   output is essentially a web app or a web service and something hits that web service and you
[00:24:41.240 --> 00:24:43.520]   get an inference point.
[00:24:43.520 --> 00:24:45.520]   Right.
[00:24:45.520 --> 00:24:50.560]   Yeah I would say those are the two big ways right now.
[00:24:50.560 --> 00:24:55.400]   I think another big thing that people don't think about a lot is metadata management and
[00:24:55.400 --> 00:24:59.440]   a lot of big companies want to do metadata management.
[00:24:59.440 --> 00:25:03.680]   In fact, I think almost every company that I've talked to over the last five years has
[00:25:03.680 --> 00:25:07.640]   said, we need some way to manage all the metadata in the data lake so that we can update the
[00:25:07.640 --> 00:25:10.120]   models and so that analysts can do the analysis.
[00:25:10.120 --> 00:25:11.880]   But there's no single tool for it.
[00:25:11.880 --> 00:25:17.480]   And I think only now have open source tools started coming out for it.
[00:25:17.480 --> 00:25:18.480]   What was it?
[00:25:18.480 --> 00:25:21.600]   Amazon was it Uber that came out with Amazon?
[00:25:21.600 --> 00:25:22.640]   I forget.
[00:25:22.640 --> 00:25:26.720]   But there's at least a couple of companies that have a metadata management.
[00:25:26.720 --> 00:25:29.840]   And so the metadata is which variables are in the model.
[00:25:29.840 --> 00:25:31.880]   When was this model updated?
[00:25:31.880 --> 00:25:33.640]   When was this table updated?
[00:25:33.640 --> 00:25:34.720]   All that kind of stuff.
[00:25:34.720 --> 00:25:39.960]   And surprisingly, people actually clamor for that more so than even visibility into how
[00:25:39.960 --> 00:25:42.120]   to manage the model.
[00:25:42.120 --> 00:25:47.280]   And so this is you give, I was kind of curious what you're going to say about what the metadata
[00:25:47.280 --> 00:25:48.280]   is.
[00:25:48.280 --> 00:25:51.080]   Because I've seen examples of like this, there's metadata about the actual input data.
[00:25:51.080 --> 00:25:54.080]   And there's also metadata about like what the model is actually doing.
[00:25:54.080 --> 00:25:55.080]   Correct.
[00:25:55.080 --> 00:25:56.080]   Both are important.
[00:25:56.080 --> 00:25:57.080]   Yeah.
[00:25:57.080 --> 00:25:58.080]   Yeah.
[00:25:58.080 --> 00:25:59.080]   Go ahead.
[00:25:59.080 --> 00:26:04.200]   I was just going to say, yeah, the biggest one is usually people create data lakes.
[00:26:04.200 --> 00:26:08.840]   They throw everything into unstructured environments like S3.
[00:26:08.840 --> 00:26:12.640]   And then they need to understand what's actually going into those environments and where it's
[00:26:12.640 --> 00:26:16.600]   coming from, which is where the metadata piece comes from.
[00:26:16.600 --> 00:26:22.480]   And I guess what kinds of trouble do people run into from like not having like a, like,
[00:26:22.480 --> 00:26:23.480]   you know, standardized metadata?
[00:26:23.480 --> 00:26:25.720]   Like what are the issues that come up?
[00:26:25.720 --> 00:26:33.080]   Well, they wouldn't know, for example, which tables they can use for what, when those tables
[00:26:33.080 --> 00:26:40.120]   are being updated, how much like whether those a big thing for big companies is whether that
[00:26:40.120 --> 00:26:44.700]   data is proprietary or not, whether they're actually allowed to use it.
[00:26:44.700 --> 00:26:49.860]   There's all sorts of controls around PII, all that kind of stuff.
[00:26:49.860 --> 00:26:54.120]   And then usually for that data lake, analysts will also want to query it and they won't
[00:26:54.120 --> 00:26:56.440]   know what's in there at all.
[00:26:56.440 --> 00:27:00.780]   So it's another way to surface it in a way so that it doesn't impact production.
[00:27:00.780 --> 00:27:06.220]   So when analysts are hitting it, for example, they don't hit like the entire redshift table
[00:27:06.220 --> 00:27:09.100]   or the entire thing in BigQuery.
[00:27:09.100 --> 00:27:13.900]   It's just they know what the data is, what's available and what they can take from it and
[00:27:13.900 --> 00:27:15.400]   what they can't.
[00:27:15.400 --> 00:27:18.940]   And so are most of the models that you're building, are they running in kind of like
[00:27:18.940 --> 00:27:25.660]   online mode or like, are they, do they kind of run offline in batches or?
[00:27:25.660 --> 00:27:26.660]   A mix.
[00:27:26.660 --> 00:27:32.860]   I would say most of the models that we built for clients are online or I'm sorry, are batches.
[00:27:32.860 --> 00:27:35.860]   I'm working on a personal project now that's online.
[00:27:35.860 --> 00:27:36.860]   Oh, cool.
[00:27:36.860 --> 00:27:38.860]   Can you say what it is?
[00:27:38.860 --> 00:27:42.640]   Yeah, I'm actually almost done with it.
[00:27:42.640 --> 00:27:48.420]   So I'm working on learning GPT-2.
[00:27:48.420 --> 00:27:52.100]   And so it's like a medium think piece generator.
[00:27:52.100 --> 00:27:53.100]   How nice.
[00:27:53.100 --> 00:27:54.100]   Oh no.
[00:27:54.100 --> 00:27:59.100]   That's too dangerous to really.
[00:27:59.100 --> 00:28:00.100]   Yeah.
[00:28:00.100 --> 00:28:08.180]   So the idea is that you put in like the first few sentences of VC blog posts in there and
[00:28:08.180 --> 00:28:11.820]   it generates like a medium think piece for you.
[00:28:11.820 --> 00:28:14.460]   So hopefully that'll be online.
[00:28:14.460 --> 00:28:17.980]   But my inference time is five minutes right now.
[00:28:17.980 --> 00:28:18.980]   So maybe it won't.
[00:28:18.980 --> 00:28:19.980]   We'll see.
[00:28:19.980 --> 00:28:31.260]   Do you do any kind of like monitoring of these models?
[00:28:31.260 --> 00:28:32.260]   Is that like an issue?
[00:28:32.260 --> 00:28:35.820]   Like, cause I could, sometimes people talk about like, oh, you know, the input data changes
[00:28:35.820 --> 00:28:36.820]   and nobody notices.
[00:28:36.820 --> 00:28:39.760]   And then the model gets broken and nobody notices.
[00:28:39.760 --> 00:28:42.300]   Is that like a real issue that you've seen?
[00:28:42.300 --> 00:28:43.300]   Yeah.
[00:28:43.300 --> 00:28:47.780]   So a lot of the, I think we're just starting this as an industry.
[00:28:47.780 --> 00:28:52.380]   I know there's a lot of talk about observability and catching model drift.
[00:28:52.380 --> 00:28:57.540]   And some of the larger companies like the FANG companies are really ahead in that space.
[00:28:57.540 --> 00:29:04.380]   In general, I would say it's very much an unsolved issue and people usually still resort
[00:29:04.380 --> 00:29:09.640]   to checking the database and making sure that the data going in is okay.
[00:29:09.640 --> 00:29:13.220]   And that's kind of like the level of checking where we are.
[00:29:13.220 --> 00:29:18.100]   And I think people are just starting to say, okay, well, this is where the model was yesterday.
[00:29:18.100 --> 00:29:19.100]   This is where it is today.
[00:29:19.100 --> 00:29:20.980]   And this is where it should be tomorrow.
[00:29:20.980 --> 00:29:21.980]   Gotcha.
[00:29:21.980 --> 00:29:22.980]   Well, okay.
[00:29:22.980 --> 00:29:23.980]   I have a question.
[00:29:23.980 --> 00:29:28.020]   I've been kind of dying to ask you, but it's kind of a little bit of a non-segregated.
[00:29:28.020 --> 00:29:32.340]   So I love like working with people that know Bash because I'm always embarrassed about
[00:29:32.340 --> 00:29:33.340]   my Bash skills.
[00:29:33.340 --> 00:29:34.580]   And I feel like I always like learn.
[00:29:34.580 --> 00:29:38.940]   Do you have any like favorite Bash command that people might not know about since you
[00:29:38.940 --> 00:29:39.940]   said it's your favorite tool?
[00:29:39.940 --> 00:29:42.340]   I didn't say it was my favorite.
[00:29:42.340 --> 00:29:44.580]   I said it was an overlooked tool.
[00:29:44.580 --> 00:29:47.700]   I don't consider myself like a Bash guru by any means.
[00:29:47.700 --> 00:29:53.780]   What's one that you learned in the last year and you're like, well, it's a cool Bash.
[00:29:53.780 --> 00:29:54.780]   XARGs.
[00:29:54.780 --> 00:30:02.700]   Yeah, it lets you do parallel processing of a lot of stuff.
[00:30:02.700 --> 00:30:07.020]   So you can simulate two processes.
[00:30:07.020 --> 00:30:08.740]   Let's see.
[00:30:08.740 --> 00:30:11.020]   Cut is one that I use a lot.
[00:30:11.020 --> 00:30:17.740]   Cut and UniqueK basically lets you do like a count, count star from a database type situation.
[00:30:17.740 --> 00:30:24.020]   Yeah, I would say those are like my most commonly used ones.
[00:30:24.020 --> 00:30:25.020]   Nice.
[00:30:25.020 --> 00:30:26.020]   My go-to.
[00:30:26.020 --> 00:30:27.020]   Cool.
[00:30:27.020 --> 00:30:32.260]   Well, yeah, we always end with a couple questions.
[00:30:32.260 --> 00:30:35.780]   I mean, we've touched on some of these, but I'd be really curious to kind of hear your
[00:30:35.780 --> 00:30:36.820]   perspective on this.
[00:30:36.820 --> 00:30:40.980]   So what's like an underrated aspect of machine learning that you think people don't pay enough
[00:30:40.980 --> 00:30:45.260]   attention to?
[00:30:45.260 --> 00:30:51.420]   I think I touched on this earlier, but like the people part of machine learning, if you
[00:30:51.420 --> 00:30:57.940]   are able to get more data or better data from people rather than banging your head against
[00:30:57.940 --> 00:31:03.900]   a smaller model, that's always going to go better than trying to figure out like an advanced
[00:31:03.900 --> 00:31:06.500]   model for it.
[00:31:06.500 --> 00:31:09.860]   I've also touched on this a fair amount, but I'm curious how you'll synthesize it.
[00:31:09.860 --> 00:31:14.860]   So what is the biggest challenge of machine learning and working in the real world right
[00:31:14.860 --> 00:31:15.860]   now?
[00:31:15.860 --> 00:31:16.860]   Putting stuff in production.
[00:31:16.860 --> 00:31:17.860]   Putting stuff in production.
[00:31:17.860 --> 00:31:18.860]   Putting stuff in production.
[00:31:18.860 --> 00:31:19.860]   Yeah.
[00:31:19.860 --> 00:31:23.820]   And like within that, what's the hardest part about putting stuff in production?
[00:31:23.820 --> 00:31:29.060]   Because there's so much that you need to get right in order to make, because it's not just
[00:31:29.060 --> 00:31:30.980]   like a software system.
[00:31:30.980 --> 00:31:36.700]   It's just as complicated, but even more because software is, you have a piece of code, you
[00:31:36.700 --> 00:31:39.500]   put it in Docker, you put it somewhere and it goes.
[00:31:39.500 --> 00:31:45.620]   This is, you have to keep track of data that's flowing in from Kafka or Kinesis or streaming.
[00:31:45.620 --> 00:31:47.580]   You have to make sure that all of that data is correct.
[00:31:47.580 --> 00:31:50.340]   You have to make sure it's serialized in the right format.
[00:31:50.340 --> 00:31:55.380]   You have to make sure that the database that the data is streaming into processes it correctly.
[00:31:55.380 --> 00:31:57.540]   You have to check all of that data.
[00:31:57.540 --> 00:31:59.780]   Then you create your models.
[00:31:59.780 --> 00:32:02.860]   Your models might work one day, you might get drift the next day.
[00:32:02.860 --> 00:32:04.620]   So you have to plan for that.
[00:32:04.620 --> 00:32:09.060]   And like I said, I think we're still in the early stages of planning for that.
[00:32:09.060 --> 00:32:14.340]   Then you have to expose your model to some service or some endpoint that's going to consume
[00:32:14.340 --> 00:32:15.620]   it.
[00:32:15.620 --> 00:32:20.540]   The model piece itself, you have to put somewhere like Docker or whatever.
[00:32:20.540 --> 00:32:24.360]   You have to make sure to orchestrate all of that.
[00:32:24.360 --> 00:32:27.540]   So this is very similar to our software.
[00:32:27.540 --> 00:32:31.940]   So I think in modern software development, we have a lot of pieces of the stack that
[00:32:31.940 --> 00:32:34.760]   we're now responsible for because of DevOps.
[00:32:34.760 --> 00:32:39.040]   So DevOps means, in theory, it's supposed to make it easier for you.
[00:32:39.040 --> 00:32:44.540]   But what it means is that the software developer also now has to be a sysadmin and understand
[00:32:44.540 --> 00:32:45.540]   some of those pieces.
[00:32:45.540 --> 00:32:49.860]   And the cloud brought in the fact that you also now have to be a network expert.
[00:32:49.860 --> 00:32:53.740]   So actually a lot of my issues are troubleshooting, like why can't this service connect to this
[00:32:53.740 --> 00:32:57.140]   service over the company firewall, basically.
[00:32:57.140 --> 00:33:02.260]   So there's all of that and you have to know the data and you have to know how the model
[00:33:02.260 --> 00:33:03.780]   that you're creating works.
[00:33:03.780 --> 00:33:07.860]   So putting that all together in production is really hard.
[00:33:07.860 --> 00:33:10.260]   And so I would say that's the biggest thing.
[00:33:10.260 --> 00:33:11.260]   Well said.
[00:33:11.260 --> 00:33:14.660]   I have a feeling a lot of people listening to this podcast are going to want to hire
[00:33:14.660 --> 00:33:15.660]   you.
[00:33:15.660 --> 00:33:21.980]   So if that's the case, where can people find you?
[00:33:21.980 --> 00:33:23.220]   What's the best way to reach out?
[00:33:23.220 --> 00:33:26.580]   Well, maybe it's Twitter where you're absolutely hilarious.
[00:33:26.580 --> 00:33:28.380]   Yeah, Twitter is the best way.
[00:33:28.380 --> 00:33:35.140]   I'm just @vboykas and I also write a newsletter called Normcore Tech about all this kind of
[00:33:35.140 --> 00:33:37.660]   stuff, data and a lot more.
[00:33:37.660 --> 00:33:39.620]   And I can vouch for the newsletter personally.
[00:33:39.620 --> 00:33:44.780]   I'm a longtime subscriber, last six months subscriber since you mentioned it.
[00:33:44.780 --> 00:33:46.140]   And I definitely enjoy it too.
[00:33:46.140 --> 00:33:49.820]   So it's an honor to talk to you today.
[00:33:49.820 --> 00:33:50.820]   Thank you.
[00:33:50.820 --> 00:33:51.820]   Thank you for having me.
[00:33:51.820 --> 00:33:51.820]   Thank you.
[00:33:51.820 --> 00:33:52.320]   Thank you.
[00:33:52.320 --> 00:33:52.820]   Thank you.
[00:33:52.820 --> 00:33:53.320]   Thank you.
[00:33:53.320 --> 00:33:53.820]   Thank you.
[00:33:53.820 --> 00:33:54.320]   Thank you.
[00:33:54.320 --> 00:34:02.320]   Thank you.

