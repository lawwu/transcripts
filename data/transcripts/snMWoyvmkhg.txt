
[00:00:00.000 --> 00:00:05.800]   the scope for what we could achieve is really extraordinarily large.
[00:00:05.800 --> 00:00:09.400]   Like maybe kind of larger than most people kind of typically entertain.
[00:00:09.400 --> 00:00:13.500]   And having this kind of property of being like anti-fragile with respect to being wrong.
[00:00:13.500 --> 00:00:17.500]   Like really celebrating and endorsing, changing your mind in a kind of loud and public way.
[00:00:17.500 --> 00:00:22.500]   But you can also do a thing which is try to make a lot of money and just, you know, make a useful product
[00:00:22.500 --> 00:00:28.500]   and then use the success of that first thing to then just think squarely.
[00:00:28.500 --> 00:00:30.500]   Like how do I just do the most good?
[00:00:30.500 --> 00:00:34.000]   If you have like 10 million in the bank and you make another 10 million,
[00:00:34.000 --> 00:00:36.500]   does your life get twice as good? Obviously not, right?
[00:00:36.500 --> 00:00:40.500]   If on the other hand you just care about like making the world go well,
[00:00:40.500 --> 00:00:43.000]   then the world's an extremely big place.
[00:00:43.000 --> 00:00:46.500]   And so you basically don't run into these diminishing returns like at all.
[00:00:46.500 --> 00:00:50.000]   There's this question of like if the many worlds view is true,
[00:00:50.000 --> 00:00:54.000]   what if anything could that mean with respect to questions about like what should we do or what's important?
[00:00:55.000 --> 00:01:04.000]   [Music]
[00:01:04.000 --> 00:01:07.000]   Today I have the pleasure of interviewing Finn Morehouse,
[00:01:07.000 --> 00:01:12.000]   who is a research scholar at the Oxford University's Future of Humanity Institute.
[00:01:12.000 --> 00:01:18.000]   And he's also an assistant to Toby Ord and also the host of the Hear This Idea podcast.
[00:01:18.000 --> 00:01:21.000]   Finn, I know you've got a ton of other projects under your belt.
[00:01:21.000 --> 00:01:24.000]   So do you want to talk about all the different things you're working on
[00:01:24.000 --> 00:01:28.000]   and how you got into EA and this kind of research?
[00:01:28.000 --> 00:01:31.000]   I think you nailed the broad strokes there.
[00:01:31.000 --> 00:01:35.000]   I think, yeah, I've kind of failed to specialize in a particular thing.
[00:01:35.000 --> 00:01:39.000]   And so I found myself just dabbling in projects that seem interesting to me,
[00:01:39.000 --> 00:01:42.000]   trying to help get some projects off the ground and just doing research on,
[00:01:42.000 --> 00:01:45.000]   you know, things that seem maybe underrated.
[00:01:45.000 --> 00:01:47.000]   I probably won't bore you with the list of things.
[00:01:47.000 --> 00:01:49.000]   And then, yeah, how did I get into EA?
[00:01:49.000 --> 00:01:52.000]   Actually, also a fairly boring story, unfortunately.
[00:01:52.000 --> 00:01:54.000]   I really loved philosophy.
[00:01:54.000 --> 00:01:58.000]   I really loved kind of pestering people by asking them all these questions.
[00:01:58.000 --> 00:02:01.000]   You know, why are you not, why are you still eating meat?
[00:02:01.000 --> 00:02:03.000]   Read kind of Peter Singer and Will McCaskill.
[00:02:03.000 --> 00:02:06.000]   And I realized I just wasn't actually living these things out myself.
[00:02:06.000 --> 00:02:10.000]   I think there's some just like force of consistency
[00:02:10.000 --> 00:02:12.000]   that pushed me into really getting involved.
[00:02:12.000 --> 00:02:14.000]   I think the second piece was just the people.
[00:02:14.000 --> 00:02:17.000]   I was lucky enough to have this student group where I went to university.
[00:02:17.000 --> 00:02:20.000]   And I think there's some dynamic of realizing
[00:02:20.000 --> 00:02:22.000]   that this isn't just a kind of free floating set of ideas,
[00:02:22.000 --> 00:02:26.000]   but there's also just like a community of people I really get on with
[00:02:26.000 --> 00:02:28.000]   and have all these like incredibly interesting
[00:02:28.000 --> 00:02:30.000]   kind of personalities and interests.
[00:02:30.000 --> 00:02:32.000]   So those two things, I think.
[00:02:32.000 --> 00:02:35.000]   Yeah, and then so what was the process like?
[00:02:35.000 --> 00:02:38.000]   I know a lot of people who are vaguely interested in EA,
[00:02:38.000 --> 00:02:41.000]   but not a lot of them then very quickly transition to,
[00:02:41.000 --> 00:02:43.000]   you know, working on research with top EA researchers.
[00:02:43.000 --> 00:02:46.000]   So yeah, just walk me through how you ended up where you are.
[00:02:46.000 --> 00:02:50.000]   Yeah, I think I got lucky with the timing of the pandemic,
[00:02:50.000 --> 00:02:53.000]   which is not something I suppose many people can say.
[00:02:53.000 --> 00:02:56.000]   I did my degree. I was quite unsure about what I wanted to do.
[00:02:56.000 --> 00:03:00.000]   There was some option of taking some kind of close to default path
[00:03:00.000 --> 00:03:02.000]   of maybe something like, you know, consulting or whatever.
[00:03:02.000 --> 00:03:05.000]   And then I was kind of, I guess, forced into this natural break
[00:03:05.000 --> 00:03:07.000]   where I had time to step back.
[00:03:07.000 --> 00:03:10.000]   And I, you know, I guess I was lucky enough
[00:03:10.000 --> 00:03:12.000]   that I could afford to kind of spend a few months
[00:03:12.000 --> 00:03:14.000]   just like figuring out what I wanted to do with my life.
[00:03:14.000 --> 00:03:16.000]   And that space was enough to like,
[00:03:16.000 --> 00:03:19.000]   maybe start like reading more about these ideas.
[00:03:19.000 --> 00:03:23.000]   Also to try kind of teaching myself skills I hadn't really tried yet.
[00:03:23.000 --> 00:03:26.000]   So try to, you know, learn to code for a lot of this time and so on.
[00:03:26.000 --> 00:03:30.000]   And then I just thought, well, I might as well wing it.
[00:03:30.000 --> 00:03:31.000]   There are some things I could apply to.
[00:03:31.000 --> 00:03:32.000]   I don't really rate my chances,
[00:03:32.000 --> 00:03:34.000]   but the cost to apply to these things is so low.
[00:03:34.000 --> 00:03:35.000]   It just seems worth it.
[00:03:35.000 --> 00:03:38.000]   And then, yeah, I guess I got very lucky and here I am.
[00:03:38.000 --> 00:03:40.000]   Awesome. Okay.
[00:03:40.000 --> 00:03:42.000]   So let's talk about one of these things you're working on,
[00:03:42.000 --> 00:03:44.000]   which is that you've set up
[00:03:44.000 --> 00:03:48.000]   and are going to be helping judging these prizes about EA writing.
[00:03:48.000 --> 00:03:52.000]   One is you're giving out five prizes for $100,000 each
[00:03:52.000 --> 00:03:56.000]   for blogs that discuss effective altruist-related ideas.
[00:03:56.000 --> 00:04:01.000]   Another is five prizes of $20,000 each to criticize EA ideas.
[00:04:01.000 --> 00:04:03.000]   So talk more about these prizes.
[00:04:03.000 --> 00:04:09.000]   Why is now an important time to be talking about and criticizing EA?
[00:04:09.000 --> 00:04:11.000]   That is a good question.
[00:04:11.000 --> 00:04:14.000]   I want to say I'm reluctant to frame this as like me personally.
[00:04:14.000 --> 00:04:18.000]   I certainly have helped set up these initiatives.
[00:04:18.000 --> 00:04:24.000]   I heard on the inside that actually you've been pulling a lot of the weight on these projects.
[00:04:24.000 --> 00:04:26.000]   Certainly, yeah.
[00:04:26.000 --> 00:04:30.000]   I find myself with the time to kind of get these things over the line,
[00:04:30.000 --> 00:04:32.000]   which I'm pretty happy with.
[00:04:32.000 --> 00:04:34.000]   So, yeah, the criticism thing, let's start with that.
[00:04:34.000 --> 00:04:38.000]   I want to say something like, in general,
[00:04:38.000 --> 00:04:43.000]   being receptive to criticism is just like obviously really important.
[00:04:43.000 --> 00:04:46.000]   And if, as a movement, you want to succeed,
[00:04:46.000 --> 00:04:50.000]   where succeed means not just like achieve things in the world,
[00:04:50.000 --> 00:04:55.000]   but also like end up having just close to correct beliefs as you can get,
[00:04:55.000 --> 00:05:00.000]   then having this kind of property of being like anti-fragile with respect to being wrong,
[00:05:00.000 --> 00:05:04.000]   like really celebrating and endorsing, changing your mind in a kind of loud and public way,
[00:05:04.000 --> 00:05:06.000]   that just seems really important.
[00:05:06.000 --> 00:05:12.000]   And so I know this is just like a kind of prima facie obvious case of wanting to incentivize criticism.
[00:05:12.000 --> 00:05:14.000]   But you might also ask, like, why now?
[00:05:14.000 --> 00:05:16.000]   There are a few things going on there.
[00:05:16.000 --> 00:05:21.000]   One is, I think, the effective altruism movement overall has reached this place
[00:05:21.000 --> 00:05:24.000]   where it's actually beginning to do like a lot of really incredible things.
[00:05:24.000 --> 00:05:31.000]   There's a lot of like funders now kind of excited to find kind of fairly ambitious, scalable projects.
[00:05:31.000 --> 00:05:34.000]   And so it seems like if there's a kind of an inflection point,
[00:05:34.000 --> 00:05:39.000]   you want to get the criticism out the door and you want to respond to it like earlier rather than later,
[00:05:39.000 --> 00:05:42.000]   because you want to set the path in the right direction rather than a just course,
[00:05:42.000 --> 00:05:44.000]   which is more expensive later on.
[00:05:44.000 --> 00:05:46.000]   Will McCaskill made this point a few months ago.
[00:05:46.000 --> 00:05:49.000]   You can also point to this dynamic in some other social movements
[00:05:49.000 --> 00:05:55.000]   where the kind of really exciting beliefs that kind of have this like period of plasticity in the early days,
[00:05:55.000 --> 00:05:59.000]   they kind of ossify and you end up with this like set of beliefs,
[00:05:59.000 --> 00:06:03.000]   it's kind of like trendy or socially rewarded to hold.
[00:06:03.000 --> 00:06:06.000]   In some sense, you feel like you need to hold certain beliefs
[00:06:06.000 --> 00:06:11.000]   in order to kind of get credit from certain people.
[00:06:11.000 --> 00:06:17.000]   And the cost to like publicly questioning some practices or beliefs become too high.
[00:06:17.000 --> 00:06:19.000]   And that is just like a failure mode.
[00:06:19.000 --> 00:06:22.000]   And it seems like one of the more salient failure modes for a movement like this.
[00:06:22.000 --> 00:06:28.000]   So it just seems really important to like be quite proactive about celebrating this dynamic
[00:06:28.000 --> 00:06:31.000]   where you notice you're doing something wrong and then you change track.
[00:06:31.000 --> 00:06:34.000]   And then maybe that means shutting something down, right?
[00:06:34.000 --> 00:06:36.000]   You set up a project, the project seems really exciting.
[00:06:36.000 --> 00:06:39.000]   You get some like feedback back from the world.
[00:06:39.000 --> 00:06:42.000]   Feedback looks more negative than you expected.
[00:06:42.000 --> 00:06:44.000]   And so you stop doing the project.
[00:06:44.000 --> 00:06:47.000]   And in some important sense, that is like a success, like you did the correct thing.
[00:06:47.000 --> 00:06:48.000]   And it's important to celebrate that.
[00:06:48.000 --> 00:06:51.000]   So I think these are some of the things that go through my head,
[00:06:51.000 --> 00:06:54.000]   just like framing criticism in like this kind of positive way.
[00:06:54.000 --> 00:06:55.000]   Yeah, it seems pretty important.
[00:06:55.000 --> 00:06:56.000]   Right, right.
[00:06:56.000 --> 00:07:03.000]   I mean, analogously, it said that losses are as important as profit in terms of motivating economic incentives.
[00:07:03.000 --> 00:07:04.000]   And it seems very similar here.
[00:07:04.000 --> 00:07:09.000]   In a Slack, we were talking and you mentioned that maybe one of the reasons it's important now is
[00:07:09.000 --> 00:07:15.000]   if a prize of $20,000 can help somebody, help us figure out how to, or not me, I don't have the money,
[00:07:15.000 --> 00:07:20.000]   but like help SVF figure out how to better allocate like $10 million, that's a steal.
[00:07:20.000 --> 00:07:25.000]   It's really impressive that effective altruism is a movement that is willing to fund criticism of itself.
[00:07:25.000 --> 00:07:29.000]   I don't know, is there any other example of a movement in history where that's been so interested in
[00:07:29.000 --> 00:07:32.000]   criticizing itself and becoming anti-fragile in this way?
[00:07:32.000 --> 00:07:35.000]   I guess one thing I want to say is like the proof is in the pudding here.
[00:07:35.000 --> 00:07:39.000]   Like it's one thing to kind of make noises to the effect that you're like interested in being criticized.
[00:07:39.000 --> 00:07:40.000]   And I'm sure lots of movements make that.
[00:07:40.000 --> 00:07:42.000]   Another thing to like really follow through on them.
[00:07:42.000 --> 00:07:44.000]   And, you know, EA is a fairly young movement.
[00:07:44.000 --> 00:07:48.000]   So I guess time will tell whether it really does that well.
[00:07:48.000 --> 00:07:49.000]   I'm very hopeful.
[00:07:49.000 --> 00:07:51.000]   I also want to say that this like particular prize is like, you know,
[00:07:51.000 --> 00:07:54.000]   one kind of part of a much, a much bigger thing, hopefully.
[00:07:54.000 --> 00:07:55.000]   That's a great question.
[00:07:55.000 --> 00:08:00.000]   I actually don't know if I have good answers, but that's not to say that there are none.
[00:08:00.000 --> 00:08:01.000]   I'm sure there are.
[00:08:01.000 --> 00:08:03.000]   Like political liberalism as a strand of thought,
[00:08:03.000 --> 00:08:06.000]   like in political philosophy comes to mind as maybe an example.
[00:08:06.000 --> 00:08:09.000]   One other random thing I want to point out or mention,
[00:08:09.000 --> 00:08:13.000]   you mentioned profits and just like doing the maths and what's the like EV of like investing
[00:08:13.000 --> 00:08:16.000]   and just red teaming an idea, like shooting an idea down.
[00:08:16.000 --> 00:08:21.000]   I think thinking about the difference between the for-profit and non-profit space is quite an interesting analogy here.
[00:08:21.000 --> 00:08:25.000]   You have this very obvious feedback mechanism in for-profit land,
[00:08:25.000 --> 00:08:29.000]   which is you have an idea, no matter how excited you are about the idea,
[00:08:29.000 --> 00:08:34.000]   you can very quickly learn whether the world is as excited, which is to say you can just fail.
[00:08:34.000 --> 00:08:40.000]   And that's like a tight, useful feedback loop to figure out whether what you're doing is worth doing.
[00:08:40.000 --> 00:08:47.000]   Those feedback loops don't by default exist if you don't expect to get anything back when you're doing these projects.
[00:08:47.000 --> 00:08:53.000]   And so that's like a reason to want to implement those things like artificially.
[00:08:53.000 --> 00:08:56.000]   Like one way you can do this is with like charity evaluators,
[00:08:56.000 --> 00:09:00.000]   which in some sense impose a kind of market-like mechanism where like now you have an incentive
[00:09:00.000 --> 00:09:04.000]   to actually be achieving the thing that you are like extensively setting out to achieve
[00:09:04.000 --> 00:09:09.000]   because there's this third party that's kind of assessing whether you're getting it.
[00:09:09.000 --> 00:09:12.000]   But I think that framing, I mean, we can try saying more about it,
[00:09:12.000 --> 00:09:15.000]   but that's like a really useful framing I think to me anyway.
[00:09:15.000 --> 00:09:22.000]   And one other reason this seems important to me is if you have a movement that's about like 10 years old like this,
[00:09:22.000 --> 00:09:26.000]   you know, we have like strains of ideas that are thousands of years old
[00:09:26.000 --> 00:09:31.000]   that have significant improvements made to them that were missing before.
[00:09:31.000 --> 00:09:36.000]   So just on that alone, it seems to me that the reason to expect some mistakes,
[00:09:36.000 --> 00:09:40.000]   either at a sort of like theoretical level or in the applications,
[00:09:40.000 --> 00:09:45.000]   that does seem like I do have a strong prior that there are such mistakes
[00:09:45.000 --> 00:09:48.000]   that could be identified in a reasonable amount of time.
[00:09:48.000 --> 00:09:56.000]   Yeah. I guess one framing that I like as well is not just thinking about here's a set of claims we have,
[00:09:56.000 --> 00:10:01.000]   we want to like figure out what's wrong, but some really good criticism can look like,
[00:10:01.000 --> 00:10:04.000]   look, you just missed this distinction, which is like a really important distinction to make.
[00:10:04.000 --> 00:10:09.000]   Or you miss this like addition to this kind of naive, like conceptual framework you're using.
[00:10:09.000 --> 00:10:11.000]   And it's really important to make that addition.
[00:10:11.000 --> 00:10:18.000]   A lot of people are like skeptical about progress in kind of non-empirical fields like philosophy, for instance.
[00:10:18.000 --> 00:10:21.000]   It's like, oh, we've been thinking about these questions for thousands of years, but we're still kind of unsure.
[00:10:21.000 --> 00:10:23.000]   And I think that misses like a really important kind of progress,
[00:10:23.000 --> 00:10:26.000]   which is something you might call like conceptual engineering or something,
[00:10:26.000 --> 00:10:32.000]   which is finding these like really useful distinctions and then like building structures on top of them.
[00:10:32.000 --> 00:10:35.000]   And so it's not like you're making claims which are necessarily true or false,
[00:10:35.000 --> 00:10:41.000]   but there are other kinds of useful criticism, which include just like getting your kind of models like more, more useful.
[00:10:41.000 --> 00:10:45.000]   Speaking of just making progress on questions like these, one thing that's really surprising to me,
[00:10:45.000 --> 00:10:48.000]   and maybe this is just like my ignorance of the philosophical history here.
[00:10:48.000 --> 00:10:53.000]   It's super surprising to me that a movement like long-termism, at least in its modern form,
[00:10:53.000 --> 00:10:58.000]   it took thousands of years of philosophy before somebody had the idea that, oh, like the future could be really big.
[00:10:58.000 --> 00:11:01.000]   Therefore, the future matters a lot.
[00:11:01.000 --> 00:11:05.000]   And so maybe you could say like, oh, you know, there's been lots of movements in history that have emphasized,
[00:11:05.000 --> 00:11:10.000]   I mean, existential risk maybe wasn't a prominent thing to think about before nuclear weapons,
[00:11:10.000 --> 00:11:16.000]   but that have emphasized that civilizational collapse is a very prominent factor that might be very bad for many centuries.
[00:11:16.000 --> 00:11:19.000]   So we should try to make sure our society is stable or something.
[00:11:19.000 --> 00:11:22.000]   But do you have some sense of, you have a philosophy background.
[00:11:22.000 --> 00:11:24.000]   So do you have some sense, what is the philosophical background here?
[00:11:24.000 --> 00:11:28.000]   And to the extent that these are relatively new ideas, how did it take so long?
[00:11:28.000 --> 00:11:31.000]   Yeah, that's like such a good question, I think.
[00:11:31.000 --> 00:11:36.000]   One name that comes to mind straight away is this historian called Tom Moynihan,
[00:11:36.000 --> 00:11:42.000]   who wrote this book about something like the history of how people think about existential risk.
[00:11:42.000 --> 00:11:46.000]   And then more recently, he's been doing work on the question you asked, which is like,
[00:11:46.000 --> 00:11:52.000]   what took people so long to reach this, like what now seems like a fairly natural thought.
[00:11:52.000 --> 00:12:00.000]   I think part of what's going on here is it's really hard or easy, I should say, to underrate just how much,
[00:12:00.000 --> 00:12:04.000]   I guess it's somewhat related to what I mentioned in the last question,
[00:12:04.000 --> 00:12:08.000]   just how much kind of conceptual apparatus we have going on.
[00:12:08.000 --> 00:12:11.000]   There's like a bit like the water we swim in now. And so it's hard to notice.
[00:12:11.000 --> 00:12:18.000]   So one example that comes to mind is thinking about probability as this thing we can talk formally about.
[00:12:18.000 --> 00:12:26.000]   This is like a shockingly new thought. Also, the idea that human history might end,
[00:12:26.000 --> 00:12:32.000]   and furthermore, that that might be within our control, that is to decide or to prevent that happening prematurely.
[00:12:32.000 --> 00:12:35.000]   These are all like really surprisingly new thoughts.
[00:12:35.000 --> 00:12:42.000]   I think it just requires a lot of imagination and effort to put yourself into the shoes of people living earlier on
[00:12:42.000 --> 00:12:49.000]   who didn't have the kind of, like I said, the kind of tools for thinking that make these ideas pop out much more naturally.
[00:12:49.000 --> 00:12:54.000]   And of course, as soon as those tools are in place, then the conclusions fall out pretty quickly.
[00:12:54.000 --> 00:13:01.000]   But it's not easy. And I agree that I appreciate that actually wasn't a very good answer, just because it's such a hard question.
[00:13:01.000 --> 00:13:08.000]   Yeah. So, you know, what's interesting is that more recently, maybe I'm unaware of the full context of the argument here,
[00:13:08.000 --> 00:13:16.000]   but I think I've heard Holden Kornofsky write somewhere that he thinks that there's more value in thinking about the issues that EA has already identified
[00:13:16.000 --> 00:13:23.000]   rather than identifying some sort of unknown risk that, for example, what AI might have been like 10, 20 years ago, AI alignment, I mean.
[00:13:23.000 --> 00:13:29.000]   Given this historical experience that you can have some very fundamental tools for thinking about the world missing
[00:13:29.000 --> 00:13:37.000]   and consequently miss some very important moral implications, does that imply that we should expect the space that AI alignment occupies
[00:13:37.000 --> 00:13:41.000]   in terms of our priorities? Should we expect something as big or bigger coming up?
[00:13:41.000 --> 00:13:46.000]   Or just generally tools of thinking like, you know, expected value thinking, for example?
[00:13:46.000 --> 00:13:55.000]   Yeah, that's a good question. I think one thing I want to say there is it seems pretty likely that the most important,
[00:13:55.000 --> 00:14:00.000]   like kind of useful concepts for finding important things are also going to be the lowest hanging.
[00:14:00.000 --> 00:14:10.000]   And I think it's like very roughly correct that we did, in fact, like over the course of building out kind of conceptual frameworks,
[00:14:10.000 --> 00:14:16.000]   we picked the most important ideas first, and now we're kind of like refining things and adding maybe somewhat more peripheral things.
[00:14:16.000 --> 00:14:29.000]   That's at least if that like trend is roughly going to hold, and that's a reason for not expecting to find like some kind of earth shattering new concept from left field.
[00:14:29.000 --> 00:14:32.000]   Although I think that's like a very weak and vague argument, to be honest.
[00:14:32.000 --> 00:14:39.000]   Also, I guess it depends on what you think your time span is like, if your time span is the entire span of time that humans have been thinking about things,
[00:14:39.000 --> 00:14:45.000]   then maybe you would think that actually, it's kind of strange, it took like 3000 years before, maybe even longer.
[00:14:45.000 --> 00:14:49.000]   I guess it depends on when you define the start point, it took, you know, 3000 years for people to realize,
[00:14:49.000 --> 00:14:53.000]   hey, we should think in terms of probabilities and in terms of expected impact.
[00:14:53.000 --> 00:14:59.000]   So in that sense, maybe it's like, I don't know, it took like 3000 years of thinking to get to this very basic, very basic idea,
[00:14:59.000 --> 00:15:02.000]   what seems to us like a very important and basic idea.
[00:15:02.000 --> 00:15:05.000]   I feel like maybe I have, I want to say two things.
[00:15:05.000 --> 00:15:10.000]   If you imagined lining up like every person who ever lived, just like in a row,
[00:15:10.000 --> 00:15:14.000]   and then you kind of like walked along that line and saw how much progress people have made across the line.
[00:15:14.000 --> 00:15:17.000]   So you're going across people rather than across time.
[00:15:17.000 --> 00:15:25.000]   If you think like progress in how people think about stuff looks a lot more like linear, and in fact started earlier,
[00:15:25.000 --> 00:15:31.000]   then maybe you might think by just looking at like progress over time.
[00:15:31.000 --> 00:15:37.000]   And if it was faster early on, then if you're kind of following the very long run trend,
[00:15:37.000 --> 00:15:45.000]   then maybe you should expect like not to find these kind of, again, totally left field ideas soon.
[00:15:45.000 --> 00:15:47.000]   So I think a second thing, which is maybe more important is like,
[00:15:47.000 --> 00:15:49.000]   I also buy this idea that in some sense,
[00:15:49.000 --> 00:15:55.000]   progress in thinking about what's like most important is really kind of boundless.
[00:15:55.000 --> 00:15:58.000]   Like David Deutsch talks about this kind of thought a lot.
[00:15:58.000 --> 00:16:01.000]   When you come up with new ideas, that just generates new problems, new questions,
[00:16:01.000 --> 00:16:03.000]   release some more ideas.
[00:16:03.000 --> 00:16:04.000]   That's very well and good.
[00:16:04.000 --> 00:16:14.000]   I think there's some sense in which, you know, one priority now could just be framed as giving us time to like make that progress.
[00:16:14.000 --> 00:16:20.000]   And even if you thought that like we have this kind of boundless capacity to come up with a bunch of new important ideas,
[00:16:20.000 --> 00:16:23.000]   it's pretty obvious that that's like a prerequisite.
[00:16:23.000 --> 00:16:31.000]   And therefore, in some sense, it's like a robust argument for thinking that like trying not to kind of throw humanity off course
[00:16:31.000 --> 00:16:38.000]   and mitigating some of these catastrophic risks is always just going to shake out as like a pretty important thing to do.
[00:16:38.000 --> 00:16:40.000]   Maybe one of the most important things.
[00:16:40.000 --> 00:16:43.000]   Yeah, I think that's reasonable.
[00:16:43.000 --> 00:16:49.000]   But then there's a question of like, even if you think that the existential risk is the most important thing,
[00:16:49.000 --> 00:16:55.000]   to what extent have you discovered all the, again, that like X risk argument?
[00:16:55.000 --> 00:17:04.000]   And by the way, earlier what you said about, you know, trying to extrapolate what we might know from the limits of physical laws,
[00:17:04.000 --> 00:17:08.000]   you know, if that can kind of constrain what we think might be possible.
[00:17:08.000 --> 00:17:10.000]   I think that's an interesting idea.
[00:17:10.000 --> 00:17:16.000]   I wonder like, partly, like one argument is just like, we don't even know how to define those physical constraints.
[00:17:16.000 --> 00:17:19.000]   And like, before you had a theory of computation, it wouldn't even make sense to say like,
[00:17:19.000 --> 00:17:25.000]   oh, like this much matter can sustain like so much so much flops, floating point operations per second.
[00:17:25.000 --> 00:17:30.000]   And then second is like, yeah, if you know that number, it still doesn't tell you like what you could do with it.
[00:17:30.000 --> 00:17:37.000]   You know what, I think an interesting thing that whole Karnazes talks about is he has this article called this can't go on,
[00:17:37.000 --> 00:17:41.000]   where he makes the argument that, listen, if you just have a compounding economic growth,
[00:17:41.000 --> 00:17:53.000]   at some point, you'll get to the point where, you know, like you'll have many, many, many, many times Earth's economy per atom in the affectable universe.
[00:17:53.000 --> 00:17:57.000]   And so it's hard to see like how you could keep having economic growth beyond that point.
[00:17:57.000 --> 00:18:02.000]   But that itself seems like, I don't know, if that's true, then there has to be like a physical law.
[00:18:02.000 --> 00:18:06.000]   That's like the maximum GDP per atom is this, right?
[00:18:06.000 --> 00:18:10.000]   Like if there's no such constant, then you can like you should be able to surpass it.
[00:18:10.000 --> 00:18:13.000]   I guess it still leaves a lot to be desired.
[00:18:13.000 --> 00:18:20.000]   Even if you could know such a number, you don't know like how interesting or what kinds of things could be done at that point.
[00:18:20.000 --> 00:18:28.000]   Yeah, I guess first one is, you know, even if you think that like, preventing these kind of very large scale risks that might like curtail human potential,
[00:18:28.000 --> 00:18:32.000]   even if you think that's just incredibly important, you might miss some of those risks,
[00:18:32.000 --> 00:18:36.000]   because you just aren't able to articulate them or really like conceptualize them.
[00:18:36.000 --> 00:18:43.000]   I feel like I just want to say at some point, we have a pretty good understanding of kind of roughly what looks most important.
[00:18:43.000 --> 00:18:50.000]   Like for instance, if you kind of get stranded on a camping trip, and you're like, we need to just survive long enough to make it out.
[00:18:50.000 --> 00:18:54.000]   And it's like, okay, what do we look out for? I don't really know what the wildlife is here because I haven't been here before.
[00:18:54.000 --> 00:18:56.000]   But probably it's going to look a bit like this.
[00:18:56.000 --> 00:19:00.000]   I can at least imagine, you know, the risk of dying of thirst, even though I've never died of thirst before.
[00:19:00.000 --> 00:19:10.000]   And then it's like, what if we haven't even begun to think of like, yeah, maybe, but it's kind of, there's just some like, you know,
[00:19:10.000 --> 00:19:14.000]   table something practical reason for focusing on the things which are most salient.
[00:19:14.000 --> 00:19:17.000]   And like definitely spending some time thinking about things we haven't thought of yet.
[00:19:17.000 --> 00:19:20.000]   But it's not like that list is just like completely endless.
[00:19:20.000 --> 00:19:22.000]   And there's a kind of, I guess, a reason for that.
[00:19:22.000 --> 00:19:29.000]   And then you said the second thing, which I don't actually know if I have like a ton of interesting things to say about,
[00:19:29.000 --> 00:19:34.000]   although maybe you could try like kind of zooming in on what what you're interested in.
[00:19:34.000 --> 00:19:38.000]   I come to think of it, I don't think the second thing has a big implications for this argument.
[00:19:38.000 --> 00:19:44.000]   But the two, yeah, we have like 20 other topics that are just as interesting that we can't move on to.
[00:19:44.000 --> 00:19:50.000]   But yeah, but just as I don't know, as a closing note, the the analogy is very interesting to me.
[00:19:50.000 --> 00:19:54.000]   They can't think Trevor, you're trying to like, do what needs to be done to survive.
[00:19:54.000 --> 00:19:57.000]   I don't know. Okay, so to extend that analogy, it might be like, I don't know,
[00:19:57.000 --> 00:20:01.000]   somebody like a laser discovers, oh, that berry that we're all about to eat,
[00:20:01.000 --> 00:20:07.000]   because we feel like that's the only way to get sustenance here while we're, you know, just almost starving.
[00:20:07.000 --> 00:20:10.000]   Don't eat that berry because that berry is poisonous.
[00:20:10.000 --> 00:20:13.000]   And then then maybe somebody could point out, okay,
[00:20:13.000 --> 00:20:16.000]   so given the fact that we've discovered one poisonous food in this environment,
[00:20:16.000 --> 00:20:20.000]   should we expect there to be other poisonous foods that we don't know about?
[00:20:20.000 --> 00:20:23.000]   I don't know. I don't know if there's anything more to say on that topic.
[00:20:23.000 --> 00:20:27.000]   I mean, one thing well, like one, I guess, kind of angle you can put on this is,
[00:20:27.000 --> 00:20:31.000]   you can ask this question, like, we have precedent for a lot of things.
[00:20:31.000 --> 00:20:39.000]   Like, we know now that igniting nuclear weapons does not ignite the atmosphere,
[00:20:39.000 --> 00:20:43.000]   which was a worry that some people had.
[00:20:43.000 --> 00:20:47.000]   So we at least have some kind of bounds on how bad certain things can be.
[00:20:47.000 --> 00:20:51.000]   And so if you ask this question, like, what is worth worrying about most
[00:20:51.000 --> 00:20:57.000]   in terms of what kinds of risks might reach this level of potentially posing an existential risk?
[00:20:57.000 --> 00:21:02.000]   Well, it's going to be the kinds of things we haven't done yet that we haven't, like, got some experience with.
[00:21:02.000 --> 00:21:07.000]   And so you can ask this question, like, what is, what things are there in the space of, like,
[00:21:07.000 --> 00:21:13.000]   kind of big seeming but totally novel precedent free changes or events?
[00:21:13.000 --> 00:21:20.000]   And it actually does seem like you can kind of try generating that list and getting at answers.
[00:21:20.000 --> 00:21:25.000]   This is why maybe, or at least one reason why AI sticks out, because it's like,
[00:21:25.000 --> 00:21:29.000]   fulfills this criteria of being pretty potentially big and transformative.
[00:21:29.000 --> 00:21:32.000]   And also the kind of thing we don't have any experience with yet.
[00:21:32.000 --> 00:21:36.000]   But again, it's not as if that list is, like, in some sense, endless.
[00:21:36.000 --> 00:21:41.000]   Like, there are only so many things we can do in the space of decades, right?
[00:21:41.000 --> 00:21:44.000]   Okay, yeah. So moving on to another topic.
[00:21:44.000 --> 00:21:51.000]   We're talking about the for profit entrepreneurship as a potentially impactful thing you can do.
[00:21:51.000 --> 00:21:55.000]   Sorry, maybe not in this conversation, but like, we separately we read one point.
[00:21:55.000 --> 00:22:04.000]   Yeah. Yeah. Yeah. So to clarify, this is not just for profit, in order to do earning to give.
[00:22:04.000 --> 00:22:06.000]   So you become a billionaire and you give your wealth away.
[00:22:06.000 --> 00:22:12.000]   To what extent can you identify opportunities where you can just build a profitable company
[00:22:12.000 --> 00:22:19.000]   that solves an important problem area or, you know, makes people's lives better?
[00:22:19.000 --> 00:22:22.000]   One example of this is Wave.
[00:22:22.000 --> 00:22:29.000]   It's a company, for example, that helps with, you know, transferring money and banking services in Africa.
[00:22:29.000 --> 00:22:35.000]   Probably has a boost at people's well-being in all kinds of different ways.
[00:22:35.000 --> 00:22:42.000]   So to what extent can we expect just a bunch of for profit opportunities for making people's lives better?
[00:22:42.000 --> 00:22:44.000]   Yeah, that's a great question.
[00:22:44.000 --> 00:22:49.000]   And there is really a sense in which some of the more, like, innovative, big for profit companies
[00:22:49.000 --> 00:22:52.000]   just are, like, doing an incredibly useful thing for the world.
[00:22:52.000 --> 00:22:54.000]   They're, like, providing a service that wouldn't otherwise exist.
[00:22:54.000 --> 00:22:58.000]   And people are obviously using it because they are a successful for profit company.
[00:22:58.000 --> 00:23:03.000]   Yeah, so I guess the question is something like, you know, you're stepping back, you're asking,
[00:23:03.000 --> 00:23:07.000]   how can I, like, have a ton of impact with what I do?
[00:23:07.000 --> 00:23:10.000]   The question is, are we, like, underrating just starting a company?
[00:23:10.000 --> 00:23:14.000]   So I feel like I want to throw a bunch of kind of disconnected observations.
[00:23:14.000 --> 00:23:15.000]   We'll see if they, like, tie together.
[00:23:15.000 --> 00:23:23.000]   There is a reason why you might, in general, expect a non-profit route to do well.
[00:23:23.000 --> 00:23:27.000]   And this is, like, obviously very naive and simple.
[00:23:27.000 --> 00:23:31.000]   But where there is a for profit opportunity, you should just expect people to kind of take it.
[00:23:31.000 --> 00:23:35.000]   This is why we don't see $20 bills lying on the sidewalk.
[00:23:35.000 --> 00:23:40.000]   But the natural incentives for, in some sense, taking opportunities to, like, help people,
[00:23:40.000 --> 00:23:45.000]   where there isn't a profit opportunity, they're going to be weaker.
[00:23:45.000 --> 00:23:48.000]   And so if you're thinking about the, like, difference you make compared to whether you do something
[00:23:48.000 --> 00:23:52.000]   or whether you don't do it, in general, you might expect that to be bigger
[00:23:52.000 --> 00:23:54.000]   where you're doing something non-profit.
[00:23:54.000 --> 00:23:57.000]   Like, in particular, this is where there isn't a market for a good thing.
[00:23:57.000 --> 00:24:02.000]   So it might be because the things you're helping, like, aren't humans.
[00:24:02.000 --> 00:24:07.000]   It might be because they, like, live in the future, so they can't pay for something.
[00:24:07.000 --> 00:24:13.000]   It could also be because maybe you want to get a really impactful technology off the ground.
[00:24:13.000 --> 00:24:16.000]   In those cases, you get a kind of free rider dynamic, I think,
[00:24:16.000 --> 00:24:22.000]   where there's less reason to, like -- where you can't protect the IP and patent something.
[00:24:22.000 --> 00:24:24.000]   There's less reason to be the first mover.
[00:24:24.000 --> 00:24:28.000]   So this is, like, maybe it's not for-profit, but starting a --
[00:24:28.000 --> 00:24:31.000]   or helping kind of get a technology off the ground, which could eventually be a space
[00:24:31.000 --> 00:24:35.000]   for a bunch of for-profit companies to make a lot of money, that seems really exciting.
[00:24:35.000 --> 00:24:38.000]   Also, creating markets where there aren't markets seems really exciting.
[00:24:38.000 --> 00:24:43.000]   So, for instance, setting up, like, AMCs, advanced market commitments, or prizes,
[00:24:43.000 --> 00:24:46.000]   or just giving -- yeah, creating incentives where there aren't any,
[00:24:46.000 --> 00:24:50.000]   so you get the, like, efficiency and competition kind of gains that you get
[00:24:50.000 --> 00:24:52.000]   from the for-profit space, that seems great.
[00:24:52.000 --> 00:24:54.000]   But that's not really answering your question, because the question is, like,
[00:24:54.000 --> 00:24:56.000]   what about actual for-profit companies?
[00:24:56.000 --> 00:25:03.000]   I don't know what I have to say here, like, in terms of whether they're being underrated.
[00:25:03.000 --> 00:25:05.000]   Yeah, actually, I'm just curious what you think.
[00:25:05.000 --> 00:25:09.000]   Okay, so I think I have, like, four different reactions to --
[00:25:09.000 --> 00:25:12.000]   which is -- I've been remembering the number four, just in case I'm at three,
[00:25:12.000 --> 00:25:13.000]   and I'm like, I think I have another thing to say.
[00:25:13.000 --> 00:25:18.000]   Okay, so, yeah, so I had a draft, an essay about this that I didn't end up publishing,
[00:25:18.000 --> 00:25:21.000]   but that led to a lot of interesting discussions between us.
[00:25:21.000 --> 00:25:25.000]   So that's why we might have -- I don't know, in case the audience feels like
[00:25:25.000 --> 00:25:29.000]   they're interrupting a conversation that was already preceded the beginning here.
[00:25:29.000 --> 00:25:34.000]   So one is that, to what extent should we expect this market to be efficient?
[00:25:34.000 --> 00:25:40.000]   So one thing you can think is, listen, the amount of potential startup ideas are so vast,
[00:25:40.000 --> 00:25:45.000]   and the amount of great founders is so small that you can have a situation where
[00:25:45.000 --> 00:25:51.000]   the most profitable ideas are -- yeah, it's right that, like, somebody like Elon Musk will come up
[00:25:51.000 --> 00:25:55.000]   and, like, pluck up, like, all the -- maybe, like, the $100 billion ideas.
[00:25:55.000 --> 00:25:59.000]   But if you have, like, a company like Wave, I'm sure they're doing really well.
[00:25:59.000 --> 00:26:03.000]   But, you know, if it's not obvious how it becomes the next Google or something --
[00:26:03.000 --> 00:26:07.000]   and I guess more importantly, if it requires a lot of context, for example,
[00:26:07.000 --> 00:26:09.000]   you talked about, like, neglected groups.
[00:26:09.000 --> 00:26:13.000]   I guess this doesn't solve for animals and future people.
[00:26:13.000 --> 00:26:15.000]   But if you have something in global health where you're, like --
[00:26:15.000 --> 00:26:19.000]   a neglected group is, for example, people living in Africa, right?
[00:26:19.000 --> 00:26:24.000]   The people who could be building companies don't necessarily have experience
[00:26:24.000 --> 00:26:27.000]   with the problems that these neglected groups have.
[00:26:27.000 --> 00:26:34.000]   So if you -- it's very likely, or I guess it's possible, that you could come upon an idea,
[00:26:34.000 --> 00:26:37.000]   if you were specifically looking at how to help, for example, you know,
[00:26:37.000 --> 00:26:41.000]   people suffering from poverty in the poor parts of the world.
[00:26:41.000 --> 00:26:44.000]   You could, like, identify a problem that just, like, people who are programmers
[00:26:44.000 --> 00:26:46.000]   in Silicon Valley just wouldn't know about.
[00:26:46.000 --> 00:26:49.000]   Okay, so a bunch of other ideas regarding the other things you said.
[00:26:49.000 --> 00:26:55.000]   One is, okay, maybe a lot of progress depends on fundamental new technologies
[00:26:55.000 --> 00:26:58.000]   and companies coming at the point where the technology is already available
[00:26:58.000 --> 00:27:02.000]   and somebody needs to really implement and put all these ideas together.
[00:27:02.000 --> 00:27:03.000]   Yeah, two things on that.
[00:27:03.000 --> 00:27:06.000]   One is, like, we don't need to go into rabbit hole on this.
[00:27:06.000 --> 00:27:11.000]   One is the argument that actually the invention itself -- not the invention.
[00:27:11.000 --> 00:27:16.000]   The innovation itself is a very important aspect and potentially a bottleneck aspect of this,
[00:27:16.000 --> 00:27:19.000]   of getting an invention off the ground and scaled.
[00:27:19.000 --> 00:27:24.000]   Another is, if you can build a $100 billion company or a $1 trillion company,
[00:27:24.000 --> 00:27:26.000]   or maybe not even, just, like, a billion-dollar company,
[00:27:26.000 --> 00:27:30.000]   you have the resources to actually invest in R&D.
[00:27:30.000 --> 00:27:32.000]   I mean, think of a company like Google, right?
[00:27:32.000 --> 00:27:40.000]   Like, how many billions of dollars have they basically poured down the drain on, like, hair-brain schemes?
[00:27:40.000 --> 00:27:43.000]   You can have, like, reactions to DeepMind with regards to AI alignment,
[00:27:43.000 --> 00:27:51.000]   but, I mean, just, like, other kinds of research things they've done seem to be, like, really interesting and really useful.
[00:27:51.000 --> 00:27:55.000]   And, yeah, all the other fang companies have, like, a program like this,
[00:27:55.000 --> 00:27:58.000]   like Microsoft Research or -- I don't know what Amazon's thing is.
[00:27:58.000 --> 00:28:01.000]   And then another thing you can point out is with regards to setting up a market
[00:28:01.000 --> 00:28:07.000]   that would make other kinds of ideas possible and other kinds of businesses possible.
[00:28:07.000 --> 00:28:11.000]   In some sense, you could maybe make the argument that maybe some of the biggest companies,
[00:28:11.000 --> 00:28:15.000]   that's exactly what they've done, right, if you think of, like, Uber.
[00:28:15.000 --> 00:28:16.000]   It's not a market for companies.
[00:28:16.000 --> 00:28:21.000]   Or maybe Amazon is a much better example here where, you know, like, theoretically,
[00:28:21.000 --> 00:28:26.000]   you had an incentive before, like, if a pandemic happens, I'm going to manufacture a lot of masks, right?
[00:28:26.000 --> 00:28:32.000]   But Amazon makes the market so much more liquid so that you can just start manufacturing masks
[00:28:32.000 --> 00:28:34.000]   and now immediately put them up on Amazon.
[00:28:34.000 --> 00:28:41.000]   So it seems in these ways, actually, maybe starting a company is an effective way to deal with those kinds of problems.
[00:28:41.000 --> 00:28:43.000]   Yeah, Matt, we've gone so async here.
[00:28:43.000 --> 00:28:46.000]   I should have just, like, said one thing and then --
[00:28:46.000 --> 00:28:49.000]   Yeah, so I'm sorry for throwing lots of things at you.
[00:28:49.000 --> 00:28:50.000]   There's a lot there.
[00:28:50.000 --> 00:28:54.000]   As far as I can remember, those are all great points.
[00:28:54.000 --> 00:28:59.000]   Yeah, I think my, like, high-level thought is I'm not sure how much we disagree,
[00:28:59.000 --> 00:29:04.000]   but I guess one thing I want to say is, again, thinking about, like, in general,
[00:29:04.000 --> 00:29:10.000]   what you expect the real biggest opportunities to typically be for, like, just having a kind of impact.
[00:29:10.000 --> 00:29:16.000]   You know, one thing you might think of is if you can optimize for two things separately,
[00:29:16.000 --> 00:29:20.000]   that is optimize for the first thing and then use that to optimize for the second thing,
[00:29:20.000 --> 00:29:26.000]   versus trying to optimize for some, like, combination of the two at the same time.
[00:29:26.000 --> 00:29:28.000]   You might expect to do better if you do the first thing.
[00:29:28.000 --> 00:29:32.000]   So, for instance, you can do a thing which looks a bit like trying to do good in the world
[00:29:32.000 --> 00:29:36.000]   and also, like, make a lot of money, like, social enterprise.
[00:29:36.000 --> 00:29:38.000]   And often that goes very well.
[00:29:38.000 --> 00:29:42.000]   But you can also do a thing which is try to make a lot of money and just, you know,
[00:29:42.000 --> 00:29:47.000]   make a useful product that is not directly aimed at, you know, proving humanity's prospects or anything,
[00:29:47.000 --> 00:29:54.000]   but it's just kind of just great and then use the success of that first thing to then just think squarely,
[00:29:54.000 --> 00:30:01.000]   like, how do I just do the most good without worrying about whether there's some kind of profit mechanism.
[00:30:01.000 --> 00:30:05.000]   I think often that strategy is going to pan out well.
[00:30:05.000 --> 00:30:08.000]   There's this thought about the kind of the tails coming apart.
[00:30:08.000 --> 00:30:13.000]   If you've had this thought that at the extremes of, like, either kind of scalability
[00:30:13.000 --> 00:30:19.000]   in terms of opportunity to make a lot of profit and at the extreme of doing, like, a huge amount of goods,
[00:30:19.000 --> 00:30:24.000]   you might expect that to be, like, not such a strong correlation.
[00:30:24.000 --> 00:30:29.000]   Again, one reason, like, in particular that you might think that is because you might think the, like,
[00:30:29.000 --> 00:30:32.000]   future really matters, like, humanity's future.
[00:30:32.000 --> 00:30:38.000]   And sorry to be, like, a stuck record, but, like, there's not really, like, a natural market there
[00:30:38.000 --> 00:30:41.000]   because these people haven't been born yet.
[00:30:41.000 --> 00:30:44.000]   So that is, like, a rambly way of saying that, okay, that's not always going to be true,
[00:30:44.000 --> 00:30:49.000]   but I basically just agree that, yeah, I would want to resist a framing of doing good,
[00:30:49.000 --> 00:30:56.000]   which just leaves out, like, also doing some, starting some successful for-profit company.
[00:30:56.000 --> 00:31:00.000]   Like, there are just a ton of really excellent examples of where that's just been a huge success
[00:31:00.000 --> 00:31:02.000]   and, yeah, should be celebrated.
[00:31:02.000 --> 00:31:04.000]   So, yeah, I think I disagree with the spirit.
[00:31:04.000 --> 00:31:10.000]   Maybe we disagree somewhat on the, like, how much we should kind of relatively emphasize these different things,
[00:31:10.000 --> 00:31:13.000]   but it doesn't seem like a kind of very deep disagreement.
[00:31:13.000 --> 00:31:20.000]   Yeah, yeah, maybe I've been spending too much time with Brian Kaplan or something.
[00:31:20.000 --> 00:31:25.000]   So, by the way, the tail is coming apart, I think, is a very interesting way to think about this.
[00:31:25.000 --> 00:31:28.000]   Scott Alexander has a good article on this, and, like, one thing he points out is, like,
[00:31:28.000 --> 00:31:32.000]   yeah, generally you expect, like, different parts of different types of strength to correlate,
[00:31:32.000 --> 00:31:37.000]   but the guy who has the strongest grip strength in the world is probably not the guy who has the biggest squad in the world, right?
[00:31:37.000 --> 00:31:41.000]   Yeah, okay, so I think that's an interesting place to leave that idea.
[00:31:41.000 --> 00:31:45.000]   Another thing I wanted to talk to you about was backtesting EA.
[00:31:45.000 --> 00:31:50.000]   So, if you have these basic ideas of we want to look at problems that are important, neglected, intractable,
[00:31:50.000 --> 00:31:56.000]   and apply them throughout history, so, like, 1,000 years back, 2,000 years back, 100 years back,
[00:31:56.000 --> 00:32:05.000]   is there a context in which applying these ideas would maybe lead to a perverse outcome, an unexpected outcome?
[00:32:05.000 --> 00:32:10.000]   Are there examples where, I mean, there's many examples in history where things,
[00:32:10.000 --> 00:32:14.000]   you could have, like, easily made things much better, but maybe made it much better
[00:32:14.000 --> 00:32:19.000]   than even conventional morality or, like, present day ideas would have made them.
[00:32:19.000 --> 00:32:23.000]   So, we'll react to the first part of the question, which, as I understand it, is something like,
[00:32:23.000 --> 00:32:28.000]   can we think about what some kind of effective altruism-like movement,
[00:32:28.000 --> 00:32:31.000]   or if these ideas were in the water, like, significantly earlier,
[00:32:31.000 --> 00:32:35.000]   whether they might have misfired sometimes, or maybe they might have succeeded.
[00:32:35.000 --> 00:32:37.000]   In fact, how do we think about that at all?
[00:32:37.000 --> 00:32:43.000]   I guess one thing I want to say is that very often the correct decision, ex ante,
[00:32:43.000 --> 00:32:50.000]   is a decision which might do really well in, like, some possible outcomes,
[00:32:50.000 --> 00:32:53.000]   but you might still expect to fail, right?
[00:32:53.000 --> 00:32:57.000]   The kind of mainline outcome is, this doesn't really pan out, but it's a moon shot,
[00:32:57.000 --> 00:32:59.000]   and if it goes well, it goes really well.
[00:32:59.000 --> 00:33:03.000]   This is, I guess, similar to certain kinds of investing, where, if that's the case,
[00:33:03.000 --> 00:33:06.000]   then you should expect, even if you follow the exact, like, correct strategy,
[00:33:06.000 --> 00:33:10.000]   you should expect to look back on the decisions you make, made, rather,
[00:33:10.000 --> 00:33:17.000]   and see a bunch of failures, where failure is, you know, you just have very little impact.
[00:33:17.000 --> 00:33:21.000]   And I think it's important not to, to kind of resist the temptation to, like,
[00:33:21.000 --> 00:33:24.000]   really kind of negatively update on whether that was the correct strategy,
[00:33:24.000 --> 00:33:25.000]   because it didn't pan out.
[00:33:25.000 --> 00:33:30.000]   And so, I don't know, if something like EA-type thinking was in the water
[00:33:30.000 --> 00:33:35.000]   and was, like, thought through very well, yep, I think it would go wrong a bunch of times,
[00:33:35.000 --> 00:33:38.000]   and that shouldn't be kind of terrible news.
[00:33:38.000 --> 00:33:42.000]   When I say go wrong, I mean, like, not pan out rather than do harm.
[00:33:42.000 --> 00:33:45.000]   If it did harm, okay, that's, like, a different thing.
[00:33:45.000 --> 00:33:49.000]   I think one thing this points to, by the way, is, like, you could take it,
[00:33:49.000 --> 00:33:53.000]   you could choose to take a strategy which looks something like minimax regret, right?
[00:33:53.000 --> 00:33:56.000]   So, you have a bunch of options.
[00:33:56.000 --> 00:34:00.000]   You can ask about the kind of roughly worst case outcome,
[00:34:00.000 --> 00:34:04.000]   or just kind of, like, you know, default outcome on each option.
[00:34:04.000 --> 00:34:10.000]   And one strategy is just, like, choose the option with the least bad kind of meh case.
[00:34:10.000 --> 00:34:14.000]   And if you take the strategy, you should expect to look back on the decisions you make
[00:34:14.000 --> 00:34:18.000]   and, like, not see as many failures.
[00:34:18.000 --> 00:34:20.000]   So, that's one point in favor of it.
[00:34:20.000 --> 00:34:23.000]   Another strategy is just, like, do the best thing in expectation.
[00:34:23.000 --> 00:34:26.000]   Like, if I made these decisions constantly,
[00:34:26.000 --> 00:34:30.000]   what in the long run just ends up, like, making the world best?
[00:34:30.000 --> 00:34:33.000]   And this looks a lot like just taking the highest EV option.
[00:34:33.000 --> 00:34:37.000]   Maybe you don't want to, like, run the risk of causing harm.
[00:34:37.000 --> 00:34:39.000]   So, you know, that's okay to include.
[00:34:39.000 --> 00:34:42.000]   And, you know, I happen to think that, like, that kind of second strategy
[00:34:42.000 --> 00:34:44.000]   is very often going to be a lot better.
[00:34:44.000 --> 00:34:48.000]   And it's really important not to be misguided by this kind of feature
[00:34:48.000 --> 00:34:52.000]   of the mini max regret strategy where you look back and kind of feel a bit better about yourself
[00:34:52.000 --> 00:34:54.000]   in many cases, if that makes sense.
[00:34:54.000 --> 00:34:55.000]   Yeah, that's super interesting.
[00:34:55.000 --> 00:35:00.000]   I mean, if you think about backtesting in terms of, like, you know, models for the stock market,
[00:35:00.000 --> 00:35:04.000]   one thing that, to analogize this, one thing that tends to happen
[00:35:04.000 --> 00:35:09.000]   is that a strategy of just, like, trying to maximize returns from a given trade,
[00:35:09.000 --> 00:35:13.000]   that results very quickly in you going bankrupt because, like, sooner or later,
[00:35:13.000 --> 00:35:16.000]   there will be a trade where you lose all your money.
[00:35:16.000 --> 00:35:18.000]   And so then there's something called the Cali criterion,
[00:35:18.000 --> 00:35:22.000]   where you reserve a big portion of your money and you only bet with a certain part of it,
[00:35:22.000 --> 00:35:26.000]   which sounds more similar to the minimized regret thing here.
[00:35:26.000 --> 00:35:30.000]   Unless your expected value includes the possibility that, I mean, in this context,
[00:35:30.000 --> 00:35:34.000]   that, like, you know, like, losing all your money is, like, an existential risk, right?
[00:35:34.000 --> 00:35:38.000]   So maybe you, like, bake into the cake in the definition of expected value
[00:35:38.000 --> 00:35:40.000]   the odds of, like, losing all your money.
[00:35:40.000 --> 00:35:42.000]   Yeah, yeah, yeah, yeah.
[00:35:42.000 --> 00:35:43.000]   That's a great, that's a really great point.
[00:35:43.000 --> 00:35:48.000]   Like, I guess in some cases you want to take something which looks a bit more like the Cali bet.
[00:35:48.000 --> 00:35:51.000]   But if you add to your margins, like, relatively small margins
[00:35:51.000 --> 00:35:53.000]   compared to the kind of pot of resources you have,
[00:35:53.000 --> 00:35:56.000]   then I think it often makes sense to take, just do the best thing,
[00:35:56.000 --> 00:36:00.000]   but not worry too much about what's the kind of, like, size of the Cali bet.
[00:36:00.000 --> 00:36:02.000]   But, yeah, that's a, it's a great point.
[00:36:02.000 --> 00:36:06.000]   And, like, I guess a naive version of doing this is just kind of losing your bankroll very quickly
[00:36:06.000 --> 00:36:10.000]   because you've, like, taken two enormous bets and forgotten that it might not pan out.
[00:36:10.000 --> 00:36:12.000]   Yeah, so I appreciate that.
[00:36:12.000 --> 00:36:14.000]   Oh, what did you mean by add to the margins?
[00:36:14.000 --> 00:36:19.000]   So if you think that there's a kind of a pool of resources from which you're drawing,
[00:36:19.000 --> 00:36:21.000]   which is something like maybe philanthropic funding
[00:36:21.000 --> 00:36:24.000]   for the kind of work that you're interested in doing,
[00:36:24.000 --> 00:36:27.000]   and you're only a relatively marginal actor,
[00:36:27.000 --> 00:36:30.000]   then that's unlike being, like, an individual investor
[00:36:30.000 --> 00:36:33.000]   where you're more sensitive to the risk of just running out of money.
[00:36:33.000 --> 00:36:37.000]   And when you're more like an individual investor,
[00:36:37.000 --> 00:36:40.000]   then you want to, like, pay attention to what the size of the Cali bet is.
[00:36:40.000 --> 00:36:45.000]   If you're acting at margins, then maybe that is less of, like, a big consideration.
[00:36:45.000 --> 00:36:48.000]   Although it is obviously still, you know, a very important point.
[00:36:48.000 --> 00:36:52.000]   Well, and then, by the way, I don't know if you saw my recent blog post
[00:36:52.000 --> 00:36:55.000]   about why I think there will be more EA billionaires.
[00:36:55.000 --> 00:36:56.000]   Yes, I did.
[00:36:56.000 --> 00:36:57.000]   Okay, yeah, yeah.
[00:36:57.000 --> 00:37:00.000]   I don't know what your reaction to any of the ideas there is.
[00:37:00.000 --> 00:37:07.000]   But, like, my claim is that we should expect the total funds dedicated to EA to grow quite a lot.
[00:37:07.000 --> 00:37:10.000]   Yeah, I think-- I really liked it, by the way.
[00:37:10.000 --> 00:37:11.000]   I think it was great.
[00:37:11.000 --> 00:37:16.000]   One thing it made me think of is that there's quite an important difference
[00:37:16.000 --> 00:37:20.000]   between trying to maximize returns for yourself
[00:37:20.000 --> 00:37:24.000]   and then trying to get the most returns just, like, for the world,
[00:37:24.000 --> 00:37:26.000]   which is to say just doing the most good,
[00:37:26.000 --> 00:37:31.000]   where one consideration we've just talked about,
[00:37:31.000 --> 00:37:34.000]   which is the risk of just, like, losing your bankroll,
[00:37:34.000 --> 00:37:37.000]   which is where, like, Kelly betting becomes relevant.
[00:37:37.000 --> 00:37:41.000]   Another consideration is that as an individual,
[00:37:41.000 --> 00:37:44.000]   just, like, trying to do the best for yourself,
[00:37:44.000 --> 00:37:49.000]   you have, like, pretty steeply diminishing returns from money
[00:37:49.000 --> 00:37:52.000]   or just, like, how well your life goes with that extra money, right?
[00:37:52.000 --> 00:37:55.000]   So, like, if you have, like, $10 million in the bank
[00:37:55.000 --> 00:37:58.000]   and you make another $10 million, does your life get twice as good?
[00:37:58.000 --> 00:37:59.000]   Obviously not, right?
[00:37:59.000 --> 00:38:01.000]   And as such, you should be kind of risk-averse
[00:38:01.000 --> 00:38:05.000]   when you're thinking about the possibility of, like, making a load of money.
[00:38:05.000 --> 00:38:09.000]   If, on the other hand, you just care about, like, making the world go well,
[00:38:09.000 --> 00:38:12.000]   then the world's an extremely big place
[00:38:12.000 --> 00:38:16.000]   and so you basically don't run into these diminishing returns, like, at all.
[00:38:16.000 --> 00:38:19.000]   And for that reason, like, if you're making money,
[00:38:19.000 --> 00:38:22.000]   at least in part to, in some sense, give it away
[00:38:22.000 --> 00:38:27.000]   or otherwise just, like, have a positive effect in some impartial sense,
[00:38:27.000 --> 00:38:30.000]   then you're going to be less risk-averse,
[00:38:30.000 --> 00:38:35.000]   which means maybe you fail more often,
[00:38:35.000 --> 00:38:38.000]   but it also means that people who succeed, like, succeed really hard.
[00:38:38.000 --> 00:38:41.000]   So I don't know if that's, in some sense, I'm just recycling what you said,
[00:38:41.000 --> 00:38:44.000]   but I think it's, like, a really kind of neat observation.
[00:38:44.000 --> 00:38:48.000]   Well, and another interesting thing is that not only is that true,
[00:38:48.000 --> 00:38:51.000]   but then you're also, you're also in a movement
[00:38:51.000 --> 00:38:54.000]   where everybody else has a similar idea.
[00:38:54.000 --> 00:38:57.000]   And not only is that true, but also the movement is full of people
[00:38:57.000 --> 00:39:02.000]   who are young, techie, smart, and, as you said, risk-neutral.
[00:39:02.000 --> 00:39:06.000]   So basically, people who are going to be way overrepresented
[00:39:06.000 --> 00:39:09.000]   in the ranks of future billionaires.
[00:39:09.000 --> 00:39:12.000]   And they're all hanging out and they have this idea that, you know,
[00:39:12.000 --> 00:39:16.000]   we can become rich together and then make the world better by doing so.
[00:39:16.000 --> 00:39:18.000]   You would expect that this would be exactly the kind of situation
[00:39:18.000 --> 00:39:21.000]   that would lead to people teaming up and starting billion-dollar companies.
[00:39:21.000 --> 00:39:24.000]   All right. Yeah, so a bunch of other topics in effective altruism
[00:39:24.000 --> 00:39:26.000]   that I wanted to ask you about.
[00:39:26.000 --> 00:39:29.000]   So one is, should it impact our decisions in any ways
[00:39:29.000 --> 00:39:32.000]   if the many-worlds interpretation of quantum mechanics is true?
[00:39:32.000 --> 00:39:35.000]   I know the argument that, oh, you can just think of,
[00:39:35.000 --> 00:39:37.000]   you can just translate amplitudes to probabilities,
[00:39:37.000 --> 00:39:41.000]   and if it's just probabilities, then decision theory doesn't change.
[00:39:41.000 --> 00:39:45.000]   My problem with this is I've gotten, like, very lucky in the last few months.
[00:39:45.000 --> 00:39:49.000]   Now, I think it, like, changes my perception of that
[00:39:49.000 --> 00:39:52.000]   if I realize, actually, most me's--
[00:39:52.000 --> 00:39:55.000]   and, okay, I know there's, like, problems with saying "me's."
[00:39:55.000 --> 00:39:57.000]   To what extent they're fungible.
[00:39:57.000 --> 00:40:04.000]   Most branches of the multiverse, like, I'm, like, significantly worse often.
[00:40:04.000 --> 00:40:09.000]   That makes it worse than, oh, I just got lucky, but, like, now I'm here.
[00:40:09.000 --> 00:40:13.000]   And another thing is, if you think of existential risk
[00:40:13.000 --> 00:40:17.000]   and you think that even if, like, existential risk is very likely,
[00:40:17.000 --> 00:40:20.000]   in some branch of the multiverse, humanity survives,
[00:40:20.000 --> 00:40:23.000]   I don't know, that seems better in the end than,
[00:40:23.000 --> 00:40:25.000]   oh, the probability was really low,
[00:40:25.000 --> 00:40:28.000]   but, like, it just resolved to we didn't survive.
[00:40:28.000 --> 00:40:30.000]   Does that make sense?
[00:40:30.000 --> 00:40:32.000]   Okay, all right, there's a lot there.
[00:40:32.000 --> 00:40:35.000]   I guess rather than doing a terrible job at trying to explain
[00:40:35.000 --> 00:40:37.000]   what this many-worlds thing is about, maybe it's worth
[00:40:37.000 --> 00:40:40.000]   just kind of pointing people towards, you know, just Googling it.
[00:40:40.000 --> 00:40:42.000]   I should also add this enormous caveat
[00:40:42.000 --> 00:40:44.000]   that I don't really know what I'm talking about.
[00:40:44.000 --> 00:40:48.000]   This is just kind of an outsider who's taken this kind of, I don't know,
[00:40:48.000 --> 00:40:50.000]   this stuff seems interesting.
[00:40:50.000 --> 00:40:53.000]   Yeah, okay, so there's this question of, like, what,
[00:40:53.000 --> 00:40:56.000]   if the many-worlds view is true,
[00:40:56.000 --> 00:40:59.000]   what, if anything, could that mean with respect to questions
[00:40:59.000 --> 00:41:01.000]   about, like, what should we do or what's important?
[00:41:01.000 --> 00:41:05.000]   And one thing I want to say is, just, like, without zooming into anything,
[00:41:05.000 --> 00:41:09.000]   it just seems like a huge deal, like, every second, every day,
[00:41:09.000 --> 00:41:13.000]   I'm in some sense, like, just kind of dissolving
[00:41:13.000 --> 00:41:16.000]   into this, like, cloud of me's and, like, just kind of
[00:41:16.000 --> 00:41:18.000]   unimaginably large number of me's.
[00:41:18.000 --> 00:41:20.000]   And that, each of those me's is kind of, in some sense,
[00:41:20.000 --> 00:41:22.000]   dissolving into more clouds.
[00:41:22.000 --> 00:41:24.000]   This is just, like, wild.
[00:41:24.000 --> 00:41:28.000]   Also seems somewhat likely to be true,
[00:41:28.000 --> 00:41:31.000]   as, like, far as I can tell.
[00:41:31.000 --> 00:41:33.000]   Okay, so, like, what does this mean?
[00:41:33.000 --> 00:41:36.000]   You, yeah, you point out that you can
[00:41:36.000 --> 00:41:39.000]   talk about having a measure over worlds.
[00:41:39.000 --> 00:41:41.000]   In some sense, you can, there's actually a problem
[00:41:41.000 --> 00:41:44.000]   of how you get, like, probabilities, or how you make sense of probabilities
[00:41:44.000 --> 00:41:46.000]   on the many worlds view.
[00:41:46.000 --> 00:41:48.000]   And there's a kind of neat way of doing that, which, like,
[00:41:48.000 --> 00:41:51.000]   makes use of questions about how you should make decisions.
[00:41:51.000 --> 00:41:54.000]   That is, you should just kind of weigh future you's
[00:41:54.000 --> 00:41:56.000]   according to, in some sense, how likely they are.
[00:41:56.000 --> 00:41:58.000]   But it's really the reverse. You're, like, explaining
[00:41:58.000 --> 00:42:00.000]   what it means for them to be more likely in terms of
[00:42:00.000 --> 00:42:02.000]   how it's rational to weigh them.
[00:42:02.000 --> 00:42:05.000]   And then, I think it's, like, a ton of very vague things
[00:42:05.000 --> 00:42:07.000]   I can try saying, so maybe I'll just try doing, like,
[00:42:07.000 --> 00:42:09.000]   a brain dump of things.
[00:42:09.000 --> 00:42:11.000]   But I think that, like, many worlds being true
[00:42:11.000 --> 00:42:14.000]   could push you towards being more risk neutral
[00:42:14.000 --> 00:42:17.000]   in certain cases, if you weren't before.
[00:42:17.000 --> 00:42:20.000]   Because, in certain cases, you're, like, translating from
[00:42:20.000 --> 00:42:23.000]   some chance of this thing happening, or it doesn't,
[00:42:23.000 --> 00:42:26.000]   into some fraction of, you know, worlds
[00:42:26.000 --> 00:42:29.000]   this thing does happen, and another fraction it doesn't.
[00:42:29.000 --> 00:42:32.000]   That's kind of, like, I do think it's worth reading too much into that,
[00:42:32.000 --> 00:42:34.000]   because I think a lot of the, like, important uncertainties
[00:42:34.000 --> 00:42:36.000]   about the world are still, like, subjective uncertainties
[00:42:36.000 --> 00:42:38.000]   about how most worlds will, in fact, turn out.
[00:42:38.000 --> 00:42:40.000]   But it's kind of interesting and notable that you can, like,
[00:42:40.000 --> 00:42:43.000]   convert between overall uncertainty about how things
[00:42:43.000 --> 00:42:46.000]   turn out to, like, more certainty about the fraction
[00:42:46.000 --> 00:42:48.000]   of ways things turn out.
[00:42:48.000 --> 00:42:52.000]   I think another, like, interesting feature of this is that
[00:42:52.000 --> 00:42:55.000]   so the question of, like, how you should act
[00:42:55.000 --> 00:42:58.000]   is no longer the question of, like, how should you, kind of,
[00:42:58.000 --> 00:43:01.000]   benefit this person who is you in the future,
[00:43:01.000 --> 00:43:04.000]   who is one person. It's more, like, how do you benefit
[00:43:04.000 --> 00:43:07.000]   this, like, cloud of people who are all successive you?
[00:43:07.000 --> 00:43:10.000]   That's just kind of, like, diffusing into the future.
[00:43:10.000 --> 00:43:12.000]   And I think you point out that you could just, like,
[00:43:12.000 --> 00:43:15.000]   basically salvage a lot of, basically, all decision theory,
[00:43:15.000 --> 00:43:18.000]   even if that's true. But the, like, picture of what's going on
[00:43:18.000 --> 00:43:22.000]   changes, and in particular, I think just intuitively, like,
[00:43:22.000 --> 00:43:26.000]   it feels to me like the gap between acting in a self-interested way
[00:43:26.000 --> 00:43:28.000]   and then, like, acting in an impartial way,
[00:43:28.000 --> 00:43:30.000]   where you're, like, helping other people.
[00:43:30.000 --> 00:43:33.000]   It kind of closes a little in a way.
[00:43:33.000 --> 00:43:36.000]   Like, you're already benefiting many people
[00:43:36.000 --> 00:43:40.000]   by doing the thing that's kind of rational to benefit you,
[00:43:40.000 --> 00:43:43.000]   which isn't so far from benefiting people who aren't, like,
[00:43:43.000 --> 00:43:45.000]   continuous with you in this special way.
[00:43:45.000 --> 00:43:47.000]   So I kind of like that as a thing.
[00:43:47.000 --> 00:43:49.000]   Ah. So interesting.
[00:43:49.000 --> 00:43:52.000]   Yeah. And then, okay, there is also this, like,
[00:43:52.000 --> 00:43:55.000]   slightly more out there thought, which is,
[00:43:55.000 --> 00:43:57.000]   here's the thing you could say.
[00:43:57.000 --> 00:44:00.000]   If many worlds is true, then there is at least a sense
[00:44:00.000 --> 00:44:03.000]   in which there are very, very many more people
[00:44:03.000 --> 00:44:06.000]   in the future compared to the past.
[00:44:06.000 --> 00:44:09.000]   Like, just unimaginably many more.
[00:44:09.000 --> 00:44:11.000]   And even, like, the next second from now,
[00:44:11.000 --> 00:44:12.000]   there are many more people.
[00:44:12.000 --> 00:44:14.000]   So you might think that should, like,
[00:44:14.000 --> 00:44:17.000]   make us have a really steep negative discount rate
[00:44:17.000 --> 00:44:19.000]   on the future, which is to say we should, like,
[00:44:19.000 --> 00:44:21.000]   value future times much more than present times.
[00:44:21.000 --> 00:44:23.000]   And, like, in a way which would just kind of,
[00:44:23.000 --> 00:44:25.000]   it wouldn't, like, modify how we should act.
[00:44:25.000 --> 00:44:28.000]   It just, like, explodes how we should think about this.
[00:44:28.000 --> 00:44:30.000]   This definitely doesn't seem right.
[00:44:30.000 --> 00:44:32.000]   Maybe one way to think about this is that
[00:44:32.000 --> 00:44:35.000]   if this thought was true, or, like,
[00:44:35.000 --> 00:44:37.000]   was kind of directionally true,
[00:44:37.000 --> 00:44:39.000]   then that might also be a reason for being
[00:44:39.000 --> 00:44:41.000]   extremely surprised that we're both speaking
[00:44:41.000 --> 00:44:44.000]   at, like, an earlier time rather than a later time.
[00:44:44.000 --> 00:44:45.000]   Because if you think you're just, like,
[00:44:45.000 --> 00:44:48.000]   randomly drawn from all the people who ever lived,
[00:44:48.000 --> 00:44:50.000]   it's, like, absolutely mind-blowing that we get drawn
[00:44:50.000 --> 00:44:52.000]   from, like, today rather than tomorrow.
[00:44:52.000 --> 00:44:53.000]   Yeah, yeah.
[00:44:53.000 --> 00:44:54.000]   Given that there's, like, 10 to the something
[00:44:54.000 --> 00:44:56.000]   many more people than tomorrow.
[00:44:56.000 --> 00:45:00.000]   So it's probably wrong, and wrong for reasons
[00:45:00.000 --> 00:45:02.000]   I don't have a very good handle on, because
[00:45:02.000 --> 00:45:04.000]   I just, like, don't know what I'm talking about.
[00:45:04.000 --> 00:45:05.000]   I mean, I can kind of try parroting the reasons,
[00:45:05.000 --> 00:45:06.000]   but, like, it's something I'm, you know,
[00:45:06.000 --> 00:45:08.000]   I'm interested in trying to really crock
[00:45:08.000 --> 00:45:10.000]   those reasons a bit more.
[00:45:10.000 --> 00:45:12.000]   That's really interesting.
[00:45:12.000 --> 00:45:14.000]   I didn't think about that argument
[00:45:14.000 --> 00:45:16.000]   for the selection argument.
[00:45:16.000 --> 00:45:18.000]   I think one resolution I've heard about this
[00:45:18.000 --> 00:45:21.000]   is that you can think of the proportion of,
[00:45:21.000 --> 00:45:23.000]   you know, Hilbert space, or, like,
[00:45:23.000 --> 00:45:25.000]   the proportion of all the, like,
[00:45:25.000 --> 00:45:27.000]   the universe's wave function.
[00:45:27.000 --> 00:45:30.000]   That could be the probability
[00:45:30.000 --> 00:45:34.000]   rather than each different branch.
[00:45:34.000 --> 00:45:35.000]   You know what I just realized?
[00:45:35.000 --> 00:45:37.000]   The selection argument you made,
[00:45:37.000 --> 00:45:40.000]   maybe that's an argument against Bostrom's
[00:45:40.000 --> 00:45:42.000]   idea of we're living in a simulation,
[00:45:42.000 --> 00:45:44.000]   because basically his argument is that
[00:45:44.000 --> 00:45:46.000]   there would be many more simulations
[00:45:46.000 --> 00:45:48.000]   than there are real copies of you,
[00:45:48.000 --> 00:45:50.000]   therefore you're probably in a simulation.
[00:45:50.000 --> 00:45:52.000]   The thing about, like, saying that
[00:45:52.000 --> 00:45:55.000]   all the simulations plus you are,
[00:45:55.000 --> 00:45:57.000]   your prior should be equally distributed
[00:45:57.000 --> 00:46:00.000]   among them seems similar to saying
[00:46:00.000 --> 00:46:02.000]   your prior of being distributed
[00:46:02.000 --> 00:46:05.000]   along each possible, like, branch
[00:46:05.000 --> 00:46:08.000]   of the wave function should be,
[00:46:08.000 --> 00:46:10.000]   your prior across them should be the same,
[00:46:10.000 --> 00:46:13.000]   whereas I think in the context of
[00:46:13.000 --> 00:46:15.000]   the wave function you were arguing
[00:46:15.000 --> 00:46:17.000]   that maybe it should be, like,
[00:46:17.000 --> 00:46:18.000]   you shouldn't think about it that way,
[00:46:18.000 --> 00:46:19.000]   you should think about, like, maybe a proportion
[00:46:19.000 --> 00:46:22.000]   of the total Hilbert space.
[00:46:22.000 --> 00:46:24.000]   Yeah, yeah, yeah.
[00:46:24.000 --> 00:46:25.000]   Does that make sense?
[00:46:25.000 --> 00:46:27.000]   I don't know if I...
[00:46:27.000 --> 00:46:29.000]   Wait, say it again, how it links
[00:46:29.000 --> 00:46:31.000]   into simulation type stuff.
[00:46:31.000 --> 00:46:33.000]   Instead of thinking about each possible
[00:46:33.000 --> 00:46:35.000]   simulation as an individual thing
[00:46:35.000 --> 00:46:38.000]   across which, that is equally as likely,
[00:46:38.000 --> 00:46:40.000]   each individual instance of a simulation
[00:46:40.000 --> 00:46:41.000]   is equally as likely as you living
[00:46:41.000 --> 00:46:43.000]   in the real world, maybe simulation
[00:46:43.000 --> 00:46:45.000]   as a whole is equally likely to you
[00:46:45.000 --> 00:46:47.000]   living in the real world, just as
[00:46:47.000 --> 00:46:49.000]   you being alive today rather than tomorrow
[00:46:49.000 --> 00:46:51.000]   is equally likely, despite the fact
[00:46:51.000 --> 00:46:53.000]   that there will be many more branches
[00:46:53.000 --> 00:46:56.000]   and new branches of the wave function tomorrow.
[00:46:56.000 --> 00:46:58.000]   Yeah, okay, there's a lot going on.
[00:46:58.000 --> 00:46:59.000]   I feel like there are people who actually
[00:46:59.000 --> 00:47:00.000]   know what they're talking about here,
[00:47:00.000 --> 00:47:01.000]   just tearing their hair out, like,
[00:47:01.000 --> 00:47:03.000]   "You want to do this obvious thing?"
[00:47:03.000 --> 00:47:05.000]   That's the nature of opening a podcast.
[00:47:05.000 --> 00:47:07.000]   That's the point.
[00:47:07.000 --> 00:47:08.000]   You...
[00:47:08.000 --> 00:47:10.000]   But by the way, if you are one such person,
[00:47:10.000 --> 00:47:12.000]   please do, like, email me or DM me
[00:47:12.000 --> 00:47:13.000]   or something, I'm very interested.
[00:47:13.000 --> 00:47:14.000]   Yeah, sounds good.
[00:47:14.000 --> 00:47:15.000]   So yeah, you mentioned, like,
[00:47:15.000 --> 00:47:17.000]   obviously there is a measure over worlds
[00:47:17.000 --> 00:47:20.000]   and this, like, lets you talk about
[00:47:20.000 --> 00:47:21.000]   things being sensible again.
[00:47:21.000 --> 00:47:24.000]   Also, maybe, like, one minor thing
[00:47:24.000 --> 00:47:26.000]   to comment on is talking about probabilities
[00:47:26.000 --> 00:47:30.000]   is kind of hard because in many worlds
[00:47:30.000 --> 00:47:32.000]   just everything happens that can happen.
[00:47:32.000 --> 00:47:34.000]   And so it's, like, difficult to get
[00:47:34.000 --> 00:47:35.000]   the language exactly right.
[00:47:35.000 --> 00:47:37.000]   But anyway, so totally get the point.
[00:47:37.000 --> 00:47:40.000]   And then the question of how it maps on
[00:47:40.000 --> 00:47:41.000]   to simulation-type thoughts.
[00:47:41.000 --> 00:47:42.000]   Here's a...
[00:47:42.000 --> 00:47:44.000]   I don't know, like, maybe a thought
[00:47:44.000 --> 00:47:46.000]   which kind of connects to this.
[00:47:46.000 --> 00:47:47.000]   Do you know, like,
[00:47:47.000 --> 00:47:49.000]   sleeping beauty-type problems?
[00:47:49.000 --> 00:47:51.000]   Um, no, no.
[00:47:51.000 --> 00:47:54.000]   Okay, this is only a vaguely remembered example.
[00:47:54.000 --> 00:47:55.000]   But let's start.
[00:47:55.000 --> 00:47:57.000]   So in the original sleeping beauty problem,
[00:47:57.000 --> 00:47:59.000]   you go to sleep.
[00:47:59.000 --> 00:48:00.000]   Okay?
[00:48:00.000 --> 00:48:02.000]   And then I flip a coin.
[00:48:02.000 --> 00:48:03.000]   Or, you know, whoever.
[00:48:03.000 --> 00:48:05.000]   Someone flips a coin.
[00:48:05.000 --> 00:48:06.000]   If it comes up tails,
[00:48:06.000 --> 00:48:09.000]   they wake you up once.
[00:48:09.000 --> 00:48:11.000]   If it comes up heads,
[00:48:11.000 --> 00:48:13.000]   they wake you up once.
[00:48:13.000 --> 00:48:14.000]   And then they...
[00:48:14.000 --> 00:48:16.000]   You go back to sleep and, you know,
[00:48:16.000 --> 00:48:17.000]   your memory is wiped.
[00:48:17.000 --> 00:48:18.000]   And then you're waking up again
[00:48:18.000 --> 00:48:20.000]   as if you're being woken up
[00:48:20.000 --> 00:48:21.000]   in the other world.
[00:48:21.000 --> 00:48:23.000]   And, um...
[00:48:23.000 --> 00:48:25.000]   Okay, so you go to sleep.
[00:48:25.000 --> 00:48:26.000]   You wake up.
[00:48:26.000 --> 00:48:27.000]   And you're asked,
[00:48:27.000 --> 00:48:31.000]   "What is the chance that the coin came up heads or tails?"
[00:48:31.000 --> 00:48:32.000]   And it feels like
[00:48:32.000 --> 00:48:34.000]   there's kind of really intuitive reasons
[00:48:34.000 --> 00:48:36.000]   for both 50% and 1/3.
[00:48:36.000 --> 00:48:39.000]   Here's a related question,
[00:48:39.000 --> 00:48:40.000]   which is maybe a bit simpler,
[00:48:40.000 --> 00:48:41.000]   at least in my head.
[00:48:41.000 --> 00:48:43.000]   I flip a coin.
[00:48:43.000 --> 00:48:44.000]   It comes up heads.
[00:48:44.000 --> 00:48:46.000]   I, like, just make a world
[00:48:46.000 --> 00:48:47.000]   with one observer in it.
[00:48:47.000 --> 00:48:49.000]   And if it comes up tails,
[00:48:49.000 --> 00:48:51.000]   I make a world with 100 observers in it.
[00:48:51.000 --> 00:48:52.000]   Maybe it could be, like,
[00:48:52.000 --> 00:48:53.000]   running simulations with 100 people.
[00:48:53.000 --> 00:48:55.000]   You, like, wake up in one of these worlds.
[00:48:55.000 --> 00:48:57.000]   You don't know how many other people
[00:48:57.000 --> 00:48:58.000]   are there in the world.
[00:48:58.000 --> 00:48:59.000]   You just know that, like,
[00:48:59.000 --> 00:49:00.000]   someone has flipped a coin
[00:49:00.000 --> 00:49:01.000]   and decided to make a world
[00:49:01.000 --> 00:49:03.000]   with either one or 100 people in it.
[00:49:03.000 --> 00:49:04.000]   What is the chance
[00:49:04.000 --> 00:49:06.000]   that you're in the world with 100 people?
[00:49:06.000 --> 00:49:07.000]   And, like,
[00:49:07.000 --> 00:49:09.000]   there's a reason for thinking it's half.
[00:49:09.000 --> 00:49:10.000]   And there's a reason for thinking
[00:49:10.000 --> 00:49:12.000]   that it's, like, I don't know,
[00:49:12.000 --> 00:49:14.000]   100 over 101.
[00:49:14.000 --> 00:49:15.000]   Does that make sense?
[00:49:15.000 --> 00:49:17.000]   So I understand the logic behind the half.
[00:49:17.000 --> 00:49:19.000]   What is the reason for thinking...
[00:49:19.000 --> 00:49:21.000]   I mean, regardless of where you ended up
[00:49:21.000 --> 00:49:22.000]   as the observer, it seems like,
[00:49:22.000 --> 00:49:25.000]   if the odds of, like, the coin coming up heads...
[00:49:25.000 --> 00:49:27.000]   Oh, I guess, is it because you'd expect
[00:49:27.000 --> 00:49:28.000]   there to be more observers
[00:49:28.000 --> 00:49:30.000]   in the other universe?
[00:49:30.000 --> 00:49:31.000]   Like, yeah.
[00:49:31.000 --> 00:49:32.000]   So what is the logic for thinking
[00:49:32.000 --> 00:49:34.000]   it might be 100 over 101?
[00:49:34.000 --> 00:49:35.000]   Well, you might think of it like this.
[00:49:35.000 --> 00:49:38.000]   How should I reason about where I am?
[00:49:38.000 --> 00:49:40.000]   Well, maybe it's something like this.
[00:49:40.000 --> 00:49:42.000]   I'm just a random observer, right?
[00:49:42.000 --> 00:49:43.000]   Of all the possible observers
[00:49:43.000 --> 00:49:44.000]   that could have come out of this.
[00:49:44.000 --> 00:49:46.000]   And there are 101 possible observers.
[00:49:46.000 --> 00:49:47.000]   And you can just imagine that
[00:49:47.000 --> 00:49:48.000]   I've been, like, randomly drawn.
[00:49:48.000 --> 00:49:49.000]   Okay?
[00:49:49.000 --> 00:49:51.000]   And if I'm randomly drawn
[00:49:51.000 --> 00:49:52.000]   from all the possible observers,
[00:49:52.000 --> 00:49:54.000]   then it's overwhelmingly likely
[00:49:54.000 --> 00:49:56.000]   that I'm in the big world.
[00:49:56.000 --> 00:49:57.000]   Huh.
[00:49:57.000 --> 00:49:59.000]   That's super interesting.
[00:49:59.000 --> 00:50:00.000]   I should say, actually,
[00:50:00.000 --> 00:50:01.000]   I should plug someone
[00:50:01.000 --> 00:50:02.000]   who does know what they're talking about
[00:50:02.000 --> 00:50:04.000]   on this, which is Joe Carlsmith,
[00:50:04.000 --> 00:50:05.000]   who has, like, a series of, like,
[00:50:05.000 --> 00:50:07.000]   really excellent blog posts.
[00:50:07.000 --> 00:50:08.000]   Oh.
[00:50:08.000 --> 00:50:09.000]   He's coming on the podcast next week.
[00:50:09.000 --> 00:50:10.000]   Yes, amazing.
[00:50:10.000 --> 00:50:11.000]   I'm definitely going to bring him.
[00:50:11.000 --> 00:50:12.000]   Okay, you should ask him about this
[00:50:12.000 --> 00:50:13.000]   because he's, like,
[00:50:13.000 --> 00:50:14.000]   he's going to talk to you about it.
[00:50:14.000 --> 00:50:15.000]   I don't want to, like --
[00:50:15.000 --> 00:50:16.000]   Okay, I don't want to scoop him,
[00:50:16.000 --> 00:50:18.000]   but one thought that comes from him,
[00:50:18.000 --> 00:50:19.000]   which is just, like, really cool,
[00:50:19.000 --> 00:50:21.000]   maybe just to kind of round this off,
[00:50:21.000 --> 00:50:23.000]   is if you're, like,
[00:50:23.000 --> 00:50:26.000]   a 100/101-er on examples like this,
[00:50:26.000 --> 00:50:28.000]   and you think there's any chance
[00:50:28.000 --> 00:50:29.000]   that, like, the universe
[00:50:29.000 --> 00:50:30.000]   is infinite in size,
[00:50:30.000 --> 00:50:32.000]   then you should think that
[00:50:32.000 --> 00:50:34.000]   the chance you're in a universe
[00:50:34.000 --> 00:50:36.000]   that is infinite in extent
[00:50:36.000 --> 00:50:38.000]   is just, like, one or close to one,
[00:50:38.000 --> 00:50:39.000]   if that makes sense.
[00:50:39.000 --> 00:50:40.000]   Hmm.
[00:50:40.000 --> 00:50:41.000]   I see. Yeah, yeah.
[00:50:41.000 --> 00:50:42.000]   Okay, so in the end,
[00:50:42.000 --> 00:50:45.000]   does your awareness of many worlds
[00:50:45.000 --> 00:50:47.000]   is, like, a good explanation?
[00:50:47.000 --> 00:50:49.000]   Has that impacted your view
[00:50:49.000 --> 00:50:51.000]   of what should be done in any way?
[00:50:51.000 --> 00:50:53.000]   Yeah, so I don't really know
[00:50:53.000 --> 00:50:55.000]   if I have a good answer.
[00:50:55.000 --> 00:50:57.000]   My best guess is that things just shake out
[00:50:57.000 --> 00:50:59.000]   to kind of where they started,
[00:50:59.000 --> 00:51:00.000]   as long as you start off
[00:51:00.000 --> 00:51:01.000]   in this kind of, like,
[00:51:01.000 --> 00:51:03.000]   relatively risk-neutral place.
[00:51:03.000 --> 00:51:06.000]   I suspect that if many worlds is true,
[00:51:06.000 --> 00:51:07.000]   this might have, like --
[00:51:07.000 --> 00:51:08.000]   this might make it much harder
[00:51:08.000 --> 00:51:11.000]   to hold on to kind of intuitive views
[00:51:11.000 --> 00:51:13.000]   about personal identity
[00:51:13.000 --> 00:51:15.000]   for the reason that, like,
[00:51:15.000 --> 00:51:17.000]   there isn't this, like, one person
[00:51:17.000 --> 00:51:18.000]   who you're, like, continuous with
[00:51:18.000 --> 00:51:20.000]   throughout time and know other people.
[00:51:20.000 --> 00:51:21.000]   It's how people tend to think about
[00:51:21.000 --> 00:51:23.000]   what it is to, like, be a person.
[00:51:23.000 --> 00:51:24.000]   And then there's this kind of, like,
[00:51:24.000 --> 00:51:25.000]   vague thing, which is just occasionally
[00:51:25.000 --> 00:51:27.000]   I, you know, just, like, remember,
[00:51:27.000 --> 00:51:28.000]   like, every other month or so,
[00:51:28.000 --> 00:51:30.000]   that maybe many worlds is true.
[00:51:30.000 --> 00:51:32.000]   And it just kind of, like, blows my mind
[00:51:32.000 --> 00:51:33.000]   that I don't know what to do about it,
[00:51:33.000 --> 00:51:35.000]   and I just, like, go on with my day.
[00:51:35.000 --> 00:51:36.000]   That's about where I am.
[00:51:36.000 --> 00:51:37.000]   Okay. All right.
[00:51:37.000 --> 00:51:40.000]   Other interesting topics to talk about.
[00:51:40.000 --> 00:51:41.000]   Talent search.
[00:51:41.000 --> 00:51:42.000]   What is the --
[00:51:42.000 --> 00:51:44.000]   What is EA doing about identifying,
[00:51:44.000 --> 00:51:45.000]   let's say, more people like you,
[00:51:45.000 --> 00:51:46.000]   basically, right?
[00:51:46.000 --> 00:51:47.000]   But maybe even, like, people like you
[00:51:47.000 --> 00:51:49.000]   who are not in, like, places
[00:51:49.000 --> 00:51:51.000]   where they're not next to Oxford.
[00:51:51.000 --> 00:51:52.000]   I don't know where you actually are
[00:51:52.000 --> 00:51:55.000]   from originally, but, like,
[00:51:55.000 --> 00:51:56.000]   if they're from, like, some --
[00:51:56.000 --> 00:51:57.000]   like, I don't know, like,
[00:51:57.000 --> 00:51:58.000]   China or India or something.
[00:51:58.000 --> 00:51:59.000]   Yeah, yeah, yeah.
[00:51:59.000 --> 00:52:01.000]   Well, what is EA doing to recruit
[00:52:01.000 --> 00:52:04.000]   more fans from places where
[00:52:04.000 --> 00:52:06.000]   they might not otherwise work on EA?
[00:52:06.000 --> 00:52:08.000]   Yeah, that's a great question.
[00:52:08.000 --> 00:52:09.000]   And, yeah, to be clear,
[00:52:09.000 --> 00:52:10.000]   I just won the lottery
[00:52:10.000 --> 00:52:11.000]   on things going right
[00:52:11.000 --> 00:52:14.000]   to kind of be lucky enough
[00:52:14.000 --> 00:52:15.000]   to do what I'm doing now.
[00:52:15.000 --> 00:52:17.000]   And so, yeah, in some sense,
[00:52:17.000 --> 00:52:18.000]   the question is how do you, like,
[00:52:18.000 --> 00:52:19.000]   print more winning lottery tickets
[00:52:19.000 --> 00:52:21.000]   and, indeed, like, find those people
[00:52:21.000 --> 00:52:22.000]   who really deserve them,
[00:52:22.000 --> 00:52:24.000]   but, like, it's currently
[00:52:24.000 --> 00:52:27.000]   not being identified.
[00:52:27.000 --> 00:52:28.000]   A lot of this comes from --
[00:52:28.000 --> 00:52:30.000]   I read that book "Talent,"
[00:52:30.000 --> 00:52:32.000]   Tyler Cowen and Daniel Grace, recently.
[00:52:32.000 --> 00:52:34.000]   And, yeah, there's something
[00:52:34.000 --> 00:52:36.000]   really powerful about this fact
[00:52:36.000 --> 00:52:39.000]   that this, like, business of, you know,
[00:52:39.000 --> 00:52:42.000]   finding really, like, smart, driven people
[00:52:42.000 --> 00:52:44.000]   and connecting them with opportunities
[00:52:44.000 --> 00:52:45.000]   to, like, do the things
[00:52:45.000 --> 00:52:46.000]   they really want to do,
[00:52:46.000 --> 00:52:47.000]   this is, like, really kind of
[00:52:47.000 --> 00:52:48.000]   still inefficient.
[00:52:48.000 --> 00:52:49.000]   And there's just still, like,
[00:52:49.000 --> 00:52:51.000]   so many people out there
[00:52:51.000 --> 00:52:52.000]   who, like, aren't kind of
[00:52:52.000 --> 00:52:53.000]   getting those opportunities.
[00:52:53.000 --> 00:52:55.000]   I actually don't know if I have much more,
[00:52:55.000 --> 00:52:57.000]   like, kind of insight to add there
[00:52:57.000 --> 00:52:59.000]   other than this is just a big deal.
[00:52:59.000 --> 00:53:00.000]   And it's, like, there's a sense
[00:53:00.000 --> 00:53:03.000]   of which it is an important consideration
[00:53:03.000 --> 00:53:05.000]   for this, like, project
[00:53:05.000 --> 00:53:06.000]   of trying to do the most good.
[00:53:06.000 --> 00:53:08.000]   Like, you really want to find people
[00:53:08.000 --> 00:53:10.000]   who can, like, put these ideas in practice.
[00:53:10.000 --> 00:53:12.000]   And I think there's a special premium
[00:53:12.000 --> 00:53:15.000]   on that kind of person now
[00:53:15.000 --> 00:53:16.000]   given that there's, like,
[00:53:16.000 --> 00:53:18.000]   a lot of philanthropic kind of funding
[00:53:18.000 --> 00:53:19.000]   ready to, like, be deployed.
[00:53:19.000 --> 00:53:20.000]   There's also a sense in which
[00:53:20.000 --> 00:53:22.000]   this is just, like, in some sense,
[00:53:22.000 --> 00:53:23.000]   like, a cause in its own right.
[00:53:23.000 --> 00:53:25.000]   It's kind of analogous to "Open Borders"
[00:53:25.000 --> 00:53:26.000]   in that sense, at least in my mind.
[00:53:26.000 --> 00:53:28.000]   I hadn't really, like, appreciated it
[00:53:28.000 --> 00:53:29.000]   on some kind of visceral level
[00:53:29.000 --> 00:53:30.000]   before I read that book.
[00:53:30.000 --> 00:53:32.000]   -And then another thing you talk about
[00:53:32.000 --> 00:53:34.000]   in the book is you want to get them
[00:53:34.000 --> 00:53:35.000]   when they're young.
[00:53:35.000 --> 00:53:37.000]   You can really shape somebody's ideas
[00:53:37.000 --> 00:53:38.000]   about what's worth doing
[00:53:38.000 --> 00:53:39.000]   if you --
[00:53:39.000 --> 00:53:41.000]   and then also their ambition
[00:53:41.000 --> 00:53:42.000]   about what they can do
[00:53:42.000 --> 00:53:44.000]   if you catch them early.
[00:53:44.000 --> 00:53:46.000]   And, you know, Tyler Crown
[00:53:46.000 --> 00:53:47.000]   also had an interesting blog post
[00:53:47.000 --> 00:53:48.000]   a while back where he pointed out
[00:53:48.000 --> 00:53:50.000]   that a lot of people applying
[00:53:50.000 --> 00:53:52.000]   to his Emergent Ventures program,
[00:53:52.000 --> 00:53:53.000]   a lot of young people applying,
[00:53:53.000 --> 00:53:55.000]   are heavily influenced
[00:53:55.000 --> 00:53:56.000]   by effective altruism,
[00:53:56.000 --> 00:53:57.000]   which seems very --
[00:53:57.000 --> 00:53:58.000]   Like, it's going to be
[00:53:58.000 --> 00:53:59.000]   a very important factor
[00:53:59.000 --> 00:54:02.000]   in the long term.
[00:54:02.000 --> 00:54:03.000]   I mean, eventually, these people
[00:54:03.000 --> 00:54:05.000]   will be in positions of power.
[00:54:05.000 --> 00:54:07.000]   Yeah, so maybe effective altruism
[00:54:07.000 --> 00:54:08.000]   is already succeeding
[00:54:08.000 --> 00:54:11.000]   to the extent that a lot of the most
[00:54:11.000 --> 00:54:12.000]   ambitious people in the world
[00:54:12.000 --> 00:54:13.000]   are identified that way,
[00:54:13.000 --> 00:54:14.000]   or at least, I mean,
[00:54:14.000 --> 00:54:15.000]   given the selection effect
[00:54:15.000 --> 00:54:17.000]   that Tyler Crown's program has.
[00:54:17.000 --> 00:54:19.000]   But, yeah, so what is it
[00:54:19.000 --> 00:54:20.000]   that can be done
[00:54:20.000 --> 00:54:22.000]   to get people when they're young?
[00:54:22.000 --> 00:54:24.000]   -Quick.
[00:54:24.000 --> 00:54:25.000]   Yeah, I mean,
[00:54:25.000 --> 00:54:26.000]   it's a very good question.
[00:54:26.000 --> 00:54:27.000]   And I think, like,
[00:54:27.000 --> 00:54:30.000]   what you point out there is right.
[00:54:30.000 --> 00:54:31.000]   There's some --
[00:54:31.000 --> 00:54:33.000]   Nick Whittaker has this blog post,
[00:54:33.000 --> 00:54:34.000]   something like the --
[00:54:34.000 --> 00:54:35.000]   It's called
[00:54:35.000 --> 00:54:37.000]   "The Lamplight Model of Talent Curation."
[00:54:37.000 --> 00:54:39.000]   -Mm-hmm.
[00:54:39.000 --> 00:54:40.000]   -He, like, draws this distinction
[00:54:40.000 --> 00:54:44.000]   between casting, like,
[00:54:44.000 --> 00:54:46.000]   a very wide net
[00:54:46.000 --> 00:54:47.000]   that's just kind of
[00:54:47.000 --> 00:54:48.000]   very legibly prestigious
[00:54:48.000 --> 00:54:49.000]   and then, you know,
[00:54:49.000 --> 00:54:50.000]   filtering through thousands
[00:54:50.000 --> 00:54:52.000]   of applications,
[00:54:52.000 --> 00:54:54.000]   or, in some sense,
[00:54:54.000 --> 00:54:57.000]   like, pulling out the bat signal
[00:54:57.000 --> 00:55:00.000]   that, in the first instance,
[00:55:00.000 --> 00:55:01.000]   just, like, attracts
[00:55:01.000 --> 00:55:03.000]   the, like, really promising people
[00:55:03.000 --> 00:55:06.000]   and maybe actually drives away
[00:55:06.000 --> 00:55:07.000]   people who would be a better fit
[00:55:07.000 --> 00:55:09.000]   for something else.
[00:55:09.000 --> 00:55:11.000]   So, like, an example is
[00:55:11.000 --> 00:55:14.000]   if you were to hypothetically
[00:55:14.000 --> 00:55:17.000]   write quite a wonky economics blog,
[00:55:17.000 --> 00:55:18.000]   like, every day
[00:55:18.000 --> 00:55:19.000]   for however many years
[00:55:19.000 --> 00:55:21.000]   and then run some
[00:55:21.000 --> 00:55:22.000]   fellowship program,
[00:55:22.000 --> 00:55:23.000]   you're just, like,
[00:55:23.000 --> 00:55:24.000]   automatically selecting
[00:55:24.000 --> 00:55:27.000]   four people who read that blog,
[00:55:27.000 --> 00:55:28.000]   and that's, like, a pretty good
[00:55:28.000 --> 00:55:30.000]   kind of starting population
[00:55:30.000 --> 00:55:31.000]   to begin with.
[00:55:31.000 --> 00:55:33.000]   So I really like that kind of thought
[00:55:33.000 --> 00:55:35.000]   of just, like, not needing to be,
[00:55:35.000 --> 00:55:37.000]   like, incredibly loud
[00:55:37.000 --> 00:55:39.000]   and, like, prestigious-sounding
[00:55:39.000 --> 00:55:41.000]   but rather just, like,
[00:55:41.000 --> 00:55:42.000]   being quite honest
[00:55:42.000 --> 00:55:43.000]   about what the thing is about
[00:55:43.000 --> 00:55:44.000]   so you just attract the people
[00:55:44.000 --> 00:55:46.000]   who, like, really sort it out
[00:55:46.000 --> 00:55:47.000]   because that's just
[00:55:47.000 --> 00:55:48.000]   quite a good feature.
[00:55:48.000 --> 00:55:49.000]   I think another thing that --
[00:55:49.000 --> 00:55:50.000]   Again, this is, like,
[00:55:50.000 --> 00:55:52.000]   not a very interesting point to make,
[00:55:52.000 --> 00:55:55.000]   but something I've really realized
[00:55:55.000 --> 00:55:57.000]   the value of is, like,
[00:55:57.000 --> 00:56:00.000]   having physical hubs.
[00:56:00.000 --> 00:56:02.000]   And so there's this model of,
[00:56:02.000 --> 00:56:03.000]   you know, running, like,
[00:56:03.000 --> 00:56:04.000]   fellowships, for instance,
[00:56:04.000 --> 00:56:05.000]   where you just, like,
[00:56:05.000 --> 00:56:06.000]   find really promising people
[00:56:06.000 --> 00:56:07.000]   and then --
[00:56:07.000 --> 00:56:08.000]   There's just so much to be said
[00:56:08.000 --> 00:56:09.000]   for, like, putting those people
[00:56:09.000 --> 00:56:10.000]   in the same place
[00:56:10.000 --> 00:56:11.000]   and, you know,
[00:56:11.000 --> 00:56:12.000]   surrounding them with maybe people
[00:56:12.000 --> 00:56:14.000]   who are a bit more, like, senior
[00:56:14.000 --> 00:56:15.000]   and just kind of, like,
[00:56:15.000 --> 00:56:17.000]   letting this natural process happen
[00:56:17.000 --> 00:56:18.000]   where people just get really excited
[00:56:18.000 --> 00:56:19.000]   that there is this, like,
[00:56:19.000 --> 00:56:20.000]   community of people
[00:56:20.000 --> 00:56:21.000]   working on stuff that previously
[00:56:21.000 --> 00:56:22.000]   you've just been kind of
[00:56:22.000 --> 00:56:23.000]   reading about in your bedroom
[00:56:23.000 --> 00:56:24.000]   on, like, some blogs.
[00:56:24.000 --> 00:56:26.000]   That, like, as a source of motivation,
[00:56:26.000 --> 00:56:27.000]   I know it's, like,
[00:56:27.000 --> 00:56:28.000]   less tangible than other things,
[00:56:28.000 --> 00:56:31.000]   but, yeah, just, like, so powerful.
[00:56:31.000 --> 00:56:33.000]   And, like, probably, I don't know,
[00:56:33.000 --> 00:56:34.000]   one of the reasons I'm, like,
[00:56:34.000 --> 00:56:36.000]   working here, maybe.
[00:56:36.000 --> 00:56:37.000]   -Yeah.
[00:56:37.000 --> 00:56:40.000]   It is one aspect of working from home
[00:56:40.000 --> 00:56:42.000]   that you don't get that.
[00:56:42.000 --> 00:56:44.000]   Regarding the first point,
[00:56:44.000 --> 00:56:48.000]   so I think maybe that should update
[00:56:48.000 --> 00:56:51.000]   in favor of not doing community outreach
[00:56:51.000 --> 00:56:52.000]   and community building.
[00:56:52.000 --> 00:56:53.000]   Like, maybe that's
[00:56:53.000 --> 00:56:54.000]   negative marginal utility
[00:56:54.000 --> 00:56:56.000]   because, like, if I think about,
[00:56:56.000 --> 00:56:59.000]   for example, my local --
[00:56:59.000 --> 00:57:01.000]   So there was an effective altruism group
[00:57:01.000 --> 00:57:03.000]   at my college that didn't attend,
[00:57:03.000 --> 00:57:04.000]   and there's also, like,
[00:57:04.000 --> 00:57:05.000]   an effective altruism group
[00:57:05.000 --> 00:57:07.000]   for the city as a whole
[00:57:07.000 --> 00:57:09.000]   in Austin that I don't attend.
[00:57:09.000 --> 00:57:11.000]   And the reason is just because,
[00:57:11.000 --> 00:57:13.000]   I don't know, the people who --
[00:57:13.000 --> 00:57:15.000]   There is some sort of
[00:57:15.000 --> 00:57:16.000]   adverse selection here
[00:57:16.000 --> 00:57:17.000]   where the people who are
[00:57:17.000 --> 00:57:18.000]   leading organizations like this
[00:57:18.000 --> 00:57:20.000]   are people who couldn't just, like,
[00:57:20.000 --> 00:57:22.000]   aren't directly doing the things
[00:57:22.000 --> 00:57:23.000]   that effective altruism says
[00:57:23.000 --> 00:57:25.000]   they might consider doing
[00:57:25.000 --> 00:57:27.000]   and are more interested
[00:57:27.000 --> 00:57:30.000]   in the social aspects of altruism.
[00:57:30.000 --> 00:57:31.000]   So, I don't know.
[00:57:31.000 --> 00:57:33.000]   I'd be much less impressed
[00:57:33.000 --> 00:57:34.000]   with the movement
[00:57:34.000 --> 00:57:35.000]   if my first introduction to it
[00:57:35.000 --> 00:57:37.000]   was these specific groups
[00:57:37.000 --> 00:57:38.000]   that, like, I've had the personal --
[00:57:38.000 --> 00:57:40.000]   I've personally interacted with
[00:57:40.000 --> 00:57:41.000]   rather than, I don't know,
[00:57:41.000 --> 00:57:43.000]   just, like, hearing Will McCaskill
[00:57:43.000 --> 00:57:45.000]   on a podcast.
[00:57:45.000 --> 00:57:47.000]   By the way, the fourth ladder
[00:57:47.000 --> 00:57:48.000]   being my first introduction
[00:57:48.000 --> 00:57:49.000]   to effective altruism.
[00:57:49.000 --> 00:57:51.000]   -Yeah, interesting.
[00:57:51.000 --> 00:57:52.000]   I feel like I really don't want
[00:57:52.000 --> 00:57:54.000]   to, like, underwrite the job
[00:57:54.000 --> 00:57:55.000]   that community builders are doing.
[00:57:55.000 --> 00:57:56.000]   I think, in fact,
[00:57:56.000 --> 00:57:57.000]   it's turned out to have been,
[00:57:57.000 --> 00:57:58.000]   like, and still is,
[00:57:58.000 --> 00:58:00.000]   just, like, incredibly valuable,
[00:58:00.000 --> 00:58:01.000]   especially just looking at
[00:58:01.000 --> 00:58:02.000]   the numbers of, like,
[00:58:02.000 --> 00:58:03.000]   what you can achieve
[00:58:03.000 --> 00:58:05.000]   as, like, a group organizer
[00:58:05.000 --> 00:58:06.000]   at your university.
[00:58:06.000 --> 00:58:07.000]   Like, maybe you could just
[00:58:07.000 --> 00:58:08.000]   change the course of, like,
[00:58:08.000 --> 00:58:10.000]   more than one person's career
[00:58:10.000 --> 00:58:11.000]   over the course of, like,
[00:58:11.000 --> 00:58:12.000]   a year of your time.
[00:58:12.000 --> 00:58:13.000]   That's, like, pretty incredible.
[00:58:13.000 --> 00:58:14.000]   But, yeah, I guess part
[00:58:14.000 --> 00:58:15.000]   of what's going on is that
[00:58:15.000 --> 00:58:17.000]   the difference between, like,
[00:58:17.000 --> 00:58:20.000]   going to your, like, local group
[00:58:20.000 --> 00:58:23.000]   or, like, engaging with stuff online
[00:58:23.000 --> 00:58:24.000]   is that you get to, kind of,
[00:58:24.000 --> 00:58:26.000]   choose the stuff you engage with.
[00:58:26.000 --> 00:58:27.000]   And, like, maybe one upshot here
[00:58:27.000 --> 00:58:30.000]   is that the, like,
[00:58:30.000 --> 00:58:31.000]   kind of set of ideas
[00:58:31.000 --> 00:58:32.000]   that might get associated
[00:58:32.000 --> 00:58:35.000]   with EA is, like, very big,
[00:58:35.000 --> 00:58:37.000]   and you don't need to buy into
[00:58:37.000 --> 00:58:38.000]   all of it or just, like,
[00:58:38.000 --> 00:58:39.000]   be passionate about all of it.
[00:58:39.000 --> 00:58:41.000]   Like, if this kind of AI stuff
[00:58:41.000 --> 00:58:43.000]   just, like, really seems interesting
[00:58:43.000 --> 00:58:44.000]   but maybe other stuff
[00:58:44.000 --> 00:58:45.000]   is, like, more peripheral,
[00:58:45.000 --> 00:58:47.000]   then, you know, one, yeah,
[00:58:47.000 --> 00:58:48.000]   like, this could push towards
[00:58:48.000 --> 00:58:49.000]   wanting to have, like,
[00:58:49.000 --> 00:58:51.000]   just a specific group
[00:58:51.000 --> 00:58:52.000]   for people who are just, like,
[00:58:52.000 --> 00:58:53.000]   you know, this AI stuff seems cool,
[00:58:53.000 --> 00:58:55.000]   other stuff, not my, like,
[00:58:55.000 --> 00:58:57.000]   cup of tea.
[00:58:57.000 --> 00:58:58.000]   So, yeah, I mean, in the future
[00:58:58.000 --> 00:58:59.000]   as, like, things get scaled up,
[00:58:59.000 --> 00:59:00.000]   as well as kind of scaling out,
[00:59:00.000 --> 00:59:02.000]   I think also maybe having this,
[00:59:02.000 --> 00:59:04.000]   like, differentiation
[00:59:04.000 --> 00:59:06.000]   and kind of diversification
[00:59:06.000 --> 00:59:07.000]   of, like, different groups,
[00:59:07.000 --> 00:59:08.000]   I mean, seems pretty good.
[00:59:08.000 --> 00:59:09.000]   But just, like, more of everything
[00:59:09.000 --> 00:59:11.000]   also seems good.
[00:59:11.000 --> 00:59:12.000]   - Yeah, yeah, I mean,
[00:59:12.000 --> 00:59:13.000]   I'm probably over-fitting
[00:59:13.000 --> 00:59:14.000]   on my own experience.
[00:59:14.000 --> 00:59:15.000]   I mean, given the fact that I don't--
[00:59:15.000 --> 00:59:17.000]   didn't actively interact
[00:59:17.000 --> 00:59:18.000]   with any of those communities,
[00:59:18.000 --> 00:59:19.000]   I'm probably not even informed
[00:59:19.000 --> 00:59:21.000]   on those experiences of those.
[00:59:21.000 --> 00:59:22.000]   But there was an interesting post
[00:59:22.000 --> 00:59:24.000]   on an effective altruism forum
[00:59:24.000 --> 00:59:25.000]   that somebody sent me
[00:59:25.000 --> 00:59:26.000]   where they were making the case
[00:59:26.000 --> 00:59:28.000]   that at their college, as well,
[00:59:28.000 --> 00:59:30.000]   they got the sense that
[00:59:30.000 --> 00:59:31.000]   the EA community-building stuff
[00:59:31.000 --> 00:59:33.000]   had the negative impact
[00:59:33.000 --> 00:59:34.000]   because people were kind of
[00:59:34.000 --> 00:59:36.000]   turned off by their peers.
[00:59:36.000 --> 00:59:38.000]   And also, there's a difference
[00:59:38.000 --> 00:59:39.000]   between, like, I don't know,
[00:59:39.000 --> 00:59:40.000]   somebody, like, saying
[00:59:40.000 --> 00:59:41.000]   "Hey, go roll McCaskill,"
[00:59:41.000 --> 00:59:42.000]   advising you--
[00:59:42.000 --> 00:59:44.000]   obviously, virtually--
[00:59:44.000 --> 00:59:47.000]   to do these kinds of things
[00:59:47.000 --> 00:59:48.000]   versus, like, I don't know,
[00:59:48.000 --> 00:59:50.000]   some sophomore at your university
[00:59:50.000 --> 00:59:52.000]   starting philosophy, right?
[00:59:52.000 --> 00:59:53.000]   [laughter]
[00:59:53.000 --> 00:59:54.000]   No offense.
[00:59:54.000 --> 00:59:56.000]   [laughter]
[00:59:56.000 --> 00:59:57.000]   - Yeah, I do, I do.
[00:59:57.000 --> 00:59:59.000]   I think my guess is that, like,
[00:59:59.000 --> 01:00:01.000]   on that, these efforts
[01:00:01.000 --> 01:00:02.000]   are still just, like,
[01:00:02.000 --> 01:00:03.000]   overwhelmingly positive.
[01:00:03.000 --> 01:00:05.000]   But, yeah, I think it's, like,
[01:00:05.000 --> 01:00:06.000]   pretty interesting that people
[01:00:06.000 --> 01:00:07.000]   have the experience
[01:00:07.000 --> 01:00:08.000]   you describe, as well.
[01:00:08.000 --> 01:00:09.000]   Yeah, and interesting to think
[01:00:09.000 --> 01:00:10.000]   about ways to kind of, like,
[01:00:10.000 --> 01:00:11.000]   get around that.
[01:00:11.000 --> 01:00:13.000]   - So, long reflection is--
[01:00:13.000 --> 01:00:15.000]   it seems like a bad idea, no?
[01:00:15.000 --> 01:00:17.000]   - I'm so glad you asked.
[01:00:17.000 --> 01:00:18.000]   Yeah, I want to say--
[01:00:18.000 --> 01:00:19.000]   I want to say no.
[01:00:19.000 --> 01:00:20.000]   I think in some sense
[01:00:20.000 --> 01:00:21.000]   I've, like, come around to it
[01:00:21.000 --> 01:00:22.000]   as an idea.
[01:00:22.000 --> 01:00:23.000]   But, yeah, okay,
[01:00:23.000 --> 01:00:24.000]   maybe it's worth, like--
[01:00:24.000 --> 01:00:26.000]   - Oh, really? Interesting.
[01:00:26.000 --> 01:00:27.000]   - Maybe it's worth, I guess,
[01:00:27.000 --> 01:00:28.000]   like, trying to explain
[01:00:28.000 --> 01:00:30.000]   what's going on with this idea.
[01:00:30.000 --> 01:00:31.000]   - Sure, yes.
[01:00:31.000 --> 01:00:33.000]   [laughter]
[01:00:33.000 --> 01:00:34.000]   - So, if you, like,
[01:00:34.000 --> 01:00:35.000]   were to zoom out, like,
[01:00:35.000 --> 01:00:38.000]   really far over time
[01:00:38.000 --> 01:00:39.000]   and consider our place now,
[01:00:39.000 --> 01:00:40.000]   like, in history,
[01:00:40.000 --> 01:00:41.000]   and you could, like,
[01:00:41.000 --> 01:00:42.000]   ask this question about--
[01:00:42.000 --> 01:00:43.000]   suppose, in some sense,
[01:00:43.000 --> 01:00:44.000]   humanity just became, like,
[01:00:44.000 --> 01:00:46.000]   perfectly coordinated.
[01:00:46.000 --> 01:00:47.000]   What's the plan?
[01:00:47.000 --> 01:00:48.000]   Like, what--
[01:00:48.000 --> 01:00:49.000]   what kind of, in general,
[01:00:49.000 --> 01:00:51.000]   should we be prioritizing?
[01:00:51.000 --> 01:00:53.000]   Like, in what stages?
[01:00:53.000 --> 01:00:55.000]   And you might say
[01:00:55.000 --> 01:00:57.000]   something like this.
[01:00:57.000 --> 01:00:58.000]   It looks like
[01:00:58.000 --> 01:00:59.000]   this moment in history,
[01:00:59.000 --> 01:01:00.000]   which is to say
[01:01:00.000 --> 01:01:02.000]   maybe this century or so,
[01:01:02.000 --> 01:01:03.000]   just looks kind of wildly
[01:01:03.000 --> 01:01:05.000]   and, like, unsustainably
[01:01:05.000 --> 01:01:07.000]   dangerous, like--
[01:01:07.000 --> 01:01:09.000]   or, kind of--
[01:01:09.000 --> 01:01:11.000]   so many things are happening at once,
[01:01:11.000 --> 01:01:12.000]   it's really hard to know
[01:01:12.000 --> 01:01:13.000]   how things are going to pan out,
[01:01:13.000 --> 01:01:14.000]   but it's, like, possible
[01:01:14.000 --> 01:01:15.000]   to imagine things panning out
[01:01:15.000 --> 01:01:16.000]   really badly
[01:01:16.000 --> 01:01:17.000]   and badly enough
[01:01:17.000 --> 01:01:18.000]   to just, like,
[01:01:18.000 --> 01:01:19.000]   more or less end history.
[01:01:19.000 --> 01:01:20.000]   Okay, so,
[01:01:20.000 --> 01:01:21.000]   before we can, like,
[01:01:21.000 --> 01:01:22.000]   worry about some kind of
[01:01:22.000 --> 01:01:23.000]   longer-term considerations,
[01:01:23.000 --> 01:01:25.000]   let's just get our act together
[01:01:25.000 --> 01:01:26.000]   and make sure we don't
[01:01:26.000 --> 01:01:27.000]   mess things up.
[01:01:27.000 --> 01:01:28.000]   So, okay, like,
[01:01:28.000 --> 01:01:29.000]   that seems like a pretty good
[01:01:29.000 --> 01:01:30.000]   first priority.
[01:01:30.000 --> 01:01:31.000]   But then, okay,
[01:01:31.000 --> 01:01:32.000]   suppose that you succeed in that
[01:01:32.000 --> 01:01:33.000]   and, like,
[01:01:33.000 --> 01:01:35.000]   we're in a significantly safer
[01:01:35.000 --> 01:01:37.000]   kind of time.
[01:01:37.000 --> 01:01:39.000]   What then?
[01:01:39.000 --> 01:01:42.000]   You might notice that
[01:01:42.000 --> 01:01:44.000]   the scope for, like,
[01:01:44.000 --> 01:01:45.000]   what we could achieve
[01:01:45.000 --> 01:01:47.000]   is, like, really
[01:01:47.000 --> 01:01:48.000]   extraordinarily large.
[01:01:48.000 --> 01:01:49.000]   Like, maybe kind of
[01:01:49.000 --> 01:01:50.000]   larger than most people
[01:01:50.000 --> 01:01:51.000]   kind of, like,
[01:01:51.000 --> 01:01:53.000]   typically entertain.
[01:01:53.000 --> 01:01:55.000]   Like, we could just do a ton of
[01:01:55.000 --> 01:01:57.000]   really exceptional things.
[01:01:57.000 --> 01:01:59.000]   But also, there's this kind of
[01:01:59.000 --> 01:02:01.000]   feature that maybe
[01:02:01.000 --> 01:02:02.000]   in the future--
[01:02:02.000 --> 01:02:03.000]   not--
[01:02:03.000 --> 01:02:05.000]   especially long-term future--
[01:02:05.000 --> 01:02:06.000]   we might, more or less,
[01:02:06.000 --> 01:02:07.000]   for the first time,
[01:02:07.000 --> 01:02:08.000]   be able to embark on these, like,
[01:02:08.000 --> 01:02:10.000]   really kind of ambitious projects
[01:02:10.000 --> 01:02:12.000]   that are, in some important sense,
[01:02:12.000 --> 01:02:14.000]   like, really hard to reverse.
[01:02:14.000 --> 01:02:15.000]   And that might make you think,
[01:02:15.000 --> 01:02:16.000]   okay, at some point,
[01:02:16.000 --> 01:02:19.000]   it'd be great to, like,
[01:02:19.000 --> 01:02:20.000]   you know, achieve
[01:02:20.000 --> 01:02:21.000]   that potential that we have.
[01:02:21.000 --> 01:02:22.000]   And just, like,
[01:02:22.000 --> 01:02:23.000]   like, for instance,
[01:02:23.000 --> 01:02:24.000]   the kind of lower bound on this
[01:02:24.000 --> 01:02:27.000]   is lifting everyone
[01:02:27.000 --> 01:02:28.000]   out of poverty
[01:02:28.000 --> 01:02:29.000]   who remains in poverty,
[01:02:29.000 --> 01:02:30.000]   and then, like,
[01:02:30.000 --> 01:02:31.000]   going even further,
[01:02:31.000 --> 01:02:33.000]   just making everyone even wealthier,
[01:02:33.000 --> 01:02:34.000]   able to do more things
[01:02:34.000 --> 01:02:35.000]   that they want to do,
[01:02:35.000 --> 01:02:37.000]   making more scientific discoveries,
[01:02:37.000 --> 01:02:38.000]   whatever.
[01:02:38.000 --> 01:02:39.000]   So we want to do that,
[01:02:39.000 --> 01:02:40.000]   but maybe something should come
[01:02:40.000 --> 01:02:41.000]   in between these two things,
[01:02:41.000 --> 01:02:42.000]   which is, like,
[01:02:42.000 --> 01:02:46.000]   figuring out what is actually good.
[01:02:46.000 --> 01:02:47.000]   And, okay,
[01:02:47.000 --> 01:02:50.000]   why should we think this?
[01:02:50.000 --> 01:02:53.000]   I think one thought here is
[01:02:53.000 --> 01:02:54.000]   it's very plausible--
[01:02:54.000 --> 01:02:55.000]   I guess this kind of links
[01:02:55.000 --> 01:02:57.000]   to what we were talking about earlier--
[01:02:57.000 --> 01:02:58.000]   that the way we think about,
[01:02:58.000 --> 01:03:00.000]   you know, like,
[01:03:00.000 --> 01:03:02.000]   really positive futures,
[01:03:02.000 --> 01:03:04.000]   like, one of the best futures,
[01:03:04.000 --> 01:03:05.000]   it's just, like,
[01:03:05.000 --> 01:03:07.000]   really kind of incomplete.
[01:03:07.000 --> 01:03:08.000]   Almost certainly we're just getting
[01:03:08.000 --> 01:03:09.000]   a bunch of things wrong
[01:03:09.000 --> 01:03:11.000]   by this kind of pessimistic induction
[01:03:11.000 --> 01:03:12.000]   on the past.
[01:03:12.000 --> 01:03:13.000]   Like, a bunch of smart people
[01:03:13.000 --> 01:03:15.000]   thought really reprehensible things,
[01:03:15.000 --> 01:03:16.000]   like, 100 years ago.
[01:03:16.000 --> 01:03:18.000]   So we're getting things wrong.
[01:03:18.000 --> 01:03:19.000]   And then this, like,
[01:03:19.000 --> 01:03:20.000]   second thought is,
[01:03:20.000 --> 01:03:21.000]   I don't know,
[01:03:21.000 --> 01:03:23.000]   it seems possible
[01:03:23.000 --> 01:03:26.000]   to actually make progress here
[01:03:26.000 --> 01:03:28.000]   in thinking about what's good.
[01:03:28.000 --> 01:03:29.000]   There's this kind of interesting point
[01:03:29.000 --> 01:03:32.000]   that most, like, work in--
[01:03:32.000 --> 01:03:33.000]   I guess you might call it, like,
[01:03:33.000 --> 01:03:34.000]   moral philosophy--
[01:03:34.000 --> 01:03:36.000]   has focused on the negatives.
[01:03:36.000 --> 01:03:37.000]   So, you know,
[01:03:37.000 --> 01:03:39.000]   avoiding doing things wrong,
[01:03:39.000 --> 01:03:40.000]   fixing harms,
[01:03:40.000 --> 01:03:42.000]   avoiding bad outcomes.
[01:03:42.000 --> 01:03:43.000]   But this idea of, like,
[01:03:43.000 --> 01:03:44.000]   studying the positive,
[01:03:44.000 --> 01:03:45.000]   studying, like,
[01:03:45.000 --> 01:03:46.000]   what we should do
[01:03:46.000 --> 01:03:47.000]   if we can kind of do, like,
[01:03:47.000 --> 01:03:48.000]   many different things,
[01:03:48.000 --> 01:03:49.000]   this is just, like,
[01:03:49.000 --> 01:03:51.000]   super, super early.
[01:03:51.000 --> 01:03:52.000]   And so we should expect
[01:03:52.000 --> 01:03:54.000]   to be able to make a ton of progress.
[01:03:54.000 --> 01:03:55.000]   And so, hey, again,
[01:03:55.000 --> 01:03:56.000]   imagining that the world
[01:03:56.000 --> 01:03:58.000]   is, like, perfectly coordinated,
[01:03:58.000 --> 01:03:59.000]   would it be a good idea
[01:03:59.000 --> 01:04:00.000]   to, like, spend some time,
[01:04:00.000 --> 01:04:02.000]   maybe a long period of time,
[01:04:02.000 --> 01:04:03.000]   kind of deliberately
[01:04:03.000 --> 01:04:04.000]   holding back from embarking
[01:04:04.000 --> 01:04:05.000]   on these, like,
[01:04:05.000 --> 01:04:06.000]   huge irreversible projects,
[01:04:06.000 --> 01:04:07.000]   which maybe involve, like,
[01:04:07.000 --> 01:04:08.000]   leaving Earth
[01:04:08.000 --> 01:04:09.000]   in kind of certain,
[01:04:09.000 --> 01:04:10.000]   you know, scenarios,
[01:04:10.000 --> 01:04:11.000]   or otherwise just, like,
[01:04:11.000 --> 01:04:12.000]   doing things
[01:04:12.000 --> 01:04:15.000]   which are hard to undo.
[01:04:15.000 --> 01:04:16.000]   Should we spend some time
[01:04:16.000 --> 01:04:17.000]   thinking before then?
[01:04:17.000 --> 01:04:18.000]   Yeah, sounds good.
[01:04:18.000 --> 01:04:19.000]   And then I guess
[01:04:19.000 --> 01:04:20.000]   the very obvious response is,
[01:04:20.000 --> 01:04:21.000]   okay,
[01:04:21.000 --> 01:04:23.000]   that's a pretty huge assumption
[01:04:23.000 --> 01:04:24.000]   that we can just, like,
[01:04:24.000 --> 01:04:25.000]   coordinate around that.
[01:04:25.000 --> 01:04:26.000]   And I think the answer is,
[01:04:26.000 --> 01:04:28.000]   yep, it is.
[01:04:28.000 --> 01:04:29.000]   But as a kind of
[01:04:29.000 --> 01:04:30.000]   directional ideal,
[01:04:30.000 --> 01:04:32.000]   should we push towards
[01:04:32.000 --> 01:04:33.000]   or away from
[01:04:33.000 --> 01:04:35.000]   the idea of, like,
[01:04:35.000 --> 01:04:36.000]   taking our time,
[01:04:36.000 --> 01:04:37.000]   holding our horses,
[01:04:37.000 --> 01:04:38.000]   kind of getting people together
[01:04:38.000 --> 01:04:39.000]   who haven't really, like,
[01:04:39.000 --> 01:04:40.000]   been part of this,
[01:04:40.000 --> 01:04:41.000]   like, conversation
[01:04:41.000 --> 01:04:42.000]   and, like, hearing them?
[01:04:42.000 --> 01:04:43.000]   Yeah.
[01:04:43.000 --> 01:04:45.000]   Definitely seems worthwhile.
[01:04:45.000 --> 01:04:46.000]   All right, so I have another
[01:04:46.000 --> 01:04:48.000]   good abstract idea
[01:04:48.000 --> 01:04:49.000]   that I want to entertain by you.
[01:04:49.000 --> 01:04:50.000]   So, you know,
[01:04:50.000 --> 01:04:51.000]   it seems, like, kind of wasteful
[01:04:51.000 --> 01:04:52.000]   that we have these
[01:04:52.000 --> 01:04:53.000]   different companies
[01:04:53.000 --> 01:04:54.000]   that are building
[01:04:54.000 --> 01:04:55.000]   the same exact product.
[01:04:55.000 --> 01:04:56.000]   But, you know,
[01:04:56.000 --> 01:04:57.000]   because they're really
[01:04:57.000 --> 01:04:58.000]   the same exact product,
[01:04:58.000 --> 01:04:59.000]   they don't have economies of scale
[01:04:59.000 --> 01:05:00.000]   and they don't have coordination.
[01:05:00.000 --> 01:05:01.000]   There's just a whole bunch of
[01:05:01.000 --> 01:05:03.000]   loss that comes from that, right?
[01:05:03.000 --> 01:05:04.000]   Wouldn't it be better
[01:05:04.000 --> 01:05:05.000]   if we could just coordinate
[01:05:05.000 --> 01:05:06.000]   and just, like,
[01:05:06.000 --> 01:05:07.000]   figure out the best person
[01:05:07.000 --> 01:05:08.000]   to produce something
[01:05:08.000 --> 01:05:09.000]   together
[01:05:09.000 --> 01:05:10.000]   and then just have them produce it?
[01:05:10.000 --> 01:05:11.000]   And then we could also
[01:05:11.000 --> 01:05:12.000]   coordinate to figure out, like,
[01:05:12.000 --> 01:05:13.000]   what is the right
[01:05:13.000 --> 01:05:15.000]   quantity and quality
[01:05:15.000 --> 01:05:16.000]   for them to produce?
[01:05:16.000 --> 01:05:17.000]   I'm not trying to say
[01:05:17.000 --> 01:05:18.000]   this is, like,
[01:05:18.000 --> 01:05:19.000]   communism or something.
[01:05:19.000 --> 01:05:20.000]   I'm just saying
[01:05:20.000 --> 01:05:21.000]   it's ignoring
[01:05:21.000 --> 01:05:22.000]   what would be required.
[01:05:22.000 --> 01:05:23.000]   Like, in this analogy,
[01:05:23.000 --> 01:05:24.000]   you're ignoring, like,
[01:05:24.000 --> 01:05:25.000]   what kinds of information
[01:05:25.000 --> 01:05:26.000]   gets lost
[01:05:26.000 --> 01:05:28.000]   and what kinds of--
[01:05:28.000 --> 01:05:29.000]   what it requires
[01:05:29.000 --> 01:05:31.000]   to do that so-called coordination
[01:05:31.000 --> 01:05:33.000]   in the communism example.
[01:05:33.000 --> 01:05:34.000]   In this example,
[01:05:34.000 --> 01:05:36.000]   it seems like you're not--
[01:05:36.000 --> 01:05:38.000]   whatever would be required
[01:05:38.000 --> 01:05:39.000]   to prevent somebody
[01:05:39.000 --> 01:05:41.000]   from realizing--
[01:05:41.000 --> 01:05:42.000]   like, let's say somebody
[01:05:42.000 --> 01:05:43.000]   has a vision for, like,
[01:05:43.000 --> 01:05:44.000]   we want to colonize
[01:05:44.000 --> 01:05:45.000]   a star system,
[01:05:45.000 --> 01:05:46.000]   we want to, like,
[01:05:46.000 --> 01:05:47.000]   I don't know,
[01:05:47.000 --> 01:05:48.000]   make some new technology, right?
[01:05:48.000 --> 01:05:49.000]   That's part of something
[01:05:49.000 --> 01:05:50.000]   that the long reflection could tell.
[01:05:50.000 --> 01:05:51.000]   Maybe I'm getting this wrong,
[01:05:51.000 --> 01:05:52.000]   but it seems like
[01:05:52.000 --> 01:05:53.000]   it would require
[01:05:53.000 --> 01:05:55.000]   almost a global panopticon
[01:05:55.000 --> 01:05:56.000]   totalitarian state
[01:05:56.000 --> 01:05:57.000]   to be able to, like,
[01:05:57.000 --> 01:05:58.000]   prevent people from
[01:05:58.000 --> 01:06:00.000]   escaping the reflection.
[01:06:00.000 --> 01:06:01.000]   Okay, so
[01:06:01.000 --> 01:06:02.000]   there's a continuum here,
[01:06:02.000 --> 01:06:04.000]   and I basically agree
[01:06:04.000 --> 01:06:05.000]   that some kind of
[01:06:05.000 --> 01:06:07.000]   panopticon-like thing
[01:06:07.000 --> 01:06:08.000]   not only is impossible,
[01:06:08.000 --> 01:06:10.000]   but actually sounds pretty bad.
[01:06:10.000 --> 01:06:12.000]   But something where you're just, like,
[01:06:12.000 --> 01:06:13.000]   pushing in the direction
[01:06:13.000 --> 01:06:15.000]   of being more coordinated
[01:06:15.000 --> 01:06:16.000]   on the international level
[01:06:16.000 --> 01:06:17.000]   about things that matter
[01:06:17.000 --> 01:06:19.000]   seems, like, desirable
[01:06:19.000 --> 01:06:21.000]   and possible.
[01:06:21.000 --> 01:06:22.000]   And in particular, like,
[01:06:22.000 --> 01:06:23.000]   preventing really bad things
[01:06:23.000 --> 01:06:24.000]   rather than, like,
[01:06:24.000 --> 01:06:25.000]   trying to get people to, like,
[01:06:25.000 --> 01:06:27.000]   all do the same thing.
[01:06:27.000 --> 01:06:29.000]   So the biological weapons convention
[01:06:29.000 --> 01:06:30.000]   strikes me as an example,
[01:06:30.000 --> 01:06:31.000]   which is, like,
[01:06:31.000 --> 01:06:33.000]   imperfect and underfunded,
[01:06:33.000 --> 01:06:34.000]   but, you know,
[01:06:34.000 --> 01:06:36.000]   nonetheless kind of directionally good.
[01:06:36.000 --> 01:06:38.000]   And maybe an extra point here is that
[01:06:38.000 --> 01:06:40.000]   there's, like, a sense in which
[01:06:40.000 --> 01:06:42.000]   the long reflection option,
[01:06:42.000 --> 01:06:43.000]   or, I guess,
[01:06:43.000 --> 01:06:45.000]   the better framing is, like,
[01:06:45.000 --> 01:06:47.000]   aiming for a bit more
[01:06:47.000 --> 01:06:49.000]   reflection rather than less,
[01:06:49.000 --> 01:06:51.000]   that's, like, the conservative option.
[01:06:51.000 --> 01:06:53.000]   That's, like, doing what we've already been doing
[01:06:53.000 --> 01:06:55.000]   just a bit longer
[01:06:55.000 --> 01:06:57.000]   rather than some, like, radical option.
[01:06:57.000 --> 01:06:58.000]   So, yeah, I agree.
[01:06:58.000 --> 01:07:00.000]   It's, like, pretty hard to imagine, like,
[01:07:00.000 --> 01:07:02.000]   you know, some kind of
[01:07:02.000 --> 01:07:03.000]   super long period
[01:07:03.000 --> 01:07:04.000]   where everyone's, like,
[01:07:04.000 --> 01:07:06.000]   perfectly agreed on doing this.
[01:07:06.000 --> 01:07:08.000]   But, yeah, I think framing it as, like,
[01:07:08.000 --> 01:07:09.000]   a directional ideal
[01:07:09.000 --> 01:07:11.000]   seems pretty worthwhile.
[01:07:11.000 --> 01:07:12.000]   And I guess, I don't know,
[01:07:12.000 --> 01:07:13.000]   maybe I'm kind of naively hopeful
[01:07:13.000 --> 01:07:14.000]   about the possibility of
[01:07:14.000 --> 01:07:16.000]   coordinating better around
[01:07:16.000 --> 01:07:18.000]   things like that.
[01:07:18.000 --> 01:07:19.000]   There's two reasons why
[01:07:19.000 --> 01:07:20.000]   this seems like a bad idea to me.
[01:07:20.000 --> 01:07:21.000]   One is...
[01:07:21.000 --> 01:07:22.000]   Okay, first of all,
[01:07:22.000 --> 01:07:23.000]   who is going to be deciding
[01:07:23.000 --> 01:07:25.000]   when we've come to a good
[01:07:25.000 --> 01:07:27.000]   consensus about...
[01:07:27.000 --> 01:07:29.000]   Okay, so we've decided, like,
[01:07:29.000 --> 01:07:31.000]   this is the way things should go.
[01:07:31.000 --> 01:07:33.000]   Now we're, like, ready to escape the long reflection
[01:07:33.000 --> 01:07:35.000]   and realize our vision
[01:07:35.000 --> 01:07:37.000]   for the rest of
[01:07:37.000 --> 01:07:38.000]   the lifespan of the universe.
[01:07:38.000 --> 01:07:40.000]   Who is going to be doing that?
[01:07:40.000 --> 01:07:41.000]   It's the people who are, presumably,
[01:07:41.000 --> 01:07:43.000]   in charge of the long reflection.
[01:07:43.000 --> 01:07:44.000]   Almost by definition,
[01:07:44.000 --> 01:07:46.000]   it'll be the people who have an incentive
[01:07:46.000 --> 01:07:49.000]   in preserving whatever power...
[01:07:49.000 --> 01:07:51.000]   well, power balances
[01:07:51.000 --> 01:07:54.000]   exist at the end of the long reflection.
[01:07:54.000 --> 01:07:56.000]   And then the second thing you'd ask is, like...
[01:07:56.000 --> 01:08:00.000]   There's, like, a difference between, I think,
[01:08:00.000 --> 01:08:02.000]   having a consensus on not using
[01:08:02.000 --> 01:08:04.000]   biological weapons or something like that,
[01:08:04.000 --> 01:08:06.000]   where you're limiting a negative,
[01:08:06.000 --> 01:08:07.000]   versus...
[01:08:07.000 --> 01:08:09.000]   It seems like when we've had...
[01:08:09.000 --> 01:08:10.000]   When we've required
[01:08:10.000 --> 01:08:12.000]   society-wide consensus
[01:08:12.000 --> 01:08:14.000]   on what we should aim
[01:08:14.000 --> 01:08:16.000]   towards achieving,
[01:08:16.000 --> 01:08:18.000]   the outcome has not been good in history.
[01:08:18.000 --> 01:08:20.000]   It seems better that on the positive end
[01:08:20.000 --> 01:08:22.000]   to just leave it open-ended,
[01:08:22.000 --> 01:08:24.000]   and then just maybe, when necessary,
[01:08:24.000 --> 01:08:27.000]   say that, like, the very bad things
[01:08:27.000 --> 01:08:30.000]   we might want to restrict together.
[01:08:30.000 --> 01:08:32.000]   Yeah, yeah, yeah. Okay.
[01:08:32.000 --> 01:08:33.000]   I think I kind of just agree
[01:08:33.000 --> 01:08:35.000]   with a lot of what you said, so...
[01:08:35.000 --> 01:08:39.000]   I think the best, like, framing of this
[01:08:39.000 --> 01:08:41.000]   is the version where when you're
[01:08:41.000 --> 01:08:43.000]   preventing something,
[01:08:43.000 --> 01:08:45.000]   which most people can agree,
[01:08:45.000 --> 01:08:47.000]   is negative.
[01:08:47.000 --> 01:08:49.000]   Which is to say, some actor,
[01:08:49.000 --> 01:08:52.000]   unilaterally deciding to, like,
[01:08:52.000 --> 01:08:53.000]   do this huge irreversible...
[01:08:53.000 --> 01:08:56.000]   Or set out on this huge irreversible project.
[01:08:56.000 --> 01:08:58.000]   Like, something you said was
[01:08:58.000 --> 01:09:01.000]   that the outcome is going to reflect
[01:09:01.000 --> 01:09:05.000]   the, like, values of
[01:09:05.000 --> 01:09:07.000]   whoever is, like, in charge.
[01:09:07.000 --> 01:09:09.000]   And not just the values, I mean, also...
[01:09:09.000 --> 01:09:11.000]   I mean, just, like, think about how guilds work, right?
[01:09:11.000 --> 01:09:13.000]   It's like...
[01:09:13.000 --> 01:09:15.000]   If the...
[01:09:15.000 --> 01:09:17.000]   Whenever we, for example, in industry,
[01:09:17.000 --> 01:09:19.000]   we let how the industry should progress,
[01:09:19.000 --> 01:09:21.000]   we let those kinds of decisions be made collectively
[01:09:21.000 --> 01:09:23.000]   by the people who are currently
[01:09:23.000 --> 01:09:25.000]   dominant in the industry.
[01:09:25.000 --> 01:09:28.000]   You know, guilds or something like that.
[01:09:28.000 --> 01:09:33.000]   Or, like, industrial conspiracies as well.
[01:09:33.000 --> 01:09:35.000]   It seems like the outcome is...
[01:09:35.000 --> 01:09:38.000]   Outcome is just bad.
[01:09:38.000 --> 01:09:41.000]   And so, like, my prior would be that
[01:09:41.000 --> 01:09:43.000]   at the end of such a situation,
[01:09:43.000 --> 01:09:45.000]   our ideas about what we should do
[01:09:45.000 --> 01:09:47.000]   would actually be worse than
[01:09:47.000 --> 01:09:49.000]   going into the long reflection.
[01:09:49.000 --> 01:09:51.000]   I mean, obviously,
[01:09:51.000 --> 01:09:53.000]   it really depends on how it's implemented, right?
[01:09:53.000 --> 01:09:55.000]   So I'm not saying that.
[01:09:55.000 --> 01:09:57.000]   But just, like, broadly,
[01:09:57.000 --> 01:09:59.000]   given all possible implementations,
[01:09:59.000 --> 01:10:01.000]   and maybe the most likely implementation
[01:10:01.000 --> 01:10:03.000]   given how governments run now.
[01:10:03.000 --> 01:10:05.000]   Yeah, yeah, yeah.
[01:10:05.000 --> 01:10:07.000]   I should say that, like, I am, in fact,
[01:10:07.000 --> 01:10:09.000]   like, pretty absurd.
[01:10:09.000 --> 01:10:12.000]   It's more enjoyable to give this thing its hearing.
[01:10:12.000 --> 01:10:14.000]   No, no, I enjoy the parts
[01:10:14.000 --> 01:10:16.000]   where we have disagreements, yeah.
[01:10:16.000 --> 01:10:19.000]   So one thought here is
[01:10:19.000 --> 01:10:22.000]   if you're worried about the future,
[01:10:22.000 --> 01:10:24.000]   like, the course of the future being determined
[01:10:24.000 --> 01:10:28.000]   by some single actor,
[01:10:28.000 --> 01:10:30.000]   I mean, that worry is just symmetrical
[01:10:30.000 --> 01:10:32.000]   with the worry of letting whoever
[01:10:32.000 --> 01:10:34.000]   wins some race first go and do,
[01:10:34.000 --> 01:10:36.000]   you know, go and do the thing,
[01:10:36.000 --> 01:10:38.000]   the, like, project where they
[01:10:38.000 --> 01:10:40.000]   more or less kind of determine
[01:10:40.000 --> 01:10:42.000]   the habits of the rest of humanity.
[01:10:42.000 --> 01:10:44.000]   So the option where you, like,
[01:10:44.000 --> 01:10:46.000]   kind of deliberately wait
[01:10:46.000 --> 01:10:48.000]   and let people, like, have some,
[01:10:48.000 --> 01:10:51.000]   like, global conversation,
[01:10:51.000 --> 01:10:53.000]   I don't know, it seems like that
[01:10:53.000 --> 01:10:55.000]   is less worrying,
[01:10:55.000 --> 01:10:57.000]   even if the worry is still there.
[01:10:57.000 --> 01:10:59.000]   I should also say,
[01:10:59.000 --> 01:11:01.000]   I can imagine the outcome
[01:11:01.000 --> 01:11:03.000]   is not unanimity.
[01:11:03.000 --> 01:11:05.000]   In fact, it would be, like, pretty wild if it was, right?
[01:11:05.000 --> 01:11:07.000]   But you want the outcome to be some kind of, like,
[01:11:07.000 --> 01:11:10.000]   stable, friendly disagreement where
[01:11:10.000 --> 01:11:12.000]   now we can kind of, like,
[01:11:12.000 --> 01:11:14.000]   maybe reach some kind of Coasean solution
[01:11:14.000 --> 01:11:16.000]   and we just, like, go and do our own things,
[01:11:16.000 --> 01:11:18.000]   like a bunch of projects which kind of go off at once.
[01:11:18.000 --> 01:11:20.000]   I don't know, that feels, like, really great to me
[01:11:20.000 --> 01:11:22.000]   compared to whoever gets there first
[01:11:22.000 --> 01:11:24.000]   determining how things turn out.
[01:11:24.000 --> 01:11:26.000]   But yeah, I mean, it's hard to talk about stuff, right?
[01:11:26.000 --> 01:11:28.000]   Because it's, like, somewhat speculative,
[01:11:28.000 --> 01:11:30.000]   but I think it's just, like, a useful, like,
[01:11:30.000 --> 01:11:32.000]   north star or something
[01:11:32.000 --> 01:11:34.000]   to try pointing towards.
[01:11:34.000 --> 01:11:36.000]   Okay, so maybe to make it more concrete,
[01:11:36.000 --> 01:11:38.000]   I wonder if your
[01:11:38.000 --> 01:11:40.000]   if your expectation
[01:11:40.000 --> 01:11:42.000]   that the consensus view
[01:11:42.000 --> 01:11:44.000]   would be better than the first-mover view.
[01:11:44.000 --> 01:11:46.000]   In, like, today's world,
[01:11:46.000 --> 01:11:48.000]   maybe, okay, either we
[01:11:48.000 --> 01:11:50.000]   either we have the form of
[01:11:50.000 --> 01:11:52.000]   government and
[01:11:52.000 --> 01:11:54.000]   not just government, but also the
[01:11:54.000 --> 01:11:56.000]   the, I mean, the
[01:11:56.000 --> 01:11:58.000]   industrial and logistical organization
[01:11:58.000 --> 01:12:00.000]   that, I don't know, like, Elon Musk
[01:12:00.000 --> 01:12:02.000]   has designed for Mars.
[01:12:02.000 --> 01:12:04.000]   So, if he's the first-mover for Mars.
[01:12:04.000 --> 01:12:06.000]   Would you prefer that
[01:12:06.000 --> 01:12:08.000]   or we have the UN
[01:12:08.000 --> 01:12:10.000]   come to a consensus between all the
[01:12:10.000 --> 01:12:12.000]   different countries about, like,
[01:12:12.000 --> 01:12:14.000]   how we should have the first Mars colony organized?
[01:12:14.000 --> 01:12:16.000]   Or would the Mars colony
[01:12:16.000 --> 01:12:18.000]   run better if, after, like, 10-20 years
[01:12:18.000 --> 01:12:20.000]   of that, they're the ones who
[01:12:20.000 --> 01:12:22.000]   decide how the first Mars colony goes?
[01:12:22.000 --> 01:12:24.000]   Global consensus view is to be better
[01:12:24.000 --> 01:12:26.000]   than first-mover views?
[01:12:26.000 --> 01:12:28.000]   Yeah, that's a good question. And, I mean,
[01:12:28.000 --> 01:12:30.000]   one obvious point is, not always, right?
[01:12:30.000 --> 01:12:32.000]   Like, there are certainly cases where the consensus
[01:12:32.000 --> 01:12:34.000]   view is just, like, somewhat worse.
[01:12:34.000 --> 01:12:36.000]   I think you limit the downside
[01:12:36.000 --> 01:12:38.000]   with the consensus view, right? Because
[01:12:38.000 --> 01:12:40.000]   you give people space to express
[01:12:40.000 --> 01:12:42.000]   why they just don't think this, like,
[01:12:42.000 --> 01:12:44.000]   one idea is bad.
[01:12:44.000 --> 01:12:46.000]   I don't know if this answers your question, but, like,
[01:12:46.000 --> 01:12:48.000]   it's a really good one.
[01:12:48.000 --> 01:12:50.000]   You can imagine the kind of the UN-led
[01:12:50.000 --> 01:12:52.000]   thing is going to be, like, way slower.
[01:12:52.000 --> 01:12:54.000]   It's going to probably be way more expensive.
[01:12:54.000 --> 01:12:56.000]   The International Space Station is a good example
[01:12:56.000 --> 01:12:58.000]   where, um,
[01:12:58.000 --> 01:13:00.000]   I don't know, I think that turned out pretty well, but
[01:13:00.000 --> 01:13:02.000]   a, like, private version of that would have happened,
[01:13:02.000 --> 01:13:04.000]   like, in terms of a lot more
[01:13:04.000 --> 01:13:06.000]   effectively.
[01:13:06.000 --> 01:13:08.000]   I guess I'm not, like, the Elon example
[01:13:08.000 --> 01:13:10.000]   is kind of a good one because
[01:13:10.000 --> 01:13:12.000]   it's not obvious why that's, like, super worrying.
[01:13:12.000 --> 01:13:14.000]   The thing I have in mind
[01:13:14.000 --> 01:13:16.000]   in the, like, long reflection example
[01:13:16.000 --> 01:13:18.000]   is maybe, like, a bit more kind of wild.
[01:13:18.000 --> 01:13:20.000]   But it's, like, really hard to make it concrete,
[01:13:20.000 --> 01:13:22.000]   so I'm, yeah, somewhat floundering.
[01:13:22.000 --> 01:13:24.000]   There's also another reason to, like,
[01:13:24.000 --> 01:13:26.000]   to the extent that somebody
[01:13:26.000 --> 01:13:28.000]   has the resources,
[01:13:28.000 --> 01:13:30.000]   I don't know, maybe this, like,
[01:13:30.000 --> 01:13:32.000]   just gets to an irreconcilable
[01:13:32.000 --> 01:13:34.000]   question about your priors
[01:13:34.000 --> 01:13:36.000]   about other kinds of political
[01:13:36.000 --> 01:13:38.000]   things, but to the extent that somebody
[01:13:38.000 --> 01:13:40.000]   has been able to build up resources privately
[01:13:40.000 --> 01:13:42.000]   to be able to be a first mover
[01:13:42.000 --> 01:13:44.000]   in a way that is going to matter for the long
[01:13:44.000 --> 01:13:46.000]   term, what do you think about
[01:13:46.000 --> 01:13:48.000]   what kind of views they're likely to have and what kind of competencies
[01:13:48.000 --> 01:13:50.000]   they're likely to have versus assuming
[01:13:50.000 --> 01:13:52.000]   that the way governments work and
[01:13:52.000 --> 01:13:54.000]   function and the quality of their
[01:13:54.000 --> 01:13:56.000]   governance doesn't change that much
[01:13:56.000 --> 01:13:58.000]   for the next hundred years, what kind
[01:13:58.000 --> 01:14:00.000]   of outcomes you will have from...
[01:14:00.000 --> 01:14:02.000]   Basically, if you think, like, the likelihood of
[01:14:02.000 --> 01:14:04.000]   leaders like Donald Trump or Joe Biden is, like,
[01:14:04.000 --> 01:14:06.000]   going to be similar for, like, the next hundred years,
[01:14:06.000 --> 01:14:08.000]   and if you think, like, the richest
[01:14:08.000 --> 01:14:10.000]   people in the world that are the first movers are going to be people
[01:14:10.000 --> 01:14:12.000]   that are similar to Elon Musk, I can
[01:14:12.000 --> 01:14:14.000]   see people having, like, genuinely different
[01:14:14.000 --> 01:14:16.000]   reasonable views about who should, like,
[01:14:16.000 --> 01:14:18.000]   should Elon Musk of a hundred
[01:14:18.000 --> 01:14:20.000]   years from now or Joe Biden of a hundred years from now
[01:14:20.000 --> 01:14:22.000]   have the power to decide the long-run
[01:14:22.000 --> 01:14:24.000]   course of humanity? Is that
[01:14:24.000 --> 01:14:26.000]   the bulk of this debate that you think is important or is that
[01:14:26.000 --> 01:14:28.000]   maybe not as relevant as I might think?
[01:14:28.000 --> 01:14:30.000]   Yeah, I guess I'll try saying some things and maybe it will, like,
[01:14:30.000 --> 01:14:32.000]   respond to that. Kind of two things are going
[01:14:32.000 --> 01:14:34.000]   through my head, so one is
[01:14:34.000 --> 01:14:36.000]   something like
[01:14:36.000 --> 01:14:38.000]   you should expect these questions
[01:14:38.000 --> 01:14:40.000]   about, like, what should we do when
[01:14:40.000 --> 01:14:42.000]   we have the capacity to do, like, a far larger
[01:14:42.000 --> 01:14:44.000]   range of things than we currently have the capacity to do.
[01:14:44.000 --> 01:14:46.000]   That question is going
[01:14:46.000 --> 01:14:48.000]   to hinge, like, much more importantly
[01:14:48.000 --> 01:14:50.000]   on, like, theories
[01:14:50.000 --> 01:14:52.000]   people have and, like, world views
[01:14:52.000 --> 01:14:54.000]   and very, kind of, particular details
[01:14:54.000 --> 01:14:56.000]   much more than it does
[01:14:56.000 --> 01:14:58.000]   now. And
[01:14:58.000 --> 01:15:00.000]   I'm going to do a bad job of trying
[01:15:00.000 --> 01:15:02.000]   to articulate this, but there's some
[01:15:02.000 --> 01:15:04.000]   kind of analogy here where if you're,
[01:15:04.000 --> 01:15:06.000]   like, fitting a curve to some points
[01:15:06.000 --> 01:15:08.000]   you can, like, overfit it.
[01:15:08.000 --> 01:15:10.000]   In fact, you can overfit it in various ways
[01:15:10.000 --> 01:15:12.000]   and they all look pretty similar. But then
[01:15:12.000 --> 01:15:14.000]   if you, like, extend the axis so you, like, see what happens
[01:15:14.000 --> 01:15:16.000]   to the curves, like, beyond the points, those
[01:15:16.000 --> 01:15:18.000]   different ways of fitting it can, like, go all over the place.
[01:15:18.000 --> 01:15:20.000]   And so, like, there's some
[01:15:20.000 --> 01:15:22.000]   analogy here where when you, kind of, expand
[01:15:22.000 --> 01:15:24.000]   the space of what
[01:15:24.000 --> 01:15:26.000]   we could possibly do, different
[01:15:26.000 --> 01:15:28.000]   views which look, kind of, similar
[01:15:28.000 --> 01:15:30.000]   right now, or at least come to similar conclusions
[01:15:30.000 --> 01:15:32.000]   they just, like, go all over the shop
[01:15:32.000 --> 01:15:34.000]   and so that is not responding to your point, but I think
[01:15:34.000 --> 01:15:36.000]   it's, like, maybe worth saying, like,
[01:15:36.000 --> 01:15:38.000]   this is a reason for expecting
[01:15:38.000 --> 01:15:40.000]   reflecting on what the right view is to be
[01:15:40.000 --> 01:15:42.000]   quite important. And, like,
[01:15:42.000 --> 01:15:44.000]   then I guess that leads into a second
[01:15:44.000 --> 01:15:46.000]   thought, which is
[01:15:46.000 --> 01:15:48.000]   something, like, I guess there's two things going on.
[01:15:48.000 --> 01:15:50.000]   One is the thing you mentioned, which is
[01:15:50.000 --> 01:15:52.000]   there are basically just a bunch of
[01:15:52.000 --> 01:15:54.000]   political dynamics where you can just, like,
[01:15:54.000 --> 01:15:56.000]   reason about where you
[01:15:56.000 --> 01:15:58.000]   should expect values to head
[01:15:58.000 --> 01:16:00.000]   for, like, political reasons
[01:16:00.000 --> 01:16:02.000]   in some senses, like, now better than
[01:16:02.000 --> 01:16:04.000]   the defaults. And what is
[01:16:04.000 --> 01:16:06.000]   that default? And then
[01:16:06.000 --> 01:16:08.000]   there's, like, a kind of different way of
[01:16:08.000 --> 01:16:10.000]   thinking about things, which is, like,
[01:16:10.000 --> 01:16:12.000]   separately from political dynamics.
[01:16:12.000 --> 01:16:14.000]   Can we actually make progress in, like, thinking better
[01:16:14.000 --> 01:16:16.000]   about what's best to do? In the same way
[01:16:16.000 --> 01:16:18.000]   that we can, like, make progress in
[01:16:18.000 --> 01:16:20.000]   science, like, kind of separately
[01:16:20.000 --> 01:16:22.000]   from the fact that, like,
[01:16:22.000 --> 01:16:24.000]   people's views about science are influenced by, like,
[01:16:24.000 --> 01:16:26.000]   political dynamics. And
[01:16:26.000 --> 01:16:28.000]   maybe, like, a disagreement here is
[01:16:28.000 --> 01:16:30.000]   a disagreement about, like, how much
[01:16:30.000 --> 01:16:32.000]   scope there is to just get better at thinking
[01:16:32.000 --> 01:16:34.000]   about these things. I mean, one,
[01:16:34.000 --> 01:16:36.000]   like, reason I can give,
[01:16:36.000 --> 01:16:38.000]   I guess I kind of mentioned this earlier, is
[01:16:38.000 --> 01:16:40.000]   this project here of, like, thinking about what's
[01:16:40.000 --> 01:16:42.000]   best to do, maybe kind of thinking better about
[01:16:42.000 --> 01:16:44.000]   ethics, is not the thing
[01:16:44.000 --> 01:16:46.000]   -- it's, like, maybe more relevant
[01:16:46.000 --> 01:16:48.000]   to think that this is, like,
[01:16:48.000 --> 01:16:50.000]   on the order of, kind of, 30 years old, rather
[01:16:50.000 --> 01:16:52.000]   than on the order of 2,000 years old.
[01:16:52.000 --> 01:16:54.000]   You might call it, like, secular ethics. Parfait
[01:16:54.000 --> 01:16:56.000]   writes about this, right? He's, like,
[01:16:56.000 --> 01:16:58.000]   talks about this kind of -- there are at least reasons
[01:16:58.000 --> 01:17:00.000]   for hope. We haven't ruled out that
[01:17:00.000 --> 01:17:02.000]   we can make -- not make a lot of progress.
[01:17:02.000 --> 01:17:04.000]   Because the thing we were doing before, like,
[01:17:04.000 --> 01:17:06.000]   we were trying to think systematically about what's best to do
[01:17:06.000 --> 01:17:08.000]   was just very unlike the thing that
[01:17:08.000 --> 01:17:10.000]   we should be interested in. I'm sorry that was, like, a huge
[01:17:10.000 --> 01:17:12.000]   rabble, but hopefully there's something there.
[01:17:12.000 --> 01:17:14.000]   Yeah, I want to go back to what you were saying
[01:17:14.000 --> 01:17:16.000]   earlier about how you can
[01:17:16.000 --> 01:17:18.000]   think of,
[01:17:18.000 --> 01:17:20.000]   I don't know, global consensus as the
[01:17:20.000 --> 01:17:22.000]   reduced variance version
[01:17:22.000 --> 01:17:24.000]   of future views. And, you know,
[01:17:24.000 --> 01:17:26.000]   I think that, like, to the extent that you think a downside
[01:17:26.000 --> 01:17:28.000]   is really bad, I think that's
[01:17:28.000 --> 01:17:30.000]   a good argument.
[01:17:30.000 --> 01:17:32.000]   And then, yeah, I mean, it's, like, similar
[01:17:32.000 --> 01:17:34.000]   to my argument against, like, monarchists,
[01:17:34.000 --> 01:17:36.000]   which is that, like, actually, I think
[01:17:36.000 --> 01:17:38.000]   it is reasonable to expect that if you could, like,
[01:17:38.000 --> 01:17:40.000]   reasonably -- you could reliably
[01:17:40.000 --> 01:17:42.000]   have people like Lee Kuan Yew who are in charge of your country
[01:17:42.000 --> 01:17:44.000]   and you have a monarchy, that things might
[01:17:44.000 --> 01:17:46.000]   be better than a democracy. It's just that
[01:17:46.000 --> 01:17:48.000]   the bad outcome is just
[01:17:48.000 --> 01:17:50.000]   so bad that it's, like, better just having, like,
[01:17:50.000 --> 01:17:52.000]   a low variance thing
[01:17:52.000 --> 01:17:54.000]   like democracy. It's a fun one to talk about.
[01:17:54.000 --> 01:17:56.000]   Maybe one last kind of trailing thought
[01:17:56.000 --> 01:17:58.000]   on what you said is,
[01:17:58.000 --> 01:18:00.000]   I think, I guess, Popper
[01:18:00.000 --> 01:18:02.000]   has this thought, and also David Deutsch
[01:18:02.000 --> 01:18:04.000]   did a really good job at kind of
[01:18:04.000 --> 01:18:06.000]   explaining it about
[01:18:06.000 --> 01:18:08.000]   one, like, underrated
[01:18:08.000 --> 01:18:10.000]   value of democracy is not just
[01:18:10.000 --> 01:18:12.000]   in some sense having this
[01:18:12.000 --> 01:18:14.000]   function to, like,
[01:18:14.000 --> 01:18:16.000]   combine
[01:18:16.000 --> 01:18:18.000]   people's views into, like,
[01:18:18.000 --> 01:18:20.000]   some kind of, you know,
[01:18:20.000 --> 01:18:22.000]   optimal path which is, like, some mishmash
[01:18:22.000 --> 01:18:24.000]   where everyone thinks. It's also, like,
[01:18:24.000 --> 01:18:26.000]   having the ability for
[01:18:26.000 --> 01:18:28.000]   people who are being governed
[01:18:28.000 --> 01:18:30.000]   to just, like, cancel
[01:18:30.000 --> 01:18:32.000]   this current experiment in governance and try
[01:18:32.000 --> 01:18:34.000]   again. So, you know, it's, like,
[01:18:34.000 --> 01:18:36.000]   we'll give you freedom to,
[01:18:36.000 --> 01:18:38.000]   you know, implement this kind of governance plan
[01:18:38.000 --> 01:18:40.000]   that seems really exciting, and then we're just going to, like,
[01:18:40.000 --> 01:18:42.000]   pull the brakes when it goes wrong.
[01:18:42.000 --> 01:18:44.000]   And that kind of option to, like,
[01:18:44.000 --> 01:18:46.000]   start again, in general, just feels, like,
[01:18:46.000 --> 01:18:48.000]   really important as some kind of tool
[01:18:48.000 --> 01:18:50.000]   you want in your, like, toolkit when you're thinking
[01:18:50.000 --> 01:18:52.000]   about these, like, pretty big futures.
[01:18:52.000 --> 01:18:54.000]   I guess my hesitation about this is I can't
[01:18:54.000 --> 01:18:56.000]   imagine a form of government
[01:18:56.000 --> 01:18:58.000]   where, at the end of it,
[01:18:58.000 --> 01:19:00.000]   I would expect that a consensus
[01:19:00.000 --> 01:19:02.000]   view from, I mean, not just,
[01:19:02.000 --> 01:19:04.000]   like, nerdy communities
[01:19:04.000 --> 01:19:06.000]   like EA, but, like, an actual global
[01:19:06.000 --> 01:19:08.000]   consensus would be something that I think
[01:19:08.000 --> 01:19:10.000]   is a good path.
[01:19:10.000 --> 01:19:12.000]   Maybe it's something, like, I don't think it's, like, the
[01:19:12.000 --> 01:19:14.000]   worst possible path, but, I mean, one thing
[01:19:14.000 --> 01:19:16.000]   about reducing variance is, like, if you think the far future
[01:19:16.000 --> 01:19:18.000]   could be really, really good,
[01:19:18.000 --> 01:19:20.000]   then by reducing variance, you're, like, cutting off
[01:19:20.000 --> 01:19:22.000]   a lot of expected value, right?
[01:19:22.000 --> 01:19:24.000]   And then you can think, like, democracy
[01:19:24.000 --> 01:19:26.000]   works much better in cases
[01:19:26.000 --> 01:19:28.000]   where the problem is, like, closer
[01:19:28.000 --> 01:19:30.000]   to something that the people can experience.
[01:19:30.000 --> 01:19:32.000]   It's, like, I don't know if democracies
[01:19:32.000 --> 01:19:34.000]   don't have famines, because
[01:19:34.000 --> 01:19:36.000]   if there's a famine, you get voted out, right?
[01:19:36.000 --> 01:19:38.000]   Or, like, you have major wars
[01:19:38.000 --> 01:19:40.000]   as well, right? But if you're talking about,
[01:19:40.000 --> 01:19:42.000]   like, some form
[01:19:42.000 --> 01:19:44.000]   of consensus way of deciding
[01:19:44.000 --> 01:19:46.000]   what should the far, far future look like,
[01:19:46.000 --> 01:19:48.000]   it's not clear to me why the consensus
[01:19:48.000 --> 01:19:50.000]   view on that would be
[01:19:50.000 --> 01:19:52.000]   as likely to be correct.
[01:19:52.000 --> 01:19:54.000]   Yeah, yeah, yeah. I think maybe
[01:19:54.000 --> 01:19:56.000]   somewhat what's going on here is
[01:19:56.000 --> 01:19:58.000]   I'd want to resist the...
[01:19:58.000 --> 01:20:00.000]   And it's my fault
[01:20:00.000 --> 01:20:02.000]   for, I think, like, suggesting this framing
[01:20:02.000 --> 01:20:04.000]   that it's, like, you just...
[01:20:04.000 --> 01:20:06.000]   You spend a bunch of time thinking
[01:20:06.000 --> 01:20:08.000]   and, like, having this conversation, and then you
[01:20:08.000 --> 01:20:10.000]   just hate this, like, international vote on what we should do.
[01:20:10.000 --> 01:20:12.000]   And I think
[01:20:12.000 --> 01:20:14.000]   maybe another
[01:20:14.000 --> 01:20:16.000]   framing is something like, let's just give the time
[01:20:16.000 --> 01:20:18.000]   for the people who, like,
[01:20:18.000 --> 01:20:20.000]   want to be involved in this to, like,
[01:20:20.000 --> 01:20:22.000]   make the progress that could be possible on thinking
[01:20:22.000 --> 01:20:24.000]   about these things, and then just, like, see where we
[01:20:24.000 --> 01:20:26.000]   end up, where...
[01:20:26.000 --> 01:20:28.000]   I don't know, there's, like, a very weak analogy
[01:20:28.000 --> 01:20:30.000]   to progress in, like,
[01:20:30.000 --> 01:20:32.000]   other fields, where we don't
[01:20:32.000 --> 01:20:34.000]   make progress in, like,
[01:20:34.000 --> 01:20:36.000]   mathematics or science by, like,
[01:20:36.000 --> 01:20:38.000]   taking enormous votes on what's true.
[01:20:38.000 --> 01:20:40.000]   But we can by just, like,
[01:20:40.000 --> 01:20:42.000]   giving people who are interested in making progress
[01:20:42.000 --> 01:20:44.000]   the space and time to do that.
[01:20:44.000 --> 01:20:46.000]   And then at the end, it's, like, often pretty obvious
[01:20:46.000 --> 01:20:48.000]   what turns out. That's, like, very begging the question
[01:20:48.000 --> 01:20:50.000]   because it's, like, way more obvious
[01:20:50.000 --> 01:20:52.000]   what's right and wrong if you're, like,
[01:20:52.000 --> 01:20:54.000]   doing maths compared to doing this kind of thing.
[01:20:54.000 --> 01:20:56.000]   But... No, but also, like,
[01:20:56.000 --> 01:20:58.000]   what happens if... This seems similar to, like,
[01:20:58.000 --> 01:21:00.000]   the question about monarchy, where
[01:21:00.000 --> 01:21:02.000]   it's, like, what happens if you pick the wrong person
[01:21:02.000 --> 01:21:04.000]   or, like, the wrong politburo
[01:21:04.000 --> 01:21:06.000]   to pick what the...
[01:21:06.000 --> 01:21:08.000]   what the charter you take
[01:21:08.000 --> 01:21:10.000]   to the rest of the universe is?
[01:21:10.000 --> 01:21:12.000]   Yeah, it seems like a hard problem to ensure that you have
[01:21:12.000 --> 01:21:14.000]   the group of people who will be deciding this.
[01:21:14.000 --> 01:21:16.000]   Either if it's a consensus
[01:21:16.000 --> 01:21:18.000]   or if it's a single person or anything in between.
[01:21:18.000 --> 01:21:20.000]   Like, it has to be some decision-maker, right?
[01:21:20.000 --> 01:21:22.000]   I think you can just imagine there being
[01:21:22.000 --> 01:21:24.000]   no decision-maker, right? So, like,
[01:21:24.000 --> 01:21:26.000]   the thing could be, let's agree
[01:21:26.000 --> 01:21:28.000]   to have some time to reflect
[01:21:28.000 --> 01:21:30.000]   on what is best,
[01:21:30.000 --> 01:21:32.000]   and we might come to some consensus.
[01:21:32.000 --> 01:21:34.000]   And then at the end, like, you know,
[01:21:34.000 --> 01:21:36.000]   one version of this is just let things happen.
[01:21:36.000 --> 01:21:38.000]   Like, there's no final decision when someone walks up.
[01:21:38.000 --> 01:21:40.000]   It's just, like, that time
[01:21:40.000 --> 01:21:42.000]   between doing the thing and thinking about it,
[01:21:42.000 --> 01:21:44.000]   just, like, extending that time for a bit seems good.
[01:21:44.000 --> 01:21:46.000]   I see. Okay. Okay.
[01:21:46.000 --> 01:21:48.000]   Yeah, so, sorry I missed that earlier.
[01:21:48.000 --> 01:21:50.000]   That's okay.
[01:21:50.000 --> 01:21:52.000]   Okay, so, actually, one of the major things
[01:21:52.000 --> 01:21:54.000]   we discussed, this is, like, one, all the things
[01:21:54.000 --> 01:21:56.000]   we discussed so far was one, like,
[01:21:56.000 --> 01:21:58.000]   quadrant of, like,
[01:21:58.000 --> 01:22:00.000]   of the conversation.
[01:22:00.000 --> 01:22:02.000]   Actually, you know, before we talk about space governance,
[01:22:02.000 --> 01:22:04.000]   let's talk about podcasting.
[01:22:04.000 --> 01:22:06.000]   So, you have your own podcast.
[01:22:06.000 --> 01:22:08.000]   I have my own. What have you,
[01:22:08.000 --> 01:22:10.000]   why did you start it, and, like, what have you,
[01:22:10.000 --> 01:22:12.000]   what have been your, like, experiences so far?
[01:22:12.000 --> 01:22:14.000]   What have you learned about the joy
[01:22:14.000 --> 01:22:16.000]   and impact of podcasting?
[01:22:16.000 --> 01:22:18.000]   So, story is, Luca,
[01:22:18.000 --> 01:22:20.000]   who's a close friend of mine who I do this podcast with,
[01:22:20.000 --> 01:22:22.000]   we're both at university together,
[01:22:22.000 --> 01:22:24.000]   and we're, like, both podcast nerds,
[01:22:24.000 --> 01:22:26.000]   and I think I remember, we were in our
[01:22:26.000 --> 01:22:28.000]   last year, and we had this
[01:22:28.000 --> 01:22:30.000]   conversation, like,
[01:22:30.000 --> 01:22:32.000]   we're, like, surrounded by all these people
[01:22:32.000 --> 01:22:34.000]   who just seem, like, incredibly interesting, like, all these, you know,
[01:22:34.000 --> 01:22:36.000]   like, academics we really love to talk about
[01:22:36.000 --> 01:22:38.000]   or talk to,
[01:22:38.000 --> 01:22:40.000]   and
[01:22:40.000 --> 01:22:42.000]   if we just, like,
[01:22:42.000 --> 01:22:44.000]   email them saying we're doing a podcast
[01:22:44.000 --> 01:22:46.000]   and wanted to interview them, that could be a pretty good
[01:22:46.000 --> 01:22:48.000]   excuse to talk to them.
[01:22:48.000 --> 01:22:50.000]   So, let's see how easy it is to do this.
[01:22:50.000 --> 01:22:52.000]   Turns out startup costs on doing a podcast
[01:22:52.000 --> 01:22:54.000]   are, like, pretty low if you want to do, like, a scrappy version
[01:22:54.000 --> 01:22:56.000]   of it, right? Did that,
[01:22:56.000 --> 01:22:58.000]   it turns out that, like, academics
[01:22:58.000 --> 01:23:00.000]   especially, but just, like, tons of people really
[01:23:00.000 --> 01:23:02.000]   love being asked to talk about
[01:23:02.000 --> 01:23:04.000]   the things they think about all day,
[01:23:04.000 --> 01:23:06.000]   right? It's just, like, a complete win-win
[01:23:06.000 --> 01:23:08.000]   where you're, like, you're trying to boost the ideas
[01:23:08.000 --> 01:23:10.000]   of someone or some actual person
[01:23:10.000 --> 01:23:12.000]   who you think deserves more airtime,
[01:23:12.000 --> 01:23:14.000]   that person gets
[01:23:14.000 --> 01:23:16.000]   to, like, talk about their work and,
[01:23:16.000 --> 01:23:18.000]   you know, spread their ideas.
[01:23:18.000 --> 01:23:20.000]   So, it's, like, huh, there's, like, no downsides
[01:23:20.000 --> 01:23:22.000]   to doing this other than the time.
[01:23:22.000 --> 01:23:24.000]   Also, I should say that the, kind of, yes rate on our
[01:23:24.000 --> 01:23:26.000]   emails was, like, considerably
[01:23:26.000 --> 01:23:28.000]   higher than we thought.
[01:23:28.000 --> 01:23:30.000]   We were, you know, like,
[01:23:30.000 --> 01:23:32.000]   two random undergrads with
[01:23:32.000 --> 01:23:34.000]   microphones,
[01:23:34.000 --> 01:23:36.000]   but there's this really nice, like, kind of snowball effect
[01:23:36.000 --> 01:23:38.000]   where if
[01:23:38.000 --> 01:23:40.000]   someone who is, like, well-known
[01:23:40.000 --> 01:23:42.000]   is, like,
[01:23:42.000 --> 01:23:44.000]   gracious enough to say yes despite
[01:23:44.000 --> 01:23:46.000]   not really knowing what you're about
[01:23:46.000 --> 01:23:48.000]   and then you do an interview and, like,
[01:23:48.000 --> 01:23:50.000]   it's a pretty good interview.
[01:23:50.000 --> 01:23:52.000]   When you're emailing the next person, you don't have to, like,
[01:23:52.000 --> 01:23:54.000]   sell yourself. You can just be, like, "Hey, I spoke to this other
[01:23:54.000 --> 01:23:56.000]   impressive person," and, of course,
[01:23:56.000 --> 01:23:58.000]   you get this, like, this kind of snowball.
[01:23:58.000 --> 01:24:00.000]   So...
[01:24:00.000 --> 01:24:02.000]   It's definitely a Fonzie scheme. It's great.
[01:24:02.000 --> 01:24:04.000]   Is it the best kind of Fonzie scheme, though?
[01:24:04.000 --> 01:24:06.000]   Podcasts as, like, a form of media are just, like,
[01:24:06.000 --> 01:24:08.000]   incredibly special. There's something
[01:24:08.000 --> 01:24:10.000]   about just the incentives between, like, guests
[01:24:10.000 --> 01:24:12.000]   and hosts just, like, aligned
[01:24:12.000 --> 01:24:14.000]   and, like, I don't know, if this was,
[01:24:14.000 --> 01:24:16.000]   like, some journalistic
[01:24:16.000 --> 01:24:18.000]   interview, it'd be, like, way kind of
[01:24:18.000 --> 01:24:20.000]   more uncomfortable.
[01:24:20.000 --> 01:24:22.000]   There's something about the fact that it's still kind of hard to, like,
[01:24:22.000 --> 01:24:24.000]   search transcripts, so
[01:24:24.000 --> 01:24:26.000]   there's less of a worry about, like, forming
[01:24:26.000 --> 01:24:28.000]   all your words in the right way
[01:24:28.000 --> 01:24:30.000]   so it's just, like, more relaxed.
[01:24:30.000 --> 01:24:32.000]   Yeah.
[01:24:32.000 --> 01:24:34.000]   Recommended. Yeah, I know.
[01:24:34.000 --> 01:24:36.000]   And it's such a natural
[01:24:36.000 --> 01:24:38.000]   form of... You can think
[01:24:38.000 --> 01:24:40.000]   of writing as a sort of way of imitating
[01:24:40.000 --> 01:24:42.000]   conversation, and audiobooks
[01:24:42.000 --> 01:24:44.000]   is a way of trying to imitate,
[01:24:44.000 --> 01:24:46.000]   like, a thing that's trying to imitate
[01:24:46.000 --> 01:24:48.000]   conversation. You're, like, audiobooks seem like
[01:24:48.000 --> 01:24:50.000]   they're supposed to... Because, like,
[01:24:50.000 --> 01:24:52.000]   writing is, like, yeah, you're
[01:24:52.000 --> 01:24:54.000]   visually perceiving
[01:24:54.000 --> 01:24:56.000]   what was originally
[01:24:56.000 --> 01:24:58.000]   an ability you
[01:24:58.000 --> 01:25:00.000]   had originally for understanding, you know,
[01:25:00.000 --> 01:25:04.000]   you know, audible ideas.
[01:25:04.000 --> 01:25:06.000]   But then, audiobooks, it's, like, you're
[01:25:06.000 --> 01:25:08.000]   going through two layers of translation there where
[01:25:08.000 --> 01:25:10.000]   you don't have the natural
[01:25:10.000 --> 01:25:12.000]   repetition and the
[01:25:12.000 --> 01:25:14.000]   ability to gauge the other person's reaction
[01:25:14.000 --> 01:25:16.000]   and so on.
[01:25:16.000 --> 01:25:18.000]   And the back and forth, obviously, that
[01:25:18.000 --> 01:25:20.000]   an actual conversation has.
[01:25:20.000 --> 01:25:22.000]   And, yeah, so that's
[01:25:22.000 --> 01:25:24.000]   why it's, like, people potentially listen
[01:25:24.000 --> 01:25:26.000]   to, like, podcasts too much where, I don't know,
[01:25:26.000 --> 01:25:28.000]   they're just, like, they have
[01:25:28.000 --> 01:25:30.000]   something in their ears the whole day,
[01:25:30.000 --> 01:25:32.000]   which you can't imagine for audiobooks, right?
[01:25:32.000 --> 01:25:34.000]   Yeah, a few things this makes me think of.
[01:25:34.000 --> 01:25:36.000]   One is there's some experiment where, I guess you can just
[01:25:36.000 --> 01:25:38.000]   do it yourself, when if you
[01:25:38.000 --> 01:25:40.000]   force people not to use disfluences,
[01:25:40.000 --> 01:25:42.000]   disfluencies, sorry, like "ums" and "ahs",
[01:25:42.000 --> 01:25:44.000]   those people just get, like, much
[01:25:44.000 --> 01:25:46.000]   worse at reading
[01:25:46.000 --> 01:25:48.000]   words. In some sense, like,
[01:25:48.000 --> 01:25:50.000]   disfluencies, like, help us, I guess I'm using
[01:25:50.000 --> 01:25:52.000]   the word "like" right now, communicate
[01:25:52.000 --> 01:25:54.000]   thoughts for some reason. And then
[01:25:54.000 --> 01:25:56.000]   if you take a podcast like this,
[01:25:56.000 --> 01:25:58.000]   I guess I can speak for myself,
[01:25:58.000 --> 01:26:00.000]   and then you
[01:26:00.000 --> 01:26:02.000]   word for word transcribe
[01:26:02.000 --> 01:26:04.000]   what you are saying, or when I say
[01:26:04.000 --> 01:26:06.000]   you, I mean me. It's, like,
[01:26:06.000 --> 01:26:08.000]   hot garbage. It's,
[01:26:08.000 --> 01:26:10.000]   like, I've just learned how to talk.
[01:26:10.000 --> 01:26:12.000]   Yes.
[01:26:12.000 --> 01:26:14.000]   But that pattern of speech,
[01:26:14.000 --> 01:26:16.000]   like you point out, is in fact easier
[01:26:16.000 --> 01:26:18.000]   to digest,
[01:26:18.000 --> 01:26:20.000]   or at least it requires less, kind of,
[01:26:20.000 --> 01:26:22.000]   stamina or effort?
[01:26:22.000 --> 01:26:24.000]   No, yeah, and it seems to have an interesting point about this
[01:26:24.000 --> 01:26:26.000]   in Antifragile, I'm vaguely
[01:26:26.000 --> 01:26:28.000]   remembering this, but he makes a point that
[01:26:28.000 --> 01:26:30.000]   sometimes when a
[01:26:30.000 --> 01:26:32.000]   signal is distorted in
[01:26:32.000 --> 01:26:34.000]   some way, it makes it
[01:26:34.000 --> 01:26:36.000]   you retain or absorb
[01:26:36.000 --> 01:26:38.000]   more of it, because you have to go through extra effort
[01:26:38.000 --> 01:26:40.000]   to understand it, which is
[01:26:40.000 --> 01:26:42.000]   a reason, for example, I think his
[01:26:42.000 --> 01:26:44.000]   example was, if,
[01:26:44.000 --> 01:26:46.000]   I don't know, if somebody is, like, speaking
[01:26:46.000 --> 01:26:48.000]   but they're, like, far away or something
[01:26:48.000 --> 01:26:50.000]   so their audio is muted, you
[01:26:50.000 --> 01:26:52.000]   have to, like, apply more concentration, which makes it
[01:26:52.000 --> 01:26:54.000]   which makes it, actually,
[01:26:54.000 --> 01:26:56.000]   which means you retain more of their content.
[01:26:56.000 --> 01:26:58.000]   So if you, like, overlay what someone says with a bit
[01:26:58.000 --> 01:27:00.000]   of noise, or you turn on the volume,
[01:27:00.000 --> 01:27:02.000]   very often people have, like, better
[01:27:02.000 --> 01:27:04.000]   comprehension of it, because of the thing you just said,
[01:27:04.000 --> 01:27:06.000]   which is, like, you're paying more attention.
[01:27:06.000 --> 01:27:08.000]   Also, I think, maybe I was misremembering the
[01:27:08.000 --> 01:27:10.000]   thing I mentioned earlier, or maybe it's a different thing,
[01:27:10.000 --> 01:27:12.000]   which is, you can, like, take
[01:27:12.000 --> 01:27:14.000]   perfect speech,
[01:27:14.000 --> 01:27:16.000]   like, recordings, and then you can, like,
[01:27:16.000 --> 01:27:18.000]   insert ums and ahs, and, like, make it worse,
[01:27:18.000 --> 01:27:20.000]   and then you can do,
[01:27:20.000 --> 01:27:22.000]   like, a comprehension test where people listen to
[01:27:22.000 --> 01:27:24.000]   the different versions and, like, can remember it, and they do
[01:27:24.000 --> 01:27:26.000]   better with the versions which are, like, less perfect.
[01:27:26.000 --> 01:27:28.000]   Is it just about having more
[01:27:28.000 --> 01:27:30.000]   space between words, like,
[01:27:30.000 --> 01:27:32.000]   or is it actually the um, like, if you just added
[01:27:32.000 --> 01:27:34.000]   space instead of ums, would that have the same effect,
[01:27:34.000 --> 01:27:36.000]   or is that something specific about
[01:27:36.000 --> 01:27:38.000]   And there's a limit to how much I can stretch
[01:27:38.000 --> 01:27:40.000]   a second year's psychology course.
[01:27:40.000 --> 01:27:42.000]   Um is some
[01:27:42.000 --> 01:27:44.000]   global consonant of
[01:27:44.000 --> 01:27:46.000]   just, like, it's, like, ohm, or something, it, like,
[01:27:46.000 --> 01:27:48.000]   evokes, it evokes, like, absolute
[01:27:48.000 --> 01:27:50.000]   concentration.
[01:27:50.000 --> 01:27:52.000]   I'm curious to ask you, like,
[01:27:52.000 --> 01:27:54.000]   I know, I want to know
[01:27:54.000 --> 01:27:56.000]   what you feel like you've learned
[01:27:56.000 --> 01:27:58.000]   during podcasting, so I don't know,
[01:27:58.000 --> 01:28:00.000]   maybe one question here is, like,
[01:28:00.000 --> 01:28:02.000]   what's some kind of
[01:28:02.000 --> 01:28:04.000]   under-appreciated difficulty of
[01:28:04.000 --> 01:28:06.000]   trying to ask good questions?
[01:28:06.000 --> 01:28:08.000]   I mean, like, obviously you are currently asking
[01:28:08.000 --> 01:28:10.000]   excellent questions, so what have you learned?
[01:28:10.000 --> 01:28:12.000]   That, um,
[01:28:12.000 --> 01:28:14.000]   one thing you, um,
[01:28:14.000 --> 01:28:16.000]   I think I've heard this advice that you want to do something
[01:28:16.000 --> 01:28:18.000]   where a thing that seems
[01:28:18.000 --> 01:28:20.000]   easier to you is difficult
[01:28:20.000 --> 01:28:22.000]   for other people. Like, I have
[01:28:22.000 --> 01:28:24.000]   tried, okay, so one obvious thing you
[01:28:24.000 --> 01:28:26.000]   can do is, like, ask on Twitter, "Hey, I'm interviewing this
[01:28:26.000 --> 01:28:28.000]   person, what should I ask them?" And you'll observe
[01:28:28.000 --> 01:28:30.000]   that all the, like, all the questions that people
[01:28:30.000 --> 01:28:32.000]   will, like, propose are, like, terrible.
[01:28:32.000 --> 01:28:34.000]   Um, and, um, so, but
[01:28:34.000 --> 01:28:36.000]   maybe it's just, like, oh, yeah, there's
[01:28:36.000 --> 01:28:38.000]   adverse selection, the people who actually could come up
[01:28:38.000 --> 01:28:40.000]   with good questions are not going to spend the time to, like, reply to your
[01:28:40.000 --> 01:28:42.000]   tweet. Um, but then
[01:28:42.000 --> 01:28:44.000]   I've even, like, um,
[01:28:44.000 --> 01:28:46.000]   hopefully they're not listening, but I've
[01:28:46.000 --> 01:28:48.000]   even, like, tried to, like, hire,
[01:28:48.000 --> 01:28:50.000]   like, I don't know, research partners or research
[01:28:50.000 --> 01:28:52.000]   assistants who can help me come up with questions more recently.
[01:28:52.000 --> 01:28:54.000]   And the questions they come up with also
[01:28:54.000 --> 01:28:56.000]   seem, um, like, it just
[01:28:56.000 --> 01:28:58.000]   left, like, uh, how did growing up in the
[01:28:58.000 --> 01:29:00.000]   Midwest, like, change your views about
[01:29:00.000 --> 01:29:02.000]   blah, blah, blah. It just, like,
[01:29:02.000 --> 01:29:04.000]   it's just a question that's, uh, whose
[01:29:04.000 --> 01:29:06.000]   answer is not interesting.
[01:29:06.000 --> 01:29:08.000]   It's not a question you would organically have if you,
[01:29:08.000 --> 01:29:10.000]   at least I hope you wouldn't
[01:29:10.000 --> 01:29:12.000]   have organically, I wanted to ask them
[01:29:12.000 --> 01:29:14.000]   if you were only talking to them one-on-one.
[01:29:14.000 --> 01:29:16.000]   So, um, it does
[01:29:16.000 --> 01:29:18.000]   seem like the skill is, um, harder
[01:29:18.000 --> 01:29:20.000]   than I would have, uh,
[01:29:20.000 --> 01:29:22.000]   it's rarer than I would have expected.
[01:29:22.000 --> 01:29:24.000]   I don't know why. I don't know if you have a good sense of, because you
[01:29:24.000 --> 01:29:26.000]   have an excellent podcast where you ask good questions.
[01:29:26.000 --> 01:29:28.000]   Uh, I don't know, what do you
[01:29:28.000 --> 01:29:30.000]   think? Have you observed this where
[01:29:30.000 --> 01:29:32.000]   the asking good questions is
[01:29:32.000 --> 01:29:34.000]   a rarer skill than you might think?
[01:29:34.000 --> 01:29:36.000]   I've observed that it's a really hard skill. I still feel
[01:29:36.000 --> 01:29:38.000]   like, kind of,
[01:29:38.000 --> 01:29:40.000]   I still feel like it's really difficult.
[01:29:40.000 --> 01:29:42.000]   I also at least
[01:29:42.000 --> 01:29:44.000]   like to think that we've got a bit better.
[01:29:44.000 --> 01:29:46.000]   First thing I thought there was
[01:29:46.000 --> 01:29:48.000]   the example you gave of, like, what was it like
[01:29:48.000 --> 01:29:50.000]   growing up in the Midwest? We always ask those
[01:29:50.000 --> 01:29:52.000]   kinds of questions. So, you know, like, how did you get
[01:29:52.000 --> 01:29:54.000]   into behavioral economics and
[01:29:54.000 --> 01:29:56.000]   why do you think it was so important?
[01:29:56.000 --> 01:29:58.000]   These are just, like, guaranteed to be, kind
[01:29:58.000 --> 01:30:00.000]   of, uninspiring answers. So
[01:30:00.000 --> 01:30:02.000]   specificity seems like a really
[01:30:02.000 --> 01:30:04.000]   good, like, kind of-
[01:30:04.000 --> 01:30:06.000]   What is your book about?
[01:30:06.000 --> 01:30:08.000]   Yeah, exactly. Um,
[01:30:08.000 --> 01:30:10.000]   yeah, tell us about yourself.
[01:30:10.000 --> 01:30:12.000]   Um, this is why I love
[01:30:12.000 --> 01:30:14.000]   conversations with Tyler. It's one of the many reasons I love it.
[01:30:14.000 --> 01:30:16.000]   He'll just, like, launch with,
[01:30:16.000 --> 01:30:18.000]   you know, like, the first question will be, like, about
[01:30:18.000 --> 01:30:20.000]   some footnote in this person's,
[01:30:20.000 --> 01:30:22.000]   like, undergrad dissertation
[01:30:22.000 --> 01:30:24.000]   and that just sets the tone so well.
[01:30:24.000 --> 01:30:26.000]   Um, also I think cutting
[01:30:26.000 --> 01:30:28.000]   off, which I've
[01:30:28.000 --> 01:30:30.000]   made very difficult for you, I guess. Cutting off
[01:30:30.000 --> 01:30:32.000]   answers is one of the interesting things that's been
[01:30:32.000 --> 01:30:34.000]   said and the elaboration
[01:30:34.000 --> 01:30:36.000]   or, like, the
[01:30:36.000 --> 01:30:38.000]   caveats on
[01:30:38.000 --> 01:30:40.000]   the, like, meat of the
[01:30:40.000 --> 01:30:42.000]   answer are often just, like,
[01:30:42.000 --> 01:30:44.000]   way less worth hearing.
[01:30:44.000 --> 01:30:46.000]   I think trying to ask questions which
[01:30:46.000 --> 01:30:48.000]   a person has no hope of knowing the answer to,
[01:30:48.000 --> 01:30:50.000]   even though it'd be great if they knew the answer to, like,
[01:30:50.000 --> 01:30:52.000]   "So what should we do about this policy?"
[01:30:52.000 --> 01:30:54.000]   is a pretty bad
[01:30:54.000 --> 01:30:56.000]   move. Also,
[01:30:56.000 --> 01:30:58.000]   um, if you
[01:30:58.000 --> 01:31:00.000]   speak to people
[01:31:00.000 --> 01:31:02.000]   who are, like, familiar with asking
[01:31:02.000 --> 01:31:04.000]   questions about, like, their book, for instance,
[01:31:04.000 --> 01:31:06.000]   in some sense you need to, like, flush out the, kind
[01:31:06.000 --> 01:31:08.000]   of, pre-prepared, like, spiel that they have
[01:31:08.000 --> 01:31:10.000]   in their heads. Um,
[01:31:10.000 --> 01:31:12.000]   like, I don't know, you could even just do this, like, before
[01:31:12.000 --> 01:31:14.000]   the interview, right? And then, like, it gets to the
[01:31:14.000 --> 01:31:16.000]   good stuff where they're actually being made to
[01:31:16.000 --> 01:31:18.000]   think about things. Rob Wiblin has a really
[01:31:18.000 --> 01:31:20.000]   good, um, like, list of
[01:31:20.000 --> 01:31:22.000]   interview tips, which I think, I don't know,
[01:31:22.000 --> 01:31:24.000]   I guess a reason this is, kind of,
[01:31:24.000 --> 01:31:26.000]   nice to talk about, other than the fact that it's, like,
[01:31:26.000 --> 01:31:28.000]   good to have some, kind of, like,
[01:31:28.000 --> 01:31:30.000]   inside baseball talk, is that, you
[01:31:30.000 --> 01:31:32.000]   know, like, skills of interviewing feel
[01:31:32.000 --> 01:31:34.000]   pretty transferable to just asking
[01:31:34.000 --> 01:31:36.000]   people good questions, which is, like,
[01:31:36.000 --> 01:31:38.000]   a generally useful skill, um,
[01:31:38.000 --> 01:31:40.000]   hopefully. Um,
[01:31:40.000 --> 01:31:42.000]   so, yeah, I guess I've found that it's, like, really difficult.
[01:31:42.000 --> 01:31:44.000]   I still get pretty frustrated with
[01:31:44.000 --> 01:31:46.000]   how hard it is, but, um,
[01:31:46.000 --> 01:31:48.000]   it's, like, a cool thing to realize that
[01:31:48.000 --> 01:31:50.000]   you are able to, like, kind of, slowly learn.
[01:31:50.000 --> 01:31:52.000]   Yeah, so, okay, so what is, how
[01:31:52.000 --> 01:31:54.000]   do you think about the value you're adding
[01:31:54.000 --> 01:31:56.000]   through your podcast, and then what
[01:31:56.000 --> 01:31:58.000]   advice do you have for somebody who might want to start their own?
[01:31:58.000 --> 01:32:00.000]   Yeah, so,
[01:32:00.000 --> 01:32:02.000]   I know, kind of, one
[01:32:02.000 --> 01:32:04.000]   reason you might think podcasts are really useful
[01:32:04.000 --> 01:32:06.000]   in general is,
[01:32:08.000 --> 01:32:10.000]   I guess the way I think about this
[01:32:10.000 --> 01:32:12.000]   is, like, you can imagine there's a, kind of, just stock
[01:32:12.000 --> 01:32:14.000]   of, like, ideas that seem really important.
[01:32:14.000 --> 01:32:16.000]   Like, if you just have a conversation
[01:32:16.000 --> 01:32:18.000]   with, I don't know, someone who's, like,
[01:32:18.000 --> 01:32:20.000]   researching some cool topic, and they tell you all this cool stuff
[01:32:20.000 --> 01:32:22.000]   that's, like, isn't written up anywhere, and you're, like,
[01:32:22.000 --> 01:32:24.000]   "Oh, my God!" It needs to, like,
[01:32:24.000 --> 01:32:26.000]   kind of, exist in the world.
[01:32:26.000 --> 01:32:28.000]   I think, in many cases, this, like,
[01:32:28.000 --> 01:32:30.000]   stock of important ideas
[01:32:30.000 --> 01:32:32.000]   just grows faster than you're able to, like,
[01:32:32.000 --> 01:32:34.000]   in some sense, pay it down and, like, put it out into the world.
[01:32:34.000 --> 01:32:36.000]   Um, and that's just a bad thing.
[01:32:36.000 --> 01:32:38.000]   So, there's this overhang you want to
[01:32:38.000 --> 01:32:40.000]   fix, and then you can ask this question of,
[01:32:40.000 --> 01:32:42.000]   "Okay, what's the, just, like, the most,
[01:32:42.000 --> 01:32:44.000]   one of the most effective ways to,
[01:32:44.000 --> 01:32:46.000]   like, communicate ideas
[01:32:46.000 --> 01:32:48.000]   relatively well,
[01:32:48.000 --> 01:32:50.000]   put them out into the world?"
[01:32:50.000 --> 01:32:52.000]   Well, I don't know, just, like,
[01:32:52.000 --> 01:32:54.000]   having a conversation with that person
[01:32:54.000 --> 01:32:56.000]   is just, like, one of the most, kind of, efficient
[01:32:56.000 --> 01:32:58.000]   ways of doing it. I think it's, like,
[01:32:58.000 --> 01:33:00.000]   interesting, in general, to consider,
[01:33:00.000 --> 01:33:02.000]   like, the, kind of, rate of information transfer
[01:33:02.000 --> 01:33:04.000]   for different kinds of, like, media
[01:33:04.000 --> 01:33:06.000]   and stuff, like, transmitting and receiving ideas.
[01:33:06.000 --> 01:33:08.000]   So, like, on the, like, best end of the spectrum,
[01:33:08.000 --> 01:33:10.000]   right, I'm sure you've had, kind of, conversations
[01:33:10.000 --> 01:33:12.000]   where you're, everyone you're talking with,
[01:33:12.000 --> 01:33:14.000]   like, shares a lot of context,
[01:33:14.000 --> 01:33:16.000]   and so you can just, kind of, blurt out this, like,
[01:33:16.000 --> 01:33:18.000]   slightly
[01:33:18.000 --> 01:33:20.000]   incoherent three-minute, like, "I just had this, kind of,
[01:33:20.000 --> 01:33:22.000]   thought in the shower," and they can
[01:33:22.000 --> 01:33:24.000]   fill in the gaps and, basically, just, like, get the idea.
[01:33:24.000 --> 01:33:26.000]   And then, at the, kind of, opposite end,
[01:33:26.000 --> 01:33:28.000]   like, maybe you want to write
[01:33:28.000 --> 01:33:30.000]   an article in, like, a, kind of, prestigious
[01:33:30.000 --> 01:33:32.000]   outlet, and so you're, like,
[01:33:32.000 --> 01:33:34.000]   kind of, covering all your bases
[01:33:34.000 --> 01:33:36.000]   and making it, like, really well-written.
[01:33:36.000 --> 01:33:38.000]   And then, just, like, the information per, kind of,
[01:33:38.000 --> 01:33:40.000]   effort is, just, like, so much lower.
[01:33:40.000 --> 01:33:42.000]   And I guess, like, certain kinds of academic papers
[01:33:42.000 --> 01:33:44.000]   are, like, way out on the other side.
[01:33:44.000 --> 01:33:46.000]   So, yeah, just, like, as a way of solving this, kind of,
[01:33:46.000 --> 01:33:48.000]   problem of, there's this overhang of
[01:33:48.000 --> 01:33:50.000]   important ideas, podcasts just seem, like, a really,
[01:33:50.000 --> 01:33:52.000]   kind of, good way to
[01:33:52.000 --> 01:33:54.000]   do that. I guess when you
[01:33:54.000 --> 01:33:56.000]   don't successfully
[01:33:56.000 --> 01:33:58.000]   put ideas out into the world,
[01:33:58.000 --> 01:34:00.000]   you get these little, kind of, like,
[01:34:00.000 --> 01:34:02.000]   clusters or, like,
[01:34:02.000 --> 01:34:04.000]   fogs of, like,
[01:34:04.000 --> 01:34:06.000]   contextual knowledge where everyone knows these ideas
[01:34:06.000 --> 01:34:08.000]   in the right circles, but
[01:34:08.000 --> 01:34:10.000]   they're hard to pick up
[01:34:10.000 --> 01:34:12.000]   from, like, legible sources.
[01:34:12.000 --> 01:34:14.000]   And it's, like, kind of, maps
[01:34:14.000 --> 01:34:16.000]   onto this idea of, like,
[01:34:16.000 --> 01:34:18.000]   context being that thing
[01:34:18.000 --> 01:34:20.000]   which is scarce. I remember, like, Tyler
[01:34:20.000 --> 01:34:22.000]   Cohen talking about that. Like, kind of, like,
[01:34:22.000 --> 01:34:24.000]   eventually made sense in that context.
[01:34:24.000 --> 01:34:26.000]   I will mention that
[01:34:26.000 --> 01:34:28.000]   it seems like, kind of,
[01:34:28.000 --> 01:34:30.000]   the thing you mentioned
[01:34:30.000 --> 01:34:32.000]   about either just
[01:34:32.000 --> 01:34:34.000]   head off on a podcast and explain
[01:34:34.000 --> 01:34:36.000]   your idea or, you know, take the time to
[01:34:36.000 --> 01:34:38.000]   do it in, like, a prestigious place.
[01:34:38.000 --> 01:34:40.000]   It seems very much like a barbell strategy.
[01:34:40.000 --> 01:34:42.000]   Whereas the middle ground
[01:34:42.000 --> 01:34:44.000]   of spending, like, four or five hours
[01:34:44.000 --> 01:34:46.000]   writing a blog post where it's not going to mean
[01:34:46.000 --> 01:34:48.000]   something that's super prestigious, you might as well just, like,
[01:34:48.000 --> 01:34:50.000]   either just put it up in a podcast if it's a thing
[01:34:50.000 --> 01:34:52.000]   you just want to get over with or,
[01:34:52.000 --> 01:34:54.000]   you know, spend some time, spend a little
[01:34:54.000 --> 01:34:56.000]   bit more time getting in a more prestigious place.
[01:34:56.000 --> 01:34:58.000]   The argument against it, I guess, is
[01:34:58.000 --> 01:35:00.000]   that
[01:35:00.000 --> 01:35:02.000]   the idea seems more accessible if it's
[01:35:02.000 --> 01:35:04.000]   in the form of a blog post for,
[01:35:04.000 --> 01:35:06.000]   I don't know, for posterity, if you just
[01:35:06.000 --> 01:35:08.000]   want that to be, like, the
[01:35:08.000 --> 01:35:10.000]   canonical source for something.
[01:35:10.000 --> 01:35:12.000]   But again, if you want it to be the canonical source, you should just
[01:35:12.000 --> 01:35:14.000]   make it a, sort of, like, more
[01:35:14.000 --> 01:35:16.000]   official thing. Because if it's just a YouTube clip,
[01:35:16.000 --> 01:35:18.000]   then it's a little
[01:35:18.000 --> 01:35:20.000]   difficult for people to, like, reference to it.
[01:35:20.000 --> 01:35:22.000]   You can kind of get the best of both worlds, so
[01:35:22.000 --> 01:35:24.000]   you can put your
[01:35:24.000 --> 01:35:26.000]   recording into, like, there are,
[01:35:26.000 --> 01:35:28.000]   you know, the software that transcribes your podcast, right?
[01:35:28.000 --> 01:35:30.000]   You can put it into that. If you're lucky enough to have someone
[01:35:30.000 --> 01:35:32.000]   to help you with this, you can get someone, or you can just do it yourself.
[01:35:32.000 --> 01:35:34.000]   Like, go through the transcript
[01:35:34.000 --> 01:35:36.000]   to make sure it's kind of, there aren't any, like,
[01:35:36.000 --> 01:35:38.000]   glaring mistakes. And now you have this, like,
[01:35:38.000 --> 01:35:40.000]   artifact that is in text form that, like, lives
[01:35:40.000 --> 01:35:42.000]   on the internet. And it's just, like, way cheaper than
[01:35:42.000 --> 01:35:44.000]   writing it in the first place. But yeah, that's
[01:35:44.000 --> 01:35:46.000]   a great point. And also, people should read your
[01:35:46.000 --> 01:35:48.000]   barbells for life. Is that it?
[01:35:48.000 --> 01:35:50.000]   Barbell strategies for life? Yeah, yeah,
[01:35:50.000 --> 01:35:52.000]   that's it. Yeah, cool. Maybe
[01:35:52.000 --> 01:35:54.000]   one last thing that seems
[01:35:54.000 --> 01:35:56.000]   worth saying on this topic
[01:35:56.000 --> 01:35:58.000]   of podcasting is,
[01:35:58.000 --> 01:36:00.000]   like, it's quite easy
[01:36:00.000 --> 01:36:02.000]   to start doing a podcast. And
[01:36:02.000 --> 01:36:04.000]   my guess is
[01:36:04.000 --> 01:36:06.000]   it's often worth at least
[01:36:06.000 --> 01:36:08.000]   trying, right? So, I don't know.
[01:36:08.000 --> 01:36:10.000]   I guess there are probably a few people listening to this who have, like, kind
[01:36:10.000 --> 01:36:12.000]   of entertained the idea. One thing
[01:36:12.000 --> 01:36:14.000]   to say is, it doesn't need to be the case that
[01:36:14.000 --> 01:36:16.000]   if you just, like, stop doing it and it doesn't really pan out
[01:36:16.000 --> 01:36:18.000]   after, like, five episodes, or even fewer,
[01:36:18.000 --> 01:36:20.000]   that it's a failure. Like, you can frame it as,
[01:36:20.000 --> 01:36:22.000]   if you wanted to make, like, a small series
[01:36:22.000 --> 01:36:24.000]   that's just, like, a useful artifact
[01:36:24.000 --> 01:36:26.000]   to have in the world, which is, like, I don't know.
[01:36:26.000 --> 01:36:28.000]   Here's this, kind of,
[01:36:28.000 --> 01:36:30.000]   bit of history that I think is underrated. I'm going to tell the story
[01:36:30.000 --> 01:36:32.000]   in, like, four different hour-long episodes.
[01:36:32.000 --> 01:36:34.000]   If you, like, set out to do that, then you have this, like, self-contained
[01:36:34.000 --> 01:36:36.000]   chunk of work. So, yeah, maybe
[01:36:36.000 --> 01:36:38.000]   that's, like, a useful framing. And there's a bunch of resources, which
[01:36:38.000 --> 01:36:40.000]   I'm sure it might be possible to link to
[01:36:40.000 --> 01:36:42.000]   on just, like, how to set up a podcast. I, like,
[01:36:42.000 --> 01:36:44.000]   tried writing, like, collecting some of those resources.
[01:36:44.000 --> 01:36:46.000]   The thing to emphasize, I think, is
[01:36:46.000 --> 01:36:48.000]   that you... I think I've
[01:36:48.000 --> 01:36:50.000]   talked to, like, at least, I don't know, three or four
[01:36:50.000 --> 01:36:52.000]   people at this point who have told me, like,
[01:36:52.000 --> 01:36:54.000]   "Oh, I have this idea for a podcast.
[01:36:54.000 --> 01:36:56.000]   It's going to be about, you know, like,
[01:36:56.000 --> 01:36:58.000]   architecture. It's going to be about, like, VR
[01:36:58.000 --> 01:37:00.000]   or whatever." They seem like good ideas.
[01:37:00.000 --> 01:37:02.000]   I'm not making up the ideas themselves.
[01:37:02.000 --> 01:37:04.000]   But I just, like, I talked to them, like,
[01:37:04.000 --> 01:37:06.000]   six months later, and it's, like,
[01:37:06.000 --> 01:37:08.000]   they haven't started it yet. And I just
[01:37:08.000 --> 01:37:10.000]   tell them, like, literally just email somebody
[01:37:10.000 --> 01:37:12.000]   right now, whoever you want to be your first guest.
[01:37:12.000 --> 01:37:14.000]   I mean, I cold emailed Brian Kaplan, and he ended up
[01:37:14.000 --> 01:37:16.000]   being my first guest.
[01:37:16.000 --> 01:37:18.000]   Just email them and, like, set something on the calendar.
[01:37:18.000 --> 01:37:20.000]   Because I don't know what it is.
[01:37:20.000 --> 01:37:22.000]   Maybe it's just about life in general. I don't know if it's
[01:37:22.000 --> 01:37:24.000]   specific to podcasting. But the amount of people I've
[01:37:24.000 --> 01:37:26.000]   talked to who have, like, vague plans of starting
[01:37:26.000 --> 01:37:28.000]   a podcast and have nothing
[01:37:28.000 --> 01:37:30.000]   scheduled or, like, no
[01:37:30.000 --> 01:37:32.000]   immediate, like, they... I don't know what
[01:37:32.000 --> 01:37:34.000]   they're expecting, like, some MP3
[01:37:34.000 --> 01:37:36.000]   file to appear on their hard drive on
[01:37:36.000 --> 01:37:38.000]   some fine day.
[01:37:38.000 --> 01:37:40.000]   So, yeah. But, yeah, just do it.
[01:37:40.000 --> 01:37:42.000]   Like, get it on the calendar now.
[01:37:42.000 --> 01:37:44.000]   - Yeah, that seems
[01:37:44.000 --> 01:37:46.000]   good. Also, there's, like, some
[01:37:46.000 --> 01:37:48.000]   way of thinking about this where you could just, like,
[01:37:48.000 --> 01:37:50.000]   if you just write off in advance
[01:37:50.000 --> 01:37:52.000]   that your first, I don't know, let's say seven
[01:37:52.000 --> 01:37:54.000]   episodes are just gonna be, like,
[01:37:54.000 --> 01:37:56.000]   embarrassing to listen to,
[01:37:56.000 --> 01:37:58.000]   that is more freeing
[01:37:58.000 --> 01:38:00.000]   because it probably is the case.
[01:38:00.000 --> 01:38:02.000]   But you, like, need to go through
[01:38:02.000 --> 01:38:04.000]   the, like, the bad episodes
[01:38:04.000 --> 01:38:06.000]   before you start getting good at anything.
[01:38:06.000 --> 01:38:08.000]   I guess it's, like, not even a podcast
[01:38:08.000 --> 01:38:10.000]   point. Yeah, also,
[01:38:10.000 --> 01:38:12.000]   there's... If you're just, like,
[01:38:12.000 --> 01:38:14.000]   brief and polite, there's, like, very little
[01:38:14.000 --> 01:38:16.000]   cost in being ambitious with the people you
[01:38:16.000 --> 01:38:18.000]   reach out to.
[01:38:18.000 --> 01:38:20.000]   So, yeah, just go for it.
[01:38:20.000 --> 01:38:22.000]   - Brad has an interesting... He wrote an interesting
[01:38:22.000 --> 01:38:24.000]   argument about this somewhere where he was pointing out
[01:38:24.000 --> 01:38:26.000]   that, actually, the cost of cold emailing
[01:38:26.000 --> 01:38:28.000]   are much lower if you're, like,
[01:38:28.000 --> 01:38:30.000]   an unknown quantity than if you are, like,
[01:38:30.000 --> 01:38:32.000]   somebody who, like, has somewhat of a reputation
[01:38:32.000 --> 01:38:34.000]   because if you're just nobody,
[01:38:34.000 --> 01:38:36.000]   then they're gonna forget you ever cold emailed them, right?
[01:38:36.000 --> 01:38:38.000]   They're just gonna ignore it in their inbox.
[01:38:38.000 --> 01:38:40.000]   If you ever run into them in the future, they're just, like, not gonna
[01:38:40.000 --> 01:38:42.000]   register... have registered you the first time.
[01:38:42.000 --> 01:38:44.000]   If you're, like, somebody who has, like, somewhat of a reputation,
[01:38:44.000 --> 01:38:46.000]   then there's, like, a mystery of, like, why are we
[01:38:46.000 --> 01:38:48.000]   not getting introduced to somebody
[01:38:48.000 --> 01:38:50.000]   who should know both of us, right?
[01:38:50.000 --> 01:38:52.000]   If you claim to be, I don't know, like, a professor
[01:38:52.000 --> 01:38:54.000]   who wants to start a podcast.
[01:38:54.000 --> 01:38:56.000]   Um...
[01:38:56.000 --> 01:38:58.000]   Yeah, but anyways,
[01:38:58.000 --> 01:39:00.000]   just reinforcing the point that the cost is really low.
[01:39:00.000 --> 01:39:02.000]   All right, cool. Okay, let's talk
[01:39:02.000 --> 01:39:04.000]   about space. Space governance.
[01:39:04.000 --> 01:39:06.000]   So this is an area where you've
[01:39:06.000 --> 01:39:08.000]   been writing about and researching
[01:39:08.000 --> 01:39:10.000]   more recently. Okay, one
[01:39:10.000 --> 01:39:12.000]   concern you
[01:39:12.000 --> 01:39:14.000]   might have is, you know,
[01:39:14.000 --> 01:39:16.000]   Toby Horta has a book about
[01:39:16.000 --> 01:39:18.000]   the precipice, about how we're
[01:39:18.000 --> 01:39:20.000]   in this time of peril
[01:39:20.000 --> 01:39:22.000]   where we have, like, a one in six odds of going extinct
[01:39:22.000 --> 01:39:24.000]   this century. Is there some
[01:39:24.000 --> 01:39:26.000]   reason to think that once we get to space,
[01:39:26.000 --> 01:39:28.000]   this will no longer be
[01:39:28.000 --> 01:39:30.000]   a problem or
[01:39:30.000 --> 01:39:32.000]   the risk of extinction for humanity
[01:39:32.000 --> 01:39:34.000]   go, you know, asymptote to zero?
[01:39:34.000 --> 01:39:36.000]   I think one
[01:39:36.000 --> 01:39:38.000]   point here. So, actually,
[01:39:38.000 --> 01:39:40.000]   maybe it's worth beginning with a kind of, like, naive case for thinking
[01:39:40.000 --> 01:39:42.000]   that, like, spreading through space is just, like,
[01:39:42.000 --> 01:39:44.000]   the ultimate hedge against extinction.
[01:39:44.000 --> 01:39:46.000]   Um, and this is,
[01:39:46.000 --> 01:39:48.000]   you know, you can imagine just, like, duplicating
[01:39:48.000 --> 01:39:50.000]   civilization or at least having, kind of,
[01:39:50.000 --> 01:39:52.000]   civilizational backup-like things, which
[01:39:52.000 --> 01:39:54.000]   are, like, in different places in space.
[01:39:54.000 --> 01:39:56.000]   If the risk of any one of them,
[01:39:56.000 --> 01:39:58.000]   like, being hit by
[01:39:58.000 --> 01:40:00.000]   an asteroid or, like, otherwise
[01:40:00.000 --> 01:40:02.000]   encountering some existential catastrophe,
[01:40:02.000 --> 01:40:04.000]   if those risks are independent,
[01:40:04.000 --> 01:40:06.000]   then you get this, like,
[01:40:06.000 --> 01:40:08.000]   it's exponent, it's, like, a power law
[01:40:08.000 --> 01:40:10.000]   where every new, um,
[01:40:10.000 --> 01:40:12.000]   backup, right, it's, like,
[01:40:12.000 --> 01:40:14.000]   it's, like, having multiple, kind of,
[01:40:14.000 --> 01:40:16.000]   backups of some data in different places
[01:40:16.000 --> 01:40:18.000]   in the world, right? So, if those risks are
[01:40:18.000 --> 01:40:20.000]   independent, then it is, in fact, the case that, like,
[01:40:20.000 --> 01:40:22.000]   going to space is just, like, incredibly good strategy.
[01:40:22.000 --> 01:40:24.000]   I think there are pretty compelling reasons
[01:40:24.000 --> 01:40:26.000]   to think that a lot of the most worrying risks
[01:40:26.000 --> 01:40:28.000]   are, like, really not
[01:40:28.000 --> 01:40:30.000]   independent at all. Um,
[01:40:30.000 --> 01:40:32.000]   so, uh,
[01:40:32.000 --> 01:40:34.000]   one example is,
[01:40:34.000 --> 01:40:36.000]   you can imagine very dangerous pathogens.
[01:40:36.000 --> 01:40:38.000]   If there's any travel between these places,
[01:40:38.000 --> 01:40:40.000]   then the pathogens are going to travel.
[01:40:40.000 --> 01:40:42.000]   But, like, maybe the more pertinent
[01:40:42.000 --> 01:40:44.000]   example is
[01:40:44.000 --> 01:40:46.000]   if you
[01:40:46.000 --> 01:40:48.000]   think it's worth being worried about
[01:40:48.000 --> 01:40:50.000]   artificial general intelligence that is,
[01:40:50.000 --> 01:40:52.000]   like, unaligned, that goes wrong,
[01:40:52.000 --> 01:40:54.000]   and, like, really relentlessly pursues
[01:40:54.000 --> 01:40:56.000]   really terrible goals, then
[01:40:56.000 --> 01:40:58.000]   just having some, like,
[01:40:58.000 --> 01:41:00.000]   some just physical space between
[01:41:00.000 --> 01:41:02.000]   places is really not
[01:41:02.000 --> 01:41:04.000]   going to work as a real,
[01:41:04.000 --> 01:41:06.000]   kind of, hedge.
[01:41:06.000 --> 01:41:08.000]   So, I'd say something like, you know, space seems, kind of,
[01:41:08.000 --> 01:41:10.000]   it's even that useful to, like,
[01:41:10.000 --> 01:41:12.000]   diversify, go to different places, but,
[01:41:12.000 --> 01:41:14.000]   like, absolutely not sufficient for,
[01:41:14.000 --> 01:41:16.000]   like, getting through this, kind of,
[01:41:16.000 --> 01:41:18.000]   time of perils. Then, yeah, I guess
[01:41:18.000 --> 01:41:20.000]   there's, kind of, this follow-up question, which is, like,
[01:41:20.000 --> 01:41:22.000]   okay, well, why expect that
[01:41:22.000 --> 01:41:24.000]   there is any hope of, like, getting the
[01:41:24.000 --> 01:41:26.000]   risk down to sustainable
[01:41:26.000 --> 01:41:28.000]   levels? If you're sympathetic
[01:41:28.000 --> 01:41:30.000]   to the possibility of,
[01:41:30.000 --> 01:41:32.000]   like, really just transformative
[01:41:32.000 --> 01:41:34.000]   artificial general intelligence, like,
[01:41:34.000 --> 01:41:36.000]   arriving, you might think that, in some
[01:41:36.000 --> 01:41:38.000]   sense, getting that transition
[01:41:38.000 --> 01:41:40.000]   right, where the
[01:41:40.000 --> 01:41:42.000]   outcome is that now you have this thing on your
[01:41:42.000 --> 01:41:44.000]   side, which, like, has your interests
[01:41:44.000 --> 01:41:46.000]   in mind, or has, like, good values in
[01:41:46.000 --> 01:41:48.000]   mind, but has this, like, general
[01:41:48.000 --> 01:41:50.000]   purpose, kind of, reasoning
[01:41:50.000 --> 01:41:52.000]   capability, that, in some
[01:41:52.000 --> 01:41:54.000]   sense, this just, like, tilts you towards
[01:41:54.000 --> 01:41:56.000]   being safe, just, like, indefinitely
[01:41:56.000 --> 01:41:58.000]   long. And one reason is
[01:41:58.000 --> 01:42:00.000]   if bad things pop up, like,
[01:42:00.000 --> 01:42:02.000]   some unaligned thing, then
[01:42:02.000 --> 01:42:04.000]   you have this much better established, safe
[01:42:04.000 --> 01:42:06.000]   unaligned thing, which
[01:42:06.000 --> 01:42:08.000]   has this, kind of, defensive advantage.
[01:42:08.000 --> 01:42:10.000]   So that's one consideration.
[01:42:10.000 --> 01:42:12.000]   And then, if you're, like, less
[01:42:12.000 --> 01:42:14.000]   sympathetic to this
[01:42:14.000 --> 01:42:16.000]   AI story, then I think you
[01:42:16.000 --> 01:42:18.000]   could also just tell a story
[01:42:18.000 --> 01:42:20.000]   about, like, being
[01:42:20.000 --> 01:42:22.000]   optimistic for
[01:42:22.000 --> 01:42:24.000]   our, kind of, capacity to,
[01:42:24.000 --> 01:42:26.000]   like, catch up
[01:42:26.000 --> 01:42:28.000]   along some kind of wisdom or coordination
[01:42:28.000 --> 01:42:30.000]   dimension. If you, like, really zoom
[01:42:30.000 --> 01:42:32.000]   out and look at how, like, quickly we just
[01:42:32.000 --> 01:42:34.000]   invented all this, like, kind of
[01:42:34.000 --> 01:42:36.000]   insane technology, that is, like, a roughly,
[01:42:36.000 --> 01:42:38.000]   kind of, exponential process.
[01:42:38.000 --> 01:42:40.000]   You might think that that
[01:42:40.000 --> 01:42:42.000]   might, kind of, eventually, like,
[01:42:42.000 --> 01:42:44.000]   slow down, but
[01:42:44.000 --> 01:42:46.000]   our, like, improvements and just how
[01:42:46.000 --> 01:42:48.000]   well we're able to coordinate
[01:42:48.000 --> 01:42:50.000]   ourselves, like, continues
[01:42:50.000 --> 01:42:52.000]   to increase, and so that you get this, like,
[01:42:52.000 --> 01:42:54.000]   defensive advantage in the long run.
[01:42:54.000 --> 01:42:56.000]   Those are two pretty weak arguments, so I think it's, like, actually
[01:42:56.000 --> 01:42:58.000]   just a very good question to think about.
[01:42:58.000 --> 01:43:00.000]   And I, like, you know, also, kind of, acknowledge
[01:43:00.000 --> 01:43:02.000]   it's, like, not a very, kind of, compelling
[01:43:02.000 --> 01:43:04.000]   answer. I'm wondering
[01:43:04.000 --> 01:43:06.000]   if there are
[01:43:06.000 --> 01:43:08.000]   aspects that
[01:43:08.000 --> 01:43:10.000]   you can discern
[01:43:10.000 --> 01:43:12.000]   from first principles about the safety
[01:43:12.000 --> 01:43:14.000]   of space, which
[01:43:14.000 --> 01:43:16.000]   suggests that either, I don't know,
[01:43:16.000 --> 01:43:18.000]   either there's no good reason to think
[01:43:18.000 --> 01:43:20.000]   the time of perils ever ends.
[01:43:20.000 --> 01:43:22.000]   I mean, because the thing about AI is, like,
[01:43:22.000 --> 01:43:24.000]   that's true whether you go to space or not, right?
[01:43:24.000 --> 01:43:26.000]   Like, if it's aligned, then
[01:43:26.000 --> 01:43:28.000]   I guess it can indefinitely reduce existential
[01:43:28.000 --> 01:43:30.000]   risk. I mean, one thought you can have is
[01:43:30.000 --> 01:43:32.000]   maybe, I don't know,
[01:43:32.000 --> 01:43:34.000]   contra the long reflection thing
[01:43:34.000 --> 01:43:36.000]   we're talking about, which is that
[01:43:36.000 --> 01:43:38.000]   if you think that one of the bottlenecks to
[01:43:38.000 --> 01:43:40.000]   a great future could be,
[01:43:40.000 --> 01:43:42.000]   I don't know, like, some sort of tyrannical
[01:43:42.000 --> 01:43:44.000]   – tyrannical
[01:43:44.000 --> 01:43:46.000]   is, like, kind of a coded term in terms of, like, conventional
[01:43:46.000 --> 01:43:48.000]   political thought, but you know what I mean.
[01:43:48.000 --> 01:43:50.000]   The diversity of political models
[01:43:50.000 --> 01:43:52.000]   that being spread out would have,
[01:43:52.000 --> 01:43:54.000]   maybe that's a positive thing.
[01:43:54.000 --> 01:43:56.000]   On the other hand, Guern
[01:43:56.000 --> 01:43:58.000]   has this interesting blog post about
[01:43:58.000 --> 01:44:00.000]   space wars where he
[01:44:00.000 --> 01:44:02.000]   points out that the logic
[01:44:02.000 --> 01:44:04.000]   of mutually assured destruction
[01:44:04.000 --> 01:44:06.000]   goes away in space, so maybe we should expect more conflict
[01:44:06.000 --> 01:44:08.000]   because it's
[01:44:08.000 --> 01:44:10.000]   hard to identify who the culprit is
[01:44:10.000 --> 01:44:12.000]   if, like, an asteroid was redirected to your planet
[01:44:12.000 --> 01:44:14.000]   and, you know, if they can sweep it up
[01:44:14.000 --> 01:44:16.000]   sufficiently fast, they can, like, basically destroy your
[01:44:16.000 --> 01:44:18.000]   above-ground civilization.
[01:44:18.000 --> 01:44:20.000]   Yeah, so, I mean,
[01:44:20.000 --> 01:44:22.000]   is there something we can
[01:44:22.000 --> 01:44:24.000]   discern from first principles about how violent
[01:44:24.000 --> 01:44:26.000]   and how,
[01:44:26.000 --> 01:44:28.000]   I don't know, how
[01:44:28.000 --> 01:44:30.000]   pleasant time
[01:44:30.000 --> 01:44:32.000]   and space will be?
[01:44:32.000 --> 01:44:34.000]   Yeah, it's a really good question. I will say that I think
[01:44:34.000 --> 01:44:36.000]   I have not, like,
[01:44:36.000 --> 01:44:38.000]   reflected on that question enough to, like, give a really
[01:44:38.000 --> 01:44:40.000]   authoritative answer.
[01:44:40.000 --> 01:44:42.000]   Incidentally, one person who absolutely has
[01:44:42.000 --> 01:44:44.000]   is Anders Sandberg
[01:44:44.000 --> 01:44:46.000]   who has been thinking
[01:44:46.000 --> 01:44:48.000]   about almost exactly these questions for a very
[01:44:48.000 --> 01:44:50.000]   long time and in some
[01:44:50.000 --> 01:44:52.000]   point of the future might have a book about
[01:44:52.000 --> 01:44:54.000]   this, so
[01:44:54.000 --> 01:44:56.000]   watch that space.
[01:44:56.000 --> 01:44:58.000]   One consideration is that you can
[01:44:58.000 --> 01:45:00.000]   start at the end.
[01:45:00.000 --> 01:45:02.000]   You can consider what
[01:45:02.000 --> 01:45:04.000]   happens very far out in the future
[01:45:04.000 --> 01:45:06.000]   and it turns out that because
[01:45:06.000 --> 01:45:08.000]   the universe is expanding
[01:45:08.000 --> 01:45:10.000]   for any
[01:45:10.000 --> 01:45:12.000]   just, like, point in space, so if you, like, consider
[01:45:12.000 --> 01:45:14.000]   the next, like, cluster over
[01:45:14.000 --> 01:45:16.000]   or maybe even the next galaxy over,
[01:45:16.000 --> 01:45:18.000]   there'll be a time in the future
[01:45:18.000 --> 01:45:20.000]   where it's impossible to reach that
[01:45:20.000 --> 01:45:22.000]   other point in space
[01:45:22.000 --> 01:45:24.000]   no matter how long you have to get there.
[01:45:24.000 --> 01:45:26.000]   So even if you sent out, like, a signal
[01:45:26.000 --> 01:45:28.000]   in the form of light, it would never reach there because
[01:45:28.000 --> 01:45:30.000]   there'll always be a time in the future where
[01:45:30.000 --> 01:45:32.000]   you start expanding faster than the speed of light
[01:45:32.000 --> 01:45:34.000]   relative to that other place. So, okay,
[01:45:34.000 --> 01:45:36.000]   there's a small consolation there, which is if you last
[01:45:36.000 --> 01:45:38.000]   long enough to get to this, kind of,
[01:45:38.000 --> 01:45:40.000]   era of isolation, then
[01:45:40.000 --> 01:45:42.000]   suddenly you become independent again
[01:45:42.000 --> 01:45:44.000]   in the, like, strict sense.
[01:45:44.000 --> 01:45:46.000]   I don't think that's especially relevant when
[01:45:46.000 --> 01:45:48.000]   we're considering, kind of, more, I guess,
[01:45:48.000 --> 01:45:50.000]   I guess, relatively speaking, neuroterm
[01:45:50.000 --> 01:45:52.000]   things. Gwen's point is really nice, so
[01:45:52.000 --> 01:45:54.000]   Gwen starts by pointing out that we have this,
[01:45:54.000 --> 01:45:56.000]   like, logic with nuclear weapons
[01:45:56.000 --> 01:45:58.000]   on Earth or mutually assorted
[01:45:58.000 --> 01:46:00.000]   destruction where the emphasis is on
[01:46:00.000 --> 01:46:02.000]   a second strike, so if
[01:46:02.000 --> 01:46:04.000]   I receive a first strike
[01:46:04.000 --> 01:46:06.000]   from someone else, I can identify
[01:46:06.000 --> 01:46:08.000]   the someone else that first strike came from
[01:46:08.000 --> 01:46:10.000]   and I can, kind of, like, credibly commit
[01:46:10.000 --> 01:46:12.000]   to retaliating
[01:46:12.000 --> 01:46:14.000]   and the thought is that this, like,
[01:46:14.000 --> 01:46:16.000]   disincentivizes that person
[01:46:16.000 --> 01:46:18.000]   from launching the first strike in the first place
[01:46:18.000 --> 01:46:20.000]   which, like, makes a ton of sense.
[01:46:20.000 --> 01:46:22.000]   Gwen's point, I guess, the thing you
[01:46:22.000 --> 01:46:24.000]   already mentioned is in
[01:46:24.000 --> 01:46:26.000]   space, there are reasons for thinking it's going to be
[01:46:26.000 --> 01:46:28.000]   much harder to, like, attribute
[01:46:28.000 --> 01:46:30.000]   where a strike came from.
[01:46:30.000 --> 01:46:32.000]   That means that you don't have, like,
[01:46:32.000 --> 01:46:34.000]   any, kind of, credible
[01:46:34.000 --> 01:46:36.000]   way to threaten a
[01:46:36.000 --> 01:46:38.000]   retaliation
[01:46:38.000 --> 01:46:40.000]   and so mutually assured destruction doesn't
[01:46:40.000 --> 01:46:42.000]   work and that's, kind of, like, actually
[01:46:42.000 --> 01:46:44.000]   a bit of an uncomfortable thought because
[01:46:44.000 --> 01:46:46.000]   the alternative to mutually assured destruction
[01:46:46.000 --> 01:46:48.000]   in some sense is just
[01:46:48.000 --> 01:46:50.000]   first strike, which is, if you're
[01:46:50.000 --> 01:46:52.000]   worried about some other actor
[01:46:52.000 --> 01:46:54.000]   being powerful enough to destroy you,
[01:46:54.000 --> 01:46:56.000]   then you should destroy their capacity to destroy you.
[01:46:56.000 --> 01:46:58.000]   So, yeah,
[01:46:58.000 --> 01:47:00.000]   it's a slightly bleak
[01:47:00.000 --> 01:47:02.000]   blog post. I think there are, like, a ton of other
[01:47:02.000 --> 01:47:04.000]   considerations, some of which are a bit more hopeful.
[01:47:04.000 --> 01:47:06.000]   One is that you might imagine there's, in general,
[01:47:06.000 --> 01:47:08.000]   a, kind of, like, defensive
[01:47:08.000 --> 01:47:10.000]   advantage in space over offensive.
[01:47:10.000 --> 01:47:12.000]   One reason is that space is
[01:47:12.000 --> 01:47:14.000]   this, like,
[01:47:14.000 --> 01:47:16.000]   dark canvas
[01:47:16.000 --> 01:47:18.000]   in 3D where there's absolutely nowhere to hide
[01:47:18.000 --> 01:47:20.000]   and so
[01:47:20.000 --> 01:47:22.000]   you can't sneak up on anyone.
[01:47:22.000 --> 01:47:24.000]   But, yeah, I think there's, like,
[01:47:24.000 --> 01:47:26.000]   a lot of stuff to say here
[01:47:26.000 --> 01:47:28.000]   and a lot of it I don't quite fully understand yet.
[01:47:28.000 --> 01:47:30.000]   But I guess that makes it
[01:47:30.000 --> 01:47:32.000]   an interesting
[01:47:32.000 --> 01:47:34.000]   and important subject to be
[01:47:34.000 --> 01:47:36.000]   studying if we
[01:47:36.000 --> 01:47:38.000]   don't know that much about how it's going to turn out.
[01:47:38.000 --> 01:47:40.000]   So, von Neumann has
[01:47:40.000 --> 01:47:42.000]   this vision that you would
[01:47:42.000 --> 01:47:44.000]   set up a, sort of, virus-like probe
[01:47:44.000 --> 01:47:46.000]   that infests a planet
[01:47:46.000 --> 01:47:48.000]   and uses up its usable
[01:47:48.000 --> 01:47:50.000]   resources to build more
[01:47:50.000 --> 01:47:52.000]   probes which go on and infect
[01:47:52.000 --> 01:47:54.000]   more planets.
[01:47:54.000 --> 01:47:56.000]   Is the long-run future of the universe
[01:47:56.000 --> 01:47:58.000]   that all
[01:47:58.000 --> 01:48:00.000]   the available low-hanging
[01:48:00.000 --> 01:48:02.000]   fruit resources are burnt up
[01:48:02.000 --> 01:48:04.000]   in, you know,
[01:48:04.000 --> 01:48:06.000]   some sort of, like, fire, you know, expanding
[01:48:06.000 --> 01:48:08.000]   fire of von Neumann probes?
[01:48:08.000 --> 01:48:10.000]   Because it seems like as long as one
[01:48:10.000 --> 01:48:12.000]   person decides that this is something they
[01:48:12.000 --> 01:48:14.000]   want to do, then,
[01:48:14.000 --> 01:48:16.000]   you know, yeah, the
[01:48:16.000 --> 01:48:18.000]   low-hanging fruit in terms of
[01:48:18.000 --> 01:48:20.000]   spreading out will just be
[01:48:20.000 --> 01:48:22.000]   burned up by somebody who
[01:48:22.000 --> 01:48:24.000]   built something like this. Yeah, that's a really
[01:48:24.000 --> 01:48:26.000]   good question.
[01:48:26.000 --> 01:48:28.000]   So, okay, maybe there's, like,
[01:48:28.000 --> 01:48:30.000]   this idea where we have on Earth,
[01:48:30.000 --> 01:48:32.000]   we have organisms which can, like,
[01:48:32.000 --> 01:48:34.000]   convert raw resources
[01:48:34.000 --> 01:48:36.000]   plus sunlight
[01:48:36.000 --> 01:48:38.000]   into more of them, and they
[01:48:38.000 --> 01:48:40.000]   replicate. It's notable
[01:48:40.000 --> 01:48:42.000]   that they don't, like,
[01:48:42.000 --> 01:48:44.000]   blanket the Earth, although
[01:48:44.000 --> 01:48:46.000]   I do, just as a tangent,
[01:48:46.000 --> 01:48:48.000]   I remember someone
[01:48:48.000 --> 01:48:50.000]   mentioned
[01:48:50.000 --> 01:48:52.000]   there's this thought of, like, an alien, you know,
[01:48:52.000 --> 01:48:54.000]   arrived on Earth and asked the question, "What is the most
[01:48:54.000 --> 01:48:56.000]   successful species?" It would probably be grass.
[01:48:56.000 --> 01:48:58.000]   But, again, the reason that, like,
[01:48:58.000 --> 01:49:00.000]   particular organisms
[01:49:00.000 --> 01:49:02.000]   that just reproduce using sunlight
[01:49:02.000 --> 01:49:04.000]   don't just kind of
[01:49:04.000 --> 01:49:06.000]   have this, like, green goo
[01:49:06.000 --> 01:49:08.000]   dynamic is because there are
[01:49:08.000 --> 01:49:10.000]   competing organisms,
[01:49:10.000 --> 01:49:12.000]   there are things like, you know, antivirals and so on.
[01:49:12.000 --> 01:49:14.000]   So, I guess,
[01:49:14.000 --> 01:49:16.000]   like you mentioned, as long as
[01:49:16.000 --> 01:49:18.000]   if, as soon as this thing gets seeded,
[01:49:18.000 --> 01:49:20.000]   it's game over,
[01:49:20.000 --> 01:49:22.000]   you can imagine trying to catch up
[01:49:22.000 --> 01:49:24.000]   with these things and stop them.
[01:49:24.000 --> 01:49:26.000]   And, I don't know, what's the equilibrium
[01:49:26.000 --> 01:49:28.000]   here, where you have things that are trying to catch things
[01:49:28.000 --> 01:49:30.000]   and things which are also spreading? It's, like, pretty unclear,
[01:49:30.000 --> 01:49:32.000]   but it's not clear that it's, everything
[01:49:32.000 --> 01:49:34.000]   gets burned down.
[01:49:34.000 --> 01:49:36.000]   Although, I know it seems, like, worth
[01:49:36.000 --> 01:49:38.000]   having on the table as
[01:49:38.000 --> 01:49:40.000]   a possible outcome.
[01:49:40.000 --> 01:49:42.000]   And then, another thought is,
[01:49:42.000 --> 01:49:44.000]   I guess, something you also, like, basically
[01:49:44.000 --> 01:49:46.000]   mentioned, Robin Hansen
[01:49:46.000 --> 01:49:48.000]   has this paper called,
[01:49:48.000 --> 01:49:50.000]   I think, Burning the Cosmic Commons.
[01:49:50.000 --> 01:49:52.000]   I think the
[01:49:52.000 --> 01:49:54.000]   things he says are, like, a little bit
[01:49:54.000 --> 01:49:56.000]   subtle, but I guess, to, kind of,
[01:49:56.000 --> 01:49:58.000]   like, bastardise the overall point,
[01:49:58.000 --> 01:50:00.000]   there's an idea that you should
[01:50:00.000 --> 01:50:02.000]   expect, kind of,
[01:50:02.000 --> 01:50:04.000]   selection effects on
[01:50:04.000 --> 01:50:06.000]   what you observe in the
[01:50:06.000 --> 01:50:08.000]   long run of, like, which kinds of
[01:50:08.000 --> 01:50:10.000]   things have won out in this, kind of, like, race for
[01:50:10.000 --> 01:50:12.000]   different parts of space. And
[01:50:12.000 --> 01:50:14.000]   in particular, the things you
[01:50:14.000 --> 01:50:16.000]   should expect to win out are these things
[01:50:16.000 --> 01:50:18.000]   which, like, burn resources very fast
[01:50:18.000 --> 01:50:20.000]   and are, like, greedy in terms of
[01:50:20.000 --> 01:50:22.000]   grabbing as much space
[01:50:22.000 --> 01:50:24.000]   as possible. And,
[01:50:24.000 --> 01:50:26.000]   I don't know, that seems, like,
[01:50:26.000 --> 01:50:28.000]   roughly correct. He also has a more recent
[01:50:28.000 --> 01:50:30.000]   bit of work called Grabby
[01:50:30.000 --> 01:50:32.000]   Aliens. I think there's a website, grabbyaliens.com,
[01:50:32.000 --> 01:50:34.000]   which, kind of, expands on this
[01:50:34.000 --> 01:50:36.000]   point and asks this question about what, you know,
[01:50:36.000 --> 01:50:38.000]   we should expect to see such,
[01:50:38.000 --> 01:50:40.000]   kind of, yeah,
[01:50:40.000 --> 01:50:42.000]   grabby civilisations.
[01:50:42.000 --> 01:50:44.000]   Yeah, I mean, one,
[01:50:44.000 --> 01:50:46.000]   maybe, kind of, one, like, slightly
[01:50:46.000 --> 01:50:48.000]   fanciful upshot here is
[01:50:48.000 --> 01:50:50.000]   you don't want this, like, greedy
[01:50:50.000 --> 01:50:52.000]   von Neumann-type probes to win out, but also
[01:50:52.000 --> 01:50:54.000]   just, like, dead. They have no, kind of,
[01:50:54.000 --> 01:50:56.000]   nothing of value.
[01:50:56.000 --> 01:50:58.000]   And so if you think you have something of value to spread,
[01:50:58.000 --> 01:51:00.000]   maybe that is a reason to spread
[01:51:00.000 --> 01:51:02.000]   more quickly than you otherwise,
[01:51:02.000 --> 01:51:04.000]   like, would have planned once you've, like, figured out what that
[01:51:04.000 --> 01:51:06.000]   thing is, if that makes sense. Yeah, so then
[01:51:06.000 --> 01:51:08.000]   does this militate
[01:51:08.000 --> 01:51:10.000]   towards the logic of a
[01:51:10.000 --> 01:51:12.000]   space race where, similar to
[01:51:12.000 --> 01:51:14.000]   the first strike, where if you're
[01:51:14.000 --> 01:51:16.000]   not sure that you're going to retaliate, you will undo a first
[01:51:16.000 --> 01:51:18.000]   strike? Maybe there's a logic to...
[01:51:18.000 --> 01:51:20.000]   As long as you have, like, at least some, what,
[01:51:20.000 --> 01:51:22.000]   a compelling vision of what the far future should look like,
[01:51:22.000 --> 01:51:24.000]   you should try to make
[01:51:24.000 --> 01:51:26.000]   sure it's you who's the first actor that
[01:51:26.000 --> 01:51:28.000]   goes out into space, even if you don't have everything sorted
[01:51:28.000 --> 01:51:30.000]   out, even if you have, like, concerns
[01:51:30.000 --> 01:51:32.000]   about how... Yeah.
[01:51:32.000 --> 01:51:34.000]   Yeah, yeah, like, you'd ideally like to spend
[01:51:34.000 --> 01:51:36.000]   more time. My guess is
[01:51:36.000 --> 01:51:38.000]   that the timescales on which these dynamics
[01:51:38.000 --> 01:51:40.000]   are relevant
[01:51:40.000 --> 01:51:42.000]   are, like, extremely
[01:51:42.000 --> 01:51:44.000]   long timescales compared to what
[01:51:44.000 --> 01:51:46.000]   we're familiar with, so I don't
[01:51:46.000 --> 01:51:48.000]   think that any of this, like, straightforwardly translates
[01:51:48.000 --> 01:51:50.000]   into, you know, wanting to speed up on
[01:51:50.000 --> 01:51:52.000]   the order of decades. And, in fact,
[01:51:52.000 --> 01:51:54.000]   if any, like, delay on
[01:51:54.000 --> 01:51:56.000]   the order of decades, I don't know, presumably also
[01:51:56.000 --> 01:51:58.000]   centuries, gives you, like, a
[01:51:58.000 --> 01:52:00.000]   marginal improvement
[01:52:00.000 --> 01:52:02.000]   in your, like, long-run speed,
[01:52:02.000 --> 01:52:04.000]   then, just because of the, like,
[01:52:04.000 --> 01:52:06.000]   again, the timescales and the
[01:52:06.000 --> 01:52:08.000]   distances involved, you almost always
[01:52:08.000 --> 01:52:10.000]   want to take that trade-off. So, yeah,
[01:52:10.000 --> 01:52:12.000]   I guess I'd want... I'd be wary of, like,
[01:52:12.000 --> 01:52:14.000]   reading too much into all this stuff
[01:52:14.000 --> 01:52:16.000]   in terms of, like, what we should expect for some kind of
[01:52:16.000 --> 01:52:18.000]   race in the near term.
[01:52:18.000 --> 01:52:20.000]   It just turns out the space is, like, extremely big,
[01:52:20.000 --> 01:52:22.000]   and there's, like, a ton of stuff there, so
[01:52:22.000 --> 01:52:24.000]   in anything, like,
[01:52:24.000 --> 01:52:26.000]   the near term, I think
[01:52:26.000 --> 01:52:28.000]   this reasoning about, like,
[01:52:28.000 --> 01:52:30.000]   "Oh, we'll run out of useful
[01:52:30.000 --> 01:52:32.000]   resources," probably
[01:52:32.000 --> 01:52:34.000]   won't kick in, but that's, like,
[01:52:34.000 --> 01:52:36.000]   that's just me speculating, so I...
[01:52:36.000 --> 01:52:38.000]   Yeah, I don't know if I have a
[01:52:38.000 --> 01:52:40.000]   clear answer to that. Okay, so
[01:52:40.000 --> 01:52:42.000]   if we're talking about space governance,
[01:52:42.000 --> 01:52:44.000]   is there any reason to think, okay,
[01:52:44.000 --> 01:52:46.000]   in the far future, we can expect that
[01:52:46.000 --> 01:52:48.000]   space will be colonized either by,
[01:52:48.000 --> 01:52:50.000]   you know, like, a fully artificial
[01:52:50.000 --> 01:52:52.000]   intelligence,
[01:52:52.000 --> 01:52:54.000]   or by
[01:52:54.000 --> 01:52:56.000]   simulations of humans,
[01:52:56.000 --> 01:52:58.000]   like M's? In either
[01:52:58.000 --> 01:53:00.000]   case, it's not clear that
[01:53:00.000 --> 01:53:02.000]   these entities would feel that
[01:53:02.000 --> 01:53:04.000]   constrained by whatever norms
[01:53:04.000 --> 01:53:06.000]   of space governance we detail
[01:53:06.000 --> 01:53:08.000]   now. What is the reason
[01:53:08.000 --> 01:53:10.000]   for thinking that, you know, any sort of
[01:53:10.000 --> 01:53:12.000]   charter or constitution that
[01:53:12.000 --> 01:53:14.000]   the UN might build,
[01:53:14.000 --> 01:53:16.000]   regardless of how, I don't know,
[01:53:16.000 --> 01:53:18.000]   how sane it is, will be the basis
[01:53:18.000 --> 01:53:20.000]   of which, like, the actual long-run
[01:53:20.000 --> 01:53:22.000]   fate of space is
[01:53:22.000 --> 01:53:24.000]   decided upon? Yeah, yeah, yeah.
[01:53:24.000 --> 01:53:26.000]   So I guess, I know, the first thing I want to say is that
[01:53:26.000 --> 01:53:28.000]   it does, in fact, feel like an extremely long
[01:53:28.000 --> 01:53:30.000]   shot to expect that
[01:53:30.000 --> 01:53:32.000]   any kind of norms you end up agreeing on
[01:53:32.000 --> 01:53:34.000]   now, even if they're good,
[01:53:34.000 --> 01:53:36.000]   flow through
[01:53:36.000 --> 01:53:38.000]   to the point where they really matter,
[01:53:38.000 --> 01:53:40.000]   if they ever do.
[01:53:40.000 --> 01:53:42.000]   But,
[01:53:42.000 --> 01:53:44.000]   okay, so you can ask, like, what are the worlds
[01:53:44.000 --> 01:53:46.000]   in which this, like, early
[01:53:46.000 --> 01:53:48.000]   thinking does end up being
[01:53:48.000 --> 01:53:50.000]   good?
[01:53:50.000 --> 01:53:52.000]   On the end point, I don't know, like, I can imagine,
[01:53:52.000 --> 01:53:54.000]   for instance, the US Constitution
[01:53:54.000 --> 01:53:56.000]   surviving in importance, at least to some extent,
[01:53:56.000 --> 01:53:58.000]   if
[01:53:58.000 --> 01:54:00.000]   digital people come along for the ride.
[01:54:00.000 --> 01:54:02.000]   It's not obvious why there's some discontinuity
[01:54:02.000 --> 01:54:04.000]   there. I guess the
[01:54:04.000 --> 01:54:06.000]   important thing is considering what happens
[01:54:06.000 --> 01:54:08.000]   after anything
[01:54:08.000 --> 01:54:10.000]   like kind of transformative artificial intelligence
[01:54:10.000 --> 01:54:12.000]   arrives.
[01:54:12.000 --> 01:54:14.000]   My guess is that the worlds
[01:54:14.000 --> 01:54:16.000]   in which this is, like, even
[01:54:16.000 --> 01:54:18.000]   kind of remotely, this, like,
[01:54:18.000 --> 01:54:20.000]   you know, super long-term
[01:54:20.000 --> 01:54:22.000]   what norms should we have for settling space?
[01:54:22.000 --> 01:54:24.000]   The worlds in which this
[01:54:24.000 --> 01:54:26.000]   matters
[01:54:26.000 --> 01:54:28.000]   or does anything worthwhile
[01:54:28.000 --> 01:54:30.000]   are worlds in which, you know,
[01:54:30.000 --> 01:54:32.000]   alignment goes well, right?
[01:54:32.000 --> 01:54:34.000]   And it goes well in the sense
[01:54:34.000 --> 01:54:36.000]   that there's a significant sense in which
[01:54:36.000 --> 01:54:38.000]   humans are still in the driving seat
[01:54:38.000 --> 01:54:40.000]   and when they're looking for precedence, they just
[01:54:40.000 --> 01:54:42.000]   look to existing, like, institutions and norms.
[01:54:42.000 --> 01:54:44.000]   So I don't know, that seems kind of
[01:54:44.000 --> 01:54:46.000]   there's, like, so many variables here that
[01:54:46.000 --> 01:54:48.000]   seems like a fairly
[01:54:48.000 --> 01:54:50.000]   narrow kind of set of worlds,
[01:54:50.000 --> 01:54:52.000]   but, I don't know, seems pretty
[01:54:52.000 --> 01:54:54.000]   possible. And then there's also kind of, like, you know,
[01:54:54.000 --> 01:54:56.000]   settling the moon or
[01:54:56.000 --> 01:54:58.000]   Mars, where that is just, like, much
[01:54:58.000 --> 01:55:00.000]   easier to imagine how this stuff actually kind of
[01:55:00.000 --> 01:55:02.000]   ends up influencing or positively
[01:55:02.000 --> 01:55:04.000]   influencing how things turn up. Feels worth
[01:55:04.000 --> 01:55:06.000]   pointing out that there are
[01:55:06.000 --> 01:55:08.000]   things that really plausibly matter
[01:55:08.000 --> 01:55:10.000]   when we're thinking about
[01:55:10.000 --> 01:55:12.000]   space that aren't just, like,
[01:55:12.000 --> 01:55:14.000]   thinking about these crazy, kind of, very
[01:55:14.000 --> 01:55:16.000]   long-run sci-fi scenarios,
[01:55:16.000 --> 01:55:18.000]   although they are, like, pretty fun to think
[01:55:18.000 --> 01:55:20.000]   about. One is that
[01:55:20.000 --> 01:55:22.000]   there's just a ton of, like, pretty important
[01:55:22.000 --> 01:55:24.000]   infrastructure, kind of, currently
[01:55:24.000 --> 01:55:26.000]   orbiting the Earth and also
[01:55:26.000 --> 01:55:28.000]   anti-satellite weapons
[01:55:28.000 --> 01:55:30.000]   are being built, and
[01:55:30.000 --> 01:55:32.000]   my impression is, well,
[01:55:32.000 --> 01:55:34.000]   in fact, I think it's the case that
[01:55:34.000 --> 01:55:36.000]   there is a kind of worryingly
[01:55:36.000 --> 01:55:38.000]   small amount of
[01:55:38.000 --> 01:55:40.000]   agreement and regulation about the use
[01:55:40.000 --> 01:55:42.000]   of those weapons. Maybe
[01:55:42.000 --> 01:55:44.000]   that puts you in a kind of analogous position
[01:55:44.000 --> 01:55:46.000]   to not having many agreements over the use of
[01:55:46.000 --> 01:55:48.000]   nuclear weapons, although maybe less
[01:55:48.000 --> 01:55:50.000]   worrying in certain respects, but still, seems worth
[01:55:50.000 --> 01:55:52.000]   taking that seriously and thinking about
[01:55:52.000 --> 01:55:54.000]   how to make progress there.
[01:55:54.000 --> 01:55:56.000]   Yeah, I think there's just, like, a ton of other, kind of, near-term considerations.
[01:55:56.000 --> 01:55:58.000]   There's this great graph, actually,
[01:55:58.000 --> 01:56:00.000]   on Outward and Data, which I guess I can send you the link to
[01:56:00.000 --> 01:56:02.000]   after this, which shows the number
[01:56:02.000 --> 01:56:04.000]   of objects launched into orbit,
[01:56:04.000 --> 01:56:06.000]   especially low-Earth orbit,
[01:56:06.000 --> 01:56:08.000]   just over time, and it's just, like,
[01:56:08.000 --> 01:56:10.000]   perfect hockey stick, and
[01:56:10.000 --> 01:56:12.000]   I know, it's, like, quite a nice illustration of
[01:56:12.000 --> 01:56:14.000]   why you might, kind of, pay to,
[01:56:14.000 --> 01:56:16.000]   like, think
[01:56:16.000 --> 01:56:18.000]   about how to make sure
[01:56:18.000 --> 01:56:20.000]   this stuff goes well, and the story behind that graph
[01:56:20.000 --> 01:56:22.000]   is kind of fun as well. I was, like,
[01:56:22.000 --> 01:56:24.000]   messing around on some UN
[01:56:24.000 --> 01:56:26.000]   website, which had, it was just, like,
[01:56:26.000 --> 01:56:28.000]   this database, incredible database, which has
[01:56:28.000 --> 01:56:30.000]   more or less every, kind of, officially recorded
[01:56:30.000 --> 01:56:32.000]   launch logged, with, like, all this data
[01:56:32.000 --> 01:56:34.000]   about, like, how many objects were contained, or whatever.
[01:56:34.000 --> 01:56:36.000]   It was, like, the clunkiest
[01:56:36.000 --> 01:56:38.000]   API you've
[01:56:38.000 --> 01:56:40.000]   ever seen. You have to, like,
[01:56:40.000 --> 01:56:42.000]   manually, like, click through each page,
[01:56:42.000 --> 01:56:44.000]   and it takes, like, five seconds to load, and you have to, like,
[01:56:44.000 --> 01:56:46.000]   scrape it, so I was, like, okay, this is great
[01:56:46.000 --> 01:56:48.000]   that this exists. I
[01:56:48.000 --> 01:56:50.000]   am not, like, remotely sophisticated enough
[01:56:50.000 --> 01:56:52.000]   to know how to, like, make use of it, but I
[01:56:52.000 --> 01:56:54.000]   emailed the Owl and Data people, saying,
[01:56:54.000 --> 01:56:56.000]   "FYI, this exists. If you happen to
[01:56:56.000 --> 01:56:58.000]   have, like, you know,
[01:56:58.000 --> 01:57:00.000]   a ton of time to burn, then
[01:57:00.000 --> 01:57:02.000]   have at it," and
[01:57:02.000 --> 01:57:04.000]   Ed's,
[01:57:04.000 --> 01:57:06.000]   from Owl and Data,
[01:57:06.000 --> 01:57:08.000]   got back to me, like, a month later, like, "Hey, I had a free
[01:57:08.000 --> 01:57:10.000]   day. All done," and it's, like,
[01:57:10.000 --> 01:57:12.000]   up on the website. It's so cool, so cool.
[01:57:12.000 --> 01:57:14.000]   Cool, okay, I think that's my
[01:57:14.000 --> 01:57:16.000]   space rambling. I'd quite
[01:57:16.000 --> 01:57:18.000]   like to ask you a couple questions, if that's all right.
[01:57:18.000 --> 01:57:20.000]   I realize I've been, kind of,
[01:57:20.000 --> 01:57:22.000]   hogging the airwaves. Yeah, so here's one thing
[01:57:22.000 --> 01:57:24.000]   I'm just interested to know. You're doing
[01:57:24.000 --> 01:57:26.000]   this, like, blogging and podcasting right now,
[01:57:26.000 --> 01:57:28.000]   but, yeah, what's next? Like,
[01:57:28.000 --> 01:57:30.000]   2024? Okay, so what is he doing?
[01:57:30.000 --> 01:57:32.000]   I think
[01:57:32.000 --> 01:57:34.000]   I'll probably be...
[01:57:34.000 --> 01:57:36.000]   I've just had... I don't know.
[01:57:36.000 --> 01:57:38.000]   The idea of
[01:57:38.000 --> 01:57:40.000]   building a startup has been very compelling to me,
[01:57:40.000 --> 01:57:42.000]   and not necessarily
[01:57:42.000 --> 01:57:44.000]   from... I think it's
[01:57:44.000 --> 01:57:46.000]   the most impactful thing
[01:57:46.000 --> 01:57:48.000]   that could possibly be done,
[01:57:48.000 --> 01:57:50.000]   although I think it is very impactful, but it just...
[01:57:50.000 --> 01:57:52.000]   I don't know. It's just, like, if you have a...
[01:57:52.000 --> 01:57:54.000]   People tend to have, like, different things
[01:57:54.000 --> 01:57:56.000]   that are, like, "I want to be a doctor," or something,
[01:57:56.000 --> 01:57:58.000]   that it's, you know,
[01:57:58.000 --> 01:58:00.000]   it's something that's stuck in your
[01:58:00.000 --> 01:58:02.000]   head. So, yeah, I think that's
[01:58:02.000 --> 01:58:04.000]   probably what I'll be
[01:58:04.000 --> 01:58:06.000]   attempting to do in 2024.
[01:58:06.000 --> 01:58:08.000]   I don't...
[01:58:08.000 --> 01:58:10.000]   I think the situation in which
[01:58:10.000 --> 01:58:12.000]   I, like, remain a blogger and podcaster
[01:58:12.000 --> 01:58:14.000]   is, if it just turns out to be,
[01:58:14.000 --> 01:58:16.000]   like, a... I don't know. If I have...
[01:58:16.000 --> 01:58:18.000]   If the podcast just becomes, like, really huge,
[01:58:18.000 --> 01:58:20.000]   right? At that point, it might make more
[01:58:20.000 --> 01:58:22.000]   sense that, "Oh, like, actually, this is a way."
[01:58:22.000 --> 01:58:24.000]   Currently, I think the impact the podcast
[01:58:24.000 --> 01:58:26.000]   has is, like,
[01:58:26.000 --> 01:58:28.000]   0.00000001.
[01:58:28.000 --> 01:58:30.000]   And then the
[01:58:30.000 --> 01:58:32.000]   0.01 is just me getting to
[01:58:32.000 --> 01:58:34.000]   learn about a lot of different things.
[01:58:34.000 --> 01:58:36.000]   So, I think for it to have any...
[01:58:36.000 --> 01:58:38.000]   Not necessarily that it has to be thought of
[01:58:38.000 --> 01:58:40.000]   in terms of impact, but in terms of, like,
[01:58:40.000 --> 01:58:42.000]   how useful is it? I think it's only the case
[01:58:42.000 --> 01:58:44.000]   if it, like, really becomes much bigger.
[01:58:44.000 --> 01:58:46.000]   Nice. That sounds great. Maybe this is going right
[01:58:46.000 --> 01:58:48.000]   to the start of the conversation. What about a
[01:58:48.000 --> 01:58:50.000]   non-profit startup?
[01:58:50.000 --> 01:58:52.000]   All the same, like, excitement.
[01:58:52.000 --> 01:58:54.000]   If you have a great idea, you kind of skip the fundraising
[01:58:54.000 --> 01:58:56.000]   stage. More freedom, because you don't need to, like,
[01:58:56.000 --> 01:58:58.000]   make... Well, no. You still have to raise money,
[01:58:58.000 --> 01:59:00.000]   right? Sure, but, like, if it's a great
[01:59:00.000 --> 01:59:02.000]   idea, then I'm sure
[01:59:02.000 --> 01:59:04.000]   there'll be, like, support to make it happen.
[01:59:04.000 --> 01:59:06.000]   Yeah. If there's something
[01:59:06.000 --> 01:59:08.000]   where I don't see a way to, like,
[01:59:08.000 --> 01:59:10.000]   profitably do it, and I think it's very important
[01:59:10.000 --> 01:59:12.000]   that it be done, yeah,
[01:59:12.000 --> 01:59:14.000]   I definitely wouldn't be opposed to it.
[01:59:14.000 --> 01:59:16.000]   Is that, by the way, where you're leading? Like, if I asked you in 2024,
[01:59:16.000 --> 01:59:18.000]   what is Finn doing? Do you have a non-profit
[01:59:18.000 --> 01:59:20.000]   startup? I don't have something concrete in mind.
[01:59:20.000 --> 01:59:22.000]   That kind of thing feels very
[01:59:22.000 --> 01:59:24.000]   exciting to me, to at least try out.
[01:59:24.000 --> 01:59:26.000]   Gotcha, gotcha.
[01:59:26.000 --> 01:59:28.000]   Yeah, I think...
[01:59:28.000 --> 01:59:30.000]   I guess my prior
[01:59:30.000 --> 01:59:32.000]   is that there are profitable
[01:59:32.000 --> 01:59:34.000]   ways to do many
[01:59:34.000 --> 01:59:36.000]   things if you're more creative about it.
[01:59:36.000 --> 01:59:38.000]   There are obvious counterexamples
[01:59:38.000 --> 01:59:40.000]   of so many different things where,
[01:59:40.000 --> 01:59:42.000]   yeah, I could not tell you how you could make that
[01:59:42.000 --> 01:59:44.000]   profitable, right? Like, if you have
[01:59:44.000 --> 01:59:46.000]   something like One Day Sooner where they're trying
[01:59:46.000 --> 01:59:48.000]   to speed up challenge trials,
[01:59:48.000 --> 01:59:50.000]   it's like, how is that a startup? It's not
[01:59:50.000 --> 01:59:52.000]   clear. So, yeah,
[01:59:52.000 --> 01:59:54.000]   I think that
[01:59:54.000 --> 01:59:56.000]   there's a big branch of the
[01:59:56.000 --> 01:59:58.000]   decision tree where I think that's the most compelling
[01:59:58.000 --> 02:00:00.000]   thing I could do. Nice.
[02:00:00.000 --> 02:00:02.000]   And maybe a connected question is
[02:00:02.000 --> 02:00:04.000]   I'm curious what you think
[02:00:04.000 --> 02:00:06.000]   EA in general is underrating
[02:00:06.000 --> 02:00:08.000]   from your point of view. Also, maybe another
[02:00:08.000 --> 02:00:10.000]   question you can answer is what you think
[02:00:10.000 --> 02:00:12.000]   I'm personally getting
[02:00:12.000 --> 02:00:14.000]   wrong or got wrong, but maybe the
[02:00:14.000 --> 02:00:16.000]   more general question is a more interesting one for most people.
[02:00:16.000 --> 02:00:18.000]   So, I think
[02:00:18.000 --> 02:00:20.000]   when you have statements
[02:00:20.000 --> 02:00:22.000]   which are somewhat
[02:00:22.000 --> 02:00:24.000]   ephemeral
[02:00:24.000 --> 02:00:26.000]   or ambiguous...
[02:00:26.000 --> 02:00:28.000]   Let's say there's some historian like
[02:00:28.000 --> 02:00:30.000]   Toynbee, right? He wrote Study of History
[02:00:30.000 --> 02:00:32.000]   and one of the things he says in it is
[02:00:32.000 --> 02:00:34.000]   civilizations die
[02:00:34.000 --> 02:00:36.000]   when the elites
[02:00:36.000 --> 02:00:38.000]   lose confidence in the norms
[02:00:38.000 --> 02:00:40.000]   that they're setting
[02:00:40.000 --> 02:00:42.000]   and lose the confidence to rule.
[02:00:42.000 --> 02:00:44.000]   So, I don't think
[02:00:44.000 --> 02:00:46.000]   that's actually an x-risk, right?
[02:00:46.000 --> 02:00:48.000]   I'm just trying to use that as an example
[02:00:48.000 --> 02:00:50.000]   of something that comes off the top of my head.
[02:00:50.000 --> 02:00:52.000]   It's the kind of thing...
[02:00:52.000 --> 02:00:54.000]   It could be true.
[02:00:54.000 --> 02:00:56.000]   I don't know how I would think about it
[02:00:56.000 --> 02:00:58.000]   in a sort of...
[02:00:58.000 --> 02:01:00.000]   I mean, it doesn't seem tractable.
[02:01:00.000 --> 02:01:02.000]   I don't know how I would even analyze
[02:01:02.000 --> 02:01:04.000]   whether it's true or not using
[02:01:04.000 --> 02:01:06.000]   the modes of
[02:01:06.000 --> 02:01:08.000]   analyzing importance of topics that
[02:01:08.000 --> 02:01:10.000]   we've been using throughout this conversation.
[02:01:10.000 --> 02:01:12.000]   I don't know what that implies for EA because it's not clear to me
[02:01:12.000 --> 02:01:14.000]   like maybe EA shouldn't be taking
[02:01:14.000 --> 02:01:16.000]   things that are vague and ambiguous
[02:01:16.000 --> 02:01:18.000]   like that seriously to begin with, right?
[02:01:18.000 --> 02:01:20.000]   Yeah, if there is some interesting
[02:01:20.000 --> 02:01:22.000]   way to think about statements like that
[02:01:22.000 --> 02:01:24.000]   from a perspective that
[02:01:24.000 --> 02:01:26.000]   EAs could appreciate, including
[02:01:26.000 --> 02:01:28.000]   myself, from a perspective that I could appreciate,
[02:01:28.000 --> 02:01:30.000]   I'd be really interested
[02:01:30.000 --> 02:01:32.000]   to see what that would be.
[02:01:32.000 --> 02:01:34.000]   There does seem to be a disconnect where
[02:01:34.000 --> 02:01:36.000]   when I talk to my friends who are intellectually
[02:01:36.000 --> 02:01:38.000]   inclined
[02:01:38.000 --> 02:01:40.000]   who have a lot of interesting ideas,
[02:01:40.000 --> 02:01:42.000]   requiring a sort of
[02:01:42.000 --> 02:01:44.000]   translation layer, almost like a
[02:01:44.000 --> 02:01:46.000]   compiler
[02:01:46.000 --> 02:01:48.000]   or like a transpiler
[02:01:48.000 --> 02:01:50.000]   that converts
[02:01:50.000 --> 02:01:52.000]   code from this language into
[02:01:52.000 --> 02:01:54.000]   assembly here,
[02:01:54.000 --> 02:01:56.000]   it does create a little bit of inefficiency
[02:01:56.000 --> 02:01:58.000]   and potentially a loss of
[02:01:58.000 --> 02:02:00.000]   topics that could be talked about.
[02:02:00.000 --> 02:02:02.000]   I think that's a great answer.
[02:02:02.000 --> 02:02:04.000]   I'd just say it's something I'm worried about as well,
[02:02:04.000 --> 02:02:06.000]   especially in leading towards
[02:02:06.000 --> 02:02:08.000]   a more speculative, long-termist end.
[02:02:08.000 --> 02:02:10.000]   It seems really important to
[02:02:10.000 --> 02:02:12.000]   keep hold of
[02:02:12.000 --> 02:02:14.000]   some real truth-seeking
[02:02:14.000 --> 02:02:16.000]   attitudes where
[02:02:16.000 --> 02:02:18.000]   the obvious feedback of whether
[02:02:18.000 --> 02:02:20.000]   you're getting things right or wrong
[02:02:20.000 --> 02:02:22.000]   is much harder and often you don't have the luxury
[02:02:22.000 --> 02:02:24.000]   of having that.
[02:02:24.000 --> 02:02:26.000]   Keeping that attitude in mind seems very important.
[02:02:26.000 --> 02:02:28.000]   I like that.
[02:02:28.000 --> 02:02:30.000]   I think that EA should improve on that.
[02:02:30.000 --> 02:02:32.000]   I guess off the top of my head
[02:02:32.000 --> 02:02:34.000]   maybe I have two answers which
[02:02:34.000 --> 02:02:36.000]   go in exact opposite directions.
[02:02:36.000 --> 02:02:38.000]   One answer is
[02:02:38.000 --> 02:02:40.000]   that something that
[02:02:40.000 --> 02:02:42.000]   looks a bit like a failure mode that I'm a bit worried about
[02:02:42.000 --> 02:02:44.000]   is if
[02:02:44.000 --> 02:02:46.000]   the movement grows significantly
[02:02:46.000 --> 02:02:48.000]   then the
[02:02:48.000 --> 02:02:50.000]   ideas that originally motivated it
[02:02:50.000 --> 02:02:52.000]   that were quite new
[02:02:52.000 --> 02:02:54.000]   and exciting and important ideas
[02:02:54.000 --> 02:02:56.000]   somewhat kind of
[02:02:56.000 --> 02:02:58.000]   dilute maybe because it's like
[02:02:58.000 --> 02:03:00.000]   I guess it's related to
[02:03:00.000 --> 02:03:02.000]   what you said, you kind of lose these
[02:03:02.000 --> 02:03:04.000]   attitudes of taking weird ideas seriously,
[02:03:04.000 --> 02:03:06.000]   scrutinizing one another quite a lot
[02:03:06.000 --> 02:03:08.000]   and it becomes a bit like, I don't know,
[02:03:08.000 --> 02:03:10.000]   greenwashing or something
[02:03:10.000 --> 02:03:12.000]   where the language stays but the real
[02:03:12.000 --> 02:03:14.000]   fire behind it of
[02:03:14.000 --> 02:03:16.000]   taking
[02:03:16.000 --> 02:03:18.000]   impact really seriously rather than just
[02:03:18.000 --> 02:03:20.000]   saying the right things, that kind of fades away.
[02:03:20.000 --> 02:03:22.000]   So I don't think
[02:03:22.000 --> 02:03:24.000]   EA is currently underrating that in any important sense
[02:03:24.000 --> 02:03:26.000]   but it's something that seems worth having
[02:03:26.000 --> 02:03:28.000]   as a
[02:03:28.000 --> 02:03:30.000]   worry on the radar.
[02:03:30.000 --> 02:03:32.000]   The roughly opposite thing that
[02:03:32.000 --> 02:03:34.000]   seems also worth worrying about is
[02:03:34.000 --> 02:03:36.000]   I think it's just really worth
[02:03:36.000 --> 02:03:38.000]   paying attention or
[02:03:38.000 --> 02:03:40.000]   it's worth considering
[02:03:40.000 --> 02:03:42.000]   best case outcomes where
[02:03:42.000 --> 02:03:44.000]   a lot of this stuff
[02:03:44.000 --> 02:03:46.000]   maybe grows
[02:03:46.000 --> 02:03:48.000]   quite considerably.
[02:03:48.000 --> 02:03:50.000]   Thinking about
[02:03:50.000 --> 02:03:52.000]   how this stuff
[02:03:52.000 --> 02:03:54.000]   could become
[02:03:54.000 --> 02:03:56.000]   mainstream, I think thinking about
[02:03:56.000 --> 02:03:58.000]   really scalable projects
[02:03:58.000 --> 02:04:00.000]   as well as just little fun kind of
[02:04:00.000 --> 02:04:02.000]   interventions on margins.
[02:04:02.000 --> 02:04:04.000]   There's at least some chance that becomes very
[02:04:04.000 --> 02:04:06.000]   important. And so as such
[02:04:06.000 --> 02:04:08.000]   one part of that is maybe just learning to
[02:04:08.000 --> 02:04:10.000]   make a lot of these fields
[02:04:10.000 --> 02:04:12.000]   legible and attractive
[02:04:12.000 --> 02:04:14.000]   to
[02:04:14.000 --> 02:04:16.000]   people who could contribute, who are
[02:04:16.000 --> 02:04:18.000]   learning about it for the first time.
[02:04:18.000 --> 02:04:20.000]   And just yeah, in general
[02:04:20.000 --> 02:04:22.000]   planning for the best case, which could
[02:04:22.000 --> 02:04:24.000]   mean just like being, thinking
[02:04:24.000 --> 02:04:26.000]   in very ambitious terms, thinking about
[02:04:26.000 --> 02:04:28.000]   things going very well, that just also seems worth doing.
[02:04:28.000 --> 02:04:30.000]   So I think it's a very vague answer but
[02:04:30.000 --> 02:04:32.000]   maybe that's, yeah, maybe not worth
[02:04:32.000 --> 02:04:34.000]   keeping in, but that's my answer.
[02:04:34.000 --> 02:04:36.000]   Perhaps, you know, opposite to what you were
[02:04:36.000 --> 02:04:38.000]   saying about EA not taking weird ideas too
[02:04:38.000 --> 02:04:40.000]   seriously in the future is maybe they are taking
[02:04:40.000 --> 02:04:42.000]   weird ideas too seriously now.
[02:04:42.000 --> 02:04:44.000]   It could be the case that
[02:04:44.000 --> 02:04:46.000]   just following
[02:04:46.000 --> 02:04:48.000]   basic common sense morality, kind of like what
[02:04:48.000 --> 02:04:50.000]   he talks about in Suburban Attachments,
[02:04:50.000 --> 02:04:52.000]   is really the most effective ways to deal
[02:04:52.000 --> 02:04:54.000]   with many threats, even weird
[02:04:54.000 --> 02:04:56.000]   threats. If you have
[02:04:56.000 --> 02:04:58.000]   areas that are more speculative
[02:04:58.000 --> 02:05:00.000]   like bio-risk or AI
[02:05:00.000 --> 02:05:02.000]   where it's not even clear
[02:05:02.000 --> 02:05:04.000]   that the things you're doing to address them are necessarily
[02:05:04.000 --> 02:05:06.000]   making them better. I know there's concern in the movement
[02:05:06.000 --> 02:05:08.000]   like the initial grant that they gave to OpenAI
[02:05:08.000 --> 02:05:10.000]   might have like sped up
[02:05:10.000 --> 02:05:12.000]   AI doom.
[02:05:12.000 --> 02:05:14.000]   Maybe the best case scenario
[02:05:14.000 --> 02:05:16.000]   in cases where there's a lot of ambiguity
[02:05:16.000 --> 02:05:18.000]   is to just
[02:05:18.000 --> 02:05:20.000]   do more common sense things like, and maybe this
[02:05:20.000 --> 02:05:22.000]   is also applicable to things like global health, where
[02:05:22.000 --> 02:05:24.000]   mullet or nets are great,
[02:05:24.000 --> 02:05:26.000]   but the way that
[02:05:26.000 --> 02:05:28.000]   hundreds of millions of people have been left out of poverty
[02:05:28.000 --> 02:05:30.000]   is just through implementing capitalism,
[02:05:30.000 --> 02:05:32.000]   right? It's not through
[02:05:32.000 --> 02:05:34.000]   targeted interventions like that.
[02:05:34.000 --> 02:05:36.000]   Again, I don't know what this implies for the
[02:05:36.000 --> 02:05:38.000]   movement in general.
[02:05:38.000 --> 02:05:40.000]   Even if just implementing
[02:05:40.000 --> 02:05:42.000]   the neoliberal agenda is the best way to decrease poverty,
[02:05:42.000 --> 02:05:44.000]   what does that mean that somebody should do
[02:05:44.000 --> 02:05:46.000]   with their, yeah, what does that mean
[02:05:46.000 --> 02:05:48.000]   you should do with a marginal million dollars, right?
[02:05:48.000 --> 02:05:50.000]   It's not clear to me. It's something I hope I'll know more about
[02:05:50.000 --> 02:05:52.000]   in five to ten years.
[02:05:52.000 --> 02:05:54.000]   I'd be very curious to talk to FutureMe about
[02:05:54.000 --> 02:05:56.000]   what does he think about common sense morality versus
[02:05:56.000 --> 02:05:58.000]   taking weird ideas seriously. I think one way
[02:05:58.000 --> 02:06:00.000]   of thinking about
[02:06:00.000 --> 02:06:02.000]   "weird ideas"
[02:06:02.000 --> 02:06:04.000]   is that in some sense they are the result of
[02:06:04.000 --> 02:06:06.000]   taking a bunch of common sense
[02:06:06.000 --> 02:06:08.000]   starting points and then
[02:06:08.000 --> 02:06:10.000]   just really reflecting on them hard and seeing what comes out.
[02:06:10.000 --> 02:06:12.000]   Yeah, so I think maybe the
[02:06:12.000 --> 02:06:14.000]   question is how much trust
[02:06:14.000 --> 02:06:16.000]   should we place on those reflective processes
[02:06:16.000 --> 02:06:18.000]   versus
[02:06:18.000 --> 02:06:20.000]   what should I
[02:06:20.000 --> 02:06:22.000]   prior be on weird
[02:06:22.000 --> 02:06:24.000]   ideas being true because they're
[02:06:24.000 --> 02:06:26.000]   weird? Is that good or bad?
[02:06:26.000 --> 02:06:28.000]   And then separately, I don't know, one thing that just seems
[02:06:28.000 --> 02:06:30.000]   kind of obvious and important is if you take these
[02:06:30.000 --> 02:06:32.000]   ideas, first of all you should ask yourself
[02:06:32.000 --> 02:06:34.000]   whether you actually believe them
[02:06:34.000 --> 02:06:36.000]   or whether they are
[02:06:36.000 --> 02:06:38.000]   kind of fun to say
[02:06:38.000 --> 02:06:40.000]   or you're just kind of saying
[02:06:40.000 --> 02:06:42.000]   that you believe them. And then sometimes, I know
[02:06:42.000 --> 02:06:44.000]   it's fun to say weird ideas but it's like, okay
[02:06:44.000 --> 02:06:46.000]   I actually don't have good grounds to believe this.
[02:06:46.000 --> 02:06:48.000]   And then second of all, if you do in fact believe something
[02:06:48.000 --> 02:06:50.000]   it's really valuable to ask
[02:06:50.000 --> 02:06:52.000]   if you think this thing is really important and true
[02:06:52.000 --> 02:06:54.000]   why aren't you working on it
[02:06:54.000 --> 02:06:56.000]   if you have the opportunity to work on it?
[02:06:56.000 --> 02:06:58.000]   This is like the Hamming question, right? What's the most important
[02:06:58.000 --> 02:07:00.000]   problem in your field? And then
[02:07:00.000 --> 02:07:02.000]   what's stopping you from
[02:07:02.000 --> 02:07:04.000]   working on it? And obviously many people have the
[02:07:04.000 --> 02:07:06.000]   luxury of dropping out everything
[02:07:06.000 --> 02:07:08.000]   and working on the things that they
[02:07:08.000 --> 02:07:10.000]   in fact believe are really important but
[02:07:10.000 --> 02:07:12.000]   if you do have that opportunity
[02:07:12.000 --> 02:07:14.000]   that's a question which I know is
[02:07:14.000 --> 02:07:16.000]   maybe just valuable to ask.
[02:07:16.000 --> 02:07:18.000]   Maybe this is a meta objection
[02:07:18.000 --> 02:07:20.000]   to EA which is that I'm aware
[02:07:20.000 --> 02:07:22.000]   of a lot of potential
[02:07:22.000 --> 02:07:24.000]   objections to EA, like the ones
[02:07:24.000 --> 02:07:26.000]   we were just talking about but there's so many other ones
[02:07:26.000 --> 02:07:28.000]   where people will identify
[02:07:28.000 --> 02:07:30.000]   yeah, yeah, that's an
[02:07:30.000 --> 02:07:32.000]   interesting point and then nobody knows
[02:07:32.000 --> 02:07:34.000]   what to do about it, right? It's like
[02:07:34.000 --> 02:07:36.000]   should we take common sense, morality
[02:07:36.000 --> 02:07:38.000]   more seriously? Should we take weird ideas more seriously?
[02:07:38.000 --> 02:07:40.000]   Oh, that is an interesting debate but how do you
[02:07:40.000 --> 02:07:42.000]   resolve that? I don't know how to resolve that.
[02:07:42.000 --> 02:07:44.000]   I don't know if somebody's come up with a good way to resolve that.
[02:07:44.000 --> 02:07:46.000]   I guess it kind of hooks into the long
[02:07:46.000 --> 02:07:48.000]   reflection stuff a little bit because
[02:07:48.000 --> 02:07:50.000]   one answer here is just time.
[02:07:50.000 --> 02:07:52.000]   I think the story of
[02:07:52.000 --> 02:07:54.000]   people raising concerns about
[02:07:54.000 --> 02:07:56.000]   AI is maybe instructive here
[02:07:56.000 --> 02:07:58.000]   where early on
[02:07:58.000 --> 02:08:00.000]   you get some real
[02:08:00.000 --> 02:08:02.000]   kind of just radical out there
[02:08:02.000 --> 02:08:04.000]   researchers or writers
[02:08:04.000 --> 02:08:06.000]   who are raising this as a worry
[02:08:06.000 --> 02:08:08.000]   there's a lot of weird baggage
[02:08:08.000 --> 02:08:10.000]   attached to what they write
[02:08:10.000 --> 02:08:12.000]   and then maybe you get a first book or two
[02:08:12.000 --> 02:08:14.000]   and then you get more
[02:08:14.000 --> 02:08:16.000]   prestigious or established
[02:08:16.000 --> 02:08:18.000]   people
[02:08:18.000 --> 02:08:20.000]   expressing concerns. I think
[02:08:20.000 --> 02:08:22.000]   one way to accelerate that
[02:08:22.000 --> 02:08:24.000]   process when it's worth accelerating
[02:08:24.000 --> 02:08:26.000]   is just to ask that question
[02:08:26.000 --> 02:08:28.000]   do I in fact see
[02:08:28.000 --> 02:08:30.000]   can I go along
[02:08:30.000 --> 02:08:32.000]   with this argument? Do I see a
[02:08:32.000 --> 02:08:34.000]   hole in it? And then if the answer
[02:08:34.000 --> 02:08:36.000]   is no, if it just kind of checks out even if you
[02:08:36.000 --> 02:08:38.000]   obviously are always going to be uncertain but if it's like
[02:08:38.000 --> 02:08:40.000]   yeah this seems kind of reasonable
[02:08:40.000 --> 02:08:42.000]   then
[02:08:42.000 --> 02:08:44.000]   by default you might just
[02:08:44.000 --> 02:08:46.000]   spend a few years being like just kind of living
[02:08:46.000 --> 02:08:48.000]   like oh yeah this is a thing that I guess I think is true
[02:08:48.000 --> 02:08:50.000]   but I'm not really acting on. You can't just skip
[02:08:50.000 --> 02:08:52.000]   that step and be like well just act it on now.
[02:08:52.000 --> 02:08:54.000]   I'm not sure
[02:08:54.000 --> 02:08:56.000]   I agree. I think
[02:08:56.000 --> 02:08:58.000]   maybe an analogy here is like
[02:08:58.000 --> 02:09:00.000]   you're in a relationship and
[02:09:00.000 --> 02:09:02.000]   you think like oh well I don't see what's
[02:09:02.000 --> 02:09:04.000]   wrong with this relationship. So
[02:09:04.000 --> 02:09:06.000]   instead of just waiting a few years
[02:09:06.000 --> 02:09:08.000]   to like try to find something wrong with it
[02:09:08.000 --> 02:09:10.000]   might as well just tie the knot now and get married.
[02:09:10.000 --> 02:09:12.000]   I think it's something similar with
[02:09:12.000 --> 02:09:14.000]   I think of
[02:09:14.000 --> 02:09:16.000]   failure mode if you maybe not
[02:09:16.000 --> 02:09:18.000]   because we're EAs we wouldn't see it in EA
[02:09:18.000 --> 02:09:20.000]   but we can see generally in the world
[02:09:20.000 --> 02:09:22.000]   is that people just come to conclusions about
[02:09:22.000 --> 02:09:24.000]   how the world works or how the world
[02:09:24.000 --> 02:09:26.000]   ought to work too early in
[02:09:26.000 --> 02:09:28.000]   life where when they don't seem to
[02:09:28.000 --> 02:09:30.000]   know that much about what is
[02:09:30.000 --> 02:09:32.000]   optimal and what is
[02:09:32.000 --> 02:09:34.000]   possible. Yeah that's a great point.
[02:09:34.000 --> 02:09:36.000]   So yeah maybe they should just wait a little longer
[02:09:36.000 --> 02:09:38.000]   just like integrate like these weird
[02:09:38.000 --> 02:09:40.000]   radical ideas as things that exist
[02:09:40.000 --> 02:09:42.000]   in the world and wait until you're like late 20s
[02:09:42.000 --> 02:09:44.000]   until you decide actually this is the
[02:09:44.000 --> 02:09:46.000]   thing I should do with the rest of my career
[02:09:46.000 --> 02:09:48.000]   or with my political
[02:09:48.000 --> 02:09:50.000]   rights or whatever.
[02:09:50.000 --> 02:09:52.000]   Yeah I think that's actually
[02:09:52.000 --> 02:09:54.000]   a really good point. I think maybe I'd want to
[02:09:54.000 --> 02:09:56.000]   kind of walk back
[02:09:56.000 --> 02:09:58.000]   on what I said based on that
[02:09:58.000 --> 02:10:00.000]   but I think there's some version of it which I'd
[02:10:00.000 --> 02:10:02.000]   still really endorse which is
[02:10:02.000 --> 02:10:04.000]   maybe like you know spend like
[02:10:04.000 --> 02:10:06.000]   some time reflecting on this such that I don't
[02:10:06.000 --> 02:10:08.000]   expect further reflection is going to like radically
[02:10:08.000 --> 02:10:10.000]   change what I think.
[02:10:10.000 --> 02:10:12.000]   You can maybe talk about
[02:10:12.000 --> 02:10:14.000]   this being the case of like a group of people rather than
[02:10:14.000 --> 02:10:16.000]   a particular person and
[02:10:16.000 --> 02:10:18.000]   I could just like really see this
[02:10:18.000 --> 02:10:20.000]   thing playing out where I just like believe it's important
[02:10:20.000 --> 02:10:22.000]   for a really long time without acting on it and that's
[02:10:22.000 --> 02:10:24.000]   the thing which seems worth
[02:10:24.000 --> 02:10:26.000]   skipping. I mean to be
[02:10:26.000 --> 02:10:28.000]   a little tiny bit more concrete like
[02:10:28.000 --> 02:10:30.000]   if you really think some of this
[02:10:30.000 --> 02:10:32.000]   these potentially catastrophic
[02:10:32.000 --> 02:10:34.000]   risks just like are real
[02:10:34.000 --> 02:10:36.000]   and you think there are things that we can do about it
[02:10:36.000 --> 02:10:38.000]   then
[02:10:38.000 --> 02:10:40.000]   sure seems good to start working on this
[02:10:40.000 --> 02:10:42.000]   stuff.
[02:10:42.000 --> 02:10:44.000]   And you really
[02:10:44.000 --> 02:10:46.000]   want to like avoid that regret
[02:10:46.000 --> 02:10:48.000]   of you know seven years down the line
[02:10:48.000 --> 02:10:50.000]   like ah I really could have just started working on that earlier.
[02:10:50.000 --> 02:10:52.000]   There are occasions where this kind of thinking
[02:10:52.000 --> 02:10:54.000]   is useful or at least kind of asking this question
[02:10:54.000 --> 02:10:56.000]   like what would I do right now if I
[02:10:56.000 --> 02:10:58.000]   just like did what my kind of
[02:10:58.000 --> 02:11:00.000]   idealized self would endorse doing.
[02:11:00.000 --> 02:11:02.000]   Maybe that's useful. So it seems that
[02:11:02.000 --> 02:11:04.000]   if you're trying to pursue I don't know
[02:11:04.000 --> 02:11:06.000]   a career related to EA there's like two steps
[02:11:06.000 --> 02:11:08.000]   where the first step is you
[02:11:08.000 --> 02:11:10.000]   have to get a position like the one you
[02:11:10.000 --> 02:11:12.000]   have right now where
[02:11:12.000 --> 02:11:14.000]   you're you know learning a lot
[02:11:14.000 --> 02:11:16.000]   and figuring out future steps
[02:11:16.000 --> 02:11:18.000]   and then the one after that is
[02:11:18.000 --> 02:11:20.000]   where you actually lead or
[02:11:20.000 --> 02:11:22.000]   take ownership
[02:11:22.000 --> 02:11:24.000]   of a specific project like
[02:11:24.000 --> 02:11:26.000]   a non-profit startup or something.
[02:11:26.000 --> 02:11:28.000]   Do you have any advice for
[02:11:28.000 --> 02:11:30.000]   somebody who is before
[02:11:30.000 --> 02:11:32.000]   step one?
[02:11:32.000 --> 02:11:34.000]   Huh. That's a really good question.
[02:11:34.000 --> 02:11:36.000]   I also will just do the annoying thing of saying
[02:11:36.000 --> 02:11:38.000]   definitely other things you can do other than
[02:11:38.000 --> 02:11:40.000]   that kind of like two step trajectory
[02:11:40.000 --> 02:11:42.000]   but yeah.
[02:11:42.000 --> 02:11:44.000]   As in go directly to step two?
[02:11:44.000 --> 02:11:46.000]   We'll just never go to step two and just like
[02:11:46.000 --> 02:11:48.000]   be a really excellent researcher or communicator
[02:11:48.000 --> 02:11:50.000]   and like anything else. Sure, sure, sure.
[02:11:50.000 --> 02:11:52.000]   I think like where
[02:11:52.000 --> 02:11:54.000]   you have the luxury of doing it
[02:11:54.000 --> 02:11:56.000]   not kind of
[02:11:56.000 --> 02:11:58.000]   rushing into the most salient like
[02:11:58.000 --> 02:12:00.000]   career option and then
[02:12:00.000 --> 02:12:02.000]   retroactively justifying why
[02:12:02.000 --> 02:12:04.000]   it was the correct option
[02:12:04.000 --> 02:12:06.000]   I think is like
[02:12:06.000 --> 02:12:08.000]   quite a nice thing to bear in mind.
[02:12:08.000 --> 02:12:10.000]   I suppose often it's quite uncomfortable.
[02:12:10.000 --> 02:12:12.000]   Do you mean something like consulting?
[02:12:12.000 --> 02:12:14.000]   Yeah, something like that.
[02:12:14.000 --> 02:12:16.000]   Yeah, I mean the kind of the obvious advice
[02:12:16.000 --> 02:12:18.000]   here is that there is a website
[02:12:18.000 --> 02:12:20.000]   designed to answer this question
[02:12:20.000 --> 02:12:22.000]   which is 80,000 hours.
[02:12:22.000 --> 02:12:24.000]   There's a particular bit of advice
[02:12:24.000 --> 02:12:26.000]   from ADK which I
[02:12:26.000 --> 02:12:28.000]   found very useful which was
[02:12:28.000 --> 02:12:30.000]   after I left uni I was like
[02:12:30.000 --> 02:12:32.000]   really unsure what I wanted to do.
[02:12:32.000 --> 02:12:34.000]   I was choosing between a couple options
[02:12:34.000 --> 02:12:36.000]   and I was like
[02:12:36.000 --> 02:12:38.000]   oh my god this is like such a big decision
[02:12:38.000 --> 02:12:40.000]   because I guess in this context it's not only
[02:12:40.000 --> 02:12:42.000]   do you have to answer
[02:12:42.000 --> 02:12:44.000]   the question of what might be a good fit
[02:12:44.000 --> 02:12:46.000]   for me, what I might enjoy but also
[02:12:46.000 --> 02:12:48.000]   sometimes what is actually most
[02:12:48.000 --> 02:12:50.000]   important maybe
[02:12:50.000 --> 02:12:52.000]   and how am I supposed to answer
[02:12:52.000 --> 02:12:54.000]   that given that
[02:12:54.000 --> 02:12:56.000]   there's a ton of disagreement
[02:12:56.000 --> 02:12:58.000]   and so I just found myself
[02:12:58.000 --> 02:13:00.000]   bashing my head against the
[02:13:00.000 --> 02:13:02.000]   wall of trying to get to a point where I was certain
[02:13:02.000 --> 02:13:04.000]   that one option was better than the other
[02:13:04.000 --> 02:13:06.000]   and
[02:13:06.000 --> 02:13:08.000]   the piece of advice that I found useful
[02:13:08.000 --> 02:13:10.000]   was that often
[02:13:10.000 --> 02:13:12.000]   you should just write off the possibility of becoming
[02:13:12.000 --> 02:13:14.000]   fully certain about what option is best
[02:13:14.000 --> 02:13:16.000]   instead what you should do is you should
[02:13:16.000 --> 02:13:18.000]   reflect on the decision
[02:13:18.000 --> 02:13:20.000]   proactively, that is talk to people
[02:13:20.000 --> 02:13:22.000]   write down your thoughts
[02:13:22.000 --> 02:13:24.000]   and just keep iterating on that
[02:13:24.000 --> 02:13:26.000]   until
[02:13:26.000 --> 02:13:28.000]   the dial stops
[02:13:28.000 --> 02:13:30.000]   moving backwards and forwards
[02:13:30.000 --> 02:13:32.000]   and just settles on some particular
[02:13:32.000 --> 02:13:34.000]   uncertainty so it's like look
[02:13:34.000 --> 02:13:36.000]   I guess I'm a kind of 60%
[02:13:36.000 --> 02:13:38.000]   70% option A is better than
[02:13:38.000 --> 02:13:40.000]   B and that hasn't really changed
[02:13:40.000 --> 02:13:42.000]   having done like a bunch of extra thinking
[02:13:42.000 --> 02:13:44.000]   that's roughly speaking
[02:13:44.000 --> 02:13:46.000]   the point where it might be best to make the decision
[02:13:46.000 --> 02:13:48.000]   rather than holding out for
[02:13:48.000 --> 02:13:50.000]   certainty. Does that make sense?
[02:13:50.000 --> 02:13:52.000]   Yeah it's like
[02:13:52.000 --> 02:13:54.000]   kind of like gradient descent where
[02:13:54.000 --> 02:13:56.000]   if the loss function hasn't changed in the last iteration
[02:13:56.000 --> 02:13:58.000]   you call it a loss
[02:13:58.000 --> 02:14:00.000]   Yeah nice, like it, like it
[02:14:00.000 --> 02:14:02.000]   Yeah that's super interesting
[02:14:02.000 --> 02:14:04.000]   Though I guess one problem maybe
[02:14:04.000 --> 02:14:06.000]   that somebody might face is that before
[02:14:06.000 --> 02:14:08.000]   they've actually done things it's hard to know
[02:14:08.000 --> 02:14:10.000]   that like that's
[02:14:10.000 --> 02:14:12.000]   actually a, like
[02:14:12.000 --> 02:14:14.000]   not that this is actually going to be my career
[02:14:14.000 --> 02:14:16.000]   but I would have, like the podcast was just something
[02:14:16.000 --> 02:14:18.000]   I did as in like I was bored during COVID
[02:14:18.000 --> 02:14:20.000]   and I, yeah
[02:14:20.000 --> 02:14:22.000]   classes went online and I just didn't have anything else to do
[02:14:22.000 --> 02:14:24.000]   I don't think it's something I would have pursued if I
[02:14:24.000 --> 02:14:26.000]   ever thought of it, well I never thought of it
[02:14:26.000 --> 02:14:28.000]   as a career right so it's like, but
[02:14:28.000 --> 02:14:30.000]   just doing things like that can
[02:14:30.000 --> 02:14:32.000]   potentially lead you down interesting
[02:14:32.000 --> 02:14:34.000]   interesting avenues. Yeah yeah yeah
[02:14:34.000 --> 02:14:36.000]   I think that's a great point
[02:14:36.000 --> 02:14:38.000]   there was um, I guess we're both involved
[02:14:38.000 --> 02:14:40.000]   with this blog prize
[02:14:40.000 --> 02:14:42.000]   and there was a
[02:14:42.000 --> 02:14:44.000]   like a kind of mini prize
[02:14:44.000 --> 02:14:46.000]   last month for people writing about
[02:14:46.000 --> 02:14:48.000]   like the idea of agency
[02:14:48.000 --> 02:14:50.000]   and what you just said I think links into that
[02:14:50.000 --> 02:14:52.000]   really nicely, there's this kind of
[02:14:52.000 --> 02:14:54.000]   property of going from realising
[02:14:54.000 --> 02:14:56.000]   you can do something to doing it which just seems
[02:14:56.000 --> 02:14:58.000]   like both really valuable
[02:14:58.000 --> 02:15:00.000]   and learnable
[02:15:00.000 --> 02:15:02.000]   so yeah just like
[02:15:02.000 --> 02:15:04.000]   going from the idea of I could maybe do a little
[02:15:04.000 --> 02:15:06.000]   podcast series to like actually
[02:15:06.000 --> 02:15:08.000]   testing it and like being
[02:15:08.000 --> 02:15:10.000]   open to the possibility that it fails but you learn
[02:15:10.000 --> 02:15:12.000]   something from it, just really valuable
[02:15:12.000 --> 02:15:14.000]   also we were talking about sending cold emails in that
[02:15:14.000 --> 02:15:16.000]   same bit of the conversation right like
[02:15:16.000 --> 02:15:18.000]   if there's someone you look up to
[02:15:18.000 --> 02:15:20.000]   and you have, you think it's like very
[02:15:20.000 --> 02:15:22.000]   plausible that you might end up in their like
[02:15:22.000 --> 02:15:24.000]   line of research and you think there's a bunch of things you could learn from them
[02:15:24.000 --> 02:15:26.000]   as long as you're not like
[02:15:26.000 --> 02:15:28.000]   demanding a huge amount of their time or attention then
[02:15:28.000 --> 02:15:30.000]   you can just like ask to talk to them
[02:15:30.000 --> 02:15:32.000]   I think finding a mentor
[02:15:32.000 --> 02:15:34.000]   in places like this
[02:15:34.000 --> 02:15:36.000]   is just like
[02:15:36.000 --> 02:15:38.000]   so useful and just like asking
[02:15:38.000 --> 02:15:40.000]   people if they could fill that role like again
[02:15:40.000 --> 02:15:42.000]   in a kind of friendly way it's just
[02:15:42.000 --> 02:15:44.000]   you know maybe
[02:15:44.000 --> 02:15:46.000]   it's a kind of a move people don't
[02:15:46.000 --> 02:15:48.000]   opt for a lot of the time
[02:15:48.000 --> 02:15:50.000]   but yeah just like taking the non-obvious options, being
[02:15:50.000 --> 02:15:52.000]   proactive about connecting
[02:15:52.000 --> 02:15:54.000]   to other people, seeing if you can like physically
[02:15:54.000 --> 02:15:56.000]   meet other people who are like interested in the same kind of
[02:15:56.000 --> 02:15:58.000]   weird things as you, yeah this is all like
[02:15:58.000 --> 02:16:00.000]   extremely obvious but I guess it's stuff I kind of
[02:16:00.000 --> 02:16:02.000]   would really have benefited from learning
[02:16:02.000 --> 02:16:04.000]   earlier on. Yeah and the unfortunate
[02:16:04.000 --> 02:16:06.000]   thing is it's like not clear how you should apply
[02:16:06.000 --> 02:16:08.000]   that in your own circumstance when you're
[02:16:08.000 --> 02:16:10.000]   when you're trying to decide what to do
[02:16:10.000 --> 02:16:12.000]   okay so yeah let's
[02:16:12.000 --> 02:16:14.000]   close out by talking about
[02:16:14.000 --> 02:16:16.000]   just like plugging the
[02:16:16.000 --> 02:16:18.000]   effective ideas, the
[02:16:18.000 --> 02:16:20.000]   blog prize you just mentioned and then the
[02:16:20.000 --> 02:16:22.000]   the red teaming EA
[02:16:22.000 --> 02:16:24.000]   contest, you want
[02:16:24.000 --> 02:16:26.000]   to talk, we already mentioned that earlier but
[02:16:26.000 --> 02:16:28.000]   if you just want to leave like links and just
[02:16:28.000 --> 02:16:30.000]   again it summarizes them for you. Cool, I appreciate
[02:16:30.000 --> 02:16:32.000]   that. Yeah so
[02:16:32.000 --> 02:16:34.000]   the criticism contest
[02:16:34.000 --> 02:16:36.000]   the deadline is the 1st of September
[02:16:36.000 --> 02:16:38.000]   the kind of canonical
[02:16:38.000 --> 02:16:40.000]   post that announces that is an EA forum post
[02:16:40.000 --> 02:16:42.000]   which I'd be very grateful if you could
[02:16:42.000 --> 02:16:44.000]   link to somewhere but I'm you know happy to do that
[02:16:44.000 --> 02:16:46.000]   and then prize pool is
[02:16:46.000 --> 02:16:48.000]   at least $100,000
[02:16:48.000 --> 02:16:50.000]   but possibly more if there's just like a lot of exceptional
[02:16:50.000 --> 02:16:52.000]   entries
[02:16:52.000 --> 02:16:54.000]   and then hopefully
[02:16:54.000 --> 02:16:56.000]   all the kind of relevant information is there
[02:16:56.000 --> 02:16:58.000]   and then yeah this blog prize
[02:16:58.000 --> 02:17:00.000]   as well which I've been kind of
[02:17:00.000 --> 02:17:02.000]   helping run
[02:17:02.000 --> 02:17:04.000]   I think you mentioned about it at the start
[02:17:04.000 --> 02:17:06.000]   so the like overall prize
[02:17:06.000 --> 02:17:08.000]   is $100,000
[02:17:08.000 --> 02:17:10.000]   and up to 5 of those prizes
[02:17:10.000 --> 02:17:12.000]   but also there are these smaller
[02:17:12.000 --> 02:17:14.000]   monthly prizes that
[02:17:14.000 --> 02:17:16.000]   I just mentioned so last month was
[02:17:16.000 --> 02:17:18.000]   the theme was agency and the theme
[02:17:18.000 --> 02:17:20.000]   this month is to
[02:17:20.000 --> 02:17:22.000]   write some response
[02:17:22.000 --> 02:17:24.000]   or some reflection on this
[02:17:24.000 --> 02:17:26.000]   series of blog posts called
[02:17:26.000 --> 02:17:28.000]   The Most Important Century
[02:17:28.000 --> 02:17:30.000]   blog post series by
[02:17:30.000 --> 02:17:32.000]   Holden Karnofsky which incidentally
[02:17:32.000 --> 02:17:34.000]   people should just read anyway I think it's just really
[02:17:34.000 --> 02:17:36.000]   truly excellent and kind of remarkable
[02:17:36.000 --> 02:17:38.000]   that one of the most
[02:17:38.000 --> 02:17:40.000]   affecting
[02:17:40.000 --> 02:17:42.000]   series of
[02:17:42.000 --> 02:17:44.000]   blog posts I've basically ever
[02:17:44.000 --> 02:17:46.000]   read was written by
[02:17:46.000 --> 02:17:48.000]   the co-CEO of this enormous
[02:17:48.000 --> 02:17:52.000]   philanthropic
[02:17:52.000 --> 02:17:54.000]   organization in his spare time
[02:17:54.000 --> 02:17:56.000]   it's just kind of insane
[02:17:56.000 --> 02:17:58.000]   yeah so
[02:17:58.000 --> 02:18:00.000]   the website is
[02:18:00.000 --> 02:18:02.000]   effectiveideas.org
[02:18:02.000 --> 02:18:04.000]   yeah and then
[02:18:04.000 --> 02:18:06.000]   obviously
[02:18:06.000 --> 02:18:08.000]   where can people find you
[02:18:08.000 --> 02:18:10.000]   so your website, twitter handle and then
[02:18:10.000 --> 02:18:12.000]   where can people find your podcast
[02:18:12.000 --> 02:18:14.000]   oh yeah so
[02:18:14.000 --> 02:18:16.000]   website is myname.com
[02:18:16.000 --> 02:18:18.000]   twitter is my name
[02:18:18.000 --> 02:18:20.000]   and podcast
[02:18:20.000 --> 02:18:22.000]   is called hear this idea
[02:18:22.000 --> 02:18:24.000]   as in listen this idea so it's just
[02:18:24.000 --> 02:18:26.000]   thatphrase.com and I'm sure
[02:18:26.000 --> 02:18:28.000]   if you kind of google it it'll come up
[02:18:28.000 --> 02:18:30.000]   but by the way what is your
[02:18:30.000 --> 02:18:32.000]   probably distribution of how impactful
[02:18:32.000 --> 02:18:34.000]   these criticisms
[02:18:34.000 --> 02:18:36.000]   end up being or just how
[02:18:36.000 --> 02:18:38.000]   good they end up being like if you had to guess
[02:18:38.000 --> 02:18:40.000]   what is like your median outcome
[02:18:40.000 --> 02:18:42.000]   and then what is like your 99th or
[02:18:42.000 --> 02:18:44.000]   90th percentile outcome of how good these end up being
[02:18:44.000 --> 02:18:46.000]   yeah okay that's a good
[02:18:46.000 --> 02:18:48.000]   question I feel like
[02:18:48.000 --> 02:18:50.000]   I want to say that doing this stuff
[02:18:50.000 --> 02:18:52.000]   is really hard
[02:18:52.000 --> 02:18:54.000]   so I don't want to
[02:18:54.000 --> 02:18:56.000]   discourage posting by saying this
[02:18:56.000 --> 02:18:58.000]   but I think you know maybe the median
[02:18:58.000 --> 02:19:00.000]   submission is
[02:19:00.000 --> 02:19:02.000]   you know like really robustly useful
[02:19:02.000 --> 02:19:04.000]   absolutely worth writing and
[02:19:04.000 --> 02:19:06.000]   submitting that said maybe the difference
[02:19:06.000 --> 02:19:08.000]   between the most valuable posts
[02:19:08.000 --> 02:19:10.000]   of this kind or work
[02:19:10.000 --> 02:19:12.000]   of this kind and the median
[02:19:12.000 --> 02:19:14.000]   kind of effort is probably very large which is
[02:19:14.000 --> 02:19:16.000]   to say that the ceiling is
[02:19:16.000 --> 02:19:18.000]   really high if you think you have
[02:19:18.000 --> 02:19:20.000]   a 1% chance of
[02:19:20.000 --> 02:19:22.000]   influencing 100 million dollars
[02:19:22.000 --> 02:19:24.000]   of philanthropic spending
[02:19:24.000 --> 02:19:26.000]   then there is some sense in which
[02:19:26.000 --> 02:19:28.000]   a you know
[02:19:28.000 --> 02:19:30.000]   impartial philanthropic donor might
[02:19:30.000 --> 02:19:32.000]   be willing to spend
[02:19:32.000 --> 02:19:34.000]   roughly 1% of that amount
[02:19:34.000 --> 02:19:36.000]   to kind of find out that information right which is like
[02:19:36.000 --> 02:19:38.000]   a million dollars so yeah
[02:19:38.000 --> 02:19:40.000]   this stuff can be like really really
[02:19:40.000 --> 02:19:42.000]   important I think yeah
[02:19:42.000 --> 02:19:44.000]   yeah okay excellent yeah so the
[02:19:44.000 --> 02:19:46.000]   stuff you're working on seems really interesting
[02:19:46.000 --> 02:19:48.000]   and the blog posts seem
[02:19:48.000 --> 02:19:50.000]   like they might have a
[02:19:50.000 --> 02:19:52.000]   potentially very big impact I mean our
[02:19:52.000 --> 02:19:54.000]   world views have been shaped so much by some of these
[02:19:54.000 --> 02:19:56.000]   bloggers we've talked about so
[02:19:56.000 --> 02:19:58.000]   yeah if this leads
[02:19:58.000 --> 02:20:00.000]   to one more of those that alone
[02:20:00.000 --> 02:20:02.000]   could be very valuable so
[02:20:02.000 --> 02:20:04.000]   Finn thanks so much for coming on the podcast this was
[02:20:04.000 --> 02:20:06.000]   the longest but also one of the most fun
[02:20:06.000 --> 02:20:08.000]   conversations I've gotten a chance to do
[02:20:08.000 --> 02:20:10.000]   the whole thing was so much fun
[02:20:10.000 --> 02:20:12.000]   thanks so much for having me
[02:20:12.000 --> 02:20:14.000]   thanks for watching I hope you enjoyed
[02:20:14.000 --> 02:20:16.000]   that episode if you did
[02:20:16.000 --> 02:20:18.000]   and you want to support the podcast
[02:20:18.000 --> 02:20:20.000]   the most helpful thing you can do
[02:20:20.000 --> 02:20:22.000]   is share it on social media
[02:20:22.000 --> 02:20:24.000]   and with your friends other than
[02:20:24.000 --> 02:20:26.000]   that please like and subscribe
[02:20:26.000 --> 02:20:28.000]   on YouTube and leave
[02:20:28.000 --> 02:20:30.000]   good reviews on podcast platforms
[02:20:30.000 --> 02:20:32.000]   cheers I'll see you next time
[02:20:32.000 --> 02:20:42.000]   [Music]

