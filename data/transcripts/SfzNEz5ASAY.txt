
[00:00:00.000 --> 00:00:13.240]   All right. Hey, everybody. Welcome back to week eight of Fastbook Reading Group. I don't
[00:00:13.240 --> 00:00:17.400]   know why I said that in a robotic voice. But anyway, welcome back to week eight of the
[00:00:17.400 --> 00:00:22.140]   Fastbook Reading Group. And I'm so excited for today. Today we're going to be wrapping
[00:00:22.140 --> 00:00:28.980]   up chapter six and we're going to do live coding. I always dread live coding. So if
[00:00:28.980 --> 00:00:34.840]   things don't go well, bear with me, but I'll try my best to do the best of my ability.
[00:00:34.840 --> 00:00:40.200]   And then we'll be using fast.ai for Kaggle's Cassava classification competition. So we'll
[00:00:40.200 --> 00:00:45.520]   learn what the competition is. We'll look at the problem statement and then we'll use
[00:00:45.520 --> 00:00:52.440]   fast.ai to find a solution for it or to build a model that can do leaf classification. The
[00:00:52.440 --> 00:00:56.840]   main idea, like it wasn't planned. It's just something seven or eight hours ago, it just
[00:00:56.840 --> 00:01:04.120]   occurred to me that, um, yeah, it just occurred to me that this week is going to be a little
[00:01:04.120 --> 00:01:10.520]   bit different. Like instead of going week after week, chapter after chapter, I felt
[00:01:10.520 --> 00:01:17.680]   that this is a good time to take a break and apply what we've learned. Because that's really
[00:01:17.680 --> 00:01:22.840]   important. I think in your journeys as deep learning practitioners, applying what you've
[00:01:22.840 --> 00:01:28.960]   learned so far will bolster the understanding. It will make the understanding even better.
[00:01:28.960 --> 00:01:38.240]   And when we apply things to a new problem, we face challenges that we wouldn't have otherwise.
[00:01:38.240 --> 00:01:43.520]   And then in trying to find solutions to those challenges is where like the real growth and
[00:01:43.520 --> 00:01:50.640]   the real learning sort of happens. Because then when we're going to use cross entropy
[00:01:50.640 --> 00:01:55.760]   loss now, or like we're going to try and build the data block API, or we're going to build
[00:01:55.760 --> 00:02:01.240]   data loaders for a new competition, then we're going to have a deeper understanding of each
[00:02:01.240 --> 00:02:08.840]   of those concepts. And that's the idea. So that's why we're going to wrap up regression.
[00:02:08.840 --> 00:02:13.320]   And that's chapter six. That's what was left. And then we're going to look at the Kaggle
[00:02:13.320 --> 00:02:20.120]   competition. So I think we'll have about 30 or 40 minutes of Kaggle live coding. So
[00:02:20.120 --> 00:02:30.880]   we'll see how that goes. Fingers crossed. As is usual, if you go to that link, 1db.me/fastbook8.
[00:02:30.880 --> 00:02:46.000]   So let me post that in the chat. 1db.me/fastbook8. So if I go to that link, that should take
[00:02:46.000 --> 00:02:52.360]   me one second. I'm just bringing my browser. Can everybody see my browser? Okay. Could
[00:02:52.360 --> 00:03:02.800]   you maybe just give me a thumbs up in the chat? If the screen's too zoomed out or like
[00:03:02.800 --> 00:03:07.520]   if it's hard for you to read text, could you maybe also post in the chat that it's hard
[00:03:07.520 --> 00:03:17.080]   to read or zoomed out? Yes, a bit. Okay. How's that? Is that better? Is that perfect for
[00:03:17.080 --> 00:03:31.600]   everybody? Bigger, please. Okay. How about that? Okay. Excellent. Cool. So we'll keep
[00:03:31.600 --> 00:03:36.760]   at that. What is that? 1.3%. All right. We'll use that zoom just because the screen resolution
[00:03:36.760 --> 00:03:44.320]   is a bit bigger. So it's hard for me to see how other people see the screen. Okay. So
[00:03:44.320 --> 00:03:49.680]   that's where we'll go. So any questions that you have, we're going to -- my focus then
[00:03:49.680 --> 00:03:53.680]   today -- I'm really excited for what's coming next and especially the Kaggle live coding
[00:03:53.680 --> 00:03:59.560]   part. So I'm going to try and wrap up regression pretty quickly and then I'm going to go to
[00:03:59.560 --> 00:04:06.560]   Kaggle Cassava very quickly. But then before we do any of that, this week has been super
[00:04:06.560 --> 00:04:14.080]   amazing. Yeah, something I did want to say is like a big thank you to everybody who's
[00:04:14.080 --> 00:04:23.160]   been writing blog posts week after week. And the one thing I feel is like the main idea
[00:04:23.160 --> 00:04:29.800]   for starting this Fastbook reading group was to make sure that whoever's on this journey
[00:04:29.800 --> 00:04:35.520]   starts this journey. But as Jeremy also said in the very first session, like the beginning,
[00:04:35.520 --> 00:04:40.320]   which we called Fastbook session zero, when Jeremy was talking about Fastbook and he was
[00:04:40.320 --> 00:04:48.880]   talking about fast AI, what Jeremy said as well was that it's important to finish. And
[00:04:48.880 --> 00:04:53.840]   that was the idea for me as well. We -- like being on this journey, there's a lot of time
[00:04:53.840 --> 00:04:59.920]   and energy that's spent in making these weekly sessions happen. But when I see the feedback
[00:04:59.920 --> 00:05:04.760]   loop or when I see like people going and writing good things and then understanding like cross
[00:05:04.760 --> 00:05:11.160]   entropy loss, tweets being retweeted by Jeremy, tweets being loved by the fast AI community,
[00:05:11.160 --> 00:05:16.600]   Zach Mueller being involved or like Sanyam being involved or like basically -- I'm sorry
[00:05:16.600 --> 00:05:21.960]   if I missed anybody's name, but the idea is not to name names, but mostly to say like
[00:05:21.960 --> 00:05:26.640]   it's really lovely to see the whole community being -- coming together and being a part
[00:05:26.640 --> 00:05:34.840]   of this. So thanks everybody who's been writing. And I hope that by writing, you're also benefiting
[00:05:34.840 --> 00:05:43.080]   from it. Like what I do feel is if you're on week seven and someone who wants to go
[00:05:43.080 --> 00:05:48.600]   back to week two, then you have your own notes which could be referenced. So I think that's
[00:05:48.600 --> 00:05:53.520]   a really powerful thing to have is to have your own notes handy. And then it also makes
[00:05:53.520 --> 00:06:00.760]   it a really powerful thing to be active in public. So publicly sharing your work is -- definitely
[00:06:00.760 --> 00:06:06.440]   leads to job opportunities, definitely leads to really good things happening. So that was
[00:06:06.440 --> 00:06:11.640]   just a small thing that I did want to say before sharing everybody's work. So Ravi,
[00:06:11.640 --> 00:06:16.200]   thanks for coming back this week again and writing about the deep learning for Codice
[00:06:16.200 --> 00:06:23.440]   chapter five. And really good quick recap on that one. Anand, loved your blog. It was
[00:06:23.440 --> 00:06:27.920]   shared just very recently before we started this session, so I couldn't read the whole
[00:06:27.920 --> 00:06:32.880]   of it. But whatever I read, I really enjoyed it. So thanks for sharing your work about
[00:06:32.880 --> 00:06:41.440]   and providing a summary of chapter five. Akash, I did read this blog as well. And loved the
[00:06:41.440 --> 00:06:46.640]   way how you used math. I even shared this on Twitter that this is a really good summary
[00:06:46.640 --> 00:06:54.080]   of SGD, Nesterov momentum, and basically gradient descent in general. And this was really lovely
[00:06:54.080 --> 00:07:03.240]   to see some nice figures coupled with a bit of math. I would also add a bit of code in
[00:07:03.240 --> 00:07:08.160]   there which would then make this the only place where people have to be. So then your
[00:07:08.160 --> 00:07:14.360]   blog's going to be complete. Nobody else has to go anywhere else. But really loved this
[00:07:14.360 --> 00:07:20.560]   blog. Thanks for sharing. Thanks, Niazi, coming back this week and writing about chapter five.
[00:07:20.560 --> 00:07:28.720]   I loved the way you write and thanks for summarizing everything that we covered last week. Kareem,
[00:07:28.720 --> 00:07:32.360]   you're standard. You come here every week after week and I only have to say thank you
[00:07:32.360 --> 00:07:38.640]   for coming back this week again and writing about chapter five and then sharing your learning
[00:07:38.640 --> 00:07:46.960]   logs with us week after week. Vinayak, I have to say I was really impressed by Vinayak's
[00:07:46.960 --> 00:07:55.360]   blog on the learning rate finder. I think he's gone in and he's basically shared his
[00:07:55.360 --> 00:08:01.760]   experimentation about the various learning rates which I kind of really enjoyed reading.
[00:08:01.760 --> 00:08:07.280]   It's not just about a theoretical blog that explains what things are but it's also more
[00:08:07.280 --> 00:08:14.200]   like a -- it has experimentation and it has really good things. And this one I think I
[00:08:14.200 --> 00:08:19.440]   did share on Twitter. I just wanted to quickly point out that I -- I'll bring up Twitter
[00:08:19.440 --> 00:08:25.840]   very quickly. And Zach I think pointed out to some resources as well. So this is something
[00:08:25.840 --> 00:08:30.440]   I did want to point out is that Zach has said that he's made some updates to the learning
[00:08:30.440 --> 00:08:35.760]   rate finder. And in fact, the new learning rate finder looks a tiny bit different. So
[00:08:35.760 --> 00:08:41.840]   as you can see, now it has all of these LR finder, Valley, Slide, Minimums, Deep. So
[00:08:41.840 --> 00:08:46.920]   in case Vinayak you want to go back and update your blog with this latest version of learning
[00:08:46.920 --> 00:08:52.480]   rate finder, feel free to reach out to me or Zach if you have any questions. But again,
[00:08:52.480 --> 00:08:58.920]   thanks for -- thanks for this wonderful blog. So that's it in terms of sharing the work.
[00:08:58.920 --> 00:09:19.040]   So let's then get straight into chapter -- chapter 6. One second. All right. Here it is. Chapter
[00:09:19.040 --> 00:09:25.480]   6, Multicat. So where we left off last week, I think this is going to be super, super cool.
[00:09:25.480 --> 00:09:30.600]   Again, I just want to say I'm not reading -- I can see there's plenty of messages on
[00:09:30.600 --> 00:09:39.240]   Zoom. If there's anything that you want me to read, please post in the 1db.me/fastbook8
[00:09:39.240 --> 00:09:44.200]   as I'm not going to be following the Zoom chat. So with that being said, let's get started.
[00:09:44.200 --> 00:09:50.360]   So we -- where we finished last week was we were looking at the multilabel classification
[00:09:50.360 --> 00:09:58.080]   problem. We built our data set and by the time we ended up, we looked at how things
[00:09:58.080 --> 00:10:02.860]   are a little bit different for multilabel classification in the sense that they can
[00:10:02.860 --> 00:10:08.040]   be multiple objects or they can be multiple classes in a single image. And then we saw
[00:10:08.040 --> 00:10:15.160]   how -- this is where we finished. We finished at 1.1.4. We saw how the data block is different
[00:10:15.160 --> 00:10:19.320]   and this is how our data kind of looked like. You had file names that was shared as a part
[00:10:19.320 --> 00:10:25.000]   of a CSV. So there was some talk about pandas and then you have your labels and then there's
[00:10:25.000 --> 00:10:31.600]   this is valid column as well that says -- that basically defines whether your image is part
[00:10:31.600 --> 00:10:38.440]   of the validation set or not. So based on that, we built our getX, we built our getY,
[00:10:38.440 --> 00:10:44.080]   we built our data block. And then we kept on going and we added things one by one. So
[00:10:44.080 --> 00:10:49.400]   we added the blocks first. So this was basically building the data block step by step. Then
[00:10:49.400 --> 00:10:55.040]   we had a look at how the labels for multilabel are different. So you can see how in this
[00:10:55.040 --> 00:10:59.560]   case, like, there's only one that is one, but the main thing we looked at is how this
[00:10:59.560 --> 00:11:06.520]   is one hot encoded and this is a different way of representing the labels. And actually,
[00:11:06.520 --> 00:11:10.800]   let me run this notebook. I haven't run this notebook. So it's running now. And then we
[00:11:10.800 --> 00:11:20.760]   saw how -- we saw how the labels are multilabel. So let me see if there's another example somewhere.
[00:11:20.760 --> 00:11:25.000]   I'm just trying to find an example where two of these are one. Here it is. So we saw how
[00:11:25.000 --> 00:11:30.520]   this is different. Like, it has this category as being one. So that's the label that's present
[00:11:30.520 --> 00:11:34.880]   in the image. And this category as being one. We don't know what categories those are right
[00:11:34.880 --> 00:11:40.760]   now. But datasets have their own vocab that we also looked at. So that was the main difference
[00:11:40.760 --> 00:11:46.760]   and that's why it is one hot encoded. Because remember, we looked at how if we present this
[00:11:46.760 --> 00:11:51.880]   as numbers like three or five, then each of the labels would have different lengths. But
[00:11:51.880 --> 00:11:55.760]   one hot encoding makes sure that the lengths are the same for every label. So that's the
[00:11:55.760 --> 00:11:59.740]   main things. Then we looked at, okay, what category -- we looked at a torch.where function
[00:11:59.740 --> 00:12:06.600]   that kind of tells us what category or, like, which index the thing -- basically the tensor
[00:12:06.600 --> 00:12:12.720]   is value of one. And based on that, we can grab our indexes. We had a look at the vocab.
[00:12:12.720 --> 00:12:17.040]   So now you can pass in the index in your vocab and you can see, okay, that's the one that's
[00:12:17.040 --> 00:12:24.280]   one. So let's keep going. So let me run those bits as well. So you can see how datasets.train
[00:12:24.280 --> 00:12:28.360]   looks like this. And then you can pretty much -- I'm just going to run those. This is just
[00:12:28.360 --> 00:12:33.040]   a recap. So this is a different thing. Then we built our data block. We saw how having
[00:12:33.040 --> 00:12:38.600]   a splitter is different. We added our own splitter this time. So I'm just going to run
[00:12:38.600 --> 00:12:45.440]   that as well. And that was it. And then we finished building our data block using our
[00:12:45.440 --> 00:12:50.440]   own custom splitter that uses the is valid column from the data frame, does the random
[00:12:50.440 --> 00:12:57.240]   resize cropping as an item transform. And we can look at batches. So this is what the
[00:12:57.240 --> 00:13:03.720]   batch looks like. So if I have another row, here we are. So this image has -- it's a really
[00:13:03.720 --> 00:13:07.640]   bad image. Let's see if there's another image that shows. Okay. This one. So this image
[00:13:07.640 --> 00:13:12.920]   has a dog and a person. So it's like two labels instead of being one. But other images have
[00:13:12.920 --> 00:13:21.600]   like a bird and dog. So that's where we finished. So as -- let me see if I can bring up one
[00:13:21.600 --> 00:13:27.240]   node. Sorry. Because I'm using two screens. One second. I'll have to -- I just need to
[00:13:27.240 --> 00:13:31.440]   duplicate my screen. Because if I write on one node, it won't show up in the screen I'm
[00:13:31.440 --> 00:13:47.240]   sharing. So one second, please. All right. That's not what you want to see. Screen sharing
[00:13:47.240 --> 00:14:00.520]   has stopped because external display is disconnected. One second. I'm doing that again. Every time
[00:14:00.520 --> 00:14:08.920]   I do this, there's some issues. So I do need to update the way I share my screen. Sorry.
[00:14:08.920 --> 00:14:28.400]   Stay with me. One second, please. Okay. That's good. Next is just need to now be able to
[00:14:28.400 --> 00:14:38.160]   share my screen. So share screen. Okay. Can everybody see my screen again? Could you please
[00:14:38.160 --> 00:14:44.160]   maybe just respond in the Zoom chat that you can? Perfect. And we're back. Awesome. So
[00:14:44.160 --> 00:14:51.560]   where we -- I lost my train of thought, but I was talking about multilabel classification.
[00:14:51.560 --> 00:15:00.580]   So let's go here. Now you should be able to see my one node. And then if I go to Fastbook,
[00:15:00.580 --> 00:15:04.840]   and then I can probably scribble on my screen and it will show up. Excellent. We're good
[00:15:04.840 --> 00:15:14.400]   now. Okay. So then the thing I want to talk about now is basically then the difference
[00:15:14.400 --> 00:15:23.080]   in multilabel classification is you have your labels that are one-hot encoded. So let's
[00:15:23.080 --> 00:15:29.800]   say that's how my label looks like. So let's say I have just one, two, three, four, five
[00:15:29.800 --> 00:15:44.360]   categories, which could be -- my five categories could be dog, cat, horse, I don't know, fish
[00:15:44.360 --> 00:15:50.320]   and human. Let's say those are my five categories. This is -- we don't have to have numbers,
[00:15:50.320 --> 00:15:55.520]   but then you can say, okay, this one is here. Cat is the second one. Horse is the third.
[00:15:55.520 --> 00:16:03.080]   Because I'm one-hot encoding them, right? So let's say I have an image where a dog is
[00:16:03.080 --> 00:16:10.080]   present and a horse is present. So then the label for this would look like dog, yes. Cat,
[00:16:10.080 --> 00:16:19.320]   no. Horse, yes. Fish, no. Human, no. Right? That's how the label for this would look like.
[00:16:19.320 --> 00:16:24.800]   And now I want you to think of how the loss function for this image could look like. Because
[00:16:24.800 --> 00:16:33.960]   what's going to happen is when we pass this image through my model, right, it's going
[00:16:33.960 --> 00:16:39.200]   to make -- however many classes I have, it's going to make that many predictions as well.
[00:16:39.200 --> 00:16:44.960]   So it's going to make five predictions. So one, two, three, four, and five. And the predictions
[00:16:44.960 --> 00:16:49.160]   are going to be like -- because it's not softmax or it's not sigmoid or nothing, it's going
[00:16:49.160 --> 00:16:55.400]   to be like just floats, minus .7, 1.2, or so on. And that's how the predictions are
[00:16:55.400 --> 00:17:03.640]   going to look like. And I need to find a loss function that can then compare -- one second.
[00:17:03.640 --> 00:17:09.160]   That's again. And I need to find a loss function that can then compare -- I'm calling these
[00:17:09.160 --> 00:17:16.240]   as my -- those are my labels. And then on the right are my preds, or my predictions.
[00:17:16.240 --> 00:17:21.040]   And I need to find a loss function that can then compare my predictions to my labels.
[00:17:21.040 --> 00:17:28.200]   Ideally, what we want our model to do is have this one as a high number. So high, this one
[00:17:28.200 --> 00:17:32.080]   as a low number, and then this one as a high number. Because see, those are the corresponding
[00:17:32.080 --> 00:17:36.800]   labels. So those are the indexes where this thing is one. And we can't really use cross
[00:17:36.800 --> 00:17:43.800]   entropy loss, because remember, cross entropy loss uses log softmax, or the negative log
[00:17:43.800 --> 00:17:48.920]   likelihood, which we covered before. And it's trying to pick one category. Like, that's
[00:17:48.920 --> 00:17:53.880]   really good for one category, but not multiple categories. So then how about this? Remember
[00:17:53.880 --> 00:18:01.000]   when we were doing our -- how about this? How about defining a loss function as this?
[00:18:01.000 --> 00:18:09.800]   So I'm just going to quickly just put my labels and predictions together. One, two, three,
[00:18:09.800 --> 00:18:13.920]   four, five, six, seven, eight, nine, 10, 1, 0, 1, 0, 0. Those are my labels. And then
[00:18:13.920 --> 00:18:21.240]   I have some preds, 1.2, minus 7.1. I don't know, just some random floats, right? And
[00:18:21.240 --> 00:18:30.760]   I need to be able to. So that's my label. And that's my pred. So how about a loss function?
[00:18:30.760 --> 00:18:37.440]   Because we want these numbers to be high, and then the other ones to be low. So we want
[00:18:37.440 --> 00:18:45.600]   a loss function. How about a loss function that says, at places where this is 1, we want
[00:18:45.600 --> 00:18:53.640]   the prediction. First of all, let's apply a sigmoid, right? What will a sigmoid do?
[00:18:53.640 --> 00:18:57.920]   We've already looked at a sigmoid. A sigmoid will convert all of these numbers between
[00:18:57.920 --> 00:19:03.440]   0 and 1. So the higher the number, the closer it is to 1. So let's say it makes like 0.8,
[00:19:03.440 --> 00:19:13.160]   0.01. I don't know, I'm just randomly making these up. Maybe this is like 0.6, and then
[00:19:13.160 --> 00:19:18.560]   this is again like a 0.8, right? And then, so the first step I do in my loss is let's
[00:19:18.560 --> 00:19:25.800]   say I apply my sigmoid to the model predictions. And then instead of now comparing, I don't
[00:19:25.800 --> 00:19:39.840]   care about like just my raw logits, I'm going to compare my label with this sigmoid version
[00:19:39.840 --> 00:19:45.780]   of my predictions. So I'm just going to call this a sigmoid. So I'm going to compare those
[00:19:45.780 --> 00:19:50.720]   two instead of comparing my label and prediction. And what my loss function is going to say
[00:19:50.720 --> 00:19:56.880]   is how about a loss function that says places where-- sorry, I need to let me try and use
[00:19:56.880 --> 00:20:04.880]   a different color-- places where this is 1, correspondingly, this should be close to 1,
[00:20:04.880 --> 00:20:12.240]   right? So how about I say that I want the loss to be 1 minus whatever the number is
[00:20:12.240 --> 00:20:20.360]   at these positions. Because I want them to be as close to 1 as possible. This is the
[00:20:20.360 --> 00:20:24.600]   MNIST loss. Remember when we were talking about MNIST and we were trying to find a loss
[00:20:24.600 --> 00:20:30.160]   function to classify 3s and 7s? Then we talked about this like distance. So when I'm saying,
[00:20:30.160 --> 00:20:35.840]   OK, I want this number to be as close as 1. And if you're not 1, then my loss in this
[00:20:35.840 --> 00:20:42.560]   case would be 1 minus that number. So in this case, it would be 1 minus 0.8. And for this
[00:20:42.560 --> 00:20:47.200]   one, it would be 1 minus 0.5. So that's the loss for this one, and it's the loss for this
[00:20:47.200 --> 00:20:53.120]   element. And for everything everywhere else where this is 0-- so let me use a different
[00:20:53.120 --> 00:20:59.440]   color-- so everywhere else where this is 0, how about then in that case, the loss is just
[00:20:59.440 --> 00:21:04.480]   whatever this number is? Because we want them to be as close to 0. So 0.01 minus 0 is just
[00:21:04.480 --> 00:21:11.680]   0.01. So the loss in this case is 0.01, 0.6, 0.8. And now what the model is going to try
[00:21:11.680 --> 00:21:17.200]   and do-- so see how this is my loss on the right? And now what the model is going to
[00:21:17.200 --> 00:21:23.520]   try and do is when we're going to apply gradient descent on this loss function, it's going
[00:21:23.520 --> 00:21:28.240]   to try and make this number close to 1, like these two. And it's going to try and make
[00:21:28.240 --> 00:21:34.640]   every other number close to 0, or like smaller numbers. So that's exactly what we want. So
[00:21:34.640 --> 00:21:40.580]   this is how we could train multilabel classification. And that's how the loss function looks like.
[00:21:40.580 --> 00:21:47.540]   And what I've just explained is the binary cross-entropy loss. So you can see how this
[00:21:47.540 --> 00:21:53.320]   is what exactly you'll see in binary cross-entropy when you read this. You'll see-- because we
[00:21:53.320 --> 00:22:00.520]   have-- let me just show this in code. So I create my learner. It's giving me some warning,
[00:22:00.520 --> 00:22:06.520]   but ignore that. I can see how my activation-- so the model-- because I have 20 classes.
[00:22:06.520 --> 00:22:10.900]   In this example, I just had five classes. So my model output a vector of however many
[00:22:10.900 --> 00:22:16.600]   times 5. In this case, because I have-- 64 is my batch size. So when you're rerunning
[00:22:16.600 --> 00:22:23.880]   the notebook, I would suggest that it's really important that NC-- why these other shapes
[00:22:23.880 --> 00:22:28.840]   that they are in. And then it says, OK, 20-- that means 20 outputs. And then if I have
[00:22:28.840 --> 00:22:33.600]   a look at my outputs, these look like these random logits or just the random outputs,
[00:22:33.600 --> 00:22:38.600]   as I said in this case, the predictions. And then if you have a look at the labels, then
[00:22:38.600 --> 00:22:42.880]   these are the labels that-- that's how they look like. And then we could define a binary
[00:22:42.880 --> 00:22:46.720]   cross-entropy loss function like this. We could first take the sigmoid and then use
[00:22:46.720 --> 00:22:53.320]   the torch.where, wherever the targets are 1. In that case, the loss is 1 minus. My input
[00:22:53.320 --> 00:22:56.960]   and the other places where it is 0, it's just the number itself. And then we could take
[00:22:56.960 --> 00:23:00.600]   the log, and we could take the mean. And that becomes your binary cross-entropy loss. So
[00:23:00.600 --> 00:23:06.880]   we could train a model. Again, PyTorch has the binary cross-entropy loss function. So
[00:23:06.880 --> 00:23:12.000]   you could just-- there's two ways of using the binary cross-entropy loss in PyTorch.
[00:23:12.000 --> 00:23:20.520]   You will see PyTorch does things slightly bit differently. So PyTorch has PyTorch NN
[00:23:20.520 --> 00:23:24.280]   functional, where that's like a-- where the things are like functions that can be called
[00:23:24.280 --> 00:23:35.000]   directly. Or it also has things as classes. So NN dot cross-entropy loss in PyTorch.
[00:23:35.000 --> 00:23:39.520]   So that's the class. Either you can use it as the class, or you can use it as a function.
[00:23:39.520 --> 00:23:45.960]   It's not a very big-- it's not going to be like a massive thing to learn. But just when
[00:23:45.960 --> 00:23:50.840]   you go back, have a read of this paragraph, and it will just say, OK, that's how-- you
[00:23:50.840 --> 00:23:54.080]   can do the same thing in different ways in PyTorch.
[00:23:54.080 --> 00:23:57.520]   So the most common way of doing it is using the class version. So I'm just going to call
[00:23:57.520 --> 00:24:04.020]   that that. And we can calculate a loss. I don't have my inputs and my outputs defined.
[00:24:04.020 --> 00:24:11.760]   So where is my input and target? Are they even defined somewhere? Did I miss something?
[00:24:11.760 --> 00:24:20.080]   Yes, I did. I did miss my input and target. So ignore that. Ignore that. OK. So then I
[00:24:20.080 --> 00:24:29.600]   could just-- when I'm training CNN Learner, I can-- my CNN Learner, in this case-- don't
[00:24:29.600 --> 00:24:36.040]   worry about all of this stuff at the back now. I think I've just-- OK, one second. Actually,
[00:24:36.040 --> 00:24:45.520]   worry about it. What I'm going to do is go to my Fastbook, and I'm going to reset. So
[00:24:45.520 --> 00:24:53.520]   what I'm doing now is I'm just-- any changes that I've made to my notebook, I'm kind of
[00:24:53.520 --> 00:24:57.280]   resetting them, because that's why the input and the target were not defined. So I'm just
[00:24:57.280 --> 00:25:02.520]   using an updated version of this notebook, which is not very helpful. So now Jupyter
[00:25:02.520 --> 00:25:08.480]   is going to load the correct version of this notebook. So that's kind of a sidetrack. What
[00:25:08.480 --> 00:25:16.080]   we've used is called Git. So you can have a look at Git and kind of look at that. So
[00:25:16.080 --> 00:25:24.040]   see now, we have the correct version, which is the original version. So where were we?
[00:25:24.040 --> 00:25:32.320]   We were looking at the binary cross-entropy loss. So we had looked at the activation shape.
[00:25:32.320 --> 00:25:36.160]   We had looked at how the activations look like. We had looked at how binary cross-entropy
[00:25:36.160 --> 00:25:42.160]   looks like. And now we can define our-- OK, we pass in actives and y. Got it. So you can
[00:25:42.160 --> 00:25:46.800]   pass in your-- basically, the model outputs, and you can pass in your labels. And then
[00:25:46.800 --> 00:25:50.360]   you can then calculate the loss. So we'll see how that loss is calculated. So that gives
[00:25:50.360 --> 00:26:16.320]   me my loss. Any questions on loss so far? Because next, we're going to look at metrics.
[00:26:16.320 --> 00:26:21.480]   I also wrote a small post about training a pet breed using what we learned. Oh, I'm really
[00:26:21.480 --> 00:26:36.000]   sorry. I missed that, Rabi. Let's see that. Building a state-of-the-art pet breed classifier.
[00:26:36.000 --> 00:26:39.280]   Thanks for that. I will definitely give this a read. Oh, you're using the latest version
[00:26:39.280 --> 00:26:46.880]   of LRFinder. That's amazing. Zach would be really happy to see this. Thanks for sharing.
[00:26:46.880 --> 00:26:52.480]   Which other loss function we can use for multiclass classification? I think go for binary cross-entropy
[00:26:52.480 --> 00:26:59.480]   loss is the way to go for multiclass classification. I'm not sure if there's any other loss function.
[00:26:59.480 --> 00:27:06.920]   It doesn't come to my head right now. So mean squared error, but it might not be as efficient
[00:27:06.920 --> 00:27:11.480]   as BC. Yeah, I'm not-- I haven't-- it doesn't come to my head straight away on any other
[00:27:11.480 --> 00:27:15.560]   loss function. Why would you want to use a different loss function if binary cross-entropy
[00:27:15.560 --> 00:27:17.560]   works?
[00:27:17.560 --> 00:27:25.120]   OK. So then the next thing is, let's have a look-- so now, when we want to calculate
[00:27:25.120 --> 00:27:33.240]   the accuracy, because now what's going to happen is-- let me, again, use my one note
[00:27:33.240 --> 00:27:41.800]   and bad handwriting to explain everything. Now my-- let's say I have still the five classes,
[00:27:41.800 --> 00:27:47.120]   and my predictions are going to look like this, right? 0.3, 0.8, because remember, I
[00:27:47.120 --> 00:27:57.280]   have applied softmax-- oh, sorry, sigmoid. So-- and my labels will say like this. 1,
[00:27:57.280 --> 00:28:05.520]   0, 1, 0, 0. So now what we want to do is we want to compare the accuracy between my predictions
[00:28:05.520 --> 00:28:11.840]   and the labels. And I couldn't just-- we can't use the accuracy that we looked at when we
[00:28:11.840 --> 00:28:17.480]   were doing single-label classification, because that just takes the argmax. So what that does
[00:28:17.480 --> 00:28:24.120]   is it will find the-- argmax just means find the point where the value is the highest,
[00:28:24.120 --> 00:28:30.560]   and then compare if-- or then you get the class ID, and then you compare the class ID
[00:28:30.560 --> 00:28:34.000]   with the actual label. So if the class IDs are the same, then your prediction is accurate.
[00:28:34.000 --> 00:28:38.120]   If they're not, then it's inaccurate. But in multiple-label classification, because
[00:28:38.120 --> 00:28:46.680]   you have multiple labels with one, what you have to do is you use something called a threshold.
[00:28:46.680 --> 00:28:52.040]   So you could have-- you could say-- you could define your own threshold, and the threshold
[00:28:52.040 --> 00:29:00.480]   could be maybe 0.5. So anything that's greater than 0.5 is 1, and anything that's less than
[00:29:00.480 --> 00:29:07.360]   0.5 is 0. So this then gets converted to 0. The second one, 0.3, gets converted to 0.
[00:29:07.360 --> 00:29:15.160]   0.8 gets converted to 1. This is 0, and this is 0. And now you can compare this with this
[00:29:15.160 --> 00:29:21.280]   one, and you can calculate the accuracy. So that's exactly how things are a bit different.
[00:29:21.280 --> 00:29:26.720]   So in accuracy multiple, you just have your inputs greater than threshold, which converts
[00:29:26.720 --> 00:29:31.080]   things into a Boolean, like 1 or 0. And then you can just compare to your target, and you
[00:29:31.080 --> 00:29:34.320]   can see how many times you're correct, and then you just take the mean. So that becomes
[00:29:34.320 --> 00:29:41.320]   your accuracy. This is just a side note about using partial functions. You'll find partial
[00:29:41.320 --> 00:29:47.280]   functions really handy when you're using deep learning in general, or just-- this is just
[00:29:47.280 --> 00:29:53.520]   a coding trick. You can have your argument as some default value, and then you can then
[00:29:53.520 --> 00:29:57.960]   apply that partial. I won't go much into the detail of partial. Have a read. It should
[00:29:57.960 --> 00:30:02.640]   be pretty straightforward. So then in metrics, you just pass your accuracy
[00:30:02.640 --> 00:30:07.320]   multi, and in this case, you're passing a threshold of 0.2, and away we go. We can train,
[00:30:07.320 --> 00:30:13.800]   we can validate, and we can then get our predictions. We can then create a graph, because we're
[00:30:13.800 --> 00:30:19.200]   trying to find a good threshold. So we can see at which threshold what my accuracy values
[00:30:19.200 --> 00:30:26.560]   are. So then I'm trying to find a good threshold, and that's basically it. So if you go back
[00:30:26.560 --> 00:30:29.320]   and read this, I think then that should be really clear.
[00:30:29.320 --> 00:30:34.920]   So before moving on to regression, I did say I'll try and wrap up regression in 20 minutes,
[00:30:34.920 --> 00:30:39.680]   but I spent 30 minutes explaining binary cross-entropy loss, so I'm really sorry for that. But let
[00:30:39.680 --> 00:30:45.280]   me just see if there's any questions on binary cross-entropy. By the way, I have a feeling
[00:30:45.280 --> 00:30:54.880]   that today's session is going to be a bit longer than 60 minutes, or maybe a bit longer
[00:30:54.880 --> 00:31:03.520]   than 90 minutes. We'll see. Is one-hot encoding used in loss function for binary cross-entropy?
[00:31:03.520 --> 00:31:13.320]   Yes. As in your labels are one-hot encoded. I hope this... This should have been clear.
[00:31:13.320 --> 00:31:16.920]   If it is not clear, then maybe I've made a mistake in trying to explain this. So see
[00:31:16.920 --> 00:31:24.640]   when I'm comparing my labels, my labels are one-hot encoded. What one-hot encoded means
[00:31:24.640 --> 00:31:31.640]   is you have them as 10100, wherever the label is one, it just means that then that category
[00:31:31.640 --> 00:31:38.720]   exists. But if it's not one-hot encoded, then you have IDs, like you have a horse represented
[00:31:38.720 --> 00:31:42.320]   by say class ID two, or like you basically have class ID. So that's the difference, but
[00:31:42.320 --> 00:31:48.640]   you're definitely using one-hot encoding. And then you compare your predictions to that
[00:31:48.640 --> 00:31:56.200]   using binary cross-entropy loss, which I explained. But, Korean, could you maybe reply to this
[00:31:56.200 --> 00:32:07.400]   comment and let me know that this is clear that we are using one-hot encoding? Because
[00:32:07.400 --> 00:32:13.040]   if this is still an issue, is this still an issue to people? Do you guys still not understand
[00:32:13.040 --> 00:32:23.760]   loss function? Because I hope it should be clear by now. Okay, I'm just going to give
[00:32:23.760 --> 00:32:32.120]   Korean some time. So then you can have your metrics as that accuracy multi with some threshold.
[00:32:32.120 --> 00:32:37.800]   And you can see how if I had a threshold of 0.1, my accuracy was 0.92. If I have a threshold
[00:32:37.800 --> 00:32:42.760]   of 0.99, my accuracy is 0.94. So you can see how it's important to pick up a threshold
[00:32:42.760 --> 00:32:48.680]   so we can get our predictions and we can get our targets. And then we can check the accuracy
[00:32:48.680 --> 00:32:54.040]   multi for different thresholds. So I can then create a simple loop that will loop through
[00:32:54.040 --> 00:32:59.440]   these different thresholds. And then I can plot this chart that says, okay, for which
[00:32:59.440 --> 00:33:06.280]   threshold, what's my accuracy? So the nicest threshold seems to be 0.6, where you get 96.5
[00:33:06.280 --> 00:33:12.240]   around that mark accuracy. So let's actually put it here and see what that is. 96.4. Yeah,
[00:33:12.240 --> 00:33:17.760]   96.5. So that's the accuracy. So that's how you pick a good threshold in multilabel classification.
[00:33:17.760 --> 00:33:25.520]   Cool. That's it for multilabel classification. Now let's go into regression. So what's the
[00:33:25.520 --> 00:33:31.760]   difference in regression? When we say regression, what's the difference in regression? And you
[00:33:31.760 --> 00:33:37.080]   would have seen regression mostly in terms of linear regression or you would have used
[00:33:37.080 --> 00:33:44.920]   in terms of machine learning. Not deep learning is not very common, although that is changing
[00:33:44.920 --> 00:33:51.680]   still, but it's still far behind in terms of computer vision and NLP. So the dataset
[00:33:51.680 --> 00:33:58.440]   that we're using in this one is this bvconnect headpost dataset. So I'm just going to show
[00:33:58.440 --> 00:34:02.320]   you what it means. Actually, while that opens, it's much easier to just go through this.
[00:34:02.320 --> 00:34:08.320]   So let me just untar that. This is going to download the dataset. That dataset has all
[00:34:08.320 --> 00:34:14.680]   of these different folders and then some objects. And then you can see how there's all of these
[00:34:14.680 --> 00:34:19.600]   different files in the dataset, the details of which aren't as important. But the idea
[00:34:19.600 --> 00:34:27.560]   is in this dataset, the dataset basically looks like this. You have images of people
[00:34:27.560 --> 00:34:35.420]   or basically like facial images. And then in this dataset, the task is you're trying
[00:34:35.420 --> 00:34:42.200]   to predict the center of the face. So you're trying to predict coordinates instead of like,
[00:34:42.200 --> 00:34:49.920]   yes, no. Basically, you're trying to predict a number. And when you're trying to predict
[00:34:49.920 --> 00:34:57.480]   a number, which is a float or like it doesn't matter, it's a regression problem. And in
[00:34:57.480 --> 00:35:01.480]   deep learning then, because until now, all we've done is we've done classification or
[00:35:01.480 --> 00:35:06.400]   we've looked at multi-table classification, but we haven't looked at how you could predict,
[00:35:06.400 --> 00:35:09.800]   how you could use the deep learning model to then predict number. But if you think of
[00:35:09.800 --> 00:35:19.520]   it, I'm just going to make a simple, I'm just going to say my model, depending on how many
[00:35:19.520 --> 00:35:27.000]   classes or whatever I do, I could classify using two classes. So one could be my x-coordinate,
[00:35:27.000 --> 00:35:34.960]   one could be my y-coordinate. And then the predictions themselves are always floats.
[00:35:34.960 --> 00:35:41.400]   The model's always predicting floats. Like it could be 101.2, this could be minus 7.3.
[00:35:41.400 --> 00:35:52.000]   Then I could treat these as coordinates. And if my actual coordinates are, say, 100 and
[00:35:52.000 --> 00:36:00.760]   say 50, then I could go y2 minus y1 squared plus x2 minus x1 squared, whole under the
[00:36:00.760 --> 00:36:08.520]   root, which is root mean square error. Or I could just use MSC without the square root.
[00:36:08.520 --> 00:36:17.560]   So that's just my mean square error. So I could calculate my loss function this way.
[00:36:17.560 --> 00:36:22.860]   The main idea is I want this number to be close to this number, and I want this number
[00:36:22.860 --> 00:36:27.860]   to be close to this number. So if I go like y2 minus y1 squared, I'm calculating whatever
[00:36:27.860 --> 00:36:31.760]   the difference between these two numbers is. And I'm going to try and minimize that difference.
[00:36:31.760 --> 00:36:38.520]   I'm just going to call it d, like the difference. And then I can, let me call it x delta, delta
[00:36:38.520 --> 00:36:44.920]   x. So delta x just stands for, if this is my x1 and this is my x2, then delta x is just
[00:36:44.920 --> 00:37:00.560]   x2 minus x1 squared. And then I could do the same thing for my y. So I have my y1, y2,
[00:37:00.560 --> 00:37:08.720]   and I could calculate my delta y, which is just y2. I'm just calling it, the mathematicians,
[00:37:08.720 --> 00:37:15.220]   please don't be angry with me. But I'm just trying to make things simple. So that's my
[00:37:15.220 --> 00:37:19.140]   difference. And what the loss function is going to try and do is it's going to try and
[00:37:19.140 --> 00:37:24.340]   minimize this difference. So when that happens, then you automatically, then you can train
[00:37:24.340 --> 00:37:32.100]   and use deep learning for regression problems as well. And that's the basic idea. There's
[00:37:32.100 --> 00:37:40.300]   not really anything that's different when you look at this FastAI notebook. So you'll
[00:37:40.300 --> 00:37:44.980]   see how there's some files. You can grab all the image files. So you have your image files
[00:37:44.980 --> 00:37:51.940]   and you can see there's the, basically your labels are part of that text file. You can
[00:37:51.940 --> 00:37:55.180]   create your image file and you can have a look at this image. That's how the image looks
[00:37:55.180 --> 00:38:01.500]   like. It's not really important on how you get the labels, but there's this, if you really
[00:38:01.500 --> 00:38:06.060]   want to understand how we're getting the labels, go and have a read of how the dataset is structured.
[00:38:06.060 --> 00:38:11.020]   And then you'll be able to see how the calculations of the center of the face or those coordinates
[00:38:11.020 --> 00:38:17.420]   can be calculated. FastAI book doesn't go into the details and I won't go into the details
[00:38:17.420 --> 00:38:23.360]   of this either, but you can basically pass a path to your image file. And then that function
[00:38:23.360 --> 00:38:29.500]   will give you your x and y coordinate. And that's it. That's the main thing. So you can
[00:38:29.500 --> 00:38:34.540]   use this as your get y because that becomes your label. So in this case, your label, instead
[00:38:34.540 --> 00:38:40.740]   of being like a category, it's just some number. My get items becomes my get image files. And
[00:38:40.740 --> 00:38:45.340]   of course I have my image block. And instead of using a category block in FastAI this time,
[00:38:45.340 --> 00:38:50.100]   I'm going to use something that's a point block. So FastAI, if you use a point block,
[00:38:50.100 --> 00:38:57.740]   FastAI will already know that instead of doing classification, you're trying to do regression.
[00:38:57.740 --> 00:39:03.220]   So you're basically going to predict numbers and you're going to do all of this stuff.
[00:39:03.220 --> 00:39:09.460]   And then the splitter in this case, what happens is because in the dataset you have images
[00:39:09.460 --> 00:39:18.580]   separated by folders. So the images are separated by folders. And then if you use a random splitter,
[00:39:18.580 --> 00:39:22.980]   then what's going to happen is your train set is going to have the image of the same
[00:39:22.980 --> 00:39:34.060]   person. And then if there's like, let's say, if there's like, say, five people in the dataset,
[00:39:34.060 --> 00:39:39.060]   one, two, three, four, five, and each one of them have, say, 10 images. So this is a
[00:39:39.060 --> 00:39:45.660]   list of my 10 images for the five. And if I use a random splitter, then it's going to
[00:39:45.660 --> 00:39:49.900]   find this in the validation set, this part in the validation set, this part in the validation
[00:39:49.900 --> 00:39:55.420]   set, this part in the validation set, or this part. But that's going to be much easier for
[00:39:55.420 --> 00:40:02.340]   the model because it would have already seen the images of that person around it. But when
[00:40:02.340 --> 00:40:06.860]   you deploy this model or you want to use this model in a real case scenario, you're pretty
[00:40:06.860 --> 00:40:11.580]   much going to have things like you're going to have your training set of like people or
[00:40:11.580 --> 00:40:18.620]   faces and then there's going to be, you're going to upload a new person's image. So it's
[00:40:18.620 --> 00:40:23.660]   much easier then to have your training set and validation splits split it this way. It's
[00:40:23.660 --> 00:40:27.780]   like your validation set is like a person with its own folder, which the model has never
[00:40:27.780 --> 00:40:33.420]   seen before. And that's what is happening here. You're just saying, okay, the folder
[00:40:33.420 --> 00:40:39.220]   name is 13. So don't use that folder in the training set. So my training set consists
[00:40:39.220 --> 00:40:45.980]   of people of different faces. And then my validation set is just that one person. So
[00:40:45.980 --> 00:40:52.700]   I can then now apply my transforms and that's pretty much it about regression. I haven't
[00:40:52.700 --> 00:41:02.500]   personally used fast AI for regression a lot or for point block, but recently I've started
[00:41:02.500 --> 00:41:10.260]   working on object detection and bounding box. In object, I'm just trying to explain the
[00:41:10.260 --> 00:41:18.380]   importance of what we've learned is like in an image, then not just, not only do you have
[00:41:18.380 --> 00:41:25.940]   to classify the label, but you also have to provide the coordinates of like X, Y, X1,
[00:41:25.940 --> 00:41:32.500]   Y1, X2, Y2, so on. You have to classify and provide these coordinates. And then in that
[00:41:32.500 --> 00:41:39.460]   case, what we learned today is when that will come in handy, but it's okay. Have a read
[00:41:39.460 --> 00:41:44.620]   of this. Then you can create your data loader. You can show batch. You can grab your first
[00:41:44.620 --> 00:41:50.140]   batch. You can see the shapes. When you're trying to run this back, have a look at why
[00:41:50.140 --> 00:41:57.780]   the shapes are what they are. If you can't figure it out, ask on Slack channel, but then
[00:41:57.780 --> 00:42:02.260]   you can see, okay, that's my label. And then you can train your models straight away. You
[00:42:02.260 --> 00:42:06.300]   have your scene and learner and you can say, okay, the outputs, please make the outputs
[00:42:06.300 --> 00:42:13.220]   to be in between this range. Fast.ai internally uses this function called sigmoid range. It
[00:42:13.220 --> 00:42:20.700]   does ask like, why does this function convert like whatever my outputs are into the range
[00:42:20.700 --> 00:42:26.420]   that I defined between minus one and one. I know I'm going very quickly over this section.
[00:42:26.420 --> 00:42:34.100]   I didn't mean to explain this part in detail, but I'm also trying to save time for the Cassava
[00:42:34.100 --> 00:42:38.580]   classification competition. So I'm just going to leave this with you guys have a read of
[00:42:38.580 --> 00:42:43.220]   this part of the section. It should fairly, it should be fairly simple and it should be
[00:42:43.220 --> 00:42:48.780]   like what we've covered so far. There shouldn't be anything different in this part of the
[00:42:48.780 --> 00:42:52.420]   notebook. And then you can just train your model and you can see your results. And then
[00:42:52.420 --> 00:42:59.740]   the models really train pretty well. That's chapter six. I know I didn't go in much detail
[00:42:59.740 --> 00:43:04.780]   in the last part of the notebook, but that's intentional. So do that in your own time,
[00:43:04.780 --> 00:43:08.660]   have a look, have a read. And if you still have questions, please ask on Slack or come
[00:43:08.660 --> 00:43:25.020]   back and we will cover this next week. So last questions. It was pretty unclear. Okay.
[00:43:25.020 --> 00:43:31.020]   I hope that's now clear and Brad's helped. So thanks for that. I want to understand the
[00:43:31.020 --> 00:43:38.060]   batch transform. So the data block, I want to understand the batch transforms of a data
[00:43:38.060 --> 00:43:44.700]   block object for regression task. Does it do the items transforms resize function in
[00:43:44.700 --> 00:43:48.900]   the batch transforms and then later normalizes? I do not know. I think that's a very fast
[00:43:48.900 --> 00:43:54.980]   AI specific question. I'm not sure. I don't have the answer. So have a look at the forum.
[00:43:54.980 --> 00:44:02.140]   Maybe asking on the forums would be a better place or yeah, I think asking on the fast
[00:44:02.140 --> 00:44:10.700]   AI forums. So going to forums.fast.ai. That's definitely a better place for this question.
[00:44:10.700 --> 00:44:17.140]   I'm sorry. I don't have the details and can't help you much there. What usually do you use
[00:44:17.140 --> 00:44:24.580]   for regression classical algorithm from scikit-learn? No, I don't. Depends on the problem I'm trying
[00:44:24.580 --> 00:44:31.460]   to solve, I guess. But then what I use for regression is very different to the problem
[00:44:31.460 --> 00:44:37.140]   that I'm trying to solve. I've been using that for object detection, but again, as I
[00:44:37.140 --> 00:44:40.620]   say, that's deep learning and then that's separate. So that shouldn't be something we
[00:44:40.620 --> 00:44:45.620]   should go into much detail today. So I'm going to skip over this question intentionally as
[00:44:45.620 --> 00:44:52.420]   well. All right. So that's that. Now we are going to the Kaggle competition. So we're
[00:44:52.420 --> 00:45:08.340]   doing the life coding part. Let's go to the Kaggle competition. All right. So now with
[00:45:08.340 --> 00:45:15.460]   fast AI, what we're going to try and do is we're going to try and build a solution for
[00:45:15.460 --> 00:45:24.660]   cassava leaf disease classification. As you read through this chapter, I'll give you some
[00:45:24.660 --> 00:45:34.460]   background. So cassava is basically a key food security crop grown by smallholder farmers.
[00:45:34.460 --> 00:45:41.680]   And this is mostly, you have some label images that introduce a data set of like these plants
[00:45:41.680 --> 00:45:50.020]   or these leaves basically of that crop. And you have 21,365 labeled images. And it mostly
[00:45:50.020 --> 00:45:56.740]   comes from Uganda. The labels were done at this particular university, I guess. And then
[00:45:56.740 --> 00:46:06.020]   the task is to classify each leaf image into the four disease categories or a fifth category
[00:46:06.020 --> 00:46:12.360]   indicating a healthy leaf. So there's like, you could think of like, there's like these
[00:46:12.360 --> 00:46:22.400]   21,367 images and each image either has like a four disease category, which we look at
[00:46:22.400 --> 00:46:27.800]   what those diseases are. But basically then each image has four diseases or there's like
[00:46:27.800 --> 00:46:35.020]   a healthy leaf. So you can think of it like being like a problem where you have to classify
[00:46:35.020 --> 00:46:40.680]   every image based on the five categories that you have. So this is not very different from,
[00:46:40.680 --> 00:46:47.000]   in fact, it's very similar to the pets data set that we looked at. You have an image instead
[00:46:47.000 --> 00:46:52.440]   of having 37 classes as in pets, you have five classes. So if you have a look at the
[00:46:52.440 --> 00:47:02.880]   data, you can see how enable to show preview. Okay. Please download it. I already have it.
[00:47:02.880 --> 00:47:15.440]   No worries. I can download it again. Train, cassava. It should be double S. Anyway. So
[00:47:15.440 --> 00:47:22.120]   if you have a look at this train.csv, you will see how for every image ID, you have
[00:47:22.120 --> 00:47:27.600]   one label. So it's single label classification. The problem, it's not multi-label, it's single
[00:47:27.600 --> 00:47:35.760]   label. So you have like a long list of image IDs and then you have the labels. So that's
[00:47:35.760 --> 00:47:46.160]   your training set. And then as for which image or like what does the leaf or like those numbers
[00:47:46.160 --> 00:47:52.520]   in terms of labels 0, 1, 2, 3, 4 represent. Zero represents this disease cassava bacterial
[00:47:52.520 --> 00:47:57.000]   blight. So if I search for that, if you need some background, you can read about it. Like
[00:47:57.000 --> 00:48:02.840]   you can see how this, how does it spread or you can have some background information about
[00:48:02.840 --> 00:48:08.140]   this task. But for me, I'm not worried about what these diseases are for now. What I'm
[00:48:08.140 --> 00:48:13.920]   worried about is the task that I have at hand is I have these images and what I want to
[00:48:13.920 --> 00:48:20.920]   do is I want to classify them to these five labels. Basically the fifth being the healthy
[00:48:20.920 --> 00:48:27.840]   one. And I want to use fast AI and now I'm trying to find solutions on how I could do
[00:48:27.840 --> 00:48:35.560]   that. The thing that comes to my mind is just to follow the pets tutorial or the pets notebook
[00:48:35.560 --> 00:48:43.760]   and try and see if I can use that to build a solution. So I don't have a solution handy.
[00:48:43.760 --> 00:48:50.520]   I didn't do this before. I actually wanted to keep it as raw as possible. The only reason
[00:48:50.520 --> 00:49:00.000]   being that we can then run into problems. The only thing I've done is I just have my
[00:49:00.000 --> 00:49:06.920]   cassava folder and in that I have a data folder. And what I did was I downloaded that zip file,
[00:49:06.920 --> 00:49:10.840]   which is, which is basically I downloaded this data set using this Kaggle competitions
[00:49:10.840 --> 00:49:16.800]   download command. If you try and run this in Google collab, that should also work. So
[00:49:16.800 --> 00:49:21.480]   what we do right now would also be the same steps in terms of like Google collab. In collab,
[00:49:21.480 --> 00:49:25.280]   you'll only have to run this command every time or like download this data set every
[00:49:25.280 --> 00:49:32.120]   time you rerun the notebook. But all I've done is I've downloaded the data set in my
[00:49:32.120 --> 00:49:39.440]   data in my data directory and I have my train images, my test images and everything here.
[00:49:39.440 --> 00:49:47.080]   So all that that's on Kaggle is now available for me to work with on my computer. And then
[00:49:47.080 --> 00:49:53.800]   I've just created a new folder called notebooks, which is empty. And we're going to use pets.
[00:49:53.800 --> 00:50:00.720]   Basically, we're trying to follow the pets template. So I'm just going to call it, let's
[00:50:00.720 --> 00:50:09.240]   call it cassava baseline. And the reason why I wanted to show you this is, yeah, it's basically
[00:50:09.240 --> 00:50:15.960]   when we work on a new kind of a problem, then you're going to run into different challenges.
[00:50:15.960 --> 00:50:24.920]   And that's where the main learning is. So from going forward from today, I want everybody
[00:50:24.920 --> 00:50:32.040]   to try their hand at new problems. So let's, let me bring up the fast book chapter, fast
[00:50:32.040 --> 00:50:39.120]   book chapter pets. Here it is. And then we're just going to follow everything that we did
[00:50:39.120 --> 00:50:46.120]   for pets and try and see if that works for cassava.
[00:50:46.120 --> 00:50:56.120]   Okay. So first thing I don't want to use fast book. I'm just using fast.ai.vision.all import
[00:50:56.120 --> 00:51:02.920]   star. Thank you. Let's see. I'm just going to use the tree command and I'm just going
[00:51:02.920 --> 00:51:09.800]   to see how, so what tree does, it will just say, okay, these are the folders. So I'm just
[00:51:09.800 --> 00:51:18.940]   using, I'm basically running bash in my Jupyter notebook. So let me then based on this, define
[00:51:18.940 --> 00:51:29.240]   my paths. So my data directory is dot dot slash data. My train directory or where my
[00:51:29.240 --> 00:51:47.600]   train files are is my data deer slash train images. My test deer is my data deer slash
[00:51:47.600 --> 00:51:57.880]   test images. And then let's just make sure they look right. They do. Perfect. Then I
[00:51:57.880 --> 00:52:02.720]   could, I guess instead of like using the tree command, you could have also done something
[00:52:02.720 --> 00:52:08.140]   like this in fast.ai data, data dot LS, which will show you, okay, these like eight things
[00:52:08.140 --> 00:52:14.520]   in there, you have a train TF records folder. There's a samples submission CSV. There's
[00:52:14.520 --> 00:52:18.840]   cassava leaf disease zip. You don't need to worry about this. We can delete this, but
[00:52:18.840 --> 00:52:22.720]   then basically, yeah, that's another way of doing, doing the same thing.
[00:52:22.720 --> 00:52:28.560]   All right. So that's the first thing that I've done. Now, what do I want to do next?
[00:52:28.560 --> 00:52:37.320]   I want to have a look at how does this train dot CSV look like? So next step is just going
[00:52:37.320 --> 00:52:43.160]   to call, I'm going to use pandas. So we, we, we touched, we looked at it last time. So
[00:52:43.160 --> 00:52:51.600]   PD dot CSV that, and then I can call head and I can see, okay. That's, that looks correct.
[00:52:51.600 --> 00:53:00.080]   Like I have my image ID. I have my label. All right. Now in pets, what we did was, so
[00:53:00.080 --> 00:53:04.680]   we have untar data. We that's basically downloading the data from Kaggle and then having it in
[00:53:04.680 --> 00:53:12.780]   a pot. So we already did that. I kind of showed you part that LS. Okay. Let's try this part.
[00:53:12.780 --> 00:53:20.840]   So my train deer, that's where my images are. Don't LS. So, okay. So that's 21,397. How
[00:53:20.840 --> 00:53:28.520]   many did Kaggle say there are 21,367. So, okay. Maybe either this is wrong or this is
[00:53:28.520 --> 00:53:33.880]   wrong. I guess we'll, we'll find out. But it is a close number. So I'm fine with that.
[00:53:33.880 --> 00:53:39.920]   So then, okay. I can see my folder has images, which is good. The images are like, this is
[00:53:39.920 --> 00:53:47.680]   the image and all end with JPG. So that's fine. And then that's just using file name.
[00:53:47.680 --> 00:53:51.120]   I don't need to worry about that. I don't need to worry about that. And then they build
[00:53:51.120 --> 00:53:57.520]   a data block in pets. Okay. Without building the data block, what can we do next? So I've
[00:53:57.520 --> 00:54:05.080]   had a look at my stuff. There's a function I can use my, so see how the data block is
[00:54:05.080 --> 00:54:11.280]   using get image files as the get items. Let me show you what this get image file does.
[00:54:11.280 --> 00:54:25.760]   So let's say my Sava image file parts are get image files in my train there. Okay. That's
[00:54:25.760 --> 00:54:31.880]   perfect. And so this get image file does is like, it goes in that train there, which is
[00:54:31.880 --> 00:54:38.720]   this directory dot dot slash train images. That's where my that's where my data or that's
[00:54:38.720 --> 00:54:45.000]   where the, all the images are. And then it will only grab the image file. So we've looked
[00:54:45.000 --> 00:54:49.640]   at this in the past. I won't go into the details, but it will only grab the image files. So
[00:54:49.640 --> 00:54:56.140]   that's how many image files I have. Now, one thing I want to do is if I check the shape
[00:54:56.140 --> 00:55:04.440]   of my basically my CSV, it's the same shape as the number of files I have. That is good
[00:55:04.440 --> 00:55:16.360]   because that means then for every image I have a label so far so good. And then the
[00:55:16.360 --> 00:55:24.200]   next thing in my head I want to do is I want to check how many, like how many images per
[00:55:24.200 --> 00:55:30.160]   each label there are. So if I do something like this, it will tell me, okay, the most
[00:55:30.160 --> 00:55:37.560]   number of images I have are for label three. And then every other label is one, two or
[00:55:37.560 --> 00:55:44.520]   four or like around 2,500. And then zero is, is thousand images. So what does this tell
[00:55:44.520 --> 00:55:54.360]   me? This tells me my data set is imbalanced. And I'm trying to show you, so see, so far
[00:55:54.360 --> 00:55:59.120]   we haven't worked with the data set that is imbalanced. So I'm just, so what we need to
[00:55:59.120 --> 00:56:15.840]   do is we, for a good solution, we need to be able to find a way to either add weights
[00:56:15.840 --> 00:56:23.080]   to my loss function. We need to find a way to balance this data set. That's what, that's
[00:56:23.080 --> 00:56:28.600]   what I'll say. And then there's like different techniques to balance the data set. I won't
[00:56:28.600 --> 00:56:35.360]   go into the details, but you could add weights to your, you could basically add weights to
[00:56:35.360 --> 00:56:39.920]   your loss function, which means when you're, when your model's trying to classify or when
[00:56:39.920 --> 00:56:45.600]   the loss is being calculated, every time you make a mistake on the rare classes, it's going
[00:56:45.600 --> 00:56:51.640]   to cost or the loss is going to be higher compared to when you make a mistake on the
[00:56:51.640 --> 00:56:56.800]   class that has more number of images. So there's like different ways, there's, there's also
[00:56:56.800 --> 00:57:03.560]   things like oversampling, undersampling, and like all those other stuff, which you will
[00:57:03.560 --> 00:57:09.200]   find, I believe in the discussion part of this Kaggle competition. So that's the reason
[00:57:09.200 --> 00:57:14.480]   why I wanted to show you this. And I wanted to involve everybody, because I will leave
[00:57:14.480 --> 00:57:21.240]   disease classification and Kaggle in general is now you're trying to see like this, this
[00:57:21.240 --> 00:57:28.520]   challenge, and then you can read the previous solutions on how people did it, and then try
[00:57:28.520 --> 00:57:33.080]   and replicate that using class AI. So that is a really good way to learn. And that is
[00:57:33.080 --> 00:57:39.640]   a really, really good way to then become a better practitioner and just get better as
[00:57:39.640 --> 00:57:44.640]   we go. So that being said, okay, that's the main thing. Now, the first thing I'm going
[00:57:44.640 --> 00:57:57.360]   to do, as I've told you, is I'm going to build, try and build a baseline. What does that mean?
[00:57:57.360 --> 00:58:07.480]   It means no tricks, just a simple working model. I don't care about imbalanced data
[00:58:07.480 --> 00:58:23.320]   set, corrupt images, or anything for that matter. I am just going to apply what I know
[00:58:23.320 --> 00:58:32.280]   and see what accuracy or what validation metric I get. So what does that mean? That basically
[00:58:32.280 --> 00:58:39.600]   means this is our first iteration. Remember when we were talking about fast AI in, I guess
[00:58:39.600 --> 00:58:43.240]   that was chapter two, when you're trying to put things in production. We're not trying
[00:58:43.240 --> 00:58:47.240]   to build a perfect solution right from the start. We're trying to build a really bad
[00:58:47.240 --> 00:58:55.040]   or like whatever I know, I'm trying to build a really easy, low effort solution. And we
[00:58:55.040 --> 00:59:01.480]   will get some accuracy and then, or we will get some validation metric value. And then
[00:59:01.480 --> 00:59:09.440]   that's where we try and improve it by adding new things. So that's how I like to work on
[00:59:09.440 --> 00:59:14.560]   these things. And rather than spending lots of time on, and that's what the fast AI kind
[00:59:14.560 --> 00:59:20.220]   of recommends. So that's the basic thing. So now what I need to do is I need to be able
[00:59:20.220 --> 00:59:28.080]   to convert this training data frame into a data block. So let's copy this, right? Because
[00:59:28.080 --> 00:59:34.960]   it's going to look the same. So my first thing is an image. My second thing is a category.
[00:59:34.960 --> 00:59:40.920]   That's right. I can use the get image files, which returns those 21,000 parts, which is
[00:59:40.920 --> 00:59:47.920]   perfect. In terms of splitter, random splitter is fine. I don't need to worry about things.
[00:59:47.920 --> 00:59:55.320]   Oh, get wire. This will need to be updated because now I'm not using regex anymore. What
[00:59:55.320 --> 01:00:00.600]   I'm going to do, so what the fast AI, as I've already said in the past is like, it's going
[01:00:00.600 --> 01:00:06.640]   to basically whatever your get items is, so this is my get items. So whatever that returns,
[01:00:06.640 --> 01:00:12.360]   this thing's being returned for each part, you're going to extract your X and you're
[01:00:12.360 --> 01:00:21.360]   going to extract your Y. So now I need to be able to define my define get Y so that
[01:00:21.360 --> 01:00:28.600]   that part gets converted to a Y label. All right. So I have a part. My Y label is in
[01:00:28.600 --> 01:00:36.360]   this train DF. So how could I convert this part, whatever this image part is, to then
[01:00:36.360 --> 01:00:45.680]   this label? Easy. What I could do is I could grab the file name of this part. Then I can
[01:00:45.680 --> 01:00:53.160]   go in this train DF and I can find the corresponding label to that file name. So let's do that.
[01:00:53.160 --> 01:01:00.840]   I have my part. So my file name, so actually I'm going to show you my image files. Oh,
[01:01:00.840 --> 01:01:07.760]   okay. That's not defined yet, is it? Did I store my get image files output anywhere?
[01:01:07.760 --> 01:01:14.440]   I did. Cassava image file parts. So I have my cassava image file parts. That returns
[01:01:14.440 --> 01:01:22.440]   a part. I can call a name function on this part. So remember, part lib part. When that
[01:01:22.440 --> 01:01:27.760]   returns a part, it's just an object of this part lib part type class.
[01:01:27.760 --> 01:01:57.080]   Okay. So I'm going to call that get image part. I'm going to call it get image part.
[01:01:57.080 --> 01:02:05.160]   I'm on your mic. Don't mute it. Oh, I'm so sorry. What was the last thing I said, Angelica?
[01:02:05.160 --> 01:02:12.840]   Before muting myself? Something about the path. Okay. Thank you. My bad. Thanks, everybody.
[01:02:12.840 --> 01:02:19.560]   All right. So I think I was just showing you. Then I'm going to use the name attribute to
[01:02:19.560 --> 01:02:27.560]   grab my file name. So if I apply that name attribute, if I use that F name, then I have
[01:02:27.560 --> 01:02:34.560]   this is my file name. Now I can use this file name to search for the corresponding label
[01:02:34.560 --> 01:02:41.220]   in my train DF. So I can go. I'm going to use pandas now. Train DF.query. I'm going
[01:02:41.220 --> 01:02:49.700]   to query my data frame. And I'm going to say find me the row where image ID is equal to
[01:02:49.700 --> 01:02:58.400]   this F name value. And you can see, okay, there's the row. All right. And that label
[01:02:58.400 --> 01:03:09.080]   is three. All we care about is this three. So we grab the label, which gets returned
[01:03:09.080 --> 01:03:16.720]   as a series. So I can convert that to a NumPy array or the value that I want. And then I
[01:03:16.720 --> 01:03:22.040]   can just grab the value from here. That's a really bad way of grabbing the value. But,
[01:03:22.040 --> 01:03:28.880]   hey, it works. And we only care about a baseline. So in the baseline, all we want to do is build
[01:03:28.880 --> 01:03:35.840]   a pipeline that works. All right. So then this is it. This is how I can grab my label.
[01:03:35.840 --> 01:03:42.720]   So my file name becomes -- I'm just going to now use it here. My file name becomes that
[01:03:42.720 --> 01:03:55.440]   path.name. And then I can use this. Return train DF.query image ID equals F name.labels
[01:03:55.440 --> 01:04:05.880]   and values at one. Perfect. So let's see if that works. Get y on my Cassava image files.
[01:04:05.880 --> 01:04:13.320]   It works. It gives me a label for that path. So I've been able to convert things to a label.
[01:04:13.320 --> 01:04:19.640]   Excellent. Are all of those -- no. Yeah. Okay. One thing I could also do, I think there's
[01:04:19.640 --> 01:04:31.960]   a -- so I'm just showing you something extra.map. Is that what it is? I'm not sure how things
[01:04:31.960 --> 01:04:39.880]   work in the list in the Fast.ai L sort of world. Would that work? I don't know. It's
[01:04:39.880 --> 01:04:46.880]   taking too long. Never mind. I'll show that sometime after. I was just trying to apply
[01:04:46.880 --> 01:04:54.960]   this function to Fast.ai L. So let's see. Fast.ai -- actually, why leave it? Let's have
[01:04:54.960 --> 01:05:00.200]   a look. That's a challenge that I have. And let's try and find a solution for it. So what
[01:05:00.200 --> 01:05:10.320]   I want to do, the problem is now I've been able to apply this function to one of the
[01:05:10.320 --> 01:05:19.360]   parts. But the Cassava image file parts is actually a Fast.ai list of that many files.
[01:05:19.360 --> 01:05:23.240]   So I want to be -- I just want to see -- I want to apply this get y function to each
[01:05:23.240 --> 01:05:29.080]   of the parts so I can grab the label for each of these parts. So let's see if there's -- what
[01:05:29.080 --> 01:05:35.080]   are the methods in Fast.ai docs? Okay. There is a map. What does L.map do? Create new L
[01:05:35.080 --> 01:05:41.160]   with F applied, with the function F applied to all the items. That's exactly what I want.
[01:05:41.160 --> 01:05:52.880]   So let's do that. It's just this time let's just say my labels from parts equals -- let's
[01:05:52.880 --> 01:06:01.440]   apply it to just the first five parts so it's not very slow. Let's see if that works. Oh,
[01:06:01.440 --> 01:06:09.080]   it does. There we go. My first five parts look like that. And then I've just been able
[01:06:09.080 --> 01:06:16.280]   to convert those into labels. Perfect. So we're able to get the labels from the part.
[01:06:16.280 --> 01:06:19.760]   Now I can -- yeah, now you can just apply it for all of the parts. And that's how it
[01:06:19.760 --> 01:06:24.240]   works. I guess I wanted to show this because I didn't know how to do this myself either.
[01:06:24.240 --> 01:06:30.720]   Like, I had some idea, but I didn't know -- and you'll -- when you're working on new problems
[01:06:30.720 --> 01:06:37.040]   or new challenges or new solutions, you'll also get into these situations where you only
[01:06:37.040 --> 01:06:42.800]   know so much, right? We only know things about the pet breeds. But everything in the world
[01:06:42.800 --> 01:06:46.320]   is not going to look like pet breeds. Things are going to look like cassava or things are
[01:06:46.320 --> 01:06:53.520]   going to look a bit different. And then documentation for the different libraries is your friend.
[01:06:53.520 --> 01:06:58.860]   That's why you're going to follow the same process. You're going to go read the documentation
[01:06:58.860 --> 01:07:03.240]   and then you'll try and find a solution for the problem.
[01:07:03.240 --> 01:07:13.560]   Okay. So, so far, then, that's fine. So I can just say copy/paste this get y over here.
[01:07:13.560 --> 01:07:18.080]   Item transforms. Let's not touch anything. Let's not touch anything. Let's just call
[01:07:18.080 --> 01:07:25.160]   this cassava. And then my data loaders become cassava.dataloaders. Instead of passing a
[01:07:25.160 --> 01:07:33.960]   path/images, I already know where my train images are. So that's that. Train dir. Let's
[01:07:33.960 --> 01:07:42.600]   see if that works. It's running. There's no error as such, which means things are probably
[01:07:42.600 --> 01:07:50.200]   running. Things are probably going okay. While that runs, let's see if there's any questions.
[01:07:50.200 --> 01:08:04.680]   Where are you, my Wits and Biases forums? It's still running. That's running too long.
[01:08:04.680 --> 01:08:18.040]   Now I'm scared. Let's see if there's any questions so far. One other thing I like to do before
[01:08:18.040 --> 01:08:23.960]   getting a baseline even is just to see what my accuracy would be if I use the same prediction
[01:08:23.960 --> 01:08:28.400]   for all the values. So in the case, if you predict everything is a three with 100%, that
[01:08:28.400 --> 01:08:35.440]   gives you the lower bound. Okay. Thanks, Kevin. That's actually a good idea as well. So Kevin's
[01:08:35.440 --> 01:08:41.160]   saying like, just make the label be everything as a three. And then you'll see, like, you
[01:08:41.160 --> 01:08:46.520]   can't go any worse than that. Right. Okay. So that worked. So I now have my data loaders.
[01:08:46.520 --> 01:08:55.720]   I can now say show batch. Let's see if that works. Deal. So show back works. Oh, nice.
[01:08:55.720 --> 01:09:06.320]   Now I have a corresponding label for each of my image. Let's see how many there are
[01:09:06.320 --> 01:09:15.860]   in my train. What's the length? I just need to find the length in my data set. So that's
[01:09:15.860 --> 01:09:24.880]   around 17,000. Did I miss something again? I keep calling it Val. Okay. So now Fast.ai,
[01:09:24.880 --> 01:09:30.960]   based on this random splitter, has done a 20% split for us, which means it's put 17,000
[01:09:30.960 --> 01:09:36.360]   files in training and it's put 4,200 files in validation. So that's it. Now, remember
[01:09:36.360 --> 01:09:40.400]   when I told you that as long as you get the data loader working in Fast.ai, things are
[01:09:40.400 --> 01:09:50.960]   done. Things are solved. So happy days. All we now need to do is... Where is it? All I
[01:09:50.960 --> 01:09:56.140]   now need to do is I have my data block working. I have my data loader working. I don't need
[01:09:56.140 --> 01:10:00.140]   to worry about the loss function. All I need to do is define a learner. So let's see how
[01:10:00.140 --> 01:10:07.800]   we define the learner. It's already been defined. Here it is. So now I can say, please define
[01:10:07.800 --> 01:10:17.520]   my learner. I can pass in my data loaders. I can pass in a model, ResNet34. In terms
[01:10:17.520 --> 01:10:22.880]   of metrics, because we are calculating accuracy, it would be nice to have a look at this. I
[01:10:22.880 --> 01:10:30.920]   can do this. My learner. And then because I'm only calculating the baseline, I could
[01:10:30.920 --> 01:10:37.280]   just call learn.finetune. But something that I kind of... We touched last week is LRFinder.
[01:10:37.280 --> 01:10:43.280]   So let's try and find a good learning rate for this problem. While that's running, I'm
[01:10:43.280 --> 01:10:48.760]   going to go back and see if there's any questions so far. No questions. Very good. I'm just
[01:10:48.760 --> 01:10:56.600]   going to run and run. So we already know what that's doing. And it gives me a suggested
[01:10:56.600 --> 01:11:01.320]   learning rate. So you can see how that curve looks like. So let's use that learning rate,
[01:11:01.320 --> 01:11:12.480]   which is 1E Neg 3. So I can say learn.finetune. And I can say 5 epochs. Where's my learning
[01:11:12.480 --> 01:11:22.760]   rate? Base LR as 1E Neg 3. So instead of actually fine, let's just say 3, just so that runs
[01:11:22.760 --> 01:11:33.640]   and finishes. Cool. And guys, there we go. We have a baseline working for Cassava competition.
[01:11:33.640 --> 01:11:46.160]   And now my model's already training. So the next step is now what we really want to do
[01:11:46.160 --> 01:11:52.760]   once that finishes. What we could do is we really want to check how we show up on...
[01:11:52.760 --> 01:11:58.800]   If I use this... Of course, we're going to have our validation metrics. It's going to
[01:11:58.800 --> 01:12:03.080]   provide us... When I say validation metrics, it's just going to say how well we performed
[01:12:03.080 --> 01:12:11.520]   on the 4,279 images that the model did not see. So we have 30% error rate, which means
[01:12:11.520 --> 01:12:17.160]   in the very first epoch, our model is 70% accurate and it's able to classify the images,
[01:12:17.160 --> 01:12:23.120]   which is pretty good. How many lines of code did we write? We wrote... Majority we just
[01:12:23.120 --> 01:12:30.800]   did... We defined a get y, about 20, 25 lines of code. That's the maximum, including everything
[01:12:30.800 --> 01:12:36.800]   that we've done. And we are already able to create a Cassava leaf classifier, which is
[01:12:36.800 --> 01:12:44.800]   an actual product out there in the world. And the best that people can do is 90%. And
[01:12:44.800 --> 01:12:51.320]   they would have spent a lot of time in getting 90%. But in 10 minutes or 20 minutes, we're
[01:12:51.320 --> 01:12:57.760]   able to already get 70% in our very first epoch. So I think that's a really good result.
[01:12:57.760 --> 01:13:01.120]   And in fact, we're actually now able to get around 78. So when that finishes, you'll see
[01:13:01.120 --> 01:13:08.400]   how... What my error rate is. Then the next thing you want to do is... I actually didn't
[01:13:08.400 --> 01:13:17.360]   do this either. So I can now go and click on late. Oh, you do not have any Kaggle notebooks.
[01:13:17.360 --> 01:13:24.120]   I'm out of my Kaggle GPU memory, actually. I'm out of... I've used all my GPUs. I can't
[01:13:24.120 --> 01:13:32.880]   really do a submission right now. Yeah. Turning on GPU availability, Kodar resets. Yeah, I've
[01:13:32.880 --> 01:13:37.480]   used all my... Sorry, I've used... I should have thought of this. I've basically used
[01:13:37.480 --> 01:13:48.280]   all my GPU Kodar on Kaggle because I'm participating in a competition. What's the best way? What
[01:13:48.280 --> 01:13:58.120]   I want you guys to do is then go back and... Actually, I know what's the best way. Let
[01:13:58.120 --> 01:14:06.640]   me... Where is this? Where is this? Where's my image? I'm going to create a same validation
[01:14:06.640 --> 01:14:14.680]   set for all of us. So this is that, right? That's my train.df. Oh, this needs to stop.
[01:14:14.680 --> 01:14:26.160]   Please stop. Yep. I already have my train.df defined. Train.df.head. Okay. That's my train.df.
[01:14:26.160 --> 01:14:31.480]   How about I... How do you even shuffle things in Pandas? I've not used Pandas in a long,
[01:14:31.480 --> 01:14:36.720]   long time. So let's see how we shuffle things in Pandas. I could have used np.random, but
[01:14:36.720 --> 01:14:43.960]   I just want to use Pandas shuffle data frame. Let's see how we can do that. How to shuffle
[01:14:43.960 --> 01:14:55.240]   the data frame. Is there any documentation? All right. Let's just try. Then the next thing
[01:14:55.240 --> 01:15:00.200]   I like to do is just use shuffle. That didn't work. So let's see how that... Let's have
[01:15:00.200 --> 01:15:04.120]   a look at a stack overflow. What does it say? Oh, there's a sample. Of course, that was
[01:15:04.120 --> 01:15:14.320]   the sample. So I can say sample. And my frac is going to be one, which means basically
[01:15:14.320 --> 01:15:20.320]   permute my whole data set. So that's that. And then I'm going to say reset index crop
[01:15:20.320 --> 01:15:28.280]   equals true. And then I've just shuffled my data set. I'm just randomly shuffling. I could
[01:15:28.280 --> 01:15:33.040]   have used the fast.ai splitter. Actually, that would have been nice. Never mind for
[01:15:33.040 --> 01:15:39.880]   that for now. So I've just shuffled my data set. And what's the length of train_df? It's
[01:15:39.880 --> 01:15:47.800]   21,000. So I'm just going to say train_df. I'm going to add a new column. Is val equals
[01:15:47.800 --> 01:15:59.840]   false. So now I have a new column. Is val equals false. And then let's see if I can
[01:15:59.840 --> 01:16:10.640]   grab the 20% of my data as the validation set. That's 4,000 files. Let's just grab...
[01:16:10.640 --> 01:16:24.320]   You will see how slow I am. Let's just grab the last 4,279 files and grab the second column.
[01:16:24.320 --> 01:16:32.000]   So then that's all false and set it to true. And then let's see how that looks like. That's
[01:16:32.000 --> 01:16:40.760]   perfect. Is val.value counts. All right. So this is false for around that number of files
[01:16:40.760 --> 01:16:56.520]   and true for around that number of files, which is great. And now train_df.csv 1db cassava
[01:16:56.520 --> 01:17:09.040]   train_val.split.csv index equals false. So let's see, did that work? Where was I running
[01:17:09.040 --> 01:17:22.400]   this notebook? Kaggle cassava. cd... Oops. cd././kaggle-cassava. Do I have that file here
[01:17:22.400 --> 01:17:32.800]   somewhere? I do. So let's create a Kaggle dataset. I'm just going to call it cassava
[01:17:32.800 --> 01:17:44.120]   train_test.split. Move 1db to cassava train_test.split. cd cassava that. And then let's call Kaggle
[01:17:44.120 --> 01:17:55.240]   datasets in it. I'm just initializing my dataset. All right. That creates a file. I'm just creating
[01:17:55.240 --> 01:18:00.600]   a dataset for you guys, just so you guys know what I'm doing. I'm creating a dataset so
[01:18:00.600 --> 01:18:13.200]   both of us have the same train_test.split. So let's call this title 1db cassava train_test.split.
[01:18:13.200 --> 01:18:24.400]   Let's call this 1db cassava. And then Kaggle datasets create p. Starting upload. Excellent.
[01:18:24.400 --> 01:18:33.040]   So that will create that dataset. Now, okay, what I've done is I've created this data frame
[01:18:33.040 --> 01:18:39.320]   which has the image ID, the labels, and the isValidation false or true. Okay. So what
[01:18:39.320 --> 01:18:46.440]   we learned today in Fastbook in Multicat is that... It was Multicat, right? Where we had
[01:18:46.440 --> 01:18:57.000]   isVal column. When that opens, actually it's open here. So we can now define our custom
[01:18:57.000 --> 01:19:03.960]   splitter. So I just want to show you what you want to do this week. So this week I'll
[01:19:03.960 --> 01:19:11.080]   tell you... Oh, sorry. One second. Yeah. The splitter is this one. All right. That's what
[01:19:11.080 --> 01:19:19.920]   I want to do. So when you go... Instead of having things like this, have your... Instead
[01:19:19.920 --> 01:19:26.680]   of passing random splitter... I'll re-run this notebook and show you what exactly you
[01:19:26.680 --> 01:19:32.400]   have to do. So when you go back this week, do this. Import. Download the dataset. But
[01:19:32.400 --> 01:19:43.880]   also download this file. Where is this? Datasets. I will share this with everybody on that 1db
[01:19:43.880 --> 01:19:54.600]   forums. Download this dataset file. 1db Cassava train test split. Download this file. You'll
[01:19:54.600 --> 01:20:00.640]   see over here there's like this 1db Cassava train test split, which is isValid, false,
[01:20:00.640 --> 01:20:15.440]   or true. True 20% of the time. So let me post that link on the forum and also in the chat.
[01:20:15.440 --> 01:20:20.320]   So I want everybody to go back, download that file on your respective computers or Google
[01:20:20.320 --> 01:20:29.880]   Collabs. So I'm just going to post it here. You need to save traindf back to... Oh, I
[01:20:29.880 --> 01:20:39.360]   did in place... I did do in place equal to true. Did I not do that? One second. Sorry.
[01:20:39.360 --> 01:20:45.880]   Kevin's pointed out something. Traindf.sample. Oh, that's right. I didn't. But that's okay.
[01:20:45.880 --> 01:20:52.360]   That shouldn't be a big, big issue. That's fine. Sorry, Kevin. And thanks for pointing
[01:20:52.360 --> 01:20:57.920]   that out. That's a good point. Anyway, you go to that link. Then you will download that
[01:20:57.920 --> 01:21:04.160]   CSV file. And then use the same train and validation split as in that file. How do you
[01:21:04.160 --> 01:21:08.960]   do that? You define this sampler. So I'm just going to go and show you. So when you have
[01:21:08.960 --> 01:21:14.960]   the file, do it like this, do it like this. Instead of reading the file from the notebook,
[01:21:14.960 --> 01:21:24.560]   read the file that's this train test split CSV file. So that becomes your traindf. And
[01:21:24.560 --> 01:21:29.280]   now you're going to have isval and you're going to have... And now what you do is you
[01:21:29.280 --> 01:21:37.360]   use this splitter. So you have the same splitter, which means traindf, basically my df.index,
[01:21:37.360 --> 01:21:45.120]   df is valid to list, and then that. Yeah, that's perfect. And then just pass this splitter
[01:21:45.120 --> 01:21:52.200]   to your... Pass that splitter to your... When you're building your... Follow the exact same
[01:21:52.200 --> 01:21:58.360]   thing so far. Instead of using a random splitter, pass that splitter. And now what that will
[01:21:58.360 --> 01:22:05.080]   do is do exactly the same steps. And what that will do is each one of us will have the
[01:22:05.080 --> 01:22:14.280]   same training and validation split. And then go back and try and beat... What's the accuracy
[01:22:14.280 --> 01:22:20.520]   I got? I got like 80% and it's still running. But I got like 80% accuracy. So go back and
[01:22:20.520 --> 01:22:30.120]   try and beat that accuracy of like 80%. Try and have your accuracy go up as high as 85.
[01:22:30.120 --> 01:22:36.280]   The ways to do that would be try a different model, try a different learning rate, try
[01:22:36.280 --> 01:22:42.440]   reading through the... Try going through the fast Cassava discussions. You can also have
[01:22:42.440 --> 01:22:47.440]   a look at the Kaggle notebooks. You will see there's lots of fast AI starters. So if you
[01:22:47.440 --> 01:22:54.000]   search here, fast AI, you'll find there's already lots of solutions. There's a few from
[01:22:54.000 --> 01:22:59.800]   Zack which already has scores like 0.892, which is really good accuracy. But have a
[01:22:59.800 --> 01:23:06.280]   read of all of these different things of fast AI and try and break this... Try and beat
[01:23:06.280 --> 01:23:14.800]   whatever this 80% accuracy is. And then just post it on this discussion forum. The data
[01:23:14.800 --> 01:23:19.760]   set looks private. I'm really sorry for that. Let me try and make that up. Let me update
[01:23:19.760 --> 01:23:23.600]   that. Thanks everybody for pointing out these things. As you can tell, I haven't done this
[01:23:23.600 --> 01:23:31.200]   before and hence so many problems. But thanks guys for bearing with me. Okay, it should
[01:23:31.200 --> 01:23:35.320]   be public now. That should work. All right. So do all these steps, go and download that
[01:23:35.320 --> 01:23:41.160]   file and then run this and post your accuracies with me. Write your blog posts about this
[01:23:41.160 --> 01:23:49.520]   week. That's the homework. The homework is work on Cassava and beat that 80% accuracy.
[01:23:49.520 --> 01:23:50.200]   Any questions?
[01:23:50.200 --> 01:24:00.280]   No questions? Anybody wants to put anything in the Zoom chat or anywhere? Or is everything
[01:24:00.280 --> 01:24:10.760]   good so far? Okay. There's nothing. Thanks Kevin. I didn't enjoy it at all though, but
[01:24:10.760 --> 01:24:16.200]   I'm glad you did. Probably you had a good laugh, but that's all good. Cool. Thanks everybody
[01:24:16.200 --> 01:24:22.440]   and I will see you guys next week. But for this week, come to this 1DB forums, come to
[01:24:22.440 --> 01:24:27.760]   this discussion and post your accuracies, post your blog posts, share them with me on
[01:24:27.760 --> 01:24:33.400]   Twitter and then we will recommence next week. Or let's just chat throughout the week on
[01:24:33.400 --> 01:24:36.960]   what are the things you've tried, what are the things you haven't tried, what are the
[01:24:36.960 --> 01:24:41.800]   things you'd like to try. And then let's keep this project going week after week. Let's
[01:24:41.800 --> 01:24:47.920]   try and work on the Cassava leaf classification for around five weeks maybe or three weeks
[01:24:47.920 --> 01:24:52.760]   or around that time. And let's try and actually get a really good accuracy and each one of
[01:24:52.760 --> 01:24:58.920]   you then have a project to showcase on your resumes. So thanks everybody and see you guys
[01:24:58.920 --> 01:25:00.400]   next week. Bye.
[01:25:00.400 --> 01:25:10.400]   [BLANK_AUDIO]

