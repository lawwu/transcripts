
[00:00:00.000 --> 00:00:02.700]   All right, everybody, welcome back to the number one podcast
[00:00:02.700 --> 00:00:06.560]   in the world, the all in podcast with me again today to mock
[00:00:06.560 --> 00:00:10.560]   polyhypertia. Your chairman dictator. How you doing,
[00:00:10.560 --> 00:00:11.480]   brother? How you feeling?
[00:00:11.480 --> 00:00:14.840]   Doing great. Fresh off the holiday spectacular.
[00:00:14.840 --> 00:00:19.400]   Good times. And then getting ready for a little ski. You and
[00:00:19.400 --> 00:00:21.760]   I will be doing a little skiing together with Friedberg. That'll
[00:00:21.760 --> 00:00:22.520]   be quite nice.
[00:00:22.520 --> 00:00:26.560]   Friedberg. I don't think you've skied with me and Jason. Jason
[00:00:26.560 --> 00:00:28.280]   is an excellent skier. I mean,
[00:00:28.400 --> 00:00:32.360]   I've heard his for Jason, I don't think I have. His brother
[00:00:32.360 --> 00:00:35.120]   is excellent, too. They're both Josh. The Black Bomber is good.
[00:00:35.120 --> 00:00:39.920]   Yeah, yeah. Shout out to Josh. The Black Bomber. He's got
[00:00:39.920 --> 00:00:44.600]   skiing hips. They kind of shift left, right. Yeah. It's kind of
[00:00:44.600 --> 00:00:46.720]   like when I went to the hips are wider than the shoulders. Yeah.
[00:00:46.720 --> 00:00:51.240]   It reminds me of I went to the Tom Ford. I went when I went to
[00:00:51.240 --> 00:00:53.720]   Tom Ford to get my suit. I'll do that in a second. But with us
[00:00:53.720 --> 00:00:56.560]   again, of course, you're cackling Sultan of science.
[00:00:56.560 --> 00:00:59.040]   Friedberg, how are you doing? Have you guys seen that clip of
[00:00:59.040 --> 00:01:02.280]   the guy with the fake bomb that runs around the city? Yes, with
[00:01:02.280 --> 00:01:05.000]   security guards. It's like some sort of a crypto put on or
[00:01:05.000 --> 00:01:09.760]   something. It's hilarious. He's got like a big Brazilian butt
[00:01:09.760 --> 00:01:13.720]   and he just runs around in tight catches. Can you find a clip of
[00:01:13.720 --> 00:01:16.720]   this guy? Oh my God, it's so ridiculous. So funny. All right,
[00:01:16.720 --> 00:01:20.560]   with us the cackling with his afterglow from the holiday
[00:01:20.560 --> 00:01:23.160]   spectacular. Let's call it what it is. It's the Christmas
[00:01:23.160 --> 00:01:27.280]   spectacular. We're gonna pick a side Friedberg. How did you like
[00:01:27.280 --> 00:01:30.640]   our Christmas? Are you being anti semitic? Bro? How dare you?
[00:01:30.640 --> 00:01:33.600]   How dare you? You can have the Hanukkah special with your two
[00:01:33.600 --> 00:01:39.920]   specials. And now with us in the red throne. It's fit sacks. It
[00:01:39.920 --> 00:01:45.440]   is stylish sacks. It is goes to work every day in venture sacks.
[00:01:45.440 --> 00:01:48.800]   His name is Rob boy. How are you my brother? Welcome.
[00:01:50.320 --> 00:01:54.800]   Jason. Happy to be here. You know, it's great being more fit
[00:01:54.800 --> 00:01:58.840]   and more fashionable that socks is a pretty low bar. So I'm
[00:01:58.840 --> 00:02:00.160]   really excited to be with
[00:02:00.160 --> 00:02:06.520]   rain man David.
[00:02:18.000 --> 00:02:22.600]   And as we love about you, you've got a little American
[00:02:22.600 --> 00:02:28.240]   exceptionalism supremacy. Dare I say, you don't f with
[00:02:28.240 --> 00:02:31.040]   dictators. That's true. You don't have with them. Yeah, I
[00:02:31.040 --> 00:02:33.560]   don't really love dictators. They're not good for society.
[00:02:33.560 --> 00:02:36.440]   They're not good for America. But you know, it's not always
[00:02:36.440 --> 00:02:39.520]   America's job to fix all of that. All right. Well, what
[00:02:39.520 --> 00:02:43.360]   about running companies? Should company CEOs be dictators? Yes,
[00:02:43.360 --> 00:02:46.720]   actually. So I believe in the founder mode, the Brian Chesky
[00:02:46.720 --> 00:02:49.000]   founder mode, I held a conference in New York recently
[00:02:49.000 --> 00:02:51.800]   that Brian was nice enough to speak out called hiring the art
[00:02:51.800 --> 00:02:55.360]   of hiring for founder mode. So specifically for people who
[00:02:55.360 --> 00:02:58.200]   subscribe founders that subscribe to that view, how do
[00:02:58.200 --> 00:03:00.280]   you hire people? And how is that different than what you would
[00:03:00.280 --> 00:03:03.600]   hire in a standard, you know, monstrosity of a company like
[00:03:03.600 --> 00:03:04.480]   Google or something?
[00:03:04.480 --> 00:03:07.440]   Yeah, good founder mode in New York, by the way, just as
[00:03:07.440 --> 00:03:10.720]   history as I remember correctly, lots of good founder mode in New
[00:03:10.720 --> 00:03:12.960]   York. Hey, Cal, do you want to give Keith's background? Well,
[00:03:12.960 --> 00:03:17.840]   the audience, of course, went to Stanford with the boys, sacks,
[00:03:17.840 --> 00:03:21.240]   and Peter Thiel went on to do PayPal. He had a stint at
[00:03:21.240 --> 00:03:26.360]   Square. He started a bunch of other founders fund. He was he
[00:03:26.360 --> 00:03:29.720]   worked, he went to LinkedIn. That's how it all started.
[00:03:29.720 --> 00:03:32.440]   LinkedIn was pretty key. Like read left PayPal started
[00:03:32.440 --> 00:03:37.360]   LinkedIn. So yes, that's true. And then after that, I went back
[00:03:37.360 --> 00:03:40.400]   with back selection from PayPal days to slide, which is on the
[00:03:40.400 --> 00:03:42.560]   outsheet of history. We don't have to talk about that. But
[00:03:42.560 --> 00:03:46.160]   then I did jump into Square as the 20th employee and helped
[00:03:46.160 --> 00:03:47.200]   build a pretty good company.
[00:03:47.200 --> 00:03:49.320]   Yeah. And then founders.
[00:03:49.320 --> 00:03:56.840]   became lazy, you know, decided to be VC in 2013. Spent six
[00:03:56.840 --> 00:04:00.520]   years of coastal ventures by the founders fund last year, almost
[00:04:00.520 --> 00:04:01.320]   last year now.
[00:04:01.320 --> 00:04:03.840]   Before we jump in, I actually have a question for you.
[00:04:03.840 --> 00:04:05.240]   Starting already.
[00:04:05.240 --> 00:04:09.600]   How does that happen? Keith? How do you? You're at founders?
[00:04:09.840 --> 00:04:15.280]   Sorry, and then you get what like, what pulled you to go and
[00:04:15.280 --> 00:04:17.360]   work with the note? And then what pulled you back? Like, how
[00:04:17.360 --> 00:04:20.440]   does that process work? Because these things are typically meant
[00:04:20.440 --> 00:04:21.920]   to be sort of forever jobs.
[00:04:21.920 --> 00:04:25.040]   That's true. So I, you know, had the benefit of having the note
[00:04:25.040 --> 00:04:28.360]   on the board of Square, which I think is typically how
[00:04:28.360 --> 00:04:33.120]   executives wind up turning into VCs, is you forge a relationship
[00:04:33.120 --> 00:04:35.880]   with board members. So like, for example, Roloff Bote, who runs
[00:04:35.880 --> 00:04:39.760]   Sequoia, had Mike Moritz on his board, Roloff was our CFO at
[00:04:39.760 --> 00:04:43.080]   PayPal, and Mike recruited him. So that's a very common same
[00:04:43.080 --> 00:04:48.240]   thing. Ravi at Sequoia today was the COO and CFO of Instacart.
[00:04:48.240 --> 00:04:50.800]   Again, same thing, Mike recruited him into Sequoia. So I
[00:04:50.800 --> 00:04:54.360]   think that's typically how people become VCs. I always knew
[00:04:54.360 --> 00:04:57.720]   I wanted to be a VC since 2003. I was a very active angel
[00:04:57.720 --> 00:05:00.760]   investor, as you know, even when I was, you know, concentrating
[00:05:00.760 --> 00:05:03.640]   on all these other jobs, I was writing a lot of checks. And so
[00:05:03.640 --> 00:05:06.200]   anybody who's writing a lot of active angel checks probably has
[00:05:06.200 --> 00:05:08.160]   the back of their mind one day, I might want to be a
[00:05:08.160 --> 00:05:10.400]   professional investor. We could talk about the merits or
[00:05:10.400 --> 00:05:13.440]   demerits of that, but like the goal was pretty clear in my
[00:05:13.440 --> 00:05:16.840]   mind. And then at some point, I think you have to make a
[00:05:16.840 --> 00:05:22.240]   decision. What do you want to be in life? You know, venture has
[00:05:22.240 --> 00:05:25.800]   long time horizons, it is a job for life, like 15-20 years, is
[00:05:25.800 --> 00:05:27.920]   pretty much what you have to commit to. So you don't really
[00:05:27.920 --> 00:05:31.320]   want to start venture when you get too old, because 20 years,
[00:05:31.320 --> 00:05:33.080]   you know, you'll be like Donald Trump age.
[00:05:33.080 --> 00:05:34.120]   Yeah.
[00:05:35.240 --> 00:05:39.240]   So I can run for president in 25 years or something.
[00:05:39.240 --> 00:05:42.600]   Keith, why did you leave Founders Fund to go to COSA?
[00:05:42.600 --> 00:05:46.160]   So COSA was great. I spent six years there. The truthful
[00:05:46.160 --> 00:05:49.720]   reason why I left, it's kind of funny given COVID and how
[00:05:49.720 --> 00:05:52.800]   history changes. I hated commuting a Sand Hill Road every
[00:05:52.800 --> 00:05:58.520]   day. We were one office, period, in office every single day. And
[00:05:58.520 --> 00:06:01.760]   I felt like the future of investing was more in San
[00:06:01.760 --> 00:06:06.160]   Francisco, than in Palo Alto at the time. And I just despise
[00:06:06.160 --> 00:06:09.400]   sitting in a car 45 minutes each direction. Turns out, you know,
[00:06:09.400 --> 00:06:11.880]   COVID changed everything, how people do their work, like we're
[00:06:11.880 --> 00:06:14.880]   recording this by Zoom. Before COVID, we'd probably all be in
[00:06:14.880 --> 00:06:18.480]   the same studio recording a podcast like this. And so, but
[00:06:18.480 --> 00:06:21.400]   Vinod and the team was very inflexible about it and Founders
[00:06:21.400 --> 00:06:24.080]   Fund was located in the city. Obviously, I knew Peter since
[00:06:24.080 --> 00:06:27.600]   college, as Jason alluded to. And I decided that, you know,
[00:06:27.600 --> 00:06:30.400]   it was better for me. I remember talking to Sam Altman. And I
[00:06:30.400 --> 00:06:33.240]   said, "Am I crazy for changing funds, mostly on a commute
[00:06:33.240 --> 00:06:36.520]   basis?" And Sam said, "You're human. And every single study of
[00:06:36.520 --> 00:06:39.200]   human happiness is, it's inversely correlated to your
[00:06:39.200 --> 00:06:41.080]   commute time." He's like, "There's nothing wrong with
[00:06:41.080 --> 00:06:44.960]   being human." In any event, there are a lot of similarities
[00:06:44.960 --> 00:06:48.640]   between FF and KV. Both are great funds that have put up,
[00:06:48.640 --> 00:06:52.360]   you know, incredible returns, have funded iconic founders and
[00:06:52.360 --> 00:06:55.960]   companies, but they're very different. KV is involved as
[00:06:55.960 --> 00:06:59.080]   early as possible. And FF is a momentum investor and is maybe
[00:06:59.080 --> 00:07:02.880]   the best on the planet at being a momentum investor. So almost
[00:07:02.880 --> 00:07:06.960]   every successful investment of Founders Fund over eight funds
[00:07:06.960 --> 00:07:11.720]   was invested at $500 million or more entry valuation. And almost
[00:07:11.720 --> 00:07:16.160]   every single investment at KV in eight funds was like the seed or
[00:07:16.160 --> 00:07:20.240]   Series A investment, with very few exceptions ever. And so
[00:07:20.240 --> 00:07:24.280]   Anduril and Ramp were the only exceptions at Founders Fund. And
[00:07:24.280 --> 00:07:28.080]   at KV, the only exception would be Stripe, which I led in, you
[00:07:28.080 --> 00:07:31.720]   know, 2013 or so, which was an order of magnitude higher
[00:07:31.720 --> 00:07:34.400]   valuation than any KV investment, initial investment
[00:07:34.400 --> 00:07:37.760]   ever. So KV is much more an input driven organization.
[00:07:37.760 --> 00:07:40.000]   Founders Fund is much more output driven. And you know,
[00:07:40.000 --> 00:07:42.000]   there's great technology companies that are input driven,
[00:07:42.000 --> 00:07:44.880]   think Amazon, Apple, and there's great technology companies that
[00:07:44.880 --> 00:07:48.160]   are output driven. So you can choose, but certain people are
[00:07:48.160 --> 00:07:49.960]   going to be better in some environments and other people
[00:07:49.960 --> 00:07:53.120]   are going to thrive, you know, in other environments. I fit in
[00:07:53.120 --> 00:07:54.480]   really, really well at KV.
[00:07:54.880 --> 00:07:58.200]   You enjoy the early stage, you enjoy year zero, year one, year
[00:07:58.200 --> 00:07:58.560]   two.
[00:07:58.560 --> 00:08:03.200]   Well, I am very good at it. You know, I think I prefer to invest
[00:08:03.200 --> 00:08:06.040]   as early as possible on a keynote deck only. Like, if I
[00:08:06.040 --> 00:08:07.760]   meet a founder, and there's a keynote deck, there's no
[00:08:07.760 --> 00:08:10.200]   product, there's no metrics. That's my sweet spot. Because I
[00:08:10.200 --> 00:08:13.240]   also know nobody else in venture is good at that. Nobody else is
[00:08:13.240 --> 00:08:14.640]   still active in venture.
[00:08:14.640 --> 00:08:17.360]   What's the secret? What's the secret from a keynote to a
[00:08:17.360 --> 00:08:17.640]   check?
[00:08:17.640 --> 00:08:19.560]   It comes down to founder assessment. At the end of the
[00:08:19.560 --> 00:08:22.880]   day, the only data point is, is this founder capable of building
[00:08:22.880 --> 00:08:26.000]   an iconic company, period. And I prefer to compete when there's
[00:08:26.000 --> 00:08:29.200]   no metrics, because you all the metrics reduce confuse you. That
[00:08:29.200 --> 00:08:32.120]   said, there's a lot of investors who are very good once there's
[00:08:32.120 --> 00:08:34.960]   product metrics, financial metrics. And so I can compete
[00:08:34.960 --> 00:08:36.920]   with people who are pretty good at what they do. So I'd prefer
[00:08:36.920 --> 00:08:39.520]   to go as early as possible. And then secondly, I like company
[00:08:39.520 --> 00:08:43.160]   building, like I think part of my role is to help the founder
[00:08:43.160 --> 00:08:46.480]   increase the amplitude or probability of success. And I
[00:08:46.480 --> 00:08:48.800]   enjoy that at FF, that's very controversial.
[00:08:49.440 --> 00:08:53.160]   Yeah, right. Yeah, the founder reigns supreme, and everybody
[00:08:53.160 --> 00:08:54.800]   else is there to get out of the way, right.
[00:08:54.800 --> 00:08:59.120]   I've had both KV and FF as investors, lead investors in
[00:08:59.120 --> 00:09:02.640]   both climate and at Ohalo now. So I know both firms really well.
[00:09:02.640 --> 00:09:04.480]   And it's really I always people always ask me about the
[00:09:04.480 --> 00:09:06.440]   difference between the two. That's always what I get to. It's
[00:09:06.440 --> 00:09:09.480]   like founders fund, they have this kind of mantra, they find
[00:09:09.480 --> 00:09:12.280]   great founders and just get out of the way, let them run. And
[00:09:12.280 --> 00:09:14.600]   they don't want to be helpful. That's not their objective. They
[00:09:14.600 --> 00:09:16.360]   feel like if they have to be helpful, it's not the kind of
[00:09:16.360 --> 00:09:20.000]   founder, I mean, keep obviously speaking outside of yourself.
[00:09:20.000 --> 00:09:23.720]   And then at coastal, as you know, Vinod has been extremely
[00:09:23.720 --> 00:09:27.000]   and the whole team there, especially climate and always
[00:09:27.000 --> 00:09:29.480]   have always been extremely helpful. So adding board
[00:09:29.480 --> 00:09:31.960]   members, introducing commercial partners being like very
[00:09:31.960 --> 00:09:36.440]   traditionally proactive, participatory VCs on the board,
[00:09:36.440 --> 00:09:39.920]   very different, both very valuable when I had a board
[00:09:39.920 --> 00:09:43.160]   issue at founders fund. And there were some board members
[00:09:43.160 --> 00:09:46.120]   that did not like my strategy, they had issues with what I was
[00:09:46.120 --> 00:09:50.080]   doing with the company climate corp at the time. Founders fund
[00:09:50.080 --> 00:09:53.400]   actually stepped up and protected me. And they got the
[00:09:53.400 --> 00:09:56.080]   board the rest of the board together to protect me in a way
[00:09:56.080 --> 00:09:58.640]   that was like actually at a very kind of crucial moment for the
[00:09:58.640 --> 00:10:01.800]   for the business. And as a result, we had a massive exit
[00:10:01.800 --> 00:10:02.360]   within a year.
[00:10:02.360 --> 00:10:06.800]   I saw Brian Singerman is leaving. So does anybody know
[00:10:06.800 --> 00:10:08.400]   which ambassadorship he's taking?
[00:10:08.400 --> 00:10:12.120]   I mean, the timing's a little interesting, is it not?
[00:10:12.120 --> 00:10:14.720]   I don't know. He's got to compete with our friend Kenny
[00:10:14.720 --> 00:10:18.880]   Howery. Yeah, Ken Howery. Where's he off to? Hopefully
[00:10:18.880 --> 00:10:23.920]   some great destination. I'm sure this time. Okay, Sweden's a
[00:10:23.920 --> 00:10:27.560]   little bit much. I'll send him my wishlist for you. Yeah, let's
[00:10:27.560 --> 00:10:30.680]   go. Like maybe like, is there like a Turks and Caicos or
[00:10:30.680 --> 00:10:32.760]   something? embassy tour, we should do an embassy tour this
[00:10:32.760 --> 00:10:35.400]   year. St. Barts. They have an embassy there. St. Barts. Yeah,
[00:10:35.400 --> 00:10:37.920]   that's a great idea. Don't unfortunately, it's a French
[00:10:37.920 --> 00:10:38.880]   protectorate. But
[00:10:38.880 --> 00:10:41.720]   well, you know what, everything's on the table. Now
[00:10:41.720 --> 00:10:44.520]   we could make them the 51st, second, third or fourth state
[00:10:44.520 --> 00:10:47.160]   and we're in the game right now. Canada's coming on board.
[00:10:47.160 --> 00:10:49.880]   Yeah, Keith, did you not want to roll in the administration
[00:10:49.880 --> 00:10:50.120]   yourself?
[00:10:50.120 --> 00:10:53.640]   Oh, you know, the thinking I love politics. If you follow my
[00:10:53.640 --> 00:10:56.160]   Twitter feed, I pay a lot of attention. I used to be
[00:10:56.160 --> 00:10:59.880]   involved in politics before I got into tech. However, what I
[00:10:59.880 --> 00:11:04.160]   realized about where I am in my career in tech is if I stopped
[00:11:04.160 --> 00:11:07.920]   doing what I do, I'm never going to come back. Like technology
[00:11:07.920 --> 00:11:10.040]   is rapidly emerging. We're going to talk about all the latest
[00:11:10.040 --> 00:11:12.760]   developments this week. Like you can't take your foot off the
[00:11:12.760 --> 00:11:16.240]   gas in the network building parts of venture for two to five
[00:11:16.240 --> 00:11:20.400]   years and come back when you're like 50 years old. I felt like
[00:11:20.400 --> 00:11:22.880]   I'm not ready to give up on venture. I'm in the prime of my
[00:11:22.880 --> 00:11:25.840]   venture career. I'm only 12 years in actually Chamath. So
[00:11:25.840 --> 00:11:30.200]   figure five or 10 more years is the sweet spot. I'd like to see
[00:11:30.200 --> 00:11:32.400]   the companies that I was involved in grow up, become
[00:11:32.400 --> 00:11:35.640]   public companies, et cetera. I didn't feel like I could ever
[00:11:35.640 --> 00:11:38.160]   come back if I quit. At some point, would I like to get
[00:11:38.160 --> 00:11:42.960]   involved in politics? Probably yes. But it's a decade out.
[00:11:42.960 --> 00:11:47.360]   Well, the households involved, big announcement, your husband
[00:11:47.360 --> 00:11:50.600]   Jacob is joining the administration. You can tell us
[00:11:50.600 --> 00:11:53.560]   a little bit about that. Yes, Jacob's very proud of that.
[00:11:53.560 --> 00:11:57.600]   Yeah, it's extremely exciting for him, obviously, for the
[00:11:57.600 --> 00:12:01.280]   country, I think, which is he's going to be the chief economic
[00:12:01.280 --> 00:12:05.800]   officer really for the for the for the country. His job is to
[00:12:05.800 --> 00:12:09.440]   build foreign policy from the business standpoint, which if
[00:12:09.440 --> 00:12:11.920]   you think about it, what's the foundation of power in the
[00:12:11.920 --> 00:12:14.640]   world? It's economic success. Why did the United States win
[00:12:14.640 --> 00:12:17.560]   World War Two is we had an economic engine that could out
[00:12:17.560 --> 00:12:21.040]   compete Germany plus Russia plus Japan, we could build more
[00:12:21.040 --> 00:12:23.480]   tanks, you know, blah, blah, blah, blah, et cetera. And so
[00:12:23.480 --> 00:12:26.360]   the economic engine is critical to this administration.
[00:12:26.360 --> 00:12:29.600]   Obviously, Trump understands that we had a great three years
[00:12:29.600 --> 00:12:32.400]   under his first administration, as he likes to say, best
[00:12:32.400 --> 00:12:36.280]   economy ever before COVID, which may or may be true. And we need
[00:12:36.280 --> 00:12:39.080]   to rebuild American strength. And Jacob's job is to export
[00:12:39.080 --> 00:12:42.360]   that, you know, philosophy. And sometimes you can build
[00:12:42.360 --> 00:12:45.240]   economic strength through working through foreign affairs.
[00:12:45.240 --> 00:12:49.440]   And so that's his main job is to be the primary point person
[00:12:49.440 --> 00:12:52.960]   under secretary of economic affairs. And then they've got a
[00:12:52.960 --> 00:12:55.600]   bunch, the Democrats and the woke people added a bunch of
[00:12:55.600 --> 00:12:58.000]   other things to the title, it used to be just undersecretary
[00:12:58.000 --> 00:13:01.360]   of state for economic affairs. And they added like, you know,
[00:13:01.360 --> 00:13:03.200]   environment and all these politically correct things. So
[00:13:03.200 --> 00:13:05.600]   hopefully, they'll subtract all that stuff and just go back to
[00:13:05.600 --> 00:13:08.240]   undersecretary of state for economic affairs.
[00:13:08.240 --> 00:13:12.600]   And interestingly, what's turning out to be interesting as
[00:13:12.600 --> 00:13:14.720]   Trump assembles this group, I'd love to get the panel's
[00:13:14.720 --> 00:13:18.840]   thoughts on it is not everybody thinks the same. Jacob's
[00:13:18.840 --> 00:13:21.800]   position on Tick Tock, which we'll get to in this show, very
[00:13:21.800 --> 00:13:24.800]   different than some other people in the administration, even
[00:13:24.800 --> 00:13:28.800]   Trump himself flip flopped a little bit on that. So what are
[00:13:28.800 --> 00:13:34.440]   your thoughts as we get started here, just on that assembly of
[00:13:34.440 --> 00:13:36.600]   people, you know, including sacks, obviously, who couldn't
[00:13:36.600 --> 00:13:39.920]   be here this week, but we'll be on future episodes. There's your
[00:13:39.920 --> 00:13:42.560]   announcement, folks. What are your thoughts on that this sort
[00:13:42.560 --> 00:13:47.320]   of diversity and opinion in the administration and how that all
[00:13:47.320 --> 00:13:49.640]   sorts out? Because some people are extremely exciting.
[00:13:49.640 --> 00:13:52.600]   I think it's very obvious watching from afar, that the way
[00:13:52.600 --> 00:13:55.160]   Trump's makes decision is he likes to ask a lot of people a
[00:13:55.160 --> 00:13:58.280]   lot of different questions. And then he makes the decision.
[00:13:58.360 --> 00:14:02.480]   That's why he's, to some people, the media, and he's very
[00:14:02.480 --> 00:14:06.680]   unpredictable, is he doesn't just take one source of input.
[00:14:06.680 --> 00:14:10.160]   And so you can never totally predict the output. But he
[00:14:10.160 --> 00:14:14.120]   arrays an interesting cast of characters and listens to them.
[00:14:14.120 --> 00:14:17.000]   Like so for example, I haven't spent that much time with him.
[00:14:17.000 --> 00:14:20.160]   But insofar as I have, he would go around the room and ask every
[00:14:20.160 --> 00:14:24.240]   single person at dinner, what's your view on X and literally go
[00:14:24.240 --> 00:14:27.440]   around a room of 28 people and listen to every single person.
[00:14:27.960 --> 00:14:29.680]   So I think that's how he makes decisions.
[00:14:29.680 --> 00:14:32.400]   x being a topic, not the website. x.
[00:14:32.400 --> 00:14:35.480]   Not actually. Yeah, everybody knows what I think he likes.
[00:14:35.480 --> 00:14:39.760]   Chamath, any thoughts on this? The wider team as we see it get
[00:14:39.760 --> 00:14:41.760]   assembled. We obviously don't have sacks here. He joined the
[00:14:41.760 --> 00:14:47.280]   team. What's your thoughts on the collection of characters and
[00:14:47.280 --> 00:14:48.040]   executives?
[00:14:48.040 --> 00:14:51.160]   Here's an interesting tweet that I saw, Nick, can you just share
[00:14:51.160 --> 00:14:52.480]   it with the with the guys?
[00:14:52.480 --> 00:14:53.680]   Oh, net worth of each one.
[00:14:53.680 --> 00:14:58.360]   Now, the reason why it was interesting to me was not the
[00:14:58.360 --> 00:15:02.320]   net worth per se. But I think this is the first time that I
[00:15:02.320 --> 00:15:06.760]   can remember. In modern history, at least that I've been in the
[00:15:06.760 --> 00:15:10.960]   United States and following US politics where such an enormous
[00:15:10.960 --> 00:15:15.680]   number of business people have been motivated to come and work
[00:15:15.680 --> 00:15:20.160]   inside of the administration. And I think that it creates this
[00:15:20.160 --> 00:15:24.600]   very interesting contrast and compare. I think that the
[00:15:24.600 --> 00:15:27.840]   Democrats would never have assembled a group of people like
[00:15:27.840 --> 00:15:31.880]   this, even though the Democratic Party has a version of this
[00:15:31.880 --> 00:15:34.760]   chart that they could have made. There's a lot of extremely
[00:15:34.760 --> 00:15:38.200]   talented business people that support the Democratic Party.
[00:15:38.200 --> 00:15:44.160]   The problem is that they believe it's deeply unfashionable to get
[00:15:44.160 --> 00:15:47.560]   strong, competent business people to take a pause in their
[00:15:47.560 --> 00:15:49.960]   business career and come work in government. And you almost
[00:15:49.960 --> 00:15:52.840]   look down on people that are successful. Whereas the
[00:15:52.840 --> 00:15:58.320]   Republican alternative here, if it creates a movement, so to
[00:15:58.320 --> 00:16:01.920]   speak, so that subsequent presidents tap folks on the
[00:16:01.920 --> 00:16:05.160]   shoulder, I think we'll be much better off. And the reason is
[00:16:05.160 --> 00:16:08.600]   pretty simple. I think that the United States economy is too
[00:16:08.600 --> 00:16:13.600]   complicated to be managed by theoreticians, by folks with
[00:16:13.600 --> 00:16:18.280]   random PhDs and absolutely no working experience in the real
[00:16:18.280 --> 00:16:23.000]   world. And when you bring those people in to oversee those PhDs
[00:16:23.000 --> 00:16:26.440]   I think you probably get better outcomes. So I hope this becomes
[00:16:26.440 --> 00:16:30.000]   a standard which is asked these very talented, clearly
[00:16:30.000 --> 00:16:34.640]   demonstrated, successful people with judgment to hit the pause
[00:16:34.640 --> 00:16:38.440]   for a year or three or five, whatever it is, step into
[00:16:38.440 --> 00:16:40.760]   government, help the country and then go back.
[00:16:40.760 --> 00:16:46.120]   And this was what the founding fathers Dave actually
[00:16:46.120 --> 00:16:49.200]   prescribed. This is what they wanted. They wanted people who
[00:16:49.200 --> 00:16:52.040]   are in business to do a tour of duty to serve their country and
[00:16:52.040 --> 00:16:54.680]   then to get out they were not interested in career
[00:16:54.680 --> 00:16:55.920]   politicians, correct.
[00:16:55.920 --> 00:16:59.520]   I've said this a number of times, but all of the founding
[00:16:59.520 --> 00:17:04.080]   fathers had jobs had professions and they stepped in to serve
[00:17:04.080 --> 00:17:09.640]   their country as a civic duty, participated in the process of
[00:17:09.640 --> 00:17:13.560]   executing the the responsibilities of government
[00:17:13.600 --> 00:17:16.360]   and then stepped out and went back to their private lives. I
[00:17:16.360 --> 00:17:21.080]   think it is such a more powerful model for government than people
[00:17:21.080 --> 00:17:25.760]   who choose to to be politicians to represent people as a living
[00:17:25.760 --> 00:17:29.120]   because it creates extraordinarily nasty incentive
[00:17:29.120 --> 00:17:32.840]   structures if that's the model, which is, for example, to curry
[00:17:32.840 --> 00:17:36.560]   favor with private industry participants and then go cash
[00:17:36.560 --> 00:17:39.600]   that favor in after you leave. And I think that this
[00:17:39.600 --> 00:17:41.960]   alternative where you have people who are everyone looks at
[00:17:41.960 --> 00:17:44.120]   them and oh, they're all billionaires and so on. They're
[00:17:44.120 --> 00:17:46.320]   actually as because they're independently wealthy, and they
[00:17:46.320 --> 00:17:48.440]   have enough money than they'll ever spend. I think Larry Page
[00:17:48.440 --> 00:17:51.040]   once said, you can never spend more than a billion dollars in
[00:17:51.040 --> 00:17:53.120]   your life that no matter how hard you try, it's literally
[00:17:53.120 --> 00:17:55.440]   impossible. People think like, oh, you could spend all that
[00:17:55.440 --> 00:17:57.520]   money. Actually, when you buy stuff, most of the stuff you buy
[00:17:57.520 --> 00:18:00.280]   are capital assets that you end up selling later. It's very hard
[00:18:00.280 --> 00:18:03.760]   to spend at that level. So when you have people that are truly
[00:18:03.760 --> 00:18:07.040]   independently wealthy, their motivation is actually quite
[00:18:07.040 --> 00:18:10.440]   different than someone who's trying to make it from 100k to
[00:18:10.440 --> 00:18:14.240]   500k of net worth or 50k to a million of net worth. And I
[00:18:14.240 --> 00:18:17.440]   think it actually creates a higher degree of freedom. And it
[00:18:17.440 --> 00:18:20.880]   aligns the people much more in the long term outcome of
[00:18:20.880 --> 00:18:24.360]   government rather than their own personal and, and they're just
[00:18:24.360 --> 00:18:28.480]   smarter. So I'll give you a simple example. Maybe we'll talk
[00:18:28.480 --> 00:18:32.720]   about this later. J. Cal, I'm not sure. But when I saw the DOJ
[00:18:32.720 --> 00:18:38.840]   theoretical guidance on the Google antitrust matter, their
[00:18:38.840 --> 00:18:44.480]   idea is to divest the browser. And I kind of scratched my head
[00:18:44.480 --> 00:18:48.680]   thinking, would any reasonable business person think that that
[00:18:48.680 --> 00:18:52.360]   was the right remedy? Meanwhile, three weeks later, Google's
[00:18:52.360 --> 00:18:57.200]   like, here's a super chip in quantum computing, that breaks
[00:18:57.200 --> 00:19:00.960]   the world. And I thought, how is it that these folks are so
[00:19:00.960 --> 00:19:03.760]   disconnected from reality that they don't understand what's
[00:19:03.760 --> 00:19:06.080]   actually sitting inside this company. And I think it's in
[00:19:06.080 --> 00:19:09.600]   part, because they don't know the right questions to ask. And
[00:19:09.600 --> 00:19:11.760]   the reason they don't know what the right questions is asked is
[00:19:11.760 --> 00:19:14.800]   they've never worked in the real working world.
[00:19:14.800 --> 00:19:17.720]   We have a professional class of politicians and their
[00:19:17.720 --> 00:19:19.520]   understanding is 10 years old.
[00:19:19.520 --> 00:19:22.440]   But it's not just politicians. This is also bureaucrats. So my
[00:19:22.440 --> 00:19:26.760]   point is, these folks need to get off the sideline and work in
[00:19:26.760 --> 00:19:30.760]   a company for a while know the bowels, they'll be much better
[00:19:30.760 --> 00:19:34.480]   able to guide these regulatory agencies if they actually just
[00:19:34.480 --> 00:19:39.040]   know what's going on. So if the right answer is some antitrust
[00:19:39.040 --> 00:19:43.160]   issue with a company where you need to divest, wouldn't it be
[00:19:43.160 --> 00:19:46.240]   great? We're like 100 smart businessmen looked at that said
[00:19:46.240 --> 00:19:47.360]   that makes sense.
[00:19:47.360 --> 00:19:50.080]   But let me give you the counter to that. Because the counter to
[00:19:50.080 --> 00:19:53.040]   that, which comes up a lot, just so you can, like frame the
[00:19:53.040 --> 00:19:56.320]   response is, why are all these people coming out of pharma
[00:19:56.320 --> 00:19:58.440]   companies to regulate pharma? Why are all these people coming
[00:19:58.440 --> 00:20:00.640]   out of big ag companies to regulate big ag? Why are all
[00:20:00.640 --> 00:20:03.000]   these people that come from energy companies coming to
[00:20:03.000 --> 00:20:07.400]   regulate energy, the common refrain is business people are
[00:20:07.400 --> 00:20:10.600]   basically bringing business interests into the government by
[00:20:10.600 --> 00:20:14.200]   transporting themselves into these regulatory bodies, versus
[00:20:14.200 --> 00:20:17.040]   having career politicians or what you call bureaucrats, be
[00:20:17.040 --> 00:20:19.040]   kind of independent regulatory authority. So what's the
[00:20:19.040 --> 00:20:22.320]   response in that context, to that, that refrain?
[00:20:22.320 --> 00:20:25.080]   They're absolutely right. And that's how it should work. The
[00:20:25.080 --> 00:20:29.440]   United States can no longer afford to be a bleeding piggy
[00:20:29.440 --> 00:20:34.920]   bank for bad ideas. So yeah, like if a bureaucrat thinks the
[00:20:34.920 --> 00:20:39.520]   right thing to do is to divest a random browser to fix Google's
[00:20:39.520 --> 00:20:42.760]   monopolistic tendencies. That's not a remedy
[00:20:42.760 --> 00:20:46.320]   or spend 10s of billions of dollars on a super on a high
[00:20:46.320 --> 00:20:48.280]   speed rail like we were talking about earlier this week,
[00:20:48.280 --> 00:20:53.120]   this is not logical. It's not meaningful. It's misguided. So
[00:20:53.120 --> 00:20:57.400]   if what we want is kindergarten soccer, where everybody gets to
[00:20:57.400 --> 00:21:00.760]   touch the ball, that's what we are getting right now, which is
[00:21:00.760 --> 00:21:05.080]   it's not useful. So I would rather have a business person
[00:21:05.080 --> 00:21:08.480]   with a direct point of view. And by the way, with the level of
[00:21:08.480 --> 00:21:11.000]   transparency, the big issue, I guess, free book that that would
[00:21:11.000 --> 00:21:15.120]   create is, could these people advantage themselves somehow to
[00:21:15.120 --> 00:21:19.680]   make more wealth. But the reality is, that would be so
[00:21:19.680 --> 00:21:23.320]   obvious and laid bare what happens today is they burrow
[00:21:23.320 --> 00:21:26.400]   with this mid level of an organization and they do exactly
[00:21:26.400 --> 00:21:29.400]   this, but it's not laid bare. Yep. So I'd rather be a
[00:21:29.400 --> 00:21:33.080]   transparent where some guy tries to take the government for $500
[00:21:33.080 --> 00:21:36.600]   million, and we castigate that person, then what's happening
[00:21:36.600 --> 00:21:41.320]   today, which is you slip in the back door, you get paid for 500
[00:21:41.320 --> 00:21:43.680]   grand from a company, then you come back to the government,
[00:21:43.680 --> 00:21:46.920]   then you go back, nobody knows who these people are. Nobody
[00:21:46.920 --> 00:21:49.680]   knows the decisions they're making. And they're altogether
[00:21:49.680 --> 00:21:53.080]   misguided, because they're not grounded in an understanding of
[00:21:53.080 --> 00:21:53.800]   the real economy.
[00:21:54.320 --> 00:21:55.840]   Keith, where do you fall? Yeah.
[00:21:55.840 --> 00:21:59.280]   Well, I share, actually, you're both right, in some ways, if you
[00:21:59.280 --> 00:22:02.640]   look at what, who's Trump's pick these successful people, they're
[00:22:02.640 --> 00:22:06.200]   not typically being assigned to industries they came from. So
[00:22:06.200 --> 00:22:08.760]   it's not like he's taking drug, you know, he's actually taking
[00:22:08.760 --> 00:22:12.880]   the opposite, like a figure RFK, for example. So actually, I
[00:22:12.880 --> 00:22:15.520]   think you can take successful people who have proven
[00:22:15.520 --> 00:22:18.360]   themselves through merit. Like, I think what that's one of the
[00:22:18.360 --> 00:22:22.760]   other benefits of the real world is the only way you get ahead is
[00:22:22.840 --> 00:22:25.200]   you're in a Darwinistic experiment with other people
[00:22:25.200 --> 00:22:28.320]   that are comparable. And to be successful, you have to
[00:22:28.320 --> 00:22:32.720]   outthink, outwork, etc. And that shows up ultimately, in
[00:22:32.720 --> 00:22:36.520]   promotions and net worths and various other metrics. So Trump
[00:22:36.520 --> 00:22:38.680]   has taken a lot of successful people. And I think we want a
[00:22:38.680 --> 00:22:42.360]   society where we aspire for our kids to be successful, we want
[00:22:42.360 --> 00:22:45.080]   to emulate successful people that needs that will yield more
[00:22:45.080 --> 00:22:48.840]   success. Like having Elon involved in the government will
[00:22:48.840 --> 00:22:51.960]   yield more success than you know, if you penalize successful
[00:22:51.960 --> 00:22:55.440]   people, you'll get stigmatized, you get less. So I think if you
[00:22:55.440 --> 00:22:58.440]   transplant successful people into industries that they're not
[00:22:58.440 --> 00:23:01.000]   from, and that they have no interest in going to after the
[00:23:01.000 --> 00:23:03.960]   government, you might get the best of both worlds. Because I
[00:23:03.960 --> 00:23:08.000]   can see some of the critiques of, you know, you're regulating,
[00:23:08.000 --> 00:23:09.920]   you know, your friends companies, and you're gonna make
[00:23:09.920 --> 00:23:12.400]   money later. That said, most of Trump's people are not going to
[00:23:12.400 --> 00:23:15.960]   do that. You can also pass laws like, you know, you can't lobby
[00:23:15.960 --> 00:23:19.640]   you can't work for for x years after. There's also this great
[00:23:19.640 --> 00:23:22.600]   data point, I think it's the last 60 years, Trump is the only
[00:23:22.600 --> 00:23:26.280]   president whose net worth went down after office, every other
[00:23:26.280 --> 00:23:30.000]   president that, you know, took a relatively modest net worth or
[00:23:30.000 --> 00:23:33.920]   mediocre net worth, and turned it into a stratosphere. So you
[00:23:33.920 --> 00:23:37.640]   think about the President is the signature example, it's great
[00:23:37.640 --> 00:23:40.160]   that Trump like is setting the opposite illustration.
[00:23:40.160 --> 00:23:43.840]   Well, I mean, we've discussed this on the other pod, Keith, a
[00:23:43.840 --> 00:23:48.440]   couple of times, which is domain expertise can be an ankle for a
[00:23:48.440 --> 00:23:50.520]   founder, you know, you got a founding team that works in the
[00:23:50.520 --> 00:23:53.160]   hotel business, they're going to look at something like Airbnb
[00:23:53.160 --> 00:23:55.520]   and say, this will never work. You got somebody who worked in
[00:23:55.520 --> 00:23:57.800]   transportation, they're gonna look at Uber and say that I'll
[00:23:57.800 --> 00:23:59.960]   never work, they'll look at PayPal, they worked in finance,
[00:23:59.960 --> 00:24:03.400]   and they did say to you, and the team that's never going to work.
[00:24:03.400 --> 00:24:07.280]   Yeah, I mean, I think it's critical and venture to not
[00:24:07.280 --> 00:24:10.360]   really fall for that trap. I always mentioned that I don't
[00:24:10.360 --> 00:24:16.000]   like people with expertise, typically, as founders, I think
[00:24:16.040 --> 00:24:18.960]   in when I call diligence, due diligence, or call experts, I
[00:24:18.960 --> 00:24:21.800]   only ask one question, which is, what is metaphysically
[00:24:21.800 --> 00:24:25.440]   impossible about this working? Like, is there a lot of physics
[00:24:25.440 --> 00:24:29.040]   that I don't understand that makes this actually impossible?
[00:24:29.040 --> 00:24:33.280]   And if they can't isolate a very specific principle, that makes
[00:24:33.280 --> 00:24:35.840]   it or fact that makes it actually impossible, then I just
[00:24:35.840 --> 00:24:38.240]   ignore everything they're saying, you know, write a check.
[00:24:38.240 --> 00:24:41.880]   Yeah, because then it's just all vibes and opinions, etc. So
[00:24:41.880 --> 00:24:44.560]   they're experts in a prior world, right? They've learned
[00:24:44.600 --> 00:24:47.480]   why not. And this is actually like to combine a couple topics
[00:24:47.480 --> 00:24:49.840]   here. The reason why Trump is so effective. So the most
[00:24:49.840 --> 00:24:53.120]   interesting question to me over the last year was, how is this
[00:24:53.120 --> 00:24:56.800]   guy who everybody in the media and everybody in you know, the
[00:24:56.800 --> 00:24:59.680]   legal groups of various things is trying to attack and hate and
[00:24:59.680 --> 00:25:02.560]   all these people publish these books. Why is he on the
[00:25:02.560 --> 00:25:04.440]   precipice of being elected President of the United States
[00:25:04.440 --> 00:25:07.840]   twice, you must have a superpower or two. Most of us
[00:25:07.840 --> 00:25:09.360]   are not going to like the President of the United States
[00:25:09.360 --> 00:25:12.120]   twice. And most of the people who are attacked by everybody
[00:25:12.120 --> 00:25:14.200]   who has power in the establishment definitely do not
[00:25:14.200 --> 00:25:17.440]   get elected President. So what it kind of came down to, and I
[00:25:17.440 --> 00:25:19.720]   interviewed a lot of people who are critics of him, but knew him
[00:25:19.720 --> 00:25:23.040]   well, like ex cabinet people that don't like him, comes down
[00:25:23.040 --> 00:25:26.720]   to he just asked a lot of why, like, why do we do this? Why do
[00:25:26.720 --> 00:25:29.120]   we have to do it this way? Why have we done it this way? And it
[00:25:29.120 --> 00:25:31.840]   turns out in politics, and in DC, most of the answers are
[00:25:31.840 --> 00:25:34.600]   pretty mediocre or weak or poor or haven't been rethought for
[00:25:34.600 --> 00:25:39.680]   2030 4050 6070 years. And so he just constantly dives in and
[00:25:39.680 --> 00:25:42.280]   says, why, why, why, why? And that's actually what predicts
[00:25:42.280 --> 00:25:45.560]   success for founders is in a domain, they don't know anything
[00:25:45.560 --> 00:25:47.920]   about they're just like, why, why do we take these hotel
[00:25:47.920 --> 00:25:50.320]   things for granted? The Airbnb case? Why should they be so
[00:25:50.320 --> 00:25:53.320]   expensive? Why should scarcity, you know, prevail in New York
[00:25:53.320 --> 00:25:55.240]   for four months of the year, etc, etc.
[00:25:55.240 --> 00:25:58.860]   All right, let's get to our docket. We got a ton of stuff to
[00:25:58.860 --> 00:26:03.280]   get to Google's new quantum chip is super impressive. Freeberg.
[00:26:03.280 --> 00:26:06.720]   And we're talking about that on the group chat on Monday, Google
[00:26:06.720 --> 00:26:10.120]   announced its latest quantum chip. It's called Willow. Here's
[00:26:10.120 --> 00:26:12.940]   the chip. If you haven't seen it, it's beautiful. It was
[00:26:12.940 --> 00:26:16.320]   fabricated in Google's new chip plant in Santa Barbara. They
[00:26:16.320 --> 00:26:19.000]   started this project back in 2012, their quantum computing
[00:26:19.000 --> 00:26:22.920]   project and the headline basically is Willow performed a
[00:26:22.920 --> 00:26:25.720]   standard benchmark computation under five minutes that would
[00:26:25.720 --> 00:26:29.840]   have taken today's fastest supercubers 10 septillion years
[00:26:29.840 --> 00:26:32.760]   or 10 to the 25th power, which is billions of times older than
[00:26:32.760 --> 00:26:35.720]   the universe. If you don't know what quantum computing is,
[00:26:35.720 --> 00:26:39.840]   Freeberg will expand on it. But basically, computers are binary
[00:26:39.840 --> 00:26:42.960]   you've heard this before one and zeros, quantums use qubits, you
[00:26:42.960 --> 00:26:49.680]   know, those are 01 or both at the same time. And Google got a
[00:26:49.680 --> 00:26:54.880]   5% pop, they're up 13% in the last five days, probably on the
[00:26:54.880 --> 00:26:57.600]   other news that Gemini 2.0 is out as well, which is
[00:26:57.600 --> 00:27:00.440]   unbelievable. I've been playing with it. What do you think
[00:27:00.440 --> 00:27:03.480]   Freeberg of this big announcement?
[00:27:03.480 --> 00:27:08.640]   Google's announcement is a paper published in nature that follows
[00:27:08.640 --> 00:27:11.200]   the pre print they actually put out in August. So this news has
[00:27:11.200 --> 00:27:13.280]   been out for a little bit. There's obviously a press cycle
[00:27:13.280 --> 00:27:16.400]   this week around it, to kind of make a big thing about it. But
[00:27:16.400 --> 00:27:22.360]   it is a very kind of important milestone in the evolution of
[00:27:22.360 --> 00:27:26.160]   quantum computing. So do you want me to kind of talk about
[00:27:26.160 --> 00:27:27.880]   quantum computing? And I think we've talked about this in the
[00:27:27.880 --> 00:27:28.320]   past,
[00:27:28.320 --> 00:27:30.680]   like, I mean, maybe a brief primer for people. But like,
[00:27:30.680 --> 00:27:33.120]   what does this mean? Practically, I think what people
[00:27:33.120 --> 00:27:36.880]   want to know is when did these things actually have an impact
[00:27:36.880 --> 00:27:39.280]   in the way say, NVIDIA's GPUs have had?
[00:27:39.280 --> 00:27:43.320]   Yeah, the big breakthrough here is that the whole basis of a
[00:27:43.320 --> 00:27:46.280]   quantum computer is called a qubit or a quantum bit. It's
[00:27:46.280 --> 00:27:50.160]   radically different than a bit, a binary digit, which we use in
[00:27:50.160 --> 00:27:54.280]   traditional digital computing, which is a one or a zero, a
[00:27:54.280 --> 00:27:56.280]   quantum bit, you can kind of think about it as a wave
[00:27:56.280 --> 00:28:03.400]   function, it's sort of a quantum state of a molecule. And if we
[00:28:03.400 --> 00:28:07.840]   can contain that quantum state, and get it to interact with
[00:28:07.840 --> 00:28:12.280]   other molecules, based on their quantum state, you can start to
[00:28:12.280 --> 00:28:17.080]   gather information as an output, that can be the result of what
[00:28:17.080 --> 00:28:20.280]   we would call quantum computation. And that sounds
[00:28:20.280 --> 00:28:22.600]   complicated. But what it really means is that instead of doing
[00:28:22.600 --> 00:28:25.720]   kind of binary computation, where we're adding numbers
[00:28:25.720 --> 00:28:29.880]   together or doing kind of other traditional arithmetic, there
[00:28:29.880 --> 00:28:33.480]   are really interesting functions you can do with qubits. qubits
[00:28:33.480 --> 00:28:38.800]   can, for example, be entangled. So two of these molecules can
[00:28:38.800 --> 00:28:42.560]   actually relate to one another at a distance, they can also
[00:28:42.560 --> 00:28:45.920]   interfere with each other. So canceling out the wave function,
[00:28:45.920 --> 00:28:49.160]   and then when you read it out, you get a result that is
[00:28:49.160 --> 00:28:54.120]   basically a very, very complex problem that is solved through
[00:28:54.120 --> 00:28:58.280]   this quantum interpretation. It's really hard to kind of
[00:28:58.280 --> 00:29:01.840]   highlight how different this is from traditional computing. So
[00:29:01.840 --> 00:29:04.920]   quantum computing creates entirely new opportunities for
[00:29:04.920 --> 00:29:08.720]   algorithms that can do really incredible things that really
[00:29:08.720 --> 00:29:10.800]   don't even make sense on a traditional computer, they're
[00:29:10.800 --> 00:29:13.600]   not possible to kind of resolve on a traditional computer. And
[00:29:13.600 --> 00:29:17.200]   sorry, let me just state one thing, the quantum bit needs to
[00:29:17.200 --> 00:29:20.560]   hold its state for a period of time in order for a computation
[00:29:20.560 --> 00:29:23.720]   to be done. And so the big challenge in quantum computing
[00:29:23.720 --> 00:29:27.440]   is how do you build a quantum computer that has multiple
[00:29:27.440 --> 00:29:31.640]   qubits that hold their state for a long enough period of time
[00:29:31.640 --> 00:29:34.960]   that they don't make enough errors that you can actually do
[00:29:34.960 --> 00:29:38.520]   a computation with them. So what Google was able to demonstrate
[00:29:38.520 --> 00:29:43.920]   here is they created these call it logical qubits. So they put
[00:29:43.920 --> 00:29:47.200]   several qubits together. And by putting several qubits together,
[00:29:47.200 --> 00:29:49.360]   they were able to kind of have an algorithm that sits on top of
[00:29:49.360 --> 00:29:52.600]   it that figures out, hey, this, this group of physical qubits
[00:29:52.600 --> 00:29:56.680]   is now one logical qubit may balance the results of each one
[00:29:56.680 --> 00:29:59.440]   of them. So each one of them has some error. And as they put more
[00:29:59.440 --> 00:30:01.520]   of these together, what they were able to demonstrate for the
[00:30:01.520 --> 00:30:05.200]   first time ever, is that the error went down. So when they
[00:30:05.200 --> 00:30:08.800]   did a three by three qubit structure, the error was higher
[00:30:08.800 --> 00:30:10.880]   than when they went to five by five, and then they went to seven
[00:30:10.880 --> 00:30:13.160]   by seven, and the error rate kept going down and down and
[00:30:13.160 --> 00:30:16.680]   down. So this is an important milestone, because now it means
[00:30:16.680 --> 00:30:20.280]   that they have the technical architecture to build a chip or
[00:30:20.280 --> 00:30:23.280]   a computer using multiple qubits that can all kind of interact
[00:30:23.280 --> 00:30:26.120]   with each other with a low enough fault tolerance or low
[00:30:26.120 --> 00:30:28.280]   enough error rate that they can start to do these quantum
[00:30:28.280 --> 00:30:32.840]   calculations. This is a big area of opportunity. One of the very
[00:30:32.840 --> 00:30:35.120]   interesting areas that a lot of people are talking about is in
[00:30:35.120 --> 00:30:40.520]   cryptography. So there's an algorithm by a professor who was
[00:30:40.520 --> 00:30:43.240]   at MIT for many years named short called shores algorithm.
[00:30:43.240 --> 00:30:48.240]   And in 1994 1995, I think around that time, he basically came up
[00:30:48.240 --> 00:30:52.000]   with this idea that you could use a quantum computer to factor
[00:30:52.020 --> 00:30:56.700]   numbers almost instantly, and all modern encryption standards.
[00:30:56.700 --> 00:30:59.660]   So all of the RSA standards, everything that bitcoins
[00:30:59.660 --> 00:31:03.060]   blockchain is built on all of our browsers, all server
[00:31:03.060 --> 00:31:06.980]   technology, all computer security technology is built on
[00:31:06.980 --> 00:31:11.820]   algorithms that are based on number factorization. So if you
[00:31:11.820 --> 00:31:15.620]   can factor a very large number, a number that's 256 digits long,
[00:31:15.620 --> 00:31:20.100]   theoretically, you could break a code. And it's really impossible
[00:31:20.100 --> 00:31:22.500]   to do that with traditional computers at the scale that we
[00:31:22.500 --> 00:31:25.780]   operate our encryption standards at today. But a quantum computer
[00:31:25.780 --> 00:31:29.300]   can do it in seconds or minutes. And that's based on shores
[00:31:29.300 --> 00:31:31.460]   algorithm. And if you want, there's some great YouTube
[00:31:31.460 --> 00:31:34.220]   videos that describe shores algorithm and how it works. But
[00:31:34.220 --> 00:31:36.860]   it's like, mind blowing when you look at it. It's like this
[00:31:36.860 --> 00:31:40.540]   really like non intuitive, but simple set of steps that when
[00:31:40.540 --> 00:31:42.620]   you put them together on a quantum computer, it's like this
[00:31:42.620 --> 00:31:44.780]   thing can instantly figure out all the factors and then you can
[00:31:44.780 --> 00:31:46.780]   break a code. One of the things that this highlights that in a
[00:31:46.780 --> 00:31:49.260]   couple of years, theoretically, if Google continues on this
[00:31:49.260 --> 00:31:52.260]   track, and now they build a large scale cubic computer, they
[00:31:52.260 --> 00:31:56.020]   theoretically would be in a position to to start to run some
[00:31:56.020 --> 00:31:58.740]   of these quantum algorithms like shorts algorithm. And so we're
[00:31:58.740 --> 00:32:01.020]   now kind of spitting distance or a couple of years, it's not
[00:32:01.020 --> 00:32:03.220]   really clear as a three years, five years, seven years, but a
[00:32:03.220 --> 00:32:06.180]   couple years away from having computers that theoretically
[00:32:06.180 --> 00:32:09.260]   could crack all encryption standards. And there are a set
[00:32:09.260 --> 00:32:11.620]   of encryption standards that are called post quantum encryption.
[00:32:11.620 --> 00:32:13.860]   And all of computing and all software is going to need to
[00:32:13.860 --> 00:32:16.380]   move to post quantum encryption in the next couple years. So
[00:32:16.380 --> 00:32:18.260]   there's like this big kind of push now to like, how do we do
[00:32:18.260 --> 00:32:19.180]   that? How do we accelerate it?
[00:32:19.180 --> 00:32:24.300]   I saw Sundar post it. I saw it in my feed. I ended up missing
[00:32:24.300 --> 00:32:27.500]   my next meeting. Because I had to figure out how long will it
[00:32:27.500 --> 00:32:30.860]   take for us to crack the encryption standards that we use
[00:32:30.860 --> 00:32:34.900]   for Bitcoin? Now, here's the answer, because I was so tilted
[00:32:34.900 --> 00:32:42.100]   by this idea. So if you think of willow, as essentially like one
[00:32:42.180 --> 00:32:49.980]   stable, logical qubit equivalent in a chip, we need about 4000
[00:32:49.980 --> 00:32:55.020]   to break RSA 2048. And we need about 8000 to break SHA 256,
[00:32:55.020 --> 00:32:57.380]   which is the underlying encryption framework for
[00:32:57.380 --> 00:33:01.820]   Bitcoin. So I think you're right, I think we're in the sort
[00:33:01.820 --> 00:33:06.580]   of like, the endgame two to five year shot clock. No, I mean, I
[00:33:06.580 --> 00:33:09.100]   think what will have to happen is some of these chains will
[00:33:09.100 --> 00:33:12.900]   need to obviously reimplement something at a pretty
[00:33:12.900 --> 00:33:17.620]   foundational level. The weird thing is, freebrook says is
[00:33:17.620 --> 00:33:22.140]   like, the willow chips error correction gets better, the more
[00:33:22.140 --> 00:33:26.460]   of these things you start to use together. Now, there's some
[00:33:26.460 --> 00:33:29.540]   really big problems and see inside these chips, like logical
[00:33:29.540 --> 00:33:32.060]   interconnects are very complicated. If you put two
[00:33:32.060 --> 00:33:35.100]   chips on a board, like the CTC communication is called this all
[00:33:35.100 --> 00:33:37.940]   this stuff that we haven't figured out how to do. But this
[00:33:37.940 --> 00:33:42.860]   is a big deal. I was really like, my god, what's going on
[00:33:42.860 --> 00:33:43.140]   here?
[00:33:43.140 --> 00:33:46.780]   Other projects at Google are finally landing, you have way
[00:33:46.780 --> 00:33:50.620]   more incredible and you have this now I mean, project moon
[00:33:50.620 --> 00:33:54.140]   might be gone. But you know, I think those projects, we're
[00:33:54.140 --> 00:33:56.020]   going to see a couple of them change the world. Yeah.
[00:33:56.020 --> 00:33:59.260]   Just to give you a sense on the numbers like Google's target for
[00:33:59.260 --> 00:34:02.020]   fault tolerance on a quantum chip to make it logically
[00:34:02.020 --> 00:34:06.940]   useful, is one times 10 to the negative six. Right now, this
[00:34:06.940 --> 00:34:11.300]   willow chip is kind of running at 99.7%. So it's still a few
[00:34:11.300 --> 00:34:15.300]   orders of magnitude away, they have a long way to go in getting
[00:34:15.300 --> 00:34:19.460]   the fault tolerance low enough to actually build logical gates
[00:34:19.460 --> 00:34:23.380]   using qubits that can resolve kind of computational output.
[00:34:23.380 --> 00:34:25.900]   And so there's still there's still a build cycle ahead. And
[00:34:25.900 --> 00:34:27.700]   that pathway is a little bit unclear. But what they've shown
[00:34:27.700 --> 00:34:30.660]   is, this almost feels like the Shockley transistor moment. It's
[00:34:30.660 --> 00:34:34.940]   like, you know, here's this transistor now everyone's like,
[00:34:35.220 --> 00:34:37.500]   you have a lossy transistor, and then you'll figure out p
[00:34:37.500 --> 00:34:40.020]   injunctions, you'll figure out all of these ways of just like
[00:34:40.020 --> 00:34:42.700]   getting the error correction down. But by the way, this
[00:34:42.700 --> 00:34:45.820]   reinforces what you said before, which is it's hard for an
[00:34:45.820 --> 00:34:50.580]   outsider like us to comprehend what's really happening inside
[00:34:50.580 --> 00:34:53.380]   of Google, because the business they built was able to fund
[00:34:53.380 --> 00:34:56.460]   this. I mean, and I went down a rabbit hole, because I'd never
[00:34:56.460 --> 00:34:59.460]   heard of who this guy was that that runs his helmet, Nevin.
[00:34:59.460 --> 00:35:01.780]   He's in Santa Barbara, they have a whole team in Santa Barbara,
[00:35:01.780 --> 00:35:02.980]   they've been running for like 10 years,
[00:35:03.020 --> 00:35:06.300]   he has his own law, Nevin's law. And then I went down the rabbit
[00:35:06.300 --> 00:35:09.900]   hole of that. But what's amazing is so valuable for humanity.
[00:35:09.900 --> 00:35:13.180]   Google had the money to fund him for the last 12 years.
[00:35:13.180 --> 00:35:16.580]   Exactly. And the greatest money printing machine of all time is
[00:35:16.580 --> 00:35:17.900]   paying dividends. Yeah.
[00:35:17.900 --> 00:35:22.820]   But isn't it great to know that Google takes these resources
[00:35:22.820 --> 00:35:26.900]   from search? And sure, maybe there's waste in or maybe they
[00:35:26.900 --> 00:35:30.140]   could have done better with the black George Washington, or
[00:35:30.140 --> 00:35:33.060]   maybe they could have done better with YouTube. But the
[00:35:33.060 --> 00:35:37.580]   other side is, they've been able to like, incubate and germinate
[00:35:37.580 --> 00:35:43.020]   these brilliant people that can toil away and create these
[00:35:43.020 --> 00:35:47.660]   important step function advances for humanity. It's really
[00:35:47.660 --> 00:35:48.100]   awesome.
[00:35:48.100 --> 00:35:50.060]   DeepMind is on that list as well. Keith, what are your
[00:35:50.060 --> 00:35:50.380]   thoughts?
[00:35:50.380 --> 00:35:55.340]   Yeah, so I think, first of all, I think there's a long time
[00:35:55.340 --> 00:35:58.420]   before this becomes a commercial product or application of any
[00:35:58.420 --> 00:36:02.220]   sort. So it's great that they're taking money, but think about
[00:36:02.220 --> 00:36:04.900]   it as like, almost like Stanford takes money or the US government
[00:36:04.900 --> 00:36:09.260]   funds basic research in some ways. This is at least a decade
[00:36:09.260 --> 00:36:12.500]   out kind of thing. There's another, I mean, this area way
[00:36:12.500 --> 00:36:14.260]   beyond my expertise, but I've been talking to a lot of smart
[00:36:14.260 --> 00:36:16.820]   people because I do do financial services innovation. And
[00:36:16.820 --> 00:36:19.020]   obviously, encryption is pretty critical, whether it's Bitcoin
[00:36:19.020 --> 00:36:23.540]   or other places. And there's a couple concerns. One is, it's
[00:36:23.540 --> 00:36:26.940]   not even clear that you can verify that this is true. By the
[00:36:26.940 --> 00:36:30.260]   way, like standard computing to explain sort of the magnitude of
[00:36:30.260 --> 00:36:35.020]   difference. Standard computing would take 10 to the 25th years
[00:36:35.020 --> 00:36:39.420]   to verify that what Google analysis accurate. So there's a
[00:36:39.420 --> 00:36:40.860]   chance that it's not even true.
[00:36:40.860 --> 00:36:43.540]   But for the thing for the for the sampling test, they ran.
[00:36:43.540 --> 00:36:43.940]   Yeah.
[00:36:43.940 --> 00:36:47.420]   2050 number of years.
[00:36:47.420 --> 00:36:50.660]   That's the big I think hole in the whole RCS benchmark that
[00:36:50.660 --> 00:36:53.740]   they use that it only is. It's a it's a framework that only a
[00:36:53.740 --> 00:36:55.700]   quantum computer could theoretically even
[00:36:55.780 --> 00:36:57.500]   So how do you know the answer is correct?
[00:36:57.500 --> 00:37:00.780]   It takes that long to solve.
[00:37:00.780 --> 00:37:03.900]   And then to be practical, like, assuming you solve all this
[00:37:03.900 --> 00:37:07.340]   and it's accurate above all, you make it fast off. Second, then
[00:37:07.340 --> 00:37:10.500]   there is the post quantum computing encryption, which, you
[00:37:10.500 --> 00:37:12.300]   know, a lot of people, a lot of things, a lot of poor things
[00:37:12.300 --> 00:37:15.780]   have switched over to. So you have historical communications
[00:37:15.780 --> 00:37:18.620]   that were encrypted under an old paradigm that would be
[00:37:18.620 --> 00:37:21.740]   vulnerable. And, you know, every year that goes by, like the
[00:37:21.740 --> 00:37:24.740]   embarrassment level or the threatening level of old
[00:37:24.740 --> 00:37:27.260]   historical communications will probably have some decay
[00:37:27.260 --> 00:37:30.340]   function or some half life. So it takes another 10 years
[00:37:30.340 --> 00:37:33.460]   communications that were drafted 20 years ago. Yeah, there'll be
[00:37:33.460 --> 00:37:36.100]   some embarrassing things and blah, blah, blah, blah. But the
[00:37:36.100 --> 00:37:38.940]   more time it takes, the more safe, you know, sort of private
[00:37:38.940 --> 00:37:42.300]   communications and exchanges will be. So I think that's
[00:37:42.300 --> 00:37:48.180]   positive. Third, is there's a question of order of magnitude
[00:37:48.180 --> 00:37:50.180]   here, you mentioned you need like three words of magnitude
[00:37:50.180 --> 00:37:52.940]   sort of improvement. Is each step function, you know,
[00:37:52.940 --> 00:37:55.900]   incrementally easier and faster? Or is each step function, you
[00:37:55.900 --> 00:37:59.180]   know, 10 years, and I don't know that anybody knows the answer to
[00:37:59.180 --> 00:38:02.140]   that. That's right. That's right. Yeah, a lot more work
[00:38:02.140 --> 00:38:05.940]   here to, to be done. You're not buying any quantum computing
[00:38:05.940 --> 00:38:10.020]   stock yet. Not yet. We have looked at KB, you know, talk
[00:38:10.020 --> 00:38:12.980]   about like technology forward, you know, VCs. Over the years,
[00:38:12.980 --> 00:38:16.060]   I've sat through partner presentations, and we've never
[00:38:16.060 --> 00:38:19.020]   really pulled the trigger. There's other reasons like
[00:38:19.020 --> 00:38:21.620]   including like, even if you have quantum computing, you have to
[00:38:21.620 --> 00:38:24.340]   rewrite software on top of it. From a different, it's a
[00:38:24.340 --> 00:38:27.340]   completely different, you know, world. Nothing maps, nothing
[00:38:27.340 --> 00:38:29.780]   maps at all. Nothing. So you're starting from scratch. So you
[00:38:29.780 --> 00:38:32.180]   have an application layer, which might be actually an interesting
[00:38:32.180 --> 00:38:33.140]   business opportunity.
[00:38:33.140 --> 00:38:36.980]   Yeah, how are these things actually going to be coded and
[00:38:36.980 --> 00:38:39.060]   how our developers going to interact with them, if at all,
[00:38:39.060 --> 00:38:41.220]   maybe by that time, we'll just be AI. How do you build a
[00:38:41.220 --> 00:38:43.820]   compiler? Who the hell knows? How do you build a language
[00:38:43.820 --> 00:38:45.540]   properly? These are all complicated.
[00:38:45.540 --> 00:38:48.220]   Yeah. Who's going to write the basic? Who's going to write like
[00:38:48.220 --> 00:38:49.220]   the Microsoft basic?
[00:38:49.220 --> 00:38:51.540]   The interesting thing is there's a lot of work that's been
[00:38:51.540 --> 00:38:53.700]   done in this space, like thinking about quantum
[00:38:53.700 --> 00:38:56.580]   computing, and quantum algorithms is like an entire
[00:38:56.580 --> 00:38:59.220]   branch, people do spend a lot of time thinking about this and
[00:38:59.220 --> 00:39:01.820]   working on this. And there are ways you can kind of simulate
[00:39:01.820 --> 00:39:05.020]   and test and start to build out models for how you could
[00:39:05.020 --> 00:39:09.180]   utilize quantum computers. But obviously, we just don't have,
[00:39:09.180 --> 00:39:11.540]   you know, industrial scale systems at this point,
[00:39:11.540 --> 00:39:17.420]   there was one interstellar Marvel Easter egg in your
[00:39:17.420 --> 00:39:19.780]   announcement that I wanted to get your thoughts on freeberg.
[00:39:19.980 --> 00:39:23.660]   Google said that this massive jump in performance, quote,
[00:39:23.660 --> 00:39:27.300]   lends credence to the notion that quantum computation occurs
[00:39:27.300 --> 00:39:31.380]   in many parallel universes in line with the idea that we live
[00:39:31.380 --> 00:39:39.580]   in a multiverse. So is that somebody in PR is high AF, or
[00:39:39.580 --> 00:39:42.300]   reads too much science fiction, you know, you know, the crazy
[00:39:42.300 --> 00:39:43.500]   thing for us.
[00:39:43.500 --> 00:39:47.780]   So the crazy thing about quantum physics is such a mind. Have you
[00:39:47.780 --> 00:39:51.700]   guys taken quantum mechanics? I have not I have Yeah, I remember
[00:39:51.700 --> 00:39:54.460]   the summer I took it, like the quantum, the first quantum
[00:39:54.460 --> 00:39:57.220]   mechanics class, and I was like, glad it was a summer course,
[00:39:57.220 --> 00:40:01.500]   because you really have to like think pretty deeply about what
[00:40:01.500 --> 00:40:03.900]   you've learned in quantum mechanics, there's just nothing
[00:40:03.900 --> 00:40:06.620]   about it that's intuitive. Like the way we've kind of think
[00:40:06.620 --> 00:40:11.500]   about the world is not the way the quantum world operates. In
[00:40:11.500 --> 00:40:14.700]   the case of a qubit, as soon as you measure the qubit, it
[00:40:14.700 --> 00:40:18.340]   collapses to a value. If you try and measure it, if you try and
[00:40:18.340 --> 00:40:21.580]   look at it goes to zero or one, the probability by which it goes
[00:40:21.580 --> 00:40:25.620]   to zero or one is defined by, you know, the quantum state,
[00:40:25.620 --> 00:40:29.140]   right at the moment you observe it, it's just such a mind. So
[00:40:29.140 --> 00:40:32.500]   effectively, this thing is existing in a superposition in
[00:40:32.500 --> 00:40:35.460]   multiple states at the same time, until you try to observe
[00:40:35.460 --> 00:40:39.140]   it. And that's the case of quantum mechanics. So what's
[00:40:39.140 --> 00:40:42.980]   kind of happening, I think, in that language, J kal is nothing
[00:40:42.980 --> 00:40:47.140]   novel was kind of discovered or represented, that's just quantum
[00:40:47.140 --> 00:40:50.020]   mechanics, it's a mind. And you can go watch hours of YouTube
[00:40:50.020 --> 00:40:52.980]   videos, if you want to, like, get taken down the mind rabbit
[00:40:52.980 --> 00:40:55.420]   hole of quantum mechanics and realize, like, nothing, one
[00:40:55.420 --> 00:40:57.220]   thing I've always found fascinating about this
[00:40:57.220 --> 00:41:02.660]   discipline is that looking at a qubit changes it, like it
[00:41:02.660 --> 00:41:07.020]   understands, it's article, there's a slit experiment. And
[00:41:07.020 --> 00:41:12.460]   if you try and observe a light, as a particle versus a wave, it
[00:41:12.460 --> 00:41:15.620]   actually changes what happens what the outcome is of it being
[00:41:15.620 --> 00:41:17.780]   a particle or a wave, same with electrons. The thing about
[00:41:17.780 --> 00:41:20.940]   quantum mechanics is the observation of a particle
[00:41:20.940 --> 00:41:22.940]   changes what happens serious question.
[00:41:22.940 --> 00:41:29.220]   There's a question here comes here. When you look at the
[00:41:29.220 --> 00:41:34.620]   quantum, can you get a better idea of the scale of Uranus?
[00:41:34.620 --> 00:41:36.780]   Let's move on. No, let's move on.
[00:41:39.420 --> 00:41:42.900]   He was queuing up and joke at the same time I was, I was
[00:41:42.900 --> 00:41:45.220]   thinking about Schrodinger's cat. That's like a
[00:41:45.220 --> 00:41:49.420]   another ridiculous thought experiment that when you it's
[00:41:49.420 --> 00:41:54.300]   the same concept. It's the quantum state of the cat. Is
[00:41:54.300 --> 00:41:56.500]   it's both in the box and not in the box. And I don't know
[00:41:56.500 --> 00:41:58.700]   whether it's in the box until you open the box until you open
[00:41:58.700 --> 00:42:01.780]   the box. And then you open the box and there's a X probability
[00:42:01.780 --> 00:42:03.140]   that it's in the box X probability, it's not in the
[00:42:03.140 --> 00:42:05.820]   box. But when when you don't see it, it's both. It's both.
[00:42:08.500 --> 00:42:11.620]   And if you're a cat lady, you have three of those boxes. And
[00:42:11.620 --> 00:42:15.780]   I'm a dog person. Well, we got more engagement from Keith on
[00:42:15.780 --> 00:42:17.180]   that science corner than we have.
[00:42:17.180 --> 00:42:28.020]   I need to I need to be on your socks. They both said, this is
[00:42:28.020 --> 00:42:32.380]   stupid. And I'll never touch it except Keith was kinder and more
[00:42:32.380 --> 00:42:35.700]   articulate and getting there. He did. That's right. It's just
[00:42:35.700 --> 00:42:40.380]   been snoring. He wouldn't he would have done his move where
[00:42:40.380 --> 00:42:41.380]   he goes stupid. I mean,
[00:42:41.380 --> 00:42:45.380]   next topic,
[00:42:45.380 --> 00:42:48.180]   there's an advantage of Monday partner meetings is I get to
[00:42:48.180 --> 00:42:51.220]   watch the science fiction stuff in every week, right? If I don't
[00:42:51.220 --> 00:42:54.340]   really understand it, but like a decade of watching science
[00:42:54.340 --> 00:42:56.060]   fiction, you pick up some tricks.
[00:42:56.060 --> 00:42:59.540]   Exactly. You play chess with Peter Thiel while that
[00:42:59.540 --> 00:43:00.420]   discussion is going on.
[00:43:00.420 --> 00:43:05.500]   Actually, so I don't play chess at all. Okay. The reason why is
[00:43:05.500 --> 00:43:07.820]   if I do something, I want to be really proficient at it. I don't
[00:43:07.820 --> 00:43:08.380]   have the time.
[00:43:08.380 --> 00:43:11.500]   Got it. Got it. Also, you have a life. You have a life. Yeah.
[00:43:11.500 --> 00:43:14.780]   Other things, things in the real world, etc.
[00:43:14.780 --> 00:43:19.140]   Big, big shout out to go cash D. My, my, my Indian friend, 18
[00:43:19.140 --> 00:43:24.300]   years old new world champion. I saw world champion. Yeah, do you
[00:43:24.300 --> 00:43:26.620]   know world champion? Did was he playing Magnus?
[00:43:26.620 --> 00:43:32.100]   Every brown guy that's done any random useful thing in the world.
[00:43:32.100 --> 00:43:34.220]   We all know each other. We're in a huge group chat.
[00:43:34.700 --> 00:43:36.140]   Oh, is it? Yeah.
[00:43:36.140 --> 00:43:40.780]   The group chat isn't like the top 10 tech companies, the group
[00:43:40.780 --> 00:43:42.900]   chats name is what can Brown do for you?
[00:43:42.900 --> 00:43:48.380]   What can Brown do for you? Okay, there is also UPS is filing a
[00:43:48.380 --> 00:43:51.740]   trademark infringement case. Hey, I'm speaking of chip news.
[00:43:51.740 --> 00:43:55.940]   Apple is making its own AI server chips for internal use a
[00:43:55.940 --> 00:44:00.460]   report just came out that Broadcom and TSMC are helping
[00:44:00.460 --> 00:44:03.900]   them develop AI inference chips, you know, inference chips like
[00:44:03.900 --> 00:44:09.020]   rock, Hatchimals. And there are obviously GPUs like NVIDIA GPUs
[00:44:09.020 --> 00:44:11.740]   are sort of like the giant dump trucks that help you build large
[00:44:11.740 --> 00:44:15.020]   language models. These inference chips are kind of like speeder
[00:44:15.020 --> 00:44:17.980]   bikes, you know, motorcycles quickly getting you the results
[00:44:17.980 --> 00:44:21.580]   from the same ones. It's called belt for Valtra. I don't know
[00:44:21.580 --> 00:44:25.500]   what that's in reference to mass production in 2026. They don't
[00:44:25.500 --> 00:44:28.660]   plan on selling the chips. They don't plan on cloud computing.
[00:44:28.660 --> 00:44:32.020]   The reason they're doing this, obviously, is because they really
[00:44:32.020 --> 00:44:36.300]   want the iPhone to be the interface. And they have planted
[00:44:36.300 --> 00:44:41.140]   their flag, that they want to have privacy, and have AI
[00:44:41.140 --> 00:44:44.420]   working off of your local device and not having access to your
[00:44:44.420 --> 00:44:47.780]   data, but having a compelling AI experience.
[00:44:47.780 --> 00:44:50.620]   My iPhone does not work. I'm sorry, I'm just gonna say it.
[00:44:50.620 --> 00:44:54.100]   Okay, I don't know what happened in you upgraded your software
[00:44:54.100 --> 00:44:57.460]   is happy. You're on iOS 18. It doesn't work. I after three
[00:44:57.460 --> 00:45:00.340]   years, I upgraded to the newest phone, I upgraded to the newest
[00:45:00.340 --> 00:45:03.940]   OS. The phone doesn't work, meaning like to call people, I
[00:45:03.940 --> 00:45:07.060]   can't call my wife anymore. I can't call my kids anymore. The
[00:45:07.060 --> 00:45:10.420]   phone bricks constantly, my photos app doesn't work. It is
[00:45:10.420 --> 00:45:13.900]   just really bad. And I think for a company of this scale, I don't
[00:45:13.900 --> 00:45:17.300]   understand how it does not go through a more complicated test
[00:45:17.300 --> 00:45:20.860]   harness that catches all of this. I'm not trying to complain,
[00:45:20.860 --> 00:45:23.380]   but because I know it's hard for them. I know it's complicated.
[00:45:23.380 --> 00:45:25.420]   And, but it's really bad.
[00:45:25.420 --> 00:45:27.940]   You're not the only person people are freaking out about
[00:45:27.940 --> 00:45:32.700]   the interface changes on photos crashing is a major thing. And
[00:45:32.700 --> 00:45:36.500]   Apple Intelligence just doesn't work. So it does seem key that
[00:45:36.500 --> 00:45:40.500]   Apple has gotten off their game of making polished stuff to race
[00:45:40.500 --> 00:45:45.220]   to try and I guess catch up to their perception of, you know,
[00:45:45.220 --> 00:45:48.940]   AI being a disruptive force at the interface level, ie your
[00:45:48.940 --> 00:45:52.700]   phone or desktop, but what are your thoughts on this new story
[00:45:52.700 --> 00:45:54.700]   about them doing more chips, they've obviously had great
[00:45:54.700 --> 00:45:58.140]   success with the processors and phones. And now the M four,
[00:45:58.140 --> 00:46:01.060]   incredible if you haven't tried the Mac mini best computer for
[00:46:01.060 --> 00:46:03.660]   the dollar in the world right now. But what are your thoughts
[00:46:03.660 --> 00:46:04.420]   on Apple?
[00:46:04.420 --> 00:46:06.780]   So the most important thing about Apple is to remember, it's
[00:46:06.780 --> 00:46:09.700]   vertically integrated, and vertically integrated companies
[00:46:09.700 --> 00:46:12.420]   when you construct them properly, have a competitive
[00:46:12.420 --> 00:46:17.140]   advantage that really cannot be assaulted for a decade, 2030 4050
[00:46:17.140 --> 00:46:20.500]   years. And so chips, classic illustration, go all the way
[00:46:20.500 --> 00:46:23.900]   down to the metal in build a chip that's perfect for your
[00:46:23.900 --> 00:46:28.300]   desired interface, your desired use cases, your desired UI, and
[00:46:28.300 --> 00:46:30.060]   nobody's gonna be able to compete with you. And if you
[00:46:30.060 --> 00:46:32.300]   have the resources, you know, because you need balance sheet
[00:46:32.300 --> 00:46:36.580]   resources to go in the chip direction, it just gives you
[00:46:36.580 --> 00:46:40.180]   another five inch hang your sort of competitive advantage. And so
[00:46:40.180 --> 00:46:42.900]   I love vertically integrated companies, you know, I posted a
[00:46:42.900 --> 00:46:45.820]   pin tweet, I think it's still my pin tweet about vertically
[00:46:45.820 --> 00:46:49.460]   integrate is the solution to the best possible companies. But
[00:46:49.460 --> 00:46:51.300]   it's very difficult, you need different teams with different
[00:46:51.300 --> 00:46:53.420]   skill sets, and you need probably more money, truthfully,
[00:46:53.460 --> 00:46:56.300]   more capital. But Apple just going to keep going down the
[00:46:56.300 --> 00:46:59.020]   vertical integration, software, hardware, you know, all day
[00:46:59.020 --> 00:47:01.980]   long. And there's nobody else who does hardware and software
[00:47:01.980 --> 00:47:04.580]   together in the planet, which is kind of shocking in some ways.
[00:47:04.580 --> 00:47:07.700]   Is there a world class company, a company that's world class,
[00:47:07.700 --> 00:47:11.300]   it's both software and hardware, other than Tesla? Yeah, maybe
[00:47:11.300 --> 00:47:16.260]   NVIDIA could be good. Well, maybe not really. Could they do
[00:47:16.260 --> 00:47:19.100]   a world class UI, you know, maybe maybe there's a
[00:47:19.100 --> 00:47:21.300]   foundation, but you don't have a different vision, maybe a
[00:47:21.300 --> 00:47:25.460]   different team, not clear. Tesla's close, I guess. I'd say
[00:47:25.460 --> 00:47:29.700]   the software is good. If you define software as it touches a
[00:47:29.700 --> 00:47:30.500]   consumer.
[00:47:30.500 --> 00:47:37.780]   Tesla, Apple, in some ways, Google, maybe meta with the
[00:47:37.780 --> 00:47:41.980]   meta glasses, trying, trying, attempting, you can't say
[00:47:41.980 --> 00:47:44.940]   NVIDIA, because I think NVIDIA touches the consumer through an
[00:47:44.940 --> 00:47:47.940]   app that then sits on top of CUDA, which I think is that's a
[00:47:47.940 --> 00:47:52.500]   brilliant strategy for them. But it's, it's, yeah, it's a hard
[00:47:52.500 --> 00:47:54.700]   Tesla, and then a long tail of people.
[00:47:54.700 --> 00:47:57.260]   Right. So anyway, this is the point, Apple has a lot of
[00:47:57.260 --> 00:48:00.620]   competitive advantages, that, you know, been actually
[00:48:00.620 --> 00:48:03.940]   leveraging for about 15 years now. And even back then, Steve,
[00:48:03.940 --> 00:48:06.300]   there's some old great Steve videos, I'll see if I can find
[00:48:06.300 --> 00:48:09.620]   you a clip where he talks about this very intentionally from the
[00:48:09.620 --> 00:48:13.900]   1990s. You know, he came back to Apple, he said, we're doing
[00:48:13.900 --> 00:48:17.100]   vertical integration, basically using those words of software
[00:48:17.100 --> 00:48:19.820]   and hardware, and there's gonna be nobody else that can compete
[00:48:19.820 --> 00:48:22.540]   with us. I think it's in an interview he did, it's published
[00:48:22.540 --> 00:48:26.260]   in, in the company of giants, I believe, in these perfect on
[00:48:26.260 --> 00:48:30.300]   point, just follow that strategy for the next 25 years. Now,
[00:48:30.300 --> 00:48:33.100]   you're seeing some of the manifestations, though, of a
[00:48:33.100 --> 00:48:35.980]   competitive strategy that gives you incredible advantages, is
[00:48:35.980 --> 00:48:38.860]   you get very sloppy in other places, especially over time,
[00:48:38.860 --> 00:48:41.700]   because you have such great competitive modes that you don't
[00:48:41.700 --> 00:48:43.580]   have to compete at the cutting edge of this, like the photos
[00:48:43.580 --> 00:48:46.620]   app is completely unusable. I'm the biggest Apple fanboy in the
[00:48:46.620 --> 00:48:49.540]   world. Like I remember interviewing once with a job for
[00:48:49.540 --> 00:48:52.700]   Tim Cook. And I walked in, and I said, he's like, why, you know,
[00:48:52.700 --> 00:48:55.140]   why you're interested. And I said, Well, you know, I own
[00:48:55.140 --> 00:48:58.340]   every SKU of every product you've ever produced, except I
[00:48:58.340 --> 00:49:02.660]   don't have every color of each, you know, iPod. And he was like,
[00:49:02.660 --> 00:49:05.380]   blown away. And but now like, my photos app is completely
[00:49:05.380 --> 00:49:07.940]   unusable. So I totally understand, you know, to off the
[00:49:07.940 --> 00:49:12.220]   frustration. And they are showing like the decay function,
[00:49:12.220 --> 00:49:15.300]   you know, culturally and otherwise, that eventually
[00:49:15.300 --> 00:49:18.460]   somebody will figure out an angle to rip them out. Yeah,
[00:49:18.460 --> 00:49:20.580]   I'll tell you, we talked about dictators at the beginning of
[00:49:20.580 --> 00:49:23.340]   this chamath. And obviously, this is your wheelhouse as a
[00:49:23.340 --> 00:49:27.900]   dictator yourself, is, you know, there has to be a constant fear
[00:49:27.900 --> 00:49:33.060]   that some a hole is going to come to your office and be like,
[00:49:33.060 --> 00:49:36.820]   what did you do to the photos app. And that fear does not
[00:49:36.820 --> 00:49:40.300]   exist inside of Apple. It's not like the mobile me you ever hear
[00:49:40.300 --> 00:49:44.540]   the mobile me story where he brought the mobile me team and
[00:49:44.540 --> 00:49:46.660]   said, How's mobile me supposed to work? They said, Well, it's
[00:49:46.660 --> 00:49:48.580]   supposed to back up everything when you buy your new phone, you
[00:49:48.580 --> 00:49:50.300]   get everything, you never have to worry about losing a phone,
[00:49:50.300 --> 00:49:52.660]   slammed his hand down and said, Well, why the F doesn't work
[00:49:52.660 --> 00:49:56.180]   that way. fired the person brought the next person in and
[00:49:56.180 --> 00:49:58.620]   said, Now make it the way he said it's supposed to be game
[00:49:58.620 --> 00:50:01.300]   over. I don't think Tim Cook's doing that. Johnny Ives not
[00:50:01.300 --> 00:50:03.940]   there. And obviously, Steve Jobs not there to terrorize people.
[00:50:03.940 --> 00:50:07.340]   Well, I don't think he look, you don't need to necessarily
[00:50:07.340 --> 00:50:11.660]   terrorize people. But I do think you have to go through uat. So I
[00:50:11.660 --> 00:50:14.180]   think it's pretty reasonable when you have a large footprint
[00:50:14.180 --> 00:50:17.100]   of consumers using an app to go through user acceptance testing
[00:50:17.100 --> 00:50:20.540]   is like first base. And typically, what happens is you
[00:50:20.540 --> 00:50:24.740]   can do a process of a few months where several 100,000 people get
[00:50:24.740 --> 00:50:28.140]   it all over the world. And as long as you do an okay job of
[00:50:28.140 --> 00:50:31.140]   getting a decent distribution of people, this would have come
[00:50:31.140 --> 00:50:34.380]   out. But I want to just talk about what Keith said as well.
[00:50:34.380 --> 00:50:37.460]   It's literally not just photos, it's like the phone doesn't
[00:50:37.460 --> 00:50:41.700]   work. So there are just core structural issues with this
[00:50:41.700 --> 00:50:48.580]   operating system. Now, that makes the iPhone maybe 10 to 30%
[00:50:48.580 --> 00:50:52.220]   less usable. And everything is really, everything is really
[00:50:52.220 --> 00:50:54.460]   frustrating, the command center, you know, when you pull up your
[00:50:54.460 --> 00:50:58.020]   little command center to change the brightness and your AirPods,
[00:50:58.020 --> 00:51:01.020]   it's just like, what are they doing? I mean, by the way, so do
[00:51:01.020 --> 00:51:04.300]   you need a chip? Do you need a machine learning chip to do
[00:51:04.300 --> 00:51:07.700]   inference to figure out that when you constantly run your
[00:51:07.700 --> 00:51:10.460]   phone at a certain level of brightness, you should just
[00:51:10.460 --> 00:51:12.860]   allow the phone to be at a certain level of brightness?
[00:51:12.860 --> 00:51:18.860]   Yes. Why does it read? This is not this is not complicated
[00:51:18.860 --> 00:51:20.100]   software engineering, guys.
[00:51:20.100 --> 00:51:24.540]   No, but this is my point. There's no arbiter of taste
[00:51:24.540 --> 00:51:26.940]   anymore. Taste is the backstop.
[00:51:26.940 --> 00:51:30.980]   Yeah, let me let me pause. Double click on that for a
[00:51:30.980 --> 00:51:33.940]   second. So I think taste is great if you have it, but
[00:51:33.940 --> 00:51:35.660]   there's only so many people on the planet that are going to
[00:51:35.660 --> 00:51:38.980]   have, you know, cutting edge taste and be right. If you
[00:51:38.980 --> 00:51:41.420]   don't have taste, what most tech companies do is they use
[00:51:41.420 --> 00:51:45.340]   data. Data is something that's approachable and leverageable.
[00:51:45.340 --> 00:51:49.020]   Because Apple has like this, the antibodies to using data to
[00:51:49.020 --> 00:51:51.900]   measure success with the user experience measure, whatever
[00:51:51.900 --> 00:51:56.380]   success. If you subtract taste, even by a bit, you don't have
[00:51:56.380 --> 00:52:00.180]   the scaffolding that every other company would use. And so you
[00:52:00.180 --> 00:52:01.420]   see the worst of both worlds.
[00:52:01.420 --> 00:52:04.660]   That's a great take. That's a great take. It's just go off
[00:52:04.660 --> 00:52:07.300]   the rails, right? You go off the rails. So Keith, you think that
[00:52:07.380 --> 00:52:10.140]   you think that what happened is like when Steve Jobs isn't there
[00:52:10.140 --> 00:52:13.380]   and Johnny Ive isn't there. There's still a bunch of folks
[00:52:13.380 --> 00:52:16.700]   that probably think they have taste, but the real taste folks
[00:52:16.700 --> 00:52:19.700]   left and there's really no scaffolding left to
[00:52:19.700 --> 00:52:23.620]   scaffolding you had at Facebook meta, obviously, or the Google
[00:52:23.620 --> 00:52:26.340]   uses would catch some of the stuff without a doubt, like no
[00:52:26.340 --> 00:52:28.980]   doubt about it, you know, that users are less thrilled and
[00:52:28.980 --> 00:52:32.180]   they'd use things less and you'd fix it. And maybe even you take
[00:52:32.180 --> 00:52:34.780]   that to a stream, you never develop taste. Like I could
[00:52:34.780 --> 00:52:37.220]   argue that about Google or meta. They don't really have taste.
[00:52:37.220 --> 00:52:40.220]   But like, yeah, you could, you could argue the paradigms. But
[00:52:40.220 --> 00:52:43.740]   fundamentally, if you don't have that backstop, if the taste
[00:52:43.740 --> 00:52:46.940]   subtracts, even 10%, not all the way down, you're just not going
[00:52:46.940 --> 00:52:49.860]   to catch this stuff. And I think there's only like how many
[00:52:49.860 --> 00:52:52.340]   people in the world really have cutting edge technology user
[00:52:52.340 --> 00:52:54.700]   experience case. I don't know too many, I would fund them
[00:52:54.700 --> 00:52:57.300]   right away. It's an incredible chess game. I have it.
[00:52:57.300 --> 00:53:03.300]   It's an incredible point because I if I'm being really insecure,
[00:53:04.260 --> 00:53:07.020]   I would want to say, Oh, yeah, no, we had a lot of taste at
[00:53:07.020 --> 00:53:10.140]   Facebook back in the day. But actually, we had so much
[00:53:10.140 --> 00:53:13.140]   scaffolding around data, probably because intuitively, we
[00:53:13.140 --> 00:53:15.380]   knew that that was way more reliable for us.
[00:53:15.380 --> 00:53:17.980]   It's more predictable scale. It's certainly more scalable,
[00:53:17.980 --> 00:53:20.620]   right? Like you take Steve out, you don't need a dictator, but
[00:53:20.620 --> 00:53:24.460]   you need a taste and taste is artistic. The same thing in
[00:53:24.460 --> 00:53:27.300]   venture like, you know, like scaling venture funds is really,
[00:53:27.300 --> 00:53:30.380]   really challenging. Because early stage investing is more
[00:53:30.380 --> 00:53:34.620]   like taste, then driven. And later stage, you can use data
[00:53:34.620 --> 00:53:38.460]   and scale it and scaffolding. So I think there's just fields,
[00:53:38.460 --> 00:53:41.380]   it's a little bit also you see like these sports teams, they
[00:53:41.380 --> 00:53:45.900]   just happened at Stanford when Jim Harbaugh left. It took years
[00:53:45.900 --> 00:53:49.700]   for the decay function for like the next coaching regime to show
[00:53:49.700 --> 00:53:51.980]   they were completely incompetent. Like the next year,
[00:53:51.980 --> 00:53:54.020]   they're pretty good next year, they lost one more game, they
[00:53:54.020 --> 00:53:56.220]   should have next year lost two more games, they should have a
[00:53:56.220 --> 00:53:59.220]   lot and then eventually became like horrible. And you know,
[00:53:59.220 --> 00:54:01.380]   there's a decay function with an organization when you take out
[00:54:01.380 --> 00:54:04.620]   the person who is the original thinker, or the leader or the
[00:54:04.620 --> 00:54:08.220]   dictator or whatever. And so I think some of this is showing up
[00:54:08.220 --> 00:54:12.020]   now. And then you know, playing on a field that's not favorable
[00:54:12.020 --> 00:54:15.260]   to them, which is there are advantages Apple has an AI, but
[00:54:15.260 --> 00:54:18.500]   there's some significant organizational structural
[00:54:18.500 --> 00:54:21.140]   disadvantages. And that's the field that people are going to
[00:54:21.140 --> 00:54:24.020]   be competing on for the next five years from a consumer
[00:54:24.020 --> 00:54:27.380]   perspective. And they're playing on a field where they don't have
[00:54:27.420 --> 00:54:29.060]   all the advantages in their favor.
[00:54:29.060 --> 00:54:32.460]   Yeah, yeah, true. Let's go to the app level here. Tick tock is
[00:54:32.460 --> 00:54:36.100]   scrambling right now after an appeals court upheld the January
[00:54:36.100 --> 00:54:42.900]   19 deadline red for investment. Here we go. Here we go. I refer
[00:54:42.900 --> 00:54:43.860]   you to my Twitter feed.
[00:54:43.860 --> 00:54:49.420]   I mean, you and Jacob must sit. I mean, what do you do? You just
[00:54:49.420 --> 00:54:53.140]   sit at dinner and feed the kids and then talk about Tick Tock
[00:54:53.140 --> 00:54:54.340]   and China. It's
[00:54:54.380 --> 00:54:58.260]   well, I think you don't even have a conversation in my view,
[00:54:58.260 --> 00:55:02.460]   like, Tick Tock is a threat to the national security in the
[00:55:02.460 --> 00:55:03.980]   United States. And that's why
[00:55:03.980 --> 00:55:07.140]   why people who are like, it's just an app. It's just a
[00:55:07.140 --> 00:55:08.340]   hundred percent. Why? Why?
[00:55:08.340 --> 00:55:10.620]   Why? So I think there's different dimensions. One of the
[00:55:10.620 --> 00:55:13.660]   problems is there's so many things wrong with you. Actually,
[00:55:13.660 --> 00:55:15.940]   sometimes people get confused because there's not just one of
[00:55:15.940 --> 00:55:18.660]   these or sometimes it's just one that so they are definitely
[00:55:18.660 --> 00:55:22.700]   using the app to track data about Americans. And there's
[00:55:22.740 --> 00:55:26.900]   evidence that regardless of what alleged protections exist,
[00:55:26.900 --> 00:55:30.740]   there are people in China monitoring what certain people
[00:55:30.740 --> 00:55:35.740]   in the United States are doing. And the CEO lied under oath to
[00:55:35.740 --> 00:55:38.020]   congressional committees about this. And the evidence is now in
[00:55:38.020 --> 00:55:41.020]   the public domain. Hopefully, this Justice Department will
[00:55:41.020 --> 00:55:43.660]   prosecute him for lying under oath, I think setting a really
[00:55:43.660 --> 00:55:46.100]   good example that you cannot just purge yourself before
[00:55:46.100 --> 00:55:46.620]   Congress.
[00:55:46.620 --> 00:55:49.180]   Sorry, Keith, can you double click into that? So what, how
[00:55:49.180 --> 00:55:52.500]   did it come into the into the open source, that what he said
[00:55:52.500 --> 00:55:53.340]   was a lie? Like, what?
[00:55:53.340 --> 00:55:57.420]   Yeah, so there are people in China, who on the record have
[00:55:57.420 --> 00:56:01.540]   said they had access and have had access to American user
[00:56:01.540 --> 00:56:05.660]   data, which he swore to under oath to the Congress that they
[00:56:05.660 --> 00:56:08.700]   were storing the data in Texas somewhere. And that there's no
[00:56:08.700 --> 00:56:12.540]   possibility that, you know, Chinese nationals in China could
[00:56:12.540 --> 00:56:16.020]   access the data. There is now several instantiations of this
[00:56:16.020 --> 00:56:18.500]   in the public record, let alone what's privately about
[00:56:18.500 --> 00:56:21.700]   Yeah, I mean, the case specifically to back in 2022,
[00:56:21.700 --> 00:56:25.500]   by dance, the parent company of Tick Tock had used an app to
[00:56:25.500 --> 00:56:28.300]   track locations had used their app to track the location of
[00:56:28.300 --> 00:56:32.380]   journalists, because they were trying to track leaks outside.
[00:56:32.380 --> 00:56:35.500]   Then secondly, let me let me keep going here, because it's
[00:56:35.500 --> 00:56:38.820]   worse. So there's a law, the fundamental problem is in China,
[00:56:38.820 --> 00:56:43.060]   there's a law that says, if you're a Chinese company, upon
[00:56:43.060 --> 00:56:47.900]   request of the CCP, you must provide all user data. So any
[00:56:47.900 --> 00:56:50.980]   Chinese company is subject to that law, period, there's no
[00:56:50.980 --> 00:56:52.900]   court intervention, you don't need a subpoena, blah, blah,
[00:56:52.900 --> 00:56:58.420]   blah. And so it any Chinese, any executive of that company is
[00:56:58.420 --> 00:57:03.140]   subject to significant penalties on the record for not providing
[00:57:03.140 --> 00:57:07.540]   any user data at the request of the CCP. So as long as that law
[00:57:07.540 --> 00:57:10.780]   exists, there's a real structural threat to United
[00:57:10.780 --> 00:57:15.300]   States. Now, then there's the is the app being used manipulated
[00:57:15.500 --> 00:57:19.340]   on a content basis to influence, you know, policy in the United
[00:57:19.340 --> 00:57:23.540]   States to disadvantage this or that or create hostilities. I
[00:57:23.540 --> 00:57:26.340]   don't really know the answer. I think there's been studies that
[00:57:26.340 --> 00:57:30.020]   suggest that and you're pretty rigorous methodologies. But
[00:57:30.020 --> 00:57:34.380]   that's a second level. And then third is there's the why the
[00:57:34.380 --> 00:57:38.860]   hell are we allowing Chinese companies to compete with us
[00:57:38.860 --> 00:57:42.060]   when no American content based organization is allowed into the
[00:57:42.060 --> 00:57:47.540]   Chinese market, whether it's whether it's meta, Google, x,
[00:57:47.540 --> 00:57:51.940]   you know, there's a strong argument there that Reddit, if
[00:57:51.940 --> 00:57:56.300]   you don't allow our content or non, you know, non Chinese
[00:57:56.300 --> 00:58:00.460]   content into your market, why should we be enabling Chinese
[00:58:00.460 --> 00:58:04.020]   companies to be successful in quotes in the US market, and
[00:58:04.020 --> 00:58:06.140]   that's more of a fair trade free trade.
[00:58:06.140 --> 00:58:09.700]   Keith, do you think that the Pegasus spyware that can
[00:58:09.820 --> 00:58:12.740]   infiltrate WhatsApp so that you can turn on the mic and listen
[00:58:12.740 --> 00:58:15.700]   remotely, and it has no fingerprints, it's very
[00:58:15.700 --> 00:58:19.420]   difficult to detect? Do you think an equivalent? Let's call
[00:58:19.420 --> 00:58:22.060]   it a backdoor exists inside of tick tock?
[00:58:22.060 --> 00:58:25.860]   Yes. So my evidence for this that I believe is an
[00:58:25.860 --> 00:58:29.020]   interpolation of what's in the public domain, is if you looked
[00:58:29.020 --> 00:58:32.940]   at that vote to ban tick tock, it was extremely bipartisan,
[00:58:32.940 --> 00:58:37.420]   despite, you know, controversy. And what happened was that vote
[00:58:37.420 --> 00:58:41.300]   was taken a week after there was an intelligence briefing to
[00:58:41.300 --> 00:58:43.780]   both the House and Senate Intelligence Committees, that
[00:58:43.780 --> 00:58:47.460]   was that's confidential. And the votes, you know, all of a
[00:58:47.460 --> 00:58:50.020]   sudden flow through, I think there's things that are not in
[00:58:50.020 --> 00:58:53.300]   the public domain about tick tock, that spooked a lot of
[00:58:53.300 --> 00:58:56.580]   elected officials and led to this bipartisan consensus. How
[00:58:56.580 --> 00:59:00.340]   many bipartisan votes do we see on a allegedly controversial
[00:59:00.340 --> 00:59:04.220]   issue that, you know, basically, the vote was like, you know,
[00:59:04.260 --> 00:59:08.940]   like, not even close in either house 360 to 58? Yeah, when do
[00:59:08.940 --> 00:59:11.460]   you see a vote like that on something meaningful? Never?
[00:59:11.460 --> 00:59:11.860]   Yeah.
[00:59:11.860 --> 00:59:15.180]   They got they definitely got spooked. And I mean, if you just
[00:59:15.180 --> 00:59:19.420]   think about Navy SEALs, special forces, they're not allowed to
[00:59:19.420 --> 00:59:21.540]   use a lot of these apps, government officials aren't to
[00:59:21.540 --> 00:59:24.300]   use these apps, because they know that all you have to do is
[00:59:24.300 --> 00:59:28.020]   if you track somebody's child, and their tick tock usage, now
[00:59:28.020 --> 00:59:29.860]   you know what the parent is, you start thinking about the
[00:59:29.860 --> 00:59:33.140]   security and safety of individuals. It's crazy.
[00:59:33.380 --> 00:59:39.100]   So 352 votes for an incredibly popular product. Yeah, about how
[00:59:39.100 --> 00:59:45.700]   bad something has to be to get a consensus of 352 votes on an app
[00:59:45.700 --> 00:59:48.300]   that's used by you know, huge fraction of the American public
[00:59:48.300 --> 00:59:52.980]   past the Senate 79 to 18. And you are right, if you say
[00:59:52.980 --> 00:59:56.300]   you're going to ban tick tock, the vague was against tick tock,
[00:59:56.300 --> 00:59:59.140]   but then he opened one, because that's where voters are, you're
[00:59:59.140 --> 01:00:02.060]   going to lose that generation of voters are going to scare
[01:00:02.060 --> 01:00:06.180]   arguably, arguably, in theory, there's some risk, I can sum it
[01:00:06.180 --> 01:00:06.700]   back.
[01:00:06.700 --> 01:00:13.900]   Biden 10. If he's awake, extend this window by 90 days. I don't
[01:00:13.900 --> 01:00:16.940]   know if grandpa is up for it, but he could extend it a bit.
[01:00:16.940 --> 01:00:20.540]   And obviously this with the Trump campaign, you know, he
[01:00:20.540 --> 01:00:23.060]   says a lot of different things. Trump has flip flopped a couple
[01:00:23.060 --> 01:00:26.340]   of times during his first term, he tried to bandit wait till he
[01:00:26.340 --> 01:00:30.580]   gets the readout that the rest of these guys got. Yeah, well,
[01:00:30.580 --> 01:00:34.060]   and then he had mega donor and tick tock investor, Jeff Yoss
[01:00:34.060 --> 01:00:38.300]   gave PAX $50 million, and he owns 15% of it.
[01:00:38.300 --> 01:00:42.380]   When I had dinner with him, that's his This is the one of
[01:00:42.380 --> 01:00:44.620]   the things that I pointed out to him was this specific thing. I
[01:00:44.620 --> 01:00:50.340]   said, you have to look at the Pegasus like equivalent
[01:00:50.340 --> 01:00:54.100]   infant infiltration of WhatsApp, having been done on tick tock,
[01:00:54.100 --> 01:00:58.180]   because the reality is, if tick tock has 200 million users, it
[01:00:58.180 --> 01:01:06.100]   is true, that will, you know, 199,999,000 just don't matter.
[01:01:06.100 --> 01:01:08.540]   So you could turn on the microphone, you're not going to
[01:01:08.540 --> 01:01:13.380]   hear anything. But there's probably 1000 to 10,000 to 15,000
[01:01:13.380 --> 01:01:18.060]   people. And I do suspect that state actors are smart enough to
[01:01:18.060 --> 01:01:22.740]   figure out how to triangulate who you would want to be able to
[01:01:22.740 --> 01:01:25.740]   turn it on when that phone is in your pocket, or when that phone
[01:01:25.740 --> 01:01:29.540]   is on a desk. And I'm sure you will hear all kinds of random
[01:01:29.540 --> 01:01:33.100]   things. And many of those things could be quite sensitive in
[01:01:33.100 --> 01:01:37.180]   nature. So I think that this is something that the government's
[01:01:37.180 --> 01:01:38.900]   gonna have to look at really intensely.
[01:01:38.900 --> 01:01:41.340]   There's such a simple test here. What are they doing with their
[01:01:41.340 --> 01:01:45.340]   own people in China, they are in a police state there, they make
[01:01:45.340 --> 01:01:48.940]   the Stasi jealous how much they're tracking individuals in
[01:01:48.940 --> 01:01:51.300]   China. So if they'll do it to their own people, they would
[01:01:51.300 --> 01:01:55.180]   have no problem doing it to an adversary. And ask yourself, if
[01:01:55.220 --> 01:01:58.100]   the shareholders care about money, they would be willing to
[01:01:58.100 --> 01:02:02.660]   divest, right? No problem. They want to take the company public.
[01:02:02.660 --> 01:02:06.820]   So if you won't divest and get off the board as the CCP, it's
[01:02:06.820 --> 01:02:09.220]   because you see this as a valuable tool, right? I mean,
[01:02:09.220 --> 01:02:11.900]   just work through the basic logic, folks, right at some
[01:02:11.900 --> 01:02:14.940]   price point, right? What I would do if I were president, I'd say,
[01:02:14.940 --> 01:02:18.140]   you tell me the fair market price for tick tock. And I'll go
[01:02:18.140 --> 01:02:21.020]   far, you know, I'll make sure that there's buyers, right?
[01:02:21.020 --> 01:02:26.300]   Because CCB won't name any price. There is no greatest
[01:02:26.300 --> 01:02:30.380]   point, right? That'd be like giving up your nukes. I don't
[01:02:30.380 --> 01:02:32.780]   think they should have to sell at a less than fair market
[01:02:32.780 --> 01:02:35.260]   price. I think that's, you know, legit, like I believe in
[01:02:35.260 --> 01:02:40.220]   capitalism. But there any free market should allow for, you
[01:02:40.220 --> 01:02:43.100]   know, some price discovery. And if they can't name any price,
[01:02:43.100 --> 01:02:46.100]   period, that suggests that you're doing something that's
[01:02:46.100 --> 01:02:49.380]   nefarious. There's no reason by the way, come off, you probably
[01:02:49.380 --> 01:02:52.260]   know this. You know, if you meet like, at least, you know, some
[01:02:52.260 --> 01:02:54.540]   of the times I've met with the president, they take your phone
[01:02:54.540 --> 01:02:58.940]   away. Why do they take your phone away? 100%? Exactly.
[01:02:58.940 --> 01:03:02.460]   Pretty obvious. Yeah, exactly. Also, there are phones that can
[01:03:02.460 --> 01:03:05.340]   be designed to be weapons, as we've seen a number of people
[01:03:05.340 --> 01:03:08.900]   got some nutshots with their pagers recently. All right,
[01:03:08.900 --> 01:03:11.060]   let's talk about venture as we wrap up here, man, Keith, for
[01:03:11.060 --> 01:03:15.980]   boy cooking with oil, new sacks, new red meat sacks. Are you a
[01:03:15.980 --> 01:03:19.340]   steak guy too? He needs to eat a steak once in a while. Oh, of
[01:03:19.340 --> 01:03:21.980]   course. What's your cut? What's your cut? Are you cool? Ed? Are
[01:03:21.980 --> 01:03:23.580]   you a ribeye guy?
[01:03:23.580 --> 01:03:30.260]   eight to 1012 ounces max. medium, you know, solid. I don't
[01:03:30.260 --> 01:03:34.700]   really care. Yeah, but why do you all I'll be a little non
[01:03:34.700 --> 01:03:36.180]   American go Wagyu. You know, like,
[01:03:36.180 --> 01:03:40.320]   yeah, sure. All right. All right. I think just shout out to
[01:03:40.320 --> 01:03:43.780]   my friend Kimbo. You know, red meat Republic. Of course, of
[01:03:43.780 --> 01:03:47.380]   course, I'm a big fan of this cool let slash the Kanye. Oh,
[01:03:47.380 --> 01:03:52.060]   you know, you know what? Yes, I just bought recently a Denver
[01:03:52.060 --> 01:03:56.180]   cut, which I'd never tried. Incredible. Denver. All right,
[01:03:56.180 --> 01:03:59.660]   let's wrap up with a venture update here at the darkest hour
[01:03:59.660 --> 01:04:05.820]   Keith sometimes before the dawn VC deal activity has getting
[01:04:05.820 --> 01:04:10.620]   close to pre COVID numbers in 2019. Obviously, major funding
[01:04:10.620 --> 01:04:13.940]   drop off happened in 2022. Yeah, it's been a couple of years of
[01:04:13.940 --> 01:04:17.500]   this. But deals, the number of deals is coming back the amount
[01:04:17.500 --> 01:04:20.580]   being put to work coming back to I would say the steady state,
[01:04:20.580 --> 01:04:27.380]   perhaps even normal level. And but VC exits, sadly, are not
[01:04:27.380 --> 01:04:30.500]   keeping up. But we did have service now go public today up
[01:04:30.500 --> 01:04:36.420]   50%. And the wrath of Lena Khan is officially over. She's out
[01:04:36.420 --> 01:04:39.980]   Andrew Ferguson is in this looks like a great appointment.
[01:04:39.980 --> 01:04:46.100]   Another one by Trump in the column of somebody who wants to
[01:04:46.100 --> 01:04:50.460]   allow business to occur and wants a free market. He wants to
[01:04:50.460 --> 01:04:53.900]   do basically everything Lena Khan didn't do, which is allow
[01:04:53.900 --> 01:04:59.380]   some M&A. Roy, what's your pulse like here? And what's your take
[01:04:59.380 --> 01:05:02.300]   on the venture industry, and then we'll go into M&A and exits.
[01:05:02.300 --> 01:05:05.140]   We'll start with just investing. Is it is it ramping up? You
[01:05:05.140 --> 01:05:06.700]   seeing high quality companies?
[01:05:07.100 --> 01:05:10.780]   Well, I'd say that you should cut that data by AI and non AI
[01:05:10.780 --> 01:05:14.420]   and you might see a tell two different cities. My anecdotal
[01:05:14.420 --> 01:05:17.620]   experiences, there's AI companies where the market's
[01:05:17.620 --> 01:05:21.020]   pretty hot, maybe cooling a little bit but hot and AI
[01:05:21.020 --> 01:05:23.860]   companies with the right team are getting funded, you know,
[01:05:23.860 --> 01:05:27.300]   frequently, quickly, etc. And then there's non AI companies.
[01:05:27.300 --> 01:05:30.300]   And I think you'll see a very different, you know, sort of
[01:05:30.300 --> 01:05:33.020]   chart there. I do think net net, you're probably at a steady
[01:05:33.020 --> 01:05:35.700]   state that looks reasonable across 40 years, you know, like
[01:05:35.700 --> 01:05:39.540]   etc. But it would be interesting and you know, you have to make
[01:05:39.540 --> 01:05:41.660]   some methodological decisions about what's an AI company,
[01:05:41.660 --> 01:05:44.580]   what's not, but if you could do that, it'd be interesting to see
[01:05:44.580 --> 01:05:48.660]   if the lines, you know, look similar or not. But it's pretty
[01:05:48.660 --> 01:05:50.940]   hot, like people started companies found founders are
[01:05:50.940 --> 01:05:53.780]   optimistic. Crypto companies also, you know, are back in
[01:05:53.780 --> 01:05:56.980]   vogue, obviously, due to the change in administration. I
[01:05:56.980 --> 01:05:59.460]   think a lot of people had been hesitant to start new crypto
[01:05:59.460 --> 01:06:02.740]   companies. And of course, yeah, confidence in the new
[01:06:02.740 --> 01:06:06.100]   administration, SEC, etc. So we'll see if innovation
[01:06:06.100 --> 01:06:09.020]   accelerates, you know, with all the new capital and all the new
[01:06:09.020 --> 01:06:14.820]   founders back in crypto, in enterprise software, it had been
[01:06:14.820 --> 01:06:18.300]   pretty cool, non AI based enterprise software. But, you
[01:06:18.300 --> 01:06:21.900]   know, like, as you mentioned, with the IPO this week, trading
[01:06:21.900 --> 01:06:25.140]   very aggressively, I think, you know, maybe there's some
[01:06:25.140 --> 01:06:28.500]   inspired inspiration there for, you know, more traditional,
[01:06:28.540 --> 01:06:32.660]   boring companies, Stripe, Stripe, Stripe should go public.
[01:06:32.660 --> 01:06:34.220]   But, you know, they don't listen to me. So
[01:06:34.220 --> 01:06:37.820]   yeah, you didn't know, why aren't they going public? Keith?
[01:06:37.820 --> 01:06:40.140]   What's what's the story with the boys over there? Why are they
[01:06:40.140 --> 01:06:42.980]   waiting to get disrupted by crypto stablecoins?
[01:06:42.980 --> 01:06:46.580]   So seriously, talk about missing your window. Get out there,
[01:06:46.580 --> 01:06:47.140]   boys.
[01:06:47.140 --> 01:06:49.380]   They bought that crypto stablecoin company for like a
[01:06:49.380 --> 01:06:52.700]   billion dollars. Yeah. And I personally believe and
[01:06:52.700 --> 01:06:54.540]   subscribe to the the company should go public as early as
[01:06:54.540 --> 01:06:56.740]   possible. And the group Bill Gurley, you know, sort of
[01:06:56.740 --> 01:06:59.140]   school thought, what amount of revenue, what amount of revenue
[01:06:59.140 --> 01:07:02.060]   for 50 million minimum, but predictability matters, you
[01:07:02.060 --> 01:07:05.500]   know, definitely. So not just 50, like, but 50 with line of
[01:07:05.500 --> 01:07:09.580]   sight to 100. And knowing 100, 200, I wrote a whole chapter in
[01:07:09.580 --> 01:07:12.020]   Ilad Gil's, you know, high growth handbook on why companies
[01:07:12.020 --> 01:07:14.620]   should go public as early as possible. So I've been on this
[01:07:14.620 --> 01:07:17.420]   crusade forever. I like accountability, transparency,
[01:07:17.420 --> 01:07:21.020]   discipline, I think are good things. And, you know, there's a
[01:07:21.020 --> 01:07:23.180]   critique that like, oh, you're not gonna be innovative
[01:07:23.180 --> 01:07:25.260]   anymore. If you look at some of the companies we've been
[01:07:25.260 --> 01:07:27.500]   talking about, what are some of the most innovative companies in
[01:07:27.500 --> 01:07:29.940]   the world? They're public companies, it just takes the
[01:07:29.940 --> 01:07:33.220]   right leader to say, I'm going to be innovative. And I don't
[01:07:33.220 --> 01:07:35.940]   care what the bureaucrats and lawyers, you know, I'm just not
[01:07:35.940 --> 01:07:39.380]   going to get distracted with that. And so I like public
[01:07:39.380 --> 01:07:41.900]   companies. And so I think you'll see a lot of the companies I am
[01:07:41.900 --> 01:07:46.380]   involved in go public at a fairly rapid clip by hysterical
[01:07:46.380 --> 01:07:49.500]   standards. Different founders, though, have different sort of
[01:07:49.500 --> 01:07:53.020]   views on this. It's very reasonable. Stripe, SpaceX, you
[01:07:53.020 --> 01:07:56.860]   know, for example, founded 2003. Who knows, you know, when it's
[01:07:56.860 --> 01:07:59.460]   going to be a public company. So you can have a very successful
[01:07:59.460 --> 01:08:06.140]   company like SpaceX or Stripe. But my preference is to go
[01:08:06.140 --> 01:08:09.540]   public early and then you have the capital resources, equity or
[01:08:09.540 --> 01:08:12.420]   capital to be strategic and per your point about potentially
[01:08:12.420 --> 01:08:15.340]   missing window, they were able to transact, you know, in that
[01:08:15.340 --> 01:08:18.500]   particular case, and get ahead of the curve, or at least not
[01:08:18.500 --> 01:08:21.260]   miss the curve. But sometimes when you're a private company,
[01:08:21.300 --> 01:08:24.900]   there are strategic assets that you can't get your hands on. And
[01:08:24.900 --> 01:08:27.660]   you know, think about Facebook buying Instagram, we talked
[01:08:27.660 --> 01:08:31.140]   about the taste issue. Instagram had taste at the time, like
[01:08:31.140 --> 01:08:36.660]   Kevin had taste. And think about where meta would be. Have they
[01:08:36.660 --> 01:08:38.380]   not been able to acquire Instagram?
[01:08:38.380 --> 01:08:42.380]   Yeah, we totally totally different timeline for the
[01:08:42.380 --> 01:08:44.980]   public currency to have bought. What's up? Yeah, well, what's
[01:08:44.980 --> 01:08:49.460]   up? So optionality increases as you have that public currency.
[01:08:49.460 --> 01:08:52.980]   If you're a private buying a private, the acquired company
[01:08:52.980 --> 01:08:55.100]   needs to believe that you're going to get it over the finish
[01:08:55.100 --> 01:08:56.900]   line, they're going to have some liquidity at some point, whereas
[01:08:56.900 --> 01:08:58.900]   with the public, I mean, they just sell within whatever their
[01:08:58.900 --> 01:09:00.300]   window is, Chamath, your thoughts?
[01:09:00.300 --> 01:09:02.580]   Well, I was just gonna ask you the question, which is what is
[01:09:02.580 --> 01:09:06.180]   the game theory behind Stripe not going out? Like, what's the
[01:09:06.180 --> 01:09:09.100]   what's the strategic rationale?
[01:09:09.100 --> 01:09:12.300]   You know, honestly, yeah, without sharing, like, you know,
[01:09:12.300 --> 01:09:15.780]   one on one conversation sort of stuff. I think the question the
[01:09:15.780 --> 01:09:18.460]   burden, they inverted the burden, which is why should we
[01:09:18.460 --> 01:09:21.740]   go public? A lot of fat, a lot of people ask the question the
[01:09:21.740 --> 01:09:26.820]   opposite way, which is, you know, why wouldn't I go public? I
[01:09:26.820 --> 01:09:29.540]   think their first principle thinkers like put put my point
[01:09:29.540 --> 01:09:32.420]   about Trump, and I think true of Elon, they asked, like, why?
[01:09:32.420 --> 01:09:36.300]   And they're like, well, what advantages would we get? And I
[01:09:36.300 --> 01:09:40.580]   actually think the M&A one is very real. I think they've been
[01:09:40.580 --> 01:09:45.380]   able to construct alternatives to most of the advantages, but
[01:09:45.380 --> 01:09:47.540]   not every company is going to be able to do that. It took a lot
[01:09:47.540 --> 01:09:50.020]   of effort, energy, and then the question is, would you substitute
[01:09:50.020 --> 01:09:52.860]   that energy into something else that might be higher value
[01:09:52.860 --> 01:09:55.980]   creation, if you weren't to like, you know, creating the
[01:09:55.980 --> 01:09:57.380]   alternatives to a public structure?
[01:09:57.380 --> 01:10:00.820]   That is a great answer. Freeberg, your thoughts here on
[01:10:00.820 --> 01:10:05.180]   public markets, M&A, the end of the wrath of Lena Khan, and what
[01:10:05.180 --> 01:10:07.980]   we might see in the new year, post January.
[01:10:07.980 --> 01:10:12.100]   I disagree with the conflation of Lena Khan into all this stuff
[01:10:12.100 --> 01:10:15.620]   too much. It's she's a big company break apart, kind of
[01:10:15.620 --> 01:10:18.540]   mandated has nothing to do with tech M&A small, like the typical
[01:10:18.540 --> 01:10:20.940]   kind of deal flow stuff that I think you're focused on. I've
[01:10:20.940 --> 01:10:23.580]   said this a number of times, so I'll just say it again. But I
[01:10:23.580 --> 01:10:26.180]   think on the on the general liquidity thing, I don't think
[01:10:26.180 --> 01:10:29.260]   that the thing holding up acquisitions and IPOs is markets.
[01:10:29.260 --> 01:10:32.660]   I actually think it's investor and board expectations on
[01:10:32.660 --> 01:10:35.380]   valuation, relative to where they put money in the last
[01:10:35.380 --> 01:10:39.740]   couple of years. So if you look at the valuations from 21 to 23,
[01:10:39.740 --> 01:10:42.860]   you know, early 23, so much money went in at such high
[01:10:42.860 --> 01:10:46.780]   prices, those investors can take those companies public today,
[01:10:46.780 --> 01:10:49.060]   there are public, there's a public market appetite at all
[01:10:49.060 --> 01:10:53.860]   times, for anything just depends on evaluation. And the real
[01:10:53.860 --> 01:10:58.020]   issue right now is that a lot of the VCs, the late stage private
[01:10:58.020 --> 01:11:00.660]   equities, the ones that did the big markups in the late stage D
[01:11:00.660 --> 01:11:03.780]   rounds, e rounds, etc. They don't want to take these things
[01:11:03.780 --> 01:11:07.740]   out and take a 60 70% haircut on the IPO, they'd rather kind of
[01:11:07.740 --> 01:11:10.420]   let this thing sit private, and see if they can earn their way
[01:11:10.420 --> 01:11:12.900]   back into the valuation that they did the market when they
[01:11:12.900 --> 01:11:16.860]   put the round together. So I 100% agree 100% agree with this.
[01:11:16.860 --> 01:11:20.500]   Yeah, I mean, there there is these overhangs are very real. I
[01:11:20.500 --> 01:11:23.460]   don't think the exit liquidity dearth fundamentally, is driven
[01:11:23.460 --> 01:11:27.420]   by markets. I think it's just driven by the the sell side and
[01:11:27.420 --> 01:11:28.340]   the investor expectations.
[01:11:28.340 --> 01:11:31.260]   There's also a culture thing I have to say, like in talking to
[01:11:31.260 --> 01:11:34.220]   a number of like, the leaders of these public companies that
[01:11:34.220 --> 01:11:38.380]   were very inquisitive and M&A, they are taking the position,
[01:11:38.420 --> 01:11:41.740]   it's easier for us to build a competing function, a competing
[01:11:41.740 --> 01:11:45.340]   adjacency than do any tuck ins. If they just told the the corp
[01:11:45.340 --> 01:11:49.100]   dev people pencils down, and they are not wanting to spend a
[01:11:49.100 --> 01:11:52.700]   year or two on a deal, when they have a breakup fee when they see
[01:11:52.700 --> 01:11:55.300]   what happened with Adobe, or they just think, well, why don't
[01:11:55.300 --> 01:11:57.700]   those are big build it ourselves? Yeah, those are big
[01:11:57.700 --> 01:11:59.940]   deals. The small deals are different. The small deals are
[01:11:59.940 --> 01:12:02.660]   more the small deals are the fact that over a couple years,
[01:12:02.660 --> 01:12:04.420]   like Google bought a cybersecurity, like there are
[01:12:04.420 --> 01:12:07.420]   these acquisitions that happened at 800 million that maybe
[01:12:07.420 --> 01:12:10.300]   should have been done at 200 to 50. And again, it's the same
[01:12:10.300 --> 01:12:13.060]   problem that the sellers of the high quality companies demand
[01:12:13.060 --> 01:12:15.860]   too high a premium, that the buyers of these rational scaled
[01:12:15.860 --> 01:12:18.460]   companies are like, that's not worth it, just like IPO
[01:12:18.460 --> 01:12:21.140]   candidates. So I don't know if you've got a
[01:12:21.140 --> 01:12:23.940]   experience. I also agree. So I used to be an antitrust
[01:12:23.940 --> 01:12:28.660]   litigator, which I know Jason knows. And I think I hate like,
[01:12:28.660 --> 01:12:31.700]   you know, the current leadership antitrust division, I called her
[01:12:31.700 --> 01:12:34.740]   a fraud. And I think she is intellectually a fraud. Why? Why
[01:12:34.740 --> 01:12:37.980]   Why? Well, she published a paper that was false, like, like,
[01:12:37.980 --> 01:12:43.460]   her whole claim to fame is this paper about Amazon. And the data
[01:12:43.460 --> 01:12:47.060]   in the Amazon, the data she used in the in the paper at Yale was
[01:12:47.060 --> 01:12:49.500]   false. And in fact, she's too smart to have done it
[01:12:49.500 --> 01:12:52.260]   accidentally. Benedict Evans wrote a good critique of it, if
[01:12:52.260 --> 01:12:54.260]   you want to read all about the data. Second thing is you look
[01:12:54.260 --> 01:12:58.300]   at Amazon, which is this alleged quintessential example. Have you
[01:12:58.300 --> 01:13:02.020]   heard of Shopify? Shopify is one of the biggest success stories
[01:13:02.020 --> 01:13:04.620]   of the last decade, right down the middle competitive with
[01:13:04.620 --> 01:13:07.380]   Amazon, and there was nothing Amazon could do. They lost the
[01:13:07.380 --> 01:13:11.180]   core market to a competitor that was started and you know, went
[01:13:11.180 --> 01:13:14.500]   public at a very low valuation. And Shopify is dominating DTC
[01:13:14.500 --> 01:13:16.860]   commerce. Nobody builds a DTC commerce brand except on
[01:13:16.860 --> 01:13:22.860]   Shopify. So everything about her is like a fraud. That said, I
[01:13:22.860 --> 01:13:25.660]   don't believe that what she's done has affected, you know,
[01:13:25.660 --> 01:13:28.460]   exits very much at all. Because like the truth is in high end
[01:13:28.460 --> 01:13:30.980]   venture, like institutional venture capital, other than the
[01:13:30.980 --> 01:13:34.580]   WhatsApp and like, maybe like a plot acquisition or something,
[01:13:34.580 --> 01:13:38.260]   you know, one every two years, you don't drive returns in
[01:13:38.260 --> 01:13:40.740]   venture to an institutional venture capital fund through an
[01:13:40.740 --> 01:13:44.900]   M&A. Like, like WhatsApp may be the only one that really drove a
[01:13:44.900 --> 01:13:48.540]   fund returning exit to a serious fund. It just doesn't happen
[01:13:48.540 --> 01:13:52.500]   that way. I need IPOs to return our funds. Our funds are like
[01:13:52.500 --> 01:13:55.180]   billions of dollars. You don't get returns on lots of
[01:13:55.180 --> 01:13:57.620]   acquisitions at $50 million to $100 million.
[01:13:57.660 --> 01:14:00.220]   Yeah, but a $300 million fund can.
[01:14:00.220 --> 01:14:04.260]   Yeah, but most funds have ballooned for lots of reasons.
[01:14:04.260 --> 01:14:06.580]   There are a lot of big ones. And that's actually led to some of
[01:14:06.580 --> 01:14:09.620]   the perversity that, you know, David talked about. If the funds
[01:14:09.620 --> 01:14:14.220]   get large, their tendency to do these things also increases. So
[01:14:14.220 --> 01:14:19.060]   yes, a seed fund can drive returns on M&A acquisitions that
[01:14:19.060 --> 01:14:22.220]   might be deterred in a bad, you know, hostile administration.
[01:14:22.220 --> 01:14:27.140]   But a venture fund of $500 million to $2 billion drive
[01:14:27.140 --> 01:14:28.620]   returns through M&A? No.
[01:14:28.620 --> 01:14:32.340]   No, I mean, they could fill in the first one x maybe.
[01:14:32.340 --> 01:14:34.420]   Yeah, I've been lobbying officials for a long time on
[01:14:34.420 --> 01:14:37.460]   this crusade. They come in and they expect that, oh, you know,
[01:14:37.460 --> 01:14:40.140]   M&A, blah, blah, blah, blah. And I'm like, No, no, no, I could
[01:14:40.140 --> 01:14:43.300]   care less. Only my most mediocre companies are quiet.
[01:14:43.300 --> 01:14:46.300]   And the IPO window has started to crack open. Just give me a
[01:14:46.300 --> 01:14:48.540]   couple more data points. Gentlemen, we ride the Chinese
[01:14:48.540 --> 01:14:54.420]   based self driving car company went public on the NASDAQ last
[01:14:54.420 --> 01:14:58.820]   month $4.5 billion market cap pony AI, another Chinese based
[01:14:58.820 --> 01:15:04.140]   self driving car company went public in November. Klarna plans
[01:15:04.140 --> 01:15:06.780]   to go public in the US. I think they quietly filed service
[01:15:06.780 --> 01:15:11.500]   Titan today, the taping of this on Thursday and sheen the fast
[01:15:11.500 --> 01:15:12.500]   fashion company.
[01:15:12.500 --> 01:15:14.460]   Well, she's gonna have some real problems in the new
[01:15:14.460 --> 01:15:15.180]   administration.
[01:15:15.180 --> 01:15:19.900]   Yeah, right tariffs. As we wrap here, that's a good place to
[01:15:19.900 --> 01:15:23.340]   wrap on. What do you think of, I guess, two issues, Keith, we'll
[01:15:23.340 --> 01:15:27.980]   end with politics as we started there. Two things seem perplexing
[01:15:27.980 --> 01:15:34.180]   to people in finance. One is tariffs and how that's going to
[01:15:34.180 --> 01:15:39.700]   work. And the other is inflation and what the impact would be on
[01:15:39.700 --> 01:15:42.900]   15 million people being shipped out of the country and the
[01:15:42.900 --> 01:15:47.460]   impact that would have on inflation vis a vis the
[01:15:47.460 --> 01:15:50.500]   unemployment rate getting even lower than the historic low
[01:15:50.500 --> 01:15:52.740]   it's been in our lifetimes. What do you think of those two
[01:15:52.740 --> 01:15:53.940]   issues? Take either one?
[01:15:53.940 --> 01:15:56.740]   Well, I think first of all, the Treasury Secretary understands
[01:15:56.740 --> 01:15:59.220]   this, like I think Scott actually, from everybody I've
[01:15:59.220 --> 01:16:02.260]   talked to, I don't know him. But I've interviewed about 10 or 12
[01:16:02.260 --> 01:16:04.740]   people that are very successful Wall Street, and every single
[01:16:04.740 --> 01:16:07.900]   one of them universally has a claim for him. And they've all
[01:16:07.900 --> 01:16:12.100]   made points to me that these kind of subtleties, he really
[01:16:12.100 --> 01:16:16.220]   does grok. And so when he when he says you can raise tariffs
[01:16:16.220 --> 01:16:19.260]   without inflation, he understands how all this
[01:16:19.260 --> 01:16:22.020]   substitution works and is running an equation in his
[01:16:22.020 --> 01:16:24.620]   brain. He's a very successful trader. And that's a really good
[01:16:24.620 --> 01:16:28.500]   skill. Also, Chamath's earlier points about Darwinistic
[01:16:28.500 --> 01:16:31.100]   evolution of brilliant people being successful, understanding
[01:16:31.100 --> 01:16:34.900]   how the economy actually works. He's perfect. So I think you're
[01:16:34.900 --> 01:16:39.260]   going to see real tariffs, like especially against China. And
[01:16:39.260 --> 01:16:42.500]   the Trump administration is completely committed to reducing
[01:16:42.500 --> 01:16:45.860]   inflation, the cost of eggs, you know, and groceries. And I think
[01:16:45.860 --> 01:16:49.420]   those things can be reconciled. I know, amateur economists, you
[01:16:49.420 --> 01:16:52.580]   know, with random degrees from Warren don't think so. But as
[01:16:52.580 --> 01:16:55.340]   Trump pointed out in the debate, and you know, JD Vance pointed
[01:16:55.340 --> 01:16:59.580]   out with some research, his first administration imposed all
[01:16:59.580 --> 01:17:03.900]   these tariffs, and there was no inflation. And then JD Vance
[01:17:03.900 --> 01:17:07.100]   pointed out that the Fed Reserve has a study that says, you know,
[01:17:07.100 --> 01:17:10.540]   housing prices, which are primary driver of affordability
[01:17:10.540 --> 01:17:13.660]   for a normal American are inflated because of illegal
[01:17:13.660 --> 01:17:16.700]   immigration. So I think this administration
[01:17:16.700 --> 01:17:18.260]   and not making more units, I mean,
[01:17:18.260 --> 01:17:21.140]   well, I think this administration, there is
[01:17:21.140 --> 01:17:23.300]   substitution, right? If the price of one good goes up,
[01:17:23.300 --> 01:17:26.220]   consumers typically have, you know, like people on this
[01:17:26.220 --> 01:17:29.580]   podcast, typically have a limited budget. And at the end
[01:17:29.580 --> 01:17:31.980]   of the day, if one price goes up, you have to substitute
[01:17:31.980 --> 01:17:34.540]   somewhere else. So the debt deflation might be negative,
[01:17:34.540 --> 01:17:38.140]   even. So in any event, this administration is not naive. And
[01:17:38.140 --> 01:17:42.340]   the people that Trump has put in place to drive this in Treasury,
[01:17:42.340 --> 01:17:45.260]   the National Economic Council absolutely understand this. And
[01:17:45.260 --> 01:17:48.740]   they're committed to driving down actual prices, the hardest
[01:17:48.740 --> 01:17:51.300]   area is going to be healthcare, that is really, really
[01:17:51.300 --> 01:17:55.100]   difficult. And it is a major driver of cost to a normal
[01:17:55.100 --> 01:17:58.100]   person. And Obamacare has been a disaster. The question is, what
[01:17:58.100 --> 01:18:00.100]   do you do about that? And I don't have an answer for you
[01:18:00.100 --> 01:18:01.940]   right away. I have some ideas.
[01:18:01.940 --> 01:18:07.060]   Jamal, your thoughts on tariffs and or immigration and
[01:18:07.060 --> 01:18:10.780]   inflation. And that seems to be a place where even across the
[01:18:10.780 --> 01:18:12.020]   aisle, you got people debating,
[01:18:12.020 --> 01:18:14.940]   I think he said it pretty well, which is that we do have a lot
[01:18:14.940 --> 01:18:19.380]   of experience with how tariffs can be implemented, and that the
[01:18:19.380 --> 01:18:24.940]   practical experience is not what the fear mongering is about.
[01:18:24.940 --> 01:18:27.580]   That being said, I think the devil will be in the details,
[01:18:27.580 --> 01:18:32.460]   which markets for which goods, what is the tariff and why. What
[01:18:32.460 --> 01:18:36.340]   I hope happens is that if we can really study the second and
[01:18:36.340 --> 01:18:41.780]   third order degree impacts of some of these markets, what the
[01:18:41.780 --> 01:18:45.980]   tariff does is it acts almost as a reverse subsidy for American
[01:18:45.980 --> 01:18:50.740]   companies to compete. I'll give you one very narrow example. If
[01:18:50.740 --> 01:18:56.540]   you believe in electrifying the American economy, one of the
[01:18:56.540 --> 01:18:58.940]   underlying things that you need are electric motors, right
[01:18:58.940 --> 01:19:01.700]   electric motors and electric batteries. And just to double
[01:19:01.700 --> 01:19:05.460]   click on electric motors for a second, electric motors needs a
[01:19:05.460 --> 01:19:08.540]   permanent magnet. If you look inside of a permanent magnet,
[01:19:08.540 --> 01:19:12.020]   there are these rare earths that are just required to make these
[01:19:12.020 --> 01:19:17.580]   magnets. But as it turns out, it is impossible for any company
[01:19:17.580 --> 01:19:21.980]   that is not a Chinese company, to be able to manufacture these
[01:19:21.980 --> 01:19:27.100]   magnets in an economically viable way. And the reason is
[01:19:27.100 --> 01:19:30.260]   because not only can China make it, and they can make it in the
[01:19:30.260 --> 01:19:33.220]   absence of a lot of environmental controls, they'll
[01:19:33.220 --> 01:19:36.300]   also subsidize. So if anybody tries to compete, they'll just
[01:19:36.300 --> 01:19:39.460]   inject a subsidy into that economic supply chain, where
[01:19:39.460 --> 01:19:43.180]   they're always cheaper. So what do you do, Jason, if you can
[01:19:43.180 --> 01:19:48.100]   observe that, and realize that we want supply chain diversity,
[01:19:48.100 --> 01:19:51.780]   what the tariff does is it starts to push back on those
[01:19:51.780 --> 01:19:54.500]   kinds of activities, because it doesn't allow it to continue to
[01:19:54.500 --> 01:19:58.500]   work. Then if you take that with what the United States
[01:19:58.500 --> 01:20:01.700]   government already does through the DOE, which is through
[01:20:01.700 --> 01:20:06.140]   subsidies and underwriting, this is what I mean by it could be a
[01:20:06.140 --> 01:20:09.940]   really amazing new moment for the American economy. And that
[01:20:09.940 --> 01:20:12.340]   would please a lot of people, including a lot of people on the
[01:20:12.340 --> 01:20:15.260]   left, if they just took the time to understand what's being
[01:20:15.260 --> 01:20:15.740]   proposed.
[01:20:15.740 --> 01:20:19.500]   I mean, look at rare earths, you mentioned them, we have some of
[01:20:19.500 --> 01:20:24.380]   the great deposits in the world in our country, why largest
[01:20:24.380 --> 01:20:27.060]   lithium deposits in the world, it's because of environmental
[01:20:27.060 --> 01:20:30.060]   regulations. And it's the same thing with Starship going up.
[01:20:30.060 --> 01:20:32.540]   We got a lot of regulations this country, I understand people
[01:20:32.540 --> 01:20:34.820]   want to do the right thing. But we also have a lot of debt. And
[01:20:34.820 --> 01:20:38.820]   if we could start mining rare earth metals here, and maybe
[01:20:38.820 --> 01:20:39.940]   loosen the regulations,
[01:20:39.940 --> 01:20:44.020]   the lithium supply chain is another really good one that's
[01:20:44.020 --> 01:20:46.700]   emblematic of all of this. But there are so many other markets
[01:20:46.700 --> 01:20:49.340]   like I'm sure that if you looked in something that's a little bit
[01:20:49.340 --> 01:20:52.780]   more down the middle, like appliances, what you would also
[01:20:52.780 --> 01:20:55.780]   see is the same thing where Whirlpool will try to make an
[01:20:55.780 --> 01:21:00.380]   appliance or Bissell will try to make a vacuum cleaner, and the
[01:21:00.380 --> 01:21:05.900]   Chinese equivalent makes it almost like fighting uphill. And
[01:21:05.900 --> 01:21:07.740]   there has to be a way to course correct that.
[01:21:07.740 --> 01:21:11.180]   But I mean, look at BYD, some of these cars, they're just copying
[01:21:11.180 --> 01:21:16.460]   wholesale, everything Tesla has done in their design and feature
[01:21:16.460 --> 01:21:19.460]   set, and they're half the price. And so they're selling really
[01:21:19.460 --> 01:21:22.580]   well in countries that allow them and they would decimate us
[01:21:22.580 --> 01:21:25.140]   automakers, and German automakers, if by the way,
[01:21:25.140 --> 01:21:28.820]   different a different version of this, I wrote this in my weekly
[01:21:28.820 --> 01:21:32.420]   newsletter, but I was curious why these Chinese models are so
[01:21:32.420 --> 01:21:35.700]   good. And I was like, the training of these models seems
[01:21:35.700 --> 01:21:38.300]   to be quite fast, the quality of these models are really good,
[01:21:38.300 --> 01:21:43.420]   the Alibaba model, etc. And part of what you realize is when they
[01:21:43.420 --> 01:21:47.300]   do their training runs, the Chinese models have no guard
[01:21:47.300 --> 01:21:52.220]   rails, in the sense that there's no copyright checks. No, there's
[01:21:52.220 --> 01:21:54.780]   just none of these things that otherwise slow down an American
[01:21:54.780 --> 01:21:58.140]   company to make a useful model. They don't have any of those
[01:21:58.140 --> 01:22:00.780]   things. So that's another example where maybe there's not
[01:22:00.780 --> 01:22:05.140]   an economic tariff, but there has to be a simple reciprocity.
[01:22:05.140 --> 01:22:08.620]   And in the absence of reciprocity, I think it's
[01:22:08.620 --> 01:22:11.380]   reasonable to say that there needs to be checks and balances.
[01:22:11.380 --> 01:22:15.380]   Freeberg, anything to add there on tariffs, or okay, for
[01:22:15.380 --> 01:22:18.420]   jamaath palya hapatia, your chairman, dictator, the Sultan
[01:22:18.420 --> 01:22:23.780]   of science, and new sacks, better BMI hates dictators,
[01:22:23.780 --> 01:22:26.220]   what's your take on Ukraine? Maybe we could fire up.
[01:22:26.820 --> 01:22:27.300]   Discussion.
[01:22:27.300 --> 01:22:37.100]   By the time you have me back, hopefully that's solved.
[01:22:37.100 --> 01:22:40.740]   Hopefully that's up day one. We're gonna solve it on day one.
[01:22:40.740 --> 01:22:44.060]   Okay. Keith, you were excellent. You're tremendous. Keith.
[01:22:44.060 --> 01:22:48.980]   Excellent husband. Very, very effective. Great, great
[01:22:48.980 --> 01:22:52.580]   American great hosts. Great. Awesome. On you may have heard
[01:22:52.580 --> 01:22:56.020]   of two words all in. Okay, you may have heard of it. David
[01:22:56.020 --> 01:22:59.740]   Sachs, brilliant crypto. I mean, we didn't even dive into
[01:22:59.740 --> 01:23:05.180]   crypto. I mean, people are going a little bit on this. What do
[01:23:05.180 --> 01:23:07.740]   you think about all this crazy crypto going on here, Keith,
[01:23:07.740 --> 01:23:12.260]   everything spiking back up XRP, Bitcoin? What do you think of
[01:23:12.260 --> 01:23:16.820]   sailor? I know the sailor fans are like obsessing about him
[01:23:16.820 --> 01:23:19.620]   coming on the pod. What do you think of sailor taking these
[01:23:19.620 --> 01:23:22.500]   loans to buy Bitcoin and then other people are following suit
[01:23:22.500 --> 01:23:23.780]   to put it in their treasury?
[01:23:23.780 --> 01:23:26.420]   Well, short version on crypto generally is I still think the
[01:23:26.420 --> 01:23:29.220]   primary use case is speculation. And not that there's anything
[01:23:29.220 --> 01:23:31.940]   wrong with that people speculate on stocks, retail trading,
[01:23:31.940 --> 01:23:35.060]   people speculate on football games, gambling, like the
[01:23:35.060 --> 01:23:38.380]   natural tendency for humans to want to speculate is very real.
[01:23:38.380 --> 01:23:43.540]   The the art to me is because everybody's now a node connected
[01:23:43.540 --> 01:23:47.060]   into these crypto, let's call Bitcoin, you know, can you build
[01:23:47.060 --> 01:23:51.220]   an application on top that would have too much inertia, too much
[01:23:51.220 --> 01:23:54.260]   friction to start from scratch, but it has real value. And I
[01:23:54.260 --> 01:23:56.980]   think it's possible because so many people are now connected
[01:23:56.980 --> 01:24:00.220]   based upon speculation initially. So that's what's
[01:24:00.220 --> 01:24:05.180]   exciting to me. But, you know, we've watched speculation before
[01:24:05.180 --> 01:24:08.780]   in crypto markets. And, you know, it's very volatile. So
[01:24:08.780 --> 01:24:12.780]   hopefully someone builds real layers of productivity or
[01:24:12.780 --> 01:24:15.660]   utility on top, it takes advantage of the network. It's a
[01:24:15.660 --> 01:24:18.420]   little bit like back in your days at Facebook tomorrow, you
[01:24:18.420 --> 01:24:20.260]   know, you had the social graph and you'd say, you know, you
[01:24:20.260 --> 01:24:22.460]   could build applications on top of the social graph. But if you
[01:24:22.460 --> 01:24:24.780]   had to create a social graph from scratch, that was a heroic
[01:24:24.780 --> 01:24:26.500]   effort and nobody else is going to do it. That was like
[01:24:26.500 --> 01:24:29.820]   literally, the Facebook monopoly story. And so I think that's
[01:24:29.820 --> 01:24:33.060]   true of crypto. There's lots of nodes, someone's got a social
[01:24:33.060 --> 01:24:35.540]   graphic, and there's no Zuckerberg to rug pull and say
[01:24:35.540 --> 01:24:38.460]   you can't use it. Someone's got to build the application on top,
[01:24:38.460 --> 01:24:40.180]   and then it'd be extremely exciting.
[01:24:40.180 --> 01:24:43.820]   What do you think, Freeberg? You you were monitoring this sailor
[01:24:43.820 --> 01:24:46.540]   situation, the Mr. MicroStrategy.
[01:24:46.540 --> 01:24:51.900]   The way it seems, well, I think the way the sailor financial
[01:24:51.900 --> 01:24:55.900]   structure is set up, is it looks a lot like a synthetic call
[01:24:55.900 --> 01:25:00.380]   option on Bitcoin, you get a convertible note, so you earn a
[01:25:00.380 --> 01:25:03.660]   coupon on the note. And then you have a conversion price, which
[01:25:03.660 --> 01:25:06.460]   is a premium to the stock price, which you can translate. If you
[01:25:06.460 --> 01:25:09.420]   look at the book value, or the net asset value of the stock,
[01:25:09.420 --> 01:25:14.220]   that's how much Bitcoin there is. And the premium is two to
[01:25:14.220 --> 01:25:16.900]   one, I think, or two and a half to one. Yeah, the premium that
[01:25:16.900 --> 01:25:20.140]   the conversion price is at tells you what Bitcoin needs to be for
[01:25:20.140 --> 01:25:23.980]   you to have equity upside. So it's effectively a convertible
[01:25:23.980 --> 01:25:26.300]   note, you're in a coupon, and then you have a conversion
[01:25:26.300 --> 01:25:29.540]   premium that you can kind of convert to equity, which
[01:25:29.540 --> 01:25:33.140]   basically gives you a call option, you're saying, the
[01:25:33.140 --> 01:25:35.420]   premium you're paying is the price for the call option,
[01:25:35.420 --> 01:25:38.100]   effectively. And you're earning this kind of coupon in the
[01:25:38.100 --> 01:25:40.740]   meantime. So it's a way for some people to kind of trade the
[01:25:40.740 --> 01:25:43.740]   price of Bitcoin. Seems like this guy said a lot of hedge
[01:25:43.740 --> 01:25:46.180]   funds. And I don't know if you guys saw the CNBC article that
[01:25:46.180 --> 01:25:48.060]   it's kind of like we're Bloomberg, it's like the hottest
[01:25:48.060 --> 01:25:51.060]   trade in hedge fund land right now, is to buy up these
[01:25:51.060 --> 01:25:53.860]   convertible notes that he's issuing. So if you actually were
[01:25:53.860 --> 01:25:59.780]   to sit down and do the black souls modeling of the value of
[01:25:59.780 --> 01:26:01.940]   the synthetic call option that he's selling you with the
[01:26:01.940 --> 01:26:04.860]   convertible note, there's a reason people are paying for it,
[01:26:04.860 --> 01:26:07.220]   they think that there's value there. So there's certainly
[01:26:07.220 --> 01:26:08.820]   something to the structure that makes sense.
[01:26:09.340 --> 01:26:13.820]   All right. So for your Sultan of science, the dictator and fit
[01:26:13.820 --> 01:26:18.060]   sacks, fit sacks here, low BMI, low heart rate, what's the heart
[01:26:18.060 --> 01:26:18.740]   rate at right now?
[01:26:18.740 --> 01:26:24.180]   40 to 42. Like by measure by age, sleep, typical.
[01:26:24.180 --> 01:26:27.180]   Oh, it's the one what do you think of our next by the way?
[01:26:27.180 --> 01:26:30.540]   Oh, well, we'll see. I don't know if I would have made that
[01:26:30.540 --> 01:26:31.780]   trade, honestly, but
[01:26:31.780 --> 01:26:34.980]   for Carl Anthony Towns, really? Yeah. Fascinating. He's playing
[01:26:34.980 --> 01:26:35.620]   so good.
[01:26:35.620 --> 01:26:38.660]   I know. But like, I think you had the chemistry all dialed in
[01:26:38.660 --> 01:26:42.420]   and hopefully they get billed back better, you know, like,
[01:26:42.420 --> 01:26:46.740]   All right, listen, we'll see you all next time. Don't worry,
[01:26:46.740 --> 01:26:49.100]   sacks isn't going anywhere. It's just a little bit busy right
[01:26:49.100 --> 01:26:53.500]   now. There's a transition going on. sacks is transitioning. And
[01:26:53.500 --> 01:26:55.580]   we'll see the pot will transition as well. We'll see
[01:26:55.580 --> 01:26:56.820]   what it looks like in the new year.
[01:26:56.820 --> 01:26:59.980]   We have a good lineup of co hosts,
[01:26:59.980 --> 01:27:04.140]   we're going to do eight weeks, it's going to be all in idle,
[01:27:04.140 --> 01:27:07.700]   all in idle is occurring. We're rotating people in and you get
[01:27:07.700 --> 01:27:10.580]   to vote on it. That was great. Thank you. You did great. Thank
[01:27:10.580 --> 01:27:16.100]   you. Thank you for joining us. Really? Okay, guys.
[01:27:16.100 --> 01:27:17.420]   Love you, boys.
[01:27:17.420 --> 01:27:22.140]   Let your winners ride.
[01:27:22.140 --> 01:27:24.700]   Rain Man David.
[01:27:24.700 --> 01:27:31.420]   We open source it to the fans and they've just gone crazy.
[01:27:31.420 --> 01:27:33.260]   Love you as a queen.
[01:27:33.700 --> 01:27:34.220]   Wow.
[01:27:34.220 --> 01:27:42.100]   Besties are gone.
[01:27:42.100 --> 01:27:45.660]   That is my dog taking a notice in your driveway.
[01:27:45.660 --> 01:27:54.020]   We should all just get a room and just have one big huge orgy
[01:27:54.020 --> 01:27:55.860]   because they're all just useless. It's like this like
[01:27:55.860 --> 01:27:58.140]   sexual tension that they just need to release somehow.
[01:27:59.300 --> 01:28:05.700]   What you're about to be. We need to get Merck.
[01:28:06.100 --> 01:28:07.900]   I'm going all in.
[01:28:07.900 --> 01:28:15.900]   I'm going all in.
[01:28:15.900 --> 01:28:18.480]   (upbeat music)

