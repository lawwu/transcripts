
[00:00:00.000 --> 00:00:19.480]   I guess the question is, is there a job to use the AI well? And it certainly does seem
[00:00:19.480 --> 00:00:23.260]   like that. I think someone was talking about, you know, prompt engineers does sort of seem
[00:00:23.260 --> 00:00:29.300]   like that. It's interesting, right? Like, you know, how, like asking a question well
[00:00:29.300 --> 00:00:36.920]   does seem like a critical task to using the GPT models well. And it's a really interesting
[00:00:36.920 --> 00:00:44.100]   skill. I mean, you know, it totally seems like the trends are in that direction. And
[00:00:44.100 --> 00:00:49.180]   I've been talking to a lot of our, you know, customers that are kind of like, you know,
[00:00:49.180 --> 00:00:53.020]   thinking about this and noticing. I mean, I don't know if you saw, there's a really
[00:00:53.020 --> 00:00:57.800]   hilarious result, right? Where if you ask the GPT a question, then you say, show your
[00:00:57.800 --> 00:01:02.900]   reasoning step-by-step after the question, you know, it gets a lot more accurate, you
[00:01:02.900 --> 00:01:09.380]   know, which is just like absolutely fascinating. Probably good advice to humans as well. But
[00:01:09.380 --> 00:01:14.540]   I think that stuff is really, really interesting. I don't think it's, I think it would be, you
[00:01:14.540 --> 00:01:19.700]   know, to pontificate now about where this goes, you'll probably be wrong, but I agree
[00:01:19.700 --> 00:01:24.220]   that it does seem like there's a new kind of job being created that currently is called
[00:01:24.220 --> 00:01:25.220]   prompt engineer.
[00:01:25.220 --> 00:01:38.340]   Yeah. I mean, especially when you're trying to do something with LLMs that require context
[00:01:38.340 --> 00:01:46.540]   over time, prompt engineering becomes incredibly important. The context size of these models
[00:01:46.540 --> 00:01:55.860]   is fairly small right now. It's about 2000 tokens, which is maybe like four megabytes,
[00:01:55.860 --> 00:02:05.220]   which is like similar to like programming in Atari or something. Like it's a fairly
[00:02:05.220 --> 00:02:11.540]   early days, requires similar to like programming early video games. You had to do a lot of
[00:02:11.540 --> 00:02:16.620]   trickery to figure out how to, you know, put things into memory. And I said, I think prompt
[00:02:16.620 --> 00:02:22.460]   engineering is pretty low level sort of thing right now. And you need to be, I see a lot
[00:02:22.460 --> 00:02:30.220]   of amazing engineering that goes into that, you know, for example, like there's this trick
[00:02:30.220 --> 00:02:37.980]   I learned about recently where let's say you have a large corpus of things that the LLM
[00:02:37.980 --> 00:02:44.260]   is supposed to be answering questions about. And you can't put the entire thing into context.
[00:02:44.260 --> 00:02:49.620]   So what you do is you take the embeddings, which is the internal representation of a
[00:02:49.620 --> 00:02:55.620]   text of the question. And then you have an index of all the embeddings of the corpus
[00:02:55.620 --> 00:03:02.620]   or the data that you have. And then you do, you do, you match those embeddings in order
[00:03:02.620 --> 00:03:12.020]   to figure out which context it include into the prompt. That's like pretty amazing sort
[00:03:12.020 --> 00:03:20.740]   of algorithm. And, and yeah, I think it's going to be a specialty. You know, even if
[00:03:20.740 --> 00:03:26.460]   they solve the context length issue, you still got to be, you still got to have to be economical
[00:03:26.460 --> 00:03:29.780]   about it because you don't want to be slugging this data back and forth between the server
[00:03:29.780 --> 00:03:38.500]   and the client. And so in addition to sort of having these prompt tricks that Lucas talked
[00:03:38.500 --> 00:03:44.940]   about, there's also just like low level engineering of like how much context to introduce for
[00:03:44.940 --> 00:03:50.980]   the LLM to repeat.
[00:03:50.980 --> 00:03:53.560]   (gentle music)

