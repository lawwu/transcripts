
[00:00:00.000 --> 00:00:07.600]   We found that velocity really matters a lot.
[00:00:07.600 --> 00:00:12.080]   The ability to validate as early as possible matters a lot.
[00:00:12.080 --> 00:00:15.880]   You don't want to push a bad model to production.
[00:00:15.880 --> 00:00:20.840]   You don't want to wait until your final stage of A/B testing in order to find out that something's
[00:00:20.840 --> 00:00:22.360]   not going to work well.
[00:00:22.360 --> 00:00:25.800]   So we found that the earlier you can validate, the better it is.
[00:00:25.800 --> 00:00:30.160]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:30.160 --> 00:00:33.080]   And I'm your host, Lukas Biewald.
[00:00:33.080 --> 00:00:39.720]   Shreya Shankar was an ML researcher at Google Brain and an ML engineer at Viaduct AI.
[00:00:39.720 --> 00:00:44.880]   And now she's a grad student at UC Berkeley, where she wrote a paper that we all love,
[00:00:44.880 --> 00:00:49.900]   Hair, Weights, and Biases, called Operationalizing Machine Learning in an Interview Study.
[00:00:49.900 --> 00:00:51.160]   This is a really fun interview.
[00:00:51.160 --> 00:00:52.160]   I hope you enjoy it.
[00:00:52.160 --> 00:00:53.160]   All right.
[00:00:53.160 --> 00:00:54.840]   Thanks so much for doing this.
[00:00:54.840 --> 00:00:55.840]   I really appreciate it.
[00:00:55.840 --> 00:00:56.840]   Of course.
[00:00:56.840 --> 00:00:57.840]   Thanks for having me.
[00:00:57.840 --> 00:00:58.840]   I'm excited.
[00:00:58.840 --> 00:01:03.000]   I think we've all been watching you on Twitter or following you on Twitter for a long time.
[00:01:03.000 --> 00:01:08.480]   So it's exciting to meet you.
[00:01:08.480 --> 00:01:14.840]   It's funny how every now and then you run into a Twitter mutual or whatever, and it's
[00:01:14.840 --> 00:01:18.440]   like, "Oh, I know you, but I don't really know you, but I know you."
[00:01:18.440 --> 00:01:19.440]   Totally.
[00:01:19.440 --> 00:01:24.800]   We were actually doing a podcast with Sarah from Amplify Partners, and we both started
[00:01:24.800 --> 00:01:26.440]   talking about how much we liked your paper.
[00:01:26.440 --> 00:01:32.480]   And I was thinking, "Really, I should just go directly to the source."
[00:01:32.480 --> 00:01:38.880]   But yeah, I guess maybe if you could tell us, before we get into your paper, which I
[00:01:38.880 --> 00:01:43.240]   really want to talk to in depth, maybe if you could tell us a little bit about your
[00:01:43.240 --> 00:01:47.840]   career and how you got excited about operationalizing machine learning.
[00:01:47.840 --> 00:01:49.320]   Yeah.
[00:01:49.320 --> 00:01:54.800]   That's such a buzzword and honestly not the most exciting thing in the world.
[00:01:54.800 --> 00:01:59.200]   So it's kind of weird to think how I got here.
[00:01:59.200 --> 00:02:05.680]   But I started out doing deep learning research, ML research in adversarial examples, because
[00:02:05.680 --> 00:02:11.640]   that was the hot stuff in 2016, 2017.
[00:02:11.640 --> 00:02:17.000]   And I had this moment of crisis when I graduated college.
[00:02:17.000 --> 00:02:22.680]   Should I do a PhD or should I go and be an engineer or go into industry?
[00:02:22.680 --> 00:02:27.760]   And I decided, "Okay, I might as well go into industry because I'm trying to write my statement
[00:02:27.760 --> 00:02:36.400]   of purpose on working on robustness in machine learning systems and ML system deployment."
[00:02:36.400 --> 00:02:40.600]   But I didn't really know what any of those words meant because I had no experience.
[00:02:40.600 --> 00:02:45.160]   So I went to a company, I went to a startup that was doing applied machine learning, and
[00:02:45.160 --> 00:02:48.480]   I was there for a couple of years.
[00:02:48.480 --> 00:02:52.880]   And that kind of changed the course of what I believe to be some of the most pressing
[00:02:52.880 --> 00:02:57.120]   problems in operationalizing, I guess, these ML systems.
[00:02:57.120 --> 00:02:59.120]   It's a very broad topic.
[00:02:59.120 --> 00:03:06.560]   I define it as anything that requires having a machine learning system that's serving some
[00:03:06.560 --> 00:03:12.560]   output that people use on a regular basis that you don't want to shut down.
[00:03:12.560 --> 00:03:18.580]   And I think that's just a completely different ballgame than just the ML research that I
[00:03:18.580 --> 00:03:19.960]   worked with.
[00:03:19.960 --> 00:03:25.560]   And there's a lot of problems in there, both technically and organizationally, like the
[00:03:25.560 --> 00:03:27.640]   processes people use to...
[00:03:27.640 --> 00:03:33.520]   Like on-call processes, things that people do to ensure reliability of their systems.
[00:03:33.520 --> 00:03:36.960]   And then of course the tools, the principles and techniques.
[00:03:36.960 --> 00:03:42.520]   I found myself really going back to the databases and data management world in terms of...
[00:03:42.520 --> 00:03:48.360]   Like, how do I create these systems so that a bunch of data scientists can train models?
[00:03:48.360 --> 00:03:53.800]   And that really led me to, I think, doing a PhD in databases, where a lot of these problems
[00:03:53.800 --> 00:03:55.120]   can...
[00:03:55.120 --> 00:04:01.520]   Or a lot of these ML ops problems can be recasted as traditional data management problems.
[00:04:01.520 --> 00:04:02.520]   Interesting.
[00:04:02.520 --> 00:04:10.560]   So, going back to when you got your first job, we hear about this a lot, but what were
[00:04:10.560 --> 00:04:15.240]   the biggest surprises about machine learning in practice versus studying machine learning
[00:04:15.240 --> 00:04:16.800]   in school?
[00:04:16.800 --> 00:04:24.000]   I think I had a nice trajectory of surprises because I started out as the first ML engineer
[00:04:24.000 --> 00:04:30.280]   and then we grew like, I don't know, like 8x, 9x in my time at the company.
[00:04:30.280 --> 00:04:34.200]   And we hired more ML engineers and more data scientists.
[00:04:34.200 --> 00:04:41.080]   And my first surprise was that training the model itself, like that whole experimentation
[00:04:41.080 --> 00:04:47.020]   to first model is a process that you don't want to replicate with as much human labor
[00:04:47.020 --> 00:04:49.960]   as you do in the initial experimental stage.
[00:04:49.960 --> 00:04:55.000]   When you deploy it, that retraining, that kind of component, you kind of zoom out on
[00:04:55.000 --> 00:04:56.320]   your entire pipeline.
[00:04:56.320 --> 00:05:01.040]   You want to automate that and your human attention doesn't go there.
[00:05:01.040 --> 00:05:07.720]   It's how do you glue together that model in relation to all of the other stuff you have?
[00:05:07.720 --> 00:05:11.000]   So that was one nice realization that I had.
[00:05:11.000 --> 00:05:15.360]   And then after that, I stopped spending so much energy and time modeling itself for the
[00:05:15.360 --> 00:05:17.720]   sake of modeling.
[00:05:17.720 --> 00:05:23.320]   Another realization that I had was when you have multiple data scientists working on the
[00:05:23.320 --> 00:05:28.720]   same kind of model or prediction task or pipeline or whatever you want to call it, all of a
[00:05:28.720 --> 00:05:32.920]   sudden you need some sort of processes to make sure everyone is on the same page.
[00:05:32.920 --> 00:05:38.240]   If I try some sort of experiment or I have this domain expertise around, "Hey, this set
[00:05:38.240 --> 00:05:42.400]   of features probably won't work well," or "This source of data is corrupted, so don't
[00:05:42.400 --> 00:05:45.120]   try to make features from that."
[00:05:45.120 --> 00:05:51.120]   How do I share this knowledge in a way that's not a stream of thoughts on Slack?
[00:05:51.120 --> 00:05:54.360]   And how do I keep this up to date as people come and leave the team?
[00:05:54.360 --> 00:05:58.500]   That happened a lot at my previous company before grad school.
[00:05:58.500 --> 00:06:04.680]   So there's a lot of these small, small problems that built as we grew organizationally, as
[00:06:04.680 --> 00:06:10.080]   well as grew in terms of the number of customers we were serving, the number of ML applications
[00:06:10.080 --> 00:06:14.600]   we were delivering, or predictions we were serving to different people.
[00:06:14.600 --> 00:06:19.280]   And yeah, those are so much, I don't know how to give you a succinct answer to that.
[00:06:19.280 --> 00:06:24.300]   Well, so it's funny because you've also had the opposite experience too.
[00:06:24.300 --> 00:06:29.200]   You know, you talked about the shock of going from academia into industry, but you actually
[00:06:29.200 --> 00:06:31.080]   went from industry back into academia.
[00:06:31.080 --> 00:06:38.600]   Were there any surprises or misconceptions that you saw going back into the academic
[00:06:38.600 --> 00:06:39.840]   world?
[00:06:39.840 --> 00:06:46.120]   I think it's different for me because I also switched fields.
[00:06:46.120 --> 00:06:51.120]   Machine learning has also been around for a long time, but the venues that have been
[00:06:51.120 --> 00:06:53.880]   popular in databases have been around for a while.
[00:06:53.880 --> 00:06:57.680]   The norms are a little bit more well-defined and they're not changing as rapidly as the
[00:06:57.680 --> 00:06:59.800]   ML research norms.
[00:06:59.800 --> 00:07:04.040]   The community isn't growing at the scale that ML research is growing.
[00:07:04.040 --> 00:07:10.160]   So in that sense, I felt like I was kind of walking into a completely different territory.
[00:07:10.160 --> 00:07:14.800]   I think what I really like about the database community is they're very open and accepting
[00:07:14.800 --> 00:07:16.680]   of new ideas and new paradigm shifts.
[00:07:16.680 --> 00:07:19.720]   I think it's because they've seen it multiple times before.
[00:07:19.720 --> 00:07:25.400]   They've seen it like SQL to unstructured data or structured to unstructured data.
[00:07:25.400 --> 00:07:30.840]   They've seen it from transactional systems to OLAP systems.
[00:07:30.840 --> 00:07:37.740]   And now they've seen web scale, all sorts of stuff, like the MapReduce era.
[00:07:37.740 --> 00:07:38.740]   Maybe that's still going.
[00:07:38.740 --> 00:07:40.520]   I don't really know.
[00:07:40.520 --> 00:07:47.280]   So in that sense, I think they're very eager and receptive to work in this kind of ML systems
[00:07:47.280 --> 00:07:54.600]   or data management for ML space, which I felt that at traditional ML venues, it was almost
[00:07:54.600 --> 00:07:59.120]   like you need to like trade models in order to have your papers accepted.
[00:07:59.120 --> 00:08:03.960]   If you weren't training models or doing model inference, then is this a research paper?
[00:08:03.960 --> 00:08:06.740]   I don't know.
[00:08:06.740 --> 00:08:11.080]   I think database is just like a better home for kind of the work that I'm doing.
[00:08:11.080 --> 00:08:15.840]   Not to like dis on all the model training work, of course.
[00:08:15.840 --> 00:08:16.840]   And why is that?
[00:08:16.840 --> 00:08:20.760]   Is that because you care about sort of practical real world applications?
[00:08:20.760 --> 00:08:22.360]   Is that a good summary?
[00:08:22.360 --> 00:08:29.360]   Yeah, I think there's a lot of problems around operationalizing models that are data management
[00:08:29.360 --> 00:08:30.880]   problems.
[00:08:30.880 --> 00:08:34.880]   And when you do research in that, what venues are going to accept that research?
[00:08:34.880 --> 00:08:37.820]   I'm not necessarily training models in this research.
[00:08:37.820 --> 00:08:41.580]   So it's less likely, I think, for ML venues to accept this work.
[00:08:41.580 --> 00:08:49.100]   And I'm also borrowing a lot of ideas from databases around how I think of models, how
[00:08:49.100 --> 00:08:56.740]   I think of provenance, how this can be used to solve a lot of like observability problems,
[00:08:56.740 --> 00:08:57.740]   things like that.
[00:08:57.740 --> 00:09:04.500]   Well, so then, you wrote this really fantastic paper, which we'll definitely link to.
[00:09:04.500 --> 00:09:07.640]   I was almost thinking maybe we should make a required reading before listening to this
[00:09:07.640 --> 00:09:09.760]   podcast so we can kind of get into the details.
[00:09:09.760 --> 00:09:14.260]   But you wrote this paper on operationalizing machine learning.
[00:09:14.260 --> 00:09:19.620]   And you went out and interviewed a whole bunch of practitioners and kind of like summarized
[00:09:19.620 --> 00:09:21.940]   the field, which is something that I always try to do.
[00:09:21.940 --> 00:09:28.060]   Like I almost feel like this podcast could be called like operationalizing machine learning.
[00:09:28.060 --> 00:09:32.440]   And I thought you really put things in a really well structured, really interesting and surprising
[00:09:32.440 --> 00:09:37.180]   results that show that you're really getting deep with the people you're interviewing.
[00:09:37.180 --> 00:09:42.740]   But I guess maybe before we get into it, could you maybe summarize the kind of key findings
[00:09:42.740 --> 00:09:46.620]   of the paper and then we can, for the folks that haven't read it, and then we can dive
[00:09:46.620 --> 00:09:49.100]   into the nitty gritty.
[00:09:49.100 --> 00:09:50.300]   Sure.
[00:09:50.300 --> 00:09:57.740]   So we interviewed around 20 practitioners and the criteria was that they have worked
[00:09:57.740 --> 00:10:02.100]   on or are working on a model that's being used in production.
[00:10:02.100 --> 00:10:07.260]   So basically it's serving some predictions or some output that customers are using and
[00:10:07.260 --> 00:10:09.820]   somebody will get an alert if the system breaks.
[00:10:09.820 --> 00:10:12.820]   Like that's kind of our definition of production.
[00:10:12.820 --> 00:10:17.620]   And we interviewed people across company sizes and across applications like self-driving
[00:10:17.620 --> 00:10:20.780]   cars, banking, whatsoever.
[00:10:20.780 --> 00:10:27.060]   And we found, we looked for common patterns across people's interviews.
[00:10:27.060 --> 00:10:36.780]   We found four kind of high level stages of their workflow around experimentation, like
[00:10:36.780 --> 00:10:42.620]   the evaluation and deployment, monitoring and response, and then data collection, which
[00:10:42.620 --> 00:10:48.780]   wasn't often performed by the ML engineers that we interviewed, but it was like a critical
[00:10:48.780 --> 00:10:52.340]   part kind of of the production ML pipeline.
[00:10:52.340 --> 00:10:57.460]   So we identified these four components or these four stages, and then we also identified
[00:10:57.460 --> 00:11:05.020]   kind of what are the kind of variables that govern how successful their deployments will
[00:11:05.020 --> 00:11:06.020]   be.
[00:11:06.020 --> 00:11:11.080]   Like what are the things to think about whenever evaluating tools to use in each of these stages?
[00:11:11.080 --> 00:11:14.940]   How do I know if I'm on the right track to a successful deployment?
[00:11:14.940 --> 00:11:18.620]   And we found that velocity really matters a lot.
[00:11:18.620 --> 00:11:23.080]   The ability to validate as early as possible matters a lot.
[00:11:23.080 --> 00:11:26.880]   You don't want to like push a bad model to production.
[00:11:26.880 --> 00:11:31.460]   You don't want to wait until your final stage of like A/B testing in order to find out that
[00:11:31.460 --> 00:11:33.380]   something is not going to work well.
[00:11:33.380 --> 00:11:36.620]   So we found that like the earlier you can validate, the better it is.
[00:11:36.620 --> 00:11:43.620]   And then finally, the last V is versioning, which is how do you manage all of the different
[00:11:43.620 --> 00:11:49.460]   versions of models that you're going to have as time goes to infinity?
[00:11:49.460 --> 00:11:55.140]   How do you think about kind of all the edge cases or corner cases that your system must
[00:11:55.140 --> 00:11:57.100]   respond to?
[00:11:57.100 --> 00:12:00.180]   And maybe that's like slapping on a different version.
[00:12:00.180 --> 00:12:03.800]   If you come from this customer or you come from like this population, we'll give you
[00:12:03.800 --> 00:12:06.740]   this version.
[00:12:06.740 --> 00:12:11.660]   And yeah, like managing that, right, is like a pain point.
[00:12:11.660 --> 00:12:13.980]   That's kind of the high level finding.
[00:12:13.980 --> 00:12:18.900]   And so I guess, you know, you obviously have a fair amount of experience in this already,
[00:12:18.900 --> 00:12:23.340]   like having kind of done this job yourself and you're pretty active on Twitter and kind
[00:12:23.340 --> 00:12:28.820]   of in the conversation around this for quite a while.
[00:12:28.820 --> 00:12:33.140]   Were there parts of what you heard that surprised you?
[00:12:33.140 --> 00:12:34.980]   Definitely.
[00:12:34.980 --> 00:12:40.700]   And selfishly, I think I conducted this study so thoroughly and like as like a research
[00:12:40.700 --> 00:12:44.420]   thing that took like one and a half years that we kind of been on the side.
[00:12:44.420 --> 00:12:49.580]   I did this because like I was so afraid that I was leaving industry and going into academia
[00:12:49.580 --> 00:12:53.660]   and then going to go into a bubble and try to build systems for people and not know what
[00:12:53.660 --> 00:12:56.260]   I'm doing or whether this is useful.
[00:12:56.260 --> 00:13:01.660]   And what I did not expect was to just like kind of change my research agenda and direction.
[00:13:01.660 --> 00:13:05.660]   And one concrete example of this is distribution shift.
[00:13:05.660 --> 00:13:10.500]   I used to believe, well, maybe it's a problem depending on how you define it.
[00:13:10.500 --> 00:13:15.300]   But the idea that if you have this static model in production, like a static set of
[00:13:15.300 --> 00:13:20.780]   parameters or a function that's being called on some features and these features are changing,
[00:13:20.780 --> 00:13:23.180]   time is going on.
[00:13:23.180 --> 00:13:28.660]   At some point, right, this is like the classic view staleness problem in databases, right?
[00:13:28.660 --> 00:13:32.340]   You need to refresh your views to keep up to date with the underlying data.
[00:13:32.340 --> 00:13:37.460]   If you think of a model as a view on a table, like the same thing exists.
[00:13:37.460 --> 00:13:43.540]   But I think a lot of the ML literature or even things that I'd been thinking about are
[00:13:43.540 --> 00:13:52.460]   how do I make my views like robust to these, I don't know, changes in the underlying distribution.
[00:13:52.460 --> 00:13:56.980]   And in practice, like sure, that's like great.
[00:13:56.980 --> 00:14:01.540]   But if something as simple as re-computing the view or retraining the model solves my
[00:14:01.540 --> 00:14:06.080]   problem of staleness, then why don't I just do that?
[00:14:06.080 --> 00:14:12.500]   And you'll find that at these like very large organizations like Meta, Google, Amazon, et
[00:14:12.500 --> 00:14:19.900]   cetera, that they're simply retraining their models like six, seven, eight times a day
[00:14:19.900 --> 00:14:22.460]   or even every hour.
[00:14:22.460 --> 00:14:26.700]   And distribution shift is not their problem.
[00:14:26.700 --> 00:14:32.940]   In this setting, when you're retraining all the time, like retraining on corrupted data
[00:14:32.940 --> 00:14:34.260]   becomes a problem.
[00:14:34.260 --> 00:14:37.100]   So how do I make sure that my data is clean and corrupted?
[00:14:37.100 --> 00:14:44.180]   How do I identify when to block a retrained model from being promoted back to production?
[00:14:44.180 --> 00:14:47.980]   Like all these sorts of problems, it's like, oh, these are very interesting research problems,
[00:14:47.980 --> 00:14:53.300]   but this is not what I thought of distribution shift to be.
[00:14:53.300 --> 00:14:55.100]   Hopefully that answers your question.
[00:14:55.100 --> 00:14:59.340]   I can think of others.
[00:14:59.340 --> 00:15:05.700]   The people that you were talking to, were they kind of like individual contributors
[00:15:05.700 --> 00:15:06.700]   building models?
[00:15:06.700 --> 00:15:08.520]   Was it more like managers?
[00:15:08.520 --> 00:15:13.780]   We often see like a separate MLOps team that's sort of doing the infrastructure while other
[00:15:13.780 --> 00:15:15.700]   people are kind of doing the training.
[00:15:15.700 --> 00:15:19.700]   Who were the kind of folks that ended up in your study?
[00:15:19.700 --> 00:15:25.900]   We required that everyone was an ML engineer, like responsible for a model or like pinged
[00:15:25.900 --> 00:15:31.780]   when the model predictions are bad or someone's complaining at some point in their career.
[00:15:31.780 --> 00:15:34.940]   Some of them had switched to the infrastructure building side.
[00:15:34.940 --> 00:15:36.020]   Some of them had become managers.
[00:15:36.020 --> 00:15:42.460]   I think two of them had become managers and that's like written in the paper.
[00:15:42.460 --> 00:15:48.340]   But everyone there like acutely knew what it was like to have like put a model in production
[00:15:48.340 --> 00:15:51.620]   and somebody complained about the predictions.
[00:15:51.620 --> 00:15:53.220]   And that's really what we wanted to drill into.
[00:15:53.220 --> 00:15:56.660]   Like, all right, so like, what did you do to fix this?
[00:15:56.660 --> 00:15:58.900]   What does your team do to fix this?
[00:15:58.900 --> 00:16:06.200]   Yeah, like simply retraining the model often fixes it like 80% of the time.
[00:16:06.200 --> 00:16:08.220]   These ML engineers have so much on their backlog.
[00:16:08.220 --> 00:16:12.100]   Like if they can kick off a retrain and get to something else on the backlog and it works
[00:16:12.100 --> 00:16:14.340]   80% of the time, that is going to be the solution.
[00:16:14.340 --> 00:16:18.020]   That is the best solution.
[00:16:18.020 --> 00:16:25.100]   I feel like more ML researchers should know this, but we could be, maybe I'm biased.
[00:16:25.100 --> 00:16:28.620]   Were these people mostly doing kind of unstructured data?
[00:16:28.620 --> 00:16:33.340]   Like one of the big dichotomies that we see is like structured versus unstructured, where
[00:16:33.340 --> 00:16:39.020]   like unstructured you often get more, you know, kind of neural net techniques.
[00:16:39.020 --> 00:16:41.100]   You get, you know, bigger models.
[00:16:41.100 --> 00:16:44.700]   You get almost like a totally different stack in many cases.
[00:16:44.700 --> 00:16:47.980]   Did you observe that too?
[00:16:47.980 --> 00:16:48.980]   Definitely.
[00:16:48.980 --> 00:16:53.060]   We talked to some people who had very image heavy, like self-driving cars or autonomous
[00:16:53.060 --> 00:16:56.220]   vehicles is a good example of this.
[00:16:56.220 --> 00:16:58.540]   And they're definitely using neural networks.
[00:16:58.540 --> 00:17:06.340]   I think when drilling though into like data quality and this kind of like data management,
[00:17:06.340 --> 00:17:11.060]   I think people tend to think about like relational data management.
[00:17:11.060 --> 00:17:16.420]   Like how do I manage the embeddings or how do I manage like tuples?
[00:17:16.420 --> 00:17:20.140]   I don't know if we've like gotten to the place where we're thinking about like traditional
[00:17:20.140 --> 00:17:26.660]   data cleaning and data quality in terms of like images or like other unstructured data.
[00:17:26.660 --> 00:17:30.340]   But we didn't focus the interviews too much on that.
[00:17:30.340 --> 00:17:35.180]   Were people doing a lot of exploration of features?
[00:17:35.180 --> 00:17:41.000]   Did feature stores show up much in your interviews?
[00:17:41.000 --> 00:17:47.340]   So we explicitly went into this not wanting to hit the buzzwords just to see what buzzwords
[00:17:47.340 --> 00:17:50.660]   would come out.
[00:17:50.660 --> 00:17:52.700]   Feature stores were almost never mentioned.
[00:17:52.700 --> 00:17:53.700]   Interesting.
[00:17:53.700 --> 00:17:55.920]   Why do you think that is?
[00:17:55.920 --> 00:18:04.140]   I think people thought about the idea of like a feature table or feature service.
[00:18:04.140 --> 00:18:07.980]   But very few people said the term like feature store.
[00:18:07.980 --> 00:18:12.580]   What mattered to them was just like having features that were available for them to query.
[00:18:12.580 --> 00:18:19.340]   And like oftentimes at organizations, it just happens to be a cron job that's like populating
[00:18:19.340 --> 00:18:20.340]   features in.
[00:18:20.340 --> 00:18:24.420]   I mean, obviously not at like Meta, they're not going to have like a Postgres table of
[00:18:24.420 --> 00:18:30.420]   features, but in a lot of like mid-sized cases, mid-sized companies where you can have Postgres
[00:18:30.420 --> 00:18:37.740]   table of features and you can have a cron job that recomputes features every day.
[00:18:37.740 --> 00:18:43.060]   And I think it ends up going back to this like view staleness problem, right?
[00:18:43.060 --> 00:18:52.340]   Like how stale does it need to get for your business to experience some like performance
[00:18:52.340 --> 00:18:56.980]   hit?
[00:18:56.980 --> 00:19:04.500]   I don't know if like you need to be computing them on demand all the time.
[00:19:04.500 --> 00:19:13.220]   I guess I love your kind of simple categorization of like data collection, experimentation,
[00:19:13.220 --> 00:19:16.860]   evaluation and monitoring and response.
[00:19:16.860 --> 00:19:23.620]   I guess, did it feel like of those categories, I think you said the data collection was usually
[00:19:23.620 --> 00:19:26.240]   like a different team.
[00:19:26.240 --> 00:19:31.100]   But where were your respondents spending most of their time and where do you think they
[00:19:31.100 --> 00:19:35.860]   felt like the most pain was?
[00:19:35.860 --> 00:19:44.100]   Because all of the pipelines that they talked about were deployed, already in production,
[00:19:44.100 --> 00:19:45.980]   people did not focus on experimentation as much.
[00:19:45.980 --> 00:19:50.180]   I imagine that this is not representative of the ML community at large.
[00:19:50.180 --> 00:19:54.340]   I think there's like a lot of people who are still working on getting their first production
[00:19:54.340 --> 00:19:55.340]   pipeline out there.
[00:19:55.340 --> 00:20:00.060]   Just to be clear, none of these questions are leading.
[00:20:00.060 --> 00:20:07.180]   I just want to say this is definitely a biased subset towards production pipelines.
[00:20:07.180 --> 00:20:14.460]   I think the evaluation and deployment, actually, I think monitoring and response, it's hard,
[00:20:14.460 --> 00:20:16.300]   50/50 on those.
[00:20:16.300 --> 00:20:20.860]   Just based on the annotations of the interviews that we did or the codes and how we group
[00:20:20.860 --> 00:20:23.620]   them, it was very 50/50 on those two.
[00:20:23.620 --> 00:20:26.940]   And they often link into each other.
[00:20:26.940 --> 00:20:33.020]   People will talk about problems with monitoring staged deployments.
[00:20:33.020 --> 00:20:36.900]   So does that fit in monitoring or staged deployments?
[00:20:36.900 --> 00:20:38.580]   I don't really know.
[00:20:38.580 --> 00:20:39.580]   Totally.
[00:20:39.580 --> 00:20:45.340]   But I think it's definitely a big pain point, evaluation and beyond.
[00:20:45.340 --> 00:20:52.500]   And I guess one of the key findings here is that monitoring for data corruption or catastrophic
[00:20:52.500 --> 00:20:55.580]   errors is more important than monitoring for data drift.
[00:20:55.580 --> 00:21:03.060]   But you'd sort of imagine that monitoring for data corruption would actually be a lot
[00:21:03.060 --> 00:21:05.780]   simpler, right?
[00:21:05.780 --> 00:21:07.980]   What makes that so challenging to do in production?
[00:21:07.980 --> 00:21:13.100]   I'm writing a paper on this based on some work with Meta.
[00:21:13.100 --> 00:21:17.900]   But in the limit, people only add features to a model.
[00:21:17.900 --> 00:21:19.060]   They don't remove features.
[00:21:19.060 --> 00:21:23.540]   So what happens is these models end up getting hundreds of features to thousands of features
[00:21:23.540 --> 00:21:26.460]   to 10,000 features.
[00:21:26.460 --> 00:21:27.460]   So that's one thing.
[00:21:27.460 --> 00:21:30.580]   You've got models in production with tens of thousands of features.
[00:21:30.580 --> 00:21:35.700]   Another thing is that people are coming and going in these organizations.
[00:21:35.700 --> 00:21:41.460]   Like the ML engineer who built the model does not exist at the company anymore.
[00:21:41.460 --> 00:21:43.740]   And the model is still in production.
[00:21:43.740 --> 00:21:50.540]   So couple that with existing data monitoring or data cleaning solutions, which is defining
[00:21:50.540 --> 00:21:58.260]   schemas for all of my features, like bounds of acceptable values, types for each one of
[00:21:58.260 --> 00:21:59.260]   them.
[00:21:59.260 --> 00:22:00.260]   Great.
[00:22:00.260 --> 00:22:06.260]   Who is going to do that and maintain that as these feature tables or as these pipelines
[00:22:06.260 --> 00:22:07.260]   evolve?
[00:22:07.260 --> 00:22:08.340]   I don't know.
[00:22:08.340 --> 00:22:14.500]   The other thing is because you have so many features, the probability that at least one
[00:22:14.500 --> 00:22:18.540]   record and one column is corrupted is so high.
[00:22:18.540 --> 00:22:19.940]   And then you get this problem, right?
[00:22:19.940 --> 00:22:23.140]   We talked about in this paper of like just the straight alert fatigue, right?
[00:22:23.140 --> 00:22:26.060]   It's so painful.
[00:22:26.060 --> 00:22:31.080]   Like at the end of the day, it doesn't matter if just a couple of records are corrupted
[00:22:31.080 --> 00:22:34.140]   in one tuple, the problem or sorry, in one column.
[00:22:34.140 --> 00:22:42.620]   The problem is like, again, when does it get so bad that it brings down the business?
[00:22:42.620 --> 00:22:46.140]   And how do I find that pretty precisely?
[00:22:46.140 --> 00:22:47.260]   You know, it's funny.
[00:22:47.260 --> 00:22:52.260]   I'm like nodding and I've like lived this myself, you know, like many, many times.
[00:22:52.260 --> 00:22:53.740]   That's why I totally agree.
[00:22:53.740 --> 00:22:58.620]   But I'm actually kind of thinking if I hadn't lived it, it might not be obvious, you know,
[00:22:58.620 --> 00:22:59.620]   how this happens.
[00:22:59.620 --> 00:23:01.460]   Is there like a concrete story?
[00:23:01.460 --> 00:23:05.780]   If you could tell about like how a feature gets corrupted in production and the havoc
[00:23:05.780 --> 00:23:07.780]   that it causes?
[00:23:07.780 --> 00:23:08.780]   Yeah.
[00:23:08.780 --> 00:23:09.780]   Okay.
[00:23:09.780 --> 00:23:12.300]   I want to give a...
[00:23:12.300 --> 00:23:16.180]   I feel like this is like a question where people will attack me for any answer.
[00:23:16.180 --> 00:23:20.660]   Like if I give an example of like a meta or a Google, they'll be like, "Oh, but not every
[00:23:20.660 --> 00:23:21.980]   company is a large company."
[00:23:21.980 --> 00:23:27.800]   Well, I mean, I think the story just sort of illustrates the chain of events.
[00:23:27.800 --> 00:23:32.200]   Not that, you know, of course we're all so smart that we never have bugs.
[00:23:32.200 --> 00:23:33.520]   Sure.
[00:23:33.520 --> 00:23:41.680]   I think I'll give one example at my previous company, which like I lived.
[00:23:41.680 --> 00:23:45.800]   So we had like features were generated from different sources.
[00:23:45.800 --> 00:23:49.880]   And when I say different sources, it's not just like weather data or whatever.
[00:23:49.880 --> 00:23:53.180]   It's like different clients have different data.
[00:23:53.180 --> 00:23:58.740]   And then also we have like different data pipelines that are like repeatedly pulling
[00:23:58.740 --> 00:24:04.620]   from Snowflake or repeatedly generating features.
[00:24:04.620 --> 00:24:09.900]   And oftentimes these pipelines will fail because like maybe there isn't enough resources or
[00:24:09.900 --> 00:24:12.540]   like there weren't spot resources available in US West.
[00:24:12.540 --> 00:24:13.540]   I don't know.
[00:24:13.540 --> 00:24:15.820]   Like things will happen.
[00:24:15.820 --> 00:24:18.180]   And these things will all be null.
[00:24:18.180 --> 00:24:24.220]   And will this corrupt model performance significantly to where I actually see a regression?
[00:24:24.220 --> 00:24:26.240]   I don't know.
[00:24:26.240 --> 00:24:28.120]   But this happens a lot.
[00:24:28.120 --> 00:24:33.740]   It also happens that like some of my clients, like they send me data every day and like
[00:24:33.740 --> 00:24:36.300]   one day they send it in a CSV or parquet.
[00:24:36.300 --> 00:24:38.500]   One day they switch the order of the columns.
[00:24:38.500 --> 00:24:42.300]   Like a totally reasonable thing.
[00:24:42.300 --> 00:24:49.900]   But again, impacts a subset of tuples, subset of columns.
[00:24:49.900 --> 00:24:51.260]   I could name a bunch of these.
[00:24:51.260 --> 00:24:55.620]   I think this is pretty generalizable to most orgs.
[00:24:55.620 --> 00:24:56.620]   Totally.
[00:24:56.620 --> 00:25:03.300]   I remember at my first job, all the features for some reason, like there's no technical
[00:25:03.300 --> 00:25:05.100]   reason this is necessary.
[00:25:05.100 --> 00:25:09.820]   But someone started just like naming each feature column with like four letters, like
[00:25:09.820 --> 00:25:12.220]   literally like four letters for no reason.
[00:25:12.220 --> 00:25:14.940]   And so they kept doing it.
[00:25:14.940 --> 00:25:19.940]   So like all the feature names were like just like really compressed like word.
[00:25:19.940 --> 00:25:24.020]   So basically at some point no one knew what they meant.
[00:25:24.020 --> 00:25:25.780]   But it was nuts.
[00:25:25.780 --> 00:25:26.780]   Yeah.
[00:25:26.780 --> 00:25:30.300]   The stuff like that like really just like makes it so hard to go back and trace these
[00:25:30.300 --> 00:25:31.300]   like bugs.
[00:25:31.300 --> 00:25:34.860]   Like why did my model regress 2% for example?
[00:25:34.860 --> 00:25:38.740]   Like, oh my God, there can be a laundry list of reasons why.
[00:25:38.740 --> 00:25:43.140]   Just to even go there and try to investigate why would be like a nightmare.
[00:25:43.140 --> 00:25:50.540]   Well I guess though, if the people you talk to are just constantly retraining over and
[00:25:50.540 --> 00:25:55.220]   over, that actually might be one way to kind of avoid data corruption.
[00:25:55.220 --> 00:25:58.560]   Like provided the way that they collect the features is the same as how the features get
[00:25:58.560 --> 00:26:01.180]   loaded into the model in production.
[00:26:01.180 --> 00:26:03.620]   Yes and no.
[00:26:03.620 --> 00:26:07.980]   Suppose I retrain every hour.
[00:26:07.980 --> 00:26:13.740]   And when I retrain, I just like fine tune my model on the last hour's data.
[00:26:13.740 --> 00:26:18.980]   I split that into training, retraining, and then I split a little bit of that into validation.
[00:26:18.980 --> 00:26:23.620]   And my criteria for passing is like it has achieved like reasonable performance on that
[00:26:23.620 --> 00:26:25.500]   small validation set.
[00:26:25.500 --> 00:26:28.500]   Suppose that whole hour of data is corrupted.
[00:26:28.500 --> 00:26:34.300]   It might just be the case that like great, like on this corrupted data, like because
[00:26:34.300 --> 00:26:37.180]   I trained on it, like I performed well on the validation set.
[00:26:37.180 --> 00:26:39.020]   Amazing, same distribution.
[00:26:39.020 --> 00:26:43.020]   Put it back in production and then somebody fixes the bug and all of a sudden the performance
[00:26:43.020 --> 00:26:50.140]   changes because that snapshot of data was different from previous snapshots or future
[00:26:50.140 --> 00:26:51.140]   snapshots.
[00:26:51.140 --> 00:26:56.100]   So I do think that there is this still data corruption problem.
[00:26:56.100 --> 00:27:04.380]   And the challenge is in identifying the corruptions at the time scale that engineers react and
[00:27:04.380 --> 00:27:06.940]   respond to bugs.
[00:27:06.940 --> 00:27:12.260]   So you don't like put models in production that like won't do well on future data.
[00:27:12.260 --> 00:27:18.060]   So I guess one of the little gems in your paper is the controversy.
[00:27:18.060 --> 00:27:23.820]   I forget exactly how you put it, like you said Jupyter Notebooks are quite controversial
[00:27:23.820 --> 00:27:27.380]   or quite a bimodal distribution of responses on that.
[00:27:27.380 --> 00:27:34.420]   I'm kind of curious your take on Jupyter Notebooks.
[00:27:34.420 --> 00:27:38.100]   So I think my take is a little bit biased.
[00:27:38.100 --> 00:27:42.300]   I'm not old enough to have lived the history of data management tools like spreadsheets
[00:27:42.300 --> 00:27:46.100]   and whatnot when they came out.
[00:27:46.100 --> 00:27:54.220]   But from my like reading of like old work, it seems that these quick and dirty prototyping
[00:27:54.220 --> 00:28:01.460]   data tools were used to tell stories and have primarily been used to tell stories regardless
[00:28:01.460 --> 00:28:05.820]   of whether it was done correctly or not.
[00:28:05.820 --> 00:28:11.140]   And I think that this is like the case for a lot of data tools.
[00:28:11.140 --> 00:28:15.020]   Jupyter Notebooks are not really an exception.
[00:28:15.020 --> 00:28:20.660]   So while like I might if I want to start a company around like my opinionated like I
[00:28:20.660 --> 00:28:25.100]   don't want errors and I want like no one's allowed to use Jupyter Notebooks.
[00:28:25.100 --> 00:28:27.900]   Like I think that's just an opinion.
[00:28:27.900 --> 00:28:32.580]   It doesn't, I feel like it's completely useless to go and try to like prescribe a philosophy
[00:28:32.580 --> 00:28:39.540]   to the industry that like has a pattern of using these data management tools.
[00:28:39.540 --> 00:28:42.020]   That's kind of interesting because I actually feel like you're kind of putting yourself
[00:28:42.020 --> 00:28:46.700]   in a place where lots of people might come to you and be like, "Hey Shreya, what should
[00:28:46.700 --> 00:28:47.700]   I do?"
[00:28:47.700 --> 00:28:50.660]   I mean people come to me all the time and I think you're more qualified to say like,
[00:28:50.660 --> 00:28:52.340]   you know, how should I set things up?
[00:28:52.340 --> 00:28:55.220]   Like should I be letting my team use Jupyter Notebooks?
[00:28:55.220 --> 00:29:00.900]   I guess if someone asked you, am I hearing you right that your answer would be no, don't
[00:29:00.900 --> 00:29:02.900]   use Jupyter Notebooks?
[00:29:02.900 --> 00:29:04.820]   Oh gosh.
[00:29:04.820 --> 00:29:09.940]   I think it really depends on the application of or like what company I'm trying to run
[00:29:09.940 --> 00:29:11.540]   or what team I'm trying to do.
[00:29:11.540 --> 00:29:16.580]   What are the like engineering predispositions of the people on the team?
[00:29:16.580 --> 00:29:18.060]   You're turning into such an academic.
[00:29:18.060 --> 00:29:19.060]   I love it.
[00:29:19.060 --> 00:29:20.060]   I'm sorry.
[00:29:20.060 --> 00:29:22.780]   I can't give a, I can't give, but I think that's the point.
[00:29:22.780 --> 00:29:27.980]   Like, so one thing about this paper is that like it's an academic paper so we can't like
[00:29:27.980 --> 00:29:30.940]   write all of our opinions in there.
[00:29:30.940 --> 00:29:35.740]   But I really wanted to like drive home the point where it's just like the reason that
[00:29:35.740 --> 00:29:41.180]   we think that people have these conflicting opinions is because they have different conflicting
[00:29:41.180 --> 00:29:42.180]   priorities.
[00:29:42.180 --> 00:29:43.180]   Right?
[00:29:43.180 --> 00:29:48.540]   Like do they want initial velocity to be a higher priority than validation?
[00:29:48.540 --> 00:29:52.780]   That's like personal or that's organizational.
[00:29:52.780 --> 00:29:54.300]   I think those values are different for everyone.
[00:29:54.300 --> 00:29:56.260]   I think that's fair.
[00:29:56.260 --> 00:30:01.660]   I would think that kind of over time your priorities would naturally shift.
[00:30:01.660 --> 00:30:07.780]   I mean, especially I guess as like a startup founder, you know, in the beginning, you know,
[00:30:07.780 --> 00:30:09.460]   you don't know how useful the model is going to be.
[00:30:09.460 --> 00:30:12.860]   You don't know if it's really going to see the light of day even, and then kind of over
[00:30:12.860 --> 00:30:18.500]   time, you know, you really want to start to like nail things down and you worry more about
[00:30:18.500 --> 00:30:20.380]   the downside risk.
[00:30:20.380 --> 00:30:26.580]   And then you have to like account for this infrastructure or like even like transition
[00:30:26.580 --> 00:30:31.820]   in your organization from notebooks to whatever if you want to deprecate them.
[00:30:31.820 --> 00:30:42.260]   Like we interviewed one engineer who they had this like whole, their quarterly goals
[00:30:42.260 --> 00:30:49.340]   or whatever were to get evaluation of models out of notebooks and put them in like this
[00:30:49.340 --> 00:30:52.220]   standard system.
[00:30:52.220 --> 00:30:53.220]   Didn't care.
[00:30:53.220 --> 00:30:56.460]   It didn't matter what like CI/CD tool or whatever, but the whole point was just like get this
[00:30:56.460 --> 00:31:03.740]   in a standardized system so that people would stop like running notebooks as a way to show
[00:31:03.740 --> 00:31:04.740]   that.
[00:31:04.740 --> 00:31:07.980]   And like everyone a different fork of the notebook.
[00:31:07.980 --> 00:31:10.900]   I don't know.
[00:31:10.900 --> 00:31:14.700]   I feel like stories like that just make me like, oh my God, like no one is working on
[00:31:14.700 --> 00:31:15.700]   ML.
[00:31:15.700 --> 00:31:20.740]   No one is working on the company, the direct company objective because they're like fighting
[00:31:20.740 --> 00:31:25.300]   their infrastructure rattles and like, I don't know, dealing with all the tech debt that
[00:31:25.300 --> 00:31:27.300]   they introduced from having the notebooks.
[00:31:27.300 --> 00:31:28.300]   So what is the trade off?
[00:31:28.300 --> 00:31:29.300]   I don't know.
[00:31:29.300 --> 00:31:34.300]   Were there other stories like that or like themes like that where there's like kind of
[00:31:34.300 --> 00:31:38.860]   a consistent regret of something that people did in the beginning that they now can't get
[00:31:38.860 --> 00:31:42.980]   rid of when things are in production?
[00:31:42.980 --> 00:31:48.220]   So the people who we interviewed that were like more senior in their roles or had been
[00:31:48.220 --> 00:31:51.540]   around for longer kind of just accepted this.
[00:31:51.540 --> 00:31:55.740]   It was like, oh yeah, like, you know, organizational turnover is a thing.
[00:31:55.740 --> 00:31:56.980]   Tech debt is a thing.
[00:31:56.980 --> 00:32:03.300]   Like our goal is not to remove it completely, but it's like, how do we like keep shipping
[00:32:03.300 --> 00:32:08.660]   new things, keep old things up and running in the face of all the tech debt?
[00:32:08.660 --> 00:32:13.740]   And I think that's a more interesting question to me.
[00:32:13.740 --> 00:32:15.460]   There are a lot of like one-off stories.
[00:32:15.460 --> 00:32:21.620]   I can't think of any of the top of my head that were specific to this, like Jupyter notebooks.
[00:32:21.620 --> 00:32:28.820]   I guess there was one other anecdote where somebody spent like three to six months trying
[00:32:28.820 --> 00:32:36.780]   to reproduce some Jupyter notebooks just to like make a point that like they shouldn't
[00:32:36.780 --> 00:32:40.700]   use Jupyter notebooks within their organization.
[00:32:40.700 --> 00:32:45.260]   And then their organization had this like push to like, this was a more of a smaller
[00:32:45.260 --> 00:32:49.180]   company, but to get rid of notebooks or notebook usage.
[00:32:49.180 --> 00:32:51.660]   So again, it's like just so polarizing.
[00:32:51.660 --> 00:32:54.460]   That's a little funny.
[00:32:54.460 --> 00:32:59.060]   I mean, my honest experience with Jupyter notebooks is they're like, I think they're
[00:32:59.060 --> 00:33:04.700]   kind of delightful, but I didn't like, I'm like a little, you know, I like I predated
[00:33:04.700 --> 00:33:05.700]   Jupyter notebooks.
[00:33:05.700 --> 00:33:08.900]   So I was doing like most of my like hands-on research before they existed.
[00:33:08.900 --> 00:33:11.620]   So I'm just like a little more comfortable in the command line.
[00:33:11.620 --> 00:33:15.340]   So I always feel like a little ashamed that like I'm not like sticking with the new trends,
[00:33:15.340 --> 00:33:21.780]   but it sounds like there may be a backlash coming to these notebooks.
[00:33:21.780 --> 00:33:23.140]   I think it's also different.
[00:33:23.140 --> 00:33:24.140]   Like different people are different.
[00:33:24.140 --> 00:33:27.660]   I'm the type of person where somebody hands me a Jupyter notebook or like something and
[00:33:27.660 --> 00:33:32.100]   like here are some results I will be like and show me how the results got here because
[00:33:32.100 --> 00:33:34.700]   I will be paranoid at every step of the way.
[00:33:34.700 --> 00:33:38.180]   We talked about this in the paper, like this like paranoia, the sense of paranoia we all
[00:33:38.180 --> 00:33:40.100]   get.
[00:33:40.100 --> 00:33:44.580]   And the same thing is like, at least the same thing is true for me when it comes to like
[00:33:44.580 --> 00:33:45.580]   SQL queries.
[00:33:45.580 --> 00:33:48.940]   Like if you give me a SQL query, I want to know like everything that's in your, I want
[00:33:48.940 --> 00:33:53.100]   to like re-execute that SQL query so that I get the same result.
[00:33:53.100 --> 00:33:54.300]   Same thing with spreadsheets.
[00:33:54.300 --> 00:33:58.260]   Like give me the spreadsheets, don't take the screenshot of the spreadsheet and save
[00:33:58.260 --> 00:33:59.260]   it to me.
[00:33:59.260 --> 00:34:00.260]   That's totally personal.
[00:34:00.260 --> 00:34:06.060]   I think people are different in their philosophies of how they do this.
[00:34:06.060 --> 00:34:08.940]   The trolley affects stuff.
[00:34:08.940 --> 00:34:12.300]   Interesting.
[00:34:12.300 --> 00:34:20.820]   I guess, what is your takeaway on the whole space of ML tooling?
[00:34:20.820 --> 00:34:27.820]   Which obviously, I run an ML ops tools company, so, but please, like you won't offend me with
[00:34:27.820 --> 00:34:28.980]   your answer here.
[00:34:28.980 --> 00:34:33.980]   I'm really curious, kind of like, did you feel like people were using tools or were
[00:34:33.980 --> 00:34:35.580]   they kind of like rolling their own tools?
[00:34:35.580 --> 00:34:39.180]   Did you feel like there's like gaps and like missing tools?
[00:34:39.180 --> 00:34:42.980]   Were you inspired to like, you know, start a company in the space from the feedback that
[00:34:42.980 --> 00:34:43.980]   you got?
[00:34:43.980 --> 00:34:47.700]   I think it'd be hard for me to contain myself, but I'm curious what your overall takeaway
[00:34:47.700 --> 00:34:48.700]   is.
[00:34:48.700 --> 00:34:54.100]   There's a lot of companies that can be started from the paper, but anyway.
[00:34:54.100 --> 00:35:01.900]   I thought that the three Vs thing made tools, or at least the viability of ML tools, make
[00:35:01.900 --> 00:35:03.100]   a lot more sense.
[00:35:03.100 --> 00:35:06.120]   Like experiment tracking, weights and biases is a great example.
[00:35:06.120 --> 00:35:10.900]   It really 10Xs the velocity experience within experimentation.
[00:35:10.900 --> 00:35:12.740]   Like truly 10Xs it.
[00:35:12.740 --> 00:35:17.940]   No longer do I have to go copy paste my results into a spreadsheet and back and forth between
[00:35:17.940 --> 00:35:19.300]   training script and spreadsheet.
[00:35:19.300 --> 00:35:20.300]   Right?
[00:35:20.300 --> 00:35:21.580]   Like it's just nice.
[00:35:21.580 --> 00:35:24.340]   Great velocity experience.
[00:35:24.340 --> 00:35:33.580]   I think most tools that I've seen in this space don't really 10X in any of these dimensions.
[00:35:33.580 --> 00:35:36.580]   What are the dimensions for someone who hasn't read the paper?
[00:35:36.580 --> 00:35:42.180]   Like velocity, validating early, and then versioning.
[00:35:42.180 --> 00:35:46.700]   Like versioning is an interesting, I think there's a lot of people trying to work on
[00:35:46.700 --> 00:35:55.460]   reproducibility and related, I have thoughts on reproducibility, but like related problems.
[00:35:55.460 --> 00:36:00.940]   But it really needs to be like a 10X experience in comparison to what people used to do with
[00:36:00.940 --> 00:36:05.100]   versioning or with one of the variables.
[00:36:05.100 --> 00:36:07.460]   I think that that's really hard to do in the ML tooling space.
[00:36:07.460 --> 00:36:09.140]   People are really trying to find that.
[00:36:09.140 --> 00:36:13.420]   People are, at least in my experience, like simply trying to throw like software engineering
[00:36:13.420 --> 00:36:19.420]   principles at ML workflows and hope they land.
[00:36:19.420 --> 00:36:25.580]   But if it doesn't really like push one of these variables, then it's unclear that to
[00:36:25.580 --> 00:36:29.180]   me it's a successful tool.
[00:36:29.180 --> 00:36:34.420]   Like this ML monitoring is also a really interesting space because people do care about the concept
[00:36:34.420 --> 00:36:35.420]   of validating.
[00:36:35.420 --> 00:36:42.220]   Like I want to validate that my predictions are good before somebody complains.
[00:36:42.220 --> 00:36:46.060]   But it's really unsolved in like how do we do this precisely?
[00:36:46.060 --> 00:36:49.660]   How do we not give people alert fatigue?
[00:36:49.660 --> 00:36:57.060]   I think people will go to a lot of extents to like, the friction of integrating an observability
[00:36:57.060 --> 00:37:03.580]   or monitoring tool can be pretty high if you get results, but people are not getting results.
[00:37:03.580 --> 00:37:07.300]   And okay, I guess what are your thoughts on reproducibility?
[00:37:07.300 --> 00:37:14.460]   Oh, then there's an interesting paper, gosh, I don't remember off the top of my head.
[00:37:14.460 --> 00:37:15.460]   This is bad.
[00:37:15.460 --> 00:37:16.460]   I shouldn't know.
[00:37:16.460 --> 00:37:21.420]   But they pose that, hey, exact reproducibility is often just like not achievable in a lot
[00:37:21.420 --> 00:37:23.340]   of ML settings.
[00:37:23.340 --> 00:37:28.020]   Just because like when you're a data scientist at a company and you're launching a job, like,
[00:37:28.020 --> 00:37:31.700]   yeah, sure you can control your random seed, but like you can't control the infrastructure
[00:37:31.700 --> 00:37:39.420]   or like the GPU provision to you is like the underlying data that you called from whatever.
[00:37:39.420 --> 00:37:44.980]   Like what matters is getting like some percentage wise, if you're trying to reproduce a model,
[00:37:44.980 --> 00:37:48.260]   like I want to get the same accuracy or a similar accuracy.
[00:37:48.260 --> 00:37:53.340]   I cannot rely on getting the same model parameters.
[00:37:53.340 --> 00:38:02.660]   So this notion quite like, I guess, it's a little bit orthogonal to like all of the provenance
[00:38:02.660 --> 00:38:07.300]   and like instrumentation of ML workflows to get exact reproducibility.
[00:38:07.300 --> 00:38:12.500]   I'm not sure like how feasible that is like in a Kubernetes environment, for example,
[00:38:12.500 --> 00:38:15.420]   or like larger scale infrastructure.
[00:38:15.420 --> 00:38:20.420]   Wait, so to summarize your position, is it that reproducibility is impossible?
[00:38:20.420 --> 00:38:24.740]   Oh, I think we just like for reproducibility tools, like we need to rethink like what it
[00:38:24.740 --> 00:38:32.140]   means to get like reproducibility, like tracing, for example, tracing like a data scientist
[00:38:32.140 --> 00:38:37.480]   workflow, saving the exact artifacts.
[00:38:37.480 --> 00:38:38.840]   Is that what matters?
[00:38:38.840 --> 00:38:43.620]   Like what is it that truly matters with reproducibility?
[00:38:43.620 --> 00:38:48.980]   If I have the artifact, but I can't reproduce that artifact or like if I have logged artifacts
[00:38:48.980 --> 00:38:53.380]   at every step of the way, but I can't reproduce them, does that help me?
[00:38:53.380 --> 00:38:59.100]   I feel like these are kind of questions that I don't have the answer to off the top of
[00:38:59.100 --> 00:39:00.100]   my head.
[00:39:00.100 --> 00:39:03.540]   Okay, I can't help but like weigh in on my own thoughts here because people ask me about
[00:39:03.540 --> 00:39:04.540]   this a lot.
[00:39:04.540 --> 00:39:11.660]   I guess I think I totally agree that reproducibility is kind of like much less of like a binary
[00:39:11.660 --> 00:39:13.740]   switch than people realize.
[00:39:13.740 --> 00:39:18.940]   Like I think there's like lots of things you can do that are like increasingly annoying
[00:39:18.940 --> 00:39:22.140]   to make things like more reproducible.
[00:39:22.140 --> 00:39:25.360]   And I kind of think that every, and I think there's a real cost, so you shouldn't necessarily
[00:39:25.360 --> 00:39:27.540]   do everything possible.
[00:39:27.540 --> 00:39:34.180]   But I do think most people would stand to gain from going like further along that like,
[00:39:34.180 --> 00:39:38.140]   you know, reproducibility trade-off curve than they're doing today.
[00:39:38.140 --> 00:39:43.780]   So I was trying to explain that to customers actually of like, hey, you know, we try to
[00:39:43.780 --> 00:39:48.460]   make it easy to like save a lot of stuff so that you have, you know, because like where
[00:39:48.460 --> 00:39:53.140]   most people starting place, at least in my experience, is like zero reproducibility.
[00:39:53.140 --> 00:39:56.780]   Like I'm talking about like a model they made six months ago, they couldn't even tell you
[00:39:56.780 --> 00:40:01.020]   like the state of the code when the model trained, like forget about the data that,
[00:40:01.020 --> 00:40:02.020]   you know, went into it.
[00:40:02.020 --> 00:40:07.040]   And so it's sort of like, you know, I think every step towards reproducibility is going
[00:40:07.040 --> 00:40:09.260]   to make your team function better.
[00:40:09.260 --> 00:40:11.300]   It's going to help you with governance.
[00:40:11.300 --> 00:40:13.340]   Like there's so many reasons you actually would want to do it.
[00:40:13.340 --> 00:40:19.380]   But I think it is incredibly expensive to get perfect reproducibility like you're saying.
[00:40:19.380 --> 00:40:20.380]   Yeah.
[00:40:20.380 --> 00:40:25.220]   I like your definition of like kind of pushing further along the reproducibility axis.
[00:40:25.220 --> 00:40:30.260]   Then the question becomes like, okay, what are the markers on this axis?
[00:40:30.260 --> 00:40:31.420]   I don't have an answer to this.
[00:40:31.420 --> 00:40:32.420]   I'm curious.
[00:40:32.420 --> 00:40:36.460]   Well, if that's your next paper, I'll definitely, if you can come back and tell us about it.
[00:40:36.460 --> 00:40:37.460]   On my next paper.
[00:40:37.460 --> 00:40:46.820]   You had kind of another kind of interesting, I love all the different kind of frameworks
[00:40:46.820 --> 00:40:47.820]   that you're introducing here.
[00:40:47.820 --> 00:40:51.540]   I feel like you're really good at sort of summarizing stuff and putting them into simple
[00:40:51.540 --> 00:40:57.460]   CEO style frameworks, but you also have this notion of like kind of like layers that tooling
[00:40:57.460 --> 00:40:58.460]   lives at, right?
[00:40:58.460 --> 00:41:00.940]   Could you maybe talk a little bit about that?
[00:41:00.940 --> 00:41:02.140]   Yeah.
[00:41:02.140 --> 00:41:09.740]   So this terminology caused a lot of controversy within the authors trying to come up with
[00:41:09.740 --> 00:41:10.740]   the names.
[00:41:10.740 --> 00:41:17.020]   I don't think anyone loves the four layer names that we have made.
[00:41:17.020 --> 00:41:21.740]   So I don't know if they're the best, but we kind of think about the stack of tools that
[00:41:21.740 --> 00:41:29.740]   an ML engineer interacts with in frequency of like most frequency to least frequency,
[00:41:29.740 --> 00:41:30.780]   top to bottom.
[00:41:30.780 --> 00:41:33.300]   So at the top is this kind of run layer.
[00:41:33.300 --> 00:41:35.940]   Then we think about a pipeline layer.
[00:41:35.940 --> 00:41:38.500]   Pipelines are made up of multiple components.
[00:41:38.500 --> 00:41:45.980]   So like, I don't know, feature engineering, feature generation, training, train to split,
[00:41:45.980 --> 00:41:47.620]   whatever components you want to have.
[00:41:47.620 --> 00:41:52.220]   And then at the bottom, like underlying is like all this infrastructure that people use,
[00:41:52.220 --> 00:41:58.820]   like their compute, like what is the workflow kind of built on top of.
[00:41:58.820 --> 00:42:06.660]   And we kind of notice or observe that when people want to make changes to their workflow,
[00:42:06.660 --> 00:42:10.700]   it's the easiest to do in the run layer.
[00:42:10.700 --> 00:42:15.460]   It is much harder to kind of like kick something out of the infrastructure layer and replace
[00:42:15.460 --> 00:42:16.460]   that.
[00:42:16.460 --> 00:42:22.860]   And it happens in certain companies, but it's like a big like organizational effort to do
[00:42:22.860 --> 00:42:23.860]   it.
[00:42:23.860 --> 00:42:26.940]   Like they all have many meetings.
[00:42:26.940 --> 00:42:27.940]   It's true.
[00:42:27.940 --> 00:42:28.940]   It's true.
[00:42:28.940 --> 00:42:33.020]   What would be an example of a change someone might make at that layer?
[00:42:33.020 --> 00:42:34.020]   Which one?
[00:42:34.020 --> 00:42:35.020]   The infrastructure layer?
[00:42:35.020 --> 00:42:36.020]   Yeah.
[00:42:36.020 --> 00:42:37.020]   Yeah.
[00:42:37.020 --> 00:42:47.060]   I think one example is like moving from like GPU clusters on prem to cloud GPU.
[00:42:47.060 --> 00:42:48.060]   That's one big one.
[00:42:48.060 --> 00:42:57.940]   Another one is like introducing like an orchestrator for these servers, like Kubernetes.
[00:42:57.940 --> 00:43:02.780]   At least these are things that we, like I personally experienced also at my previous
[00:43:02.780 --> 00:43:07.180]   company that I was at.
[00:43:07.180 --> 00:43:12.060]   And like even like the pipeline layer is super annoying to make changes to.
[00:43:12.060 --> 00:43:17.420]   Like if you have like Airflow, Kubeflow as your orchestrator, like it is such a pain
[00:43:17.420 --> 00:43:20.620]   to migrate that.
[00:43:20.620 --> 00:43:24.620]   Every I don't know, like planning meeting will be like, and should we migrate?
[00:43:24.620 --> 00:43:26.100]   Yeah, we know we need to migrate.
[00:43:26.100 --> 00:43:27.780]   Oh, but it's going to be so much work.
[00:43:27.780 --> 00:43:28.780]   Yeah, I know.
[00:43:28.780 --> 00:43:32.220]   Like stuff like that where it's like if I'm a tool builder, I really don't want to get
[00:43:32.220 --> 00:43:39.940]   into that space because like I'll have to sell so hard to people to switch a tool at
[00:43:39.940 --> 00:43:42.220]   that layer.
[00:43:42.220 --> 00:43:44.220]   Yeah.
[00:43:44.220 --> 00:43:49.020]   So I guess in that sense, like one thing we found is like the open source tools that could
[00:43:49.020 --> 00:43:52.420]   be integrated at the run layer, like Waze and Bias is actually a great example of this,
[00:43:52.420 --> 00:43:59.700]   where it's like, yeah, like one engineer can like simply integrate that into their pipeline
[00:43:59.700 --> 00:44:03.380]   and it will be useful for multiple runs of that pipeline.
[00:44:03.380 --> 00:44:09.420]   Like they're not replacing the pipeline layer tool.
[00:44:09.420 --> 00:44:15.300]   We found that those were the tools that the, I guess, interviewees were willing to adopt
[00:44:15.300 --> 00:44:20.180]   the most.
[00:44:20.180 --> 00:44:27.820]   But I feel like we could have done a lot more in kind of like running a survey, a quantitative
[00:44:27.820 --> 00:44:29.660]   survey or something on this.
[00:44:29.660 --> 00:44:34.140]   Maybe that's like somebody's future workers, whoever's interested in that.
[00:44:34.140 --> 00:44:38.700]   So I'm hesitant to like prescribe this as like the end all be all.
[00:44:38.700 --> 00:44:40.580]   Prescribe the layers or prescribe like specific...
[00:44:40.580 --> 00:44:46.660]   Oh yeah, the layers and the idea of like, if you're trying to build an MLOps tool, don't
[00:44:46.660 --> 00:44:54.980]   build for, like don't try to like, I don't know, replace TensorFlow or PyTorch.
[00:44:54.980 --> 00:44:58.020]   Those people have their vote.
[00:44:58.020 --> 00:45:03.540]   People are not going to rewrite all of their deep learning models in your framework.
[00:45:03.540 --> 00:45:04.540]   Maybe unless you build...
[00:45:04.540 --> 00:45:05.540]   What if it's Jax?
[00:45:05.540 --> 00:45:06.540]   Maybe they will do it.
[00:45:06.540 --> 00:45:10.620]   Yeah, and then I don't want to get into, again, I don't want to get into like these debates,
[00:45:10.620 --> 00:45:15.740]   but it feels like one layer is easier than the other.
[00:45:15.740 --> 00:45:17.940]   Well, okay.
[00:45:17.940 --> 00:45:23.900]   I mean, I really appreciate your kind of open mindedness about like sort of like workflows
[00:45:23.900 --> 00:45:27.740]   and setups and I totally share it.
[00:45:27.740 --> 00:45:32.860]   But you know, here we are on October 2022 and like somebody like listening to this podcast,
[00:45:32.860 --> 00:45:36.940]   yeah, they probably like what they're, I think what a lot of the people like listening to
[00:45:36.940 --> 00:45:43.100]   it or like looking for is actually some help in like navigating, you know, like there's
[00:45:43.100 --> 00:45:45.820]   just so many options for tech stacks.
[00:45:45.820 --> 00:45:50.780]   Like there's like, you know, like SageMaker is not the same as Vertex.
[00:45:50.780 --> 00:45:53.580]   It's not the same as Azure ML.
[00:45:53.580 --> 00:45:55.140]   Totally.
[00:45:55.140 --> 00:45:57.700]   Like where would you start?
[00:45:57.700 --> 00:46:02.540]   Like let's put on an example of like, you know, like a startup CEO kind of just getting
[00:46:02.540 --> 00:46:07.140]   product market fit, like doesn't have like a lot of resources, but ML is like important
[00:46:07.140 --> 00:46:08.940]   to them.
[00:46:08.940 --> 00:46:17.020]   What would you, where would you recommend her to like begin their looking at a stack
[00:46:17.020 --> 00:46:20.940]   or how would they think about that?
[00:46:20.940 --> 00:46:23.420]   I will say a stack that is tried and true.
[00:46:23.420 --> 00:46:29.980]   It may not be the best stack, but I would recommend like getting like an AWS account
[00:46:29.980 --> 00:46:34.220]   or something, having some easy to cluster setup.
[00:46:34.220 --> 00:46:39.700]   If you're working with large amounts of data, I have my qualms about Spark, but I mean,
[00:46:39.700 --> 00:46:44.380]   Spark is useful and people know how to use Spark as a query engine on like large amounts
[00:46:44.380 --> 00:46:45.380]   of data.
[00:46:45.380 --> 00:46:46.380]   Okay.
[00:46:46.380 --> 00:46:52.540]   What are your qualms about Spark in a nutshell?
[00:46:52.540 --> 00:46:58.900]   I'm like sitting in the lab where Spark was invented.
[00:46:58.900 --> 00:47:06.940]   I don't like the idea, so I think I subscribe to like the database community of like MapReduce
[00:47:06.940 --> 00:47:13.220]   as a philosophy of processing large amounts of data is not great because it forces the
[00:47:13.220 --> 00:47:16.860]   application developer to reason about record level problems.
[00:47:16.860 --> 00:47:21.540]   Like if one record is corrupted, how do I handle it?
[00:47:21.540 --> 00:47:25.780]   It also forces the application developer to reason about consistency.
[00:47:25.780 --> 00:47:29.780]   Like if I'm controlling the parallelization, right?
[00:47:29.780 --> 00:47:36.020]   If I'm programming like a Spark job or MapReduce job or something, I shouldn't have to do that
[00:47:36.020 --> 00:47:39.540]   if I'm a data scientist, I shouldn't be controlling these kinds of variables.
[00:47:39.540 --> 00:47:44.500]   And this is like what DVMSs are really great for is like they abstract away these like
[00:47:44.500 --> 00:47:51.820]   parallels and internals, they abstract away like acid, how people achieve acid, the DVMS
[00:47:51.820 --> 00:47:52.820]   is doing that.
[00:47:52.820 --> 00:47:58.700]   So in that sense, I'm not a really big fan of this like kind of MapReduce style of work.
[00:47:58.700 --> 00:48:03.220]   It's also like kind of inefficient to like have like a SQL layer on top of a MapReduce
[00:48:03.220 --> 00:48:11.780]   layer on top of whatever like the storage is.
[00:48:11.780 --> 00:48:16.940]   But that's like a separate thing aside that's not like a really user experience thing.
[00:48:16.940 --> 00:48:25.260]   I like as an aside, like I think it's interesting, like the whole DuckDB, at least, explosion
[00:48:25.260 --> 00:48:31.260]   that we see going on of bringing like these kinds of large scale data queries back to
[00:48:31.260 --> 00:48:32.340]   the DVMS.
[00:48:32.340 --> 00:48:33.860]   I'm curious where that's going to go.
[00:48:33.860 --> 00:48:34.860]   Interesting.
[00:48:34.860 --> 00:48:35.860]   All right.
[00:48:35.860 --> 00:48:38.300]   But I totally derailed you.
[00:48:38.300 --> 00:48:43.740]   So you're saying like kind of AWS, like EC2, start there.
[00:48:43.740 --> 00:48:50.340]   Get an EMR cluster for Spark and like have some way for data scientists to interface
[00:48:50.340 --> 00:48:53.260]   with these machines.
[00:48:53.260 --> 00:48:59.780]   Kubernetes, this might be a lot, but some way for them to like, I guess, access the
[00:48:59.780 --> 00:49:03.660]   machines that you have in your computing cluster.
[00:49:03.660 --> 00:49:05.320]   So that's one thing.
[00:49:05.320 --> 00:49:11.140]   I think another big thing is like once you get to like production level stuff, right?
[00:49:11.140 --> 00:49:18.860]   Or if you have data pipelines at your company, you need to have some sort of workflow scheduler,
[00:49:18.860 --> 00:49:23.060]   something to define like a DAG and then execute those DAGs.
[00:49:23.060 --> 00:49:24.700]   Airflow is like really tried and true.
[00:49:24.700 --> 00:49:27.980]   I know a ton of people hate it, but it will work.
[00:49:27.980 --> 00:49:30.540]   It is also known to work in cloud-based settings.
[00:49:30.540 --> 00:49:35.080]   They also have a nice Python DSL to be able to define DAGs.
[00:49:35.080 --> 00:49:37.920]   They have a nice UI to trigger the DAG.
[00:49:37.920 --> 00:49:40.120]   They have a nice UI to backfill the DAG.
[00:49:40.120 --> 00:49:43.140]   So I would say that's good enough.
[00:49:43.140 --> 00:49:48.960]   And then on top of that, I think like people, I don't know, hire a good like data science
[00:49:48.960 --> 00:49:55.040]   lead, ML engineer to do the ML stuff.
[00:49:55.040 --> 00:50:00.940]   Do you have any advice on hiring a first ML engineer or data scientist?
[00:50:00.940 --> 00:50:03.260]   I wish.
[00:50:03.260 --> 00:50:08.620]   I almost want to say don't hire somebody right out of college, but I was hired right out
[00:50:08.620 --> 00:50:09.620]   of college.
[00:50:09.620 --> 00:50:10.900]   So I shouldn't say that.
[00:50:10.900 --> 00:50:13.560]   But like the problem with that was I didn't know anything.
[00:50:13.560 --> 00:50:17.080]   Like here I was like writing deep learning models in Jupyter notebooks, saving them to
[00:50:17.080 --> 00:50:20.140]   S3 and then like telling other people to like read.
[00:50:20.140 --> 00:50:23.540]   No, this is like a terrible workflow.
[00:50:23.540 --> 00:50:27.860]   Or a terrible workflow if you want multiple people to collaborate on the same thing and
[00:50:27.860 --> 00:50:32.700]   share learnings effectively and also scale.
[00:50:32.700 --> 00:50:40.940]   I think people who have data engineering experience tend to like at least think about the right,
[00:50:40.940 --> 00:50:47.280]   not just terminology, but concepts like SLAs are a good example of like ML people should
[00:50:47.280 --> 00:50:49.460]   also be thinking about SLAs.
[00:50:49.460 --> 00:50:54.980]   Like what is a minimum accuracy that my product should have before somebody complains about
[00:50:54.980 --> 00:50:57.540]   the predictions, like stuff like that.
[00:50:57.540 --> 00:51:01.700]   How do I schedule things on a recurring basis?
[00:51:01.700 --> 00:51:05.560]   A lot of about pipelines, you can be casted as like data pipelines, right?
[00:51:05.560 --> 00:51:10.180]   So I would hire somebody who has data engineering experience for sure.
[00:51:10.180 --> 00:51:13.740]   Ideally, hopefully someone who's trained to model.
[00:51:13.740 --> 00:51:17.660]   I honestly think that's less relevant than having the data engineering experience.
[00:51:17.660 --> 00:51:22.460]   I feel like I've talked to so many people who have like, who have ML experience, but
[00:51:22.460 --> 00:51:25.220]   then don't have any software engineering or data engineering experience.
[00:51:25.220 --> 00:51:32.220]   And that's like really hard to convert when you're starting a company.
[00:51:32.220 --> 00:51:33.220]   Makes sense.
[00:51:33.220 --> 00:51:34.220]   Well, you know, we always end with two questions.
[00:51:34.220 --> 00:51:35.540]   I want to make sure we get them in.
[00:51:35.540 --> 00:51:47.340]   So I guess one is, what is a underappreciated topic in machine learning broadly?
[00:51:47.340 --> 00:51:53.780]   Like if you were to go back into machine learning and could like, you know, investigate something,
[00:51:53.780 --> 00:51:58.140]   where would you be like wanting to look more?
[00:51:58.140 --> 00:52:00.540]   That is not a data management problem.
[00:52:00.540 --> 00:52:04.900]   This is an exercise for myself.
[00:52:04.900 --> 00:52:13.340]   I think the idea of interpretability, but I think of it as provenance.
[00:52:13.340 --> 00:52:19.380]   Like the influence functions paper is interesting, was like, if I have a image and I have a prediction
[00:52:19.380 --> 00:52:24.820]   of this image, what are the training examples that like most likely helped the model get
[00:52:24.820 --> 00:52:27.860]   to this image?
[00:52:27.860 --> 00:52:33.860]   But techniques to do this are kind of computationally expensive or are limited to like a single
[00:52:33.860 --> 00:52:40.620]   point in the training data or require like a full pass through the training set for every...
[00:52:40.620 --> 00:52:44.420]   I think there's a lot of work that can be done there kind of on interpretability, but
[00:52:44.420 --> 00:52:49.620]   not of the model, but like explaining how a prediction got to where it is based on the
[00:52:49.620 --> 00:52:51.180]   data that a model was trained on.
[00:52:51.180 --> 00:52:52.180]   Interesting.
[00:52:52.180 --> 00:52:53.740]   Do you remember the name of that paper?
[00:52:53.740 --> 00:52:57.220]   I'd love to put it in the show notes.
[00:52:57.220 --> 00:53:00.500]   Understanding Black Box Predictions via Influence Functions.
[00:53:00.500 --> 00:53:01.620]   Nice.
[00:53:01.620 --> 00:53:05.660]   And then I guess the final question, which you are incredibly qualified to answer, I
[00:53:05.660 --> 00:53:11.820]   think is when you look at the life cycle from kind of research paper to running successfully
[00:53:11.820 --> 00:53:18.100]   in production, where do you see the biggest bottleneck or the most surprising bottleneck?
[00:53:18.100 --> 00:53:26.020]   Evaluating if this research or this new idea actually provides a worthwhile gain over another
[00:53:26.020 --> 00:53:27.300]   solution.
[00:53:27.300 --> 00:53:28.420]   Is there something off the shelf?
[00:53:28.420 --> 00:53:33.060]   Is there something baseline that can get something very similar or get a performance that's very
[00:53:33.060 --> 00:53:37.340]   similar but not have to go through this headache of understanding the new thing and implementing
[00:53:37.340 --> 00:53:38.340]   this headache?
[00:53:38.340 --> 00:53:40.540]   I don't even think we have frameworks to think about this.
[00:53:40.540 --> 00:53:46.780]   So you're just saying it's painful to try all the different things and see if they actually
[00:53:46.780 --> 00:53:47.780]   work?
[00:53:47.780 --> 00:53:48.780]   Yeah.
[00:53:48.780 --> 00:53:49.780]   Yeah.
[00:53:49.780 --> 00:53:53.860]   It's like, do we even need to try all the different things and see if they work?
[00:53:53.860 --> 00:54:00.260]   If I'm a practitioner at a company, when do I actually pay attention to some ML research
[00:54:00.260 --> 00:54:01.260]   development?
[00:54:01.260 --> 00:54:03.180]   How do I actually integrate it into my system?
[00:54:03.180 --> 00:54:07.820]   I don't think that people have a framework for thinking about this.
[00:54:07.820 --> 00:54:08.820]   I haven't heard of people.
[00:54:08.820 --> 00:54:09.820]   All right.
[00:54:09.820 --> 00:54:10.820]   Well, thank you so much for your time.
[00:54:10.820 --> 00:54:11.820]   That was super fun.
[00:54:11.820 --> 00:54:12.820]   I really appreciate it.
[00:54:12.820 --> 00:54:13.820]   Yeah.
[00:54:13.820 --> 00:54:14.820]   Thanks for having me.
[00:54:14.820 --> 00:54:23.580]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:54:23.580 --> 00:54:28.300]   to the show notes in the description where you can find links to all the papers that
[00:54:28.300 --> 00:54:32.460]   are mentioned, supplemental material, and a transcription that we worked really hard
[00:54:32.460 --> 00:54:33.460]   to produce.
[00:54:33.460 --> 00:54:33.820]   So check it out.
[00:54:33.820 --> 00:54:36.400]   (upbeat music)
[00:54:36.400 --> 00:54:38.280]   [music fades out]

