
[00:00:00.000 --> 00:00:06.000]   Rosanne is a senior researcher at Uber AI, which she joined via the Geometric Intelligence Acquisition.
[00:00:06.000 --> 00:00:14.600]   A lot of this class has been about the practical nuts and bolts of how do you actually take these amazing tools,
[00:00:14.600 --> 00:00:17.600]   which are deep neural networks, and make them work on real problems.
[00:00:17.600 --> 00:00:21.600]   So we thought it would be appropriate to wrap up with something of a different flavor,
[00:00:21.600 --> 00:00:23.600]   which is something more fundamental.
[00:00:23.600 --> 00:00:26.600]   And a lot of Rosanne's research is on exactly that.
[00:00:26.600 --> 00:00:31.600]   It's on understanding, looking into the black box of neural nets, and understanding what's going on.
[00:00:31.600 --> 00:00:34.600]   So thanks for being here, and excited to hear your talk.
[00:00:34.600 --> 00:00:36.600]   Yeah, thank you. Thanks.
[00:00:36.600 --> 00:00:39.600]   Hi, it's a pleasure to be here, and thanks for the invite.
[00:00:39.600 --> 00:00:43.600]   So it turns out I'm the last speaker, not just for today, but for the whole series.
[00:00:43.600 --> 00:00:47.600]   I want to first congratulate you guys for having gone so far.
[00:00:47.600 --> 00:00:52.600]   Just bear with me for another 45 minutes, or maybe less than that, and you're going to be done with.
[00:00:52.600 --> 00:00:58.600]   Not done with being lectured at, not done with learning and done with projects, for sure.
[00:00:58.600 --> 00:01:04.600]   So you'll notice that I don't have a title yet, because I feel like the thing's a little bit different here.
[00:01:04.600 --> 00:01:08.600]   So usually when I go out and give a talk, I know what I'm going to talk about.
[00:01:08.600 --> 00:01:12.600]   I'm at a conference. I'm going to talk about the paper I published at that conference,
[00:01:12.600 --> 00:01:18.600]   and people are at that conference coming to hear about that talk that I'm going to talk about.
[00:01:18.600 --> 00:01:23.600]   So what people want and what I can offer are perfectly, perfectly aligned in that situation.
[00:01:23.600 --> 00:01:27.600]   I feel like it's different here, because all you guys have very different backgrounds,
[00:01:27.600 --> 00:01:33.600]   and some may be interested in kind of becoming an independent researcher, like I am.
[00:01:33.600 --> 00:01:40.600]   So my experience will be directly applicable, but some may be interested in, say, building a research lab
[00:01:40.600 --> 00:01:47.600]   or trying to hire researchers, so maybe you want to learn from what I think is a good researcher or research attitude.
[00:01:47.600 --> 00:01:51.600]   So all the things are different, so I'm thinking I offer the same thing.
[00:01:51.600 --> 00:01:59.600]   I offer my experience, but I wanted to do it in a way that you guys can do different things with the same data.
[00:01:59.600 --> 00:02:06.600]   So what I'm saying is what I'm going to talk about for the next 40 minutes or even less are three things, who I am.
[00:02:06.600 --> 00:02:14.600]   I have to give you a context and some of the things I've done, because that's the only thing I can talk about, my experience and what I learned.
[00:02:14.600 --> 00:02:19.600]   So basically I'm giving you some data. You guys have been working with machine learning problems, so you know what's next, right?
[00:02:19.600 --> 00:02:22.600]   With the data, you're going to build a model of me.
[00:02:22.600 --> 00:02:26.600]   Each of you guys can build a model differently, because you have different brains.
[00:02:26.600 --> 00:02:32.600]   And I trust your brain that you can do a few-shot learning, one-shot learning, so super fast, I don't have to wait for you to train.
[00:02:32.600 --> 00:02:37.600]   And the model is going to be used for yourself to start doing some inference.
[00:02:37.600 --> 00:02:41.600]   So you guys are going to ask questions that suit your need, right?
[00:02:41.600 --> 00:02:44.600]   You have the same data, but you guys are going to have to ask different questions.
[00:02:44.600 --> 00:02:55.600]   So I hope through this mechanism of both me talking and you guys actively thinking what you want out of this talk, we can make better use of this time.
[00:02:55.600 --> 00:03:02.600]   So following this schedule, who I am, really quick, I'm right now a senior research scientist at Uber AI.
[00:03:02.600 --> 00:03:13.600]   I've been at the place for two years, which I've witnessed it growing from 15% group to right now about 120%.
[00:03:13.600 --> 00:03:18.600]   So that's when Uber AI started. I was part of the starting team.
[00:03:18.600 --> 00:03:23.600]   And we have different divisions in the lab, and one of them is core research, so you just publish.
[00:03:23.600 --> 00:03:26.600]   You're like a PhD student there or post-doctorate there.
[00:03:26.600 --> 00:03:31.600]   And I happen to be in that division, but I also know a little bit of what the other divisions are doing.
[00:03:31.600 --> 00:03:40.600]   So you're more interested in, say, how does machine learning apply in Uber products like Uber Eats and the drive and ride and everything,
[00:03:40.600 --> 00:03:46.600]   or how data science is executed there, how we build a platform so more people from the product team can use machine learning.
[00:03:46.600 --> 00:03:51.600]   And I know a little bit of that, but mostly I'm in core research.
[00:03:51.600 --> 00:03:56.600]   So before that, I was a machine learning researcher at Geometric Intelligence, which no longer exists now.
[00:03:56.600 --> 00:04:01.600]   If you search for it, there's news that Uber acquired us, which is good, right?
[00:04:01.600 --> 00:04:06.600]   But it's old news, so we don't have a website or anything anymore. I was there for a year.
[00:04:06.600 --> 00:04:12.600]   My title was machine learning researcher, but as you know, Josh talked about different roles last time.
[00:04:12.600 --> 00:04:16.600]   They can mean different things. Even at a startup, they can really mean just wild things.
[00:04:16.600 --> 00:04:19.600]   So it was a startup of less than 10 people when I joined.
[00:04:19.600 --> 00:04:26.600]   So we're just basically doing everything, infrastructure, research, writing papers and everything.
[00:04:26.600 --> 00:04:29.600]   Before that, I was a PhD in Northwestern.
[00:04:29.600 --> 00:04:35.600]   So some of the things I've done, I was trying to come up with a simple concept of describing it.
[00:04:35.600 --> 00:04:40.600]   I think the way to describe it is I work in AI, luckily.
[00:04:40.600 --> 00:04:44.600]   So if you think about AI, it's been going on for five, six decades.
[00:04:44.600 --> 00:04:49.600]   And really, what's really amazing things are happening is the last decade.
[00:04:49.600 --> 00:04:57.600]   And really, if you think about it, why it is happening is because neural networks start working out of big compute, big data, whatever factor.
[00:04:57.600 --> 00:05:00.600]   But that's the only reason that amazing things are happening these days.
[00:05:00.600 --> 00:05:07.600]   And I happen to be having a PhD and developing my first career and second career in that little green zone.
[00:05:07.600 --> 00:05:09.600]   So that's the only thing I can talk about.
[00:05:09.600 --> 00:05:19.600]   My work happens to be just following the theme of staring at this thing, which is neural networks, if you don't recognize it, really hard.
[00:05:19.600 --> 00:05:24.600]   And try to figure out what I know about it, what I don't know about it, and try to understand it more.
[00:05:24.600 --> 00:05:29.600]   So that's one sentence summary of what I do.
[00:05:29.600 --> 00:05:31.600]   I'm going to talk about what I learned.
[00:05:31.600 --> 00:05:38.600]   But I'm going to come back to this because it might make more sense when I deeply dive into a couple of practices I do.
[00:05:38.600 --> 00:05:47.600]   But just to give you a sneak peek, I think what I learned from doing deep learning projects is that deep learning these days can be both easy and hard.
[00:05:47.600 --> 00:05:51.600]   And for me, I think the best way to do it is to try to understand it.
[00:05:51.600 --> 00:05:55.600]   Even the things that you think you understand, you might not understand fully.
[00:05:55.600 --> 00:05:58.600]   And I think it's really important to have a researcher's mind.
[00:05:58.600 --> 00:06:00.600]   Do you have a question?
[00:06:00.600 --> 00:06:01.600]   Okay, sorry.
[00:06:01.600 --> 00:06:03.600]   We'll come back to this slide, and maybe you'll understand more.
[00:06:03.600 --> 00:06:05.600]   But just to give a sneak peek.
[00:06:05.600 --> 00:06:09.600]   So back to what I've done, and that's the main meat of this talk.
[00:06:09.600 --> 00:06:11.600]   And hopefully right now I can bring up the title.
[00:06:11.600 --> 00:06:21.600]   So the thing I've done is I've been staring at a neural network really hard, coming up with a few insights out of these few attempts of trying to understand them.
[00:06:21.600 --> 00:06:27.600]   So I'm going to have three parts of the talk, indicated by this corner that says "Part 1, 2, 3."
[00:06:27.600 --> 00:06:30.600]   And the first part is this thing that we're looking at, loss.
[00:06:30.600 --> 00:06:32.600]   So you guys have been doing projects.
[00:06:32.600 --> 00:06:35.600]   Or trained something, I hope.
[00:06:35.600 --> 00:06:43.600]   Can someone tell me what you do when you look at a training and know whether it's good or not, whether it's successful or not?
[00:06:43.600 --> 00:06:44.600]   >> Loss going down.
[00:06:44.600 --> 00:06:45.600]   >> Loss going down.
[00:06:45.600 --> 00:06:46.600]   Cool.
[00:06:46.600 --> 00:06:49.600]   Yeah, you look at this one thing, one scalar, that's loss.
[00:06:49.600 --> 00:06:51.600]   You want to see something like this.
[00:06:51.600 --> 00:07:01.600]   It starts somewhere random, goes down, maybe in the middle you do something nice, you drop the learning rate, it goes down faster or something.
[00:07:01.600 --> 00:07:05.600]   So we look at this and we think it's really odd because the whole network is so big.
[00:07:05.600 --> 00:07:07.600]   You have so many parameters.
[00:07:07.600 --> 00:07:09.600]   And at the end you're just looking at one metric.
[00:07:09.600 --> 00:07:11.600]   I mean, it's nice to have one metric.
[00:07:11.600 --> 00:07:13.600]   You can make decisions really fast.
[00:07:13.600 --> 00:07:15.600]   But it's also odd.
[00:07:15.600 --> 00:07:18.600]   It also says how little we know about neural network training.
[00:07:18.600 --> 00:07:20.600]   We're just looking at this one number.
[00:07:20.600 --> 00:07:29.600]   So if you think about what really happens in a single iteration, well, simply one thing happened, the loss moved.
[00:07:29.600 --> 00:07:31.600]   But why is it moving?
[00:07:31.600 --> 00:07:33.600]   The data didn't change.
[00:07:33.600 --> 00:07:40.600]   I mean, if you forget about the mini-batch, but just think of the loss as the whole training set loss, data didn't change.
[00:07:40.600 --> 00:07:42.600]   The only thing changing is network weights.
[00:07:42.600 --> 00:07:48.600]   So the weights changed, which is the cause of the loss changing.
[00:07:48.600 --> 00:07:55.600]   So really what we want to see here is how is every parameter's contribution to this loss change.
[00:07:55.600 --> 00:08:00.600]   So ideally, we want to have a per-parameter loss contribution.
[00:08:00.600 --> 00:08:03.600]   We want to associate a number to each parameter in this network.
[00:08:03.600 --> 00:08:06.600]   If you have millions, then let it be millions.
[00:08:06.600 --> 00:08:13.600]   And ideally, what it means is that if you have k parameters, you would have k loss curves.
[00:08:13.600 --> 00:08:17.600]   If you sum them all, you recover the loss curve that you had over here.
[00:08:17.600 --> 00:08:20.600]   And you can sum them over different dimensions.
[00:08:20.600 --> 00:08:26.600]   So you can sum them over neurons, over layers, over channels if you're doing a comp net.
[00:08:26.600 --> 00:08:29.600]   And the math is here is actually not that hard.
[00:08:29.600 --> 00:08:37.600]   I'm not going to go deep into the math, but if you think about it, it's just each parameter's gradient, which you already have per-parameter, times that parameter's movement.
[00:08:37.600 --> 00:08:39.600]   So that's the contribution it does on the loss.
[00:08:39.600 --> 00:08:42.600]   It's an approximation. It's a Taylor approximation.
[00:08:42.600 --> 00:08:46.600]   This is not super accurate, but you can work out some math to make it more accurate.
[00:08:46.600 --> 00:08:51.600]   At the end, we want to make sure that if you sum everything, it is this loss curve.
[00:08:51.600 --> 00:08:55.600]   So how do you imagine that many loss curves are like?
[00:08:55.600 --> 00:08:57.600]   So the idea is we can visualize it.
[00:08:57.600 --> 00:09:01.600]   And the hint is here is not all of them are going down.
[00:09:01.600 --> 00:09:03.600]   Some of them are just doing bad things.
[00:09:03.600 --> 00:09:08.600]   So I'm going to play a quick video here showing that.
[00:09:08.600 --> 00:09:12.600]   So each pixel here is a weight.
[00:09:12.600 --> 00:09:15.600]   And you can sum them to make different neurons.
[00:09:15.600 --> 00:09:18.600]   And if you see bars going down, that means that weight is helping.
[00:09:18.600 --> 00:09:21.600]   You see green is helping. You see purple is hurting.
[00:09:21.600 --> 00:09:24.600]   So a lot of weights are hurting the network as the network is training.
[00:09:24.600 --> 00:09:26.600]   So this is a training curve.
[00:09:26.600 --> 00:09:29.600]   So in the beginning, of course, lots of things are happening.
[00:09:29.600 --> 00:09:36.600]   As you're training, lots of things are happening, but you can see the amount of purpleness and greenness seems to be consistent.
[00:09:36.600 --> 00:09:40.600]   So lots of things are hurting. Lots of things are helping at the same time.
[00:09:40.600 --> 00:09:47.600]   You can't really come up with a solid conclusion, but this is the way that you can see a big bunch of data really quickly.
[00:09:47.600 --> 00:09:52.600]   And then you can make some nice plots to see, to build up.
[00:09:52.600 --> 00:09:59.600]   You can have some hypotheses and then come up with plots and measures to see if the hypothesis is true.
[00:09:59.600 --> 00:10:04.600]   So, OK. So for this one, say you can try to count.
[00:10:04.600 --> 00:10:07.600]   Now you have parameter per iteration contribution.
[00:10:07.600 --> 00:10:14.600]   You can try to count how many of them are hurting and how many of them are helping through training in the iteration.
[00:10:14.600 --> 00:10:21.600]   So if I indicate green as helping, red as hurting, so surprisingly not even half of the parameter here is helping.
[00:10:21.600 --> 00:10:25.600]   So almost half of the parameter in the network is hurting the training.
[00:10:25.600 --> 00:10:28.600]   So every iteration is making the loss worse.
[00:10:28.600 --> 00:10:31.600]   So here you see a bunch of white because it's MNIST.
[00:10:31.600 --> 00:10:38.600]   In MNIST, some weights are just never trained because some pixels are just bad. They're never on.
[00:10:38.600 --> 00:10:43.600]   So now you know that this number of weights are hurting and helping.
[00:10:43.600 --> 00:10:46.600]   You may want to know how much they're hurting versus helping.
[00:10:46.600 --> 00:10:50.600]   So that's where a histogram comes in handy.
[00:10:50.600 --> 00:10:54.600]   So this histogram is saying it's log scale.
[00:10:54.600 --> 00:10:58.600]   So it's really saying that most of the weights are making very, very little contributions,
[00:10:58.600 --> 00:11:01.600]   whether they're on the hurting side or the helping side.
[00:11:01.600 --> 00:11:06.600]   And only a few weights are doing big contributions. There's a heavy tilt.
[00:11:06.600 --> 00:11:09.600]   This is kind of like a behavior science analysis on weights.
[00:11:09.600 --> 00:11:12.600]   So you have a population of weights. How are they doing?
[00:11:12.600 --> 00:11:15.600]   So you can summarize by layers.
[00:11:15.600 --> 00:11:18.600]   So you see how much percentage of each layer is helping.
[00:11:18.600 --> 00:11:20.600]   They're all around 50.
[00:11:20.600 --> 00:11:24.600]   And you can plot a histogram of per weight.
[00:11:24.600 --> 00:11:28.600]   So we know that half of them is helping, half of them is hurting.
[00:11:28.600 --> 00:11:32.600]   But do you know is helping one always helping? Hurting one is always hurting?
[00:11:32.600 --> 00:11:35.600]   This histogram says no.
[00:11:35.600 --> 00:11:40.600]   Over 50% of the time, one weight is hurting.
[00:11:40.600 --> 00:11:43.600]   So this histogram centers around 50%.
[00:11:43.600 --> 00:11:47.600]   That means weight jumps around hurting and helping constantly.
[00:11:47.600 --> 00:11:50.600]   So that's MNIST. You can do the exact same thing for CIFAR.
[00:11:50.600 --> 00:11:54.600]   The white band is gone in CIFAR. Almost every weight is doing something.
[00:11:54.600 --> 00:11:58.600]   But this number is still hovering around 50%. That's surprising.
[00:11:58.600 --> 00:12:02.600]   A lot, half of the network is doing something bad.
[00:12:02.600 --> 00:12:06.600]   And this is a particularly good network. It's a REST net, right?
[00:12:06.600 --> 00:12:11.600]   It's supposed to be doing something good, but this number never changes across different networks.
[00:12:11.600 --> 00:12:13.600]   It's the same.
[00:12:13.600 --> 00:12:18.600]   So almost 50% of the network is hurting, and each parameter hurts 50% of the time.
[00:12:18.600 --> 00:12:21.600]   So the learning is very heavy-tailed. That's what we found.
[00:12:21.600 --> 00:12:25.600]   We ended up finding a lot of additional insights,
[00:12:25.600 --> 00:12:30.600]   including some layers actually just hurts all the time, like REST net, so this one.
[00:12:30.600 --> 00:12:33.600]   So in MNIST, everything was fine.
[00:12:33.600 --> 00:12:36.600]   This bar going down means it's helping.
[00:12:36.600 --> 00:12:39.600]   So negative is helping because you helped the loss going down.
[00:12:39.600 --> 00:12:41.600]   And in REST net, it's just super weird.
[00:12:41.600 --> 00:12:44.600]   The first layer and the last layer always hurts.
[00:12:44.600 --> 00:12:46.600]   So that's one thing we found in this section.
[00:12:46.600 --> 00:12:49.600]   And then there's another insight that if you plot--
[00:12:49.600 --> 00:12:52.600]   so each weight is learning. Now you have curves of each weight.
[00:12:52.600 --> 00:12:58.600]   You can just plot the moments of learning, right, the weights going--like having a big drop.
[00:12:58.600 --> 00:13:05.600]   And if you highlight them in red, you'll see that each layer actually learns about the same time.
[00:13:05.600 --> 00:13:07.600]   So layer learning is synchronized.
[00:13:07.600 --> 00:13:10.600]   If you break that into classes, it's also synchronized.
[00:13:10.600 --> 00:13:14.600]   So a lot of insights are developed once we have this measurement
[00:13:14.600 --> 00:13:18.600]   that allows you a much richer view of the training process.
[00:13:18.600 --> 00:13:20.600]   So that's the first part I want to talk about,
[00:13:20.600 --> 00:13:24.600]   looking at the neural network really hard to understand the training dynamics. Yes?
[00:13:24.600 --> 00:13:28.600]   [audience member] How can you measure parameters of contribution?
[00:13:28.600 --> 00:13:30.600]   [Yi] Yeah, so the method is actually easy.
[00:13:30.600 --> 00:13:33.600]   So you already have one thing that's per parameter, that's gradients, right?
[00:13:33.600 --> 00:13:37.600]   Once you have a loss, you have a gradient that's already per parameter.
[00:13:37.600 --> 00:13:39.600]   And now you have another thing that's per parameter,
[00:13:39.600 --> 00:13:44.600]   that is how much that parameter moved, the value that it moved.
[00:13:44.600 --> 00:13:48.600]   If you dot product the gradient, which is the direction, versus how much it moved,
[00:13:48.600 --> 00:13:51.600]   that indicates how much it moved the loss.
[00:13:51.600 --> 00:13:56.600]   So it's an approximation because the loss curve may not be linear,
[00:13:56.600 --> 00:14:00.600]   and you might take too big of a step, and you might have momentums and things like that,
[00:14:00.600 --> 00:14:03.600]   so your gradient is not exactly your direction.
[00:14:03.600 --> 00:14:08.600]   But we come up with math that is approximately pretty accurate.
[00:14:08.600 --> 00:14:13.600]   So I hope I get time to cover the other parts.
[00:14:13.600 --> 00:14:19.600]   So the other part of the talk is about, again, looking at the neural network really hard
[00:14:19.600 --> 00:14:21.600]   and trying to figure out what happens there.
[00:14:21.600 --> 00:14:23.600]   So here we're looking at confidence.
[00:14:23.600 --> 00:14:25.600]   So confidence is so widely used these days,
[00:14:25.600 --> 00:14:30.600]   and it's doing amazing things like recognizing a dog's breed really easily,
[00:14:30.600 --> 00:14:33.600]   like drawing tiny, tiny bounding boxes around human faces,
[00:14:33.600 --> 00:14:36.600]   little really, really accurate object detection,
[00:14:36.600 --> 00:14:39.600]   trying to play games right from pixels,
[00:14:39.600 --> 00:14:43.600]   and they can even generate images from random noise.
[00:14:43.600 --> 00:14:45.600]   So I'm not even -- I didn't even update this,
[00:14:45.600 --> 00:14:48.600]   so now we have way better results in generative modeling.
[00:14:48.600 --> 00:14:52.600]   And one common thing that all this amazing story is that they all use ConvNet,
[00:14:52.600 --> 00:14:56.600]   so ConvNet is very successful in all those domains.
[00:14:56.600 --> 00:14:59.600]   So if you think about how amazing ConvNet is,
[00:14:59.600 --> 00:15:04.600]   that is it can somehow find this mapping between a vector of random noise,
[00:15:04.600 --> 00:15:06.600]   of random numbers,
[00:15:06.600 --> 00:15:11.600]   and somehow this mapping is found between this one and this amazing picture,
[00:15:11.600 --> 00:15:15.600]   you would think that this task shouldn't be that hard.
[00:15:15.600 --> 00:15:21.600]   The task here is if I give you a coordinate index, you paint that pixel for me.
[00:15:21.600 --> 00:15:26.600]   You just paint one pixel, not a colorful pixel, not hair, not eyes, not human faces,
[00:15:26.600 --> 00:15:28.600]   just one pixel.
[00:15:28.600 --> 00:15:36.600]   And of course if I move that coordinate, you paint another pixel accordingly.
[00:15:36.600 --> 00:15:38.600]   So you would think that this shouldn't be as hard
[00:15:38.600 --> 00:15:42.600]   if I am allowed to build as big a ConvNet as possible,
[00:15:42.600 --> 00:15:46.600]   but it turns out we found that ConvNet failed at this simple task,
[00:15:46.600 --> 00:15:53.600]   and we call this task coordinate transform so that we can call it easier.
[00:15:53.600 --> 00:15:57.600]   So the task is that if you're given a Cartesian location
[00:15:57.600 --> 00:16:00.600]   and you're asked to draw a pixel,
[00:16:00.600 --> 00:16:03.600]   ConvNet actually does this really, really badly.
[00:16:03.600 --> 00:16:04.600]   And how bad?
[00:16:04.600 --> 00:16:06.600]   So we trained a bunch of models.
[00:16:06.600 --> 00:16:08.600]   I'm not going to show you that result,
[00:16:08.600 --> 00:16:10.600]   but basically we trained hundreds of models.
[00:16:10.600 --> 00:16:11.600]   None of them does well.
[00:16:11.600 --> 00:16:15.600]   When I say does well, I want it 100% because it's such an easy problem.
[00:16:15.600 --> 00:16:18.600]   And if you think about it, it's a classification problem
[00:16:18.600 --> 00:16:21.600]   because you're given two numbers, you're given one pixel,
[00:16:21.600 --> 00:16:25.600]   so you can really use the best loss that's out there, softmax, cross entropy.
[00:16:25.600 --> 00:16:28.600]   It should be good on everything, but it was so bad.
[00:16:28.600 --> 00:16:31.600]   The best one was only like 80-some percent,
[00:16:31.600 --> 00:16:34.600]   and here we're just digging in to see why it is so bad
[00:16:34.600 --> 00:16:37.600]   because the convolution neural network at the end
[00:16:37.600 --> 00:16:42.600]   gives you a softmax probability output over the whole space,
[00:16:42.600 --> 00:16:45.600]   and you can really see why is it doing so bad
[00:16:45.600 --> 00:16:51.600]   by both looking at the softmax and looking at the lodges
[00:16:51.600 --> 00:16:53.600]   right before the softmax layer.
[00:16:53.600 --> 00:16:56.600]   So you can see that it's trying to get to the target location,
[00:16:56.600 --> 00:16:57.600]   but never quite.
[00:16:57.600 --> 00:16:59.600]   Sometimes it gets it right, but some probability
[00:16:59.600 --> 00:17:02.600]   still leaks outside of this target location.
[00:17:02.600 --> 00:17:03.600]   So that's how bad it is.
[00:17:03.600 --> 00:17:08.600]   So this is kind of a debugging step to see why it is doing such a bad job.
[00:17:08.600 --> 00:17:10.600]   So this direction, we proved that it's hard.
[00:17:10.600 --> 00:17:13.600]   We were trying, like, what about this direction?
[00:17:13.600 --> 00:17:16.600]   What if I give you a picture with one pixel on
[00:17:16.600 --> 00:17:20.600]   and I want you to tell me what was the location in Cartesian space?
[00:17:20.600 --> 00:17:25.600]   So an easy story here is that it's also very, very hard.
[00:17:25.600 --> 00:17:30.600]   It paints pixels, kind of, but it's a very sloppy manner.
[00:17:30.600 --> 00:17:36.600]   So this is also harder, and then we actually didn't start here.
[00:17:36.600 --> 00:17:38.600]   This was the end of a long investigation
[00:17:38.600 --> 00:17:42.600]   because in the beginning, we were actually trying to train
[00:17:42.600 --> 00:17:46.600]   some generative models to generate some colorful objects moving around,
[00:17:46.600 --> 00:17:48.600]   and it was never moving around,
[00:17:48.600 --> 00:17:53.600]   so that's why we were trying to debug why is Kalmanite doing this thing so badly,
[00:17:53.600 --> 00:17:57.600]   and we keep simplifying it and reached this coordinate transform problem.
[00:17:57.600 --> 00:18:01.600]   So this itself has already been a debugging process
[00:18:01.600 --> 00:18:03.600]   to figure out why it is so hard.
[00:18:03.600 --> 00:18:06.600]   So it turns out Kalmanite suck at all these tasks
[00:18:06.600 --> 00:18:08.600]   when it has to do with coordinates.
[00:18:08.600 --> 00:18:09.600]   Yeah.
[00:18:09.600 --> 00:18:11.600]   - What's the data?
[00:18:11.600 --> 00:18:13.600]   - So, yeah, for this task, it's easy.
[00:18:13.600 --> 00:18:16.600]   You just generate all these locations.
[00:18:16.600 --> 00:18:19.600]   You can have a picture, say, 64 by 64,
[00:18:19.600 --> 00:18:21.600]   so you would have all those locations.
[00:18:21.600 --> 00:18:23.600]   Each has one pixel on,
[00:18:23.600 --> 00:18:26.600]   and you have accordingly their Cartesian location,
[00:18:26.600 --> 00:18:28.600]   so it's a totally supervised task.
[00:18:28.600 --> 00:18:30.600]   - And you have a holdout set?
[00:18:30.600 --> 00:18:32.600]   - Yeah, I have a holdout set.
[00:18:32.600 --> 00:18:35.600]   I didn't do that, but you can have a uniform holdout set
[00:18:35.600 --> 00:18:38.600]   when this can be training, this can be testing,
[00:18:38.600 --> 00:18:40.600]   or you can have a harder holdout set
[00:18:40.600 --> 00:18:43.600]   when you have the whole quadrant, like this one,
[00:18:43.600 --> 00:18:46.600]   this lower right corner holdout as test set.
[00:18:46.600 --> 00:18:48.600]   So it was bad on both holdouts.
[00:18:48.600 --> 00:18:50.600]   - And on bad data training?
[00:18:50.600 --> 00:18:53.600]   - So the training was, yeah, also bad.
[00:18:53.600 --> 00:18:55.600]   You can get trained to be 100%.
[00:18:55.600 --> 00:18:58.600]   You can always train a network to memorize,
[00:18:58.600 --> 00:19:03.600]   but, yeah, the real thing that we care is the task set.
[00:19:03.600 --> 00:19:05.600]   Yeah, I should...
[00:19:05.600 --> 00:19:07.600]   Let me see.
[00:19:07.600 --> 00:19:10.600]   I still have the slides, but in the interest of time,
[00:19:10.600 --> 00:19:12.600]   I didn't want to show it.
[00:19:12.600 --> 00:19:15.600]   So this will be, like, all the 200 models that we've trained.
[00:19:15.600 --> 00:19:18.600]   So you can definitely train some 100% training.
[00:19:18.600 --> 00:19:20.600]   This is train accuracy.
[00:19:20.600 --> 00:19:22.600]   So to have 100% train accuracy is easy,
[00:19:22.600 --> 00:19:24.600]   but they overfit, right?
[00:19:24.600 --> 00:19:26.600]   So this just means that test accuracy is low.
[00:19:26.600 --> 00:19:28.600]   This will be good. We want it to be here.
[00:19:28.600 --> 00:19:30.600]   But all the models, they are either very bad
[00:19:30.600 --> 00:19:33.600]   or they're getting better, but they start overfitting.
[00:19:33.600 --> 00:19:35.600]   And uniform holdout is easier.
[00:19:35.600 --> 00:19:40.600]   Quadrant, this holdout, which you hold out the lower...
[00:19:40.600 --> 00:19:44.600]   Like, this corner, as Tess said, which is really bad.
[00:19:44.600 --> 00:19:46.600]   Sorry.
[00:19:46.600 --> 00:19:48.600]   Yeah.
[00:19:48.600 --> 00:19:50.600]   - Did you try fully connected?
[00:19:50.600 --> 00:19:52.600]   - Yeah, this I don't.
[00:19:52.600 --> 00:19:54.600]   So this, we were trying-- you're right.
[00:19:54.600 --> 00:19:56.600]   So fully connected will solve this easier,
[00:19:56.600 --> 00:19:58.600]   but we were trying to figure out a convolution problem
[00:19:58.600 --> 00:20:00.600]   because we really-- in the beginning,
[00:20:00.600 --> 00:20:03.600]   we want to see why it cannot paint things that move.
[00:20:03.600 --> 00:20:07.600]   So eventually, we would have to use a convolutional neural network
[00:20:07.600 --> 00:20:09.600]   to make-- to solve this problem.
[00:20:09.600 --> 00:20:11.600]   We were, like, simplifying it to find this problem,
[00:20:11.600 --> 00:20:14.600]   but eventually, we want to make a conf net better
[00:20:14.600 --> 00:20:17.600]   to make things move better.
[00:20:17.600 --> 00:20:19.600]   So that was, like, investigation.
[00:20:19.600 --> 00:20:21.600]   When you find a problem, it's sometimes good enough.
[00:20:21.600 --> 00:20:23.600]   You can publish a paper already saying that,
[00:20:23.600 --> 00:20:25.600]   "I have this problem. Who can solve it?"
[00:20:25.600 --> 00:20:28.600]   But lucky for us, we also found a solution.
[00:20:28.600 --> 00:20:30.600]   So we called it "quad-conf"
[00:20:30.600 --> 00:20:34.600]   because a convolution works this way, as most of you know.
[00:20:34.600 --> 00:20:38.600]   It takes input tensor, outputs an output tensor
[00:20:38.600 --> 00:20:40.600]   through some sharing of filters.
[00:20:40.600 --> 00:20:43.600]   So quad-conf works very similarly.
[00:20:43.600 --> 00:20:46.600]   You just add two channels of coordinates to the input tensor,
[00:20:46.600 --> 00:20:48.600]   doing nothing, running the same conf--
[00:20:48.600 --> 00:20:51.600]   or deconf if you're painting pixels.
[00:20:51.600 --> 00:20:54.600]   So it is so simple so that it keeps all the good things
[00:20:54.600 --> 00:20:56.600]   that convolution has.
[00:20:56.600 --> 00:20:59.600]   So convolution was good for so many reasons.
[00:20:59.600 --> 00:21:02.600]   It has few parameters because of the large weight sharing.
[00:21:02.600 --> 00:21:05.600]   It runs fast, and it has translation equivariant,
[00:21:05.600 --> 00:21:08.600]   which a lot of cases are needed, like image classification.
[00:21:08.600 --> 00:21:12.600]   So quad-conf keeps all these good properties.
[00:21:12.600 --> 00:21:14.600]   And as to translation equivariance,
[00:21:14.600 --> 00:21:18.600]   it can learn to either keep it or forget about it,
[00:21:18.600 --> 00:21:20.600]   become translation-dependent by, you know,
[00:21:20.600 --> 00:21:24.600]   assigning weights on these two channels or not.
[00:21:24.600 --> 00:21:28.600]   So you wrap it into a layer, and it's easy to use.
[00:21:28.600 --> 00:21:31.600]   So with any network, like this one, AlexNet,
[00:21:31.600 --> 00:21:34.600]   you can just swap out any conf into quad-conf.
[00:21:34.600 --> 00:21:37.600]   One of them, three of them, all of them, or none of them,
[00:21:37.600 --> 00:21:38.600]   as much as you want.
[00:21:38.600 --> 00:21:42.600]   So it's really easy to experiment with your own models.
[00:21:42.600 --> 00:21:45.600]   So it turns out to fix this problem really easily.
[00:21:45.600 --> 00:21:50.600]   Not only that it generates 100% accuracy on test set all the time,
[00:21:50.600 --> 00:21:52.600]   but, you know, the logic-- even the logic part
[00:21:52.600 --> 00:21:56.600]   is, like, nicely and smoothly laid out.
[00:21:56.600 --> 00:21:59.600]   Solved the regression problem perfectly
[00:21:59.600 --> 00:22:01.600]   because now it knows the coordination--
[00:22:01.600 --> 00:22:04.600]   it can--it knows where to paint.
[00:22:04.600 --> 00:22:08.600]   And so we solved the problem, but then people are suspicious,
[00:22:08.600 --> 00:22:10.600]   that's only a toy problem.
[00:22:10.600 --> 00:22:12.600]   Why do we care about toy problem in research?
[00:22:12.600 --> 00:22:14.600]   We care about real-world problems
[00:22:14.600 --> 00:22:16.600]   or real-world in the research domain.
[00:22:16.600 --> 00:22:17.600]   That is, all these problems.
[00:22:17.600 --> 00:22:19.600]   Like recognizing a dog, I care about.
[00:22:19.600 --> 00:22:21.600]   I care about drawing bounding boxes,
[00:22:21.600 --> 00:22:24.600]   care about generative models, and maybe reinforcement learning.
[00:22:24.600 --> 00:22:26.600]   So because it was so easy to use everywhere, use conf,
[00:22:26.600 --> 00:22:28.600]   we can just swap in quad-conf.
[00:22:28.600 --> 00:22:29.600]   So we just tried all those.
[00:22:29.600 --> 00:22:33.600]   As expected, it doesn't improve much on image classification
[00:22:33.600 --> 00:22:36.600]   because that is where you exactly don't need to know
[00:22:36.600 --> 00:22:37.600]   the location of a dog.
[00:22:37.600 --> 00:22:39.600]   If a dog is on this corner of an image,
[00:22:39.600 --> 00:22:41.600]   that corner of an image is the same--it's a dog.
[00:22:41.600 --> 00:22:44.600]   So it doesn't improve image classification,
[00:22:44.600 --> 00:22:46.600]   but the good news is it doesn't make it worse.
[00:22:46.600 --> 00:22:48.600]   It's the same, so nothing happens there.
[00:22:48.600 --> 00:22:52.600]   Object detection is where you really need to know the location,
[00:22:52.600 --> 00:22:55.600]   so it actually helps a lot in our little test.
[00:22:55.600 --> 00:22:58.600]   So, like, maybe a tree is always in this corner.
[00:22:58.600 --> 00:23:01.600]   It's never on the sky, so it helps you detect the location
[00:23:01.600 --> 00:23:04.600]   and the category of a tree much better
[00:23:04.600 --> 00:23:07.600]   when you're drawing bounding boxes with quad-conf.
[00:23:07.600 --> 00:23:11.600]   In generative models, it helps with objects moving.
[00:23:11.600 --> 00:23:14.600]   It finds a better latent space where the location is encoded,
[00:23:14.600 --> 00:23:18.600]   and in reinforcement learning, it helps agents get better scores.
[00:23:18.600 --> 00:23:21.600]   So some examples of generative models, like this one,
[00:23:21.600 --> 00:23:25.600]   we're trying to just generate objects with different colors.
[00:23:25.600 --> 00:23:29.600]   So you can train a GAN with both conf and quad-conf easily,
[00:23:29.600 --> 00:23:32.600]   and when you sample, the sample seems fine.
[00:23:32.600 --> 00:23:36.600]   They all seem to do the job--two objects, one red and one blue.
[00:23:36.600 --> 00:23:39.600]   But the real difference happens when you try to interpolate them,
[00:23:39.600 --> 00:23:43.600]   because that's something you can do in GANs or generative models.
[00:23:43.600 --> 00:23:47.600]   So you see that with convolution, there's so many artifacts.
[00:23:47.600 --> 00:23:51.600]   Like, the pixels just appear and disappear.
[00:23:51.600 --> 00:23:53.600]   The objects never move.
[00:23:53.600 --> 00:23:57.600]   You never learn the object, just learn to paint single pixels.
[00:23:57.600 --> 00:24:00.600]   But with quad-conf, things are moving around more smoothly
[00:24:00.600 --> 00:24:03.600]   because it now has a better latent representation
[00:24:03.600 --> 00:24:07.600]   of the object's location.
[00:24:07.600 --> 00:24:10.600]   You can scale it up to paint bigger things.
[00:24:10.600 --> 00:24:12.600]   Back then, people cared about El Son.
[00:24:12.600 --> 00:24:14.600]   I guess now not anymore.
[00:24:14.600 --> 00:24:19.600]   But with conf, you can see that things fade in and fade out.
[00:24:19.600 --> 00:24:21.600]   Things just appear and disappear.
[00:24:21.600 --> 00:24:24.600]   With quad-conf, you see more moving around.
[00:24:24.600 --> 00:24:28.600]   It has learned this geometric transformation.
[00:24:28.600 --> 00:24:30.600]   You can use it to play games.
[00:24:30.600 --> 00:24:33.600]   So Ms. Pac-Man really helps a lot with quad-conf
[00:24:33.600 --> 00:24:37.600]   because you probably need to know both where Pac-Man is
[00:24:37.600 --> 00:24:39.600]   and what is Pac-Man.
[00:24:39.600 --> 00:24:41.600]   It kind of makes sense.
[00:24:41.600 --> 00:24:43.600]   So the red is quad-conf. It makes it better.
[00:24:43.600 --> 00:24:46.600]   Another game helps it learn faster.
[00:24:46.600 --> 00:24:48.600]   Another game, there's no difference.
[00:24:48.600 --> 00:24:50.600]   Maybe the location doesn't matter there.
[00:24:50.600 --> 00:24:52.600]   So we tried lots of games.
[00:24:52.600 --> 00:24:54.600]   Some helped. Six of them helped.
[00:24:54.600 --> 00:24:57.600]   Two of them were similar, and one of them was worse.
[00:24:57.600 --> 00:25:00.600]   So the conclusion of this part of the talk, quad-conf,
[00:25:00.600 --> 00:25:04.600]   is that we find this curious inability of convolution,
[00:25:04.600 --> 00:25:06.600]   and we solved it with quad-conf.
[00:25:06.600 --> 00:25:08.600]   It helped solve this toy domain
[00:25:08.600 --> 00:25:12.600]   and helped performance boost in a range of other applications.
[00:25:12.600 --> 00:25:16.600]   And that was the paper published last year.
[00:25:16.600 --> 00:25:19.600]   Cool. I have time to get to part three.
[00:25:19.600 --> 00:25:21.600]   I thought I didn't.
[00:25:21.600 --> 00:25:24.600]   So part three, we are, like, taking a step back
[00:25:24.600 --> 00:25:26.600]   and now looking at training again.
[00:25:26.600 --> 00:25:28.600]   So not a specific neural network,
[00:25:28.600 --> 00:25:30.600]   but just the training of a neural network.
[00:25:30.600 --> 00:25:32.600]   So when you think about how you train a neural network,
[00:25:32.600 --> 00:25:34.600]   what you do is actually you have a neural network.
[00:25:34.600 --> 00:25:37.600]   You have all the ways. You take all the ways out.
[00:25:37.600 --> 00:25:41.600]   And if you imagine a space in front of you of all the ways,
[00:25:41.600 --> 00:25:43.600]   a super high-dimensional space,
[00:25:43.600 --> 00:25:46.600]   what training really is is actually starting from somewhere
[00:25:46.600 --> 00:25:49.600]   with higher loss and then just traversing the space
[00:25:49.600 --> 00:25:52.600]   and end up somewhere, hopefully, with a lower loss.
[00:25:52.600 --> 00:25:53.600]   That's all training is.
[00:25:53.600 --> 00:25:55.600]   You're moving the ways, each of them,
[00:25:55.600 --> 00:25:57.600]   so that the loss is lower.
[00:25:57.600 --> 00:25:59.600]   So we're really thinking,
[00:25:59.600 --> 00:26:01.600]   "Do you have to train that way?
[00:26:01.600 --> 00:26:05.600]   And what if you make it more restrictive?"
[00:26:05.600 --> 00:26:08.600]   So we propose this random subspace training
[00:26:08.600 --> 00:26:10.600]   where you start the same place.
[00:26:10.600 --> 00:26:12.600]   You are still moving the ways,
[00:26:12.600 --> 00:26:15.600]   but you're restricted to move the ways in a subspace.
[00:26:15.600 --> 00:26:18.600]   That is, having fewer dimensions of the original space.
[00:26:18.600 --> 00:26:21.600]   So if the original space is big D
[00:26:21.600 --> 00:26:24.600]   and the subspace will be small d,
[00:26:24.600 --> 00:26:27.600]   the ways are only allowed to move in in this space.
[00:26:27.600 --> 00:26:30.600]   And this small space is initialized randomly.
[00:26:30.600 --> 00:26:33.600]   You can do it in any way you want.
[00:26:33.600 --> 00:26:35.600]   And you still optimize it the same way.
[00:26:35.600 --> 00:26:41.600]   Now, instead of optimizing the original space,
[00:26:41.600 --> 00:26:43.600]   you optimize in this space,
[00:26:43.600 --> 00:26:45.600]   and then you use a random projection
[00:26:45.600 --> 00:26:47.600]   to map it back to the original space,
[00:26:47.600 --> 00:26:48.600]   because you need the original space
[00:26:48.600 --> 00:26:51.600]   and all the ways to work in your network.
[00:26:51.600 --> 00:26:54.600]   And this is initialized randomly and then frozen,
[00:26:54.600 --> 00:26:56.600]   never really trained.
[00:26:56.600 --> 00:26:59.600]   What do we train is this thing.
[00:26:59.600 --> 00:27:04.600]   So in this, you can develop a metric telling you
[00:27:04.600 --> 00:27:09.600]   how large must this little d be to solve the problem.
[00:27:09.600 --> 00:27:13.600]   So if you think about how well it works,
[00:27:13.600 --> 00:27:14.600]   it actually just happens,
[00:27:14.600 --> 00:27:18.600]   depends on the dimension, small d, subspace dimension.
[00:27:18.600 --> 00:27:20.600]   Because on one end, if you have just a dimension of one,
[00:27:20.600 --> 00:27:22.600]   you basically just have one line
[00:27:22.600 --> 00:27:25.600]   and trying to find a solution by just following one line.
[00:27:25.600 --> 00:27:28.600]   It's very, very rare that you are going to find a solution
[00:27:28.600 --> 00:27:29.600]   in this one dimension.
[00:27:29.600 --> 00:27:32.600]   But if you're allowed to use a dimension of big D,
[00:27:32.600 --> 00:27:33.600]   which is the original dimension,
[00:27:33.600 --> 00:27:35.600]   it's almost guaranteed that you will find a solution
[00:27:35.600 --> 00:27:37.600]   because you found a solution already
[00:27:37.600 --> 00:27:39.600]   with the original network.
[00:27:39.600 --> 00:27:43.600]   So there must be a place when this transition happened
[00:27:43.600 --> 00:27:47.600]   that a network with some dimension starts to work suddenly.
[00:27:47.600 --> 00:27:48.600]   So we can call this dimension
[00:27:48.600 --> 00:27:53.600]   the intrinsic dimension of the network plus the problem.
[00:27:53.600 --> 00:27:54.600]   Yeah.
[00:27:54.600 --> 00:27:56.600]   - What's the reason why I would,
[00:27:56.600 --> 00:27:58.600]   I guess that this would be the shape,
[00:27:58.600 --> 00:27:59.600]   why wouldn't it just be linear?
[00:27:59.600 --> 00:28:01.600]   - Yeah, it could be linear.
[00:28:01.600 --> 00:28:02.600]   We are supposing it's this shape,
[00:28:02.600 --> 00:28:05.600]   and I think in experiment, it's mostly this shape.
[00:28:05.600 --> 00:28:09.600]   - Is there a reason or intuitive reasoning
[00:28:09.600 --> 00:28:11.600]   behind that this should be linear?
[00:28:11.600 --> 00:28:14.600]   - No, we don't think it will be linear.
[00:28:14.600 --> 00:28:16.600]   In experiments, it's something like this.
[00:28:16.600 --> 00:28:18.600]   We think of something like this.
[00:28:18.600 --> 00:28:20.600]   There's no perfectness in there.
[00:28:20.600 --> 00:28:22.600]   And each problem is different.
[00:28:22.600 --> 00:28:23.600]   Yeah, this is just some drawing
[00:28:23.600 --> 00:28:27.600]   that we think would be the concept behind it.
[00:28:27.600 --> 00:28:29.600]   So bearing this thought, right,
[00:28:29.600 --> 00:28:31.600]   so we're just trying to find this line,
[00:28:31.600 --> 00:28:32.600]   where the switch happens
[00:28:32.600 --> 00:28:34.600]   when network just suddenly starts to work,
[00:28:34.600 --> 00:28:37.600]   which they have a higher enough dimension.
[00:28:37.600 --> 00:28:39.600]   And then we can try it on all kinds of models
[00:28:39.600 --> 00:28:41.600]   and all kinds of networks that we have.
[00:28:41.600 --> 00:28:44.600]   Say you have an MNIST, people like MNIST these days.
[00:28:44.600 --> 00:28:47.600]   And if you have an FFC network that's of this size,
[00:28:47.600 --> 00:28:50.600]   say 784, 200, 210,
[00:28:50.600 --> 00:28:54.600]   so in this case, this big D is about 200,000.
[00:28:54.600 --> 00:28:56.600]   That's how many parameters you have in this network.
[00:28:56.600 --> 00:28:58.600]   So guess what sub-dimensional
[00:28:58.600 --> 00:29:01.600]   or intrinsic dimension of this problem is.
[00:29:01.600 --> 00:29:02.600]   It's actually 750.
[00:29:02.600 --> 00:29:07.600]   It's so small that it's only 0.4% of the original space.
[00:29:07.600 --> 00:29:10.600]   So with a dimension of, sub-dimension of 750,
[00:29:10.600 --> 00:29:12.600]   you can already solve this problem pretty well.
[00:29:12.600 --> 00:29:15.600]   And what's interesting is 750 is even less
[00:29:15.600 --> 00:29:18.600]   than the input space, which makes sense for MNIST
[00:29:18.600 --> 00:29:20.600]   because a lot of pixels are just dead,
[00:29:20.600 --> 00:29:23.600]   are just always black.
[00:29:23.600 --> 00:29:24.600]   And, yeah.
[00:29:24.600 --> 00:29:26.600]   - When you say the intrinsic dimension
[00:29:26.600 --> 00:29:27.600]   in terms of GW,
[00:29:27.600 --> 00:29:30.600]   does it depend on the projection that you are doing?
[00:29:30.600 --> 00:29:32.600]   - It does depend on the projection,
[00:29:32.600 --> 00:29:34.600]   but if you run it 10 times with 10 projections,
[00:29:34.600 --> 00:29:36.600]   it always gives you that number.
[00:29:36.600 --> 00:29:38.600]   So we make sure that it's robust.
[00:29:38.600 --> 00:29:40.600]   It definitely depends on the projection,
[00:29:40.600 --> 00:29:44.600]   but we make sure it's robust against the dimension.
[00:29:44.600 --> 00:29:45.600]   Yes.
[00:29:45.600 --> 00:29:46.600]   - I have a question.
[00:29:46.600 --> 00:29:48.600]   What would be an example of a dimension
[00:29:48.600 --> 00:29:52.600]   just to get more practical?
[00:29:52.600 --> 00:29:55.600]   What would be an example of a dimension, for example?
[00:29:55.600 --> 00:29:58.600]   - So you can think of a dimension as,
[00:29:58.600 --> 00:30:01.600]   so all the search space in your network.
[00:30:01.600 --> 00:30:03.600]   So the network have 200,000 parameters.
[00:30:03.600 --> 00:30:05.600]   That's like all the things you can move,
[00:30:05.600 --> 00:30:06.600]   all the control variables,
[00:30:06.600 --> 00:30:09.600]   or degrees of freedom, as you may.
[00:30:09.600 --> 00:30:11.600]   And this number we find is,
[00:30:11.600 --> 00:30:15.600]   if we restrict it to search only in the small dimension,
[00:30:15.600 --> 00:30:16.600]   so you have a--
[00:30:16.600 --> 00:30:18.600]   - So you can only move 750,
[00:30:18.600 --> 00:30:21.600]   update 750 neurons in the whole network?
[00:30:21.600 --> 00:30:25.600]   - No, you're only allowed to move in a direction
[00:30:25.600 --> 00:30:27.600]   that's of 750 dimensions.
[00:30:27.600 --> 00:30:29.600]   It's harder to think in a high-dimensional space,
[00:30:29.600 --> 00:30:32.600]   but if you think in a three-dimensional space,
[00:30:32.600 --> 00:30:33.600]   you have a 3D world,
[00:30:33.600 --> 00:30:36.600]   but I restrict you to only move in a 2D plane.
[00:30:36.600 --> 00:30:39.600]   You can only move here and try to search for a solution.
[00:30:39.600 --> 00:30:41.600]   That's probably the thing.
[00:30:41.600 --> 00:30:45.600]   - How would you actually apply, impose that restriction?
[00:30:45.600 --> 00:30:48.600]   - So you have a small vector that's 750.
[00:30:48.600 --> 00:30:49.600]   You just search it here.
[00:30:49.600 --> 00:30:50.600]   Whenever you have a solution,
[00:30:50.600 --> 00:30:52.600]   you map it back to the big one
[00:30:52.600 --> 00:30:55.600]   through a random matrix.
[00:30:55.600 --> 00:30:58.600]   That one will become your weight matrix.
[00:30:58.600 --> 00:31:01.600]   You run the forward the same way.
[00:31:01.600 --> 00:31:04.600]   You run the backward, and this has gradient.
[00:31:04.600 --> 00:31:07.600]   So this matrix and this small thing would have gradient.
[00:31:07.600 --> 00:31:10.600]   So you update the small thing only.
[00:31:10.600 --> 00:31:12.600]   That make sense?
[00:31:12.600 --> 00:31:16.600]   - So you're imposing linear constraints between the--
[00:31:16.600 --> 00:31:17.600]   - Yeah, yeah.
[00:31:17.600 --> 00:31:19.600]   It's a linear projection.
[00:31:19.600 --> 00:31:21.600]   True.
[00:31:21.600 --> 00:31:24.600]   So you have 750 for MNIST plus FC network,
[00:31:24.600 --> 00:31:26.600]   for this particular FC network.
[00:31:26.600 --> 00:31:30.600]   And interesting, if you make it slightly wider,
[00:31:30.600 --> 00:31:33.600]   like 225 instead of 200,
[00:31:33.600 --> 00:31:35.600]   number is still 750.
[00:31:35.600 --> 00:31:36.600]   So this number drops a little bit,
[00:31:36.600 --> 00:31:38.600]   but it's still 750.
[00:31:38.600 --> 00:31:40.600]   If you make it much wider and much deeper,
[00:31:40.600 --> 00:31:44.600]   try a lot of things, and this number is still 750.
[00:31:44.600 --> 00:31:46.600]   So what this tells you is that this measure,
[00:31:46.600 --> 00:31:48.600]   intrinsic dimension, is fairly stable
[00:31:48.600 --> 00:31:53.600]   for this problem with this family of model, the FC model.
[00:31:53.600 --> 00:31:55.600]   And something else it tells you is that
[00:31:55.600 --> 00:31:57.600]   people know that with deeper, larger neural networks,
[00:31:57.600 --> 00:32:01.600]   it's easier to train, and maybe this explains why.
[00:32:01.600 --> 00:32:03.600]   Because the intrinsic dimension is the same,
[00:32:03.600 --> 00:32:05.600]   so when you make the network bigger,
[00:32:05.600 --> 00:32:08.600]   you're basically just having more solution space.
[00:32:08.600 --> 00:32:10.600]   Because the solution space, if you think about it,
[00:32:10.600 --> 00:32:13.600]   is the original space minus the dimension
[00:32:13.600 --> 00:32:16.600]   that you can search.
[00:32:16.600 --> 00:32:17.600]   Yeah.
[00:32:17.600 --> 00:32:20.600]   - Can you try to combine this with the first problem
[00:32:20.600 --> 00:32:21.600]   you described?
[00:32:21.600 --> 00:32:25.600]   Like, does the interest in dimension of the problem
[00:32:25.600 --> 00:32:29.600]   has anything to do with the negative contributing--
[00:32:29.600 --> 00:32:30.600]   - Yeah, it could be.
[00:32:30.600 --> 00:32:33.600]   This work is much earlier, a year earlier,
[00:32:33.600 --> 00:32:36.600]   but that one was new, but yeah, that's a good point.
[00:32:36.600 --> 00:32:40.600]   You can see that whether a network is more
[00:32:40.600 --> 00:32:42.600]   or less intrinsic dimension
[00:32:42.600 --> 00:32:44.600]   would have less parameters hurting.
[00:32:44.600 --> 00:32:48.600]   That could definitely be a valid direction.
[00:32:48.600 --> 00:32:51.600]   So, but this is just a small network.
[00:32:51.600 --> 00:32:55.600]   Let's just try more, 'cause we can try more.
[00:32:55.600 --> 00:32:59.600]   Okay, so we know that MNIST plus FC is 750.
[00:32:59.600 --> 00:33:02.600]   What about a ConvNet?
[00:33:02.600 --> 00:33:04.600]   Okay, cool.
[00:33:04.600 --> 00:33:06.600]   What about ConvNet?
[00:33:06.600 --> 00:33:09.600]   I revealed the answer too soon, but it's lower
[00:33:09.600 --> 00:33:13.600]   because ConvNet is a better network to solve this problem.
[00:33:13.600 --> 00:33:16.600]   So a lower number would be, it's easier to search
[00:33:16.600 --> 00:33:18.600]   for an answer in this network.
[00:33:18.600 --> 00:33:22.600]   So that just suggests that ConvNet in this situation
[00:33:22.600 --> 00:33:24.600]   is a better network for MNIST.
[00:33:24.600 --> 00:33:26.600]   What if we shuffle pixels?
[00:33:26.600 --> 00:33:30.600]   People sometimes do this to study generalization
[00:33:30.600 --> 00:33:31.600]   and overfitting.
[00:33:31.600 --> 00:33:34.600]   If you shuffle the pixels, it doesn't change
[00:33:34.600 --> 00:33:37.600]   for FC network because the order doesn't matter
[00:33:37.600 --> 00:33:39.600]   for the FC network.
[00:33:39.600 --> 00:33:42.600]   So it's still 750, but if you run the same ConvNet,
[00:33:42.600 --> 00:33:44.600]   guess what, it's much, much larger.
[00:33:44.600 --> 00:33:48.600]   So ConvNet is a better model when your input data
[00:33:48.600 --> 00:33:50.600]   has structure, spatial structure.
[00:33:50.600 --> 00:33:51.600]   When you break the spatial structure,
[00:33:51.600 --> 00:33:53.600]   ConvNet is a much, much worse model.
[00:33:53.600 --> 00:33:54.600]   I guess people already know that,
[00:33:54.600 --> 00:33:56.600]   but this number just vividly tells you that
[00:33:56.600 --> 00:33:59.600]   and how much worse it will be.
[00:33:59.600 --> 00:34:03.600]   So this shear difference tells you that.
[00:34:03.600 --> 00:34:06.600]   But enough of MNIST, people care about larger problems, right?
[00:34:06.600 --> 00:34:08.600]   So we measure CIFAR.
[00:34:08.600 --> 00:34:12.600]   CIFAR with an FC is about 9,000.
[00:34:12.600 --> 00:34:16.600]   CIFAR with a ConvNet is about 2,900.
[00:34:16.600 --> 00:34:18.600]   So that's interesting when you compare
[00:34:18.600 --> 00:34:22.600]   this 290 versus 2.9K.
[00:34:22.600 --> 00:34:24.600]   So now you can tell people that CIFAR is harder
[00:34:24.600 --> 00:34:25.600]   than MNIST, but how much harder?
[00:34:25.600 --> 00:34:27.600]   Maybe 10 times harder.
[00:34:27.600 --> 00:34:29.600]   It's like a number that you can directly use.
[00:34:29.600 --> 00:34:30.600]   You can do ImageNet.
[00:34:30.600 --> 00:34:33.600]   It's much harder to scale, so we use a SqueezeNet,
[00:34:33.600 --> 00:34:36.600]   and at that time, before the conference deadline,
[00:34:36.600 --> 00:34:38.600]   we didn't really fully train it,
[00:34:38.600 --> 00:34:41.600]   but we know it's a number that's larger than 500K
[00:34:41.600 --> 00:34:43.600]   for this network, at least.
[00:34:43.600 --> 00:34:45.600]   And one particularly interesting thing to do is
[00:34:45.600 --> 00:34:48.600]   you can use it anywhere that you use a network.
[00:34:48.600 --> 00:34:50.600]   So you can really use it on RL,
[00:34:50.600 --> 00:34:52.600]   and you can come up with numbers now
[00:34:52.600 --> 00:34:54.600]   that you can compare with supervised learning,
[00:34:54.600 --> 00:34:56.600]   which is never done before.
[00:34:56.600 --> 00:34:59.600]   So now you can really say that, well,
[00:34:59.600 --> 00:35:03.600]   having a humanoid walk in RL is about the same difficulty
[00:35:03.600 --> 00:35:07.600]   as training FC on MNIST, which people can never make
[00:35:07.600 --> 00:35:09.600]   the analogy before this tool.
[00:35:09.600 --> 00:35:13.600]   And playing upon is about as hard as training
[00:35:13.600 --> 00:35:15.600]   a CIFAR-10 on FC network, something like that.
[00:35:15.600 --> 00:35:19.600]   It's roughly, but of course, now you can say that.
[00:35:19.600 --> 00:35:23.600]   And know that this problem has only four,
[00:35:23.600 --> 00:35:25.600]   so this is a super easy problem.
[00:35:25.600 --> 00:35:28.600]   Before, if you ask which problem is easy or hard
[00:35:28.600 --> 00:35:30.600]   in RL versus supervised learning,
[00:35:30.600 --> 00:35:32.600]   you have to have someone that works in both fields
[00:35:32.600 --> 00:35:35.600]   and have played with them really heavily to tell you that.
[00:35:35.600 --> 00:35:38.600]   And people usually work on one field or another.
[00:35:38.600 --> 00:35:40.600]   Rarely you see people working on both fields
[00:35:40.600 --> 00:35:42.600]   to really make this comparison.
[00:35:42.600 --> 00:35:44.600]   So they may have this comparison in the back of their head,
[00:35:44.600 --> 00:35:46.600]   but it's very hard to really say that.
[00:35:46.600 --> 00:35:50.600]   But now you can say, you know, they're this much comparable,
[00:35:50.600 --> 00:35:52.600]   and this problem is actually super easy
[00:35:52.600 --> 00:35:55.600]   when you play with RL.
[00:35:55.600 --> 00:35:58.600]   So, yeah, interesting.
[00:35:58.600 --> 00:36:00.600]   I have some free time.
[00:36:00.600 --> 00:36:02.600]   Cool, so, yeah.
[00:36:02.600 --> 00:36:03.600]   Questions?
[00:36:03.600 --> 00:36:06.600]   - Does that change when you change
[00:36:06.600 --> 00:36:08.600]   the random initialization?
[00:36:08.600 --> 00:36:09.600]   - Yeah, no.
[00:36:09.600 --> 00:36:11.600]   So that, we make sure that, well,
[00:36:11.600 --> 00:36:14.600]   you change a little bit, but it must be within some bound.
[00:36:14.600 --> 00:36:17.600]   So every number there is tried with at least 10, 20 times,
[00:36:17.600 --> 00:36:20.600]   and every time the random projection is different,
[00:36:20.600 --> 00:36:22.600]   is a random seed.
[00:36:22.600 --> 00:36:24.600]   So to summarize what I've done.
[00:36:24.600 --> 00:36:26.600]   - Do you want to find out
[00:36:26.600 --> 00:36:29.600]   the theta signature of our problem?
[00:36:29.600 --> 00:36:30.600]   - I'm sorry?
[00:36:30.600 --> 00:36:32.600]   - Is there a tool or method to find out
[00:36:32.600 --> 00:36:35.600]   the intrinsic dimension for, let's say, these networks?
[00:36:35.600 --> 00:36:37.600]   Like, say, any network that we come up with,
[00:36:37.600 --> 00:36:38.600]   can we find out?
[00:36:38.600 --> 00:36:39.600]   - Yeah.
[00:36:39.600 --> 00:36:43.600]   Yeah, so here, you can just use this tool to find out.
[00:36:43.600 --> 00:36:45.600]   It's a little bit expensive,
[00:36:45.600 --> 00:36:47.600]   'cause really you need to try all these dimensions
[00:36:47.600 --> 00:36:49.600]   until you find the switching point
[00:36:49.600 --> 00:36:51.600]   that network starts working, right?
[00:36:51.600 --> 00:36:54.600]   There's not a direct way to find out it yet.
[00:36:54.600 --> 00:36:56.600]   'Cause there are some investigations we've done.
[00:36:56.600 --> 00:36:58.600]   But, yeah, not a direct way.
[00:36:58.600 --> 00:36:59.600]   You can just measure it once.
[00:36:59.600 --> 00:37:01.600]   And it's like, you have to train lots of them.
[00:37:01.600 --> 00:37:03.600]   But this paper is more of a proof
[00:37:03.600 --> 00:37:06.600]   that there exists such a thing.
[00:37:06.600 --> 00:37:09.600]   So that summarizes my piece of work
[00:37:09.600 --> 00:37:11.600]   of staring at something really hard.
[00:37:11.600 --> 00:37:13.600]   And of course, I have to thank my collaborators.
[00:37:13.600 --> 00:37:16.600]   Every work, as Peter mentioned, is a teamwork in research.
[00:37:16.600 --> 00:37:20.600]   Nothing is, like, one person group cannot do much.
[00:37:20.600 --> 00:37:24.600]   And for the published work,
[00:37:24.600 --> 00:37:27.600]   we have written blog posts and filmed videos,
[00:37:27.600 --> 00:37:30.600]   which does a better job than me presenting here.
[00:37:30.600 --> 00:37:32.600]   And this one's still in submission.
[00:37:32.600 --> 00:37:33.600]   It's pretty new.
[00:37:33.600 --> 00:37:36.600]   So back to the slides I showed before.
[00:37:36.600 --> 00:37:37.600]   I think what I learned here
[00:37:37.600 --> 00:37:40.600]   is that deep learning can be both easy and hard.
[00:37:40.600 --> 00:37:44.600]   Because some of you might have that intuition already.
[00:37:44.600 --> 00:37:46.600]   Like, people, on one side of the story,
[00:37:46.600 --> 00:37:47.600]   people are claiming deep learning
[00:37:47.600 --> 00:37:49.600]   is doing these amazing things,
[00:37:49.600 --> 00:37:52.600]   solving goals, solving stock credit and everything.
[00:37:52.600 --> 00:37:53.600]   And on the other side,
[00:37:53.600 --> 00:37:55.600]   when you try to apply it to some real-world problem,
[00:37:55.600 --> 00:37:58.600]   it's just, like, really hard to get it working.
[00:37:58.600 --> 00:38:01.600]   Somehow, if it's not an exact image of that problem,
[00:38:01.600 --> 00:38:02.600]   it's just really hard to work.
[00:38:02.600 --> 00:38:04.600]   Although there are so many tools out there,
[00:38:04.600 --> 00:38:06.600]   so many papers published every day,
[00:38:06.600 --> 00:38:09.600]   it's just still really hard to manage.
[00:38:09.600 --> 00:38:10.600]   So I feel like this is true.
[00:38:10.600 --> 00:38:12.600]   So the first sentence is just saying
[00:38:12.600 --> 00:38:13.600]   that it's definitely true.
[00:38:13.600 --> 00:38:15.600]   And for me, I think it's just because
[00:38:15.600 --> 00:38:18.600]   people lack a really good understanding
[00:38:18.600 --> 00:38:19.600]   of a lot of things,
[00:38:19.600 --> 00:38:21.600]   even things that are working.
[00:38:21.600 --> 00:38:23.600]   Like, even for two of the projects,
[00:38:23.600 --> 00:38:25.600]   things are just working there, right?
[00:38:25.600 --> 00:38:26.600]   Things are just training.
[00:38:26.600 --> 00:38:29.600]   But we're trying to understand why,
[00:38:29.600 --> 00:38:30.600]   even if it's working,
[00:38:30.600 --> 00:38:32.600]   can we understand more of why it is working
[00:38:32.600 --> 00:38:33.600]   or where it is working?
[00:38:33.600 --> 00:38:35.600]   And for the things that don't work,
[00:38:35.600 --> 00:38:36.600]   of course, you want to understand
[00:38:36.600 --> 00:38:37.600]   why it's not working.
[00:38:37.600 --> 00:38:39.600]   I feel like a lot of things
[00:38:39.600 --> 00:38:41.600]   that are not happening in industries
[00:38:41.600 --> 00:38:43.600]   that people might be lacking in a researcher's mind,
[00:38:43.600 --> 00:38:45.600]   that is, things that work,
[00:38:45.600 --> 00:38:46.600]   we just quickly deliver it.
[00:38:46.600 --> 00:38:48.600]   Things that don't work, we quickly abandon it.
[00:38:48.600 --> 00:38:51.600]   But maybe, like, I know people there
[00:38:51.600 --> 00:38:53.600]   have less of a luxury than, you know,
[00:38:53.600 --> 00:38:54.600]   really in a researcher lab,
[00:38:54.600 --> 00:38:57.600]   you're allowed the luxury to study a problem
[00:38:57.600 --> 00:38:59.600]   with more time and more resource.
[00:38:59.600 --> 00:39:02.600]   But I'm just saying that I think
[00:39:02.600 --> 00:39:04.600]   that might be the little solution there
[00:39:04.600 --> 00:39:07.600]   that people just have to be treating problems,
[00:39:07.600 --> 00:39:08.600]   whether they're working or not,
[00:39:08.600 --> 00:39:09.600]   working with more patients.
[00:39:09.600 --> 00:39:11.600]   And that might be, you know, something,
[00:39:11.600 --> 00:39:14.600]   a good recipe for you guys to take home.
[00:39:14.600 --> 00:39:15.600]   Cool.
[00:39:15.600 --> 00:39:17.600]   And that ends my talk.
[00:39:17.600 --> 00:39:20.600]   And if you have time for questions, do we?
[00:39:20.600 --> 00:39:23.600]   [applause]
[00:39:23.600 --> 00:39:28.600]   Yeah?
[00:39:28.600 --> 00:39:30.600]   - How do you see the intrinsic dimension thing
[00:39:30.600 --> 00:39:33.600]   relating to the luxury ticket analysis?
[00:39:33.600 --> 00:39:34.600]   - Yeah, definitely.
[00:39:34.600 --> 00:39:37.600]   There's definitely some linkage there.
[00:39:37.600 --> 00:39:39.600]   Something that's small maybe is harder.
[00:39:39.600 --> 00:39:41.600]   With a small intrinsic dimension,
[00:39:41.600 --> 00:39:44.600]   it might be easier to find luxury tickets.
[00:39:44.600 --> 00:39:46.600]   There might be more luxury tickets around.
[00:39:46.600 --> 00:39:48.600]   There still lacks a way to find
[00:39:48.600 --> 00:39:51.600]   how many luxury tickets are there in a network.
[00:39:51.600 --> 00:39:53.600]   So that paper says luxury tickets exist.
[00:39:53.600 --> 00:39:55.600]   But if you can, like, count that number,
[00:39:55.600 --> 00:39:58.600]   I believe that number has to do with intrinsic dimension.
[00:39:58.600 --> 00:39:59.600]   In a reverse way, though.
[00:39:59.600 --> 00:40:02.600]   Smaller intrinsic dimension, more luxury tickets.
[00:40:02.600 --> 00:40:05.600]   Kind of what I believe.
[00:40:05.600 --> 00:40:12.600]   - Yeah.
[00:40:12.600 --> 00:40:16.600]   So you're comparing two complexities,
[00:40:16.600 --> 00:40:19.600]   290 versus 2.9k.
[00:40:19.600 --> 00:40:23.600]   Do you think it is, like, you know, I guess, linear?
[00:40:23.600 --> 00:40:28.600]   Or is it, like, do you think you can compare it that way?
[00:40:28.600 --> 00:40:29.600]   - I don't know.
[00:40:29.600 --> 00:40:32.600]   It's a rough measure that people never studied before.
[00:40:32.600 --> 00:40:35.600]   I think it's just fun to say it this way.
[00:40:35.600 --> 00:40:38.600]   It's not super, super scientifically sound.
[00:40:38.600 --> 00:40:40.600]   But when you say some problem is harder,
[00:40:40.600 --> 00:40:41.600]   how many times harder,
[00:40:41.600 --> 00:40:44.600]   it's like already a vague thing that you're putting out there.
[00:40:44.600 --> 00:40:47.600]   So it's never really scientifically sound to start with.
[00:40:47.600 --> 00:40:50.600]   But yeah, there's something you can do with this.
[00:40:50.600 --> 00:40:54.600]   Is it a good way?
[00:40:54.600 --> 00:40:57.600]   Yeah, true.
[00:40:57.600 --> 00:40:58.600]   Okay.
[00:40:58.600 --> 00:41:02.600]   - So if you have a bunch of parameters in your network
[00:41:02.600 --> 00:41:05.600]   that are actually making the loss function worse,
[00:41:05.600 --> 00:41:08.600]   then, like, kind of, obviously, the trial would be, like,
[00:41:08.600 --> 00:41:10.600]   maybe we should do something about this.
[00:41:10.600 --> 00:41:11.600]   Maybe we should set the weights to zero,
[00:41:11.600 --> 00:41:13.600]   or we should adjust the learning rate or something.
[00:41:13.600 --> 00:41:15.600]   - Yeah, so we did something like that.
[00:41:15.600 --> 00:41:17.600]   And interestingly, I think it's, like,
[00:41:17.600 --> 00:41:19.600]   it's like the same thing with behavioral science with people.
[00:41:19.600 --> 00:41:22.600]   Like, if you fire all the people that are bad behaviors,
[00:41:22.600 --> 00:41:25.600]   some people just slowly shift to be bad behaviors or something.
[00:41:25.600 --> 00:41:26.600]   So we found the same thing.
[00:41:26.600 --> 00:41:28.600]   If you, like, freeze the layer that are hurting,
[00:41:28.600 --> 00:41:31.600]   like, some other layers just start to hurt.
[00:41:31.600 --> 00:41:34.600]   I feel like this is an interesting analogy to be made here.
[00:41:34.600 --> 00:41:37.600]   So, like, it's a group behavior.
[00:41:37.600 --> 00:41:40.600]   And somehow, if you kill all the bad actors,
[00:41:40.600 --> 00:41:42.600]   some actors just become bad actors.
[00:41:42.600 --> 00:41:44.600]   - I have a little bit more basic question.
[00:41:44.600 --> 00:41:47.600]   You know, like, so when you say that it's hurting,
[00:41:47.600 --> 00:41:50.600]   you're saying that it's making the loss function worse, right?
[00:41:50.600 --> 00:41:51.600]   - It's making the loss go up.
[00:41:51.600 --> 00:41:55.600]   - So is that just an artifact of a fancier gradient descent algorithm?
[00:41:55.600 --> 00:41:57.600]   Like, if you just did the most basic thing
[00:41:57.600 --> 00:42:00.600]   and moved in the direction of the gradient,
[00:42:00.600 --> 00:42:03.600]   wouldn't only the loss go down?
[00:42:03.600 --> 00:42:05.600]   - Yeah, it is true for that mini-batch.
[00:42:05.600 --> 00:42:07.600]   But the loss that I'm talking about
[00:42:07.600 --> 00:42:09.600]   is measured the whole training set.
[00:42:09.600 --> 00:42:11.600]   So if you use only gradient, not fancy thing,
[00:42:11.600 --> 00:42:14.600]   and you use the full batch of the whole set,
[00:42:14.600 --> 00:42:15.600]   then that would be true.
[00:42:15.600 --> 00:42:18.600]   But people don't do that, because that's not even effective.
[00:42:18.600 --> 00:42:22.600]   - Yeah, related to the past two questions,
[00:42:22.600 --> 00:42:24.600]   when you say that half of the layers,
[00:42:24.600 --> 00:42:26.600]   or whatever percentage, are doing bad,
[00:42:26.600 --> 00:42:28.600]   are they consistently the same weights
[00:42:28.600 --> 00:42:30.600]   that are doing bad throughout the training, or do they--
[00:42:30.600 --> 00:42:31.600]   - Yeah, no.
[00:42:31.600 --> 00:42:34.600]   So that number is consistent.
[00:42:34.600 --> 00:42:35.600]   50% are doing bad.
[00:42:35.600 --> 00:42:37.600]   And then another number is consistent,
[00:42:37.600 --> 00:42:40.600]   that is, per weight, 50% of the time it is doing bad.
[00:42:40.600 --> 00:42:42.600]   So that means that they just switch it.
[00:42:42.600 --> 00:42:45.600]   - So even if you zero them out,
[00:42:45.600 --> 00:42:48.600]   at the times where they're good,
[00:42:48.600 --> 00:42:50.600]   you won't use them, so it would be bad.
[00:42:50.600 --> 00:42:52.600]   - Yeah, but one thing we found is consistent,
[00:42:52.600 --> 00:42:54.600]   that is layer-wise, at least for ResNet,
[00:42:54.600 --> 00:42:57.600]   the last layer is just always a bad actor.
[00:42:57.600 --> 00:42:58.600]   It's always hurting.
[00:42:58.600 --> 00:43:02.600]   So the last layer, the last FC layer, basically,
[00:43:02.600 --> 00:43:04.600]   right after the global pooling.
[00:43:04.600 --> 00:43:06.600]   So maybe it's too big, and it's like,
[00:43:06.600 --> 00:43:09.600]   FC is just always weird in a conf net.
[00:43:09.600 --> 00:43:11.600]   The last FC is also always weird.
[00:43:11.600 --> 00:43:13.600]   So that one, because it's so consistent,
[00:43:13.600 --> 00:43:15.600]   all the weights in that layer are just always hurting.
[00:43:15.600 --> 00:43:18.600]   If you freeze that one, it helps a little bit.
[00:43:18.600 --> 00:43:19.600]   Yeah.
[00:43:19.600 --> 00:43:22.600]   How many questions are allowed today?
[00:43:22.600 --> 00:43:25.600]   - Yeah, time for, let's say, two more.
[00:43:25.600 --> 00:43:26.600]   - Okay.
[00:43:26.600 --> 00:43:27.600]   Yeah, go ahead.
[00:43:27.600 --> 00:43:30.600]   - Does the dimension complexity, like MNIST,
[00:43:30.600 --> 00:43:33.600]   or, say, for being 10 times harder than MNIST,
[00:43:33.600 --> 00:43:36.600]   correlate with compute time for those--
[00:43:36.600 --> 00:43:38.600]   - Hmm, that's a good question.
[00:43:38.600 --> 00:43:40.600]   That's a good question.
[00:43:40.600 --> 00:43:46.600]   Maybe, yeah, maybe.
[00:43:46.600 --> 00:43:48.600]   But it's hard to compare this way,
[00:43:48.600 --> 00:43:51.600]   because CIFAR is also, there's more input channels.
[00:43:51.600 --> 00:43:53.600]   It's a harder problem.
[00:43:53.600 --> 00:43:54.600]   I think--
[00:43:54.600 --> 00:43:55.600]   - You mean it's a mix of the problem,
[00:43:55.600 --> 00:43:57.600]   not the problem with the non-complex image location?
[00:43:57.600 --> 00:44:00.600]   - Yeah, it's like, almost always a harder problem
[00:44:00.600 --> 00:44:01.600]   we indicate that you need more data,
[00:44:01.600 --> 00:44:03.600]   you need more compute.
[00:44:03.600 --> 00:44:04.600]   So it's not directly compute,
[00:44:04.600 --> 00:44:07.600]   but it has also to do with the data.
[00:44:07.600 --> 00:44:10.600]   In CIFAR, MNIST, CIFAR has about the same level
[00:44:10.600 --> 00:44:12.600]   of MNIST data, but it's a much, much harder problem.
[00:44:12.600 --> 00:44:15.600]   So we'd say that I would agree that it's more compute.
[00:44:15.600 --> 00:44:17.600]   So probably slower.
[00:44:17.600 --> 00:44:19.600]   It was the same network, though.
[00:44:19.600 --> 00:44:24.600]   - Does the dimension change if you double the data?
[00:44:24.600 --> 00:44:26.600]   - That we didn't try.
[00:44:26.600 --> 00:44:29.600]   Yeah, I think, yeah, you would make things easier.
[00:44:29.600 --> 00:44:31.600]   It's basically a measure of how hard the problem,
[00:44:31.600 --> 00:44:33.600]   plus this current network you're using is.
[00:44:33.600 --> 00:44:35.600]   If you make a network more powerful,
[00:44:35.600 --> 00:44:38.600]   the dimension would change.
[00:44:38.600 --> 00:44:39.600]   Yeah.
[00:44:39.600 --> 00:44:43.600]   - Can you do a granular search within the two numbers?
[00:44:43.600 --> 00:44:45.600]   It's more than just a random number,
[00:44:45.600 --> 00:44:47.600]   but more than a random number.
[00:44:47.600 --> 00:44:49.600]   - Yeah, we just try different numbers,
[00:44:49.600 --> 00:44:51.600]   and you see some switch here,
[00:44:51.600 --> 00:44:53.600]   and then you do more granular search
[00:44:53.600 --> 00:44:55.600]   within that two numbers.
[00:44:55.600 --> 00:44:58.600]   Yeah, it's just mostly for scientific purpose,
[00:44:58.600 --> 00:45:00.600]   not super practical here.
[00:45:00.600 --> 00:45:03.600]   It's just basically a proof that this thing exists.
[00:45:03.600 --> 00:45:05.600]   And once you found it, you can make use of it
[00:45:05.600 --> 00:45:07.600]   for model compression and everything.
[00:45:07.600 --> 00:45:11.600]   But this paper was basically saying that this thing exists.
[00:45:11.600 --> 00:45:13.600]   - Yeah, I'm still a little bit confused
[00:45:13.600 --> 00:45:18.600]   about how you measured a neuron is hurting
[00:45:18.600 --> 00:45:21.600]   the overall loss versus not.
[00:45:21.600 --> 00:45:26.600]   So you would freeze all the other weights,
[00:45:26.600 --> 00:45:29.600]   change only that weight in some direction,
[00:45:29.600 --> 00:45:33.600]   and see if that hurts the mean dimension also.
[00:45:33.600 --> 00:45:35.600]   - Yeah, so basically when you train everything,
[00:45:35.600 --> 00:45:37.600]   everything moves, right?
[00:45:37.600 --> 00:45:38.600]   And everything moves a little bit
[00:45:38.600 --> 00:45:40.600]   towards the direction of their indicated direction,
[00:45:40.600 --> 00:45:42.600]   mostly indicated by the gradient,
[00:45:42.600 --> 00:45:44.600]   but with some fancy other terms.
[00:45:44.600 --> 00:45:46.600]   So you're moving towards that direction.
[00:45:46.600 --> 00:45:48.600]   That's the direction you're moving.
[00:45:48.600 --> 00:45:50.600]   And then the amount of how much you're moving
[00:45:50.600 --> 00:45:51.600]   is the amount.
[00:45:51.600 --> 00:45:53.600]   So if you're on a weight space,
[00:45:53.600 --> 00:45:55.600]   you move this much along this direction.
[00:45:55.600 --> 00:45:58.600]   So this direction times the amount
[00:45:58.600 --> 00:46:01.600]   is the drop of loss.
[00:46:01.600 --> 00:46:03.600]   If you think about loss as like a surface,
[00:46:03.600 --> 00:46:06.600]   you move along the surface this much,
[00:46:06.600 --> 00:46:08.600]   going down this way.
[00:46:08.600 --> 00:46:11.600]   The loss is actually this bit.
[00:46:11.600 --> 00:46:14.600]   - But the neurons, if it's much layered,
[00:46:14.600 --> 00:46:16.600]   then it has more of a cascaded, right?
[00:46:16.600 --> 00:46:19.600]   If you change one neuron weight,
[00:46:19.600 --> 00:46:22.600]   it doesn't mean that you can directly map it
[00:46:22.600 --> 00:46:25.600]   to the overall final end loss.
[00:46:25.600 --> 00:46:27.600]   It might have some unseen,
[00:46:27.600 --> 00:46:31.600]   ununderstood impact along the way,
[00:46:31.600 --> 00:46:33.600]   even though you have the back problem,
[00:46:33.600 --> 00:46:35.600]   which is just an estimation.
[00:46:35.600 --> 00:46:37.600]   - Well, gradient already is calculated
[00:46:37.600 --> 00:46:39.600]   based on all those dependencies.
[00:46:39.600 --> 00:46:41.600]   But once you have each per weight gradient,
[00:46:41.600 --> 00:46:44.600]   you almost can neglect the dependency
[00:46:44.600 --> 00:46:47.600]   because now you just have your own direction to move.
[00:46:47.600 --> 00:46:50.600]   And how much you move depends on something else.
[00:46:50.600 --> 00:46:54.600]   And that's like exact how much loss you are impacting,
[00:46:54.600 --> 00:46:56.600]   how much you're impacting the loss.
[00:46:56.600 --> 00:46:58.600]   - Oh, the gradient for each, okay, got it.
[00:46:58.600 --> 00:47:00.600]   - Yeah. - All right.
[00:47:00.600 --> 00:47:01.600]   Unfortunately, that's all the time we have.
[00:47:01.600 --> 00:47:02.600]   Let's thank Rosanna again.
[00:47:02.600 --> 00:47:03.600]   - Thank you. - Thank you.

