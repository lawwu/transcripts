
[00:00:00.000 --> 00:00:10.000]   [MUSIC]
[00:00:10.000 --> 00:00:18.000]   I mean, there's always a straight off between parameter size and
[00:00:18.000 --> 00:00:21.400]   latency and it feels like pretty linear.
[00:00:21.400 --> 00:00:28.800]   I don't know if Lucas agrees with that, but like, so just the bigger, the smarter.
[00:00:28.800 --> 00:00:31.800]   It's almost like IQ.
[00:00:31.800 --> 00:00:33.600]   That's like pretty straightforward.
[00:00:33.600 --> 00:00:38.200]   So the one we deployed today is 2.7 billion parameters.
[00:00:38.200 --> 00:00:47.300]   And surprisingly, it's really the sweet spot like where less than 2B is actually like pretty dumb.
[00:00:47.300 --> 00:00:52.300]   And more than like 3 billion, it becomes kind of slow, visibly slow.
[00:00:52.300 --> 00:01:01.100]   So we found the sort of the Goldilocks of language models, 2.7 billion parameters.
[00:01:01.100 --> 00:01:06.300]   In terms of fine-tuning, it doesn't really have any bearing on latency.
[00:01:06.300 --> 00:01:10.300]   The tricky thing there is like, again, what we talked about with benchmarking,
[00:01:10.300 --> 00:01:15.300]   is that you could do well on benchmark, and then go try it in the real world.
[00:01:15.300 --> 00:01:19.300]   It's actually made the model performance worse.
[00:01:19.300 --> 00:01:24.800]   We had one case, for example, where it was doing better on Python after some fine-tuning,
[00:01:24.800 --> 00:01:31.300]   but then it forgot how to write JSX, which is JavaScript's like React syntax.
[00:01:31.300 --> 00:01:33.600]   And so we call testing by vibes.
[00:01:33.600 --> 00:01:38.300]   So in addition to the benchmark, after we...
[00:01:38.300 --> 00:01:42.800]   It's like best practice, I'll tell you.
[00:01:42.800 --> 00:01:48.800]   After we, that's one stage, that's not the whole thing.
[00:01:48.800 --> 00:01:51.800]   We kind of check the vibes of the model.
[00:01:51.800 --> 00:02:00.300]   And then if it passes the vibe check, it goes into an A/B test.
[00:02:00.300 --> 00:02:04.300]   So in A/B test, we check the acceptance rate.
[00:02:04.300 --> 00:02:09.300]   So typically our acceptance rate, I think, hovers around 25% of all suggestions.
[00:02:09.300 --> 00:02:13.300]   And if it enters op ports, then we're doing something well.
[00:02:13.300 --> 00:02:17.300]   If it's neutral, then maybe whatever we did wasn't useful.
[00:02:17.300 --> 00:02:21.800]   And if it goes down, that's definitely bad.
[00:02:21.800 --> 00:02:24.300]   And so that's sort of the last test.
[00:02:24.300 --> 00:02:28.300]   We'd love to get more objective about it, but we haven't found a way,
[00:02:28.300 --> 00:02:31.800]   other than just building up more and more benchmark over time.
[00:02:31.800 --> 00:02:38.800]   The vibe-driven development.
[00:02:38.800 --> 00:02:40.300]   Strong, rough line.
[00:02:41.300 --> 00:02:46.300]   [Music]
[00:02:46.300 --> 00:02:48.740]   [MUSIC PLAYING]

