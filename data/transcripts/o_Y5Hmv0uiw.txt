
[00:00:00.000 --> 00:00:07.320]   My talk today, as Thavania said, is about learning to--
[00:00:07.320 --> 00:00:09.320]   basically learning deep reinforcement learning,
[00:00:09.320 --> 00:00:12.080]   and the path that I took through this,
[00:00:12.080 --> 00:00:16.680]   and some of the pain points of trying
[00:00:16.680 --> 00:00:18.760]   to get some of these algorithms to work.
[00:00:18.760 --> 00:00:24.000]   And I started doing this about two months ago.
[00:00:24.000 --> 00:00:27.680]   And in this talk, the topics that I'm going to cover,
[00:00:27.680 --> 00:00:31.120]   they're mostly-- I guess they're kind of beginnerish.
[00:00:31.120 --> 00:00:33.960]   But I'm going to talk about what deep reinforcement learning is,
[00:00:33.960 --> 00:00:35.660]   or rather what reinforcement learning is,
[00:00:35.660 --> 00:00:39.040]   and then how we involve a neural network.
[00:00:39.040 --> 00:00:42.520]   And so to that end, I'm going to discuss
[00:00:42.520 --> 00:00:44.840]   the classical reinforcement learning algorithm called
[00:00:44.840 --> 00:00:46.760]   Q-learning, as well as this variant that
[00:00:46.760 --> 00:00:48.520]   uses a neural net.
[00:00:48.520 --> 00:00:51.560]   And then I'm also going to talk a little bit about exploration,
[00:00:51.560 --> 00:00:54.360]   which is an important topic in reinforcement learning,
[00:00:54.360 --> 00:00:59.960]   and how exploration became kind of a difficult thing in one
[00:00:59.960 --> 00:01:02.160]   of the problems that I was working on for my channel,
[00:01:02.160 --> 00:01:07.800]   where I was trying to train a model to play the game Snake.
[00:01:07.800 --> 00:01:09.960]   A little bit about me and some shameless plugs.
[00:01:09.960 --> 00:01:14.680]   So I'm a senior computer vision engineer at Simbi Robotics.
[00:01:14.680 --> 00:01:19.880]   We build a robot that scans shelving in retail stores,
[00:01:19.880 --> 00:01:21.960]   something that's become extremely
[00:01:21.960 --> 00:01:23.860]   relevant in the past few weeks.
[00:01:23.860 --> 00:01:27.040]   And then tells the store employees
[00:01:27.040 --> 00:01:28.680]   when things are out of stock.
[00:01:28.680 --> 00:01:32.240]   And I sort of been with them from roughly the very
[00:01:32.240 --> 00:01:36.520]   beginning, soon after they got their first round of funding.
[00:01:36.520 --> 00:01:39.040]   And I've been working on their computer vision stack
[00:01:39.040 --> 00:01:40.920]   and sort of building that out.
[00:01:40.920 --> 00:01:42.760]   I also recently started--
[00:01:42.760 --> 00:01:46.040]   about a year ago, I started a YouTube channel
[00:01:46.040 --> 00:01:48.160]   by the name of Jack of Some.
[00:01:48.160 --> 00:01:52.800]   And as was said in my introduction, I make videos.
[00:01:52.800 --> 00:01:54.800]   I started with making videos about text editors.
[00:01:54.800 --> 00:01:57.120]   I use Space Max, and I really like it.
[00:01:57.120 --> 00:01:58.800]   And so there weren't enough tutorials,
[00:01:58.800 --> 00:01:59.960]   and I started making videos.
[00:01:59.960 --> 00:02:02.200]   And then since then, I've been making videos
[00:02:02.200 --> 00:02:07.160]   about Python and reinforcement learning in general.
[00:02:07.160 --> 00:02:08.920]   I also have a Twitter.
[00:02:08.920 --> 00:02:09.760]   Come find me.
[00:02:09.760 --> 00:02:11.800]   I'll probably be yelling about something
[00:02:11.800 --> 00:02:14.220]   related to Python there.
[00:02:14.220 --> 00:02:16.620]   So my reinforcement learning work, a lot of it
[00:02:16.620 --> 00:02:20.400]   actually is tied to my channel, because I
[00:02:20.400 --> 00:02:21.760]   don't do this professionally.
[00:02:21.760 --> 00:02:27.040]   I do deep learning, but mostly it's
[00:02:27.040 --> 00:02:29.880]   in the context of computer vision
[00:02:29.880 --> 00:02:33.120]   and having to do with object segmentation, object detection,
[00:02:33.120 --> 00:02:35.500]   and things like OCR.
[00:02:35.500 --> 00:02:38.040]   I wanted to learn reinforcement learning for a long time,
[00:02:38.040 --> 00:02:41.080]   and I never got an opportunity to do that at work.
[00:02:41.080 --> 00:02:43.360]   So instead, I decided to tie it to my channel.
[00:02:43.360 --> 00:02:46.360]   And I decided that I would do videos and live streams
[00:02:46.360 --> 00:02:47.960]   about reinforcement learning.
[00:02:47.960 --> 00:02:51.200]   And in that process, I will sort of maybe grow my channel,
[00:02:51.200 --> 00:02:52.680]   and also learn this new thing.
[00:02:52.680 --> 00:02:53.960]   So I did this live stream.
[00:02:53.960 --> 00:02:55.500]   And the point of the live stream was
[00:02:55.500 --> 00:02:58.000]   to sort of apply basic Q-learning
[00:02:58.000 --> 00:02:59.840]   to a very simple maze-solving problem
[00:02:59.840 --> 00:03:02.520]   that you can see on the upper right here,
[00:03:02.520 --> 00:03:04.600]   sort of do it from scratch and see how that goes.
[00:03:04.600 --> 00:03:07.200]   And that ended up being fairly well-received,
[00:03:07.200 --> 00:03:10.360]   even though it's ridiculously long.
[00:03:10.360 --> 00:03:12.920]   So let's talk a little bit about what reinforcement learning is
[00:03:12.920 --> 00:03:15.040]   for those that might not know.
[00:03:15.040 --> 00:03:16.540]   The typical classical example that's
[00:03:16.540 --> 00:03:20.280]   given of the structure of what reinforcement learning is,
[00:03:20.280 --> 00:03:23.880]   you have some agent, so some thing
[00:03:23.880 --> 00:03:25.600]   that you can consider that as being
[00:03:25.600 --> 00:03:27.560]   a player in a video game.
[00:03:27.560 --> 00:03:29.000]   That can be a robot.
[00:03:29.000 --> 00:03:30.420]   There's a number of different ways
[00:03:30.420 --> 00:03:31.920]   to imagine what the agent is.
[00:03:31.920 --> 00:03:33.320]   And then there's some environment.
[00:03:33.320 --> 00:03:36.200]   So that could be the level of the video game.
[00:03:36.200 --> 00:03:38.640]   That could be the physical environment
[00:03:38.640 --> 00:03:42.440]   that a robot resides in, any number of things.
[00:03:42.440 --> 00:03:45.600]   And we'll see that sometimes the agent actually
[00:03:45.600 --> 00:03:47.640]   needs to be considered as part of the environment,
[00:03:47.640 --> 00:03:50.960]   because it's a significant piece in that environment.
[00:03:50.960 --> 00:03:54.440]   The agent has-- being it's an agent, it has some agency.
[00:03:54.440 --> 00:03:56.360]   It can do some actions.
[00:03:56.360 --> 00:03:58.840]   So given-- there are a few things that it can do.
[00:03:58.840 --> 00:04:01.600]   In doing so, it's going to change the environment.
[00:04:01.600 --> 00:04:03.520]   And the environment is going to respond to it
[00:04:03.520 --> 00:04:05.600]   by giving it some sort of a reward.
[00:04:05.600 --> 00:04:09.160]   And the reward signal is there to--
[00:04:09.160 --> 00:04:13.240]   it's more for learning, less so, I think, for the actual--
[00:04:13.240 --> 00:04:14.680]   when you actually run the agent.
[00:04:14.680 --> 00:04:16.440]   But the reward signal is there so
[00:04:16.440 --> 00:04:19.720]   that the environment can tell the agent if what it's doing
[00:04:19.720 --> 00:04:20.520]   is good or not.
[00:04:20.520 --> 00:04:25.400]   So a simple example might be a bump sensor on a robot,
[00:04:25.400 --> 00:04:28.120]   where if it runs into a wall, the reward is negative 1,
[00:04:28.120 --> 00:04:30.240]   because, hey, you should not be running into a wall.
[00:04:30.240 --> 00:04:32.840]   And the point there is that then by setting up
[00:04:32.840 --> 00:04:35.160]   some very simple rules and very simple rewards,
[00:04:35.160 --> 00:04:40.200]   we can get a neural net or just a general function
[00:04:40.200 --> 00:04:44.080]   approximator to learn how to take the best
[00:04:44.080 --> 00:04:47.480]   actions in an environment without actually teaching it,
[00:04:47.480 --> 00:04:51.160]   without actually creating some sort of a code that
[00:04:51.160 --> 00:04:52.640]   says, if this, do that.
[00:04:52.640 --> 00:04:53.840]   We just train a function.
[00:04:53.840 --> 00:04:56.440]   And then that function is able to decide the best way
[00:04:56.440 --> 00:04:59.040]   to navigate the environment.
[00:04:59.040 --> 00:05:01.760]   Now, as the agent takes actions, there's also state changes.
[00:05:01.760 --> 00:05:06.760]   So in the case of a robot, if the action is move a finger,
[00:05:06.760 --> 00:05:07.800]   then the state changes.
[00:05:07.800 --> 00:05:10.520]   The finger has been moved, and it's in a new location.
[00:05:10.520 --> 00:05:13.720]   And then there's terminal states.
[00:05:13.720 --> 00:05:15.220]   In the case of a video game, you can
[00:05:15.220 --> 00:05:17.120]   think of this as a character dying.
[00:05:17.120 --> 00:05:18.620]   And in the case of a robot, it could
[00:05:18.620 --> 00:05:20.160]   be an action that's just unacceptable,
[00:05:20.160 --> 00:05:21.600]   like, again, bumping into a wall.
[00:05:21.600 --> 00:05:23.840]   And in that case, you can say, hey, the test has ended.
[00:05:23.840 --> 00:05:25.200]   You got a big negative reward.
[00:05:25.200 --> 00:05:26.320]   Don't do this again.
[00:05:26.320 --> 00:05:31.200]   And then the hope is that the agent can learn to avoid that.
[00:05:31.200 --> 00:05:32.920]   To make this a little bit more concrete--
[00:05:32.920 --> 00:05:35.040]   and it's not exactly concrete, because we're still
[00:05:35.040 --> 00:05:37.160]   talking about a virtual agent--
[00:05:37.160 --> 00:05:39.780]   the benchmark example that I've been working with for some time
[00:05:39.780 --> 00:05:43.640]   to learn reinforcement learning has been the game of Snake.
[00:05:43.640 --> 00:05:46.040]   And as it turns out, I picked the game of Snake
[00:05:46.040 --> 00:05:47.800]   because I thought it was going to be easy,
[00:05:47.800 --> 00:05:49.480]   but it's actually supremely difficult.
[00:05:49.480 --> 00:05:52.960]   And I'll talk about why in a second.
[00:05:52.960 --> 00:05:56.480]   But the agent in this case is the head of the snake.
[00:05:56.480 --> 00:05:58.680]   I like to think the agent is just being the head,
[00:05:58.680 --> 00:06:01.680]   because the body of the snake is actually the environment.
[00:06:01.680 --> 00:06:04.620]   And the head can actually collide with the body
[00:06:04.620 --> 00:06:08.480]   and receive a negative reward, essentially death in this case.
[00:06:08.480 --> 00:06:10.780]   The actions that are possible are going up, going down,
[00:06:10.780 --> 00:06:12.120]   going left, and going right.
[00:06:12.120 --> 00:06:14.900]   And then the reward signals are for eating the food.
[00:06:14.900 --> 00:06:17.520]   The snake gets a plus 1.
[00:06:17.520 --> 00:06:20.160]   If it dies by running into the walls of the environment
[00:06:20.160 --> 00:06:22.680]   or by running into itself, it gets a negative 1.
[00:06:22.680 --> 00:06:25.600]   And then if it exists and doesn't do anything
[00:06:25.600 --> 00:06:27.380]   for a frame other than just moving,
[00:06:27.380 --> 00:06:31.320]   then it just gets pain, because all existence is pain.
[00:06:31.320 --> 00:06:33.740]   That translates to a very small negative reward.
[00:06:33.740 --> 00:06:35.520]   This is something that's very commonly done
[00:06:35.520 --> 00:06:39.480]   in reinforcement learning to teach the agent that it doesn't
[00:06:39.480 --> 00:06:40.880]   just need to accomplish the action,
[00:06:40.880 --> 00:06:42.180]   it needs to accomplish it fast.
[00:06:42.180 --> 00:06:45.120]   Otherwise, it'll get this constant small ticking
[00:06:45.120 --> 00:06:48.080]   negative reward.
[00:06:48.080 --> 00:06:50.760]   So enter the Q-learning algorithm.
[00:06:50.760 --> 00:06:55.160]   The Q-learning algorithm is meant to essentially help
[00:06:55.160 --> 00:06:57.680]   you learn a function.
[00:06:57.680 --> 00:06:59.920]   Classically, this function is often a table,
[00:06:59.920 --> 00:07:03.040]   but then the more contemporary versions of this function
[00:07:03.040 --> 00:07:05.040]   are neural nets.
[00:07:05.040 --> 00:07:07.480]   And the function's job is very simple.
[00:07:07.480 --> 00:07:09.600]   You are given a state.
[00:07:09.600 --> 00:07:11.880]   And going back to this example, the state in this case
[00:07:11.880 --> 00:07:14.680]   could be the structure of the snake
[00:07:14.680 --> 00:07:16.960]   or the image of the game itself.
[00:07:16.960 --> 00:07:21.240]   And then it's supposed to predict basically a vector that
[00:07:21.240 --> 00:07:23.960]   says, hey, if you take action number 1,
[00:07:23.960 --> 00:07:27.560]   then the max reward that you would get in this environment
[00:07:27.560 --> 00:07:31.080]   or in this episode is going to be some number.
[00:07:31.080 --> 00:07:33.200]   And then if you take A2, the max reward
[00:07:33.200 --> 00:07:34.560]   would be some number, et cetera.
[00:07:34.560 --> 00:07:38.400]   So using-- and this is supposed to be-- when I say max reward,
[00:07:38.400 --> 00:07:41.240]   this is supposed to be like encountering both the reward
[00:07:41.240 --> 00:07:43.680]   that it will get immediately as well as the reward that it
[00:07:43.680 --> 00:07:45.120]   would get in the future.
[00:07:45.120 --> 00:07:49.120]   So the point here is that by just evaluating this vector
[00:07:49.120 --> 00:07:51.760]   and looking at the highest reward value,
[00:07:51.760 --> 00:07:54.000]   that tells you the best action.
[00:07:54.000 --> 00:07:57.320]   So you don't have to do any kind of model-based prediction.
[00:07:57.320 --> 00:07:59.880]   You can if you want to, but standard control systems
[00:07:59.880 --> 00:08:02.160]   used to do a lot of model-based prediction.
[00:08:02.160 --> 00:08:04.240]   But in this case, you don't have to do that
[00:08:04.240 --> 00:08:06.640]   because once you've trained this function
[00:08:06.640 --> 00:08:09.760]   and once it's able to predict these Q values correctly,
[00:08:09.760 --> 00:08:13.520]   you can simply just pick the row that gives you the max Q value
[00:08:13.520 --> 00:08:16.160]   and then take that action and just keep flowing forward.
[00:08:16.160 --> 00:08:19.560]   So this process of making a decision
[00:08:19.560 --> 00:08:22.040]   becomes constant time.
[00:08:22.040 --> 00:08:25.760]   So like I said, the future needs to be considered.
[00:08:25.760 --> 00:08:27.640]   So as an example, let's consider, again,
[00:08:27.640 --> 00:08:28.640]   the game of snake.
[00:08:28.640 --> 00:08:30.800]   In this case, the snake somehow got itself
[00:08:30.800 --> 00:08:32.240]   into this predicament.
[00:08:32.240 --> 00:08:34.440]   And that could have happened through random actions.
[00:08:34.440 --> 00:08:37.360]   Maybe it's just exploring the environment in order to learn.
[00:08:37.360 --> 00:08:40.360]   And it's gotten to a point where going into one direction,
[00:08:40.360 --> 00:08:41.400]   it'll get food.
[00:08:41.400 --> 00:08:43.480]   But then immediately after that, it'll die.
[00:08:43.480 --> 00:08:45.120]   So let's-- I wrote that down.
[00:08:45.120 --> 00:08:47.280]   So going into the right, it'll get--
[00:08:47.280 --> 00:08:48.880]   in one time step, it'll get food.
[00:08:48.880 --> 00:08:50.400]   And that's a positive 1.
[00:08:50.400 --> 00:08:51.880]   That's a good action.
[00:08:51.880 --> 00:08:53.920]   But then in step 2, it's going to die.
[00:08:53.920 --> 00:08:55.120]   There's no way out of this.
[00:08:55.120 --> 00:08:56.240]   There's no escape.
[00:08:56.240 --> 00:09:01.520]   So it looks like, from the perspective of a model that's
[00:09:01.520 --> 00:09:04.240]   just sort of greedily trying to find the next best reward,
[00:09:04.240 --> 00:09:06.880]   this looks like maybe a good direction to go.
[00:09:06.880 --> 00:09:08.680]   But if you consider the future, it's
[00:09:08.680 --> 00:09:10.440]   not a good direction to go.
[00:09:10.440 --> 00:09:12.200]   On the other hand, if you go left,
[00:09:12.200 --> 00:09:14.200]   that's actually immediately bad.
[00:09:14.200 --> 00:09:18.000]   So from a greedy standpoint, if you go left, it's just pain.
[00:09:18.000 --> 00:09:19.080]   You don't get any reward.
[00:09:19.080 --> 00:09:21.680]   You just get a negative 0.001.
[00:09:21.680 --> 00:09:23.800]   And then second time step, as you go further left,
[00:09:23.800 --> 00:09:25.960]   it's also negative 0.001.
[00:09:25.960 --> 00:09:27.200]   And we just keep going on.
[00:09:27.200 --> 00:09:30.680]   And maybe after 25 time steps, finally it gets to eat food.
[00:09:31.680 --> 00:09:35.800]   But that's good because it's not dead.
[00:09:35.800 --> 00:09:39.560]   So what we're trying to do in terms of creating,
[00:09:39.560 --> 00:09:41.360]   quote unquote, an intelligence is
[00:09:41.360 --> 00:09:44.720]   to learn a function that can make this decision of going
[00:09:44.720 --> 00:09:46.880]   left in this case rather than going right,
[00:09:46.880 --> 00:09:51.120]   even though the immediate reward is to the right.
[00:09:51.120 --> 00:09:54.520]   So here I've just written out the reward, the actual Q values
[00:09:54.520 --> 00:09:56.080]   that you would expect in these cases.
[00:09:56.080 --> 00:09:57.680]   This is not the actual correct Q value.
[00:09:57.680 --> 00:10:01.160]   I'm just looking at--
[00:10:01.160 --> 00:10:04.280]   it's just the case if you only have one additional fruit
[00:10:04.280 --> 00:10:06.920]   and the game ends, the value would actually
[00:10:06.920 --> 00:10:10.200]   be higher for a normal game of Snake.
[00:10:10.200 --> 00:10:12.680]   So the learning in this case, how we actually
[00:10:12.680 --> 00:10:14.920]   learn that function, how we fit it,
[00:10:14.920 --> 00:10:17.320]   that's done through something called the Bellman equation.
[00:10:17.320 --> 00:10:18.720]   And the Bellman equation--
[00:10:18.720 --> 00:10:21.360]   sorry for throwing this much math here.
[00:10:21.360 --> 00:10:22.820]   I was trying to keep things simple,
[00:10:22.820 --> 00:10:24.720]   but I think I kind of have to talk about it.
[00:10:24.720 --> 00:10:25.600]   I'm not going to necessarily talk
[00:10:25.600 --> 00:10:29.000]   about the context of how the Bellman equation comes to be.
[00:10:29.000 --> 00:10:31.920]   We can just sort of take it on faith at the moment.
[00:10:31.920 --> 00:10:36.160]   But the Bellman equation says, hey, by definition,
[00:10:36.160 --> 00:10:40.960]   the Q value of your agent being in some state at time t
[00:10:40.960 --> 00:10:44.760]   and having taken some action at time t
[00:10:44.760 --> 00:10:46.520]   is going to be equal to the reward
[00:10:46.520 --> 00:10:50.000]   that that action got it for that one step plus whatever
[00:10:50.000 --> 00:10:53.840]   the maximum that you can expect through taking any actions
[00:10:53.840 --> 00:10:55.880]   for the future state.
[00:10:55.880 --> 00:10:57.720]   So this is a recursive relationship.
[00:10:57.720 --> 00:11:01.000]   And you can sort of expand this right point out all the way
[00:11:01.000 --> 00:11:03.360]   until the end of the episode or some time horizon
[00:11:03.360 --> 00:11:06.960]   that you've chosen and get sort of what the actual Q
[00:11:06.960 --> 00:11:08.000]   value is supposed to be.
[00:11:08.000 --> 00:11:17.220]   So we can use this for learning by, let's say,
[00:11:17.220 --> 00:11:19.880]   having a table over the states and actions
[00:11:19.880 --> 00:11:22.520]   because it's a two-variable function.
[00:11:22.520 --> 00:11:25.640]   And then you can sort of start that table randomly
[00:11:25.640 --> 00:11:27.160]   or with zeros.
[00:11:27.160 --> 00:11:33.880]   And then any time you observe the reward
[00:11:33.880 --> 00:11:38.240]   from a particular state, you can kind of start from the end
[00:11:38.240 --> 00:11:43.200]   and work your way back to slowly update and slowly increment
[00:11:43.200 --> 00:11:48.240]   the cells of the Q table to try to learn what the Q value is
[00:11:48.240 --> 00:11:49.680]   actually supposed to be.
[00:11:49.680 --> 00:11:53.840]   And so as an example, at the end of the live stream,
[00:11:53.840 --> 00:11:56.880]   I was able to show that for a very, very simple problem where
[00:11:56.880 --> 00:11:59.480]   you're trying to solve a maze and only solve
[00:11:59.480 --> 00:12:02.560]   that one particular maze, you can sort of randomly
[00:12:02.560 --> 00:12:05.160]   explore this entire environment.
[00:12:05.160 --> 00:12:12.320]   And then as it starts to be able to get to the end here,
[00:12:12.320 --> 00:12:15.160]   the reward is sort of going to propagate back
[00:12:15.160 --> 00:12:17.760]   through this path.
[00:12:17.760 --> 00:12:20.720]   And as you look at the Q table, which unfortunately I'm not
[00:12:20.720 --> 00:12:24.400]   showing here, it starts to show that taking the action here
[00:12:24.400 --> 00:12:26.640]   gives you an eventual reward of 1.
[00:12:26.640 --> 00:12:29.240]   And taking the action here gives you an eventual reward of 1.
[00:12:29.240 --> 00:12:30.940]   And then going into any of these obstacles
[00:12:30.940 --> 00:12:32.520]   gives you a negative 1 immediately.
[00:12:32.520 --> 00:12:34.840]   So this works really well when you
[00:12:34.840 --> 00:12:37.400]   have a small number of states, which in this case, I believe,
[00:12:37.400 --> 00:12:40.600]   was 64 states and a small number of actions.
[00:12:40.600 --> 00:12:43.120]   But as you sort of make things more complex--
[00:12:43.120 --> 00:12:46.080]   so like what happens when the input is really complex?
[00:12:46.080 --> 00:12:48.160]   You have this image.
[00:12:48.160 --> 00:12:52.240]   In this case, this image was a 320 by 320 image, I think.
[00:12:52.240 --> 00:12:57.880]   So if you consider sort of that, that just
[00:12:57.880 --> 00:12:59.960]   turns into a lot of different combinations
[00:12:59.960 --> 00:13:02.840]   and a really, really large state space.
[00:13:02.840 --> 00:13:07.280]   And using a table to fit this isn't really feasible.
[00:13:07.280 --> 00:13:09.160]   And so instead, we use neural nets
[00:13:09.160 --> 00:13:11.520]   to approximate that function.
[00:13:11.520 --> 00:13:13.480]   Note that neural nets don't have to be--
[00:13:13.480 --> 00:13:15.240]   they're not the only way to do this.
[00:13:15.240 --> 00:13:17.000]   Any arbitrary function approximator
[00:13:17.000 --> 00:13:19.840]   can work with Q-learning.
[00:13:19.840 --> 00:13:23.600]   Neural nets are just the best ones we have right now.
[00:13:23.600 --> 00:13:28.080]   So as an example of how this ends up working,
[00:13:28.080 --> 00:13:30.000]   I'm showing a slide here from the--
[00:13:30.000 --> 00:13:33.840]   I believe this was the Nature paper from DeepMind,
[00:13:33.840 --> 00:13:36.400]   where they were sort of explaining the system
[00:13:36.400 --> 00:13:40.920]   that they built to play Atari games at a superhuman level.
[00:13:40.920 --> 00:13:43.800]   And the input here is the same as what I said for Snake.
[00:13:43.800 --> 00:13:45.640]   So it's a bunch of pixels.
[00:13:45.640 --> 00:13:47.600]   And then we have a convolutional neural net
[00:13:47.600 --> 00:13:51.120]   with some ReLU activations and then some fully connected
[00:13:51.120 --> 00:13:52.000]   layers at the end.
[00:13:52.000 --> 00:13:53.700]   And then there are some number of inputs.
[00:13:53.700 --> 00:13:57.080]   In this case, various joystick directions,
[00:13:57.080 --> 00:13:58.660]   and then pressing the button, and then
[00:13:58.660 --> 00:14:02.240]   pressing the button with various joystick directions.
[00:14:02.240 --> 00:14:05.880]   And instead of using a Q-table, now we
[00:14:05.880 --> 00:14:08.040]   can use the same Bellman equation
[00:14:08.040 --> 00:14:10.960]   to train this neural net.
[00:14:10.960 --> 00:14:13.840]   And the loss function now has the reward
[00:14:13.840 --> 00:14:18.440]   plus this maximum of the Q-value minus the observed Q-value
[00:14:18.440 --> 00:14:20.360]   from the current state.
[00:14:20.360 --> 00:14:23.040]   And so this maximum of the Q-value
[00:14:23.040 --> 00:14:24.520]   then comes from the neural net.
[00:14:24.520 --> 00:14:28.760]   And this Q-value comes from the observation.
[00:14:28.760 --> 00:14:31.720]   And using this approach-- and there's a bit of nuance
[00:14:31.720 --> 00:14:33.960]   here that I'm not going to cover in this presentation,
[00:14:33.960 --> 00:14:36.720]   because it'll take much longer.
[00:14:36.720 --> 00:14:40.520]   But using this, we can construct a mean squared error loss
[00:14:40.520 --> 00:14:44.160]   and then fit a neural net.
[00:14:44.160 --> 00:14:48.600]   So going back to when I was trying to do this to solve
[00:14:48.600 --> 00:14:51.160]   the snake problem, I was trying to do this in Keras.
[00:14:51.160 --> 00:14:54.640]   And this requires a custom loss function.
[00:14:54.640 --> 00:14:56.800]   It has a weird target that you need to fit to,
[00:14:56.800 --> 00:15:00.520]   because if we go back to this equation,
[00:15:00.520 --> 00:15:02.800]   we need to take a maximum of this Q-value
[00:15:02.800 --> 00:15:04.400]   over a particular action.
[00:15:04.400 --> 00:15:08.080]   And doing that in Keras becomes really, really weird.
[00:15:08.080 --> 00:15:10.440]   And so this was not a fun experience
[00:15:10.440 --> 00:15:14.360]   when I was trying to do it, because I found myself
[00:15:14.360 --> 00:15:18.480]   going into doing things in a really weird way in Keras
[00:15:18.480 --> 00:15:19.720]   that I didn't quite like.
[00:15:19.720 --> 00:15:24.760]   I'm trying to show how that works.
[00:15:24.760 --> 00:15:27.120]   So the model here is the actual model
[00:15:27.120 --> 00:15:28.280]   that I'm trying to train.
[00:15:28.280 --> 00:15:30.600]   But then because of how the Keras API is laid out,
[00:15:30.600 --> 00:15:33.080]   if you're just trying to use the main Keras API,
[00:15:33.080 --> 00:15:35.040]   you have to wrap it in a new model
[00:15:35.040 --> 00:15:38.320]   where the inputs are now not just the image,
[00:15:38.320 --> 00:15:41.480]   but also this target value and the mask.
[00:15:41.480 --> 00:15:44.320]   And then you just pass those through the model
[00:15:44.320 --> 00:15:47.760]   and then construct your custom loss function
[00:15:47.760 --> 00:15:51.080]   using all of those pieces as being present.
[00:15:51.080 --> 00:15:53.600]   And that was not particularly nice.
[00:15:53.600 --> 00:15:56.360]   And then in addition to that, when
[00:15:56.360 --> 00:15:58.200]   I was trying to research this, I ended up
[00:15:58.200 --> 00:15:59.800]   running into this problem where--
[00:15:59.800 --> 00:16:02.040]   and I hate to pick on this article,
[00:16:02.040 --> 00:16:04.920]   but this seemed to be the most clear article.
[00:16:04.920 --> 00:16:07.560]   And then I later found out that the formulation that they
[00:16:07.560 --> 00:16:11.440]   had presented was totally ignoring the nuance
[00:16:11.440 --> 00:16:14.400]   that I showed here in the Bellman equation
[00:16:14.400 --> 00:16:15.560]   for constructing the loss.
[00:16:15.560 --> 00:16:17.760]   And so it worked fine for small problems.
[00:16:17.760 --> 00:16:20.640]   And I was trying to work on this with maze solving
[00:16:20.640 --> 00:16:22.280]   on a small grid, and it worked fine.
[00:16:22.280 --> 00:16:24.160]   But any time I went to a bigger problem,
[00:16:24.160 --> 00:16:27.760]   it fell apart, because it wasn't following the Bellman equation
[00:16:27.760 --> 00:16:30.000]   correctly.
[00:16:30.000 --> 00:16:31.840]   So as I was doing this exploration,
[00:16:31.840 --> 00:16:34.720]   trying to find easier ways to do this,
[00:16:34.720 --> 00:16:37.240]   I landed on this library called Keras RL.
[00:16:37.240 --> 00:16:39.800]   And it's a really well put together library.
[00:16:39.800 --> 00:16:41.340]   And I highly recommend anybody that's
[00:16:41.340 --> 00:16:45.760]   starting out in reinforcement learning that
[00:16:45.760 --> 00:16:47.520]   wants to use Keras and not necessarily
[00:16:47.520 --> 00:16:49.040]   go into the nitty gritty of things,
[00:16:49.040 --> 00:16:50.880]   you should check this out.
[00:16:50.880 --> 00:16:52.400]   But-- oh, sorry.
[00:16:52.400 --> 00:16:54.560]   There's no but yet.
[00:16:54.560 --> 00:16:56.560]   This ended up working reasonably well for me.
[00:16:56.560 --> 00:16:59.640]   I constructed this snake benchmark problem.
[00:16:59.640 --> 00:17:03.200]   And this is from training, but it was just sort of out
[00:17:03.200 --> 00:17:07.240]   of the box using their DQN implementation.
[00:17:07.240 --> 00:17:11.040]   I was able to get it to work reasonably well.
[00:17:11.040 --> 00:17:15.220]   The problem is that Keras RL has a few issues.
[00:17:15.220 --> 00:17:17.840]   It is, at the moment, if you try to install it from source,
[00:17:17.840 --> 00:17:19.920]   I believe it is broken, something
[00:17:19.920 --> 00:17:22.760]   to do with an integration that was added.
[00:17:22.760 --> 00:17:25.280]   And as far as I can tell, it's not currently
[00:17:25.280 --> 00:17:26.480]   being actively maintained.
[00:17:26.480 --> 00:17:28.160]   That might change in the future.
[00:17:28.160 --> 00:17:32.520]   And then it also focuses only on the Q-learning
[00:17:32.520 --> 00:17:33.400]   related algorithms.
[00:17:33.400 --> 00:17:36.280]   There's an entire other class of RL algorithms
[00:17:36.280 --> 00:17:37.880]   called policy gradient.
[00:17:37.880 --> 00:17:40.540]   And Keras RL currently does not support that.
[00:17:40.540 --> 00:17:43.920]   And I suspect that it would get really hairy supporting
[00:17:43.920 --> 00:17:49.400]   all of those in Keras if you're just using the old Keras API.
[00:17:49.400 --> 00:17:52.440]   Positive thing, though, through Keras RL,
[00:17:52.440 --> 00:17:54.880]   I got my first introduction to weights and biases
[00:17:54.880 --> 00:17:57.960]   because that's integrated into Keras RL.
[00:17:57.960 --> 00:17:59.520]   And then I started looking into it.
[00:17:59.520 --> 00:18:03.280]   And it's been really great for me since.
[00:18:03.280 --> 00:18:06.840]   So moving forward, I abandoned Keras RL as well
[00:18:06.840 --> 00:18:09.600]   and went the route of trying to implement these things
[00:18:09.600 --> 00:18:13.920]   by myself from using the new TensorFlow 2 API.
[00:18:13.920 --> 00:18:16.080]   And that actually ended up being much nicer.
[00:18:16.080 --> 00:18:18.360]   Now, I know I've thrown up a bunch of code here.
[00:18:18.360 --> 00:18:20.360]   And it actually does take a little bit of time
[00:18:20.360 --> 00:18:23.600]   to go through this and correlate it with the equation.
[00:18:23.600 --> 00:18:25.660]   But the nice thing here is that I no longer
[00:18:25.660 --> 00:18:27.360]   have to create a dummy model that
[00:18:27.360 --> 00:18:28.720]   has pass-through inputs.
[00:18:28.720 --> 00:18:30.280]   I can just create this train function
[00:18:30.280 --> 00:18:35.080]   and pass in everything as it comes
[00:18:35.080 --> 00:18:38.240]   from the simulated environment, and then
[00:18:38.240 --> 00:18:42.520]   go through a series of functions and construct my loss that way.
[00:18:42.520 --> 00:18:47.040]   And then just use an optimizer to do back propagation
[00:18:47.040 --> 00:18:47.560]   through that.
[00:18:47.560 --> 00:18:50.480]   So that's the other thing is if you're
[00:18:50.480 --> 00:18:52.360]   willing to get your hands dirty a little bit,
[00:18:52.360 --> 00:18:54.720]   then don't go with an existing framework or something
[00:18:54.720 --> 00:18:55.220]   like that.
[00:18:55.220 --> 00:18:58.420]   Try to implement these things from scratch.
[00:18:58.420 --> 00:19:01.180]   And consider using something like PyTorch or TensorFlow 2,
[00:19:01.180 --> 00:19:03.460]   because they're really, really nice APIs now.
[00:19:03.460 --> 00:19:09.260]   And they fit to these problems really well.
[00:19:09.260 --> 00:19:11.740]   So around this time--
[00:19:11.740 --> 00:19:13.900]   now, the video that I was showing before of the snake
[00:19:13.900 --> 00:19:15.740]   exploring, it never really could do better
[00:19:15.740 --> 00:19:17.340]   than what I was showing there.
[00:19:17.340 --> 00:19:23.180]   And one of the reasons for that was that exploration is--
[00:19:23.180 --> 00:19:24.940]   I'll talk a little bit about what this is.
[00:19:24.940 --> 00:19:30.100]   And I'll get back into how this connects to the snake problem.
[00:19:30.100 --> 00:19:33.660]   So typically, when you're using deep Q learning,
[00:19:33.660 --> 00:19:36.980]   the standard way of doing exploration is called--
[00:19:36.980 --> 00:19:39.740]   sorry-- it's called epsilon greedy.
[00:19:39.740 --> 00:19:42.540]   And epsilon greedy essentially says, in the beginning,
[00:19:42.540 --> 00:19:44.500]   the model has no idea what to do.
[00:19:44.500 --> 00:19:48.340]   So at the start of the training time, because we can't--
[00:19:48.340 --> 00:19:50.560]   we have absolutely no idea what should be done.
[00:19:50.560 --> 00:19:51.980]   We'll just do something random.
[00:19:51.980 --> 00:19:54.380]   So in the case of snake, we'll take one random direction
[00:19:54.380 --> 00:19:55.500]   and move to that.
[00:19:55.500 --> 00:19:58.000]   And every time step, we're just going to do something random
[00:19:58.000 --> 00:20:01.340]   and keep track of all the state changes and all the rewards
[00:20:01.340 --> 00:20:04.460]   and everything, and then start training on that.
[00:20:04.460 --> 00:20:08.220]   And then the idea is it's not necessarily very easy
[00:20:08.220 --> 00:20:10.700]   to evaluate if your model has--
[00:20:10.700 --> 00:20:12.940]   how well your model has learned.
[00:20:12.940 --> 00:20:16.420]   So we're just going to create some time horizon,
[00:20:16.420 --> 00:20:18.900]   let's say, like a million steps in the environment
[00:20:18.900 --> 00:20:19.940]   or something like that.
[00:20:19.940 --> 00:20:21.520]   And then over that time horizon, we're
[00:20:21.520 --> 00:20:24.020]   slowly going to decrease the amount of randomness
[00:20:24.020 --> 00:20:27.540]   or what's called epsilon until it becomes really, really
[00:20:27.540 --> 00:20:28.060]   small.
[00:20:28.060 --> 00:20:30.260]   It never goes to zero, because you never
[00:20:30.260 --> 00:20:33.500]   want to create a situation where the model can't explore anymore
[00:20:33.500 --> 00:20:39.740]   because it's always following its current best strategy.
[00:20:39.740 --> 00:20:43.500]   And so usually, it asymptotically
[00:20:43.500 --> 00:20:46.020]   reaches some very small value, like 1% or something
[00:20:46.020 --> 00:20:48.340]   like that.
[00:20:48.340 --> 00:20:50.620]   So yeah, at the beginning, you start very random.
[00:20:50.620 --> 00:20:54.440]   And then at the end, we get not very random.
[00:20:54.440 --> 00:20:56.060]   There's a problem with that.
[00:20:56.060 --> 00:20:58.060]   And this is not necessarily--
[00:20:58.060 --> 00:20:59.620]   there's a number of problems with it.
[00:20:59.620 --> 00:21:02.860]   But in many environments, this problem doesn't show up.
[00:21:02.860 --> 00:21:05.980]   But in the game of Snake, it shows up very, very clearly,
[00:21:05.980 --> 00:21:09.220]   which is that even 1% randomness, or even smaller
[00:21:09.220 --> 00:21:13.100]   than that, depending on if the game of Snake is really large,
[00:21:13.100 --> 00:21:15.980]   it's too random in the late game.
[00:21:15.980 --> 00:21:18.300]   And the reason for that is I'm showing here
[00:21:18.300 --> 00:21:20.340]   a 6 by 6 game of Snake.
[00:21:20.340 --> 00:21:22.020]   And by the time that it's gotten here,
[00:21:22.020 --> 00:21:23.980]   it's gone through maybe 200 moves.
[00:21:23.980 --> 00:21:26.940]   And I did some sort of back of the envelope math
[00:21:26.940 --> 00:21:31.900]   to see if the randomness percentage was 1%,
[00:21:31.900 --> 00:21:34.200]   then what is the chance that by the time
[00:21:34.200 --> 00:21:37.100]   it reaches this particular scenario, what
[00:21:37.100 --> 00:21:39.980]   is the chance that it would have taken at least one
[00:21:39.980 --> 00:21:41.260]   move that would have killed it?
[00:21:41.260 --> 00:21:43.820]   And there was a 63% chance of that.
[00:21:43.820 --> 00:21:49.060]   And so this percentage becomes higher and higher
[00:21:49.060 --> 00:21:51.260]   as we make the game of Snake bigger
[00:21:51.260 --> 00:21:53.900]   and as it plays longer and longer through this game.
[00:21:53.900 --> 00:21:56.780]   So epsilon greedy maybe might work
[00:21:56.780 --> 00:21:59.660]   for a really, really small epsilon value,
[00:21:59.660 --> 00:22:02.020]   but it'll probably take a very, very long time.
[00:22:02.020 --> 00:22:03.820]   And maybe it's not necessarily the best way
[00:22:03.820 --> 00:22:07.940]   to do exploration for Snake.
[00:22:07.940 --> 00:22:11.900]   So one of the approaches that's often used in this scenario
[00:22:11.900 --> 00:22:16.500]   is what if we can reinterpret the Q values as probabilities?
[00:22:16.500 --> 00:22:18.980]   So in this case, going back to the situation
[00:22:18.980 --> 00:22:22.620]   that I was showing before, we have an expected Q value of 0
[00:22:22.620 --> 00:22:24.220]   if the Snake goes this direction,
[00:22:24.220 --> 00:22:26.100]   and then negative 1 for up and down,
[00:22:26.100 --> 00:22:28.860]   and then 0.975 for going left.
[00:22:28.860 --> 00:22:33.780]   We can softmax this, and that gives us a probability
[00:22:33.780 --> 00:22:39.060]   distribution where it's sort of reinterpreting those values
[00:22:39.060 --> 00:22:44.220]   as what is the probability that taking this is a good action?
[00:22:44.220 --> 00:22:45.660]   And of course, this is not perfect
[00:22:45.660 --> 00:22:48.540]   because that creates a 10% chance of going up here
[00:22:48.540 --> 00:22:50.780]   and 10% chance of going down here, both of which
[00:22:50.780 --> 00:22:52.420]   would cause immediate death.
[00:22:52.420 --> 00:22:55.980]   But now the total chance that it will take a good action
[00:22:55.980 --> 00:22:59.340]   in this scenario, which is kind of fairly in late game,
[00:22:59.340 --> 00:23:00.540]   is relatively high.
[00:23:00.540 --> 00:23:01.780]   It's like 60%.
[00:23:01.780 --> 00:23:03.540]   So we can just sample from this distribution
[00:23:03.540 --> 00:23:05.500]   and use that for exploration.
[00:23:05.500 --> 00:23:07.980]   But there's actually-- things are a little bit better
[00:23:07.980 --> 00:23:11.660]   than this because as the Snake learns more and more,
[00:23:11.660 --> 00:23:14.260]   it's going to know that the Q value of moving
[00:23:14.260 --> 00:23:17.420]   in this left direction is not 0.975.
[00:23:17.420 --> 00:23:18.960]   It's actually much higher than that.
[00:23:18.960 --> 00:23:20.820]   The expected reward of going here
[00:23:20.820 --> 00:23:23.660]   is much higher because the game actually continues on,
[00:23:23.660 --> 00:23:26.740]   and the Snake can sort of continue to eat more and more.
[00:23:26.740 --> 00:23:29.140]   So if we use this and softmax that,
[00:23:29.140 --> 00:23:31.340]   then the actual probability distribution
[00:23:31.340 --> 00:23:33.740]   is very, very heavily weighted now
[00:23:33.740 --> 00:23:35.460]   towards the Snake going left in this case,
[00:23:35.460 --> 00:23:38.620]   not going right, up, or down, because the probabilities there
[00:23:38.620 --> 00:23:40.220]   are just exceedingly small.
[00:23:40.220 --> 00:23:42.560]   So as the Snake learns more and more,
[00:23:42.560 --> 00:23:45.100]   and as the Q values become larger and larger
[00:23:45.100 --> 00:23:47.340]   in this scenario, this approach, which
[00:23:47.340 --> 00:23:51.740]   is called Boltzmann exploration, becomes much, much better.
[00:23:51.740 --> 00:23:58.500]   So as an example of that, using this approach with DQN,
[00:23:58.500 --> 00:24:01.900]   the Snake agent is now able to almost win a 10 by 10 game.
[00:24:01.900 --> 00:24:04.820]   I didn't continue training this because I eventually
[00:24:04.820 --> 00:24:06.460]   moved on to policy gradients.
[00:24:06.460 --> 00:24:08.540]   But this is sort of an example game
[00:24:08.540 --> 00:24:12.900]   that I was able to train with Boltzmann exploration.
[00:24:12.900 --> 00:24:16.460]   And this is something that I was never able to do this well
[00:24:16.460 --> 00:24:17.340]   with epsilon grading.
[00:24:17.340 --> 00:24:22.780]   And now we have an abrupt ending because my original plan
[00:24:22.780 --> 00:24:25.200]   for this presentation was to cover more ground,
[00:24:25.200 --> 00:24:27.340]   but I realized that just covering Q-learning was
[00:24:27.340 --> 00:24:28.780]   actually going to take much longer.
[00:24:28.780 --> 00:24:30.900]   I'm going to talk a little bit about--
[00:24:30.900 --> 00:24:32.820]   oh, sorry, before I talk about the resources,
[00:24:32.820 --> 00:24:35.940]   I want to talk about how weights and biases has helped me do
[00:24:35.940 --> 00:24:37.400]   some of this experimentation.
[00:24:37.400 --> 00:24:39.060]   So this is one of the dashboards that I
[00:24:39.060 --> 00:24:42.780]   was using to track when I was working
[00:24:42.780 --> 00:24:43.980]   through the game of Snake.
[00:24:43.980 --> 00:24:45.980]   And one of the things that I really, really like
[00:24:45.980 --> 00:24:48.500]   in weights and biases over something like TensorFlow
[00:24:48.500 --> 00:24:51.100]   is that naming the runs is really, really straightforward.
[00:24:51.100 --> 00:24:53.380]   And you can change these after the fact.
[00:24:53.380 --> 00:24:57.020]   And then looking at all this data
[00:24:57.020 --> 00:24:59.580]   is really, really easy across different runs.
[00:24:59.580 --> 00:25:02.060]   So I was having to keep track of, hey,
[00:25:02.060 --> 00:25:05.160]   I beat a 4 by 4 game and a 6 by 6 game.
[00:25:05.160 --> 00:25:07.780]   I kind of want to see how those behaved
[00:25:07.780 --> 00:25:08.980]   versus a 10 by 10 game.
[00:25:08.980 --> 00:25:13.980]   So being able to have access to this on the fly on my phone
[00:25:13.980 --> 00:25:16.420]   was very, very helpful through this process,
[00:25:16.420 --> 00:25:18.320]   and it continues to be helpful.
[00:25:18.320 --> 00:25:20.060]   A little bit about resources for those
[00:25:20.060 --> 00:25:21.980]   that might be interested in learning more
[00:25:21.980 --> 00:25:23.260]   about reinforcement learning.
[00:25:23.260 --> 00:25:25.980]   David Silver's course is really useful.
[00:25:25.980 --> 00:25:27.020]   It's on YouTube.
[00:25:27.020 --> 00:25:28.060]   It's free.
[00:25:28.060 --> 00:25:32.880]   It's something-- lectures that he gave in 2015 at DeepMind.
[00:25:32.880 --> 00:25:34.500]   One of the things that I would recommend
[00:25:34.500 --> 00:25:36.140]   if you're going to watch this class
[00:25:36.140 --> 00:25:41.420]   is to start doing some experimentation on your own
[00:25:41.420 --> 00:25:43.780]   while or before you're watching them.
[00:25:43.780 --> 00:25:46.660]   And what that'll do is it'll create a nice feedback where
[00:25:46.660 --> 00:25:49.160]   you will have a much better idea of what he's talking about,
[00:25:49.160 --> 00:25:52.220]   because you've sort of already struggled with it.
[00:25:52.220 --> 00:25:54.180]   Another resource that's really useful
[00:25:54.180 --> 00:25:57.500]   is this series of repositories called RL Adventure
[00:25:57.500 --> 00:25:59.460]   from this GitHub account, Higgsfield.
[00:25:59.460 --> 00:26:03.660]   And RL Adventure contains mostly DQN-related stuff,
[00:26:03.660 --> 00:26:07.900]   and then RL Adventure 2 contains policy gradient-related stuff,
[00:26:07.900 --> 00:26:09.780]   which is something that I couldn't talk about
[00:26:09.780 --> 00:26:11.900]   in this presentation.
[00:26:11.900 --> 00:26:13.520]   A third really useful resource, if you're
[00:26:13.520 --> 00:26:15.460]   using PyTorch, which is what I'm using now--
[00:26:15.460 --> 00:26:18.180]   I kind of moved on from TensorFlow 2--
[00:26:18.180 --> 00:26:24.280]   is Ilya Kostrikov's repository, where he's implementing three--
[00:26:24.280 --> 00:26:28.800]   actually four-- policy gradient methods that
[00:26:28.800 --> 00:26:30.540]   are sort of the more--
[00:26:30.540 --> 00:26:32.320]   I guess the more modern algorithms that are
[00:26:32.320 --> 00:26:34.780]   used in reinforcement learning right now.
[00:26:34.780 --> 00:26:36.300]   And it's really well laid out.
[00:26:36.300 --> 00:26:37.840]   The code is really easy to understand.
[00:26:37.840 --> 00:26:40.580]   And if you're looking to check out
[00:26:40.580 --> 00:26:42.180]   the guts of some of these algorithms,
[00:26:42.180 --> 00:26:45.100]   it's a really good repository for that.
[00:26:45.100 --> 00:26:46.860]   And that's all I got.
[00:26:46.860 --> 00:26:48.180]   I don't have a conclusion slide.
[00:26:48.180 --> 00:26:53.140]   So if there's any questions, I'll take those now.
[00:26:53.140 --> 00:26:54.060]   That was really good.
[00:26:54.060 --> 00:26:54.980]   Thank you.
[00:26:54.980 --> 00:26:58.700]   I see one question from Pierce.
[00:26:58.700 --> 00:27:00.340]   He asks, what kind of environments
[00:27:00.340 --> 00:27:02.840]   can you apply reinforcement learning to?
[00:27:02.840 --> 00:27:05.760]   And is it always supposed to be a gaming environment?
[00:27:05.760 --> 00:27:08.120]   Or are there other domains where you could apply
[00:27:08.120 --> 00:27:09.720]   reinforcement learning?
[00:27:09.720 --> 00:27:12.880]   No, reinforcement learning does not always--
[00:27:12.880 --> 00:27:15.680]   I'm going to go to a less busy slide.
[00:27:15.680 --> 00:27:19.360]   Games are just the easiest place that we
[00:27:19.360 --> 00:27:23.800]   can sort of study and research reinforcement learning.
[00:27:23.800 --> 00:27:28.060]   The actual field is really old.
[00:27:28.060 --> 00:27:31.560]   And it's actually been applied a lot in things like finance.
[00:27:31.560 --> 00:27:35.560]   But it's never been applied in--
[00:27:35.560 --> 00:27:36.640]   finance is real world.
[00:27:36.640 --> 00:27:38.060]   But I was going to say, it's never
[00:27:38.060 --> 00:27:41.760]   been applied in something "real world," quote unquote.
[00:27:41.760 --> 00:27:45.840]   In, let's say, robotics or something like that,
[00:27:45.840 --> 00:27:48.680]   for stuff like that, it's relatively new.
[00:27:48.680 --> 00:27:55.200]   And it gets this bad perception that, oh, people are just
[00:27:55.200 --> 00:27:56.240]   playing games with it.
[00:27:56.240 --> 00:27:57.440]   Oh, yeah, of course.
[00:27:57.440 --> 00:27:58.840]   Fine, you beat the game of Go.
[00:27:58.840 --> 00:28:01.200]   But what does that mean in real world terms?
[00:28:01.200 --> 00:28:03.600]   Or now they're playing StarCraft and Dota
[00:28:03.600 --> 00:28:06.160]   at a superhuman level with reinforcement learning.
[00:28:06.160 --> 00:28:08.920]   And people kind of roll their eyes at those news
[00:28:08.920 --> 00:28:11.800]   because it's like, why are we wasting time doing this?
[00:28:11.800 --> 00:28:13.720]   Games are for people to play.
[00:28:13.720 --> 00:28:16.080]   But it's just an abstraction.
[00:28:16.080 --> 00:28:19.200]   It's kind of like how researchers in the '60s
[00:28:19.200 --> 00:28:22.280]   and '70s were always working on the inverse pendulum
[00:28:22.280 --> 00:28:28.960]   problem and the problem of balancing a stick on your finger
[00:28:28.960 --> 00:28:31.080]   and solving that with mathematics.
[00:28:31.080 --> 00:28:33.080]   And that sounds like a silly problem.
[00:28:33.080 --> 00:28:36.920]   But it was actually really important for rockets
[00:28:36.920 --> 00:28:39.680]   and a number of other things that have more recently
[00:28:39.680 --> 00:28:42.040]   become relevant.
[00:28:42.040 --> 00:28:44.400]   So games are just a useful abstraction.
[00:28:44.400 --> 00:28:46.680]   Outside of games, like I said, in the financial sector,
[00:28:46.680 --> 00:28:49.120]   it's actually been used for decades now.
[00:28:49.120 --> 00:28:51.080]   And that use continues.
[00:28:51.080 --> 00:28:54.800]   And then it's actually used in a lot of recommendation systems,
[00:28:54.800 --> 00:28:59.120]   which some might say recommendations or systems
[00:28:59.120 --> 00:29:02.280]   are almost like the backbone of so many online services.
[00:29:02.280 --> 00:29:05.720]   And so things like Netflix and Google particularly,
[00:29:05.720 --> 00:29:11.240]   a lot of deep mind's work goes into developing and improving
[00:29:11.240 --> 00:29:12.960]   Google's recommendation systems.
[00:29:12.960 --> 00:29:15.120]   So that's another real world scenario
[00:29:15.120 --> 00:29:17.520]   where RL is already being used.
[00:29:17.520 --> 00:29:19.760]   It's not something that we really think about or talk
[00:29:19.760 --> 00:29:21.960]   about a lot, but it's there.
[00:29:21.960 --> 00:29:24.160]   And then another place is a lot of people
[00:29:24.160 --> 00:29:26.000]   are now applying it to robots.
[00:29:26.000 --> 00:29:28.880]   That's very researchy at the moment.
[00:29:28.880 --> 00:29:31.440]   I don't know if a single robot out there that's actively
[00:29:31.440 --> 00:29:33.160]   using reinforcement learning, but I
[00:29:33.160 --> 00:29:38.120]   expect that to start changing because the results are so--
[00:29:38.120 --> 00:29:39.960]   much better than we could have expected.
[00:29:39.960 --> 00:29:42.600]   So some examples are coming from OpenAI,
[00:29:42.600 --> 00:29:47.520]   where they did this sort of co-simulation
[00:29:47.520 --> 00:29:50.000]   where they trained a reinforcement learning
[00:29:50.000 --> 00:29:55.680]   algorithm to use a robot hand to sort of solve a Rubik's cube.
[00:29:55.680 --> 00:29:57.160]   It wasn't solving the Rubik's cube.
[00:29:57.160 --> 00:29:59.480]   It knew what actions to take, but it had to learn
[00:29:59.480 --> 00:30:00.800]   how to move its fingers.
[00:30:00.800 --> 00:30:03.800]   And that was trained first in simulation, which is a game,
[00:30:03.800 --> 00:30:07.280]   and then transferred onto a real world robot
[00:30:07.280 --> 00:30:10.040]   and trained sort of further until that got refined.
[00:30:10.040 --> 00:30:12.680]   And it created a really impressive looking algorithm,
[00:30:12.680 --> 00:30:15.160]   something that is not something--
[00:30:15.160 --> 00:30:16.840]   something that's not very easy to do
[00:30:16.840 --> 00:30:18.880]   with conventional methods.
[00:30:18.880 --> 00:30:21.720]   And it was also very robust to things like noise.
[00:30:21.720 --> 00:30:23.200]   So I hope that kind of clears that up,
[00:30:23.200 --> 00:30:27.640]   that there's a lot of avenues for real world application.
[00:30:27.640 --> 00:30:30.440]   And the reason we see games is because at least
[00:30:30.440 --> 00:30:33.760]   the current approaches are much more straightforward
[00:30:33.760 --> 00:30:37.520]   to apply on games than they are on something real.
[00:30:44.680 --> 00:30:47.920]   I think you're on mute.
[00:30:47.920 --> 00:30:48.720]   Thank you.
[00:30:48.720 --> 00:30:52.960]   I posted a link to the OpenAI robotic hand
[00:30:52.960 --> 00:30:55.280]   that you mentioned in the chat.
[00:30:55.280 --> 00:30:56.820]   Also, there's another question if you
[00:30:56.820 --> 00:30:59.040]   want to take that really fast, and then we'll move on.
[00:30:59.040 --> 00:31:07.560]   Oh, so this person asked, for a high number of input states,
[00:31:07.560 --> 00:31:09.640]   eGreedy might even force the model
[00:31:09.640 --> 00:31:13.040]   to learn that pain is the best possible outcome for a model
[00:31:13.040 --> 00:31:18.480]   to get, as in a 10 by 10 matrix probability to get food
[00:31:18.480 --> 00:31:22.280]   is very low if you're computing the max reward in the next 10
[00:31:22.280 --> 00:31:23.440]   steps.
[00:31:23.440 --> 00:31:25.240]   Am I approaching the problem correctly?
[00:31:25.240 --> 00:31:33.120]   I don't know if I necessarily fully understand.
[00:31:33.120 --> 00:31:34.880]   The question?
[00:31:34.880 --> 00:31:35.600]   Yeah.
[00:31:35.600 --> 00:31:38.200]   Sounds like basically what they're trying to ask
[00:31:38.200 --> 00:31:43.080]   is, if you know that you're going to lose,
[00:31:43.080 --> 00:31:46.240]   is that ever the right strategy?
[00:31:46.240 --> 00:31:48.280]   You just choose to die as the algorithm.
[00:31:48.280 --> 00:31:49.960]   Does that ever happen?
[00:31:49.960 --> 00:31:52.800]   Yes.
[00:31:52.800 --> 00:31:53.800]   Sorry.
[00:31:53.800 --> 00:31:56.680]   My presentation really should have
[00:31:56.680 --> 00:32:00.320]   been titled Reinforcement Learning a Story in Pain,
[00:32:00.320 --> 00:32:03.920]   or an Experience of Immense Pain.
[00:32:03.920 --> 00:32:08.040]   Reward and how the reward is shaped is very important.
[00:32:08.040 --> 00:32:12.360]   And this ties to exploration as well.
[00:32:12.360 --> 00:32:18.160]   And it is possible to latch on to degenerate strategies, where
[00:32:18.160 --> 00:32:21.800]   the model learns to do the least painful thing, because, hey,
[00:32:21.800 --> 00:32:26.680]   I'm going to die 10 steps from now, so I may as well die now.
[00:32:26.680 --> 00:32:29.520]   But the fixes for that-- first of all,
[00:32:29.520 --> 00:32:31.320]   the fixes for that is you can sometimes
[00:32:31.320 --> 00:32:33.480]   change the reward structure.
[00:32:33.480 --> 00:32:37.320]   For a much larger grid, the amount of pain for every step
[00:32:37.320 --> 00:32:38.560]   would be lower.
[00:32:38.560 --> 00:32:40.520]   Or in some cases, you can change it
[00:32:40.520 --> 00:32:42.760]   so that as the snake gets longer and longer,
[00:32:42.760 --> 00:32:47.520]   the pain of existence becomes smaller and smaller.
[00:32:47.520 --> 00:32:51.120]   And that negative reward becomes smaller and smaller,
[00:32:51.120 --> 00:32:54.600]   because we expect it to have to take a longer route.
[00:32:54.600 --> 00:32:56.600]   There are other cases where you don't actually
[00:32:56.600 --> 00:32:59.160]   use the constant negative reward,
[00:32:59.160 --> 00:33:01.600]   and instead use a sparse negative reward.
[00:33:01.600 --> 00:33:03.600]   So the snake could have something like a stamina.
[00:33:03.600 --> 00:33:08.720]   So it needs to find the next food in n by n steps,
[00:33:08.720 --> 00:33:10.880]   where n is the size of the board.
[00:33:10.880 --> 00:33:12.880]   And if it doesn't, then it dies, and it gets
[00:33:12.880 --> 00:33:14.360]   the reward of negative 1.
[00:33:14.360 --> 00:33:16.840]   That's sometimes a useful strategy in situations
[00:33:16.840 --> 00:33:22.320]   like that, so that it doesn't learn that, hey,
[00:33:22.320 --> 00:33:23.960]   by the time I take the next 100 steps,
[00:33:23.960 --> 00:33:26.000]   I'm already going to have a reward of negative 1,
[00:33:26.000 --> 00:33:27.360]   so I may as well just give up.
[00:33:27.360 --> 00:33:30.160]   Another way of mitigating some of this,
[00:33:30.160 --> 00:33:32.720]   especially along long time horizons,
[00:33:32.720 --> 00:33:35.680]   is to use something called a discount factor, where
[00:33:35.680 --> 00:33:41.000]   you don't just look at the max Q value of all the future steps,
[00:33:41.000 --> 00:33:44.320]   but you're looking at a discounted max Q value.
[00:33:44.320 --> 00:33:49.120]   So something that happens 1,000, 2,000, 3,000 steps down the line
[00:33:49.120 --> 00:33:53.040]   actually has a very small impact on what you're going to do now,
[00:33:53.040 --> 00:33:56.800]   because that reward recursively gets discounted
[00:33:56.800 --> 00:33:59.680]   through the Bellman update.
[00:33:59.680 --> 00:34:02.960]   And that sort of punishment doesn't show up
[00:34:02.960 --> 00:34:08.760]   as much as the immediate future reward that you're going to get.
[00:34:08.760 --> 00:34:09.400]   Cool.
[00:34:09.400 --> 00:34:12.840]   I have a question that's kind of tied to this.
[00:34:12.840 --> 00:34:15.440]   Could reinforcement learning algorithms
[00:34:15.440 --> 00:34:17.720]   learn to optimize for pain in the short term
[00:34:17.720 --> 00:34:21.720]   so they can get a bigger reward in the long term?
[00:34:21.720 --> 00:34:22.400]   Yes.
[00:34:22.400 --> 00:34:27.360]   And that's the-- well--
[00:34:27.360 --> 00:34:33.560]   Amita has to get into the scales a little bit.
[00:34:33.560 --> 00:34:35.280]   The general answer is yes, but a lot of it
[00:34:35.280 --> 00:34:36.720]   comes down to the specific problem.
[00:34:36.720 --> 00:34:38.600]   So in the case of Snake--
[00:34:38.600 --> 00:34:40.520]   excuse me one moment--
[00:34:40.520 --> 00:34:44.360]   in the case of Snake, you're essentially doing that, right?
[00:34:44.360 --> 00:34:46.640]   When you have to take a really long route
[00:34:46.640 --> 00:34:50.080]   to get to the food so that you don't die three steps from now,
[00:34:50.080 --> 00:34:52.840]   it's essentially doing the same thing.
[00:34:52.840 --> 00:34:55.480]   But beyond that, I think a lot just comes down
[00:34:55.480 --> 00:34:58.520]   to the dynamics of that specific environment and the reward
[00:34:58.520 --> 00:35:00.760]   structure that's being used.

