
[00:00:00.000 --> 00:00:02.820]   The following is a conversation with Francis Collins,
[00:00:02.820 --> 00:00:06.920]   director of the NIH, the National Institutes of Health,
[00:00:06.920 --> 00:00:10.320]   appointed and reappointed to the role by three presidents,
[00:00:10.320 --> 00:00:13.160]   Obama, Trump, and Biden.
[00:00:13.160 --> 00:00:16.680]   He oversees 27 separate institutes and centers,
[00:00:16.680 --> 00:00:20.840]   including NIAID, which makes him Anthony Fauci's boss.
[00:00:20.840 --> 00:00:23.240]   At the NIH, Francis helped launch
[00:00:23.240 --> 00:00:25.920]   and led a huge number of projects
[00:00:25.920 --> 00:00:29.400]   that pushed the frontiers of science, health, and medicine,
[00:00:29.400 --> 00:00:32.880]   including one of my favorites, the BRAIN Initiative,
[00:00:32.880 --> 00:00:34.680]   that seeks to map the human brain
[00:00:34.680 --> 00:00:37.360]   and understand how the function arises
[00:00:37.360 --> 00:00:39.600]   from neural circuitry.
[00:00:39.600 --> 00:00:43.220]   Before the NIH, Francis led the Human Genome Project,
[00:00:43.220 --> 00:00:45.800]   one of the largest and most ambitious efforts
[00:00:45.800 --> 00:00:47.900]   in the history of science.
[00:00:47.900 --> 00:00:52.720]   Given all that, Francis is a humble, thoughtful, kind man.
[00:00:52.720 --> 00:00:54.240]   And because of this, to me,
[00:00:54.240 --> 00:00:58.320]   he's one of the best representatives of science in the world.
[00:00:58.320 --> 00:00:59.800]   He's a man of God,
[00:00:59.800 --> 00:01:03.160]   and yet also a friend of the late Christopher Hitchens,
[00:01:03.160 --> 00:01:04.880]   who called him, quote,
[00:01:04.880 --> 00:01:08.280]   "One of the greatest living Americans."
[00:01:08.280 --> 00:01:10.360]   This is the Lex Friedman Podcast.
[00:01:10.360 --> 00:01:11.200]   To support it,
[00:01:11.200 --> 00:01:13.560]   please check out our sponsors in the description.
[00:01:13.560 --> 00:01:17.420]   And now, here's my conversation with Francis Collins.
[00:01:17.420 --> 00:01:21.320]   Science at its best is a source of hope.
[00:01:21.320 --> 00:01:23.480]   So for me, it's been difficult to watch,
[00:01:23.480 --> 00:01:25.760]   as it has during the pandemic,
[00:01:25.760 --> 00:01:28.200]   become at times a source of division.
[00:01:28.200 --> 00:01:30.900]   What I would love to do in this conversation with you
[00:01:30.900 --> 00:01:32.920]   is touch some difficult topics
[00:01:32.920 --> 00:01:35.160]   and do so with empathy and humility
[00:01:35.160 --> 00:01:39.040]   so that we may begin to regain a sense of trust in science,
[00:01:39.040 --> 00:01:41.980]   and that it may once again become a source of hope.
[00:01:41.980 --> 00:01:43.200]   I hope that's okay with you.
[00:01:43.200 --> 00:01:45.020]   - I love the goal.
[00:01:45.020 --> 00:01:47.620]   - Let's start with some hard questions.
[00:01:47.620 --> 00:01:49.120]   You called for, quote,
[00:01:49.120 --> 00:01:51.920]   "Thorough, expert-driven, and objective inquiry
[00:01:51.920 --> 00:01:54.240]   "into the origins of COVID-19."
[00:01:54.240 --> 00:01:55.520]   So let me ask,
[00:01:55.520 --> 00:01:58.400]   is there a reasonable chance that COVID-19 leaked
[00:01:58.400 --> 00:01:59.240]   from a lab?
[00:01:59.240 --> 00:02:01.920]   - I can't exclude that.
[00:02:01.920 --> 00:02:04.140]   I think it's fairly unlikely.
[00:02:04.140 --> 00:02:07.720]   I wish we had more ability to be able to ask questions
[00:02:07.720 --> 00:02:09.040]   of the Chinese government
[00:02:09.040 --> 00:02:11.820]   and learn more about what kind of records
[00:02:11.820 --> 00:02:13.360]   might have been in the lab
[00:02:13.360 --> 00:02:15.600]   that we've never been able to see.
[00:02:15.600 --> 00:02:19.560]   But most likely, this was a natural origin of a virus,
[00:02:19.560 --> 00:02:20.880]   probably starting in a bat,
[00:02:20.880 --> 00:02:23.240]   perhaps traveling through some other intermediate,
[00:02:23.240 --> 00:02:25.160]   yet to be identified host,
[00:02:25.160 --> 00:02:27.440]   and finding its way into humans.
[00:02:27.440 --> 00:02:29.880]   - Is answering this question within the realm of science,
[00:02:29.880 --> 00:02:31.920]   do you think, will we ever know?
[00:02:31.920 --> 00:02:35.560]   - I think we might know if we find that intermediate host.
[00:02:35.560 --> 00:02:39.560]   And there has not yet been a thorough enough investigation
[00:02:39.560 --> 00:02:41.880]   to say that that's not going to happen.
[00:02:41.880 --> 00:02:44.680]   And remember, it takes a while to do this.
[00:02:44.680 --> 00:02:47.620]   With SARS, it was 14 years before we figured out
[00:02:47.620 --> 00:02:50.760]   it was the civet cat that was the intermediate host.
[00:02:50.760 --> 00:02:52.280]   With MERS, it was a little quicker
[00:02:52.280 --> 00:02:54.060]   to discover it was the camel.
[00:02:54.060 --> 00:02:56.820]   With SARS-CoV-2, there's been some looking,
[00:02:56.820 --> 00:02:59.800]   but especially now, with everything really tense
[00:02:59.800 --> 00:03:01.480]   between the US and China,
[00:03:01.480 --> 00:03:02.600]   if there's looking going on,
[00:03:02.600 --> 00:03:05.000]   we're not getting told about it.
[00:03:05.000 --> 00:03:06.640]   - Do you think it's a scientific question
[00:03:06.640 --> 00:03:08.320]   or a political question?
[00:03:08.320 --> 00:03:09.680]   - It's a scientific question,
[00:03:09.680 --> 00:03:12.320]   but it has political implications.
[00:03:12.320 --> 00:03:14.560]   - So the world is full of scientists
[00:03:14.560 --> 00:03:16.040]   that are working together,
[00:03:16.040 --> 00:03:17.240]   but in the political space,
[00:03:17.240 --> 00:03:21.320]   in the political science space, there's tensions.
[00:03:21.320 --> 00:03:23.820]   What is it like to do great science
[00:03:23.820 --> 00:03:27.860]   in a time of a pandemic when there's political tensions?
[00:03:27.860 --> 00:03:29.960]   - It's very unfortunate.
[00:03:29.960 --> 00:03:34.100]   Pasteur said science knows no one country.
[00:03:34.100 --> 00:03:35.820]   He was right about that.
[00:03:35.820 --> 00:03:39.420]   My whole career, in genetics especially,
[00:03:39.420 --> 00:03:42.320]   has depended upon international collaboration
[00:03:42.320 --> 00:03:44.580]   between scientists as a way
[00:03:44.580 --> 00:03:46.300]   to make discoveries, get things done.
[00:03:46.300 --> 00:03:48.620]   Scientists, by their nature,
[00:03:48.620 --> 00:03:52.200]   like to be involved in international collaborations.
[00:03:52.200 --> 00:03:54.800]   The Human Genome Project, for heaven's sake,
[00:03:54.800 --> 00:03:58.860]   2,400 scientists in six countries working together,
[00:03:58.860 --> 00:04:00.520]   not worrying who was gonna get the credit,
[00:04:00.520 --> 00:04:02.240]   giving all the data away.
[00:04:02.240 --> 00:04:03.600]   I was the person who was supposed
[00:04:03.600 --> 00:04:04.960]   to keep all that coordinated.
[00:04:04.960 --> 00:04:07.120]   It was a wonderful experience.
[00:04:07.120 --> 00:04:08.380]   And that included China.
[00:04:08.380 --> 00:04:10.640]   That was sort of their first real entry
[00:04:10.640 --> 00:04:14.600]   into a big international, big science kind of project,
[00:04:14.600 --> 00:04:15.940]   and they did their part.
[00:04:15.940 --> 00:04:18.520]   It's very different now.
[00:04:19.100 --> 00:04:21.840]   - Continuing the line of difficult questions,
[00:04:21.840 --> 00:04:26.580]   especially difficult ethical questions.
[00:04:26.580 --> 00:04:30.740]   In 2014, US put a hold on gain-of-function research
[00:04:30.740 --> 00:04:34.060]   in response to a number of laboratory biosecurity incidents,
[00:04:34.060 --> 00:04:37.000]   including anthrax, smallpox, and influenza.
[00:04:37.000 --> 00:04:42.000]   In December 2017, NIH lifted this ban because, quote,
[00:04:42.000 --> 00:04:44.020]   "Gain-of-function research is important
[00:04:44.020 --> 00:04:46.620]   "in helping us identify, understand,
[00:04:46.620 --> 00:04:49.040]   "and develop strategies and effective countermeasures
[00:04:49.040 --> 00:04:50.920]   "against rapidly evolving pathogens
[00:04:50.920 --> 00:04:53.120]   "that pose a threat to public health."
[00:04:53.120 --> 00:04:57.800]   All difficult questions have arguments on both sides.
[00:04:57.800 --> 00:04:59.800]   Can you argue the pros and cons
[00:04:59.800 --> 00:05:02.560]   of gain-of-function research with viruses?
[00:05:02.560 --> 00:05:04.960]   - I can.
[00:05:04.960 --> 00:05:09.000]   And first, let me say this term, gain-of-function,
[00:05:09.000 --> 00:05:12.360]   is causing such confusion that I need to take a minute
[00:05:12.360 --> 00:05:16.260]   and just sort of talk about what the common scientific
[00:05:16.260 --> 00:05:19.800]   use of that term is and where it is very different
[00:05:19.800 --> 00:05:22.680]   when we're talking about the current oversight
[00:05:22.680 --> 00:05:25.320]   of potentially dangerous human pathogens.
[00:05:25.320 --> 00:05:29.360]   As you know, in science, we're doing gain-of-function
[00:05:29.360 --> 00:05:31.000]   experiments all the time.
[00:05:31.000 --> 00:05:36.240]   We support a lot of cancer immunotherapy at NIH.
[00:05:36.240 --> 00:05:37.960]   Right here in our clinical center,
[00:05:37.960 --> 00:05:41.000]   there are trials going on where people's immune cells
[00:05:41.000 --> 00:05:44.400]   are taken out of their body, treated with a genetic therapy
[00:05:44.400 --> 00:05:47.380]   that revs up their ability to discover the cancer
[00:05:47.380 --> 00:05:50.620]   that that patient currently has, maybe even at stage four,
[00:05:50.620 --> 00:05:54.940]   and then give them back as those little ninja warriors
[00:05:54.940 --> 00:05:56.140]   go after the cancer.
[00:05:56.140 --> 00:05:58.740]   And it sometimes works dramatically.
[00:05:58.740 --> 00:06:00.180]   That's gain-of-function.
[00:06:00.180 --> 00:06:02.800]   You gave that patient a gain in their immune function
[00:06:02.800 --> 00:06:04.640]   that may have saved their life.
[00:06:04.640 --> 00:06:05.860]   So we gotta be careful not to say,
[00:06:05.860 --> 00:06:08.020]   "Oh, gain-of-function is bad."
[00:06:08.020 --> 00:06:11.600]   Most of what we do in science that's good
[00:06:11.600 --> 00:06:14.180]   involves quite a bit of that.
[00:06:14.180 --> 00:06:16.320]   And we are all living with gains-of-function every day.
[00:06:16.320 --> 00:06:17.960]   I have a gain-of-function 'cause I'm wearing
[00:06:17.960 --> 00:06:21.000]   these eyeglasses, otherwise I would not be seeing you
[00:06:21.000 --> 00:06:22.020]   as clearly.
[00:06:22.020 --> 00:06:24.740]   I'm happy for that gain-of-function.
[00:06:24.740 --> 00:06:27.800]   So that's where a lot of confusion has happened.
[00:06:27.800 --> 00:06:30.580]   The kind of gain-of-function which is now subject
[00:06:30.580 --> 00:06:35.440]   to very rigorous and very carefully defined oversight
[00:06:35.440 --> 00:06:38.880]   is when you are working with an established human pathogen
[00:06:38.880 --> 00:06:42.740]   that is known to be potentially causing a pandemic,
[00:06:42.740 --> 00:06:46.720]   and you are enhancing or potentially enhancing
[00:06:46.720 --> 00:06:49.940]   its transmissibility or its virulence.
[00:06:49.940 --> 00:06:54.940]   We call that EPPP, Enhanced Potential Pandemic Pathogen.
[00:06:54.940 --> 00:07:01.040]   That requires this very stringent oversight,
[00:07:01.040 --> 00:07:03.480]   worked out over three years
[00:07:03.480 --> 00:07:08.240]   by the National Science Advisory Board on Biosecurity
[00:07:08.240 --> 00:07:11.340]   that needs to be looked at by a panel
[00:07:11.340 --> 00:07:13.720]   that goes well beyond NIH to decide,
[00:07:13.720 --> 00:07:17.160]   are the benefits worth the risks in that situation?
[00:07:17.160 --> 00:07:21.320]   Most of the time, it's not worth the risk.
[00:07:21.320 --> 00:07:25.120]   Only three times in the last three or four years
[00:07:25.120 --> 00:07:28.320]   have experiments been given permission to go forward.
[00:07:28.320 --> 00:07:29.800]   They were all on influenza.
[00:07:29.800 --> 00:07:33.160]   So I will argue that if you're worried
[00:07:33.160 --> 00:07:35.600]   about the next pandemic,
[00:07:35.600 --> 00:07:37.920]   the more you know about the coming enemy,
[00:07:37.920 --> 00:07:40.040]   the better chance you have to recognize
[00:07:40.040 --> 00:07:41.960]   when trouble is starting.
[00:07:41.960 --> 00:07:44.400]   And so if you can do it safely,
[00:07:44.400 --> 00:07:48.840]   studying influenza or coronaviruses like SARS, MERS,
[00:07:48.840 --> 00:07:51.780]   and SARS-CoV-2 would be a good thing
[00:07:51.780 --> 00:07:53.200]   to be able to know about,
[00:07:53.200 --> 00:07:55.400]   but you have to be able to do it safely
[00:07:55.400 --> 00:07:59.160]   because we all know lab accidents can happen.
[00:07:59.160 --> 00:08:02.640]   I mean, look at SARS where there have been lab accidents
[00:08:02.640 --> 00:08:05.080]   and people who have gotten sick as a result.
[00:08:05.080 --> 00:08:06.760]   We don't want to take that chance
[00:08:06.760 --> 00:08:08.800]   unless there's a compelling scientific reason.
[00:08:08.800 --> 00:08:13.360]   That's why we have this very stringent oversight.
[00:08:13.360 --> 00:08:15.000]   The experiments being done
[00:08:15.000 --> 00:08:17.680]   at the Wuhan Institute of Virology
[00:08:17.680 --> 00:08:22.680]   as a subaward to our grant to EcoHealth in New York
[00:08:22.680 --> 00:08:24.840]   did not meet that standard
[00:08:24.840 --> 00:08:27.520]   of requiring that kind of stringent oversight.
[00:08:27.520 --> 00:08:29.160]   I want to be really clear about that
[00:08:29.160 --> 00:08:32.160]   'cause there's been so much thrown around about it.
[00:08:32.160 --> 00:08:33.560]   Was it gain of function?
[00:08:33.560 --> 00:08:36.080]   Well, in the standard use of that term
[00:08:36.080 --> 00:08:38.000]   that you would use in science in general,
[00:08:38.000 --> 00:08:38.960]   you might say it was,
[00:08:38.960 --> 00:08:41.720]   but in the use of that term that applies
[00:08:41.720 --> 00:08:46.200]   to this very specific example
[00:08:46.200 --> 00:08:50.160]   of a potential pandemic pathogen, absolutely not.
[00:08:50.160 --> 00:08:54.040]   So nothing went on there that should not have happened
[00:08:54.040 --> 00:08:55.800]   based upon the oversight.
[00:08:55.800 --> 00:08:59.920]   There was an instance where the grantee institution
[00:08:59.920 --> 00:09:02.960]   failed to notify us about the result of an experiment
[00:09:02.960 --> 00:09:04.120]   that they were supposed to tell us
[00:09:04.120 --> 00:09:08.000]   where they mixed and matched some viral genomes
[00:09:08.000 --> 00:09:11.760]   and got a somewhat larger viral load as a result,
[00:09:11.760 --> 00:09:13.960]   but it was not EPPP.
[00:09:13.960 --> 00:09:16.680]   It was not getting into that zone
[00:09:16.680 --> 00:09:19.240]   that would have required this higher level of scrutiny.
[00:09:19.240 --> 00:09:20.920]   It was all bat viruses.
[00:09:20.920 --> 00:09:22.920]   These were not human pathogens.
[00:09:22.920 --> 00:09:25.840]   - So they didn't cross a threshold
[00:09:25.840 --> 00:09:29.160]   within that gray area that makes for an EPPP?
[00:09:29.160 --> 00:09:30.360]   - They did not.
[00:09:30.360 --> 00:09:33.040]   And anybody who's willing to take the time
[00:09:33.040 --> 00:09:37.040]   to look at what EPPP means and what those experiments were
[00:09:37.040 --> 00:09:39.080]   would have to agree with what I just said.
[00:09:39.080 --> 00:09:40.640]   - What is the biggest reason
[00:09:40.640 --> 00:09:42.000]   it didn't cross that threshold?
[00:09:42.000 --> 00:09:45.440]   Is it because it wasn't jumping to humans?
[00:09:45.440 --> 00:09:48.560]   Is it because it did not have a sufficient increase
[00:09:48.560 --> 00:09:49.960]   in virulence or transmissibility?
[00:09:49.960 --> 00:09:51.480]   What's your sense?
[00:09:51.480 --> 00:09:54.640]   - EPPP only applies to agents
[00:09:54.640 --> 00:09:59.640]   that are known human pathogens of pandemic potential.
[00:10:00.760 --> 00:10:04.560]   These were all bat viruses derived in the wild,
[00:10:04.560 --> 00:10:07.240]   not shown to be infectious to humans.
[00:10:07.240 --> 00:10:08.520]   Just looking at what happened
[00:10:08.520 --> 00:10:10.640]   if you took four different bat viruses
[00:10:10.640 --> 00:10:13.480]   and you tried moving the spike protein gene
[00:10:13.480 --> 00:10:15.120]   from one into one of the others
[00:10:15.120 --> 00:10:18.960]   to see whether it would bind better to the ACE2 receptor.
[00:10:18.960 --> 00:10:21.080]   That doesn't get across that threshold.
[00:10:21.080 --> 00:10:22.320]   And let me also say,
[00:10:22.320 --> 00:10:25.280]   for those who are trying to connect the dots here,
[00:10:25.280 --> 00:10:27.320]   which is the most troubling part of this,
[00:10:27.320 --> 00:10:31.080]   and say, well, this is how SARS-CoV-2 got started,
[00:10:31.080 --> 00:10:34.640]   that is absolutely demonstrably false.
[00:10:34.640 --> 00:10:37.560]   These bat viruses that were being studied
[00:10:37.560 --> 00:10:41.240]   had only about 80% similarity in their genomes
[00:10:41.240 --> 00:10:42.400]   to SARS-CoV-2.
[00:10:42.400 --> 00:10:45.800]   They were like decades away in evolutionary terms.
[00:10:45.800 --> 00:10:48.000]   And it is really irresponsible for people
[00:10:48.000 --> 00:10:49.080]   to claim otherwise.
[00:10:49.080 --> 00:10:54.760]   - Speaking of people who claim otherwise,
[00:10:54.760 --> 00:10:58.480]   Rand Paul, what do you make of the battle of words
[00:10:58.480 --> 00:11:01.840]   between Senator Rand Paul and Dr. Anthony Fauci
[00:11:01.840 --> 00:11:03.560]   over this particular point?
[00:11:03.560 --> 00:11:07.200]   - I don't want to talk about specific members of Congress,
[00:11:07.200 --> 00:11:09.440]   but I will say it's really unfortunate
[00:11:09.440 --> 00:11:12.440]   that Tony Fauci, who is the epitome
[00:11:12.440 --> 00:11:14.800]   of a dedicated public servant,
[00:11:14.800 --> 00:11:19.200]   has now somehow been targeted for political reasons
[00:11:19.200 --> 00:11:23.440]   as somebody that certain figures are trying to discredit,
[00:11:23.440 --> 00:11:26.440]   perhaps to try to distract from their own failings.
[00:11:26.440 --> 00:11:28.280]   This never should have happened.
[00:11:28.280 --> 00:11:31.960]   Here's a person who's dedicated his whole life
[00:11:31.960 --> 00:11:35.560]   to trying to prevent illnesses from infectious diseases,
[00:11:35.560 --> 00:11:38.720]   including HIV, in the 1980s and '90s,
[00:11:38.720 --> 00:11:42.440]   and now probably the most knowledgeable
[00:11:42.440 --> 00:11:45.160]   infectious disease physician in the world,
[00:11:45.160 --> 00:11:48.200]   and also a really good communicator,
[00:11:48.200 --> 00:11:50.200]   is out there telling the truth
[00:11:50.200 --> 00:11:52.680]   about where we are with SARS-CoV-2
[00:11:52.680 --> 00:11:55.520]   to certain political figures who don't want to hear it,
[00:11:55.520 --> 00:11:58.600]   and who are therefore determined to discredit him,
[00:11:58.600 --> 00:12:00.360]   and that is disgraceful.
[00:12:00.360 --> 00:12:01.760]   - So with politicians,
[00:12:01.760 --> 00:12:04.640]   they often play games with black and white.
[00:12:04.640 --> 00:12:09.640]   They try to sort of use the gray areas of science
[00:12:09.640 --> 00:12:12.000]   and then paint their own picture.
[00:12:12.000 --> 00:12:14.960]   But I have a question about the gray areas of science.
[00:12:14.960 --> 00:12:18.280]   So like you mentioned, gain of function is a term
[00:12:18.280 --> 00:12:20.640]   that has very specific scientific meaning,
[00:12:20.640 --> 00:12:23.280]   but it also has a more general term.
[00:12:23.280 --> 00:12:26.320]   And it's very possible to argue that the,
[00:12:26.320 --> 00:12:28.880]   not to argue, not the way politicians argue,
[00:12:28.880 --> 00:12:31.120]   but just as human beings and scientists,
[00:12:31.120 --> 00:12:34.840]   that there was a gain of function achieved
[00:12:34.840 --> 00:12:37.880]   at the Wuhan Institute of Virology,
[00:12:37.880 --> 00:12:39.720]   but it didn't cross a threshold.
[00:12:39.720 --> 00:12:43.480]   I mean, there's a, it's a, but it could have too.
[00:12:43.480 --> 00:12:44.520]   So here's the thing.
[00:12:44.520 --> 00:12:47.240]   When you do these kinds of experiments,
[00:12:47.240 --> 00:12:50.320]   unexpected results may be achieved,
[00:12:50.320 --> 00:12:52.360]   and that's the gray area of science.
[00:12:52.360 --> 00:12:55.180]   You're taking risks with such experiments.
[00:12:55.180 --> 00:13:00.920]   And I am very uncomfortable that we can't discuss
[00:13:00.920 --> 00:13:03.640]   the uncertainty in the gray area of this.
[00:13:03.640 --> 00:13:06.240]   - Oh, I'm comfortable discussing the gray area.
[00:13:06.240 --> 00:13:08.820]   What I'm uncomfortable with is people deciding
[00:13:08.820 --> 00:13:12.400]   to define for themselves what that threshold is
[00:13:12.400 --> 00:13:14.840]   based on sort of some political argument.
[00:13:14.840 --> 00:13:18.740]   The threshold was very explicitly laid out.
[00:13:18.740 --> 00:13:21.700]   Everybody agreed to that in the basis
[00:13:21.700 --> 00:13:23.760]   of this three years of deliberation.
[00:13:23.760 --> 00:13:24.760]   So that's what it is.
[00:13:24.760 --> 00:13:27.200]   If that threshold needs to be reconsidered,
[00:13:27.200 --> 00:13:30.380]   let's reconsider it, but let's not try to take
[00:13:30.380 --> 00:13:32.440]   an experiment that's already been done
[00:13:32.440 --> 00:13:35.160]   and decide that the threshold isn't what it was,
[00:13:35.160 --> 00:13:37.680]   'cause that really is doing a disservice
[00:13:37.680 --> 00:13:38.880]   to the whole process.
[00:13:38.880 --> 00:13:40.160]   - I wish there was a discussion,
[00:13:40.160 --> 00:13:43.040]   even in response to Rand Paul,
[00:13:43.040 --> 00:13:45.160]   and I know we're not talking about specific senators,
[00:13:45.160 --> 00:13:48.360]   but just that particular case, I'm saying stuff here.
[00:13:48.360 --> 00:13:51.040]   I wish there was an opportunity to talk about,
[00:13:51.040 --> 00:13:54.960]   given the current threshold, this is not gain of function,
[00:13:54.960 --> 00:13:56.860]   but maybe we need to reconsider the threshold
[00:13:56.860 --> 00:13:58.480]   and have an actual, that's an opportunity
[00:13:58.480 --> 00:14:01.160]   for a discussion about the ethics of gain of function.
[00:14:01.160 --> 00:14:03.000]   You said that there was three studies
[00:14:03.000 --> 00:14:05.340]   that passed that threshold with influenza.
[00:14:05.340 --> 00:14:07.200]   That's a fascinating human question,
[00:14:07.200 --> 00:14:09.280]   scientific question about ethics,
[00:14:09.280 --> 00:14:13.880]   because like you said, there's pros and cons.
[00:14:13.880 --> 00:14:16.720]   You're taking risks here to prevent
[00:14:18.120 --> 00:14:21.640]   horribly destructive viruses in the future,
[00:14:21.640 --> 00:14:25.000]   but you also are risking creating
[00:14:25.000 --> 00:14:26.880]   such viruses in the future.
[00:14:26.880 --> 00:14:29.180]   With nuclear weapons and nuclear energy,
[00:14:29.180 --> 00:14:35.160]   nuclear energy promises a lot of positive effects,
[00:14:35.160 --> 00:14:37.200]   and yet you're taking risks here.
[00:14:37.200 --> 00:14:39.840]   With mutually assured destruction,
[00:14:39.840 --> 00:14:41.920]   nations possessing nuclear weapons.
[00:14:41.920 --> 00:14:42.760]   - Oh my.
[00:14:42.760 --> 00:14:45.080]   I hope we're not going there.
[00:14:45.080 --> 00:14:47.840]   - Well, we're not, but a lot of people argue
[00:14:47.840 --> 00:14:49.040]   that that's the reason we've,
[00:14:49.040 --> 00:14:52.400]   nuclear weapons is the reason we've prevented world wars,
[00:14:52.400 --> 00:14:56.520]   and yet they also have the risk of starting world wars.
[00:14:56.520 --> 00:14:59.240]   And this is what we have to be honest about
[00:14:59.240 --> 00:15:01.760]   with the benefits and risks of science,
[00:15:01.760 --> 00:15:04.480]   that you have to make that calculation.
[00:15:04.480 --> 00:15:06.160]   What are the pros and what are the cons?
[00:15:06.160 --> 00:15:09.380]   - I'm totally with you, but I want to reassure you, Lex,
[00:15:09.380 --> 00:15:12.000]   that this is not an issue that's been ignored.
[00:15:12.000 --> 00:15:12.840]   - Yes.
[00:15:12.840 --> 00:15:15.360]   - That this issue about the kind of gain of function
[00:15:15.360 --> 00:15:18.360]   that might result in a serious human pathogen
[00:15:18.360 --> 00:15:21.640]   has been front and center in many deliberations
[00:15:21.640 --> 00:15:23.080]   for a decade or more,
[00:15:23.080 --> 00:15:26.140]   involved a lot of my time along the way, by the way,
[00:15:26.140 --> 00:15:29.560]   and has been discussed publicly on multiple occasions,
[00:15:29.560 --> 00:15:31.640]   including two major meetings
[00:15:31.640 --> 00:15:34.220]   of the National Academy of Sciences,
[00:15:34.220 --> 00:15:35.800]   getting input from everybody,
[00:15:35.800 --> 00:15:38.520]   and ultimately arriving at our current framework.
[00:15:38.520 --> 00:15:43.360]   Now, we actually, back in January of 2020,
[00:15:43.360 --> 00:15:46.680]   just before COVID-19 changed everything,
[00:15:46.680 --> 00:15:48.560]   had planned and even charged
[00:15:48.560 --> 00:15:53.560]   that same National Science Advisory Board on Biosecurity
[00:15:53.560 --> 00:15:57.360]   to reconvene and look at the current framework and say,
[00:15:57.360 --> 00:15:58.600]   "Do we have it right?
[00:15:58.600 --> 00:16:01.180]   "Let's look at the experience over those three years
[00:16:01.180 --> 00:16:05.160]   "and say, is the threshold too easy, too hard?
[00:16:05.160 --> 00:16:06.280]   "Do we need to reconsider it?
[00:16:06.280 --> 00:16:07.920]   "Let's look at the experience."
[00:16:07.920 --> 00:16:10.460]   COVID came along, the members of the board said,
[00:16:10.460 --> 00:16:12.300]   "Please, we're all infectious disease experts.
[00:16:12.300 --> 00:16:14.120]   "We don't have time for this right now,
[00:16:14.120 --> 00:16:16.600]   "but I think the time is right to do this."
[00:16:16.600 --> 00:16:18.140]   I'm totally supportive of that,
[00:16:18.140 --> 00:16:20.280]   and that should be just as public a discussion
[00:16:20.280 --> 00:16:23.000]   as you can imagine about what are the benefits and the risks.
[00:16:23.000 --> 00:16:25.900]   And if somebody decided, ultimately,
[00:16:25.900 --> 00:16:26.860]   this came together and said,
[00:16:26.860 --> 00:16:28.680]   "We just shouldn't be doing these experiments
[00:16:28.680 --> 00:16:30.000]   "under any circumstances,"
[00:16:30.000 --> 00:16:31.320]   if that was the conclusion,
[00:16:31.320 --> 00:16:32.760]   well, that would be the conclusion,
[00:16:32.760 --> 00:16:34.200]   but it hasn't been so far.
[00:16:34.200 --> 00:16:36.640]   - If we can briefly look out
[00:16:36.640 --> 00:16:39.200]   into the next hundred years on this.
[00:16:40.820 --> 00:16:44.280]   I apologize for the existential questions,
[00:16:44.280 --> 00:16:47.380]   but it seems obvious to me
[00:16:47.380 --> 00:16:52.220]   that as gain-of-function type of research and development
[00:16:52.220 --> 00:16:54.000]   becomes easier and cheaper,
[00:16:54.000 --> 00:16:58.060]   it will become greater and greater risk.
[00:16:58.060 --> 00:17:01.580]   So if it doesn't no longer need to be contained
[00:17:01.580 --> 00:17:04.900]   within laboratories of high security,
[00:17:04.900 --> 00:17:08.540]   it feels like this is one of the greatest threats
[00:17:08.540 --> 00:17:10.500]   facing human civilization.
[00:17:10.500 --> 00:17:12.880]   Do you worry that at some point in the future,
[00:17:12.880 --> 00:17:14.680]   a leaked man-made virus
[00:17:14.680 --> 00:17:18.640]   may destroy most of human civilization?
[00:17:18.640 --> 00:17:20.680]   - I do worry about the risks.
[00:17:20.680 --> 00:17:24.420]   And at the moment where we have the greatest control,
[00:17:24.420 --> 00:17:26.040]   the greatest oversight,
[00:17:26.040 --> 00:17:29.080]   is when this is federally funded research.
[00:17:29.080 --> 00:17:30.280]   But as you're alluding,
[00:17:30.280 --> 00:17:33.160]   there's no reason to imagine that's the only place
[00:17:33.160 --> 00:17:35.800]   that this kind of activity would go on.
[00:17:35.800 --> 00:17:40.800]   If there was an evil source that wished to create a virus
[00:17:40.800 --> 00:17:43.540]   that was highly pathogenic in their garage,
[00:17:43.540 --> 00:17:46.580]   the technology does get easier.
[00:17:46.580 --> 00:17:50.500]   And there is no international oversight about this either
[00:17:50.500 --> 00:17:52.580]   that you could say has the same stringency
[00:17:52.580 --> 00:17:54.920]   as what we have in the United States.
[00:17:54.920 --> 00:17:58.100]   So yes, that is a concern.
[00:17:58.100 --> 00:18:03.020]   It would take a seriously deranged group or person
[00:18:03.020 --> 00:18:05.460]   to undertake this on purpose,
[00:18:05.460 --> 00:18:08.420]   given the likelihood that they too would go down.
[00:18:08.420 --> 00:18:13.420]   We don't imagine there are going to be bioweapons
[00:18:13.420 --> 00:18:15.700]   that only kill your enemies and don't kill you.
[00:18:15.700 --> 00:18:18.340]   Sorry, we're too much alike for that to work.
[00:18:18.340 --> 00:18:23.100]   So I don't see it as an imminent risk.
[00:18:23.100 --> 00:18:27.580]   There's lots of scary novels and movies written about it,
[00:18:27.580 --> 00:18:30.940]   but I do think it's something we have to consider.
[00:18:30.940 --> 00:18:32.980]   What are all the things that ought to be watched?
[00:18:32.980 --> 00:18:36.340]   You may not know that if somebody is ordering
[00:18:36.340 --> 00:18:41.340]   a particular oligonucleotide from one of the main suppliers
[00:18:41.340 --> 00:18:45.800]   and it happens to match smallpox, they're gonna get caught.
[00:18:45.800 --> 00:18:50.220]   So there is effort underway to try to track
[00:18:50.220 --> 00:18:52.660]   any nefarious actions that might be going on.
[00:18:52.660 --> 00:18:54.100]   - In the United States or internationally?
[00:18:54.100 --> 00:18:55.780]   Is there an international collaboration
[00:18:55.780 --> 00:18:57.340]   of trying to track this stuff?
[00:18:57.340 --> 00:18:58.180]   - There is some.
[00:18:58.180 --> 00:18:59.620]   I wish it were stronger.
[00:19:00.480 --> 00:19:03.400]   - This is a general issue, Lex, in terms of,
[00:19:03.400 --> 00:19:05.800]   do we have a mechanism, particularly when it comes
[00:19:05.800 --> 00:19:09.680]   to ethical issues, to be able to decide what's allowable
[00:19:09.680 --> 00:19:11.360]   and what's not and enforce it?
[00:19:11.360 --> 00:19:14.200]   I mean, look where we are with germline genome editing
[00:19:14.200 --> 00:19:15.640]   for humans, for instance.
[00:19:15.640 --> 00:19:17.600]   There is no enforcement mechanism.
[00:19:17.600 --> 00:19:19.920]   There's just bully pulpits and governments
[00:19:19.920 --> 00:19:21.880]   that get to decide for themselves.
[00:19:21.880 --> 00:19:23.080]   - So you talked about evil.
[00:19:23.080 --> 00:19:24.440]   What about incompetence?
[00:19:24.440 --> 00:19:25.360]   Does that worry you?
[00:19:25.360 --> 00:19:27.180]   I was born in the Soviet Union.
[00:19:28.040 --> 00:19:31.000]   My dad, a physicist, worked at Chernobyl.
[00:19:31.000 --> 00:19:32.440]   That comes to mind.
[00:19:32.440 --> 00:19:33.640]   That wasn't evil.
[00:19:33.640 --> 00:19:36.420]   That was, I don't know what word you wanna put it.
[00:19:36.420 --> 00:19:38.400]   Maybe incompetence is too harsh.
[00:19:38.400 --> 00:19:40.980]   Maybe it's the inherent incompetence of bureaucracy.
[00:19:40.980 --> 00:19:41.820]   I don't know.
[00:19:41.820 --> 00:19:43.880]   But for whatever reason, there was an accident.
[00:19:43.880 --> 00:19:45.120]   Does that worry you?
[00:19:45.120 --> 00:19:46.800]   - Of course it does.
[00:19:46.800 --> 00:19:50.220]   We know that SARS, for instance, did manage to leak
[00:19:50.220 --> 00:19:53.400]   out of a lab in China two or three times,
[00:19:53.400 --> 00:19:55.720]   and at least in some instances, people died.
[00:19:55.720 --> 00:19:57.580]   Unfortunately, quickly contained.
[00:19:57.580 --> 00:20:00.680]   All one can do in that circumstance,
[00:20:00.680 --> 00:20:04.320]   because you need to study the virus and understand it
[00:20:04.320 --> 00:20:07.560]   in order to keep it from causing a broader pandemic,
[00:20:07.560 --> 00:20:11.100]   but you need to insist upon the kind of biosecurity,
[00:20:11.100 --> 00:20:14.080]   the BSL two, three, and four framework
[00:20:14.080 --> 00:20:17.080]   under which those experiments have to be done.
[00:20:17.080 --> 00:20:20.660]   And certainly at NIH, we're extremely rigorous about that,
[00:20:20.660 --> 00:20:23.640]   but you can't count on every human being
[00:20:23.640 --> 00:20:26.200]   to always do exactly what they're supposed to.
[00:20:26.200 --> 00:20:28.560]   So there's a risk there, which is another reason
[00:20:28.560 --> 00:20:32.000]   why if we're contemplating supporting research
[00:20:32.000 --> 00:20:35.000]   on pathogens that might be the next pandemic,
[00:20:35.000 --> 00:20:36.440]   you have to factor that in,
[00:20:36.440 --> 00:20:39.240]   not just whether people are gonna do something
[00:20:39.240 --> 00:20:40.520]   that we couldn't have predicted,
[00:20:40.520 --> 00:20:42.280]   where all of a sudden they created a virus
[00:20:42.280 --> 00:20:44.280]   that's much worse without knowing they were gonna do that,
[00:20:44.280 --> 00:20:46.400]   but also just having an accident.
[00:20:46.400 --> 00:20:49.980]   That's in the mix when those estimates are done
[00:20:49.980 --> 00:20:52.080]   about whether the risk is worth it or not.
[00:20:53.080 --> 00:20:56.360]   - Continuing on line of difficult questions.
[00:20:56.360 --> 00:20:57.400]   (laughing)
[00:20:57.400 --> 00:20:58.920]   - We're gonna get to fun stuff after a while.
[00:20:58.920 --> 00:21:00.800]   - We will soon, I promise.
[00:21:00.800 --> 00:21:06.820]   You are the director of the NIH.
[00:21:06.820 --> 00:21:11.920]   You are Dr. Anthony Fauci's, technically his boss.
[00:21:11.920 --> 00:21:12.760]   - Yep.
[00:21:12.760 --> 00:21:14.440]   - You have stood behind him.
[00:21:14.440 --> 00:21:16.080]   You have supported him,
[00:21:16.080 --> 00:21:18.540]   just like you did already in this conversation.
[00:21:18.540 --> 00:21:21.920]   It is painful for me to see division and distrust,
[00:21:21.920 --> 00:21:25.240]   but many people in politics and elsewhere
[00:21:25.240 --> 00:21:27.960]   have called for Anthony Fauci to be fired.
[00:21:27.960 --> 00:21:31.280]   When there's such calls of distrust in public
[00:21:31.280 --> 00:21:33.240]   about a leader like Anthony Fauci,
[00:21:33.240 --> 00:21:37.700]   who should garner trust, do you think he should be fired?
[00:21:37.700 --> 00:21:39.180]   - Absolutely not.
[00:21:39.180 --> 00:21:45.440]   To do so would be basically to give the opportunity
[00:21:45.440 --> 00:21:50.320]   for those who wanna make up stories about anybody
[00:21:50.320 --> 00:21:51.600]   to destroy them.
[00:21:51.600 --> 00:21:55.160]   There is nothing in the ways in which Tony Fauci
[00:21:55.160 --> 00:21:58.540]   has been targeted that is based upon truth.
[00:21:58.540 --> 00:22:04.720]   How could we then accept those cries for his firing
[00:22:04.720 --> 00:22:07.000]   as having legitimacy?
[00:22:07.000 --> 00:22:08.320]   It's a circular argument.
[00:22:08.320 --> 00:22:10.500]   They've decided they don't like Tony,
[00:22:10.500 --> 00:22:13.600]   so they make up stuff and they twist comments
[00:22:13.600 --> 00:22:15.960]   that he's made about things like gain of function,
[00:22:15.960 --> 00:22:19.440]   where he's referring to the very specific gain of function
[00:22:19.440 --> 00:22:21.560]   that's covered by this policy.
[00:22:21.560 --> 00:22:24.360]   And they're trying to say he lied to the Congress.
[00:22:24.360 --> 00:22:26.280]   That's simply not true.
[00:22:26.280 --> 00:22:28.920]   They don't like the fact that Tony changes
[00:22:28.920 --> 00:22:33.240]   the medical recommendations about what to do with COVID-19
[00:22:33.240 --> 00:22:35.640]   over the space of more than a year.
[00:22:35.640 --> 00:22:37.920]   And they call that flip-flopping and you can't trust the guy
[00:22:37.920 --> 00:22:40.320]   'cause he says one thing last year and one thing this year.
[00:22:40.320 --> 00:22:42.640]   Well, the science has changed.
[00:22:42.640 --> 00:22:44.860]   Delta variant has changed everything.
[00:22:44.860 --> 00:22:47.080]   You don't want him to be saying the same thing
[00:22:47.080 --> 00:22:48.040]   he did a year ago.
[00:22:48.040 --> 00:22:49.060]   That would be wrong now.
[00:22:49.060 --> 00:22:50.520]   It was the best we could do then.
[00:22:50.520 --> 00:22:52.000]   People don't understand that
[00:22:52.000 --> 00:22:54.800]   or else they don't want to understand that.
[00:22:54.800 --> 00:22:59.600]   So when you basically whip up a largely political argument
[00:22:59.600 --> 00:23:03.280]   against a scientist and hammer at it over and over again
[00:23:03.280 --> 00:23:06.640]   to the point where he now has to have 24/7 security
[00:23:06.640 --> 00:23:08.100]   to protect him against people
[00:23:08.100 --> 00:23:10.560]   who really want to do violence to him,
[00:23:10.560 --> 00:23:13.520]   for that to be a reason to say that then he should be fired
[00:23:13.520 --> 00:23:17.020]   is to hand the evil forces the victory.
[00:23:17.020 --> 00:23:18.260]   I will not do that.
[00:23:19.260 --> 00:23:21.540]   (sighs)
[00:23:21.540 --> 00:23:23.580]   - Yet there's something difficult
[00:23:23.580 --> 00:23:26.180]   I'm going to try to express to you.
[00:23:26.180 --> 00:23:28.720]   So it may be your guitar playing.
[00:23:28.720 --> 00:23:33.580]   It may be something else, but there's a humility to you.
[00:23:33.580 --> 00:23:36.260]   It may be because you're a man of God.
[00:23:36.260 --> 00:23:41.260]   There's a humility to you that garners trust.
[00:23:41.260 --> 00:23:47.980]   And when you're in a leadership position,
[00:23:47.980 --> 00:23:49.220]   representing science,
[00:23:49.220 --> 00:23:52.640]   especially in catastrophic events like the pandemic,
[00:23:52.640 --> 00:23:55.540]   it feels like as a leader,
[00:23:55.540 --> 00:24:00.540]   you have to go far above and beyond your usual duties.
[00:24:00.540 --> 00:24:02.680]   And I think there's no question
[00:24:02.680 --> 00:24:06.980]   that Anthony Fauci has delivered on his duties,
[00:24:06.980 --> 00:24:09.740]   but it feels like he needs to go above
[00:24:09.740 --> 00:24:11.080]   as a science communicator.
[00:24:11.080 --> 00:24:13.340]   And if there's a large number of people
[00:24:13.340 --> 00:24:15.240]   that are distrusting him,
[00:24:17.620 --> 00:24:20.780]   it's also his responsibility to garner their trust,
[00:24:20.780 --> 00:24:22.000]   to gain their trust.
[00:24:22.000 --> 00:24:26.860]   As a person who's the face of science,
[00:24:26.860 --> 00:24:28.180]   are you torn on this?
[00:24:28.180 --> 00:24:31.340]   The responsibility of Anthony Fauci, of yourself,
[00:24:31.340 --> 00:24:33.600]   to represent science,
[00:24:33.600 --> 00:24:37.860]   not just the communication of advising what should be done,
[00:24:37.860 --> 00:24:42.860]   but giving people hope, giving people trust in science,
[00:24:42.860 --> 00:24:45.420]   and alleviating division.
[00:24:45.420 --> 00:24:48.060]   Do you think that's also a responsibility of a leader,
[00:24:48.060 --> 00:24:49.480]   or is that unfair to ask?
[00:24:49.480 --> 00:24:52.660]   - I think the best way you give people trust
[00:24:52.660 --> 00:24:54.260]   is to tell them the truth.
[00:24:54.260 --> 00:24:57.580]   And so they recognize that when you're sharing information,
[00:24:57.580 --> 00:24:59.320]   it's the best you've got at that point.
[00:24:59.320 --> 00:25:02.900]   And Tony Fauci does that at every moment.
[00:25:02.900 --> 00:25:06.340]   I don't think him expressing more humility
[00:25:06.340 --> 00:25:09.220]   would change the fact that they're looking for a target
[00:25:09.220 --> 00:25:11.300]   of somebody to blame,
[00:25:11.300 --> 00:25:13.460]   to basically distract people
[00:25:13.460 --> 00:25:16.820]   from the failings of their own political party.
[00:25:16.820 --> 00:25:20.380]   Maybe I'm less targeted, not because of a difference
[00:25:20.380 --> 00:25:23.060]   in the way in which I convey the information,
[00:25:23.060 --> 00:25:24.820]   I'm less visible.
[00:25:24.820 --> 00:25:28.700]   If Tony were out of the scene and I was placed in that role,
[00:25:28.700 --> 00:25:31.700]   I'd probably be seeing a ratcheting up
[00:25:31.700 --> 00:25:33.000]   of that same targeting.
[00:25:33.000 --> 00:25:39.460]   - I would like to believe that if Tony Fauci said
[00:25:39.460 --> 00:25:42.680]   that when I originally made recommendations
[00:25:42.680 --> 00:25:44.600]   not to wear masks,
[00:25:44.600 --> 00:25:47.780]   that was given on our best available data,
[00:25:47.780 --> 00:25:50.440]   and now we know that is a mistake.
[00:25:50.440 --> 00:25:53.060]   So admit with humility that there's an error.
[00:25:53.060 --> 00:25:56.900]   That's not actually correct,
[00:25:56.900 --> 00:25:59.940]   but that's a statement of humility.
[00:25:59.940 --> 00:26:04.380]   And I would like to believe, despite the attacks,
[00:26:04.380 --> 00:26:06.980]   he would win a lot of people over with that.
[00:26:06.980 --> 00:26:09.100]   So a lot of people, as you're saying,
[00:26:09.100 --> 00:26:11.820]   would use that, see that, here we go,
[00:26:11.820 --> 00:26:15.140]   here's that Dr. Anthony Fauci making mistakes.
[00:26:15.140 --> 00:26:16.820]   How can we trust him on anything?
[00:26:16.820 --> 00:26:18.520]   I believe if he was,
[00:26:18.520 --> 00:26:25.120]   that public display of humility to say that I made an error,
[00:26:25.120 --> 00:26:28.060]   that would win a lot of people over.
[00:26:28.060 --> 00:26:31.980]   That's kind of my sense,
[00:26:31.980 --> 00:26:34.980]   to face the fire of the attacks on politics,
[00:26:34.980 --> 00:26:37.900]   like politicians will attack no matter what.
[00:26:37.900 --> 00:26:39.580]   But the question is the people,
[00:26:40.620 --> 00:26:41.740]   to win over the people.
[00:26:41.740 --> 00:26:44.180]   The biggest concern I've had
[00:26:44.180 --> 00:26:47.820]   is that there was this distrust of science
[00:26:47.820 --> 00:26:49.540]   that's been brewing.
[00:26:49.540 --> 00:26:51.980]   And maybe you can correct me,
[00:26:51.980 --> 00:26:54.580]   but I'm a little bit unwilling to fully blame
[00:26:54.580 --> 00:26:55.900]   the politicians,
[00:26:55.900 --> 00:26:58.620]   'cause politicians play their games no matter what.
[00:26:58.620 --> 00:27:02.800]   It just feels like this was an opportunity
[00:27:02.800 --> 00:27:05.320]   to inspire people with the power of science.
[00:27:05.320 --> 00:27:07.500]   The development of the vaccines,
[00:27:07.500 --> 00:27:09.880]   no matter what you think of those vaccines,
[00:27:09.880 --> 00:27:11.500]   is one of the greatest accomplishments
[00:27:11.500 --> 00:27:12.660]   in the history of science.
[00:27:12.660 --> 00:27:13.720]   - It is indeed.
[00:27:13.720 --> 00:27:17.140]   - And the fact that that's not inspiring,
[00:27:17.140 --> 00:27:18.900]   listen, I host a podcast.
[00:27:18.900 --> 00:27:21.200]   Whenever I say positive stuff about the vaccine,
[00:27:21.200 --> 00:27:23.980]   I get to hear a lot of different opinions.
[00:27:23.980 --> 00:27:24.820]   - I bet you do.
[00:27:24.820 --> 00:27:28.400]   - The fact that I do is a big problem to me,
[00:27:28.400 --> 00:27:30.360]   because it's an incredible,
[00:27:30.360 --> 00:27:33.320]   an incredible accomplishment of science.
[00:27:33.320 --> 00:27:36.460]   And so I'm sorry,
[00:27:36.460 --> 00:27:40.020]   but I have to put responsibility on the leaders,
[00:27:40.020 --> 00:27:42.420]   even if it's not their mistakes.
[00:27:42.420 --> 00:27:43.800]   That's what the leadership is.
[00:27:43.800 --> 00:27:44.840]   That's what leadership is.
[00:27:44.840 --> 00:27:47.300]   You take responsibility for the situation.
[00:27:47.300 --> 00:27:48.700]   I wonder if there's something
[00:27:48.700 --> 00:27:50.760]   that could have been done better
[00:27:50.760 --> 00:27:54.480]   to give people hope
[00:27:54.480 --> 00:27:55.980]   that science will save us
[00:27:55.980 --> 00:27:58.220]   as opposed to science will divide us.
[00:27:58.220 --> 00:28:03.440]   - I think you have more confidence
[00:28:03.440 --> 00:28:06.900]   in the ability to get beyond our current divisions
[00:28:06.900 --> 00:28:09.700]   than I do after seeing just how deep
[00:28:09.700 --> 00:28:12.040]   and dark they have become.
[00:28:12.040 --> 00:28:14.660]   Tony Fauci has said multiple times
[00:28:14.660 --> 00:28:17.620]   the recommendation about not wearing masks
[00:28:17.620 --> 00:28:19.380]   was for two reasons,
[00:28:19.380 --> 00:28:22.380]   a shortage of masks, which were needed in hospitals,
[00:28:22.380 --> 00:28:25.140]   and a lack of realization early
[00:28:25.140 --> 00:28:27.180]   in the course of the epidemic
[00:28:27.180 --> 00:28:28.620]   that this was a virus
[00:28:28.620 --> 00:28:32.540]   that could heavily infect asymptomatic people.
[00:28:33.520 --> 00:28:35.280]   Has that changed?
[00:28:35.280 --> 00:28:36.480]   He changed.
[00:28:36.480 --> 00:28:37.600]   Now, did he make an error?
[00:28:37.600 --> 00:28:39.440]   No, he was making a judgment
[00:28:39.440 --> 00:28:41.720]   based on the data available at the time,
[00:28:41.720 --> 00:28:45.240]   but he certainly made that clear over and over again.
[00:28:45.240 --> 00:28:48.400]   It has not stopped those who would like to demonize him
[00:28:48.400 --> 00:28:50.400]   from saying, "Well, he just flip-flopped.
[00:28:50.400 --> 00:28:52.580]   "You can't trust a guy.
[00:28:52.580 --> 00:28:54.980]   "He says one thing today and one thing tomorrow."
[00:28:54.980 --> 00:28:58.320]   - Well, masks is a tricky one.
[00:28:58.320 --> 00:29:00.080]   So I'm actually-- - It is a tricky one.
[00:29:00.080 --> 00:29:02.140]   - Early on, I'm a co-author on a paper,
[00:29:02.140 --> 00:29:04.880]   one of many, but this was a survey paper
[00:29:04.880 --> 00:29:08.400]   overlooking the evidence.
[00:29:08.400 --> 00:29:10.320]   It's a summary of the evidence we have
[00:29:10.320 --> 00:29:12.420]   for the effectiveness of masks.
[00:29:12.420 --> 00:29:15.640]   It seems that it's difficult
[00:29:15.640 --> 00:29:18.480]   to do rigorous scientific study on masks.
[00:29:18.480 --> 00:29:19.880]   - It is difficult.
[00:29:19.880 --> 00:29:22.040]   - There's a lot of philosophical and ethical questions
[00:29:22.040 --> 00:29:22.880]   I want to ask you.
[00:29:22.880 --> 00:29:24.320]   Well, within this,
[00:29:24.320 --> 00:29:29.520]   it's back to your words and Anthony Fauci's words.
[00:29:30.560 --> 00:29:33.780]   When you're dealing with so much uncertainty
[00:29:33.780 --> 00:29:36.100]   and so much potential uncertainty
[00:29:36.100 --> 00:29:39.240]   about how catastrophic this virus is in the early days,
[00:29:39.240 --> 00:29:44.900]   and knowing that each word you say may create panic,
[00:29:44.900 --> 00:29:48.480]   how do you communicate science with the world?
[00:29:48.480 --> 00:29:53.520]   It's a philosophical, it's an ethical,
[00:29:53.520 --> 00:29:55.640]   it's a practical question.
[00:29:55.640 --> 00:29:59.280]   There was a discussion about masks a century ago
[00:29:59.280 --> 00:30:01.280]   and that too led to panic.
[00:30:01.280 --> 00:30:08.160]   So, I mean, I'm trying to put myself in your mind
[00:30:08.160 --> 00:30:10.400]   and the mind of Anthony Fauci in those early days,
[00:30:10.400 --> 00:30:13.160]   knowing that there's limited supply of masks.
[00:30:13.160 --> 00:30:15.040]   Like, what do you say?
[00:30:15.040 --> 00:30:18.880]   Do you fully convey the uncertainty of the situation,
[00:30:18.880 --> 00:30:22.840]   of the challenges of the supply chain?
[00:30:22.840 --> 00:30:24.940]   Or do you say that masks don't work?
[00:30:26.680 --> 00:30:29.680]   That's a complicated calculation.
[00:30:29.680 --> 00:30:31.380]   How do you make that calculation?
[00:30:31.380 --> 00:30:35.400]   - It is a complicated calculation.
[00:30:35.400 --> 00:30:39.080]   As a scientist, your temptation would be
[00:30:39.080 --> 00:30:43.360]   to give a full brain dump of all the details
[00:30:43.360 --> 00:30:45.920]   of the information about what's known and what isn't known
[00:30:45.920 --> 00:30:47.760]   and what experiments need to be done.
[00:30:47.760 --> 00:30:51.000]   Most of the time, that's not gonna play well
[00:30:51.000 --> 00:30:53.280]   in a soundbite on the evening news.
[00:30:53.280 --> 00:30:56.040]   So you have to kind of distill it down to a recommendation
[00:30:56.040 --> 00:30:58.600]   that is the best you can do at that time
[00:30:58.600 --> 00:31:00.200]   with the information you've got.
[00:31:00.200 --> 00:31:03.400]   - So you're a man of God.
[00:31:03.400 --> 00:31:05.480]   And we'll return to that to talk about
[00:31:05.480 --> 00:31:09.860]   some also unanswerable philosophical questions.
[00:31:09.860 --> 00:31:13.160]   But first, let's linger on the vaccine
[00:31:13.160 --> 00:31:16.640]   because in the religious, in the Christian community,
[00:31:16.640 --> 00:31:18.960]   there was some hesitancy with the vaccine.
[00:31:18.960 --> 00:31:19.880]   - Still is.
[00:31:19.880 --> 00:31:20.720]   - Still is.
[00:31:20.720 --> 00:31:24.160]   There's a lot of data showing high efficacy
[00:31:24.160 --> 00:31:27.760]   and safety of vaccines, of COVID vaccines,
[00:31:27.760 --> 00:31:31.840]   but still they are far from perfect as all vaccines are.
[00:31:31.840 --> 00:31:33.880]   Can you empathize with people who are hesitant
[00:31:33.880 --> 00:31:35.320]   to take the COVID vaccine
[00:31:35.320 --> 00:31:38.960]   or to have their children take the COVID vaccine?
[00:31:38.960 --> 00:31:41.520]   - I can totally empathize,
[00:31:41.520 --> 00:31:43.280]   especially when people are barraged
[00:31:43.280 --> 00:31:45.360]   by conflicting information coming at them
[00:31:45.360 --> 00:31:47.100]   from all kinds of directions.
[00:31:47.100 --> 00:31:50.600]   I've spent a lot of my time in the last year
[00:31:50.600 --> 00:31:54.000]   trying to figure out how to do a better job of listening
[00:31:54.000 --> 00:31:58.920]   because I think we have all got the risk
[00:31:58.920 --> 00:32:03.200]   of assuming we know the basis for somebody's hesitancy.
[00:32:03.200 --> 00:32:07.600]   And that often doesn't turn out to be what you thought.
[00:32:07.600 --> 00:32:11.640]   And the variety of reasons is quite broad.
[00:32:11.640 --> 00:32:16.840]   I think a big concern is just this sense of uncertainty
[00:32:16.840 --> 00:32:19.240]   about whether this was done too fast
[00:32:19.240 --> 00:32:20.640]   and that corners were cut.
[00:32:20.640 --> 00:32:23.280]   And there are good answers to that.
[00:32:23.280 --> 00:32:28.200]   Along with that, a sense that maybe this vaccine
[00:32:28.200 --> 00:32:30.640]   will have long-term effects that we won't know about
[00:32:30.640 --> 00:32:32.340]   for years to come.
[00:32:32.340 --> 00:32:35.840]   And one can say that hasn't been seen with other vaccines.
[00:32:35.840 --> 00:32:37.480]   And there's no particular reason to think
[00:32:37.480 --> 00:32:38.800]   this one's going to be different
[00:32:38.800 --> 00:32:41.100]   than the dozens of others that we have experience with.
[00:32:41.100 --> 00:32:43.200]   But you can't absolutely say,
[00:32:43.200 --> 00:32:44.740]   no, there's no chance of that.
[00:32:46.040 --> 00:32:49.320]   So it does come down to listening
[00:32:49.320 --> 00:32:54.320]   and then trying in a fashion that doesn't convey a message
[00:32:54.320 --> 00:32:58.320]   that you're smarter than the person you're talking to
[00:32:58.320 --> 00:32:59.760]   'cause that isn't gonna help
[00:32:59.760 --> 00:33:03.980]   to really address what the substance is of the concerns.
[00:33:03.980 --> 00:33:07.720]   But my heart goes out to so many people
[00:33:07.720 --> 00:33:11.480]   who are fearful about this because of all the information
[00:33:11.480 --> 00:33:13.820]   that has been dumped on them.
[00:33:14.980 --> 00:33:18.540]   Some of it by politicians, a lot of it by the internet,
[00:33:18.540 --> 00:33:20.920]   some of it by parts of the media
[00:33:20.920 --> 00:33:25.920]   that seem to take pleasure in stirring up this kind of fear
[00:33:25.920 --> 00:33:29.520]   for their own reasons.
[00:33:29.520 --> 00:33:31.440]   And that is shameful.
[00:33:31.440 --> 00:33:34.160]   I'm really sympathetic with the people
[00:33:34.160 --> 00:33:36.680]   who are confused and fearful.
[00:33:36.680 --> 00:33:38.400]   I am not sympathetic with people
[00:33:38.400 --> 00:33:41.720]   who are distributing information that's demonstrably false
[00:33:41.720 --> 00:33:43.480]   and continue to do so.
[00:33:43.480 --> 00:33:45.460]   They're taking lives.
[00:33:45.460 --> 00:33:50.460]   I didn't realize how strong that sector of disinformation
[00:33:50.460 --> 00:33:53.320]   would be.
[00:33:53.320 --> 00:33:56.740]   And it's been in many ways more effective
[00:33:56.740 --> 00:33:58.640]   than the means of spreading the truth.
[00:33:58.640 --> 00:34:02.200]   This is gonna take us into another place.
[00:34:02.200 --> 00:34:06.040]   But Lex, if there's something I'm really worried about
[00:34:06.040 --> 00:34:08.500]   in this country, and it's not just this country,
[00:34:08.500 --> 00:34:10.080]   but it's the one I live in,
[00:34:10.080 --> 00:34:14.480]   is that we have another epidemic besides COVID-19,
[00:34:14.480 --> 00:34:18.500]   and it's an epidemic of the loss of the anchor of truth.
[00:34:18.500 --> 00:34:23.240]   The truth as a means of making decisions,
[00:34:23.240 --> 00:34:25.560]   truth as a means of figuring out
[00:34:25.560 --> 00:34:28.720]   how to wrestle with a question like,
[00:34:28.720 --> 00:34:32.080]   should I get this vaccine for myself or my children,
[00:34:32.080 --> 00:34:34.460]   seems to have lost its primacy.
[00:34:34.460 --> 00:34:38.840]   And instead, it's an opinion of somebody
[00:34:38.840 --> 00:34:42.600]   who expressed it very strongly,
[00:34:42.600 --> 00:34:46.760]   or some Facebook post that I read two hours ago.
[00:34:46.760 --> 00:34:52.700]   And for those to become substitutes for objective truth,
[00:34:52.700 --> 00:34:56.760]   not just, of course, for vaccines,
[00:34:56.760 --> 00:34:58.080]   but for many other issues,
[00:34:58.080 --> 00:35:01.560]   like was the 2020 election actually fair?
[00:35:01.560 --> 00:35:04.180]   This worries me deeply.
[00:35:05.040 --> 00:35:08.840]   It's bad enough to have polarization and divisions,
[00:35:08.840 --> 00:35:11.680]   but to have no way of resolving those
[00:35:11.680 --> 00:35:13.840]   by actually saying, okay, what's true here,
[00:35:13.840 --> 00:35:17.440]   makes me very worried about the path we're on.
[00:35:17.440 --> 00:35:18.920]   And I'm usually an optimist.
[00:35:18.920 --> 00:35:22.880]   - Well, to give you an optimistic angle on this,
[00:35:22.880 --> 00:35:26.880]   I actually think that this sense
[00:35:26.880 --> 00:35:29.720]   that there's no one place for truth
[00:35:29.720 --> 00:35:33.480]   is just a thing that will inspire leaders
[00:35:33.480 --> 00:35:35.680]   and science communicators to speak,
[00:35:35.680 --> 00:35:37.280]   not from a place of authority,
[00:35:37.280 --> 00:35:39.120]   but from a place of humility.
[00:35:39.120 --> 00:35:41.080]   I think it's just challenging people
[00:35:41.080 --> 00:35:45.040]   to communicate in a new way, to be listeners first.
[00:35:45.040 --> 00:35:47.680]   I think the problem isn't that
[00:35:47.680 --> 00:35:49.860]   there's a lot of misinformation.
[00:35:49.860 --> 00:35:54.500]   I think that people,
[00:35:54.500 --> 00:36:00.520]   the internet and the world are distrustful
[00:36:00.680 --> 00:36:04.880]   of people who speak as if they possess the truth
[00:36:04.880 --> 00:36:08.160]   with an authoritarian kind of tone,
[00:36:08.160 --> 00:36:10.120]   which was, I think, defining
[00:36:10.120 --> 00:36:12.520]   for what science was in the 20th century.
[00:36:12.520 --> 00:36:15.320]   I just think it has to sound different in the 21st.
[00:36:15.320 --> 00:36:21.280]   In the battle of ideas, I think humility and love wins.
[00:36:21.280 --> 00:36:24.680]   And that's how science wins,
[00:36:24.680 --> 00:36:27.140]   not through having quote-unquote truth.
[00:36:27.140 --> 00:36:30.840]   'Cause now everybody can just say, "I have the truth."
[00:36:30.840 --> 00:36:34.140]   I think you have to speak, like I said,
[00:36:34.140 --> 00:36:35.500]   from humility, not authority.
[00:36:35.500 --> 00:36:39.100]   And so it just challenges our leaders to go back
[00:36:39.100 --> 00:36:43.260]   and learn to be, pardon my French, less assholes
[00:36:43.260 --> 00:36:45.540]   and more kind.
[00:36:45.540 --> 00:36:47.940]   And like you said, to listen,
[00:36:47.940 --> 00:36:51.460]   to listen to the experiences of people that are good people,
[00:36:51.460 --> 00:36:53.620]   not the ones who are trying to manipulate the system
[00:36:53.620 --> 00:36:55.140]   or play a game and so on,
[00:36:55.140 --> 00:36:59.300]   but real people who are just afraid of uncertainty,
[00:36:59.300 --> 00:37:02.540]   of hurting those they loved and so on.
[00:37:02.540 --> 00:37:04.320]   So I think it's just an opportunity for leaders
[00:37:04.320 --> 00:37:07.420]   to go back and take a class on effective communication.
[00:37:07.420 --> 00:37:13.020]   - I'm with you on shifting more from where we are
[00:37:13.020 --> 00:37:14.180]   to humility and love.
[00:37:14.180 --> 00:37:15.460]   That's gotta be the right answer.
[00:37:15.460 --> 00:37:17.100]   That's very biblical, by the way.
[00:37:17.100 --> 00:37:19.280]   - We'll get there.
[00:37:19.280 --> 00:37:22.460]   I have to bring up Joe Rogan.
[00:37:22.460 --> 00:37:24.100]   I don't know if you know who he is.
[00:37:24.100 --> 00:37:24.940]   - I do.
[00:37:24.940 --> 00:37:27.780]   - He's a podcaster, comedian, fighting commentator,
[00:37:27.780 --> 00:37:30.340]   and my now friend.
[00:37:30.340 --> 00:37:32.700]   - And Ivermectin believer too.
[00:37:32.700 --> 00:37:35.700]   - Yes, that is the question I have to ask you about.
[00:37:35.700 --> 00:37:39.020]   He has gotten some flack in the mainstream media
[00:37:39.020 --> 00:37:40.860]   for not getting vaccinated.
[00:37:40.860 --> 00:37:42.820]   And when he got COVID recently,
[00:37:42.820 --> 00:37:46.940]   taking Ivermectin as part of a cocktail of treatments.
[00:37:46.940 --> 00:37:50.540]   The NIH actually has a nice page on Ivermectin saying,
[00:37:50.540 --> 00:37:53.740]   quote, "There's insufficient evidence to recommend
[00:37:53.740 --> 00:37:57.180]   either for or against the use of Ivermectin
[00:37:57.180 --> 00:37:59.340]   for the treatment of COVID-19.
[00:37:59.340 --> 00:38:02.580]   Results from adequately powered, well-designed,
[00:38:02.580 --> 00:38:04.800]   and well-conducted clinical trials are needed
[00:38:04.800 --> 00:38:07.700]   to provide more specific evidence-based guidance
[00:38:07.700 --> 00:38:11.380]   on the role of Ivermectin in the treatment of COVID-19."
[00:38:11.380 --> 00:38:14.660]   So let me ask, why do you think there has been
[00:38:14.660 --> 00:38:18.400]   so much attack on Joe Rogan and anyone else
[00:38:18.400 --> 00:38:20.260]   that's talking about Ivermectin
[00:38:20.260 --> 00:38:23.400]   when there's insufficient evidence for or against?
[00:38:24.400 --> 00:38:26.600]   - Well, let's unpack that.
[00:38:26.600 --> 00:38:28.680]   First of all, I think the concerns about Joe
[00:38:28.680 --> 00:38:32.080]   are not limited to his taking Ivermectin.
[00:38:32.080 --> 00:38:35.680]   Much more seriously, his being fairly publicly negative
[00:38:35.680 --> 00:38:39.060]   about vaccines at a time where people are dying.
[00:38:39.060 --> 00:38:43.160]   700,000 people have died from COVID-19.
[00:38:43.160 --> 00:38:47.160]   Estimates by Kaiser are at least 100,000 of those
[00:38:47.160 --> 00:38:50.400]   were unnecessary deaths of unvaccinated people.
[00:38:50.400 --> 00:38:52.760]   And for Joe to promote that further,
[00:38:52.760 --> 00:38:57.760]   even as this pandemic rages through our population,
[00:38:57.760 --> 00:39:00.680]   is simply irresponsible.
[00:39:00.680 --> 00:39:03.020]   So yeah, the Ivermectin is just one other twist.
[00:39:03.020 --> 00:39:05.920]   Obviously, Ivermectin has been controversial
[00:39:05.920 --> 00:39:07.360]   for months and months.
[00:39:07.360 --> 00:39:10.440]   The reason that it got particular attention
[00:39:10.440 --> 00:39:12.400]   is because of the way in which it seemed
[00:39:12.400 --> 00:39:16.320]   to have captured the imagination of a lot of people,
[00:39:16.320 --> 00:39:18.500]   and to the point where they were taking doses
[00:39:18.500 --> 00:39:20.960]   that were intended for livestock.
[00:39:20.960 --> 00:39:23.000]   And some of them got pretty sick as a result
[00:39:23.000 --> 00:39:25.160]   from overdosing on this stuff.
[00:39:25.160 --> 00:39:26.680]   That was not good judgment.
[00:39:26.680 --> 00:39:31.560]   The drug itself remains uncertain.
[00:39:31.560 --> 00:39:35.360]   There's a recent review that looks at all of the studies
[00:39:35.360 --> 00:39:38.300]   of Ivermectin and basically concludes
[00:39:38.300 --> 00:39:40.600]   that it probably doesn't work.
[00:39:40.600 --> 00:39:42.160]   We are running a study right now.
[00:39:42.160 --> 00:39:44.040]   I looked at that data this morning
[00:39:44.040 --> 00:39:48.240]   in a trial called ACTIV6, which is one of the ones
[00:39:48.240 --> 00:39:51.000]   that my public-private partnership is running.
[00:39:51.000 --> 00:39:53.920]   We're up to about 400 patients who've been randomized
[00:39:53.920 --> 00:39:57.720]   to Ivermectin or placebo, and should know,
[00:39:57.720 --> 00:39:59.480]   perhaps as soon as a month from now,
[00:39:59.480 --> 00:40:01.900]   in a very carefully controlled trial,
[00:40:01.900 --> 00:40:03.640]   did it help or did it not?
[00:40:03.640 --> 00:40:06.160]   So there will be an answer.
[00:40:06.160 --> 00:40:09.520]   Coming back to Joe, again, I don't think,
[00:40:09.520 --> 00:40:11.160]   the fact that he took Ivermectin,
[00:40:11.160 --> 00:40:14.700]   hoping it might work, is that big a knock against him.
[00:40:14.700 --> 00:40:19.300]   It's more the conveying of, we don't trust what science says,
[00:40:19.300 --> 00:40:21.100]   which is vaccines are gonna save your life.
[00:40:21.100 --> 00:40:22.900]   We're gonna trust what's on the internet
[00:40:22.900 --> 00:40:25.140]   that says Ivermectin and hydroxychloroquine
[00:40:25.140 --> 00:40:27.280]   really do work, even though the scientific community
[00:40:27.280 --> 00:40:28.380]   says probably not.
[00:40:28.380 --> 00:40:31.220]   - So let me push back on that a little bit.
[00:40:31.220 --> 00:40:35.380]   So he doesn't say, let's not listen to science.
[00:40:35.380 --> 00:40:38.460]   He doesn't say don't get vaccinated.
[00:40:38.460 --> 00:40:44.060]   He says it's okay to ask questions.
[00:40:44.060 --> 00:40:44.980]   - I'm okay with that.
[00:40:44.980 --> 00:40:48.900]   - How risky is the vaccine for certain populations?
[00:40:48.900 --> 00:40:51.860]   What are the benefits and risks?
[00:40:51.860 --> 00:40:55.900]   There's other friends of Joe and friends of mine,
[00:40:55.900 --> 00:40:59.820]   like Sam Harris, who says, if you look at the data,
[00:40:59.820 --> 00:41:03.180]   it's obvious that the benefits outweigh the risks.
[00:41:03.180 --> 00:41:05.700]   And what Joe says is, yes,
[00:41:05.700 --> 00:41:09.500]   but let's still openly talk about risks.
[00:41:09.500 --> 00:41:12.060]   And he often brings up anecdotal evidence
[00:41:12.060 --> 00:41:17.060]   of people who've had highly negative effects from vaccines.
[00:41:17.060 --> 00:41:20.340]   Science is not done with anecdotal evidence.
[00:41:20.340 --> 00:41:23.500]   And so you could infer a lot of stuff
[00:41:23.500 --> 00:41:24.900]   from the way he expresses it,
[00:41:24.900 --> 00:41:27.700]   but he also communicates a lot of interesting questions.
[00:41:27.700 --> 00:41:31.740]   And that's something maybe you can comment on is,
[00:41:31.740 --> 00:41:34.980]   there's certain groups that are healthy.
[00:41:34.980 --> 00:41:39.980]   They're younger, they exercise a lot,
[00:41:39.980 --> 00:41:43.420]   they get nutrition and all those kinds of things.
[00:41:43.420 --> 00:41:48.420]   He shows skepticism on whether it's so obvious
[00:41:48.420 --> 00:41:50.020]   that they should get vaccinated.
[00:41:50.020 --> 00:41:52.980]   And the same is he makes this,
[00:41:52.980 --> 00:41:57.260]   he kind of presents the same kind of skepticism for kids,
[00:41:57.260 --> 00:41:58.380]   for young kids.
[00:41:58.380 --> 00:42:03.380]   So with empathy and listening my Russian ineloquent
[00:42:03.380 --> 00:42:09.260]   description of what Joe believes,
[00:42:09.260 --> 00:42:12.220]   what is your kind of response to that?
[00:42:12.220 --> 00:42:16.260]   Why should certain categories of healthy and young people
[00:42:16.260 --> 00:42:18.180]   still get vaccinated, do you think?
[00:42:18.180 --> 00:42:19.180]   - Well, first, just to say,
[00:42:19.180 --> 00:42:22.140]   it's great for Joe to be a skeptic, to ask questions.
[00:42:22.140 --> 00:42:23.780]   We should all be doing that.
[00:42:23.780 --> 00:42:26.580]   But then the next step is to go and see what the data says
[00:42:26.580 --> 00:42:29.540]   and see if there are actually answers to those questions.
[00:42:29.540 --> 00:42:31.380]   So coming to healthy people,
[00:42:31.380 --> 00:42:35.180]   I've done a bunch of podcasts besides this one.
[00:42:35.180 --> 00:42:37.700]   The one I think I remember most
[00:42:37.700 --> 00:42:42.700]   was a podcast with a worldwide wrestling superstar.
[00:42:42.700 --> 00:42:43.780]   - Very nice.
[00:42:43.780 --> 00:42:48.140]   - He's about six foot six and just absolutely solid muscle.
[00:42:48.140 --> 00:42:51.360]   And he got COVID and he almost died.
[00:42:51.360 --> 00:42:54.540]   And recovering from that, he said,
[00:42:54.540 --> 00:42:58.460]   "I've got to let my supporters know."
[00:42:58.460 --> 00:43:00.980]   'Cause you can imagine worldwide wrestling fans
[00:43:00.980 --> 00:43:05.980]   are probably not big embracers of the need for vaccines.
[00:43:06.940 --> 00:43:11.940]   And he just turned himself into a spokesperson
[00:43:11.940 --> 00:43:15.020]   for the fact that this virus doesn't care
[00:43:15.020 --> 00:43:17.900]   how healthy you are, how much you exercise,
[00:43:17.900 --> 00:43:19.380]   what a great specimen you are.
[00:43:19.380 --> 00:43:21.900]   It wiped him out.
[00:43:21.900 --> 00:43:23.060]   And we see that.
[00:43:23.060 --> 00:43:27.900]   The average person in the ICU right now with COVID-19
[00:43:27.900 --> 00:43:30.100]   is under age 50.
[00:43:30.100 --> 00:43:31.660]   I think there's a lot of people still thinking,
[00:43:31.660 --> 00:43:33.700]   "Oh, it's just those old people in the nursing homes.
[00:43:33.700 --> 00:43:35.020]   "That's not gonna be about me."
[00:43:35.020 --> 00:43:36.380]   They're wrong.
[00:43:36.380 --> 00:43:38.300]   There are plenty of instances of people
[00:43:38.300 --> 00:43:41.500]   who were totally healthy with no underlying diseases,
[00:43:41.500 --> 00:43:43.620]   taking good care of themselves, not obese,
[00:43:43.620 --> 00:43:46.980]   exercising who have died from this disease.
[00:43:46.980 --> 00:43:52.620]   700 children have died from this disease.
[00:43:52.620 --> 00:43:55.880]   Yes, some of them had underlying factors like obesity,
[00:43:55.880 --> 00:43:57.980]   but a lot of them did not.
[00:43:57.980 --> 00:44:02.500]   So it's fair to say younger people are less susceptible
[00:44:02.500 --> 00:44:05.860]   to serious illness, kids even less so
[00:44:05.860 --> 00:44:09.540]   than young adults, but it ain't zero.
[00:44:09.540 --> 00:44:14.060]   And if the vaccine is really safe and really effective,
[00:44:14.060 --> 00:44:17.540]   then you probably want everybody to take advantage of that.
[00:44:17.540 --> 00:44:20.820]   Even though some are dropping their risks more than others,
[00:44:20.820 --> 00:44:22.940]   everybody's dropping their risks some.
[00:44:22.940 --> 00:44:26.060]   - Are you worried about variants?
[00:44:26.060 --> 00:44:28.140]   So looking out into the future,
[00:44:28.140 --> 00:44:32.460]   what's your vision for all the possible trajectories
[00:44:32.460 --> 00:44:34.940]   that this virus takes in human society?
[00:44:34.940 --> 00:44:37.940]   - I'm totally worried about the variants.
[00:44:37.940 --> 00:44:42.060]   Delta was such an impressive arrival on the scene
[00:44:42.060 --> 00:44:43.300]   in all the wrong ways.
[00:44:43.300 --> 00:44:46.980]   I mean, it took over the world
[00:44:46.980 --> 00:44:49.140]   in the space of just a couple months
[00:44:49.140 --> 00:44:52.740]   because of its extremely contagious ability.
[00:44:52.740 --> 00:44:55.020]   - Viruses would be beautiful if they weren't terrifying.
[00:44:55.020 --> 00:44:56.180]   - Yeah, exactly.
[00:44:56.180 --> 00:44:58.460]   I mean, this whole story of viral evolution,
[00:44:58.460 --> 00:45:01.560]   scientifically, is just amazingly elegant.
[00:45:01.560 --> 00:45:03.300]   Anybody who really wanted to understand
[00:45:03.300 --> 00:45:07.940]   how evolution works in real time, study SARS-CoV-2,
[00:45:07.940 --> 00:45:09.780]   'cause it's not just Delta, it's Alpha,
[00:45:09.780 --> 00:45:10.980]   it's Beta, and it's Gamma,
[00:45:10.980 --> 00:45:14.700]   and it's the fact that these sweep through
[00:45:14.700 --> 00:45:16.540]   the world's population
[00:45:16.540 --> 00:45:20.680]   by fairly minor differences in fitness.
[00:45:20.680 --> 00:45:23.580]   So the real question many people are wrestling is,
[00:45:23.580 --> 00:45:24.940]   is Delta it?
[00:45:24.940 --> 00:45:27.020]   Is it such a fit virus
[00:45:27.020 --> 00:45:30.260]   that nothing else will be able to displace it?
[00:45:30.260 --> 00:45:31.300]   I don't know.
[00:45:31.300 --> 00:45:34.740]   I mean, there's now Delta-AY4,
[00:45:34.740 --> 00:45:37.060]   which is a variant of Delta
[00:45:37.060 --> 00:45:40.780]   that at least in the UK seems to be taking over
[00:45:40.780 --> 00:45:42.240]   the Delta population
[00:45:42.240 --> 00:45:45.280]   as though it's maybe even a little more contagious.
[00:45:45.280 --> 00:45:47.020]   That might be the first hint
[00:45:47.020 --> 00:45:49.180]   that we're seeing something new here.
[00:45:49.180 --> 00:45:51.660]   It's not a completely different virus.
[00:45:51.660 --> 00:45:53.980]   It's still Delta, but it's Delta plus.
[00:45:53.980 --> 00:45:57.540]   You know, the big worry, Alex,
[00:45:57.540 --> 00:46:00.820]   is what's out there that is so different
[00:46:00.820 --> 00:46:04.660]   that the vaccine protection doesn't work?
[00:46:04.660 --> 00:46:07.340]   (laughs)
[00:46:07.340 --> 00:46:10.020]   - And we don't know how different it needs to be
[00:46:10.020 --> 00:46:11.540]   for the vaccine to start working.
[00:46:11.540 --> 00:46:14.860]   That's the terrifying thing about each of these variants.
[00:46:14.860 --> 00:46:18.060]   It's like, it's always a pleasant surprise
[00:46:18.060 --> 00:46:21.020]   that the vaccine seems to still have efficacy.
[00:46:21.020 --> 00:46:23.920]   - And hooray for our immune system, may I say,
[00:46:23.920 --> 00:46:26.180]   because the vaccine immunized you
[00:46:26.180 --> 00:46:28.960]   against that original Wuhan virus.
[00:46:28.960 --> 00:46:35.220]   Now we can see that especially after two doses
[00:46:35.220 --> 00:46:37.540]   and even more so after a booster,
[00:46:37.540 --> 00:46:39.700]   your immune system is so clever
[00:46:39.700 --> 00:46:43.460]   that it's also making a diversity of antibodies
[00:46:43.460 --> 00:46:46.660]   to cover some other things that might happen to that virus
[00:46:46.660 --> 00:46:48.420]   to make it a little different.
[00:46:48.420 --> 00:46:51.900]   And you're still getting really good coverage.
[00:46:51.900 --> 00:46:56.500]   Even for beta, which was South Africa, B1351,
[00:46:56.500 --> 00:46:59.900]   which is the most different, it looks pretty good.
[00:46:59.900 --> 00:47:02.780]   But that doesn't mean it will always be as good as that
[00:47:02.780 --> 00:47:06.140]   if something gets really far away from the original virus.
[00:47:06.140 --> 00:47:08.540]   Now, the good news is we would know what to do
[00:47:08.540 --> 00:47:10.140]   in that situation.
[00:47:10.140 --> 00:47:14.260]   The mRNA vaccines allow you to redesign the vaccine
[00:47:14.260 --> 00:47:17.340]   like that and to quickly get it through
[00:47:17.340 --> 00:47:19.980]   a few thousand participants in a clinical trial
[00:47:19.980 --> 00:47:21.420]   to be sure it's raising antibodies
[00:47:21.420 --> 00:47:23.580]   and then bang, you could go.
[00:47:23.580 --> 00:47:25.220]   But I don't wanna have to do that.
[00:47:25.220 --> 00:47:28.580]   There will be people's lives at risk in the meantime.
[00:47:28.580 --> 00:47:30.380]   And what's the best way to keep that from happening?
[00:47:30.380 --> 00:47:33.700]   Well, try to cut down the number of infections
[00:47:33.700 --> 00:47:34.980]   'cause you don't get variants
[00:47:34.980 --> 00:47:37.300]   unless the virus is replicating in a person.
[00:47:37.300 --> 00:47:40.740]   - So how do we solve this thing?
[00:47:40.740 --> 00:47:43.060]   How do we get out of this pandemic?
[00:47:43.060 --> 00:47:46.300]   What's, like if you had like a wand or something
[00:47:46.300 --> 00:47:50.500]   or you could really implement policies,
[00:47:50.500 --> 00:47:53.020]   what's the full cocktail of solutions here?
[00:47:53.020 --> 00:47:53.980]   - It's a full cocktail.
[00:47:53.980 --> 00:47:56.020]   It's not just one thing.
[00:47:56.020 --> 00:47:58.020]   In our own country here in the US,
[00:47:58.020 --> 00:48:01.300]   it would be getting those 64 million reluctant people
[00:48:01.300 --> 00:48:03.020]   to actually go ahead and get vaccinated.
[00:48:03.020 --> 00:48:05.260]   - There's 64 million people who didn't get vaccinated?
[00:48:05.260 --> 00:48:06.180]   - Adults, yes.
[00:48:06.180 --> 00:48:07.940]   Not even counting the kids.
[00:48:07.940 --> 00:48:09.620]   64 million.
[00:48:09.620 --> 00:48:10.740]   Isn't that astounding?
[00:48:10.740 --> 00:48:13.380]   Get the kids vaccinated.
[00:48:13.380 --> 00:48:17.020]   Hopefully their parents will see that as a good thing too.
[00:48:17.020 --> 00:48:19.660]   Get those of us who are due for boosters boosted
[00:48:19.660 --> 00:48:21.340]   because that's gonna reduce our likelihood
[00:48:21.340 --> 00:48:24.540]   of having breakthrough infections and keep spreading it.
[00:48:24.540 --> 00:48:27.460]   Convince people that until we're really done with this
[00:48:27.460 --> 00:48:28.780]   and we're not now,
[00:48:28.780 --> 00:48:31.620]   that social distancing and mask wearing indoors
[00:48:31.620 --> 00:48:35.260]   are still critical to cut down the number of new infections.
[00:48:35.260 --> 00:48:38.860]   But of course, that's our country.
[00:48:38.860 --> 00:48:41.020]   This is a worldwide pandemic.
[00:48:41.020 --> 00:48:43.860]   I worry greatly about the fact
[00:48:43.860 --> 00:48:45.460]   that low and middle income countries
[00:48:45.460 --> 00:48:47.900]   have for the most part not even gotten started
[00:48:47.900 --> 00:48:49.500]   with access to vaccines.
[00:48:49.500 --> 00:48:51.900]   And we have to figure out a way to speed that up
[00:48:51.900 --> 00:48:56.140]   because otherwise that's where the next variant
[00:48:56.140 --> 00:48:58.180]   will probably arrive.
[00:48:58.180 --> 00:48:59.700]   And who knows how bad it will be
[00:48:59.700 --> 00:49:01.500]   and it will cross the world quickly
[00:49:01.500 --> 00:49:05.100]   as we've seen happen repeatedly in the last 22 months.
[00:49:05.100 --> 00:49:09.780]   - I think I'm really surprised, annoyed, frustrated
[00:49:09.780 --> 00:49:13.620]   that testing, rapid at-home testing
[00:49:13.620 --> 00:49:14.620]   from the very beginning
[00:49:14.620 --> 00:49:17.180]   wasn't a big, big part of the solution.
[00:49:17.180 --> 00:49:19.780]   It seems, first of all, nobody's against it.
[00:49:19.780 --> 00:49:22.220]   That's one huge plus for testing.
[00:49:22.220 --> 00:49:24.220]   It's everybody supports.
[00:49:24.220 --> 00:49:27.260]   Second of all, like that's what America is good at
[00:49:27.260 --> 00:49:29.860]   is like mass manufacturer stuff.
[00:49:29.860 --> 00:49:32.340]   Like stepping up, engineer stepping up
[00:49:32.340 --> 00:49:33.980]   and really deploying it.
[00:49:33.980 --> 00:49:35.820]   Plus without the collection of data
[00:49:35.820 --> 00:49:39.700]   is giving people freedom, is giving them information
[00:49:39.700 --> 00:49:42.620]   and then freedom to decide what to do with that information.
[00:49:42.620 --> 00:49:44.060]   It's such a powerful solution.
[00:49:44.060 --> 00:49:45.260]   I don't understand.
[00:49:45.300 --> 00:49:47.460]   - Well, now I think the Biden administration
[00:49:47.460 --> 00:49:50.380]   is I think emphasized like the scaling
[00:49:50.380 --> 00:49:51.540]   of testing manufacturers.
[00:49:51.540 --> 00:49:54.020]   So, but I just feel like it's an obvious solution.
[00:49:54.020 --> 00:49:57.420]   Get a test that's cost less than a dollar to manufacture,
[00:49:57.420 --> 00:49:59.340]   cost less than a dollar to buy
[00:49:59.340 --> 00:50:02.460]   and just everybody gets tested every single day.
[00:50:02.460 --> 00:50:03.900]   Don't share that data with anyone.
[00:50:03.900 --> 00:50:05.140]   You just make the decisions.
[00:50:05.140 --> 00:50:07.660]   And I believe in the intelligence of people
[00:50:07.660 --> 00:50:09.500]   to make the right decision to stay at home
[00:50:09.500 --> 00:50:11.140]   when the test is positive.
[00:50:11.140 --> 00:50:13.460]   - I am so completely with you on that.
[00:50:13.460 --> 00:50:15.340]   And NIH has been smack in the middle
[00:50:15.340 --> 00:50:17.780]   of trying to make that dream come true.
[00:50:17.780 --> 00:50:21.820]   We're running a trial right now in Georgia,
[00:50:21.820 --> 00:50:23.860]   Indiana, Hawaii.
[00:50:23.860 --> 00:50:26.460]   And where's the other one?
[00:50:26.460 --> 00:50:28.900]   Oh, Kentucky.
[00:50:28.900 --> 00:50:32.980]   Basically blanketing a community with free testing.
[00:50:32.980 --> 00:50:33.820]   - That's beautiful.
[00:50:33.820 --> 00:50:36.540]   - And look to see what happens as far as stemming
[00:50:36.540 --> 00:50:40.100]   the spread of the epidemic and measuring it by wastewater
[00:50:40.100 --> 00:50:42.580]   'cause you can really tell whether you've cut back
[00:50:42.580 --> 00:50:44.940]   the amount of infection in the community.
[00:50:44.940 --> 00:50:47.100]   Yeah, I'm so with you.
[00:50:47.100 --> 00:50:49.580]   We got off to such a bad start with testing.
[00:50:49.580 --> 00:50:52.420]   And of course, all the testing was being done
[00:50:52.420 --> 00:50:55.940]   for the first several months in big box laboratories
[00:50:55.940 --> 00:50:57.820]   where you had to send the sample off
[00:50:57.820 --> 00:50:59.300]   and put it through the mail somehow
[00:50:59.300 --> 00:51:01.460]   and get the result back sometimes five days later
[00:51:01.460 --> 00:51:03.900]   after you've already infected a dozen people.
[00:51:03.900 --> 00:51:05.740]   It was just a completely wrong model,
[00:51:05.740 --> 00:51:06.780]   but it's what we had.
[00:51:06.780 --> 00:51:09.820]   And everybody was like, oh, we got to stick with PCR
[00:51:09.820 --> 00:51:12.020]   because if you start using those home tests
[00:51:12.020 --> 00:51:14.820]   that are based on antigens lateral flow,
[00:51:14.820 --> 00:51:16.580]   probably there's gonna be false positives
[00:51:16.580 --> 00:51:17.420]   and false negatives.
[00:51:17.420 --> 00:51:20.460]   Okay, sure, no test is perfect,
[00:51:20.460 --> 00:51:23.420]   but having a test that's not acceptable
[00:51:23.420 --> 00:51:26.100]   or accessible is the worst setting.
[00:51:26.100 --> 00:51:29.580]   So we, NIH, with some requests from Congress,
[00:51:29.580 --> 00:51:32.620]   got a billion dollars to create this program
[00:51:32.620 --> 00:51:36.740]   called Rapid Acceleration of Diagnostics, RADx.
[00:51:36.740 --> 00:51:39.140]   And we turned into a venture capital organization
[00:51:39.140 --> 00:51:41.620]   and we invited every small business or academic lab
[00:51:41.620 --> 00:51:44.340]   that had a cool idea about how to do home testing
[00:51:44.340 --> 00:51:45.700]   to bring it forward.
[00:51:45.700 --> 00:51:48.300]   And we threw them into what we called our shark tank
[00:51:48.300 --> 00:51:51.060]   of business experts, engineers, technology people.
[00:51:51.060 --> 00:51:53.820]   People understood how to deal
[00:51:53.820 --> 00:51:56.780]   with supply chains and manufacturing.
[00:51:56.780 --> 00:52:00.260]   And right now today, there are about 2 million tests
[00:52:00.260 --> 00:52:03.540]   being done based on what came out of that program,
[00:52:03.540 --> 00:52:05.620]   including most of the home tests
[00:52:05.620 --> 00:52:07.620]   that you can now buy on the pharmacy shelves.
[00:52:07.620 --> 00:52:10.340]   We did that and I wish we had done it faster,
[00:52:10.340 --> 00:52:13.700]   but it was an amazingly speedy effort.
[00:52:13.700 --> 00:52:15.460]   And you're right, companies are really good.
[00:52:15.460 --> 00:52:18.140]   Once they've gotten FDA emergency use authorization,
[00:52:18.140 --> 00:52:20.300]   and we helped a lot of them get that,
[00:52:20.300 --> 00:52:22.900]   they can scale up their manufacturing.
[00:52:22.900 --> 00:52:27.900]   I think in December, we should have about 410 million tests
[00:52:27.900 --> 00:52:30.260]   for that month ready to go.
[00:52:30.260 --> 00:52:34.020]   And if we can get one or two more platforms approved,
[00:52:34.020 --> 00:52:36.700]   and by the way, we are now helping FDA
[00:52:36.700 --> 00:52:39.140]   by being their validation lab.
[00:52:39.140 --> 00:52:41.060]   If we can get a couple more of these approved,
[00:52:41.060 --> 00:52:44.820]   we could be in the half a billion tests a month,
[00:52:44.820 --> 00:52:46.860]   which is really getting where we need to be.
[00:52:46.860 --> 00:52:48.940]   - Wow, yeah, that's a dream.
[00:52:48.940 --> 00:52:49.780]   That's a dream for me.
[00:52:49.780 --> 00:52:52.780]   It seems like an obvious solution, engineering solution.
[00:52:52.780 --> 00:52:53.900]   Everybody's behind it.
[00:52:53.900 --> 00:52:56.020]   It leads to hope versus division.
[00:52:56.020 --> 00:52:57.260]   I love it.
[00:52:57.260 --> 00:52:58.100]   Okay.
[00:52:58.100 --> 00:53:00.660]   - A happy story.
[00:53:00.660 --> 00:53:01.500]   - A happy story.
[00:53:01.500 --> 00:53:02.740]   - I was waiting for one.
[00:53:02.740 --> 00:53:03.580]   - Yeah, all right.
[00:53:03.580 --> 00:53:06.420]   Well, one last dive into the not happy,
[00:53:06.420 --> 00:53:08.940]   but you won't even have to comment on it.
[00:53:08.940 --> 00:53:11.340]   - Well, comment on the broader philosophical question.
[00:53:11.340 --> 00:53:16.340]   So NIH, again, I said Joe Rogan
[00:53:16.340 --> 00:53:18.820]   as the first one who pointed me to this.
[00:53:18.820 --> 00:53:21.860]   NIH was recently accused of funding research of a paper
[00:53:21.860 --> 00:53:24.060]   that had images of sedated puppies
[00:53:24.060 --> 00:53:26.460]   with their heads inserted into small enclosures
[00:53:26.460 --> 00:53:28.780]   containing disease carrying sand flies.
[00:53:28.780 --> 00:53:34.620]   So I could just say that this story is not true,
[00:53:34.620 --> 00:53:37.220]   or at least the,
[00:53:37.220 --> 00:53:39.020]   I think it is true that the paper
[00:53:39.020 --> 00:53:43.300]   that showed those images cited NIH as a funding source,
[00:53:43.300 --> 00:53:45.140]   but that citation is not correct.
[00:53:45.140 --> 00:53:45.980]   - That was not correct.
[00:53:45.980 --> 00:53:47.500]   - Yeah.
[00:53:47.500 --> 00:53:51.580]   But that brings up a bigger philosophical question
[00:53:51.580 --> 00:53:54.620]   that it could have been correct.
[00:53:54.620 --> 00:53:57.380]   How difficult is it as a director of NIH
[00:53:57.380 --> 00:53:58.860]   or just NIH as an organization
[00:53:58.860 --> 00:54:03.060]   that's funding so many amazing deep research studies
[00:54:03.060 --> 00:54:07.260]   to ensure the ethical fortitude of those studies
[00:54:07.260 --> 00:54:10.100]   when the ethics of science is,
[00:54:10.100 --> 00:54:12.260]   there's such a gray area between what is
[00:54:12.260 --> 00:54:13.500]   and what isn't ethical?
[00:54:13.500 --> 00:54:16.700]   - Well, tough issues.
[00:54:16.700 --> 00:54:20.020]   Certainly animal research is a tough issue.
[00:54:20.020 --> 00:54:21.460]   - I was going to bring up,
[00:54:21.460 --> 00:54:23.820]   it's a good example of that tough issue,
[00:54:23.820 --> 00:54:27.020]   is in 2015, you announced that NIH
[00:54:27.020 --> 00:54:29.700]   would no longer support any biomedical research
[00:54:29.700 --> 00:54:31.900]   involving chimpanzees.
[00:54:31.900 --> 00:54:36.780]   So that's like a one example of looking in the mirror,
[00:54:36.780 --> 00:54:39.700]   thinking deeply about what is and isn't ethical.
[00:54:39.700 --> 00:54:42.460]   And there was a conclusion that biomedical research
[00:54:42.460 --> 00:54:45.060]   on chimps is not ethical.
[00:54:45.060 --> 00:54:46.420]   - That was the conclusion.
[00:54:46.420 --> 00:54:48.260]   That was based on a lot of deep thinking
[00:54:48.260 --> 00:54:50.460]   and a lot of input from people
[00:54:50.460 --> 00:54:52.020]   who have considered this issue
[00:54:52.020 --> 00:54:54.940]   and a panel of the National Academy of Sciences
[00:54:54.940 --> 00:54:57.660]   that was asked to review the issue.
[00:54:57.660 --> 00:55:01.420]   I mean, the question that I wanted them to look at was,
[00:55:01.420 --> 00:55:05.420]   are we actually learning anything that's really essential
[00:55:05.420 --> 00:55:08.980]   from chimpanzee invasive research at this point?
[00:55:08.980 --> 00:55:13.900]   Or is it time to say that these closest relatives of ours
[00:55:13.900 --> 00:55:16.100]   should not be subjected to that any further
[00:55:16.100 --> 00:55:19.100]   and ought to be retired to a sanctuary?
[00:55:19.100 --> 00:55:20.420]   And that was the conclusion,
[00:55:20.420 --> 00:55:24.660]   that there was really no kind of medical experimentation
[00:55:24.660 --> 00:55:27.820]   that needed to be done on chimps in order to proceed.
[00:55:27.820 --> 00:55:29.660]   So why are we still doing this?
[00:55:29.660 --> 00:55:32.740]   Many of these were chimpanzees that were purchased
[00:55:32.740 --> 00:55:37.740]   because we thought they would be good hosts for HIV/AIDS,
[00:55:37.740 --> 00:55:39.700]   and they sort of weren't.
[00:55:39.700 --> 00:55:43.260]   And they were kept around in these primate laboratories
[00:55:43.260 --> 00:55:45.820]   with people coming up with other things to do,
[00:55:45.820 --> 00:55:48.340]   but they weren't compelling scientifically.
[00:55:48.340 --> 00:55:50.380]   So I think that was the right decision.
[00:55:50.380 --> 00:55:51.900]   I took a lot of flack
[00:55:51.900 --> 00:55:53.580]   from some of the scientific communities said,
[00:55:53.580 --> 00:55:56.620]   "Well, you're caving in to the animal rights people.
[00:55:56.620 --> 00:55:59.300]   And now that you've said no more research on chimps,
[00:55:59.300 --> 00:56:00.460]   what's next?"
[00:56:00.460 --> 00:56:04.940]   Certainly when it comes to companion animals,
[00:56:04.940 --> 00:56:09.020]   everybody's heart starts to be hurting
[00:56:09.020 --> 00:56:12.460]   when you see anything done that seems harmful
[00:56:12.460 --> 00:56:14.100]   to a dog or a cat.
[00:56:14.100 --> 00:56:16.380]   I have a cat, I don't have a dog.
[00:56:16.380 --> 00:56:18.100]   And I understand that completely.
[00:56:18.100 --> 00:56:21.420]   That's why we have these oversight groups
[00:56:21.420 --> 00:56:24.380]   that decide before you do any of that kind of research,
[00:56:24.380 --> 00:56:26.340]   is it justified?
[00:56:26.340 --> 00:56:29.740]   And what kind of provision is going to be made
[00:56:29.740 --> 00:56:31.500]   to avoid pain and suffering?
[00:56:31.500 --> 00:56:35.420]   And those have input from the public
[00:56:35.420 --> 00:56:37.900]   as well as the scientific community.
[00:56:37.900 --> 00:56:40.500]   Is that completely saying that every step
[00:56:40.500 --> 00:56:45.100]   that's happening there is ethical by some standard
[00:56:45.100 --> 00:56:48.900]   that would be hard for anybody to agree to?
[00:56:48.900 --> 00:56:50.820]   No, but at least it's a consensus
[00:56:50.820 --> 00:56:53.060]   of what people think is acceptable.
[00:56:54.380 --> 00:56:58.180]   Dogs are the only host for some diseases
[00:56:58.180 --> 00:57:01.580]   like Leishmaniasis, which was that paper
[00:57:01.580 --> 00:57:03.300]   that we were not responsible for,
[00:57:03.300 --> 00:57:05.540]   but I know why they were doing the experiment.
[00:57:05.540 --> 00:57:08.020]   Or like lymphatic filariasis,
[00:57:08.020 --> 00:57:11.780]   which is an experiment that we are supporting in Georgia
[00:57:11.780 --> 00:57:14.860]   that involves dogs getting infected with a parasite
[00:57:14.860 --> 00:57:16.780]   because that's the only model we have to know
[00:57:16.780 --> 00:57:18.940]   whether a treatment is gonna work or not.
[00:57:18.940 --> 00:57:21.980]   So I will defend that.
[00:57:21.980 --> 00:57:24.420]   I am not in the place of those who think
[00:57:24.420 --> 00:57:27.220]   all animal research is evil,
[00:57:27.220 --> 00:57:29.700]   'cause I think if there's something that's gonna be done
[00:57:29.700 --> 00:57:32.900]   to save a child from a terrible disease or an adult,
[00:57:32.900 --> 00:57:34.340]   and it involves animal research
[00:57:34.340 --> 00:57:36.260]   that's been carefully reviewed,
[00:57:36.260 --> 00:57:39.380]   then I think ethically, while it doesn't make me comfortable,
[00:57:39.380 --> 00:57:42.340]   it still seems like it's the right choice.
[00:57:42.340 --> 00:57:45.140]   I think to say all animal research
[00:57:45.140 --> 00:57:48.860]   should be taken off the table is also very unethical
[00:57:48.860 --> 00:57:52.860]   'cause that means you have basically doomed a lot of people
[00:57:52.860 --> 00:57:55.020]   for whom that research might have saved their lives
[00:57:55.020 --> 00:57:56.180]   to having no more hope.
[00:57:56.180 --> 00:57:59.340]   - And to me personally,
[00:57:59.340 --> 00:58:01.660]   there's far greater concerns ethically
[00:58:01.660 --> 00:58:04.020]   in terms of factory farming, for example,
[00:58:04.020 --> 00:58:06.300]   the treatment of animals in other contexts.
[00:58:06.300 --> 00:58:08.100]   - Oh, there's so much that goes on
[00:58:08.100 --> 00:58:12.500]   outside of medical research that is much more troubling.
[00:58:12.500 --> 00:58:15.500]   - That said, I think all cats have to go.
[00:58:15.500 --> 00:58:17.620]   That's just my off the record opinion.
[00:58:17.620 --> 00:58:19.980]   That's why I'm not involved with any ethical decisions.
[00:58:19.980 --> 00:58:22.300]   I'm just joking internet, I love cats.
[00:58:22.300 --> 00:58:23.260]   - You're a dog person.
[00:58:23.260 --> 00:58:25.060]   - I'm a dog person, I'm sorry.
[00:58:25.060 --> 00:58:26.620]   - You've seen the New Yorker cartoon
[00:58:26.620 --> 00:58:30.060]   where there are two dogs in the bar having a martini,
[00:58:30.060 --> 00:58:32.540]   and one is saying they're dressed up in their business suits
[00:58:32.540 --> 00:58:34.060]   and one says to the other,
[00:58:34.060 --> 00:58:37.540]   "You know, it's not enough for the dogs to win.
[00:58:37.540 --> 00:58:39.340]   "The cats have to lose."
[00:58:39.340 --> 00:58:40.860]   (laughing)
[00:58:40.860 --> 00:58:41.820]   - That's beautiful.
[00:58:41.820 --> 00:58:45.660]   So a few weeks ago, you've announced
[00:58:45.660 --> 00:58:49.100]   that you're resigning from the NIH at the end of the year.
[00:58:49.100 --> 00:58:50.220]   - I'm stepping down.
[00:58:50.220 --> 00:58:53.420]   I'm still gonna be at NIH in a different capacity.
[00:58:53.420 --> 00:58:54.860]   - Different capacity, right.
[00:58:54.860 --> 00:58:58.940]   And it's over a decade of an incredible career
[00:58:58.940 --> 00:59:01.180]   overseeing the NIH as its director.
[00:59:01.180 --> 00:59:04.020]   What are the things you're most proud of
[00:59:04.020 --> 00:59:06.980]   of the NIH in your time here
[00:59:06.980 --> 00:59:10.540]   as its director, maybe memorable moments?
[00:59:12.820 --> 00:59:15.540]   - There's a lot in 12 years.
[00:59:15.540 --> 00:59:19.700]   Science has just progressed in amazing ways
[00:59:19.700 --> 00:59:21.380]   over those 12 years.
[00:59:21.380 --> 00:59:25.100]   Think about where we are right now.
[00:59:25.100 --> 00:59:26.580]   Something like gene editing,
[00:59:26.580 --> 00:59:29.420]   being able to make changes in DNA,
[00:59:29.420 --> 00:59:31.060]   even for therapeutic purposes,
[00:59:31.060 --> 00:59:33.980]   which is now curing sickle cell disease.
[00:59:33.980 --> 00:59:38.140]   Unthinkable when I became director in 2009.
[00:59:38.140 --> 00:59:41.580]   The ability to study single cells
[00:59:41.580 --> 00:59:44.540]   and ask them what they're doing and get an answer.
[00:59:44.540 --> 00:59:47.140]   Single cell biology just has emerged
[00:59:47.140 --> 00:59:49.220]   in this incredibly powerful way.
[00:59:49.220 --> 00:59:53.900]   Having the courage to be able to say,
[00:59:53.900 --> 00:59:57.060]   "We could actually understand the human brain,"
[00:59:57.060 --> 00:59:59.020]   seemed like so far out there.
[00:59:59.020 --> 01:00:01.340]   And we're in the process of doing that
[01:00:01.340 --> 01:00:02.940]   with the Brain Initiative.
[01:00:02.940 --> 01:00:06.260]   Taking all that we've learned about the genome
[01:00:06.260 --> 01:00:09.020]   and applying it to cancer,
[01:00:09.020 --> 01:00:13.220]   to make individual cancer treatment really precision,
[01:00:13.220 --> 01:00:15.060]   and developing cancer immunotherapy,
[01:00:15.060 --> 01:00:17.340]   which seemed like sort of a backwater
[01:00:17.340 --> 01:00:19.340]   into some of the hottest science around.
[01:00:19.340 --> 01:00:22.620]   All those things sort of erupting,
[01:00:22.620 --> 01:00:23.860]   and much more to come, I'm sure.
[01:00:23.860 --> 01:00:27.980]   We're on an exponential curve of medical research advances,
[01:00:27.980 --> 01:00:30.380]   and that's glorious to watch.
[01:00:30.380 --> 01:00:31.900]   And of course, COVID-19,
[01:00:31.900 --> 01:00:35.540]   as a beneficiary of decades of basic science,
[01:00:35.540 --> 01:00:37.740]   understanding what mRNA is,
[01:00:37.740 --> 01:00:41.220]   understanding basics about coronaviruses and spike proteins,
[01:00:41.220 --> 01:00:43.540]   and how to combine structural biology,
[01:00:43.540 --> 01:00:46.500]   and immunology, and genomics into this package
[01:00:46.500 --> 01:00:49.380]   that allows you to make a vaccine in 11 months.
[01:00:49.380 --> 01:00:53.620]   Just, I would never have imagined that possible in 2009.
[01:00:53.620 --> 01:00:56.460]   So to have been able to kind of be the midwife,
[01:00:56.460 --> 01:00:59.060]   helping all of those things get birthed,
[01:00:59.060 --> 01:01:02.300]   that's been just an amazing 12 years.
[01:01:02.300 --> 01:01:06.700]   And as NIH director, you have this convening power,
[01:01:06.700 --> 01:01:09.180]   and this ability to look across the whole landscape
[01:01:09.180 --> 01:01:10.340]   of biomedical research,
[01:01:10.340 --> 01:01:13.820]   and identify areas that are just like ready
[01:01:13.820 --> 01:01:15.820]   for something big to happen,
[01:01:15.820 --> 01:01:17.600]   but isn't gonna happen spontaneously
[01:01:17.600 --> 01:01:18.940]   without some encouragement,
[01:01:18.940 --> 01:01:21.740]   without pulling people together from different disciplines
[01:01:21.740 --> 01:01:22.780]   who don't know each other,
[01:01:22.780 --> 01:01:24.940]   and maybe don't know how to quite understand
[01:01:24.940 --> 01:01:26.660]   each other's scientific language,
[01:01:26.660 --> 01:01:29.060]   and create an environment for that to happen.
[01:01:29.060 --> 01:01:32.100]   That has been just an amazing experience.
[01:01:32.100 --> 01:01:35.380]   I mean, I mentioned the BRAIN Initiative as one of those.
[01:01:35.380 --> 01:01:36.540]   The BRAIN Initiative right now,
[01:01:36.540 --> 01:01:39.880]   I think there's about 600 investigators working on this.
[01:01:39.880 --> 01:01:43.420]   Last week, the whole issue of Nature Magazine
[01:01:43.420 --> 01:01:45.980]   was about the output of the BRAIN Initiative,
[01:01:45.980 --> 01:01:48.700]   basically now giving us a cell census
[01:01:48.700 --> 01:01:51.220]   of what those cells in the brain are doing,
[01:01:51.220 --> 01:01:53.900]   which has just never been imaginable.
[01:01:53.900 --> 01:01:58.900]   And interestingly, more than half of the investigators
[01:01:58.900 --> 01:02:00.880]   in the BRAIN Initiative are engineers.
[01:02:00.880 --> 01:02:04.020]   They're not biologists in a traditional sense.
[01:02:04.020 --> 01:02:04.860]   I love that.
[01:02:04.860 --> 01:02:08.260]   Maybe partly 'cause my PhD is in quantum mechanics.
[01:02:08.260 --> 01:02:10.700]   So I think it's really a good idea
[01:02:10.700 --> 01:02:14.180]   to bring disciplines together and see what happens.
[01:02:14.180 --> 01:02:15.520]   That's an exciting thing.
[01:02:15.520 --> 01:02:19.700]   And I will not ever forget having the chance
[01:02:19.700 --> 01:02:22.700]   to announce that program in the East Room
[01:02:22.700 --> 01:02:25.860]   in that White House with President Obama,
[01:02:25.860 --> 01:02:28.620]   who totally got it and totally loved science,
[01:02:28.620 --> 01:02:32.380]   and working with him in some of those rare moments
[01:02:32.380 --> 01:02:35.100]   of sort of one-on-one conversation in the Oval Office,
[01:02:35.100 --> 01:02:36.900]   just him and me about science.
[01:02:36.900 --> 01:02:37.980]   That's a gift.
[01:02:37.980 --> 01:02:41.500]   - What's it like talking to Barack Obama about science?
[01:02:41.500 --> 01:02:43.860]   He seems to be a sponge.
[01:02:43.860 --> 01:02:46.980]   I've heard him, I'm an artificial intelligence person,
[01:02:46.980 --> 01:02:48.700]   and I've heard him talk about AI.
[01:02:48.700 --> 01:02:51.260]   And it was like, it made me think,
[01:02:51.260 --> 01:02:53.300]   is somebody like whispering in his ear or something?
[01:02:53.300 --> 01:02:56.260]   Because he was saying stuff that totally passed the BS test,
[01:02:56.260 --> 01:02:58.500]   like he really understands stuff.
[01:02:58.500 --> 01:02:59.500]   - He does.
[01:02:59.500 --> 01:03:02.380]   - That means he listened to a bunch of experts on AI.
[01:03:02.380 --> 01:03:03.940]   He was explaining the difference
[01:03:03.940 --> 01:03:07.300]   between narrow artificial intelligence and strong AI.
[01:03:07.300 --> 01:03:08.180]   He was saying all this,
[01:03:08.180 --> 01:03:10.420]   both technical and philosophical stuff.
[01:03:10.420 --> 01:03:12.220]   And it just made me, I don't know,
[01:03:12.220 --> 01:03:15.980]   it made me hopeful about the depth of understanding
[01:03:15.980 --> 01:03:18.500]   that a human being in political office can attain.
[01:03:18.500 --> 01:03:22.220]   - That gave me hope as well, and having those experiences.
[01:03:22.220 --> 01:03:24.940]   Oftentimes in a group, I mean, another example
[01:03:24.940 --> 01:03:26.900]   where I was trying to figure out,
[01:03:26.900 --> 01:03:29.120]   how do we take what we've learned about the genome
[01:03:29.120 --> 01:03:31.380]   and really apply it at scale
[01:03:31.380 --> 01:03:33.320]   to figure out how to prevent illness,
[01:03:33.320 --> 01:03:35.460]   not just treat it, but prevent it,
[01:03:35.460 --> 01:03:38.060]   out of which came this program called All of Us,
[01:03:38.060 --> 01:03:42.740]   this million strong American cohort of participants
[01:03:42.740 --> 01:03:44.740]   who make their electronic health records
[01:03:44.740 --> 01:03:46.980]   and their genome sequences and everything else available
[01:03:46.980 --> 01:03:48.620]   for researchers to look at.
[01:03:48.620 --> 01:03:51.820]   That came out of a couple of conversations
[01:03:51.820 --> 01:03:55.060]   with Obama and others in his office.
[01:03:55.060 --> 01:03:58.580]   And he asked the best questions.
[01:03:58.580 --> 01:04:00.560]   That was what struck me so much.
[01:04:00.560 --> 01:04:02.680]   I mean, a room full of scientists,
[01:04:02.680 --> 01:04:05.160]   and we'd be talking about the possible approaches,
[01:04:05.160 --> 01:04:07.000]   and he would come up with this
[01:04:07.000 --> 01:04:09.440]   incredibly insightful, penetrating question.
[01:04:09.440 --> 01:04:11.320]   Not that he knew what the answer was gonna be,
[01:04:11.320 --> 01:04:13.360]   but he knew what the right question was.
[01:04:13.360 --> 01:04:17.040]   - I think the core to that is curiosity.
[01:04:17.040 --> 01:04:18.080]   - Yeah.
[01:04:18.080 --> 01:04:19.240]   - I don't think he's even like,
[01:04:19.240 --> 01:04:20.400]   he's trying to be a good leader.
[01:04:20.400 --> 01:04:22.360]   He's legit curious.
[01:04:22.360 --> 01:04:24.800]   - Yes, legit.
[01:04:24.800 --> 01:04:26.680]   - That he, almost like a kid in a candy store
[01:04:26.680 --> 01:04:28.280]   gets to talk to the world experts.
[01:04:28.280 --> 01:04:30.940]   He somehow sneaked into this office
[01:04:30.940 --> 01:04:33.240]   and gets to talk to the world experts.
[01:04:33.240 --> 01:04:36.880]   And that's the kind of energy that I think leads
[01:04:36.880 --> 01:04:40.200]   to beautiful leadership in the space of science.
[01:04:40.200 --> 01:04:41.320]   - Indeed.
[01:04:41.320 --> 01:04:43.560]   Another thing I've been able to do as director
[01:04:43.560 --> 01:04:45.720]   is to try to break down some of the boundaries
[01:04:45.720 --> 01:04:47.280]   that seem to be traditional
[01:04:47.280 --> 01:04:49.240]   between the public and the private sectors
[01:04:49.240 --> 01:04:50.840]   when it comes to areas of science
[01:04:50.840 --> 01:04:53.920]   that really could and should be open access anyway.
[01:04:53.920 --> 01:04:56.280]   Why don't we work together?
[01:04:56.280 --> 01:04:58.320]   And that was obvious early on.
[01:04:58.320 --> 01:05:03.320]   And after identifying a few possible collaborators
[01:05:03.320 --> 01:05:07.480]   who are chief scientists of pharmaceutical companies,
[01:05:07.480 --> 01:05:08.800]   it looked as like we might be able
[01:05:08.800 --> 01:05:10.420]   to do something in that space.
[01:05:10.420 --> 01:05:12.200]   Out of that was born something called
[01:05:12.200 --> 01:05:15.040]   the Accelerating Medicines Partnership, AMP.
[01:05:15.040 --> 01:05:19.320]   And it took a couple of years of convening people
[01:05:19.320 --> 01:05:21.440]   who usually didn't talk to each other.
[01:05:21.440 --> 01:05:24.880]   And there was a lot of suspicion, academic scientists saying,
[01:05:24.880 --> 01:05:28.080]   oh, those scientists in pharma, they're not that smart.
[01:05:28.080 --> 01:05:30.000]   They're just trying to make money.
[01:05:30.000 --> 01:05:33.140]   And the academic scientists getting the wrap
[01:05:33.140 --> 01:05:34.560]   from the pharmaceutical scientists,
[01:05:34.560 --> 01:05:35.980]   all they wanna do is publish papers.
[01:05:35.980 --> 01:05:38.200]   They don't really care about helping anybody.
[01:05:38.200 --> 01:05:41.060]   And we found out both of those stereotypes were wrong.
[01:05:41.060 --> 01:05:44.280]   And over the course of that couple of years,
[01:05:44.280 --> 01:05:47.600]   built a momentum behind three starting projects,
[01:05:47.600 --> 01:05:50.000]   one on Alzheimer's, one on diabetes,
[01:05:50.000 --> 01:05:52.100]   one on rheumatoid arthritis and lupus.
[01:05:52.100 --> 01:05:54.440]   Very different, each one of them trying to identify
[01:05:54.440 --> 01:05:58.480]   what is an area that we both really need to see advance
[01:05:58.480 --> 01:06:00.160]   and we could do better together.
[01:06:00.160 --> 01:06:01.840]   And it's gonna have to be open access,
[01:06:01.840 --> 01:06:03.920]   otherwise NIH is not gonna play.
[01:06:03.920 --> 01:06:05.280]   And guess what, industry?
[01:06:05.280 --> 01:06:06.600]   If you really wanna do this,
[01:06:06.600 --> 01:06:08.160]   you gotta have skin in the game.
[01:06:08.160 --> 01:06:10.720]   We'll cover half the cost, you gotta cover the other half.
[01:06:10.720 --> 01:06:11.640]   - I love it.
[01:06:11.640 --> 01:06:16.200]   Enforcing open access, so resulting in open science.
[01:06:16.200 --> 01:06:17.880]   - Millions of dollars gone into this.
[01:06:17.880 --> 01:06:19.960]   And it has been a wild success.
[01:06:19.960 --> 01:06:22.440]   After many people were skeptical,
[01:06:22.440 --> 01:06:27.120]   a couple of years later we had another project on Parkinson's
[01:06:27.120 --> 01:06:29.560]   more recently we've added one on schizophrenia.
[01:06:29.560 --> 01:06:34.040]   Just this week, we added one on gene therapy,
[01:06:34.040 --> 01:06:38.080]   on bespoke gene therapy for ultra rare diseases,
[01:06:38.080 --> 01:06:41.160]   which otherwise aren't gonna have enough commercial appeal.
[01:06:41.160 --> 01:06:42.320]   But if we did this together,
[01:06:42.320 --> 01:06:45.560]   especially with FDA at the table, and they have been,
[01:06:45.560 --> 01:06:46.880]   we could make something happen,
[01:06:46.880 --> 01:06:49.800]   turn this into a sort of standardized approach
[01:06:49.800 --> 01:06:52.480]   where everything didn't have to be a one-off.
[01:06:52.480 --> 01:06:54.320]   I'm really excited about that.
[01:06:54.320 --> 01:06:56.600]   So what began as three projects is six,
[01:06:56.600 --> 01:06:58.480]   and it's about to be seven next year
[01:06:58.480 --> 01:07:00.640]   with a heart failure project.
[01:07:00.640 --> 01:07:03.600]   And all of us have gotten to know each other.
[01:07:03.600 --> 01:07:05.520]   And if it weren't for that background,
[01:07:05.520 --> 01:07:08.840]   when COVID came along, it would have been a lot harder
[01:07:08.840 --> 01:07:11.040]   to build the partnership called ACTIV,
[01:07:11.040 --> 01:07:14.440]   which has been my passion for the last 20 months,
[01:07:14.440 --> 01:07:17.520]   accelerating COVID-19 therapeutic interventions
[01:07:17.520 --> 01:07:18.360]   and vaccines.
[01:07:18.360 --> 01:07:21.120]   I was at our leadership team meeting this morning.
[01:07:21.120 --> 01:07:23.160]   It was amazing what's been accomplished.
[01:07:23.160 --> 01:07:26.880]   That's pretty much a hundred people who dropped everything
[01:07:26.880 --> 01:07:29.080]   just to work on this, about half from industry
[01:07:29.080 --> 01:07:31.360]   and half from government and academia.
[01:07:31.360 --> 01:07:36.520]   And that's how we got vaccine master protocols designed.
[01:07:36.520 --> 01:07:39.200]   So we all agreed about what the end points had to be.
[01:07:39.200 --> 01:07:42.560]   And you wondered why are there 30,000 participants
[01:07:42.560 --> 01:07:43.560]   in each of these trials?
[01:07:43.560 --> 01:07:47.040]   That's 'cause of ACTIV's group mapping out
[01:07:47.040 --> 01:07:49.720]   what the power needed to be for this to be convincing.
[01:07:49.720 --> 01:07:53.160]   Same with therapeutics.
[01:07:53.160 --> 01:07:57.920]   We have run at least 20 therapeutic agents through trials
[01:07:57.920 --> 01:08:00.960]   that ACTIV supported in record time.
[01:08:00.960 --> 01:08:03.880]   That's how we got monoclonal antibodies that we know work.
[01:08:03.880 --> 01:08:08.840]   That's been, that would not have been possible
[01:08:08.840 --> 01:08:12.440]   if I didn't already have a sense of how to work
[01:08:12.440 --> 01:08:15.440]   with the private sector that came out of AMP.
[01:08:15.440 --> 01:08:17.080]   AMP took two years to get started.
[01:08:17.080 --> 01:08:18.440]   ACTIV took two weeks.
[01:08:18.440 --> 01:08:20.600]   We just kept the lawyers-
[01:08:20.600 --> 01:08:21.880]   - Wow, to get a hundred people over?
[01:08:21.880 --> 01:08:24.440]   - Yeah, kept the lawyers out of the room and away.
[01:08:24.440 --> 01:08:27.120]   (laughing)
[01:08:27.120 --> 01:08:28.960]   - Now you're gonna get yourself in trouble.
[01:08:28.960 --> 01:08:30.880]   (laughing)
[01:08:30.880 --> 01:08:34.680]   I do hope one day the story of this incredible vaccine,
[01:08:34.680 --> 01:08:36.440]   development of vaccine protocols and trials
[01:08:36.440 --> 01:08:37.680]   and all this kind of details,
[01:08:37.680 --> 01:08:41.600]   the messy, beautiful details of science and engineering
[01:08:41.600 --> 01:08:44.320]   and that led to the manufacturing, the deployment
[01:08:44.320 --> 01:08:45.200]   and the scientific test.
[01:08:45.200 --> 01:08:48.560]   It's such a nice dance between engineering
[01:08:48.560 --> 01:08:50.520]   in the space of manufacture of the vaccines.
[01:08:50.520 --> 01:08:53.120]   You start before the studies are complete,
[01:08:53.120 --> 01:08:56.080]   you start making the vaccines just in case
[01:08:56.080 --> 01:08:58.040]   if the studies prove to be positive,
[01:08:58.040 --> 01:08:59.720]   then you can start deploying them.
[01:08:59.720 --> 01:09:04.000]   Just like so many parties, like you said,
[01:09:04.000 --> 01:09:05.680]   private and public playing together.
[01:09:05.680 --> 01:09:10.120]   That's just a beautiful dance that is one of the,
[01:09:10.120 --> 01:09:11.960]   is one of, for me, the sources of hope
[01:09:11.960 --> 01:09:14.760]   in this very tricky time where there's a lot
[01:09:14.760 --> 01:09:19.320]   of things to be cynical about in terms
[01:09:19.320 --> 01:09:23.280]   of the games politicians play and the hardship experience
[01:09:23.280 --> 01:09:25.360]   of the economy and all those kinds of things.
[01:09:25.360 --> 01:09:30.360]   But to me, this dance was a vaccine development
[01:09:30.360 --> 01:09:33.200]   was done just beautifully and it gives me hope.
[01:09:33.200 --> 01:09:34.440]   - It does me as well.
[01:09:34.440 --> 01:09:37.400]   And it was in many ways, the finest hour
[01:09:37.400 --> 01:09:41.600]   that science has had in a long time being called upon
[01:09:41.600 --> 01:09:44.080]   when every day counted and making sure
[01:09:44.080 --> 01:09:46.400]   that time was not wasted
[01:09:46.400 --> 01:09:49.960]   and things were done rigorously, but quickly.
[01:09:49.960 --> 01:09:54.840]   - So you're incredibly good as a leader of the NIH.
[01:09:54.840 --> 01:09:57.920]   It seems like you're having a heck of a lot of fun.
[01:09:57.920 --> 01:10:02.720]   Why step down from this role after so much fun?
[01:10:02.720 --> 01:10:06.920]   - Well, no other NIH director has served more
[01:10:06.920 --> 01:10:09.360]   than one president after being appointed
[01:10:09.360 --> 01:10:10.720]   by one, you're sort of done.
[01:10:10.720 --> 01:10:12.960]   And the idea of being carried over
[01:10:12.960 --> 01:10:14.760]   for a second presidency with Trump
[01:10:14.760 --> 01:10:18.640]   and now a third one with Biden is unheard of.
[01:10:18.640 --> 01:10:22.640]   I just think, Lex, that scientific organizations benefit
[01:10:22.640 --> 01:10:26.360]   from new vision and 12 years is a really long time
[01:10:26.360 --> 01:10:28.480]   to have the same leader.
[01:10:28.480 --> 01:10:30.240]   And if I wasn't gonna stick it out
[01:10:30.240 --> 01:10:33.200]   for the entire Biden four-year term,
[01:10:33.200 --> 01:10:36.800]   it's good not to wait too late during that
[01:10:36.800 --> 01:10:38.680]   to signal an intent to step down
[01:10:38.680 --> 01:10:40.880]   'cause the president's gotta find the right person,
[01:10:40.880 --> 01:10:44.160]   gotta nominate them, gotta get the Senate to confirm them,
[01:10:44.160 --> 01:10:47.520]   which is a unpredictable process right now.
[01:10:47.520 --> 01:10:49.240]   And you don't wanna try to do that
[01:10:49.240 --> 01:10:52.800]   in the second half of somebody's term as president.
[01:10:52.800 --> 01:10:53.920]   This has gotta happen now.
[01:10:53.920 --> 01:10:56.400]   So I kind of decided back at the end of May
[01:10:56.400 --> 01:10:59.120]   that this should be my final year.
[01:10:59.120 --> 01:11:00.800]   And I'm okay with that.
[01:11:00.800 --> 01:11:05.800]   I do have some mixed emotions 'cause I love the NIH.
[01:11:05.800 --> 01:11:07.720]   I love the job.
[01:11:07.720 --> 01:11:09.640]   It's exhausting.
[01:11:09.640 --> 01:11:13.320]   I'm traditionally for the last 20 months anyway,
[01:11:13.320 --> 01:11:14.640]   working 100 hours a week.
[01:11:14.640 --> 01:11:18.840]   It's just, that's what it takes to juggle all of this.
[01:11:18.840 --> 01:11:21.320]   And that keeps me from having a lot of time
[01:11:21.320 --> 01:11:23.240]   for anything else.
[01:11:23.240 --> 01:11:25.840]   And I wouldn't mind, 'cause I don't think I'm done yet.
[01:11:25.840 --> 01:11:29.000]   I wouldn't mind having some time to really think
[01:11:29.000 --> 01:11:31.360]   about what the next chapter should be.
[01:11:31.360 --> 01:11:33.840]   And I have none of that time right now.
[01:11:33.840 --> 01:11:35.200]   Do I have another calling?
[01:11:35.200 --> 01:11:37.000]   Is there something else I could contribute
[01:11:37.000 --> 01:11:38.360]   that's different than this?
[01:11:38.360 --> 01:11:41.000]   I'd like to find that out.
[01:11:41.000 --> 01:11:46.040]   - I think the right answer is you're just stepping down
[01:11:46.040 --> 01:11:47.640]   to focus on your music career.
[01:11:47.640 --> 01:11:49.040]   (laughing)
[01:11:49.040 --> 01:11:50.560]   But-- - That might not be
[01:11:50.560 --> 01:11:53.960]   a good plan for anything, very sustainable.
[01:11:53.960 --> 01:11:56.120]   - But I think that is a sign of a great leader
[01:11:56.120 --> 01:11:59.880]   as George Washington did stepping down at the right time.
[01:11:59.880 --> 01:12:00.840]   - Ted Williams.
[01:12:00.840 --> 01:12:03.080]   - Yes.
[01:12:03.080 --> 01:12:06.000]   - He quit when, I think he hit a home run on his last at-bat
[01:12:06.000 --> 01:12:08.520]   and his average was 400 at the time.
[01:12:08.520 --> 01:12:10.400]   - No one to walk away.
[01:12:10.400 --> 01:12:13.240]   I mean, it's hard, but it's beautiful to see in a leader.
[01:12:13.240 --> 01:12:17.280]   You also oversaw the Human Genome Project.
[01:12:17.280 --> 01:12:19.280]   You mentioned the Brain Initiative,
[01:12:19.280 --> 01:12:24.280]   which has, it's a dream to map the human brain.
[01:12:24.280 --> 01:12:28.960]   And there's the dream to map the human code,
[01:12:28.960 --> 01:12:30.440]   which was the Human Genome Project.
[01:12:30.440 --> 01:12:32.840]   And you've said that it is humbling for me
[01:12:32.840 --> 01:12:34.920]   and awe-inspiring to realize
[01:12:34.920 --> 01:12:37.120]   that we have caught the first glimpse
[01:12:37.120 --> 01:12:42.120]   of our own instruction book, previously known only to God.
[01:12:42.120 --> 01:12:46.400]   How does that, if you can just kind of wax poetic
[01:12:46.400 --> 01:12:49.280]   for a second, how does it make you feel
[01:12:49.280 --> 01:12:52.760]   that we were able to map this instruction book,
[01:12:52.760 --> 01:12:57.600]   look into our own code and be able to reverse engineer it?
[01:12:57.600 --> 01:13:00.840]   - It's breathtaking.
[01:13:00.840 --> 01:13:02.680]   It's so fundamental.
[01:13:02.680 --> 01:13:05.720]   And yet, for all of human history,
[01:13:05.720 --> 01:13:08.640]   we're ignorant of the details
[01:13:08.640 --> 01:13:11.240]   of what that instruction book looked like.
[01:13:11.240 --> 01:13:13.480]   And then we crossed a bridge
[01:13:13.480 --> 01:13:16.200]   into the territory of the known.
[01:13:16.200 --> 01:13:17.720]   And we had that in front of us,
[01:13:17.720 --> 01:13:19.120]   still written in a language
[01:13:19.120 --> 01:13:21.600]   that we had to learn how to read.
[01:13:21.600 --> 01:13:23.240]   And we're in the process of doing that
[01:13:23.240 --> 01:13:25.080]   and will be for decades to come.
[01:13:25.080 --> 01:13:27.640]   But we owned it, we had it.
[01:13:27.640 --> 01:13:30.440]   And it has such profound consequences.
[01:13:30.440 --> 01:13:33.720]   It's both a book about our history.
[01:13:33.720 --> 01:13:39.520]   It's a book of sort of the parts list of a human being,
[01:13:39.520 --> 01:13:43.000]   the genes that are in there and how they're regulated.
[01:13:43.000 --> 01:13:45.400]   And it's also a medical textbook
[01:13:45.400 --> 01:13:48.720]   that can teach us things that will provide answers
[01:13:48.720 --> 01:13:50.960]   to illnesses we don't understand
[01:13:50.960 --> 01:13:53.520]   and alleviate suffering and premature death.
[01:13:53.520 --> 01:13:57.120]   So it's a pretty amazing thing to contemplate.
[01:13:57.120 --> 01:14:00.240]   And it has utterly transformed the way we do science.
[01:14:00.240 --> 01:14:02.360]   And it is in the process of transforming
[01:14:02.360 --> 01:14:04.480]   the way we do medicine,
[01:14:04.480 --> 01:14:07.080]   although much of that still lies ahead.
[01:14:07.080 --> 01:14:11.200]   You know, while we were working on the Genome Project,
[01:14:11.200 --> 01:14:16.200]   it was sort of hard to get this sense of a wow-ness
[01:14:16.200 --> 01:14:18.840]   because it was just hard work.
[01:14:18.840 --> 01:14:21.200]   And you were getting, you know, another megabase.
[01:14:21.200 --> 01:14:22.840]   Okay, this is good.
[01:14:22.840 --> 01:14:26.880]   But when did you actually step back and say, we did it.
[01:14:26.880 --> 01:14:29.680]   It's the profoundness of that.
[01:14:29.680 --> 01:14:31.760]   I mean, there were two points, I guess.
[01:14:31.760 --> 01:14:34.600]   One was the announcement on that June 26, 2000,
[01:14:34.600 --> 01:14:37.240]   where the whole world heard, well, we don't quite have it,
[01:14:37.240 --> 01:14:39.280]   but we got a pretty good draft.
[01:14:39.280 --> 01:14:41.600]   And suddenly people were like realizing,
[01:14:41.600 --> 01:14:44.000]   oh, this is a big deal.
[01:14:44.000 --> 01:14:48.280]   For me, it was more when we got the full analysis of it,
[01:14:48.280 --> 01:14:51.880]   published it in February, 2001, in that issue of Nature,
[01:14:51.880 --> 01:14:54.360]   paper that Eric Lander and Bob Waterston and I
[01:14:54.360 --> 01:14:55.800]   were the main authors.
[01:14:55.800 --> 01:14:59.480]   And we toiled over and tried to get as much insight
[01:14:59.480 --> 01:15:02.840]   as we could in there about what the meaning of all this was.
[01:15:02.840 --> 01:15:05.720]   But you also had this sense that we are
[01:15:05.720 --> 01:15:07.720]   such beginning readers here.
[01:15:07.720 --> 01:15:11.360]   We are still in kindergarten trying to make sense
[01:15:11.360 --> 01:15:14.640]   out of this 3 billion letter book.
[01:15:14.640 --> 01:15:17.400]   And we're gonna be at this for generations to come.
[01:15:17.400 --> 01:15:22.280]   - You are a man of faith, Christian,
[01:15:22.280 --> 01:15:25.160]   and you are a man of science.
[01:15:25.160 --> 01:15:29.320]   What is the role of religion and of science in society
[01:15:29.320 --> 01:15:34.120]   and in the individual human mind and heart like yours?
[01:15:34.120 --> 01:15:39.240]   - Well, I was not a person of faith when I was growing up.
[01:15:39.240 --> 01:15:42.200]   I became a believer in my 20s,
[01:15:42.200 --> 01:15:46.800]   influenced as a medical student by a recognition
[01:15:46.800 --> 01:15:48.880]   that I hadn't really thought through the issues
[01:15:48.880 --> 01:15:51.960]   of what's the meaning of life?
[01:15:51.960 --> 01:15:53.600]   Why are we all here?
[01:15:53.600 --> 01:15:55.280]   What happens when you die?
[01:15:55.280 --> 01:15:56.800]   Is there a God?
[01:15:56.840 --> 01:16:00.320]   Science is not so helpful in answering those questions.
[01:16:00.320 --> 01:16:02.680]   So I had to look around in other places
[01:16:02.680 --> 01:16:07.040]   and ultimately came to my own conclusion that atheism,
[01:16:07.040 --> 01:16:08.680]   which is where I had been,
[01:16:08.680 --> 01:16:11.480]   was the least supportable of the choices
[01:16:11.480 --> 01:16:14.800]   because it was the assertion of a universal negative,
[01:16:14.800 --> 01:16:16.760]   which scientists aren't supposed to do.
[01:16:16.760 --> 01:16:21.840]   And agnosticism came as an attractive option
[01:16:21.840 --> 01:16:23.520]   but felt a little bit like a cop-out.
[01:16:23.520 --> 01:16:26.000]   So I had to keep going, trying to figure out
[01:16:26.000 --> 01:16:28.400]   why do believers actually believe this stuff?
[01:16:28.400 --> 01:16:32.560]   And came to realize it was all pretty compelling.
[01:16:32.560 --> 01:16:33.560]   That there's no proof.
[01:16:33.560 --> 01:16:36.280]   I can't prove to you or anybody else that God exists,
[01:16:36.280 --> 01:16:38.440]   but I can say it's pretty darn plausible.
[01:16:38.440 --> 01:16:43.680]   And ultimately, what kind of God is it caused me
[01:16:43.680 --> 01:16:46.320]   to search through various religions and see,
[01:16:46.320 --> 01:16:48.520]   well, what do people think about that?
[01:16:48.520 --> 01:16:53.080]   And to my surprise, encountered the person of Jesus Christ
[01:16:53.080 --> 01:16:56.320]   as unique in every possible way
[01:16:56.320 --> 01:16:58.960]   and answering a lot of the questions
[01:16:58.960 --> 01:17:00.840]   I couldn't otherwise answer.
[01:17:00.840 --> 01:17:06.680]   And somewhat kicking and screaming, I became a Christian.
[01:17:06.680 --> 01:17:10.400]   Even though at the time, as a medical student
[01:17:10.400 --> 01:17:12.080]   already interested in genetics,
[01:17:12.080 --> 01:17:14.680]   people predicted my head would then explode
[01:17:14.680 --> 01:17:18.080]   'cause these were incompatible worldviews.
[01:17:18.080 --> 01:17:20.600]   They really have not been for me.
[01:17:20.600 --> 01:17:23.920]   I am so fortunate, I think, that in a given day,
[01:17:23.920 --> 01:17:27.160]   wrestling with an issue,
[01:17:27.160 --> 01:17:31.000]   it can have both the rigorous scientific component
[01:17:31.000 --> 01:17:33.040]   and it can have the spiritual component.
[01:17:33.040 --> 01:17:35.560]   COVID-19 is a great example.
[01:17:35.560 --> 01:17:39.640]   These vaccines are both an amazing scientific achievement
[01:17:39.640 --> 01:17:40.800]   and an answer to prayer.
[01:17:40.800 --> 01:17:44.760]   When I'm wrestling with vaccine hesitancy
[01:17:44.760 --> 01:17:47.320]   and trying to figure out what answers to come up with,
[01:17:47.320 --> 01:17:51.680]   I get so frustrated sometimes and I'm comforted
[01:17:51.680 --> 01:17:55.360]   by reassurances that God is aware of that.
[01:17:55.360 --> 01:17:56.840]   I don't have to do this alone.
[01:17:56.840 --> 01:18:02.480]   So I know there are people like your friend, Sam Harris,
[01:18:02.480 --> 01:18:04.760]   who feel differently.
[01:18:04.760 --> 01:18:09.280]   Sam wrote a rather famous op-ed in the New York Times
[01:18:09.280 --> 01:18:11.800]   when I was nominated as the NIH director
[01:18:11.800 --> 01:18:14.800]   saying, "This is a terrible mistake."
[01:18:14.800 --> 01:18:16.960]   You can't do this. - Oh no, Sam.
[01:18:17.960 --> 01:18:20.200]   You can't have somebody who believes in God
[01:18:20.200 --> 01:18:21.200]   running the NIH.
[01:18:21.200 --> 01:18:23.800]   He's just gonna completely ruin the place.
[01:18:23.800 --> 01:18:26.960]   - Well, I have a testimonial.
[01:18:26.960 --> 01:18:31.240]   Christopher Hitchens, a devout atheist, if I could say so,
[01:18:31.240 --> 01:18:33.640]   was a friend of yours and referred to you as,
[01:18:33.640 --> 01:18:36.520]   quote, "One of the greatest living Americans,"
[01:18:36.520 --> 01:18:39.120]   and stated that you were one of the most devout believers
[01:18:39.120 --> 01:18:40.680]   he has ever met.
[01:18:40.680 --> 01:18:43.280]   He further stated that you were sequencing the genome
[01:18:43.280 --> 01:18:46.080]   of the cancer that would ultimately claim his life
[01:18:46.080 --> 01:18:47.040]   and that your friendship,
[01:18:47.040 --> 01:18:50.000]   despite their differing opinions on religion,
[01:18:50.000 --> 01:18:55.000]   was an example of the greatest armed truce in modern times.
[01:18:55.000 --> 01:18:58.080]   What did you learn from Christopher Hitchens about life
[01:18:58.080 --> 01:19:00.960]   or perhaps what is a fond memory you have of this man
[01:19:00.960 --> 01:19:05.080]   with whom you've disagreed but who is also your friend?
[01:19:05.080 --> 01:19:08.240]   - Yeah, I loved Hitch.
[01:19:08.240 --> 01:19:10.640]   I'm sorry he's gone.
[01:19:10.640 --> 01:19:12.520]   Iron sharpens iron.
[01:19:12.520 --> 01:19:15.240]   There's nothing better for trying to figure out
[01:19:15.240 --> 01:19:18.120]   where you are with your own situation
[01:19:18.120 --> 01:19:20.680]   and your own opinions, your own world views,
[01:19:20.680 --> 01:19:24.320]   than encountering somebody who's completely in another space
[01:19:24.320 --> 01:19:26.520]   and who's got the gift, as Hitch did,
[01:19:26.520 --> 01:19:28.200]   of challenging everything
[01:19:28.200 --> 01:19:31.300]   and doing so over a glass of scotch or two or three.
[01:19:31.300 --> 01:19:35.240]   Yeah, we got off to a rough start
[01:19:35.240 --> 01:19:39.640]   where in an interaction we had at a rather highbrow dinner,
[01:19:39.640 --> 01:19:45.160]   he was really deeply insulting of a question I was asking,
[01:19:45.160 --> 01:19:47.800]   but I was like, okay, that's fine.
[01:19:47.800 --> 01:19:51.360]   Let's figure out how we can have a more civil conversation.
[01:19:51.360 --> 01:19:54.960]   And then I really learned to greatly admire his intellect
[01:19:54.960 --> 01:19:58.680]   and to find the jousting with him,
[01:19:58.680 --> 01:20:01.760]   and it wasn't all about faith, although it often was,
[01:20:01.760 --> 01:20:05.460]   was really inspiring and enervating, energizing.
[01:20:05.460 --> 01:20:07.960]   And then when he got cancer,
[01:20:07.960 --> 01:20:09.840]   I became sort of his ally,
[01:20:09.840 --> 01:20:13.920]   trying to help him find pathways through the various options
[01:20:13.920 --> 01:20:17.880]   and maybe helped him to stay around on this planet
[01:20:17.880 --> 01:20:19.760]   for an extra six months or so.
[01:20:19.760 --> 01:20:23.120]   And I have the warmest feelings
[01:20:23.120 --> 01:20:25.920]   of being in his apartment downtown
[01:20:25.920 --> 01:20:32.160]   over a glass of wine, talking about whatever.
[01:20:32.160 --> 01:20:34.880]   Sometimes it was science, he was fascinated by science.
[01:20:34.880 --> 01:20:37.840]   Sometimes it was Thomas Jefferson.
[01:20:37.840 --> 01:20:40.840]   Sometimes it was faith.
[01:20:40.840 --> 01:20:43.560]   And I knew it would always be really interesting.
[01:20:44.200 --> 01:20:45.680]   - So he's now gone.
[01:20:45.680 --> 01:20:46.520]   - Yeah.
[01:20:46.520 --> 01:20:49.960]   - Do you think about your own mortality?
[01:20:49.960 --> 01:20:51.680]   Are you afraid of death?
[01:20:51.680 --> 01:20:52.560]   - I'm not afraid.
[01:20:52.560 --> 01:20:53.840]   I'm not looking forward to it.
[01:20:53.840 --> 01:20:55.080]   I don't wanna rush it,
[01:20:55.080 --> 01:20:58.880]   'cause I feel like I got some things I can still do here.
[01:20:58.880 --> 01:21:02.520]   But as a person of faith, I don't think I'm afraid.
[01:21:02.520 --> 01:21:03.740]   I'm 71.
[01:21:03.740 --> 01:21:06.960]   I know I don't have an infinite amount of time left,
[01:21:06.960 --> 01:21:09.400]   and I wanna use the time I've got
[01:21:09.400 --> 01:21:12.240]   in some sort of way that matters.
[01:21:12.240 --> 01:21:15.120]   I'm not ready to become a full-time golfer.
[01:21:15.120 --> 01:21:17.720]   (laughing)
[01:21:17.720 --> 01:21:20.000]   But I don't quite know what that is.
[01:21:20.000 --> 01:21:23.320]   I do feel that I've had a chance
[01:21:23.320 --> 01:21:27.840]   to do amazingly powerful things as far as experiences,
[01:21:27.840 --> 01:21:30.600]   and maybe God has something else in mind.
[01:21:30.600 --> 01:21:34.400]   I wrote this book 16 years ago,
[01:21:34.400 --> 01:21:37.220]   "The Language of God," about science and faith,
[01:21:37.220 --> 01:21:39.840]   trying to explain how, from my perspective,
[01:21:39.840 --> 01:21:43.840]   these are compatible, these are in harmony.
[01:21:43.840 --> 01:21:46.280]   They're complementary if you are careful
[01:21:46.280 --> 01:21:48.520]   about which kind of question you're asking.
[01:21:48.520 --> 01:21:51.240]   And to my surprise, a lot of people
[01:21:51.240 --> 01:21:52.520]   seemed to be interested in that.
[01:21:52.520 --> 01:21:55.420]   They were tired of hearing the extreme voices,
[01:21:55.420 --> 01:22:00.960]   like Dawkins at one end, and people like Ken Ham
[01:22:00.960 --> 01:22:02.880]   and Answers in Genesis on the other end,
[01:22:02.880 --> 01:22:05.720]   saying if you trust science, you're going to hell.
[01:22:05.720 --> 01:22:07.620]   And they thought there must be a way
[01:22:07.620 --> 01:22:09.200]   that these things could get along,
[01:22:09.200 --> 01:22:10.680]   and that's what I tried to put forward.
[01:22:10.680 --> 01:22:13.480]   And then I started a foundation, BioLogos,
[01:22:13.480 --> 01:22:16.760]   which then I had to step away from to become NIH director,
[01:22:16.760 --> 01:22:19.360]   which has just flourished, maybe because I stepped away,
[01:22:19.360 --> 01:22:20.440]   I don't know. (laughing)
[01:22:20.440 --> 01:22:23.200]   But it now has millions of people
[01:22:23.200 --> 01:22:26.240]   who come to that website, and they run amazing meetings.
[01:22:26.240 --> 01:22:29.120]   And I think a lot of people have really come to a sense
[01:22:29.120 --> 01:22:32.240]   that this is okay, I can love science, and I can love God,
[01:22:32.240 --> 01:22:33.580]   and that's not a bad thing.
[01:22:33.580 --> 01:22:37.360]   So maybe there's something more I can do in that space.
[01:22:37.360 --> 01:22:40.280]   Maybe that book is ready for a second edition.
[01:22:40.280 --> 01:22:41.480]   - I think so.
[01:22:41.480 --> 01:22:44.960]   But when you look back, life is finite.
[01:22:44.960 --> 01:22:47.440]   What do you hope your legacy is?
[01:22:47.440 --> 01:22:51.240]   - I don't know, this whole legacy thing
[01:22:51.240 --> 01:22:54.200]   seems a little bit hard to embrace.
[01:22:54.200 --> 01:22:56.240]   It feels a little self-promoting, doesn't it?
[01:22:56.240 --> 01:22:57.840]   I sort of feel like in many ways,
[01:22:57.840 --> 01:23:01.080]   I went to my own funeral on October 5th
[01:23:01.080 --> 01:23:02.880]   when I announced that I was stepping down,
[01:23:02.880 --> 01:23:06.760]   and I got the most amazing responses from people,
[01:23:06.760 --> 01:23:08.000]   some of whom I knew really well,
[01:23:08.000 --> 01:23:10.120]   some of whom I didn't know at all,
[01:23:10.120 --> 01:23:12.080]   who were just telling me stories
[01:23:12.080 --> 01:23:15.280]   about something that I had contributed to
[01:23:15.280 --> 01:23:16.960]   that made a difference to them.
[01:23:16.960 --> 01:23:19.760]   And that was incredibly heartwarming, and that's enough.
[01:23:19.760 --> 01:23:22.440]   I don't wanna build an edifice,
[01:23:22.440 --> 01:23:25.720]   I don't have a plan for a monument or a statue,
[01:23:25.720 --> 01:23:27.680]   God help us. (laughing)
[01:23:27.680 --> 01:23:30.280]   I do feel like I've been incredibly fortunate,
[01:23:30.280 --> 01:23:32.800]   I've had the chance to play a role
[01:23:32.800 --> 01:23:35.200]   in things that were pretty profound
[01:23:35.200 --> 01:23:39.520]   from the Genome Project to NIH to COVID vaccines,
[01:23:39.520 --> 01:23:41.600]   and I ought to be plenty satisfied
[01:23:41.600 --> 01:23:46.160]   that I've had enough experiences here to feel pretty good
[01:23:46.160 --> 01:23:49.360]   about the way in which my life panned out.
[01:23:49.360 --> 01:23:53.400]   - We did a bunch of difficult questions in this conversation,
[01:23:53.400 --> 01:23:55.320]   let me ask the most difficult one,
[01:23:55.320 --> 01:24:01.060]   that perhaps is the reason you turned to God,
[01:24:01.060 --> 01:24:04.280]   what is the meaning of life? (laughing)
[01:24:05.120 --> 01:24:07.160]   Have you figured it out yet?
[01:24:07.160 --> 01:24:10.200]   - Expect me to put that into three sentences?
[01:24:10.200 --> 01:24:12.600]   - We only have a couple of minutes, so please hurry up.
[01:24:12.600 --> 01:24:15.520]   (laughing)
[01:24:15.520 --> 01:24:16.360]   - Well, that's not a question
[01:24:16.360 --> 01:24:18.080]   that I think science helps me with,
[01:24:18.080 --> 01:24:20.360]   so you're gonna push me into the faith zone,
[01:24:20.360 --> 01:24:23.480]   which is where I'd wanna go with that.
[01:24:23.480 --> 01:24:25.760]   I think, well, what is the meaning, why are we here,
[01:24:25.760 --> 01:24:27.280]   what are we put here to do?
[01:24:27.280 --> 01:24:31.640]   I do believe we're here for just a blink of an eye,
[01:24:31.640 --> 01:24:35.360]   and that our existence somehow goes on beyond that
[01:24:35.360 --> 01:24:37.760]   in a way that I don't entirely understand,
[01:24:37.760 --> 01:24:39.680]   despite efforts to do so.
[01:24:39.680 --> 01:24:43.920]   I think we are called upon in this blink of an eye
[01:24:43.920 --> 01:24:45.760]   to try to make the world a better place,
[01:24:45.760 --> 01:24:50.760]   to try to love people, to try to do a better job
[01:24:50.760 --> 01:24:55.800]   of our more altruistic instincts,
[01:24:55.800 --> 01:24:59.440]   and less of our selfish instincts,
[01:24:59.440 --> 01:25:03.080]   to try to be what God calls us to be,
[01:25:03.080 --> 01:25:07.940]   people who are holy, not people who are
[01:25:07.940 --> 01:25:11.920]   driven by self-indulgence.
[01:25:11.920 --> 01:25:13.920]   And sometimes I'm better at that than others.
[01:25:13.920 --> 01:25:15.320]   (laughing)
[01:25:15.320 --> 01:25:18.400]   But I think that, for me as a Christian, is a pretty clear,
[01:25:18.400 --> 01:25:22.020]   I mean, it's to live out the Sermon on the Mount.
[01:25:22.020 --> 01:25:25.920]   Once I read that, I couldn't unread it,
[01:25:27.480 --> 01:25:30.260]   all those beatitudes, all the blesseds.
[01:25:30.260 --> 01:25:32.960]   That's what we're supposed to do.
[01:25:32.960 --> 01:25:36.240]   And the meaning of life is to strive for that standard,
[01:25:36.240 --> 01:25:40.120]   recognizing you're going to fail over and over again,
[01:25:40.120 --> 01:25:41.440]   and that God forgives you.
[01:25:41.440 --> 01:25:44.440]   - Hopefully to put a little bit of love
[01:25:44.440 --> 01:25:45.480]   out there into the world.
[01:25:45.480 --> 01:25:47.400]   - That's what it's about.
[01:25:47.400 --> 01:25:52.400]   - Francis, I'm truly humbled and inspired
[01:25:52.400 --> 01:25:56.080]   by both your brilliance and your humility,
[01:25:56.080 --> 01:25:58.200]   and that you would spend your extremely
[01:25:58.200 --> 01:25:59.400]   valuable time with me today.
[01:25:59.400 --> 01:26:00.480]   It was really an honor.
[01:26:00.480 --> 01:26:02.080]   Thank you so much for talking today.
[01:26:02.080 --> 01:26:05.040]   - I was glad to, and you ask really good questions.
[01:26:05.040 --> 01:26:08.720]   So your reputation as the best podcaster
[01:26:08.720 --> 01:26:10.920]   has borne itself out here this afternoon.
[01:26:10.920 --> 01:26:12.720]   - Thank you so much.
[01:26:12.720 --> 01:26:14.240]   Thanks for listening to this conversation
[01:26:14.240 --> 01:26:15.460]   with Francis Collins.
[01:26:15.460 --> 01:26:17.280]   To support this podcast, please check out
[01:26:17.280 --> 01:26:19.160]   our sponsors in the description.
[01:26:19.160 --> 01:26:21.400]   And now, let me leave you with some words
[01:26:21.400 --> 01:26:24.780]   from Isaac Newton, reflecting on his life and work.
[01:26:25.720 --> 01:26:28.120]   I seem to have been only like a boy,
[01:26:28.120 --> 01:26:31.120]   playing on the seashore and diverting myself
[01:26:31.120 --> 01:26:33.820]   in now and then finding a smoother pebble
[01:26:33.820 --> 01:26:36.320]   or a prettier shell than ordinary,
[01:26:36.320 --> 01:26:38.720]   whilst the great ocean of truth
[01:26:38.720 --> 01:26:40.840]   lay all undiscovered before me.
[01:26:40.840 --> 01:26:44.760]   Thank you for listening, and hope to see you next time.
[01:26:44.760 --> 01:26:47.340]   (upbeat music)
[01:26:47.340 --> 01:26:49.920]   (upbeat music)
[01:26:49.920 --> 01:26:59.920]   [BLANK_AUDIO]

