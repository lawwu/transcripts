
[00:00:00.000 --> 00:00:05.840]   what drives the response to what happened with FTX and Alameda is that if you think the story
[00:00:05.840 --> 00:00:10.800]   is pure fraud, it's very easy to say you would never do that. I do think that the impact of
[00:00:10.800 --> 00:00:15.680]   new drugs on financial markets is underrated. It's maybe merciful that the atoms to bits
[00:00:15.680 --> 00:00:21.360]   interface has not been fully completed while we still have time to deal with malevolent,
[00:00:21.360 --> 00:00:27.920]   unfriendly EA. So, that's good. Your big concern is not your portfolio. In a world where an
[00:00:27.920 --> 00:00:33.440]   invasion is the reason AI did not happen. One possibility is that every other society got it
[00:00:33.440 --> 00:00:38.160]   wrong and that the monastic tradition was stupid and it has been independently discovered by
[00:00:38.160 --> 00:00:41.920]   numerous stupid civilizations that have all been around for much longer than effective altruism.
[00:00:41.920 --> 00:00:49.280]   Okay. Today, I have the pleasure of interviewing Bern Hobart again for the second time now,
[00:00:49.280 --> 00:00:56.800]   who writes at thediff.co. The way I would describe Bern is every time I have a question about a
[00:00:56.800 --> 00:01:02.560]   concept or an event in finance, I Google the name of that event or concept into Google and then I'd
[00:01:02.560 --> 00:01:08.000]   put in "Bern Hobart" at the end of that search query. And 9 times out of 10, it's the best thing
[00:01:08.000 --> 00:01:13.040]   I've read about that topic. And it's just so interesting. It's just like the most schizophrenic
[00:01:13.040 --> 00:01:22.080]   and galaxy brain it takes about how Machiavelli's discourses relate to big tech or how Soros'
[00:01:22.080 --> 00:01:28.080]   Theory of Real Flexibility explains hiring in finance and tech. So just very interesting stuff.
[00:01:28.080 --> 00:01:31.360]   I'm glad to have him back on again. Yeah, great to be back.
[00:01:31.360 --> 00:01:38.880]   Awesome. Okay. So first, I really want to jump into the FTX saga. What the hell happened? Let
[00:01:38.880 --> 00:01:46.000]   me just leave an open-ended question for you. Yeah. So I think the first thing to say is that
[00:01:46.000 --> 00:01:51.200]   there's a lot we don't know. There's a lot we may never know because so many of the decisions
[00:01:51.200 --> 00:01:58.480]   that FTX were made through self-auto-deleting encrypted chat. So there are some holes we will
[00:01:58.480 --> 00:02:03.120]   never be able to fill in. The lack of accounting is also going to make it tough. Basically, I think
[00:02:03.120 --> 00:02:09.360]   you can tell a bunch of different stories here. The really obvious one is fraud. And you can
[00:02:09.360 --> 00:02:14.480]   debate over exactly when it started. One version of the story, which is getting some currency,
[00:02:14.480 --> 00:02:21.120]   is that SPF had this entity, Alameda. And it was supposed to be this really hot crypto trading fund.
[00:02:21.120 --> 00:02:26.480]   But maybe it was a Ponzi scheme all along. And then maybe at some point, that Ponzi scheme
[00:02:26.480 --> 00:02:31.280]   started to run short on cash. So he decided to start an exchange. And the exchange got more cash.
[00:02:31.280 --> 00:02:35.120]   And then he used the cash to pay off briefs, investments, whatever. I think that's one
[00:02:35.120 --> 00:02:41.680]   version. And then the maximally exculpatory version, which actually is still really bad,
[00:02:41.680 --> 00:02:47.840]   is Alameda was a real company. They really made money trading. They took tons of risks.
[00:02:47.840 --> 00:02:55.280]   And SPF has talked about why he thinks that's a good thing. That FTX cut some corners when
[00:02:55.280 --> 00:03:01.040]   they were raising money. And that they had really bad internal accounting. And that basically,
[00:03:01.040 --> 00:03:08.960]   the extended entity of Alameda and FTX sort of lost track of whose money was where. And it ended
[00:03:08.960 --> 00:03:16.000]   up with Alameda spending FTX customer money. One way to look at that is, if you think, "Okay,
[00:03:16.000 --> 00:03:22.800]   fraud is twice as bad as just incompetently losing money." Well, it's not as if we had a $4
[00:03:22.800 --> 00:03:26.160]   billion fraud instead of $8 billion fraud. Everyone would be like, "Well, that's fine.
[00:03:26.160 --> 00:03:31.920]   That's normal. Why are you giving Sky hard time?" It's bad no matter what. Running a big company
[00:03:31.920 --> 00:03:35.920]   that is systemically important in crypto, and then having that company completely vaporize over the
[00:03:35.920 --> 00:03:40.560]   course of a couple days, really, really bad and worth understanding what happened. But it's partly
[00:03:40.560 --> 00:03:44.560]   worth understanding what happened because there are just different solutions that present themselves
[00:03:44.560 --> 00:03:48.400]   depending on what you think the story is. If the story is fraud, it's actually a lot harder to
[00:03:48.400 --> 00:03:55.840]   solve. Because there are just a lot of people who are willing and able to commit fraud and to lie.
[00:03:55.840 --> 00:04:01.120]   If the story is bad accounting, then that's actually a lot more solvable. Because then you
[00:04:01.120 --> 00:04:05.600]   could say things like, "The solution is, make sure you never invest in a crypto exchange that
[00:04:05.600 --> 00:04:12.640]   doesn't have a real auditor. And make sure that they have their proof of reserves calculation,
[00:04:12.640 --> 00:04:17.280]   and it's happening consistently, and that you can audit that." There are different solution sets.
[00:04:17.280 --> 00:04:23.120]   And then I think the actual story is going to be somewhere in the middle of extreme risk tolerance,
[00:04:23.120 --> 00:04:30.160]   plus extremely poor accounting, plus fraud at some point. But I suspect the fraud actually
[00:04:30.160 --> 00:04:36.400]   happened pretty late. If it happened, which I think there's 80, 90% chance that there was some
[00:04:36.400 --> 00:04:42.320]   level of fraud versus pure incompetence. But if so, I think may have happened fairly late in the
[00:04:42.320 --> 00:04:50.640]   story and as a last desperate move. I think part of what drives the response to what happened with
[00:04:50.640 --> 00:04:57.200]   FTX and Alameda is that if you think the story is pure fraud, it's very easy to say you would never
[00:04:57.200 --> 00:05:02.320]   do that. I can say very easily, I would definitely never start a Ponzi scheme and then start another
[00:05:02.320 --> 00:05:06.800]   bigger Ponzi scheme to pay off the first Ponzi scheme. That's not me. That's not most people.
[00:05:06.800 --> 00:05:13.280]   But I think if you draw the scenario where they discover at some point, a couple of months ago,
[00:05:13.280 --> 00:05:19.760]   or even a month ago, they realize, "Hey, there's a billion dollars plus that was supposed to be
[00:05:19.760 --> 00:05:23.840]   customer money, but we thought it was Alameda money, and we actually spent it. And now it's
[00:05:23.840 --> 00:05:29.520]   gone. We've lost it." What would you do in that circumstance? I think the ideal answer is, "Well,
[00:05:29.520 --> 00:05:34.880]   I'd immediately come clean and step down and commit myself to getting everyone paid back and
[00:05:34.880 --> 00:05:40.160]   made whole." I think there's also the possibility that the realistic answer is more like, "Well,
[00:05:40.160 --> 00:05:44.960]   I would scramble and try to make sure that that didn't cause the company to collapse and try to
[00:05:44.960 --> 00:05:51.440]   pay people back later." At that point, you've backed your way into fraud through earlier episodes
[00:05:51.440 --> 00:05:59.120]   of incompetence. But I think one of the problems with the fraud story is frauds have to be good at
[00:05:59.120 --> 00:06:04.960]   accounting. There's a very rough schematic sense. They have to be twice as good at accounting as
[00:06:04.960 --> 00:06:10.160]   everybody else because not only do they have to have the real books that tell them how much money
[00:06:10.160 --> 00:06:13.920]   the business has and whether or not the next check they're able to balance, but they have to have the
[00:06:13.920 --> 00:06:17.760]   fake set of books, and they have to have a way to make those tie out with one another.
[00:06:17.760 --> 00:06:24.720]   So accounting frauds tend to be fairly sophisticated. They tend to really dive
[00:06:24.720 --> 00:06:30.240]   into edge cases. I was reading up on MF Global, which was a big futures brokerage that collapsed
[00:06:30.240 --> 00:06:33.920]   in part because they were dipping into customer funds and making some investments they shouldn't
[00:06:33.920 --> 00:06:40.800]   have. And they did a lot of clever and shady stuff. One of the things they would do is,
[00:06:40.800 --> 00:06:44.640]   there was one point where they were transferring money at the last minute
[00:06:44.640 --> 00:06:51.680]   out of their customer funds in order to make margin calls. And what they would do is,
[00:06:51.680 --> 00:06:55.360]   they would send the wire from the customer account to a different company account.
[00:06:55.360 --> 00:06:59.440]   And they'd send it a couple of minutes before the wires closed for the night. And then they would
[00:06:59.440 --> 00:07:02.400]   send this email right after the wires closed saying, "Hey, we just realized we set this
[00:07:02.400 --> 00:07:06.320]   transfer fraud account. We've got to reverse it tomorrow." But that gave them at least one night
[00:07:06.320 --> 00:07:12.400]   of enough liquidity to survive. Now, you can only do that kind of fraud if you are actually
[00:07:12.400 --> 00:07:16.720]   keeping really close track of where your money is, where it's supposed to be, what the rules are,
[00:07:16.720 --> 00:07:22.480]   so that you know exactly how to break those rules. I don't think FTX was in any position
[00:07:22.480 --> 00:07:26.080]   to commit that kind of fraud. I think that if they tried to do something like that,
[00:07:26.080 --> 00:07:28.880]   they'd wire the money from an account that didn't have any money in it or something,
[00:07:28.880 --> 00:07:33.360]   or send it to the wrong account. There are these stories about them accidentally burning a bunch
[00:07:33.360 --> 00:07:37.840]   of USDC by sending it to an address that didn't exist or something like that. The operational
[00:07:37.840 --> 00:07:41.840]   slip ups actually make it harder for them to have committed fraud. And it's unquestionable
[00:07:41.840 --> 00:07:48.400]   at this point that their record keeping was very bad. Yeah, yeah. To your point about the fraud
[00:07:48.400 --> 00:07:53.200]   being harder, it's like a classic story about if you just tell the truth, it's just going to be
[00:07:53.200 --> 00:07:59.440]   much easier for you. You just don't have to keep track of that many things. But the one thing I've
[00:07:59.440 --> 00:08:04.400]   been thinking about, I interviewed him for an hour. And before that, I tried to do quite a bit
[00:08:04.400 --> 00:08:12.640]   of research into how FTX worked and what was going on. And I had this impression that this guy was
[00:08:12.640 --> 00:08:19.840]   the most competent genius that had ever graced finance. And this was a common impression.
[00:08:19.840 --> 00:08:28.800]   But it turns out that out of sheer incompetence, he loses track of billions of dollars,
[00:08:28.800 --> 00:08:33.360]   the internal operations, him putting together spreadsheets and throwing them around and
[00:08:33.360 --> 00:08:40.080]   putting emojis on Slack messages, asking for payments. And I just like, I want to understand
[00:08:40.080 --> 00:08:46.720]   how it is that this guy put out the impression out there that he is just hyper competent. And
[00:08:46.720 --> 00:08:51.360]   it turns out that it's like the opposite. It's not even that he's mediocre. It's the opposite.
[00:08:51.360 --> 00:08:57.120]   Right? Yeah. So I think you can tell a couple stories there. Like one story. And I know I've
[00:08:57.120 --> 00:09:00.080]   been saying a lot like you can tell multiple stories. There are multiple stories that fit
[00:09:00.080 --> 00:09:06.000]   the facts. We have lots of different weird things to explain and therefore many different weird
[00:09:06.000 --> 00:09:11.680]   explanations that fit them. So I think one version is, okay, he's never all that smart and decided
[00:09:11.680 --> 00:09:19.840]   that he could just play up this weird, eccentric genius thing and that he'd be able to get away
[00:09:19.840 --> 00:09:24.080]   with it. And there's these anecdotes about how someone told him to cut his hair and he said,
[00:09:24.080 --> 00:09:31.600]   "No, I have to look kind of crazy for this." And so that fits in. And it is kind of an MIT
[00:09:31.600 --> 00:09:36.240]   thing to do that, to play up your eccentricity because you know that there are these super
[00:09:36.240 --> 00:09:41.040]   brilliant, very eccentric people and you can be like them. It's kind of like, a lot of people,
[00:09:41.040 --> 00:09:47.600]   they read about Steve Jobs and they're like, "Well, the secret to success is be this brilliant
[00:09:47.600 --> 00:09:51.840]   perfectionist who can always see the future and also be just a giant asshole to everyone you meet."
[00:09:51.840 --> 00:09:54.800]   And I'm going to try to do both of those things. And it turns out one of those is really,
[00:09:54.800 --> 00:09:58.240]   really easy to do. And then one of them is really, really hard and you have to do both
[00:09:58.240 --> 00:10:04.880]   to be Steve Jobs. But you can give this service level impression of Jobsian-ness by just being
[00:10:04.880 --> 00:10:12.480]   really obnoxious to everyone. So I think some of it is that. But the other is that if you get really
[00:10:12.480 --> 00:10:18.880]   good at just very narrow domain-specific stuff, you might miss what other stuff people have to
[00:10:18.880 --> 00:10:25.680]   be good at for that skill set to be valuable. And so I think, thinking about his previous
[00:10:25.680 --> 00:10:32.800]   background where he worked at a prop trading firm and seemed to do well there. It's Jane Street,
[00:10:32.800 --> 00:10:37.200]   they're very, very selective in who they hire, very hard to get in, and they're very profitable,
[00:10:37.200 --> 00:10:43.200]   so good to get in. And it's entirely possible that part of what happened was just that
[00:10:44.000 --> 00:10:50.080]   Jane Street has its operations people, they have their trading people. And there may have been
[00:10:50.080 --> 00:10:57.040]   enough siloing within that, that if your job is just identify discrepancies in ETF prices and
[00:10:57.040 --> 00:11:01.600]   take advantage of them, you don't actually have to know things like, "How do we figure out which
[00:11:01.600 --> 00:11:04.880]   counterparties are creditworthy? How do we make sure we have enough liquidity? How do we have
[00:11:04.880 --> 00:11:08.480]   backup plans upon backup plans upon backup plans in case something goes wrong with our
[00:11:08.480 --> 00:11:15.840]   liquidity situation?" Because part of the Jane Street model seems to be, they're very opaque in
[00:11:15.840 --> 00:11:20.480]   terms of their trading operations, but part of the model seems to be that they want to be the
[00:11:20.480 --> 00:11:26.160]   trader who is there and trading and making a market when everything falls apart. And what that
[00:11:26.160 --> 00:11:31.600]   means is that the way you make the most money in trading is when markets are insanely volatile,
[00:11:31.600 --> 00:11:35.760]   volume is very, very high, and you're still trading. But the reason that markets get really
[00:11:35.760 --> 00:11:40.640]   volatile when prices collapse and there's a lot of chaos going on is that other people who would
[00:11:40.640 --> 00:11:46.880]   love to be trading can't trade because maybe the broker they use is suddenly insolvent and they
[00:11:46.880 --> 00:11:52.880]   can't get to a new broker, their money is frozen. So if you're planning to be there when everybody
[00:11:52.880 --> 00:11:56.800]   else is out of the market, then you have to have lots and lots of contingency plans. And it's not
[00:11:56.800 --> 00:12:02.000]   enough to buy lots of deep out of the money put options as Jane Street does, you also have to make
[00:12:02.000 --> 00:12:06.080]   sure that you're buying those options from the counterparty who will actually send you the money
[00:12:06.080 --> 00:12:12.080]   when you need it, or that you want to structure those things so the actual cash gets to your
[00:12:12.080 --> 00:12:18.000]   account at the time that it needs to be there. And that maybe is something that a prop trader
[00:12:18.000 --> 00:12:21.680]   should not be spending most of their time thinking about. It's one of those things where it's like,
[00:12:21.680 --> 00:12:30.240]   if you own a house and over the last 24 hours, you learned a whole lot about electrical wiring,
[00:12:30.240 --> 00:12:34.320]   or you learned a whole lot about how plumbing works, or how septic tanks work, that's not good.
[00:12:34.320 --> 00:12:39.040]   That means something very, very bad happened in your house. And it could be nice to be an expert
[00:12:39.040 --> 00:12:41.760]   on those things. But if you suddenly became an expert, it's because somebody else wasn't doing
[00:12:41.760 --> 00:12:50.000]   their job. So I think you could be a trader like that, where they can be very good at the finding
[00:12:50.000 --> 00:12:55.120]   little pricing discrepancies thing and have just no awareness of what the operation stuff is,
[00:12:55.120 --> 00:12:58.480]   especially because the better the operations team is, the less anyone else needs to be aware of
[00:12:58.480 --> 00:13:03.440]   them. You only email them when something is going wrong. So if nothing is going wrong,
[00:13:03.440 --> 00:13:09.680]   you never email them and then you forget they exist. Yeah, yeah, no, that's a good point. In
[00:13:09.680 --> 00:13:14.400]   fact, in the interview I did of him, he mentioned that I asked him, what is the difference between
[00:13:14.400 --> 00:13:20.720]   Chainstreet and FTX? And he mentioned that at Chainstreet, there was this button he could press
[00:13:20.720 --> 00:13:28.000]   to buy. And all the intermediaries, all the servers, it was just taken care of. And what's
[00:13:28.000 --> 00:13:33.360]   really funny is then he said, and just getting a bank account and he goes, and let's talk about
[00:13:33.360 --> 00:13:38.480]   that. Just getting a bank account is so hard when you're an independent. And it apparently turns out
[00:13:38.480 --> 00:13:42.400]   it's so hard that you might have like commingled funds because you couldn't manage to separate
[00:13:42.400 --> 00:13:48.080]   them out. Yeah, no, that's crazy. You had this really interesting take. I think one point we
[00:13:48.080 --> 00:13:53.360]   were talking about how every single market crash can be explained by the drug that was common in
[00:13:53.360 --> 00:14:00.240]   the industry at the time. And we finally achieved like the hypergrade meth stage of, I forgot the
[00:14:00.240 --> 00:14:04.480]   name of like that patch he was taking, but it's like stronger than Adderall or whatever, but.
[00:14:04.480 --> 00:14:09.280]   So it was, I think it's saying every crash could be explained by the drug they were taking at the
[00:14:09.280 --> 00:14:15.680]   time. I think it's a little far, but I do think that the impact of drugs, of new drugs on financial
[00:14:15.680 --> 00:14:22.320]   markets is underrated. And you can have examples of this going back pretty far. Like there is
[00:14:22.320 --> 00:14:27.680]   some connection between caffeine consumption and like extroversion and risk-taking. Like you
[00:14:27.680 --> 00:14:34.880]   temporarily get a little bit more willing to do deals when you consume caffeine. And in Lloyd's
[00:14:34.880 --> 00:14:39.280]   of London, before it was this insurance consortium, it was a coffee shop. It was Lloyd's coffee shop.
[00:14:39.280 --> 00:14:49.360]   So you do have some history of coffee shops being associated with financial centers. And then you
[00:14:49.360 --> 00:14:55.040]   have to zoom forward because we just haven't had that many novel stimulants, I guess, depressants,
[00:14:55.040 --> 00:14:58.800]   delirium, whatever, like other drug categories probably just don't lead to that much financial
[00:14:58.800 --> 00:15:05.040]   activity. Like I don't know how someone would trade differently or invest differently if they
[00:15:05.040 --> 00:15:11.040]   had a really strong asset trip or took ecstasy or something. But the stimulants where people can just
[00:15:11.040 --> 00:15:15.840]   consistently reuse them, they keep people alert, they make them active and wanting to do things.
[00:15:16.400 --> 00:15:19.600]   It seems like stimulants would have a connection with financial markets. So yeah, the theory is
[00:15:19.600 --> 00:15:26.240]   if you look at the 1980s, there were a lot of these hostile takeover deals where someone would
[00:15:26.240 --> 00:15:31.040]   find a company that's underperforming. And when you look at the spreadsheets and say,
[00:15:31.040 --> 00:15:34.240]   "This company is underperforming," what you're often looking at is a story that is more like,
[00:15:34.240 --> 00:15:38.160]   "This company believes that they have this social obligation to the community where people work,
[00:15:38.160 --> 00:15:42.800]   and that they have an obligation to give their customers a fairly-priced product. And they give
[00:15:42.800 --> 00:15:46.480]   them really good customer service that doesn't really pay for itself, and it's the right thing
[00:15:46.480 --> 00:15:52.880]   to do." Well, maybe especially if you are a cokehead with cokehead morality, you just say,
[00:15:52.880 --> 00:15:55.440]   "Well, that's not the right thing to do at all. We should actually just take the money,
[00:15:55.440 --> 00:15:58.320]   and we should fire these people and replace them with cheaper employees."
[00:15:58.320 --> 00:16:03.520]   So levering up a company, and then levering up in order to buy out a bigger company,
[00:16:03.520 --> 00:16:08.720]   and then firing everyone, and shutting down the pension plan, and distributing the surplus to
[00:16:08.720 --> 00:16:13.120]   shareholders, it is just very standard cokehead behavior. Whereas, if you look at the mortgage
[00:16:13.120 --> 00:16:17.840]   backed securities boom, and structured products generally in the mid-2000s, the way that people
[00:16:17.840 --> 00:16:22.080]   made money in that was just by being very, very detail-oriented, and being able to make these
[00:16:22.080 --> 00:16:27.280]   incredibly fine-grained distinctions between different products that were basically similar,
[00:16:27.280 --> 00:16:33.920]   but one of them pays 5.7%, and one of them pays 5.75%. And if you lever up that difference enough
[00:16:33.920 --> 00:16:38.720]   times, you're actually making really good money consistently. It's super boring, but maybe with
[00:16:38.720 --> 00:16:46.960]   enough Adderall, it's actually very tolerable work that you can enjoy. So I do think that within
[00:16:46.960 --> 00:16:52.880]   stimulants, the difference between short-acting stimulants and long-acting stimulants does mean
[00:16:52.880 --> 00:16:59.280]   the difference between a hostile takeover boom and a structured products boom. I think the drug is
[00:16:59.280 --> 00:17:05.360]   called MSEM or something, which is like a Parkinson's treatment. And there's some evidence
[00:17:05.360 --> 00:17:09.760]   from pretty small sample size studies that one of the side effects of this drug is compulsive
[00:17:09.760 --> 00:17:21.680]   gambling. So yeah, and the drug story, there have been very, very fun tweets about this claim. And
[00:17:21.680 --> 00:17:26.640]   then there have been these official denials from the company doctor. On the other hand, if you're
[00:17:26.640 --> 00:17:30.400]   a company that has a company doctor, maybe that says something about the level of medication
[00:17:30.400 --> 00:17:34.880]   you're consuming. And maybe the company doctor's job is partly to say, "As a doctor, I can assure
[00:17:34.880 --> 00:17:39.920]   you I would never give someone three times the normal dose of Adderall just because their boss
[00:17:39.920 --> 00:17:47.680]   hired me to do that specifically." I think dealers don't exactly have patient confidentiality norms,
[00:17:47.680 --> 00:17:52.320]   doctors do. So maybe you hire a doctor instead of a dealer specifically to get that plausible
[00:17:52.320 --> 00:17:58.800]   deniability. - Other than drugs, I also want to ask you about the phenotype of the founder. You
[00:17:58.800 --> 00:18:03.520]   wrote a post, I think it was just a couple of weeks before this crash happened, where you were
[00:18:03.520 --> 00:18:10.720]   pointing out that this idea of a founder who comes in shorts and a t-shirt and a crazy haircut. By
[00:18:10.720 --> 00:18:15.520]   the way, so FTX had a barber who would come in every Tuesday to cut everybody's hair. It might
[00:18:15.520 --> 00:18:22.000]   have been Thursday. And so he could have just sat in line and gotten his haircut. That was
[00:18:22.000 --> 00:18:28.400]   completely unnecessary, the way he dressed. And it was very purposeful. But yeah, so if that
[00:18:28.400 --> 00:18:33.200]   archetype of a founder who's in a t-shirt and shorts, if that's been priced in and that's
[00:18:33.200 --> 00:18:38.080]   beta instead of alpha now, what is the new phenotype and physiognomy of the founder? Where
[00:18:38.080 --> 00:18:45.680]   are you looking for alpha? - Well, I guess I would draw the distinction between the physical type of
[00:18:45.680 --> 00:18:53.920]   someone versus their presentation and their dress. I don't know. I'm sure someone could run some
[00:18:53.920 --> 00:18:59.280]   interesting numbers on that, but I don't have a good sense of what exactly they'd get from that.
[00:18:59.280 --> 00:19:05.200]   But in terms of how people publicly present themselves, my guess is that, yeah, there will
[00:19:05.200 --> 00:19:10.720]   be this swing towards investing in people who look a little bit more formal, a little bit more boring.
[00:19:10.720 --> 00:19:22.320]   And these things are somewhat cyclical. I think part of the norm on basically treating the suit
[00:19:22.320 --> 00:19:29.280]   as a negative signal is that a lot of investors have this view that when the MBAs come into an
[00:19:29.280 --> 00:19:36.640]   industry, a lot of the alpha is gone. And it is true that MBAs, at least there's... It's like a
[00:19:36.640 --> 00:19:40.880]   decent market timing signal, apparently, that if a lot of people from Harvard Business School go
[00:19:40.880 --> 00:19:46.640]   straight into some field, that field is probably peaking. There's a little bit to that, where the
[00:19:46.640 --> 00:19:52.080]   suit is some example of conformity. On the other hand, wearing a suit in Silicon Valley is an
[00:19:52.080 --> 00:19:57.840]   example of non-conformity. And I guess outside of New York, within the US, most of the time,
[00:19:57.840 --> 00:20:03.120]   wearing a suit as a tech company founder would be this weird sign that you're either like,
[00:20:03.120 --> 00:20:06.480]   you don't know what you're doing, you don't know what the right signals are, or you're about to
[00:20:06.480 --> 00:20:13.360]   testify to Congress, and that's why you have a suit now. Not generally a great sign, but maybe
[00:20:13.360 --> 00:20:18.720]   it is a sign that you are willing to do some more conformist things and that you could pay attention
[00:20:18.720 --> 00:20:26.000]   to details, the details are boring. And also that you're making some kind of financial investment
[00:20:26.000 --> 00:20:34.400]   in that particular appearance. So yeah, I would guess that there will be a tilt away from the
[00:20:34.400 --> 00:20:39.920]   hyper-informal founders. But I also think that if you treat that hyper-informality as either
[00:20:39.920 --> 00:20:44.560]   this attempt to game the system and just say, "I'm going to be as much... I'm going to try to
[00:20:44.560 --> 00:20:49.680]   remind people of Mark Zuckerberg circa 2005 as much as possible, so I can raise money and pretend
[00:20:49.680 --> 00:20:54.080]   to be the next big thing." That's one thing people are signaling. And then the other thing is they're
[00:20:54.080 --> 00:21:00.320]   just accidentally signaling total indifference to anything except the thing they're working on.
[00:21:00.320 --> 00:21:05.680]   And maybe that's a good thing, but maybe it's a good thing in unregulated domains,
[00:21:05.680 --> 00:21:09.440]   and then a really, really bad thing in regulated domains. If you're investing in a medical devices
[00:21:09.440 --> 00:21:15.440]   company, you probably don't want a founder who just cannot focus on anything except the product,
[00:21:15.440 --> 00:21:22.880]   because there are rules they have to follow and norms and things. It gets bad if all they're
[00:21:22.880 --> 00:21:28.720]   focused on is this one element. If the hyper-focus is just right, perfectly calibrated, that's good.
[00:21:28.720 --> 00:21:34.640]   But then maybe adjusting your appearance is this way to say that you have correctly
[00:21:34.640 --> 00:21:39.040]   calibrated your hyper-focus, and you're going to get one thing right. And it's going to be really,
[00:21:39.040 --> 00:21:42.080]   really right. You're going to get things right. They're going to be really, really right. And
[00:21:42.080 --> 00:21:43.840]   you've identified what things matter and what things don't.
[00:21:43.840 --> 00:21:50.400]   Yep. You'll lose track of your bank accounts. That's the dress itself. But I also want to ask
[00:21:50.400 --> 00:21:55.040]   about the other characteristics. You had this really interesting point in that blog post about
[00:21:55.040 --> 00:22:02.160]   how when you try to scout for talent when the talent is young, you're over-indexing for parental
[00:22:02.160 --> 00:22:07.200]   involvement. And I'm curious, if you had to identify somebody who had to be under the age
[00:22:07.200 --> 00:22:12.640]   of 18 or under the age of 20, what is the metric you're looking at that least indexes for parental
[00:22:12.640 --> 00:22:15.520]   involvement, where they're being forced or encouraged by their parents to do it?
[00:22:17.680 --> 00:22:24.880]   I think the closest you could get is something that is either totally illegible to the parent's
[00:22:24.880 --> 00:22:28.880]   status, like understanding of status, or something that is actively low status.
[00:22:28.880 --> 00:22:35.120]   And it's hard to enumerate those and not just get swamped in, "Well, should this thing be low
[00:22:35.120 --> 00:22:38.720]   status? Is it high status? Is it actually terrible to say that you'd ever want to hire someone who
[00:22:38.720 --> 00:22:44.400]   was really good at X for some value of X?" But I do think that you... So basically,
[00:22:44.960 --> 00:22:52.240]   the origin of that point was arguing that if you look at people who are at some percentile
[00:22:52.240 --> 00:22:58.400]   and they're in their 20s or 30s, at a high percentile, a lot of it has to be that they
[00:22:58.400 --> 00:23:02.240]   have some combination of talent and have tried really hard. There's probably been some element
[00:23:02.240 --> 00:23:08.000]   of luck. But over time, the luck starts to wash out, hopefully. But the younger you go...
[00:23:08.000 --> 00:23:12.800]   And this is probably just my experience with having kids. If you talk to your kids every day
[00:23:12.800 --> 00:23:16.480]   about multiplication, they will start doing multiplication at a pretty early age.
[00:23:16.480 --> 00:23:22.640]   And it's not that they are really, really smart, and they got to multiplication a couple of years
[00:23:22.640 --> 00:23:27.280]   early. It's that you push them in that direction, and they were able to do it early. So the earlier
[00:23:27.280 --> 00:23:31.520]   you go, the more you are over-indexing on what the parents did, what they emphasized, and also
[00:23:31.520 --> 00:23:36.320]   what they told the kids was just part of the script. And there are anecdotes about this from...
[00:23:36.320 --> 00:23:41.520]   None of the specifics come to mind. But I remember anecdotes about people who grew up
[00:23:41.520 --> 00:23:48.080]   in lower middle class or below circumstances, but would have one distant relative who owned a
[00:23:48.080 --> 00:23:52.400]   business. And that made them aware that they could own a business. And this is a thing they can do.
[00:23:52.400 --> 00:23:57.040]   It's part of the script now. And that wasn't the only reason that they would have started a
[00:23:57.040 --> 00:24:01.200]   business. But it could be a reason that they decided to do that when they did.
[00:24:01.200 --> 00:24:07.760]   And you have to imagine that for everyone who had one uncle who owned a scrap dealer or something,
[00:24:07.760 --> 00:24:13.280]   that maybe there are 5 or 10 or 50 people who grew up in similar circumstances, had a similar
[00:24:13.280 --> 00:24:19.440]   level of inability, and just didn't have anyone in their social circle who demonstrated to them
[00:24:19.440 --> 00:24:26.880]   that this was something you could actually do. So I think, getting back to the talent
[00:24:26.880 --> 00:24:31.680]   identification problem, but part of my thesis there was that it's really hard. And it's getting
[00:24:31.680 --> 00:24:37.520]   harder that you had Y Combinator going after the relatively young talent versus what the
[00:24:37.520 --> 00:24:42.960]   median VC was going after when YC started. And then stuff like Pioneer and Emergent Ventures
[00:24:42.960 --> 00:24:50.080]   is going even younger. And the younger you get, the more it is this luck-driven thing that is
[00:24:50.080 --> 00:24:56.240]   about what they got exposed to, with the exception of prodigies. So I'd like to think that if I
[00:24:56.240 --> 00:25:01.280]   encountered an eight-year-old Mozart, I would be able to identify this person as just an extraordinary
[00:25:01.280 --> 00:25:05.760]   talent. Even if their parents were making them practice 10 hours a day, they couldn't be that
[00:25:05.760 --> 00:25:12.160]   good without talent. And maybe something similar with the Polar Sisters, where if I encounter a
[00:25:12.160 --> 00:25:17.200]   six-year-old who can routinely beat me at chess, and so I go read some chess books and then go back
[00:25:17.200 --> 00:25:21.520]   and try to beat them again, and they're actually better, and they're laughing at me. At some point,
[00:25:21.520 --> 00:25:28.480]   you decide that this is actually natural talent. But for a lot of other domains, there's just so
[00:25:28.480 --> 00:25:33.360]   much room for parents to push one thing, and through some combination of their kid's talent
[00:25:33.360 --> 00:25:38.000]   and their own emphasis, to get their kids really good at it. And that's very hard to adjust for,
[00:25:38.000 --> 00:25:41.680]   especially because if you ask the parents, they're going to underestimate how much they
[00:25:41.680 --> 00:25:45.440]   overemphasize things, because to them, this is just a normal thing that everyone should be interested
[00:25:45.440 --> 00:25:51.200]   in. And so you won't get a good signal from asking parents, and then you won't get a good signal from
[00:25:51.200 --> 00:25:56.160]   asking other people, because they don't know how this family spends time at home. And if the median
[00:25:56.160 --> 00:26:04.000]   family has more YouTube and Netflix time and less math practice time, that family is just going to
[00:26:04.000 --> 00:26:10.480]   assume it's pretty much their behavior is normal. It's a bit confusing, because you also want to
[00:26:10.480 --> 00:26:15.840]   potentially include parental involvement in your estimate of how good this person will end up being.
[00:26:15.840 --> 00:26:23.120]   If you think, for example, that giving somebody a shot to get started programming early is actually
[00:26:23.120 --> 00:26:28.640]   a big factor in putting them on that loop where they get better by practicing and they enjoy it
[00:26:28.640 --> 00:26:36.320]   more and so on, you might expect momentum more than mean reversion in that early start.
[00:26:36.320 --> 00:26:43.200]   Sure. So I think part of what this gets to is the question of what are you optimizing for when
[00:26:43.200 --> 00:26:48.400]   you're doing a talent search? And I think this is maybe one reason there could be some alpha
[00:26:48.400 --> 00:26:56.160]   left in talent search among people who are super young, is that a lot of the academic institutions
[00:26:56.160 --> 00:27:00.960]   that are doing some form of talent search, what they're pretty much optimizing for is how does
[00:27:00.960 --> 00:27:08.320]   this person do over the next year? So if someone is a math prodigy, and they get to join the math
[00:27:08.320 --> 00:27:13.280]   team at that school, the school is not trying to optimize for will this person be proving novel
[00:27:13.280 --> 00:27:20.080]   theorems when they're 25? It's really, will this seven-year-old be doing algebra by the time
[00:27:20.080 --> 00:27:26.320]   they're eight? And that is still very tied to parental involvement, especially once parents
[00:27:26.320 --> 00:27:31.200]   like kids, they like structure. And if you tell them, this is the appropriate next thing to do
[00:27:31.200 --> 00:27:35.840]   with your kid, then they're more likely to do it. So you can post on that momentum for a while.
[00:27:35.840 --> 00:27:43.840]   But what I think you, the trap you can run into is that you identify people who are like 95th
[00:27:43.840 --> 00:27:50.160]   percentile talent with 99th percentile, just super aggressive parents. And that combination gets them
[00:27:50.160 --> 00:27:54.720]   to 99th percentile performance until they leave home. And then they never do whatever that thing
[00:27:54.720 --> 00:27:58.560]   is ever again, because they didn't really like it. It was just something their parents pressured
[00:27:58.560 --> 00:28:04.400]   them into. Now, maybe the ideal would be you get 99th percentile on both. So the parents are putting
[00:28:04.400 --> 00:28:08.400]   them on this trajectory, but the parents are actually aiming a very powerful rocket ship,
[00:28:08.400 --> 00:28:15.360]   and it's going to go right in the right direction, which is ideal. And I think there's a reasonable
[00:28:15.360 --> 00:28:21.760]   possibility of that. I think there's some level of just imprinting that young kids have where a lot
[00:28:21.760 --> 00:28:27.040]   of kids learn about programming when they're very young. And that's something that they do from a
[00:28:27.040 --> 00:28:31.280]   very, very early age. And then it becomes the thing that they work on for their entire career.
[00:28:32.320 --> 00:28:39.840]   Obviously, that has to be fairly new because it's not like anyone who was born before 1970 just had
[00:28:39.840 --> 00:28:45.920]   this constant yearning to program computers and could never satisfy it. Those kids found something
[00:28:45.920 --> 00:28:50.400]   else to do. Maybe a generation before, it was repairing transistor radios like he did when he
[00:28:50.400 --> 00:28:56.480]   was a kid. And maybe a century before that, it was experimenting by building little internal combustion
[00:28:56.480 --> 00:29:00.560]   engines and seeing whether or not they explode, like Henry Ford did with his friends at school.
[00:29:01.520 --> 00:29:07.280]   And maybe before that, the earlier you get, the harder it gets to really map these activities to
[00:29:07.280 --> 00:29:13.840]   anything concrete that we understand and can relate to. But there's probably some extent to
[00:29:13.840 --> 00:29:22.160]   which you can sort of direct kids into whatever the modern instantiation of this long-term enduring
[00:29:22.160 --> 00:29:28.800]   tendency is. And I guess one interesting example of that, I've been reading the Robert Caro LBJ
[00:29:28.800 --> 00:29:35.440]   biography. And there's this bit towards the end of the first volume where LBJ is put in charge of
[00:29:35.440 --> 00:29:41.120]   this fundraising organization for Democrats in Congress. And when you read about it, he sounds
[00:29:41.120 --> 00:29:45.840]   like a traitor. He sounds like someone who was just born to be slinging currency derivatives or
[00:29:45.840 --> 00:29:50.560]   something because he is constantly on the phone, constantly picking up rumors, constantly sending
[00:29:50.560 --> 00:29:55.600]   money here and there and everywhere else. And he's always sending money overnight and then sending
[00:29:55.600 --> 00:30:00.400]   someone a telegram the day before saying, "You're going to get a package from Lyndon Baines Johnson,
[00:30:00.400 --> 00:30:05.600]   and you're welcome." So he's doing this thing where he's constantly, relentlessly optimizing
[00:30:05.600 --> 00:30:11.200]   every little tiny detail of some very complicated process. Clearly, it requires enormous working
[00:30:11.200 --> 00:30:16.880]   memory, requires a very strong, basically a very strong poker face. He has to be able to
[00:30:16.880 --> 00:30:23.840]   differentiate between someone who is begging for money because they're pulling at 49% and with a
[00:30:23.840 --> 00:30:30.320]   little bit more money for newspaper ads, they'd get to 50.1% versus someone who just wants the
[00:30:30.320 --> 00:30:35.600]   money or just is constantly freaking out by their nature. So it requires a lot of the same character
[00:30:35.600 --> 00:30:41.040]   traits, but 1930s were just not a great time to go to Wall Street. Maybe if LBJ had been born at
[00:30:41.040 --> 00:30:44.800]   a slightly different time, that's just what he would have done. And he would have been a very
[00:30:44.800 --> 00:30:50.080]   successful private equity executive or something. But sometimes these general skills, they can
[00:30:50.080 --> 00:30:55.040]   translate into a lot of different areas, and they get honed into very specific skills through
[00:30:55.040 --> 00:30:59.360]   deliberate practice in those areas. So if you have that combination of natural tendency and
[00:30:59.360 --> 00:31:04.320]   some level of motivation, which in LBJ's case, his dad was also a politician. So he had this
[00:31:04.320 --> 00:31:08.720]   example of this is part of the life script, you can't do it. But he also had the example of his
[00:31:08.720 --> 00:31:14.720]   dad was broke after a while. And so he had this example of what not to do and ended up making
[00:31:14.720 --> 00:31:18.480]   good money for himself in addition to his political career. Yeah, yeah, yeah. I'm glad
[00:31:18.480 --> 00:31:23.280]   you brought in the biography. I'm reading it right now as well. And the other biography by
[00:31:23.280 --> 00:31:28.400]   Robert Caro, The Power Broker, just for the audience, the last episode, or the second to
[00:31:28.400 --> 00:31:33.760]   last episode in the feed is, but we go deep into deep into that biography and talk about why it
[00:31:33.760 --> 00:31:38.080]   might be inaccurate in certain respects. But what is what it is accurate. And I think what Carol has
[00:31:38.080 --> 00:31:44.960]   a genius in is talking about the personalities of these great, great men about the people who
[00:31:44.960 --> 00:31:51.280]   have really shaped their cities or their countries for decades and centuries. There's many places
[00:31:51.280 --> 00:31:54.800]   where I mean, I'm sure this is true for you, if you understand like the economics of an issue,
[00:31:54.800 --> 00:32:01.040]   he's talking about, there's a lot to be left to Carol's explanation. But the actual, like the,
[00:32:01.040 --> 00:32:06.000]   the sort of breakdown of the personalities is just so fascinating and worth reading Carol for.
[00:32:06.000 --> 00:32:10.640]   But you know, come to think of it, so maybe the difference between the cases where you want to
[00:32:10.640 --> 00:32:16.320]   price in the parents involvement and the ones where you don't is where in situations, like maybe
[00:32:16.320 --> 00:32:22.480]   being a politician where it really is about building a network, building know-how, building
[00:32:22.480 --> 00:32:27.680]   this sort of inarticulable knowledge from an early age. It might be the case that in those situations,
[00:32:27.680 --> 00:32:32.000]   just having connections and having parental involvement gets you far. But if it's like
[00:32:32.000 --> 00:32:36.400]   becoming a programmer, sure, you'll like have done data structures by the time you're 16,
[00:32:36.400 --> 00:32:40.640]   but eventually you'll get to the point where, you know, everybody knows the basics and now you
[00:32:40.640 --> 00:32:45.360]   actually had to do interesting and cool things in computer science. And now you're like a 95th
[00:32:45.360 --> 00:32:50.160]   percentile of spatial reasoning IQ is not going to get you that far. But let me ask you about the
[00:32:50.160 --> 00:32:53.840]   Carol biography, because you had a really interesting comment that I've been wondering
[00:32:53.840 --> 00:32:59.280]   about as well in your, in your review of the book or in your comment about the book, you said,
[00:32:59.280 --> 00:33:05.360]   it's worth speculating on how many LBJ level figures exist today, perhaps in domains outside
[00:33:05.360 --> 00:33:12.000]   of politics and how many Carol level biographers there are who could do them justice. So do you
[00:33:12.000 --> 00:33:16.960]   have some idea of who these figures are, or if not that, at least what areas you'd expect them to be
[00:33:16.960 --> 00:33:24.560]   in? I think a lot of people who are close to that tier and have some of the same personality types
[00:33:24.560 --> 00:33:29.280]   are in sales and corporate development and stuff like that, where they, you know, they're, they're
[00:33:29.280 --> 00:33:34.000]   building a big network. They are constantly building out this giant levered balance sheet
[00:33:34.000 --> 00:33:38.800]   of favors, you know, favors owed to them, favors they owe to other people. And like all forms of
[00:33:38.800 --> 00:33:43.360]   leverage, it does allow you to grow a lot faster, but you occasionally run into these big, big
[00:33:43.360 --> 00:33:51.360]   blowups. So that's, that's one place I would look. I think if you try to look at the more, you know,
[00:33:51.360 --> 00:33:57.920]   pure executive founder types, then it gets harder to find someone who would have exactly that kind
[00:33:57.920 --> 00:34:06.080]   of personality. It's like part of what made LBJ's methods work was that he was adjacent to a bunch
[00:34:06.080 --> 00:34:11.840]   of these really big institutions and he could sort of siphon off some of the power that these
[00:34:11.840 --> 00:34:16.080]   institutions had, and in some cases could make them more powerful. So I'm about a third of the
[00:34:16.080 --> 00:34:18.960]   way through master of the Senate right now. So it's, it's just getting to the point where it's
[00:34:18.960 --> 00:34:25.040]   really getting cooking and really making the Senate more, more effective than it used to be.
[00:34:25.040 --> 00:34:32.960]   And also making it an organization where someone where it's less seniority based. So you kind of,
[00:34:32.960 --> 00:34:38.400]   you need to be attached to something much bigger than yourself for that particular skill set to
[00:34:38.400 --> 00:34:42.240]   work really well. That said, you could have a really big impact because it is, it's another
[00:34:42.240 --> 00:34:47.440]   form of leverage. So if you are one of a hundred senators, or I guess at the point, at that point,
[00:34:47.440 --> 00:34:54.240]   it was 96 senators, and you're, you're able to exert a lot more influence and be, you know,
[00:34:54.240 --> 00:35:00.320]   be the equivalent to 40 senators, for example, then you can get a whole lot done because it's,
[00:35:00.320 --> 00:35:04.960]   it's the US Senate. But if you have that same kind of skill set and you're the CEO of your company,
[00:35:04.960 --> 00:35:08.880]   well, you're, you're already in front of the company. Like there's only so much extra force
[00:35:08.880 --> 00:35:14.320]   you can exert. So you, you kind of see a figure with exactly that kind of personality trait
[00:35:14.320 --> 00:35:19.280]   in a case where there are big institutions that have slowed down somewhat. And this is another
[00:35:19.280 --> 00:35:24.000]   interesting point that is raised early in the Senate is that the Senate was getting old. And
[00:35:24.000 --> 00:35:28.000]   if you look at these long-term charts of average age of politicians, we're,
[00:35:28.000 --> 00:35:33.440]   we're definitely in a bull market for extremely, extremely old politicians in the US right now.
[00:35:33.440 --> 00:35:38.320]   But we've gone through cycles before. And one of the things that, that tends to cause a reset is
[00:35:38.320 --> 00:35:44.640]   the war where wars among other things, cause this huge reset in social capital. So the people who
[00:35:44.640 --> 00:35:49.760]   made mistakes in the early stages all get discredited. And then the, the social bonds
[00:35:49.760 --> 00:35:55.600]   that people forge from actually fighting alongside one another and the prestige you get from actually
[00:35:55.600 --> 00:36:01.120]   being part of the winning side, that is very hard to replicate. And so you end up with
[00:36:01.120 --> 00:36:05.920]   much younger people in much, you know, in positions of a lot more power, whereas the,
[00:36:07.920 --> 00:36:13.520]   the, the way that that worked a decade and a half earlier was in the 1930s,
[00:36:13.520 --> 00:36:17.120]   there just weren't a lot of organizations that were hiring heavily and looking for
[00:36:17.120 --> 00:36:19.920]   really ambitious young people who are going to shake things up, but the US government was.
[00:36:19.920 --> 00:36:25.760]   So that's, that's how LBJ got in and started on his path was that the New Deal created these big
[00:36:25.760 --> 00:36:31.200]   programs like the National Youth Administration, and they needed people like Johnson to, to run
[00:36:31.200 --> 00:36:36.160]   them. So when you look at, when you look at an industry that is aging, it's usually an industry
[00:36:36.160 --> 00:36:41.680]   where ambitious people stay away from it. Like they recognize it's becoming more seniority focused
[00:36:41.680 --> 00:36:47.760]   and there's just less going on, but there becomes this huge opportunity when the aging stops because
[00:36:47.760 --> 00:36:52.320]   a bunch of people either retire or they get discredited and have to leave. And suddenly
[00:36:52.320 --> 00:36:56.880]   the average age of the industry ratchets down. And you can basically look at the set of opportunities
[00:36:56.880 --> 00:37:01.840]   that were missed over the previous decade, for example, because because the industry was
[00:37:01.840 --> 00:37:06.720]   whatever this institution was, was too risk averse. You, you get to take all of those
[00:37:06.720 --> 00:37:11.440]   opportunities at once. So you have tons and tons of low hanging fruit when that shift happens.
[00:37:11.440 --> 00:37:16.080]   So I think that's, that's the other thing to look for is look for cases where there's some,
[00:37:16.080 --> 00:37:21.600]   some institution, some part of the economy or society that has just been slowing down for a
[00:37:21.600 --> 00:37:25.840]   long time, clearly getting to the limit of whatever its current operating model is,
[00:37:25.840 --> 00:37:31.360]   hasn't found a new model. And there's someone young and disruptive who's just entering it. So
[00:37:31.360 --> 00:37:37.680]   I mean, maybe, maybe the place to look for the next LBJ is someone doing independent films and
[00:37:37.680 --> 00:37:44.160]   someone who looks at the top box office results and sees that everything is a spinoff of a spinoff
[00:37:44.160 --> 00:37:49.440]   of a spinoff. And it's, you know, 50% Marvel and says, this is disgusting. We have to destroy it.
[00:37:49.440 --> 00:37:52.800]   And I'm going to build something completely different. Like maybe that person is actually
[00:37:52.800 --> 00:37:57.920]   the kind of LBJ archetype. Now, the other half of this question is the Kero archetype. And part of
[00:37:57.920 --> 00:38:04.080]   what I found fun about this was that I felt like Kero had this kind of, like, he was kind of
[00:38:04.080 --> 00:38:08.880]   disgusted with himself when he realized how similar is some of his methods were to LBJs,
[00:38:08.880 --> 00:38:14.640]   because he's writing this story about this guy who's will do anything to make a sort of friendship,
[00:38:14.640 --> 00:38:19.280]   but it's really a fake friendship just to accomplish his goals. And he's constantly doing,
[00:38:19.280 --> 00:38:23.200]   doing the reading that other people aren't doing and doing the work and making the calls and
[00:38:23.200 --> 00:38:27.520]   reiterating and reiterating, reiterating just endless patients. And then you read about how Kero
[00:38:27.520 --> 00:38:32.080]   works. And he does things like moves to DC for a while, talks to everyone in DC, befriends people,
[00:38:32.080 --> 00:38:37.280]   moves to Texas, talks, you know, moves to the hill country and gets to know people there.
[00:38:37.280 --> 00:38:42.000]   He has these anecdotes in the book, because the book is like, it's sort of has these hints of
[00:38:42.000 --> 00:38:47.440]   gonzo journalism, where sometimes Kero will just narrate, it's that he'll, he will go from,
[00:38:47.440 --> 00:38:52.880]   here's what happened in 1946, to here's what happened to me in the 70s, while I was talking
[00:38:52.880 --> 00:38:58.400]   to this guy about what he did in 1946. And sometimes he will basically come out and say,
[00:38:58.400 --> 00:39:04.320]   I waited until the person who paid this bribe had Alzheimer's. And then I asked him if he
[00:39:04.320 --> 00:39:07.440]   remembered paying the bribe, and he remembered that he did it and didn't remember he wasn't
[00:39:07.440 --> 00:39:12.720]   supposed to say it. So that's how I know. And there's this line that Kero keeps quoting from
[00:39:12.720 --> 00:39:19.440]   LBJ, which I think was from LBJ's speech coach days, or speech, like, debate team coach days,
[00:39:19.440 --> 00:39:26.400]   where his line was, if you do everything, you will win. And Kero does everything. So I think
[00:39:26.400 --> 00:39:31.360]   probably the population of Kero's is smaller than the population of LBJ's, because the people who
[00:39:31.360 --> 00:39:36.640]   have that skill set probably have ambitions other than writing a canonical book about one particular
[00:39:36.640 --> 00:39:43.600]   person, or writing two canonical books, two canonical works on two important people. But
[00:39:43.600 --> 00:39:46.160]   maybe a lot of those people are just doing things other than typing.
[00:39:46.160 --> 00:39:53.360]   Man, there's so many threads there that I'm tempted to just spend the rest of the episode
[00:39:53.360 --> 00:39:59.040]   just digesting and talking about it. But one thing that, like, there's so many interesting
[00:39:59.040 --> 00:40:06.320]   things about Kero's story, and I guess the impact it's had. One of them is, there's been this focus
[00:40:06.320 --> 00:40:10.960]   in terms of thinking about impact, especially in, like, circles like Effective Altruism of trying to
[00:40:10.960 --> 00:40:16.400]   crunch the numbers. And there's no reasonable crunching of the numbers you could have come up
[00:40:16.400 --> 00:40:21.280]   with before The Power Broker was written, where you say, I'm going to spend, by the way, this is,
[00:40:21.280 --> 00:40:24.800]   he tries to downplay his accomplishments as a journalist before he wrote The Power Broker,
[00:40:24.800 --> 00:40:29.120]   but he was nominated for the Pulitzer Prize for his journalism before The Power Broker.
[00:40:29.120 --> 00:40:34.320]   So he's like a top level investigative journalist. And then you say, here's, I'm going to spend my
[00:40:34.320 --> 00:40:40.320]   talents, I'm going to spend eight years looking into and researching every conceivable person who
[00:40:40.320 --> 00:40:46.320]   has even potentially been in the same room as or been impacted by Robert Moses. And I'm going to
[00:40:46.320 --> 00:40:50.160]   document all of this, I'm going to write a book where that's like a million words or something.
[00:40:50.160 --> 00:40:55.040]   But in fact, that's, he probably didn't think about it this way, right? But what was the result?
[00:40:55.040 --> 00:40:59.920]   That book probably changed how many of the most influential people who came up through politics
[00:41:00.560 --> 00:41:04.880]   think about politics, think it probably changed how urban governance is done, how we think about
[00:41:04.880 --> 00:41:08.080]   accountability and transparency for good or ill, right, depending on your perspective.
[00:41:08.080 --> 00:41:13.600]   And just that example alone really makes me suspect the sort of number crunching way of
[00:41:13.600 --> 00:41:18.320]   thinking about what to do. And rather just like, I don't know, I got to understand how to, you know,
[00:41:18.320 --> 00:41:21.680]   from a precarious perspective, I got to understand how this guy accumulated this power. He doesn't,
[00:41:21.680 --> 00:41:24.800]   and it like completely transforms, you know, how urban governance is done.
[00:41:24.800 --> 00:41:29.920]   Yeah, you know, it actually kind of looping back to the parental influence thing. I think part of
[00:41:29.920 --> 00:41:34.160]   what happened was that the more Cairo dug into it, the more he realized this is actually a big
[00:41:34.160 --> 00:41:39.760]   and compelling project. And there's this kind of fun phenomenon that you can get when you're
[00:41:39.760 --> 00:41:46.640]   researching something where you've read enough that when you read something new, and you see
[00:41:46.640 --> 00:41:51.280]   that there's a footnote, you actually know what is going to be cited in that footnote. And maybe
[00:41:51.280 --> 00:41:55.280]   you've also read the thing about how the thing in that footnote is wrong, and here's why.
[00:41:55.280 --> 00:42:01.360]   And, you know, you're picking up information a lot faster, you get that nice convexity where
[00:42:01.360 --> 00:42:06.080]   you can skim through the stuff you know, and everything you read is new information and
[00:42:06.080 --> 00:42:11.440]   challenges something about what you previously knew. And that's just a really intoxicating feeling.
[00:42:11.440 --> 00:42:16.560]   And I can imagine that it's even more fun if you're actually digging up the primary sources. So,
[00:42:16.560 --> 00:42:20.080]   you know, if you're Cairo, you've gone through the New York Times archives, you've read through
[00:42:20.080 --> 00:42:25.200]   all of the all the external coverage of what people said about those time, and then you start
[00:42:25.200 --> 00:42:30.240]   talking to people and you realize, here are things that we got completely wrong. Like, we thought
[00:42:30.240 --> 00:42:34.000]   Moses didn't want X to happen. And it turns out that he kept scheming and plotting to make X
[00:42:34.000 --> 00:42:40.800]   happen and just wanted to pretend that it wasn't his doing. You so I think that, but what happens
[00:42:40.800 --> 00:42:44.640]   is you, you build this ongoing motivation, and then you can you can make something that you just
[00:42:44.640 --> 00:42:50.720]   wouldn't be able to make before. And I think if, if you start out saying, I'm going to write a
[00:42:50.720 --> 00:42:58.960]   million words about how cities are run, you will probably fail. But if you keep writing another
[00:42:58.960 --> 00:43:05.360]   500 words a day about how Robert Moses operated and what he did, and then you have some reflections
[00:43:05.360 --> 00:43:10.480]   throughout that on what that city is, then then maybe maybe you actually get there in the end.
[00:43:10.480 --> 00:43:15.600]   So, and, and maybe some of this is like you, you want to have an adversary, like a lot of these,
[00:43:15.600 --> 00:43:20.640]   like the Karo books do seem partly to be this cross examination of, of who he's writing about.
[00:43:20.640 --> 00:43:26.560]   And often, he, he seems to have very mixed feelings, like he, you know, with, I think one of
[00:43:26.560 --> 00:43:32.640]   the one of the really interesting things in, in the years of Lyndon Johnson is the Karo's description
[00:43:32.640 --> 00:43:39.440]   of Coke Stevenson and how they contrast him with FBJ, because it's really clear that Karo's politics
[00:43:39.440 --> 00:43:45.360]   are completely opposed to Stevenson's and that when Karo's writing about LBJ, there's like the
[00:43:45.360 --> 00:43:50.160]   good stuff he did, which is the Great Society and his, his participation in the New Deal. And then
[00:43:50.160 --> 00:43:55.280]   there's the bad stuff, which is anything that wasn't bad. And so he clearly like, he likes
[00:43:55.280 --> 00:44:01.360]   what LBJ accomplished and despises the person and then really likes the person of Coke Stevenson
[00:44:01.360 --> 00:44:07.520]   and kind of wishes him well, but also doesn't actually want people like that to be in charge
[00:44:07.520 --> 00:44:14.320]   of anything. And so it's like a, you know, it's partly, partly Karo debating with his subject
[00:44:14.320 --> 00:44:19.520]   and interrogating his subject and partly Karo debating with himself and asking these very
[00:44:19.520 --> 00:44:24.320]   longstanding questions about whether or not justify the ends. And, you know, would it be
[00:44:24.320 --> 00:44:32.000]   worth it to not have a Great Society in exchange for not letting LBJ steal an election in 1948?
[00:44:32.000 --> 00:44:37.040]   And I don't think that, like, if he's good at his writing, he shouldn't be coming to firm
[00:44:37.040 --> 00:44:42.160]   conclusions on that. And he should be presenting this very, very mixed picture where you really
[00:44:42.160 --> 00:44:47.040]   only get the things you really want. If you also accept that there are some very bad things that
[00:44:47.040 --> 00:44:51.120]   come along with that, as long as, as long as the things you want come from powerful,
[00:44:51.120 --> 00:44:56.320]   ambitious people who will do anything to win. Yep. Yep. No, and it's worth remembering that
[00:44:56.320 --> 00:45:00.800]   it takes him a decade to write each of those volumes and each of, I guess, in the case of
[00:45:00.800 --> 00:45:05.280]   the Power Broker or that entire book. But in the course of a decade, just imagine how many times
[00:45:05.280 --> 00:45:09.360]   you would change your mind on a given subject. And you really notice this when you read different
[00:45:09.360 --> 00:45:13.760]   paragraphs of like, for example, the Power Broker, where you notice early on, if you just read the
[00:45:13.760 --> 00:45:18.960]   first third or the first half of the Power Broker, you're like, clearly Karo is like writing about
[00:45:18.960 --> 00:45:24.560]   Robert Moses, the way he writes about Robert Lyndon Johnson, where it's like, yeah, this guy
[00:45:24.560 --> 00:45:28.320]   had some flaws, but like, look at the cool shit he did and the awesome stuff he did for New York.
[00:45:28.320 --> 00:45:34.160]   And then the tone completely changes. But you got to remember, it's he's just writing this so many
[00:45:34.160 --> 00:45:40.400]   years in between. I do want to talk about the thing about young people being able to,
[00:45:40.400 --> 00:45:46.000]   you know, young people, I guess, a war being a catalyst for young people entering an arena.
[00:45:46.000 --> 00:45:51.520]   I did an interview of Alexander Mikhailovich, I forgot his last name, but anyways, he wrote a
[00:45:51.520 --> 00:45:55.120]   really interesting book about Napoleonic Wars. And this is actually one of the things we talked
[00:45:55.120 --> 00:46:01.760]   about. There's a line from War and Peace where one of the Russian aristocrats is mad that his son
[00:46:01.760 --> 00:46:08.400]   is joining is joining the war. And he goes, you know, it's that man, Napoleon, you've all seen
[00:46:08.400 --> 00:46:15.360]   him and now you all want to like go off to war. And I'm curious, like, filmmaking doesn't seem
[00:46:15.360 --> 00:46:20.720]   like we're super quantitative and super smart and super competent, like somebody who has simos and
[00:46:20.720 --> 00:46:26.720]   the desire to dominate and the desire to achieve recognition. I mean, do you really think he's
[00:46:26.720 --> 00:46:31.280]   making films? Like, where is he really? Is he like still trying to start a startup? Or is that like
[00:46:31.280 --> 00:46:34.720]   now a decade too old? And now he's trying to dominate some other arena?
[00:46:34.720 --> 00:46:41.120]   I mean, maybe the lame answer is we don't actually know because the way like, you know,
[00:46:41.120 --> 00:46:46.400]   Paul Graham has that essay about the trope of startups starting in garages. And I think it's
[00:46:46.400 --> 00:46:50.320]   called the power of the marginal. And it's all about how the really interesting projects are
[00:46:50.320 --> 00:46:54.000]   the ones that can barely get off the ground, because they're so weird and so out there that
[00:46:54.000 --> 00:46:58.240]   there's no infrastructure to support them. And what that ends up doing is selecting people who
[00:46:58.240 --> 00:47:02.880]   are extremely passionate about that project, and also people who are extremely willful and will get
[00:47:02.880 --> 00:47:10.800]   impossible things done. So you it's hard to just rattle off a bunch of examples of that because
[00:47:10.800 --> 00:47:16.080]   you your hit rate would be like 99 things out of 100 are just like things you read one fun blog
[00:47:16.080 --> 00:47:20.240]   post speculating about and they're actually never going to happen. And then you know, one of them
[00:47:20.240 --> 00:47:24.880]   maybe maybe you're right, but it's very hard to tell which one it is. And you know, if it were
[00:47:24.880 --> 00:47:33.040]   if it were easy, venture capital would not have such skewed returns. So yeah. So maybe, maybe it
[00:47:33.040 --> 00:47:40.240]   is like harder to optimize for what area do you look for? Maybe it's actually easier to do the
[00:47:40.240 --> 00:47:47.760]   meta optimization of identifying the things you would quit, you know, quit podcasting and go work
[00:47:47.760 --> 00:47:52.800]   on given the opportunity. And, you know, it's like good to have that sort of dread list. Like,
[00:47:52.800 --> 00:47:58.160]   I, I had that mental list of like, you know, if someone at Spotify, ping me, and they're like,
[00:47:58.160 --> 00:48:04.960]   we really need a product manager who can help us display classical music such that we don't list
[00:48:04.960 --> 00:48:09.440]   like tons of redundant information and the first 50 characters of the track name and the actual
[00:48:09.440 --> 00:48:13.600]   incremental useful information in the 10 characters, you have to wait for it to scroll through
[00:48:13.600 --> 00:48:17.280]   unless it doesn't actually scroll through like, if someone pinged me, it was like, we really need
[00:48:17.280 --> 00:48:22.640]   someone to fix that. Can you come and do this? I'd be sorely tempted to do the same way about
[00:48:22.640 --> 00:48:28.240]   Google Finance. Like, if, if, if someone emails me and says, you have a mandate to make Google
[00:48:28.240 --> 00:48:36.880]   Finance good, I'd be tempted. But I think thinking of like, what industries would have that kind of
[00:48:36.880 --> 00:48:42.880]   pull for you? And then what can you do to really dig into those industries, you probably find the,
[00:48:42.880 --> 00:48:49.440]   the, the proto successful people in spaces like that versus trying to optimize in advance for
[00:48:49.440 --> 00:48:53.840]   well, if I were, you know, if I were someone who thinks like nobody else thinks, and we're a true
[00:48:53.840 --> 00:48:59.200]   natural contrarian, and also had spent several years learning about different opportunities,
[00:48:59.200 --> 00:49:03.200]   which one would I have ended up picking? Because then you're sort of magicing away all the things
[00:49:03.200 --> 00:49:08.720]   that actually make the person you're looking for worth looking for. So yeah, can't quite be done
[00:49:08.720 --> 00:49:15.040]   that way. Yeah, yeah. I want to go back to that thing you said a moment ago about how you couldn't
[00:49:15.040 --> 00:49:20.560]   have written a million words that were as impactful about just, you know, how cities work,
[00:49:20.560 --> 00:49:24.800]   but if you just wrote 500 words at a time about how Robert Moses accumulated power did the things
[00:49:24.800 --> 00:49:29.920]   he did, you can actually have a really interesting and influential piece of work. Is that how you see
[00:49:29.920 --> 00:49:36.080]   the diff that you can't write 1 million words at a time about where, where technology is going,
[00:49:36.080 --> 00:49:40.160]   what's happening with the productivity slowdown, what's happening with all these emerging industries.
[00:49:40.160 --> 00:49:45.280]   But if you just write 2000 words a day about what's happening with any particular, you know,
[00:49:45.280 --> 00:49:50.640]   company or industry, then you can compile this really interesting overall worldview about
[00:49:50.640 --> 00:49:55.600]   finance and tech. That's the hope. And I might be projecting things about my own attention span
[00:49:55.600 --> 00:50:01.120]   on to, on to Cairo, when I say that you can't just set out to do a million words on topic X,
[00:50:01.120 --> 00:50:07.440]   and then do it. But I do think, you know, I hope that I am, by increments producing something that
[00:50:07.440 --> 00:50:12.480]   is a lot more than the sum of a bunch of business profiles and a bunch of, you know, strategy
[00:50:12.480 --> 00:50:16.560]   breakdowns and things like that. Like, and that's, that's one of the reasons that I spent time on
[00:50:16.560 --> 00:50:20.800]   things like reading Machiavelli and thinking about how Machiavelli's thoughts, not just,
[00:50:20.800 --> 00:50:25.360]   not just the, the totally cynical, amoral stuff, but the other stuff he wrote at the same time,
[00:50:25.360 --> 00:50:32.000]   which he may have met more seriously about how to build a sustainable and good Republic rather than
[00:50:32.000 --> 00:50:39.280]   how to be a completely amoral monarch. I try to read that kind of thing, because I do think that
[00:50:39.280 --> 00:50:44.720]   it's valuable to have that more rounded view of the human condition. And, and I think that it
[00:50:44.720 --> 00:50:48.960]   contributes a lot to, to writing about these individual companies. Like, you know, technology
[00:50:48.960 --> 00:50:55.920]   changes a lot. Humans change very slowly. So if you, if you want to understand technology,
[00:50:55.920 --> 00:51:00.720]   you do have to study the specific object level case of what is this thing? What does it do
[00:51:00.720 --> 00:51:04.560]   differently? What is it a substitute for? What are the complements to it, et cetera.
[00:51:04.560 --> 00:51:09.760]   But if you're trying to understand things like, why did this company do X? Like, why,
[00:51:09.760 --> 00:51:15.920]   why did they fire, fire this person and not that person? And why did they choose to acquire this
[00:51:15.920 --> 00:51:20.640]   other business? Why is the CEO dumping tons of money into this thing that seems like it's,
[00:51:20.640 --> 00:51:26.880]   it doesn't make much sense? Well, you can find lots of historical examples of people in power
[00:51:26.880 --> 00:51:31.600]   making these decisions that just get continuously worse and continuously more costly, and they
[00:51:31.600 --> 00:51:35.280]   refuse to back down. Sometimes they turn out to be right. Sometimes they turn out to be very,
[00:51:35.280 --> 00:51:40.880]   very wrong. But you'll find more examples of that if you go back further in history. And they're
[00:51:40.880 --> 00:51:46.080]   often just a lot more fun to read about. Whereas like, you know, if you, you can read about things
[00:51:46.080 --> 00:51:51.840]   like Ford spending too much money on the Edsel and it not working out, or IBM investing a ton
[00:51:51.840 --> 00:51:57.440]   in the 360 and not working out very nicely. But you know, you can also go back to the Iliad and
[00:51:57.440 --> 00:52:03.120]   read another case where sunk cost fallacy dominated rational, strictly rational decision-making
[00:52:03.120 --> 00:52:07.920]   and, you know, only divine intervention could ultimately lead to a good outcome for,
[00:52:07.920 --> 00:52:12.240]   for the attacker. And even then maybe not such a great outcome, all things considered.
[00:52:12.240 --> 00:52:20.560]   The, that particular question about where, trying to predict if somebody's overstepping
[00:52:21.120 --> 00:52:25.440]   or if they're making the best bet of their life is something that I've been trying to think about.
[00:52:25.440 --> 00:52:31.200]   And I really have no reasonable method for, I mean, if you think about like what Elon Musk is
[00:52:31.200 --> 00:52:37.440]   doing with Twitter, is this like Napoleon trying to conquer Russia and it's the super ego filled
[00:52:37.440 --> 00:52:44.160]   and pride filled, you know, completely illogical bet from somebody who has just had like 20
[00:52:44.160 --> 00:52:48.560]   consecutive wins in a row. And he thinks he's invincible. Or is it like Elon Musk, like 20
[00:52:48.560 --> 00:52:53.120]   years ago where he's like, yeah, I did PayPal and now let's, you know, let's build some rockets and
[00:52:53.120 --> 00:53:00.320]   let's build some electric vehicles. Yeah, exactly. And in each of these cases, there's, there's like
[00:53:00.320 --> 00:53:06.480]   so many analogies to like complete bust. And there's so many analogies to, oh, this is just
[00:53:06.480 --> 00:53:11.200]   like part one of this grand plan. And how do you figure out which one is happening? Like
[00:53:13.360 --> 00:53:18.640]   how do you distinguish the visionary from the collapsing, you know, star?
[00:53:18.640 --> 00:53:25.920]   Um, well, the cynical answer is you wait about 200 years, and then write about how it was obvious
[00:53:25.920 --> 00:53:31.760]   all along. Like, yeah, you really don't. And I mean, even there are a lot of cases that are
[00:53:31.760 --> 00:53:37.440]   actually still ambiguous. So like, Alexander, you know, conquered most of the known world,
[00:53:37.440 --> 00:53:43.200]   at least most of the world that that people knew of around where he grew up. And, and then just goes
[00:53:43.200 --> 00:53:47.600]   to Babylon and drinks himself to death. And that's the end, right? You know, there, there could have
[00:53:47.600 --> 00:53:52.560]   been an alternate story where he gets his life together a little bit and runs a giant sprawling
[00:53:52.560 --> 00:53:58.000]   empire. On the other hand, like reading the story battle to battle a lot of it, it actually is
[00:53:58.000 --> 00:54:05.840]   basically this Ponzi scheme, where every time he conquers a city, he gets enough, enough to pay off
[00:54:05.840 --> 00:54:09.840]   the people he hired to help him conquer the city, and then has to move to the next city because they
[00:54:09.840 --> 00:54:15.520]   want to get paid again. And so he's sort of, you know, was sort of being chased by his, his
[00:54:15.520 --> 00:54:21.360]   obligations the entire way through, until he finally got got just ahead of them enough to get
[00:54:21.360 --> 00:54:25.600]   a lot of loot and, and a lot of land that could give people instead of just giving them money,
[00:54:25.600 --> 00:54:32.080]   to give them like bars of silver and things. So. So yeah, even even that story, it's very hard to
[00:54:32.080 --> 00:54:36.880]   say, you know, he rolled the dice a bunch of times and won every time. So barely, he was just one of
[00:54:36.880 --> 00:54:42.320]   those people who wanted to win. Maybe it was sort of like he actually backed himself into a bunch of
[00:54:42.320 --> 00:54:47.600]   corners over and over and over again, and then desperately fought his way out every single time,
[00:54:47.600 --> 00:54:51.440]   and then was just completely sick of it and burnt out by the time he was in his early 30s.
[00:54:51.440 --> 00:54:57.280]   In terms of how you would figure it out in advance, like, I think some of it does come down
[00:54:57.280 --> 00:55:03.360]   to getting a sense of whether they're responding to circumstances or whether they actually have,
[00:55:03.360 --> 00:55:09.120]   have a long term plan. But then lots of like, you know, there's probably nothing more dangerous than
[00:55:09.120 --> 00:55:14.000]   a long term plan that someone actually has the means to execute, you know, five year plan does
[00:55:14.000 --> 00:55:18.960]   not have a good connotation. Stalin had some of those and didn't turn out well for a lot of people.
[00:55:18.960 --> 00:55:27.760]   So even within that, there's, there's some difficulty in evaluating like, I think there's
[00:55:27.760 --> 00:55:32.480]   kind of that that medical layer where if they don't know what they're doing, then probably it's
[00:55:32.480 --> 00:55:35.280]   dumb luck that they keep succeeding. On the other hand, if they do know what they're doing, then
[00:55:35.280 --> 00:55:40.080]   maybe you hope that the world is lucky enough that they get unlucky and can't actually pull off
[00:55:40.080 --> 00:55:47.920]   whatever it is that they're planning to do. Maybe, I guess another thing would be, is there,
[00:55:47.920 --> 00:55:52.320]   is there like an end state that they can get to? Because I think, you know, someone like Alexander,
[00:55:52.320 --> 00:55:56.960]   he basically just kept going until he couldn't go any farther until his troops were basically
[00:55:56.960 --> 00:56:01.440]   on the point of mutiny, and then just turned around and went, not all the way home, but went
[00:56:01.440 --> 00:56:08.160]   to like the nicest place halfway home, hung out there and partied. But, you know, if, if the story,
[00:56:08.160 --> 00:56:14.320]   if you look at someone, if the story is less about conquest and more about reconquest and
[00:56:14.320 --> 00:56:19.040]   restoration of something, then there are these natural limits. You can say like, you go this far
[00:56:19.040 --> 00:56:24.080]   and you don't go any farther because you've actually finished your task. So something like,
[00:56:24.080 --> 00:56:30.240]   you know, I think, I don't actually know who was, who, which generals were on the other side of
[00:56:30.240 --> 00:56:35.920]   Napoleon, but the ones who chased him out of Russia, like for them, the master plan was not,
[00:56:35.920 --> 00:56:40.960]   we're going to conquer all of Europe. The master plan was like, we're getting our country back and
[00:56:40.960 --> 00:56:46.160]   then we're going to chase him far enough that he doesn't feel like he can just wait a year and do
[00:56:46.160 --> 00:56:51.600]   this again when it's not winter. So, so maybe that's, that's another way to constrain it. But
[00:56:51.600 --> 00:56:56.880]   then, then you end up naturally selecting for less ambitious people. It's like one way to,
[00:56:56.880 --> 00:57:01.920]   one way to have these guardrails on your behavior is just don't have very big ambitions.
[00:57:01.920 --> 00:57:07.920]   So you might, and in that case, those people are also stuck responding to circumstances.
[00:57:07.920 --> 00:57:12.640]   So, so maybe, maybe you just end up with many different iterations of the same thing on
[00:57:12.640 --> 00:57:16.480]   different scales where everyone is stuck in certain historical circumstances. They have,
[00:57:16.480 --> 00:57:19.280]   they have their skills, they have their opportunities. They can, they can go after
[00:57:19.280 --> 00:57:23.360]   some things. Maybe they achieve great things. Maybe they completely fail, but either way,
[00:57:23.360 --> 00:57:28.480]   eventually their luck runs out or they run out of ideas. And then there's nothing to do except
[00:57:28.480 --> 00:57:33.120]   go home or just keep trying to like, keep, keep being bolder until you eventually fail.
[00:57:33.120 --> 00:57:39.520]   On Musk particularly, I, I don't really, I don't really understand it. I think there's like a
[00:57:39.520 --> 00:57:45.680]   remote possibility that he actually has a bunch of specific concrete ideas for how to increase
[00:57:45.680 --> 00:57:49.920]   Twitter's free cashflow and how to pay down the debt and make it a more profitable company.
[00:57:50.880 --> 00:57:55.520]   Maybe he just had that sense that it was overstaffed and that it should survive with
[00:57:55.520 --> 00:58:00.480]   a smaller headcount. And if you cut headcount enough, then you, you end up with with a
[00:58:00.480 --> 00:58:08.720]   profitable business. It could also just have been fun and seems fun so far. And I think like that,
[00:58:08.720 --> 00:58:14.080]   you know, the, the pursuit of fun is, is not to be discounted. Like you, if you're super rich,
[00:58:14.080 --> 00:58:18.960]   you can afford to do all sorts of things, varying levels of entertainment, but it may be that the
[00:58:18.960 --> 00:58:24.480]   only thing that is actually like truly novel, thrill-seeking fun opportunity is something like
[00:58:24.480 --> 00:58:30.640]   buy Twitter and then turn it into, you know, what it is. And it is like, there's, um, I think Ross
[00:58:30.640 --> 00:58:36.880]   doubted this, um, this point about how the nature of Twitter's legitimacy has changed and that now
[00:58:36.880 --> 00:58:42.800]   it is a, it is under the rule of a single monarch instead of, um, ruled by these sort of faceless
[00:58:42.800 --> 00:58:46.720]   bureaucracies. So now, you know, if something, if Twitter does something you don't like,
[00:58:46.720 --> 00:58:50.960]   there's actually a specific person you can blame. And because you have Twitter, you can actually
[00:58:50.960 --> 00:58:56.240]   yell at that person and potentially get an answer. Whereas if Twitter bans you because you made a
[00:58:56.240 --> 00:59:01.760]   joke and the joke looked like it was serious, um, there's really, there's no recourse. And,
[00:59:01.760 --> 00:59:07.600]   you know, there's, there's nothing lower status than, um, someone like arguing with someone in
[00:59:07.600 --> 00:59:11.680]   authority about how serious or they should take your jokes. Um, there's like, you know,
[00:59:11.680 --> 00:59:17.840]   it's like a weird component of, um, and it works both ways. So like there's, I think I started
[00:59:17.840 --> 00:59:22.560]   noticing this years ago because there, there are these, uh, underscore TXT Twitter accounts where
[00:59:22.560 --> 00:59:28.080]   they're just posting out of context comments from some niche community. And the comments always
[00:59:28.080 --> 00:59:33.520]   sound deranged in a lot of cases, to me, the comments read as someone who is doing a bit,
[00:59:33.520 --> 00:59:36.640]   they're playing a role. They know it's funny. They're exaggerating for their friends. And then
[00:59:36.640 --> 00:59:41.120]   you take it out of context and read it as totally seriously. And then you get to say, these people
[00:59:41.120 --> 00:59:45.840]   are all like this and they're all crazy. Um, but it is like, uh, it is a marker of high status to
[00:59:45.840 --> 00:59:53.440]   be able to not get jokes and to, you know, be able to be like righteously angry at someone because
[00:59:53.440 --> 00:59:56.560]   they made a joke. And if they'd been serious, that would have been an appalling thing to say,
[00:59:56.560 --> 01:00:01.200]   but they obviously weren't. Um, if you, if you can get away with saying, no, I actually don't
[01:00:01.200 --> 01:00:04.560]   think it was a joke at all. These people are humorless and they must've been totally serious.
[01:00:04.560 --> 01:00:09.200]   Then that's, that's actually, you know, that's cool. That's high status, um, makes you impressive.
[01:00:09.840 --> 01:00:15.920]   But, um, yeah, it must like musk must rule as this more, you know, personal Mark. Um,
[01:00:15.920 --> 01:00:23.360]   I think it's, uh, it speaks to this question of legitimacy. Like why do people trust moderation
[01:00:23.360 --> 01:00:27.760]   and why do they trust sites to operate in the way that they do? And you can either say,
[01:00:27.760 --> 01:00:31.840]   these are like really high quality institutions. So, you know, you can take the discourses of
[01:00:31.840 --> 01:00:37.040]   oblivion approach and say, we built these systems such that anyone can be dropped in and can do a
[01:00:37.040 --> 01:00:41.040]   reasonably good job. It's very hard for bad people to do a very bad job because there are so many
[01:00:41.040 --> 01:00:45.120]   checks and balances, or you can say, no, we actually trust this one person to do a really
[01:00:45.120 --> 01:00:49.600]   exceptional job that nobody else can do. And if we don't want institutional constraints on them,
[01:00:49.600 --> 01:00:55.040]   um, those, those philosophies go in and out of fashion. And, um, like even within,
[01:00:55.040 --> 01:00:59.120]   even within systems that nominally don't change, you know, there, the U S was, um,
[01:00:59.120 --> 01:01:04.480]   a lot closer to that kind of centralized system with personal legitimacy invested in one person,
[01:01:04.480 --> 01:01:09.600]   um, in under FDR than it was under Calvin Coolidge. And under Coolidge, it was a lot more
[01:01:09.600 --> 01:01:13.520]   of like, there's this institution, there are a bunch of rules. People follow the rules. You have
[01:01:13.520 --> 01:01:19.040]   this nice new England guy who, um, you know, he, he gives a, uh, an annual update, a state of the
[01:01:19.040 --> 01:01:23.280]   union, but it's just written down. And then he has a clerk read it to Congress. You know, it's,
[01:01:23.280 --> 01:01:26.400]   you're not betting on charisma. You're not betting on judgment. You're just betting that
[01:01:26.400 --> 01:01:31.360]   thing. The rules are pretty good. And as long as things keep working, according to the rules,
[01:01:31.360 --> 01:01:37.280]   they'll keep on working. Yeah. Yep. Yep. Yep. No, the, the musk example is like the sort of
[01:01:37.280 --> 01:01:43.760]   consumption by like playing this game. It's similar to how some people will load up of
[01:01:43.760 --> 01:01:48.960]   like a horribly broken game of Civ where their civilization is losing because they've gotten so
[01:01:48.960 --> 01:01:53.840]   good at the game that they just need to like some new to send them their safe file of just like
[01:01:53.840 --> 01:01:57.040]   complete, complete carnage. And they're losing their cities and stuff. And then,
[01:01:57.040 --> 01:02:01.920]   then the fun is you like load this up and you try to win anyways. Um, but you know,
[01:02:01.920 --> 01:02:04.640]   one thing you've written about, and I find really interesting, we're both fans of
[01:02:04.640 --> 01:02:10.400]   Fukuyama's book, the end of history. And if you read the last quarter of that book,
[01:02:10.400 --> 01:02:14.560]   you'll come up with the impression that he actually did. I mean, it just like completely
[01:02:14.560 --> 01:02:18.720]   contradicts. I don't know the, the first three quarters of the book where he's just saying,
[01:02:18.720 --> 01:02:24.000]   you know what, actually these men at the end of the end of history are these pathetic last men
[01:02:24.000 --> 01:02:27.920]   who have no desire for recognition. They just want to be comfortable. Um, and you've made the
[01:02:27.920 --> 01:02:35.360]   comparison with that and big tech, um, at least, uh, um, before the crash. And one of the things
[01:02:35.360 --> 01:02:40.320]   Fukuyama talks about in the book is once there is a great war, once there is a struggle that requires
[01:02:40.320 --> 01:02:45.680]   the first men of history who can withstand adversity and can accomplish great things,
[01:02:45.680 --> 01:02:50.560]   you won't have them around by the time that, you know, like things have gotten comfortable for a
[01:02:50.560 --> 01:02:55.680]   while. Are there enough first men left in companies like Twitter and Facebook that now they do face
[01:02:55.680 --> 01:03:04.560]   adversity? They can, you know, just like reboot and go into wartime again. Yeah. Um, I suspect
[01:03:04.560 --> 01:03:09.600]   they are. I think, I think, uh, Tolkien gets it right that, um, you know, just because someone
[01:03:09.600 --> 01:03:13.280]   is born a hobbit and they live in Hobbiton and they have this nice, comfortable life,
[01:03:13.280 --> 01:03:18.160]   like they still have that capacity for and yearning for adventure and that in the right
[01:03:18.160 --> 01:03:22.720]   circumstances, they will, they will rise to the occasion and go ahead and do it. And this seems
[01:03:22.720 --> 01:03:29.040]   to happen with, um, with a lot of countries when they face these great stresses, like some, sometimes
[01:03:29.040 --> 01:03:33.360]   a civilization just can't withstand it and it collapses. And, you know, the sea people just
[01:03:33.360 --> 01:03:39.040]   take everything and then you have no civilization left and you're all just back to farming. But in
[01:03:39.040 --> 01:03:44.320]   a lot of other cases, they, um, even if they ultimately don't survive, they go through a very
[01:03:44.320 --> 01:03:50.400]   long decline because they do fight to fight, to maintain what they have for, for an extended
[01:03:50.400 --> 01:03:57.680]   period. So I think, um, yeah, like even trying to determine a mechanism by which you can,
[01:03:57.680 --> 01:04:03.120]   you can eradicate that, um, that thirst for glory and that ability to rise to the occasion. It's,
[01:04:03.120 --> 01:04:06.560]   it's hard to think of, I mean, you know, unless you think there's like, unless it's like
[01:04:06.560 --> 01:04:12.160]   microplastics or something, maybe, maybe that does constitute the end of history. Um, in which case,
[01:04:12.160 --> 01:04:15.600]   you know, hopefully we, we exported up microplastics to make sure that we don't have,
[01:04:15.600 --> 01:04:22.160]   uh, any, you know, any last pockets of the most, um, sort of like the Scott Alexander rip about,
[01:04:22.160 --> 01:04:27.840]   um, the, um, step nomad invasion risk where it's an existential risk that comes along every couple
[01:04:27.840 --> 01:04:35.760]   of hundred years. Um, yeah, you, you want to avoid that. Um, but yeah, yeah. It's like part of,
[01:04:35.760 --> 01:04:42.320]   I think part of having that kind of through most and thirst for glory should be that you can't
[01:04:42.320 --> 01:04:49.200]   actually habituate. You can't be so habituated to a life of ease and comfort and lack of difficulty
[01:04:49.200 --> 01:04:54.000]   that you just, you won't actually respond appropriately when there's an external threat
[01:04:54.000 --> 01:04:58.960]   that you need to respond to. Um, then, you know, maybe, maybe you weren't, um, you weren't first
[01:04:58.960 --> 01:05:05.760]   man material after all, if, uh, if you can and you just want to stay on your couch. Um, so, so yeah,
[01:05:05.760 --> 01:05:11.920]   I don't, I, I'm sure we can sort of deplete that reserve. And, um, there, there were, I think,
[01:05:11.920 --> 01:05:16.880]   definitely like post post-World War II US was definitely a country where there were a lot more
[01:05:16.880 --> 01:05:22.240]   people who had taken very serious risks. They'd gone through, you know, a lot of hardship on the
[01:05:22.240 --> 01:05:26.720]   other hand. Um, so I recently read that book, um, the economics of World War II, which was comparing
[01:05:26.720 --> 01:05:29.600]   a bunch of countries and how their economies perform in World War II. And one of the things
[01:05:29.600 --> 01:05:36.160]   that sort of about the U S was that, um, in a lot of terms of, um, material consumption, the U S
[01:05:36.160 --> 01:05:40.800]   wasn't really that much, uh, didn't really look like a country going through a war. Like, um,
[01:05:40.800 --> 01:05:45.520]   in most other countries, you saw this decline in literally how much food people had to eat and, um,
[01:05:45.520 --> 01:05:51.600]   especially how much protein and fat they had to eat. And, um, so I think like a lot of places,
[01:05:51.600 --> 01:05:55.840]   calories, calorie intake had dropped by like a third by the end of the year or by the end of the
[01:05:55.840 --> 01:06:01.920]   war. And then in the U S calorie consumption actually went up. So, um, U S was like on the
[01:06:01.920 --> 01:06:08.480]   home front was inconvenienced by the war and things like gas and tires were hard to get,
[01:06:08.480 --> 01:06:12.640]   but, uh, people were still eating well. Whereas in a lot of other parts of the world, people,
[01:06:12.640 --> 01:06:16.400]   like they were literally going hungry so that their country could continue to fight the war.
[01:06:16.400 --> 01:06:21.280]   So maybe there's like, you know, there's some level of hormetic response where you, you suffer
[01:06:21.280 --> 01:06:25.840]   a bit because your country is, is contributing to this. And then, um, you're, you're, you're
[01:06:25.840 --> 01:06:30.640]   hardier for it. And the country has accumulated a lot of social capital and, um, you know, you've
[01:06:30.640 --> 01:06:34.800]   had to get really good at organizing and building things. And then, um, maybe there, there is some
[01:06:34.800 --> 01:06:39.840]   level of, uh, there was some level of suffering from conflict where you've just, you've totally
[01:06:39.840 --> 01:06:44.320]   had enough and you're never doing anything like that ever again. And, uh, you're just, you're
[01:06:44.320 --> 01:06:49.600]   done. And then I think one of the interesting things to consider is like the extent to which
[01:06:49.600 --> 01:06:55.280]   different countries fit into that model. So one of my, um, I'm very interested in Japan and
[01:06:55.280 --> 01:07:00.640]   Japanese industrial policy and how, how the Japanese post-war recovery went. And one of the
[01:07:00.640 --> 01:07:04.880]   annoying things about that is like, I thought that was the question was how did Japan have this
[01:07:04.880 --> 01:07:08.560]   wonderful post-war recovery. But when you look at a lot of the institutions involved, they don't
[01:07:08.560 --> 01:07:15.520]   start in 1945 or 1951 or whatever. They actually started before World War II. And so you can
[01:07:15.520 --> 01:07:22.000]   actually sort of see World War II as part of this arc of, um, of the same historical process that
[01:07:22.000 --> 01:07:28.080]   continued post-war, which is Japan wanted to be economically self-sufficient and independent and
[01:07:28.080 --> 01:07:33.280]   a country that could determine its own fate. And, um, the, you know, in the, during the last gasp
[01:07:33.280 --> 01:07:38.000]   of imperialism, one way to do that was invade countries with lots of natural resources, take
[01:07:38.000 --> 01:07:42.960]   those resources and then manufacture things at home. But, um, when that became untenable,
[01:07:42.960 --> 01:07:48.800]   then the next best option was be within the sphere of influence of the most, the most powerful
[01:07:48.800 --> 01:07:54.560]   military in the world, and be very closely tied to their import and export markets, and then
[01:07:54.560 --> 01:07:59.200]   import everything you need under the protection of the U.S. military, and then export things to
[01:07:59.200 --> 01:08:03.840]   the U.S. in order to pay for those imports and, um, basically run, run the same strategy just
[01:08:03.840 --> 01:08:09.360]   with someone else doing the military part. So, um, you know, in one sense, that was like a total
[01:08:09.360 --> 01:08:14.480]   defeat of the imperialist model. In another sense, it was like this strategic realignment, but
[01:08:14.480 --> 01:08:21.120]   actually basically same end goal and, um, you know, very, very different external facing view
[01:08:21.120 --> 01:08:25.600]   of that goal. Um, but yeah, same, same ultimate idea. There's a, there's this book called "Princes
[01:08:25.600 --> 01:08:29.680]   of the Yen," which is, um, mostly about Japanese central banking policy, but it has some early bits
[01:08:29.680 --> 01:08:34.800]   about how the structure of Japan's economy works. And the way the author describes it is that Japan,
[01:08:34.800 --> 01:08:39.920]   post-war Japan, had a war economy in peacetime with lots of centralized control and suppressed
[01:08:39.920 --> 01:08:46.160]   consumption and lots of heavy, heavy industry, heavy manufacturing, um, that also a lot of
[01:08:46.160 --> 01:08:51.920]   companies in Japan, their modern structure dates back to the wartime period, um, sometimes the
[01:08:51.920 --> 01:08:56.480]   post-war, but sometimes literally the wartime period, including, um, the biggest, uh, biggest
[01:08:56.480 --> 01:09:01.120]   advertising agency in Japan was apparently like this, this wartime or immediately pre-war attempt
[01:09:01.120 --> 01:09:06.480]   to agglomerate all the smaller ad companies into one big, more efficient company that, um, would
[01:09:06.480 --> 01:09:10.800]   free up resources that could be used for building battleships and other, other stuff like that. So,
[01:09:10.800 --> 01:09:16.160]   um, yeah, it's kind of, kind of the same story just being, being expressed in a bunch of different
[01:09:16.160 --> 01:09:21.040]   ways. And, um, I think you can, you can look at other countries and, and try to see like what,
[01:09:21.040 --> 01:09:25.600]   what threads of continuity there are between the post-war and, um, the, between the pre-war and
[01:09:25.600 --> 01:09:30.960]   post-war order, uh, Tony Judt's post-war book is a really phenomenal look at, at that question.
[01:09:30.960 --> 01:09:35.440]   And, um, in a lot of cases there's like, there's a surprising level of continuity. There are some
[01:09:35.440 --> 01:09:38.720]   things that totally broke and had to be totally reformed. And there are some things that just
[01:09:38.720 --> 01:09:43.280]   kept going exactly the way they've been going before. Yeah. Yeah. Yeah. Um, everybody wants
[01:09:43.280 --> 01:09:48.960]   to be a first man, but nobody wants to go on a diet. Um, but, uh, but you've mentioned this line
[01:09:48.960 --> 01:09:53.840]   before, but there's a line from how Asia works where they're talking about the reparations that
[01:09:53.840 --> 01:09:59.840]   Korea got after World War II from Japan and how they're using that to build up their industrial
[01:09:59.840 --> 01:10:04.720]   capacity. And there's like a line from one of the line managers or in the factory, it goes,
[01:10:04.720 --> 01:10:10.080]   listen, you guys had to work like 14 hours a day, seven days a week. And the reason is this money
[01:10:10.080 --> 01:10:14.480]   is blood money. It's our blood. It's like the, this money was like gotten from like ripping your
[01:10:14.480 --> 01:10:19.040]   mother and killing your father. And if you can't use that money to like rebuild our country, like
[01:10:19.040 --> 01:10:27.680]   what good are you? You might as well just kill yourself. Um, yeah. Um, yeah. Um, and based, um,
[01:10:27.680 --> 01:10:32.080]   but there's a, and then you read about, um, lean production. I was reading about lean production
[01:10:32.080 --> 01:10:36.800]   when I was, before I interviewed Austin Vernon and in all these books, they're talking about
[01:10:36.800 --> 01:10:42.000]   how America's, uh, America was never able to replicate the productivity of Japanese lean
[01:10:42.000 --> 01:10:46.880]   production. And it's just because you're talking about Americans who have, you know, like trying
[01:10:46.880 --> 01:10:51.840]   to save up their pensions and working eight hours a day and have a hour long lunches. And you just
[01:10:51.840 --> 01:10:57.840]   have these hardcore Japanese who they just got to like world war two and they barely survived.
[01:10:57.840 --> 01:11:02.080]   And you know, like it just like the thymus is completely different. You just can't replicate
[01:11:02.080 --> 01:11:08.320]   that, um, in America. Yeah. Yeah. There's a, there's this book, um, called the reckoning
[01:11:08.320 --> 01:11:13.440]   about the U S our industry and how, how it dealt with the rise of Japanese exports in the seventies
[01:11:13.440 --> 01:11:18.080]   and eighties. And when it's talking about the, the post-war recovery in Japan, there's this bit
[01:11:18.080 --> 01:11:25.120]   where I think, I think the character they're following works for a bank. And he in the bank's
[01:11:25.120 --> 01:11:31.040]   office in some city in Japan, he likes to work in the room where there's a fire because there's like
[01:11:31.040 --> 01:11:36.640]   a fire with this big pot of stew. And the stew is the food that the employees will eat at the bank
[01:11:36.640 --> 01:11:42.640]   as part of their benefits package. And that's the only room in the bank that is either warm enough
[01:11:42.640 --> 01:11:49.040]   that you can actually work. Um, and that's just like, that is a level of, um, material austerity
[01:11:49.040 --> 01:11:55.360]   that is inconceivable to me. And like, I can't, you know, I don't know anyone who grew up in such
[01:11:55.360 --> 01:12:01.120]   poor circumstances. I can't imagine it. And then, um, it did not take very long at all for the
[01:12:01.120 --> 01:12:05.840]   country to significantly recover from that. And yeah, you know, other countries that also had
[01:12:05.840 --> 01:12:10.960]   this massive catch-up growth where they went from very poor subsistence level, or even below that,
[01:12:10.960 --> 01:12:16.400]   in some cases to actually being middle income or even, you know, fairly rich countries.
[01:12:16.400 --> 01:12:22.560]   Yep. Yep. I want to touch on the fact that Sam Beckman Freed was an effective altruist and that
[01:12:22.560 --> 01:12:27.680]   he was a strong proponent of risk neutrality. We were having, we were talking like many months ago,
[01:12:27.680 --> 01:12:33.360]   and you made this really interesting comment that in many belief systems, they have a way of
[01:12:33.360 --> 01:12:38.800]   segregating and limiting the impact of the most hardcore believers. You know, they, and so if
[01:12:38.800 --> 01:12:41.840]   you're like a Christian, the people who take it the most seriously, you can just make them monks.
[01:12:41.840 --> 01:12:46.480]   So they don't cause that much damage to the rest of the world. And EA's don't have that,
[01:12:46.480 --> 01:12:50.320]   right? So if you're like a hardcore risk neutral utilitarian, you know, you're out in the world,
[01:12:50.320 --> 01:12:55.840]   you're like making billion dollar crypto companies. Um, as a side note, by the way,
[01:12:55.840 --> 01:13:00.640]   it's interesting a year ago, I feel like the meme was, oh, look at these useless rationalists.
[01:13:00.640 --> 01:13:05.680]   They're just reading blogs all day and they have these, you know, mind palaces and whatever.
[01:13:05.680 --> 01:13:10.800]   And what good are they? You know, and then now, now everybody's like, oh, these received
[01:13:10.800 --> 01:13:15.040]   neutral utilitarians are going to wager our entire civilization and these 5149 schemes.
[01:13:15.040 --> 01:13:22.880]   But anyways, yeah, I just want to get your commentary on, uh, on all this.
[01:13:22.880 --> 01:13:27.520]   Yeah. Yeah. Like, I think, I think it's a useful pattern to observe because, I mean, it goes back
[01:13:27.520 --> 01:13:31.040]   to that, that point that human nature just doesn't change all that fast to the extent that it ever
[01:13:31.040 --> 01:13:36.720]   does. And that we've had the problem with different civilizations have had this problem of,
[01:13:36.720 --> 01:13:42.080]   okay, we've got some rules and we've got these beliefs and they're, they're generally going to
[01:13:42.080 --> 01:13:46.400]   guide people to behave the right way, but they're going to guide people to be the right kind of
[01:13:46.400 --> 01:13:50.720]   normal person and not to be someone whose life is entirely defined by this incredibly strict,
[01:13:50.720 --> 01:13:56.240]   rigid whole code. And by whatever you get, if you take the premises of that and just extrapolate
[01:13:56.240 --> 01:14:00.320]   them linearly as far as they can go. And I think that gets especially dangerous with really smart
[01:14:00.320 --> 01:14:04.640]   people because you can give them a set of first principles and they can ask really, really
[01:14:04.640 --> 01:14:08.880]   interesting questions and come up with edge cases. And sometimes, like, I think for some people,
[01:14:08.880 --> 01:14:13.760]   like the first philosophy class where they encountered these edge cases, they, they just
[01:14:13.760 --> 01:14:20.800]   reject it as stupid and say things like, you know, I, um, you know, if you, if you shove the fat man
[01:14:20.800 --> 01:14:23.760]   in front of the trolley, why do you think the trolley would stop? The trolley would just kill
[01:14:23.760 --> 01:14:28.720]   him too. Like this is dumb. And, um, I think that it's like, it is useful to keep in mind that the
[01:14:28.720 --> 01:14:32.560]   thought experiments are designed to be implausible and they are supposed to be the situation pumps.
[01:14:32.560 --> 01:14:38.640]   But the more you get this complicated, highly abstract economy where an increasing share of
[01:14:38.640 --> 01:14:43.440]   it is software interactive software. Well, software doesn't have that common sense break
[01:14:43.440 --> 01:14:49.440]   on behavior. And, um, if you have this very composable economy, you can find cases where
[01:14:49.440 --> 01:14:55.520]   first principles thinking actually is action guiding and can, can guide you to extreme behaviors.
[01:14:55.520 --> 01:14:58.720]   Unfortunately, those extreme behaviors are things like trading cryptocurrencies with lots
[01:14:58.720 --> 01:15:03.760]   and lots of leverage. Um, and you know, we, we have like, it's, it's maybe merciful that the,
[01:15:03.760 --> 01:15:09.840]   the atoms to bits interface has not been fully completed. Um, while we still have time to deal
[01:15:09.840 --> 01:15:17.200]   with, um, you know, malevolent, um, unfriendly EA. So, um, that's, that's good. But yeah, it is
[01:15:17.200 --> 01:15:22.560]   like, it's a problem that you come up with. You see it a lot. Um, and you see a lot of different
[01:15:22.560 --> 01:15:26.400]   societies and they do tend to have some kind of safety valve. And they're like, if you,
[01:15:26.400 --> 01:15:30.960]   you really think that praying all day is the thing you should do, you should go do it somewhere else.
[01:15:30.960 --> 01:15:35.680]   And you shouldn't really be part of what we're doing. And, um, I think that's healthy. And I
[01:15:35.680 --> 01:15:40.160]   think in some cases it's like a temporary thing. Like you do that, you get it out of your system
[01:15:40.160 --> 01:15:43.440]   and either you come back as this totally cynical person who doesn't believe in any of it, or you
[01:15:43.440 --> 01:15:49.600]   come back as someone who is still deeply religious and is willing to integrate with society in,
[01:15:49.600 --> 01:15:55.280]   in a productive way. And I think even like, even within the, the monastic system, you,
[01:15:55.280 --> 01:16:00.880]   you do have different levels of engagement with the outside world. Um, and just, yeah, different,
[01:16:00.880 --> 01:16:06.000]   different levels of interaction. So I, I think that that's something that EA should take,
[01:16:06.000 --> 01:16:12.640]   take seriously as an observation, as like a design pattern for societies that you typically don't
[01:16:12.640 --> 01:16:20.480]   want the people in charge to be the most fanatical people. And that, um, because EA beliefs do tend
[01:16:20.480 --> 01:16:26.640]   to correlate with, um, being, you know, a very effective shape rotator or a very effective
[01:16:26.640 --> 01:16:32.240]   manipulator, uh, like symbol, symbol manipulator. And those skills are very lucrative and money does
[01:16:32.240 --> 01:16:36.800]   have some exchange rate with power. Like you, you basically have a system where very smart people can
[01:16:36.800 --> 01:16:42.480]   become very powerful. And if very smart people can also become very crazy, then, um, you, you tend to
[01:16:42.480 --> 01:16:47.520]   increase the correlation between power and craziness. And, um, it doesn't take very long
[01:16:47.520 --> 01:16:51.680]   clicking through Wikipedia articles on various leaders in world history to say that you ideally
[01:16:51.680 --> 01:16:55.120]   do not want your powerful people to be all that crazy or you're crazy people to be all that
[01:16:55.120 --> 01:17:01.120]   powerful. Um, and as far as what to actually do about that, um, like I think one, you know,
[01:17:01.120 --> 01:17:07.360]   one model is that smart people should be advisors, but not in an executive capacity. Like they
[01:17:07.360 --> 01:17:11.760]   shouldn't be executives or like you, you don't want the smartest person in the organization.
[01:17:11.760 --> 01:17:15.760]   Also being the person who makes the final decisions for various reasons. And, but you do
[01:17:15.760 --> 01:17:19.440]   want them around and you want the person making final decisions to be like reasonably smart,
[01:17:19.440 --> 01:17:23.120]   like smart enough. They understand what the smart person is telling them and why,
[01:17:23.120 --> 01:17:27.680]   why that might be wrong with the flaws might be. So that might be one model is that you want the
[01:17:27.680 --> 01:17:34.560]   EAs dispersed throughout different organizations in the world as someone working with non EAs and
[01:17:34.560 --> 01:17:39.360]   kind of nudging them at an EA friendly direction, giving them helpful advice, but not actually being
[01:17:39.360 --> 01:17:45.440]   the executive. Um, one possibility is that every other society got it wrong and that the monastic
[01:17:45.440 --> 01:17:50.320]   tradition was stupid and it has been independently discovered by numerous stupid civilizations that
[01:17:50.320 --> 01:17:54.000]   have all been around for much longer than effective altruism. So one possibility you
[01:17:54.000 --> 01:17:58.400]   can't discount it. Um, but I think if you, if you run the probabilities, that's probably not the case.
[01:17:58.400 --> 01:18:07.360]   Yeah. Yeah. And I, in general, it's always a little bit, the leaders who take ideas seriously
[01:18:07.360 --> 01:18:11.120]   don't necessarily have a great track record, right? Like Stalin apparently had a book of
[01:18:11.120 --> 01:18:17.040]   like 20,000, uh, library of like 20,000 books. Like if you listen to Putin's speech on Ukraine,
[01:18:17.040 --> 01:18:21.280]   it's, you know, laden with all kinds of historical references. Um, obviously, you know, there's like
[01:18:21.280 --> 01:18:25.920]   many ways you can disagree with it, but it's like a man of ideas. And do you want a man of ideas in
[01:18:25.920 --> 01:18:31.040]   charge of important institutions? It's not clear. I mean, the founding, I mean, well, founding
[01:18:31.040 --> 01:18:36.480]   fathers, I was going to say, you know, a lot of them were wordsmiths and we basically had, um,
[01:18:36.480 --> 01:18:41.360]   we have whole collections of Anons blaming each other through pamphlets, the pamphlets papers.
[01:18:41.360 --> 01:18:48.320]   So in one sense, they, it was a nation of nerds. On the other hand, Washington didn't, as far as I
[01:18:48.320 --> 01:18:53.600]   know, did not have huge contributions to that literary corpus. So, so maybe that is actually
[01:18:53.600 --> 01:18:58.480]   the model is like, you want the nerds, you want them to debate things. You want the debates to
[01:18:58.480 --> 01:19:02.320]   either reach interesting conclusions or at least tell you where the fault lines are. Like,
[01:19:02.320 --> 01:19:07.120]   what are the things nobody can actually come to a good agreement on? And then you want someone who
[01:19:07.120 --> 01:19:13.520]   is not quite that smart, not really into flame wars, um, to actually make the final call.
[01:19:13.520 --> 01:19:18.160]   Yeah. No, that's a really good point. And I mean, like, forget about Jefferson, think of like
[01:19:18.160 --> 01:19:22.000]   if, if Thomas Paine was made president of the United States, that would be very bad news.
[01:19:22.000 --> 01:19:27.280]   Yeah. Yeah. Like it was good to have, like, it's, it's important to note, like, it's,
[01:19:27.280 --> 01:19:31.360]   it's good to have, it's better to have some level of fanaticism than no fanaticism, you know,
[01:19:31.360 --> 01:19:36.880]   there's like an optimal amount of the most, and there's like an optimal place for it. But, um,
[01:19:36.880 --> 01:19:40.080]   I think, yeah, from a totally cynical perspective, like your most demotic people,
[01:19:40.080 --> 01:19:44.880]   maybe they are at the front lines doing things and taking risks, but also not making the decisions
[01:19:44.880 --> 01:19:48.880]   about who goes to the front lines. Or I think the other thing is like making sure that some of the
[01:19:48.880 --> 01:19:53.920]   person deciding where the front lines are and saying, you know, the front line is like, we,
[01:19:53.920 --> 01:19:59.520]   we keep France safe from the invaders and not the front line is Moscow. So get to Moscow and burn it
[01:19:59.520 --> 01:20:06.960]   down, which there's a, the, the book, the mind of Newell, um, there's a recent Napoleon biography
[01:20:06.960 --> 01:20:12.320]   that I'm also in the middle of, um, it's been a good year for reading about power tripping people.
[01:20:12.320 --> 01:20:19.520]   Um, it does point out that technically France was actually, uh, declared Napoleon had more
[01:20:19.520 --> 01:20:25.680]   countries declared war on him than he declared war on. So on average, on average, France was
[01:20:25.680 --> 01:20:30.240]   fighting defensive wars during the Napoleonic era. It's just, you know, they kept defending
[01:20:30.240 --> 01:20:36.320]   farther and farther from France. Yeah. Yeah. Yeah. Uh, defense requires some strange kinds
[01:20:36.320 --> 01:20:45.280]   of offense often. Um, okay. So one, one like sort of meta question I've had is there in all other
[01:20:45.280 --> 01:20:50.080]   kinds of discourse, there's this question about, you know, whether you're trying to figure out how
[01:20:50.080 --> 01:20:52.880]   to do, which charities do the most good, but they're trying to figure out which policies are
[01:20:52.880 --> 01:20:56.160]   best, whether you're trying to figure out how you should promote leaders, anything you're,
[01:20:56.160 --> 01:20:59.520]   there's a question, there's like two kinds of discourse. There's one that's like, we've got
[01:20:59.520 --> 01:21:04.800]   these few dozen RCTs and let's see how we can extrapolate the data from these in the least
[01:21:04.800 --> 01:21:09.440]   theory late in way. And there's another where it's like, I've just read a shit ton of classics
[01:21:09.440 --> 01:21:13.440]   and I've, you know, I'm like a thinking person. I think a lot about culture and philosophy,
[01:21:13.440 --> 01:21:19.200]   and here's my sort of like big intricate worldview about how these things are going to shape out.
[01:21:19.200 --> 01:21:25.600]   And investing is an interesting realm because there's both kinds of people and you can see
[01:21:25.600 --> 01:21:31.360]   the track records over long periods of time. So having seen this track record, is there any
[01:21:31.360 --> 01:21:36.160]   indication to you where there are this sort of first sort of microeconomic approach actually
[01:21:36.160 --> 01:21:42.000]   leads to better concrete results than somebody like Teal or Soros who are motivated by a sort
[01:21:42.000 --> 01:21:46.480]   of intricate worldview that's based on philosophy or something, which one actually makes better
[01:21:46.480 --> 01:21:53.440]   concrete predictions that are actionable. So I think typically the greats have some
[01:21:53.440 --> 01:21:59.520]   synthesis of the two and it probably, probably leans more towards big worldview than towards
[01:21:59.520 --> 01:22:05.280]   micro-level observations. I think like one way to divide things is to say that the quants are all
[01:22:05.280 --> 01:22:10.880]   these micro-level observations. Like you can be a quant who does not actually know what the numbers
[01:22:10.880 --> 01:22:14.480]   mean, you know, doesn't know what the product is, doesn't know the timescale, is just looking for
[01:22:14.480 --> 01:22:19.760]   patterns and finds them. And I mean, people have, people have done it that way, but it seems like
[01:22:19.760 --> 01:22:23.920]   quantitative strategies get more successful when you're doing that, you find some anomaly and then
[01:22:23.920 --> 01:22:28.480]   you find an explanation for the anomaly. And the explanation might be some psychological factor
[01:22:28.480 --> 01:22:33.120]   you've identified. And maybe you find studies indicating that loss aversion is real and this
[01:22:33.120 --> 01:22:37.280]   affects how fast stocks go down versus how fast they should go down. And that gives you a trading
[01:22:37.280 --> 01:22:44.400]   strategy. Maybe it's something more, more mundane. Like maybe there is some large investor who has
[01:22:44.400 --> 01:22:49.440]   some policy, like we rebalance between stocks and bonds on the first day of every quarter.
[01:22:49.440 --> 01:22:54.560]   And if you know that these, like the investors who have that policy control X trillion dollars
[01:22:54.560 --> 01:23:00.400]   of assets, and you know how they'll rebalance, then every, at the end of every quarter, you know,
[01:23:00.400 --> 01:23:05.200]   money is sloshing between stocks and bonds and that's predictable. So a lot of the quantitative
[01:23:05.200 --> 01:23:09.120]   strategies that have those theories behind them tend to blow up more rarely because they sort of
[01:23:09.120 --> 01:23:14.240]   know why the strategy works and then they know why it'll stop working. And data mining is always a
[01:23:14.240 --> 01:23:18.320]   risk. If you find an anomaly and there's no explanation, it keeps repeating itself. One
[01:23:18.320 --> 01:23:22.800]   of the explanations is other people found the anomaly too, and they are exacerbating it by
[01:23:22.800 --> 01:23:30.880]   trading it. Then I think on the other end, like if you have these just totally theory driven views,
[01:23:30.880 --> 01:23:38.800]   usually what kills these totally abstract theory driven views is time. Because a lot of the best
[01:23:38.800 --> 01:23:42.720]   abstract theories are you look at some part of the economy and you say,
[01:23:42.720 --> 01:23:48.640]   this is obviously unsustainable. And then the problem is you can say that at any point during
[01:23:48.640 --> 01:23:55.200]   its arc, and it can look sustainable to other people for a very long time. So like, one of my
[01:23:55.200 --> 01:24:00.800]   favorite examples of this is that there's this snappy one liner that it's something it's like
[01:24:00.800 --> 01:24:06.480]   looking at housing and it's like a subprime borrower who didn't put down a down payment is
[01:24:06.480 --> 01:24:12.320]   basically just a renter with, you know, with upside or something. It's like some line about
[01:24:12.320 --> 01:24:16.240]   how the economics are the same. And like, you know, these, these people are not actually safe
[01:24:16.240 --> 01:24:23.760]   borrowers. But the paper that it came from came out in 2001. And so if you had read that had been
[01:24:23.760 --> 01:24:27.360]   like, American housing market is broken, people are massively overpaying for houses, they're all
[01:24:27.360 --> 01:24:32.480]   over leveraged flaps, you could have shorted housing stocks and then lost 400% of your money
[01:24:32.480 --> 01:24:38.960]   as homebuilders sort over the next four or five years. So usually the way you one of the ways you
[01:24:38.960 --> 01:24:42.240]   get around that is like, you have the high level theory, you say, okay, here's what's actually
[01:24:42.240 --> 01:24:46.720]   going on in the world. Like, here's what people don't understand. But you also have to have this
[01:24:46.720 --> 01:24:51.760]   lower level theory of here's what they think is going on. Here's why things keep moving in the
[01:24:51.760 --> 01:24:57.200]   like, keep ratifying the theory that they have. And then the next step is, okay, what actually
[01:24:57.200 --> 01:25:03.120]   breaks down that causes reality to collide, like perception to collide with reality? And,
[01:25:03.120 --> 01:25:08.080]   and then the other question of like, can perception actually undershoot in the same direction? So,
[01:25:08.080 --> 01:25:11.840]   you know, a lot of money was made by people who looked at the tech bubble in the late 90s said,
[01:25:11.840 --> 01:25:16.640]   this is going to blow up. And we're going to figure out when to short it. And then we'll short
[01:25:16.640 --> 01:25:21.280]   it, we know it'll just keep going down for a while that it's not, you know, it wasn't the dot coms
[01:25:21.280 --> 01:25:26.480]   were 20% too expensive, it's that most of them were worth zero. So those people made a lot of
[01:25:26.480 --> 01:25:30.160]   money by shorting after things started declining, knowing they would keep on declining. But a lot
[01:25:30.160 --> 01:25:35.040]   more money was made by people in 2003, 2004 saying, you know, this didn't actually discredit
[01:25:35.040 --> 01:25:39.040]   the internet, it's still a good technology. And it's not like we fundamentally can't make money
[01:25:39.040 --> 01:25:42.880]   online, it's that you can't make money online, if you don't know what you're doing, you massively
[01:25:42.880 --> 01:25:47.680]   overspend. And there just aren't that many people online and no one's spending money online yet. So
[01:25:47.680 --> 01:25:52.160]   a lot more money was made by people who were able to take advantage of the overshooting in the
[01:25:52.160 --> 01:25:56.400]   opposite direction, rather than the people who figured out from first principles that the bubble
[01:25:56.400 --> 01:26:01.120]   had bubble characteristics and was eventually going to pop. And that just requires a lot more
[01:26:01.120 --> 01:26:08.240]   of this micro level analysis. So you know, a lot of a lot of macro people are looking at what
[01:26:08.240 --> 01:26:13.440]   individual companies are doing and what they're saying and how how consumer sentiment is changing
[01:26:13.440 --> 01:26:18.240]   month to month and all these other very low level indicators where the indicators are not a thesis,
[01:26:18.240 --> 01:26:21.360]   but the indicators tell you something about when your thesis will become true.
[01:26:21.360 --> 01:26:25.440]   Mm hmm. Interesting. That's really interesting. I'm just gonna do some rapid fire questions for
[01:26:25.440 --> 01:26:30.320]   you burn now in the final few minutes. First, how can somebody be long AI but hedge for the
[01:26:30.320 --> 01:26:35.120]   possibility that Taiwan will be invaded? So you know, you want to I don't know if I should put
[01:26:35.120 --> 01:26:39.680]   money into TSMC, but I know that GPUs are going to be the next big thing, or we're going to be
[01:26:39.680 --> 01:26:46.400]   very important in the future. How do you make that position concrete? Oh, man, that's really hard.
[01:26:46.400 --> 01:26:54.080]   Like, I guess the next best thing would be looking at the Korean fabs because they are
[01:26:54.080 --> 01:26:59.120]   they're close, but I guess they don't produce the same chips. But I guess I guess the bet would be
[01:26:59.120 --> 01:27:06.240]   that Korea is the country most likely to catch up to Taiwan in the event that we're mostly going to
[01:27:06.240 --> 01:27:13.280]   catch up to where Taiwan is today if I want a longer an option. But I think like if you're
[01:27:13.280 --> 01:27:18.320]   betting if you're trying to make the AI bet conditional on Taiwan bet, I think a lot of
[01:27:18.320 --> 01:27:23.920]   what you want to do is actually think about how you underwrite the the Taiwan invasion that because
[01:27:23.920 --> 01:27:29.600]   that's probably the thing with the bigger long term impact. Maybe, maybe not. It's tricky. Like
[01:27:29.600 --> 01:27:34.080]   there's, you know, sometimes sometimes geopolitical changes can just lead to these these permanent
[01:27:34.080 --> 01:27:39.840]   inflections, like we have the the data on there's some kind of emissions that you can measure that
[01:27:39.840 --> 01:27:44.000]   is the result of copper mining. And so we can see how much copper mining changes changed year to
[01:27:44.000 --> 01:27:48.000]   year throughout history. And we do actually see like it was rising during the Roman Empire and
[01:27:48.000 --> 01:27:53.360]   then peaked and then went down and then didn't come back for like a millennium. So yeah, sometimes,
[01:27:53.360 --> 01:27:58.160]   sometimes there is a really unfortunate geopolitical inflection in underlying technology.
[01:27:58.160 --> 01:28:01.360]   But if you think about what that means in the real world, what it probably means is like
[01:28:01.360 --> 01:28:07.680]   that is that is the end, like, your big concern is not your portfolio in in a world where we
[01:28:07.680 --> 01:28:15.280]   were an invasion causes AI is the reason AI does not happen. So yeah, I would separate the invasion
[01:28:15.280 --> 01:28:23.120]   bet from the AI bet. And then I guess next best thing would be, I mean, the sad answer is, Intel
[01:28:23.120 --> 01:28:29.600]   is maybe Intel and TSM are sort of America's last hopes on this. I think there's like you can you
[01:28:29.600 --> 01:28:36.640]   can tell a story where invasion becomes more and more likely and the US does a sort of operation
[01:28:36.640 --> 01:28:41.200]   paperclip with with no connotation about the the political views of the engineers involved. But,
[01:28:41.200 --> 01:28:47.840]   you know, operation paperclips, all the best TSM engineers out to Arizona and has them all work on
[01:28:47.840 --> 01:28:54.160]   building those chips in the US, in which case TSM is still, you know, still a play, although it's
[01:28:54.160 --> 01:29:00.080]   certainly lost some valuable assets. But that's that's a very tricky question. And it may be one
[01:29:00.080 --> 01:29:03.840]   of those things where it's kind of kind of hard to hit, like there are there are sufficiently bad
[01:29:03.840 --> 01:29:08.320]   things, you know, there's there's not really a good meteor hedge, like, you know, in the in the
[01:29:08.320 --> 01:29:13.680]   minutes before the meteor hits us. Maybe maybe treasuries do outperform equities, but you don't
[01:29:13.680 --> 01:29:19.680]   really care. Okay, yeah, yeah. All right. I'll definitely have to have you on again in a few
[01:29:19.680 --> 01:29:23.600]   weeks because we've gotten through like, I don't know, like a quarter of the question. So okay.
[01:29:23.600 --> 01:29:28.880]   But it was really interesting. This is like probably the most fun episode I've done so far.
[01:29:28.880 --> 01:29:36.320]   So awesome. Yeah. So just another plug. It's the diff.co that's with two F's and the Twitter
[01:29:36.320 --> 01:29:40.480]   handle. What is your Twitter handle burn? It's my full name at burn Hobart.
[01:29:40.480 --> 01:29:44.080]   Okay, cool. Cool. Yeah. And highly recommend it for the most schizophrenic
[01:29:44.080 --> 01:29:50.160]   galaxy brain takes. Visit the diff.co. Awesome. Thanks, Vern. You bet.
[01:29:50.160 --> 01:29:59.520]   Hey, thanks for listening. If you enjoyed that episode, I would really, really, really appreciate
[01:29:59.520 --> 01:30:06.160]   it if you could share it. This is still a pretty small podcast. So it is a huge help when anyone
[01:30:06.160 --> 01:30:11.120]   of you shares an episode that you're like, posted on Twitter, send it to friends who you think might
[01:30:11.120 --> 01:30:17.280]   like it, put it in your group chats. Just let the word go forth. It helps out a ton. Many thanks to
[01:30:17.280 --> 01:30:26.080]   my amazing editor, Graham Bessaloo, for producing this podcast, and to Mia Ayana for creating the
[01:30:26.080 --> 01:30:31.520]   amazing transcripts that accompany each episode, which have helpful links, and you can find them
[01:30:31.520 --> 01:30:38.160]   at the link in the description below. Remember to subscribe on YouTube and your favorite podcast
[01:30:38.160 --> 01:30:50.880]   platforms. Cheers. See you next time.

