
[00:00:00.000 --> 00:00:03.360]   The following is a conversation with Charles Isbell and Michael Whitman.
[00:00:03.360 --> 00:00:07.000]   Charles is the dean of the College of Computing at Georgia Tech,
[00:00:07.000 --> 00:00:10.480]   and Michael is a computer science professor at Brown University.
[00:00:10.480 --> 00:00:14.480]   I've spoken with each of them individually on this podcast,
[00:00:14.480 --> 00:00:17.000]   and since they are good friends in real life,
[00:00:17.000 --> 00:00:20.920]   we all thought it would be fun to have a conversation together.
[00:00:20.920 --> 00:00:25.440]   Quick mention of each sponsor, followed by some thoughts related to the episode.
[00:00:25.440 --> 00:00:28.080]   Thank you to Athletic Greens,
[00:00:28.320 --> 00:00:32.680]   the all-in-one drink that I start every day with to cover all my nutritional bases,
[00:00:32.680 --> 00:00:38.120]   8 Sleep, a mattress that cools itself and gives me yet another reason to enjoy sleep,
[00:00:38.120 --> 00:00:43.280]   Masterclass, online courses from some of the most amazing humans in history,
[00:00:43.280 --> 00:00:46.720]   and Cash App, the app I use to send money to friends.
[00:00:46.720 --> 00:00:50.640]   Please check out the sponsors in the description to get a discount
[00:00:50.640 --> 00:00:52.280]   and to support this podcast.
[00:00:52.280 --> 00:00:57.360]   As a side note, let me say that having two guests on the podcast is an experiment
[00:00:57.680 --> 00:00:59.520]   that I've been meaning to do for a while,
[00:00:59.520 --> 00:01:02.400]   in particular because down the road,
[00:01:02.400 --> 00:01:06.600]   I would like to occasionally be a kind of moderator for debates
[00:01:06.600 --> 00:01:10.120]   between people that may disagree in some interesting ways.
[00:01:10.120 --> 00:01:14.680]   If you have suggestions for who you would like to see debate on this podcast,
[00:01:14.680 --> 00:01:15.800]   let me know.
[00:01:15.800 --> 00:01:20.160]   As with all experiments of this kind, it is a learning process.
[00:01:20.160 --> 00:01:23.160]   Both the video and the audio might need improvement.
[00:01:23.160 --> 00:01:27.280]   I realized I think I should probably do three or more cameras next time,
[00:01:27.280 --> 00:01:28.400]   as opposed to just two,
[00:01:28.400 --> 00:01:33.800]   and also try different ways to mount the microphone for the third person.
[00:01:33.800 --> 00:01:36.480]   Also, after recording this intro,
[00:01:36.480 --> 00:01:41.800]   I'm going to have to go figure out the thumbnail for the video version of the podcast,
[00:01:41.800 --> 00:01:45.120]   since I usually put the guest's head on the thumbnail,
[00:01:45.120 --> 00:01:51.840]   and now there's two heads and two names to try to fit into the thumbnail.
[00:01:51.840 --> 00:01:54.800]   It's a kind of a bin packing problem,
[00:01:55.080 --> 00:02:01.560]   which in theoretical computer science happens to be an NP hard problem.
[00:02:01.560 --> 00:02:05.280]   Whatever I come up with, if you have better ideas for the thumbnail,
[00:02:05.280 --> 00:02:06.120]   let me know as well.
[00:02:06.120 --> 00:02:09.840]   And in general, I always welcome ideas how this thing can be improved.
[00:02:09.840 --> 00:02:14.720]   If you enjoy it, subscribe on YouTube, review it with Five Stars on Apple Podcast,
[00:02:14.720 --> 00:02:20.440]   follow on Spotify, support on Patreon, or connect with me on Twitter at Lex Friedman.
[00:02:21.040 --> 00:02:26.040]   And now, here's my conversation with Charles Isbell and Michael Littman.
[00:02:26.040 --> 00:02:29.760]   You'll probably disagree about this question,
[00:02:29.760 --> 00:02:34.920]   but what is your biggest, would you say, disagreement about either something
[00:02:34.920 --> 00:02:39.440]   profound and very important or something completely not important at all?
[00:02:39.440 --> 00:02:41.040]   I don't think I have any disagreements at all.
[00:02:41.040 --> 00:02:43.280]   Ah, I'm not sure that's true.
[00:02:43.280 --> 00:02:45.360]   We walked into that one, didn't we?
[00:02:45.360 --> 00:02:48.760]   So one thing that you sometimes mention is that,
[00:02:48.760 --> 00:02:51.440]   and we did this one on air too, as it were,
[00:02:51.440 --> 00:02:55.200]   whether or not machine learning is computational statistics.
[00:02:55.200 --> 00:02:55.760]   It's not.
[00:02:55.760 --> 00:02:57.720]   But it is.
[00:02:57.720 --> 00:02:58.400]   Well, it's not.
[00:02:58.400 --> 00:03:02.520]   And in particular, and more importantly, it is not just computational statistics.
[00:03:02.520 --> 00:03:04.160]   So what's missing in the picture?
[00:03:04.160 --> 00:03:05.160]   All the rest of it.
[00:03:05.160 --> 00:03:07.320]   What's missing?
[00:03:07.320 --> 00:03:08.400]   That which is missing.
[00:03:08.400 --> 00:03:10.280]   Oh, well, you can't be wrong now.
[00:03:10.280 --> 00:03:11.480]   Well, it's not just the statistics.
[00:03:11.480 --> 00:03:12.480]   He doesn't even believe this.
[00:03:12.480 --> 00:03:14.040]   We've had this conversation before.
[00:03:14.040 --> 00:03:18.120]   If it were just the statistics, then we would be happy with where we are.
[00:03:18.840 --> 00:03:19.760]   But it's not just the statistics.
[00:03:19.760 --> 00:03:21.440]   That's why it's computational statistics.
[00:03:21.440 --> 00:03:22.640]   Or if it were just the computational...
[00:03:22.640 --> 00:03:24.600]   I agree that machine learning is not just statistics.
[00:03:24.600 --> 00:03:25.480]   It is not just statistics.
[00:03:25.480 --> 00:03:26.280]   We can agree on that.
[00:03:26.280 --> 00:03:28.000]   Nor is it just computational statistics.
[00:03:28.000 --> 00:03:29.280]   It's computational statistics.
[00:03:29.280 --> 00:03:30.080]   It is computational.
[00:03:30.080 --> 00:03:32.760]   What is the computational and computational statistics?
[00:03:32.760 --> 00:03:34.880]   Does this take us into the realm of computing?
[00:03:34.880 --> 00:03:35.600]   It does.
[00:03:35.600 --> 00:03:42.400]   But I think perhaps the way I can get him to admit that he's wrong is that it's about rules.
[00:03:42.400 --> 00:03:44.120]   It's about rules.
[00:03:44.120 --> 00:03:45.000]   It's about symbols.
[00:03:45.000 --> 00:03:45.960]   It's about all these other things.
[00:03:45.960 --> 00:03:47.480]   But statistics is not about rules?
[00:03:47.480 --> 00:03:48.840]   I'm going to say statistics is about rules.
[00:03:48.840 --> 00:03:50.120]   But it's not just the statistics, right?
[00:03:50.120 --> 00:03:52.560]   It's not just a random variable that you choose and you have a probability...
[00:03:52.560 --> 00:03:54.440]   I think you have a narrow view of statistics.
[00:03:54.440 --> 00:03:54.760]   Okay.
[00:03:54.760 --> 00:04:02.760]   Well, then what would be the broad view of statistics that would still allow it to be statistics and not say history that would make computational statistics okay?
[00:04:02.760 --> 00:04:03.320]   Well, okay.
[00:04:03.320 --> 00:04:11.080]   So I had my first sort of research mentor, a guy named Tom Landauer, taught me to do some statistics.
[00:04:11.080 --> 00:04:12.160]   Right.
[00:04:12.360 --> 00:04:18.600]   And I was annoyed all the time because the statistics would say that what I was doing was not statistically significant.
[00:04:18.600 --> 00:04:20.240]   And I was like, but...
[00:04:20.240 --> 00:04:21.200]   But...
[00:04:21.200 --> 00:04:21.760]   But...
[00:04:21.760 --> 00:04:28.880]   And basically what he said to me is statistics is how you're going to keep from lying to yourself, which I thought was really deep.
[00:04:28.880 --> 00:04:33.160]   It is a way to keep yourself honest in a particular way.
[00:04:33.160 --> 00:04:33.880]   I agree with that.
[00:04:33.880 --> 00:04:34.240]   Yeah.
[00:04:34.240 --> 00:04:35.840]   And so you're trying to find rules.
[00:04:35.840 --> 00:04:38.400]   I'm just going to bring it back to rules.
[00:04:38.400 --> 00:04:39.400]   Wait, wait, wait.
[00:04:40.360 --> 00:04:43.800]   Could you possibly try to define rules?
[00:04:43.800 --> 00:04:51.160]   Even regular statisticians, non-computational statisticians, do spend some of their time evaluating rules, right?
[00:04:51.160 --> 00:04:54.720]   Applying statistics to try and understand, does this rule capture this?
[00:04:54.720 --> 00:04:55.640]   Does this not capture that?
[00:04:55.640 --> 00:04:56.920]   You mean like hypothesis testing kind of thing?
[00:04:56.920 --> 00:04:57.560]   Sure.
[00:04:57.560 --> 00:04:59.360]   Or like confidence intervals?
[00:04:59.360 --> 00:05:01.560]   I think more like hypothesis.
[00:05:01.560 --> 00:05:06.480]   Like I feel like the word statistic literally means like a summary, like a number that summarizes other numbers.
[00:05:06.480 --> 00:05:06.840]   Right.
[00:05:06.840 --> 00:05:14.800]   But I think the field of statistics actually applies that idea to things like rules, to understand whether or not a rule is valid.
[00:05:14.800 --> 00:05:16.640]   So software engineering statistics?
[00:05:16.640 --> 00:05:18.480]   No.
[00:05:18.480 --> 00:05:20.160]   Programming languages statistics?
[00:05:20.160 --> 00:05:21.160]   No.
[00:05:21.160 --> 00:05:29.280]   Because I think it's useful to think about a lot of what AI and machine learning is, or certainly should be, as software engineering, as programming languages.
[00:05:29.280 --> 00:05:35.520]   Just to put it in language that you might understand, the hyperparameters beyond the problem itself.
[00:05:35.520 --> 00:05:37.640]   The hyperparameters is too many syllables for me to understand.
[00:05:37.640 --> 00:05:38.680]   The hyperparameters.
[00:05:38.680 --> 00:05:40.080]   That's better.
[00:05:40.080 --> 00:05:41.080]   That goes around it, right?
[00:05:41.080 --> 00:05:42.800]   It's the decisions you choose to make.
[00:05:42.800 --> 00:05:44.800]   It's the metrics you choose to use.
[00:05:44.800 --> 00:05:45.680]   It's the loss function you choose to focus on.
[00:05:45.680 --> 00:05:50.000]   You want to say the practice of machine learning is different than the practice of statistics.
[00:05:50.000 --> 00:05:54.240]   Like the things you have to worry about and how you worry about them are different, therefore they're different.
[00:05:54.240 --> 00:05:54.800]   Right.
[00:05:54.800 --> 00:05:58.920]   At a very little, I mean, at the very least, it's that much is true.
[00:05:58.920 --> 00:06:01.960]   It doesn't mean that statistics, computational or otherwise, aren't important.
[00:06:01.960 --> 00:06:02.920]   I think they are.
[00:06:02.920 --> 00:06:05.240]   I mean, I do a lot of that, for example.
[00:06:05.640 --> 00:06:06.880]   But I think it goes beyond that.
[00:06:06.880 --> 00:06:12.280]   I think that we could think about game theory in terms of statistics, but I don't think it's very as useful to do.
[00:06:12.280 --> 00:06:16.600]   I mean, the way I would think about it, or a way I would think about it is this way.
[00:06:16.600 --> 00:06:18.760]   Chemistry is just physics.
[00:06:18.760 --> 00:06:23.280]   But I don't think it's as useful to think about chemistry as being just physics.
[00:06:23.280 --> 00:06:25.200]   It's useful to think about it as chemistry.
[00:06:25.200 --> 00:06:26.920]   The level of abstraction really matters here.
[00:06:26.920 --> 00:06:30.000]   So I think it is, there are contexts in which it is useful.
[00:06:30.000 --> 00:06:30.640]   Yes.
[00:06:30.640 --> 00:06:31.240]   To think of it that way, right?
[00:06:31.240 --> 00:06:31.480]   No, that is.
[00:06:31.480 --> 00:06:33.120]   And so finding that connection is actually helpful.
[00:06:33.120 --> 00:06:36.080]   And I think that's when I emphasize the computational statistics thing.
[00:06:36.080 --> 00:06:41.280]   I think I want to befriend statistics and not absorb them.
[00:06:41.280 --> 00:06:44.720]   Here's the A way to think about it beyond what I just said, right?
[00:06:44.720 --> 00:06:49.240]   So what would you say, and I want you to think back to a conversation we had a very long time ago.
[00:06:49.240 --> 00:06:56.600]   What would you say is the difference between, say, the early 2000s ICML and what we used to call NIPS, NIRPS?
[00:06:56.600 --> 00:06:58.000]   Is there a difference?
[00:06:58.000 --> 00:07:00.480]   A lot of, particularly on the machine learning that was done there?
[00:07:00.480 --> 00:07:02.560]   ICML was around that long?
[00:07:02.880 --> 00:07:03.200]   Oh, yeah.
[00:07:03.200 --> 00:07:05.800]   So ICLR is the new conference, newish.
[00:07:05.800 --> 00:07:07.320]   Yeah, I guess so.
[00:07:07.320 --> 00:07:09.600]   And ICML was around the 2000?
[00:07:09.600 --> 00:07:11.320]   Oh, ICML predates that.
[00:07:11.320 --> 00:07:14.840]   I think my most cited ICML paper is from '94.
[00:07:14.840 --> 00:07:17.880]   Michael knows this better than me because, of course, he's significantly older than I am.
[00:07:17.880 --> 00:07:24.320]   But the point is, what is the difference between ICML and NIRPS in the late '90s, early 2000s?
[00:07:24.320 --> 00:07:32.800]   I don't know what everyone else's perspective would be, but I had a particular perspective at that time, which is I felt like ICML was more of a computer science
[00:07:32.800 --> 00:07:39.880]   place and that NIPS, NIRPS was more of an engineering place, like the kind of math that happened at the two places.
[00:07:39.880 --> 00:07:47.960]   As a computer scientist, I felt more comfortable with the ICML math and the NIRPS people would say that that's because I'm dumb.
[00:07:47.960 --> 00:07:50.800]   And that's such an engineering thing to say.
[00:07:50.800 --> 00:07:52.560]   So I agree with that part of it.
[00:07:52.560 --> 00:07:53.280]   I do a little different.
[00:07:53.280 --> 00:07:58.440]   We actually had a nice conversation with Tom Dietrich about this on Twitter just a couple of days ago.
[00:07:58.440 --> 00:08:10.640]   I put it a little differently, which is that ICML was machine learning done by computer scientists and NIRPS was machine learning done by computer scientists trying to impress statisticians.
[00:08:10.640 --> 00:08:16.880]   Which was weird because it was the same people, at least by the time I started paying attention.
[00:08:16.880 --> 00:08:18.800]   But it just felt very, very different.
[00:08:18.800 --> 00:08:28.400]   And I think that that perspective of whether you're trying to impress the statisticians or you're trying to impress the programmers is actually very different and has real impact on what you do.
[00:08:28.400 --> 00:08:31.440]   You choose to worry about and what kind of outcomes you come to.
[00:08:31.440 --> 00:08:32.720]   So I think it really matters.
[00:08:32.720 --> 00:08:34.480]   And computational statistics is a means to an end.
[00:08:34.480 --> 00:08:35.800]   It is not an end in some sense.
[00:08:35.800 --> 00:08:43.080]   And I think that really matters here in the same way that I don't think computer science is just engineering or just science or just math or whatever.
[00:08:43.080 --> 00:08:46.200]   Okay, so I'd have to now agree that now we agree on everything.
[00:08:46.200 --> 00:08:46.800]   Yes.
[00:08:46.800 --> 00:08:47.540]   Yes.
[00:08:47.540 --> 00:08:54.560]   The important thing here is that my opinions may have changed, but not the fact that I'm right, I think is what we just came to.
[00:08:54.560 --> 00:08:57.040]   Right. And my opinions may have changed and not the fact that I'm wrong.
[00:08:57.040 --> 00:08:57.860]   That's right.
[00:08:58.860 --> 00:08:59.620]   You lost me.
[00:08:59.620 --> 00:09:00.220]   I'm not even...
[00:09:00.220 --> 00:09:01.620]   I think I lost myself there too.
[00:09:01.620 --> 00:09:03.300]   But anyway, we're back.
[00:09:03.300 --> 00:09:05.380]   This happens to us sometimes.
[00:09:05.380 --> 00:09:05.880]   We're sorry.
[00:09:05.880 --> 00:09:14.500]   How does neural networks change this, just to even linger on this topic, change this idea of statistics?
[00:09:14.500 --> 00:09:18.820]   How big of a pie statistics is within the machine learning thing?
[00:09:18.820 --> 00:09:23.740]   Like, because it sounds like hyperparameters and also just the role of data.
[00:09:24.180 --> 00:09:33.300]   You know, people are starting to use this terminology of software 2.0, which is like the act of programming as a...
[00:09:33.300 --> 00:09:43.460]   Like you're a designer in the hyperparameter space of neural networks, and you're also the collector and the organizer and the cleaner of the data.
[00:09:43.460 --> 00:09:45.700]   And that's part of the programming.
[00:09:46.940 --> 00:09:57.060]   So how did, on the NeurIPS versus ICML topic, what's the role of neural networks in redefining the size and the role of machine learning?
[00:09:57.060 --> 00:10:00.980]   I can't wait to hear what Michael thinks about this, but I would add one.
[00:10:00.980 --> 00:10:01.480]   But you will.
[00:10:01.480 --> 00:10:03.140]   But that's true.
[00:10:03.140 --> 00:10:04.140]   I will. I'll force myself to.
[00:10:04.140 --> 00:10:10.700]   I think there's one other thing I would add to your description, which is the kind of software engineering part of what does it mean to debug, for example.
[00:10:10.900 --> 00:10:20.740]   But this is a difference between the kind of computational statistics view of machine learning and the computational view of machine learning, which is, I think, one is worried about the equation, as it were.
[00:10:20.740 --> 00:10:22.980]   And by the way, this is not a value judgment.
[00:10:22.980 --> 00:10:24.580]   I just think it's about perspective.
[00:10:24.580 --> 00:10:32.740]   But the kind of questions you would ask, you start asking yourself, well, what does it mean to program and develop and build the system, is a very computer science-y view of the problem.
[00:10:33.020 --> 00:10:44.540]   I mean, if you get on data science Twitter and econ Twitter, you actually hear this a lot with the economist and the data scientist complaining about the machine learning people.
[00:10:44.540 --> 00:10:46.220]   Well, it's just statistics.
[00:10:46.220 --> 00:10:49.300]   And I don't know why they don't see this, but they're not even asking the same questions.
[00:10:49.300 --> 00:10:52.780]   They're not thinking about it as a kind of programming problem.
[00:10:52.780 --> 00:10:55.660]   And I think that that really matters, just asking this question.
[00:10:55.660 --> 00:11:02.620]   I actually think it's a little different from programming and hyperparameter space and sort of collecting the data.
[00:11:02.900 --> 00:11:05.980]   But I do think that that immersion really matters.
[00:11:05.980 --> 00:11:08.620]   So I'll give you a quick example, the way I think about this.
[00:11:08.620 --> 00:11:09.780]   So I teach machine learning.
[00:11:09.780 --> 00:11:16.940]   Michael and I have co-taught a machine learning class, which has now reached, I don't know, 10,000 people at least over the last several years or somewhere there's about.
[00:11:16.940 --> 00:11:21.180]   And my machine learning assignments are of this form.
[00:11:21.180 --> 00:11:31.140]   So the first one is something like implement these five algorithms, you know, K and N and SVMs and boosting and decision trees and neural networks.
[00:11:31.140 --> 00:11:31.860]   And maybe that's it.
[00:11:31.860 --> 00:11:32.420]   I can't remember.
[00:11:32.580 --> 00:11:34.300]   And when I say implement, I mean steal the code.
[00:11:34.300 --> 00:11:35.980]   I'm completely uninterested.
[00:11:35.980 --> 00:11:38.500]   You get zero points for getting the thing to work.
[00:11:38.500 --> 00:11:47.420]   I don't want you spending your time worrying about getting the corner case right of, you know, what happens when you are trying to normalize distances and the points on the thing.
[00:11:47.420 --> 00:11:48.260]   And so you divide by zero.
[00:11:48.260 --> 00:11:49.420]   I'm not interested in that, right?
[00:11:49.420 --> 00:11:50.620]   Steal the code.
[00:11:50.620 --> 00:11:55.460]   However, you're going to run those algorithms on two data sets.
[00:11:55.460 --> 00:11:57.420]   The data sets have to be interesting.
[00:11:57.420 --> 00:11:58.820]   What does it mean to be interesting?
[00:11:58.820 --> 00:12:05.940]   Well, data sets are interesting if it reveals differences between algorithms, which presumably are all the same because they can represent whatever they can represent.
[00:12:05.940 --> 00:12:10.100]   And two data sets are interesting together if they show different differences, as it were.
[00:12:10.100 --> 00:12:11.860]   And you have to analyze them.
[00:12:11.860 --> 00:12:14.740]   You have to justify their interestingness and you have to analyze them in a whole bunch of ways.
[00:12:14.740 --> 00:12:18.300]   But all I care about is the data in your analysis, not the programming.
[00:12:18.300 --> 00:12:21.260]   And I occasionally end up in these long discussions with students.
[00:12:21.260 --> 00:12:22.580]   Well, I don't really.
[00:12:22.580 --> 00:12:28.180]   I copy and paste the things that I've said the other 15,000 times it's come up, which is, they go, but the only way to learn.
[00:12:28.980 --> 00:12:35.140]   Really understand is to code them up, which is a very programmer software engineering view of the world.
[00:12:35.140 --> 00:12:40.180]   If you don't program it, you don't understand it, which is, by the way, I think is wrong in a very specific way.
[00:12:40.180 --> 00:12:44.500]   But it is a way that you come to understand because then you have to wrestle with the algorithm.
[00:12:44.500 --> 00:12:49.100]   But the thing about machine learning is not just sorting numbers where in some sense the data doesn't matter.
[00:12:49.100 --> 00:12:51.780]   What matters is, well, does the algorithm work on these abstract things?
[00:12:51.780 --> 00:12:52.620]   And one less than the other.
[00:12:52.620 --> 00:12:54.700]   In machine learning, the data matters.
[00:12:54.700 --> 00:12:56.540]   It matters more than almost anything.
[00:12:57.180 --> 00:12:59.740]   And not everything, but almost anything.
[00:12:59.740 --> 00:13:04.820]   And so as a result, you have to live with the data and don't get distracted by the algorithm per se.
[00:13:04.820 --> 00:13:23.100]   And I think that that focus on the data and what it can tell you and what question it's actually answering for you, as opposed to the question you thought you were asking, is a key and important thing about machine learning and is a way that computationalists, as opposed to statisticians, bring a particular view about how to think about the process.
[00:13:23.300 --> 00:13:35.700]   The statisticians, by contrast, bring, I think I'd be willing to say, a better view about the kind of formal math that's behind it and what an actual number ultimately is saying about the data.
[00:13:35.700 --> 00:13:37.740]   And those are both important, but they're also different.
[00:13:37.740 --> 00:13:50.180]   - I didn't really think of it this way, is to build intuition about the role of data, the different characteristics of data, by having two data sets that are different and that reveal the differences in the differences.
[00:13:50.820 --> 00:13:54.660]   That's a really fascinating, that's a really interesting educational approach.
[00:13:54.660 --> 00:13:57.060]   - The students love it, but not right away.
[00:13:57.060 --> 00:13:58.620]   - No, they love it at the end. - They love it later.
[00:13:58.620 --> 00:14:00.740]   - They love it at the end, not at the beginning.
[00:14:00.740 --> 00:14:04.020]   - Not even immediately after.
[00:14:04.020 --> 00:14:07.380]   - I feel like there's a deep, profound lesson about education there.
[00:14:07.380 --> 00:14:07.940]   - Yeah.
[00:14:07.940 --> 00:14:14.900]   - That you can't listen to students about whether what you're doing is the right or the wrong thing.
[00:14:16.060 --> 00:14:26.420]   - Well, as a wise, Michael Lippman once said to me about children, which I think applies to teaching, is you have to give them what they need without bending to their will.
[00:14:26.420 --> 00:14:27.980]   And students are like that.
[00:14:27.980 --> 00:14:29.020]   You have to figure out what they need.
[00:14:29.020 --> 00:14:29.620]   You're a curator.
[00:14:29.620 --> 00:14:34.980]   Your whole job is to curate and to present, because on their own, they're not going to necessarily know where to search.
[00:14:34.980 --> 00:14:38.340]   So you're providing pushes in some direction and learn space.
[00:14:38.940 --> 00:14:49.140]   And you have to give them what they need in a way that keeps them engaged enough so that they eventually discover what they want and they get the tools they need to go and learn other things off of.
[00:14:49.140 --> 00:14:52.020]   - What's your view?
[00:14:52.020 --> 00:14:55.620]   Let me put on my Russian hat, which believes that life is suffering.
[00:14:55.620 --> 00:14:56.740]   - I like Russian hats, by the way.
[00:14:56.740 --> 00:14:58.020]   If you have one, I would like this.
[00:14:58.020 --> 00:14:58.900]   - Those are ridiculous.
[00:14:58.900 --> 00:14:59.300]   Yes.
[00:14:59.300 --> 00:15:01.380]   - But in a delightful way.
[00:15:01.380 --> 00:15:01.940]   But sure.
[00:15:01.940 --> 00:15:07.700]   - What do you think is the role of, we talked about balance a little bit.
[00:15:08.020 --> 00:15:11.140]   What do you think is the role of hardship in education?
[00:15:11.140 --> 00:15:24.260]   Like, I think the biggest things I've learned, like what made me fall in love with math, for example, is by being bad at it until I got good at it.
[00:15:24.260 --> 00:15:32.580]   So like struggling with a problem, which increased the level of joy I felt when I finally figured it out.
[00:15:33.380 --> 00:15:44.180]   And it always felt with me, with teachers, especially modern discussions of education, how can we make education more fun, more engaging, more all those things?
[00:15:44.180 --> 00:15:51.540]   Or from my perspective, it's like you're maybe missing the point that education, that life is suffering.
[00:15:51.540 --> 00:15:59.140]   Education is supposed to be hard and that actually what increases the joy you feel when you actually learn something.
[00:15:59.140 --> 00:16:01.260]   Is that ridiculous?
[00:16:02.300 --> 00:16:03.900]   - Do you like to see your students suffer?
[00:16:03.900 --> 00:16:06.740]   - Okay, so this may be a point where we differ.
[00:16:06.740 --> 00:16:08.300]   - I suspect not.
[00:16:08.300 --> 00:16:09.220]   I'm gonna do go on.
[00:16:09.220 --> 00:16:11.100]   - Well, what would your answer be?
[00:16:11.100 --> 00:16:11.900]   - I wanna hear you first.
[00:16:11.900 --> 00:16:14.140]   - Okay, well, I was gonna not answer the question.
[00:16:14.140 --> 00:16:17.940]   - So you don't want the students to know you enjoy them suffering?
[00:16:17.940 --> 00:16:18.900]   - No, no, no, no, no, no.
[00:16:18.900 --> 00:16:24.940]   I was gonna say that there's, I think there's a distinction that you can make in the kind of suffering, right?
[00:16:24.940 --> 00:16:33.340]   So I think you can be in a mode where you're suffering in a hopeless way versus you're suffering in a hopeful way, right?
[00:16:33.340 --> 00:16:41.900]   Where you're like, you can see that you still have, you can still imagine getting to the end, right?
[00:16:41.900 --> 00:16:48.980]   And as long as people are in that mindset where they're struggling, but it's not a hopeless kind of struggling, that's productive.
[00:16:48.980 --> 00:16:50.060]   I think that's really helpful.
[00:16:50.460 --> 00:17:00.380]   But if struggling, like if you break their will, if you leave them hopeless, no, that don't, sure, some people are gonna, whatever, lift themselves up by their bootstraps.
[00:17:00.380 --> 00:17:03.420]   But like mostly you give up and certainly it takes the joy out of it.
[00:17:03.420 --> 00:17:07.100]   And you're not gonna spend a lot of time on something that brings you no joy.
[00:17:07.100 --> 00:17:10.300]   So it is a bit of a delicate balance, right?
[00:17:10.300 --> 00:17:15.980]   You have to thwart people in a way that they still believe that there's a way through.
[00:17:15.980 --> 00:17:17.140]   - Right.
[00:17:17.140 --> 00:17:19.900]   So that's a, that we strongly agree, actually.
[00:17:19.900 --> 00:17:23.660]   So I think, well, first off, struggling and suffering aren't the same thing, right?
[00:17:23.660 --> 00:17:25.020]   - Yeah, just being poetic.
[00:17:25.020 --> 00:17:27.580]   - Oh, no, no, I actually appreciate the poetry.
[00:17:27.580 --> 00:17:32.780]   And I, one of the reasons I appreciate it is that they are often the same thing and often quite different, right?
[00:17:32.780 --> 00:17:34.380]   So you can struggle without suffering.
[00:17:34.380 --> 00:17:37.100]   You can certainly suffer pretty easily.
[00:17:37.100 --> 00:17:38.700]   You don't necessarily have to struggle to suffer.
[00:17:38.700 --> 00:17:42.860]   So I think that you want people to struggle, but that hope matters.
[00:17:42.860 --> 00:17:46.380]   You have to, they have to understand that they're gonna get through it on the other side.
[00:17:46.380 --> 00:17:48.380]   And it's very easy to confuse the two.
[00:17:48.940 --> 00:17:55.260]   I actually think Brown University has a very, just philosophically has a very different take
[00:17:55.260 --> 00:17:59.500]   on the relationship with their students, particularly undergrads from say, a place like Georgia Tech, which is--
[00:17:59.500 --> 00:18:00.780]   - Which university is better?
[00:18:00.780 --> 00:18:03.180]   - Well, I have my opinions on that.
[00:18:03.180 --> 00:18:07.020]   - I mean, remember Charles said, "It doesn't matter what the facts are, I'm always right."
[00:18:07.020 --> 00:18:09.580]   - The correct answer is that it doesn't matter.
[00:18:09.580 --> 00:18:13.020]   They're different, but they're clearly answers to that.
[00:18:13.020 --> 00:18:18.060]   - He went to a school like the school where he is as an undergrad.
[00:18:18.060 --> 00:18:20.860]   I went to a school, specifically the same school,
[00:18:20.860 --> 00:18:23.180]   though it changed a bit in the intervening years.
[00:18:23.180 --> 00:18:23.980]   - Brown or Georgia Tech?
[00:18:23.980 --> 00:18:25.100]   - No, I was talking about Georgia Tech.
[00:18:25.100 --> 00:18:25.660]   And I went--
[00:18:25.660 --> 00:18:25.980]   - Georgia Tech's changed.
[00:18:25.980 --> 00:18:29.580]   - Yeah, and I went to an undergrad place that's a lot like the place where I work now.
[00:18:29.580 --> 00:18:32.940]   And so it does seem like we're more familiar with these models.
[00:18:32.940 --> 00:18:34.860]   - There's a similarity between Brown and Yale?
[00:18:34.860 --> 00:18:38.060]   - Yeah, I think they're quite similar, yeah.
[00:18:38.060 --> 00:18:38.700]   - And Duke.
[00:18:38.700 --> 00:18:42.620]   - Duke has some similarities too, but it's got a little Southern draw.
[00:18:42.620 --> 00:18:47.340]   - You've kind of worked, you've sort of worked at universities that are like the places where you
[00:18:48.060 --> 00:18:48.460]   learned.
[00:18:48.460 --> 00:18:51.580]   And the same would be true for me.
[00:18:51.580 --> 00:18:55.900]   - Are you uncomfortable venturing outside the box?
[00:18:55.900 --> 00:18:56.700]   Is that what you're saying?
[00:18:56.700 --> 00:18:57.020]   - Mm-hmm.
[00:18:57.020 --> 00:18:57.580]   - Journeying out?
[00:18:57.580 --> 00:18:58.220]   - Not what I'm saying.
[00:18:58.220 --> 00:18:59.340]   - Yeah, Charles is definitely.
[00:18:59.340 --> 00:18:59.820]   - But it--
[00:18:59.820 --> 00:19:02.540]   - He only goes to places that have institute in the name, right?
[00:19:02.540 --> 00:19:03.980]   - It has worked out that way.
[00:19:03.980 --> 00:19:05.980]   Well, academic places anyway.
[00:19:05.980 --> 00:19:10.780]   Well, no, I was a visiting scientist at UPenn, or visiting something at UPenn.
[00:19:10.780 --> 00:19:13.500]   - Oh, wow, I just understood your joke.
[00:19:13.500 --> 00:19:14.380]   - Which one?
[00:19:14.380 --> 00:19:15.580]   (laughing)
[00:19:15.580 --> 00:19:17.180]   - Five minutes later.
[00:19:17.180 --> 00:19:19.340]   - I like to set these sort of time bombs.
[00:19:19.340 --> 00:19:24.860]   - The institute is in the, that Charles only goes to places that have institute in the name.
[00:19:24.860 --> 00:19:30.380]   So I guess Georgia, I forget that Georgia Tech is Georgia Institute of Technology.
[00:19:30.380 --> 00:19:35.420]   - The number of people who refer to it as Georgia Tech University is large and incredibly irritating.
[00:19:35.420 --> 00:19:36.380]   (laughing)
[00:19:36.380 --> 00:19:38.940]   This is one of the few things that genuinely gets under my skin.
[00:19:38.940 --> 00:19:43.660]   - But like schools like Georgia Tech and MIT have, as part of the ethos, like there is,
[00:19:43.660 --> 00:19:49.500]   I wanna say there's an abbreviation that someone taught me, like IHTFP, something like that.
[00:19:49.500 --> 00:19:54.780]   Like there's an expression which is basically, I hate being here, which they say so proudly.
[00:19:54.780 --> 00:19:57.820]   And that is definitely not the ethos at Brown.
[00:19:57.820 --> 00:20:02.060]   Like Brown is, there's a little more pampering and empowerment and stuff.
[00:20:02.060 --> 00:20:04.860]   And it's not like we're gonna crush you and you're gonna love it.
[00:20:04.860 --> 00:20:08.300]   So yeah, I think there's a, I think the ethoses are different.
[00:20:08.300 --> 00:20:09.980]   - Mm-hmm, that's interesting, yeah.
[00:20:09.980 --> 00:20:11.100]   - We had drone proofer.
[00:20:11.100 --> 00:20:12.380]   - What's that?
[00:20:12.380 --> 00:20:15.820]   - In order to graduate from Georgia Tech, this is a true thing, feel free to look it up.
[00:20:15.820 --> 00:20:16.860]   (laughing)
[00:20:16.860 --> 00:20:17.260]   If you--
[00:20:17.260 --> 00:20:18.700]   - A lot of schools have this, by the way.
[00:20:18.700 --> 00:20:20.700]   - No, actually Georgia Tech was really the first--
[00:20:20.700 --> 00:20:21.420]   - Brandeis has it.
[00:20:21.420 --> 00:20:22.220]   - Had it.
[00:20:22.220 --> 00:20:27.020]   - I feel like Georgia Tech was the first in a lot of ways.
[00:20:27.020 --> 00:20:28.700]   - It was the first in a lot of things.
[00:20:28.700 --> 00:20:29.500]   Had the first master's degree--
[00:20:29.500 --> 00:20:30.220]   - Bumblebee mascot.
[00:20:30.220 --> 00:20:30.940]   - Stop that.
[00:20:30.940 --> 00:20:32.140]   (laughing)
[00:20:32.140 --> 00:20:34.060]   First master's in computer science, actually.
[00:20:34.060 --> 00:20:35.100]   - Right, online master's.
[00:20:35.100 --> 00:20:37.020]   - Well, that too, but way back in the '60s.
[00:20:37.020 --> 00:20:37.580]   - Oh, really?
[00:20:37.580 --> 00:20:38.540]   - Yeah, yeah.
[00:20:38.540 --> 00:20:38.700]   - Nice.
[00:20:38.700 --> 00:20:41.820]   - We had the first information and computer science master's degree in the country.
[00:20:42.700 --> 00:20:48.140]   But the Georgia Tech, it used to be the case in order to graduate from Georgia Tech,
[00:20:48.140 --> 00:20:50.700]   you had to take a drown proofing class, where effectively,
[00:20:50.700 --> 00:20:54.380]   they threw you in the water, tied you up, and if you didn't drown, you got to graduate.
[00:20:54.380 --> 00:20:55.180]   - Tied you up?
[00:20:55.180 --> 00:20:56.140]   - I believe so.
[00:20:56.140 --> 00:20:56.380]   - No.
[00:20:56.380 --> 00:20:59.260]   - You basically, there were certainly versions of it, but I mean, luckily,
[00:20:59.260 --> 00:21:02.780]   they ended it just before I had to graduate, because otherwise I would have never graduated.
[00:21:02.780 --> 00:21:02.860]   (laughing)
[00:21:02.860 --> 00:21:04.060]   It wasn't gonna happen.
[00:21:04.060 --> 00:21:08.940]   I wanna say '84, '83, someone around then, they ended it, but yeah,
[00:21:08.940 --> 00:21:12.940]   you used to have to prove you could tread water for some ridiculous amount of time.
[00:21:12.940 --> 00:21:13.660]   - Two minutes.
[00:21:13.660 --> 00:21:15.420]   - You couldn't graduate. No, it was more than two minutes.
[00:21:15.420 --> 00:21:16.060]   - I bet it was two minutes.
[00:21:16.060 --> 00:21:16.860]   - Okay, well, we'll look at that.
[00:21:16.860 --> 00:21:17.980]   - And it was in a bathtub.
[00:21:17.980 --> 00:21:18.700]   (laughing)
[00:21:18.700 --> 00:21:21.340]   - It was in a pool, but it was a real thing.
[00:21:21.340 --> 00:21:22.860]   But that idea that, you know, push you--
[00:21:22.860 --> 00:21:23.900]   - Fully clothed.
[00:21:23.900 --> 00:21:25.020]   - Yeah, fully clothed.
[00:21:25.020 --> 00:21:30.220]   - I bet it was that and not tied up, because like, who needs to learn how to swim when you're tied?
[00:21:30.220 --> 00:21:34.060]   Nobody, but who needs to learn to swim when you're actually falling into the water dressed?
[00:21:34.060 --> 00:21:34.700]   That's a real thing.
[00:21:34.700 --> 00:21:37.180]   - I think your facts are getting in the way with a good story.
[00:21:37.180 --> 00:21:38.300]   - Oh, that's fair, that's fair.
[00:21:38.300 --> 00:21:39.100]   (laughing)
[00:21:39.100 --> 00:21:40.380]   - All right, so they tie you up.
[00:21:40.380 --> 00:21:43.180]   - Sometimes the narrative matters more, but whatever it was, you had to,
[00:21:43.180 --> 00:21:44.700]   it was called drown-proofing for a reason.
[00:21:44.700 --> 00:21:47.820]   The point of the story, Michael, is--
[00:21:47.820 --> 00:21:48.460]   - Struggle.
[00:21:48.460 --> 00:21:50.460]   - Well, no, but that's good.
[00:21:50.460 --> 00:21:51.020]   - That was, yeah.
[00:21:51.020 --> 00:21:52.220]   - It does bring it back to struggle.
[00:21:52.220 --> 00:21:56.220]   That's a part of what Georgia Tech has always been, and we struggle with that, by the way,
[00:21:56.220 --> 00:21:59.740]   about what we want to be, particularly as things go.
[00:21:59.740 --> 00:22:06.540]   But you sort of, how much can you be pushed without breaking?
[00:22:06.540 --> 00:22:08.780]   And you come out of the other end stronger, right?
[00:22:08.780 --> 00:22:10.460]   There's this saying we used to have when I was an undergrad there,
[00:22:10.460 --> 00:22:13.260]   which was Georgia Tech, building tomorrow the night before.
[00:22:13.260 --> 00:22:13.820]   (laughing)
[00:22:13.820 --> 00:22:17.660]   Right, and it was just kind of idea that, you know,
[00:22:17.660 --> 00:22:20.780]   give me something impossible to do, and I'll do it in a couple of days,
[00:22:20.780 --> 00:22:23.580]   because that's what I just spent the last four or five or six years doing.
[00:22:23.580 --> 00:22:26.060]   - That ethos definitely stuck to you.
[00:22:26.060 --> 00:22:28.860]   Having now done a number of projects with you,
[00:22:28.860 --> 00:22:30.220]   you definitely will do it the night before.
[00:22:30.220 --> 00:22:31.180]   - That's not entirely true.
[00:22:31.180 --> 00:22:33.500]   There's nothing wrong with waiting until the last minute.
[00:22:33.500 --> 00:22:35.260]   The secret is knowing when the last minute is.
[00:22:35.260 --> 00:22:37.980]   - Right, that's brilliantly put.
[00:22:37.980 --> 00:22:42.940]   - Yeah, that is a definite Charles statement that I am trying not to embrace.
[00:22:42.940 --> 00:22:43.940]   (laughing)
[00:22:43.940 --> 00:22:47.180]   - Wow, and I appreciate that, because you helped move my last minute up.
[00:22:47.180 --> 00:22:50.620]   - That's the social construct, the way you converge together,
[00:22:50.620 --> 00:22:52.300]   what the definition of last minute is.
[00:22:52.300 --> 00:22:52.540]   - Right.
[00:22:52.540 --> 00:22:54.540]   - And we figure that out all together.
[00:22:54.540 --> 00:22:58.380]   In fact, MIT, you know, I'm sure a lot of universities have this,
[00:22:58.380 --> 00:23:03.580]   but MIT has like MIT time that everyone is always agreed together
[00:23:03.580 --> 00:23:08.620]   that there is such a concept and everyone just keeps showing up like 10 to 15 to 20,
[00:23:08.620 --> 00:23:11.260]   depending on the department, late to everything.
[00:23:11.260 --> 00:23:13.420]   So there's like a weird drift that happens.
[00:23:13.420 --> 00:23:14.620]   It's kind of fascinating.
[00:23:14.620 --> 00:23:15.500]   - Yeah, we're five minutes.
[00:23:15.500 --> 00:23:16.380]   - We're five minutes.
[00:23:16.380 --> 00:23:20.220]   - In fact, the classes will say, you know, well, this is no longer true, actually.
[00:23:20.220 --> 00:23:23.980]   But it used to be a class would start at eight, but actually it started at 8.05,
[00:23:23.980 --> 00:23:25.740]   it ends at nine, actually it ends at 8.55.
[00:23:25.740 --> 00:23:28.620]   Everything's five minutes off, and nobody expects anything to start
[00:23:28.620 --> 00:23:30.540]   until five minutes after the half hour, whatever it is.
[00:23:30.540 --> 00:23:31.980]   It still exists.
[00:23:31.980 --> 00:23:32.700]   - That hurts my head.
[00:23:32.700 --> 00:23:38.460]   - Well, let's rewind the clock back to the '50s and '60s when you guys met.
[00:23:38.460 --> 00:23:39.980]   I'm just kidding.
[00:23:39.980 --> 00:23:40.700]   I don't know.
[00:23:40.700 --> 00:23:42.940]   But can you tell the story of how you met?
[00:23:42.940 --> 00:23:50.140]   So like the internet and the world kind of knows you as connected in some ways
[00:23:50.140 --> 00:23:52.460]   in terms of education, of teaching the world.
[00:23:52.460 --> 00:23:54.620]   That's like the public facing thing.
[00:23:54.620 --> 00:23:59.980]   But how did you as human beings and as collaborators meet?
[00:23:59.980 --> 00:24:01.660]   - I think there's two stories.
[00:24:01.660 --> 00:24:05.980]   One is how we met, and the other is how we got to know each other.
[00:24:05.980 --> 00:24:08.140]   I'm not gonna say fall in love.
[00:24:08.140 --> 00:24:13.580]   I'm gonna say that we came to understand that we had some common something.
[00:24:13.580 --> 00:24:17.180]   Yeah, it's funny, 'cause on the surface, I think we're different in a lot of ways.
[00:24:17.180 --> 00:24:20.180]   But there's something that just consonant.
[00:24:20.180 --> 00:24:21.580]   - Yeah, I mean, now we just leave each other's...
[00:24:21.580 --> 00:24:21.900]   There you go.
[00:24:21.900 --> 00:24:22.540]   - Afternoons.
[00:24:22.540 --> 00:24:28.060]   - So I will tell the story of how we met, and I'll let Michael tell the story of how we met.
[00:24:28.060 --> 00:24:28.460]   - Okay, all right.
[00:24:28.460 --> 00:24:29.980]   - Okay, so here's how we met.
[00:24:29.980 --> 00:24:32.780]   I was already at that point, it was AT&T Labs.
[00:24:32.780 --> 00:24:33.980]   There's a long, interesting story there.
[00:24:33.980 --> 00:24:38.620]   But anyway, I was there, and Michael was coming to interview.
[00:24:38.620 --> 00:24:43.820]   He was a professor at Duke at the time, but decided for reasons that he wanted to be in New Jersey.
[00:24:43.820 --> 00:24:48.700]   And so that would mean Bell Labs/AT&T Labs.
[00:24:48.700 --> 00:24:49.500]   And we were doing the interview.
[00:24:49.500 --> 00:24:51.500]   Interviews are very much like academic interviews.
[00:24:51.500 --> 00:24:52.380]   And so I had to be there.
[00:24:52.380 --> 00:24:56.060]   We all had to meet with him afterwards and so on, one-on-one.
[00:24:56.060 --> 00:24:58.700]   But it was obvious to me that he was gonna be hired.
[00:24:59.420 --> 00:25:00.860]   No matter what, because everyone loved him.
[00:25:00.860 --> 00:25:03.100]   They were just talking about all the great stuff he did.
[00:25:03.100 --> 00:25:06.300]   "Oh, he did this great thing, and you had just won something at AAAI, I think,
[00:25:06.300 --> 00:25:08.060]   or maybe you got 18 papers in AAAI that year."
[00:25:08.060 --> 00:25:10.940]   - "I got the best paper award at AAAI for the crossword stuff."
[00:25:10.940 --> 00:25:11.900]   - Right, exactly.
[00:25:11.900 --> 00:25:14.620]   So that had all happened, and everyone was going on and on and on about it.
[00:25:14.620 --> 00:25:16.700]   Actually, so Tinder was saying incredibly nice things about you.
[00:25:16.700 --> 00:25:17.100]   - Really?
[00:25:17.100 --> 00:25:17.420]   - Yes.
[00:25:17.420 --> 00:25:19.100]   - He can be very grumpy.
[00:25:19.100 --> 00:25:19.420]   - Yes.
[00:25:19.420 --> 00:25:20.860]   - So that's nice to hear.
[00:25:20.860 --> 00:25:22.380]   - He was grumpily saying very nice things about you.
[00:25:22.380 --> 00:25:23.100]   - Oh, that makes sense.
[00:25:23.100 --> 00:25:23.900]   - Yeah, it does make sense.
[00:25:23.900 --> 00:25:25.820]   So he was gonna come.
[00:25:25.820 --> 00:25:27.900]   So why was I meeting him?
[00:25:27.900 --> 00:25:28.860]   I had something else I had to do.
[00:25:28.860 --> 00:25:29.660]   I can't remember what it was.
[00:25:29.660 --> 00:25:31.100]   It probably involved comic books.
[00:25:31.100 --> 00:25:33.820]   - So he remembers meeting me as inconveniencing his afternoon.
[00:25:33.820 --> 00:25:34.780]   - So he came.
[00:25:34.780 --> 00:25:35.980]   So I eventually came to my office.
[00:25:35.980 --> 00:25:36.940]   I was in the middle of trying to do something.
[00:25:36.940 --> 00:25:37.740]   I can't remember what.
[00:25:37.740 --> 00:25:38.860]   And he came, and he sat down.
[00:25:38.860 --> 00:25:42.460]   And for reasons that are purely accidental, despite what Michael thinks,
[00:25:42.460 --> 00:25:46.540]   my desk at the time was set up in such a way that it had sort of an L shape,
[00:25:46.540 --> 00:25:50.220]   and the chair on the outside was always lower than the chair that I was in.
[00:25:50.220 --> 00:25:52.380]   And the kind of point was to--
[00:25:52.380 --> 00:25:55.900]   - The only reason I think that it was on purpose is because you told me it was on purpose.
[00:25:55.900 --> 00:25:56.700]   - I don't remember that.
[00:25:56.700 --> 00:25:58.620]   Anyway, the thing is that it kind of--
[00:25:58.620 --> 00:26:02.540]   - His guest chair was really low so that he could look down at everybody.
[00:26:02.540 --> 00:26:06.060]   - The idea was just to simply create a nice environment that you were asking for a mortgage,
[00:26:06.060 --> 00:26:06.940]   and I was gonna say no.
[00:26:06.940 --> 00:26:07.500]   That was the point.
[00:26:07.500 --> 00:26:08.060]   (laughing)
[00:26:08.060 --> 00:26:09.260]   It's a very simple idea here.
[00:26:09.260 --> 00:26:12.060]   Anyway, so we sat there, and we just talked for a little while.
[00:26:12.060 --> 00:26:14.860]   And I think he got the impression that I didn't like him, which wasn't true.
[00:26:14.860 --> 00:26:15.500]   - I strongly got that impression.
[00:26:15.500 --> 00:26:16.780]   - The talk was really good.
[00:26:16.780 --> 00:26:18.540]   - The talk, by the way, was terrible.
[00:26:18.540 --> 00:26:23.020]   And right after the talk, I said to my host, Michael Kearns, who ultimately was my boss--
[00:26:23.020 --> 00:26:23.740]   - I'm a huge fan.
[00:26:23.740 --> 00:26:25.580]   I'm a friend and a huge fan of Michael, yeah.
[00:26:25.580 --> 00:26:27.740]   - Yeah, he is a remarkable person.
[00:26:28.620 --> 00:26:30.860]   I, after my talk, I went into the--
[00:26:30.860 --> 00:26:31.260]   - He's good at basketball.
[00:26:31.260 --> 00:26:32.460]   - I went--
[00:26:32.460 --> 00:26:32.940]   - Racquetball?
[00:26:32.940 --> 00:26:33.580]   He's good at everything.
[00:26:33.580 --> 00:26:34.140]   - No, basketball.
[00:26:34.140 --> 00:26:35.900]   - No, but basketball, racquetball too.
[00:26:35.900 --> 00:26:36.060]   - Squash.
[00:26:36.060 --> 00:26:37.900]   - Squash, squash, squash, not racquetball.
[00:26:37.900 --> 00:26:39.180]   - Yeah, squash, which is not--
[00:26:39.180 --> 00:26:41.260]   Racquetball, yes.
[00:26:41.260 --> 00:26:42.060]   Squash, no.
[00:26:42.060 --> 00:26:43.500]   And I hope you hear that, Michael.
[00:26:43.500 --> 00:26:44.300]   (laughing)
[00:26:44.300 --> 00:26:45.740]   - Oh, Michael Kearns.
[00:26:45.740 --> 00:26:48.540]   - As a game, not his skill level, 'cause I'm pretty sure he's--
[00:26:48.540 --> 00:26:48.940]   - Both.
[00:26:48.940 --> 00:26:49.820]   (laughing)
[00:26:49.820 --> 00:26:51.420]   All right, there's some competitiveness there.
[00:26:51.420 --> 00:26:54.220]   But the point is that it was like the middle of the day.
[00:26:54.220 --> 00:26:55.580]   I had a full day of interviews.
[00:26:55.580 --> 00:26:56.380]   Like, I met with people.
[00:26:56.380 --> 00:26:58.460]   But then in the middle of the day, I gave a job talk.
[00:26:58.460 --> 00:27:01.420]   And then there was gonna be more interviews.
[00:27:01.420 --> 00:27:04.780]   But I pulled Michael aside and I said,
[00:27:04.780 --> 00:27:07.100]   "I think it's in both of our best interest
[00:27:07.100 --> 00:27:08.620]   if I just leave now."
[00:27:08.620 --> 00:27:12.380]   Because that was so bad that it'd just be embarrassing
[00:27:12.380 --> 00:27:13.900]   if I have to talk to any more people.
[00:27:13.900 --> 00:27:16.060]   Like, "You look bad for having invited me."
[00:27:16.060 --> 00:27:18.860]   Like, it's just, let's just forget this ever happened.
[00:27:18.860 --> 00:27:21.420]   So I don't think the talk went well.
[00:27:21.420 --> 00:27:23.500]   - That's one of the most Michael Lipman set of sentences
[00:27:23.500 --> 00:27:24.460]   I think I've ever heard.
[00:27:24.460 --> 00:27:25.180]   He did great.
[00:27:25.180 --> 00:27:27.020]   Or at least everyone knew he was great.
[00:27:27.020 --> 00:27:28.060]   So maybe it didn't matter.
[00:27:28.060 --> 00:27:28.620]   I was there.
[00:27:28.620 --> 00:27:29.740]   I remember the talk.
[00:27:29.740 --> 00:27:32.940]   And I remember him being very much the way I remember him now
[00:27:32.940 --> 00:27:33.820]   on any given week.
[00:27:33.820 --> 00:27:34.620]   So it was good.
[00:27:34.620 --> 00:27:36.460]   And we met and we talked about stuff.
[00:27:36.460 --> 00:27:37.660]   He thinks I didn't like him, but--
[00:27:37.660 --> 00:27:38.860]   - 'Cause he was so grumpy.
[00:27:38.860 --> 00:27:40.300]   - It must've been the chair thing.
[00:27:40.300 --> 00:27:42.380]   - The chair thing and the low voice, I think.
[00:27:42.380 --> 00:27:43.500]   Like, he obviously--
[00:27:43.500 --> 00:27:46.700]   - And that like slight, like skeptical look.
[00:27:46.700 --> 00:27:47.180]   - Yes.
[00:27:47.180 --> 00:27:47.500]   - Like, you're like--
[00:27:47.500 --> 00:27:49.900]   - I have no idea what you're talking about.
[00:27:49.900 --> 00:27:50.220]   (laughing)
[00:27:50.220 --> 00:27:51.740]   - Well, I probably didn't have any idea
[00:27:51.740 --> 00:27:52.540]   what you were talking about.
[00:27:52.540 --> 00:27:53.660]   (laughing)
[00:27:53.660 --> 00:27:54.460]   Anyway, I liked him.
[00:27:54.460 --> 00:27:55.500]   - He asked me questions.
[00:27:55.500 --> 00:27:56.300]   I answered questions.
[00:27:56.300 --> 00:27:57.260]   I felt bad about myself.
[00:27:57.260 --> 00:27:58.060]   It was a normal day.
[00:27:58.060 --> 00:27:59.060]   (laughing)
[00:27:59.060 --> 00:28:00.300]   - It was a normal day.
[00:28:00.300 --> 00:28:00.940]   And then he left.
[00:28:00.940 --> 00:28:01.980]   - And then he left.
[00:28:01.980 --> 00:28:02.940]   And that's how you met.
[00:28:02.940 --> 00:28:03.020]   - That's how we met.
[00:28:03.020 --> 00:28:03.660]   - Can we take it--
[00:28:03.660 --> 00:28:05.260]   - And then I got hired and I was in the group.
[00:28:05.260 --> 00:28:08.220]   - Can we take a slight tangent on this topic of,
[00:28:08.220 --> 00:28:12.460]   it sounds like, maybe you could speak to the bigger picture.
[00:28:12.460 --> 00:28:14.140]   It sounds like you're quite self-critical.
[00:28:14.140 --> 00:28:15.500]   - Who, Charles?
[00:28:15.500 --> 00:28:16.060]   - No, you.
[00:28:16.060 --> 00:28:16.860]   - Oh.
[00:28:16.860 --> 00:28:17.980]   I think I can do better.
[00:28:17.980 --> 00:28:18.460]   I can do better.
[00:28:18.460 --> 00:28:19.580]   I'll try me again.
[00:28:19.580 --> 00:28:20.380]   I'll do better.
[00:28:20.380 --> 00:28:21.500]   (laughing)
[00:28:21.500 --> 00:28:22.460]   I'll be self-critical.
[00:28:22.460 --> 00:28:23.340]   I won't, I won't.
[00:28:23.340 --> 00:28:26.220]   - Yeah, that was like a three out of 10 response.
[00:28:26.220 --> 00:28:27.500]   (laughing)
[00:28:27.500 --> 00:28:29.580]   So let's try to work it up to five and six.
[00:28:29.580 --> 00:28:35.100]   You know, I remember Marvin Minsky said on a video interview
[00:28:35.100 --> 00:28:38.700]   something that the key to success in academic research
[00:28:38.700 --> 00:28:40.060]   is to hate everything you do.
[00:28:40.060 --> 00:28:42.060]   - Oh.
[00:28:42.060 --> 00:28:44.060]   - For some reason--
[00:28:44.060 --> 00:28:46.460]   - I think I followed that because I hate everything he's done.
[00:28:46.460 --> 00:28:48.220]   (laughing)
[00:28:48.220 --> 00:28:49.580]   - That's a good line.
[00:28:49.580 --> 00:28:50.860]   That's a six.
[00:28:50.860 --> 00:28:52.300]   (laughing)
[00:28:52.300 --> 00:28:53.260]   Maybe that's a keeper.
[00:28:53.260 --> 00:28:57.580]   But do you find that resonates with you at all
[00:28:57.580 --> 00:28:59.500]   in how you think about talks and so on?
[00:28:59.500 --> 00:29:00.780]   - I would say a different line.
[00:29:00.780 --> 00:29:01.580]   It's not that.
[00:29:01.580 --> 00:29:02.140]   - No, not really.
[00:29:02.140 --> 00:29:04.300]   - That's such an MIT view of the world though.
[00:29:04.300 --> 00:29:08.300]   So I remember talking about this when, as a student, you know,
[00:29:08.300 --> 00:29:09.260]   you were basically told,
[00:29:09.260 --> 00:29:12.060]   I will clean it up for the purposes of the podcast.
[00:29:12.060 --> 00:29:16.060]   My work is crap, my work is crap, my work is crap, my work is crap.
[00:29:16.060 --> 00:29:17.580]   Then you like go to a conference or something.
[00:29:17.580 --> 00:29:18.860]   You're like, everybody else's work is crap.
[00:29:18.860 --> 00:29:19.900]   Everybody else's work is crap.
[00:29:19.900 --> 00:29:21.180]   And you feel better and better about it.
[00:29:21.180 --> 00:29:21.680]   - Yeah.
[00:29:22.160 --> 00:29:23.200]   - Relatively speaking.
[00:29:23.200 --> 00:29:24.400]   And then you sort of keep working on it.
[00:29:24.400 --> 00:29:26.640]   I don't hate my work.
[00:29:26.640 --> 00:29:27.520]   - That resonates with me.
[00:29:27.520 --> 00:29:32.480]   - Yes. I've never hated my work, but I have been dissatisfied with it.
[00:29:32.480 --> 00:29:35.920]   And I think being dissatisfied,
[00:29:35.920 --> 00:29:38.480]   being okay with the fact that you've taken a positive step,
[00:29:38.480 --> 00:29:39.520]   the derivative's positive.
[00:29:39.520 --> 00:29:42.240]   Maybe even the second derivative's positive.
[00:29:42.240 --> 00:29:45.040]   That's important because that's a part of the hope, right?
[00:29:45.040 --> 00:29:47.360]   But you have to, but I haven't gotten there yet.
[00:29:47.360 --> 00:29:49.840]   If that's not there, that I haven't gotten there yet,
[00:29:49.840 --> 00:29:53.280]   then it's hard to move forward, I think.
[00:29:53.280 --> 00:29:56.160]   So I buy that, which is a little different from hating everything that you do.
[00:29:56.160 --> 00:29:56.720]   - Yeah.
[00:29:56.720 --> 00:30:01.120]   I mean, there's things that I've done that I like better than I like myself.
[00:30:01.120 --> 00:30:03.920]   So it's separating me from the work, essentially.
[00:30:03.920 --> 00:30:06.640]   So I think I am very critical of myself,
[00:30:06.640 --> 00:30:08.560]   but sometimes the work I'm really excited about.
[00:30:08.560 --> 00:30:09.840]   And sometimes I think it's kind of good.
[00:30:09.840 --> 00:30:11.120]   - Does that happen right away?
[00:30:11.120 --> 00:30:16.320]   So I found the work that I've liked, that I've done, most of it,
[00:30:16.320 --> 00:30:20.400]   I liked it in retrospect more when I was far away from it in time.
[00:30:20.400 --> 00:30:21.040]   - Oh, interesting.
[00:30:21.040 --> 00:30:24.000]   I have to be fairly excited about it to get done.
[00:30:24.000 --> 00:30:26.800]   - No, excited at the time, but then happy with the result.
[00:30:26.800 --> 00:30:29.520]   But years later, or even I might go back, "You know what?
[00:30:29.520 --> 00:30:31.760]   That actually turned out to matter."
[00:30:31.760 --> 00:30:34.080]   Or, "Oh gosh, it turns out I've been thinking about that.
[00:30:34.080 --> 00:30:37.200]   It's actually influenced all the work that I've done since without realizing it."
[00:30:37.200 --> 00:30:38.800]   - Boy, that guy was smart.
[00:30:38.800 --> 00:30:40.720]   - Yeah, that guy had a future.
[00:30:40.720 --> 00:30:41.360]   - Yeah.
[00:30:41.360 --> 00:30:41.360]   - Yeah.
[00:30:41.360 --> 00:30:44.000]   - He's going places.
[00:30:44.720 --> 00:30:46.960]   - I think there's something to it.
[00:30:46.960 --> 00:30:49.360]   I think there's something to the idea you've got to hate what you do,
[00:30:49.360 --> 00:30:50.240]   but it's not quite hate.
[00:30:50.240 --> 00:30:52.400]   It's just being unsatisfied.
[00:30:52.400 --> 00:30:54.160]   And different people motivate themselves differently.
[00:30:54.160 --> 00:30:56.480]   I don't happen to motivate myself with self-loathing.
[00:30:56.480 --> 00:30:58.240]   I happen to motivate myself with something else.
[00:30:58.240 --> 00:31:02.000]   - So you're able to sit back and be proud of,
[00:31:02.000 --> 00:31:04.400]   in retrospect, of the work you've done.
[00:31:04.400 --> 00:31:07.200]   - Well, and it's easier when you can connect it with other people
[00:31:07.200 --> 00:31:08.560]   because then you can be proud of them.
[00:31:08.560 --> 00:31:09.840]   - Proud of the people, yeah.
[00:31:09.840 --> 00:31:10.720]   - And then the question is--
[00:31:10.720 --> 00:31:12.960]   - And then you can still safely hate yourself privately.
[00:31:12.960 --> 00:31:13.600]   - Yeah, that's right.
[00:31:13.600 --> 00:31:15.040]   It's win-win, Michael.
[00:31:15.040 --> 00:31:16.880]   Or at least win-lose, which is what you're looking for.
[00:31:16.880 --> 00:31:19.280]   - Oh, wow.
[00:31:19.280 --> 00:31:21.200]   There's so many brilliant minds in this.
[00:31:21.200 --> 00:31:22.720]   - There's levels.
[00:31:22.720 --> 00:31:25.520]   - So how did you actually meet me?
[00:31:25.520 --> 00:31:26.240]   - Yeah, Michael.
[00:31:26.240 --> 00:31:28.400]   - So the way I think about it is,
[00:31:28.400 --> 00:31:31.840]   because we didn't do much research together at AT&T.
[00:31:31.840 --> 00:31:32.320]   - No.
[00:31:32.320 --> 00:31:34.480]   - But then we all got laid off.
[00:31:34.480 --> 00:31:35.600]   So that was--
[00:31:35.600 --> 00:31:37.760]   - By the way, sorry to interrupt,
[00:31:37.760 --> 00:31:41.360]   but that was one of the most magical places, historically speaking.
[00:31:42.160 --> 00:31:43.600]   - They did not appreciate what they had.
[00:31:43.600 --> 00:31:46.080]   - And how do we--
[00:31:46.080 --> 00:31:49.520]   I feel like there's a profound lesson in there, too.
[00:31:49.520 --> 00:31:51.200]   How do we get it--
[00:31:51.200 --> 00:31:52.960]   like, why was it so magical?
[00:31:52.960 --> 00:31:54.720]   Is it just a coincidence of history?
[00:31:54.720 --> 00:31:56.000]   Or is there something special about--
[00:31:56.000 --> 00:31:57.600]   - There were some really good managers
[00:31:57.600 --> 00:32:00.400]   and people who really believed in machine learning
[00:32:00.400 --> 00:32:02.320]   as this is going to be important.
[00:32:02.320 --> 00:32:05.440]   Let's get the people who are thinking about this
[00:32:05.440 --> 00:32:07.920]   in creative and insightful ways
[00:32:07.920 --> 00:32:09.840]   and put them in one place and stir.
[00:32:09.840 --> 00:32:11.280]   - Yeah, but even beyond that, right?
[00:32:11.280 --> 00:32:15.440]   It was Bell Labs at its heyday.
[00:32:15.440 --> 00:32:16.800]   And even when we were there,
[00:32:16.800 --> 00:32:17.760]   which I think was past its heyday--
[00:32:17.760 --> 00:32:19.600]   - And to be clear, he's gotten to be at Bell Labs.
[00:32:19.600 --> 00:32:20.880]   I never got to be at Bell Labs.
[00:32:20.880 --> 00:32:22.000]   I joined after that.
[00:32:22.000 --> 00:32:24.400]   - Yeah, I should have been 91 as a grad student.
[00:32:24.400 --> 00:32:28.160]   So I was there for a long time, every summer, except for--
[00:32:28.160 --> 00:32:29.440]   - So twice I worked for companies
[00:32:29.440 --> 00:32:31.040]   that had just stopped being Bell Labs.
[00:32:31.040 --> 00:32:31.600]   - Right, Bell Lab.
[00:32:31.600 --> 00:32:33.360]   - Bell Corp and then AT&T Labs.
[00:32:33.360 --> 00:32:37.040]   - So Bell Labs was several locations for the research?
[00:32:37.040 --> 00:32:37.600]   Or is it one--
[00:32:37.600 --> 00:32:38.640]   - Definitely several.
[00:32:38.640 --> 00:32:40.480]   - Or were there jerseys involved somehow?
[00:32:40.480 --> 00:32:41.680]   - They're all in Jersey.
[00:32:41.680 --> 00:32:42.560]   - Yeah, they're all over the place.
[00:32:42.560 --> 00:32:43.840]   - But they were in a couple places in Jersey.
[00:32:43.840 --> 00:32:46.400]   - Murray Hill was the Bell Labs place.
[00:32:46.400 --> 00:32:49.680]   - So you had an office at Murray Hill
[00:32:49.680 --> 00:32:50.720]   at one point in your career.
[00:32:50.720 --> 00:32:53.280]   - Yeah, and I played ultimate frisbee
[00:32:53.280 --> 00:32:56.320]   on the cricket pitch at Bell Labs at Murray Hill.
[00:32:56.320 --> 00:32:57.520]   And then it became AT&T Labs
[00:32:57.520 --> 00:33:00.000]   when it split off with Luce during what we called
[00:33:00.000 --> 00:33:00.720]   trivestiture, as opposed to divestiture.
[00:33:00.720 --> 00:33:03.200]   - Are you better than Michael Korns at ultimate frisbee?
[00:33:03.200 --> 00:33:03.680]   - Yeah.
[00:33:03.680 --> 00:33:04.160]   - Oh, yeah.
[00:33:04.160 --> 00:33:04.720]   - Okay.
[00:33:04.720 --> 00:33:06.480]   - But I think that one's not boasting.
[00:33:06.480 --> 00:33:08.400]   I think Charles plays a lot of ultimate.
[00:33:08.400 --> 00:33:09.760]   And I don't think Michael does.
[00:33:09.760 --> 00:33:11.920]   - No, I was, yes, but that wasn't the point.
[00:33:11.920 --> 00:33:12.480]   The point is, yes.
[00:33:12.480 --> 00:33:12.720]   - Sorry.
[00:33:12.720 --> 00:33:13.360]   - I'm finally better.
[00:33:13.360 --> 00:33:14.480]   - Oh, yes, yes, sorry, sorry.
[00:33:14.480 --> 00:33:17.280]   Okay, I have played on a championship-winning
[00:33:17.280 --> 00:33:19.280]   ultimate frisbee team or whatever,
[00:33:19.280 --> 00:33:20.640]   ultimate team with Charles.
[00:33:20.640 --> 00:33:22.480]   So I know how good he is.
[00:33:22.480 --> 00:33:22.960]   He's really good.
[00:33:22.960 --> 00:33:24.480]   - How good I was anyway when I was younger.
[00:33:24.480 --> 00:33:25.200]   But the thing is--
[00:33:25.200 --> 00:33:26.640]   - I know how young he was when he was younger.
[00:33:26.640 --> 00:33:27.200]   - That's true.
[00:33:27.200 --> 00:33:28.480]   - So much younger than now.
[00:33:28.480 --> 00:33:29.440]   - He's older now.
[00:33:29.440 --> 00:33:30.400]   - Yeah, I'm older.
[00:33:30.400 --> 00:33:32.880]   Michael was a much better basketball player than I was.
[00:33:32.880 --> 00:33:33.520]   - Michael Korns.
[00:33:33.520 --> 00:33:35.120]   - Yes, no, not Michael.
[00:33:35.120 --> 00:33:36.080]   (laughing)
[00:33:36.080 --> 00:33:36.960]   Let's be very clear about that.
[00:33:36.960 --> 00:33:38.640]   - Let's be clear, I've not played basketball with you.
[00:33:38.640 --> 00:33:40.320]   So you don't know how terrible I am.
[00:33:40.320 --> 00:33:41.840]   But you have a probably pretty good guess.
[00:33:41.840 --> 00:33:43.600]   - That you're not as good as Michael Korns.
[00:33:43.600 --> 00:33:45.600]   - He's tall and athletic.
[00:33:45.600 --> 00:33:46.480]   - And he cared about it.
[00:33:46.480 --> 00:33:47.680]   He's very athletic, he's very good.
[00:33:47.680 --> 00:33:48.560]   - And probably competitive.
[00:33:48.560 --> 00:33:50.080]   - I love hanging out with Michael.
[00:33:50.080 --> 00:33:51.520]   Anyway, but we were talking about something else,
[00:33:51.520 --> 00:33:52.800]   although I no longer remember what it was.
[00:33:52.800 --> 00:33:53.440]   What were we talking about?
[00:33:53.440 --> 00:33:54.960]   - How you met Bell Labs.
[00:33:54.960 --> 00:33:55.680]   - But also Labs.
[00:33:55.680 --> 00:33:59.440]   So this was kind of cool about what was magical about it.
[00:33:59.440 --> 00:34:01.200]   The first thing you have to know
[00:34:01.200 --> 00:34:03.360]   is that Bell Labs was an arm of the government, right?
[00:34:03.360 --> 00:34:05.120]   Because AT&T was an arm of the government.
[00:34:05.120 --> 00:34:06.160]   It was a monopoly.
[00:34:06.720 --> 00:34:10.640]   And every month you paid a little thing on your phone bill,
[00:34:10.640 --> 00:34:12.880]   which turned out was a tax for all the research
[00:34:12.880 --> 00:34:14.160]   that Bell Labs was doing.
[00:34:14.160 --> 00:34:16.560]   And they invented transistors and the laser
[00:34:16.560 --> 00:34:17.440]   and whatever else that they did.
[00:34:17.440 --> 00:34:20.320]   - The big bang or whatever, the cosmic background radiation.
[00:34:20.320 --> 00:34:21.200]   - Yeah, they did all that stuff.
[00:34:21.200 --> 00:34:23.200]   They had some amazing stuff with directional microphones,
[00:34:23.200 --> 00:34:23.520]   by the way.
[00:34:23.520 --> 00:34:27.200]   I got to go in this room where they had all these panels
[00:34:27.200 --> 00:34:28.720]   and everything, and we would talk.
[00:34:28.720 --> 00:34:30.720]   And one another, and he'd move some panels around.
[00:34:30.720 --> 00:34:33.520]   And then he'd have me step two steps to the left,
[00:34:33.520 --> 00:34:34.960]   and I couldn't hear a thing he was saying
[00:34:34.960 --> 00:34:37.120]   because nothing was bouncing off the walls.
[00:34:37.120 --> 00:34:38.320]   And then he would shut it all down
[00:34:38.320 --> 00:34:39.760]   and you could hear your heartbeat,
[00:34:39.760 --> 00:34:40.080]   - Yeah.
[00:34:40.080 --> 00:34:43.120]   - which is deeply disturbing to hear your heartbeat.
[00:34:43.120 --> 00:34:43.920]   You can feel it.
[00:34:43.920 --> 00:34:44.800]   I mean, you can feel it now.
[00:34:44.800 --> 00:34:46.400]   There's so much, all this sort of noise around.
[00:34:46.400 --> 00:34:48.320]   Anyway, Bell Labs was about pure research.
[00:34:48.320 --> 00:34:50.320]   It was a university, in some sense,
[00:34:50.320 --> 00:34:52.960]   the purest sense of a university, but without students.
[00:34:52.960 --> 00:34:56.240]   So it was all the faculty working with one another
[00:34:56.240 --> 00:34:57.680]   and students would come in to learn.
[00:34:57.680 --> 00:34:58.880]   They would come in for three or four months,
[00:34:58.880 --> 00:35:00.640]   you know, during the summer and they would go away.
[00:35:00.640 --> 00:35:02.720]   But it was just this kind of wonderful experience.
[00:35:02.720 --> 00:35:03.760]   I could walk out my door.
[00:35:04.480 --> 00:35:06.400]   In fact, I would often have to walk out my door
[00:35:06.400 --> 00:35:08.240]   and deal with Rich Sutton and Michael Kearns
[00:35:08.240 --> 00:35:10.800]   yelling at each other about whatever it is
[00:35:10.800 --> 00:35:11.600]   they were yelling about,
[00:35:11.600 --> 00:35:14.480]   the proper way to prove something or another.
[00:35:14.480 --> 00:35:15.440]   And I could just do that.
[00:35:15.440 --> 00:35:17.840]   And Dave McAllister and Peter Stone
[00:35:17.840 --> 00:35:20.480]   and all of these other people, including,
[00:35:20.480 --> 00:35:22.560]   it's a Tinder and then eventually Michael.
[00:35:22.560 --> 00:35:25.120]   And it was just a place where you could think, thoughts.
[00:35:25.120 --> 00:35:29.200]   And it was okay because so long as once every 25 years or so,
[00:35:29.200 --> 00:35:31.680]   somebody invented a transistor, it paid for everything else.
[00:35:31.680 --> 00:35:33.280]   You could afford to take the risk.
[00:35:34.080 --> 00:35:35.440]   And then when that all went away,
[00:35:35.440 --> 00:35:39.280]   it became harder and harder and harder to justify it
[00:35:39.280 --> 00:35:41.520]   as far as the folks who were very far away were concerned.
[00:35:41.520 --> 00:35:43.440]   And there was such a fast turnaround
[00:35:43.440 --> 00:35:46.240]   among middle management on the AT&T side
[00:35:46.240 --> 00:35:48.240]   that you never had a chance to really build a relationship.
[00:35:48.240 --> 00:35:49.680]   At least people like us didn't have a chance
[00:35:49.680 --> 00:35:51.360]   to build a relationship.
[00:35:51.360 --> 00:35:54.560]   So when the diaspora happened, it was amazing, right?
[00:35:54.560 --> 00:35:55.040]   - Yeah.
[00:35:55.040 --> 00:35:57.520]   - Everybody left and I think everybody ended up
[00:35:57.520 --> 00:35:59.520]   at a great place and made a huge,
[00:35:59.520 --> 00:36:01.600]   made a, continued to do really good work
[00:36:01.600 --> 00:36:02.560]   with machine learning.
[00:36:02.560 --> 00:36:03.600]   But it was a wonderful place.
[00:36:03.600 --> 00:36:04.880]   And people will ask me,
[00:36:04.880 --> 00:36:06.960]   what's the best job you've ever had?
[00:36:06.960 --> 00:36:09.920]   And as a professor, anyway, the answer that I would give is,
[00:36:09.920 --> 00:36:16.000]   well, probably Bell Labs in some very real sense.
[00:36:16.000 --> 00:36:17.520]   And I will never have a job like that again
[00:36:17.520 --> 00:36:19.200]   because Bell Labs doesn't exist anymore.
[00:36:19.200 --> 00:36:22.160]   And Microsoft research is great and Google does good stuff
[00:36:22.160 --> 00:36:24.080]   and you can pick IBM, you can tell if you want to,
[00:36:24.080 --> 00:36:25.760]   but Bell Labs was magical.
[00:36:25.760 --> 00:36:27.840]   It was around for, it was an important time
[00:36:27.840 --> 00:36:30.400]   and it represents a high watermark
[00:36:30.400 --> 00:36:32.400]   in basic research in the US.
[00:36:32.400 --> 00:36:33.680]   - Is there something you could say
[00:36:33.680 --> 00:36:36.560]   about the physical proximity and the chance collisions?
[00:36:36.560 --> 00:36:39.280]   Like we live in this time of the pandemic
[00:36:39.280 --> 00:36:43.760]   where everyone is maybe trying to see the silver lining
[00:36:43.760 --> 00:36:46.080]   and accepting the remote nature of things.
[00:36:46.080 --> 00:36:50.080]   Is there, one of the things that people like faculty
[00:36:50.080 --> 00:36:55.200]   that I talk to miss is the procrastination.
[00:36:55.200 --> 00:36:58.640]   Like the chance to make,
[00:36:58.640 --> 00:37:00.880]   everything is about meetings that are supposed to be,
[00:37:00.880 --> 00:37:03.920]   there's not a chance to just talk about comic book
[00:37:03.920 --> 00:37:05.760]   or whatever, like go into discussion
[00:37:05.760 --> 00:37:07.040]   that's totally pointless.
[00:37:07.040 --> 00:37:09.520]   - So it's funny you say this because that's how we met.
[00:37:09.520 --> 00:37:10.880]   Met, it was exactly that.
[00:37:10.880 --> 00:37:12.640]   So I'll let Michael say that, but I'll just add one thing,
[00:37:12.640 --> 00:37:15.520]   which is just that research is a social process
[00:37:15.520 --> 00:37:20.000]   and it helps to have random social interactions,
[00:37:20.000 --> 00:37:21.360]   even if they don't feel social at the time,
[00:37:21.360 --> 00:37:22.560]   that's how you get things done.
[00:37:22.560 --> 00:37:25.760]   One of the great things about the AI lab when I was there,
[00:37:25.760 --> 00:37:27.680]   I don't quite know what it looks like now
[00:37:27.680 --> 00:37:28.640]   once they move buildings,
[00:37:28.640 --> 00:37:30.720]   but we had entire walls that were whiteboards
[00:37:30.720 --> 00:37:31.680]   and people would just get up there
[00:37:31.680 --> 00:37:33.440]   and they would just write and people would walk up
[00:37:33.440 --> 00:37:34.320]   and you'd have arguments
[00:37:34.320 --> 00:37:36.000]   and you'd explain things to one another
[00:37:36.000 --> 00:37:39.440]   and you got so much out of the freedom to do that.
[00:37:39.440 --> 00:37:42.240]   You had to be okay with people challenging
[00:37:42.240 --> 00:37:44.240]   every fricking word you said,
[00:37:44.240 --> 00:37:47.120]   which I would sometimes find deeply irritating,
[00:37:47.120 --> 00:37:49.440]   but most of the time it was quite useful.
[00:37:49.440 --> 00:37:51.920]   But the sort of pointlessness and the interaction
[00:37:51.920 --> 00:37:54.400]   was in some sense the point, at least for me.
[00:37:54.400 --> 00:37:56.960]   - Yeah, I mean, I think offline yesterday
[00:37:56.960 --> 00:37:59.280]   I mentioned Josh Tenenbaum and he's very much,
[00:37:59.840 --> 00:38:06.080]   he's such an inspiration in the child-like way
[00:38:06.080 --> 00:38:07.680]   that he pulls you in on any topic.
[00:38:07.680 --> 00:38:09.360]   It doesn't even have to be about machine learning
[00:38:09.360 --> 00:38:11.520]   or the brain.
[00:38:11.520 --> 00:38:14.960]   He'll just pull you into a closest writable surface,
[00:38:14.960 --> 00:38:18.960]   which is still, you can find whiteboards at MIT everywhere.
[00:38:18.960 --> 00:38:23.760]   And just basically cancel all meetings
[00:38:23.760 --> 00:38:26.800]   and talk for a couple hours about some aimless thing.
[00:38:26.800 --> 00:38:28.640]   And it feels like the whole world,
[00:38:28.640 --> 00:38:30.640]   the time-space continuum kind of warps
[00:38:30.640 --> 00:38:32.560]   and that becomes the most important thing.
[00:38:32.560 --> 00:38:33.920]   And then it's just-- - It's so true.
[00:38:33.920 --> 00:38:38.640]   - It's definitely something worth missing in this world
[00:38:38.640 --> 00:38:39.920]   where everything's remote.
[00:38:39.920 --> 00:38:42.640]   There's some magic to the physical presence.
[00:38:42.640 --> 00:38:45.200]   - Whenever I wonder myself whether MIT really is as great
[00:38:45.200 --> 00:38:47.680]   as I remember it, I just go talk to Josh.
[00:38:47.680 --> 00:38:49.600]   - Yeah, you know, that's funny.
[00:38:49.600 --> 00:38:51.120]   There's a few people in this world
[00:38:51.120 --> 00:38:54.160]   that carry the best
[00:38:54.160 --> 00:38:56.080]   of what particular institutions stand for, right?
[00:38:56.080 --> 00:38:57.760]   And there's-- - There's Josh.
[00:38:57.760 --> 00:39:00.560]   I mean, I don't, my guess is he's unaware of this.
[00:39:00.560 --> 00:39:01.440]   - That's the point.
[00:39:01.440 --> 00:39:02.080]   - Yeah.
[00:39:02.080 --> 00:39:05.440]   - That the masters are not aware of their mastery.
[00:39:05.440 --> 00:39:07.920]   So-- - How did I meet?
[00:39:07.920 --> 00:39:09.760]   (laughing)
[00:39:09.760 --> 00:39:12.080]   - Yes, but first a tangent, no.
[00:39:12.080 --> 00:39:13.600]   (laughing)
[00:39:13.600 --> 00:39:14.480]   How did you meet me?
[00:39:14.480 --> 00:39:16.000]   - So I'm not sure what you were thinking,
[00:39:16.000 --> 00:39:19.040]   but when it started to dawn on me
[00:39:19.040 --> 00:39:21.440]   that maybe we had a longer-term bond
[00:39:21.440 --> 00:39:23.120]   was after we all got laid off.
[00:39:23.120 --> 00:39:26.480]   And you had decided at that point
[00:39:26.480 --> 00:39:28.320]   that we were still paid.
[00:39:28.320 --> 00:39:30.880]   We were given an opportunity to do a job search
[00:39:30.880 --> 00:39:32.240]   and kind of make a transition,
[00:39:32.240 --> 00:39:34.240]   but it was clear that we were done.
[00:39:34.240 --> 00:39:38.240]   And I would go to my office to work,
[00:39:38.240 --> 00:39:40.480]   and you would go to my office to keep me from working.
[00:39:40.480 --> 00:39:40.960]   - Yeah.
[00:39:40.960 --> 00:39:43.360]   - That was my recollection of it.
[00:39:43.360 --> 00:39:45.120]   You had decided that there was really no point
[00:39:45.120 --> 00:39:46.400]   in working for the company
[00:39:46.400 --> 00:39:49.360]   'cause our relationship with the company was done.
[00:39:49.360 --> 00:39:51.120]   - Yeah, but remember, I felt that way beforehand.
[00:39:51.120 --> 00:39:52.080]   It wasn't about the company.
[00:39:52.080 --> 00:39:53.280]   It was about the set of people there
[00:39:53.280 --> 00:39:54.080]   doing really cool things,
[00:39:54.080 --> 00:39:55.600]   and it had always been that way.
[00:39:55.600 --> 00:39:57.120]   But we were working on something together.
[00:39:57.120 --> 00:39:58.560]   - Oh, yeah, yeah, yeah, that's right.
[00:39:58.560 --> 00:40:00.160]   So at the very end, we all got laid off,
[00:40:00.160 --> 00:40:02.080]   but then our boss came to,
[00:40:02.080 --> 00:40:04.000]   our boss's boss came to us
[00:40:04.000 --> 00:40:05.520]   'cause our boss was Michael Kearns,
[00:40:05.520 --> 00:40:07.760]   and he had jumped ship brilliantly,
[00:40:07.760 --> 00:40:08.880]   like perfect timing,
[00:40:08.880 --> 00:40:11.920]   like things, like right before the ship was about to sink,
[00:40:11.920 --> 00:40:13.600]   he was like, "Gotta go,"
[00:40:13.600 --> 00:40:17.840]   and landed perfectly because Michael Kearns.
[00:40:17.840 --> 00:40:18.800]   - 'Cause Michael Kearns.
[00:40:18.800 --> 00:40:22.080]   - And leaving the rest of us to go like,
[00:40:22.080 --> 00:40:23.440]   "This is fine."
[00:40:23.440 --> 00:40:25.600]   And then it was clear that it wasn't fine,
[00:40:25.600 --> 00:40:27.280]   and we were all toast.
[00:40:27.280 --> 00:40:29.040]   So we had this sort of long period of time,
[00:40:29.040 --> 00:40:30.320]   but then our boss figured out,
[00:40:30.320 --> 00:40:32.560]   "Okay, wait, maybe we can save a couple of these people
[00:40:32.560 --> 00:40:36.000]   if we can have them do something really useful."
[00:40:36.000 --> 00:40:39.280]   And the useful thing was
[00:40:39.280 --> 00:40:42.080]   we were gonna make basically an automated assistant
[00:40:42.080 --> 00:40:43.760]   that could help you with your calendar.
[00:40:43.760 --> 00:40:45.520]   You could like tell it things,
[00:40:45.520 --> 00:40:47.600]   and it would respond appropriately.
[00:40:47.600 --> 00:40:48.720]   It would just kind of integrate
[00:40:48.720 --> 00:40:52.720]   across all sorts of your personal information.
[00:40:53.440 --> 00:40:56.000]   And so me and Charles and Peter Stone
[00:40:56.000 --> 00:40:58.720]   were set up as the crack team
[00:40:58.720 --> 00:41:00.080]   to actually solve this problem.
[00:41:00.080 --> 00:41:02.640]   Other people maybe were too theoretical,
[00:41:02.640 --> 00:41:03.200]   they thought,
[00:41:03.200 --> 00:41:05.520]   but we could actually get something done.
[00:41:05.520 --> 00:41:07.120]   So we sat down to get something done,
[00:41:07.120 --> 00:41:08.160]   and there wasn't time,
[00:41:08.160 --> 00:41:09.920]   and it wouldn't have saved us anyway.
[00:41:09.920 --> 00:41:11.920]   And so it all kind of went downhill.
[00:41:11.920 --> 00:41:15.280]   But the interesting, I think, coda to that
[00:41:15.280 --> 00:41:18.240]   is that our boss's boss is a guy named Ron Brockman.
[00:41:18.240 --> 00:41:21.920]   And when he left AT&T,
[00:41:21.920 --> 00:41:23.040]   'cause we were all laid off,
[00:41:23.040 --> 00:41:24.640]   he went to DARPA,
[00:41:24.640 --> 00:41:28.800]   started up a program there that became CALO,
[00:41:28.800 --> 00:41:31.920]   which is the program from which Siri sprung,
[00:41:31.920 --> 00:41:33.840]   which is a digital assistant
[00:41:33.840 --> 00:41:35.040]   that helps you with your calendar
[00:41:35.040 --> 00:41:36.160]   and a bunch of other things.
[00:41:36.160 --> 00:41:40.720]   It really, in some ways, got its start
[00:41:40.720 --> 00:41:42.320]   with me and Charles and Peter
[00:41:42.320 --> 00:41:45.440]   trying to implement this vision that Ron Brockman had
[00:41:45.440 --> 00:41:47.600]   that he ultimately got implemented
[00:41:47.600 --> 00:41:49.280]   through his role at DARPA.
[00:41:49.280 --> 00:41:51.200]   So when I'm trying to feel less bad
[00:41:51.200 --> 00:41:52.240]   about having been laid off
[00:41:52.240 --> 00:41:54.800]   from what is possibly the greatest job of all time,
[00:41:54.800 --> 00:41:59.040]   I think about, well, we kind of helped birth Siri.
[00:41:59.040 --> 00:42:00.560]   That's something.
[00:42:00.560 --> 00:42:02.960]   - And then he did other things too.
[00:42:02.960 --> 00:42:06.480]   But we got to spend a lot of time in his office
[00:42:06.480 --> 00:42:07.600]   and talk about-
[00:42:07.600 --> 00:42:09.360]   - We got to spend a lot of time in my office.
[00:42:09.360 --> 00:42:10.880]   Yeah, yeah, yeah.
[00:42:10.880 --> 00:42:13.360]   And so then we went on our merry way.
[00:42:13.360 --> 00:42:15.200]   Everyone went to different places.
[00:42:15.200 --> 00:42:16.400]   Charles landed at Georgia Tech,
[00:42:16.400 --> 00:42:20.080]   which was what he always dreamed he would do.
[00:42:20.080 --> 00:42:22.160]   And so that worked out well.
[00:42:22.160 --> 00:42:25.120]   I came up with a saying at the time,
[00:42:25.120 --> 00:42:26.720]   which is luck favors the Charles.
[00:42:26.720 --> 00:42:29.680]   It's kind of like luck favors the prepared.
[00:42:29.680 --> 00:42:32.720]   But Charles, he wished something
[00:42:32.720 --> 00:42:34.320]   and then it would basically happen
[00:42:34.320 --> 00:42:35.280]   just the way he wanted.
[00:42:35.280 --> 00:42:38.080]   It was inspirational to see things go that way.
[00:42:38.080 --> 00:42:38.960]   - Things worked out.
[00:42:38.960 --> 00:42:40.080]   - And we stayed in touch.
[00:42:40.080 --> 00:42:43.040]   And then I think it really helped
[00:42:43.040 --> 00:42:45.920]   when you were working on,
[00:42:45.920 --> 00:42:46.960]   I mean, you'd kept me in the loop
[00:42:46.960 --> 00:42:48.080]   for things like threads
[00:42:48.080 --> 00:42:49.600]   and the work that you were doing at Georgia Tech.
[00:42:49.600 --> 00:42:50.800]   But then when they were starting
[00:42:50.800 --> 00:42:52.080]   their online master's program,
[00:42:52.080 --> 00:42:53.840]   he knew that I was really excited
[00:42:53.840 --> 00:42:55.920]   about MOOCs and online teaching.
[00:42:55.920 --> 00:42:57.760]   And he's like, "I have a plan."
[00:42:57.760 --> 00:42:58.720]   And I'm like, "Tell me your plan."
[00:42:58.720 --> 00:43:00.480]   He's like, "I can't tell you the plan yet."
[00:43:00.480 --> 00:43:02.800]   'Cause they were deep in negotiations
[00:43:02.800 --> 00:43:05.440]   between Georgia Tech and Udacity to make this happen.
[00:43:05.440 --> 00:43:07.200]   And they didn't want it to leak.
[00:43:07.200 --> 00:43:09.040]   So Charles would kept teasing me about it,
[00:43:09.040 --> 00:43:10.560]   but wouldn't tell me what was actually going on.
[00:43:10.560 --> 00:43:12.000]   And eventually it was announced.
[00:43:12.000 --> 00:43:14.000]   And he said, "I would like you to teach
[00:43:14.000 --> 00:43:15.440]   "the machine learning course with me."
[00:43:15.440 --> 00:43:17.200]   I'm like, "That can't possibly work."
[00:43:17.200 --> 00:43:19.200]   But it was a great idea.
[00:43:19.200 --> 00:43:20.720]   And it was super fun.
[00:43:20.720 --> 00:43:22.000]   It was a lot of work to put together,
[00:43:22.000 --> 00:43:23.520]   but it was really great.
[00:43:23.520 --> 00:43:25.840]   - Was that the first time you thought about,
[00:43:25.840 --> 00:43:27.520]   first of all, was it the first time
[00:43:27.520 --> 00:43:29.200]   you got seriously into teaching?
[00:43:29.200 --> 00:43:31.680]   - I mean, I was a professor.
[00:43:31.680 --> 00:43:32.240]   - I'm trying to get the timing right.
[00:43:32.240 --> 00:43:35.360]   Also, this was already after you jumped to,
[00:43:35.360 --> 00:43:38.480]   so there's a little bit of jumping around in time.
[00:43:38.480 --> 00:43:39.200]   - Yeah, sorry about that.
[00:43:39.200 --> 00:43:40.240]   - There's a pretty big jump in time.
[00:43:40.240 --> 00:43:41.840]   - So the MOOCs thing-
[00:43:41.840 --> 00:43:44.080]   - So Charles got to Georgia Tech and he,
[00:43:44.080 --> 00:43:45.680]   I mean, maybe Charles, maybe this is a Charles story.
[00:43:45.680 --> 00:43:46.720]   - I think this was like 2002.
[00:43:46.720 --> 00:43:48.640]   - He got to Georgia Tech in 2002.
[00:43:48.640 --> 00:43:49.200]   - Yeah.
[00:43:49.200 --> 00:43:52.640]   - And worked on things like revamping the curriculum,
[00:43:52.640 --> 00:43:53.600]   the undergraduate curriculum,
[00:43:53.600 --> 00:43:57.600]   so that it had some kind of semblance of modular structure,
[00:43:57.600 --> 00:43:59.360]   because computer science was, at the time,
[00:43:59.360 --> 00:44:03.520]   moving from a fairly narrow, specific set of topics
[00:44:03.520 --> 00:44:08.160]   to touching a lot of other parts of intellectual life.
[00:44:08.160 --> 00:44:10.720]   And the curriculum was supposed to reflect that.
[00:44:10.720 --> 00:44:12.960]   And so Charles played a big role
[00:44:12.960 --> 00:44:15.360]   in kind of redesigning that.
[00:44:15.360 --> 00:44:15.840]   And then the-
[00:44:15.840 --> 00:44:20.000]   - And for my labors, I ended up the associate dean.
[00:44:20.000 --> 00:44:22.320]   - Right, he got to become associate dean
[00:44:22.320 --> 00:44:24.560]   of charge of educational stuff.
[00:44:24.560 --> 00:44:26.560]   - Well, that would be a valuable lesson.
[00:44:26.560 --> 00:44:28.480]   If you're good at something,
[00:44:28.480 --> 00:44:32.960]   they will give you responsibility to do more of that thing.
[00:44:32.960 --> 00:44:33.760]   - Well-
[00:44:33.760 --> 00:44:34.320]   - Until you-
[00:44:34.320 --> 00:44:35.360]   - Don't show competence.
[00:44:35.360 --> 00:44:36.800]   - Don't show competence if you-
[00:44:36.800 --> 00:44:37.760]   - Well, you know what they say.
[00:44:37.760 --> 00:44:38.480]   - Don't hold responsibility.
[00:44:38.480 --> 00:44:39.440]   - Here's what they say.
[00:44:39.440 --> 00:44:39.760]   - Yeah.
[00:44:39.760 --> 00:44:42.640]   - The reward for good work is more work.
[00:44:42.640 --> 00:44:43.040]   - Yeah.
[00:44:43.040 --> 00:44:45.040]   - The reward for bad work is less work.
[00:44:45.840 --> 00:44:47.920]   Which, I don't know,
[00:44:47.920 --> 00:44:49.920]   depending on what you're trying to do that week,
[00:44:49.920 --> 00:44:51.040]   one of those is better than the other.
[00:44:51.040 --> 00:44:52.640]   - Well, one of the problems with the word work,
[00:44:52.640 --> 00:44:53.600]   sorry to interrupt,
[00:44:53.600 --> 00:44:56.560]   is that it seems to be an antonym
[00:44:56.560 --> 00:44:59.040]   in this particular language.
[00:44:59.040 --> 00:45:01.040]   We have the opposite of happiness.
[00:45:01.040 --> 00:45:02.480]   But it seems like they're,
[00:45:02.480 --> 00:45:06.720]   that's one of, you know, we talked about balance.
[00:45:06.720 --> 00:45:09.520]   It's always like work-life balance.
[00:45:09.520 --> 00:45:12.800]   It always rubbed me the wrong way as a terminology.
[00:45:12.800 --> 00:45:13.600]   I know it's just words.
[00:45:13.600 --> 00:45:15.280]   - Right, the opposite of work is play.
[00:45:15.280 --> 00:45:17.600]   But ideally, work is play.
[00:45:17.600 --> 00:45:20.160]   - Oh, I can't tell you how much time I'd spend,
[00:45:20.160 --> 00:45:21.680]   certainly when I was at Bell Labs,
[00:45:21.680 --> 00:45:23.600]   except for a few very key moments.
[00:45:23.600 --> 00:45:24.960]   As a professor, I would do this too.
[00:45:24.960 --> 00:45:25.440]   I would just say,
[00:45:25.440 --> 00:45:27.200]   "I cannot believe they're paying me to do this."
[00:45:27.200 --> 00:45:29.200]   'Cause it's fun.
[00:45:29.200 --> 00:45:31.440]   It's something that I would do for a hobby
[00:45:31.440 --> 00:45:34.160]   if I could anyway.
[00:45:34.160 --> 00:45:35.360]   So that's what it worked out.
[00:45:35.360 --> 00:45:36.880]   - You sure you wanna be saying that
[00:45:36.880 --> 00:45:38.000]   when this is being recorded?
[00:45:38.000 --> 00:45:40.080]   - As a dean, that is not true at all.
[00:45:40.080 --> 00:45:40.720]   I need a raise.
[00:45:40.720 --> 00:45:41.040]   - Yeah.
[00:45:41.040 --> 00:45:43.280]   - But I think here with this,
[00:45:43.280 --> 00:45:44.560]   that even though a lot of time passed,
[00:45:44.560 --> 00:45:46.640]   Michael and I talked almost every,
[00:45:46.640 --> 00:45:49.360]   well, we texted almost every day during the period.
[00:45:49.360 --> 00:45:52.000]   - Charles, at one point, took me,
[00:45:52.000 --> 00:45:53.920]   there was the ICML conference,
[00:45:53.920 --> 00:45:56.480]   the machine learning conference was in Atlanta.
[00:45:56.480 --> 00:46:00.320]   I was the chair, the general chair of the conference.
[00:46:00.320 --> 00:46:03.200]   Charles was my publicity chair, something like that,
[00:46:03.200 --> 00:46:04.800]   or fundraising chair.
[00:46:04.800 --> 00:46:05.360]   - Fundraising chair.
[00:46:05.360 --> 00:46:05.680]   - Yeah.
[00:46:05.680 --> 00:46:07.840]   But he decided it'd be really funny
[00:46:07.840 --> 00:46:09.520]   if he didn't actually show up for the conference
[00:46:09.520 --> 00:46:10.720]   in his own home city.
[00:46:10.720 --> 00:46:12.320]   So he didn't.
[00:46:12.320 --> 00:46:13.360]   But he did at one point,
[00:46:13.360 --> 00:46:16.080]   pick me up at the conference in his Tesla
[00:46:16.080 --> 00:46:18.480]   and drove me to the Atlanta mall
[00:46:18.480 --> 00:46:21.920]   and forced me to buy an iPhone
[00:46:21.920 --> 00:46:25.520]   because he didn't like how it was to text with me
[00:46:25.520 --> 00:46:27.040]   and thought it would be better for him
[00:46:27.040 --> 00:46:29.920]   if I had an iPhone, the text would be somehow smoother.
[00:46:29.920 --> 00:46:30.720]   - And it was.
[00:46:30.720 --> 00:46:31.440]   - And it was.
[00:46:31.440 --> 00:46:32.720]   - And it is, and his life is better.
[00:46:32.720 --> 00:46:33.600]   - And my life is better.
[00:46:33.600 --> 00:46:34.400]   And so, yeah.
[00:46:34.400 --> 00:46:38.080]   But it was, yeah, Charles forced me to get an iPhone
[00:46:38.080 --> 00:46:40.160]   so that he could text me more efficiently.
[00:46:40.160 --> 00:46:41.600]   I thought that was an interesting moment.
[00:46:41.600 --> 00:46:42.400]   - It works for me.
[00:46:42.400 --> 00:46:43.920]   Anyway, so we kept talking the whole time
[00:46:43.920 --> 00:46:46.320]   and then eventually we did the teaching thing
[00:46:46.320 --> 00:46:46.960]   and it was great.
[00:46:46.960 --> 00:46:48.720]   And there's a couple of reasons for that, by the way.
[00:46:48.720 --> 00:46:51.360]   One is I really wanted to do something different.
[00:46:51.360 --> 00:46:53.120]   Like you've got this medium here,
[00:46:53.120 --> 00:46:54.400]   people claim it can change things.
[00:46:54.400 --> 00:46:56.800]   What's a thing that you could do in this medium
[00:46:56.800 --> 00:46:58.160]   that you could not do otherwise?
[00:46:58.160 --> 00:47:00.320]   Besides edit, right?
[00:47:00.320 --> 00:47:01.120]   I mean, what could you do?
[00:47:01.120 --> 00:47:03.760]   And being able to do something with another person
[00:47:03.760 --> 00:47:04.400]   was that kind of thing.
[00:47:04.400 --> 00:47:04.880]   It's very hard.
[00:47:04.880 --> 00:47:06.000]   I mean, you can take turns,
[00:47:06.000 --> 00:47:08.080]   but teaching together, having conversations,
[00:47:08.080 --> 00:47:08.960]   it's very hard, right?
[00:47:08.960 --> 00:47:10.080]   So that was a cool thing.
[00:47:10.080 --> 00:47:11.200]   The second thing, it gave me an excuse
[00:47:11.200 --> 00:47:12.160]   to do more stuff with him.
[00:47:12.160 --> 00:47:14.800]   - Yeah, I always thought, he makes it sound brilliant
[00:47:14.800 --> 00:47:17.040]   and it is, I guess.
[00:47:17.040 --> 00:47:18.960]   But at the time it really felt like
[00:47:18.960 --> 00:47:22.560]   I've got a lot to do, Charles is saying,
[00:47:22.560 --> 00:47:25.600]   and it would be great if Michael could teach the course
[00:47:25.600 --> 00:47:26.480]   and I could just-
[00:47:26.480 --> 00:47:27.440]   - Hang out.
[00:47:27.440 --> 00:47:29.040]   - Yeah, just kind of coast on that.
[00:47:29.040 --> 00:47:31.360]   - Well, that's what the second class was more like that.
[00:47:31.360 --> 00:47:31.920]   Because the second class-
[00:47:31.920 --> 00:47:33.120]   - Second class was explicit like that.
[00:47:33.120 --> 00:47:34.880]   But the first class, it was at least half.
[00:47:34.880 --> 00:47:37.120]   - Yeah, but I knew all the stuff.
[00:47:37.120 --> 00:47:37.680]   - So the structure that we came up-
[00:47:37.680 --> 00:47:40.400]   - I wish you were once again letting the facts get in the way.
[00:47:40.400 --> 00:47:41.360]   - No, but good story.
[00:47:41.360 --> 00:47:42.160]   - Good story.
[00:47:42.160 --> 00:47:44.400]   - I should just let Charles talk about it.
[00:47:44.400 --> 00:47:45.760]   - But that's the facts that he saw.
[00:47:45.760 --> 00:47:47.520]   But so that was kind of true-
[00:47:47.520 --> 00:47:48.240]   - That's your facts.
[00:47:48.240 --> 00:47:50.000]   - Yeah, that was sort of true for 7642,
[00:47:50.000 --> 00:47:51.200]   which is the reinforcement learning class,
[00:47:51.200 --> 00:47:52.320]   because that was really his class.
[00:47:52.320 --> 00:47:53.680]   - You started with reinforcement learning?
[00:47:53.680 --> 00:47:54.560]   - No, we started with, I did the-
[00:47:54.560 --> 00:47:55.120]   - Machine learning?
[00:47:55.120 --> 00:47:57.280]   - Intro to machine learning, 7641,
[00:47:57.280 --> 00:48:00.240]   which is supervised learning, unsupervised learning,
[00:48:00.240 --> 00:48:01.920]   and reinforcement learning and decision-making,
[00:48:01.920 --> 00:48:02.880]   cram all that in there,
[00:48:02.880 --> 00:48:04.880]   the kind of assignments that we talked about earlier.
[00:48:04.880 --> 00:48:06.320]   And then eventually, about a year later,
[00:48:06.320 --> 00:48:08.320]   we did a follow-on 7642,
[00:48:08.320 --> 00:48:10.640]   which is reinforcement learning and decision-making.
[00:48:10.640 --> 00:48:12.160]   The first class was based on something
[00:48:12.160 --> 00:48:14.320]   I'd been teaching at that point for well over a decade.
[00:48:14.320 --> 00:48:15.920]   And the second class was based on something
[00:48:15.920 --> 00:48:17.360]   Michael had been teaching.
[00:48:17.360 --> 00:48:20.240]   Actually, I learned quite a bit teaching that class with him,
[00:48:20.240 --> 00:48:21.440]   but he drove most of that.
[00:48:21.440 --> 00:48:23.680]   But the first one I drove most of it was all my material.
[00:48:23.680 --> 00:48:26.160]   Although I had stolen that material originally
[00:48:26.160 --> 00:48:28.480]   from slides I found online from Michael,
[00:48:28.480 --> 00:48:30.400]   who had originally stolen that material
[00:48:30.400 --> 00:48:32.000]   from, I guess, slides he found online,
[00:48:32.000 --> 00:48:33.040]   probably from Andrew Moore,
[00:48:33.040 --> 00:48:34.480]   'cause the jokes were the same anyway.
[00:48:34.480 --> 00:48:36.560]   At least some of the, at least when I found the slides,
[00:48:36.560 --> 00:48:37.760]   some of the stuff was there.
[00:48:37.760 --> 00:48:40.480]   Yes, every machine learning class taught in the early 2000s
[00:48:40.480 --> 00:48:41.760]   stole from Andrew Moore.
[00:48:41.760 --> 00:48:43.280]   - A particular joke or two.
[00:48:43.280 --> 00:48:44.800]   - At least the structure.
[00:48:44.800 --> 00:48:46.560]   Now I did, and he did actually,
[00:48:46.560 --> 00:48:48.800]   a lot more with reinforcement learning and such
[00:48:48.800 --> 00:48:50.240]   and game theory and those kinds of things.
[00:48:50.240 --> 00:48:51.520]   But we all sort of built--
[00:48:51.520 --> 00:48:52.400]   - You mean in the research world.
[00:48:52.400 --> 00:48:53.280]   - No, no, no, in that class.
[00:48:53.280 --> 00:48:54.560]   - No, I mean in teaching that class.
[00:48:54.560 --> 00:48:55.920]   - The coverage was different than--
[00:48:55.920 --> 00:48:57.440]   - Than what other people were doing.
[00:48:57.440 --> 00:48:58.880]   Most people were just doing supervised learning
[00:48:58.880 --> 00:49:01.600]   and maybe a little bit of clustering and whatnot.
[00:49:01.600 --> 00:49:02.560]   But we took it all the way to machine learning.
[00:49:02.720 --> 00:49:04.480]   - A lot of it just comes from Tom Mitchell's book.
[00:49:04.480 --> 00:49:06.000]   - Oh no, yeah, except, well,
[00:49:06.000 --> 00:49:07.920]   half of it comes from Tom Mitchell's book, right?
[00:49:07.920 --> 00:49:09.040]   The other half doesn't.
[00:49:09.040 --> 00:49:12.320]   This is why it's all readings, right?
[00:49:12.320 --> 00:49:13.760]   'Cause certain things weren't invented when Tom--
[00:49:13.760 --> 00:49:14.560]   - Yeah, okay, that's true.
[00:49:14.560 --> 00:49:17.520]   - Right, but it was quite good.
[00:49:17.520 --> 00:49:19.360]   But there's a reason for that besides,
[00:49:19.360 --> 00:49:20.880]   you know, just I wanted to do it.
[00:49:20.880 --> 00:49:21.760]   I wanted to do something new
[00:49:21.760 --> 00:49:23.200]   and I wanted to do something with him,
[00:49:23.200 --> 00:49:24.480]   which is a realization,
[00:49:24.480 --> 00:49:26.480]   which is despite what you might believe,
[00:49:26.480 --> 00:49:29.280]   he's an introvert and I'm an introvert,
[00:49:29.280 --> 00:49:31.360]   or I'm on the edge of being an introvert anyway.
[00:49:32.000 --> 00:49:33.520]   But both of us, I think,
[00:49:33.520 --> 00:49:37.040]   enjoy the energy of the crowd, right?
[00:49:37.040 --> 00:49:39.520]   There's something about talking to people
[00:49:39.520 --> 00:49:41.760]   and bringing them into whatever we find interesting
[00:49:41.760 --> 00:49:45.280]   that is empowering, energizing, or whatever.
[00:49:45.280 --> 00:49:50.400]   And I found the idea of staring alone at a computer screen
[00:49:50.400 --> 00:49:52.000]   and then talking off of materials
[00:49:52.000 --> 00:49:54.560]   less inspiring than I wanted it to be.
[00:49:54.560 --> 00:49:59.120]   - And I had in fact done a MOOC for Udacity on algorithms
[00:49:59.120 --> 00:50:04.480]   and it was a week in a dark room talking at the screen,
[00:50:04.480 --> 00:50:06.960]   writing on the little pad.
[00:50:06.960 --> 00:50:09.120]   And I didn't know this was happening,
[00:50:09.120 --> 00:50:12.320]   but the crew had watched some of the videos
[00:50:12.320 --> 00:50:13.520]   while in the middle of this
[00:50:13.520 --> 00:50:15.200]   and they're like, "Something's wrong.
[00:50:15.200 --> 00:50:18.800]   You're sort of shutting down."
[00:50:18.800 --> 00:50:21.680]   And I think a lot of it was I'll make jokes
[00:50:21.680 --> 00:50:23.520]   and no one would laugh.
[00:50:23.520 --> 00:50:24.000]   - Yeah.
[00:50:24.000 --> 00:50:26.400]   - And I felt like the crowd hated me.
[00:50:26.400 --> 00:50:27.680]   Now, of course, there was no crowd.
[00:50:27.680 --> 00:50:29.760]   So it wasn't rational.
[00:50:29.760 --> 00:50:32.800]   But each time I tried it and I got no reaction,
[00:50:32.800 --> 00:50:37.120]   it just was taking the energy out of my performance,
[00:50:37.120 --> 00:50:38.320]   out of my presentation.
[00:50:38.320 --> 00:50:40.240]   - Such a fantastic metaphor for grad school.
[00:50:40.240 --> 00:50:41.760]   Anyway, by working together,
[00:50:41.760 --> 00:50:43.920]   we could play off each other and have a--
[00:50:43.920 --> 00:50:45.280]   - And keep the energy up
[00:50:45.280 --> 00:50:48.720]   because you can't let your guard down for a moment with Charles.
[00:50:48.720 --> 00:50:50.880]   He'll just overpower you.
[00:50:50.880 --> 00:50:52.080]   - I have no idea what you're talking about.
[00:50:52.080 --> 00:50:53.840]   But we would work really well together, I thought,
[00:50:53.840 --> 00:50:54.640]   and we knew each other.
[00:50:54.640 --> 00:50:56.400]   So I knew that we could sort of make it work.
[00:50:56.400 --> 00:50:57.600]   Plus I was the associate dean.
[00:50:57.600 --> 00:50:59.760]   So they had to do what I told them to do.
[00:50:59.760 --> 00:51:01.360]   So we had to make it work.
[00:51:01.360 --> 00:51:02.960]   And so it worked out very well, I thought.
[00:51:02.960 --> 00:51:04.480]   Well enough that we--
[00:51:04.480 --> 00:51:06.240]   - With great power comes great power.
[00:51:06.240 --> 00:51:06.960]   - That's right.
[00:51:06.960 --> 00:51:09.200]   And we became smooth and curly.
[00:51:09.200 --> 00:51:15.360]   And that's when we did the overfitting thriller video.
[00:51:15.360 --> 00:51:16.480]   - Yeah, yeah, yeah.
[00:51:16.480 --> 00:51:17.040]   That's a thing.
[00:51:17.040 --> 00:51:20.560]   - So can we just like smooth and curly?
[00:51:20.560 --> 00:51:21.280]   Where did that come from?
[00:51:21.280 --> 00:51:24.400]   - Okay, so it happened, it was completely spontaneous.
[00:51:24.400 --> 00:51:25.600]   - These are nicknames you go by.
[00:51:25.600 --> 00:51:26.160]   - Yeah, so--
[00:51:26.160 --> 00:51:28.400]   - It's what the students call us.
[00:51:28.400 --> 00:51:30.080]   - He was lecturing.
[00:51:30.080 --> 00:51:31.840]   So the way that we structure the lectures
[00:51:31.840 --> 00:51:33.120]   is one of us is the lecturer
[00:51:33.120 --> 00:51:35.200]   and one of us is basically the student.
[00:51:35.200 --> 00:51:37.040]   And so he was lecturing on--
[00:51:37.040 --> 00:51:38.800]   - The lecturer prepares all the materials,
[00:51:38.800 --> 00:51:40.240]   comes up with the quizzes,
[00:51:40.240 --> 00:51:42.880]   and then the student comes in not knowing anything.
[00:51:42.880 --> 00:51:44.560]   So it was just like being on campus.
[00:51:44.560 --> 00:51:47.840]   And I was doing game theory in particular,
[00:51:47.840 --> 00:51:49.120]   "The Prisoner's Dilemma."
[00:51:49.120 --> 00:51:52.000]   - And so he needed to set up a little "Prisoner's Dilemma" grid.
[00:51:52.000 --> 00:51:54.400]   So he drew it and I could see what he was drawing.
[00:51:54.400 --> 00:51:57.360]   And the "Prisoner's Dilemma" consists of two players,
[00:51:57.360 --> 00:51:59.920]   two parties, so he decided he would make little cartoons
[00:51:59.920 --> 00:52:01.040]   of the two of us.
[00:52:01.040 --> 00:52:04.640]   And so there was two criminals, right,
[00:52:04.640 --> 00:52:06.800]   that were deciding whether or not to rat each other out.
[00:52:06.800 --> 00:52:10.960]   One of them he drew as a circle with a smiley face
[00:52:10.960 --> 00:52:14.080]   and a kind of goatee thing, smooth head.
[00:52:14.080 --> 00:52:16.160]   And the other one with all sorts of curly hair.
[00:52:16.160 --> 00:52:18.240]   And he said, "This is smooth and curly."
[00:52:18.240 --> 00:52:19.520]   I said, "Smooth and curly?"
[00:52:19.520 --> 00:52:21.440]   He said, "No, no, smooth with a V."
[00:52:21.440 --> 00:52:22.880]   It's very important that it have a V.
[00:52:22.880 --> 00:52:25.760]   - And that stuck, I actually watched that video.
[00:52:25.760 --> 00:52:27.040]   - The students really took to that.
[00:52:27.040 --> 00:52:28.800]   Like they found that relatable.
[00:52:28.800 --> 00:52:31.040]   - He started singing "Smooth Criminal" by Michael Jackson.
[00:52:31.040 --> 00:52:31.760]   - Yeah, yeah, yeah.
[00:52:31.760 --> 00:52:33.360]   And those names stuck.
[00:52:33.360 --> 00:52:36.880]   - So we now have a video series, an episode,
[00:52:36.880 --> 00:52:39.360]   our kind of first actual episode should be coming out today,
[00:52:39.360 --> 00:52:43.120]   "Smooth and Curly on Video,"
[00:52:43.120 --> 00:52:47.040]   where the two of us discuss episodes of "Westworld."
[00:52:47.040 --> 00:52:48.800]   We watch "Westworld" and we're like,
[00:52:48.800 --> 00:52:51.600]   "Huh, what does this say about computer science and AI?"
[00:52:51.600 --> 00:52:53.760]   - And we did not watch it.
[00:52:53.760 --> 00:52:55.600]   I mean, I know it's on season three or whatever we have.
[00:52:55.600 --> 00:52:57.360]   As of this recording, it's on season three.
[00:52:57.360 --> 00:52:59.680]   - We've watched now two episodes total.
[00:52:59.680 --> 00:53:00.800]   - Yeah, I think I watched three.
[00:53:00.800 --> 00:53:02.400]   - What do you think about "Westworld?"
[00:53:02.400 --> 00:53:03.360]   - Two episodes in.
[00:53:03.360 --> 00:53:05.200]   So I can tell you so far,
[00:53:05.200 --> 00:53:08.080]   I'm just guessing what's gonna happen next.
[00:53:08.080 --> 00:53:10.000]   It seems like bad things are gonna happen
[00:53:10.000 --> 00:53:11.040]   with the robots uprising.
[00:53:11.040 --> 00:53:12.000]   - Spoiler alert.
[00:53:12.000 --> 00:53:14.480]   - So I have not, I mean, you know,
[00:53:14.480 --> 00:53:16.080]   I vaguely remember a movie existing,
[00:53:16.080 --> 00:53:17.920]   so I assume it's related to that.
[00:53:17.920 --> 00:53:20.000]   - That was more my time than your time, Charles.
[00:53:20.000 --> 00:53:21.440]   - That's right, 'cause you're much older than I am.
[00:53:21.440 --> 00:53:24.720]   I think the important thing here is that it's narrative,
[00:53:24.720 --> 00:53:25.920]   right, it's all about telling a story.
[00:53:25.920 --> 00:53:27.280]   That's the whole driving thing.
[00:53:27.280 --> 00:53:29.520]   But the idea that they would give these reveries,
[00:53:29.520 --> 00:53:31.200]   that they would make people--
[00:53:31.200 --> 00:53:32.320]   - Let them remember--
[00:53:32.320 --> 00:53:34.800]   - Remember the awful things that happened.
[00:53:34.800 --> 00:53:35.200]   - The terrible things that happened.
[00:53:35.200 --> 00:53:37.600]   - Who could possibly think that was gonna, I gotta,
[00:53:37.600 --> 00:53:39.440]   I mean, I don't know, I've only seen the first two episodes
[00:53:39.440 --> 00:53:40.160]   or maybe the third one.
[00:53:40.160 --> 00:53:40.720]   I think I've only seen the first two.
[00:53:40.720 --> 00:53:41.520]   - You know what it was?
[00:53:41.520 --> 00:53:42.640]   You know what the problem is?
[00:53:42.640 --> 00:53:45.200]   That the robots were actually designed by Hannibal Lecter.
[00:53:45.200 --> 00:53:45.920]   - That's true.
[00:53:45.920 --> 00:53:46.720]   (laughs)
[00:53:46.720 --> 00:53:47.200]   They were.
[00:53:47.200 --> 00:53:49.680]   - So like, what do you think's gonna happen?
[00:53:49.680 --> 00:53:50.240]   Bad things.
[00:53:50.240 --> 00:53:51.680]   - It's clear that things are happening
[00:53:51.680 --> 00:53:52.800]   and characters being introduced
[00:53:52.800 --> 00:53:54.160]   and we don't yet know anything.
[00:53:54.160 --> 00:53:57.680]   But still, I was just struck by how it's all driven
[00:53:57.680 --> 00:53:58.480]   by narrative and story.
[00:53:58.480 --> 00:53:59.680]   And there's all these implied things,
[00:53:59.680 --> 00:54:02.400]   like programming, the programming interface
[00:54:02.400 --> 00:54:05.520]   is talking to them about what's going on in their heads,
[00:54:05.520 --> 00:54:08.480]   which is both, I mean, artistically,
[00:54:08.480 --> 00:54:10.000]   it's probably useful to film it that way.
[00:54:10.000 --> 00:54:11.360]   But think about how it would work in real life.
[00:54:11.360 --> 00:54:12.320]   That just seems very crazy.
[00:54:12.320 --> 00:54:14.160]   But there was, we saw in the second episode,
[00:54:14.160 --> 00:54:15.440]   there's a screen, you could see things--
[00:54:15.440 --> 00:54:16.160]   - They were wearing like cool glasses.
[00:54:16.160 --> 00:54:17.120]   - That sort of stayed in the world.
[00:54:17.120 --> 00:54:19.920]   It was quite interesting to just kind of ask this question.
[00:54:20.240 --> 00:54:21.920]   So far, I mean, I assume it veers off
[00:54:21.920 --> 00:54:23.280]   into Never Never Land at some point.
[00:54:23.280 --> 00:54:25.680]   - So we don't know, we can't answer that question.
[00:54:25.680 --> 00:54:28.560]   - I'm also a fan of a guy named Alex Garland.
[00:54:28.560 --> 00:54:30.160]   He's a director of Ex Machina.
[00:54:30.160 --> 00:54:31.120]   - Oh.
[00:54:31.120 --> 00:54:33.040]   - And he is the first,
[00:54:33.040 --> 00:54:36.080]   I wonder if Kubrick was like this, actually.
[00:54:36.080 --> 00:54:38.400]   Is he like studies,
[00:54:38.400 --> 00:54:41.600]   what would it take to program an AI system?
[00:54:41.600 --> 00:54:43.920]   Like he's curious enough to go into that direction.
[00:54:43.920 --> 00:54:48.400]   On the Westworld side, I felt there was more emphasis
[00:54:48.400 --> 00:54:50.800]   on the narratives than like actually asking
[00:54:50.800 --> 00:54:52.160]   like computer science questions.
[00:54:52.160 --> 00:54:54.560]   How would you build this?
[00:54:54.560 --> 00:54:56.720]   How would you, and--
[00:54:56.720 --> 00:54:57.680]   - How would you debug it?
[00:54:57.680 --> 00:55:00.800]   I still think, to me, that's the key issue.
[00:55:00.800 --> 00:55:02.000]   They were terrible debuggers.
[00:55:02.000 --> 00:55:02.480]   - Yeah.
[00:55:02.480 --> 00:55:04.800]   - Well, they said specifically, so we make a change
[00:55:04.800 --> 00:55:05.760]   and we put it out in the world.
[00:55:05.760 --> 00:55:07.680]   And that's bad because something terrible could happen.
[00:55:07.680 --> 00:55:09.360]   Like if you're putting things out in the world
[00:55:09.360 --> 00:55:10.960]   and you're not sure whether something terrible
[00:55:10.960 --> 00:55:13.040]   is gonna happen, your process is probably--
[00:55:13.040 --> 00:55:14.560]   - I just feel like there should have been someone
[00:55:14.560 --> 00:55:16.560]   whose sole job it was, was to walk around,
[00:55:16.560 --> 00:55:19.040]   poke his head in and say, "What could possibly go wrong?"
[00:55:19.040 --> 00:55:20.160]   Just over and over again.
[00:55:20.160 --> 00:55:22.000]   - I would have loved if there was an,
[00:55:22.000 --> 00:55:24.640]   and I did watch a lot more, I'm not giving anything away.
[00:55:24.640 --> 00:55:26.960]   I would have loved it if there was like an episode
[00:55:26.960 --> 00:55:29.840]   where like the new intern is like debugging
[00:55:29.840 --> 00:55:32.560]   a new model or something and like it just keeps failing.
[00:55:32.560 --> 00:55:34.000]   And they're like, all right.
[00:55:34.000 --> 00:55:36.640]   And then it's more turns into like an episode
[00:55:36.640 --> 00:55:38.160]   of Silicon Valley or something like that.
[00:55:38.160 --> 00:55:38.660]   - Yeah.
[00:55:38.660 --> 00:55:41.840]   - Versus like this ominous AI systems
[00:55:41.840 --> 00:55:45.520]   that are constantly like threatening the fabric
[00:55:45.520 --> 00:55:47.040]   of this world that's been created.
[00:55:47.040 --> 00:55:47.520]   - Yeah.
[00:55:47.520 --> 00:55:50.960]   Yeah. And you know, this reminds me of something that,
[00:55:50.960 --> 00:55:52.400]   so I agree with that, that actually would be very cool,
[00:55:52.400 --> 00:55:54.560]   at least for the small percentage of people
[00:55:54.560 --> 00:55:56.480]   who care about debugging systems.
[00:55:56.480 --> 00:55:57.120]   But the other thing is--
[00:55:57.120 --> 00:55:58.960]   - Right, debugging, the series.
[00:55:58.960 --> 00:56:01.280]   - Yeah, it falls into, think of the sequels,
[00:56:01.280 --> 00:56:02.000]   fear of the debugging.
[00:56:02.000 --> 00:56:02.560]   - Oh my gosh.
[00:56:02.560 --> 00:56:03.840]   - And anyway, so--
[00:56:03.840 --> 00:56:05.520]   - It's a nightmare show.
[00:56:05.520 --> 00:56:06.640]   It's a horror movie.
[00:56:06.640 --> 00:56:08.720]   - I think that's where we lose people, by the way,
[00:56:08.720 --> 00:56:10.480]   early on is the people who either decide,
[00:56:10.480 --> 00:56:12.640]   either figure out debugging or think debugging is terrible.
[00:56:12.640 --> 00:56:13.200]   This is part of--
[00:56:13.200 --> 00:56:14.560]   - Where we lose people in computer science.
[00:56:14.560 --> 00:56:16.880]   - This is part of the struggle versus suffering, right?
[00:56:16.880 --> 00:56:19.440]   You get through it and you kind of get the skills of it,
[00:56:19.440 --> 00:56:20.720]   or you're just like, this is dumb,
[00:56:20.720 --> 00:56:21.920]   and this is a dumb way to do anything.
[00:56:21.920 --> 00:56:23.360]   And I think that's when we lose people.
[00:56:23.360 --> 00:56:26.560]   But, well, I'll leave it at that.
[00:56:26.560 --> 00:56:28.560]   But I think that there's something
[00:56:28.560 --> 00:56:34.000]   really, really neat about framing it that way.
[00:56:34.000 --> 00:56:37.440]   But what I don't like about all of these things,
[00:56:37.440 --> 00:56:38.800]   and I love Tex Machina, by the way,
[00:56:38.800 --> 00:56:40.960]   I thought the ending was very depressing.
[00:56:40.960 --> 00:56:43.440]   - Well, again, one of the things
[00:56:43.440 --> 00:56:46.320]   I have to talk to Alex about,
[00:56:46.320 --> 00:56:49.920]   he says that the thing that nobody noticed he put in
[00:56:49.920 --> 00:56:53.360]   is at the end, spoiler alert,
[00:56:53.360 --> 00:56:57.120]   the robot turns and looks at the camera
[00:56:57.120 --> 00:57:00.080]   and smiles briefly.
[00:57:00.080 --> 00:57:04.560]   And to him, he thought that his definition
[00:57:04.560 --> 00:57:08.400]   of passing the general version of the Turing test,
[00:57:08.400 --> 00:57:12.160]   or the consciousness test, is smiling for no one.
[00:57:13.120 --> 00:57:17.040]   - Oh. - Like, not,
[00:57:17.040 --> 00:57:20.400]   oh, it's like the Chinese room kind of experiment.
[00:57:20.400 --> 00:57:22.640]   It's not always trying to act for others,
[00:57:22.640 --> 00:57:24.080]   but just on your own,
[00:57:24.080 --> 00:57:26.160]   being able to have a relationship
[00:57:26.160 --> 00:57:29.680]   with the actual experience and just take it in.
[00:57:29.680 --> 00:57:32.560]   I don't know, he said nobody noticed the magic of it.
[00:57:32.560 --> 00:57:34.880]   - I have this vague feeling that I remember the smile,
[00:57:34.880 --> 00:57:36.960]   but now you've just put the memory in my head,
[00:57:36.960 --> 00:57:38.000]   so probably not.
[00:57:38.000 --> 00:57:40.000]   But I do think that that's interesting.
[00:57:40.000 --> 00:57:41.760]   Although, by looking at the camera,
[00:57:41.760 --> 00:57:43.440]   you are smiling for the audience, right?
[00:57:43.440 --> 00:57:44.720]   You're breaking the fourth wall.
[00:57:44.720 --> 00:57:46.320]   It seems, I mean, well,
[00:57:46.320 --> 00:57:48.080]   that's a limitation of the medium,
[00:57:48.080 --> 00:57:49.440]   but I like that idea.
[00:57:49.440 --> 00:57:51.360]   But here's the problem I have with all of those movies,
[00:57:51.360 --> 00:57:53.520]   all of them, is that,
[00:57:53.520 --> 00:57:54.560]   but I know why it's this way,
[00:57:54.560 --> 00:57:57.360]   and I enjoy those movies, and "Westworld,"
[00:57:57.360 --> 00:58:02.800]   is it sets up the problem of AI as succeeding,
[00:58:02.800 --> 00:58:04.640]   and then having something we cannot control.
[00:58:04.640 --> 00:58:08.240]   But it's not the bad part of AI.
[00:58:08.240 --> 00:58:09.840]   The bad part of AI is the stuff
[00:58:09.840 --> 00:58:10.960]   we're living through now, right?
[00:58:10.960 --> 00:58:13.600]   It's using the data to make decisions that are terrible.
[00:58:13.600 --> 00:58:15.680]   It's not the intelligence that's gonna go out there
[00:58:15.680 --> 00:58:17.840]   and surpass us and take over the world,
[00:58:17.840 --> 00:58:20.880]   or lock us into a room to starve to death slowly
[00:58:20.880 --> 00:58:22.560]   over multiple days.
[00:58:22.560 --> 00:58:26.000]   It's instead the tools that we're building
[00:58:26.000 --> 00:58:30.160]   that are allowing us to make the terrible decisions
[00:58:30.160 --> 00:58:32.160]   we would have less efficiently made before, right?
[00:58:32.160 --> 00:58:35.520]   Computers are very good at making us more efficient,
[00:58:35.520 --> 00:58:38.160]   including being more efficient at doing terrible things.
[00:58:38.160 --> 00:58:40.080]   And that's the part of the AI we have to worry about.
[00:58:40.080 --> 00:58:44.000]   It's not the true intelligence that we're gonna build
[00:58:44.000 --> 00:58:46.560]   sometime in the future, probably long after we're around.
[00:58:46.560 --> 00:58:54.720]   I think that whole framing of it sort of misses the point,
[00:58:54.720 --> 00:58:55.840]   even though it is inspiring.
[00:58:55.840 --> 00:58:57.600]   And I was inspired by those ideas, right?
[00:58:57.600 --> 00:58:58.880]   I got into this in part
[00:58:58.880 --> 00:59:00.640]   'cause I wanted to build something like that.
[00:59:00.640 --> 00:59:02.000]   Philosophical questions are interesting to me,
[00:59:02.000 --> 00:59:04.560]   but that's not where the terror comes from.
[00:59:04.560 --> 00:59:05.680]   The terror comes from the everyday.
[00:59:05.680 --> 00:59:07.840]   - And you can construct,
[00:59:07.840 --> 00:59:09.520]   it's in the subtlety of the interaction
[00:59:09.520 --> 00:59:11.040]   between AI and the human,
[00:59:11.040 --> 00:59:14.160]   like with social networks,
[00:59:14.160 --> 00:59:15.040]   all the stuff you're doing
[00:59:15.040 --> 00:59:17.120]   with interactive artificial intelligence.
[00:59:17.120 --> 00:59:21.600]   But I feel like Cal 9000 came a little bit closer to that
[00:59:21.600 --> 00:59:24.240]   in 2001 Space Odyssey
[00:59:24.240 --> 00:59:28.000]   'cause it felt like a personal assistant.
[00:59:28.000 --> 00:59:31.120]   It felt like closer to the AI systems we have today.
[00:59:31.120 --> 00:59:35.760]   And the real things we might actually encounter,
[00:59:35.760 --> 00:59:41.360]   which is over-relying in some fundamental way
[00:59:41.360 --> 00:59:44.720]   on our dumb assistants or on social networks,
[00:59:44.720 --> 00:59:46.960]   like over offloading too much of us
[00:59:46.960 --> 00:59:55.040]   onto things that require internet and power and so on,
[00:59:55.040 --> 00:59:59.600]   and thereby becoming powerless as a standalone entity.
[00:59:59.600 --> 01:00:02.160]   And then when that thing starts to misbehave
[01:00:02.160 --> 01:00:05.520]   in some subtle way, it creates a lot of problems.
[01:00:05.520 --> 01:00:08.400]   And those problems are dramatized when you're in space
[01:00:08.400 --> 01:00:11.200]   because you don't have a way to walk away.
[01:00:11.200 --> 01:00:12.240]   - Well, as the man said,
[01:00:12.240 --> 01:00:15.200]   once we started making the decisions for you,
[01:00:15.200 --> 01:00:17.040]   it stopped being your world, right?
[01:00:17.040 --> 01:00:20.000]   That's the matrix, Michael, in case you don't remember.
[01:00:20.000 --> 01:00:20.320]   - I didn't catch it, thank you.
[01:00:20.320 --> 01:00:23.040]   - But on the other hand, I could say no,
[01:00:23.040 --> 01:00:24.880]   because isn't that what we do with people anyway?
[01:00:24.880 --> 01:00:28.160]   This kind of the shared intelligence that is humanity
[01:00:28.160 --> 01:00:30.080]   is relying on other people constantly.
[01:00:30.080 --> 01:00:32.160]   I mean, we hyper-specialize, right?
[01:00:32.160 --> 01:00:34.400]   As individuals, we're still generally intelligent.
[01:00:34.400 --> 01:00:35.840]   We make our own decisions in a lot of ways,
[01:00:35.840 --> 01:00:37.440]   but we leave most of this up to other people,
[01:00:37.440 --> 01:00:39.760]   and that's perfectly fine.
[01:00:39.760 --> 01:00:43.040]   And by the way, everyone doesn't necessarily share our goals.
[01:00:43.040 --> 01:00:44.960]   Sometimes they seem to be quite against us.
[01:00:44.960 --> 01:00:47.760]   Sometimes we make decisions that others would see
[01:00:47.760 --> 01:00:48.880]   as against our own interests,
[01:00:48.880 --> 01:00:51.200]   and yet we somehow manage it, manage to survive.
[01:00:51.200 --> 01:00:53.280]   I'm not entirely sure why an AI
[01:00:53.280 --> 01:00:55.360]   would actually make that worse,
[01:00:55.360 --> 01:00:59.120]   or even different, really.
[01:00:59.120 --> 01:01:01.280]   - You mentioned the matrix.
[01:01:01.280 --> 01:01:03.600]   Do you think we're living in a simulation?
[01:01:04.320 --> 01:01:08.000]   - It does feel like a thought game
[01:01:08.000 --> 01:01:10.560]   more than a real scientific question.
[01:01:10.560 --> 01:01:12.000]   - Well, I'll tell you why I think
[01:01:12.000 --> 01:01:13.360]   it's an interesting thought experiment,
[01:01:13.360 --> 01:01:14.000]   see what you think.
[01:01:14.000 --> 01:01:16.080]   From a computer science perspective,
[01:01:16.080 --> 01:01:20.080]   it's a good experiment of how difficult would it be
[01:01:20.080 --> 01:01:22.720]   to create a sufficiently realistic world
[01:01:22.720 --> 01:01:24.720]   that us humans would enjoy being in.
[01:01:24.720 --> 01:01:27.440]   That's almost like a competition.
[01:01:27.440 --> 01:01:28.960]   - I mean, if we're living in a simulation,
[01:01:28.960 --> 01:01:31.440]   then I don't believe that we were put in the simulation.
[01:01:31.440 --> 01:01:34.080]   I believe that it's just physics playing out,
[01:01:34.080 --> 01:01:35.680]   and we came out of that.
[01:01:35.680 --> 01:01:38.160]   I don't think-
[01:01:38.160 --> 01:01:40.560]   - So you think you have to build the universe?
[01:01:40.560 --> 01:01:42.320]   - I think that the universe itself,
[01:01:42.320 --> 01:01:43.680]   we can think of that as a simulation.
[01:01:43.680 --> 01:01:46.640]   And in fact, sometimes I try to think about,
[01:01:46.640 --> 01:01:49.440]   to understand what it's like for a computer
[01:01:49.440 --> 01:01:52.480]   to start to think about the world.
[01:01:52.480 --> 01:01:54.000]   I try to think about the world,
[01:01:54.000 --> 01:01:56.720]   things like quantum mechanics,
[01:01:56.720 --> 01:01:59.120]   where it doesn't feel very natural to me at all.
[01:02:00.320 --> 01:02:02.000]   And it really strikes me as,
[01:02:02.000 --> 01:02:04.960]   I don't understand this thing that we're living in.
[01:02:04.960 --> 01:02:07.440]   There's weird things happening in it
[01:02:07.440 --> 01:02:09.520]   that don't feel natural to me at all.
[01:02:09.520 --> 01:02:12.880]   Now, if you wanna call that the result of a simulator,
[01:02:12.880 --> 01:02:14.080]   okay, I'm fine with that.
[01:02:14.080 --> 01:02:16.400]   - 'Cause there's the bugs in the simulation.
[01:02:16.400 --> 01:02:17.520]   - There's the bugs.
[01:02:17.520 --> 01:02:19.440]   I mean, the interesting thing about simulation
[01:02:19.440 --> 01:02:21.040]   is that it might have bugs.
[01:02:21.040 --> 01:02:22.240]   I mean, that's the thing that I-
[01:02:22.240 --> 01:02:23.760]   - But there wouldn't be bugs
[01:02:23.760 --> 01:02:24.960]   for the people in the simulation.
[01:02:24.960 --> 01:02:26.640]   They're just, that's just reality.
[01:02:26.640 --> 01:02:29.120]   - Unless you were aware enough to know that there was a bug.
[01:02:29.120 --> 01:02:30.240]   But I think-
[01:02:30.240 --> 01:02:31.280]   - Back to the matrix.
[01:02:31.280 --> 01:02:32.240]   - Yeah, the way you put the question, though.
[01:02:32.240 --> 01:02:35.200]   - I don't think that we live in a simulation created for us.
[01:02:35.200 --> 01:02:36.160]   Okay, I would say that.
[01:02:36.160 --> 01:02:36.800]   - I think that's interesting.
[01:02:36.800 --> 01:02:38.000]   I've actually never thought about it that way.
[01:02:38.000 --> 01:02:39.760]   I mean, the way you asked the question, though,
[01:02:39.760 --> 01:02:42.960]   is could you create a world that is enough for us humans?
[01:02:42.960 --> 01:02:45.360]   It's an interestingly sort of self-referential question
[01:02:45.360 --> 01:02:49.840]   because the beings that created the simulation
[01:02:49.840 --> 01:02:51.280]   probably have not created the simulation
[01:02:51.280 --> 01:02:52.240]   that's realistic for them.
[01:02:52.240 --> 01:02:55.440]   But we're in the simulation, and so it's realistic for us.
[01:02:56.080 --> 01:02:59.040]   So we could create a simulation that is fine
[01:02:59.040 --> 01:03:02.000]   for the people in the simulation, as it were,
[01:03:02.000 --> 01:03:03.520]   that would not necessarily be fine for us
[01:03:03.520 --> 01:03:04.800]   as the creators of the simulation.
[01:03:04.800 --> 01:03:07.280]   - But, well, you can forget.
[01:03:07.280 --> 01:03:08.720]   I mean, when you go into the,
[01:03:08.720 --> 01:03:10.720]   if you play video games and virtual reality,
[01:03:10.720 --> 01:03:15.120]   if some suspension of disbelief or whatever-
[01:03:15.120 --> 01:03:17.200]   - It becomes a world.
[01:03:17.200 --> 01:03:20.000]   - It becomes a world, even like in brief moments,
[01:03:20.000 --> 01:03:22.160]   you forget that another world exists.
[01:03:22.160 --> 01:03:24.080]   I mean, that's what good stories do.
[01:03:24.080 --> 01:03:25.120]   They pull you in.
[01:03:25.120 --> 01:03:27.440]   The question is, is it possible to pull,
[01:03:27.440 --> 01:03:29.280]   our brains are limited.
[01:03:29.280 --> 01:03:31.120]   Is it possible to pull the brain in
[01:03:31.120 --> 01:03:32.640]   to where we actually stay in that world
[01:03:32.640 --> 01:03:34.720]   longer and longer and longer and longer?
[01:03:34.720 --> 01:03:38.880]   And not only that, but we don't wanna leave.
[01:03:38.880 --> 01:03:41.360]   And so, especially, this is the key thing
[01:03:41.360 --> 01:03:43.760]   about the developing brain,
[01:03:43.760 --> 01:03:47.920]   is if we journey into that world early on in life, often.
[01:03:47.920 --> 01:03:48.960]   - How would you even know?
[01:03:48.960 --> 01:03:49.680]   Yeah.
[01:03:49.680 --> 01:03:53.040]   - Yeah, but from a video game design perspective,
[01:03:53.040 --> 01:03:54.800]   from a Westworld perspective,
[01:03:54.800 --> 01:03:57.440]   I think it's an important thing
[01:03:57.440 --> 01:04:00.640]   for even computer scientists to think about,
[01:04:00.640 --> 01:04:03.680]   'cause it's clear that video games are getting much better.
[01:04:03.680 --> 01:04:06.400]   And virtual reality,
[01:04:06.400 --> 01:04:08.320]   although it's been ups and downs,
[01:04:08.320 --> 01:04:09.760]   just like artificial intelligence,
[01:04:09.760 --> 01:04:14.640]   it feels like virtual reality will be here
[01:04:14.640 --> 01:04:16.160]   in a very impressive form
[01:04:16.160 --> 01:04:18.880]   if we were to fast forward 100 years into the future
[01:04:18.880 --> 01:04:22.000]   in a way that might change society fundamentally.
[01:04:22.000 --> 01:04:23.040]   Like, if I were to,
[01:04:23.040 --> 01:04:25.120]   I'm very limited in predicting the future,
[01:04:25.120 --> 01:04:26.320]   as all of us are,
[01:04:26.320 --> 01:04:28.160]   but if I were to try to predict,
[01:04:28.160 --> 01:04:32.400]   like, in which way I'd be surprised
[01:04:32.400 --> 01:04:34.880]   to see the world 100 years from now,
[01:04:34.880 --> 01:04:39.360]   it'd be that, or impressed,
[01:04:39.360 --> 01:04:41.760]   it'd be that we're all no longer
[01:04:41.760 --> 01:04:43.120]   living in this physical world,
[01:04:43.120 --> 01:04:44.640]   that we're all living in a virtual world.
[01:04:44.640 --> 01:04:48.720]   - You really need to read "Calculating God" by Sawyer.
[01:04:48.720 --> 01:04:52.800]   It's a, he'll read it in a night.
[01:04:52.960 --> 01:04:54.160]   It's a very easy read,
[01:04:54.160 --> 01:04:56.080]   but it's, assuming you're that kind of reader,
[01:04:56.080 --> 01:04:58.240]   but it's a good story,
[01:04:58.240 --> 01:04:59.520]   and it's kind of about this,
[01:04:59.520 --> 01:05:01.280]   but not in a way that it appears.
[01:05:01.280 --> 01:05:05.600]   And I really enjoyed the thought experiment.
[01:05:05.600 --> 01:05:08.080]   And I think it's pretty sure it's Robert Sawyer.
[01:05:08.080 --> 01:05:10.000]   But anyway, he's apparently
[01:05:10.000 --> 01:05:12.240]   Canadian's top science fiction writer,
[01:05:12.240 --> 01:05:14.960]   which is why the story mostly takes place in Toronto.
[01:05:14.960 --> 01:05:16.400]   But it's a very good,
[01:05:16.400 --> 01:05:18.400]   it's a very good sort of story
[01:05:18.400 --> 01:05:21.200]   that sort of imagines this.
[01:05:21.200 --> 01:05:24.320]   Very different kind of simulation hypothesis
[01:05:24.320 --> 01:05:28.160]   sort of thing from say, "The Egg," for example.
[01:05:28.160 --> 01:05:30.000]   I'm talking about the short story
[01:05:30.000 --> 01:05:33.440]   by the guy who did "The Martian."
[01:05:33.440 --> 01:05:35.520]   Who wrote "The Martian?"
[01:05:35.520 --> 01:05:36.160]   - Mm-hmm.
[01:05:36.160 --> 01:05:36.640]   - You know what I'm talking about.
[01:05:36.640 --> 01:05:37.120]   - "The Martian."
[01:05:37.120 --> 01:05:37.680]   - Matt Damon.
[01:05:37.680 --> 01:05:38.400]   - No.
[01:05:38.400 --> 01:05:39.280]   - The book.
[01:05:39.280 --> 01:05:41.440]   - So we had this whole discussion
[01:05:41.440 --> 01:05:44.080]   that Michael doesn't partake
[01:05:44.080 --> 01:05:45.440]   in this exercise of reading.
[01:05:45.440 --> 01:05:46.640]   - Yeah, he doesn't seem to like it,
[01:05:46.640 --> 01:05:47.920]   which seems very strange to me,
[01:05:47.920 --> 01:05:49.280]   considering how much he has to read.
[01:05:49.280 --> 01:05:50.800]   I read all the time.
[01:05:50.800 --> 01:05:53.120]   I used to read 10 books every week
[01:05:53.120 --> 01:05:55.120]   when I was in sixth grade or whatever.
[01:05:55.120 --> 01:05:56.880]   I was, a lot of it's science fiction,
[01:05:56.880 --> 01:05:58.480]   a lot of it history,
[01:05:58.480 --> 01:05:59.760]   but I love to read.
[01:05:59.760 --> 01:06:01.600]   But anyway, you should read "Calculating God."
[01:06:01.600 --> 01:06:02.080]   I think you'll,
[01:06:02.080 --> 01:06:04.800]   it's very easy to read, like I said.
[01:06:04.800 --> 01:06:06.720]   And I think you'll enjoy
[01:06:06.720 --> 01:06:08.240]   sort of the ideas that it presents.
[01:06:08.240 --> 01:06:10.240]   - Yeah, I think the thought experiment
[01:06:10.240 --> 01:06:11.120]   is quite interesting.
[01:06:11.120 --> 01:06:15.440]   One thing I've noticed about people growing up now,
[01:06:15.440 --> 01:06:17.120]   I mean, we talk about social media,
[01:06:17.120 --> 01:06:19.440]   but video games is a much bigger,
[01:06:19.440 --> 01:06:21.520]   bigger and bigger and bigger part of their lives.
[01:06:21.520 --> 01:06:24.080]   And the video games have become much more realistic.
[01:06:24.080 --> 01:06:25.680]   I think it's possible that
[01:06:25.680 --> 01:06:29.600]   the three of us are not,
[01:06:29.600 --> 01:06:32.800]   and maybe the two of you are not familiar
[01:06:32.800 --> 01:06:35.040]   exactly with the numbers we're talking about here.
[01:06:35.040 --> 01:06:36.800]   I think the number of people--
[01:06:36.800 --> 01:06:38.320]   - It's bigger than movies, right?
[01:06:38.320 --> 01:06:39.680]   It's huge.
[01:06:39.680 --> 01:06:41.360]   I used to do a lot of the narrative,
[01:06:41.360 --> 01:06:42.480]   computational narrative stuff.
[01:06:42.480 --> 01:06:45.520]   - I understand that economists can actually see
[01:06:45.520 --> 01:06:48.480]   the impact of video games on the labor market,
[01:06:48.480 --> 01:06:54.240]   that there's fewer young men of a certain age
[01:06:54.240 --> 01:06:58.240]   participating in like paying jobs than you'd expect.
[01:06:58.240 --> 01:07:01.040]   And that they trace it back to video games.
[01:07:01.040 --> 01:07:02.640]   - I mean, the problem with "Star Trek"
[01:07:02.640 --> 01:07:05.280]   was not warp drive or teleportation.
[01:07:05.280 --> 01:07:07.760]   It was the holodeck.
[01:07:07.760 --> 01:07:10.800]   Like if you have the holodeck, that's it.
[01:07:10.800 --> 01:07:13.360]   That's it, you go in the holodeck, you never come out.
[01:07:13.360 --> 01:07:15.920]   I mean, it just never made,
[01:07:15.920 --> 01:07:17.760]   once I saw that, I thought, okay, well,
[01:07:17.760 --> 01:07:19.840]   so this is the end of humanity as we know it, right?
[01:07:19.840 --> 01:07:20.880]   They've invented the holodeck.
[01:07:20.880 --> 01:07:23.040]   - Because that feels like the singularity,
[01:07:23.040 --> 01:07:24.960]   not some AGI or whatever.
[01:07:24.960 --> 01:07:28.160]   It's some possibility to go into another world
[01:07:28.160 --> 01:07:30.720]   that can be artificially made better than this one.
[01:07:30.720 --> 01:07:34.000]   - And slowing it down so you live forever,
[01:07:34.000 --> 01:07:35.600]   or speeding it up so you appear to live forever,
[01:07:35.600 --> 01:07:37.680]   or making the decision of when to die.
[01:07:37.680 --> 01:07:42.160]   - And then most of us will just be old people on the porch
[01:07:42.160 --> 01:07:44.480]   yelling at the kids these days in their virtual reality.
[01:07:44.480 --> 01:07:47.040]   (Lex laughing)
[01:07:47.040 --> 01:07:47.920]   - But they won't hear us
[01:07:47.920 --> 01:07:49.040]   because they've got headphones on.
[01:07:49.040 --> 01:07:52.720]   - So, I mean, rewinding back to MOOCs,
[01:07:52.720 --> 01:07:55.840]   is there lessons that you've,
[01:07:55.840 --> 01:07:57.040]   speaking to kids these days?
[01:07:57.040 --> 01:07:58.400]   - There you go.
[01:07:58.400 --> 01:07:59.120]   That was a transition.
[01:07:59.120 --> 01:07:59.920]   - That was fantastic.
[01:07:59.920 --> 01:08:02.800]   - All right, I'll fix it in post.
[01:08:02.800 --> 01:08:04.400]   (Charles laughing)
[01:08:04.400 --> 01:08:06.000]   - That's Charles's favorite phrase.
[01:08:06.000 --> 01:08:06.800]   - Fix it in post?
[01:08:06.800 --> 01:08:07.440]   - Fix it in post.
[01:08:07.440 --> 01:08:08.000]   - Fix it in post.
[01:08:08.000 --> 01:08:09.840]   We said all, when we were recording,
[01:08:09.840 --> 01:08:12.320]   all the time, whenever the editor didn't like something
[01:08:12.320 --> 01:08:14.240]   or whatever, I would say, "We'll fix it in post."
[01:08:14.240 --> 01:08:15.200]   He hated that.
[01:08:15.200 --> 01:08:15.600]   - Yeah.
[01:08:15.600 --> 01:08:16.640]   - He hated that more than anything.
[01:08:16.640 --> 01:08:17.840]   - 'Cause it was Charles's way of saying,
[01:08:17.840 --> 01:08:18.800]   "I'm not gonna do it again."
[01:08:18.800 --> 01:08:20.240]   (Lex laughing)
[01:08:20.240 --> 01:08:21.760]   You know, "You're on your own for this one."
[01:08:21.760 --> 01:08:23.440]   - But it always got fixed in post.
[01:08:23.440 --> 01:08:24.320]   - Exactly.
[01:08:24.320 --> 01:08:24.640]   - Anyway.
[01:08:24.640 --> 01:08:28.160]   - So, is there something you've learned about,
[01:08:28.160 --> 01:08:29.680]   I mean, it's interesting to talk about MOOCs.
[01:08:29.680 --> 01:08:30.560]   Is there something you've learned
[01:08:30.560 --> 01:08:32.000]   about the process of education,
[01:08:32.000 --> 01:08:35.600]   about thinking about the present?
[01:08:35.600 --> 01:08:38.640]   I think there's two lines of conversation to be had here,
[01:08:38.640 --> 01:08:41.360]   is the future of education in general
[01:08:41.360 --> 01:08:42.720]   that you've learned about,
[01:08:42.720 --> 01:08:46.640]   and more presciently,
[01:08:46.640 --> 01:08:50.320]   is the education in the times of COVID.
[01:08:50.320 --> 01:08:51.040]   - Yeah.
[01:08:51.040 --> 01:08:52.400]   The second thing, in some ways,
[01:08:52.400 --> 01:08:53.440]   matters more than the first,
[01:08:53.440 --> 01:08:55.760]   for at least in my head,
[01:08:55.760 --> 01:08:56.960]   not just because it's happening now,
[01:08:56.960 --> 01:09:00.480]   but because I think it's reminded us of a lot of things.
[01:09:00.480 --> 01:09:01.920]   Coincidentally, today,
[01:09:01.920 --> 01:09:03.680]   there's an article out by a good friend of mine
[01:09:03.680 --> 01:09:06.000]   who's also a professor at Georgia Tech,
[01:09:06.000 --> 01:09:07.760]   but more importantly, a writer and editor
[01:09:07.760 --> 01:09:09.680]   at the Atlantic, a guy named Ian Bogost.
[01:09:10.640 --> 01:09:12.080]   And the title is something like,
[01:09:12.080 --> 01:09:15.200]   "Americans Will Sacrifice Anything
[01:09:15.200 --> 01:09:16.320]   "for the College Experience."
[01:09:16.320 --> 01:09:20.160]   And it's about why we went back to college
[01:09:20.160 --> 01:09:22.240]   and why people wanted us to go back to college.
[01:09:22.240 --> 01:09:24.480]   And it's not greedy presidents
[01:09:24.480 --> 01:09:26.160]   trying to get the last dollar from someone.
[01:09:26.160 --> 01:09:27.840]   It's because they want to go to college.
[01:09:27.840 --> 01:09:29.680]   And what they're paying for is not the classes.
[01:09:29.680 --> 01:09:32.000]   What they're paying for is the college experience.
[01:09:32.000 --> 01:09:32.880]   It's not the education.
[01:09:32.880 --> 01:09:33.440]   It's being there.
[01:09:33.440 --> 01:09:34.800]   I've believed this for a long time,
[01:09:34.800 --> 01:09:37.760]   that we continually make this mistake
[01:09:37.760 --> 01:09:40.480]   of people want to go back to college.
[01:09:40.480 --> 01:09:42.080]   As being people want to go back to class.
[01:09:42.080 --> 01:09:42.400]   They don't.
[01:09:42.400 --> 01:09:43.280]   They want to go back to campus.
[01:09:43.280 --> 01:09:44.240]   They want to move away from home.
[01:09:44.240 --> 01:09:45.200]   They want to do all those things
[01:09:45.200 --> 01:09:46.480]   that people experience.
[01:09:46.480 --> 01:09:47.920]   It's a rite of passage.
[01:09:47.920 --> 01:09:50.080]   It's an identity,
[01:09:50.080 --> 01:09:53.600]   if I can steal some of Ian's words here.
[01:09:53.600 --> 01:09:54.640]   And I think that's right.
[01:09:54.640 --> 01:09:56.400]   And I think what we've learned through COVID
[01:09:56.400 --> 01:09:59.600]   is it has made it,
[01:09:59.600 --> 01:10:02.080]   the disaggregation was not the disaggregation
[01:10:02.080 --> 01:10:03.360]   of the education from the place,
[01:10:03.360 --> 01:10:04.960]   the university place,
[01:10:04.960 --> 01:10:06.800]   and that you can get the best anywhere you want to.
[01:10:06.800 --> 01:10:08.000]   It turns out there's lots of reasons
[01:10:08.000 --> 01:10:10.080]   why that is not necessarily true.
[01:10:10.080 --> 01:10:13.200]   The disaggregation is having it shoved in our faces
[01:10:13.200 --> 01:10:14.800]   that the reason to go again,
[01:10:14.800 --> 01:10:16.560]   that the reason to go to college
[01:10:16.560 --> 01:10:18.160]   is not necessarily to learn.
[01:10:18.160 --> 01:10:20.000]   It's to have the college experience.
[01:10:20.000 --> 01:10:21.760]   And that's very difficult for us to accept,
[01:10:21.760 --> 01:10:23.680]   even though we behave that way,
[01:10:23.680 --> 01:10:25.280]   most of us when we were undergrads.
[01:10:25.280 --> 01:10:28.640]   A lot of us didn't go to every single class.
[01:10:28.640 --> 01:10:29.760]   We learned and we got it.
[01:10:29.760 --> 01:10:30.480]   And we look back on it
[01:10:30.480 --> 01:10:32.240]   and we're happy we had the learning experience as well.
[01:10:32.240 --> 01:10:33.280]   Obviously, particularly us,
[01:10:33.280 --> 01:10:35.280]   'cause this is the kind of thing that we do.
[01:10:35.280 --> 01:10:36.720]   And my guess is that's true
[01:10:36.720 --> 01:10:39.280]   of the vast majority of your audience.
[01:10:39.280 --> 01:10:41.440]   But that doesn't mean the,
[01:10:41.440 --> 01:10:43.280]   I'm standing in front of you telling you this
[01:10:43.280 --> 01:10:46.320]   is the thing that people are excited about.
[01:10:46.320 --> 01:10:48.960]   And that's why they want to be there,
[01:10:48.960 --> 01:10:50.800]   primarily why they want to be there.
[01:10:50.800 --> 01:10:53.440]   So to me, that's what COVID has forced us to deal with,
[01:10:53.440 --> 01:10:57.120]   even though I think we're still all in deep denial about it
[01:10:57.120 --> 01:10:59.840]   and hoping that it'll go back to that.
[01:10:59.840 --> 01:11:01.360]   And I think about 85% of it will.
[01:11:01.360 --> 01:11:02.160]   We'll be able to pretend
[01:11:02.160 --> 01:11:03.440]   that that's really the way it is again.
[01:11:03.440 --> 01:11:05.200]   And we'll forget the lessons of this.
[01:11:05.200 --> 01:11:07.280]   But technically what will come out of it
[01:11:07.280 --> 01:11:08.880]   or technologically what will come out of it
[01:11:08.880 --> 01:11:12.480]   is a way of providing a more dispersed experience
[01:11:12.480 --> 01:11:13.600]   through online education
[01:11:13.600 --> 01:11:15.760]   and these kinds of remote things that we've learned.
[01:11:15.760 --> 01:11:18.160]   And we'll have to come up with new ways to engage them
[01:11:18.160 --> 01:11:20.320]   in the experience of college,
[01:11:20.320 --> 01:11:21.840]   which includes not just the parties
[01:11:21.840 --> 01:11:23.440]   or the whatever kids do,
[01:11:23.440 --> 01:11:25.280]   but the learning part of it
[01:11:25.280 --> 01:11:27.920]   so that they actually come out four or five or six years later
[01:11:27.920 --> 01:11:30.720]   with having actually learned something.
[01:11:30.720 --> 01:11:33.840]   So I think the world will be radically different afterwards.
[01:11:33.840 --> 01:11:36.000]   And I think technology will matter for that,
[01:11:36.000 --> 01:11:38.240]   just not in the way that the people
[01:11:38.240 --> 01:11:40.640]   who were building the technology originally
[01:11:40.640 --> 01:11:41.920]   imagined it would be.
[01:11:41.920 --> 01:11:45.040]   And I think this would have been true even without COVID,
[01:11:45.040 --> 01:11:47.680]   but COVID has accelerated that reality.
[01:11:47.680 --> 01:11:50.000]   So it's happening in two or three years or five years
[01:11:50.000 --> 01:11:51.200]   as opposed to 10 or 15.
[01:11:51.200 --> 01:11:55.040]   - That was an amazing answer that I did not understand.
[01:11:55.040 --> 01:11:56.160]   - So-
[01:11:56.160 --> 01:11:57.520]   - It was passionate and-
[01:11:57.520 --> 01:11:58.640]   - Shots fired.
[01:11:58.640 --> 01:12:00.240]   - But I don't know, I just didn't,
[01:12:00.240 --> 01:12:01.280]   no, I'm not trying to criticize it.
[01:12:01.280 --> 01:12:03.120]   I think I'm, I don't think I'm getting it.
[01:12:03.120 --> 01:12:05.280]   So you mentioned disaggregation.
[01:12:05.280 --> 01:12:05.840]   So what's that?
[01:12:06.240 --> 01:12:09.280]   - Well, so, you know, the power of technology
[01:12:09.280 --> 01:12:11.440]   that if you go on the West Coast and hang out long enough
[01:12:11.440 --> 01:12:13.440]   is all about, we're gonna disaggregate these things together,
[01:12:13.440 --> 01:12:15.760]   the books from the bookstore, you know, that kind of a thing.
[01:12:15.760 --> 01:12:17.680]   And then suddenly Amazon controls the universe, right?
[01:12:17.680 --> 01:12:19.280]   And technology is a disruptor, right?
[01:12:19.280 --> 01:12:20.720]   And people have been predicting that
[01:12:20.720 --> 01:12:22.720]   for higher education for a long time,
[01:12:22.720 --> 01:12:23.360]   but certainly in the-
[01:12:23.360 --> 01:12:25.360]   - So is this the sort of idea like
[01:12:25.360 --> 01:12:30.000]   students can aggregate on a campus someplace
[01:12:30.000 --> 01:12:33.200]   and then take classes over the network anywhere?
[01:12:33.200 --> 01:12:34.720]   - Yeah, this is what people thought was gonna happen,
[01:12:34.720 --> 01:12:36.400]   or at least people claimed it was gonna happen, right?
[01:12:36.400 --> 01:12:38.720]   - 'Cause my daughter is essentially doing that now.
[01:12:38.720 --> 01:12:40.800]   She's on one campus, but learning in a different campus.
[01:12:40.800 --> 01:12:42.720]   - Sure, and COVID makes that possible, right?
[01:12:42.720 --> 01:12:44.080]   COVID makes that-
[01:12:44.080 --> 01:12:45.520]   - Legal.
[01:12:45.520 --> 01:12:46.880]   - All but avoidable, right?
[01:12:46.880 --> 01:12:47.360]   - All but avoidable.
[01:12:47.360 --> 01:12:49.280]   - But the idea originally was that, you know,
[01:12:49.280 --> 01:12:50.960]   you and I were gonna create this machine learning class
[01:12:50.960 --> 01:12:51.760]   and it was gonna be great.
[01:12:51.760 --> 01:12:52.560]   And then no one else would,
[01:12:52.560 --> 01:12:54.720]   there'd be the machine learning class everyone takes, right?
[01:12:54.720 --> 01:12:55.920]   That was never gonna happen.
[01:12:55.920 --> 01:12:57.200]   But, you know, something like that, you can see how-
[01:12:57.200 --> 01:12:58.480]   - But I feel like you didn't address that.
[01:12:58.480 --> 01:13:01.760]   So why, why, why isn't that, why?
[01:13:01.760 --> 01:13:04.000]   - I don't think that will be the thing that happens.
[01:13:04.000 --> 01:13:05.200]   - So the college experience,
[01:13:05.200 --> 01:13:07.120]   maybe I missed what the college experience was.
[01:13:07.120 --> 01:13:09.920]   I thought it was peers, like people hanging around.
[01:13:09.920 --> 01:13:11.520]   - A large part of it is peers.
[01:13:11.520 --> 01:13:13.120]   Well, it's peers and independence.
[01:13:13.120 --> 01:13:16.240]   - Yeah, but none of that, you can do classes online
[01:13:16.240 --> 01:13:16.800]   for all of that.
[01:13:16.800 --> 01:13:18.320]   - No, no, no, no.
[01:13:18.320 --> 01:13:20.560]   Because we're social people, right?
[01:13:20.560 --> 01:13:22.160]   - So when we take the classes,
[01:13:22.160 --> 01:13:24.240]   that also has to be part of an experience.
[01:13:24.240 --> 01:13:27.200]   - It's in a context and the context is the university.
[01:13:27.200 --> 01:13:28.720]   And by the way, it actually matters
[01:13:28.720 --> 01:13:31.760]   that Georgia Tech really is different from Brown.
[01:13:31.760 --> 01:13:35.920]   - I see, because then students can choose
[01:13:35.920 --> 01:13:37.040]   the kind of experience they think
[01:13:37.040 --> 01:13:38.080]   is gonna be best for them.
[01:13:38.080 --> 01:13:40.240]   - Okay, I think we're giving too much agency
[01:13:40.240 --> 01:13:42.160]   to the students in making an informed decision.
[01:13:42.160 --> 01:13:42.480]   - Okay.
[01:13:42.480 --> 01:13:45.280]   - But the truth, but yes, they will make choices
[01:13:45.280 --> 01:13:46.640]   and they will have different experiences.
[01:13:46.640 --> 01:13:48.480]   And some of those choices will be made for them.
[01:13:48.480 --> 01:13:49.840]   Some of them will be choices they're making
[01:13:49.840 --> 01:13:51.440]   'cause they think it's this, that, or the other.
[01:13:51.440 --> 01:13:53.520]   I just don't want to say, I don't want to give the idea-
[01:13:53.520 --> 01:13:54.480]   - It's not homogenous.
[01:13:54.480 --> 01:13:56.720]   - Yes, it's certainly not homogenous, right?
[01:13:56.720 --> 01:13:59.280]   I mean, Georgia Tech is different from Brown.
[01:13:59.280 --> 01:14:03.280]   Brown is different from, pick your favorite state school
[01:14:03.280 --> 01:14:05.600]   in Iowa, Iowa State, okay?
[01:14:05.600 --> 01:14:07.280]   Which I guess is my favorite state school in Iowa.
[01:14:07.280 --> 01:14:07.760]   - Sure.
[01:14:07.760 --> 01:14:09.520]   - But these are all different.
[01:14:09.520 --> 01:14:10.560]   They have different contexts.
[01:14:10.560 --> 01:14:13.360]   And a lot of those contexts are, they're about history, yes,
[01:14:13.360 --> 01:14:15.760]   but they're also about the location of where you are.
[01:14:15.760 --> 01:14:18.080]   They're about the larger group of people who are around you,
[01:14:18.080 --> 01:14:20.480]   whether you're in Athens, Georgia,
[01:14:20.480 --> 01:14:22.320]   and you're basically the only thing that's there
[01:14:22.320 --> 01:14:25.200]   as a university, you're responsible for all the jobs,
[01:14:25.200 --> 01:14:26.960]   or whether you're at Georgia State University,
[01:14:26.960 --> 01:14:30.000]   which is an urban campus where you're surrounded by,
[01:14:30.000 --> 01:14:32.480]   you know, 6 million people, and your campus,
[01:14:32.480 --> 01:14:33.760]   where it ends and begins in the city,
[01:14:33.760 --> 01:14:35.600]   ends and begins, we don't know.
[01:14:35.600 --> 01:14:37.120]   It actually matters whether you're a small campus
[01:14:37.120 --> 01:14:38.240]   or a large campus, these things matter.
[01:14:38.240 --> 01:14:41.520]   - Why is it that if you go to Georgia Tech,
[01:14:41.520 --> 01:14:44.720]   you're like forever proud of that?
[01:14:44.720 --> 01:14:47.280]   And you like say that to people at dinner,
[01:14:47.280 --> 01:14:48.560]   like bars and whatever.
[01:14:48.560 --> 01:14:53.840]   And if you, not to, you know, if you get a degree
[01:14:53.840 --> 01:14:56.480]   at an online university somewhere, you don't,
[01:14:56.480 --> 01:14:58.240]   that's not a thing that comes up at a bar.
[01:14:58.240 --> 01:14:59.680]   - Well, it's funny you say that.
[01:14:59.680 --> 01:15:02.320]   So the students who take our online masters,
[01:15:02.320 --> 01:15:06.400]   by several measures, are more loyal
[01:15:06.400 --> 01:15:07.680]   than the students who come on campus,
[01:15:07.680 --> 01:15:09.200]   certainly for the master's degree.
[01:15:09.200 --> 01:15:11.680]   The reason for that, I think, and you'd have to ask them,
[01:15:11.680 --> 01:15:13.520]   but based on my conversations with them,
[01:15:13.520 --> 01:15:15.360]   I feel comfortable saying this,
[01:15:15.360 --> 01:15:17.920]   is because this didn't exist before.
[01:15:17.920 --> 01:15:19.600]   I mean, we talk about this online masters
[01:15:19.600 --> 01:15:21.920]   and that it's reaching, you know, 11,000 students,
[01:15:21.920 --> 01:15:22.800]   and that's an amazing thing.
[01:15:22.800 --> 01:15:24.880]   And we're admitting everyone we believe who can succeed.
[01:15:24.880 --> 01:15:26.480]   We've got a 60% acceptance rate.
[01:15:26.480 --> 01:15:27.600]   It's amazing, right?
[01:15:27.600 --> 01:15:29.520]   It's also a $6,600 degree.
[01:15:29.520 --> 01:15:31.840]   The entire degree costs 6,600 or 7,000,
[01:15:31.840 --> 01:15:34.320]   depending on how long you take, a dollar degree,
[01:15:34.320 --> 01:15:36.480]   as opposed to the 46,000 it costs you to come on campus.
[01:15:36.480 --> 01:15:40.400]   So that feels, and I can do it while I'm working full time,
[01:15:40.400 --> 01:15:42.080]   and I've got a family and a mortgage
[01:15:42.080 --> 01:15:43.280]   and all these other things.
[01:15:43.280 --> 01:15:46.080]   So it's an opportunity to do something you wanted to do,
[01:15:46.080 --> 01:15:47.520]   but you didn't think was possible
[01:15:47.520 --> 01:15:50.160]   without giving up two years of your life,
[01:15:50.160 --> 01:15:51.760]   as well as all the money and everything else
[01:15:51.760 --> 01:15:53.040]   in the life that you had built.
[01:15:53.040 --> 01:15:56.800]   So I think we created something that's had an impact,
[01:15:56.800 --> 01:15:59.520]   but importantly, we gave a set of people opportunities
[01:15:59.520 --> 01:16:00.640]   they otherwise didn't feel they had.
[01:16:00.640 --> 01:16:02.720]   So I think people feel very loyal about that.
[01:16:02.720 --> 01:16:04.000]   And my biggest piece of evidence for that,
[01:16:04.000 --> 01:16:07.040]   besides the surveys, is that we have somewhere north
[01:16:07.040 --> 01:16:09.920]   of 80 students, might be 100 at this point,
[01:16:09.920 --> 01:16:14.880]   who graduated, but come back in TA for this class
[01:16:14.880 --> 01:16:16.640]   for basically minimum wage,
[01:16:16.640 --> 01:16:17.840]   even though they're working full time
[01:16:17.840 --> 01:16:21.760]   because they believe in sort of having that opportunity
[01:16:21.760 --> 01:16:23.200]   and they wanna be a part of something.
[01:16:23.200 --> 01:16:25.840]   Now, will generation three feel this way?
[01:16:25.840 --> 01:16:27.920]   15 years from now, will people have that same sense?
[01:16:27.920 --> 01:16:28.400]   I don't know.
[01:16:28.400 --> 01:16:31.040]   But right now, they kind of do.
[01:16:31.040 --> 01:16:32.800]   And so it's not the online,
[01:16:32.800 --> 01:16:35.920]   it's a matter of feeling as if you're a part of something.
[01:16:35.920 --> 01:16:37.120]   Right, we're all very tribal.
[01:16:37.120 --> 01:16:37.680]   - Yeah.
[01:16:37.680 --> 01:16:38.080]   - Right?
[01:16:38.080 --> 01:16:42.000]   And I think there's something very tribal
[01:16:42.000 --> 01:16:44.160]   about being a part of something like that.
[01:16:44.160 --> 01:16:45.760]   Being on campus makes that easier.
[01:16:45.760 --> 01:16:48.080]   Going through a shared experience makes that easier.
[01:16:48.080 --> 01:16:49.600]   It's harder to have that shared experience
[01:16:49.600 --> 01:16:51.920]   if you're alone looking at a computer screen.
[01:16:51.920 --> 01:16:53.200]   We can create ways to make that--
[01:16:53.200 --> 01:16:53.920]   - But is it possible?
[01:16:53.920 --> 01:16:54.800]   - It is possible.
[01:16:54.800 --> 01:16:57.440]   - The question is, it still is the intuition to me,
[01:16:57.440 --> 01:17:01.360]   and it was at the beginning when I saw something
[01:17:01.360 --> 01:17:03.760]   like the online master's program
[01:17:03.760 --> 01:17:07.280]   is that this is gonna replace universities.
[01:17:07.280 --> 01:17:09.280]   - No, it won't replace universities, but it will--
[01:17:09.280 --> 01:17:10.400]   - But like, why is it, why?
[01:17:10.400 --> 01:17:12.400]   - Because it's living in a different part
[01:17:12.400 --> 01:17:13.760]   of the ecosystem, right?
[01:17:13.760 --> 01:17:15.520]   The people who are taking it are already adults.
[01:17:15.520 --> 01:17:17.840]   They've gone through their undergrad experience.
[01:17:18.560 --> 01:17:21.040]   I think their goals have shifted from when they were 17.
[01:17:21.040 --> 01:17:23.120]   They have other things that are going on.
[01:17:23.120 --> 01:17:23.280]   - Right.
[01:17:23.280 --> 01:17:25.360]   - But it does do something really important,
[01:17:25.360 --> 01:17:27.120]   something very social and very important, right?
[01:17:27.120 --> 01:17:30.080]   You know this whole thing about, you know,
[01:17:30.080 --> 01:17:31.920]   don't build the sidewalks, just leave the grass,
[01:17:31.920 --> 01:17:33.440]   and the students will, or the people will walk,
[01:17:33.440 --> 01:17:35.200]   and you put the sidewalks where they create paths,
[01:17:35.200 --> 01:17:35.920]   this kind of thing.
[01:17:35.920 --> 01:17:36.400]   - That's interesting, yeah.
[01:17:36.400 --> 01:17:38.880]   - There are architects who apparently believe
[01:17:38.880 --> 01:17:39.920]   that's the right way to do things.
[01:17:39.920 --> 01:17:45.120]   The metaphor here is that we created this environment.
[01:17:45.120 --> 01:17:48.000]   We didn't quite know how to think about the social aspect,
[01:17:48.640 --> 01:17:50.320]   but, you know, we didn't have time to solve all,
[01:17:50.320 --> 01:17:52.240]   do all the social engineering, right?
[01:17:52.240 --> 01:17:54.080]   The students did it themselves.
[01:17:54.080 --> 01:17:57.440]   They created, you know, these groups,
[01:17:57.440 --> 01:17:59.680]   like on Google+, there were like 30-something groups
[01:17:59.680 --> 01:18:03.200]   created in the first year because somebody had used Google+.
[01:18:03.200 --> 01:18:05.600]   And they created these groups,
[01:18:05.600 --> 01:18:07.200]   and they divided up in ways that made sense.
[01:18:07.200 --> 01:18:08.080]   We live in the same state,
[01:18:08.080 --> 01:18:09.120]   or we're working on the same things,
[01:18:09.120 --> 01:18:10.480]   or we have the same background, or whatever.
[01:18:10.480 --> 01:18:11.920]   And they created these social things.
[01:18:11.920 --> 01:18:13.920]   We sent them T-shirts, and they wear,
[01:18:13.920 --> 01:18:16.400]   we have all these great pictures of students
[01:18:16.400 --> 01:18:18.240]   putting on their T-shirts as they travel around the world.
[01:18:18.240 --> 01:18:20.400]   I climbed this mountaintop, I'm putting this T-shirt on,
[01:18:20.400 --> 01:18:21.120]   I'm a part of this.
[01:18:21.120 --> 01:18:22.000]   They were a part of them.
[01:18:22.000 --> 01:18:24.560]   They created the social environment
[01:18:24.560 --> 01:18:27.280]   on top of the social network and the social media that existed
[01:18:27.280 --> 01:18:29.280]   to create this sense of belonging
[01:18:29.280 --> 01:18:30.400]   and being a part of something.
[01:18:30.400 --> 01:18:32.640]   They found a way to do it, right?
[01:18:32.640 --> 01:18:34.960]   And I think they had other,
[01:18:34.960 --> 01:18:38.000]   it scratched an itch that they had,
[01:18:38.000 --> 01:18:40.000]   but they had scratched some of that itch
[01:18:40.000 --> 01:18:41.360]   that might've required they be physically
[01:18:41.360 --> 01:18:44.320]   in the same place long before, right?
[01:18:44.320 --> 01:18:47.040]   So I think, yes, it's possible,
[01:18:47.040 --> 01:18:49.440]   and it's more than possible, it's necessary.
[01:18:49.440 --> 01:18:54.080]   But I don't think it's going to replace the university
[01:18:54.080 --> 01:18:54.960]   as we know it.
[01:18:54.960 --> 01:18:56.640]   The university as we know it will change.
[01:18:56.640 --> 01:18:58.960]   But there's just a lot of power
[01:18:58.960 --> 01:19:00.240]   in the kind of rite of passage
[01:19:00.240 --> 01:19:01.360]   you're kind of going off to yourself.
[01:19:01.360 --> 01:19:02.960]   Now, maybe there'll be some other rite of passage
[01:19:02.960 --> 01:19:03.520]   that'll happen.
[01:19:03.520 --> 01:19:04.320]   - Right, that's the question.
[01:19:04.320 --> 01:19:05.200]   - That'll drive us somewhere else.
[01:19:05.200 --> 01:19:05.920]   - You can separate.
[01:19:05.920 --> 01:19:11.120]   So the university is such a fascinating mess of things.
[01:19:11.120 --> 01:19:14.480]   So just even the faculty position is a fascinating mess.
[01:19:14.480 --> 01:19:15.760]   Like it doesn't make any sense.
[01:19:15.760 --> 01:19:17.520]   It's stabilized itself.
[01:19:17.520 --> 01:19:20.960]   But like why are the world-class researchers
[01:19:20.960 --> 01:19:25.040]   spending a huge amount of time,
[01:19:25.040 --> 01:19:27.760]   of their time teaching and service?
[01:19:27.760 --> 01:19:29.200]   Like you're doing like three jobs.
[01:19:29.200 --> 01:19:29.700]   - Yeah.
[01:19:29.700 --> 01:19:33.280]   - And I mean, it turns,
[01:19:33.280 --> 01:19:35.680]   it's maybe an accident of history or human evolution.
[01:19:35.680 --> 01:19:36.240]   I don't know.
[01:19:36.240 --> 01:19:38.320]   It seems like the people who are really good at teaching
[01:19:38.320 --> 01:19:40.320]   are often really good at research.
[01:19:40.320 --> 01:19:42.080]   There seems to be a parallel there.
[01:19:42.080 --> 01:19:44.320]   But like it doesn't make any sense
[01:19:44.320 --> 01:19:45.360]   that you should be doing that.
[01:19:45.360 --> 01:19:47.920]   At the same time, it also doesn't seem to make sense
[01:19:47.920 --> 01:19:51.680]   that your place where you party
[01:19:51.680 --> 01:19:56.240]   is the same place where you go to learn calculus
[01:19:56.240 --> 01:19:56.800]   or whatever.
[01:19:56.800 --> 01:19:58.320]   - But it's a safe space.
[01:19:58.320 --> 01:20:00.240]   - Safe space for everything.
[01:20:00.240 --> 01:20:02.240]   - Yeah, relatively speaking, it's a safe space.
[01:20:02.240 --> 01:20:05.120]   Now, by the way, I feel the need very strongly
[01:20:05.120 --> 01:20:07.040]   to point out that we are living
[01:20:07.040 --> 01:20:09.440]   in a very particular weird bubble, right?
[01:20:09.440 --> 01:20:10.720]   Most people don't go to college.
[01:20:10.720 --> 01:20:12.720]   And by the way, the ones who do go to college,
[01:20:12.720 --> 01:20:14.160]   they're not 18 years old, right?
[01:20:14.160 --> 01:20:15.360]   They're like 25 or something.
[01:20:15.360 --> 01:20:16.240]   I forget the numbers.
[01:20:16.240 --> 01:20:19.520]   You know, the places where we've been, where we are,
[01:20:19.520 --> 01:20:22.240]   they look like whatever we think
[01:20:22.240 --> 01:20:25.440]   the traditional movie version of universities are.
[01:20:25.440 --> 01:20:27.280]   But for most people, it's not that way at all.
[01:20:27.280 --> 01:20:28.800]   By the way, most people who drop out of college,
[01:20:28.800 --> 01:20:31.120]   it's entirely for financial reasons, right?
[01:20:31.120 --> 01:20:34.880]   So, you know, we were talking about a particular experience.
[01:20:34.880 --> 01:20:38.560]   And so for that set of people,
[01:20:38.560 --> 01:20:42.800]   which is very small, but larger than it was a decade
[01:20:42.800 --> 01:20:44.640]   or two or three or four, certainly ago,
[01:20:44.640 --> 01:20:47.120]   I don't think that will change.
[01:20:47.120 --> 01:20:50.320]   My concern, which I think is kind of implicit
[01:20:50.320 --> 01:20:51.440]   in some of these questions,
[01:20:51.440 --> 01:20:54.000]   is that somehow we will divide the world up further
[01:20:54.000 --> 01:20:56.880]   into the people who get to have this experience
[01:20:56.880 --> 01:20:57.760]   and get to have the network
[01:20:57.760 --> 01:20:59.200]   and they sort of benefit from it
[01:20:59.200 --> 01:21:01.760]   and everyone else while increasingly requiring
[01:21:01.760 --> 01:21:03.040]   that they have more and more credentials
[01:21:03.040 --> 01:21:05.760]   in order to get a job as a barista, right?
[01:21:05.760 --> 01:21:06.880]   You gotta have a master's degree
[01:21:06.880 --> 01:21:08.800]   in order to work at Starbucks.
[01:21:08.800 --> 01:21:10.560]   We're gonna force people to do these things,
[01:21:10.560 --> 01:21:12.320]   but they're not gonna get to have that experience.
[01:21:12.320 --> 01:21:13.680]   And there'll be a small group of people who do
[01:21:13.680 --> 01:21:15.520]   who continue to, you know, positive feedback,
[01:21:15.520 --> 01:21:16.800]   look at it, et cetera, et cetera, et cetera.
[01:21:16.800 --> 01:21:19.360]   I worry a lot about that, which is why for me,
[01:21:19.360 --> 01:21:21.840]   and by the way, here's an answer
[01:21:21.840 --> 01:21:22.720]   to your question about faculty,
[01:21:22.720 --> 01:21:24.240]   which is why to me that you have to focus
[01:21:24.240 --> 01:21:25.920]   on access and the mission.
[01:21:25.920 --> 01:21:28.000]   I think the reason, whether it's good, bad or strange,
[01:21:28.000 --> 01:21:29.520]   I mean, I agree it's strange,
[01:21:29.520 --> 01:21:32.000]   but I think it's useful to have the faculty member,
[01:21:32.000 --> 01:21:33.760]   particularly at large R1 universities
[01:21:33.760 --> 01:21:35.600]   where we've all had experiences
[01:21:36.480 --> 01:21:40.880]   that you tie what they get to do
[01:21:40.880 --> 01:21:43.760]   and with the fundamental mission of the university
[01:21:43.760 --> 01:21:44.880]   and let the mission drive.
[01:21:44.880 --> 01:21:46.880]   What I hear when I talk to faculty is
[01:21:46.880 --> 01:21:48.160]   they love their PhD students
[01:21:48.160 --> 01:21:50.720]   because they're creating, they're reproducing basically,
[01:21:50.720 --> 01:21:52.720]   right, and it lets them do their research and multiply.
[01:21:52.720 --> 01:21:57.120]   But they understand that the mission is the undergrads.
[01:21:57.120 --> 01:22:00.240]   And so they will do it without complaint mostly
[01:22:00.240 --> 01:22:02.320]   because it's a part of the mission and why they're here.
[01:22:02.320 --> 01:22:04.000]   And they have experiences with it themselves.
[01:22:04.000 --> 01:22:06.160]   And it was important to get them
[01:22:06.160 --> 01:22:07.200]   where they were going.
[01:22:07.200 --> 01:22:08.880]   The people who tend to get squeezed in that, by the way,
[01:22:08.880 --> 01:22:10.160]   are the master's students, right,
[01:22:10.160 --> 01:22:12.000]   who are neither the PhDs who are like us
[01:22:12.000 --> 01:22:14.640]   nor the undergrads we have already bought into the idea
[01:22:14.640 --> 01:22:16.320]   that we have to teach though.
[01:22:16.320 --> 01:22:18.000]   That's increasingly changing.
[01:22:18.000 --> 01:22:20.880]   Anyway, I think tying that mission in really matters.
[01:22:20.880 --> 01:22:23.120]   And it gives you a way to unify people
[01:22:23.120 --> 01:22:26.000]   around making it an actual higher calling.
[01:22:26.000 --> 01:22:28.000]   Education feels like more of a higher calling to me
[01:22:28.000 --> 01:22:29.360]   than even research.
[01:22:29.360 --> 01:22:32.560]   Because education, you cannot treat it as a hobby
[01:22:32.560 --> 01:22:34.640]   if you're going to do it well.
[01:22:34.640 --> 01:22:38.320]   - But that's the pushback on this whole system
[01:22:38.320 --> 01:22:42.480]   is that you should, education should be a full-time job.
[01:22:42.480 --> 01:22:46.000]   Right, and like, it's almost like research
[01:22:46.000 --> 01:22:48.080]   is a distraction from that.
[01:22:48.080 --> 01:22:51.120]   - Yes, although I think most of our colleagues,
[01:22:51.120 --> 01:22:53.200]   many of our colleagues would say that research is the job
[01:22:53.200 --> 01:22:54.800]   and education is the distraction.
[01:22:54.800 --> 01:22:56.480]   - Right, but that's the beautiful dance.
[01:22:56.480 --> 01:22:58.880]   It seems to be that that tension in itself
[01:22:58.880 --> 01:23:03.280]   seems to work, seems to bring out the best
[01:23:03.280 --> 01:23:07.040]   in the faculty or like the--
[01:23:07.040 --> 01:23:08.160]   - But I will point out two things.
[01:23:08.160 --> 01:23:09.040]   One thing I'm going to point out,
[01:23:09.040 --> 01:23:10.560]   and the other thing I want Michael to point out
[01:23:10.560 --> 01:23:11.760]   because I think Michael is much closer
[01:23:11.760 --> 01:23:16.880]   to sort of the ideal professor in some sense than I am.
[01:23:16.880 --> 01:23:17.440]   - Well, he is the dean.
[01:23:17.440 --> 01:23:19.360]   - You're the platonic sense of a professor.
[01:23:19.360 --> 01:23:20.720]   - Yeah, I don't know what he meant by that,
[01:23:20.720 --> 01:23:23.120]   but he is a dean, so he has a different experience.
[01:23:23.120 --> 01:23:26.080]   - I'm giving him time to think of the profound thing
[01:23:26.080 --> 01:23:26.400]   he's going to say.
[01:23:26.400 --> 01:23:26.800]   - That's good.
[01:23:26.800 --> 01:23:29.520]   - But let me point this out, which is that
[01:23:29.520 --> 01:23:33.120]   we have lecturers in the College of Computing where I am.
[01:23:33.120 --> 01:23:35.520]   There's 10 or 12 of them depending on how you count
[01:23:35.520 --> 01:23:38.960]   as opposed to the 90 or so tenure track faculty.
[01:23:38.960 --> 01:23:40.960]   Those 10 lecturers who only teach,
[01:23:40.960 --> 01:23:42.720]   well, they don't only teach, they also do service.
[01:23:42.720 --> 01:23:43.840]   Some of them do research as well,
[01:23:43.840 --> 01:23:45.360]   but primarily they teach.
[01:23:45.360 --> 01:23:49.360]   They teach 50%, over 50% of our credit hours,
[01:23:49.360 --> 01:23:51.120]   and we teach everybody, right?
[01:23:51.120 --> 01:23:53.920]   So they're doing not just,
[01:23:53.920 --> 01:23:55.680]   they're doing more than eight times the work
[01:23:55.680 --> 01:23:58.000]   of the tenure track faculty
[01:23:58.000 --> 01:24:00.880]   by just more closer to nine or 10.
[01:24:00.880 --> 01:24:02.880]   And that's including our grad courses, right?
[01:24:02.880 --> 01:24:04.960]   So they're doing this, they're teaching more,
[01:24:04.960 --> 01:24:06.960]   they're touching more than anyone,
[01:24:06.960 --> 01:24:08.720]   and they're beloved for it.
[01:24:08.720 --> 01:24:10.640]   I mean, so we recently had a survey.
[01:24:10.640 --> 01:24:12.720]   Everyone does these alumni surveys.
[01:24:12.720 --> 01:24:14.480]   You hire someone from the outside to do whatever,
[01:24:14.480 --> 01:24:15.680]   and I was really struck by something.
[01:24:15.680 --> 01:24:17.200]   You saw all these really cool numbers.
[01:24:17.200 --> 01:24:18.080]   I'm not going to talk about it
[01:24:18.080 --> 01:24:19.680]   because it's all internal confidential stuff.
[01:24:19.680 --> 01:24:21.040]   But one thing I will talk about
[01:24:21.040 --> 01:24:23.040]   is there was a single question we asked our alumni.
[01:24:23.040 --> 01:24:24.240]   These are people who graduated,
[01:24:24.240 --> 01:24:25.680]   born in the 30s and 40s,
[01:24:25.680 --> 01:24:27.760]   all the way up to people who graduated last week, right?
[01:24:27.760 --> 01:24:30.320]   Well, last semester.
[01:24:30.320 --> 01:24:30.880]   - Okay, good.
[01:24:30.880 --> 01:24:33.920]   - Yeah, yeah, time flies.
[01:24:33.920 --> 01:24:36.320]   - And it was the question,
[01:24:36.320 --> 01:24:39.600]   name a single person who had a strong,
[01:24:39.600 --> 01:24:41.360]   positive impact on you, something like that.
[01:24:41.360 --> 01:24:44.080]   - I think it was special impact.
[01:24:44.080 --> 01:24:45.440]   - Yeah, special impact on you.
[01:24:45.440 --> 01:24:47.040]   And then, so they got all the answers from people
[01:24:47.040 --> 01:24:48.160]   and they created a word cloud.
[01:24:48.160 --> 01:24:50.400]   It was clearly a word cloud created by people
[01:24:50.400 --> 01:24:52.080]   who don't do word clouds for a living
[01:24:52.080 --> 01:24:54.640]   'cause they had one person whose name like appeared
[01:24:54.640 --> 01:24:56.080]   like nine different times,
[01:24:56.080 --> 01:24:59.120]   like Philip, Phil, Dr. Phil, you know, but whatever.
[01:24:59.120 --> 01:25:00.000]   But they got all this.
[01:25:00.000 --> 01:25:02.240]   And I looked at it and I noticed something really cool.
[01:25:02.240 --> 01:25:06.560]   The five people from the College of Computing,
[01:25:06.560 --> 01:25:08.320]   I recognized were in that cloud.
[01:25:08.320 --> 01:25:13.520]   And four of them were lecturers,
[01:25:13.520 --> 01:25:14.560]   the people who teach.
[01:25:14.560 --> 01:25:17.200]   Two of them relatively modern,
[01:25:17.200 --> 01:25:19.680]   both were chairs of our division of computing instruction.
[01:25:19.680 --> 01:25:22.160]   One just, one retired, one is gonna retire soon.
[01:25:22.160 --> 01:25:24.400]   And the other two were lecturers I remembered
[01:25:24.400 --> 01:25:25.520]   from the 1980s.
[01:25:25.520 --> 01:25:27.840]   Two of those four actually have--
[01:25:27.840 --> 01:25:29.440]   - By the way, the fifth person was Charles.
[01:25:29.440 --> 01:25:30.160]   - That's not important.
[01:25:30.160 --> 01:25:32.800]   I don't tell people that.
[01:25:32.800 --> 01:25:34.400]   But the two of those people
[01:25:34.400 --> 01:25:35.840]   are teaching awards are named after.
[01:25:35.840 --> 01:25:36.560]   Thank you, Michael.
[01:25:36.560 --> 01:25:39.520]   Two of those are teaching awards are named after, right?
[01:25:39.520 --> 01:25:41.600]   So when you ask students, alumni,
[01:25:41.600 --> 01:25:44.080]   people who are now 60, 70 years old even,
[01:25:44.080 --> 01:25:45.120]   you know, who touched them?
[01:25:45.120 --> 01:25:46.480]   They say the Dean of Students.
[01:25:46.480 --> 01:25:48.080]   They say the big teachers who taught
[01:25:48.080 --> 01:25:50.080]   the big introductory classes that got me into it.
[01:25:50.080 --> 01:25:52.160]   There's a guy named Richard Barker's on there
[01:25:52.160 --> 01:25:55.120]   who's known as a great teacher.
[01:25:55.120 --> 01:25:56.640]   The Phil Adler guy who,
[01:25:58.320 --> 01:25:59.920]   I probably just said his last name wrong,
[01:25:59.920 --> 01:26:00.880]   but I know the first name's Phil
[01:26:00.880 --> 01:26:02.240]   'cause it kept showing up over and over again.
[01:26:02.240 --> 01:26:04.320]   - Adler is what it said.
[01:26:04.320 --> 01:26:04.720]   - Okay, good.
[01:26:04.720 --> 01:26:06.640]   - But different people spelled it differently.
[01:26:06.640 --> 01:26:07.760]   So he appeared multiple times.
[01:26:07.760 --> 01:26:08.240]   - Right.
[01:26:08.240 --> 01:26:10.320]   So he was a, clearly,
[01:26:10.320 --> 01:26:12.640]   he was a professor in the business school.
[01:26:12.640 --> 01:26:15.680]   But when you read about him,
[01:26:15.680 --> 01:26:17.280]   I went to read about him 'cause I was curious who he was,
[01:26:17.280 --> 01:26:18.480]   you know, it's all about his teaching
[01:26:18.480 --> 01:26:19.920]   and the students that he touched, right?
[01:26:19.920 --> 01:26:22.080]   So whatever it is that we're doing
[01:26:22.080 --> 01:26:23.280]   and we think we're doing that's important
[01:26:23.280 --> 01:26:25.120]   or why we think the universities function,
[01:26:25.120 --> 01:26:26.320]   the people who go through it,
[01:26:27.520 --> 01:26:29.280]   they remember the people who were kind to them,
[01:26:29.280 --> 01:26:31.200]   the people who taught them something.
[01:26:31.200 --> 01:26:32.240]   And they do remember it.
[01:26:32.240 --> 01:26:33.280]   They remember it later.
[01:26:33.280 --> 01:26:35.600]   I think that's important.
[01:26:35.600 --> 01:26:36.720]   That's why the mission matters.
[01:26:36.720 --> 01:26:37.600]   - Yeah.
[01:26:37.600 --> 01:26:41.760]   Not to completely lose track of the fundamental problem
[01:26:41.760 --> 01:26:46.800]   of how do we replace the party aspect of universities.
[01:26:46.800 --> 01:26:47.920]   - That's right.
[01:26:47.920 --> 01:26:50.960]   - Before we go to what makes the platonic professor,
[01:26:50.960 --> 01:26:53.920]   do you think,
[01:26:55.280 --> 01:26:57.840]   like what in your sense is the role of MOOCs
[01:26:57.840 --> 01:26:59.680]   in this whole picture during COVID?
[01:26:59.680 --> 01:27:04.080]   Like are we, should we desperately be clamoring
[01:27:04.080 --> 01:27:05.760]   to get back on campus
[01:27:05.760 --> 01:27:08.320]   or is this a stable place to be for a little while?
[01:27:08.320 --> 01:27:09.200]   - I don't know.
[01:27:09.200 --> 01:27:09.840]   I know that it's,
[01:27:09.840 --> 01:27:12.480]   that the online teaching experience
[01:27:12.480 --> 01:27:15.840]   and learning experience has been really rough.
[01:27:15.840 --> 01:27:18.160]   I think that people find it to be a struggle
[01:27:18.160 --> 01:27:21.840]   in a way that's not a happy, positive struggle.
[01:27:21.840 --> 01:27:22.960]   That when you got through it,
[01:27:22.960 --> 01:27:24.640]   you just feel like glad that it's over
[01:27:24.640 --> 01:27:26.720]   as opposed to I've achieved something.
[01:27:26.720 --> 01:27:29.520]   So, I worry about that.
[01:27:29.520 --> 01:27:33.680]   But I worry about just even before this happened,
[01:27:33.680 --> 01:27:35.200]   I worry about lecture teaching
[01:27:35.200 --> 01:27:38.560]   as how well is that actually really working
[01:27:38.560 --> 01:27:40.560]   as far as a way to do education,
[01:27:40.560 --> 01:27:42.800]   as a way to inspire people.
[01:27:42.800 --> 01:27:46.880]   I mean, all the data that I'm aware of seems to indicate,
[01:27:46.880 --> 01:27:49.600]   and this kind of fits, I think, with Charles's story,
[01:27:49.600 --> 01:27:54.320]   is that people respond to connection, right?
[01:27:54.320 --> 01:27:55.120]   They actually feel,
[01:27:55.120 --> 01:27:58.960]   if they feel connected to the person teaching the class,
[01:27:58.960 --> 01:28:00.480]   they're more likely to go along with it.
[01:28:00.480 --> 01:28:02.800]   They're more able to retain information.
[01:28:02.800 --> 01:28:04.880]   They're more motivated to be involved
[01:28:04.880 --> 01:28:06.000]   in the class in some way.
[01:28:06.000 --> 01:28:09.120]   And that really matters.
[01:28:09.120 --> 01:28:11.760]   - You mean to the human themselves.
[01:28:11.760 --> 01:28:12.320]   - Yeah.
[01:28:12.320 --> 01:28:14.480]   - So, can't you do that actually,
[01:28:14.480 --> 01:28:18.160]   perhaps more effectively online?
[01:28:18.160 --> 01:28:20.160]   Like you mentioned science communication.
[01:28:20.160 --> 01:28:24.800]   So, I literally, I think, learned linear algebra
[01:28:24.800 --> 01:28:28.080]   from Gilbert Strang by watching MIT OpenCourseWare
[01:28:28.080 --> 01:28:28.880]   when I was in track.
[01:28:28.880 --> 01:28:30.880]   And he was a personality.
[01:28:30.880 --> 01:28:33.040]   He was a bit like a tiny,
[01:28:33.040 --> 01:28:34.960]   in this tiny little world of math,
[01:28:34.960 --> 01:28:36.400]   he's a bit of a rockstar, right?
[01:28:36.400 --> 01:28:40.640]   So, you kind of look up to that person.
[01:28:40.640 --> 01:28:44.560]   Can't that replace the in-person education?
[01:28:44.560 --> 01:28:45.520]   - It can help.
[01:28:45.520 --> 01:28:46.480]   I will point out something.
[01:28:46.480 --> 01:28:47.360]   I can't share the numbers,
[01:28:47.360 --> 01:28:49.840]   but we have surveyed our students
[01:28:49.840 --> 01:28:51.280]   and even though they have feelings
[01:28:51.280 --> 01:28:54.160]   about what I would interpret as connection,
[01:28:54.160 --> 01:28:54.960]   I like that word,
[01:28:54.960 --> 01:28:57.520]   in the different modes of classrooms,
[01:28:57.520 --> 01:28:59.520]   there's no difference between
[01:28:59.520 --> 01:29:01.600]   how well they think they're learning.
[01:29:01.600 --> 01:29:04.720]   For them, the thing that makes them unhappy
[01:29:04.720 --> 01:29:06.800]   is the situation they're in.
[01:29:06.800 --> 01:29:08.560]   And I think the last lack of connection,
[01:29:08.560 --> 01:29:10.400]   it's not whether they're learning anything.
[01:29:10.400 --> 01:29:12.160]   They seem to think they're learning something anyway.
[01:29:12.160 --> 01:29:13.200]   All right.
[01:29:13.200 --> 01:29:14.240]   In fact, they seem to think
[01:29:14.240 --> 01:29:15.440]   they're learning it equally well,
[01:29:15.440 --> 01:29:19.120]   presumably because the faculty
[01:29:19.120 --> 01:29:21.920]   are putting in, or the instructors,
[01:29:21.920 --> 01:29:22.880]   more generally speaking,
[01:29:22.880 --> 01:29:25.760]   are putting in the energy and effort
[01:29:25.760 --> 01:29:28.800]   to try to make certain that what they've curated
[01:29:28.800 --> 01:29:30.480]   can be expressed to them in a useful way.
[01:29:30.480 --> 01:29:31.680]   But the connection is missing.
[01:29:31.680 --> 01:29:34.000]   And so there's huge differences in what they prefer.
[01:29:34.000 --> 01:29:34.800]   And as far as I can tell,
[01:29:34.800 --> 01:29:37.200]   what they prefer is more connection, not less.
[01:29:37.200 --> 01:29:38.480]   That connection just doesn't have to be
[01:29:38.480 --> 01:29:40.000]   physically in a classroom.
[01:29:40.000 --> 01:29:43.280]   I mean, look, I used to teach 348 students
[01:29:43.280 --> 01:29:44.640]   on a machine learning class on campus.
[01:29:44.640 --> 01:29:45.200]   Do you know why?
[01:29:45.200 --> 01:29:46.640]   'Cause that was the biggest classroom on campus.
[01:29:48.640 --> 01:29:50.720]   They're sitting in theater seats.
[01:29:50.720 --> 01:29:53.440]   I'm literally on a stage looking down on them
[01:29:53.440 --> 01:29:55.600]   and talking to them, right?
[01:29:55.600 --> 01:29:59.440]   There's no, I mean, we're not sitting down
[01:29:59.440 --> 01:30:01.200]   having a one-on-one conversation,
[01:30:01.200 --> 01:30:02.560]   reading each other's body language,
[01:30:02.560 --> 01:30:03.920]   trying to communicate and going,
[01:30:03.920 --> 01:30:05.200]   we're not doing any of that.
[01:30:05.200 --> 01:30:07.280]   So if you're past the third row,
[01:30:07.280 --> 01:30:08.640]   it might as well be online anyway,
[01:30:08.640 --> 01:30:10.160]   is the kind of thing that people have said.
[01:30:10.160 --> 01:30:12.160]   Daphne has actually said some version of this,
[01:30:12.160 --> 01:30:14.320]   that online starts on the third row
[01:30:14.320 --> 01:30:15.440]   or something like that.
[01:30:15.440 --> 01:30:18.400]   And I think that's not, yeah, I like it.
[01:30:18.400 --> 01:30:20.160]   I think it captures something important.
[01:30:20.160 --> 01:30:22.000]   But people still came, by the way.
[01:30:22.000 --> 01:30:23.760]   Even the people who had access to our material
[01:30:23.760 --> 01:30:24.640]   would still come to class.
[01:30:24.640 --> 01:30:26.320]   - I mean, there's a certain element
[01:30:26.320 --> 01:30:27.840]   about looking to the person next to you.
[01:30:27.840 --> 01:30:28.160]   - Yeah.
[01:30:28.160 --> 01:30:30.080]   - It's just like their presence there,
[01:30:30.080 --> 01:30:34.880]   their boredom, and like when the parts are boring
[01:30:34.880 --> 01:30:37.200]   and their excitement when the parts are exciting,
[01:30:37.200 --> 01:30:39.440]   like in sharing in that,
[01:30:39.440 --> 01:30:43.200]   like unspoken kind of, yeah, communication.
[01:30:43.200 --> 01:30:44.800]   Like in part, the connection is
[01:30:44.800 --> 01:30:46.080]   with the other people in the room.
[01:30:46.080 --> 01:30:46.480]   - Yeah.
[01:30:46.480 --> 01:30:51.920]   - Watching the circus on TV alone is not really.
[01:30:51.920 --> 01:30:52.160]   (laughing)
[01:30:52.160 --> 01:30:53.280]   Have you ever been to a movie theater
[01:30:53.280 --> 01:30:55.120]   and been the only one there at a comedy?
[01:30:55.120 --> 01:30:58.080]   It's not as funny as when you're in a room
[01:30:58.080 --> 01:30:59.360]   full of people all laughing.
[01:30:59.360 --> 01:31:02.320]   - Well, you need, maybe you need just another person.
[01:31:02.320 --> 01:31:04.480]   It's like, as opposed to many.
[01:31:04.480 --> 01:31:05.760]   Maybe there's some kind of--
[01:31:05.760 --> 01:31:07.200]   - Well, there's different kinds of connection, right?
[01:31:07.200 --> 01:31:08.720]   - And there's different kinds of comedy.
[01:31:08.720 --> 01:31:11.520]   (laughing)
[01:31:11.520 --> 01:31:12.240]   Well, in the sense that--
[01:31:12.240 --> 01:31:13.200]   - As we're learning today.
[01:31:13.200 --> 01:31:14.720]   (laughing)
[01:31:14.720 --> 01:31:16.000]   - I wasn't sure if that was gonna land.
[01:31:16.000 --> 01:31:17.120]   (laughing)
[01:31:17.120 --> 01:31:20.560]   Just the idea that different jokes,
[01:31:20.560 --> 01:31:22.960]   I've now done a little bit of standup.
[01:31:22.960 --> 01:31:26.160]   And so different jokes work in different size crowds too.
[01:31:26.160 --> 01:31:26.880]   - No, that's true.
[01:31:26.880 --> 01:31:30.080]   - Where sometimes if it's a big enough crowd,
[01:31:30.080 --> 01:31:33.360]   then even a really subtle joke can take root someplace
[01:31:33.360 --> 01:31:34.640]   and then that cues other people.
[01:31:34.640 --> 01:31:37.600]   And it kind of, there's a whole statistics of,
[01:31:37.600 --> 01:31:39.920]   I did this terrible thing to my brother.
[01:31:39.920 --> 01:31:41.200]   - So when I was really young,
[01:31:41.200 --> 01:31:44.560]   I decided that my brother was only laughing
[01:31:44.560 --> 01:31:46.240]   as it comes when I laughed.
[01:31:46.240 --> 01:31:48.000]   But he was taking cues from me.
[01:31:48.000 --> 01:31:50.880]   So I purposely didn't laugh just to see if I was right.
[01:31:50.880 --> 01:31:52.160]   - And did you laugh at non-funny things?
[01:31:52.160 --> 01:31:52.400]   - Yes.
[01:31:52.400 --> 01:31:53.520]   - You really wanna do both sides.
[01:31:53.520 --> 01:31:54.400]   - I did both sides.
[01:31:54.400 --> 01:31:57.760]   And at the end of it, I told him what I did.
[01:31:57.760 --> 01:31:58.240]   - Oh, that's so funny.
[01:31:58.240 --> 01:31:59.440]   - He was very upset about this.
[01:31:59.440 --> 01:31:59.760]   - Yeah.
[01:31:59.760 --> 01:32:00.960]   - And from that day on--
[01:32:00.960 --> 01:32:02.880]   - He lost his sense of humor.
[01:32:02.880 --> 01:32:03.600]   - No, no, no, no.
[01:32:03.600 --> 01:32:05.440]   Well, yes, but from that day on,
[01:32:05.440 --> 01:32:07.520]   he laughed on his own.
[01:32:07.520 --> 01:32:08.560]   He stopped taking cues from me.
[01:32:08.560 --> 01:32:09.040]   - I see.
[01:32:09.040 --> 01:32:11.520]   - So I wanna say that it was a good thing that I did.
[01:32:11.520 --> 01:32:13.840]   - Yes, yes, you saved that man's life.
[01:32:13.840 --> 01:32:14.960]   - Yes, but it was mostly me.
[01:32:14.960 --> 01:32:15.840]   But it's true though.
[01:32:15.840 --> 01:32:16.720]   It's true, right?
[01:32:16.720 --> 01:32:18.720]   That people, I think you're right.
[01:32:18.720 --> 01:32:20.720]   - But okay, so where does that get us?
[01:32:20.720 --> 01:32:22.800]   That gets us the idea that,
[01:32:22.800 --> 01:32:26.080]   I mean, certainly movie theaters are a thing, right?
[01:32:26.080 --> 01:32:28.160]   Where people like to be watching together
[01:32:28.160 --> 01:32:30.640]   even though the people on the screen
[01:32:30.640 --> 01:32:32.880]   aren't really co-present with the people in the audience.
[01:32:32.880 --> 01:32:34.720]   The audience is co-present with itself.
[01:32:34.720 --> 01:32:36.080]   - By the way, on that point,
[01:32:36.080 --> 01:32:38.480]   it's an open question that's being raised by this,
[01:32:38.480 --> 01:32:40.880]   whether movies will no longer be a thing
[01:32:40.880 --> 01:32:43.600]   because Netflix's audience is growing.
[01:32:43.600 --> 01:32:47.040]   So that's, it's a very parallel question for education.
[01:32:47.040 --> 01:32:49.920]   Will movie theaters still be a thing in 2021?
[01:32:49.920 --> 01:32:51.840]   - No, but I think the argument is
[01:32:51.840 --> 01:32:54.560]   that there is a feeling of being in the crowd
[01:32:54.560 --> 01:32:57.360]   that isn't replicated by being at home watching it
[01:32:57.360 --> 01:32:59.120]   and that there's value in that.
[01:32:59.120 --> 01:32:59.920]   And then I think just--
[01:32:59.920 --> 01:33:01.920]   - But, but.
[01:33:01.920 --> 01:33:03.120]   - It scales better online.
[01:33:03.120 --> 01:33:06.720]   - But I feel like we're having a conversation
[01:33:06.720 --> 01:33:09.200]   about whether concerts will still exist
[01:33:09.200 --> 01:33:12.880]   after the invention of the record or the CD
[01:33:12.880 --> 01:33:13.760]   or wherever it is, right?
[01:33:13.760 --> 01:33:14.240]   - They won't.
[01:33:14.240 --> 01:33:15.920]   - You're right, concerts are dead.
[01:33:15.920 --> 01:33:19.440]   - Well, okay, I think the joke is only funny
[01:33:19.440 --> 01:33:21.120]   if you say it before now.
[01:33:21.120 --> 01:33:23.200]   - Right, yeah, that's true.
[01:33:23.200 --> 01:33:24.240]   - Like three years ago.
[01:33:24.240 --> 01:33:25.200]   It's like, well, no, obviously--
[01:33:25.200 --> 01:33:27.600]   - I'll wait to publish this until we have a vaccine.
[01:33:27.600 --> 01:33:30.240]   - You know, we'll fix it in post.
[01:33:30.240 --> 01:33:32.880]   But I think the important thing is--
[01:33:32.880 --> 01:33:34.160]   - Fix the virus, bust it.
[01:33:34.160 --> 01:33:35.680]   - Concerts changed, right?
[01:33:35.680 --> 01:33:37.200]   - Concerts changed.
[01:33:37.200 --> 01:33:39.360]   - First of all, movie theaters weren't this way, right?
[01:33:39.360 --> 01:33:41.760]   In like the '60s and '70s, they weren't like this.
[01:33:41.760 --> 01:33:43.840]   Like blockbusters were basically what?
[01:33:43.840 --> 01:33:46.080]   With "Jaws" and "Star Wars" created blockbusters, right?
[01:33:46.080 --> 01:33:47.600]   Before then, there weren't.
[01:33:47.600 --> 01:33:49.600]   Like the whole shared summer experience
[01:33:49.600 --> 01:33:52.160]   didn't exist in our lifetimes, right?
[01:33:52.160 --> 01:33:53.600]   Certainly you were well into adulthood
[01:33:53.600 --> 01:33:54.800]   by the time this was true, right?
[01:33:54.800 --> 01:33:56.960]   So it's just a very different, it's very different.
[01:33:56.960 --> 01:33:59.360]   So what we've been experiencing in the last 10 years
[01:33:59.360 --> 01:34:01.600]   is not like the majority of human history.
[01:34:01.600 --> 01:34:03.440]   But more importantly, concerts, right?
[01:34:03.440 --> 01:34:04.640]   Concerts mean something different.
[01:34:04.640 --> 01:34:07.520]   Most people don't go to concerts anymore.
[01:34:07.520 --> 01:34:09.440]   Like there's an age where you care about it.
[01:34:09.440 --> 01:34:10.240]   You sort of stop doing it,
[01:34:10.240 --> 01:34:11.920]   but you keep listening to music or whatever.
[01:34:11.920 --> 01:34:21.760]   So I think that's a painful way of saying that it will change.
[01:34:21.760 --> 01:34:23.760]   It was not the same thing as a going away.
[01:34:23.760 --> 01:34:26.880]   Replace is too strong of a word, but it will change.
[01:34:26.880 --> 01:34:27.360]   It has to.
[01:34:27.360 --> 01:34:29.120]   - I actually like to push back.
[01:34:29.120 --> 01:34:31.280]   I wonder, because I think you're probably
[01:34:31.280 --> 01:34:32.880]   just throwing that, your intuition out.
[01:34:32.880 --> 01:34:33.920]   - Oh, I'm sorry, man.
[01:34:33.920 --> 01:34:36.560]   - And it's possible that concerts,
[01:34:36.560 --> 01:34:38.880]   more people go to concerts now,
[01:34:39.680 --> 01:34:42.640]   but obviously much more people listen to,
[01:34:42.640 --> 01:34:45.760]   well, this is dumb, than before there was records.
[01:34:45.760 --> 01:34:50.640]   It's possible to argue that, if you look at the data,
[01:34:50.640 --> 01:34:55.680]   that it just expanded the pie of what music listening means.
[01:34:55.680 --> 01:34:59.200]   So it's possible that like universities grow in the parallel
[01:34:59.200 --> 01:35:02.480]   or the theaters grow, but also more people get to watch movies,
[01:35:02.480 --> 01:35:05.360]   more people get to be educated.
[01:35:05.360 --> 01:35:06.880]   - Yeah, I hope that is true.
[01:35:06.880 --> 01:35:09.040]   - Yeah, and to the extent that we can grow the pie,
[01:35:09.520 --> 01:35:11.840]   and have education be not just something you do
[01:35:11.840 --> 01:35:15.600]   for four years when you're done with your other education,
[01:35:15.600 --> 01:35:18.960]   but it be a more lifelong thing,
[01:35:18.960 --> 01:35:20.480]   that would have tremendous benefits,
[01:35:20.480 --> 01:35:24.160]   especially as the economy and the world change rapidly.
[01:35:24.160 --> 01:35:27.040]   Like people need opportunities to stay abreast
[01:35:27.040 --> 01:35:28.720]   of these changes.
[01:35:28.720 --> 01:35:30.720]   And so, I don't know, I could,
[01:35:30.720 --> 01:35:33.280]   that's all part of the ecosystem.
[01:35:33.280 --> 01:35:34.080]   - It's all to the good.
[01:35:34.080 --> 01:35:36.720]   I mean, I'm not gonna have an argument
[01:35:36.720 --> 01:35:39.120]   about whether we lost fidelity when we went
[01:35:39.120 --> 01:35:42.880]   from Laserdisc to DVDs or record players to CDs.
[01:35:42.880 --> 01:35:45.520]   I mean, I'm willing to grant that that is true,
[01:35:45.520 --> 01:35:47.600]   but convenience matters.
[01:35:47.600 --> 01:35:51.600]   And the ability to do something that you couldn't do otherwise
[01:35:51.600 --> 01:35:53.600]   because of that convenience matters.
[01:35:53.600 --> 01:35:55.920]   And you can tell me I'm only getting 90% of the experience,
[01:35:55.920 --> 01:35:57.520]   but I'm getting the experience.
[01:35:57.520 --> 01:35:59.920]   I wasn't getting it before, or it wasn't lasting as long,
[01:35:59.920 --> 01:36:00.720]   or it wasn't as easy.
[01:36:00.720 --> 01:36:03.280]   I mean, this just seems straightforward to me.
[01:36:03.280 --> 01:36:05.360]   It's going to change.
[01:36:05.360 --> 01:36:08.160]   It is for the good that more people get access.
[01:36:08.160 --> 01:36:10.400]   And it is our job to do two separate things.
[01:36:10.400 --> 01:36:13.280]   One, to educate them and make access available.
[01:36:13.280 --> 01:36:14.560]   That's our mission.
[01:36:14.560 --> 01:36:16.880]   But also for very simple, selfish reasons,
[01:36:16.880 --> 01:36:18.160]   we need to figure out how to do it better
[01:36:18.160 --> 01:36:20.000]   so that we individually stay in business.
[01:36:20.000 --> 01:36:21.680]   We can do both of those things at the same time.
[01:36:21.680 --> 01:36:24.240]   They are not in, they may be intention,
[01:36:24.240 --> 01:36:26.480]   but they are not mutually exclusive.
[01:36:26.480 --> 01:36:31.680]   - So you've educated some scary number of people.
[01:36:34.800 --> 01:36:37.280]   So you've seen a lot of people succeed,
[01:36:37.280 --> 01:36:38.800]   find their path through life.
[01:36:38.800 --> 01:36:43.920]   Is there a device that you can give to a young person today
[01:36:43.920 --> 01:36:48.000]   about computer science education,
[01:36:48.000 --> 01:36:52.000]   about education in general, about life,
[01:36:52.000 --> 01:36:59.360]   about whatever the journey that one takes in their,
[01:36:59.360 --> 01:37:01.840]   maybe in their teens, in their early twenties,
[01:37:01.840 --> 01:37:04.240]   sort of in those underground years,
[01:37:04.960 --> 01:37:08.960]   as you try to go through the essential process of partying
[01:37:08.960 --> 01:37:10.560]   and not going to classes,
[01:37:10.560 --> 01:37:12.240]   and yet somehow trying to get a degree?
[01:37:12.240 --> 01:37:16.320]   - If you get to the point where you're far enough up
[01:37:16.320 --> 01:37:18.720]   in the hierarchy of needs
[01:37:18.720 --> 01:37:21.760]   that you can actually make decisions like this,
[01:37:21.760 --> 01:37:24.320]   then find the thing that you're passionate about
[01:37:24.320 --> 01:37:25.520]   and pursue it.
[01:37:25.520 --> 01:37:27.680]   And sometimes it's the thing that drives your life
[01:37:27.680 --> 01:37:28.960]   and sometimes it's secondary.
[01:37:28.960 --> 01:37:30.160]   And you'll do other things
[01:37:30.160 --> 01:37:31.440]   'cause you've got to eat, right?
[01:37:31.440 --> 01:37:32.640]   You got a family, you got to feed,
[01:37:32.640 --> 01:37:34.320]   you got people you have to help or whatever.
[01:37:34.320 --> 01:37:36.240]   And I understand that and it's not easy for everyone,
[01:37:36.240 --> 01:37:40.800]   but always take a moment or two to pursue the things
[01:37:40.800 --> 01:37:44.400]   that you love, the things that bring passion
[01:37:44.400 --> 01:37:45.280]   and happiness to your life.
[01:37:45.280 --> 01:37:46.640]   And if you don't, I know that sounds corny,
[01:37:46.640 --> 01:37:47.840]   but I genuinely believe it.
[01:37:47.840 --> 01:37:49.760]   And if you don't have such a thing,
[01:37:49.760 --> 01:37:51.280]   then you're lying to yourself.
[01:37:51.280 --> 01:37:52.240]   You have such a thing.
[01:37:52.240 --> 01:37:53.360]   You just have to find it.
[01:37:53.360 --> 01:37:55.840]   And it's okay if it takes you a long time to get there.
[01:37:55.840 --> 01:37:58.640]   Rodney Dangerfield became a comedian in his fifties,
[01:37:58.640 --> 01:38:01.600]   I think, certainly wasn't his twenties.
[01:38:01.600 --> 01:38:03.520]   And lots of people failed for a very long time
[01:38:03.520 --> 01:38:05.120]   before getting to where they were going.
[01:38:05.120 --> 01:38:09.600]   I try to have hope and it wasn't obvious.
[01:38:09.600 --> 01:38:12.960]   I mean, you and I talked about the experience
[01:38:12.960 --> 01:38:15.360]   that I had a long time ago
[01:38:15.360 --> 01:38:17.360]   with a particular police officer.
[01:38:17.360 --> 01:38:19.840]   Wasn't my first one and it wasn't my last one.
[01:38:19.840 --> 01:38:24.000]   But in my view, I wasn't supposed to be here after that
[01:38:24.000 --> 01:38:24.720]   and I'm here.
[01:38:24.720 --> 01:38:25.760]   So it's all gravy.
[01:38:25.760 --> 01:38:28.880]   So you might as well go ahead and grab life as you can
[01:38:28.880 --> 01:38:29.680]   because of that.
[01:38:29.680 --> 01:38:31.040]   That's sort of how I see it.
[01:38:31.040 --> 01:38:34.320]   While recognizing, again, the delusion matters, right?
[01:38:34.320 --> 01:38:35.760]   Allow yourself to be deluded.
[01:38:35.760 --> 01:38:37.920]   Allow yourself to believe that it's all gonna work out.
[01:38:37.920 --> 01:38:40.960]   Just don't be so deluded that you miss the obvious
[01:38:40.960 --> 01:38:42.720]   and you're gonna be fine.
[01:38:42.720 --> 01:38:44.000]   It's gonna be there.
[01:38:44.000 --> 01:38:45.280]   It's gonna be there.
[01:38:45.280 --> 01:38:45.920]   It's gonna work out.
[01:38:45.920 --> 01:38:47.040]   - What do you think?
[01:38:47.040 --> 01:38:49.760]   - I like to say, choose your parents wisely
[01:38:49.760 --> 01:38:52.960]   because that has a big impact on your life.
[01:38:52.960 --> 01:38:53.280]   - It's different.
[01:38:53.280 --> 01:38:57.200]   - Yeah, I mean, there's a whole lot of things
[01:38:57.200 --> 01:38:58.160]   that you don't get to pick.
[01:38:59.440 --> 01:39:02.720]   And whether you get to have one kind of life
[01:39:02.720 --> 01:39:03.600]   or a different kind of life
[01:39:03.600 --> 01:39:06.000]   can depend a lot on things out of your control.
[01:39:06.000 --> 01:39:10.160]   But I really do believe in the passion, excitement thing.
[01:39:10.160 --> 01:39:11.840]   I was talking to my mom on the phone the other day
[01:39:11.840 --> 01:39:18.800]   and essentially what came out is that computer science
[01:39:18.800 --> 01:39:21.840]   is really popular right now.
[01:39:21.840 --> 01:39:25.680]   And I get to be a professor teaching something
[01:39:25.680 --> 01:39:28.000]   that's very attractive to people.
[01:39:28.720 --> 01:39:33.360]   And she was like trying to give me some appreciation
[01:39:33.360 --> 01:39:37.520]   for how foresightful I was for choosing this line of work
[01:39:37.520 --> 01:39:39.840]   as if somehow I knew that this is what was gonna happen
[01:39:39.840 --> 01:39:40.640]   in 2020.
[01:39:40.640 --> 01:39:43.840]   But that's not how it went for me at all.
[01:39:43.840 --> 01:39:45.440]   Like I studied computer science
[01:39:45.440 --> 01:39:47.760]   because I was just interested.
[01:39:47.760 --> 01:39:49.360]   It was just so interesting to me.
[01:39:49.360 --> 01:39:53.600]   I didn't think it would be particularly lucrative.
[01:39:53.600 --> 01:39:55.920]   And I've done everything I've can
[01:39:55.920 --> 01:39:58.000]   to keep it as un-lucrative as possible.
[01:39:58.000 --> 01:40:00.880]   - Yeah. - Some of my friends
[01:40:00.880 --> 01:40:03.440]   and colleagues have not done that.
[01:40:03.440 --> 01:40:07.680]   And I pride myself on my ability to remain un-rich.
[01:40:07.680 --> 01:40:13.040]   But I do believe that, like I'm glad.
[01:40:13.040 --> 01:40:14.960]   I mean, I'm glad that it worked out for me.
[01:40:14.960 --> 01:40:15.840]   It could have been like,
[01:40:15.840 --> 01:40:17.680]   oh, what I was really fascinated by
[01:40:17.680 --> 01:40:19.040]   is this particular kind of engraving
[01:40:19.040 --> 01:40:20.000]   that nobody cares about.
[01:40:20.000 --> 01:40:22.560]   So I got lucky and the thing that I cared about
[01:40:22.560 --> 01:40:23.200]   happened to be a thing
[01:40:23.200 --> 01:40:24.960]   that other people eventually cared about.
[01:40:24.960 --> 01:40:27.840]   But I don't think I would have had a fun time
[01:40:27.840 --> 01:40:28.960]   choosing anything else.
[01:40:28.960 --> 01:40:30.400]   Like this was the thing that kept me
[01:40:30.400 --> 01:40:32.400]   interested and engaged.
[01:40:32.400 --> 01:40:34.480]   - Well, one thing that people tell me,
[01:40:34.480 --> 01:40:38.000]   especially around early undergraduate,
[01:40:38.000 --> 01:40:41.360]   and the internet is part of the problem here,
[01:40:41.360 --> 01:40:44.000]   is they say they're passionate about so many things.
[01:40:44.000 --> 01:40:46.320]   How do I choose a thing?
[01:40:46.320 --> 01:40:50.080]   Which is a harder thing for me to know what to do with.
[01:40:50.080 --> 01:40:51.520]   Is there any-- - I mean,
[01:40:51.520 --> 01:40:53.520]   don't you know what you, I mean,
[01:40:53.520 --> 01:40:56.400]   you know, look, a long time ago,
[01:40:56.400 --> 01:40:57.520]   I walked down a hallway
[01:40:57.520 --> 01:40:58.320]   and I took a left turn.
[01:40:58.320 --> 01:41:00.160]   - Yeah. - I could have taken
[01:41:00.160 --> 01:41:02.720]   a right turn and my world could be better
[01:41:02.720 --> 01:41:03.600]   or it could be worse.
[01:41:03.600 --> 01:41:04.400]   I have no idea.
[01:41:04.400 --> 01:41:05.120]   I have no way of knowing.
[01:41:05.120 --> 01:41:06.960]   - Is there anything about this particular hallway
[01:41:06.960 --> 01:41:08.800]   that's relevant or you're just in general choices?
[01:41:08.800 --> 01:41:09.600]   - Yeah, you were on the left.
[01:41:09.600 --> 01:41:11.680]   - It sounds like you regret not taking the right turn.
[01:41:11.680 --> 01:41:13.360]   - Oh, no, not at all. - You brought it up.
[01:41:13.360 --> 01:41:16.320]   - Well, because there was a turn there.
[01:41:16.320 --> 01:41:17.920]   On the left was Michael Dimmon's office, right?
[01:41:17.920 --> 01:41:19.440]   I mean, these sorts of things happen, right?
[01:41:19.440 --> 01:41:20.480]   - Yes. - But here's the thing--
[01:41:20.480 --> 01:41:21.360]   - On the right, by the way,
[01:41:21.360 --> 01:41:22.480]   there was just a blank wall.
[01:41:22.480 --> 01:41:23.040]   (laughing)
[01:41:23.040 --> 01:41:24.960]   It wasn't a huge choice. - It would have really hurt.
[01:41:24.960 --> 01:41:25.920]   - He tried first.
[01:41:26.080 --> 01:41:27.440]   - No, but it's true, right?
[01:41:27.440 --> 01:41:29.680]   That I think about Ron Brockman, right?
[01:41:29.680 --> 01:41:33.200]   I took a trip I wasn't supposed to take
[01:41:33.200 --> 01:41:38.000]   and I ended up talking to Ron about this
[01:41:38.000 --> 01:41:39.920]   and I ended up going down this entire path
[01:41:39.920 --> 01:41:42.720]   that allowed me to, I think, get tenure.
[01:41:42.720 --> 01:41:45.600]   But by the way, I decided to say yes to something
[01:41:45.600 --> 01:41:46.560]   that didn't make any sense
[01:41:46.560 --> 01:41:48.080]   and I went down this educational path.
[01:41:48.080 --> 01:41:50.400]   But it would have been, who knows, right?
[01:41:50.400 --> 01:41:52.000]   Maybe if I hadn't done that,
[01:41:52.000 --> 01:41:54.160]   I would be a billionaire right now.
[01:41:54.160 --> 01:41:55.200]   I'd be Elon Musk.
[01:41:55.200 --> 01:41:56.880]   My life could be so much better.
[01:41:56.880 --> 01:41:58.400]   My life could also be so much worse.
[01:41:58.400 --> 01:42:01.440]   You just gotta feel that sometimes
[01:42:01.440 --> 01:42:02.880]   you have decisions you're gonna make.
[01:42:02.880 --> 01:42:04.080]   You cannot know what's gonna,
[01:42:04.080 --> 01:42:05.520]   you should think about it, right?
[01:42:05.520 --> 01:42:07.360]   Some things are clearly smarter than other things.
[01:42:07.360 --> 01:42:08.720]   You gotta play the odds a little bit.
[01:42:08.720 --> 01:42:11.360]   But in the end, if you've got multiple choices,
[01:42:11.360 --> 01:42:12.720]   there are lots of things you think you might love.
[01:42:12.720 --> 01:42:14.240]   Go with the thing that you actually love,
[01:42:14.240 --> 01:42:15.760]   the thing that jumps out at you
[01:42:15.760 --> 01:42:17.120]   and sort of pursue it for a little while.
[01:42:17.120 --> 01:42:18.800]   The worst thing that'll happen is you took a left turn
[01:42:18.800 --> 01:42:21.200]   instead of a right turn and you ended up merely happy.
[01:42:21.200 --> 01:42:23.440]   - Beautiful quote.
[01:42:23.440 --> 01:42:27.680]   - So accepting, so taking the step and just accepting,
[01:42:27.680 --> 01:42:30.560]   accepting that don't question the choice.
[01:42:30.560 --> 01:42:32.320]   - I like to think that life is long
[01:42:32.320 --> 01:42:36.480]   and there's time to actually pursue.
[01:42:36.480 --> 01:42:37.520]   - Every once in a while,
[01:42:37.520 --> 01:42:40.400]   you have to put on a leather suit
[01:42:40.400 --> 01:42:42.240]   and make a thriller video.
[01:42:42.240 --> 01:42:43.760]   - Every once in a while.
[01:42:43.760 --> 01:42:47.040]   - If I ever get the chance again, I'm doing it.
[01:42:47.040 --> 01:42:47.540]   - Yeah.
[01:42:47.540 --> 01:42:50.800]   I was told that you actually dance,
[01:42:50.800 --> 01:42:52.800]   but that part was edited out.
[01:42:52.800 --> 01:42:54.160]   - I don't dance.
[01:42:54.160 --> 01:42:58.960]   There was a thing where we did do the zombie thing.
[01:42:58.960 --> 01:43:00.480]   We did do the zombie thing.
[01:43:00.480 --> 01:43:01.760]   That wasn't edited out.
[01:43:01.760 --> 01:43:03.920]   It just wasn't put into the final thing.
[01:43:03.920 --> 01:43:06.080]   I'm quite happy.
[01:43:06.080 --> 01:43:07.360]   But there was a reason for that too, right?
[01:43:07.360 --> 01:43:09.280]   Like I wasn't wearing something right.
[01:43:09.280 --> 01:43:10.080]   There was a reason for that.
[01:43:10.080 --> 01:43:10.800]   I can't remember what it was.
[01:43:10.800 --> 01:43:11.600]   - No, leather suit.
[01:43:11.600 --> 01:43:12.880]   - Is that what it was?
[01:43:12.880 --> 01:43:13.520]   I can't remember.
[01:43:13.520 --> 01:43:14.560]   Anyway, the right thing happened.
[01:43:14.560 --> 01:43:17.680]   - Exactly, you took the left turn and it ended up--
[01:43:17.680 --> 01:43:19.360]   - Took the left turn and it ended up being the right thing.
[01:43:19.440 --> 01:43:23.760]   So a lot of people ask me that are a little bit tangential
[01:43:23.760 --> 01:43:25.680]   to the programming, the computing world,
[01:43:25.680 --> 01:43:28.160]   and they're interested to learn programming,
[01:43:28.160 --> 01:43:30.240]   like all kinds of disciplines that are outside
[01:43:30.240 --> 01:43:32.560]   of the particular discipline of computer science.
[01:43:32.560 --> 01:43:35.920]   What advice do you have for people
[01:43:35.920 --> 01:43:37.920]   that want to learn how to program
[01:43:37.920 --> 01:43:44.640]   or want to either taste this little skill set or discipline
[01:43:44.640 --> 01:43:48.160]   or try to see if it can be used somehow in their own life?
[01:43:48.160 --> 01:43:49.840]   - What stage of life are they in?
[01:43:49.840 --> 01:43:54.960]   - It feels, well, one of the magic things about the internet
[01:43:54.960 --> 01:43:57.120]   of the people that write me is I don't know.
[01:43:57.120 --> 01:43:58.960]   - Because my answer is different.
[01:43:58.960 --> 01:44:02.320]   My daughter is taking AP Computer Science right now.
[01:44:02.320 --> 01:44:02.880]   Hi, Jenny.
[01:44:02.880 --> 01:44:06.160]   She's amazing and doing amazing things.
[01:44:06.160 --> 01:44:07.920]   And my son's beginning to get interested
[01:44:07.920 --> 01:44:10.160]   and I'll be really curious where he takes it.
[01:44:10.160 --> 01:44:12.240]   I think his mind actually works very well
[01:44:12.240 --> 01:44:14.080]   for this sort of thing and she's doing great.
[01:44:14.080 --> 01:44:17.120]   But one of the things I have to tell her all the time,
[01:44:17.120 --> 01:44:18.800]   she points, well, I want to make a rhythm game.
[01:44:18.800 --> 01:44:23.120]   So I want to go for two weeks and then build a rhythm game.
[01:44:23.120 --> 01:44:24.560]   Show me how to build a rhythm game.
[01:44:24.560 --> 01:44:27.600]   And start small, learn the building blocks
[01:44:27.600 --> 01:44:29.440]   and how we take the time, have patience.
[01:44:29.440 --> 01:44:30.960]   Eventually you'll build a rhythm game.
[01:44:30.960 --> 01:44:34.000]   I was in grad school when I suddenly woke up one day
[01:44:34.000 --> 01:44:37.040]   over the Royal East and I thought, wait a minute,
[01:44:37.040 --> 01:44:37.840]   I'm a computer scientist.
[01:44:37.840 --> 01:44:39.600]   I should be able to write "Pac-Man" in an afternoon.
[01:44:39.600 --> 01:44:41.840]   And I did, not with great graphics.
[01:44:41.840 --> 01:44:42.960]   It was actually a very cool game.
[01:44:42.960 --> 01:44:44.960]   I had to figure out how the ghost moved and everything.
[01:44:44.960 --> 01:44:46.720]   And I did it in an afternoon in Pascal.
[01:44:47.200 --> 01:44:48.560]   On an old Apple IIgs.
[01:44:48.560 --> 01:44:52.400]   But if I had started out trying to build "Pac-Man",
[01:44:52.400 --> 01:44:54.960]   I think it probably would have ended very poorly for me.
[01:44:54.960 --> 01:44:58.000]   Luckily back then there weren't these magical devices
[01:44:58.000 --> 01:45:00.000]   we call phones and software everywhere
[01:45:00.000 --> 01:45:02.240]   to give me this illusion that I could create something
[01:45:02.240 --> 01:45:05.600]   by myself from the basics inside of a weekend like that.
[01:45:05.600 --> 01:45:09.520]   I mean, that was a culmination of years and years and years
[01:45:09.520 --> 01:45:12.000]   before I decided I should be able to write this and I could.
[01:45:12.000 --> 01:45:14.240]   So my advice if you're early on is,
[01:45:15.920 --> 01:45:16.720]   you've got the internet.
[01:45:16.720 --> 01:45:18.720]   There are lots of people there to give you the information.
[01:45:18.720 --> 01:45:20.400]   Find someone who cares about this.
[01:45:20.400 --> 01:45:22.560]   Remember they've been doing it for a very long time.
[01:45:22.560 --> 01:45:25.600]   Take it slow, learn the little pieces, get excited about it.
[01:45:25.600 --> 01:45:28.320]   And then keep the big project you want to build in mind.
[01:45:28.320 --> 01:45:29.520]   You'll get there soon enough.
[01:45:29.520 --> 01:45:32.720]   Because as a wise man once said, life is long.
[01:45:32.720 --> 01:45:35.520]   Sometimes it doesn't seem that long, but it is long.
[01:45:35.520 --> 01:45:38.960]   And you'll have enough time to build it all out.
[01:45:38.960 --> 01:45:40.720]   All the information is out there.
[01:45:40.720 --> 01:45:41.680]   But start small.
[01:45:41.680 --> 01:45:44.480]   You know, generate Fibonacci numbers.
[01:45:44.480 --> 01:45:47.120]   That's not exciting, but it'll get you around.
[01:45:47.120 --> 01:45:48.480]   - One programming language.
[01:45:48.480 --> 01:45:50.800]   - Well, there's only one programming language, it's Lisp.
[01:45:50.800 --> 01:45:53.360]   But if you have to pick a programming language,
[01:45:53.360 --> 01:45:55.280]   I guess in today's day, what would I do?
[01:45:55.280 --> 01:45:55.840]   I guess I'd do--
[01:45:55.840 --> 01:45:59.440]   - Python is basically Lisp, but with better syntax.
[01:45:59.440 --> 01:46:00.960]   - Blasphemy.
[01:46:00.960 --> 01:46:03.120]   - Yeah, with C syntax, how about that?
[01:46:03.120 --> 01:46:06.880]   - So you're gonna argue that C syntax is better than anything?
[01:46:06.880 --> 01:46:09.760]   Anyway, also I'm gonna answer Python despite what he said.
[01:46:09.760 --> 01:46:12.320]   - Tell me, tell your story about somebody's dissertation
[01:46:12.320 --> 01:46:14.080]   that had a Lisp program in it.
[01:46:14.080 --> 01:46:15.280]   - It was so funny.
[01:46:15.280 --> 01:46:18.400]   This is Dave's, Dave's dissertation was like Dave McAllister,
[01:46:18.400 --> 01:46:19.840]   who was a professor at MIT for a while.
[01:46:19.840 --> 01:46:21.920]   - And then he came in our group at Bell Labs.
[01:46:21.920 --> 01:46:25.760]   - And now he's at Technology Technical Institute of Chicago.
[01:46:25.760 --> 01:46:28.720]   A brilliant guy, such an interesting guy.
[01:46:28.720 --> 01:46:33.760]   Anyway, his thesis, it was a theorem prover.
[01:46:33.760 --> 01:46:38.160]   And he decided to have as an appendix his actual code,
[01:46:38.160 --> 01:46:39.440]   which of course was all written in Lisp,
[01:46:39.440 --> 01:46:40.720]   because of course it was.
[01:46:40.720 --> 01:46:43.360]   And like the last 20 pages are just right parentheses.
[01:46:43.360 --> 01:46:45.200]   (laughing)
[01:46:45.200 --> 01:46:45.920]   It's just wonderful.
[01:46:45.920 --> 01:46:48.240]   That's programming right there.
[01:46:48.240 --> 01:46:50.800]   Just pages upon pages of right parentheses.
[01:46:50.800 --> 01:46:52.640]   Anyway, Lisp is the only real language,
[01:46:52.640 --> 01:46:54.240]   but I understand that that's not necessarily
[01:46:54.240 --> 01:46:55.040]   the place where you start.
[01:46:55.040 --> 01:46:56.800]   Python is just fine.
[01:46:56.800 --> 01:46:58.240]   Python is good.
[01:46:58.240 --> 01:47:01.120]   If you're of a certain age, if you're really young
[01:47:01.120 --> 01:47:02.960]   and trying to figure it out, graphical languages
[01:47:02.960 --> 01:47:04.720]   that let you kind of see how the thing works,
[01:47:04.720 --> 01:47:06.240]   and that's fine too, they're all fine.
[01:47:06.240 --> 01:47:07.040]   It almost doesn't matter.
[01:47:07.040 --> 01:47:08.800]   But there are people who spend a lot of time
[01:47:08.800 --> 01:47:12.800]   thinking about how to build languages that get people in.
[01:47:12.800 --> 01:47:14.640]   The question is, are you trying to get in
[01:47:14.640 --> 01:47:15.680]   and figure out what it is?
[01:47:15.680 --> 01:47:17.920]   Or do you already know what you want?
[01:47:17.920 --> 01:47:19.760]   And that's why I asked you what stage of life people are in.
[01:47:19.760 --> 01:47:21.040]   Because if you're different stages of life,
[01:47:21.040 --> 01:47:22.880]   you would attack it differently.
[01:47:22.880 --> 01:47:25.280]   - The answer to that question of which language
[01:47:25.280 --> 01:47:28.640]   keeps changing, I mean, there's some value to exploring.
[01:47:28.640 --> 01:47:32.160]   A lot of people write to me about Julia.
[01:47:32.160 --> 01:47:35.040]   There's these like more modern languages
[01:47:35.040 --> 01:47:38.320]   that keep being invented, Rust and Kotlin.
[01:47:39.120 --> 01:47:42.240]   There's stuff that for people who love
[01:47:42.240 --> 01:47:43.760]   functional languages like Lisp,
[01:47:43.760 --> 01:47:46.720]   apparently there's echoes of that,
[01:47:46.720 --> 01:47:49.200]   but much better in the modern languages.
[01:47:49.200 --> 01:47:50.480]   And it's worthwhile to,
[01:47:50.480 --> 01:47:53.200]   especially when you're learning languages,
[01:47:53.200 --> 01:47:55.200]   it feels like it's okay to try one
[01:47:55.200 --> 01:47:57.520]   that's not like the popular one.
[01:47:57.520 --> 01:47:58.880]   - Oh yeah, but you want some--
[01:47:58.880 --> 01:48:01.360]   - I think you get that way of thinking
[01:48:01.360 --> 01:48:03.520]   almost no matter what language.
[01:48:03.520 --> 01:48:07.840]   And if you push far enough, like it can be assembly language,
[01:48:07.840 --> 01:48:09.440]   but you need to push pretty far
[01:48:09.440 --> 01:48:11.360]   before you start to hit the really deep concepts
[01:48:11.360 --> 01:48:13.200]   that you would get sooner in other languages.
[01:48:13.200 --> 01:48:16.480]   But like, I don't know, computation is kind of computation,
[01:48:16.480 --> 01:48:19.360]   is kind of Turing equivalent, is kind of computation.
[01:48:19.360 --> 01:48:22.080]   And so it matters how you express things,
[01:48:22.080 --> 01:48:25.360]   but you have to build out that mental structure in your mind.
[01:48:25.360 --> 01:48:28.480]   And I don't think it super matters which language.
[01:48:28.480 --> 01:48:29.840]   - I mean, it matters a little
[01:48:29.840 --> 01:48:32.080]   because some things are just at the wrong level of abstraction.
[01:48:32.080 --> 01:48:33.680]   I think assembly's at the wrong level of abstraction
[01:48:33.680 --> 01:48:34.720]   for someone coming in new.
[01:48:34.720 --> 01:48:37.120]   I think that if you start--
[01:48:37.120 --> 01:48:38.160]   - For someone coming in new.
[01:48:38.160 --> 01:48:41.840]   - Yes, for frameworks, big frameworks are quite a bit.
[01:48:41.840 --> 01:48:43.520]   You know, you've got to get to the point
[01:48:43.520 --> 01:48:44.800]   where I want to learn a new language,
[01:48:44.800 --> 01:48:46.080]   means I just pick up a reference book
[01:48:46.080 --> 01:48:49.040]   and I think of a project and I go through it in a weekend.
[01:48:49.040 --> 01:48:50.240]   But you got to get there.
[01:48:50.240 --> 01:48:52.640]   You're right though, the languages that are designed for that
[01:48:52.640 --> 01:48:54.800]   are, it almost doesn't matter.
[01:48:54.800 --> 01:48:57.680]   Pick the ones that people have built tutorials
[01:48:57.680 --> 01:48:58.960]   and infrastructure around to help you
[01:48:58.960 --> 01:49:00.640]   get kind of ease into it.
[01:49:00.640 --> 01:49:01.280]   'Cause it's hard.
[01:49:01.280 --> 01:49:02.720]   I mean, I did this little experiment once.
[01:49:05.120 --> 01:49:08.240]   I was teaching intro to CS in the summer as a favor.
[01:49:08.240 --> 01:49:12.240]   Which is, anyway, I was teaching--
[01:49:12.240 --> 01:49:13.040]   - Pleasant memories.
[01:49:13.040 --> 01:49:15.040]   - I was teaching intro to CS as a favor.
[01:49:15.040 --> 01:49:16.800]   And it was very funny 'cause I'd go in every single time
[01:49:16.800 --> 01:49:18.000]   and I would think to myself,
[01:49:18.000 --> 01:49:20.720]   how am I possibly gonna fill up an hour and a half
[01:49:20.720 --> 01:49:22.960]   talking about for loops, right?
[01:49:22.960 --> 01:49:24.400]   And there wasn't enough time.
[01:49:24.400 --> 01:49:26.480]   It took me a while to realize this, right?
[01:49:26.480 --> 01:49:27.760]   There are only three things, right?
[01:49:27.760 --> 01:49:28.960]   There's reading from a variable,
[01:49:28.960 --> 01:49:30.800]   writing to a variable and conditional branching.
[01:49:30.800 --> 01:49:33.840]   Everything else is syntactic sugar, right?
[01:49:33.840 --> 01:49:35.920]   The syntactic sugar matters, but that's it.
[01:49:35.920 --> 01:49:38.800]   And when I say that's it, I don't mean it's simple.
[01:49:38.800 --> 01:49:40.000]   I mean, it's hard.
[01:49:40.000 --> 01:49:43.280]   Like conditional branching, loops, variable,
[01:49:43.280 --> 01:49:45.040]   those are really hard concepts.
[01:49:45.040 --> 01:49:47.040]   So you shouldn't be discouraged by this.
[01:49:47.040 --> 01:49:48.080]   Here's a simple experiment.
[01:49:48.080 --> 01:49:49.120]   I'm gonna ask you a question now.
[01:49:49.120 --> 01:49:49.440]   You ready?
[01:49:49.440 --> 01:49:49.760]   - Uh-oh.
[01:49:49.760 --> 01:49:51.360]   - X equals three.
[01:49:51.360 --> 01:49:51.760]   - Okay.
[01:49:51.760 --> 01:49:54.560]   - Y equals four.
[01:49:54.560 --> 01:49:55.040]   - Okay.
[01:49:55.040 --> 01:49:55.840]   - What is X?
[01:49:55.840 --> 01:49:57.200]   - Three.
[01:49:57.200 --> 01:49:58.000]   - What is Y?
[01:49:58.000 --> 01:49:59.200]   - Four.
[01:49:59.200 --> 01:50:00.480]   - Y equals X.
[01:50:00.480 --> 01:50:00.880]   - I'm gonna mess this up.
[01:50:00.880 --> 01:50:02.480]   - No, it's easier.
[01:50:02.480 --> 01:50:03.280]   Y equals X.
[01:50:03.840 --> 01:50:04.560]   - Y equals X.
[01:50:04.560 --> 01:50:05.200]   - What is Y?
[01:50:05.200 --> 01:50:08.000]   - Three.
[01:50:08.000 --> 01:50:09.120]   - That's right.
[01:50:09.120 --> 01:50:10.000]   X equals seven.
[01:50:10.000 --> 01:50:11.840]   What is Y?
[01:50:11.840 --> 01:50:15.440]   - That's one of the trickiest things to get for programmers,
[01:50:15.440 --> 01:50:19.040]   that there's a memory and the variables are pointing
[01:50:19.040 --> 01:50:20.880]   to a particular thing in memory.
[01:50:20.880 --> 01:50:23.120]   And sometimes the languages hide that from you
[01:50:23.120 --> 01:50:24.240]   and they bring it closer
[01:50:24.240 --> 01:50:26.080]   to the way you think mathematics works.
[01:50:26.080 --> 01:50:26.720]   - Right.
[01:50:26.720 --> 01:50:28.320]   So in fact, Mark Guzdow,
[01:50:28.320 --> 01:50:29.840]   who worries about these sorts of things,
[01:50:29.840 --> 01:50:31.520]   or used to worry about these sorts of things anyway,
[01:50:32.320 --> 01:50:35.760]   had this kind of belief that actually people,
[01:50:35.760 --> 01:50:36.800]   when they see these statements,
[01:50:36.800 --> 01:50:38.640]   X equals something, Y equals something, Y equals X,
[01:50:38.640 --> 01:50:42.720]   that you have now made a mathematical statement
[01:50:42.720 --> 01:50:43.920]   that Y and X are the same.
[01:50:43.920 --> 01:50:47.520]   - Which you can if you just put like an anchor in front of it.
[01:50:47.520 --> 01:50:49.680]   - Yes, but people, that's not what you're doing.
[01:50:49.680 --> 01:50:49.920]   - Yeah.
[01:50:49.920 --> 01:50:50.240]   - Right?
[01:50:50.240 --> 01:50:53.920]   I thought, and I kind of asked the question,
[01:50:53.920 --> 01:50:55.360]   and I think I had some evidence for this,
[01:50:55.360 --> 01:50:56.480]   I'm hardly a study,
[01:50:56.480 --> 01:50:59.040]   is that most of the people who didn't know the answer,
[01:50:59.040 --> 01:51:00.160]   weren't sure about the answer,
[01:51:00.160 --> 01:51:01.280]   they had used spreadsheets.
[01:51:01.280 --> 01:51:03.120]   - Ah, interesting.
[01:51:03.120 --> 01:51:10.800]   - And so it's by reference, or by name really, right?
[01:51:10.800 --> 01:51:13.120]   And so depending upon what you think they are,
[01:51:13.120 --> 01:51:14.560]   you get completely different answers.
[01:51:14.560 --> 01:51:16.240]   The fact that I could go,
[01:51:16.240 --> 01:51:19.280]   or one could go two thirds of the way through a semester,
[01:51:19.280 --> 01:51:22.400]   and people still hadn't figured out in their heads,
[01:51:22.400 --> 01:51:24.240]   when you say Y equals X, what that meant,
[01:51:24.240 --> 01:51:26.080]   tells you it's actually hard.
[01:51:26.080 --> 01:51:28.880]   Because all those answers are possible.
[01:51:28.880 --> 01:51:29.920]   And in fact, when you said,
[01:51:29.920 --> 01:51:31.680]   "Oh, if you just put an ampersand in front of it,"
[01:51:31.680 --> 01:51:33.520]   I mean, that doesn't make any sense for an intro class.
[01:51:33.520 --> 01:51:34.560]   And of course, a lot of language
[01:51:34.560 --> 01:51:36.080]   don't even give you the ability to think about it
[01:51:36.080 --> 01:51:37.120]   in terms of ampersand.
[01:51:37.120 --> 01:51:38.800]   Do we want to have a 45 minute discussion
[01:51:38.800 --> 01:51:42.000]   about the difference between equal EQ and equal in Lisp?
[01:51:42.000 --> 01:51:42.400]   - Yeah.
[01:51:42.400 --> 01:51:43.440]   - I know you do.
[01:51:43.440 --> 01:51:45.840]   (laughing)
[01:51:45.840 --> 01:51:47.280]   But you could do that.
[01:51:47.280 --> 01:51:48.960]   This is actually really hard stuff.
[01:51:48.960 --> 01:51:52.240]   So you shouldn't be, it's not too hard, we all do it,
[01:51:52.240 --> 01:51:53.680]   but you shouldn't be discouraged.
[01:51:53.680 --> 01:51:55.120]   It's why you should start small,
[01:51:55.120 --> 01:51:56.400]   so that you can figure out these things,
[01:51:56.400 --> 01:51:58.240]   so you have the right model in your head,
[01:51:58.240 --> 01:51:59.760]   so that when you write the language,
[01:51:59.760 --> 01:52:01.840]   you can execute it and build the machine
[01:52:01.840 --> 01:52:03.040]   that you want to build, right?
[01:52:03.040 --> 01:52:05.040]   - Yeah, the funny thing about programming
[01:52:05.040 --> 01:52:08.960]   and those very basic things is the very basics
[01:52:08.960 --> 01:52:10.320]   are not often made explicit,
[01:52:10.320 --> 01:52:13.760]   which is actually what drives everybody away
[01:52:13.760 --> 01:52:15.120]   from basically any discipline,
[01:52:15.120 --> 01:52:16.880]   but programming is just another one.
[01:52:16.880 --> 01:52:19.200]   Like even a simpler version of the equal sign
[01:52:19.200 --> 01:52:22.960]   that I kind of forget is in mathematics,
[01:52:22.960 --> 01:52:25.040]   equals is not assignment.
[01:52:25.040 --> 01:52:25.520]   - Yeah.
[01:52:25.520 --> 01:52:26.400]   - Right.
[01:52:26.480 --> 01:52:30.160]   Like I think basically every single programming language
[01:52:30.160 --> 01:52:34.400]   with just a few handful of exceptions equals is assignment.
[01:52:34.400 --> 01:52:34.880]   - Mm-hmm.
[01:52:34.880 --> 01:52:38.000]   - And you have some other operator for equality.
[01:52:38.000 --> 01:52:38.720]   - Yeah.
[01:52:38.720 --> 01:52:41.440]   - And even that, like everyone kind of knows it.
[01:52:41.440 --> 01:52:42.560]   - Mm-hmm.
[01:52:42.560 --> 01:52:44.880]   - Once you started doing it,
[01:52:44.880 --> 01:52:46.960]   but like you need to say that explicitly
[01:52:46.960 --> 01:52:50.080]   or you just realize it like yourself.
[01:52:50.080 --> 01:52:53.120]   Otherwise you might be stuck for,
[01:52:53.120 --> 01:52:54.560]   you said like half a semester,
[01:52:54.560 --> 01:52:57.040]   you could be stuck for quite a long time.
[01:52:57.040 --> 01:52:59.520]   And I think also part of the programming
[01:52:59.520 --> 01:53:04.320]   is being okay in that state of confusion for a while.
[01:53:04.320 --> 01:53:06.640]   It's to the debugging point.
[01:53:06.640 --> 01:53:08.800]   It's like, I just wrote two lines of code.
[01:53:08.800 --> 01:53:10.880]   Why doesn't this work?
[01:53:10.880 --> 01:53:12.960]   And staring at that for like hours
[01:53:12.960 --> 01:53:14.480]   - Mm-hmm.
[01:53:14.480 --> 01:53:15.760]   - And trying to figure out.
[01:53:15.760 --> 01:53:16.880]   And then every once in a while,
[01:53:16.880 --> 01:53:18.320]   you just have to restart your computer
[01:53:18.320 --> 01:53:19.440]   and everything works again.
[01:53:19.440 --> 01:53:24.080]   And then you just kind of stare into the void
[01:53:24.080 --> 01:53:26.240]   with the tears slowly rolling down your eye.
[01:53:26.240 --> 01:53:28.160]   - By the way, the fact that they didn't get this
[01:53:28.160 --> 01:53:29.840]   actually had no impact on,
[01:53:29.840 --> 01:53:31.760]   I mean, they were still able to do their assignments.
[01:53:31.760 --> 01:53:32.080]   - Right.
[01:53:32.080 --> 01:53:34.240]   - 'Cause it turns out their misunderstanding
[01:53:34.240 --> 01:53:36.640]   wasn't being revealed to them.
[01:53:36.640 --> 01:53:37.040]   - Yes.
[01:53:37.040 --> 01:53:40.080]   - By the problem sets we were giving them.
[01:53:40.080 --> 01:53:41.040]   - It's pretty profound actually, yeah.
[01:53:41.040 --> 01:53:44.240]   - I wrote a program a long time ago,
[01:53:44.240 --> 01:53:46.240]   actually for my master's thesis.
[01:53:46.240 --> 01:53:49.280]   And in C++ I think, or C, I guess it was C.
[01:53:49.280 --> 01:53:51.440]   And it was all memory management and terrible.
[01:53:52.640 --> 01:53:55.040]   And it wouldn't work for a while.
[01:53:55.040 --> 01:53:57.360]   And it was some kind of,
[01:53:57.360 --> 01:53:59.520]   it was clear to me that it was overriding memory.
[01:53:59.520 --> 01:54:01.200]   And I just couldn't, I was like,
[01:54:01.200 --> 01:54:03.280]   "Look, I got a paper done, time for this."
[01:54:03.280 --> 01:54:05.840]   So I basically declared a variable
[01:54:05.840 --> 01:54:10.000]   at the front and the main that was like 400K,
[01:54:10.000 --> 01:54:11.200]   just an array.
[01:54:11.200 --> 01:54:11.760]   And it worked.
[01:54:11.760 --> 01:54:14.160]   Because wherever I was scribbling over memory,
[01:54:14.160 --> 01:54:16.080]   it would scribble into that space and it didn't matter.
[01:54:16.080 --> 01:54:18.640]   And so I never figured out what the bug was.
[01:54:18.640 --> 01:54:21.600]   But I did create something to sort of deal with it.
[01:54:21.600 --> 01:54:22.400]   - To work around it.
[01:54:22.400 --> 01:54:25.520]   - And it, you know, that's crazy, that's crazy.
[01:54:25.520 --> 01:54:27.120]   It was okay, 'cause that's what I wanted.
[01:54:27.120 --> 01:54:30.080]   But I knew enough about memory management to go,
[01:54:30.080 --> 01:54:32.080]   "You know, I'm just gonna create an empty array here
[01:54:32.080 --> 01:54:33.120]   "and hope that that deals with
[01:54:33.120 --> 01:54:34.800]   "the scribbling memory problem."
[01:54:34.800 --> 01:54:35.200]   And it did.
[01:54:35.200 --> 01:54:36.880]   That takes a long time to figure out.
[01:54:36.880 --> 01:54:38.400]   And by the way, the language you first learn
[01:54:38.400 --> 01:54:39.680]   probably is garbage collection anyway,
[01:54:39.680 --> 01:54:42.000]   so you're not even gonna come across that problem.
[01:54:42.000 --> 01:54:45.920]   - So we talked about the Minsky idea
[01:54:45.920 --> 01:54:48.400]   of hating everything you do and hating yourself.
[01:54:49.440 --> 01:54:53.040]   So let's end on a question that's gonna make
[01:54:53.040 --> 01:54:54.400]   both of you very uncomfortable.
[01:54:54.400 --> 01:54:54.880]   - Okay.
[01:54:54.880 --> 01:54:57.840]   - Which is, what is your, Charles,
[01:54:57.840 --> 01:55:00.560]   what's your favorite thing
[01:55:00.560 --> 01:55:03.040]   that you're grateful for about Michael?
[01:55:03.040 --> 01:55:06.480]   And Michael, what is your favorite thing
[01:55:06.480 --> 01:55:08.640]   that you're grateful for about Charles?
[01:55:08.640 --> 01:55:11.360]   - Well, that answer is actually quite easy.
[01:55:11.360 --> 01:55:12.480]   His friendship.
[01:55:12.480 --> 01:55:15.280]   - He stole the easy answer.
[01:55:15.280 --> 01:55:15.600]   - I did.
[01:55:15.600 --> 01:55:17.280]   - Yeah, I can tell you what I hate about Charles.
[01:55:17.280 --> 01:55:18.720]   He steals my good answers.
[01:55:18.720 --> 01:55:20.960]   - The thing I like most about Charles,
[01:55:20.960 --> 01:55:23.920]   he sees the world in a similar enough,
[01:55:23.920 --> 01:55:26.960]   but different way that it's sort of like having
[01:55:26.960 --> 01:55:28.720]   another life.
[01:55:28.720 --> 01:55:31.200]   It's sort of like I get to experience things
[01:55:31.200 --> 01:55:32.800]   that I wouldn't otherwise get to experience
[01:55:32.800 --> 01:55:35.920]   because I would not naturally gravitate to them that way.
[01:55:35.920 --> 01:55:38.880]   And so he just shows me a whole other world.
[01:55:38.880 --> 01:55:39.360]   It's awesome.
[01:55:39.360 --> 01:55:43.280]   - Yeah, the inner product is not zero for sure.
[01:55:43.280 --> 01:55:46.480]   It's not quite one, 0.7 maybe.
[01:55:47.440 --> 01:55:49.040]   - Just enough that you can learn.
[01:55:49.040 --> 01:55:51.840]   - Just enough that you can learn.
[01:55:51.840 --> 01:55:54.160]   - That's the definition of friendship.
[01:55:54.160 --> 01:55:55.520]   The inner product is 0.7.
[01:55:55.520 --> 01:55:56.480]   - Yeah, I think so.
[01:55:56.480 --> 01:55:57.600]   That's the answer to life really.
[01:55:57.600 --> 01:55:59.200]   - Charles sometimes believes in me
[01:55:59.200 --> 01:56:00.880]   when I have not believed in me.
[01:56:00.880 --> 01:56:04.160]   He also sometimes works as an outward confidence
[01:56:04.160 --> 01:56:07.200]   that he has so much, so much confidence
[01:56:07.200 --> 01:56:10.800]   and self, I don't know, comfortableness.
[01:56:10.800 --> 01:56:11.920]   - Okay, let's go with that.
[01:56:11.920 --> 01:56:15.440]   - That I feel better a little bit.
[01:56:16.000 --> 01:56:17.520]   If he thinks I'm okay,
[01:56:17.520 --> 01:56:19.280]   then maybe I'm not as bad as I think I am.
[01:56:19.280 --> 01:56:22.880]   - At the end of the day, luck favors the Charles.
[01:56:22.880 --> 01:56:26.000]   It's a huge honor to talk with you.
[01:56:26.000 --> 01:56:28.560]   Thank you so much for taking this time,
[01:56:28.560 --> 01:56:30.560]   wasting your time with me.
[01:56:30.560 --> 01:56:32.000]   It was an awesome conversation.
[01:56:32.000 --> 01:56:35.040]   You guys are an inspiration to a huge number of people
[01:56:35.040 --> 01:56:36.960]   and to me, so really enjoyed this.
[01:56:36.960 --> 01:56:37.280]   Thanks for talking.
[01:56:37.280 --> 01:56:38.000]   - I enjoyed it as well.
[01:56:38.000 --> 01:56:38.880]   Thank you so much.
[01:56:38.880 --> 01:56:40.240]   And by the way, if luck favors the Charles,
[01:56:40.240 --> 01:56:41.280]   then it's certainly the case
[01:56:41.280 --> 01:56:42.880]   that I've been very lucky to know you.
[01:56:42.880 --> 01:56:44.720]   - Oh, I'm gonna edit that part out.
[01:56:44.720 --> 01:56:46.720]   (all laughing)
[01:56:46.720 --> 01:56:49.120]   - Thanks for listening to this conversation
[01:56:49.120 --> 01:56:50.880]   with Charles Isbell and Michael Littman.
[01:56:50.880 --> 01:56:53.680]   And thank you to our sponsors,
[01:56:53.680 --> 01:56:56.240]   Athletic Greens, Super Nutritional Drink,
[01:56:56.240 --> 01:56:59.360]   Eight Sleep, Self-Cooling Mattress,
[01:56:59.360 --> 01:57:02.720]   Masterclass Online Courses
[01:57:02.720 --> 01:57:05.040]   from some of the most amazing humans in history,
[01:57:05.040 --> 01:57:08.800]   and Cash App, the app I use to send money to friends.
[01:57:08.800 --> 01:57:12.240]   Please check out the sponsors in the description
[01:57:12.240 --> 01:57:15.120]   to get a discount and to support this podcast.
[01:57:15.120 --> 01:57:18.240]   If you enjoy this thing, subscribe on YouTube,
[01:57:18.240 --> 01:57:20.480]   review it with Five Stars on Apple Podcast,
[01:57:20.480 --> 01:57:23.280]   follow on Spotify, support it on Patreon,
[01:57:23.280 --> 01:57:25.920]   connect with me on Twitter, @lexfriedman.
[01:57:25.920 --> 01:57:28.240]   And now let me leave you with some words
[01:57:28.240 --> 01:57:29.440]   from Desmond Tutu.
[01:57:29.440 --> 01:57:33.760]   Don't raise your voice, improve your argument.
[01:57:33.760 --> 01:57:37.600]   Thank you for listening and hope to see you next time.
[01:57:37.920 --> 01:57:40.500]   (upbeat music)
[01:57:40.500 --> 01:57:43.080]   (upbeat music)
[01:57:43.080 --> 01:57:53.080]   [BLANK_AUDIO]

