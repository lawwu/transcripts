
[00:00:00.000 --> 00:00:03.480]   if you haven't seen it, Sora is the text video model from open
[00:00:03.480 --> 00:00:09.000]   AI. And this, if you weren't studying it, would look like a
[00:00:09.000 --> 00:00:10.640]   major feature film.
[00:00:10.640 --> 00:00:13.480]   The traditional approach for rendering video is you create
[00:00:13.480 --> 00:00:15.920]   three dimensional objects. And then you have a rendering
[00:00:15.920 --> 00:00:18.080]   engine that renders those objects. And then you have a
[00:00:18.080 --> 00:00:20.760]   system that defines where the camera goes. And that's how you
[00:00:20.760 --> 00:00:24.480]   get the visual that you use to generate a 2d movie like this.
[00:00:24.480 --> 00:00:28.320]   This doesn't do that. This was a trained model. So how would you
[00:00:28.320 --> 00:00:33.280]   train a model to do this without having a 3d space, the compute
[00:00:33.280 --> 00:00:36.640]   necessary to define each of those objects, place them in 3d
[00:00:36.640 --> 00:00:40.880]   space is practically impossible today, my guess is that open AI
[00:00:40.880 --> 00:00:45.760]   used a tool like Unreal Engine five and generated tons and
[00:00:45.760 --> 00:00:49.960]   tons of video content, tagged it labeled it. And we're then able
[00:00:49.960 --> 00:00:53.720]   to use that to train this model that can for whatever reason
[00:00:53.720 --> 00:00:55.560]   that we don't understand, do this.
[00:00:55.560 --> 00:00:57.240]   You're referring to synthetic training data.

