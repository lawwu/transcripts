
[00:00:00.000 --> 00:00:04.040]   And so one of the fun issues that we've been facing lately
[00:00:04.040 --> 00:00:06.800]   is actually a lot of our early users who we love
[00:00:06.800 --> 00:00:08.680]   don't know about the breadth of our platform.
[00:00:08.680 --> 00:00:11.960]   And we now do lots more than we did even a year ago.
[00:00:11.960 --> 00:00:13.720]   And we have a real point of view about how
[00:00:13.720 --> 00:00:16.520]   to make ML projects effective.
[00:00:16.520 --> 00:00:19.200]   And so I feel so happy to introduce
[00:00:19.200 --> 00:00:22.920]   Kerry Phelps, who is the person interviewing OpenAI
[00:00:22.920 --> 00:00:25.800]   in that slide I showed earlier, to give you
[00:00:25.800 --> 00:00:27.400]   a demo of our end-to-end platform
[00:00:27.400 --> 00:00:29.240]   and our thoughts on best practices.
[00:00:29.240 --> 00:00:30.880]   Kerry has been a PM at the company
[00:00:30.880 --> 00:00:32.400]   since the very beginning.
[00:00:32.400 --> 00:00:35.160]   And she's responsible for a lot of the long-term vision
[00:00:35.160 --> 00:00:36.960]   and a lot of the little details that
[00:00:36.960 --> 00:00:38.960]   make the product delightful.
[00:00:38.960 --> 00:00:40.800]   I'm so excited to get a look at this, Kerry.
[00:00:40.800 --> 00:00:41.300]   Thank you.
[00:00:41.300 --> 00:00:45.200]   [APPLAUSE]
[00:00:45.200 --> 00:00:46.360]   Thank you.
[00:00:46.360 --> 00:00:47.880]   What a kind introduction.
[00:00:47.880 --> 00:00:49.560]   I really appreciate it.
[00:00:49.560 --> 00:00:51.280]   I've really enjoyed getting to work here
[00:00:51.280 --> 00:00:52.840]   for the past five years.
[00:00:52.840 --> 00:00:58.120]   And one of the things that we're really seeing to ground us
[00:00:58.120 --> 00:01:03.440]   is this simple ML workflow, this common pattern.
[00:01:03.440 --> 00:01:07.480]   And so I want to ask, how does a model go
[00:01:07.480 --> 00:01:10.080]   from creation to deployment?
[00:01:10.080 --> 00:01:14.800]   Almost every mature team we work with has three personas.
[00:01:14.800 --> 00:01:17.280]   And I want to see who's here in the audience today.
[00:01:17.280 --> 00:01:20.320]   So first, who's an ML practitioner in the audience?
[00:01:20.320 --> 00:01:21.920]   Can I see a show of hands?
[00:01:21.920 --> 00:01:26.760]   Yes, so training models, running new experiments,
[00:01:26.760 --> 00:01:29.880]   tracking metrics, visualizing results.
[00:01:29.880 --> 00:01:34.320]   So we also have ML ops engineers using weights and biases.
[00:01:34.320 --> 00:01:35.760]   Who here is an ML ops engineer?
[00:01:35.760 --> 00:01:37.200]   Can I see a show of hands?
[00:01:37.200 --> 00:01:37.960]   Awesome.
[00:01:37.960 --> 00:01:39.660]   So those are the people who are setting up
[00:01:39.660 --> 00:01:43.760]   infrastructure for the team, managing and deploying models.
[00:01:43.760 --> 00:01:46.880]   And now, who is an ML team leader?
[00:01:46.880 --> 00:01:48.960]   Who's running an ML team right now?
[00:01:48.960 --> 00:01:49.920]   Awesome, awesome.
[00:01:49.920 --> 00:01:51.520]   Great to see you in the audience.
[00:01:51.520 --> 00:01:54.500]   And those folks reviewing and deploying models,
[00:01:54.500 --> 00:01:57.200]   driving stakeholder alignment.
[00:01:57.200 --> 00:02:00.040]   As a lot of you know, sometimes one person
[00:02:00.040 --> 00:02:02.160]   is doing all three of these things.
[00:02:02.160 --> 00:02:06.800]   But what can we do to tighten the cycle of iteration
[00:02:06.800 --> 00:02:11.280]   across these different personas, these different stakeholders?
[00:02:11.280 --> 00:02:15.240]   Well, there is a critical piece that
[00:02:15.240 --> 00:02:19.400]   is flashing on the screen because the HDMI cable is not
[00:02:19.400 --> 00:02:20.600]   well-connected.
[00:02:20.600 --> 00:02:23.600]   [LAUGHTER]
[00:02:23.600 --> 00:02:26.180]   So I'll stand over here.
[00:02:26.180 --> 00:02:28.500]   So as you all well know, anything
[00:02:28.500 --> 00:02:31.580]   we can do to tighten that iteration cycle,
[00:02:31.580 --> 00:02:35.220]   to reduce compute costs and time investment,
[00:02:35.220 --> 00:02:38.740]   the more efficient the process is going to be.
[00:02:38.740 --> 00:02:42.060]   So what does that lifecycle look like?
[00:02:42.060 --> 00:02:44.980]   So we've got, of course, training data sets,
[00:02:44.980 --> 00:02:47.500]   potentially a foundation model or a pre-trained model
[00:02:47.500 --> 00:02:48.740]   you're pulling in.
[00:02:48.740 --> 00:02:51.460]   You're training and evaluating that model,
[00:02:51.460 --> 00:02:54.520]   then communicating those results and documenting them,
[00:02:54.520 --> 00:02:57.460]   and finally, deploying the model into production
[00:02:57.460 --> 00:02:58.920]   on an inference server.
[00:02:58.920 --> 00:03:02.960]   So what tools do you need across each of those steps?
[00:03:02.960 --> 00:03:06.160]   So we've got a whole suite of tools for experiment tracking
[00:03:06.160 --> 00:03:07.800]   for an ML practitioner.
[00:03:07.800 --> 00:03:10.520]   We also have tools for the ML Ops persona.
[00:03:10.520 --> 00:03:13.560]   So that's someone who might want to launch new models
[00:03:13.560 --> 00:03:17.880]   and get them evaluated systematically and rigorously,
[00:03:17.880 --> 00:03:20.440]   then communicate those results to the team
[00:03:20.440 --> 00:03:22.940]   leader in a report, and then deploy
[00:03:22.940 --> 00:03:24.940]   that model into production.
[00:03:24.940 --> 00:03:27.060]   And this can seem really high level, right?
[00:03:27.060 --> 00:03:29.420]   This is a big problem to solve.
[00:03:29.420 --> 00:03:32.740]   So I'm going to go through a whirlwind demo today.
[00:03:32.740 --> 00:03:33.780]   Buckle up.
[00:03:33.780 --> 00:03:36.040]   This is going to be five minutes of showing you
[00:03:36.040 --> 00:03:41.180]   how to make this process simple and repeatable with WMB.
[00:03:41.180 --> 00:03:44.940]   So to ground this in a real example,
[00:03:44.940 --> 00:03:47.940]   let's imagine that I'm an ML practitioner.
[00:03:47.940 --> 00:03:49.620]   So I'm a new team member.
[00:03:49.620 --> 00:03:52.440]   I'm joining the Singularity company.
[00:03:52.440 --> 00:03:55.960]   And they've just assigned me my first project.
[00:03:55.960 --> 00:03:59.080]   So we're contracting with Manchester United,
[00:03:59.080 --> 00:04:00.400]   the football club.
[00:04:00.400 --> 00:04:03.320]   And we're looking at images of soccer matches.
[00:04:03.320 --> 00:04:04.760]   We're trying to analyze them.
[00:04:04.760 --> 00:04:06.960]   And as part of our pipeline, we have
[00:04:06.960 --> 00:04:09.720]   a model that classifies what's on the field in a frame
[00:04:09.720 --> 00:04:10.800]   of video.
[00:04:10.800 --> 00:04:12.120]   But we have a big problem.
[00:04:12.120 --> 00:04:13.740]   We've detected some drift.
[00:04:13.740 --> 00:04:16.560]   The model in production is no longer performing well.
[00:04:16.560 --> 00:04:18.700]   And we have to retrain that model
[00:04:18.700 --> 00:04:21.200]   on some fresh labeled data we've just
[00:04:21.200 --> 00:04:23.060]   gotten back from the labelers.
[00:04:23.060 --> 00:04:26.620]   And so the team has asked me to come in and retrain
[00:04:26.620 --> 00:04:28.900]   a model on the latest data.
[00:04:28.900 --> 00:04:30.560]   So this could seem really simple, right?
[00:04:30.560 --> 00:04:33.300]   This is a straightforward process, train a model.
[00:04:33.300 --> 00:04:34.660]   But you'd be surprised.
[00:04:34.660 --> 00:04:37.740]   Even some of our most advanced customers,
[00:04:37.740 --> 00:04:40.020]   the process, the wild goose chase
[00:04:40.020 --> 00:04:42.940]   of finding all of the details to reproduce a model
[00:04:42.940 --> 00:04:44.940]   can sometimes be really frustrating.
[00:04:44.940 --> 00:04:47.980]   And so I'm going to walk through a very quick demo
[00:04:47.980 --> 00:04:50.700]   to show you how this is possible in WMB.
[00:04:50.700 --> 00:04:54.500]   So retraining, evaluating, and deploying.
[00:04:54.500 --> 00:04:55.980]   Are you ready?
[00:04:55.980 --> 00:04:57.900]   Let's get started.
[00:04:57.900 --> 00:04:59.740]   So here on the side of the page, I
[00:04:59.740 --> 00:05:03.060]   have all of the previous experiments in the project.
[00:05:03.060 --> 00:05:05.580]   So I can actually see all of the previous work.
[00:05:05.580 --> 00:05:07.460]   I'm not starting in the dark.
[00:05:07.460 --> 00:05:09.800]   I can also compare the model's performance.
[00:05:09.800 --> 00:05:12.260]   So I can see how different models line up.
[00:05:12.260 --> 00:05:15.220]   And I can even see sample predictions.
[00:05:15.220 --> 00:05:18.260]   So what's the actual data the model's looking at?
[00:05:18.260 --> 00:05:21.100]   And here, it's the soccer field.
[00:05:21.100 --> 00:05:24.000]   And I can see notes that the team has shared with me.
[00:05:24.000 --> 00:05:27.460]   So any context that I might need to get started.
[00:05:27.460 --> 00:05:31.580]   And here, they've actually set up a training script
[00:05:31.580 --> 00:05:34.900]   that I can fork and reuse here in this job.
[00:05:34.900 --> 00:05:36.860]   So I'm going to come in here and just pull down
[00:05:36.860 --> 00:05:40.580]   that previous work in this model training job.
[00:05:40.580 --> 00:05:43.300]   And I can actually see all of the previous runs
[00:05:43.300 --> 00:05:45.580]   that were created using this job.
[00:05:45.580 --> 00:05:48.420]   So really easy to access those previous results
[00:05:48.420 --> 00:05:50.220]   and see the details.
[00:05:50.220 --> 00:05:52.180]   So the ML Ops team that set this up
[00:05:52.180 --> 00:05:56.980]   is recommending I use our AWS cluster that's connected to WMB
[00:05:56.980 --> 00:05:58.500]   and has shared a little bit about how
[00:05:58.500 --> 00:06:01.500]   to update the config, so the hyper parameters
[00:06:01.500 --> 00:06:03.460]   and the data set for training.
[00:06:03.460 --> 00:06:06.820]   Now, let's actually launch this directly from WMB.
[00:06:06.820 --> 00:06:10.900]   So here, I can easily just fill in all of these details
[00:06:10.900 --> 00:06:12.100]   from a previous run.
[00:06:12.100 --> 00:06:13.700]   I don't have to start from scratch.
[00:06:13.700 --> 00:06:16.340]   So I'm picking a previous run to fill in the config.
[00:06:16.340 --> 00:06:17.840]   That's the hyper parameters.
[00:06:17.840 --> 00:06:20.920]   I can also tweak settings if I want to manually.
[00:06:20.920 --> 00:06:24.580]   And crucially, I can update the data set to the latest data
[00:06:24.580 --> 00:06:27.060]   set with just one line here.
[00:06:27.060 --> 00:06:28.820]   So now I'm pointing to the latest data.
[00:06:28.820 --> 00:06:29.500]   That's it.
[00:06:29.500 --> 00:06:31.700]   That's how I'm going to retrain the model.
[00:06:31.700 --> 00:06:34.380]   And how do I send it to the AWS cluster?
[00:06:34.380 --> 00:06:37.340]   Well, it's already connected right in here for me.
[00:06:37.340 --> 00:06:40.140]   I also have some other compute clusters available to me
[00:06:40.140 --> 00:06:42.060]   if I want to launch somewhere else.
[00:06:42.060 --> 00:06:45.540]   But in this case, I'm going to use AWS.
[00:06:45.540 --> 00:06:47.900]   Now I'm ready to launch that new training
[00:06:47.900 --> 00:06:50.140]   run using the latest data.
[00:06:50.140 --> 00:06:51.500]   So retraining is happening.
[00:06:51.500 --> 00:06:53.100]   We just did the first step.
[00:06:53.100 --> 00:06:56.060]   OK, so we can see in this first row of the table,
[00:06:56.060 --> 00:06:59.660]   we've got that new run starting on our AWS cluster.
[00:06:59.660 --> 00:07:01.680]   And I want to clarify, this isn't a cluster
[00:07:01.680 --> 00:07:03.200]   that WMB is hosting.
[00:07:03.200 --> 00:07:05.860]   This is actually something that the ML Ops team is managing
[00:07:05.860 --> 00:07:07.700]   and just connected to WMB.
[00:07:07.700 --> 00:07:09.900]   So it's very flexible and configurable.
[00:07:09.900 --> 00:07:11.380]   And it can be anywhere.
[00:07:11.380 --> 00:07:13.700]   Now let's look at how that model is performing.
[00:07:13.700 --> 00:07:16.180]   How's the training going for my new model?
[00:07:16.180 --> 00:07:17.300]   That's looking pretty good.
[00:07:17.300 --> 00:07:19.140]   It's beating our previous results.
[00:07:19.140 --> 00:07:20.100]   So what happens now?
[00:07:20.100 --> 00:07:22.420]   I want to take the model that was produced from that
[00:07:22.420 --> 00:07:24.540]   experiment and do something with it.
[00:07:24.540 --> 00:07:26.780]   I want to move it to the next step.
[00:07:26.780 --> 00:07:30.100]   So I can come here and just pull up that model that was saved
[00:07:30.100 --> 00:07:32.440]   and then move it from training, where I just
[00:07:32.440 --> 00:07:34.640]   retrained it, to evaluation.
[00:07:34.640 --> 00:07:36.720]   And here I can do that with a simple click,
[00:07:36.720 --> 00:07:38.800]   linking it to the model registry.
[00:07:38.800 --> 00:07:42.320]   So that's that centralized home for all of our best models
[00:07:42.320 --> 00:07:44.720]   that are good candidates for deployment.
[00:07:44.720 --> 00:07:46.680]   And here I'm linking it in.
[00:07:46.680 --> 00:07:48.680]   Great, now I'm pulling up the registry.
[00:07:48.680 --> 00:07:52.280]   And the team probably cares that I just did this, right?
[00:07:52.280 --> 00:07:53.760]   But who do I notify?
[00:07:53.760 --> 00:07:57.200]   Well, in Slack, they just got a notification in the channel.
[00:07:57.200 --> 00:07:59.080]   And you can see that here in the corner,
[00:07:59.080 --> 00:08:02.020]   the Slack channel just pinged, hey, you've got a new model.
[00:08:02.020 --> 00:08:05.380]   So everybody's on the same page about the changes I'm making.
[00:08:05.380 --> 00:08:09.580]   You're keeping people organized about these production models.
[00:08:09.580 --> 00:08:13.220]   So now here's a description of the task for the model.
[00:08:13.220 --> 00:08:15.140]   So if someone's browsing our model registry,
[00:08:15.140 --> 00:08:18.140]   they can find a task related to their interests.
[00:08:18.140 --> 00:08:20.500]   And we can also see the latest version that's
[00:08:20.500 --> 00:08:23.200]   just been added in here and all of the previous versions
[00:08:23.200 --> 00:08:24.540]   of the model as well.
[00:08:24.540 --> 00:08:27.780]   Now, this is the really exciting part, the automations.
[00:08:27.780 --> 00:08:31.860]   So I can automatically evaluate any new model I linked in here.
[00:08:31.860 --> 00:08:33.660]   And this actually just kicked off.
[00:08:33.660 --> 00:08:35.180]   This is happening now.
[00:08:35.180 --> 00:08:36.900]   And deployment as well.
[00:08:36.900 --> 00:08:39.780]   So I can have an automation to deploy a model
[00:08:39.780 --> 00:08:41.540]   using an external tool.
[00:08:41.540 --> 00:08:43.740]   So let's look at the evaluation one.
[00:08:43.740 --> 00:08:46.300]   So that's actually using a GitHub Action.
[00:08:46.300 --> 00:08:48.740]   So when I linked the model, it automatically
[00:08:48.740 --> 00:08:51.980]   kicked off this external tool, this GitHub Action,
[00:08:51.980 --> 00:08:54.600]   to run this test and report.
[00:08:54.600 --> 00:08:56.580]   Now, what does the report look like?
[00:08:56.580 --> 00:08:58.940]   How can I see if the model is doing well?
[00:08:58.940 --> 00:09:02.180]   Well, I can actually track those results right back in WMB.
[00:09:02.180 --> 00:09:05.220]   So this external tool completely connects back
[00:09:05.220 --> 00:09:07.260]   to the central page where you were looking
[00:09:07.260 --> 00:09:08.780]   at the rest of your results.
[00:09:08.780 --> 00:09:11.260]   And here's the report that was just generated.
[00:09:11.260 --> 00:09:14.260]   And this is where you can easily see
[00:09:14.260 --> 00:09:16.280]   the comparison between my new model
[00:09:16.280 --> 00:09:18.340]   and the current production model.
[00:09:18.340 --> 00:09:20.260]   So I can see, OK, there's the GitHub Action that
[00:09:20.260 --> 00:09:21.340]   generated it.
[00:09:21.340 --> 00:09:25.080]   Here's the model, so the new model that I just trained.
[00:09:25.080 --> 00:09:28.020]   And I can also see metrics in the same central view.
[00:09:28.020 --> 00:09:31.180]   So I can compare how the model did on different metrics,
[00:09:31.180 --> 00:09:34.660]   as well as sample predictions of model performance.
[00:09:34.660 --> 00:09:36.780]   And those can be directly organized
[00:09:36.780 --> 00:09:40.460]   in this central report that's entirely customizable.
[00:09:40.460 --> 00:09:42.060]   Now, what do we do for deployment?
[00:09:42.060 --> 00:09:44.740]   Well, we probably want to ask somebody for a review.
[00:09:44.740 --> 00:09:48.180]   So I can comment in here and directly ask my colleague Ken
[00:09:48.180 --> 00:09:49.900]   to take a look at this model.
[00:09:49.900 --> 00:09:52.500]   Now, if I have role-based access controls,
[00:09:52.500 --> 00:09:54.900]   I can also actually deploy the model directly
[00:09:54.900 --> 00:09:58.720]   from this report using this simple button, where I just
[00:09:58.720 --> 00:10:00.740]   set this model to production.
[00:10:00.740 --> 00:10:03.320]   And then that kicks off another GitHub Action
[00:10:03.320 --> 00:10:05.120]   to deploy the model to production,
[00:10:05.120 --> 00:10:06.800]   just like we saw before.
[00:10:06.800 --> 00:10:09.180]   So this has gone through all of those three steps.
[00:10:09.180 --> 00:10:11.240]   But we're always trying to make this more simple.
[00:10:11.240 --> 00:10:13.520]   So here's something that we're cooking up.
[00:10:13.520 --> 00:10:16.080]   The idea is adding a deployment button directly
[00:10:16.080 --> 00:10:17.400]   to the Vottle registry.
[00:10:17.400 --> 00:10:19.800]   So you could just pick, say, a SageMaker endpoint,
[00:10:19.800 --> 00:10:20.800]   and then click Deploy.
[00:10:20.800 --> 00:10:23.120]   And it would run that job on your behalf,
[00:10:23.120 --> 00:10:25.440]   so easily deploy your model.
[00:10:25.440 --> 00:10:29.760]   Now, I know that has been just a real whirlwind.
[00:10:29.760 --> 00:10:34.760]   So I'm going to pause and summarize.
[00:10:34.760 --> 00:10:36.920]   Beyond experiment tracking, we're
[00:10:36.920 --> 00:10:41.020]   making the production workflow more efficient.
[00:10:41.020 --> 00:10:44.680]   We're making it easy to automate evaluation, communicate
[00:10:44.680 --> 00:10:49.520]   your results, and then deploy models to production.
[00:10:49.520 --> 00:10:52.640]   It's been so awesome to get to share this with you today.
[00:10:52.640 --> 00:10:54.920]   Now, back over to you, Lucas.
[00:10:54.920 --> 00:10:57.720]   [APPLAUSE]
[00:10:57.720 --> 00:11:01.080]   [MUSIC PLAYING]
[00:11:01.080 --> 00:11:04.540]   [MUSIC]

