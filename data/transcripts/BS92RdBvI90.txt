
[00:00:00.000 --> 00:00:15.000]   JXX LIU:
[00:00:15.000 --> 00:00:15.600]   JXX LIU: Hello, everyone.
[00:00:15.600 --> 00:00:16.480]   My name is JXX.
[00:00:16.480 --> 00:00:19.280]   And I'm a founding engineer at K-Skill Labs.
[00:00:19.280 --> 00:00:22.080]   We build open source human robots from hardware
[00:00:22.080 --> 00:00:26.520]   to software to machine learning models.
[00:00:26.520 --> 00:00:29.360]   And we build it especially for developers.
[00:00:29.360 --> 00:00:31.920]   JXX LIU: Yeah, so humanoids have been getting a lot of hype
[00:00:31.920 --> 00:00:32.620]   recently.
[00:00:32.620 --> 00:00:33.980]   You saw the Tesla Optimus.
[00:00:33.980 --> 00:00:35.140]   You have Unitree robots.
[00:00:35.140 --> 00:00:38.300]   You have 1x and et cetera.
[00:00:38.300 --> 00:00:40.860]   They're quite proprietary, and they're quite expensive.
[00:00:40.860 --> 00:00:42.820]   And the humanoids are getting so much hype
[00:00:42.820 --> 00:00:46.700]   is because of very big problems, like physical labor shortage,
[00:00:46.700 --> 00:00:51.160]   consumer household, and also like off-world exploration
[00:00:51.160 --> 00:00:52.900]   and et cetera.
[00:00:52.900 --> 00:00:55.540]   Yeah, so for us at K-Skill Labs, our goal
[00:00:55.540 --> 00:00:58.720]   is to really solve general purpose robotics for everyone,
[00:00:58.720 --> 00:01:02.240]   and open sourcing the entire stack to the entire world.
[00:01:02.240 --> 00:01:04.760]   So everyone will be benefiting from this really, really
[00:01:04.760 --> 00:01:10.900]   useful technology instead of a few different companies.
[00:01:10.900 --> 00:01:14.740]   Yeah, and our team is about 15 people in Palo Alto.
[00:01:14.740 --> 00:01:15.900]   You could visit us anytime.
[00:01:15.900 --> 00:01:19.020]   We're launching some robots in the next coming month.
[00:01:19.020 --> 00:01:22.560]   And yeah, I'll be demoing some robots in the slides.
[00:01:22.560 --> 00:01:25.940]   Cool.
[00:01:25.940 --> 00:01:29.560]   So we're currently working on two robots, the K-Bot and Z-Bot.
[00:01:29.560 --> 00:01:33.180]   The K-Bot is a 4.11 humanoid robot
[00:01:33.180 --> 00:01:35.060]   that we made in the last five months.
[00:01:35.060 --> 00:01:37.340]   It has a full aluminum body, runs an RL controller
[00:01:37.340 --> 00:01:38.620]   for locomotion.
[00:01:38.620 --> 00:01:41.140]   And it's pretty sensor complete.
[00:01:41.140 --> 00:01:43.140]   So you can do all the really cool tasks
[00:01:43.140 --> 00:01:46.300]   that the previous presenter was showing.
[00:01:46.300 --> 00:01:48.760]   And this will also be one of the cheapest humanoid robots
[00:01:48.760 --> 00:01:49.640]   on the market.
[00:01:49.640 --> 00:01:54.260]   And it's ready for pre-order right now, delivered by October.
[00:01:54.260 --> 00:01:54.920]   Yeah.
[00:01:54.920 --> 00:01:58.900]   So if you come visit, there's a demo we do.
[00:01:58.900 --> 00:02:00.800]   You can kick the robot.
[00:02:00.800 --> 00:02:02.520]   The controller is quite robust.
[00:02:02.520 --> 00:02:06.140]   And the robot can take a lot of damage.
[00:02:06.140 --> 00:02:08.240]   On the robot itself, you also have--
[00:02:08.240 --> 00:02:12.080]   basically, you can do VR teleoperation with building hands.
[00:02:12.080 --> 00:02:14.760]   I'll describe the modularity a bit more.
[00:02:14.760 --> 00:02:16.980]   But yeah.
[00:02:16.980 --> 00:02:19.040]   So the robot is able to run a bunch
[00:02:19.040 --> 00:02:20.900]   of different local manipulation policies,
[00:02:20.900 --> 00:02:24.760]   running our own RL training framework.
[00:02:24.760 --> 00:02:26.760]   So when we started building the K-Bot,
[00:02:26.760 --> 00:02:30.880]   we thought about how do we make humanoids scale in the future?
[00:02:30.880 --> 00:02:34.420]   And what makes it possible for people
[00:02:34.420 --> 00:02:36.080]   to adopt humanoid robots?
[00:02:36.080 --> 00:02:38.920]   So we basically designed this humanoid robot
[00:02:38.920 --> 00:02:42.460]   to be the simplest factor as possible.
[00:02:42.460 --> 00:02:45.240]   And it's going to be the most affordable developer
[00:02:45.240 --> 00:02:48.880]   and research-grade humanoid robot at $9,000.
[00:02:48.880 --> 00:02:52.040]   The next cheapest option is probably at $40,000,
[00:02:52.040 --> 00:02:53.920]   which is the unit tree robot.
[00:02:53.920 --> 00:02:57.800]   This is roughly the cost of most robot arms today.
[00:02:57.800 --> 00:03:02.320]   Like if you buy UR5, that's, I think, $15,000, et cetera.
[00:03:02.320 --> 00:03:04.800]   So the entire robot is going to be open source.
[00:03:04.800 --> 00:03:07.160]   What that means is we're going to have open bomb.
[00:03:07.160 --> 00:03:10.100]   Every single piece of the hardware, CAD design, electronics,
[00:03:10.100 --> 00:03:14.160]   PCB, software, machine learning models will be fully open sourced.
[00:03:14.160 --> 00:03:17.920]   So you can replicate those if you want to.
[00:03:17.920 --> 00:03:20.700]   And it's also going to be very, very modular, which means
[00:03:20.700 --> 00:03:23.880]   that you can basically change out end effectors.
[00:03:23.880 --> 00:03:27.040]   We have different mechanical designs that you can easily
[00:03:27.040 --> 00:03:29.680]   make interface with our end effectors.
[00:03:29.680 --> 00:03:32.040]   And you can just take out the hand to--
[00:03:32.040 --> 00:03:35.920]   for a parallel gripper instead of a five fingers hand.
[00:03:35.920 --> 00:03:39.520]   or you can use whatever end effector you want to use,
[00:03:39.520 --> 00:03:41.000]   like the wumi grippers or et cetera.
[00:03:41.000 --> 00:03:44.800]   And that means we can also easily upgrade and also fix the robot.
[00:03:44.800 --> 00:03:49.200]   So our goal with selling this to developers is that when we have new hardware updates,
[00:03:49.200 --> 00:03:53.800]   you can easily just re-screw the robot in with brand new legs or brand new arms
[00:03:53.800 --> 00:03:56.760]   and also brand new head.
[00:03:56.760 --> 00:04:00.440]   So, you know, compute improves, like you get new NVIDIA chips,
[00:04:00.440 --> 00:04:03.240]   you can easily just add a new head onto the robot.
[00:04:03.240 --> 00:04:11.120]   Yeah, we're also building--we have built an entire Python/Rust SDK for people to use.
[00:04:11.120 --> 00:04:14.920]   If you come visit us, you can start programming this robot basically immediately.
[00:04:14.920 --> 00:04:17.880]   It's like pip install package and you can start working on it.
[00:04:17.880 --> 00:04:22.840]   And it's capable of running the latest state-of-the-art ML algorithms.
[00:04:22.840 --> 00:04:28.680]   So in terms of local motion, you can use like NVIDIA, Isaacson to train a PPO policy.
[00:04:28.680 --> 00:04:34.160]   We use MJX, which I'll explain a bit later, but you can use all kinds of different frameworks.
[00:04:34.160 --> 00:04:37.760]   You can also run like different VLMs, like language models on the robot.
[00:04:37.760 --> 00:04:40.600]   Maybe not locally directly, but you can run it through cloud.
[00:04:40.600 --> 00:04:45.160]   Or you can run VLAs and et cetera as well.
[00:04:45.160 --> 00:04:48.960]   So this robot, by default, it's going to be five DOF arms,
[00:04:48.960 --> 00:04:51.880]   but you can easily interchange to a seven DOF arm.
[00:04:51.880 --> 00:04:55.400]   So that'll suit most of the research need, research labs need.
[00:04:55.400 --> 00:05:00.680]   And we'll make continuous like model and software improvements with OTA rollouts.
[00:05:00.680 --> 00:05:05.160]   So every week, basically, we'll make new changes to the software as we go.
[00:05:05.160 --> 00:05:09.240]   Yeah, here are some more specs.
[00:05:09.240 --> 00:05:11.720]   I can't release the full spec yet because we're launching soon.
[00:05:11.720 --> 00:05:20.520]   But yeah, you see, it uses MIT cheetah actuators, pretty standard components with like MUs,
[00:05:20.520 --> 00:05:27.800]   just different audio modules, displays, cameras, and up to 250 TOPS compute currently.
[00:05:30.680 --> 00:05:35.240]   Yeah, and we really started this project in about October last year.
[00:05:35.240 --> 00:05:36.680]   So we'll be moving pretty fast.
[00:05:36.680 --> 00:05:39.480]   We have brought this to mass manufacturing.
[00:05:39.480 --> 00:05:43.800]   And we have a new, basically, design that we completed that I can't show you now,
[00:05:43.800 --> 00:05:45.800]   but you'll be able to see in two weeks.
[00:05:45.800 --> 00:05:50.600]   Yeah, so we started off this -- the KBOT is like the KScale Stompy project,
[00:05:50.600 --> 00:05:53.480]   like a full-sized humanoid robot that's 3D printable.
[00:05:53.480 --> 00:05:58.520]   Then we move to like a prototype, and we work with different manufacturing partners to actually make
[00:05:58.520 --> 00:06:00.840]   the new one that you see.
[00:06:00.840 --> 00:06:06.920]   Yeah, it's launching soon if you're interested in the KBOT.
[00:06:06.920 --> 00:06:08.360]   And you can go to kscale.dev.
[00:06:08.360 --> 00:06:11.320]   Yeah, I'll give you a second.
[00:06:11.320 --> 00:06:12.520]   No, I'm done.
[00:06:12.520 --> 00:06:14.200]   Whoa, that's only one robot.
[00:06:14.200 --> 00:06:16.040]   Not even one-third finished.
[00:06:16.040 --> 00:06:17.960]   I'm joking, joking.
[00:06:17.960 --> 00:06:22.680]   Yeah, and so what if you can't spend $9,000 on a cool humanoid robot?
[00:06:22.680 --> 00:06:26.520]   What if your, like, wife or husband doesn't allow you to do it?
[00:06:26.520 --> 00:06:36.280]   Well, introducing the ZBOT, which is a 1.5 feet humanoid robot that we also made at KScale Labs.
[00:06:36.280 --> 00:06:38.600]   So this started from a hackathon project we did.
[00:06:38.600 --> 00:06:42.120]   It became really popular on Twitter and also WeChat.
[00:06:42.120 --> 00:06:47.000]   And so we're bringing this robot to mass manufacturing as well.
[00:06:47.000 --> 00:06:49.480]   It runs the same locomotion and software stack.
[00:06:49.480 --> 00:06:54.360]   Like, that means you can basically program stuff for the small robot,
[00:06:54.360 --> 00:06:56.840]   but you can also put it on the big robot.
[00:06:56.840 --> 00:06:58.760]   So, you know, if you make, like, a voice chatting app,
[00:06:58.760 --> 00:07:01.080]   you can just put it on the idle robots.
[00:07:01.080 --> 00:07:03.880]   And it runs also locomotion policy as well.
[00:07:03.880 --> 00:07:06.200]   It works out with all the simulators.
[00:07:06.200 --> 00:07:12.680]   Yeah, we really got inspired by the Google DeepMinds robot soccer paper,
[00:07:12.680 --> 00:07:14.440]   where, you know, it runs around play soccer.
[00:07:14.440 --> 00:07:16.600]   That's really how we were envisioning it.
[00:07:16.600 --> 00:07:22.920]   So, yeah, we have a pretty good-- the launch went really well for the 3D printing one.
[00:07:22.920 --> 00:07:25.400]   So our Discord has about 5,000 people.
[00:07:25.400 --> 00:07:31.400]   I think a few hundred people have actually made a 3D printed one on the orange one on the bottom.
[00:07:31.400 --> 00:07:39.320]   Yeah, so we also started this project in November, and we are already bringing it into mass manufacturing,
[00:07:39.320 --> 00:07:41.160]   which we'll also be launching very soon.
[00:07:41.160 --> 00:07:46.360]   Some people-- yeah, we also run, like, monthly hackathons, so you can just come try out the robot.
[00:07:46.360 --> 00:07:50.760]   Yeah, same, same website.
[00:07:50.760 --> 00:07:51.800]   Okay.
[00:07:51.800 --> 00:07:53.800]   Okay, that's the hardware stuff.
[00:07:53.800 --> 00:07:57.800]   We talked about the hardware components we just open sourced.
[00:07:57.800 --> 00:08:00.840]   Oh, yeah, also Zbot will be fully open sourced as well.
[00:08:00.840 --> 00:08:05.960]   And so we also open sourced our entire ML and software stack.
[00:08:05.960 --> 00:08:13.320]   So really, like, our core angle is basically to make this-- make the kbot autonomous.
[00:08:13.320 --> 00:08:17.080]   Well, so, you know, it's a pretty standard dual policy.
[00:08:17.080 --> 00:08:19.320]   You have the high-level controller, which is a VLA.
[00:08:19.320 --> 00:08:22.040]   Then you have the RL whole body locomotion policy.
[00:08:22.040 --> 00:08:23.320]   Yeah.
[00:08:25.400 --> 00:08:30.520]   So what we really want right now is to basically finish-- we're currently working on both, basically.
[00:08:30.520 --> 00:08:32.760]   The RL part and also the VLA part.
[00:08:32.760 --> 00:08:37.240]   And we also made our own firmware/software architecture to power these robots in Rust.
[00:08:37.240 --> 00:08:39.480]   Yeah.
[00:08:39.480 --> 00:08:41.960]   Our end goal is basically to make the robot so easy to use.
[00:08:41.960 --> 00:08:46.200]   Any developer could write apps for robots.
[00:08:46.200 --> 00:08:49.240]   So, you know, Python application that you can re-share with people.
[00:08:49.240 --> 00:08:54.360]   You make the robot do some very specific use cases that can be reused by other people.
[00:08:54.360 --> 00:08:56.200]   It's almost like an app store.
[00:08:56.200 --> 00:09:01.960]   And to do that, basically, we offer a lot of really cool developer tools we've been working
[00:09:01.960 --> 00:09:03.160]   on in the last six months.
[00:09:03.160 --> 00:09:08.440]   So we open sourced the library for, basically, GPU-accelerated robot learning.
[00:09:08.440 --> 00:09:11.480]   Well, it's mostly like locomotion manipulation training.
[00:09:11.480 --> 00:09:13.720]   We used MJX for this.
[00:09:13.720 --> 00:09:18.840]   And, yeah, the video of you saw us kicking the robot, it runs the controller--
[00:09:18.840 --> 00:09:23.240]   RL controller-- RL model that we trained in this training framework.
[00:09:23.240 --> 00:09:25.480]   Yeah.
[00:09:25.480 --> 00:09:31.240]   We also are working on to basically being able to integrate and fine-tune all the different VLA
[00:09:31.240 --> 00:09:34.680]   and generalist policies that you see from Pi Zero and also NVIDIA group
[00:09:34.680 --> 00:09:36.680]   that we're also presenting today.
[00:09:36.680 --> 00:09:38.520]   So this robot will be able to run--
[00:09:38.520 --> 00:09:42.760]   you know, we're trying to make infrastructure very easy to run any cool models that you see
[00:09:42.760 --> 00:09:43.560]   that will be useful.
[00:09:43.560 --> 00:09:47.240]   Yeah.
[00:09:47.240 --> 00:09:52.040]   We also made this operating system, which is like a software framework plus like a Python
[00:09:52.040 --> 00:09:55.880]   interface that you can use to program the robot using Python or Rust.
[00:09:55.880 --> 00:09:57.800]   So, you know, you can just instead of--
[00:09:57.800 --> 00:10:01.080]   I don't know, if you guys use Rust 1, Rust 2, they're pretty hard to set up.
[00:10:01.080 --> 00:10:05.000]   But using our system, you can just install Python package, like pip install KOS,
[00:10:05.000 --> 00:10:06.760]   and you can start programming a robot.
[00:10:06.760 --> 00:10:08.120]   You just connect to IP.
[00:10:08.120 --> 00:10:09.960]   It's very, very easy to use.
[00:10:09.960 --> 00:10:13.000]   And we also have a digital twin in simulation.
[00:10:13.000 --> 00:10:14.120]   We call it a KOS-SIM.
[00:10:14.120 --> 00:10:18.920]   It has the same gRPC interface you can use for controlling robot in simulation.
[00:10:18.920 --> 00:10:23.640]   And all you have to do between programming something in simulation and real is by changing the IP address.
[00:10:24.680 --> 00:10:28.920]   So you can prototype really, really rapidly without having to worry about breaking the robot,
[00:10:28.920 --> 00:10:30.120]   which is very cool.
[00:10:30.120 --> 00:10:32.760]   So, yeah, this is also fully open sourced.
[00:10:32.760 --> 00:10:33.960]   You can try today.
[00:10:33.960 --> 00:10:38.200]   You can actually program the robot just by using KOS-SIM.
[00:10:38.200 --> 00:10:43.480]   Oh, I don't know what happened to the images.
[00:10:43.480 --> 00:10:44.040]   But yeah.
[00:10:44.040 --> 00:10:49.720]   And then, basically, what the machine learning and the operating system layers enables for us
[00:10:49.720 --> 00:10:54.280]   to run different policies, VLA models, and on our robot hardware.
[00:10:54.280 --> 00:10:56.680]   And for people to develop really cool applications with.
[00:10:56.680 --> 00:10:58.840]   So, yeah.
[00:10:58.840 --> 00:11:02.840]   I'm just going to go through like a very, very quick RL training and deployment examples
[00:11:02.840 --> 00:11:09.080]   of how researchers and developers could use our robot to train a local manipulation policy
[00:11:09.080 --> 00:11:13.080]   for the robot to, you know, grab different things or walk around or even dance.
[00:11:15.320 --> 00:11:15.880]   So, yeah.
[00:11:15.880 --> 00:11:15.880]   So, yeah.
[00:11:15.880 --> 00:11:17.480]   Our training setup is very easy.
[00:11:17.480 --> 00:11:19.560]   You just get cloned the repository.
[00:11:19.560 --> 00:11:22.920]   And then, all you have to do is run python-m train.
[00:11:22.920 --> 00:11:28.040]   And in this train.py, you effectively have all the training code you need abstracted.
[00:11:28.040 --> 00:11:30.440]   It's about 500 lines for walking.
[00:11:32.600 --> 00:11:32.920]   Yeah.
[00:11:32.920 --> 00:11:37.480]   And then, basically, you can run this on like, you know, using run-power your local GPU.
[00:11:37.480 --> 00:11:38.200]   It's MJX.
[00:11:38.200 --> 00:11:42.520]   So, it's like, you know, it's accelerated, accelerated compute.
[00:11:42.520 --> 00:11:46.840]   And training and walking policy roughly takes one hour to two hours.
[00:11:46.840 --> 00:11:47.480]   And, yeah.
[00:11:47.480 --> 00:11:51.000]   So, we're just going to run through like millions of different, not examples.
[00:11:51.000 --> 00:11:52.360]   Yeah.
[00:11:52.360 --> 00:11:54.600]   Iterations of the robot performing tasks you want.
[00:11:54.600 --> 00:11:57.560]   And you can tune the reward functions and et cetera.
[00:11:57.560 --> 00:12:02.200]   And you can see the loss and reward functions in our observability.
[00:12:02.200 --> 00:12:03.320]   Basically, tensor board.
[00:12:03.320 --> 00:12:05.480]   Yeah.
[00:12:05.480 --> 00:12:11.080]   And afterwards, when the robots finish training, you can easily evaluate it in KOSM.
[00:12:11.080 --> 00:12:16.760]   So, all you have to do is like, k-inverse-sim, like, you run the policy.
[00:12:17.720 --> 00:12:18.040]   Yeah.
[00:12:18.040 --> 00:12:20.200]   And in simulation, and you see the robot.
[00:12:20.200 --> 00:12:24.280]   If it's walking, if you can see if it's doing the thing you want it to be doing.
[00:12:24.280 --> 00:12:28.440]   For example, like walking, standing, picking up objects.
[00:12:28.440 --> 00:12:32.920]   And if that's really good in simulation, then you can just easily change the IP address.
[00:12:32.920 --> 00:12:35.080]   And then, you have sim to row deployment.
[00:12:35.080 --> 00:12:38.760]   Yeah.
[00:12:38.760 --> 00:12:40.840]   And it's very cool.
[00:12:40.840 --> 00:12:45.800]   Like, you can basically get a robot to work in like, one-tenth of the time of like,
[00:12:45.800 --> 00:12:49.480]   what you would take to set up most training libraries right now.
[00:12:49.480 --> 00:12:53.480]   And also, like, we're a team of 15 people.
[00:12:53.480 --> 00:12:56.040]   We do everything from hardware to software to ML.
[00:12:56.040 --> 00:13:01.080]   So, how we're able to do this is actually by working with our open source community.
[00:13:01.080 --> 00:13:07.000]   Currently, we have about like 5,000 active Discord members in a few servers.
[00:13:07.000 --> 00:13:10.200]   We have a lot of public open bounties that people are tackling.
[00:13:10.760 --> 00:13:16.440]   And because of our software's MIT lessons, yeah, a lot of people are coming to help us.
[00:13:16.440 --> 00:13:21.560]   And we also run hackathons almost on a bi-monthly basis that a lot of people come to participate.
[00:13:21.560 --> 00:13:24.120]   Yeah.
[00:13:24.120 --> 00:13:27.800]   So, we're also hiring electrical firmware and ML engineers.
[00:13:27.800 --> 00:13:31.800]   So, if you're interested, feel free to ask me.
[00:13:31.800 --> 00:13:35.160]   And then, also, go on the kscout.dev/joint website.
[00:13:35.160 --> 00:13:39.800]   Yeah, we're trying to hire a lot more cracked people to join us.
[00:13:42.040 --> 00:13:45.400]   So, we're launching the robots and software stack in about two, three weeks.
[00:13:45.400 --> 00:13:48.200]   If you're interested, follow on the website.
[00:13:48.200 --> 00:13:49.880]   I'll be happy to answer any questions.
[00:13:57.960 --> 00:13:58.680]   Yeah, sounds good.
[00:13:58.680 --> 00:14:00.040]   Go ahead.
[00:14:00.040 --> 00:14:00.280]   Yes.
[00:14:00.280 --> 00:14:03.720]   Where's the power?
[00:14:03.720 --> 00:14:04.120]   The battery?
[00:14:04.120 --> 00:14:05.640]   Is it battery packed?
[00:14:05.640 --> 00:14:07.080]   Yeah, it's battery packed.
[00:14:07.080 --> 00:14:08.600]   Yeah, it just has a battery.
[00:14:08.600 --> 00:14:10.920]   I don't know if I can show you in the picture.
[00:14:10.920 --> 00:14:13.080]   Yeah, it's behind this, basically.
[00:14:13.080 --> 00:14:16.040]   You can just slot in and it clicks in.
[00:14:16.040 --> 00:14:16.840]   Yeah.
[00:14:16.840 --> 00:14:30.680]   Yeah, the weight of the battery versus longevity.
[00:14:30.680 --> 00:14:31.080]   What do you mean?
[00:14:31.080 --> 00:14:32.760]   Yeah, longevity on the chart.
[00:14:32.760 --> 00:14:33.720]   Oh, like how long is?
[00:14:33.720 --> 00:14:34.440]   Yeah.
[00:14:34.440 --> 00:14:38.280]   Yeah, walking so far, our test is about two hours.
[00:14:38.280 --> 00:14:39.800]   But you can pass through.
[00:14:39.800 --> 00:14:41.800]   So, you can power through the wall plug.
[00:14:41.800 --> 00:14:46.760]   So, it's, yeah, you can just keep letting it charge and also run at the same time.
[00:14:47.400 --> 00:14:47.640]   Yeah.
[00:14:47.640 --> 00:14:49.880]   Yes.
[00:14:49.880 --> 00:14:53.000]   Well, black jacket, yeah.
[00:14:53.000 --> 00:14:58.920]   What are the use cases that you guys are imagining to start off with?
[00:14:58.920 --> 00:15:02.680]   Is this going to be more for like commercial, like, you know, factory kind of use cases?
[00:15:02.680 --> 00:15:05.640]   Or do you envision this more in the home, like helping out?
[00:15:05.640 --> 00:15:12.760]   Yeah, so basically, our bet, so a lot of companies, like especially in the US, are betting on like B2B.
[00:15:12.760 --> 00:15:15.000]   So, like Figure, for example, are selling to factories.
[00:15:15.000 --> 00:15:17.560]   Sims, Tesla itself is the customer.
[00:15:17.560 --> 00:15:21.960]   For us, our really bet is becoming the first US consumer robotics company.
[00:15:21.960 --> 00:15:24.920]   Like, a robot, a humanoid robotics company.
[00:15:24.920 --> 00:15:28.920]   Yeah, so we're really selling it to anyone that's interested in developing robotics.
[00:15:28.920 --> 00:15:33.320]   So, a lot of our current customers, we accidentally launched our robots.
[00:15:33.320 --> 00:15:37.080]   Like, people accidentally started buying our robots through our Shopify page.
[00:15:37.080 --> 00:15:39.000]   That was a complete mistake.
[00:15:39.000 --> 00:15:44.680]   But a lot of people that bought were just people genuinely interested in using for household tasks,
[00:15:44.680 --> 00:15:47.000]   like programming for different research.
[00:15:47.000 --> 00:15:52.840]   And also, there are a lot of companies also interested in working with us to make B2B businesses.
[00:15:52.840 --> 00:15:56.040]   Like, for example, the food demo that we just saw.
[00:15:56.040 --> 00:15:57.640]   Yeah.
[00:15:57.640 --> 00:15:59.080]   Sorry, if I can ask a follow-up question.
[00:15:59.080 --> 00:16:03.240]   Like, what household chores do you think it would be well suited for?
[00:16:03.240 --> 00:16:05.560]   Like, unloading the dishwasher, for example?
[00:16:05.560 --> 00:16:06.200]   Yeah, yeah.
[00:16:06.200 --> 00:16:11.720]   I mean, right now, we don't have any, I don't think anyone really has a fully working VLA model yet.
[00:16:11.720 --> 00:16:15.800]   So, right now, it's pretty limited to teleoperating system, sorry, teleoperation.
[00:16:15.800 --> 00:16:17.000]   So, yeah.
[00:16:17.000 --> 00:16:22.040]   But soon, we hope to be able to have, like, this navigation VLA stack for you to do, like,
[00:16:22.040 --> 00:16:26.040]   you know, folding clothes or, like, doing dishwashing as the model capabilities improve.
[00:16:26.040 --> 00:16:28.040]   Yes.
[00:16:28.040 --> 00:16:29.800]   White shirt?
[00:16:29.800 --> 00:16:33.160]   Oh, I mean, green shirt.
[00:16:33.160 --> 00:16:34.040]   Yeah, you can go first.
[00:16:34.040 --> 00:16:35.960]   Can I ask you?
[00:16:35.960 --> 00:16:36.680]   Yeah, of course.
[00:16:36.680 --> 00:16:37.720]   Okay, yeah.
[00:16:37.720 --> 00:16:43.320]   So, you kind of alluded to complexity of ROS2 in terms of the setup.
[00:16:43.320 --> 00:16:44.280]   Uh-huh.
[00:16:44.280 --> 00:16:48.440]   I'm wondering if there are other, like, benefits and trade-offs that you considered
[00:16:48.440 --> 00:16:51.720]   for foregoing something like ROS and ROS2 in that ecosystem?
[00:16:51.720 --> 00:16:52.120]   Oh, yeah.
[00:16:52.120 --> 00:16:53.720]   Like, why not use ROS, basically?
[00:16:53.720 --> 00:16:53.720]   Yeah.
[00:16:53.720 --> 00:16:54.280]   Yeah.
[00:16:54.280 --> 00:16:55.880]   Well, there are a lot of reasons.
[00:16:55.880 --> 00:16:57.320]   Our robot is mostly programmed.
[00:16:57.320 --> 00:17:00.120]   So, ROS is really good because, like, the nodes and stuff, right?
[00:17:00.120 --> 00:17:02.440]   So, like, the async, like, the communication.
[00:17:02.440 --> 00:17:05.720]   But for our robot, we really don't have that many sensors.
[00:17:05.720 --> 00:17:09.720]   And we really want to do this, like, model-based, like, policy-based robot.
[00:17:09.720 --> 00:17:14.600]   So, we don't have many complicated sensors that we need to, like, async-communicate at all times.
[00:17:14.600 --> 00:17:19.080]   The other part is, like, where I'm pretty opinionated.
[00:17:19.080 --> 00:17:22.520]   I used to ROS1 and ROS2, Foxy and Noetic.
[00:17:22.520 --> 00:17:27.640]   I've just had a pretty bad experience using it, having to set up Ubuntu, you know.
[00:17:27.640 --> 00:17:33.240]   Like, I just want a robot that I can just buy, open the box, it stands or walks,
[00:17:33.240 --> 00:17:36.120]   and then I can just start programming it using my computer.
[00:17:36.120 --> 00:17:36.360]   Yeah.
[00:17:36.360 --> 00:17:46.360]   What kind of AI accelerator is on each of the robots?
[00:17:46.360 --> 00:17:46.840]   Yeah.
[00:17:46.840 --> 00:17:51.720]   So, basically, for the KBOT, it's going to be JSON, Nano, and AGX.
[00:17:51.720 --> 00:17:53.720]   Yeah.
[00:17:53.720 --> 00:17:58.520]   So, yeah, there are different compute options you'll be able to choose when we launch.
[00:18:02.280 --> 00:18:03.240]   Yes, yeah, go ahead.
[00:18:03.240 --> 00:18:05.160]   How do you develop for this?
[00:18:05.160 --> 00:18:07.240]   So, you can just put either VR headset.
[00:18:07.240 --> 00:18:09.400]   So, there are a few different methods.
[00:18:09.400 --> 00:18:11.960]   So, the preferred option for a lot of people is VR headset.
[00:18:11.960 --> 00:18:15.880]   We have this, like, we also turn, like, a pseudo-IK.
[00:18:15.880 --> 00:18:21.080]   So, basically, it's, like, going to position using, like, an RL model,
[00:18:21.080 --> 00:18:22.600]   instead of just, like, calculating an IK.
[00:18:22.600 --> 00:18:25.000]   But, it works pretty well with our VR setup.
[00:18:25.000 --> 00:18:28.840]   So, you can move the hand gestures, you can click button to open and close gripper,
[00:18:28.840 --> 00:18:31.000]   and just, yeah, move your arm and stuff.
[00:18:31.000 --> 00:18:31.720]   Yeah.
[00:18:31.720 --> 00:18:33.560]   Can you still operate the small ones here?
[00:18:33.560 --> 00:18:34.120]   Yes.
[00:18:34.120 --> 00:18:34.440]   Yeah.
[00:18:34.440 --> 00:18:35.320]   You'll be able to.
[00:18:35.320 --> 00:18:37.000]   It runs the exact same software stack.
[00:18:37.000 --> 00:18:37.800]   Yeah.
[00:18:37.800 --> 00:18:38.360]   Last question.
[00:18:38.360 --> 00:18:40.920]   How does it compare to the Tesla?
[00:18:40.920 --> 00:18:43.480]   The Tesla humanoids?
[00:18:43.480 --> 00:18:49.160]   In terms of, like, mechanical powerness, like, you know, the Tesla's way more powerful.
[00:18:49.160 --> 00:18:51.480]   It has, like, linear actuators and et cetera.
[00:18:51.480 --> 00:18:55.320]   But, in terms of, like, actual use cases, I don't think it's really that different.
[00:18:55.320 --> 00:18:56.040]   Yeah.
[00:18:56.040 --> 00:19:01.000]   I mean, Tesla is actually built for, like, a factory type of use cases.
[00:19:01.000 --> 00:19:05.400]   But, in terms of, like, you want to, for people to actually buy and use this robot, it's not very different.
[00:19:05.400 --> 00:19:12.200]   I think the last time I heard Tesla Optimus is about 60k, at least.
[00:19:12.200 --> 00:19:14.920]   We can ask some Tesla engineers.
[00:19:14.920 --> 00:19:24.920]   But, our robot's $9,000 before mass production.

