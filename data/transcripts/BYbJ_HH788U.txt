
[00:00:00.000 --> 00:00:03.800]   Hi, welcome to this video on the Functional API, which
[00:00:03.800 --> 00:00:06.960]   is the second of the two approaches for building models
[00:00:06.960 --> 00:00:09.200]   in TensorFlow Keras.
[00:00:09.200 --> 00:00:12.560]   So we're going to go ahead and start our notebook.
[00:00:12.560 --> 00:00:14.600]   I'm just going to import TensorFlow.
[00:00:14.600 --> 00:00:20.360]   So the Functional API can produce
[00:00:20.360 --> 00:00:23.200]   the same architectures as the sequential model,
[00:00:23.200 --> 00:00:27.920]   plus a few more, thanks to a high level of flexibility.
[00:00:27.920 --> 00:00:29.920]   So the Functional API is ideal for when
[00:00:29.920 --> 00:00:32.480]   we have multiple inputs or outputs,
[00:00:32.480 --> 00:00:35.560]   different streams of information between layers,
[00:00:35.560 --> 00:00:39.520]   or anything else that is more complex than a simple linear
[00:00:39.520 --> 00:00:41.960]   model, where each input and output is
[00:00:41.960 --> 00:00:45.720]   feeding into the next one, and there's nothing fancy going on.
[00:00:45.720 --> 00:00:48.560]   An example of what this might look like
[00:00:48.560 --> 00:00:50.960]   is something like this.
[00:00:50.960 --> 00:00:54.400]   So you can see at the bottom here, we have two inputs.
[00:00:54.400 --> 00:00:56.440]   And then in the middle here, we also
[00:00:56.440 --> 00:01:00.720]   have this stream of data being split into this one layer,
[00:01:00.720 --> 00:01:04.720]   and then also being fed forward into the next layer as well.
[00:01:04.720 --> 00:01:08.320]   This is something that you could not do with the sequential
[00:01:08.320 --> 00:01:09.040]   model.
[00:01:09.040 --> 00:01:10.840]   You would have to use Functional API, which
[00:01:10.840 --> 00:01:13.000]   we're going to go through now.
[00:01:13.000 --> 00:01:16.520]   So rather than defining the model first,
[00:01:16.520 --> 00:01:21.440]   we instead define the input layers, the network layers,
[00:01:21.440 --> 00:01:24.200]   which is the same as what we did with the sequential model.
[00:01:24.200 --> 00:01:26.480]   And then we initialize the model with something
[00:01:26.480 --> 00:01:28.800]   called tf.keras.model.
[00:01:28.800 --> 00:01:31.760]   Let's go ahead and define the input layer first.
[00:01:31.760 --> 00:01:38.600]   So here, we use something different.
[00:01:38.600 --> 00:01:40.200]   We don't use a dense layer.
[00:01:40.200 --> 00:01:42.320]   We're going to use a input layer, which is obviously
[00:01:42.320 --> 00:01:43.640]   specifically for inputs.
[00:01:43.640 --> 00:01:50.120]   Whereas before, we use input_shape
[00:01:50.120 --> 00:01:53.960]   to define the input shape, here we are using shape.
[00:01:54.760 --> 00:01:56.680]   And we're going to build the exact same model
[00:01:56.680 --> 00:02:00.560]   that we did before, but using the functional model instead.
[00:02:00.560 --> 00:02:05.840]   So the only difference here is that we also
[00:02:05.840 --> 00:02:08.080]   need to define the data type going
[00:02:08.080 --> 00:02:12.680]   into our inputs, which in this case will be industry of 32.
[00:02:12.680 --> 00:02:17.120]   If we don't define this, it will default to float with 32.
[00:02:17.120 --> 00:02:18.760]   And then we can execute that.
[00:02:18.760 --> 00:02:21.160]   And then we need to set up our network layers, which
[00:02:21.160 --> 00:02:23.800]   is exactly the same as what we did before.
[00:02:23.800 --> 00:02:27.360]   So if we go over to this image again,
[00:02:27.360 --> 00:02:30.880]   we can see that we have this sort of network architecture.
[00:02:30.880 --> 00:02:33.120]   So we have the 10 inputs, which is what we've already
[00:02:33.120 --> 00:02:34.320]   just defined.
[00:02:34.320 --> 00:02:38.280]   And then we have our 32 units in the middle here and 2 units
[00:02:38.280 --> 00:02:40.400]   on the output layer.
[00:02:40.400 --> 00:02:44.720]   So we need to build that using densely connected neural
[00:02:44.720 --> 00:02:46.320]   network layers.
[00:02:46.320 --> 00:02:49.080]   And we do that just like we did before.
[00:02:49.280 --> 00:02:51.080]   The only difference is that here,
[00:02:51.080 --> 00:02:57.200]   we are passing the previous layer into our new layer.
[00:02:57.200 --> 00:03:00.560]   So our previous layer, we have placed into this variable here,
[00:03:00.560 --> 00:03:03.360]   input sensor.
[00:03:03.360 --> 00:03:07.840]   And we also want to pass it to our new layer here.
[00:03:07.840 --> 00:03:10.600]   And then we are also putting this new layer
[00:03:10.600 --> 00:03:13.240]   or new transformation into a new layer.
[00:03:13.240 --> 00:03:17.440]   So we're just going to pass it into our new layer here.
[00:03:17.440 --> 00:03:20.040]   And we're going to pass this new transformation
[00:03:20.040 --> 00:03:23.720]   into another variable, which is x.
[00:03:23.720 --> 00:03:25.920]   I mean, you can use these names as you want.
[00:03:25.920 --> 00:03:30.240]   But the inputs and the output layer,
[00:03:30.240 --> 00:03:33.840]   they need to have names which are not overwritten
[00:03:33.840 --> 00:03:35.640]   by any of your other layers, because we
[00:03:35.640 --> 00:03:39.720]   need to pass those layers into our model initializer
[00:03:39.720 --> 00:03:41.720]   in a moment, which you will see.
[00:03:41.720 --> 00:03:45.200]   So we will call our output layer y.
[00:03:45.200 --> 00:03:53.920]   And then we're going to use the softmax activation here again.
[00:03:53.920 --> 00:03:57.640]   And we pass x, which is our previous layer.
[00:03:57.640 --> 00:04:00.200]   Now, if we had more than just these layers,
[00:04:00.200 --> 00:04:02.640]   we could use x in the middle and overwrite.
[00:04:02.640 --> 00:04:05.040]   And that wouldn't cause any issues,
[00:04:05.040 --> 00:04:07.600]   just as long as we don't overwrite
[00:04:07.600 --> 00:04:12.160]   the variables which define our output and input.
[00:04:12.160 --> 00:04:20.240]   And the reason that we don't overwrite those
[00:04:20.240 --> 00:04:24.360]   is because we need them to initialize our model, which
[00:04:24.360 --> 00:04:25.120]   we do like this.
[00:04:25.120 --> 00:04:35.720]   So inputs, we have our input tensor.
[00:04:35.720 --> 00:04:38.960]   And outputs, we have y.
[00:04:38.960 --> 00:04:42.400]   So this is where, if we did have multiple inputs or outputs,
[00:04:42.400 --> 00:04:45.640]   we would be able to put them together here.
[00:04:45.640 --> 00:04:49.840]   So rather than having y, we could have y and y2,
[00:04:49.840 --> 00:04:51.120]   for example.
[00:04:51.120 --> 00:04:54.880]   But in our case, we are only using one.
[00:04:54.880 --> 00:04:56.880]   So now we initialize the model.
[00:04:56.880 --> 00:05:00.800]   And if we print our model summary,
[00:05:00.800 --> 00:05:04.560]   we get the same output as we did with the sequential model
[00:05:04.560 --> 00:05:06.560]   with a few small differences.
[00:05:06.560 --> 00:05:09.480]   So at the top here, rather than saying sequential,
[00:05:09.480 --> 00:05:11.040]   it says model, which obviously, this
[00:05:11.040 --> 00:05:13.520]   is because we're using the sequential model
[00:05:13.520 --> 00:05:14.960]   or the functional API.
[00:05:14.960 --> 00:05:17.400]   It will change what we have up here.
[00:05:17.400 --> 00:05:19.840]   And then for the functional API, we also
[00:05:19.840 --> 00:05:23.360]   have a specific definition for the input layer
[00:05:23.360 --> 00:05:27.080]   as well, which is just because we have actually explicitly
[00:05:27.080 --> 00:05:30.320]   defined it here rather than it being an input shape to one
[00:05:30.320 --> 00:05:31.880]   of the other layers.
[00:05:31.880 --> 00:05:35.840]   And that is everything for the functional API.
[00:05:35.840 --> 00:05:39.440]   So I hope that has been useful and I will see you in the next one.
[00:05:39.440 --> 00:05:40.440]   Thanks for watching.

