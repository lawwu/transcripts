
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:14.040]   - Hey everybody, welcome back to the WNB Salon Series.
[00:00:14.040 --> 00:00:16.040]   This has not happened for a long time,
[00:00:16.040 --> 00:00:18.240]   but we're super excited to bring it back.
[00:00:18.240 --> 00:00:20.320]   And I'm super excited about the fact
[00:00:20.320 --> 00:00:22.080]   that you have a better host today.
[00:00:22.080 --> 00:00:25.720]   Hamil Hussain will be hosting today's session on AutoML.
[00:00:25.720 --> 00:00:26.920]   What does that word even mean?
[00:00:26.920 --> 00:00:30.240]   And how is it going to impact our lives?
[00:00:30.240 --> 00:00:32.640]   The awesome panel here will be discussing that
[00:00:32.640 --> 00:00:35.600]   and Hamil will be leading this discussion.
[00:00:35.600 --> 00:00:38.920]   Hamil is currently building ML tools at Autobounce.
[00:00:38.920 --> 00:00:42.440]   He's one of those triple six contributors of FastAI,
[00:00:42.440 --> 00:00:44.640]   one of the core maintainers.
[00:00:44.640 --> 00:00:47.720]   Previously he was at GitHub, Airbnb and DataRobot.
[00:00:47.720 --> 00:00:51.600]   So he has a good amount of experience in AutoML tools
[00:00:51.600 --> 00:00:52.960]   and tools generally speaking.
[00:00:52.960 --> 00:00:55.420]   With that, Hamil I'd like to hand it over to you.
[00:00:56.420 --> 00:00:58.580]   - Thanks a lot, Sanyam.
[00:00:58.580 --> 00:01:02.460]   So yeah, I'm really excited to be here to talk about AutoML.
[00:01:02.460 --> 00:01:06.900]   But first I wanna introduce the all-star panel that we have.
[00:01:06.900 --> 00:01:09.060]   So I'll start with Shreya.
[00:01:09.060 --> 00:01:11.500]   So Shreya is here.
[00:01:11.500 --> 00:01:15.300]   She builds systems that operationalize ML workflows
[00:01:15.300 --> 00:01:18.780]   and has conducted lots of research on ML systems,
[00:01:18.780 --> 00:01:21.780]   including end-to-end observability.
[00:01:21.780 --> 00:01:23.540]   She has done ML and software engineering
[00:01:23.540 --> 00:01:26.060]   at Google Brain, Facebook and Viaduct
[00:01:26.060 --> 00:01:30.100]   and is currently doing a PhD at UC Berkeley,
[00:01:30.100 --> 00:01:32.940]   where she's taking a deeper look into databases.
[00:01:32.940 --> 00:01:36.700]   And it's really exciting to have her here
[00:01:36.700 --> 00:01:38.020]   to talk about AutoML.
[00:01:38.020 --> 00:01:40.460]   Next we have Zach Mayer.
[00:01:40.460 --> 00:01:42.760]   So Zach actually is the person
[00:01:42.760 --> 00:01:44.780]   that got me into data science.
[00:01:44.780 --> 00:01:49.780]   I was kind of just a lost consultant at one point.
[00:01:50.620 --> 00:01:54.460]   And Zach was kind of like one of the people that came in
[00:01:54.460 --> 00:01:59.460]   and kind of showed me what ML and data science is all about.
[00:01:59.460 --> 00:02:02.340]   He's actually one of the core contributors
[00:02:02.340 --> 00:02:05.500]   to a package called Carrot, which is an R package,
[00:02:05.500 --> 00:02:07.260]   which is my first experience with something
[00:02:07.260 --> 00:02:10.180]   that felt like AutoML.
[00:02:10.180 --> 00:02:12.100]   It was really interesting. - I did.
[00:02:12.100 --> 00:02:16.900]   - And it's a software that allows you to quickly try
[00:02:16.900 --> 00:02:19.980]   many different types of models and like a consistent API.
[00:02:20.940 --> 00:02:24.820]   And it was kind of was the thing in the time
[00:02:24.820 --> 00:02:27.220]   where there was this like classic ML,
[00:02:27.220 --> 00:02:30.820]   like classic ML before deep learning.
[00:02:30.820 --> 00:02:33.220]   It helped to kind of make sense of that space.
[00:02:33.220 --> 00:02:35.900]   So it's really exciting to have him here.
[00:02:35.900 --> 00:02:39.500]   Zach is currently leading data science at DataRobot.
[00:02:39.500 --> 00:02:42.740]   Next we have Ted Kortler.
[00:02:42.740 --> 00:02:46.340]   Ted is a data science educator.
[00:02:46.340 --> 00:02:49.640]   He's also done a lot of work with applied ML,
[00:02:49.640 --> 00:02:51.220]   especially in insurance.
[00:02:51.220 --> 00:02:55.460]   He specifically works on fairness and bias
[00:02:55.460 --> 00:02:57.260]   in ML systems as well,
[00:02:57.260 --> 00:02:59.660]   which is a very important topic for AutoML.
[00:02:59.660 --> 00:03:04.180]   And I've generally found that Ted is really good
[00:03:04.180 --> 00:03:07.140]   at focusing on the business impact of machine learning.
[00:03:07.140 --> 00:03:11.500]   And he's currently leading this genre of work at DataRobot.
[00:03:11.500 --> 00:03:15.460]   Next we have Robert Nishihara.
[00:03:15.460 --> 00:03:18.060]   He's a CEO of AnyScale, which is a framework
[00:03:18.060 --> 00:03:20.620]   that helps you scale and deploy ML,
[00:03:20.620 --> 00:03:24.140]   and which is built on the open source library Ray.
[00:03:24.140 --> 00:03:28.260]   He's also a member of RiseLab and Amplab at UC Berkeley,
[00:03:28.260 --> 00:03:31.620]   which has been the incubator of all kinds
[00:03:31.620 --> 00:03:36.280]   of cool technologies like Spark and Ray and other things.
[00:03:36.280 --> 00:03:40.140]   Next we have Dan Becker.
[00:03:40.140 --> 00:03:43.260]   He's currently the VP of product at DataRobot.
[00:03:43.260 --> 00:03:46.700]   But before this, he founded Decision AI,
[00:03:46.700 --> 00:03:49.580]   which where he built tools to help people measure
[00:03:49.580 --> 00:03:52.180]   and optimize the business value of ML.
[00:03:52.180 --> 00:03:54.300]   He's also the former head of Kaggle Learn.
[00:03:54.300 --> 00:03:57.580]   One of my favorite courses that he's made on there
[00:03:57.580 --> 00:04:00.260]   is a course about ML explainability.
[00:04:00.260 --> 00:04:02.560]   So I highly recommend everyone checks that out.
[00:04:02.560 --> 00:04:06.100]   Dan is also an economist by training,
[00:04:06.100 --> 00:04:08.180]   which is a nice perspective
[00:04:08.180 --> 00:04:10.740]   in a room of machine learning engineers.
[00:04:10.740 --> 00:04:13.540]   He's also the VP of product at DataRobot currently.
[00:04:15.700 --> 00:04:19.420]   - Okay, so with that introduction,
[00:04:19.420 --> 00:04:21.740]   first kind of question I think on everybody's mind,
[00:04:21.740 --> 00:04:24.780]   I think this is where people get really confused
[00:04:24.780 --> 00:04:28.580]   in my experience is like, what is AutoML?
[00:04:28.580 --> 00:04:33.580]   So like, for some reason, when I say the word AutoML,
[00:04:33.580 --> 00:04:37.220]   a lot of times people think that that is,
[00:04:37.220 --> 00:04:38.760]   the scope of that is really small,
[00:04:38.760 --> 00:04:41.500]   that it's just hyperparameter tuning
[00:04:41.500 --> 00:04:43.360]   and neural architecture search.
[00:04:44.620 --> 00:04:47.100]   But for me, it's a lot more than that.
[00:04:47.100 --> 00:04:49.020]   So I kind of wanna get everyone's thoughts
[00:04:49.020 --> 00:04:54.020]   kind of go around and kind of get your viewpoints on like,
[00:04:54.020 --> 00:04:56.260]   how would you define AutoML?
[00:04:56.260 --> 00:04:59.860]   Like, what does it mean to you to start with?
[00:04:59.860 --> 00:05:04.560]   So, okay, we can start with Dan.
[00:05:04.560 --> 00:05:10.740]   - Yeah, I mean, it's interesting.
[00:05:10.740 --> 00:05:14.380]   You said, you started by saying that to you,
[00:05:14.380 --> 00:05:16.540]   other people think that it can be as small
[00:05:16.540 --> 00:05:18.920]   as hyperparameter tuning and model search.
[00:05:18.920 --> 00:05:21.860]   And I think you view it more broadly.
[00:05:21.860 --> 00:05:25.300]   I actually would, I think there's a pretty broad range
[00:05:25.300 --> 00:05:30.300]   of AutoML systems, but if you had a for loop
[00:05:30.300 --> 00:05:33.340]   or something like a grid search in scikit-learn,
[00:05:33.340 --> 00:05:34.180]   and then you run a for loop,
[00:05:34.180 --> 00:05:35.540]   instead of got several different models,
[00:05:35.540 --> 00:05:37.180]   I'm gonna try them.
[00:05:37.180 --> 00:05:40.820]   I would describe that as a pretty rudimentary
[00:05:40.820 --> 00:05:42.400]   AutoML system.
[00:05:42.400 --> 00:05:46.460]   And then there are others that are, where AutoML,
[00:05:46.460 --> 00:05:52.060]   I think scaled up somewhat from that would be like TPOT,
[00:05:52.060 --> 00:05:53.920]   which is an open source Python library,
[00:05:53.920 --> 00:05:58.920]   but again, is mostly like model search.
[00:05:58.920 --> 00:06:01.480]   And then there are other AutoML systems
[00:06:01.480 --> 00:06:05.240]   where you have model explainability that's built into it.
[00:06:05.240 --> 00:06:08.920]   And it tries to be much more of the whole workflow.
[00:06:08.920 --> 00:06:13.920]   Some AutoML systems do have nice EDA functionality.
[00:06:13.920 --> 00:06:16.980]   And I guess I still call that part of,
[00:06:16.980 --> 00:06:19.100]   at least they consider that part of their system.
[00:06:19.100 --> 00:06:20.300]   So I think there's a pretty broad range,
[00:06:20.300 --> 00:06:22.800]   but I think it can be something as lightweight
[00:06:22.800 --> 00:06:26.720]   as having grid search in scikit-learn
[00:06:26.720 --> 00:06:29.360]   or neural architecture search.
[00:06:29.360 --> 00:06:32.520]   Yeah, I think there's a pretty broad range
[00:06:32.520 --> 00:06:35.120]   of things that I consider AutoML.
[00:06:36.760 --> 00:06:41.760]   - Cool, Shreya, what's your take on AutoML?
[00:06:41.760 --> 00:06:44.520]   Or how would you kind of characterize it
[00:06:44.520 --> 00:06:45.840]   from your point of view?
[00:06:45.840 --> 00:06:48.760]   - I think it's this vision that people are talking about
[00:06:48.760 --> 00:06:53.480]   where I have a data lake and I push a button
[00:06:53.480 --> 00:06:56.520]   and then out comes a model that's perfect.
[00:06:56.520 --> 00:06:58.840]   And somebody else will do something
[00:06:58.840 --> 00:07:01.540]   to get it to this perfect model or perfect predictions.
[00:07:01.540 --> 00:07:06.240]   And maybe some people are thinking non-data scientists,
[00:07:06.240 --> 00:07:07.800]   people who don't know anything about ML
[00:07:07.800 --> 00:07:11.000]   can now use AutoML and reap the benefits.
[00:07:11.000 --> 00:07:13.480]   Or maybe it's like the ML engineer who's toiling away
[00:07:13.480 --> 00:07:16.760]   can abstract away all the nitty gritty
[00:07:16.760 --> 00:07:18.400]   feature engineering code that they have to do
[00:07:18.400 --> 00:07:20.240]   and get something great, I don't know.
[00:07:20.240 --> 00:07:23.640]   But I think it's this larger vision
[00:07:23.640 --> 00:07:26.020]   where we just feel like we don't have any pain
[00:07:26.020 --> 00:07:28.020]   in developing ML models.
[00:07:28.020 --> 00:07:32.100]   That's very vague, but probably how I would define it.
[00:07:33.840 --> 00:07:36.300]   - Interesting, yeah, that makes a lot of sense.
[00:07:36.300 --> 00:07:38.720]   - Hamel, can I ask a quick question?
[00:07:38.720 --> 00:07:39.560]   - Yeah.
[00:07:39.560 --> 00:07:43.280]   - Of the group, do you guys feel like the operation,
[00:07:43.280 --> 00:07:48.080]   is it just on ML dev or is it also getting it to ML ops
[00:07:48.080 --> 00:07:51.400]   or getting it to a secured endpoint?
[00:07:51.400 --> 00:07:54.280]   Do you consider that part of the AutoML perspective?
[00:07:54.280 --> 00:07:55.480]   - I think it's a great question
[00:07:55.480 --> 00:07:58.360]   because no research paper I've ever read in AutoML
[00:07:58.360 --> 00:08:01.180]   has the word ops or concept of ops in it.
[00:08:01.180 --> 00:08:03.860]   But somehow they're able to pick the best features,
[00:08:03.860 --> 00:08:05.220]   do some end-to-end deep learning
[00:08:05.220 --> 00:08:07.060]   and come out with this great model.
[00:08:07.060 --> 00:08:07.900]   - Yeah.
[00:08:07.900 --> 00:08:10.260]   - I think it's an important part of the puzzle,
[00:08:10.260 --> 00:08:11.700]   but not the whole thing.
[00:08:11.700 --> 00:08:13.300]   - I think it's an important part of the story,
[00:08:13.300 --> 00:08:15.980]   but it's not part of the image that I have in my head
[00:08:15.980 --> 00:08:17.940]   when I hear the term AutoML.
[00:08:17.940 --> 00:08:20.340]   When I hear AutoML, it's similar to what Shreya was saying.
[00:08:20.340 --> 00:08:22.660]   I'm thinking of essentially a black box
[00:08:22.660 --> 00:08:26.740]   that takes in your data and then outputs a program
[00:08:26.740 --> 00:08:30.660]   or a model that can do the machine learning
[00:08:30.660 --> 00:08:32.260]   and you don't actually care about
[00:08:32.260 --> 00:08:34.340]   whether that's a neural network or a decision tree
[00:08:34.340 --> 00:08:35.940]   or how it works under the hood.
[00:08:35.940 --> 00:08:39.980]   (child mumbling)
[00:08:39.980 --> 00:08:45.300]   - I was gonna say, I think my perspective on ML
[00:08:45.300 --> 00:08:49.100]   is a lot like any kind of other automation.
[00:08:49.100 --> 00:08:51.820]   Hambly said I used to do consulting,
[00:08:51.820 --> 00:08:52.980]   like that's when I learned to program
[00:08:52.980 --> 00:08:56.940]   'cause I found myself doing a lot of repetitive work
[00:08:56.940 --> 00:09:00.860]   in Excel and thinking, man, I just spent four hours
[00:09:00.860 --> 00:09:03.460]   updating this workbook and it sure would be nice
[00:09:03.460 --> 00:09:05.780]   if there wasn't a manual process I had to do
[00:09:05.780 --> 00:09:08.900]   every time the managing director showed up
[00:09:08.900 --> 00:09:10.260]   with a new set of numbers.
[00:09:10.260 --> 00:09:18.500]   I think programming is really exciting
[00:09:18.500 --> 00:09:20.860]   'cause in a lot of cases, when you're writing code,
[00:09:20.860 --> 00:09:23.260]   you're automating something to some degree
[00:09:23.260 --> 00:09:25.220]   that you used to have to do manually.
[00:09:25.220 --> 00:09:27.500]   So I had a pretty good run
[00:09:27.500 --> 00:09:29.180]   at a couple of different consulting firms
[00:09:29.180 --> 00:09:34.180]   of turning Excel workbooks that took weeks
[00:09:34.180 --> 00:09:37.020]   to update into R scripts
[00:09:37.020 --> 00:09:38.660]   where you could just put in some new inputs
[00:09:38.660 --> 00:09:41.500]   and get some new outputs and you were done.
[00:09:41.500 --> 00:09:44.100]   And so I think that's a lot of how I think about AutoML
[00:09:44.100 --> 00:09:45.620]   is early in my career with machine learning,
[00:09:45.620 --> 00:09:47.100]   there's a lot of really repetitive tasks.
[00:09:47.100 --> 00:09:50.180]   It's like, man, I'm just writing the same code
[00:09:50.180 --> 00:09:54.140]   to take some data and turn it into numbers
[00:09:54.140 --> 00:09:57.180]   that XGBoost understands and then the same tuning
[00:09:57.180 --> 00:09:58.020]   and then the same thing.
[00:09:58.020 --> 00:10:00.260]   And it's like, I keep writing the same block of code
[00:10:00.260 --> 00:10:01.580]   over and over again.
[00:10:01.580 --> 00:10:03.380]   I should package it up in a function somewhere
[00:10:03.380 --> 00:10:06.740]   so I can just import my XGBoost pipeline
[00:10:06.740 --> 00:10:08.980]   and use it wherever I'm writing code.
[00:10:08.980 --> 00:10:10.900]   And so I don't know, that's very much how I think
[00:10:10.900 --> 00:10:13.220]   about AutoML is it's just,
[00:10:13.220 --> 00:10:17.420]   you've been doing something manually and it's repetitive.
[00:10:17.420 --> 00:10:19.740]   And at some point you can teach a computer how to do it
[00:10:19.740 --> 00:10:22.580]   and you can move up a level of distraction
[00:10:22.580 --> 00:10:24.660]   and start thinking about bigger problems.
[00:10:24.660 --> 00:10:27.180]   - That's a good observation.
[00:10:27.180 --> 00:10:32.180]   And like one kind of sort of argument against AutoML
[00:10:32.180 --> 00:10:38.260]   or some kind of objection that a lot of folks have is,
[00:10:38.260 --> 00:10:40.380]   or they get caught up in this debate of,
[00:10:40.380 --> 00:10:43.020]   hey, can AutoML replace a data scientist?
[00:10:43.020 --> 00:10:48.020]   And that's an interesting conversation.
[00:10:48.620 --> 00:10:50.980]   I think, I'm really curious,
[00:10:50.980 --> 00:10:56.300]   if you've used AutoML,
[00:10:56.300 --> 00:10:58.580]   how has it changed the nature of your work?
[00:10:58.580 --> 00:11:00.700]   So I'll share a little bit about that.
[00:11:00.700 --> 00:11:06.340]   AutoML has been really interesting for me to do things
[00:11:06.340 --> 00:11:10.020]   like get a really good baseline before I start working,
[00:11:10.020 --> 00:11:12.700]   whether or not I use any models from AutoML or not,
[00:11:12.700 --> 00:11:14.780]   it's just really good to have a baseline.
[00:11:14.780 --> 00:11:15.660]   Another thing is,
[00:11:16.540 --> 00:11:20.900]   anytime AutoML frameworks can give me any diagnostics
[00:11:20.900 --> 00:11:25.460]   or any kind of information about what's working
[00:11:25.460 --> 00:11:26.460]   and what's not working,
[00:11:26.460 --> 00:11:28.420]   kind of like riffing on Robert's point,
[00:11:28.420 --> 00:11:31.420]   but like some AutoML frameworks are like black boxes.
[00:11:31.420 --> 00:11:33.780]   You just get a model, you don't get anything else.
[00:11:33.780 --> 00:11:34.980]   But it doesn't have to be that way.
[00:11:34.980 --> 00:11:37.620]   Like, there's some AutoML stuff out there
[00:11:37.620 --> 00:11:40.420]   that gives you a lot of information.
[00:11:40.420 --> 00:11:43.060]   So I'm just curious like how everybody in the room
[00:11:43.060 --> 00:11:44.380]   maybe views AutoML.
[00:11:44.380 --> 00:11:47.740]   Like, is it really, can it really,
[00:11:47.740 --> 00:11:49.260]   like, is there any surprising ways
[00:11:49.260 --> 00:11:50.980]   in which it's like changed your work,
[00:11:50.980 --> 00:11:52.860]   even if you are building models?
[00:11:52.860 --> 00:11:55.700]   Even if you do it, like build models the classic way,
[00:11:55.700 --> 00:11:56.980]   do you still use AutoML?
[00:11:56.980 --> 00:11:58.340]   Just out of curiosity.
[00:11:58.340 --> 00:12:04.140]   - For me, I liked Zach's description of like,
[00:12:04.140 --> 00:12:06.420]   five or 10 years ago,
[00:12:06.420 --> 00:12:08.060]   we were operating at one level of abstraction,
[00:12:08.060 --> 00:12:10.140]   doing all these little details.
[00:12:10.140 --> 00:12:13.580]   And we wanted to have something that automates that.
[00:12:13.580 --> 00:12:16.180]   And now we're not so, so zoomed in on things
[00:12:16.180 --> 00:12:18.700]   because like code can do them.
[00:12:18.700 --> 00:12:22.780]   And so for me, I think it has changed my work
[00:12:22.780 --> 00:12:26.740]   from I'm working at like really zoomed in.
[00:12:26.740 --> 00:12:27.660]   And I'm frequently like,
[00:12:27.660 --> 00:12:29.420]   oh, maybe this other hyperparameter
[00:12:29.420 --> 00:12:31.660]   can get my accuracy up by like 0.1,
[00:12:31.660 --> 00:12:34.620]   or it can get my AUC up by like 0.001.
[00:12:34.620 --> 00:12:35.580]   And instead of saying like,
[00:12:35.580 --> 00:12:37.820]   what's the target that I should be predicting?
[00:12:37.820 --> 00:12:39.820]   Which is like actually the thing that we,
[00:12:39.820 --> 00:12:43.420]   has a 10X or 100X bigger impact in reality.
[00:12:43.420 --> 00:12:46.020]   And like, am I producing something useful?
[00:12:46.020 --> 00:12:51.020]   But by automating the pieces that are tempting to look at,
[00:12:51.020 --> 00:12:52.140]   much like we're all tempted,
[00:12:52.140 --> 00:12:54.820]   like we watch our deep learning models,
[00:12:54.820 --> 00:12:56.140]   like train, you watch that little loss curve.
[00:12:56.140 --> 00:12:57.820]   Like that's not a good use of our time,
[00:12:57.820 --> 00:12:59.140]   but it's so tempting.
[00:12:59.140 --> 00:13:00.380]   And in the same way as like,
[00:13:00.380 --> 00:13:03.660]   I used to get pulled into little details.
[00:13:03.660 --> 00:13:07.180]   And by automating those away,
[00:13:07.180 --> 00:13:08.980]   it doesn't replace the need for a data scientist,
[00:13:08.980 --> 00:13:11.260]   but it allows a data scientist to think about,
[00:13:11.260 --> 00:13:12.340]   well, what should I be predicting?
[00:13:12.340 --> 00:13:14.020]   How should I be validating it?
[00:13:14.020 --> 00:13:16.260]   How am I gonna wanna deploy it?
[00:13:16.260 --> 00:13:19.380]   Do I really believe that whatever accuracy I saw in training,
[00:13:19.380 --> 00:13:21.060]   like it's actually gonna have that in the future,
[00:13:21.060 --> 00:13:23.780]   or do I think there's gonna be some sort of drift?
[00:13:23.780 --> 00:13:25.180]   Is there some way I can change the problems
[00:13:25.180 --> 00:13:26.220]   that I think there's gonna be less drift?
[00:13:26.220 --> 00:13:28.500]   Like these are more abstract questions
[00:13:28.500 --> 00:13:29.540]   that we're free to think about
[00:13:29.540 --> 00:13:32.540]   when we don't have to be bogged down by like,
[00:13:32.540 --> 00:13:33.660]   what's a better learning rate?
[00:13:33.660 --> 00:13:35.340]   Or like what's the better learning rate cycle
[00:13:35.340 --> 00:13:40.260]   or some other like complex detail
[00:13:40.260 --> 00:13:41.900]   that doesn't matter that much.
[00:13:41.900 --> 00:13:43.060]   >>I actually agree, Dan.
[00:13:43.060 --> 00:13:46.780]   I think project scoping is super important, right?
[00:13:46.780 --> 00:13:50.060]   And this allows people to spend time on the project scoping
[00:13:50.060 --> 00:13:52.380]   and thinking about these systems.
[00:13:52.380 --> 00:13:54.740]   And really, unless you're at some massive scale
[00:13:54.740 --> 00:13:57.500]   of some large, extremely large enterprise,
[00:13:57.500 --> 00:14:01.060]   or optimizing to the last nth degree
[00:14:01.060 --> 00:14:04.340]   is not necessarily the best use of your time.
[00:14:04.340 --> 00:14:08.540]   So I think that it does free up the capacity
[00:14:08.540 --> 00:14:10.540]   to think holistically about the impacts
[00:14:10.540 --> 00:14:12.500]   that your models are gonna have.
[00:14:12.500 --> 00:14:16.260]   My work, we talk about modeling bias,
[00:14:16.260 --> 00:14:17.460]   so think proxy features,
[00:14:17.460 --> 00:14:19.580]   how do we inadvertently pick up race, gender,
[00:14:19.580 --> 00:14:20.700]   stuff like that.
[00:14:20.700 --> 00:14:23.300]   If you're moving fast and trying to just optimize,
[00:14:23.300 --> 00:14:26.180]   like you couldn't end up bringing in proxy features
[00:14:26.180 --> 00:14:29.780]   because it increases the AUC slightly, right?
[00:14:29.780 --> 00:14:31.340]   But you haven't thought holistically about it.
[00:14:31.340 --> 00:14:33.740]   So I think it does free things up.
[00:14:33.740 --> 00:14:35.340]   And I think for the other thing I would say
[00:14:35.340 --> 00:14:39.260]   is not all data science projects are made the same.
[00:14:39.260 --> 00:14:43.500]   So for instance, like if I actually do think
[00:14:43.500 --> 00:14:47.660]   that there is a group of use cases
[00:14:47.660 --> 00:14:49.740]   that are, let's call them low risk,
[00:14:49.740 --> 00:14:51.060]   that I think an auto ML system
[00:14:51.060 --> 00:14:52.660]   could replace the data scientists,
[00:14:52.660 --> 00:14:56.140]   or you could get a less experienced one,
[00:14:56.140 --> 00:14:58.340]   and that frees up the data scientists
[00:14:58.340 --> 00:15:00.700]   to again, work on the maybe higher impact
[00:15:00.700 --> 00:15:02.740]   or higher risk models,
[00:15:02.740 --> 00:15:05.860]   because I think that's where we wanna put our attention.
[00:15:05.860 --> 00:15:08.180]   I think the pushback to me though,
[00:15:08.180 --> 00:15:12.340]   is that data scientists love the model building, right?
[00:15:12.340 --> 00:15:15.140]   They go to school and they learn KNN with Iris
[00:15:15.140 --> 00:15:19.980]   and they learn regression with Titanic.
[00:15:19.980 --> 00:15:21.100]   And they're like, this is amazing,
[00:15:21.100 --> 00:15:22.020]   I've unlocked a puzzle
[00:15:22.020 --> 00:15:23.940]   and building the model is the exciting thing.
[00:15:23.940 --> 00:15:26.460]   It's like, no, it's not really the exciting thing,
[00:15:26.460 --> 00:15:28.380]   the impact you have, the business,
[00:15:28.380 --> 00:15:29.980]   the organization, and otherwise.
[00:15:29.980 --> 00:15:33.900]   >>But, sorry to jump in again,
[00:15:33.900 --> 00:15:35.580]   but Ted, like you said, it's funny
[00:15:35.580 --> 00:15:37.820]   'cause Hamill brought up the carrot package
[00:15:37.820 --> 00:15:39.860]   and I remember the first time I discovered carrot,
[00:15:39.860 --> 00:15:42.780]   it was, I think I was trying to do a KNN model in R
[00:15:42.780 --> 00:15:44.900]   and I couldn't figure out how to make it work
[00:15:44.900 --> 00:15:46.900]   because the problem with R
[00:15:46.900 --> 00:15:48.740]   is every single package has a different interface.
[00:15:48.740 --> 00:15:51.020]   >>And the syntax, I think, can't help us Tom.
[00:15:51.020 --> 00:15:54.300]   >>And like, yeah, and carrot just showed up,
[00:15:54.300 --> 00:15:59.300]   it's like, hey, Max got annoyed with the same problem,
[00:15:59.300 --> 00:16:00.300]   so he just wrote a package
[00:16:00.300 --> 00:16:03.460]   that unified the interface of all the models.
[00:16:03.460 --> 00:16:05.300]   And so it's funny, that's kind of like,
[00:16:05.300 --> 00:16:07.060]   I look at that as in a lot of ways
[00:16:07.060 --> 00:16:08.260]   the start of my journey to AutoML,
[00:16:08.260 --> 00:16:11.820]   it's just like, I am not good at remembering details
[00:16:11.820 --> 00:16:15.100]   and figuring out how to make a KNN model work
[00:16:15.100 --> 00:16:16.620]   versus a linear regression model work.
[00:16:16.620 --> 00:16:19.060]   Like, that's not something I'm good at
[00:16:19.060 --> 00:16:20.940]   and I don't want to be doing it.
[00:16:20.940 --> 00:16:21.780]   >>Right.
[00:16:21.780 --> 00:16:28.620]   >>Yeah, you know, the black box that,
[00:16:28.620 --> 00:16:30.660]   well, that's right, I described,
[00:16:30.660 --> 00:16:32.460]   being able to really just push a button
[00:16:32.460 --> 00:16:34.380]   and get your model that is,
[00:16:34.380 --> 00:16:36.900]   while that's incredibly useful
[00:16:36.900 --> 00:16:39.540]   and that's a pretty ambitious project,
[00:16:39.540 --> 00:16:41.220]   it's far smaller in scope
[00:16:41.220 --> 00:16:43.180]   than fully replacing data scientists.
[00:16:43.180 --> 00:16:44.820]   I think fully replacing data scientists,
[00:16:44.820 --> 00:16:47.460]   there are many other pieces to that
[00:16:47.460 --> 00:16:49.140]   and things that data scientists do
[00:16:49.140 --> 00:16:52.300]   around scoping the projects, asking the right questions,
[00:16:52.300 --> 00:16:55.900]   and that outer loop around training models.
[00:16:55.900 --> 00:17:01.100]   You know, the way that this has impacted
[00:17:01.100 --> 00:17:02.100]   the tools that I've used,
[00:17:02.100 --> 00:17:05.180]   it's not so much that one day I switched from using,
[00:17:06.460 --> 00:17:09.340]   you know, doing all the low-level hyperparameter search
[00:17:09.340 --> 00:17:11.500]   and everything to just having a button to push.
[00:17:11.500 --> 00:17:13.020]   It didn't really happen that way.
[00:17:13.020 --> 00:17:15.180]   But machine learning has become a lot easier
[00:17:15.180 --> 00:17:16.820]   to apply over time.
[00:17:16.820 --> 00:17:18.980]   They think there are things like,
[00:17:18.980 --> 00:17:19.980]   you know, it used to be very hard
[00:17:19.980 --> 00:17:21.580]   to get neural networks to work.
[00:17:21.580 --> 00:17:25.540]   Everything from, you know, we have better techniques now
[00:17:25.540 --> 00:17:27.700]   for initializing neural networks.
[00:17:27.700 --> 00:17:28.860]   We have better model architectures.
[00:17:28.860 --> 00:17:31.220]   Even just, it's not just searching over defaults
[00:17:31.220 --> 00:17:32.620]   or searching over different options.
[00:17:32.620 --> 00:17:34.900]   It's also just having much better defaults
[00:17:34.900 --> 00:17:36.980]   that to start off with.
[00:17:36.980 --> 00:17:40.100]   And so, you know, even without getting all the way
[00:17:40.100 --> 00:17:42.900]   to the fully to having the black box,
[00:17:42.900 --> 00:17:44.220]   there's been a lot of progress
[00:17:44.220 --> 00:17:47.380]   on making it easier and easier to apply.
[00:17:47.380 --> 00:17:51.460]   - I love making web dev analogies.
[00:17:51.460 --> 00:17:53.780]   So I will make a web dev analogy here.
[00:17:53.780 --> 00:17:57.340]   I think with any like technology implemented correctly,
[00:17:57.340 --> 00:17:59.140]   you will see more data scientists
[00:17:59.140 --> 00:18:02.100]   or more people of that role come in
[00:18:02.100 --> 00:18:04.340]   and I don't know, do what they're supposed to do.
[00:18:04.340 --> 00:18:06.980]   Like, I don't know when React came out,
[00:18:06.980 --> 00:18:08.260]   when even like website builders,
[00:18:08.260 --> 00:18:09.300]   like Squarespace and what,
[00:18:09.300 --> 00:18:12.260]   you don't see like fewer web development engineers
[00:18:12.260 --> 00:18:14.740]   or full stack or sorry, front end engineers.
[00:18:14.740 --> 00:18:17.700]   You see more, you see people of different backgrounds
[00:18:17.700 --> 00:18:19.460]   join the discipline.
[00:18:19.460 --> 00:18:21.340]   The barrier to entry is lowered, right?
[00:18:21.340 --> 00:18:22.620]   If done correctly for AutoML.
[00:18:22.620 --> 00:18:24.380]   I think we're in a unique point with AutoML
[00:18:24.380 --> 00:18:26.620]   where the barrier to entry is not lowered.
[00:18:26.620 --> 00:18:29.380]   It's very odd and weird.
[00:18:29.380 --> 00:18:31.780]   Like we have all these like custom packages
[00:18:31.780 --> 00:18:34.380]   in scikit-learn or in R or whatnot
[00:18:34.380 --> 00:18:37.180]   where you don't need like any extra
[00:18:37.180 --> 00:18:39.020]   or you don't need less, how do I say this?
[00:18:39.020 --> 00:18:41.740]   You still need the same amount of domain expertise
[00:18:41.740 --> 00:18:43.620]   to do it or to leverage it.
[00:18:43.620 --> 00:18:45.500]   So I think it'll be really cool
[00:18:45.500 --> 00:18:48.780]   once we're at that stage where people
[00:18:48.780 --> 00:18:51.940]   who don't really know how to program can,
[00:18:51.940 --> 00:18:55.340]   I don't know, all of a sudden like be building ML models.
[00:18:55.340 --> 00:18:57.500]   That's like a super far off pipe for you.
[00:18:57.500 --> 00:18:58.420]   - That's the goal, right?
[00:18:58.420 --> 00:19:01.900]   To have this in data science in everyone's hands,
[00:19:01.900 --> 00:19:04.300]   diverse set of uses, right?
[00:19:04.300 --> 00:19:08.060]   Diverse set of perspectives on building, sure.
[00:19:08.060 --> 00:19:11.540]   - Yeah, I'm told that historically,
[00:19:11.540 --> 00:19:14.660]   the early versions of what we now call compilers
[00:19:14.660 --> 00:19:16.860]   were called automatic programmers
[00:19:16.860 --> 00:19:18.900]   'cause they basically translated from,
[00:19:18.900 --> 00:19:20.620]   I think the first language where that was called
[00:19:20.620 --> 00:19:21.700]   was like Fortran.
[00:19:21.700 --> 00:19:26.660]   But like they created assembly code
[00:19:26.660 --> 00:19:28.540]   from something that was higher level.
[00:19:28.540 --> 00:19:29.580]   And it's funny to think now,
[00:19:29.580 --> 00:19:31.340]   like if we'd never had compilers,
[00:19:31.340 --> 00:19:32.420]   like if we didn't have Python
[00:19:32.420 --> 00:19:33.620]   or like any high level languages
[00:19:33.620 --> 00:19:35.580]   and we were all using assembly language,
[00:19:35.580 --> 00:19:38.140]   would there be more programmers or fewer?
[00:19:38.140 --> 00:19:40.460]   Like I don't really know.
[00:19:40.460 --> 00:19:43.540]   I just feel like computers would be way less cool
[00:19:43.540 --> 00:19:45.420]   'cause there'd be a lot less software.
[00:19:45.420 --> 00:19:48.820]   And I think that we'll probably get to a point
[00:19:48.820 --> 00:19:53.140]   where the same way we can now use high level languages
[00:19:53.140 --> 00:19:54.580]   and say we're programming,
[00:19:54.580 --> 00:19:56.820]   we'll someday be at that place
[00:19:56.820 --> 00:19:58.660]   with things that we now consider auto-embedded.
[00:19:58.660 --> 00:19:59.500]   We'll be like, oh yeah,
[00:19:59.500 --> 00:20:02.340]   that's just doing data science or machine learning.
[00:20:02.340 --> 00:20:06.620]   - I'm really curious, like Shreya's observation
[00:20:06.620 --> 00:20:11.100]   about people that don't know how to program
[00:20:11.100 --> 00:20:13.020]   but still leveraging data science.
[00:20:13.020 --> 00:20:14.980]   So to ask a kind of pointed question,
[00:20:14.980 --> 00:20:18.500]   is that what data, is like something like DataRobot,
[00:20:18.500 --> 00:20:21.740]   does that supposed to allow that?
[00:20:21.740 --> 00:20:22.700]   Yes or no?
[00:20:22.700 --> 00:20:23.540]   Like what is your--
[00:20:23.540 --> 00:20:24.780]   - It's not the DataRobot.
[00:20:24.780 --> 00:20:27.740]   - Oh no, no, no, I'm asking the room.
[00:20:27.740 --> 00:20:29.420]   I'm asking the whole room.
[00:20:29.420 --> 00:20:31.540]   I'm asking the people here from DataRobot
[00:20:31.540 --> 00:20:33.540]   just because it's like interesting to me.
[00:20:33.540 --> 00:20:36.460]   Like what's been your experience there
[00:20:36.460 --> 00:20:39.300]   like from interacting with people using that,
[00:20:39.300 --> 00:20:40.740]   has it come to that point?
[00:20:40.740 --> 00:20:43.900]   And tell me something about that.
[00:20:43.900 --> 00:20:45.900]   Like you must have experience with that.
[00:20:45.900 --> 00:20:49.380]   - My observation, I mean, Zach can jump in it.
[00:20:49.380 --> 00:20:52.180]   But my observation is it's dependent
[00:20:52.180 --> 00:20:53.580]   on like the organization
[00:20:53.580 --> 00:20:56.100]   and the data fluency of that organization.
[00:20:56.100 --> 00:21:00.020]   So, I know a guy's government relations guy
[00:21:00.020 --> 00:21:02.860]   is not data fluent, two days in a DataRobot class,
[00:21:02.860 --> 00:21:05.020]   he now knows how to build a model, right?
[00:21:05.020 --> 00:21:07.940]   But does he really understand like what the use case could be
[00:21:07.940 --> 00:21:11.220]   or is his mind open up to all these different projects?
[00:21:11.220 --> 00:21:13.500]   So I think some of it is data fluency
[00:21:13.500 --> 00:21:14.940]   and culturally of the organization.
[00:21:14.940 --> 00:21:19.740]   And the other thing is, yes, a system like DataRobot
[00:21:19.740 --> 00:21:23.980]   will lower the barrier for people
[00:21:23.980 --> 00:21:26.340]   who already kind of have that cognitive aptitude
[00:21:26.340 --> 00:21:29.660]   or that are aspiring to be kind of,
[00:21:29.660 --> 00:21:32.220]   you know, like some of the people on this panel.
[00:21:32.220 --> 00:21:34.620]   But I also think that organizations
[00:21:34.620 --> 00:21:36.940]   underestimate the state of their data, right?
[00:21:36.940 --> 00:21:39.900]   And that can be another barrier that's up.
[00:21:39.900 --> 00:21:41.900]   So it's not just the people, it's also,
[00:21:41.900 --> 00:21:43.820]   do you have the data?
[00:21:43.820 --> 00:21:47.060]   Is it organized in the way that you think it is?
[00:21:47.060 --> 00:21:48.780]   Do you have a version of the truth, right?
[00:21:48.780 --> 00:21:49.620]   You might be measuring it
[00:21:49.620 --> 00:21:51.740]   in three different tables differently.
[00:21:51.740 --> 00:21:53.860]   I think that those things are important.
[00:21:53.860 --> 00:21:55.260]   And those have been some of the barriers
[00:21:55.260 --> 00:21:59.660]   to having this kind of utopia of like full data science.
[00:21:59.660 --> 00:22:01.900]   It's a total abstraction, I just click a button, right?
[00:22:01.900 --> 00:22:04.820]   So I do think some scaling on education helps
[00:22:04.820 --> 00:22:05.860]   and systems like DataRobot
[00:22:05.860 --> 00:22:07.700]   are moving in that direction certainly.
[00:22:07.700 --> 00:22:11.460]   But I think also the underlying data is so varied.
[00:22:11.460 --> 00:22:13.700]   And, you know, a lot of these architectures and systems
[00:22:13.700 --> 00:22:16.260]   were built, you know, a decade ago, right?
[00:22:16.260 --> 00:22:17.700]   Or longer.
[00:22:17.700 --> 00:22:21.220]   And so things change in terms of what they're housed in
[00:22:21.220 --> 00:22:22.940]   and how they've been measured and things like that.
[00:22:22.940 --> 00:22:24.420]   - So that makes a lot of sense.
[00:22:24.420 --> 00:22:26.820]   Like, yeah, I would be,
[00:22:26.820 --> 00:22:29.140]   it seems like you still have to bring your data
[00:22:29.140 --> 00:22:30.700]   to the AutoML system.
[00:22:30.700 --> 00:22:32.620]   And it seems difficult to do that
[00:22:32.620 --> 00:22:34.980]   without any programming of some kind.
[00:22:34.980 --> 00:22:38.460]   - Yeah, like when I was at the insurance company,
[00:22:38.460 --> 00:22:42.380]   we would have like, we were doing like auto premium.
[00:22:42.380 --> 00:22:45.460]   And because that's so important to the insurance carrier,
[00:22:45.460 --> 00:22:47.580]   we'd have a team that would say,
[00:22:47.580 --> 00:22:49.940]   this is, we've gone into the data swamp
[00:22:49.940 --> 00:22:50.900]   and we've figured it out.
[00:22:50.900 --> 00:22:53.780]   And this is the data that you should be modeling, right?
[00:22:53.780 --> 00:22:54.820]   So it wasn't even,
[00:22:54.820 --> 00:22:56.460]   'cause they didn't want us spending that time
[00:22:56.460 --> 00:22:57.380]   doing that data munging.
[00:22:57.380 --> 00:22:59.020]   And then we all kind of have a different version
[00:22:59.020 --> 00:22:59.860]   of the truth.
[00:22:59.860 --> 00:23:03.020]   They would help curate that for us.
[00:23:03.020 --> 00:23:06.780]   That was the state of what we had to work through,
[00:23:06.780 --> 00:23:07.620]   I would say.
[00:23:07.620 --> 00:23:09.820]   - Makes sense.
[00:23:09.820 --> 00:23:13.060]   - Yeah, I mean, my sense is, first of all,
[00:23:13.060 --> 00:23:15.340]   like you said, you need to get the data in shape.
[00:23:15.340 --> 00:23:18.460]   That can be done with visual data prep tools,
[00:23:18.460 --> 00:23:20.620]   you know, like Alteryx can be done in SQL,
[00:23:20.620 --> 00:23:23.580]   which I consider coding, but maybe some people don't.
[00:23:23.580 --> 00:23:29.940]   But it does require a lot of effort to get the data,
[00:23:29.940 --> 00:23:31.380]   to get your data,
[00:23:31.380 --> 00:23:36.380]   even at the state where it can enter today's auto ML systems.
[00:23:36.380 --> 00:23:39.260]   Like we're working hard to lower that bar
[00:23:39.260 --> 00:23:43.020]   so that we can take data at an earlier state, a data robot.
[00:23:44.020 --> 00:23:46.980]   But I think like it still typically requires some effort.
[00:23:46.980 --> 00:23:48.500]   And then the other half is like,
[00:23:48.500 --> 00:23:52.660]   okay, so programming is hard, it takes years to learn,
[00:23:52.660 --> 00:23:54.060]   but also figuring out
[00:23:54.060 --> 00:23:55.860]   what is an important machine learning problem
[00:23:55.860 --> 00:23:58.100]   and figuring out like where are,
[00:23:58.100 --> 00:24:00.380]   the API for a machine learning model
[00:24:00.380 --> 00:24:03.980]   is not dot tell me what to do, it is dot predict.
[00:24:03.980 --> 00:24:06.220]   And so to figure like where are predictions useful
[00:24:06.220 --> 00:24:09.220]   and like what would I do with them if I had them
[00:24:09.220 --> 00:24:14.220]   is a problem which I think even fewer people
[00:24:14.220 --> 00:24:17.340]   know how to solve than know how to do programming.
[00:24:17.340 --> 00:24:22.340]   And so I'm actually a little less optimistic
[00:24:22.340 --> 00:24:24.820]   about the future where there are like a million people
[00:24:24.820 --> 00:24:28.580]   using machine learning for practical purposes.
[00:24:28.580 --> 00:24:31.460]   And I think like auto ML tools,
[00:24:31.460 --> 00:24:34.460]   data robot included are useful in that direction,
[00:24:34.460 --> 00:24:36.300]   but I don't think that we're getting close to the world
[00:24:36.300 --> 00:24:38.100]   that Shreya has described up front,
[00:24:38.100 --> 00:24:40.340]   like this world where a lot of people
[00:24:40.340 --> 00:24:42.580]   are ready to use machine learning,
[00:24:42.580 --> 00:24:43.940]   because even if they could build the models,
[00:24:43.940 --> 00:24:46.180]   like for those models,
[00:24:46.180 --> 00:24:47.340]   after you've built the model,
[00:24:47.340 --> 00:24:48.500]   if you're not in a Kaggle competition,
[00:24:48.500 --> 00:24:49.900]   you're sort of like, oh, wait,
[00:24:49.900 --> 00:24:52.660]   am I supposed to use this for something?
[00:24:52.660 --> 00:24:55.220]   And I think that that part is still pretty hard
[00:24:55.220 --> 00:24:57.300]   and we haven't figured out how to automate that.
[00:24:57.300 --> 00:24:59.140]   - Yeah, I call that last mile delivery.
[00:24:59.140 --> 00:25:02.860]   What's that last mile to get to just even a simple web form
[00:25:02.860 --> 00:25:05.020]   where I can click submit, right?
[00:25:06.020 --> 00:25:08.260]   Because you could be a data scientist
[00:25:08.260 --> 00:25:09.820]   and it's in a Jupyter notebook,
[00:25:09.820 --> 00:25:11.700]   but that only gets you so far, right?
[00:25:11.700 --> 00:25:15.500]   - But yeah, I think there are two parts to that.
[00:25:15.500 --> 00:25:17.580]   When you say it's in a Jupyter notebook,
[00:25:17.580 --> 00:25:20.140]   now that's the upside.
[00:25:20.140 --> 00:25:21.340]   If we had a, if you could,
[00:25:21.340 --> 00:25:23.140]   all right, you say maybe it's a model
[00:25:23.140 --> 00:25:24.900]   that can be pickled or exported to Onyx.
[00:25:24.900 --> 00:25:26.660]   Now I bring it somewhere.
[00:25:26.660 --> 00:25:28.820]   Now I've got a RESTful endpoint.
[00:25:28.820 --> 00:25:30.180]   - Yeah, there's still something else.
[00:25:30.180 --> 00:25:33.300]   There's still a mobile app.
[00:25:33.300 --> 00:25:36.700]   There's still a, deliver some value to the organization
[00:25:36.700 --> 00:25:39.500]   or to your community, whatever you're building for, right?
[00:25:39.500 --> 00:25:41.980]   - Yeah, and part of that is technological,
[00:25:41.980 --> 00:25:43.420]   but part of that is like,
[00:25:43.420 --> 00:25:48.340]   why do, what even conceptually could these predictions,
[00:25:48.340 --> 00:25:50.460]   what could they be useful for?
[00:25:50.460 --> 00:25:51.700]   And I think there are many cases
[00:25:51.700 --> 00:25:54.020]   where we have had so much excitement
[00:25:54.020 --> 00:25:55.820]   around machine learning,
[00:25:55.820 --> 00:25:57.060]   the people build machine learning models
[00:25:57.060 --> 00:25:57.900]   and they're like, actually,
[00:25:57.900 --> 00:26:00.180]   I have no idea what I would do with that model,
[00:26:00.180 --> 00:26:02.380]   even if I could get it like operationalized.
[00:26:02.380 --> 00:26:05.020]   I just built it 'cause like machine learning is cool.
[00:26:05.020 --> 00:26:11.580]   - There's definitely a lot of that going on for sure.
[00:26:11.580 --> 00:26:14.860]   Okay, another question,
[00:26:14.860 --> 00:26:17.740]   another kind of thing I wanna pose to the panel is,
[00:26:17.740 --> 00:26:21.540]   so there's some like criticisms of AutoML
[00:26:21.540 --> 00:26:23.060]   that are commonly put forth.
[00:26:23.060 --> 00:26:24.940]   So like, I'll just list some of them.
[00:26:24.940 --> 00:26:29.180]   One is it's computationally prohibitive is one view.
[00:26:31.180 --> 00:26:35.260]   And another one is it lets people do ML
[00:26:35.260 --> 00:26:36.580]   that don't know what they're doing,
[00:26:36.580 --> 00:26:40.500]   thus encourages like bad practices or sloppiness.
[00:26:40.500 --> 00:26:44.620]   Another kind of criticism is it takes away
[00:26:44.620 --> 00:26:47.220]   the most fun aspect of ML, which is building the model,
[00:26:47.220 --> 00:26:48.980]   which is not the most fun aspect for me,
[00:26:48.980 --> 00:26:50.140]   but people do say that.
[00:26:50.140 --> 00:26:53.580]   Another criticism is like,
[00:26:53.580 --> 00:26:56.500]   there's no way it can beat a data, a human being.
[00:26:56.500 --> 00:26:57.500]   There's no way,
[00:26:57.500 --> 00:27:00.780]   that people just have the baseline skepticism.
[00:27:00.780 --> 00:27:04.260]   And then the last one that kind of comes up is,
[00:27:04.260 --> 00:27:07.940]   AutoML just overfits your data by mining it aggressively.
[00:27:07.940 --> 00:27:09.980]   So I mean, it would be interesting
[00:27:09.980 --> 00:27:12.260]   to get some thoughts on that.
[00:27:12.260 --> 00:27:13.980]   - I mean, I've got a really strong reaction
[00:27:13.980 --> 00:27:14.820]   to that last one,
[00:27:14.820 --> 00:27:16.260]   which is I worked in consulting for years
[00:27:16.260 --> 00:27:17.700]   where we overfit the data
[00:27:17.700 --> 00:27:21.100]   by mining it aggressively manually.
[00:27:21.100 --> 00:27:22.980]   And those consultants all made fun of me
[00:27:22.980 --> 00:27:25.260]   for going to an AutoML startup,
[00:27:25.260 --> 00:27:26.100]   'cause they're like,
[00:27:26.100 --> 00:27:28.220]   "Oh, you're gonna go work in an overfitting company."
[00:27:28.220 --> 00:27:31.460]   And I was like, you know, like you can,
[00:27:31.460 --> 00:27:32.900]   like, for example,
[00:27:32.900 --> 00:27:34.900]   cross validation is something you can program
[00:27:34.900 --> 00:27:35.740]   into a computer,
[00:27:35.740 --> 00:27:38.020]   but it's hard to program into a human.
[00:27:38.020 --> 00:27:39.580]   And so like, I just,
[00:27:39.580 --> 00:27:41.140]   that's the one that I really pushed back on
[00:27:41.140 --> 00:27:44.020]   because like humans are very good at overfitting
[00:27:44.020 --> 00:27:45.580]   and fooling themselves too.
[00:27:45.580 --> 00:27:48.140]   And I just, yes,
[00:27:48.140 --> 00:27:51.780]   you can program an AutoML system to overfit a dataset
[00:27:51.780 --> 00:27:53.900]   and you know, that does happen,
[00:27:53.900 --> 00:27:55.340]   but you can also,
[00:27:55.340 --> 00:27:57.620]   if you're thinking about your problem framing correctly,
[00:27:57.620 --> 00:27:59.060]   you can program an AutoML system
[00:27:59.060 --> 00:28:03.020]   to be honest about how well it performs on new data.
[00:28:03.020 --> 00:28:05.940]   - One kind of like leveraging that question,
[00:28:05.940 --> 00:28:07.820]   I'm just, this is kind of funny,
[00:28:07.820 --> 00:28:10.540]   but like in the,
[00:28:10.540 --> 00:28:12.140]   when we talk about human biases,
[00:28:12.140 --> 00:28:14.220]   when it comes to ML,
[00:28:14.220 --> 00:28:15.260]   one thing Zach,
[00:28:15.260 --> 00:28:17.660]   you and I talk like a joke about all the time
[00:28:17.660 --> 00:28:19.540]   is like bag of words,
[00:28:19.540 --> 00:28:23.060]   like how good bag of words performs.
[00:28:23.060 --> 00:28:24.740]   - Right, I literally changed my Twitter bio
[00:28:24.740 --> 00:28:26.340]   to be like, have you tried bag of words?
[00:28:26.340 --> 00:28:27.180]   Yeah.
[00:28:27.180 --> 00:28:28.500]   - Can you expand on that a little bit?
[00:28:28.500 --> 00:28:29.340]   Like what is behind,
[00:28:29.340 --> 00:28:31.060]   like just like explain that a little.
[00:28:31.060 --> 00:28:33.020]   - So, you know, riffing a little on what you said,
[00:28:33.020 --> 00:28:36.060]   like, you know,
[00:28:36.060 --> 00:28:37.140]   modeling can be fun
[00:28:37.140 --> 00:28:39.460]   and it's not always the part that I find fun,
[00:28:39.460 --> 00:28:41.540]   but like fiddling with the numbers
[00:28:41.540 --> 00:28:43.100]   and watching your loss get lower
[00:28:43.100 --> 00:28:44.180]   and watching your learning curve,
[00:28:44.180 --> 00:28:45.220]   like, I don't know,
[00:28:45.220 --> 00:28:48.220]   like there's a whole genre of video games
[00:28:48.220 --> 00:28:49.380]   called like, you know,
[00:28:49.380 --> 00:28:50.860]   idle minors that I just love,
[00:28:50.860 --> 00:28:51.700]   where it's just,
[00:28:51.700 --> 00:28:53.180]   you're watching a number go up basically.
[00:28:53.180 --> 00:28:54.420]   And there's something that's kind of fun to be like,
[00:28:54.420 --> 00:28:55.380]   oh, the number's bigger today.
[00:28:55.380 --> 00:28:56.340]   Like, that's great.
[00:28:57.180 --> 00:29:00.500]   And so, you know,
[00:29:00.500 --> 00:29:03.340]   I don't know, I've lost my train of thought
[00:29:03.340 --> 00:29:05.060]   'cause I got so excited thinking about video games.
[00:29:05.060 --> 00:29:05.900]   Can you repeat the question?
[00:29:05.900 --> 00:29:08.220]   - Oh no, so I was asking to explain the humor
[00:29:08.220 --> 00:29:09.620]   behind the bag of words thing.
[00:29:09.620 --> 00:29:10.460]   - Yeah, right, right.
[00:29:10.460 --> 00:29:13.300]   - And so, so text is an area
[00:29:13.300 --> 00:29:15.100]   where there's just an explosion of research right now.
[00:29:15.100 --> 00:29:18.260]   There's all kinds of really, really exciting new stuff
[00:29:18.260 --> 00:29:20.220]   in text right now.
[00:29:20.220 --> 00:29:22.700]   And, you know, if you're starting a new problem,
[00:29:22.700 --> 00:29:24.940]   it's just way more fun to go straight
[00:29:24.940 --> 00:29:26.180]   to the state of the art.
[00:29:26.180 --> 00:29:28.820]   And say, you know, I'm gonna do, you know,
[00:29:28.820 --> 00:29:31.460]   a deep learning attention-based model
[00:29:31.460 --> 00:29:32.980]   that's gonna do all this crazy stuff.
[00:29:32.980 --> 00:29:35.380]   And it's gonna understand every language that's spoken.
[00:29:35.380 --> 00:29:38.220]   And it's just gonna, you know, be awesome
[00:29:38.220 --> 00:29:40.420]   and spend a whole bunch of time doing that.
[00:29:40.420 --> 00:29:41.860]   'Cause that's really fun if you're focused
[00:29:41.860 --> 00:29:44.220]   on the fun of building models.
[00:29:44.220 --> 00:29:45.420]   And the flip side of that is that like,
[00:29:45.420 --> 00:29:47.900]   there's a lot of text-based problems
[00:29:47.900 --> 00:29:52.460]   where just looking at the words is probably good enough.
[00:29:52.460 --> 00:29:53.940]   And like one that comes up all the time
[00:29:53.940 --> 00:29:55.580]   that, you know, I really like automating.
[00:29:55.580 --> 00:29:57.380]   It's like classifying tickets
[00:29:57.380 --> 00:29:58.780]   in some kind of ticketing system.
[00:29:58.780 --> 00:30:02.420]   Like, you know, a customer comes in and says something
[00:30:02.420 --> 00:30:04.220]   and you're trying to say, is this a feature request?
[00:30:04.220 --> 00:30:06.100]   Is this a bug report?
[00:30:06.100 --> 00:30:07.780]   You know, is this asking, someone asking
[00:30:07.780 --> 00:30:09.660]   for technical support?
[00:30:09.660 --> 00:30:11.580]   And like very frequently, you don't need
[00:30:11.580 --> 00:30:14.620]   a deep learning model or anything crazy and sophisticated.
[00:30:14.620 --> 00:30:16.060]   You can just build a very, very simple system
[00:30:16.060 --> 00:30:18.660]   that like looks at the words in the text and says,
[00:30:18.660 --> 00:30:21.700]   geez, when people say the word error,
[00:30:21.700 --> 00:30:23.700]   it usually means they have a bug report.
[00:30:23.700 --> 00:30:25.700]   And when they say the words, it would be nice,
[00:30:25.700 --> 00:30:27.460]   that means they've got a feature request.
[00:30:27.460 --> 00:30:30.060]   And when they say the word help, they need help.
[00:30:30.060 --> 00:30:31.900]   You know, and that's level of automation.
[00:30:31.900 --> 00:30:34.300]   You use just a computer that just says every ticket
[00:30:34.300 --> 00:30:36.660]   with the word help in it, send it to the, you know,
[00:30:36.660 --> 00:30:39.100]   send it to the tech support people who can call them up
[00:30:39.100 --> 00:30:42.380]   and give them help and like help them write better code
[00:30:42.380 --> 00:30:43.220]   or whatever.
[00:30:43.220 --> 00:30:45.020]   And, you know, every ticket that's a product request,
[00:30:45.020 --> 00:30:45.940]   send it to a product manager.
[00:30:45.940 --> 00:30:47.580]   Every ticket is bug report sent to engineering
[00:30:47.580 --> 00:30:49.100]   and you know, you're done.
[00:30:49.100 --> 00:30:50.500]   And, you know, importantly,
[00:30:50.500 --> 00:30:52.420]   the cost of misclassification there is low.
[00:30:52.420 --> 00:30:57.140]   Like if 10% of the time you're misclassifying, you know,
[00:30:57.140 --> 00:30:58.460]   a bug is a feature request,
[00:30:58.460 --> 00:31:00.100]   it's not hard for the engineering team to just say,
[00:31:00.100 --> 00:31:01.300]   ah, this is actually a feature request,
[00:31:01.300 --> 00:31:03.700]   send it back over to PM, you know.
[00:31:03.700 --> 00:31:06.260]   So I just, I think, I don't know,
[00:31:06.260 --> 00:31:07.820]   text is especially one where it's like,
[00:31:07.820 --> 00:31:08.980]   and I believe this anywhere,
[00:31:08.980 --> 00:31:12.340]   like in any job I've ever had and maybe working consultant
[00:31:12.340 --> 00:31:13.460]   kind of put this mindset on me,
[00:31:13.460 --> 00:31:18.460]   but like always try the simple, easy thing first.
[00:31:18.460 --> 00:31:21.180]   And if that solves your problem, don't worry about it
[00:31:21.180 --> 00:31:22.460]   and move on to the next problem.
[00:31:22.460 --> 00:31:23.980]   'Cause there's always more problems to solve.
[00:31:23.980 --> 00:31:26.700]   Like getting bogged down in the perfect
[00:31:26.700 --> 00:31:28.780]   or most exciting solution to something like,
[00:31:28.780 --> 00:31:31.780]   like start simple and if that doesn't work,
[00:31:31.780 --> 00:31:33.620]   you know, go more complicated.
[00:31:33.620 --> 00:31:35.220]   And AutoML systems are really good at that.
[00:31:35.220 --> 00:31:37.300]   Like, especially with text,
[00:31:37.300 --> 00:31:38.380]   an AutoML system will tell you,
[00:31:38.380 --> 00:31:40.460]   hey, bag of words works great on this problem.
[00:31:40.460 --> 00:31:42.980]   It's simple, it's explainable, it's fast, you're done,
[00:31:42.980 --> 00:31:45.220]   move on to the next problem.
[00:31:45.220 --> 00:31:46.860]   >>Yeah, I can't tell you how many times
[00:31:46.860 --> 00:31:49.220]   I've reviewed a problem.
[00:31:49.220 --> 00:31:50.620]   'Cause like, you know, as data scientists,
[00:31:50.620 --> 00:31:53.300]   we want to get excited about the state of the art thing.
[00:31:53.300 --> 00:31:56.700]   It like makes, you can sell your work a little bit better.
[00:31:56.700 --> 00:31:58.620]   It's exciting, more exciting to talk about,
[00:31:58.620 --> 00:31:59.820]   so on and so forth.
[00:31:59.820 --> 00:32:02.980]   But I can't tell you how many times I've been surprised
[00:32:02.980 --> 00:32:06.260]   by throwing an AutoML system at a thing
[00:32:06.260 --> 00:32:08.660]   that's using a transformer and saying, oh wow,
[00:32:08.660 --> 00:32:11.540]   like I have, is like this bag of words model
[00:32:11.540 --> 00:32:14.140]   is giving me really good results.
[00:32:14.140 --> 00:32:16.980]   So it's like a, it's interesting way of keeping
[00:32:16.980 --> 00:32:18.940]   things honest in a sense.
[00:32:18.940 --> 00:32:21.580]   >>Right, right, I mean, this happens to us at DataRobot.
[00:32:21.580 --> 00:32:23.380]   We get customers who show up and say, you know,
[00:32:23.380 --> 00:32:26.260]   we need, you know, named entity extraction
[00:32:26.260 --> 00:32:29.180]   and sentiment analysis and like this long list
[00:32:29.180 --> 00:32:32.020]   of like specific technologies they've got in text.
[00:32:32.020 --> 00:32:33.940]   And you'll ask them, well, so what's the problem
[00:32:33.940 --> 00:32:34.780]   you're trying to solve?
[00:32:34.780 --> 00:32:36.780]   Like, well, we're trying to classify documents.
[00:32:36.780 --> 00:32:38.740]   And it can be sometimes like, well,
[00:32:38.740 --> 00:32:42.500]   have you just tried bag of words, you know,
[00:32:42.500 --> 00:32:44.940]   which doesn't have all those technologies in it
[00:32:44.940 --> 00:32:48.460]   and is way less effort to implement in, you know,
[00:32:48.460 --> 00:32:50.900]   DataRobot and a million other systems do it well,
[00:32:50.900 --> 00:32:52.580]   but you know, anyway.
[00:32:52.580 --> 00:32:54.380]   >>Makes sense.
[00:32:54.380 --> 00:32:56.660]   And like, so about these other criticism of AutoML,
[00:32:56.660 --> 00:32:58.860]   just throwing it to the panel,
[00:32:58.860 --> 00:33:02.540]   what is any of those criticism resonate with you
[00:33:02.540 --> 00:33:04.460]   or what do you think about that or what's like?
[00:33:04.460 --> 00:33:07.180]   >>I mean, I think, you know, a lot of the criticisms
[00:33:07.180 --> 00:33:09.880]   are criticisms, not of the goal,
[00:33:09.880 --> 00:33:12.260]   but of the state of AutoML today, right?
[00:33:12.260 --> 00:33:13.940]   They're saying it's hard to do well,
[00:33:13.940 --> 00:33:16.780]   and it is hard to do well, but of course,
[00:33:16.780 --> 00:33:18.300]   we'll make it better and better over time.
[00:33:18.300 --> 00:33:19.140]   We'll get there.
[00:33:19.140 --> 00:33:22.100]   So I don't think there's, that's sort of my reaction
[00:33:22.100 --> 00:33:24.120]   to a lot of the criticisms there.
[00:33:24.120 --> 00:33:30.100]   >>I would, I take issue with that a human
[00:33:30.100 --> 00:33:34.580]   can always beat AutoML because my experience,
[00:33:34.580 --> 00:33:36.500]   like the first time that I saw DataRobot,
[00:33:36.500 --> 00:33:38.180]   I was gambling on basketball.
[00:33:38.180 --> 00:33:40.420]   I spent a lot of time on my model,
[00:33:40.420 --> 00:33:43.340]   a lot of time on my model.
[00:33:43.340 --> 00:33:46.300]   30 minutes into DataRobot, a nice June kernel SVM,
[00:33:46.300 --> 00:33:49.340]   and I said, "I don't even know what that kernel is."
[00:33:49.340 --> 00:33:53.180]   So cool, I guess I spent six months building a model
[00:33:53.180 --> 00:33:55.140]   that is now worthless.
[00:33:55.140 --> 00:33:57.180]   Now it's not to say that like, obviously,
[00:33:57.180 --> 00:33:59.460]   you guys that are better data scientists couldn't beat it,
[00:33:59.460 --> 00:34:01.740]   but I think there are times it doesn't matter
[00:34:01.740 --> 00:34:04.220]   about beating it because what is it to beat it?
[00:34:04.220 --> 00:34:07.120]   To Zach's point of like, solve the problem
[00:34:07.120 --> 00:34:11.700]   as well as you can and move on to the next problem, right?
[00:34:11.700 --> 00:34:14.340]   So I think that there's a value in that as well.
[00:34:14.340 --> 00:34:18.340]   And to beat a model just by saying six decimal places out,
[00:34:18.340 --> 00:34:21.940]   my log loss is improved, I don't think is really
[00:34:21.940 --> 00:34:26.940]   the right framing of how do you contribute to the field.
[00:34:26.940 --> 00:34:29.620]   >>Shreya, I think you have.
[00:34:29.620 --> 00:34:32.220]   >>Yeah, I really want to echo Robert's point
[00:34:32.220 --> 00:34:33.980]   on the state of the AutoML.
[00:34:33.980 --> 00:34:35.140]   And then also Ted's point.
[00:34:35.140 --> 00:34:37.740]   I think there's this interesting culture
[00:34:37.740 --> 00:34:40.020]   that maybe is taught in academia or in classes
[00:34:40.020 --> 00:34:41.580]   that you should try to eke out
[00:34:41.580 --> 00:34:43.140]   as much performance as possible.
[00:34:43.140 --> 00:34:45.580]   You should try all the architectures,
[00:34:45.580 --> 00:34:48.780]   feel bad if you haven't exhausted every architecture
[00:34:48.780 --> 00:34:51.220]   so you know that you're getting the best possible
[00:34:51.220 --> 00:34:53.860]   performance on some, I don't know, holdout set
[00:34:53.860 --> 00:34:56.020]   or whatever cross validation you want to do.
[00:34:56.020 --> 00:34:58.700]   One of the projects, but this is weird
[00:34:58.700 --> 00:34:59.880]   and I'll explain why.
[00:34:59.880 --> 00:35:02.780]   So one of the projects I'm working on right now
[00:35:02.780 --> 00:35:04.900]   is an interview study of ML practitioners.
[00:35:04.900 --> 00:35:07.020]   And specifically, the big question is,
[00:35:07.020 --> 00:35:09.020]   how do you feel confident in your models
[00:35:09.020 --> 00:35:10.580]   when you're deploying them?
[00:35:10.580 --> 00:35:13.980]   And most of the more mature organizations
[00:35:13.980 --> 00:35:17.340]   are saying we have this dynamic evaluation set,
[00:35:17.340 --> 00:35:20.180]   like we have this eval notebook or something
[00:35:20.180 --> 00:35:22.300]   and we just keep adding examples to it
[00:35:22.300 --> 00:35:24.540]   as we see examples fail in the wild.
[00:35:24.540 --> 00:35:26.020]   Something that's always changing.
[00:35:26.020 --> 00:35:28.020]   So it doesn't make any sense whatsoever
[00:35:28.020 --> 00:35:30.100]   to try out all the hyper parameter combos
[00:35:30.100 --> 00:35:31.900]   because tomorrow is going to be different
[00:35:31.900 --> 00:35:34.140]   when this evaluation set changes.
[00:35:34.140 --> 00:35:36.860]   Do you use AutoML is a question that we ask these people
[00:35:36.860 --> 00:35:38.860]   and they're like, "Eh."
[00:35:38.860 --> 00:35:41.900]   Yeah, to inform good model choices
[00:35:41.900 --> 00:35:43.060]   or things about our data,
[00:35:43.060 --> 00:35:46.020]   but not to get the best model selection
[00:35:46.020 --> 00:35:47.900]   or hyper parameter selection or something like that.
[00:35:47.900 --> 00:35:49.460]   I think that's super interesting, right?
[00:35:49.460 --> 00:35:52.860]   Like this, how do we move away from the culture
[00:35:52.860 --> 00:35:55.180]   of just like trying to beat performance
[00:35:55.180 --> 00:35:58.140]   for performance sake and like create something sustainable?
[00:35:58.140 --> 00:36:02.980]   - I, yeah, I also, Robert's description
[00:36:02.980 --> 00:36:03.820]   I thought was really good.
[00:36:03.820 --> 00:36:07.660]   And it reminded me like when Google Cloud
[00:36:07.660 --> 00:36:11.820]   first came out with AutoML, to run AutoML
[00:36:11.820 --> 00:36:14.340]   they did everything with neural architecture search.
[00:36:14.340 --> 00:36:18.020]   And like you could be uploaded a Titanic dataset.
[00:36:18.020 --> 00:36:19.940]   It would cost more than $50 to run it.
[00:36:19.940 --> 00:36:21.620]   Like just to run a training job
[00:36:21.620 --> 00:36:23.740]   and it would take several hours.
[00:36:23.740 --> 00:36:28.540]   And that was like the case for, I think months.
[00:36:28.540 --> 00:36:31.300]   And then they said like, this is not good.
[00:36:31.300 --> 00:36:35.100]   And they replaced how they did a model selection.
[00:36:35.100 --> 00:36:37.420]   And now it probably costs, I don't know, $2
[00:36:37.420 --> 00:36:39.260]   and runs in minutes.
[00:36:39.260 --> 00:36:42.380]   And that's just the nature of progress is like
[00:36:42.380 --> 00:36:44.500]   AutoML now is not where it's gonna end up.
[00:36:44.500 --> 00:36:46.540]   And I think that's exactly what Robert said.
[00:36:46.540 --> 00:36:48.700]   I think there are plenty of examples of it.
[00:36:48.700 --> 00:36:53.300]   - How important is in AutoML?
[00:36:53.300 --> 00:36:55.780]   So like, yeah, there's definitely this genre of AutoML
[00:36:55.780 --> 00:36:59.300]   that is a black box, like Google Cloud, you know, product
[00:36:59.300 --> 00:37:00.940]   you kind of upload your data, you get a model.
[00:37:00.940 --> 00:37:02.540]   You have no idea what is going on.
[00:37:02.540 --> 00:37:04.100]   Maybe you don't care.
[00:37:04.100 --> 00:37:09.100]   What, like in your experiences, you know,
[00:37:09.100 --> 00:37:13.420]   how important are like the insights in AutoML?
[00:37:13.420 --> 00:37:15.620]   Like, you know, like some AutoML systems
[00:37:15.620 --> 00:37:19.980]   provide a lot of diagnostics, a lot of, you know, things
[00:37:19.980 --> 00:37:23.660]   that maybe, you know, some people use
[00:37:23.660 --> 00:37:25.460]   even more than any models.
[00:37:25.460 --> 00:37:27.060]   Like just to hear, have you seen that?
[00:37:27.060 --> 00:37:29.100]   Is that, you know, what's your thoughts?
[00:37:29.100 --> 00:37:32.540]   - Yeah, we, I think that's super important.
[00:37:32.540 --> 00:37:35.540]   I like the data quality checks that happen,
[00:37:35.540 --> 00:37:37.020]   things like target leakage.
[00:37:37.020 --> 00:37:41.740]   I think a lot of newer data scientists may not know
[00:37:41.740 --> 00:37:44.060]   how to check for that or what that looks like.
[00:37:44.060 --> 00:37:47.700]   Obviously like my team, I know I'm partial,
[00:37:47.700 --> 00:37:49.500]   but we do a whole thing on bias insights.
[00:37:49.500 --> 00:37:52.380]   If you declare a protected feature
[00:37:52.380 --> 00:37:55.540]   and we'll help you understand maybe cross-class disparity
[00:37:55.540 --> 00:37:59.820]   or what is the proportional parity, things like that.
[00:37:59.820 --> 00:38:03.340]   I think there's real value in understanding your data set
[00:38:03.340 --> 00:38:05.380]   as quickly as possible and the flaws therein
[00:38:05.380 --> 00:38:09.220]   or the things that might be super valuable in the modeling,
[00:38:09.220 --> 00:38:10.820]   even if they end up modeling elsewhere.
[00:38:10.820 --> 00:38:14.620]   So I do think there is value kind of in that auto EDA
[00:38:14.620 --> 00:38:16.900]   or more even using models and getting insights
[00:38:16.900 --> 00:38:19.540]   for explanatory purposes, I think is important.
[00:38:19.540 --> 00:38:23.660]   - Yeah, I think it varies quite a bit
[00:38:23.660 --> 00:38:28.100]   from one project to the next.
[00:38:28.100 --> 00:38:32.980]   But one of the concerns with AutoML that I said earlier
[00:38:32.980 --> 00:38:34.460]   is like, even if you had the ability
[00:38:34.460 --> 00:38:36.980]   to build machine learning models very easily,
[00:38:36.980 --> 00:38:38.660]   frequently you're like, okay,
[00:38:38.660 --> 00:38:41.060]   but now I have a way to make predictions.
[00:38:41.060 --> 00:38:41.900]   Why do I care?
[00:38:41.900 --> 00:38:43.100]   What do I do with them?
[00:38:43.100 --> 00:38:45.740]   And I see it, I mean, this isn't unique to AutoML.
[00:38:45.740 --> 00:38:48.860]   This can be done with regular Python libraries
[00:38:48.860 --> 00:38:50.660]   that are not, don't call themselves AutoML libraries,
[00:38:50.660 --> 00:38:53.700]   but to get permutation importance.
[00:38:53.700 --> 00:38:56.180]   There are a lot of problems where just understanding
[00:38:56.180 --> 00:39:00.580]   like what are the factors that most impact churn
[00:39:00.580 --> 00:39:03.500]   is actually more like, if I said,
[00:39:03.500 --> 00:39:05.060]   this person is 4% likely to churn,
[00:39:05.060 --> 00:39:06.460]   this person is 7% likely to churn,
[00:39:06.460 --> 00:39:08.300]   this person is 6% likely to churn,
[00:39:08.300 --> 00:39:09.820]   I think you could very easily say like,
[00:39:09.820 --> 00:39:12.180]   okay, but what are we gonna do with that?
[00:39:12.180 --> 00:39:17.180]   Whereas instead I said, people who bought something online
[00:39:17.180 --> 00:39:21.420]   instead of in store are less likely to churn.
[00:39:21.420 --> 00:39:23.220]   Like now you're starting to get to something
[00:39:23.220 --> 00:39:25.420]   that's actually actionable and someone can make,
[00:39:25.420 --> 00:39:28.340]   a person can make better decisions with.
[00:39:28.340 --> 00:39:30.140]   And so I think there are a lot of projects
[00:39:30.140 --> 00:39:34.340]   where the insights are actually the deliverable
[00:39:34.340 --> 00:39:36.220]   or like the thing that matters.
[00:39:36.220 --> 00:39:38.780]   And the way that we typically think about operationalizing
[00:39:38.780 --> 00:39:41.660]   ML and deploying it and getting predictions
[00:39:41.660 --> 00:39:43.460]   isn't so critical.
[00:39:43.460 --> 00:39:45.700]   - Yeah, I mean, I think permutation importance
[00:39:45.700 --> 00:39:46.900]   is such a great example, Dan,
[00:39:46.900 --> 00:39:49.460]   because like in theory, I know how to program that,
[00:39:49.460 --> 00:39:52.260]   in practice, I never have,
[00:39:52.260 --> 00:39:54.420]   because it's just like, it's kind of annoying
[00:39:54.420 --> 00:39:55.820]   to get it right for every different model
[00:39:55.820 --> 00:39:56.660]   you're working with.
[00:39:56.660 --> 00:39:59.180]   And like, I basically just use auto ML systems,
[00:39:59.180 --> 00:40:01.540]   even if I written my own code, just say, okay,
[00:40:01.540 --> 00:40:04.060]   like here's a dataset, here's a function to fit a model,
[00:40:04.060 --> 00:40:06.860]   tell me what variables this model thinks are important
[00:40:06.860 --> 00:40:08.980]   in like a model agnostic way that I can compare
[00:40:08.980 --> 00:40:11.620]   across models is really, really useful.
[00:40:11.620 --> 00:40:13.140]   And that's a nice thing to have automated
[00:40:13.140 --> 00:40:15.060]   'cause it's annoying to do it by hand.
[00:40:15.060 --> 00:40:19.380]   - I really like the point, or go ahead.
[00:40:19.380 --> 00:40:21.740]   I really like the point that sometimes the insights
[00:40:21.740 --> 00:40:23.460]   are actually the deliverable.
[00:40:23.460 --> 00:40:25.100]   I think that's really good.
[00:40:25.100 --> 00:40:27.940]   One thing I'll add is that when the model,
[00:40:27.940 --> 00:40:31.660]   in the cases where the model is actually the deliverable,
[00:40:31.660 --> 00:40:34.460]   the importance of insights sometimes comes down
[00:40:34.460 --> 00:40:36.660]   to just how well it works,
[00:40:36.660 --> 00:40:39.340]   like how well the auto ML works, right?
[00:40:39.340 --> 00:40:42.980]   If it really, really, really just nails it,
[00:40:42.980 --> 00:40:44.500]   you may need less insights.
[00:40:44.500 --> 00:40:46.300]   And if it's, but of course,
[00:40:46.300 --> 00:40:48.620]   if it doesn't do exactly what you want the first time,
[00:40:48.620 --> 00:40:51.740]   then there's going to be some outer loop of debugging
[00:40:51.740 --> 00:40:53.500]   or improving or tweaking things.
[00:40:53.500 --> 00:40:55.940]   And then you're gonna need the insights
[00:40:55.940 --> 00:40:58.540]   in order to make progress there
[00:40:58.540 --> 00:40:59.940]   and to understand what's going on.
[00:40:59.940 --> 00:41:01.620]   Like if there is something that's going wrong,
[00:41:01.620 --> 00:41:04.260]   that's when you need to be able to look a little bit more
[00:41:04.260 --> 00:41:05.100]   under the hood.
[00:41:05.100 --> 00:41:06.820]   - Yeah, there's actually,
[00:41:06.820 --> 00:41:07.940]   I just wanna call our attention to,
[00:41:07.940 --> 00:41:09.340]   there's a question in the chat
[00:41:09.340 --> 00:41:10.780]   that seems relevant to what we're talking about,
[00:41:10.780 --> 00:41:12.660]   which is how do you address misalignment
[00:41:12.660 --> 00:41:14.580]   between the model prediction and human intuition
[00:41:14.580 --> 00:41:16.580]   to a non-technical stakeholder?
[00:41:16.580 --> 00:41:17.820]   Is that the current limitation
[00:41:17.820 --> 00:41:19.420]   to put neural networks into production?
[00:41:19.540 --> 00:41:22.620]   And I mean, I think you just totally answered that question,
[00:41:22.620 --> 00:41:25.060]   which is when you see misalignment,
[00:41:25.060 --> 00:41:26.740]   that's where it's so important to go look at
[00:41:26.740 --> 00:41:29.340]   some kind of automated insights for the model and say,
[00:41:29.340 --> 00:41:31.180]   here's why the model thinks this is the case.
[00:41:31.180 --> 00:41:33.420]   And you'd be like, oh, well, that's obviously wrong.
[00:41:33.420 --> 00:41:34.460]   The model's not correct.
[00:41:34.460 --> 00:41:36.580]   And now you know you need to update the model,
[00:41:36.580 --> 00:41:38.660]   or the flip side of that, which is like, oh, interesting.
[00:41:38.660 --> 00:41:40.980]   The model has discovered a relationship that makes sense
[00:41:40.980 --> 00:41:44.340]   that we didn't consider when we judged this as a human.
[00:41:44.340 --> 00:41:47.420]   - Yeah, I know there's human resource systems
[00:41:47.980 --> 00:41:49.580]   doing document classification.
[00:41:49.580 --> 00:41:52.860]   So they have a human reviewer,
[00:41:52.860 --> 00:41:55.220]   and there's a recommendation that comes with it.
[00:41:55.220 --> 00:41:56.820]   But you'll want to hold some of those back
[00:41:56.820 --> 00:41:58.900]   and look at that human and machine alignment
[00:41:58.900 --> 00:42:00.860]   and understand when there's disagreement.
[00:42:00.860 --> 00:42:02.100]   And it could be a flaw of the model.
[00:42:02.100 --> 00:42:04.900]   It could be a coaching opportunity of your human reviewers.
[00:42:04.900 --> 00:42:07.420]   Maybe they have an unconscious bias.
[00:42:07.420 --> 00:42:09.940]   So I think absolutely it's important.
[00:42:09.940 --> 00:42:14.940]   And it's hard to earn the trust of non-technical stakeholders.
[00:42:14.940 --> 00:42:18.260]   But for me, it's not saying here's the average prediction.
[00:42:18.260 --> 00:42:19.700]   Here's how it can go wrong.
[00:42:19.700 --> 00:42:21.020]   Here's a very high prediction.
[00:42:21.020 --> 00:42:22.140]   Here's a very low prediction.
[00:42:22.140 --> 00:42:25.260]   Just like a non-technical person interviewing someone
[00:42:25.260 --> 00:42:27.980]   doesn't ask for their CT scan of their brain.
[00:42:27.980 --> 00:42:30.140]   They just interrogate the person.
[00:42:30.140 --> 00:42:31.860]   They ask questions of the person.
[00:42:31.860 --> 00:42:35.340]   And so if I get handed Xcode,
[00:42:35.340 --> 00:42:37.940]   I literally probably could not understand it.
[00:42:37.940 --> 00:42:41.620]   But I can interrogate a model
[00:42:41.620 --> 00:42:45.180]   and get provided with counterfactuals
[00:42:45.180 --> 00:42:49.660]   and highs and lows and where does it go wrong?
[00:42:49.660 --> 00:42:51.100]   Where's its region of uncertainty?
[00:42:51.100 --> 00:42:52.980]   Those types of things help me earn trust
[00:42:52.980 --> 00:42:54.220]   in that type of model.
[00:42:54.220 --> 00:42:55.980]   And those can be, you don't have to do that
[00:42:55.980 --> 00:42:57.380]   through an AutoML system,
[00:42:57.380 --> 00:43:00.540]   but those types of interrogations can be made
[00:43:00.540 --> 00:43:03.540]   to be easier to get a lot of that out
[00:43:03.540 --> 00:43:04.940]   in the right type of system.
[00:43:04.940 --> 00:43:08.340]   - Yeah, I think going back,
[00:43:08.340 --> 00:43:10.580]   the question is addressing misalignment
[00:43:10.580 --> 00:43:11.940]   between what the models learned
[00:43:11.940 --> 00:43:14.500]   and what a human's intuition.
[00:43:14.500 --> 00:43:16.780]   I think of that as like the best case scenario
[00:43:16.780 --> 00:43:18.420]   is when that happens,
[00:43:18.420 --> 00:43:19.860]   'cause that's our chance to learn.
[00:43:19.860 --> 00:43:22.500]   And it could be that the human's intuition is wrong.
[00:43:22.500 --> 00:43:24.780]   And in most cases, that requires doing some analytics
[00:43:24.780 --> 00:43:25.620]   and showing them like,
[00:43:25.620 --> 00:43:29.540]   "Hey, you thought that people who had X
[00:43:29.540 --> 00:43:31.140]   would have a higher churn rate.
[00:43:31.140 --> 00:43:32.820]   And actually it turns out that's not the case."
[00:43:32.820 --> 00:43:34.140]   And maybe we can even break that down
[00:43:34.140 --> 00:43:36.140]   with simple statistics,
[00:43:36.140 --> 00:43:38.580]   which is frequently gonna happen outside of AutoML.
[00:43:38.580 --> 00:43:41.180]   That's a chance for the human to learn.
[00:43:41.180 --> 00:43:45.540]   Or if we have the right ways of interrogating the model
[00:43:45.540 --> 00:43:47.100]   like Ted was talking about,
[00:43:47.100 --> 00:43:48.900]   then we can learn something about,
[00:43:48.900 --> 00:43:52.540]   maybe you were missing certain features.
[00:43:52.540 --> 00:43:55.900]   So we wanted to interpret a certain feature
[00:43:55.900 --> 00:43:58.140]   as like a causal impact.
[00:43:58.140 --> 00:44:00.460]   And it showed up as having high permutation points
[00:44:00.460 --> 00:44:05.460]   and the pressure dependence plot went up sharply,
[00:44:08.540 --> 00:44:10.540]   but actually it's a proxy for something else.
[00:44:10.540 --> 00:44:12.100]   And depending what we're gonna do with the model,
[00:44:12.100 --> 00:44:13.740]   like that could be super important.
[00:44:13.740 --> 00:44:15.740]   And so it's when you get these misalignments,
[00:44:15.740 --> 00:44:18.460]   like that is the opportunity to say
[00:44:18.460 --> 00:44:19.620]   there really is something wrong
[00:44:19.620 --> 00:44:21.540]   about at least one of these two,
[00:44:21.540 --> 00:44:23.380]   and that's our chance to improve them.
[00:44:23.380 --> 00:44:29.420]   - Yeah, so we talked about like ways,
[00:44:29.420 --> 00:44:30.780]   as it sounds like there are ways
[00:44:30.780 --> 00:44:33.380]   that AutoML can make data scientists,
[00:44:33.380 --> 00:44:34.740]   like augment data scientists
[00:44:34.740 --> 00:44:36.100]   and help them be more productive
[00:44:36.100 --> 00:44:38.900]   by giving them insights to models,
[00:44:38.900 --> 00:44:41.940]   baselines, so on and so forth.
[00:44:41.940 --> 00:44:45.660]   My observation is like some interesting observation,
[00:44:45.660 --> 00:44:47.660]   at least from my experiences,
[00:44:47.660 --> 00:44:51.940]   for some reason like Silicon Valley type company,
[00:44:51.940 --> 00:44:53.260]   tech companies have not,
[00:44:53.260 --> 00:44:58.380]   I don't see really any AutoML happening there
[00:44:58.380 --> 00:45:03.260]   versus let's say middle America or even East Coast.
[00:45:04.820 --> 00:45:08.620]   And it's like a strong resistance to it in a lot of cases
[00:45:08.620 --> 00:45:11.700]   in kind of like tech companies
[00:45:11.700 --> 00:45:15.540]   wanting to do things from scratch, like the whole way.
[00:45:15.540 --> 00:45:20.620]   So first question is, do you feel the same way?
[00:45:20.620 --> 00:45:23.540]   Or in second, like does anyone have any ideas
[00:45:23.540 --> 00:45:27.580]   of why that might be if you do share the same observation?
[00:45:27.580 --> 00:45:32.100]   - I'll tell you my view, which is,
[00:45:33.340 --> 00:45:35.580]   so actually last Friday,
[00:45:35.580 --> 00:45:39.580]   I was talking to someone who works on a product at Google.
[00:45:39.580 --> 00:45:42.940]   She's doing basically identifying
[00:45:42.940 --> 00:45:45.780]   what messages through a bunch of different Google channels
[00:45:45.780 --> 00:45:48.700]   are likely to be phishing, just building ML models.
[00:45:48.700 --> 00:45:51.460]   So that team does nothing but AutoML.
[00:45:51.460 --> 00:45:53.420]   And it's like, they've got a problem.
[00:45:53.420 --> 00:45:55.780]   They found that it is easier to operationalize
[00:45:55.780 --> 00:45:57.700]   if the models all come out in a consistent format
[00:45:57.700 --> 00:45:59.060]   and it's easier to deliver that
[00:45:59.060 --> 00:46:01.140]   with some AutoML technology.
[00:46:01.140 --> 00:46:04.300]   And so they use AutoML.
[00:46:04.300 --> 00:46:08.540]   I worked at Kaggle after it existed for seven or eight years
[00:46:08.540 --> 00:46:12.780]   the people at Kaggle really do care about machine learning.
[00:46:12.780 --> 00:46:15.500]   And yet we existed for seven or eight years
[00:46:15.500 --> 00:46:18.140]   before ever having a machine learning model
[00:46:18.140 --> 00:46:22.500]   that was actually part of like how Kaggle works.
[00:46:22.500 --> 00:46:26.140]   And then they wanted to do some automatic moderation
[00:46:26.140 --> 00:46:26.980]   in the forums.
[00:46:26.980 --> 00:46:29.340]   They thought the easiest way to do it
[00:46:29.340 --> 00:46:30.740]   was to build a model with AutoML.
[00:46:30.740 --> 00:46:31.940]   So that's what they did.
[00:46:31.940 --> 00:46:35.180]   And so I think there are people who are savvy enough
[00:46:35.180 --> 00:46:37.500]   that they just want to make things easy
[00:46:37.500 --> 00:46:41.660]   and they don't feel that they need to try
[00:46:41.660 --> 00:46:43.980]   and make things hard for themselves to impress each other.
[00:46:43.980 --> 00:46:45.620]   And I think that that group
[00:46:45.620 --> 00:46:48.420]   is frequently trying to use AutoML.
[00:46:48.420 --> 00:46:52.700]   And then you've got a group who is probably most startups
[00:46:52.700 --> 00:46:56.780]   who take pride in their ML skill
[00:46:56.780 --> 00:46:59.060]   and don't wanna use AutoML because it's not that impressive
[00:46:59.060 --> 00:47:01.660]   and you feel like you're taking the easy route.
[00:47:01.660 --> 00:47:03.260]   And then I think there's probably the other side
[00:47:03.260 --> 00:47:06.500]   which is actually sort of like that meme of the IQ
[00:47:06.500 --> 00:47:08.780]   where people in the ends are using AutoML.
[00:47:08.780 --> 00:47:09.700]   But I think that the people
[00:47:09.700 --> 00:47:11.380]   who are trying to make their lives easier
[00:47:11.380 --> 00:47:14.220]   because they just feel like this is not our core competency
[00:47:14.220 --> 00:47:16.340]   they also are finding AutoML appealing
[00:47:16.340 --> 00:47:18.460]   but it's that middle ground of people who say like
[00:47:18.460 --> 00:47:20.660]   we're proud of our ML skill
[00:47:20.660 --> 00:47:24.900]   and we're willing to sacrifice productivity to show off
[00:47:24.900 --> 00:47:28.140]   are the ones who are gonna find AutoML least appealing.
[00:47:28.140 --> 00:47:31.100]   - That sounds a lot like the not invented ear syndrome.
[00:47:31.100 --> 00:47:33.020]   Like you see that all the time in tech.
[00:47:33.020 --> 00:47:35.060]   Like you've got a piece of technology
[00:47:35.060 --> 00:47:36.660]   you really wanna do it yourself
[00:47:36.660 --> 00:47:38.980]   because it's fun to build something.
[00:47:38.980 --> 00:47:40.700]   And sometimes the most productive thing to do
[00:47:40.700 --> 00:47:43.100]   is just grab the off the shelf thing
[00:47:43.100 --> 00:47:45.500]   that does it for you and move on.
[00:47:45.500 --> 00:47:48.020]   - This is super interesting
[00:47:48.020 --> 00:47:50.540]   because I feel like I had a different experience
[00:47:50.540 --> 00:47:54.380]   with AutoML in that I found that it works well
[00:47:54.380 --> 00:47:56.940]   when your infrastructure is very solid or robust
[00:47:56.940 --> 00:47:58.460]   when you can trust all of the data
[00:47:58.460 --> 00:48:01.860]   you can trust the pipes going in and out
[00:48:01.860 --> 00:48:03.460]   your data is produced consistently.
[00:48:03.460 --> 00:48:04.700]   Somebody is owning it.
[00:48:04.700 --> 00:48:05.980]   If something goes wrong
[00:48:05.980 --> 00:48:08.700]   there's somebody upstream you can bug.
[00:48:08.700 --> 00:48:10.900]   But like whenever you don't have that
[00:48:10.900 --> 00:48:13.900]   AutoML will deliver different things every time
[00:48:13.900 --> 00:48:16.500]   or you'll be confused or curious
[00:48:16.500 --> 00:48:18.500]   why isn't my model working?
[00:48:18.500 --> 00:48:20.700]   And there's this tendency to blame it on
[00:48:20.700 --> 00:48:22.100]   I don't know what you last did
[00:48:22.100 --> 00:48:24.100]   or the most complex piece of your system
[00:48:24.100 --> 00:48:26.060]   rather than like maybe it's just the plumbing
[00:48:26.060 --> 00:48:27.460]   that doesn't work.
[00:48:27.460 --> 00:48:29.980]   And these like fan companies and bigger companies
[00:48:29.980 --> 00:48:32.340]   right like have the plumbing nailed to a T
[00:48:32.340 --> 00:48:33.900]   they have like five data engineers
[00:48:33.900 --> 00:48:36.260]   for every pipeline that exists.
[00:48:36.260 --> 00:48:38.580]   Anyone is on call any second of the day.
[00:48:38.580 --> 00:48:40.740]   And I don't know really how to bring that culture
[00:48:40.740 --> 00:48:44.340]   to all of the Silicon Valley kind of data startups.
[00:48:44.340 --> 00:48:52.660]   - My experience was pushed back to an AutoML system
[00:48:52.660 --> 00:48:55.580]   by pointing out everything it didn't do.
[00:48:55.580 --> 00:48:59.500]   And of course any abstraction will just have limitations.
[00:48:59.500 --> 00:49:01.540]   Like despite Zach and Dan's best effort
[00:49:01.540 --> 00:49:04.340]   they're not able to build everything into our app.
[00:49:04.340 --> 00:49:05.940]   Right, so they get into these corner cases
[00:49:05.940 --> 00:49:07.220]   and they say the whole system's flawed
[00:49:07.220 --> 00:49:10.980]   'cause it doesn't do this, you know, these scans.
[00:49:10.980 --> 00:49:12.620]   It's like, yeah, but are you ever gonna use them?
[00:49:12.620 --> 00:49:13.580]   Do you ever need them?
[00:49:13.580 --> 00:49:15.620]   Like, what are we talking about here, right?
[00:49:15.620 --> 00:49:18.260]   Are you gonna actually deploy that, right?
[00:49:18.260 --> 00:49:20.940]   And so you get kind of into that perspective.
[00:49:20.940 --> 00:49:21.940]   I don't know where it stems from
[00:49:21.940 --> 00:49:23.980]   but that was like a real experience that I had.
[00:49:23.980 --> 00:49:28.660]   I know someone who was part of the evaluation team
[00:49:28.660 --> 00:49:31.460]   for an AutoML system and I'm not joking.
[00:49:31.460 --> 00:49:34.100]   She was a brilliant data scientist, PhD,
[00:49:34.100 --> 00:49:35.580]   amazing data scientist.
[00:49:35.580 --> 00:49:40.580]   And she clicked on every single button within the system
[00:49:40.580 --> 00:49:42.740]   and documented everything it didn't do.
[00:49:42.740 --> 00:49:44.500]   Not necessarily what it did.
[00:49:44.500 --> 00:49:50.620]   - Robert, you were saying something?
[00:49:50.620 --> 00:49:55.300]   - No, no, I don't know if I have any special insight
[00:49:55.300 --> 00:49:57.380]   on this particular topic.
[00:49:57.380 --> 00:50:00.780]   - No, no, that's totally fine.
[00:50:00.780 --> 00:50:04.140]   - I had one more thought, which is just, you know,
[00:50:04.140 --> 00:50:06.140]   education is valuable too.
[00:50:06.140 --> 00:50:07.500]   And there's a certain degree
[00:50:07.500 --> 00:50:08.940]   to which building something yourself,
[00:50:08.940 --> 00:50:10.260]   you learn how it works
[00:50:10.260 --> 00:50:13.340]   and that helps you become better at it.
[00:50:13.340 --> 00:50:16.180]   And I think if you look at it through that lens,
[00:50:16.180 --> 00:50:17.740]   you know, there's an opportunity for AutoML,
[00:50:17.740 --> 00:50:21.620]   which is, hey, if AutoML was better
[00:50:21.620 --> 00:50:24.100]   at explaining itself to you and saying,
[00:50:24.100 --> 00:50:26.700]   hey, when you use XGBoost models,
[00:50:26.700 --> 00:50:28.820]   the one hot encoding is actually really bad
[00:50:28.820 --> 00:50:30.180]   and normal encoding is really good,
[00:50:30.180 --> 00:50:32.780]   even though that's super counterintuitive and here's why,
[00:50:32.780 --> 00:50:35.780]   you know, now you know something, you've learned something
[00:50:35.780 --> 00:50:37.940]   and the AutoML system just doing it for you,
[00:50:37.940 --> 00:50:40.660]   like that's valuable, but telling you why it did it
[00:50:40.660 --> 00:50:43.300]   and helping you, you know, learn more data science
[00:50:43.300 --> 00:50:45.820]   is valuable to like the educational part of it
[00:50:45.820 --> 00:50:48.020]   that you get from figuring out yourself,
[00:50:48.020 --> 00:50:50.060]   you know, does have value.
[00:50:50.060 --> 00:50:50.900]   - I'm glad you said that,
[00:50:50.900 --> 00:50:53.820]   like one of my earliest experience with AutoML,
[00:50:53.820 --> 00:50:56.860]   so like when I was working at DataRobot,
[00:50:56.860 --> 00:50:58.860]   I remember this one conversation.
[00:50:58.860 --> 00:51:01.060]   So DataRobot, you know, like they have this,
[00:51:01.060 --> 00:51:05.420]   when you evaluate your models on a holdout set,
[00:51:05.420 --> 00:51:09.340]   it's a hidden behind this, it's obfuscated,
[00:51:09.340 --> 00:51:11.260]   the holdout score is obfuscated
[00:51:11.260 --> 00:51:13.060]   and you can only look at the holdout set
[00:51:13.060 --> 00:51:15.780]   after clicking through some buttons
[00:51:15.780 --> 00:51:16.740]   and it like warns you,
[00:51:16.740 --> 00:51:19.260]   are you sure you wanna look at the holdout set?
[00:51:19.260 --> 00:51:22.620]   And I remember when demoing this to a customer,
[00:51:22.620 --> 00:51:26.380]   they, you know, there was a debate,
[00:51:26.380 --> 00:51:28.700]   like why do you have,
[00:51:28.700 --> 00:51:30.620]   why is there so many clicks to look at the holdout set
[00:51:30.620 --> 00:51:32.500]   and why shouldn't I just choose my model
[00:51:32.500 --> 00:51:33.740]   based on the holdout set?
[00:51:33.740 --> 00:51:37.140]   And this actually conversation happened
[00:51:37.140 --> 00:51:40.620]   tens or hundreds of times,
[00:51:40.620 --> 00:51:45.060]   which is really interesting kind of like
[00:51:45.060 --> 00:51:47.300]   with regard to the education perspective,
[00:51:47.300 --> 00:51:51.540]   there's some things that I feel that, you know,
[00:51:51.540 --> 00:51:54.260]   could be automated that help people learn.
[00:51:54.260 --> 00:51:56.500]   But that was surprising to me,
[00:51:56.500 --> 00:51:59.700]   like, you know, some surprising aspect of AutoML.
[00:51:59.700 --> 00:52:03.100]   Is there any other surprising things that you find
[00:52:03.100 --> 00:52:05.500]   with people that are using AutoML in practice?
[00:52:05.500 --> 00:52:09.340]   Like, you know, that when they first start to use AutoML,
[00:52:09.340 --> 00:52:11.460]   you find like people learning certain things
[00:52:11.460 --> 00:52:14.500]   or doing certain things, you know,
[00:52:14.500 --> 00:52:17.220]   that are like, let's say counterintuitive.
[00:52:17.220 --> 00:52:19.540]   I'm just curious, like what you see out there.
[00:52:19.540 --> 00:52:26.540]   I see the wheels turning in everyone's head.
[00:52:26.540 --> 00:52:32.580]   - I think the use of it for experimentation,
[00:52:32.580 --> 00:52:34.380]   that it doesn't always,
[00:52:34.380 --> 00:52:35.380]   like if you had the ratio
[00:52:35.380 --> 00:52:37.940]   of how many AutoML projects are kicked off
[00:52:37.940 --> 00:52:40.220]   versus how many make it to production,
[00:52:40.220 --> 00:52:42.820]   that unbalanced aspect of it
[00:52:44.020 --> 00:52:46.020]   was something I really wasn't,
[00:52:46.020 --> 00:52:47.580]   I mean, I guess I shouldn't have been surprised,
[00:52:47.580 --> 00:52:48.780]   but I was surprised, right?
[00:52:48.780 --> 00:52:51.780]   You might have a typical data scientist
[00:52:51.780 --> 00:52:54.100]   before AutoML or coding up manually,
[00:52:54.100 --> 00:52:56.860]   they may spend six months building a model
[00:52:56.860 --> 00:52:59.020]   and, you know, various models
[00:52:59.020 --> 00:53:00.860]   with tuning parameters and so on,
[00:53:00.860 --> 00:53:03.420]   and then maybe get one into production, right?
[00:53:03.420 --> 00:53:07.780]   And then just observing that in an automated system
[00:53:07.780 --> 00:53:09.620]   and like seeing that huge discrepancy,
[00:53:09.620 --> 00:53:10.660]   'cause when I learned about it,
[00:53:10.660 --> 00:53:12.900]   it was like all textbook cases.
[00:53:12.900 --> 00:53:14.940]   It's like, yeah, of course this seems to be the best model,
[00:53:14.940 --> 00:53:16.780]   so that would be the one we move forward with, right?
[00:53:16.780 --> 00:53:18.940]   And then the next week you learn a new modeling type
[00:53:18.940 --> 00:53:21.780]   or whatever, and that's the next best model, right?
[00:53:21.780 --> 00:53:24.700]   So that was something that I found surprising,
[00:53:24.700 --> 00:53:27.340]   that experimentation to actual production,
[00:53:27.340 --> 00:53:28.700]   that drop off is huge.
[00:53:28.700 --> 00:53:36.020]   - One sort of question I have for both Robert and Shreya
[00:53:36.020 --> 00:53:40.900]   is kind of like in the academia research realms,
[00:53:40.900 --> 00:53:42.500]   what's like the state of AutoML?
[00:53:42.500 --> 00:53:44.500]   Is there any interesting work going on there
[00:53:44.500 --> 00:53:46.620]   that you know of, just out of curiosity,
[00:53:46.620 --> 00:53:50.100]   like anything that comes to mind?
[00:53:50.100 --> 00:53:53.660]   - Shreya, do you wanna start?
[00:53:53.660 --> 00:53:57.380]   - Nothing super new in particular,
[00:53:57.380 --> 00:53:59.100]   I read a couple of surveys.
[00:53:59.100 --> 00:54:02.420]   People are recently starting to think about
[00:54:02.420 --> 00:54:07.260]   how your AutoML configs change
[00:54:07.260 --> 00:54:10.620]   as you deploy your models in production.
[00:54:10.620 --> 00:54:13.180]   So like nature of learning from new data,
[00:54:13.180 --> 00:54:14.740]   incorporating active learning
[00:54:14.740 --> 00:54:18.980]   into these AutoML systems and so forth.
[00:54:18.980 --> 00:54:21.460]   But it's not my area of research,
[00:54:21.460 --> 00:54:23.340]   so I could be very vastly wrong.
[00:54:23.340 --> 00:54:26.180]   Maybe yesterday something awesome came out on archive
[00:54:26.180 --> 00:54:29.860]   and this feels too fast, I can't keep up.
[00:54:29.860 --> 00:54:36.140]   - Yeah, I think one interesting area is around cost,
[00:54:36.140 --> 00:54:38.580]   making things more efficient, making things cheaper,
[00:54:38.580 --> 00:54:41.900]   especially if you're running tons and tons of experiments,
[00:54:41.900 --> 00:54:44.100]   they're all using a lot of GPUs.
[00:54:44.100 --> 00:54:48.700]   I think some of the exciting results have been around
[00:54:48.700 --> 00:54:52.780]   being able to conserve resources
[00:54:52.780 --> 00:54:55.620]   by stopping experiments earlier,
[00:54:55.620 --> 00:54:57.220]   being able to invest more resources
[00:54:57.220 --> 00:55:01.220]   in the better performing experiments and things like that.
[00:55:01.220 --> 00:55:03.740]   I think there's still a lot of low hanging fruit there.
[00:55:05.020 --> 00:55:10.020]   Another interesting difference between how I see
[00:55:10.020 --> 00:55:12.460]   some of the AutoML research
[00:55:12.460 --> 00:55:16.700]   or hyperparameter research research in academia
[00:55:16.700 --> 00:55:19.380]   versus in practice is that a lot of times
[00:55:19.380 --> 00:55:21.060]   when people talk about hyperparameter search
[00:55:21.060 --> 00:55:22.220]   and tuning and things like that,
[00:55:22.220 --> 00:55:24.220]   it's about the discussion is around
[00:55:24.220 --> 00:55:26.980]   how to get to the absolute best possible performance,
[00:55:26.980 --> 00:55:30.260]   how to really eat the last few percentages out.
[00:55:30.260 --> 00:55:34.100]   Whereas in practice, a lot of hyperparameter search
[00:55:34.100 --> 00:55:35.540]   and tuning and a lot of this work
[00:55:35.540 --> 00:55:37.900]   is around just getting something to work at all,
[00:55:37.900 --> 00:55:42.500]   making sure that you're not wildly misconfiguring something
[00:55:42.500 --> 00:55:45.460]   and just seeing if the technique is plausible.
[00:55:45.460 --> 00:55:49.460]   And so that's one difference I've seen.
[00:55:49.460 --> 00:55:53.420]   - And that goes back to,
[00:55:53.420 --> 00:55:56.660]   I think you were talking about good defaults earlier,
[00:55:56.660 --> 00:56:00.540]   like just having libraries that set things up in a way
[00:56:00.540 --> 00:56:01.700]   that they'll tend to succeed
[00:56:01.700 --> 00:56:03.580]   the first time you try them is so important.
[00:56:03.580 --> 00:56:05.260]   And that takes time to learn
[00:56:05.260 --> 00:56:07.300]   like what are good defaults for neural networks?
[00:56:07.300 --> 00:56:08.780]   Like that's a whole area of research.
[00:56:08.780 --> 00:56:11.260]   Like there's that, I forget who it is,
[00:56:11.260 --> 00:56:13.300]   there's one guy who's published three papers that I love
[00:56:13.300 --> 00:56:14.580]   and I can't remember his name now,
[00:56:14.580 --> 00:56:15.420]   it's a terrible name,
[00:56:15.420 --> 00:56:16.820]   but they're just all about
[00:56:16.820 --> 00:56:18.820]   how do you actually make the model work well?
[00:56:18.820 --> 00:56:20.140]   I think he's one of the guys who first published
[00:56:20.140 --> 00:56:22.140]   about cyclic learning rates or something.
[00:56:22.140 --> 00:56:26.500]   - One other thing that I think is interesting
[00:56:26.500 --> 00:56:30.320]   is different programming languages and ecosystems
[00:56:30.320 --> 00:56:33.380]   make people more or less likely to use AutoML tools
[00:56:33.380 --> 00:56:36.820]   like Python is very, very ML rich
[00:56:36.820 --> 00:56:40.580]   and you can very easily prototype a lot of deep learning,
[00:56:40.580 --> 00:56:42.660]   whatever you want in Python.
[00:56:42.660 --> 00:56:44.780]   And I've noticed that at least like in the interview study
[00:56:44.780 --> 00:56:47.100]   that people who are more familiar with Python
[00:56:47.100 --> 00:56:50.500]   or less or use AutoML techniques less
[00:56:50.500 --> 00:56:54.060]   than like R or other programming languages.
[00:56:54.060 --> 00:56:58.020]   - That's really interesting.
[00:56:58.020 --> 00:56:59.900]   I have to read more about that.
[00:56:59.900 --> 00:57:03.580]   That's a little bit counterintuitive to me in some ways.
[00:57:03.580 --> 00:57:06.380]   - I think it makes sense from the lens of like,
[00:57:06.380 --> 00:57:08.860]   if I can build you a model tomorrow
[00:57:08.860 --> 00:57:13.100]   or like try out 15 models really, really easily,
[00:57:13.100 --> 00:57:14.400]   I might as well do that.
[00:57:14.400 --> 00:57:18.860]   I know how to do it, it's mindless sort of philosophy,
[00:57:18.860 --> 00:57:19.700]   but I don't know.
[00:57:19.700 --> 00:57:21.580]   - You can write a for loop like Dan said
[00:57:21.580 --> 00:57:24.140]   and I mean, argue that it is a kind of AutoML,
[00:57:24.140 --> 00:57:26.900]   like it's just easy to roll it yourself in Python
[00:57:26.900 --> 00:57:29.700]   'cause all the libraries have the same interface,
[00:57:29.700 --> 00:57:30.700]   which is really nice.
[00:57:30.700 --> 00:57:33.900]   - I guess I'm just like super lazy.
[00:57:33.900 --> 00:57:35.300]   I don't even wanna do that.
[00:57:35.300 --> 00:57:37.660]   (all laughing)
[00:57:37.660 --> 00:57:39.020]   - I mean, honestly, this is like,
[00:57:39.020 --> 00:57:41.420]   I really love Python too and Scikit-learn,
[00:57:41.420 --> 00:57:43.100]   but I think there's a future for AutoML,
[00:57:43.100 --> 00:57:44.060]   which is really cool.
[00:57:44.060 --> 00:57:46.220]   I think TPOT is getting a little bit there,
[00:57:46.220 --> 00:57:49.780]   but like if AutoML can output a Scikit-learn pipeline,
[00:57:49.780 --> 00:57:51.500]   like people get that, they understand it,
[00:57:51.500 --> 00:57:53.380]   that's something you can fiddle around with.
[00:57:53.380 --> 00:57:54.820]   And I feel like that strikes a good balance
[00:57:54.820 --> 00:57:56.220]   between well, I've automated it,
[00:57:56.220 --> 00:57:58.020]   but then I can still fiddle with it afterwards
[00:57:58.020 --> 00:57:59.620]   'cause I understand all the parts
[00:57:59.620 --> 00:58:00.860]   in the Scikit-learn pipeline,
[00:58:00.860 --> 00:58:02.460]   like they're easy to comprehend.
[00:58:02.460 --> 00:58:06.940]   - Cool, so it looks like we're getting
[00:58:06.940 --> 00:58:10.740]   into the end of this discussion.
[00:58:10.740 --> 00:58:14.420]   Just wanna, just in case anyone wants to say anything,
[00:58:14.420 --> 00:58:17.220]   just wanna give people opportunity,
[00:58:17.220 --> 00:58:19.900]   otherwise we can say goodbye.
[00:58:19.900 --> 00:58:23.260]   - Yeah, I think there's just a lot of,
[00:58:23.260 --> 00:58:24.580]   a lot of the interesting challenges
[00:58:24.580 --> 00:58:27.260]   or some of the bottlenecks are around moving to production
[00:58:27.260 --> 00:58:29.500]   and really, like we talked about,
[00:58:29.500 --> 00:58:30.820]   once you train the model,
[00:58:30.820 --> 00:58:32.780]   integrating that into an actual product
[00:58:32.780 --> 00:58:34.900]   or integrating that into another application
[00:58:34.900 --> 00:58:35.860]   that you're building.
[00:58:35.860 --> 00:58:38.460]   And there, there's a big software engineering component,
[00:58:38.460 --> 00:58:41.020]   there's a big infrastructure component,
[00:58:41.020 --> 00:58:44.220]   and a lot of these things are bottlenecks as well,
[00:58:44.220 --> 00:58:46.620]   that in addition to the algorithmic side.
[00:58:46.620 --> 00:58:52.140]   - Makes sense.
[00:58:52.140 --> 00:58:54.620]   Yeah, so it was really great to kind of hang out
[00:58:54.620 --> 00:58:57.900]   with all of you for this hour, talk about AutoML.
[00:58:57.900 --> 00:59:01.460]   It was a really cool conversation.
[00:59:01.460 --> 00:59:03.900]   - Yeah, thanks for hosting.
[00:59:03.900 --> 00:59:04.740]   - Yeah.
[00:59:05.260 --> 00:59:07.860]   (upbeat music)
[00:59:07.860 --> 00:59:10.460]   (upbeat music)
[00:59:10.460 --> 00:59:13.060]   (upbeat music)
[00:59:13.060 --> 00:59:15.640]   (upbeat music)
[00:59:15.640 --> 00:59:17.640]   You

