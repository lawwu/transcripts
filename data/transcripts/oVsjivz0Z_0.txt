
[00:00:00.000 --> 00:00:06.840]   So since I'm so terrified of messing up, I always wait to
[00:00:06.840 --> 00:00:09.920]   hear an echo. And for that duration, I'm just looking
[00:00:09.920 --> 00:00:13.520]   around the screen everywhere just looking for a sign to see
[00:00:13.520 --> 00:00:14.520]   if I'm live or not.
[00:00:14.520 --> 00:00:18.480]   I wanted to also have a feedback on another monitor, but I
[00:00:18.480 --> 00:00:19.920]   thought it might generate some echo.
[00:00:19.920 --> 00:00:27.920]   Also, I think I can see myself and let me make sure my audio is
[00:00:27.920 --> 00:00:28.600]   connected.
[00:00:28.600 --> 00:00:42.760]   I think we're live. That's perfect.
[00:00:42.760 --> 00:00:48.640]   Awesome. I'll just get started. Let me share my screen real
[00:00:48.640 --> 00:00:54.200]   quick. And as a proof, I was still in the middle of
[00:00:54.200 --> 00:00:57.080]   researching about Derek's journey. So that's why you see
[00:00:57.080 --> 00:01:00.240]   this blog post open now I'm on the right screen. Hey, everybody,
[00:01:00.240 --> 00:01:03.760]   thank you for joining us on a Friday evening. I'm super
[00:01:03.760 --> 00:01:07.840]   excited to be hosting another Chai Time live Kaggle Talk. This
[00:01:07.840 --> 00:01:11.160]   time with Derek Kwecek. I learned the Polish pronunciation
[00:01:11.160 --> 00:01:14.600]   of his name yesterday. So I hope I got that right. Derek, thank
[00:01:14.600 --> 00:01:16.440]   you so much for joining me today.
[00:01:16.440 --> 00:01:20.120]   Thank you. It's an honor to be on your show, Sam.
[00:01:20.120 --> 00:01:25.720]   I'm always inspired and always honored to be learning from my
[00:01:25.720 --> 00:01:30.280]   fastA peers, if I may, although I, I'm sorry to call you my
[00:01:30.280 --> 00:01:33.560]   peer, it's more that I'm a very junior person who's still
[00:01:33.560 --> 00:01:37.120]   starting to learn but I'm super excited to learn about your
[00:01:37.120 --> 00:01:37.480]   journey.
[00:01:37.480 --> 00:01:42.040]   I think when I when I started fastA, I might have started with
[00:01:42.040 --> 00:01:45.920]   your blog post on how to do it. So you might be the reason I'm
[00:01:45.920 --> 00:01:49.120]   actually here and talking, talking to you. So it's, it's
[00:01:49.120 --> 00:01:51.160]   this universe where all of the things are connected.
[00:01:53.720 --> 00:01:57.520]   I'm sorry, you found that blog post. I know there are resources
[00:01:57.520 --> 00:01:58.400]   to get started.
[00:01:58.400 --> 00:02:03.920]   Awesome. I'll quickly introduce the session. Welcome, everybody
[00:02:03.920 --> 00:02:08.280]   and jump into the questions from there. So anyone who's joining
[00:02:08.280 --> 00:02:12.400]   us for the first time, this is the version 2.0 of the series
[00:02:12.400 --> 00:02:16.240]   I've been hosting called Chai Time Data Science. Today, Derek
[00:02:16.240 --> 00:02:20.440]   who's solo one, solo one, the Chai, Tamil and question
[00:02:20.440 --> 00:02:23.320]   answering competition will be sharing his journey and his
[00:02:23.880 --> 00:02:28.120]   solution with us. I really emphasized on solo winning the
[00:02:28.120 --> 00:02:31.680]   competition because anyone who's ever participated in a Kaggle
[00:02:31.680 --> 00:02:35.040]   competition knows it's really hard to be up in the leaderboard.
[00:02:35.040 --> 00:02:38.920]   And especially when you're competing by yourself, finishing
[00:02:38.920 --> 00:02:41.480]   first is truly a dream to many Kagglers.
[00:02:41.480 --> 00:02:47.880]   So this is a live talk and all of you are welcome to participate
[00:02:47.880 --> 00:02:51.160]   in the questions and answers. Let me post this link in the
[00:02:51.160 --> 00:02:52.000]   YouTube chart.
[00:02:52.000 --> 00:03:03.200]   And pin this comment. So if you head over to this link, anyone
[00:03:03.200 --> 00:03:06.880]   that is there in the chart, it should take you to our forums
[00:03:06.880 --> 00:03:08.320]   and you can ask your questions here.
[00:03:08.320 --> 00:03:19.080]   The reason for using the forums is I tried to copy Jeremy
[00:03:19.080 --> 00:03:22.800]   Howard, both of Derek and my guru's approach where you can
[00:03:22.800 --> 00:03:25.600]   actually come back and see the questions and it's not like I'm
[00:03:25.600 --> 00:03:28.680]   talking to a disappearing window. So please consider
[00:03:28.680 --> 00:03:31.120]   asking questions here so that everyone who's watching later
[00:03:31.120 --> 00:03:31.840]   can also learn.
[00:03:31.840 --> 00:03:36.800]   With that, here's the agenda for today. I'll start by asking
[00:03:36.800 --> 00:03:39.440]   Derek about his journey, then we'll try to understand the
[00:03:39.440 --> 00:03:43.360]   competition and then his solution walkthrough. I always
[00:03:43.360 --> 00:03:46.800]   act as the rubber duck or the junior engineer who's asking
[00:03:46.800 --> 00:03:50.360]   silly questions. You can expect a lot of those. And of course,
[00:03:50.360 --> 00:03:52.720]   everyone is welcome to ask any questions.
[00:03:52.720 --> 00:04:00.760]   I wanted to point out Derek's Kaggle profile. He's currently
[00:04:00.760 --> 00:04:06.640]   ranked 28 in the worldwide ranking system on competitions.
[00:04:06.640 --> 00:04:09.520]   Competition is one of the hardest categories in my opinion
[00:04:09.520 --> 00:04:14.160]   on Kaggle. And he's in the top 30 of the worldwide rankings
[00:04:14.160 --> 00:04:17.760]   amongst the best of the best Kagglers. So we all are in a
[00:04:17.760 --> 00:04:21.360]   tree to learn from him. He's a IT director of intelligent
[00:04:21.360 --> 00:04:24.560]   automation and the PNG group. And I would highly suggest that
[00:04:24.560 --> 00:04:28.200]   you can find him on Twitter, which you should at DK 21.
[00:04:28.200 --> 00:04:35.160]   So, Derek, I now want to start by asking about your journey. I
[00:04:35.160 --> 00:04:38.360]   always start around this question, but what inspired you
[00:04:38.360 --> 00:04:42.880]   to pick data science up you were very far ahead in your career
[00:04:42.880 --> 00:04:45.800]   and you decided to switch directions what led up to that
[00:04:45.800 --> 00:04:48.480]   direction and why did you decide to do it?
[00:04:48.480 --> 00:04:54.560]   A good question. I don't think that was a decision. I think it
[00:04:54.560 --> 00:04:59.000]   was more of a process of falling in love with with machine
[00:04:59.000 --> 00:05:03.560]   learning and with data science. Like like many, many others I
[00:05:03.560 --> 00:05:06.560]   like my first encounter, my first proper encounter with
[00:05:06.560 --> 00:05:11.200]   machine learning was the famous Andrew Ng course on Coursera,
[00:05:11.320 --> 00:05:15.400]   the one with Octave. So at that point, I thought like, this is
[00:05:15.400 --> 00:05:19.200]   really neat. Like, I love I love this concept, but I cannot
[00:05:19.200 --> 00:05:21.960]   really do anything about it. Like it was in the strange
[00:05:21.960 --> 00:05:25.120]   language. Like I didn't really I wasn't really able to do
[00:05:25.120 --> 00:05:28.240]   anything practical with it. But I thought that I thought it's
[00:05:28.240 --> 00:05:34.440]   interesting. And so at some point later on, I when my
[00:05:34.440 --> 00:05:37.400]   daughter was born, I went on a paternity leave for a couple of
[00:05:37.400 --> 00:05:40.800]   months to spend time with her. And I also wanted to do
[00:05:40.800 --> 00:05:44.120]   something for for myself during that time. And I thought maybe
[00:05:44.120 --> 00:05:48.200]   it's the time to actually get to to to to practice machine
[00:05:48.200 --> 00:05:51.000]   learning and get to get to figure out how it how it all
[00:05:51.000 --> 00:05:56.800]   works. And, and I wanted to do something practical. I wanted to
[00:05:56.800 --> 00:05:59.840]   prove my skills and I decided I will, I will build an
[00:05:59.840 --> 00:06:03.120]   application that will generate nursery rhymes for children,
[00:06:03.200 --> 00:06:09.120]   like children poet in Polish. And, and to do that, I did the
[00:06:09.120 --> 00:06:12.880]   deep learning specialization on Coursera again by Andrew Ng.
[00:06:12.880 --> 00:06:16.360]   And that was the time where RNNs, recurrent neural
[00:06:16.360 --> 00:06:20.240]   networks were really popular. There was this famous article by
[00:06:20.240 --> 00:06:24.280]   by Andrei Karpaty about the power of RNN. So I experimented
[00:06:24.280 --> 00:06:28.520]   with that a little bit. I got my first, I created my first
[00:06:28.520 --> 00:06:34.080]   data set, I got the network to learn, I got the the network to
[00:06:34.080 --> 00:06:37.080]   go really bad. And I thought, okay, this is probably because
[00:06:37.080 --> 00:06:39.640]   my learning rate is too high. And I thought, well, wow, I
[00:06:39.640 --> 00:06:43.520]   understand how this this concept works. And, and I can practice
[00:06:43.520 --> 00:06:47.720]   it. And there are some outputs like they look like poetry. They
[00:06:47.720 --> 00:06:51.520]   weren't super, super, so the RNNs didn't work so well when it
[00:06:51.520 --> 00:06:55.560]   comes to generating poetry. I did ship that up. I think it's
[00:06:55.560 --> 00:06:58.880]   important to ship, ship the product that you plan to build
[00:06:58.880 --> 00:07:04.000]   it, it had a much simpler Ngram language model behind it, it
[00:07:04.000 --> 00:07:07.680]   worked pretty well. I think it's still somewhere on the free
[00:07:07.680 --> 00:07:12.280]   Heroku server in the in the internet. But at that point, I
[00:07:12.280 --> 00:07:15.880]   I thought, well, I can build something and it's so much fun.
[00:07:15.880 --> 00:07:21.680]   So, so, so I consider this this Coursera and Andrew Ng classes
[00:07:21.680 --> 00:07:26.800]   my undergraduate degree. And my graduate degree is fast.ai. So
[00:07:26.800 --> 00:07:30.040]   at some point, I might have stumbled on your article about
[00:07:30.040 --> 00:07:35.000]   how to do online courses and how to do or not to do fast.ai. And
[00:07:35.000 --> 00:07:39.640]   I started following the course I followed up with the approach, I
[00:07:39.640 --> 00:07:42.800]   started following Jeremy's advice. And, and one of the
[00:07:42.800 --> 00:07:46.320]   pieces of the advice was to was to get on Kaggle and Kaggle
[00:07:46.320 --> 00:07:51.000]   competitions. So so that's how I came and started to do the
[00:07:51.000 --> 00:07:54.360]   competitions on the platform. I actually signed up a couple
[00:07:54.360 --> 00:07:58.680]   years earlier. So when I signed up, I don't remember when or
[00:07:58.680 --> 00:08:02.400]   why I thought I heard about Kaggle, I signed up, I picked a
[00:08:02.400 --> 00:08:08.640]   really dummy username, I didn't consider it seriously. But at
[00:08:08.640 --> 00:08:11.640]   that point, I didn't understand what Kaggle was about. So I had
[00:08:11.640 --> 00:08:14.960]   an account, but I didn't really follow follow anything there.
[00:08:14.960 --> 00:08:20.040]   But but only after after I did the fast.ai lectures, I started
[00:08:20.040 --> 00:08:25.880]   to understand and try to participate in competitions. And
[00:08:25.880 --> 00:08:30.000]   the competition experience was actually super useful, because
[00:08:30.000 --> 00:08:33.280]   at some point, I got the confidence that hey, these are
[00:08:33.280 --> 00:08:37.320]   real business problems, I can solve them. And at some point,
[00:08:37.320 --> 00:08:40.480]   when I when I got the confidence that I can actually build
[00:08:40.480 --> 00:08:45.080]   solutions, I talked to my boss, hey, I want to do some of this
[00:08:45.080 --> 00:08:49.000]   stuff at work. And fortunately, it's it was around the time
[00:08:49.000 --> 00:08:52.400]   where we actually needed to use some of the skills in my work.
[00:08:52.400 --> 00:08:56.280]   So I also started practicing this, this at work. So it was it
[00:08:56.280 --> 00:09:00.280]   was it was a very long process and falling in love with machine
[00:09:00.280 --> 00:09:05.040]   learning. And I, I don't think there was ever a decision to
[00:09:05.040 --> 00:09:06.960]   start doing it, to be honest.
[00:09:06.960 --> 00:09:09.640]   Sanyam Bhutani: That's, that's really interesting. I'd love to
[00:09:09.640 --> 00:09:12.600]   dive into different details. Thanks for the shout out to this
[00:09:12.600 --> 00:09:16.720]   article I had written for the audience called how not to do
[00:09:16.720 --> 00:09:21.360]   fast. And in this, I talk about the struggle of understanding
[00:09:21.360 --> 00:09:24.840]   top down learning. And I have this question for you. You've
[00:09:24.840 --> 00:09:27.280]   been in the industry for so long, and you've worked across
[00:09:27.280 --> 00:09:32.000]   very senior positions. Was it natural to you the top down
[00:09:32.000 --> 00:09:34.800]   learning approach you you mentioned you shipped the
[00:09:34.800 --> 00:09:39.000]   nurseries, even though it wasn't working effectively? To someone
[00:09:39.000 --> 00:09:42.880]   who's just started their journey? How can we like, really
[00:09:42.880 --> 00:09:46.480]   implement this mindset? Because it's always this scary feeling
[00:09:46.480 --> 00:09:51.160]   of sending out something that's incomplete and that that error
[00:09:51.160 --> 00:09:52.000]   that comes with it?
[00:09:52.000 --> 00:10:00.480]   Yeah. Yeah, this is a good question. I'm thinking about
[00:10:00.480 --> 00:10:04.240]   when you're asking this question, I am trying to figure
[00:10:04.240 --> 00:10:07.680]   out if I if I always follow this, this top down approach.
[00:10:07.680 --> 00:10:13.720]   Definitely for me, like when when I think I, I recognize like
[00:10:13.720 --> 00:10:16.040]   this concept of top down learning when when doing fast
[00:10:16.040 --> 00:10:20.240]   AI and listening to Jeremy talk about this, this HPA, I think it
[00:10:20.240 --> 00:10:24.160]   was it was Harvard Research or Harvard professor that talked
[00:10:24.160 --> 00:10:30.040]   about this this top down approach. And let me maybe maybe
[00:10:30.040 --> 00:10:33.040]   use a different experience from my life other than the
[00:10:33.040 --> 00:10:37.200]   professional setting. So I've been practicing Kung Fu for, for
[00:10:37.200 --> 00:10:41.120]   I think 16 years now, it's a longer time than I than I work.
[00:10:41.120 --> 00:10:45.760]   So it was it's quite quite a while. And the what you do when
[00:10:45.760 --> 00:10:50.920]   you practice Kung Fu is you you try to use like, there is a set
[00:10:50.920 --> 00:10:53.760]   of techniques that you're trying to learn. And all of these
[00:10:53.760 --> 00:10:57.440]   techniques have practical application. And if you just use
[00:10:57.440 --> 00:11:00.040]   them, kind of like try to practice them without
[00:11:00.040 --> 00:11:02.600]   understanding their applications, like it's not
[00:11:02.600 --> 00:11:04.960]   gonna work, because you will be just doing the movements
[00:11:04.960 --> 00:11:07.920]   without, without really understanding why, why you're
[00:11:07.920 --> 00:11:10.800]   doing this movement. So when you practice competitions, like
[00:11:10.800 --> 00:11:14.280]   you also have people or the teacher, the master that that
[00:11:14.280 --> 00:11:17.800]   that shows you that technique, and you try to you try to
[00:11:17.800 --> 00:11:21.160]   emulate, like you try to do the same stuff, and then you try to
[00:11:21.160 --> 00:11:23.640]   apply it in practice. So in Kung Fu, we had this this
[00:11:23.640 --> 00:11:26.520]   exercise that in Paris when like somebody tries to attack me and
[00:11:26.520 --> 00:11:30.480]   I try to defend myself, or we try to apply this technique. So
[00:11:30.480 --> 00:11:33.320]   I think this concept of application is super important,
[00:11:33.320 --> 00:11:41.960]   because without it, it's pure theory. And, and, and what I
[00:11:41.960 --> 00:11:46.360]   found myself is, is if I can, if I can use knowledge and build
[00:11:46.360 --> 00:11:50.840]   something, build something, then then I, I feel like making
[00:11:50.840 --> 00:11:55.600]   progress. If I just keep learning, and it's sometimes
[00:11:55.600 --> 00:11:58.600]   it's to participate, understand, I feel I'm making progress, but
[00:11:58.600 --> 00:12:02.440]   it's not tangible. So, so I think it always helps to apply
[00:12:02.440 --> 00:12:05.720]   it to apply the knowledge and big thanks to Jeremy for like
[00:12:05.720 --> 00:12:08.800]   putting out the research behind it and make it really, it's
[00:12:08.800 --> 00:12:09.680]   really straightforward.
[00:12:09.680 --> 00:12:15.640]   I think, and again, sorry to compare myself with you, but I
[00:12:15.640 --> 00:12:21.440]   I gave up. I was studying karate and I gave up around blue belt,
[00:12:21.440 --> 00:12:24.080]   I understand you're a black belt. And one of the things
[00:12:24.080 --> 00:12:28.760]   they really teach in karate or in any form of martial arts is
[00:12:29.200 --> 00:12:32.960]   so much more practice than theory. I think it was like four
[00:12:32.960 --> 00:12:36.920]   months of just doing the same step again, and like probably
[00:12:36.920 --> 00:12:40.120]   one or two sessions of theory throughout that event. So I
[00:12:40.120 --> 00:12:44.680]   really relate to that as well. So connecting that to Kaggle,
[00:12:44.680 --> 00:12:48.520]   how did you find your sifu's on Kaggle? How did you find the
[00:12:48.520 --> 00:12:51.560]   people to follow? And how did you improve on Kaggle?
[00:12:51.560 --> 00:12:58.760]   So, so definitely your show was one of the one of the place to
[00:12:58.760 --> 00:13:04.080]   find all of the sifu's. So, so I, so during during Chinese
[00:13:04.080 --> 00:13:07.800]   competition, I was, I think there were a couple of episodes
[00:13:07.800 --> 00:13:11.080]   that I listened a couple of times in a row just to just
[00:13:11.080 --> 00:13:14.480]   figure out, you know, what, what some of the, some of the
[00:13:14.480 --> 00:13:21.360]   grandmasters are doing and try to try to emulate them. So, so,
[00:13:21.360 --> 00:13:26.280]   and my journey on Kaggle is, has been evolving, like I don't
[00:13:26.280 --> 00:13:29.240]   have one way to approach competition. Like when I started
[00:13:29.240 --> 00:13:32.480]   like very transparently, the first competition, when I scored
[00:13:32.480 --> 00:13:36.360]   a silver medal, I spoke to public colonel, I made some
[00:13:36.360 --> 00:13:39.200]   modifications, I tried to understand what's going on, but
[00:13:39.200 --> 00:13:42.520]   I didn't really fully understand it. I, I just blended
[00:13:42.520 --> 00:13:46.320]   something and, and I submitted it and I was, I was shaken up
[00:13:46.320 --> 00:13:50.480]   into into the silver zone. I didn't learn much, I got hooked
[00:13:50.480 --> 00:13:55.560]   for sure. But I didn't learn much in this competition. So, so
[00:13:56.200 --> 00:13:59.920]   but it was, it was, it was an interesting experience on how to
[00:13:59.920 --> 00:14:05.080]   approach it. And then, and then I tried different, different
[00:14:05.080 --> 00:14:08.600]   ways. I think I invested quite a bit of effort into the TensorFlow
[00:14:08.600 --> 00:14:13.880]   question answering. So, so that I wasn't very proficient back
[00:14:13.880 --> 00:14:17.880]   then, but I tried to put a lot of a lot of effort into
[00:14:17.880 --> 00:14:20.200]   understanding the data and looking at the model
[00:14:20.200 --> 00:14:23.480]   predictions. I think I also, I also learned it from from your
[00:14:23.480 --> 00:14:27.040]   show about, you know, becoming one with data and trying to
[00:14:27.040 --> 00:14:31.440]   really, to really grasp it. So that that was that was helpful.
[00:14:31.440 --> 00:14:35.640]   I think my my big breakthrough was that with sentiment
[00:14:35.640 --> 00:14:39.120]   competition, and that one I actually ended up somewhere in
[00:14:39.120 --> 00:14:43.360]   the I think in the third hundred, like, like somewhere
[00:14:43.360 --> 00:14:46.440]   down down there. So I didn't I didn't get a good score. But
[00:14:46.440 --> 00:14:50.400]   that was the first competition where I, where I started to
[00:14:50.400 --> 00:14:54.000]   actually implement some of my own ideas and, and read papers
[00:14:54.000 --> 00:14:58.600]   and try to implement paper. And at the very end, when I when I
[00:14:58.600 --> 00:15:02.680]   read the top solution, the top solutions on the forum, I
[00:15:02.680 --> 00:15:06.040]   noticed that some of the some of the winning solutions had
[00:15:06.040 --> 00:15:08.640]   similar ideas to what I was exploring. And that was, that
[00:15:08.640 --> 00:15:12.360]   was a huge confidence boost, like, okay, I can get good
[00:15:12.360 --> 00:15:15.760]   ideas, I can, I can approach this competition in the right
[00:15:15.760 --> 00:15:19.920]   way. It's now it's a method of execution. Right. So and here,
[00:15:19.920 --> 00:15:22.840]   again, this continuous improvement comes in from from
[00:15:22.840 --> 00:15:25.680]   Kubefor or Karate if you practice it, you you know, it's
[00:15:25.680 --> 00:15:28.720]   like you try to, like every competition, you try to do it a
[00:15:28.720 --> 00:15:32.240]   bit better. There was a period where I did a bunch of
[00:15:32.240 --> 00:15:36.800]   competitions, and I tried an approach where I would take a
[00:15:36.800 --> 00:15:39.880]   good public notebook, I would try to rewrite it and try to
[00:15:39.880 --> 00:15:43.520]   understand what I'm doing as I realize it. So rather than
[00:15:43.520 --> 00:15:46.920]   forking, just write it, you know, from scratch, but like
[00:15:46.920 --> 00:15:49.760]   really, really writing what whatever was in the public
[00:15:49.760 --> 00:15:54.560]   notebook. And the idea, the concept I had was first, I will
[00:15:54.560 --> 00:15:58.000]   check, I will learn how to write my own kernel. And second,
[00:15:58.000 --> 00:16:02.240]   when I can read the winning solutions, then I will already
[00:16:02.240 --> 00:16:04.680]   have a stake in the in the game, I want to spend what the
[00:16:04.680 --> 00:16:08.080]   competition is about, and, and I will be able to grab some of
[00:16:08.080 --> 00:16:11.080]   this, some of these ideas. I follow this for a couple of
[00:16:11.080 --> 00:16:14.880]   competitions, but then also, like, I'm not sure if this
[00:16:14.880 --> 00:16:18.880]   works really well. It probably helped me, but it was not
[00:16:18.880 --> 00:16:24.400]   sustainable. And I think another breakthrough was the HPA
[00:16:24.400 --> 00:16:29.960]   competition. So this is where I decided to start super early to
[00:16:29.960 --> 00:16:35.000]   write my own PDA kernel to write my own baseline kernel. And,
[00:16:35.000 --> 00:16:39.040]   and when I when I did it, I really fell in love with the
[00:16:39.040 --> 00:16:42.600]   problem with the data, like all of this microscopic images of
[00:16:42.600 --> 00:16:45.200]   human cells, like that was fascinating. The problem was
[00:16:45.200 --> 00:16:48.480]   really challenging. So it required some out of the box
[00:16:48.480 --> 00:16:53.160]   thinking on how to how to attack it, which was my first team up
[00:16:53.160 --> 00:16:58.000]   experience. So and I only had one team up experience on Kaggle
[00:16:58.000 --> 00:17:04.520]   so far, so I'm still a noob when it comes to teaming. So, and
[00:17:04.520 --> 00:17:07.760]   that's that's helped as well, like it helps to have someone to
[00:17:07.760 --> 00:17:11.840]   talk about like to discuss ideas and to brainstorm together. So
[00:17:11.880 --> 00:17:15.520]   my approach to Kaggle is, is definitely keeps evolving. I
[00:17:15.520 --> 00:17:19.440]   think Chai was was yet another type of experience. And I'm
[00:17:19.440 --> 00:17:22.120]   pretty sure like the next couple of competitions like this
[00:17:22.120 --> 00:17:26.880]   approach will be will be will be changing as well. So, so I guess
[00:17:26.880 --> 00:17:29.680]   applying what you need to be successful with with machine
[00:17:29.680 --> 00:17:32.600]   learning is you need to run a lot of experiments, and learn
[00:17:32.600 --> 00:17:35.680]   from these experiments and try to take this learning and put
[00:17:35.680 --> 00:17:38.280]   them into the next the next experiment. And I think at the
[00:17:38.280 --> 00:17:41.440]   macro level, like you can treat each competition as an
[00:17:41.440 --> 00:17:43.920]   experiment, like try and approach, figure out how to
[00:17:43.920 --> 00:17:47.400]   approach it, and then see like, have you learned something like
[00:17:47.400 --> 00:17:50.200]   what was your success measure? Was it getting a medal? Was it
[00:17:50.200 --> 00:17:52.840]   learning something and then and then try to improve from the
[00:17:52.840 --> 00:17:55.760]   next competition, kind of approach it from a bit
[00:17:55.760 --> 00:17:59.320]   different, different perspective. So, so I think
[00:17:59.320 --> 00:18:03.200]   experimenting with the competition is also an, a good
[00:18:03.200 --> 00:18:03.560]   skill.
[00:18:03.560 --> 00:18:10.800]   I want to really thank the NLP meetup from the, I think this
[00:18:10.800 --> 00:18:15.360]   was from the Polish NLP meetup group. This is a screenshot from
[00:18:15.360 --> 00:18:19.000]   their video. And they you shared three different modes of doing
[00:18:19.000 --> 00:18:23.720]   Kaggle. This is the full mode, quick mode and points mode. I
[00:18:23.720 --> 00:18:27.840]   think I'm still in the quick mode, to some extent. I want to
[00:18:27.840 --> 00:18:32.680]   understand from you as as a fast AI student, you learn about
[00:18:32.680 --> 00:18:35.560]   certain things that work really well and have been designed for
[00:18:35.560 --> 00:18:40.320]   different lectures. And then there's this disconnect of how
[00:18:40.320 --> 00:18:44.480]   do you take that knowledge and apply it to Kaggle? So could you
[00:18:44.480 --> 00:18:47.000]   could you please help me understand that that middle
[00:18:47.000 --> 00:18:50.840]   ground of struggle? How do you keep captive? Maybe it was
[00:18:50.840 --> 00:18:53.160]   really easy for you. But for me, it's really hard. How do I
[00:18:53.160 --> 00:18:56.760]   motivate myself? And how do I improve there? Because I always
[00:18:56.760 --> 00:18:59.280]   find myself cheating and looking at public notebooks and not
[00:18:59.280 --> 00:19:01.320]   being able to design anything by myself.
[00:19:01.320 --> 00:19:05.680]   Yeah, I think it's not easy. I think it's not easy. And I
[00:19:05.680 --> 00:19:12.600]   think what what is important is being getting comfortable with
[00:19:12.600 --> 00:19:18.800]   making mistakes, and ideally in public. So so that was this, I
[00:19:18.800 --> 00:19:23.280]   think was quite recent, there was G2Net competition, which was
[00:19:23.280 --> 00:19:29.960]   about signal processing. And, and I wanted to apply this
[00:19:29.960 --> 00:19:33.720]   approach where I tried to build my own baseline or EDA very
[00:19:33.720 --> 00:19:38.040]   very quickly to this to this competition. And I used my
[00:19:38.040 --> 00:19:41.560]   skills from Fast.ai. In Fast.ai we didn't really dive too much
[00:19:41.560 --> 00:19:45.360]   into signal processing. So I think I just converted the
[00:19:45.360 --> 00:19:49.400]   signal into an image and then I applied to CNN and I made
[00:19:49.400 --> 00:19:52.320]   predictions and they published it. And I thought, okay, let me
[00:19:52.320 --> 00:19:58.120]   just do it. It's probably not the right approach. But it's
[00:19:58.120 --> 00:20:02.800]   not a big, it's not a big, big cost to just try and fail. And
[00:20:02.840 --> 00:20:06.160]   what I learned later is, okay, I took a step back and try to
[00:20:06.160 --> 00:20:09.600]   understand how to do this properly. And you have all of
[00:20:09.600 --> 00:20:12.840]   these concepts like Fourier transforms, and I still don't
[00:20:12.840 --> 00:20:15.520]   know, not sure if I really understand like what this all
[00:20:15.520 --> 00:20:19.680]   means. But I think getting comfortable with with time
[00:20:19.680 --> 00:20:26.520]   things and making mistakes is helpful. Like, and then if you
[00:20:26.520 --> 00:20:30.200]   if you really try, if you really spend a lot of time, you know,
[00:20:30.200 --> 00:20:32.760]   trying to understand the concepts, like I think there are
[00:20:32.760 --> 00:20:36.400]   there are some Fast.ai lectures that I listened like maybe five
[00:20:36.400 --> 00:20:40.040]   to 10 times. So like some of these concepts, like they keep
[00:20:40.040 --> 00:20:44.080]   keep keep coming back to you. I think Fast.ai book is also a
[00:20:44.080 --> 00:20:47.560]   great resource. It definitely helped me a lot in the HPA
[00:20:47.560 --> 00:20:52.400]   competition. Like, there is a chapter on how CNNs work. And
[00:20:52.400 --> 00:20:57.000]   and translating this knowledge about how CNNs work to a
[00:20:57.000 --> 00:21:00.440]   specific problem, like at some point, you get this, you get
[00:21:00.440 --> 00:21:04.640]   this insight, like, well, maybe I can apply this concept that I
[00:21:04.640 --> 00:21:07.680]   remember now to this type of problem and to try it. And
[00:21:07.680 --> 00:21:11.840]   sometimes it works like maybe one, one in 100 times it works
[00:21:11.840 --> 00:21:16.760]   like 99 out of 100, it does not work. But that one time is
[00:21:16.760 --> 00:21:19.160]   worth it. It's worth the 99 tries.
[00:21:19.160 --> 00:21:25.600]   And sorry, sorry to ask so many stupid questions. But I'm just
[00:21:25.600 --> 00:21:28.240]   I'm just trying to understand it maybe for my own struggles.
[00:21:28.240 --> 00:21:31.560]   But when you're trying to find these ideas to implement, it's
[00:21:31.560 --> 00:21:35.160]   again, this challenge of first understanding the people then
[00:21:35.160 --> 00:21:38.760]   being able to implement them and doing it for so many different
[00:21:38.760 --> 00:21:42.480]   ideas across competition. Is there any secret to getting
[00:21:42.480 --> 00:21:46.040]   better at it? To being able to understand people and implement
[00:21:46.040 --> 00:21:46.920]   them fast enough?
[00:21:46.920 --> 00:21:53.720]   I yeah, it's there are a couple of secrets. One is one is
[00:21:54.400 --> 00:21:57.560]   searching through GitHub. Another one is searching through
[00:21:57.560 --> 00:22:02.480]   Stack Overflow. I'm not so I don't consider myself a very
[00:22:02.480 --> 00:22:07.240]   good programmer. Like I, I start like I did some programming when
[00:22:07.240 --> 00:22:10.800]   I was studying many years ago, then for many years, I did not
[00:22:10.800 --> 00:22:14.400]   do any programming. And then I picked up Python when I when I
[00:22:14.400 --> 00:22:17.720]   did all of this, this online courses in machine learning. So
[00:22:17.720 --> 00:22:21.320]   I'm not super good programmer. I don't think I'm really that
[00:22:21.320 --> 00:22:25.720]   good in math. Like, again, I did something at university, then I
[00:22:25.720 --> 00:22:29.160]   did some math on Coursera. So I more or less understand it. But
[00:22:29.160 --> 00:22:35.040]   I still remember this, this famous example from Jeremy about
[00:22:35.040 --> 00:22:38.360]   I think Adam of divisors, like, if there's this, this number of
[00:22:38.360 --> 00:22:41.120]   math equations, but when you look at what what it actually
[00:22:41.120 --> 00:22:44.680]   does, it's, it's just a couple of lines of code, or maybe just
[00:22:44.680 --> 00:22:50.760]   one line of code. So, so what I did, like for for chi, there are
[00:22:50.760 --> 00:22:54.800]   a couple of ideas that I tried that the idea was either coming
[00:22:54.800 --> 00:22:59.040]   from a paper or from a previous competition. I think there were
[00:22:59.040 --> 00:23:02.000]   a bunch of interesting insights from the tweets and to my
[00:23:02.000 --> 00:23:05.000]   competition about different functions and different,
[00:23:05.000 --> 00:23:09.840]   different types of model architecture. And, and I, I
[00:23:09.840 --> 00:23:15.480]   tried to implement some of those. I normally I tried to
[00:23:15.480 --> 00:23:19.320]   timebox it. So let's say I set myself a time for one day for an
[00:23:19.320 --> 00:23:23.760]   experiment. And I tried to do as much as I can and test it. And
[00:23:23.760 --> 00:23:27.640]   if it works, then great. And I spent more time and try to
[00:23:27.640 --> 00:23:31.920]   improve it. If it doesn't work, then, then it depends. Like, in
[00:23:31.920 --> 00:23:34.240]   most of the cases, I will just drop on and move to another
[00:23:34.240 --> 00:23:38.080]   idea. Like sometimes if I really feel like, well, this, this is
[00:23:38.080 --> 00:23:40.840]   this is exciting, I would I would put more effort into it.
[00:23:40.840 --> 00:23:45.760]   But I'm pretty sure some of the one of the reasons some of my
[00:23:45.760 --> 00:23:48.880]   experiments didn't work out is because I didn't properly
[00:23:48.880 --> 00:23:52.760]   implement that. I may get like some out of that. But I think
[00:23:52.760 --> 00:23:56.480]   it's located being okay with mistakes is an important skill.
[00:23:56.480 --> 00:24:03.600]   That's helpful to know. I'll use this answer of yours of talking
[00:24:03.600 --> 00:24:07.440]   about chai competition to transitioning to starting and
[00:24:07.440 --> 00:24:10.440]   talking about the competition. Now, as a reminder to the
[00:24:10.440 --> 00:24:12.760]   audience, please keep the questions coming and I'll keep
[00:24:12.760 --> 00:24:16.120]   asking them during the relevant portions. We'll definitely try
[00:24:16.120 --> 00:24:19.880]   to go through all of them. And as a reminder, here's the link
[00:24:19.880 --> 00:24:27.520]   to find our forums. So now moving on to the competition,
[00:24:27.520 --> 00:24:31.640]   it's called chai Hindi and Tamil question and answering. So in
[00:24:31.640 --> 00:24:35.080]   this section, what I'll try to do is I'll try to understand and
[00:24:35.080 --> 00:24:38.080]   explain the problem and direct please interrupt me and correct
[00:24:38.080 --> 00:24:44.120]   me anywhere if I'm wrong. So as I understood the challenge here
[00:24:44.120 --> 00:24:48.360]   was there are two very prominent languages in India. And there's
[00:24:48.360 --> 00:24:52.880]   this thing we say about India if you travel 50 kilometers in any
[00:24:52.880 --> 00:24:56.320]   direction, the local language, local food and local culture
[00:24:56.320 --> 00:25:00.680]   changes so much. So we do have a lot of languages. But Hindi is
[00:25:00.680 --> 00:25:04.000]   the most spoken out of them. And Tamil is also one of the most
[00:25:04.000 --> 00:25:06.880]   spoken I've lived in Tamil Nadu for a long time. And I think
[00:25:06.880 --> 00:25:10.200]   it's a really difficult language. It was for me to learn
[00:25:10.200 --> 00:25:14.880]   and speak. So I think it's a challenge in itself. And here,
[00:25:14.880 --> 00:25:18.800]   Google Research is trying to create this task where you can
[00:25:18.800 --> 00:25:23.360]   understand and answer questions in both of the languages. So
[00:25:23.360 --> 00:25:26.920]   that broadly was the goal of this competition to try and be
[00:25:26.920 --> 00:25:31.200]   able to interpret this. And this is again, one of the underrated
[00:25:31.200 --> 00:25:35.040]   problems in NLP because these are local languages and most of
[00:25:35.040 --> 00:25:40.000]   the data on the internet is just in English. So so so far, I got
[00:25:40.000 --> 00:25:40.840]   the context for it.
[00:25:40.840 --> 00:25:44.960]   Yeah, yeah, no, no, that's definitely and you definitely
[00:25:44.960 --> 00:25:49.160]   know much, much more about Indian languages than I do. I
[00:25:49.160 --> 00:25:53.080]   think I think one important point is that task itself here
[00:25:53.080 --> 00:25:57.360]   is a pretty straightforward task. Like it's question
[00:25:57.360 --> 00:26:01.640]   answering, where you have some context, the context is usually
[00:26:01.640 --> 00:26:05.080]   a Wikipedia article, then you have a question and you need to
[00:26:05.080 --> 00:26:09.880]   find a fragment of that context that answer the question. So
[00:26:09.880 --> 00:26:13.240]   the challenge is that the task is pretty straightforward. The
[00:26:13.240 --> 00:26:18.440]   challenge here is, is the fact that, well, this is not in
[00:26:18.440 --> 00:26:20.920]   English. And obviously, most of the models that we have
[00:26:20.920 --> 00:26:26.480]   available are in English. And specifically here, the fact that
[00:26:26.480 --> 00:26:30.680]   the training set is very small and noisy. That was one of the
[00:26:30.680 --> 00:26:32.320]   key challenges in this competition.
[00:26:32.320 --> 00:26:37.840]   Absolutely. And, again, just emphasizing on the fact most of
[00:26:37.840 --> 00:26:41.160]   the pre trained models just exist around English. And,
[00:26:41.160 --> 00:26:46.000]   again, it's difficult or an interesting challenge is to be
[00:26:46.000 --> 00:26:49.280]   able to understand this. So continuing further, let's let's
[00:26:49.280 --> 00:26:53.040]   look at the data. As Derek mentioned, it's it was quite a
[00:26:53.040 --> 00:26:56.280]   small data set, I think there were about 700 examples in the
[00:26:56.280 --> 00:27:00.240]   training set for one language and 300 for another language,
[00:27:00.240 --> 00:27:04.360]   which is very, very few compared to what other models have been
[00:27:04.360 --> 00:27:09.600]   trained on. Inside the data set, you could find the context. So
[00:27:09.600 --> 00:27:13.200]   that tells you the text if it's in Hindi or Tamil that those are
[00:27:13.200 --> 00:27:16.640]   the two languages. You can see if the question is in Hindi or
[00:27:16.640 --> 00:27:20.480]   Tamil and what is the answer there? And where does the answer
[00:27:20.480 --> 00:27:24.480]   start? I'll probably suggest you to check out a kernel by Derek
[00:27:24.480 --> 00:27:28.960]   where he has simply explained how to go about this approach.
[00:27:28.960 --> 00:27:32.840]   And there he explains if you have to simply get one answer,
[00:27:32.840 --> 00:27:35.920]   for example, when did something happen and inside of a long
[00:27:35.920 --> 00:27:39.160]   answer of text, it says 42, you just need to be able to create a
[00:27:39.160 --> 00:27:46.120]   model where you can pick that context and share the answer. I
[00:27:46.120 --> 00:27:48.960]   think I've mentioned all of the details. I also have your ADA
[00:27:48.960 --> 00:27:52.600]   kernel open. But that also mentions the same thing. And as
[00:27:52.600 --> 00:27:56.440]   I said earlier, Hindi has about 750 examples. Tamil has about
[00:27:56.440 --> 00:28:03.840]   370 examples in the training data set. And the muted here, I
[00:28:03.840 --> 00:28:07.560]   didn't, I didn't do too much work on the CDA. I started this
[00:28:07.560 --> 00:28:11.840]   EDA, I thought I will dive into the data. But then I noticed
[00:28:11.840 --> 00:28:14.560]   it's, it's, it's like a very straightforward question
[00:28:14.560 --> 00:28:18.080]   answering. So I moved on to the baseline very quickly. I think I
[00:28:18.080 --> 00:28:21.960]   didn't do a good job on the on the EDA here. And there are some
[00:28:21.960 --> 00:28:25.560]   some folks in the on the forums that have spent much more time
[00:28:25.560 --> 00:28:30.160]   and gotten some good insight. Looking at the data,
[00:28:30.160 --> 00:28:34.240]   actually followed the lazy approach where I sorted by
[00:28:34.240 --> 00:28:37.960]   votes for EDA kernel and found your kernel. That's, that's how
[00:28:37.960 --> 00:28:42.800]   I landed on your kernel. But again, quickly pointing the
[00:28:42.800 --> 00:28:46.240]   metric out to the audience is called the Jaccard score. If
[00:28:46.240 --> 00:28:49.000]   you're new to NLP, you can simply go to the forums and
[00:28:49.000 --> 00:28:52.680]   understand it. It's explained in the evaluation sector, but it
[00:28:52.680 --> 00:28:58.880]   basically is the intersection over union metric. So I think
[00:28:58.880 --> 00:29:01.840]   that was what I wanted to explain in the context of the
[00:29:01.840 --> 00:29:04.640]   problem statement. And now I would love to understand how did
[00:29:04.640 --> 00:29:08.080]   you approach the problem and about your solution.
[00:29:08.960 --> 00:29:27.080]   Sure, let me share my screen. Let me know if you can, if you
[00:29:27.080 --> 00:29:31.000]   can see the strategy. Yeah, okay. Let's start with the
[00:29:31.000 --> 00:29:34.320]   first book. So I think we covered some of these points
[00:29:34.320 --> 00:29:38.640]   already. So the fact that we have a small and noisy training
[00:29:38.640 --> 00:29:43.760]   set, that was, that was one of the key features of this
[00:29:43.760 --> 00:29:46.400]   competition. So you need to think like, first, you need,
[00:29:46.400 --> 00:29:50.840]   like, if you have this type of this type of data set, I guess
[00:29:50.840 --> 00:29:54.560]   the follow up question is what about the test set? Because it's
[00:29:54.560 --> 00:29:58.560]   it's okay if your training set is noisy. But if your test set
[00:29:58.560 --> 00:30:02.000]   is noisy, then it becomes a lottery competition where you
[00:30:02.000 --> 00:30:07.800]   can you can play like you're playing lottery. The answer from
[00:30:07.800 --> 00:30:12.080]   the host was that actually the test set was annotated in a
[00:30:12.080 --> 00:30:15.120]   different way from training. So training sets has one way
[00:30:15.120 --> 00:30:18.720]   annotations, which means like each question and context, there
[00:30:18.720 --> 00:30:24.040]   was one person highlighting the answer. But for the for the test
[00:30:24.040 --> 00:30:27.120]   set, there were three people highlighting the answer for a
[00:30:27.120 --> 00:30:30.240]   given question and context. So that means that even if one
[00:30:30.240 --> 00:30:33.480]   person made a mistake, like there were two others, like
[00:30:33.480 --> 00:30:37.680]   four, with a backup. So, so the idea here is that test set
[00:30:37.680 --> 00:30:42.120]   should be more robust than the training set. And that means
[00:30:42.120 --> 00:30:48.640]   also that if you if you try to do CV on cross validation on
[00:30:48.640 --> 00:30:52.160]   your training set, it's very likely to be very noisy, because
[00:30:52.160 --> 00:30:56.600]   you have very little data, and the data is noisy. But if you
[00:30:56.600 --> 00:31:00.280]   look at public leaderboard, it should be more robust. And it
[00:31:00.280 --> 00:31:04.800]   should like public leaderboard and the private leaderboard
[00:31:04.800 --> 00:31:06.720]   should come from the same distribution because they were
[00:31:06.720 --> 00:31:10.320]   annotated in the same way. So that was a big, big leap of
[00:31:10.320 --> 00:31:14.600]   faith. Okay, can I can I trust the public leaderboard? If I
[00:31:14.600 --> 00:31:17.440]   can, then that means that I can participate and put all of this
[00:31:17.440 --> 00:31:23.440]   effort into the experiment. If, if that is not, if that is not
[00:31:23.440 --> 00:31:26.320]   a good measure of performance, then that means like you can do
[00:31:26.320 --> 00:31:30.200]   this hundreds of experiments and spend many hours, but at the end
[00:31:30.200 --> 00:31:34.120]   you will be shaken down significantly. So, so that was a
[00:31:34.120 --> 00:31:37.240]   bit of a leap of faith. But I thought like my intuition was
[00:31:37.240 --> 00:31:40.560]   like, yes, I should be able to trust the public leaderboard.
[00:31:40.560 --> 00:31:45.720]   And, and that's how I designed all of my experiments. I, I used
[00:31:45.720 --> 00:31:49.440]   only publicly that was for validation. I did not look at my
[00:31:49.440 --> 00:31:51.120]   at my local CV at all.
[00:31:51.120 --> 00:31:55.360]   Sanyam Bhutani: I'm sorry, sorry to interrupt. When you say noisy
[00:31:55.360 --> 00:31:58.240]   training set, do you mean the labels were sometimes incorrect?
[00:31:58.240 --> 00:32:00.080]   Or what does noisy here refer to?
[00:32:00.760 --> 00:32:03.800]   Stefano Cossu: Yeah, yeah, yeah, the labels have been sometimes
[00:32:03.800 --> 00:32:07.560]   incorrect, or there were many potential answers that you could
[00:32:07.560 --> 00:32:10.920]   pick. I think there was one feature, well, there was one
[00:32:10.920 --> 00:32:15.480]   feature that that some teams exploited, I did not exploit
[00:32:15.480 --> 00:32:19.800]   that feature where the annotators, they picked the
[00:32:19.800 --> 00:32:26.360]   answer from the context. But then they, like sometimes in
[00:32:26.400 --> 00:32:30.600]   some somewhere in this data processing step, the place from
[00:32:30.600 --> 00:32:32.880]   where they think the answer was dropped, like it was not there
[00:32:32.880 --> 00:32:37.480]   in the data. So what the host did is they automatically found
[00:32:37.480 --> 00:32:41.360]   like this first, the first string in the context that match
[00:32:41.360 --> 00:32:45.240]   the answer. And they said, this is the, this is the location of
[00:32:45.240 --> 00:32:48.480]   the answer. But in some of the context, an answer might have
[00:32:48.480 --> 00:32:52.520]   appeared like 10 or 15 times. So maybe the fact that a certain
[00:32:52.520 --> 00:32:55.880]   word appears at the very beginning of the text does not
[00:32:55.880 --> 00:32:58.240]   mean that it's the place where you should look for the answer,
[00:32:58.240 --> 00:33:01.200]   maybe the answer should be somewhere, somewhere later on in
[00:33:01.200 --> 00:33:04.360]   that context. So some of the teams have done a correction
[00:33:04.360 --> 00:33:08.520]   there and pick the proper, the proper location of the answer in
[00:33:08.520 --> 00:33:14.160]   the training sets. I actually missed that opportunity. So, so
[00:33:14.160 --> 00:33:17.560]   kudos to the to the folks that actually did that connection.
[00:33:17.560 --> 00:33:24.040]   The, the obvious idea, given this is noisy training set was
[00:33:24.040 --> 00:33:29.520]   well, we should use external data. And that was my initial
[00:33:29.520 --> 00:33:34.400]   thinking, like if, if somebody spends time labeling data,
[00:33:34.400 --> 00:33:38.800]   taking, like, for example, you know, you can consider this as a
[00:33:38.800 --> 00:33:42.080]   as a leisure exercise, like you read a Wikipedia article, you
[00:33:42.080 --> 00:33:45.400]   write questions, you pick the answer. And then if you really
[00:33:45.400 --> 00:33:49.520]   put effort into that, you could build a pretty sizable data set
[00:33:49.520 --> 00:33:53.520]   and use it for training and hopefully getting a good score.
[00:33:54.000 --> 00:33:57.720]   And I think that was the idea from the host as well, because
[00:33:57.720 --> 00:34:02.120]   they, they wanted to use it as a chance to collect data sets in
[00:34:02.120 --> 00:34:06.760]   those languages that can be used by the by the community. I think
[00:34:06.760 --> 00:34:09.840]   at the end, actually, people did not do that. It seems like this
[00:34:09.840 --> 00:34:13.000]   was a much harder work than just experimenting with the models.
[00:34:13.000 --> 00:34:21.520]   But, but I guess one of the one of the insights was we have data
[00:34:21.560 --> 00:34:26.040]   we have, we don't have much data for for Tamil, actually, there
[00:34:26.040 --> 00:34:30.120]   was some data for Hindi, some existing data sets from Hindi
[00:34:30.120 --> 00:34:33.800]   that I think everyone used. This was translated data sets, but
[00:34:33.800 --> 00:34:36.400]   they were manually translated. So the quality should be good.
[00:34:36.400 --> 00:34:42.200]   And, but that was also data in in similar languages. So there
[00:34:42.200 --> 00:34:46.400]   is this big data set that is called tidy, that contains
[00:34:46.880 --> 00:34:52.520]   question answering data in in various languages, including
[00:34:52.520 --> 00:34:56.880]   Bengali and Telugu. And from from what I googled, I think
[00:34:56.880 --> 00:35:00.400]   Bengali should be from the same language family as Hindi, and
[00:35:00.400 --> 00:35:04.800]   Telugu should be from the same language family as Tamil. So
[00:35:04.800 --> 00:35:08.760]   using these languages along with English, like that, that was the
[00:35:08.760 --> 00:35:11.520]   external data that was really, really helpful in this in this
[00:35:11.520 --> 00:35:15.840]   competition. So it made up for the fact that we have little
[00:35:15.880 --> 00:35:22.240]   Tamil and Hindi, Hindi data. And the other thing that was pretty,
[00:35:22.240 --> 00:35:27.960]   pretty, like, like, that, that was at least my insight at the
[00:35:27.960 --> 00:35:30.320]   beginning of the competition, because we have this novelty and
[00:35:30.320 --> 00:35:34.440]   small training set, we, we should try to have a diverse
[00:35:34.440 --> 00:35:39.760]   ensemble, because that that should kind of like, prevent
[00:35:39.760 --> 00:35:42.960]   prevent the risk of overfitting in one or one or the other
[00:35:42.960 --> 00:35:45.840]   direction that it should have with the with having a good
[00:35:45.840 --> 00:35:50.560]   score on the on the private leaderboard. It, it turned out
[00:35:50.560 --> 00:35:54.080]   in the end that if somebody bets properly, and by betting
[00:35:54.080 --> 00:35:57.520]   properly, I mean, betting on Mouril backbone, which was,
[00:35:57.520 --> 00:36:02.280]   which was kind of like an Indic language, but model, then that
[00:36:02.280 --> 00:36:05.600]   might have been a very good choice. So, so Mouril turned out
[00:36:05.600 --> 00:36:08.280]   to be to be good, but it was very hard to know it in advance.
[00:36:08.280 --> 00:36:12.640]   So, so I think diverse ensemble was was very helpful here as
[00:36:12.640 --> 00:36:17.400]   well. So, these were like early insights from from the
[00:36:17.400 --> 00:36:23.640]   competition. So, my strategy on how to approach it was, so
[00:36:23.640 --> 00:36:27.280]   first, I started pretty early, then I took a break, like, I
[00:36:27.280 --> 00:36:31.920]   think this was like a three month competition. And I, I took
[00:36:31.920 --> 00:36:36.400]   a break for like, maybe two, two months. And then when I when I
[00:36:36.400 --> 00:36:39.280]   joined in the last, I think, more or less month of the
[00:36:39.280 --> 00:36:43.760]   competition, then I said, I am taking it really seriously. And
[00:36:43.760 --> 00:36:47.600]   I want to put effort and try to do it as good as I can, as well
[00:36:47.600 --> 00:36:52.760]   as I can. And I try to put most the most effort into training
[00:36:52.760 --> 00:36:59.000]   good single models. My idea was that, and I'll talk about it in
[00:36:59.000 --> 00:37:01.920]   a in a moment, like there was this, like most people use this
[00:37:01.920 --> 00:37:05.560]   two stage approach where there was a stage of training on, for
[00:37:05.560 --> 00:37:11.240]   example, squat, and then training on on the chi data. So
[00:37:11.240 --> 00:37:14.280]   I would, I would spend some time training this, this first level
[00:37:14.280 --> 00:37:17.680]   models and the second level model, and then testing
[00:37:17.680 --> 00:37:20.120]   different backbones, there were a couple of backbones that were
[00:37:20.120 --> 00:37:25.000]   available. I had this idea that I had in my head, since you
[00:37:25.000 --> 00:37:28.720]   asked about it, like, how do you take fast AI ideas and, and
[00:37:28.720 --> 00:37:32.000]   implement them. So, so the idea of using this image like
[00:37:32.000 --> 00:37:34.760]   augmentations, I tried it already a couple of times in
[00:37:34.760 --> 00:37:39.520]   different competitions before it sometimes worked okay, but it
[00:37:39.520 --> 00:37:42.480]   never worked super well. So I thought, okay, maybe this time
[00:37:42.480 --> 00:37:47.760]   it will work. So I'm going to try it again. Try to study the
[00:37:47.760 --> 00:37:51.000]   previous competition, TensorFlow, QA and input
[00:37:51.000 --> 00:37:56.080]   sentiment were the most similar ones. Try to look at papers for
[00:37:56.080 --> 00:38:01.520]   for what are the results and approaches. And what I found
[00:38:01.520 --> 00:38:05.480]   super useful was actually checking, like we had different
[00:38:05.480 --> 00:38:08.160]   backbones here, different different models like XLM,
[00:38:08.160 --> 00:38:13.320]   Roberta, Muriel, Rembert. And each of these models requires a
[00:38:13.320 --> 00:38:17.000]   bit different fine tuning protocol, and checking in the
[00:38:17.000 --> 00:38:20.840]   papers, like what is the way that the authors of this model
[00:38:20.840 --> 00:38:24.320]   fine tune them, it was it was super helpful, because it
[00:38:24.320 --> 00:38:27.840]   speeded up the like, you didn't need so many experiments with
[00:38:27.920 --> 00:38:31.640]   with any of these models, if you just followed the, the recipes
[00:38:31.640 --> 00:38:35.320]   from the paper. And then I think in the last couple of days, I,
[00:38:35.320 --> 00:38:39.480]   that was the time that I had for for for ensembling. I think this
[00:38:39.480 --> 00:38:42.440]   is the approach like that also got from one of your interviews
[00:38:42.440 --> 00:38:45.600]   with with Peter, like spent a lot of time, you know, trying to
[00:38:45.600 --> 00:38:49.520]   very quickly experiment with with simple models. And then at
[00:38:49.520 --> 00:38:52.800]   the end, just just just try to do to do ensemble.
[00:38:56.120 --> 00:39:01.800]   Okay, so, so moving on, this is the, this was a typical
[00:39:01.800 --> 00:39:07.400]   approach, there is this, there was a very, very popular public
[00:39:07.400 --> 00:39:12.200]   notebook. And that that used this approach that got a very
[00:39:12.200 --> 00:39:16.080]   good score on the on the public leaderboard, I think by by
[00:39:16.080 --> 00:39:21.960]   Kosal. And the approach was like, first you take, you take
[00:39:21.960 --> 00:39:25.280]   the model that like, we obviously we all we used, we
[00:39:25.280 --> 00:39:27.440]   pre trained models here, the models were pre trained on
[00:39:27.440 --> 00:39:31.960]   mask language modeling. And we use multilingual models, that
[00:39:31.960 --> 00:39:35.000]   were some, I think, single language models, I think there
[00:39:35.000 --> 00:39:38.720]   was one team that actually trained, like transferred
[00:39:38.720 --> 00:39:45.000]   Roberta into Hindi, and Tamil, I thought that most of the teams
[00:39:45.000 --> 00:39:49.320]   use multi multi language models. And I thought that was also
[00:39:49.320 --> 00:39:51.680]   helpful, because then you can use all of this data that you
[00:39:51.680 --> 00:39:54.840]   have in other languages for training. So, so most of the
[00:39:54.840 --> 00:40:00.480]   teams use this multilingual models. And that was a very
[00:40:00.480 --> 00:40:03.880]   early discovery by by Nicholas Broad, that he shared on the
[00:40:03.880 --> 00:40:08.800]   forum that if you take XLM Roberta, that was pre trained on
[00:40:08.800 --> 00:40:13.080]   English squad by by a company called deep sets and a huge
[00:40:13.080 --> 00:40:17.480]   shout out to deep sets, they are, they are, they doing a lot
[00:40:17.480 --> 00:40:21.320]   of good work in question answering, and offering some,
[00:40:21.320 --> 00:40:24.760]   some frameworks for that. So if you're into question answering
[00:40:24.760 --> 00:40:27.960]   checkout, checkout deep sets, so they train this model, this
[00:40:27.960 --> 00:40:31.640]   multilingual XLM Roberta model on English squad, and just
[00:40:31.640 --> 00:40:35.720]   taking that photo, and, and applying it in this competition,
[00:40:35.720 --> 00:40:39.560]   I think it's exceeded the score of the baseline prepared by the
[00:40:39.560 --> 00:40:45.960]   host. So, so, so that's that showed the power of using model
[00:40:45.960 --> 00:40:48.840]   trained on a different language and applying it to Hindi and
[00:40:48.840 --> 00:40:53.360]   Tamil. But then if you took this model, and you trained it on
[00:40:53.360 --> 00:40:56.600]   chi, and this additional data that that we have, so we had
[00:40:56.600 --> 00:41:00.640]   some substrates of MLQA and X squad, which was a question
[00:41:00.640 --> 00:41:04.440]   answering data set in Hindi. And then some people also translated
[00:41:04.440 --> 00:41:08.720]   squat into English or into Hindi or Tamil. So if you fine tune
[00:41:08.720 --> 00:41:12.680]   that that model on those data sets, then that led to really
[00:41:12.680 --> 00:41:17.480]   good results from the public leaderboard. Now, there were
[00:41:17.480 --> 00:41:20.160]   there, there were two opportunities with this
[00:41:20.160 --> 00:41:25.280]   approach that, that I recognized a bit later, but, but that
[00:41:25.280 --> 00:41:29.000]   ultimately helped me. So the first opportunity is, if you
[00:41:29.000 --> 00:41:32.560]   follow this approach, then between these two stages, you
[00:41:32.560 --> 00:41:36.600]   reset your optimizer state and, and the learning rate schedule.
[00:41:36.600 --> 00:41:40.840]   And the way I'm thinking about this optimizer statement and
[00:41:40.840 --> 00:41:43.680]   schedule, like if you have all of this, this big loss
[00:41:43.680 --> 00:41:47.400]   landscape, and then you have a ball that is rolling down those
[00:41:47.400 --> 00:41:51.280]   hills of the loss landscape, and it has certain speed and
[00:41:51.280 --> 00:41:54.160]   momentum, and it's trying to find your loss. But if you just
[00:41:54.160 --> 00:41:57.640]   reset it, and you put it into a random, into a random, well,
[00:41:57.640 --> 00:42:02.160]   like whatever it ended up, and then you try to try to train
[00:42:02.160 --> 00:42:05.200]   again from scratch, then you're losing all of that, all of that
[00:42:05.200 --> 00:42:08.520]   momentum that you gather in the early phase of your of your
[00:42:08.520 --> 00:42:13.000]   training. So, so, so maybe you can do better by by combining
[00:42:13.000 --> 00:42:17.360]   this, this stages. And the second, the second thing that
[00:42:17.360 --> 00:42:20.920]   was an opportunity here is this, this time training data set is
[00:42:20.920 --> 00:42:24.800]   pretty small. And what most people did is they, they prepare
[00:42:24.800 --> 00:42:31.120]   the data. So they, they, they prepare the features by, by
[00:42:31.120 --> 00:42:35.200]   sliding the window over the context and, and then tokenizing
[00:42:35.200 --> 00:42:39.800]   that and, and connecting that with these questions. So there
[00:42:39.800 --> 00:42:43.160]   is a fixed training set, and then you train it for a couple
[00:42:43.160 --> 00:42:46.320]   of epochs. And every epoch, like you just shuffle the data, but
[00:42:46.320 --> 00:42:48.880]   the data is always the same. So there is a risk of overfitting.
[00:42:48.880 --> 00:42:53.120]   So, so there are a couple of places where we can do better
[00:42:53.120 --> 00:42:58.960]   with this approach. And, and one of the insights that I got it
[00:42:58.960 --> 00:43:04.800]   was from an experiment where I, I moved, and I still, I still
[00:43:04.800 --> 00:43:08.320]   try to follow this two stage approach. But I thought like,
[00:43:08.320 --> 00:43:12.160]   maybe I can, I can take this, this handy data into the
[00:43:12.160 --> 00:43:15.480]   pre-training stage along with English squats. And maybe I can
[00:43:15.480 --> 00:43:18.400]   also use tidy for pre-training. So this is what we call this
[00:43:18.400 --> 00:43:18.920]   English.
[00:43:18.920 --> 00:43:25.720]   Sorry, I wanted to interrupt you and confirm. So when you take a
[00:43:25.720 --> 00:43:30.080]   model for the first phase here, is it already like does it have
[00:43:30.080 --> 00:43:33.760]   weights that have been trained on an English data set? Or are
[00:43:33.760 --> 00:43:37.240]   you just training them for the first time on these data sets?
[00:43:38.080 --> 00:43:40.880]   Yeah, good, good question. So this is an already like a pre
[00:43:40.880 --> 00:43:44.760]   trained model. And an XLM Roberta would be pre trained on
[00:43:44.760 --> 00:43:47.800]   mask language modeling. So you have this, this task where you
[00:43:47.800 --> 00:43:51.640]   are you just trying to predict what's in the text. And the
[00:43:51.640 --> 00:43:54.640]   multilingual Roberta would be pre trained on many languages,
[00:43:54.640 --> 00:43:57.040]   which would already be pre trained with mask language
[00:43:57.040 --> 00:44:00.480]   modeling, Hindi, Tamil, English, Bengali, Tadugu, all of those
[00:44:00.480 --> 00:44:05.280]   languages. But it will not be pre trained specifically on QA
[00:44:05.280 --> 00:44:08.440]   task. So I think this pre training and fine tuning, I'm
[00:44:08.440 --> 00:44:12.720]   probably using the terms in the wrong way. But it's kind of like
[00:44:12.720 --> 00:44:15.520]   a two stage approach where you train the model on some data
[00:44:15.520 --> 00:44:19.080]   first, and then you take that checkpoint and you train it
[00:44:19.080 --> 00:44:22.400]   again on on another set of data. So in this
[00:44:22.400 --> 00:44:25.840]   again, sorry for another silly question. But when you take this
[00:44:25.840 --> 00:44:30.120]   XLM Roberta backbone, is this a Q&A model already? Or did you
[00:44:30.120 --> 00:44:33.680]   modify the heads to make it understand and interpret
[00:44:33.680 --> 00:44:34.760]   questions and answers?
[00:44:35.320 --> 00:44:39.360]   Yeah, yeah. So so good question. And hugging face, fortunately
[00:44:39.360 --> 00:44:42.680]   makes it super easy. There is a class that is called Auto model
[00:44:42.680 --> 00:44:47.000]   for question answering, where you say I want an auto model for
[00:44:47.000 --> 00:44:51.520]   question answering from pre trained XLM Roberta. And it's it
[00:44:51.520 --> 00:44:55.760]   takes this pre trained backbone and it puts a very simple head
[00:44:55.760 --> 00:44:59.240]   on top of it, where you're trying to predict the start
[00:44:59.240 --> 00:45:02.600]   loges and the end loges of the answer. So you're looking at
[00:45:02.640 --> 00:45:05.320]   into the context and you're trying to predict like what is
[00:45:05.320 --> 00:45:09.200]   the start of the answer and what's the end of the answer in
[00:45:09.200 --> 00:45:13.320]   that context. So it's a very standard architecture. It's like
[00:45:13.320 --> 00:45:17.440]   like it's it's an auto kind of model in hugging face. So this
[00:45:17.440 --> 00:45:21.760]   this head specifically is like you started from scratch, but
[00:45:21.760 --> 00:45:25.120]   all of the backbone the model is pre trained on Mark-Language
[00:45:25.120 --> 00:45:25.640]   modeling.
[00:45:25.640 --> 00:45:28.280]   Okay, thank you.
[00:45:28.280 --> 00:45:32.400]   So so you can put all of this different data into the
[00:45:32.400 --> 00:45:35.200]   pre-training stage, except for child, right? So you can, you
[00:45:35.200 --> 00:45:38.000]   can, you can put this first stage of training from tidy,
[00:45:38.000 --> 00:45:41.080]   different languages from the tidy data sets, you can put
[00:45:41.080 --> 00:45:46.280]   English there, Hindi, MQA and XBLA. And, and then like you
[00:45:46.280 --> 00:45:49.720]   train this, you train this model. So you you will buy this,
[00:45:49.720 --> 00:45:52.920]   like after you finish this phase one, you have a pretty good
[00:45:52.920 --> 00:45:56.880]   question answering model, but not specifically trained or
[00:45:56.880 --> 00:46:01.480]   Tamil or Hindi except for this, maybe this translated data set.
[00:46:02.000 --> 00:46:04.920]   So the idea was then okay, so let's take that model and
[00:46:04.920 --> 00:46:11.080]   fine-tune it on on child. And, and the insight here was, I did
[00:46:11.080 --> 00:46:14.800]   this for a couple of experiments. And, and the
[00:46:14.800 --> 00:46:18.680]   performance of this phase one model was actually better than
[00:46:18.680 --> 00:46:21.640]   than the performance of this phase two model, which included
[00:46:21.640 --> 00:46:27.160]   a fine tuning. So, so, so there was, like either the data was
[00:46:27.160 --> 00:46:32.680]   actually so much noisy that it resulted in, in that signal for
[00:46:32.680 --> 00:46:37.480]   the model, or maybe like this data was actually so, so good
[00:46:37.480 --> 00:46:41.360]   that, that, that, that after restarting, like all of this
[00:46:41.360 --> 00:46:45.720]   optimizer state and all of that stuff, like, I, like my model
[00:46:45.720 --> 00:46:49.760]   went into the wrong direction, right? So, so, so looking at
[00:46:49.760 --> 00:46:53.800]   this, I thought, okay, maybe I skipped this phase two phase
[00:46:53.800 --> 00:46:57.960]   approach, and I just, I will just do one phase. So this is,
[00:46:57.960 --> 00:47:00.760]   this is the single stage approach where I don't, I don't,
[00:47:00.760 --> 00:47:03.680]   I don't split my training into the phases, I just have one
[00:47:03.680 --> 00:47:08.160]   single stage training where I preserve the optimizer state,
[00:47:08.160 --> 00:47:13.000]   the learning rate schedule, but I still want to, I still want to
[00:47:13.000 --> 00:47:17.520]   control the order in, into how I put different data into the
[00:47:17.520 --> 00:47:20.480]   model. So I still want to start like put squat into the
[00:47:20.480 --> 00:47:23.840]   beginning of the training. And the idea is squat is a pretty
[00:47:23.840 --> 00:47:28.240]   simple data set. And it's pretty straightforward. So it should be
[00:47:28.240 --> 00:47:30.960]   easy for the model to start learning question answering
[00:47:30.960 --> 00:47:34.720]   called squat, and then, and then move to more difficult tasks,
[00:47:34.720 --> 00:47:40.240]   kind of as it gets knowledge during the training. So, so, so
[00:47:40.240 --> 00:47:43.280]   I wanted to control this order. So this is what I what I call
[00:47:43.280 --> 00:47:47.480]   data recipe, where other than, you know, taking some data, and
[00:47:47.480 --> 00:47:51.120]   then putting this into like into a data set, and then putting
[00:47:51.120 --> 00:47:53.640]   this into a training group. And what typically happens in a
[00:47:53.640 --> 00:47:57.160]   training group is you shuffle the data. So it comes in a random
[00:47:57.160 --> 00:48:01.200]   order, and then, and then you train it for for x number of
[00:48:01.200 --> 00:48:05.600]   epochs. So I thought I want to, I want to train for one epoch,
[00:48:05.600 --> 00:48:08.600]   but I want to be very deliberate when I put what type of data
[00:48:08.600 --> 00:48:11.880]   into into my into into the training.
[00:48:13.560 --> 00:48:17.920]   There's a question. There's a question from the audience. And
[00:48:17.920 --> 00:48:20.800]   this is by money. See, one is and I'm sure you would know him
[00:48:20.800 --> 00:48:24.760]   through the fast a community. Yeah, so the question. So he
[00:48:24.760 --> 00:48:28.600]   asked, what was the tokenization approach in the earlier two
[00:48:28.600 --> 00:48:31.720]   stage approach and in this approach as well? Did you take a
[00:48:31.720 --> 00:48:33.760]   special tokenization approach or?
[00:48:33.760 --> 00:48:39.320]   Good question. So the conversation approach was the
[00:48:39.320 --> 00:48:45.240]   same. There was so so and I actually have a notebook where
[00:48:45.240 --> 00:48:49.560]   I where I will show some of this stuff in code in a moment, that
[00:48:49.560 --> 00:48:54.320]   the tokenization work in the same way. And what I actually
[00:48:54.320 --> 00:48:59.680]   tried to change here is the augmentation and I changed
[00:48:59.680 --> 00:49:05.400]   augmentation by running the same tokenizer on the same data a
[00:49:05.400 --> 00:49:09.160]   couple of times to get different different windows into
[00:49:09.160 --> 00:49:15.280]   into my text. And hopefully I will explain this in a in a
[00:49:15.280 --> 00:49:19.920]   moment. But yeah, it may be money if you if you if you can
[00:49:19.920 --> 00:49:22.480]   make the question a bit more specific than I can answer it.
[00:49:22.480 --> 00:49:27.360]   But generally, the tokenization approach was the same in both
[00:49:27.360 --> 00:49:29.880]   in both of these approaches. So that that element did not
[00:49:29.880 --> 00:49:30.400]   change.
[00:49:30.400 --> 00:49:35.040]   So you were talking about the data recipes?
[00:49:35.200 --> 00:49:39.720]   Yeah, so so so so I put the I put the data in a specific order
[00:49:39.720 --> 00:49:43.520]   that's trying to figure out how to do that was was actually took
[00:49:43.520 --> 00:49:47.160]   me some time but but again hugging face like him with a
[00:49:47.160 --> 00:49:50.640]   really nice they have this very nice transformers library, but
[00:49:50.640 --> 00:49:53.640]   they also have a very awesome data sets library and this data
[00:49:53.640 --> 00:49:58.760]   set library was super useful here. So so what so what I what
[00:49:58.760 --> 00:50:05.040]   I do is I I take this tidy data I add squats I add this and then
[00:50:05.040 --> 00:50:08.440]   I add both Hindi and Chai in the very beginning of my training,
[00:50:08.440 --> 00:50:13.600]   then I take the same data I take out squats because squat is is
[00:50:13.600 --> 00:50:16.760]   easy. So I just wanted to use it only in the beginning of the
[00:50:16.760 --> 00:50:21.240]   training. And I replace squats with natural questions. And the
[00:50:21.240 --> 00:50:23.880]   idea is that natural questions should be much more difficult.
[00:50:23.880 --> 00:50:28.640]   But natural questions is actually very different from
[00:50:28.640 --> 00:50:32.280]   Chai because like Chai is questions asked by annotators
[00:50:32.320 --> 00:50:36.000]   natural questions like all of the random questions from the
[00:50:36.000 --> 00:50:40.200]   internet. So I only took a subset of natural questions and I
[00:50:40.200 --> 00:50:42.960]   tried to build a model that will predict which of natural
[00:50:42.960 --> 00:50:47.880]   questions are similar to tidy or Chai and only take those similar
[00:50:47.880 --> 00:50:51.600]   similar questions there. So so I put natural questions into the
[00:50:51.600 --> 00:50:54.600]   next stage and then I finish it off with with with just Chai
[00:50:54.600 --> 00:50:57.720]   data in some experiments later on I figured that this finishing
[00:50:57.720 --> 00:51:02.040]   off with Chai is not necessarily super helpful. But then there is
[00:51:02.040 --> 00:51:05.440]   still some data that is repeated here. So I put tidy in the
[00:51:05.440 --> 00:51:08.760]   beginning, then I put tidy into the next stage. And then I have
[00:51:08.760 --> 00:51:11.760]   Chai here and I have Chai a bit later. So there is this risk
[00:51:11.760 --> 00:51:16.320]   that my my model will see the same example a couple of times.
[00:51:16.320 --> 00:51:20.560]   So in order to prevent that, I use this augmentation
[00:51:20.560 --> 00:51:24.360]   techniques. So so I tried three types, three types of
[00:51:24.360 --> 00:51:27.520]   augmentation. The first one is random cropping. The second one
[00:51:27.520 --> 00:51:30.840]   is progressive resizing and third one is cut out and or
[00:51:30.840 --> 00:51:34.680]   random masking and they all come from from image augmentations.
[00:51:34.680 --> 00:51:38.760]   And that's, that's an inspiration from from fast AI. I
[00:51:38.760 --> 00:51:43.680]   think in in one of the lectures, when Jeremy talked about his
[00:51:43.680 --> 00:51:46.920]   work on ULM fit, and how he took this concept of pre training a
[00:51:46.920 --> 00:51:50.800]   model on image net and apply this text with with NLP. He
[00:51:50.800 --> 00:51:53.360]   mentioned that are, I think he mentioned that's what I
[00:51:53.360 --> 00:51:57.120]   remember, that are many other concepts from from computer
[00:51:57.120 --> 00:52:00.880]   vision that you can apply to NLP. So so I thought maybe
[00:52:00.880 --> 00:52:07.360]   augmentations could be could be the thing. And, and the way, so
[00:52:07.360 --> 00:52:09.600]   so the way I'm thinking about this augmentations and how to
[00:52:09.600 --> 00:52:14.160]   translate them. So random crops, like on images, it's pretty
[00:52:14.160 --> 00:52:18.200]   straightforward, you just take a crop from an image. And on text
[00:52:18.200 --> 00:52:21.600]   is it's also pretty straightforward. So you in
[00:52:21.600 --> 00:52:24.480]   question answering, you have this long context document, and
[00:52:24.480 --> 00:52:27.840]   then somewhere in the document, we have the answer. And then
[00:52:27.840 --> 00:52:31.240]   typically go with a sliding window through that text with a
[00:52:31.240 --> 00:52:35.960]   static window of a certain size, like 256 tokens, for example,
[00:52:35.960 --> 00:52:39.440]   with a certain strides. And then it means that maybe you pick up
[00:52:39.440 --> 00:52:43.400]   two or three examples with with this answer, but this example
[00:52:43.400 --> 00:52:47.160]   will always be the same. But rather than doing that, you can
[00:52:47.160 --> 00:52:51.160]   like take every time a different and different crop from that
[00:52:51.200 --> 00:52:55.960]   from that text. So I think doing this already add some some good
[00:52:55.960 --> 00:52:59.640]   diversity to your model. So you don't you never have to similar
[00:52:59.640 --> 00:53:03.480]   like the same example twice in your in your data always you try
[00:53:03.480 --> 00:53:08.240]   to put some variation there. Another example, another
[00:53:08.240 --> 00:53:10.920]   augmentation that will happen computer vision is cut out, where
[00:53:10.920 --> 00:53:14.680]   you just introduce some noise to your data by by cutting out
[00:53:14.680 --> 00:53:18.720]   pieces of your image. And you can do the same with text by
[00:53:18.760 --> 00:53:24.000]   randomly masking some, some some some words. And this, I think
[00:53:24.000 --> 00:53:27.080]   this was also used in Twitch sentiment by Chris de Oudt. So
[00:53:27.080 --> 00:53:31.640]   big shout out. I also learned a lot from from Chris, including,
[00:53:31.640 --> 00:53:35.000]   including the interviews that you've done. So thanks for
[00:53:35.000 --> 00:53:40.680]   those. And then, and then there is this this fast AI idea of
[00:53:40.680 --> 00:53:43.160]   progressive image sizing, where you start training on small
[00:53:43.160 --> 00:53:46.960]   images, and then you increase the image size as you keep
[00:53:46.960 --> 00:53:51.040]   training. And you can do the same with text because you can,
[00:53:51.040 --> 00:53:54.480]   you can start training, for example, with sequence like of
[00:53:54.480 --> 00:53:58.560]   256 tokens, and then you can increase it to 384. And then you
[00:53:58.560 --> 00:54:03.280]   can increase it to, for example, 512. And if somebody pointed out
[00:54:03.280 --> 00:54:06.040]   on Twitter that this was actually used when pre training
[00:54:06.040 --> 00:54:11.960]   BERT, so BERT was pre trained, I think on 128 tokens, and then at
[00:54:11.960 --> 00:54:17.240]   the end with 512. So it's not entirely new on NLP. I think the
[00:54:17.240 --> 00:54:20.960]   nice trick is doing this, like without actually resetting the
[00:54:20.960 --> 00:54:25.240]   state of your optimizer, you just, you just sequentially like
[00:54:25.240 --> 00:54:29.080]   as you as you as you go on with training, you, you increase the
[00:54:29.080 --> 00:54:33.160]   length of your of your sequences. So using this
[00:54:33.160 --> 00:54:36.120]   pre-augmentation techniques means that you actually, we have
[00:54:36.120 --> 00:54:39.920]   a lot of data because we took all of this external data. And
[00:54:39.960 --> 00:54:42.480]   even when we are repeating examples from the same data set,
[00:54:42.480 --> 00:54:46.040]   we use a different, a different variation of those examples
[00:54:46.040 --> 00:54:51.320]   every time. So it introduces variety to the model and, and
[00:54:51.320 --> 00:54:53.320]   hopefully it helps with with learning.
[00:54:53.320 --> 00:54:57.640]   Sanyam Bhutani: This was so incredible. I was also telling
[00:54:57.640 --> 00:55:00.600]   you before right before our interview as well, I was really,
[00:55:00.600 --> 00:55:04.280]   really mind blown by how you decided to take this approach
[00:55:04.280 --> 00:55:06.800]   and compare the image augmentations with NLP
[00:55:06.800 --> 00:55:10.040]   augmentations. People in the chat are also absolutely
[00:55:10.040 --> 00:55:12.680]   appreciating how creative this approach was.
[00:55:12.680 --> 00:55:17.120]   Dr. Sebastian Raschka: That's awesome. So, so one of the
[00:55:17.120 --> 00:55:20.720]   ideas now and I think I published it on on both the fast
[00:55:20.720 --> 00:55:24.160]   AI and the hanging phase discord servers is to do some ablation
[00:55:24.160 --> 00:55:27.560]   studies because like sometimes you come up with this ideas and
[00:55:27.560 --> 00:55:30.720]   you really, really want them to work. I think it's also a
[00:55:30.720 --> 00:55:33.520]   feature of research, right? You have this idea and you really
[00:55:33.520 --> 00:55:36.840]   want to make it to make it work. So I think I've done so many
[00:55:36.840 --> 00:55:39.120]   experiments with this augmentation that at the end,
[00:55:39.120 --> 00:55:43.200]   I'm not really sure if they were so helpful. Maybe if you if you
[00:55:43.200 --> 00:55:45.800]   don't do them, like the model was still trained pretty well.
[00:55:45.800 --> 00:55:50.800]   But it's fun to try them out. I think now is the place where I
[00:55:50.800 --> 00:55:54.120]   like to do some ablation study and see like which of them are
[00:55:54.120 --> 00:55:57.320]   actually helpful. And it might depend also on the size of the
[00:55:57.320 --> 00:56:01.720]   data that you have and the type of the problem. But it's, it's
[00:56:01.720 --> 00:56:04.800]   always fun to try something different and see if it's going
[00:56:04.800 --> 00:56:08.280]   to help. It makes it it makes the competition more
[00:56:08.280 --> 00:56:14.960]   interesting. So I don't want to kind of do slides only. So I
[00:56:14.960 --> 00:56:20.280]   also prepared the demo, a demo training notebook that is now I
[00:56:20.280 --> 00:56:24.800]   just published it today on Kaggle. So so if you want to
[00:56:24.800 --> 00:56:29.440]   see how this ideas are implemented in code, then in
[00:56:29.440 --> 00:56:35.880]   this notebook, I implemented all of these ideas. This notebook is
[00:56:35.880 --> 00:56:39.520]   so I train a single fold in this notebook, I checked how it
[00:56:39.520 --> 00:56:43.840]   performed, it would it would end up in the gold range on the on
[00:56:43.840 --> 00:56:48.240]   the on the on the private leaderboard. So I think it's it
[00:56:48.240 --> 00:56:54.200]   proves that this this concepts work. I it's a bit different
[00:56:54.200 --> 00:56:57.560]   from the scores that I got from my on my on my home machine
[00:56:57.560 --> 00:57:00.760]   because because cousin has a bit different environment, but it's
[00:57:00.760 --> 00:57:05.240]   still it's worked pretty well. So I will just point out, yeah,
[00:57:05.240 --> 00:57:09.040]   some some other insights. It seems like hugging space has a
[00:57:09.040 --> 00:57:12.000]   question answering trainer, I am not sure if it's part of the
[00:57:12.000 --> 00:57:15.560]   library, but it's in the sample script. And by using this
[00:57:15.560 --> 00:57:18.400]   question answering trainer, you have the training group with the
[00:57:18.400 --> 00:57:22.280]   squad metrics implemented. So that's, I think most of the
[00:57:22.280 --> 00:57:25.360]   people in the in the forums, they looked at the validation
[00:57:25.360 --> 00:57:30.280]   loss. But validation loss is not always correlating well with
[00:57:30.280 --> 00:57:34.640]   with your final performance metric. So if you I actually did
[00:57:34.640 --> 00:57:38.640]   not implement the card metric, but looking at the squad metrics,
[00:57:38.640 --> 00:57:42.360]   like exact match and F1 score, I think it was it was correlating
[00:57:42.360 --> 00:57:45.400]   kits. Well, using this question answering trainer from hugging
[00:57:45.400 --> 00:57:50.120]   space was was super helpful. There is a lot of code here that
[00:57:50.120 --> 00:57:54.400]   I did not write. So it's a it's a copy paste from from hugging
[00:57:54.400 --> 00:57:58.280]   face script. And a lot of this code is actually written by
[00:57:58.280 --> 00:58:02.360]   Sylvain Huber. So, so again, another shout out to the faster
[00:58:02.360 --> 00:58:06.640]   AI to the faster AI community. It was super helpful. So a lot
[00:58:06.640 --> 00:58:09.440]   of the code I just just directly applied from hugging face
[00:58:09.440 --> 00:58:13.720]   script. There is one modification here. So in I
[00:58:13.720 --> 00:58:18.080]   overload this trainer, and I make one change where I change
[00:58:18.080 --> 00:58:22.080]   the sampler into sequential sampler. And if you if you
[00:58:22.120 --> 00:58:26.480]   remember what I tried to do here with with the data recipe, I
[00:58:26.480 --> 00:58:30.160]   want to control the order in which I put data into the model.
[00:58:30.160 --> 00:58:33.400]   So if I use like the default random sampler, like it will
[00:58:33.400 --> 00:58:37.040]   just shuffle all the data as it comes into the training group. I
[00:58:37.040 --> 00:58:40.720]   don't want that I want to be very specific about it. So I
[00:58:40.720 --> 00:58:44.440]   changed my sampler to a sequential sampler. So that's
[00:58:44.440 --> 00:58:50.840]   one important modification. What else? Yet all of this preparing
[00:58:50.840 --> 00:58:53.920]   features and QA post processing is coming straight from the
[00:58:53.920 --> 00:58:57.760]   hugging face demo question answering script. So I have not
[00:58:57.760 --> 00:59:03.360]   done any modifications here. So I won't go through it. I want to
[00:59:03.360 --> 00:59:07.560]   move into Yeah, this is this is one I think thing that that was
[00:59:07.560 --> 00:59:12.800]   helpful. So a lot of this, a lot of this example that we use,
[00:59:12.800 --> 00:59:16.120]   specifically from tidy natural questions, they have very long
[00:59:16.120 --> 00:59:20.240]   context. And if you have very long context, then that means
[00:59:20.240 --> 00:59:24.080]   that you have a lot of negative examples. So if you if you go
[00:59:24.080 --> 00:59:27.560]   with a sliding sliding window through that context, then there
[00:59:27.560 --> 00:59:30.800]   will be many windows where there is a question and you don't have
[00:59:30.800 --> 00:59:33.840]   the answer to your, your question in that in that
[00:59:33.840 --> 00:59:37.040]   segment. And if you just have too many of these negative
[00:59:37.040 --> 00:59:40.120]   examples, then the model does not like really learn what you
[00:59:40.120 --> 00:59:43.000]   wanted to learn. So removing some of these negative samples
[00:59:43.000 --> 00:59:49.080]   is helpful. So this negative sample, something was useful. I
[00:59:49.080 --> 00:59:52.240]   think this this post processing is coming from Abhishek. So
[00:59:52.240 --> 00:59:56.000]   again, big shout out to Abhishek Thakur. I also learned a lot
[00:59:56.000 --> 01:00:00.280]   from from him and and and his channel on YouTube. I especially
[01:00:00.280 --> 01:00:02.800]   when when being with Transcement, I would, I would
[01:00:02.800 --> 01:00:06.960]   try to code along with Abhishek and learn how his his link the
[01:00:06.960 --> 01:00:10.880]   the end like he's solving an LP problem. So that was that was
[01:00:10.880 --> 01:00:16.360]   super helpful. This, this helped with with XLM Roberta and
[01:00:16.680 --> 01:00:20.880]   Rembert because their tokenizers were sometimes concatenated,
[01:00:20.880 --> 01:00:24.000]   like they were they were not splitting the punctuation marks
[01:00:24.000 --> 01:00:29.080]   from from some words. And, and that was necessary for for for
[01:00:29.080 --> 01:00:35.640]   matching the answer to it the with with the labels. So so it
[01:00:35.640 --> 01:00:38.920]   was helpful for for those models. It was necessary for
[01:00:38.920 --> 01:00:43.480]   Mural, which is the in kind of Indic languages, but Mural was
[01:00:43.480 --> 01:00:48.520]   the strongest backbone in this in this competition. So data
[01:00:48.520 --> 01:00:51.840]   recipes, progressive resizing and random cropping, it's all
[01:00:51.840 --> 01:00:56.480]   implemented in this single function. There's a lot of
[01:00:56.480 --> 01:01:00.720]   pandas boilerplate here where I I take, I take all of the
[01:01:00.720 --> 01:01:05.160]   different data sets. And then I pre-paint features. So going
[01:01:05.160 --> 01:01:09.080]   back to the questions from Mani, I use this tokenization, like
[01:01:09.080 --> 01:01:12.920]   in the same like, this is my tokenization function that I
[01:01:12.920 --> 01:01:16.800]   pre-paint features. So it does not differ. It does not differ
[01:01:16.800 --> 01:01:20.560]   between the one stage or two stage approach. What I what I do
[01:01:20.560 --> 01:01:26.000]   change is if you look like this is child data set, and I apply
[01:01:26.000 --> 01:01:29.840]   this tokenization three times. So the first time I do it with
[01:01:29.840 --> 01:01:36.520]   the sequence like of 256 and, and stride of 200, 128. Second
[01:01:36.520 --> 01:01:39.240]   time, I use a different sequence length. So this is progressive
[01:01:39.240 --> 01:01:42.480]   resizing. And I use a different stride, which helps to move
[01:01:42.480 --> 01:01:46.000]   this random cropping. So every time that changes, by doing
[01:01:46.000 --> 01:01:48.600]   variation on the strides, that means that you have different
[01:01:48.600 --> 01:01:52.080]   examples coming into your your model every time. And then if
[01:01:52.080 --> 01:01:54.960]   you if you mix that with this random negative sampling that
[01:01:54.960 --> 01:02:01.480]   that way you introduce the variety. And yeah, this is,
[01:02:01.480 --> 01:02:04.480]   there is some negative sampling here as well. There's a lot of
[01:02:04.480 --> 01:02:09.320]   kind of code here. And then this is like, this is the key. So I,
[01:02:09.600 --> 01:02:12.560]   like I take all of these data sets that I tokenized here, I
[01:02:12.560 --> 01:02:16.040]   concatenate them and shuffle them like and I put here like
[01:02:16.040 --> 01:02:21.000]   specifically epoch one and epoch two. So in epoch one, I do, I do
[01:02:21.000 --> 01:02:26.440]   tidy and squat and, and this Hindi, MLQA and x squat and
[01:02:26.440 --> 01:02:30.320]   child, and I shuffle it together. Then in epoch two, I
[01:02:30.320 --> 01:02:36.200]   put again tidy, but now this is the different sequence. So tidy
[01:02:36.200 --> 01:02:39.400]   this time I take natural questions, and again, Hindi and
[01:02:39.400 --> 01:02:43.280]   again, child. And again, I shuffle this and then I put like
[01:02:43.280 --> 01:02:46.400]   I concatenate epoch one and epoch two, but this time I don't
[01:02:46.400 --> 01:02:49.840]   shuffle because I want the first epoch to come before the second
[01:02:49.840 --> 01:02:54.120]   one. And that becomes my my training data set. So this is
[01:02:54.120 --> 01:02:57.480]   like what this this is this is the data recipe. Like it's it's
[01:02:57.480 --> 01:03:00.840]   pretty boring. Like there is nothing super advanced here. But
[01:03:00.840 --> 01:03:04.440]   I did a lot of experiments with different recipes. Like in tidy
[01:03:04.440 --> 01:03:07.080]   we have like eight different languages, I think. So I tried
[01:03:07.080 --> 01:03:10.080]   like to check like is Japanese language helping with this
[01:03:10.080 --> 01:03:13.320]   with this challenge is Russian language helping with this
[01:03:13.320 --> 01:03:17.760]   challenge. So I tried to use different data and kind of
[01:03:17.760 --> 01:03:23.080]   different way of ordering that data. So it felt like I'm, I'm
[01:03:23.080 --> 01:03:25.360]   cooking right and trying to prepare some dishes and
[01:03:25.360 --> 01:03:28.000]   experimenting with different ingredients and proportions of
[01:03:28.000 --> 01:03:32.080]   those ingredients. So look, so so and maybe this is like there
[01:03:32.080 --> 01:03:36.680]   is this data centric AI movement, right? That's, yeah,
[01:03:36.680 --> 01:03:40.280]   that's I think I'm doing this for having tonight. Like for me,
[01:03:40.280 --> 01:03:42.920]   this was like this data centric approach, like you try to see
[01:03:42.920 --> 01:03:45.760]   like how to use different data, like the model is fixed, right?
[01:03:45.760 --> 01:03:48.880]   But you try to put a lot of variation in your data and see
[01:03:48.880 --> 01:03:50.200]   which one works best.
[01:03:50.200 --> 01:03:53.440]   Sanyam Bhutani: Did you did you consider back translation here?
[01:03:53.440 --> 01:03:56.920]   So where you take one language translated to another translated
[01:03:56.920 --> 01:03:57.320]   back?
[01:03:57.480 --> 01:04:01.080]   Yeah, yeah, I was thinking about it. I was specifically
[01:04:01.080 --> 01:04:07.120]   thinking about it for the questions. Well, so actually,
[01:04:07.120 --> 01:04:10.480]   let me let me take it back. So so one thing that was actually a
[01:04:10.480 --> 01:04:13.360]   lot of a lot of people experimented with and I did some
[01:04:13.360 --> 01:04:16.520]   experiments with it is not back translating, but just
[01:04:16.520 --> 01:04:20.160]   translating data. So we had this English squad, we have a lot of
[01:04:20.160 --> 01:04:23.680]   English data set. And some people translated that, like
[01:04:23.680 --> 01:04:27.240]   through Google Translate API is automatically into like Tamil
[01:04:27.240 --> 01:04:30.520]   and Hindi. And that's that already gave you a significant
[01:04:30.520 --> 01:04:33.960]   like significantly more data. The problem with this automated
[01:04:33.960 --> 01:04:40.040]   translation is that is that, well, it's, it's not always
[01:04:40.040 --> 01:04:43.680]   helpful, right? Sometimes it adds some, you know, artifacts
[01:04:43.680 --> 01:04:46.400]   that may be actually preventing your model from from learning.
[01:04:46.400 --> 01:04:50.160]   So, so I did some experiments with translated data, but at the
[01:04:50.160 --> 01:04:54.160]   end, it wasn't, it wasn't very helpful. So I did not use it. I
[01:04:54.160 --> 01:04:56.520]   was thinking about back translating, but only back
[01:04:56.520 --> 01:04:59.800]   translating the questions, because the context that was
[01:04:59.800 --> 01:05:03.520]   like, when you had a question that was like, maybe 10 words,
[01:05:03.520 --> 01:05:07.680]   and then a context of like 4000 words, then you could have a lot
[01:05:07.680 --> 01:05:11.400]   of variety in the context just by taking different crops from
[01:05:11.400 --> 01:05:15.040]   that context and, and applying this augmentation of like
[01:05:15.040 --> 01:05:18.440]   masking, like you already had a lot of variation in the, in the
[01:05:18.440 --> 01:05:21.480]   context. So I didn't think that back translating that would
[01:05:21.480 --> 01:05:24.760]   help. I was thinking about back translating the question,
[01:05:24.760 --> 01:05:26.920]   because question was pretty short, and you wanted to have
[01:05:26.920 --> 01:05:32.880]   some variation there. So that was, that was on my on my bucket
[01:05:32.880 --> 01:05:37.000]   list, but I didn't, I didn't, I didn't actually get to do it. So
[01:05:37.000 --> 01:05:40.400]   it might be an interesting experiment to check.
[01:05:40.400 --> 01:05:43.720]   Thanks. Thanks for the answer.
[01:05:43.720 --> 01:05:50.920]   Cool. So one more thing that I that we have here is is cut out.
[01:05:50.920 --> 01:05:54.600]   So random masking, random masking, and you asked about
[01:05:54.600 --> 01:05:58.600]   this question, like how to implement those those ideas. So,
[01:05:58.600 --> 01:06:03.640]   so the way I implemented this, again, I'm not a super good
[01:06:03.640 --> 01:06:08.680]   coder, but I'm trying to connect the dots. So so cut out is, is
[01:06:08.680 --> 01:06:11.280]   really random masking, right? You try to replace some tokens
[01:06:11.280 --> 01:06:15.440]   with a random mask. And you also need random masking for mask
[01:06:15.440 --> 01:06:20.120]   language modeling. Right? So in mask language modeling, you also
[01:06:20.120 --> 01:06:24.640]   like apply this mask, and then you try to predict what is what
[01:06:24.640 --> 01:06:29.720]   is the work behind that that mask. So, so Hugging Space has a
[01:06:29.720 --> 01:06:32.760]   function that is called data collator for mask language
[01:06:32.760 --> 01:06:36.520]   modeling, where they implemented this mask, this this random
[01:06:36.520 --> 01:06:42.240]   random masking. So what I do here is I, I, I look, I look up
[01:06:42.240 --> 01:06:45.800]   that that that random data collator for mask language
[01:06:45.800 --> 01:06:48.840]   modeling. And I try to understand how it works. And
[01:06:48.840 --> 01:06:51.920]   most of my code is a copy paste with some changes, right?
[01:06:51.920 --> 01:06:54.960]   Because I don't need the labels. I don't, I'm not trying to
[01:06:54.960 --> 01:06:59.520]   predict the value of the mask here. But, but it's not like I
[01:06:59.520 --> 01:07:02.080]   come up with a lot of code by myself. Sometimes it's just,
[01:07:02.080 --> 01:07:05.320]   just trying figuring out what to look, what to look for a good
[01:07:05.320 --> 01:07:11.680]   example and trying to adjust it. Good. And now this is and the
[01:07:11.680 --> 01:07:14.040]   training is super simple. So again, I'm using the Hugging
[01:07:14.040 --> 01:07:18.600]   Space trainer. I usually use the hyper parameters from from the
[01:07:18.600 --> 01:07:22.200]   paper. So this is using the Morial backbone. Morial is
[01:07:22.200 --> 01:07:24.960]   actually trained with a pretty like science with a pretty high
[01:07:24.960 --> 01:07:29.440]   learning rate. So in the paper, they mentioned 3e to minus 5.
[01:07:29.440 --> 01:07:34.120]   And I think I trained most of my Morial backbones with 3e to
[01:07:34.120 --> 01:07:37.280]   minus 5. At some point, I made it a bit smaller, because
[01:07:37.280 --> 01:07:41.680]   sometimes the training would collapse. But then most of the
[01:07:41.680 --> 01:07:45.840]   other hyper parameters I did not, I did not change. And then,
[01:07:46.440 --> 01:07:49.000]   and then if you use this question answering trainer from
[01:07:49.000 --> 01:07:52.400]   Hugging Space, then you can very nicely like see how your
[01:07:52.400 --> 01:07:56.040]   training loss is going down. Actually, because of the data
[01:07:56.040 --> 01:08:00.040]   recipe. At the very beginning, we put some of the simple data
[01:08:00.040 --> 01:08:03.400]   like squats, so the loss is getting pretty low here. But
[01:08:03.400 --> 01:08:05.680]   then you're adding some more complex data, and then you're
[01:08:05.680 --> 01:08:08.280]   increasing your simplest length. So your losses training cost is
[01:08:08.280 --> 01:08:13.200]   actually going up. But if you look at your validation, then
[01:08:13.200 --> 01:08:16.720]   the validation actually keeps going up. So it's a good sign.
[01:08:16.720 --> 01:08:21.000]   And this has the squat measures. For some reason, I wasn't able
[01:08:21.000 --> 01:08:24.040]   to get validation loss to work with the trainer. But I think
[01:08:24.040 --> 01:08:27.240]   these measures were more meaningful than validation loss.
[01:08:27.240 --> 01:08:31.800]   So I was okay with with this trade off. Yeah, and then you
[01:08:31.800 --> 01:08:35.760]   can look at your final final results. And you can calculate
[01:08:35.760 --> 01:08:39.560]   the Jaccard score for the validation. So if you look at
[01:08:39.560 --> 01:08:43.880]   the inference for this notebook, it it would score a gold medal
[01:08:43.880 --> 01:08:47.720]   in the competition and it's it runs in thinking on Kaggle
[01:08:47.720 --> 01:08:51.200]   kernel it completed late in around eight hours.
[01:08:51.200 --> 01:08:56.000]   Sanyam Bhutani: I just wanted to say how beautiful the solution
[01:08:56.000 --> 01:08:58.240]   was, especially the image augmentation part and
[01:08:58.240 --> 01:09:01.600]   everything else. Also, it's you try to understand winning
[01:09:01.600 --> 01:09:04.600]   solutions, they're complex, but just having this experience of
[01:09:04.600 --> 01:09:07.520]   understanding them and understanding what goes into a
[01:09:07.520 --> 01:09:10.040]   gold medal, especially with something this creative, it's
[01:09:10.040 --> 01:09:15.560]   such an incredible experience always. Just just wanted to say
[01:09:15.560 --> 01:09:16.480]   that instantly.
[01:09:16.480 --> 01:09:22.200]   Yeah, appreciated. I still have a couple of ideas that didn't
[01:09:22.200 --> 01:09:25.440]   work out. So if you if you want, I can I can I can quickly go
[01:09:25.440 --> 01:09:30.000]   through them before we move to questions. So yeah, this is so
[01:09:30.000 --> 01:09:32.960]   this is my final example. So just want to quickly highlight
[01:09:32.960 --> 01:09:37.640]   that. So at the end, I use three, three backbones. And the
[01:09:37.640 --> 01:09:41.560]   reason I use three is because I missed one, there was apparently
[01:09:41.560 --> 01:09:46.320]   an info XLM model that was also a strong multi-ring backbone.
[01:09:46.320 --> 01:09:50.080]   But somehow like it was not mentioned on the forums, and I
[01:09:50.080 --> 01:09:53.320]   didn't find out about it. So I use the three that were
[01:09:53.320 --> 01:09:57.720]   discussed on the forum. And Muriel is a pretty well, like
[01:09:57.720 --> 01:10:01.280]   both Muriel and Rembert are pretty new. And Muriel is the
[01:10:01.280 --> 01:10:05.000]   strongest backbone. So it's it was, it's, it has the bird
[01:10:05.000 --> 01:10:10.600]   architecture, but large. And it was pre trained on on Indic
[01:10:10.600 --> 01:10:14.360]   languages specifically. And it proved to work work super well
[01:10:14.360 --> 01:10:20.440]   here. So a single, like my best, my best single model. So this
[01:10:20.440 --> 01:10:23.080]   was my best single model on the public leaderboard. It's called
[01:10:23.080 --> 01:10:28.360]   802. And it was it called 783 on the pilot. So my best single
[01:10:28.360 --> 01:10:31.960]   model would still get the first place in the competition. So I
[01:10:31.960 --> 01:10:39.240]   was pretty about that. The sad thing about this single model
[01:10:39.240 --> 01:10:42.480]   is I think I did not use some of the augmentation techniques in
[01:10:42.480 --> 01:10:45.440]   this training class. So this is this is why this ablation study
[01:10:45.440 --> 01:10:48.960]   might be might be important. It doesn't use the data recipe. So
[01:10:48.960 --> 01:10:53.120]   that was helpful. But I think this some of this this
[01:10:53.120 --> 01:10:56.280]   augmentation they have at the end with with the ensembles and
[01:10:56.280 --> 01:11:00.800]   with adding more diversity into the into the the total ensemble.
[01:11:00.800 --> 01:11:04.360]   So, so that the standard way of approaching and something like
[01:11:04.360 --> 01:11:07.080]   with the same backbone is you just blend it, right. So you
[01:11:07.080 --> 01:11:11.040]   average the outputs of the model. So the blend of the three
[01:11:11.040 --> 01:11:17.920]   Muriel models was 808 on public and it was 787, which was the
[01:11:17.920 --> 01:11:21.520]   same as my final kind of ensemble score. So Muriel was
[01:11:21.520 --> 01:11:23.840]   definitely the strongest performer here. I think one
[01:11:23.840 --> 01:11:27.360]   thing that just with Muriel it was shaken up from somewhat like
[01:11:27.360 --> 01:11:31.040]   below into the third place. So another evidence that Muriel was
[01:11:31.040 --> 01:11:34.760]   was a super strong model here. And then I had a blend of five
[01:11:34.760 --> 01:11:40.200]   Rembrandt models and five XLM Roberta models. And then after
[01:11:40.200 --> 01:11:44.360]   blending within each of the backbone, I did voting. So I had
[01:11:44.360 --> 01:11:49.800]   three, I had three outputs. So three sets of words and then and
[01:11:49.800 --> 01:11:54.400]   then I did majority voting at what level. So if two out of the
[01:11:54.400 --> 01:11:57.520]   three blends told me that a certain word should be in my
[01:11:57.520 --> 01:12:02.120]   final prediction, then I would keep that word. If that did not
[01:12:02.120 --> 01:12:05.920]   happen, then I would take that word out. So the final score was
[01:12:05.920 --> 01:12:09.840]   it so it was second on public leaderboard. But it's when I saw
[01:12:09.840 --> 01:12:13.400]   I was waiting for the final leaderboard to be reviewed. And
[01:12:13.400 --> 01:12:18.040]   when I saw it first place, I was like in shock. I was in total
[01:12:18.040 --> 01:12:20.200]   shock. I did not believe what was happening.
[01:12:20.200 --> 01:12:26.800]   Sanyam Bhutani: I remember talking to you then and I was
[01:12:26.800 --> 01:12:30.760]   very pleasantly happy and you still you said shocked by the
[01:12:30.760 --> 01:12:31.280]   result.
[01:12:31.280 --> 01:12:35.320]   Dr. Arif Karim Barani: Yeah, it was a shock. I did not I did not
[01:12:35.320 --> 01:12:40.400]   anticipate it. Yeah, so that's that was the this is the
[01:12:40.400 --> 01:12:45.480]   architecture of the final, final ensemble. And things that
[01:12:45.480 --> 01:12:48.520]   didn't work. So I tried some architecture modifications and
[01:12:48.520 --> 01:12:52.360]   those function modifications. And I, at first I said, like
[01:12:52.360 --> 01:12:55.680]   these things didn't work. But I think I should say I didn't make
[01:12:55.680 --> 01:12:58.560]   them work. Because the reason they there might be many reasons
[01:12:58.560 --> 01:13:01.760]   why they didn't work either I did. I made some implementation
[01:13:01.760 --> 01:13:04.920]   error or maybe I didn't put enough time into you know,
[01:13:04.920 --> 01:13:07.880]   testing maybe there was a like if you change the architecture,
[01:13:07.880 --> 01:13:10.480]   maybe you should also test like, like have a different set of
[01:13:10.480 --> 01:13:15.080]   hyper parameters, right? So I think maybe these modifications
[01:13:15.080 --> 01:13:19.680]   might help given more time and effort. At the end, I tried to
[01:13:19.680 --> 01:13:23.600]   timebox each experiment with certain, certain duration so
[01:13:23.600 --> 01:13:26.000]   that I can keep moving and experiment with different
[01:13:26.000 --> 01:13:29.320]   things. So I tried using a different head architecture.
[01:13:29.320 --> 01:13:33.360]   That is this XLNet architecture where you first try to predict
[01:13:33.360 --> 01:13:36.080]   the start tokens and then take that into account when you
[01:13:36.080 --> 01:13:40.520]   predict the end tokens or the end logics. So that didn't work
[01:13:40.520 --> 01:13:45.520]   for me. There was this, there is a way to do a specific loss
[01:13:45.520 --> 01:13:48.880]   function for that would that that should correlate better
[01:13:48.880 --> 01:13:52.480]   with your card score. So I tried this as well. I think this one
[01:13:52.480 --> 01:13:55.440]   for like, I'm pretty sure I didn't implement this properly
[01:13:55.440 --> 01:13:59.800]   because, because I'm not I'm not super good with math, but at
[01:13:59.800 --> 01:14:04.200]   least I tried. And then I was super bullish about mt5. So this
[01:14:04.200 --> 01:14:08.240]   is the this is another architecture which is sequence
[01:14:08.240 --> 01:14:12.080]   to sequence. And the reason I was bullish is first like, like
[01:14:12.080 --> 01:14:16.680]   mt5, the largest version of mt5 is, it's kind of like the state
[01:14:16.680 --> 01:14:21.720]   of the art for like small data regimes, like, like what we had
[01:14:21.720 --> 01:14:27.840]   here. And I also participated in a Polish competition, Polish NLP
[01:14:27.840 --> 01:14:31.880]   competition that was about question answering. And I, I was
[01:14:31.880 --> 01:14:36.520]   knocked out by another person that used mt5 model. And I
[01:14:36.520 --> 01:14:40.480]   thought, wow, maybe I should, I should try this, this one. But
[01:14:40.480 --> 01:14:44.160]   well, in that competition, we didn't do it wasn't a code
[01:14:44.160 --> 01:14:47.560]   competition. So the person like that, that and Mateusz, shout
[01:14:47.560 --> 01:14:52.360]   out to you Mateusz, like he had access to a TPU and he used the
[01:14:52.360 --> 01:14:57.240]   largest mt5 model. And that, that was a big, that was a big
[01:14:57.240 --> 01:15:01.840]   contributor to success. I don't think that the large, like the
[01:15:01.840 --> 01:15:05.720]   largest model can be can be done in a Kaggle notebook. At least I
[01:15:05.720 --> 01:15:10.080]   wasn't able to do it. So and the one that we were able to fit
[01:15:10.080 --> 01:15:14.040]   into the notebook, like it didn't work so well. I tried to
[01:15:14.040 --> 01:15:16.880]   do some label error corrections, but without understanding the
[01:15:16.880 --> 01:15:23.120]   language, correcting the labels is a pretty, is a pretty risky,
[01:15:23.120 --> 01:15:27.280]   risky thing to do. So I tried to do it with Google Translate. So
[01:15:27.280 --> 01:15:32.200]   so I, I ran like a like a like an early version of my model.
[01:15:32.200 --> 01:15:35.280]   Whenever the model disagrees with the labels, I try to
[01:15:35.280 --> 01:15:39.200]   translate it and figure out okay, is my model better or is
[01:15:39.200 --> 01:15:43.480]   the label better. And I tried to choose based on whatever Google
[01:15:43.480 --> 01:15:47.400]   was telling me. So I tried this, it doesn't help. But maybe
[01:15:47.400 --> 01:15:51.520]   again, I didn't put enough effort into that. There was a
[01:15:51.520 --> 01:15:55.040]   way to improve the score with with tokenization because of the
[01:15:55.040 --> 01:15:57.880]   part that this this tokens and the punctuation marks were
[01:15:57.880 --> 01:16:01.080]   sometimes connected. So if you can split that, then that should
[01:16:01.080 --> 01:16:04.240]   help the model. And that's one of the reasons Mouril was so
[01:16:04.240 --> 01:16:08.760]   strong. But, but I wasn't able to make it work. And then I did
[01:16:08.760 --> 01:16:11.200]   some post processing, which actually helped me on public
[01:16:11.200 --> 01:16:17.240]   leaderboard. But it's when I looked at this, like, like my
[01:16:17.240 --> 01:16:21.200]   models with and without post processing, it improved on the
[01:16:21.200 --> 01:16:25.280]   public leaderboard, but it was actually worse on private. So,
[01:16:25.280 --> 01:16:29.040]   so that was, that was actually what publicly that was not that
[01:16:29.040 --> 01:16:36.440]   helpful. So yeah, so this is, I think this is the highlight.
[01:16:36.440 --> 01:16:38.880]   Happy to take any questions.
[01:16:38.880 --> 01:16:42.240]   Before we dive into the community, once if you could
[01:16:42.240 --> 01:16:45.440]   please go back to the previous slide, I was, I was really
[01:16:45.440 --> 01:16:48.160]   curious, how are you? How are you tracking these experiments?
[01:16:48.160 --> 01:16:53.600]   And what did you what process were you using just to keep on
[01:16:53.600 --> 01:16:54.920]   top of all experiments?
[01:16:55.800 --> 01:17:02.040]   Yeah, so, so I, this is one of the one of the areas where I
[01:17:02.040 --> 01:17:10.000]   know I have a big opportunity to improve. I am. And I, so I did.
[01:17:10.000 --> 01:17:13.160]   So in this competition, I use Google Sheets, right? So I try
[01:17:13.160 --> 01:17:18.680]   to take notes in a Google Sheet and just keep note of what I'm
[01:17:18.680 --> 01:17:23.240]   what I'm doing. And, and, and what's what's the what's the
[01:17:23.240 --> 01:17:28.800]   score. I, I remember your interview with, I think, Dieter
[01:17:28.800 --> 01:17:31.880]   and Philip. So one of the competitions where they, where
[01:17:31.880 --> 01:17:36.760]   they went, like really, they, they came up with this approach
[01:17:36.760 --> 01:17:39.680]   where they experiment, like they had an experiment tracker, and
[01:17:39.680 --> 01:17:44.560]   they use GitHub, and they use like, and I was so I was, I was
[01:17:44.560 --> 01:17:48.040]   so amazed by this approach. I thought I want to do it that
[01:17:48.040 --> 01:17:53.200]   way. But in this competition, I don't think I had enough time
[01:17:53.200 --> 01:17:56.760]   to, you really need to learn how to do it properly to get the
[01:17:56.760 --> 01:18:00.400]   benefit out of it. So I'm still I'm still learning, like, and I
[01:18:00.400 --> 01:18:03.960]   think that might be one of the next competitions where I try to
[01:18:03.960 --> 01:18:06.560]   get better at tracking my experiment.
[01:18:06.560 --> 01:18:10.920]   Thanks. Thanks for that answer. If you could please stop
[01:18:10.920 --> 01:18:12.840]   sharing so that I may share the forums.
[01:18:12.840 --> 01:18:13.400]   Sure.
[01:18:18.080 --> 01:18:23.800]   One moment. And again, as a reminder to everyone, we're
[01:18:23.800 --> 01:18:26.200]   taking questions from the community. Here's the link.
[01:18:26.200 --> 01:18:30.120]   Again, I'll drop it in the chat to anyone that's still with us,
[01:18:30.120 --> 01:18:32.840]   please feel free to ask any questions. This portion is for
[01:18:32.840 --> 01:18:40.240]   you. So let's start from the top. In your opinion, what is
[01:18:40.240 --> 01:18:42.680]   one technique in machine learning or deep learning that
[01:18:42.680 --> 01:18:44.920]   is underrated, but very useful?
[01:18:45.280 --> 01:18:51.760]   I, I'm not sure about. I'm not sure if that's one technique,
[01:18:51.760 --> 01:18:55.800]   and I'm not sure if it's, if it's underrated. I think
[01:18:55.800 --> 01:19:00.280]   generally following Jeremy's advice is always useful. But I
[01:19:00.280 --> 01:19:07.600]   think people recognize that. In general. I think, I think one of
[01:19:07.600 --> 01:19:11.320]   the things that I found in this competition that worked, that
[01:19:11.320 --> 01:19:15.480]   worked really well for me is, is the data recipe where you,
[01:19:15.480 --> 01:19:18.960]   where you try to control the order in which you put data
[01:19:18.960 --> 01:19:23.960]   into your training group. And, and, and you're very deliberate
[01:19:23.960 --> 01:19:27.080]   about it. So you don't rest at your optimizer, you train, you
[01:19:27.080 --> 01:19:30.440]   train for, for, let's say for one epoch, but then you put data
[01:19:30.440 --> 01:19:34.360]   at a certain sequence. I have not, and maybe somebody has done
[01:19:34.360 --> 01:19:38.760]   it before, I have not seen that personally. But, but I think
[01:19:38.760 --> 01:19:43.800]   data is like the way you use data and external data is, is,
[01:19:43.800 --> 01:19:46.200]   is often, is often super helpful.
[01:19:46.200 --> 01:19:52.200]   Interesting. Maybe, maybe your abolition studies will tell us
[01:19:52.200 --> 01:19:53.080]   more about that.
[01:19:53.080 --> 01:19:56.160]   Hopefully.
[01:19:56.160 --> 01:20:01.360]   So this is Mani, I'm sure you recognize him. And I personally
[01:20:01.360 --> 01:20:04.200]   also, I'm really curious about this. How do you manage
[01:20:04.200 --> 01:20:06.000]   everything that you do?
[01:20:06.000 --> 01:20:08.840]   And how do you overcome the challenges that you might face
[01:20:08.840 --> 01:20:13.160]   with family responsibilities, the leadership role? And, and
[01:20:13.160 --> 01:20:13.680]   Kaggle?
[01:20:13.680 --> 01:20:17.440]   Yeah, that's, that's a tough one. I don't think there's a
[01:20:17.440 --> 01:20:21.880]   good answer to it. Like my answer is like, I, I, I'm
[01:20:21.880 --> 01:20:25.320]   taking a break now from Kaggle because I, I cannot be in this
[01:20:25.320 --> 01:20:29.080]   mode of competing, especially like with like competing
[01:20:29.080 --> 01:20:34.400]   seriously, like, like, in any other competition, I'm not
[01:20:34.400 --> 01:20:38.160]   really like, like, and maybe like also timeboxing
[01:20:38.160 --> 01:20:41.880]   competition. So, so with with time, what I think what worked
[01:20:41.880 --> 01:20:45.840]   really well for me is I, I didn't, I didn't put the same
[01:20:45.840 --> 01:20:47.800]   effort throughout the whole competition, which was three
[01:20:47.800 --> 01:20:52.520]   months. But, but I really focused for, for a month out of
[01:20:52.520 --> 01:20:56.800]   the three months at the very end. And, and I sacrificed, I
[01:20:56.800 --> 01:21:00.080]   would say, primarily speak during that time. And, and
[01:21:00.080 --> 01:21:02.880]   obviously, like this had implication because like the
[01:21:02.880 --> 01:21:08.160]   amount of attention or energy I could put in, in other places
[01:21:08.160 --> 01:21:13.720]   was, was lower. And I, I tried not to, not to reduce my, my
[01:21:13.720 --> 01:21:16.760]   attention to family or to my job. But, but for sure, the
[01:21:16.760 --> 01:21:20.440]   fact that I speak fewer hours have had an implication. So you
[01:21:20.440 --> 01:21:24.480]   cannot do that on a, on a, on a longer basis. So I tried to
[01:21:24.480 --> 01:21:28.480]   take a break now, you know, get some more sleep, spend more time
[01:21:28.480 --> 01:21:31.880]   with family, focus on my job. And then at some point, like
[01:21:31.880 --> 01:21:34.600]   maybe that into another competition, but like, you need
[01:21:34.600 --> 01:21:38.800]   to be careful, like Kaggle can get so addictive that, that
[01:21:38.800 --> 01:21:41.880]   unless you, like you stop yourself, like I'm seeing this
[01:21:41.880 --> 01:21:44.280]   new competition that are coming and they are so interesting
[01:21:44.280 --> 01:21:47.080]   that I, I'm trying to keep myself from jumping here
[01:21:47.080 --> 01:21:50.280]   because I know I should not do that. I need to do other stuff
[01:21:50.280 --> 01:21:50.600]   first.
[01:21:50.600 --> 01:21:54.400]   That's a really good answer.
[01:21:54.400 --> 01:21:58.560]   I think that's a really honest answer.
[01:21:58.560 --> 01:22:00.280]   Yeah.
[01:22:01.040 --> 01:22:05.440]   And so I think I wasn't active in the competition. But from
[01:22:05.440 --> 01:22:07.960]   what I understand, this was discussed in the forums, maybe
[01:22:07.960 --> 01:22:11.120]   you could correct me. How did you decide to use external data?
[01:22:11.120 --> 01:22:13.400]   And what was the logic behind this?
[01:22:13.400 --> 01:22:18.440]   Yeah, I think this is this is probably so that how do I know
[01:22:18.440 --> 01:22:23.560]   that external data was required? I, I think that was, that was
[01:22:23.560 --> 01:22:28.640]   like the fact that we had very little training data. That
[01:22:28.640 --> 01:22:32.000]   meant that the obvious way to improve your performance was to
[01:22:32.000 --> 01:22:36.000]   just get more data. And I think what I think the host was very
[01:22:36.000 --> 01:22:39.360]   explicit about it, that they are looking for the competitors to
[01:22:39.360 --> 01:22:42.840]   come up with new data sets and potentially create them. I think
[01:22:42.840 --> 01:22:46.520]   the idea was to kind of collectively improve the state of
[01:22:46.520 --> 01:22:53.400]   the Indic languages NLP by creating more data sets. At the
[01:22:53.400 --> 01:22:56.920]   end, it turned out that you can actually get pretty far by
[01:22:56.920 --> 01:23:00.240]   using external data from other languages from similar language
[01:23:00.240 --> 01:23:03.480]   families. So I'm not sure if that was anticipated by the
[01:23:03.480 --> 01:23:07.400]   host, but that's what turned out to be to be really helpful here.
[01:23:07.400 --> 01:23:15.680]   Makes sense. I'll skip this one since we've discussed it. This
[01:23:15.680 --> 01:23:18.240]   is interesting. Are you thinking of any data sets for the
[01:23:18.240 --> 01:23:22.200]   application studies you mentioned? Or have you thought
[01:23:22.200 --> 01:23:23.000]   about it at all?
[01:23:24.600 --> 01:23:27.280]   Yeah, that's a very good question. I and I don't have an
[01:23:27.280 --> 01:23:30.200]   answer to that yet. So Manny, if you have any ideas, I'm super,
[01:23:30.200 --> 01:23:36.600]   super open. I posted this on the on the discord server and
[01:23:36.600 --> 01:23:39.720]   hugging face. I'm looking for people with ideas on how to test
[01:23:39.720 --> 01:23:44.240]   it. I'm not, I'm not, I'm not really a researcher. So I need
[01:23:44.240 --> 01:23:47.640]   to figure out how to do it, how to do it properly. But if
[01:23:47.640 --> 01:23:51.120]   somebody knows how to do it, I am super open and welcome, like
[01:23:51.120 --> 01:23:52.080]   advice is welcome.
[01:23:52.920 --> 01:23:57.880]   Okay. Maybe someone could start a thread on our forums and we
[01:23:57.880 --> 01:24:00.120]   could figure out a way. But again, that's that's for the
[01:24:00.120 --> 01:24:03.720]   audience to decide. I'm really probably the biggest move in
[01:24:03.720 --> 01:24:08.280]   this chart right now to even comment on that. Question around
[01:24:08.280 --> 01:24:12.880]   the ensemble. So did you end up using all of the models? Or if
[01:24:12.880 --> 01:24:14.040]   you could just clarify on that?
[01:24:14.040 --> 01:24:20.080]   Yeah, yeah. So, so yeah, so this sample was consisted of 13
[01:24:20.080 --> 01:24:24.560]   models, and they were divided into three subgroups. And they,
[01:24:24.560 --> 01:24:27.800]   like each of them had different training schemes. So there was
[01:24:27.800 --> 01:24:31.920]   variation. So sometimes, so sometimes like there was, there
[01:24:31.920 --> 01:24:34.720]   was definitely randomness. I'm not necessarily different
[01:24:34.720 --> 01:24:37.360]   data sets, because sometimes I think with the same data sets,
[01:24:37.360 --> 01:24:40.520]   but I would put them in second order, or I would apply
[01:24:40.520 --> 01:24:43.240]   different augmentation, there was some randomness with this
[01:24:43.240 --> 01:24:48.400]   negative sampling. And so, so each of them had had variation
[01:24:48.400 --> 01:24:51.720]   on the data. But sometimes the data sets were the same.
[01:24:51.720 --> 01:24:57.920]   Okay. Thanks. Thanks for that clarification. I'll give anyone
[01:24:57.920 --> 01:25:01.360]   another minute to ask any questions. I see someone is
[01:25:01.360 --> 01:25:05.080]   typing in the meantime, since we're almost on the top of the
[01:25:05.080 --> 01:25:08.680]   hour, I'd love to point everyone to Derek's profile on Kaggle.
[01:25:08.680 --> 01:25:12.080]   If you go there, it's simple to find him. He said he didn't
[01:25:12.080 --> 01:25:14.560]   think of his username. I think it's a very interesting
[01:25:14.560 --> 01:25:18.400]   interesting username. His username is the doctor cat. I
[01:25:18.400 --> 01:25:20.200]   know Dr. HP who
[01:25:20.200 --> 01:25:24.760]   for sure. I'm more of a cat than a doctor to be honest.
[01:25:24.760 --> 01:25:30.000]   But I wanted to say I know Dr. HP who's also from the fast
[01:25:30.000 --> 01:25:33.440]   tech community also doing amazing on Kaggle. So just
[01:25:33.440 --> 01:25:36.480]   wanted to point that out. If you go to his profile, you should
[01:25:36.480 --> 01:25:39.560]   be able to find his Twitter profile from there. His handle
[01:25:39.560 --> 01:25:44.400]   is DK21. Please make sure you find him on Twitter. And he
[01:25:44.400 --> 01:25:47.560]   also has amazing blog posts. There are a few I hope to see
[01:25:47.560 --> 01:25:49.960]   more here. Derek, if you find the time, I don't know how you
[01:25:49.960 --> 01:25:55.720]   find the time even for this, but the website is skok.ai. So I
[01:25:55.720 --> 01:26:02.240]   would highly encourage everyone to check the blog out. Okay,
[01:26:02.240 --> 01:26:08.800]   maybe they didn't ask the question. All right. With that,
[01:26:08.800 --> 01:26:12.320]   as a reminder to everyone who might be watching the recording
[01:26:12.320 --> 01:26:15.480]   later on or who's live, if you want to find these links, I'll
[01:26:15.480 --> 01:26:19.240]   post these on our forums after the conversation. Thanks again,
[01:26:19.240 --> 01:26:22.040]   everyone for joining us. And thank you so much again, Derek
[01:26:22.040 --> 01:26:25.480]   for sharing your journey and sharing your, I would say very
[01:26:25.480 --> 01:26:29.000]   beautiful solution with us. I'm sure the audience learned a lot
[01:26:29.000 --> 01:26:29.320]   today.
[01:26:29.320 --> 01:26:34.240]   Thank you. Thank you. It's a great honor to have people
[01:26:34.240 --> 01:26:37.280]   listening to what I have to say. So I appreciate everyone
[01:26:37.280 --> 01:26:40.560]   listening live for listening later on on YouTube. And, and
[01:26:40.560 --> 01:26:43.320]   thanks a lot. Thanks a lot, Sajam for being such a such a
[01:26:43.320 --> 01:26:47.160]   great host. Like you make you make it you make it a real fun
[01:26:47.160 --> 01:26:50.200]   to be having this discussion. Appreciate it.
[01:26:50.200 --> 01:26:55.760]   I just I just drink chai and ask stupid questions. That's all I
[01:26:55.760 --> 01:27:00.520]   do. Awesome. I'll wrap up here. Thanks, everyone for joining.
[01:27:00.520 --> 01:27:03.720]   Hope you have a great weekend ahead. As a reminder to anyone
[01:27:03.720 --> 01:27:06.960]   who's watching this, we have the JAX study group happening
[01:27:06.960 --> 01:27:10.760]   tomorrow. I won't waste directs to time by talking too much
[01:27:10.760 --> 01:27:13.480]   about that. But the long story short version if if you want to
[01:27:13.480 --> 01:27:16.200]   learn about JAX, we'll be starting a study course
[01:27:16.200 --> 01:27:19.240]   tomorrow. Please make sure to join that you can find more
[01:27:19.240 --> 01:27:23.120]   details on our forums. Awesome. I'll end the live stream here.
[01:27:23.120 --> 01:27:23.920]   Thanks, everyone.
[01:27:24.040 --> 01:27:26.040]   Transcribed by https://otter.ai
[01:27:26.040 --> 01:27:36.040]   [BLANK_AUDIO]

