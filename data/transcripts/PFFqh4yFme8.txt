
[00:00:00.000 --> 00:00:08.940]   I've been given the formidable challenge today of presenting two papers on quite, not necessarily
[00:00:08.940 --> 00:00:12.160]   a smooth transition in ideas between the two.
[00:00:12.160 --> 00:00:20.080]   So I think what I'll ask for is, Charles, if you could remind me if we get to 5.50 and
[00:00:20.080 --> 00:00:26.400]   I still haven't switched over, then I'll switch over to talking about a very exciting second
[00:00:26.400 --> 00:00:31.640]   idea which is actually with a co-first author called Chirag who's lovely and it's all about
[00:00:31.640 --> 00:00:37.280]   estimating example importance and how do we surface automatically examples for human and
[00:00:37.280 --> 00:00:38.280]   loop inspection.
[00:00:38.280 --> 00:00:44.720]   But to start with, since this is a special request, I will be talking without lovely
[00:00:44.720 --> 00:00:48.560]   slides but I think that's almost better for this type of paper.
[00:00:48.560 --> 00:00:54.800]   I'll be talking about a project that I recently shared which is called the Hardware Lottery.
[00:00:54.800 --> 00:01:01.360]   And I think that this project, I don't know if slides would do it justice because in some
[00:01:01.360 --> 00:01:10.320]   ways it's really about this idea of how our choices about hardware and software have really
[00:01:10.320 --> 00:01:14.200]   had implications for the direction of research and the progress that we've made.
[00:01:14.200 --> 00:01:17.280]   What I will invite because of that is a lot of questions.
[00:01:17.280 --> 00:01:23.320]   So maybe what I'll start with is telling a little bit about how I arrived at this idea
[00:01:23.320 --> 00:01:24.680]   because I think it's always fun.
[00:01:24.680 --> 00:01:31.640]   You can always go read the paper and in fact maybe I'll just present the background of
[00:01:31.640 --> 00:01:39.880]   the paper behind.
[00:01:39.880 --> 00:01:46.440]   But perhaps it's fun to start by talking a little bit about why I came to this idea and
[00:01:46.440 --> 00:01:48.840]   why I took the time to write it down.
[00:01:48.840 --> 00:01:54.320]   A lot of my research has been on the topic of beyond test set accuracy.
[00:01:54.320 --> 00:01:57.400]   So there was a question earlier about should we care?
[00:01:57.400 --> 00:01:58.400]   So we should.
[00:01:58.400 --> 00:02:03.440]   So a lot of my research has been how do we train models to not just have high test set
[00:02:03.440 --> 00:02:12.680]   accuracy but to also essentially have interpretability, fulfill fairness requirements, be compact.
[00:02:12.680 --> 00:02:17.880]   And compactness is interesting because the area of compactness research, there's almost
[00:02:17.880 --> 00:02:20.280]   two different motivations you could care about.
[00:02:20.280 --> 00:02:27.440]   So one is this idea that you want to arrive at a theoretically most compact representation
[00:02:27.440 --> 00:02:29.680]   of the network for the task.
[00:02:29.680 --> 00:02:32.600]   And a lot of people are interested in compression for that reason.
[00:02:32.600 --> 00:02:39.560]   What is the minimal amount of weights or minimal amount of neurons we can have in order to
[00:02:39.560 --> 00:02:43.440]   still perform and converge well on a task?
[00:02:43.440 --> 00:02:50.920]   There's another motivation which is that if you have compactness, it really helps in deployment.
[00:02:50.920 --> 00:02:56.680]   And so one reason compression is really exciting is because you can democratize AI in much
[00:02:56.680 --> 00:02:59.080]   more resource constrained environments.
[00:02:59.080 --> 00:03:02.400]   And I came into this research area caring about both.
[00:03:02.400 --> 00:03:08.720]   I'm really interested in how we can design more theoretically anchored algorithms where
[00:03:08.720 --> 00:03:11.280]   we achieve higher levels of compression.
[00:03:11.280 --> 00:03:12.780]   But I wanted to translate.
[00:03:12.780 --> 00:03:17.960]   I grew up in Africa and I really want these algorithms to work in the resource constrained
[00:03:17.960 --> 00:03:21.680]   environment of low memory, expensive data.
[00:03:21.680 --> 00:03:25.680]   You often don't have a place to charge your phone.
[00:03:25.680 --> 00:03:30.460]   And what was interesting is that the hardware lottery really started because I was trying
[00:03:30.460 --> 00:03:34.840]   to figure out how does my theoretical work translate.
[00:03:34.840 --> 00:03:36.680]   And that might seem like a simple question.
[00:03:36.680 --> 00:03:42.560]   But it actually became kind of an eight month odyssey and exploration.
[00:03:42.560 --> 00:03:47.720]   Because what I found was the more I asked, the more people said, well, we're not quite
[00:03:47.720 --> 00:03:48.720]   sure.
[00:03:48.720 --> 00:03:49.720]   So there's some things we do know.
[00:03:49.720 --> 00:03:56.080]   So for example, a lot of compression research is focused on unstructured sparsity.
[00:03:56.080 --> 00:04:00.240]   And what that means is that you're removing weights of a network rather than neurons,
[00:04:00.240 --> 00:04:01.240]   filters.
[00:04:01.240 --> 00:04:04.720]   And unstructured sparsity we know achieves high levels of compression.
[00:04:04.720 --> 00:04:07.560]   So a lot of people theoretically are very excited about it.
[00:04:07.560 --> 00:04:09.800]   But it doesn't translate to current hardware.
[00:04:09.800 --> 00:04:15.320]   Because as far as matrix multipliers are actually very expensive.
[00:04:15.320 --> 00:04:17.440]   So we know things like this.
[00:04:17.440 --> 00:04:19.920]   The hardware is stacked against certain ideas.
[00:04:19.920 --> 00:04:21.520]   But there's other things in the software.
[00:04:21.520 --> 00:04:23.720]   So for example, TF Lite was mentioned.
[00:04:23.720 --> 00:04:24.960]   TF Lite is wonderful.
[00:04:24.960 --> 00:04:29.480]   But in TF Lite, if you want to work in that type of compressed environment right now,
[00:04:29.480 --> 00:04:34.920]   you have to build your world, your entire algorithm, using less than 100 operations.
[00:04:34.920 --> 00:04:37.040]   I mean, it's fascinating.
[00:04:37.040 --> 00:04:42.280]   And so that really lays the groundwork for understanding how the choices we make at a
[00:04:42.280 --> 00:04:47.400]   hardware and software level ripple through and impact the algorithm.
[00:04:47.400 --> 00:04:52.200]   And what I realized was the following, is that even though I was stumbling and kind
[00:04:52.200 --> 00:04:58.080]   of peppering all these amazing teams at Google with these questions, historically, these
[00:04:58.080 --> 00:05:01.800]   communities have evolved totally in separation.
[00:05:01.800 --> 00:05:07.680]   For example, I would challenge anyone, go read a systems paper tonight.
[00:05:07.680 --> 00:05:12.160]   If you're an algorithms researcher, go read a systems paper.
[00:05:12.160 --> 00:05:17.520]   And please let me know if you fully comprehend it.
[00:05:17.520 --> 00:05:25.840]   It is a totally different body of knowledge, manner of publishing, manner of engaging.
[00:05:25.840 --> 00:05:30.360]   And that's because for a lot of computer science history, there's been very little incentive
[00:05:30.360 --> 00:05:36.320]   for these three groups, algorithms, software, and hardware to work together.
[00:05:36.320 --> 00:05:39.680]   And that is a lot of what the hardware lottery is about.
[00:05:39.680 --> 00:05:47.760]   The hardware lottery really starts with this grumpy statement, which is that for computer
[00:05:47.760 --> 00:05:54.400]   science history, hardware and software have frequently determined which ideas succeed
[00:05:54.400 --> 00:05:55.800]   and fail.
[00:05:55.800 --> 00:06:01.800]   And then a lot of the essay is unpacking that claim and saying, the reason why we say this
[00:06:01.800 --> 00:06:06.760]   is that we know from early computer science history, there have been all these occasions
[00:06:06.760 --> 00:06:10.240]   where the idea could not be executed.
[00:06:10.240 --> 00:06:12.760]   There was huge lags.
[00:06:12.760 --> 00:06:18.920]   So with the analytical machine, close to a century, but even more recently with deep
[00:06:18.920 --> 00:06:25.000]   neural networks, because the idea was proposed, but we didn't have the hardware and software
[00:06:25.000 --> 00:06:27.520]   to operationalize it.
[00:06:27.520 --> 00:06:29.760]   The reason-- oh, sorry.
[00:06:29.760 --> 00:06:30.760]   Go ahead, Charles.
[00:06:30.760 --> 00:06:32.760]   You look like you're going to say something.
[00:06:32.760 --> 00:06:33.760]   Oh, no.
[00:06:33.760 --> 00:06:34.760]   I was just thinking--
[00:06:34.760 --> 00:06:35.760]   You were just nodding.
[00:06:35.760 --> 00:06:36.760]   OK.
[00:06:36.760 --> 00:06:37.760]   Excellent.
[00:06:37.760 --> 00:06:40.760]   Well, I appreciate the nodding as I go through this.
[00:06:40.760 --> 00:06:43.160]   It was good feedback loop.
[00:06:43.160 --> 00:06:46.920]   And I think what's interesting is now we've laid the scene.
[00:06:46.920 --> 00:06:49.040]   So this is why I got into this essay.
[00:06:49.040 --> 00:06:54.760]   And a lot of what the first part is, is essentially talking about examples of this hardware lottery.
[00:06:54.760 --> 00:07:02.640]   The reason why I finally felt obliged to put it on archive-- and it's quite funny, because
[00:07:02.640 --> 00:07:06.400]   I put it on archive.
[00:07:06.400 --> 00:07:08.720]   This is very different from a typical research paper.
[00:07:08.720 --> 00:07:15.320]   So in many ways, it's been quite fun to see the variety of audience who is connected with
[00:07:15.320 --> 00:07:16.320]   it.
[00:07:16.320 --> 00:07:20.920]   But the reason why I felt compelled to do that was the second part of the essay, which
[00:07:20.920 --> 00:07:28.860]   is that really the position of this paper is that while the hardware lottery has existed,
[00:07:28.860 --> 00:07:36.600]   and it's dominated a lot of the rate of computer science progress, I posit that this, in fact,
[00:07:36.600 --> 00:07:41.080]   will become a bigger gap between the winners and losers.
[00:07:41.080 --> 00:07:45.280]   And the reason why is that our ecosystem of software and hardware is becoming even more
[00:07:45.280 --> 00:07:46.680]   fragmented.
[00:07:46.680 --> 00:07:54.080]   So we have, as the last talk very well captured, we're starting to get all these different
[00:07:54.080 --> 00:07:59.280]   tooling kits at the software level, but also with the advent of domain specialized hardware.
[00:07:59.280 --> 00:08:04.440]   And this is hardware which, in an interesting way, should have solved for the pain points
[00:08:04.440 --> 00:08:06.200]   of us developing in isolation.
[00:08:06.200 --> 00:08:12.080]   It's hardware where essentially the focus is on specializing for specific algorithms.
[00:08:12.080 --> 00:08:16.640]   But the tricky part is that because it's specializing for certain algorithms, if another
[00:08:16.640 --> 00:08:21.920]   new algorithm comes along, it becomes harder to take that hardware and show that the new
[00:08:21.920 --> 00:08:24.720]   algorithm is a success.
[00:08:24.720 --> 00:08:31.760]   And when you have this, when you have essentially more combinations, but more locked in combinations,
[00:08:31.760 --> 00:08:36.720]   and more effort going to certain, it becomes more costly to stray off the path, the beaten
[00:08:36.720 --> 00:08:38.440]   path of research.
[00:08:38.440 --> 00:08:42.320]   And right now the beaten path of research is deep neural networks.
[00:08:42.320 --> 00:08:47.120]   So deep neural networks came around in 2012.
[00:08:47.120 --> 00:08:51.800]   And essentially, it was an overnight empirical success.
[00:08:51.800 --> 00:08:58.480]   It was one of those rare snapshots in computer science history where everyone overnight switched.
[00:08:58.480 --> 00:09:03.800]   And that was mainly because there was not the hardware prior to 2012.
[00:09:03.800 --> 00:09:07.880]   There'd been iterations throughout the 2000s.
[00:09:07.880 --> 00:09:12.160]   But it was really GPUs coming along that allowed deep neural networks to train to the depth
[00:09:12.160 --> 00:09:16.160]   that they needed to show the empirical success.
[00:09:16.160 --> 00:09:22.680]   And for subsequent years, essentially, particularly for computer vision, and now across multiple
[00:09:22.680 --> 00:09:27.240]   domains, that is what has dominated.
[00:09:27.240 --> 00:09:30.080]   But there are clear limitations to deep neural networks.
[00:09:30.080 --> 00:09:33.560]   And there are clear signs that perhaps this is not the only way forward, particularly
[00:09:33.560 --> 00:09:36.400]   for-- and there's almost two lenses here.
[00:09:36.400 --> 00:09:40.040]   So there are many clear commercial use cases of deep neural networks.
[00:09:40.040 --> 00:09:44.040]   So I think, Charles, you must bring a lot of people on your show who are really excited
[00:09:44.040 --> 00:09:48.680]   about how do we translate this breakthrough to commercial use cases.
[00:09:48.680 --> 00:09:53.280]   And I feel like there we're just-- we're at the beginning of the road.
[00:09:53.280 --> 00:09:55.080]   It's a super fun road ahead.
[00:09:55.080 --> 00:09:58.320]   There's many different domains that we haven't yet enabled.
[00:09:58.320 --> 00:10:02.640]   I mean, this is part of the motivation for resource-constrained environments, why I'm
[00:10:02.640 --> 00:10:05.360]   so interested in that.
[00:10:05.360 --> 00:10:10.520]   But there's another side, which is that in terms of an overarching goal of furthering
[00:10:10.520 --> 00:10:17.880]   this history of artificial intelligence research, the clear limitations of deep neural networks
[00:10:17.880 --> 00:10:24.040]   are starting to show, one of which is that it's becoming increasingly costly to scale.
[00:10:24.040 --> 00:10:29.040]   They're very expensive models in the sense that we're throwing a lot of parameters at
[00:10:29.040 --> 00:10:32.000]   a problem for not much return.
[00:10:32.000 --> 00:10:36.840]   A good example of this is the enormous growth in weights of deep neural networks.
[00:10:36.840 --> 00:10:44.360]   So you go from inception V3 to inception V4, it's almost a doubling of weights.
[00:10:44.360 --> 00:10:48.560]   But you only get an extra 2 percentage points in test inaccuracy.
[00:10:48.560 --> 00:10:57.600]   So both the training cost as well as the decreasing returns are becoming apparent.
[00:10:57.600 --> 00:11:01.960]   But furthermore, there's key issues to deep neural networks that we don't see in other
[00:11:01.960 --> 00:11:07.280]   forms of biological intelligence, such as deep neural networks evidence catastrophic
[00:11:07.280 --> 00:11:12.440]   forgetting, which is a very snazzy word to mean that when you train deep neural networks
[00:11:12.440 --> 00:11:19.600]   on new data, it tends to forget what it was taught before, which humans don't have because
[00:11:19.600 --> 00:11:24.280]   we show this remarkable versatility to pick up skills over the course of our lifetime
[00:11:24.280 --> 00:11:29.880]   while retaining skills that we may have learned years ago.
[00:11:29.880 --> 00:11:37.280]   I think the other thing is that we show far more efficiency in terms of how we treat examples.
[00:11:37.280 --> 00:11:43.280]   So we often don't need to do a full forward and backward pass.
[00:11:43.280 --> 00:11:46.920]   In fact, most of what you are seeing right now, what I'm seeing right now, unless you
[00:11:46.920 --> 00:11:53.080]   do something crazy, like put your hand out here, is virtual simulation.
[00:11:53.080 --> 00:11:55.960]   Like I've already simulated what I expect you to do.
[00:11:55.960 --> 00:12:02.000]   And if that matches what my prediction is, I don't do a full forward pass of the inputs.
[00:12:02.000 --> 00:12:03.760]   And that's usually efficient.
[00:12:03.760 --> 00:12:07.400]   It makes us able to do far more with far less.
[00:12:07.400 --> 00:12:13.240]   Our brain runs on the power of an electric shaver, razor.
[00:12:13.240 --> 00:12:18.200]   And in contrast, GPT-3 costs $12 million strain.
[00:12:18.200 --> 00:12:27.160]   So all this to say, the limits of our current approach, the cliff of this hardware, software,
[00:12:27.160 --> 00:12:30.600]   algorithm combination are becoming apparent.
[00:12:30.600 --> 00:12:37.040]   And that's why this was fun to write and to think about because it also, in my own mind,
[00:12:37.040 --> 00:12:41.400]   forced me to think about what I would recommend to solve for this cliff.
[00:12:41.400 --> 00:12:46.400]   But I'm going to pause there because I kind of talked a bit and hopefully gave a flavor
[00:12:46.400 --> 00:12:47.400]   of the work.
[00:12:47.400 --> 00:12:49.640]   And maybe I'll return it to you.
[00:12:49.640 --> 00:12:58.520]   Yeah, thanks for that context, both sort of how you came to thinking about this problem
[00:12:58.520 --> 00:13:01.280]   and how you were thinking about it at a meta level.
[00:13:01.280 --> 00:13:08.080]   And in addition, sort of what your presentation of the contents of the paper is all very interesting.
[00:13:08.080 --> 00:13:16.520]   One thing that really stuck out to me, I think, was you referred a couple of times to biological
[00:13:16.520 --> 00:13:22.520]   intelligence as an alternative, as a source for ideas.
[00:13:22.520 --> 00:13:25.160]   And so my PhD is in neuroscience.
[00:13:25.160 --> 00:13:29.480]   I was at the Redwood Center for Theoretical Neuroscience, which is a group that does a
[00:13:29.480 --> 00:13:34.000]   lot of sort of looking to biology for inspiration to design algorithms.
[00:13:34.000 --> 00:13:38.240]   And I think, yeah, we often found there's kind of this struggle that a lot of people
[00:13:38.240 --> 00:13:44.280]   sort of feel, we learn to fly not by putting feathers on our wings, but by building jet
[00:13:44.280 --> 00:13:45.280]   engines.
[00:13:45.280 --> 00:13:49.200]   And that's like a refrain you hear often from folks who are, especially folks who are working
[00:13:49.200 --> 00:13:54.520]   in deep learning and don't think that we should get our inspiration from biological systems.
[00:13:54.520 --> 00:14:00.360]   So I'm curious, you know, what do you think, you mentioned predictive coding, I guess,
[00:14:00.360 --> 00:14:02.400]   what other kinds of things do you think we can learn?
[00:14:02.400 --> 00:14:06.020]   Like is it, you know, event based, you know, spike based models?
[00:14:06.020 --> 00:14:07.600]   Is it additional recurrence?
[00:14:07.600 --> 00:14:14.160]   Is it, is it, yeah, what is it that you think biological intelligence can offer us as ways
[00:14:14.160 --> 00:14:15.160]   forward?
[00:14:15.160 --> 00:14:19.080]   I think it offers us counterfactual data points, right?
[00:14:19.080 --> 00:14:25.880]   So right now, essentially, because, because this journey, this mission is so recent.
[00:14:25.880 --> 00:14:29.760]   So the Dartmouth Council was in 1946.
[00:14:29.760 --> 00:14:34.360]   And since then, that was what really pulled together people's first talking about transferring
[00:14:34.360 --> 00:14:38.120]   to machines skills normally reserved for humans.
[00:14:38.120 --> 00:14:43.560]   But we're still less than a century out from what when that occurred.
[00:14:43.560 --> 00:14:51.160]   In many ways, our search space is very small, we have, and it's primarily dictated what
[00:14:51.160 --> 00:14:56.520]   combination of ideas we can explore is primarily dictated by the stack, by the degrees of freedom
[00:14:56.520 --> 00:15:00.420]   that we lose as we first choose our hardware, then we choose our software.
[00:15:00.420 --> 00:15:06.600]   So my point with biological examples is not to suggest this is the way and in fact, I
[00:15:06.600 --> 00:15:13.760]   think I would be a foolish person to make that bet, particularly documented like this.
[00:15:13.760 --> 00:15:21.560]   But it's to say that we at least know there are things that we biologically do differently,
[00:15:21.560 --> 00:15:24.080]   that we are not doing with our current models.
[00:15:24.080 --> 00:15:29.800]   And the reason why that's important is that domain specialized hardware is really, really,
[00:15:29.800 --> 00:15:32.200]   really tailored to our current models.
[00:15:32.200 --> 00:15:39.320]   Essentially, the emphasis there is on making our current models commercially viable with
[00:15:39.320 --> 00:15:45.680]   a distant secondary consideration, distant, being future research directions.
[00:15:45.680 --> 00:15:50.720]   And so while I'm not going to take the bet that deep neural networks are not the way
[00:15:50.720 --> 00:15:58.480]   forward, I'm going to suggest that there are clearly multiple ways of mapping the world.
[00:15:58.480 --> 00:16:03.760]   For example, I work in computer vision a lot, thinking about the interpretability of computer
[00:16:03.760 --> 00:16:07.720]   vision, as well as the compression of computer vision models.
[00:16:07.720 --> 00:16:13.480]   Computer vision and CNNs in particular are interesting because we often for interpretability
[00:16:13.480 --> 00:16:19.280]   expect these models to extract importance in the same way that we do.
[00:16:19.280 --> 00:16:25.000]   However, there's no such constraint that we train these models with to do that.
[00:16:25.000 --> 00:16:30.600]   In many ways, the reason why CNNs demonstrate this remarkable ability, particularly on medical
[00:16:30.600 --> 00:16:36.800]   images to extract features that we can, is that our vision is log scale on purpose.
[00:16:36.800 --> 00:16:42.960]   We need a noticeable change in order to register something happening for our own robustness.
[00:16:42.960 --> 00:16:46.360]   There's no such constraint on deep neural networks for the same thing.
[00:16:46.360 --> 00:16:51.680]   In fact, that's why deep neural networks can extract these pixel wise differences.
[00:16:51.680 --> 00:16:59.200]   And so things like that, I often think it's not so much that we articulate it as this
[00:16:59.200 --> 00:17:03.280]   is the way, it's more that we need to increase our search space and make it cheaper to explore
[00:17:03.280 --> 00:17:04.280]   that search space.
[00:17:04.280 --> 00:17:05.280]   I see.
[00:17:05.280 --> 00:17:12.000]   I do want to ask one more question before we pass over to the variants of gradient work,
[00:17:12.000 --> 00:17:18.000]   which is like what I saw as a big impediment was basically a certain amount of like conservatism
[00:17:18.000 --> 00:17:23.560]   or on the part of granting agencies and of large companies that could possibly fund the
[00:17:23.560 --> 00:17:28.840]   kind of research necessary to break us out of like this current iteration of the hardware
[00:17:28.840 --> 00:17:33.680]   lottery that they were, you know, like on the one hand, you've got people doing really
[00:17:33.680 --> 00:17:37.400]   exciting new work constantly with neural networks.
[00:17:37.400 --> 00:17:40.400]   And then on the other hand, you have people who still are in the world of sort of, you
[00:17:40.400 --> 00:17:46.800]   know, toy models saying, oh, my, like, I think the, you know, oscillation based or optical
[00:17:46.800 --> 00:17:50.000]   networks or, you know, spiking networks are really cool.
[00:17:50.000 --> 00:17:51.520]   And that's what we should be spending money on.
[00:17:51.520 --> 00:17:56.920]   So do you think that there's a way that these institutions can be convinced at this, at
[00:17:56.920 --> 00:18:02.160]   this, you know, scale necessary to break us out of the out of this lottery?
[00:18:02.160 --> 00:18:05.840]   Or do you think that it do you have a sort of pessimistic view that this is just a feature
[00:18:05.840 --> 00:18:11.320]   of the way research is done, and we aren't going to break out of it?
[00:18:11.320 --> 00:18:14.920]   So it is a feature of how research is done, we're never going to eliminate the hardware
[00:18:14.920 --> 00:18:15.920]   lottery.
[00:18:15.920 --> 00:18:20.560]   But the main point of the essay is that right now it dominates computer science progress,
[00:18:20.560 --> 00:18:23.500]   and the degree to which it dominates is problematic.
[00:18:23.500 --> 00:18:28.320]   So in order for it to dominate less, we need to make it quicker to iterate.
[00:18:28.320 --> 00:18:34.320]   And quicker to iterate in the hardware space is extremely expensive, you need a lot of,
[00:18:34.320 --> 00:18:38.880]   you know, bucks, of whatever currency denomination you want to talk in.
[00:18:38.880 --> 00:18:41.120]   And that is the tricky part.
[00:18:41.120 --> 00:18:48.440]   So there needs to be more support from, essentially, I would say, governments, because private
[00:18:48.440 --> 00:18:54.880]   sector is always going to lean towards hardware that's optimized for commercial use cases.
[00:18:54.880 --> 00:18:59.600]   And that has its benefits, too, you need, you know, a mix of different people who are
[00:18:59.600 --> 00:19:01.520]   designing and thinking about these problems.
[00:19:01.520 --> 00:19:05.880]   But right now, the gap in terms of government expenditure in this area is painful.
[00:19:05.880 --> 00:19:09.160]   It's I mean, it's kind of fascinating.
[00:19:09.160 --> 00:19:15.480]   I think even the discrepancy between US expenditure and Chinese expenditure, China is so it's
[00:19:15.480 --> 00:19:19.440]   thrown so much more into this ring than the US has.
[00:19:19.440 --> 00:19:22.680]   And so it's really interesting to look at those dynamics as well.
[00:19:22.680 --> 00:19:25.840]   Software engineers have the highest burden on their shoulders.
[00:19:25.840 --> 00:19:28.840]   And at this point, we're just going to end up showing a slide from the other presentation.
[00:19:28.840 --> 00:19:29.840]   But that's okay.
[00:19:29.840 --> 00:19:32.880]   Because I think I think these topics are interesting.
[00:19:32.880 --> 00:19:37.000]   And you know, when you said it, let's do two, I just I the writing was on the wall.
[00:19:37.000 --> 00:19:45.240]   I was like, but no, it's software engineers, you have the biggest burden to play and biggest
[00:19:45.240 --> 00:19:47.040]   role to play in many ways.
[00:19:47.040 --> 00:19:54.000]   Because the tricky thing is, is that a lot of the bottleneck now is in the design of
[00:19:54.000 --> 00:19:55.000]   compilers.
[00:19:55.000 --> 00:19:58.000]   And it's also in the design of the software layers.
[00:19:58.000 --> 00:20:03.200]   And partly, it's making us understand what is the cost of using different hardware, because
[00:20:03.200 --> 00:20:06.600]   right now, it's just very costly to even try out different types of hardware.
[00:20:06.600 --> 00:20:11.560]   In my research flow, I basically have a single type of hardware that I use, if I need to
[00:20:11.560 --> 00:20:16.680]   release code and switch hardware, but I'm using a different type, it's, it's like multiple
[00:20:16.680 --> 00:20:22.640]   days of switching out operations, it's not something I do experiment to experiment.
[00:20:22.640 --> 00:20:29.080]   And that's a great example of a pain point, which is really an engineering pain point.
[00:20:29.080 --> 00:20:31.640]   The secondary thing is that we just need better feedback loops.
[00:20:31.640 --> 00:20:36.560]   I mean, error messages at a hardware level are painful.
[00:20:36.560 --> 00:20:43.560]   I mean, as a researcher, I want to care about hardware, but I don't have the time to because
[00:20:43.560 --> 00:20:48.440]   it really is not a precise enough feedback loop for it to be actionable.
[00:20:48.440 --> 00:20:52.200]   And I think that's another main point here is that despite hardware and software mattering
[00:20:52.200 --> 00:20:57.040]   so much, researchers do not talk about it.
[00:20:57.040 --> 00:21:00.920]   And the reason why is that it's quite rational, why we don't talk about it.
[00:21:00.920 --> 00:21:03.800]   It's because it's seen as something we can't change.
[00:21:03.800 --> 00:21:10.720]   It's seen as something so, so time consuming to try and influence that we almost abstracted
[00:21:10.720 --> 00:21:12.520]   away.
[00:21:12.520 --> 00:21:17.800]   And I think that that's the interesting part is how can we make it cheaper for us to engage
[00:21:17.800 --> 00:21:18.800]   in that conversation.
[00:21:18.800 --> 00:21:25.160]   I am going to put up the slide because I do want my lovely co author, Sharag, he's my
[00:21:25.160 --> 00:21:28.680]   first story, he's fantastic.
[00:21:28.680 --> 00:21:31.680]   So let me honest to goodness, really excited about this paper.
[00:21:31.680 --> 00:21:35.440]   So you know, if you can stick around for an extra five minutes, yeah, yeah, 10 minutes
[00:21:35.440 --> 00:21:36.440]   on it.
[00:21:36.440 --> 00:21:37.440]   Oh, okay.
[00:21:37.440 --> 00:21:44.200]   Yeah, I'm happy to, um, I'll at least put this up while we because we only have three
[00:21:44.200 --> 00:21:45.200]   minutes left.
[00:21:45.200 --> 00:21:46.560]   So we'll stay on the hardware lottery.
[00:21:46.560 --> 00:21:51.960]   And then I'll do a brief summary for the next for people who want to stick around.
[00:21:51.960 --> 00:21:57.240]   But yeah, the paper with Sharag, who is my lovely co first author, and this paper is
[00:21:57.240 --> 00:22:01.440]   about estimating example difficulty.
[00:22:01.440 --> 00:22:04.600]   And so I'm going to do a little bit of a hat switcheroo here.
[00:22:04.600 --> 00:22:10.680]   And I want to talk about really another property of beyond tested accuracy, which is how do
[00:22:10.680 --> 00:22:11.960]   we audit our models?
[00:22:11.960 --> 00:22:18.680]   And how do we surface an understanding of how the models learn a decision boundary to
[00:22:18.680 --> 00:22:25.720]   people who need to navigate and to make decisions about AI safety.
[00:22:25.720 --> 00:22:31.600]   And the big obstacle in terms of ranking examples by difficulty to date has been the computational
[00:22:31.600 --> 00:22:33.720]   cost, it's very expensive.
[00:22:33.720 --> 00:22:38.200]   And so the work with Sharag is really proposing a much cheaper measure to achieve this.
[00:22:38.200 --> 00:22:42.200]   In fact, what we leverage is the variance of the gradients during the training cycle
[00:22:42.200 --> 00:22:43.200]   as well.
[00:22:43.200 --> 00:22:48.180]   So it fits into your traditional training regime where you save checkpoints as you train,
[00:22:48.180 --> 00:22:52.920]   you can leverage those checkpoints to understand what examples are challenging and what are
[00:22:52.920 --> 00:22:56.720]   easy by looking at which gradients converge the fastest.
[00:22:56.720 --> 00:23:02.720]   And it turns out to be very effective at servicing the most problematic examples for humans take
[00:23:02.720 --> 00:23:08.120]   a look at either for further annotation or to audit for accountability and fairness.
[00:23:08.120 --> 00:23:11.520]   So that was very jazzy.
[00:23:11.520 --> 00:23:14.120]   I think I got that under two minutes.
[00:23:14.120 --> 00:23:15.120]   That's possible.
[00:23:15.120 --> 00:23:17.960]   That was that was quite a compact summary.
[00:23:17.960 --> 00:23:19.200]   Yeah, yeah.
[00:23:19.200 --> 00:23:20.960]   Well compressed, you know.
[00:23:20.960 --> 00:23:24.560]   So makes me wonder what that perhaps forgot.
[00:23:24.560 --> 00:23:32.200]   But yeah, I mean, if you want to spend like a little bit talking about the maybe showing
[00:23:32.200 --> 00:23:36.960]   some of the results or the key figures of the paper just in the you know, folks will
[00:23:36.960 --> 00:23:40.520]   stick around, I think for a couple minutes, and we post these on YouTube afterwards.
[00:23:40.520 --> 00:23:42.560]   And folks, I'm sure would be happy to sit.
[00:23:42.560 --> 00:23:43.560]   Oh, I see.
[00:23:43.560 --> 00:23:44.560]   Okay, so I can go rogue.
[00:23:44.560 --> 00:23:45.560]   Okay, you can do whatever.
[00:23:45.560 --> 00:23:49.920]   I'll go rogue.
[00:23:49.920 --> 00:23:51.920]   Maybe I'll even squeeze in eight minutes.
[00:23:51.920 --> 00:23:52.920]   That's fine.
[00:23:52.920 --> 00:23:53.920]   All right.
[00:23:53.920 --> 00:23:58.920]   So if so, yeah, absolutely.
[00:23:58.920 --> 00:24:05.000]   And a lot of what this research is focused on, and I guess I already spoke to this, but
[00:24:05.000 --> 00:24:09.200]   my research is really on this area of going beyond test accuracy.
[00:24:09.200 --> 00:24:15.760]   And that's because we know that, and perhaps I'll just present this.
[00:24:15.760 --> 00:24:16.760]   Lovely.
[00:24:16.760 --> 00:24:20.440]   Okay, we're rolling.
[00:24:20.440 --> 00:24:26.000]   And a lot of the motivation for this work is that we often have accuracy without true
[00:24:26.000 --> 00:24:27.320]   learning.
[00:24:27.320 --> 00:24:31.200]   And so top line metrics often hide critical model behavior.
[00:24:31.200 --> 00:24:36.560]   And that's really the role these interpretability tools in my mind is to provide intuition and
[00:24:36.560 --> 00:24:43.000]   to provide meaningful examples for humans to be able to audit a lot of the issues.
[00:24:43.000 --> 00:24:50.040]   And a lot of work to date on interpretability has focused on local explanations.
[00:24:50.040 --> 00:24:51.200]   And what is the difference?
[00:24:51.200 --> 00:24:56.160]   So local explanation, you may have seen a saliency map before.
[00:24:56.160 --> 00:25:01.320]   They're widely used by practitioners, where essentially, for a given example, you estimate
[00:25:01.320 --> 00:25:05.120]   what is important for the model prediction.
[00:25:05.120 --> 00:25:08.080]   So this is, I guess it's a sandwich.
[00:25:08.080 --> 00:25:09.520]   Is it a meat sandwich, perhaps?
[00:25:09.520 --> 00:25:12.600]   It's like a pulled meat.
[00:25:12.600 --> 00:25:14.000]   Some variety of sandwich.
[00:25:14.000 --> 00:25:17.400]   And the model is highlighted, these parts of the sandwich is contributing the most to
[00:25:17.400 --> 00:25:18.400]   the model prediction.
[00:25:18.400 --> 00:25:24.120]   And the reason why a lot of work on interpretability has been limited to these single examples,
[00:25:24.120 --> 00:25:26.080]   there's actually a few reasons.
[00:25:26.080 --> 00:25:28.280]   So one is the high dimensional input space.
[00:25:28.280 --> 00:25:32.960]   So it's just so expensive to try and scale feature importance across a data set, because
[00:25:32.960 --> 00:25:37.520]   even a single image may have a quarter of a million features.
[00:25:37.520 --> 00:25:43.840]   And the other is that, in fact, a lot of the onus has been on providing single end user
[00:25:43.840 --> 00:25:44.840]   explanations.
[00:25:44.840 --> 00:25:49.240]   So for example, if I'm in a doctor's office, I'm always going to want the explanation from
[00:25:49.240 --> 00:25:51.240]   my X-ray.
[00:25:51.240 --> 00:25:54.520]   And you can imagine multiple use cases in which you will always want to try and explain
[00:25:54.520 --> 00:25:56.520]   a single prediction.
[00:25:56.520 --> 00:26:03.160]   However, a lot of my earlier research was showing that, essentially, meaningful explanation
[00:26:03.160 --> 00:26:04.560]   does not always equate with reliable.
[00:26:04.560 --> 00:26:10.600]   And so you can have these saliency maps, which look very meaningful, but can be easily manipulated.
[00:26:10.600 --> 00:26:12.360]   And in fact, we show that.
[00:26:12.360 --> 00:26:17.160]   We show that it can be manipulated to show a kitty imposed on top of a seven, which is
[00:26:17.160 --> 00:26:18.880]   fun.
[00:26:18.880 --> 00:26:25.000]   And we also did work, which was showing that often these estimates of feature importance
[00:26:25.000 --> 00:26:28.040]   are no better than a random estimate.
[00:26:28.040 --> 00:26:32.600]   And so a lot of my recent research is focused on what next.
[00:26:32.600 --> 00:26:39.520]   And what next is really thought about what happens after a human sees a saliency map.
[00:26:39.520 --> 00:26:43.120]   What actionable decision do they make?
[00:26:43.120 --> 00:26:49.880]   And if I were to ask you, Chris, essentially, what would you, Charles, what would you do
[00:26:49.880 --> 00:26:55.360]   when you saw this, you may actually struggle to make a decision.
[00:26:55.360 --> 00:26:59.720]   Is this a good saliency map or is this a bad saliency map?
[00:26:59.720 --> 00:27:03.400]   And in fact, you may want to look at additional saliency maps.
[00:27:03.400 --> 00:27:09.600]   And this is normal because a lot of our understanding of what is good or bad is relative.
[00:27:09.600 --> 00:27:15.320]   And this is where it becomes tricky because can a human inspect all the saliency maps
[00:27:15.320 --> 00:27:20.560]   of a dataset, particularly with training datasets now that are extremely large?
[00:27:20.560 --> 00:27:27.440]   And so really this question of how do we surface what is important for a domain expert to understand
[00:27:27.440 --> 00:27:32.200]   and particularly to surface a tractable subset, something that they can actually, within their
[00:27:32.200 --> 00:27:37.840]   annotation budget or inspection budget, be able to understand the behavior of the model.
[00:27:37.840 --> 00:27:41.640]   And this is the work with Chirag, who's my lovely collaborator.
[00:27:41.640 --> 00:27:45.520]   And really the work revolves around how do we understand how feature importance forms
[00:27:45.520 --> 00:27:47.120]   of the course of training.
[00:27:47.120 --> 00:27:51.800]   So what you see here is there's very interesting research which suggests that there's distinct
[00:27:51.800 --> 00:27:53.800]   stages of training.
[00:27:53.800 --> 00:28:03.320]   So for example, the chart on your left, I believe, yes, your left, is critical learning
[00:28:03.320 --> 00:28:05.880]   periods and deep neural networks.
[00:28:05.880 --> 00:28:13.800]   And so in fact, this is a chart which shows that in fact, if you train a model on corrupted
[00:28:13.800 --> 00:28:19.320]   data early on, it can no longer converge to better data later on, suggesting that some
[00:28:19.320 --> 00:28:24.800]   special attributes that early stage of training that determine how feature importance is formed.
[00:28:24.800 --> 00:28:27.640]   And we leverage this fact in this work.
[00:28:27.640 --> 00:28:34.040]   And in fact, what we say is the hypothesis that we formulate is that for easy examples,
[00:28:34.040 --> 00:28:38.880]   the model will converge quickly on a fairly narrow range of gradients because it's consistently
[00:28:38.880 --> 00:28:42.400]   predicting the same prediction for that image.
[00:28:42.400 --> 00:28:45.920]   For hard examples, the model is trying to place the image at different parts of the
[00:28:45.920 --> 00:28:47.240]   decision boundary.
[00:28:47.240 --> 00:28:50.240]   And so in fact, it will keep on oscillating the gradients.
[00:28:50.240 --> 00:28:54.640]   And we'll see higher average variance in the gradients.
[00:28:54.640 --> 00:28:55.640]   And so that's what we do.
[00:28:55.640 --> 00:28:59.480]   We look at checkpoints of the course of training in the same way that you would typically save
[00:28:59.480 --> 00:29:02.400]   your checkpoints in a normal training regime.
[00:29:02.400 --> 00:29:05.520]   And then we compute the input gradients.
[00:29:05.520 --> 00:29:10.520]   And we look at the variance of that image consistently over the course of training.
[00:29:10.520 --> 00:29:17.120]   And what we find is that it actually provides a really useful ranking of each class.
[00:29:17.120 --> 00:29:19.160]   And how do we evaluate that?
[00:29:19.160 --> 00:29:23.920]   So we qualitatively visualize the lowest VOG and the highest.
[00:29:23.920 --> 00:29:28.280]   And you can kind of see that they have these very distinct semantic clustering traits.
[00:29:28.280 --> 00:29:34.480]   So lowest VOG has these much more, I guess, centered, crisp images.
[00:29:34.480 --> 00:29:37.360]   Highest VOG is much more cluttered.
[00:29:37.360 --> 00:29:43.000]   But we also look qualitatively.
[00:29:43.000 --> 00:29:48.480]   And we see that VOG is much more effective at discriminating between easy and challenging
[00:29:48.480 --> 00:29:49.800]   examples.
[00:29:49.800 --> 00:29:52.120]   So this is CFAR 100.
[00:29:52.120 --> 00:29:54.320]   And this is the bottom 10.
[00:29:54.320 --> 00:29:56.320]   So the bottom 10 percentile of VOG.
[00:29:56.360 --> 00:30:01.000]   And you can see it's much lower test set accuracy.
[00:30:01.000 --> 00:30:03.400]   And then-- oh, apologies.
[00:30:03.400 --> 00:30:05.520]   And then this is the top 10 percentile.
[00:30:05.520 --> 00:30:08.400]   And you can see it's much higher test set accuracy.
[00:30:08.400 --> 00:30:13.720]   And so VOG is doing a great job of ranking the entire data set so that it surfaces as
[00:30:13.720 --> 00:30:18.320]   high score the most difficult examples to evaluate.
[00:30:18.320 --> 00:30:22.880]   And the benefits is that it really fits into Patricia's current workflow.
[00:30:22.880 --> 00:30:26.120]   And you can leverage the checkpoints stored across training.
[00:30:26.120 --> 00:30:31.280]   I'm not going to talk about these other two works, which essentially try and get at this
[00:30:31.280 --> 00:30:34.480]   common theme of ranking and surfacing.
[00:30:34.480 --> 00:30:40.920]   But a lot of it is how do we leverage differences in models to surface what is worthy of what
[00:30:40.920 --> 00:30:44.960]   we're doing, which is a theme that I think is very important for interpretability, particularly
[00:30:44.960 --> 00:30:50.760]   as we try and apply it to production systems and figure out how to equip humans to check
[00:30:50.760 --> 00:30:55.360]   before they deploy if there are any potential issues with the model.
[00:30:55.360 --> 00:30:58.360]   So I'm going to stop there.
[00:30:58.360 --> 00:31:01.600]   And maybe I'll turn it back over to you, Charles.
[00:31:01.600 --> 00:31:06.080]   Yeah, I think maybe just a couple of questions before we go.
[00:31:06.080 --> 00:31:10.240]   I at least want to catch the US presidential debate.
[00:31:10.240 --> 00:31:16.240]   Hopefully they'll discuss how to appropriately respond to the hardware lottery and the need
[00:31:16.240 --> 00:31:19.520]   for additional investment in hardware.
[00:31:19.520 --> 00:31:23.160]   But I don't have high hopes that that will be discussed.
[00:31:23.160 --> 00:31:27.280]   A question from Swear Shah.
[00:31:27.280 --> 00:31:29.840]   Do you know whether this approach of looking at variance and gradients, would this work
[00:31:29.840 --> 00:31:32.080]   for models like semantic segmentation?
[00:31:32.080 --> 00:31:38.680]   Or do you think this is something that might be more specific to classification in computer
[00:31:38.680 --> 00:31:39.680]   vision?
[00:31:39.680 --> 00:31:43.600]   You can use it for semantic segmentation.
[00:31:43.600 --> 00:31:49.720]   The formulation just has to be changed a little because you end up predicting an area.
[00:31:49.720 --> 00:31:54.440]   And actually, there's an internal team at Google right now that's testing it for segmentation
[00:31:54.440 --> 00:31:55.440]   tasks.
[00:31:55.440 --> 00:32:03.040]   So partly, I believe we will be open sourcing the code within the next few weeks.
[00:32:03.040 --> 00:32:07.480]   So I think we could also include a variant for that.
[00:32:07.480 --> 00:32:08.480]   Great.
[00:32:08.480 --> 00:32:10.040]   Yeah, it'd be interesting to see.
[00:32:10.040 --> 00:32:17.720]   I guess the one other domain in which it can often be hard to translate from computer vision
[00:32:17.720 --> 00:32:21.960]   is the reinforcement learning, deep reinforcement learning.
[00:32:21.960 --> 00:32:25.280]   I feel like every time I look at the papers in that field, they're using different optimizers.
[00:32:25.280 --> 00:32:27.560]   And they're thinking about the problems very differently.
[00:32:27.560 --> 00:32:32.720]   So you think that looking at some sort of variance of reward or variance of prediction
[00:32:32.720 --> 00:32:37.680]   error or something like that might be able to translate this to figure out where RL agents
[00:32:37.680 --> 00:32:40.600]   are most confused?
[00:32:40.600 --> 00:32:47.680]   In some ways, the primary hypothesis of this is that the gradient itself, the variance
[00:32:47.680 --> 00:32:53.320]   of it, the degree to which it changes at different intervals is indicative of uncertainty.
[00:32:53.320 --> 00:32:58.680]   And that applies to any model that's trained in a gradient optimization-based approach,
[00:32:58.680 --> 00:33:05.440]   which is kind of all of them at this point, or at least a lot of state-of-the-art models
[00:33:05.440 --> 00:33:07.480]   in different fields.
[00:33:07.480 --> 00:33:10.880]   So I think it's interesting to think about.
[00:33:10.880 --> 00:33:15.240]   There, you naturally have just a lot more noise to begin with, because essentially,
[00:33:15.240 --> 00:33:17.280]   you also have task switching.
[00:33:17.280 --> 00:33:22.680]   But also, the intervals at which you do task switching are often not predetermined.
[00:33:22.680 --> 00:33:30.400]   So the one caveat I would say is that we do class-normalized variance of gradients, because
[00:33:30.400 --> 00:33:34.720]   essentially, we look at class rankings.
[00:33:34.720 --> 00:33:38.560]   It's far less clear how you would do that in a setting where you don't have a distinct
[00:33:38.560 --> 00:33:42.480]   class or you don't have a crisp sense of demarcation.
[00:33:42.480 --> 00:33:45.440]   But that's kind of cool to think about, because perhaps you don't need that.
[00:33:45.440 --> 00:33:48.320]   Maybe you can just use the roar variance.
[00:33:48.320 --> 00:33:50.640]   So yeah, I see you nodding, Charles.
[00:33:50.640 --> 00:33:54.080]   If you want to take this up as a research direction, let me know.
[00:33:54.080 --> 00:33:55.080]   I don't know.
[00:33:55.080 --> 00:33:57.640]   I don't have much time for research these days.
[00:33:57.640 --> 00:34:01.320]   I'm mostly doing webinars in which I ask people about their really cool research.
[00:34:01.320 --> 00:34:02.920]   So there's not so much time.
[00:34:02.920 --> 00:34:03.920]   Yeah, fair.
[00:34:03.920 --> 00:34:06.640]   You can't optimize for it all.
[00:34:06.640 --> 00:34:08.320]   Something has to give.
[00:34:08.320 --> 00:34:12.400]   I do want to-- I guess I got one more question before I let you go.
[00:34:12.400 --> 00:34:17.120]   So one thing I notice when I look at the distributions of my gradients, these are marginal distributions,
[00:34:17.120 --> 00:34:18.120]   so not class-specific.
[00:34:18.120 --> 00:34:21.000]   But I see this incredible degree of sparsity.
[00:34:21.000 --> 00:34:27.880]   I see almost like a Cauchy or a Laplace distribution, where it's almost all zero, and then there's
[00:34:27.880 --> 00:34:28.880]   heavy tails.
[00:34:28.880 --> 00:34:33.400]   Do you think that the higher moments, like skew and kurtosis, have anything to say?
[00:34:33.400 --> 00:34:38.440]   Or do you think that the variance of gradients basically captures all that there is to capture
[00:34:38.440 --> 00:34:41.080]   in those higher moments?
[00:34:41.080 --> 00:34:47.520]   Well, I guess in many ways what we're capturing with variance is time, right?
[00:34:47.520 --> 00:34:52.040]   So we're looking at different snapshots across training, which is really the information
[00:34:52.040 --> 00:34:53.720]   that's being conveyed.
[00:34:53.720 --> 00:34:58.200]   Because our primary assumption is that time, after a certain amount of time, the model
[00:34:58.200 --> 00:35:00.040]   has already learned the easy examples.
[00:35:00.040 --> 00:35:05.080]   And so the variance will be downweighted by the convergence over time.
[00:35:05.080 --> 00:35:08.200]   And for hard, it will just keep on oscillating.
[00:35:08.200 --> 00:35:11.560]   So in that sense, I don't know if a single snapshot is competitive.
[00:35:11.560 --> 00:35:14.960]   That's partly why I think we had such cool clustering.
[00:35:14.960 --> 00:35:20.520]   And it was so exciting, because this is a far cheaper way to get at much more expensive
[00:35:20.520 --> 00:35:24.360]   ways that have attempted in the past to do this by looking at a single snapshot.
[00:35:24.360 --> 00:35:30.080]   There's a fantastic paper which was about influence functions, which is about single
[00:35:30.080 --> 00:35:31.080]   snapshot.
[00:35:31.080 --> 00:35:33.080]   But it's extremely costly.
[00:35:33.080 --> 00:35:36.280]   There's another paper about C-score, which is super interesting, but it requires like
[00:35:36.280 --> 00:35:39.720]   20,000 retraining of your model.
[00:35:39.720 --> 00:35:46.480]   So this is in many ways-- the exciting part of this is that it's a cheap proxy to get
[00:35:46.480 --> 00:35:52.360]   at the single snapshot, much more complex formulation of the ranking.
[00:35:52.360 --> 00:35:53.360]   Gotcha.
[00:35:53.360 --> 00:35:54.360]   Yeah.
[00:35:54.360 --> 00:35:55.360]   Yeah.
[00:35:55.360 --> 00:36:00.760]   Maybe no need to overcomplicate it by calculating, say, the entropy of the gradient, when you
[00:36:00.760 --> 00:36:05.280]   can probably well approximate the behavior of that with something like the variance.
[00:36:05.280 --> 00:36:09.480]   Thank you so much, Sarah, for coming and being willing to talk about your work and answer
[00:36:09.480 --> 00:36:10.840]   everyone's questions.
[00:36:10.840 --> 00:36:11.840]   It was a real pleasure.
[00:36:11.840 --> 00:36:12.840]   Yes, lovely.
[00:36:12.840 --> 00:36:13.840]   Yeah, nice chatting.
[00:36:13.840 --> 00:36:14.840]   Bye, Charles.
[00:36:14.840 --> 00:36:15.840]   Take care.
[00:36:15.840 --> 00:36:16.840]   Bye.
[00:36:16.840 --> 00:36:17.840]   Bye.
[00:36:17.840 --> 00:36:17.840]   Bye.
[00:36:17.840 --> 00:36:18.840]   Bye.
[00:36:18.840 --> 00:36:18.840]   Bye.
[00:36:18.840 --> 00:36:19.840]   Bye.
[00:36:19.840 --> 00:36:19.840]   Bye.
[00:36:19.840 --> 00:36:20.840]   Bye.
[00:36:20.840 --> 00:36:20.840]   Bye.
[00:36:20.840 --> 00:36:21.840]   Bye.
[00:36:21.840 --> 00:36:21.840]   Bye.
[00:36:21.840 --> 00:36:22.840]   Bye.
[00:36:22.840 --> 00:36:22.840]   Bye.
[00:36:22.840 --> 00:36:32.840]   [BLANK_AUDIO]

