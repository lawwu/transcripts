
[00:00:00.000 --> 00:00:02.480]   "The following is a conversation with Cal Newport.
[00:00:02.480 --> 00:00:04.640]   "He's a friend and someone who's writing,
[00:00:04.640 --> 00:00:06.920]   "like his book, 'Deep Work' for example,
[00:00:06.920 --> 00:00:09.600]   "has guided how I strive to approach productivity
[00:00:09.600 --> 00:00:11.220]   "and life in general.
[00:00:11.220 --> 00:00:13.100]   "He doesn't use social media,
[00:00:13.100 --> 00:00:15.640]   "and in his book, 'Digital Minimalism',
[00:00:15.640 --> 00:00:17.760]   "he encourages people to find the right amount
[00:00:17.760 --> 00:00:21.120]   "of social media usage that provides value and joy.
[00:00:21.120 --> 00:00:24.700]   "He has a new book out called 'A World Without Email',
[00:00:24.700 --> 00:00:27.240]   "where he argues, brilliantly I would say,
[00:00:27.240 --> 00:00:30.120]   "that email is destroying productivity in companies
[00:00:30.120 --> 00:00:31.660]   "and in our lives.
[00:00:31.660 --> 00:00:35.500]   "And, very importantly, he offers solutions.
[00:00:35.500 --> 00:00:38.680]   "He is a computer scientist at Georgetown University
[00:00:38.680 --> 00:00:41.060]   "who practices what he preaches.
[00:00:41.060 --> 00:00:42.820]   "To do theoretical computer science
[00:00:42.820 --> 00:00:44.540]   "at the level that he does it,
[00:00:44.540 --> 00:00:46.780]   "you really have to live a focused life
[00:00:46.780 --> 00:00:48.300]   "that minimizes distractions
[00:00:48.300 --> 00:00:51.100]   "and maximizes hours of deep work.
[00:00:51.100 --> 00:00:54.340]   "Lastly, he's a host of an amazing podcast
[00:00:54.340 --> 00:00:57.600]   "called 'Deep Questions' that I highly recommend
[00:00:57.600 --> 00:01:00.720]   "for anyone who wants to improve their productive life.
[00:01:00.720 --> 00:01:02.840]   "Quick mention of our sponsors,
[00:01:02.840 --> 00:01:06.680]   "ExpressVPN, Linode Linux Virtual Machines,
[00:01:06.680 --> 00:01:09.000]   "Sun Basket Meal Delivery Service,
[00:01:09.000 --> 00:01:11.340]   "and Simply Safe Home Security.
[00:01:11.340 --> 00:01:13.360]   "Click the sponsor links to get a discount
[00:01:13.360 --> 00:01:15.520]   "and to support this podcast."
[00:01:15.520 --> 00:01:17.460]   As a side note, let me say that 'Deep Work'
[00:01:17.460 --> 00:01:20.800]   or long periods of deep, focused thinking
[00:01:20.800 --> 00:01:22.840]   have been something I've been chasing more and more
[00:01:22.840 --> 00:01:24.560]   over the past few years.
[00:01:24.560 --> 00:01:28.080]   Deep work is hard, but it's ultimately the thing
[00:01:28.080 --> 00:01:30.960]   that makes life so damn amazing.
[00:01:30.960 --> 00:01:33.820]   The ability to create things you're passionate about
[00:01:33.820 --> 00:01:35.880]   in a flow state where the distractions
[00:01:35.880 --> 00:01:37.960]   of the world just fade away.
[00:01:37.960 --> 00:01:41.560]   Social media, yes, reading the comments,
[00:01:41.560 --> 00:01:43.520]   yes, I still read the comments,
[00:01:43.520 --> 00:01:46.960]   is a source of joy for me in strict moderation.
[00:01:46.960 --> 00:01:49.260]   Too much takes away the focused mind,
[00:01:49.260 --> 00:01:51.600]   and too little, at least I think,
[00:01:51.600 --> 00:01:53.760]   takes away all of the fun.
[00:01:53.760 --> 00:01:57.000]   We need both, the focus and the fun.
[00:01:57.000 --> 00:01:59.320]   If you enjoy this thing, subscribe on YouTube,
[00:01:59.320 --> 00:02:02.760]   review it on Apple Podcast, follow on Spotify,
[00:02:02.760 --> 00:02:06.100]   support on Patreon, or connect with me on Twitter
[00:02:06.100 --> 00:02:08.760]   at Lex Friedman if you can only figure out
[00:02:08.760 --> 00:02:10.120]   how to spell that.
[00:02:10.120 --> 00:02:13.980]   And now, here's my conversation with Cal Newport.
[00:02:13.980 --> 00:02:16.640]   What is deep work?
[00:02:16.640 --> 00:02:18.960]   Let's start with the big question.
[00:02:18.960 --> 00:02:20.480]   - So, I mean, it's my term
[00:02:20.480 --> 00:02:23.680]   for when you're focusing without distraction
[00:02:23.680 --> 00:02:25.880]   on a cognitively demanding task,
[00:02:25.880 --> 00:02:28.520]   which is something we've all done,
[00:02:28.520 --> 00:02:31.720]   but we had never really given it a name, necessarily,
[00:02:31.720 --> 00:02:33.520]   that was separate from other type of work.
[00:02:33.520 --> 00:02:36.560]   And so, I gave it a name and said,
[00:02:36.560 --> 00:02:38.480]   let's compare that to other types of efforts
[00:02:38.480 --> 00:02:40.680]   you might do while you're working
[00:02:40.680 --> 00:02:42.320]   and see that the deep work efforts
[00:02:42.320 --> 00:02:44.320]   actually have a huge benefit
[00:02:44.320 --> 00:02:45.840]   that we might be underestimating.
[00:02:45.840 --> 00:02:49.000]   - What does it mean to work deeply on something?
[00:02:49.000 --> 00:02:51.840]   - I had been calling it hard focus
[00:02:51.840 --> 00:02:54.360]   in my writing before that.
[00:02:54.360 --> 00:02:55.720]   Well, so the context you would understand,
[00:02:55.720 --> 00:02:58.400]   I was in the theory group in CSAIL at MIT, right?
[00:02:58.400 --> 00:03:00.000]   So I was surrounded at the time
[00:03:00.000 --> 00:03:01.840]   when I was coming up with these ideas
[00:03:01.840 --> 00:03:03.560]   by these professional theoreticians.
[00:03:03.560 --> 00:03:06.920]   And that's like a murderer's row of thinkers there, right?
[00:03:06.920 --> 00:03:08.960]   I mean, it's like Turing Award, Turing Award,
[00:03:08.960 --> 00:03:10.120]   MacArthur, Turing Award.
[00:03:10.120 --> 00:03:12.720]   I mean, you know the crew, right?
[00:03:12.720 --> 00:03:13.920]   - Theoretical computer science.
[00:03:13.920 --> 00:03:15.880]   - Theoretical computer science, yeah, yeah.
[00:03:15.880 --> 00:03:18.120]   So I'm in the theory group, right?
[00:03:18.120 --> 00:03:20.000]   Doing theoretical computer science.
[00:03:20.000 --> 00:03:21.400]   And I publish a book.
[00:03:21.400 --> 00:03:25.400]   So I was in this milieu where I was being exposed to people
[00:03:25.400 --> 00:03:27.720]   where focus was their tier one skill.
[00:03:27.720 --> 00:03:29.080]   Like that's what you would talk about, right?
[00:03:29.080 --> 00:03:31.520]   Like how intensely I can focus,
[00:03:31.520 --> 00:03:33.400]   that was the key skill.
[00:03:33.400 --> 00:03:34.840]   It's like your 440 time or something
[00:03:34.840 --> 00:03:37.160]   if you were an athlete, right?
[00:03:37.160 --> 00:03:39.120]   - So this is something that people were actually,
[00:03:39.120 --> 00:03:41.720]   the theory folks are thinking about?
[00:03:41.720 --> 00:03:42.560]   - Oh yeah.
[00:03:42.560 --> 00:03:43.400]   - Really? - Oh yeah.
[00:03:43.400 --> 00:03:45.880]   - Like they're openly discussing like, how do you focus?
[00:03:46.040 --> 00:03:48.520]   I mean, I don't know if they would quantify it,
[00:03:48.520 --> 00:03:51.560]   but focus was the tier one skill.
[00:03:51.560 --> 00:03:53.720]   So you would come in, here'd be a typical day.
[00:03:53.720 --> 00:03:56.480]   You'd come in and Eric Domain would be sitting
[00:03:56.480 --> 00:03:58.360]   in front of a whiteboard, right?
[00:03:58.360 --> 00:04:00.200]   With a whole group of visitors
[00:04:00.200 --> 00:04:01.280]   who had come to work with them.
[00:04:01.280 --> 00:04:03.680]   And maybe they projected like a grid on there
[00:04:03.680 --> 00:04:06.440]   because they're working on some graph theory problem.
[00:04:06.440 --> 00:04:10.040]   You go to lunch, you go to the gym, you come back,
[00:04:10.040 --> 00:04:12.960]   they're sitting there staring at the same whiteboard, right?
[00:04:12.960 --> 00:04:14.240]   Like that's the tier one skill.
[00:04:14.240 --> 00:04:16.120]   - This is the difference between different disciplines.
[00:04:16.120 --> 00:04:20.640]   Like I often feel for many reasons like a fraud,
[00:04:20.640 --> 00:04:22.760]   but I definitely feel like a fraud when I hang out
[00:04:22.760 --> 00:04:25.440]   with like either mathematicians or physicists.
[00:04:25.440 --> 00:04:29.000]   It's like, it feels like they're doing the legit work
[00:04:29.000 --> 00:04:30.200]   because when you talk,
[00:04:30.200 --> 00:04:32.720]   closer in computer science, you get to programming
[00:04:32.720 --> 00:04:33.840]   or like machine learning,
[00:04:33.840 --> 00:04:38.400]   like the experimental machine learning
[00:04:38.400 --> 00:04:41.120]   or like just the engineering version of it.
[00:04:41.120 --> 00:04:44.560]   It feels like you're gone so far away
[00:04:44.560 --> 00:04:47.880]   from what's required to solve something fundamental
[00:04:47.880 --> 00:04:49.000]   about this universe.
[00:04:49.000 --> 00:04:51.160]   It feels like you're just like cheating your way
[00:04:51.160 --> 00:04:53.560]   into like some kind of trick to figure out
[00:04:53.560 --> 00:04:56.200]   how to solve a problem in this one particular case.
[00:04:56.200 --> 00:04:57.400]   That's how it feels.
[00:04:57.400 --> 00:05:02.840]   I'd be interested to hear what you think about that
[00:05:02.840 --> 00:05:07.040]   because programming doesn't always feel
[00:05:07.040 --> 00:05:11.440]   like you need to think deeply, to work deeply,
[00:05:11.440 --> 00:05:12.840]   but sometimes it does.
[00:05:12.840 --> 00:05:14.360]   So it's a weird dance.
[00:05:14.360 --> 00:05:15.760]   - For sure code does, right?
[00:05:15.760 --> 00:05:17.560]   I mean, especially if you're coming up
[00:05:17.560 --> 00:05:20.680]   with original algorithmic designs,
[00:05:20.680 --> 00:05:22.720]   I think it's a great example of deep work.
[00:05:22.720 --> 00:05:24.960]   I mean, yeah, the hardcore theoreticians,
[00:05:24.960 --> 00:05:26.480]   they push it to an extreme.
[00:05:26.480 --> 00:05:29.040]   I mean, I think it's like knowing
[00:05:29.040 --> 00:05:31.080]   that athletic endeavor is good
[00:05:31.080 --> 00:05:33.800]   and then hanging out with a Olympic athlete.
[00:05:33.800 --> 00:05:35.680]   Like, oh, I see that's what it is.
[00:05:35.680 --> 00:05:37.080]   Now for the grad students like me,
[00:05:37.080 --> 00:05:38.680]   we're not anywhere near that level,
[00:05:38.680 --> 00:05:41.520]   but the faculty in that group,
[00:05:41.520 --> 00:05:44.600]   these were the cognitive Olympic athletes.
[00:05:44.600 --> 00:05:48.040]   But coding, I think, is a classic example of deep work
[00:05:48.040 --> 00:05:50.840]   because I got this problem I wanna solve.
[00:05:50.840 --> 00:05:52.160]   I have all of these tools
[00:05:52.160 --> 00:05:55.760]   and I have to combine them somehow creatively
[00:05:55.760 --> 00:05:56.600]   and on the fly.
[00:05:56.600 --> 00:05:58.880]   So basically I had been exposed to that.
[00:05:58.880 --> 00:06:00.800]   So I was used to this notion when I was in grad school
[00:06:00.800 --> 00:06:03.280]   and I was writing my blog, I'd write about hard focus.
[00:06:03.280 --> 00:06:05.000]   You know, that was the term I used.
[00:06:05.000 --> 00:06:07.720]   Then I published this book, "So Good They Can't Ignore You,"
[00:06:07.720 --> 00:06:09.360]   which came out in 2012.
[00:06:09.360 --> 00:06:12.120]   So like right as I began as a professor.
[00:06:12.120 --> 00:06:13.480]   And that book had this notion
[00:06:13.480 --> 00:06:16.480]   of skill being really important for career satisfaction,
[00:06:16.480 --> 00:06:19.320]   that it's not just following your passion.
[00:06:19.320 --> 00:06:21.000]   You have to actually really get good at something
[00:06:21.000 --> 00:06:22.480]   and then you use that skills as leverage.
[00:06:22.480 --> 00:06:24.360]   And there was this big follow-up question to that book
[00:06:24.360 --> 00:06:27.080]   of, okay, well, how do I get really good at this?
[00:06:27.080 --> 00:06:28.960]   And then I look back to my grad school experience.
[00:06:28.960 --> 00:06:31.480]   I was like, huh, there was this focus thing
[00:06:31.480 --> 00:06:32.320]   that we used to do.
[00:06:32.320 --> 00:06:35.400]   I wonder how generally applicable that is
[00:06:35.400 --> 00:06:36.280]   into the knowledge sector.
[00:06:36.280 --> 00:06:38.280]   And so as I started thinking about it,
[00:06:38.280 --> 00:06:40.320]   it became clear there's this interesting storyline
[00:06:40.320 --> 00:06:41.440]   that emerged that, okay,
[00:06:41.440 --> 00:06:44.000]   actually undistracted concentration is not just important
[00:06:44.000 --> 00:06:46.280]   for esoteric theoreticians.
[00:06:46.280 --> 00:06:47.440]   It's important here, it's important here,
[00:06:47.440 --> 00:06:48.680]   it's important here.
[00:06:48.680 --> 00:06:51.160]   And that involved into the deep work hypothesis,
[00:06:51.160 --> 00:06:55.160]   which is across the whole knowledge work sector,
[00:06:55.160 --> 00:06:56.480]   focus is very important
[00:06:56.480 --> 00:06:58.480]   and we've accidentally created circumstances
[00:06:58.480 --> 00:07:00.440]   where we just don't do a lot of it.
[00:07:00.440 --> 00:07:03.680]   - So focus is the sort of prerequisite for basically,
[00:07:03.680 --> 00:07:04.520]   you say knowledge work,
[00:07:04.520 --> 00:07:07.040]   but basically any kind of skill acquisition,
[00:07:07.040 --> 00:07:09.320]   any kind of major effort in this world.
[00:07:09.320 --> 00:07:11.640]   Can we break that apart a little bit?
[00:07:11.640 --> 00:07:15.280]   - Yeah, so a key aspect of focus
[00:07:15.280 --> 00:07:17.840]   is not just that you're concentrating hard on something,
[00:07:17.840 --> 00:07:20.080]   but you do it without distraction.
[00:07:20.080 --> 00:07:24.720]   So a big theme of my work is that context shifting
[00:07:24.720 --> 00:07:27.000]   kills the human capacity to think.
[00:07:27.000 --> 00:07:29.520]   So if I change what I'm paying attention to
[00:07:29.520 --> 00:07:31.120]   to something different,
[00:07:31.120 --> 00:07:32.000]   really, even if it's brief
[00:07:32.000 --> 00:07:34.280]   and then try to bring it back to the main thing I'm doing,
[00:07:34.280 --> 00:07:36.040]   that causes a huge cognitive pileup
[00:07:36.040 --> 00:07:38.120]   that makes it very hard to think clearly.
[00:07:38.120 --> 00:07:40.400]   So even if you think, okay, look, I'm writing this code
[00:07:40.400 --> 00:07:43.640]   or I'm writing this essay and I'm not multitasking
[00:07:43.640 --> 00:07:47.120]   and all my windows are closed and I have no notifications on
[00:07:47.120 --> 00:07:50.200]   but every five or six minutes, you quickly check
[00:07:50.200 --> 00:07:51.920]   like an inbox or your phone,
[00:07:51.920 --> 00:07:54.240]   that initiates a context shift in your brain, right?
[00:07:54.240 --> 00:07:55.960]   We're gonna start to suppress some neural networks,
[00:07:55.960 --> 00:07:57.440]   we're gonna try to amplify some others.
[00:07:57.440 --> 00:07:59.280]   It's a pretty complicated process, actually.
[00:07:59.280 --> 00:08:02.280]   There's a sort of neurological cascade that happens.
[00:08:02.280 --> 00:08:04.440]   You rip yourself away from that halfway through
[00:08:04.440 --> 00:08:05.520]   and go back to what you're doing.
[00:08:05.520 --> 00:08:07.160]   And now it's trying to switch back to the original thing,
[00:08:07.160 --> 00:08:08.800]   even though it's also in your brain's in the process
[00:08:08.800 --> 00:08:09.960]   of switching to these emails
[00:08:09.960 --> 00:08:11.760]   and trying to understand those contexts.
[00:08:11.760 --> 00:08:14.640]   And as a result, your ability to think clearly
[00:08:14.640 --> 00:08:16.440]   just goes really down.
[00:08:16.440 --> 00:08:17.360]   And it's fatiguing too.
[00:08:17.360 --> 00:08:19.480]   I mean, you do this long enough, you get midday
[00:08:19.480 --> 00:08:22.120]   and you're like, okay, I can't think anymore.
[00:08:22.120 --> 00:08:23.280]   You've exhausted yourself.
[00:08:23.280 --> 00:08:27.960]   - Is there some kind of perfect number of minutes,
[00:08:27.960 --> 00:08:28.880]   would you say?
[00:08:28.880 --> 00:08:32.280]   So we're talking about focusing on a particular task
[00:08:32.280 --> 00:08:37.200]   for one minute, five minutes, 10 minutes, 30 minutes.
[00:08:37.200 --> 00:08:40.120]   Is it possible to kind of context switch
[00:08:40.120 --> 00:08:44.240]   while maintaining deep focus every 20 minutes or so?
[00:08:44.240 --> 00:08:46.560]   So if you're thinking of like this,
[00:08:46.560 --> 00:08:48.680]   again, maybe it's a selfish kind of perspective,
[00:08:48.680 --> 00:08:50.400]   but if you think about programming,
[00:08:50.400 --> 00:08:53.680]   you're focused on a particular design of a little bit,
[00:08:53.680 --> 00:08:55.880]   maybe a small scale on a particular function
[00:08:55.880 --> 00:08:59.560]   or large scale on a system.
[00:08:59.560 --> 00:09:02.840]   And then the shift of focus happens like this,
[00:09:02.840 --> 00:09:04.960]   which is like, wait a minute,
[00:09:04.960 --> 00:09:07.400]   is there a library that can achieve this little task
[00:09:07.400 --> 00:09:08.280]   or something like that?
[00:09:08.280 --> 00:09:10.200]   And then you have to look it up.
[00:09:10.200 --> 00:09:11.480]   This is the danger zone.
[00:09:11.480 --> 00:09:13.040]   You go to the internets.
[00:09:13.040 --> 00:09:17.040]   And so you have to, now it is a kind of context switch
[00:09:17.040 --> 00:09:20.000]   because as opposed to thinking about the particular problem,
[00:09:20.000 --> 00:09:25.000]   you now have switched thinking about like consuming
[00:09:25.000 --> 00:09:27.920]   and integrating knowledge that's out there
[00:09:27.920 --> 00:09:31.160]   that can plug into your solution to a particular problem.
[00:09:31.160 --> 00:09:33.560]   It definitely feels like a context switch,
[00:09:33.560 --> 00:09:36.000]   but is that a really bad thing to do?
[00:09:36.000 --> 00:09:38.560]   So should you be setting it aside always
[00:09:38.560 --> 00:09:42.160]   and really trying to as much as possible go deep
[00:09:42.160 --> 00:09:45.960]   and stay there for like a really long period of time?
[00:09:45.960 --> 00:09:48.680]   - Well, I mean, I think if you're looking up a library
[00:09:48.680 --> 00:09:51.360]   that's relevant to what you're doing, that's probably okay.
[00:09:51.360 --> 00:09:52.480]   And I don't know that I would count that
[00:09:52.480 --> 00:09:53.720]   as a full context shift
[00:09:53.720 --> 00:09:56.520]   because the semantic networks involved
[00:09:56.520 --> 00:09:58.040]   are relatively similar, right?
[00:09:58.040 --> 00:10:00.520]   You're thinking about this type of solution.
[00:10:00.520 --> 00:10:01.640]   You're thinking about coding.
[00:10:01.640 --> 00:10:03.600]   You're thinking about this type of functions.
[00:10:03.600 --> 00:10:04.620]   Where you're really gonna get hit
[00:10:04.620 --> 00:10:08.000]   is if you switch your context to something that's different
[00:10:08.000 --> 00:10:09.800]   and if there's unresolved obligation.
[00:10:09.800 --> 00:10:11.720]   So really the worst possible thing you could do
[00:10:11.720 --> 00:10:14.360]   would be to look at like an email inbox, right?
[00:10:14.360 --> 00:10:16.040]   'Cause here's 20 emails.
[00:10:16.040 --> 00:10:18.020]   I can't answer most of these right now.
[00:10:18.020 --> 00:10:19.560]   They're completely different.
[00:10:19.560 --> 00:10:21.440]   Like the context of these emails, like, okay,
[00:10:21.440 --> 00:10:23.360]   there's a grant funding issue or something like this.
[00:10:23.360 --> 00:10:25.520]   It's very different than the coding I'm doing.
[00:10:25.520 --> 00:10:27.200]   And I'm leaving it unresolved.
[00:10:27.200 --> 00:10:29.200]   So it's like, someone needs something from me
[00:10:29.200 --> 00:10:30.680]   and I'm gonna try to pull my attention back.
[00:10:30.680 --> 00:10:31.840]   The second worst would be something
[00:10:31.840 --> 00:10:33.360]   that's emotionally arousing.
[00:10:33.360 --> 00:10:35.180]   So if you're like, let me just glance over at Twitter.
[00:10:35.180 --> 00:10:37.520]   I'm sure it's nice and calm and peaceful over there, right?
[00:10:37.520 --> 00:10:38.560]   That could be devastating
[00:10:38.560 --> 00:10:39.800]   because you're gonna expose yourself
[00:10:39.800 --> 00:10:41.560]   to something that's emotionally arousing.
[00:10:41.560 --> 00:10:43.920]   That's gonna completely mess up the cognitive plateau there.
[00:10:43.920 --> 00:10:45.040]   And then when you come back to,
[00:10:45.040 --> 00:10:47.840]   okay, let me try to code again, it's really difficult.
[00:10:47.840 --> 00:10:50.100]   - So it's both the information and the emotion.
[00:10:50.100 --> 00:10:53.420]   Yeah, both can be killers if what you're trying to do.
[00:10:53.420 --> 00:10:55.940]   So I would recommend at least an hour at a time
[00:10:55.940 --> 00:10:57.600]   'cause it could take up to 20 minutes
[00:10:57.600 --> 00:10:59.320]   to completely clear out the residue
[00:10:59.320 --> 00:11:02.660]   from whatever it was you were thinking about before.
[00:11:02.660 --> 00:11:04.040]   So if you're coding for 30 minutes,
[00:11:04.040 --> 00:11:05.700]   you might only be getting 10 or 15 minutes
[00:11:05.700 --> 00:11:08.860]   of actual sort of peak lex going on there, right?
[00:11:08.860 --> 00:11:11.840]   So an hour, at least you get a good 40, 45 minutes plus.
[00:11:11.840 --> 00:11:15.540]   I'm partial to 90 minutes as a really good chunk.
[00:11:15.540 --> 00:11:18.620]   We can get a lot done, but just before you get exhausted,
[00:11:18.620 --> 00:11:21.240]   you can sort of pull back a little bit.
[00:11:21.240 --> 00:11:22.880]   - Yeah, and one of the beautiful,
[00:11:22.880 --> 00:11:27.040]   and people can read about it in your book, "Deep Work,"
[00:11:27.040 --> 00:11:29.540]   but, and I know this has been out for a long time
[00:11:29.540 --> 00:11:31.960]   and people are probably familiar with many of the concepts,
[00:11:31.960 --> 00:11:33.800]   but it's still pretty profound
[00:11:33.800 --> 00:11:35.880]   and it has stayed with me for a long time.
[00:11:35.880 --> 00:11:39.400]   There's something about adding the terms to it
[00:11:39.400 --> 00:11:42.160]   that actually solidifies the concepts.
[00:11:42.160 --> 00:11:44.360]   Like words matter, it's pretty cool.
[00:11:44.360 --> 00:11:47.640]   And just for me, sort of as a comment,
[00:11:48.600 --> 00:11:52.680]   there's, it's a struggle and it's very difficult
[00:11:52.680 --> 00:11:56.160]   to maintain focus for a prolonged period of time,
[00:11:56.160 --> 00:11:59.360]   but the days on which I'm able to accomplish
[00:11:59.360 --> 00:12:03.360]   several hours of that kind of work, I'm happy.
[00:12:03.360 --> 00:12:05.540]   So forget being productive and all that.
[00:12:05.540 --> 00:12:07.880]   I'm just satisfied with my life.
[00:12:07.880 --> 00:12:10.600]   I feel fulfilled.
[00:12:10.600 --> 00:12:11.560]   It's like joyful.
[00:12:11.560 --> 00:12:14.440]   And then I can be, I'm less of a dick
[00:12:14.440 --> 00:12:16.680]   to other people in my life afterwards.
[00:12:17.560 --> 00:12:19.120]   It's a beautiful thing.
[00:12:19.120 --> 00:12:22.280]   And I find the opposite.
[00:12:22.280 --> 00:12:25.480]   When I don't do that kind of thing, I'm much more irritable.
[00:12:25.480 --> 00:12:27.320]   Like I feel like I didn't accomplish anything
[00:12:27.320 --> 00:12:29.160]   and there's this stress that then,
[00:12:29.160 --> 00:12:31.760]   the negative emotion builds up to where you're no longer
[00:12:31.760 --> 00:12:35.480]   able to sort of enjoy a lot of this amazing life.
[00:12:35.480 --> 00:12:37.880]   So in that sense, "Deep Work" has been a source
[00:12:37.880 --> 00:12:39.640]   of a lot of happiness.
[00:12:39.640 --> 00:12:42.280]   I'd love to ask you, how do you,
[00:12:42.280 --> 00:12:43.480]   again, you cover this in the book,
[00:12:43.480 --> 00:12:46.480]   but how do you integrate "Deep Work" into your life?
[00:12:46.480 --> 00:12:48.560]   What are different scheduling strategies
[00:12:48.560 --> 00:12:51.160]   that you would recommend just at a high level?
[00:12:51.160 --> 00:12:52.560]   What are different ideas there?
[00:12:52.560 --> 00:12:55.320]   - Well, I mean, I'm a big fan of time blocking.
[00:12:55.320 --> 00:12:58.480]   So if you're facing your workday,
[00:12:58.480 --> 00:13:02.800]   don't allow your inbox or to-do list to sort of drive you.
[00:13:02.800 --> 00:13:04.000]   Don't just come into your day and think,
[00:13:04.000 --> 00:13:05.840]   "What do I wanna do next?"
[00:13:05.840 --> 00:13:06.720]   I mean, I'm a big planner saying,
[00:13:06.720 --> 00:13:09.640]   "Here's the time available.
[00:13:09.640 --> 00:13:11.480]   "Let me make a plan for it."
[00:13:11.480 --> 00:13:13.360]   So I have a meeting here, I have an appointment here.
[00:13:13.360 --> 00:13:14.200]   Here's what's left.
[00:13:14.200 --> 00:13:15.400]   What do I actually wanna do with it?
[00:13:15.400 --> 00:13:17.960]   So in this half hour, I'm gonna work on this.
[00:13:17.960 --> 00:13:19.680]   For this 90-minute block, I'm gonna work on that.
[00:13:19.680 --> 00:13:21.320]   And during this hour, I'm gonna try to fit this in.
[00:13:21.320 --> 00:13:22.680]   And then actually I have this half hour gap
[00:13:22.680 --> 00:13:23.520]   between two meetings.
[00:13:23.520 --> 00:13:24.680]   So why don't I take advantage of that
[00:13:24.680 --> 00:13:25.800]   to go run five errands?
[00:13:25.800 --> 00:13:27.720]   I can kind of batch those together.
[00:13:27.720 --> 00:13:30.120]   But blocking out in advance,
[00:13:30.120 --> 00:13:32.480]   this is what I wanna do with the time available.
[00:13:32.480 --> 00:13:33.720]   I mean, I find that's much more effective.
[00:13:33.720 --> 00:13:34.600]   Now, once you're doing this,
[00:13:34.600 --> 00:13:36.320]   once you're in a discipline of time blocking,
[00:13:36.320 --> 00:13:37.800]   it's much easier to actually see,
[00:13:37.800 --> 00:13:40.160]   this is where I want, for example, to deep work.
[00:13:40.160 --> 00:13:41.840]   And I can get a handle on the other things
[00:13:41.840 --> 00:13:44.880]   that need to happen and find better places to fit them
[00:13:44.880 --> 00:13:46.080]   so I can prioritize this.
[00:13:46.080 --> 00:13:48.520]   And you're gonna get a lot more of that done
[00:13:48.520 --> 00:13:49.680]   than if it's just going through your day
[00:13:49.680 --> 00:13:51.400]   and saying, what's next?
[00:13:51.400 --> 00:13:53.400]   - And schedule every single day kind of thing.
[00:13:53.400 --> 00:13:54.880]   So is that you could try to in the morning
[00:13:54.880 --> 00:13:57.000]   to try to have a plan?
[00:13:57.000 --> 00:14:00.280]   - Yeah, so I do a quarterly, weekly, daily planning.
[00:14:00.280 --> 00:14:02.320]   So at the semester or quarterly level,
[00:14:02.320 --> 00:14:04.800]   I have a big picture vision
[00:14:04.800 --> 00:14:07.320]   for what I'm trying to get done during the fall, let's say,
[00:14:07.320 --> 00:14:08.360]   or during the winter.
[00:14:08.360 --> 00:14:11.200]   There's a deadline coming up for academic papers
[00:14:11.200 --> 00:14:12.120]   at the end of the season.
[00:14:12.120 --> 00:14:13.480]   Here's what I'm working on.
[00:14:13.480 --> 00:14:14.840]   I wanna have this many chapters done.
[00:14:14.840 --> 00:14:15.840]   I wanna have a book, something like this.
[00:14:15.840 --> 00:14:18.200]   Like you have the big picture vision
[00:14:18.200 --> 00:14:20.080]   of what you wanna get done.
[00:14:20.080 --> 00:14:22.240]   Then weekly, you look at that
[00:14:22.240 --> 00:14:23.960]   and then you look at your week
[00:14:23.960 --> 00:14:25.280]   and you put together a plan for like, okay,
[00:14:25.280 --> 00:14:27.200]   what's my week gonna look like?
[00:14:27.200 --> 00:14:28.200]   What do I need to do?
[00:14:28.200 --> 00:14:29.880]   Or how am I gonna make progress on these things?
[00:14:29.880 --> 00:14:32.640]   Maybe I need to do an hour every morning
[00:14:32.640 --> 00:14:34.640]   or I see that Monday is my only really empty day.
[00:14:34.640 --> 00:14:36.440]   So that's gonna be the day that I really need to nail
[00:14:36.440 --> 00:14:38.080]   on writing or something like this.
[00:14:38.080 --> 00:14:41.160]   And then every day, you look at your weekly plan
[00:14:41.160 --> 00:14:42.760]   and see, let me block off the actual hours.
[00:14:42.760 --> 00:14:44.720]   So you do that three scales,
[00:14:44.720 --> 00:14:47.840]   the quarterly down to weekly, down to daily.
[00:14:47.840 --> 00:14:50.800]   - And we're talking about actual times of day versus,
[00:14:50.800 --> 00:14:55.080]   so the alternative is what I end up doing a lot,
[00:14:55.080 --> 00:14:56.640]   and I'm not sure it's the best way to do it,
[00:14:56.640 --> 00:15:01.640]   is scheduling the duration of time.
[00:15:01.640 --> 00:15:04.520]   This is called the luxury when you don't have any meetings.
[00:15:04.520 --> 00:15:07.840]   I'm like, religiously don't do meetings.
[00:15:07.840 --> 00:15:09.840]   - All other academics are jealous of you, by the way.
[00:15:09.840 --> 00:15:11.520]   - Yeah. - I know.
[00:15:11.520 --> 00:15:13.440]   No Zoom meetings.
[00:15:13.440 --> 00:15:18.240]   I find those are, that's one of the worst tragedies
[00:15:18.240 --> 00:15:21.360]   of the pandemic, is both the opportunity to,
[00:15:21.360 --> 00:15:24.040]   well, okay, the positive thing is to have more time
[00:15:24.040 --> 00:15:27.320]   with your family, sort of reconnect in many ways,
[00:15:27.320 --> 00:15:28.960]   and that's really interesting.
[00:15:28.960 --> 00:15:34.120]   Be able to remotely sort of not waste time on travel
[00:15:34.120 --> 00:15:35.200]   and all those kinds of things.
[00:15:35.200 --> 00:15:38.400]   The negative is, actually, both those things
[00:15:38.400 --> 00:15:40.400]   are also sources of the negative.
[00:15:40.400 --> 00:15:42.720]   But the negative is, it seems like people
[00:15:42.720 --> 00:15:44.280]   have multiplied the number of meetings
[00:15:44.280 --> 00:15:46.240]   because they're so easy to schedule,
[00:15:46.240 --> 00:15:49.920]   and there's nothing more draining to me,
[00:15:49.920 --> 00:15:54.240]   intellectually, philosophically, just my spirit
[00:15:54.240 --> 00:15:57.880]   is destroyed by even a 10-minute Zoom meeting.
[00:15:57.880 --> 00:15:59.720]   Like, what are we doing here?
[00:15:59.720 --> 00:16:00.960]   - What's the meaning of life?
[00:16:00.960 --> 00:16:02.720]   Come on, what is this all about?
[00:16:02.720 --> 00:16:05.720]   - Every Zoom meeting, I have an existential crisis.
[00:16:05.720 --> 00:16:08.200]   - Kierkegaard with a internet connection.
[00:16:10.320 --> 00:16:13.760]   - So, what the hell were we talking about?
[00:16:13.760 --> 00:16:16.720]   Oh, so when you don't have meetings,
[00:16:16.720 --> 00:16:21.560]   there's a luxury to really allow for certain things
[00:16:21.560 --> 00:16:25.080]   if they need to, like the important things,
[00:16:25.080 --> 00:16:28.000]   like deep work sessions to last way longer
[00:16:28.000 --> 00:16:30.840]   than you maybe planned for.
[00:16:30.840 --> 00:16:33.320]   I mean, that's my goal, is to try to schedule.
[00:16:33.320 --> 00:16:35.520]   The goal is to schedule, to sit and focus
[00:16:35.520 --> 00:16:37.480]   for a particular task for an hour,
[00:16:37.480 --> 00:16:40.000]   and hope I can keep going.
[00:16:40.000 --> 00:16:41.920]   And hope I can get lost in it.
[00:16:41.920 --> 00:16:46.920]   And do you find that this is at all an okay way to go?
[00:16:46.920 --> 00:16:51.800]   And the time blocking is just something you have to do
[00:16:51.800 --> 00:16:54.880]   to actually be an adult and operate in this real world?
[00:16:54.880 --> 00:16:57.960]   Or is there some magic to the time blocking?
[00:16:57.960 --> 00:17:00.520]   - Well, I mean, there's magic to the intention.
[00:17:00.520 --> 00:17:05.040]   There's magic to it if you have varied responsibilities.
[00:17:05.040 --> 00:17:08.800]   So, I'm often juggling multiple jobs, essentially.
[00:17:08.800 --> 00:17:11.160]   There's academic stuff, there's teaching stuff,
[00:17:11.160 --> 00:17:13.560]   there's book stuff, there's the business
[00:17:13.560 --> 00:17:16.560]   surrounding my book stuff.
[00:17:16.560 --> 00:17:19.000]   But I'm of your same mindset.
[00:17:19.000 --> 00:17:22.240]   If a deep work session is going well,
[00:17:22.240 --> 00:17:24.480]   you just rock and roll and let it go on.
[00:17:24.480 --> 00:17:26.400]   So, one of the big keys of time block,
[00:17:26.400 --> 00:17:29.280]   at least the way I do it, so I even sell this planner
[00:17:29.280 --> 00:17:31.880]   to help people time block, it has many columns.
[00:17:31.880 --> 00:17:32.960]   Because the discipline is,
[00:17:32.960 --> 00:17:35.960]   oh, if your initial schedule changes,
[00:17:35.960 --> 00:17:36.880]   you just move over one.
[00:17:36.880 --> 00:17:38.960]   Next time you get a chance to move over one column,
[00:17:38.960 --> 00:17:41.280]   and then you just fix it for the time that's remaining.
[00:17:41.280 --> 00:17:44.200]   So, in other words, there's no bonus for,
[00:17:44.200 --> 00:17:46.320]   I made a schedule and I stuck with it.
[00:17:46.320 --> 00:17:47.160]   Like, there's actually,
[00:17:47.160 --> 00:17:48.640]   it's not like you get a prize for it, right?
[00:17:48.640 --> 00:17:50.600]   Like, for me, the prize is,
[00:17:50.600 --> 00:17:52.960]   I have an intentional plan for my time.
[00:17:52.960 --> 00:17:54.680]   And if I have to change that plan, that's fine.
[00:17:54.680 --> 00:17:56.320]   Like, the state I wanna be is basically
[00:17:56.320 --> 00:17:57.200]   at any point in the day,
[00:17:57.200 --> 00:17:58.760]   I've thought about what time remains
[00:17:58.760 --> 00:18:01.160]   and gave it some thought for what to do.
[00:18:01.160 --> 00:18:02.120]   Because I'll do the same thing,
[00:18:02.120 --> 00:18:04.680]   even though I have a lot more meetings
[00:18:04.680 --> 00:18:06.960]   and other types of things I have to do in my various jobs.
[00:18:06.960 --> 00:18:09.720]   And I basically prioritize the deep work
[00:18:09.720 --> 00:18:11.080]   and then get yelled at a lot.
[00:18:11.080 --> 00:18:11.920]   - Yeah, got it. - So that's kind of
[00:18:11.920 --> 00:18:14.000]   my strategy is like, just be okay,
[00:18:14.000 --> 00:18:15.560]   just be okay getting yelled at a lot.
[00:18:15.560 --> 00:18:18.160]   Because I feel you, if you're rolling, yeah.
[00:18:18.160 --> 00:18:19.480]   Well, that's what it is for me.
[00:18:19.480 --> 00:18:20.720]   Like with writing, I think it's,
[00:18:20.720 --> 00:18:22.920]   writing's so hard in a certain way that it's,
[00:18:22.920 --> 00:18:24.920]   you don't really get on a roll in some sense.
[00:18:24.920 --> 00:18:26.800]   Like, it's just difficult.
[00:18:26.800 --> 00:18:28.440]   But working on proofs,
[00:18:28.440 --> 00:18:32.360]   it's very hard to pull yourself away from a proof
[00:18:32.360 --> 00:18:33.900]   if you start to get some traction.
[00:18:33.900 --> 00:18:35.520]   Just, you've been at it for a couple hours
[00:18:35.520 --> 00:18:37.800]   and you feel the pins and tumblers
[00:18:37.800 --> 00:18:40.520]   starting to click together and progress is being made.
[00:18:40.520 --> 00:18:42.400]   It's really hard to pull away from that.
[00:18:42.400 --> 00:18:45.200]   So I'm willing to get yelled at by almost everyone.
[00:18:45.200 --> 00:18:48.560]   - Of course, there is also a positive effect
[00:18:48.560 --> 00:18:53.200]   to pulling yourself out of it when things are going great.
[00:18:53.200 --> 00:18:55.700]   Because then you're kind of excited to resume.
[00:18:55.700 --> 00:18:57.000]   - Yeah. - As opposed to stopping
[00:18:57.000 --> 00:18:58.640]   on a dead end.
[00:18:58.640 --> 00:18:59.480]   - That's true.
[00:18:59.480 --> 00:19:01.760]   - There's an, yeah, there's a,
[00:19:03.640 --> 00:19:06.060]   there's an extra force of procrastination that comes with
[00:19:06.060 --> 00:19:08.940]   if you stop on a dead end to return to the task.
[00:19:08.940 --> 00:19:10.860]   - Yeah, or a cold start.
[00:19:10.860 --> 00:19:12.340]   - Yeah. - Like, whenever I,
[00:19:12.340 --> 00:19:15.420]   like I'm in a stage now, I submitted a few papers recently.
[00:19:15.420 --> 00:19:18.620]   So now we're sort of starting something up from cold.
[00:19:18.620 --> 00:19:20.540]   And it takes way too long to get going
[00:19:20.540 --> 00:19:22.220]   because it's very hard to,
[00:19:22.220 --> 00:19:24.420]   it's very hard to get the motivation to schedule a time
[00:19:24.420 --> 00:19:25.860]   when it's not, yeah, we're in it.
[00:19:25.860 --> 00:19:26.900]   Like, here's where we are.
[00:19:26.900 --> 00:19:28.340]   We feel like something's about to give here.
[00:19:28.340 --> 00:19:30.660]   We're in the very early stages where it's just,
[00:19:30.660 --> 00:19:32.900]   I don't know, I'm gonna read hard papers
[00:19:32.900 --> 00:19:34.120]   and it's gonna be hard to understand them.
[00:19:34.120 --> 00:19:35.640]   I'm gonna have no idea how to make progress.
[00:19:35.640 --> 00:19:38.480]   It's not motivating.
[00:19:38.480 --> 00:19:39.720]   - What about deadlines?
[00:19:39.720 --> 00:19:42.760]   Can we, okay, so this is like a therapy session.
[00:19:42.760 --> 00:19:44.240]   (laughing)
[00:19:44.240 --> 00:19:48.640]   Is why, it seems like I only get stuff done
[00:19:48.640 --> 00:19:50.160]   that has deadlines.
[00:19:50.160 --> 00:19:53.240]   And so the, one of the implied powerful things
[00:19:53.240 --> 00:19:56.160]   about time blocking is there's a kind of deadline
[00:19:56.160 --> 00:19:59.920]   or there's a artificial or real sense of urgency.
[00:19:59.920 --> 00:20:02.820]   Do you think it's possible to get anything done
[00:20:02.820 --> 00:20:04.260]   in this world without deadlines?
[00:20:04.260 --> 00:20:06.160]   Why do deadlines work so well?
[00:20:06.160 --> 00:20:08.500]   - Well, I mean, it's a clear motivational signal,
[00:20:08.500 --> 00:20:10.900]   but in the short term,
[00:20:10.900 --> 00:20:12.700]   you do get an effect like that in time blocking.
[00:20:12.700 --> 00:20:15.460]   I think the strong effect you get by saying,
[00:20:15.460 --> 00:20:18.140]   this is the exact time I'm gonna work on this,
[00:20:18.140 --> 00:20:20.020]   is that you don't have the debate with yourself
[00:20:20.020 --> 00:20:23.260]   every three minutes about, should I take a break now?
[00:20:23.260 --> 00:20:24.860]   Right, like this is the big issue with just saying,
[00:20:24.860 --> 00:20:26.320]   you know, I'm gonna go write.
[00:20:26.320 --> 00:20:27.780]   I'm gonna write for a while and that's it
[00:20:27.780 --> 00:20:29.240]   because your mind is saying, well, obviously,
[00:20:29.240 --> 00:20:30.940]   we're gonna take some breaks, right?
[00:20:30.940 --> 00:20:32.380]   We're not just gonna write forever.
[00:20:32.380 --> 00:20:34.380]   And so why not right now?
[00:20:34.380 --> 00:20:35.220]   You have to like, well, not right now,
[00:20:35.220 --> 00:20:36.340]   let's go a little bit longer, five minutes.
[00:20:36.340 --> 00:20:37.300]   So why don't we just take a break now?
[00:20:37.300 --> 00:20:38.820]   Like we should probably look at the internet.
[00:20:38.820 --> 00:20:40.280]   Now you have to constantly have this battle.
[00:20:40.280 --> 00:20:42.340]   On the other hand, if you're in a time block schedule,
[00:20:42.340 --> 00:20:44.300]   like I've got these two hours put aside for writing,
[00:20:44.300 --> 00:20:46.020]   that's what I'm supposed to be doing.
[00:20:46.020 --> 00:20:48.300]   I have a break scheduled over here.
[00:20:48.300 --> 00:20:50.060]   I don't have to fight with myself, right?
[00:20:50.060 --> 00:20:51.300]   And maybe at a larger scale,
[00:20:51.300 --> 00:20:53.580]   deadlines give you a similar sort of effect.
[00:20:53.580 --> 00:20:55.340]   Is I know this is what I'm supposed to be working on
[00:20:55.340 --> 00:20:57.980]   because it's due.
[00:20:57.980 --> 00:21:01.240]   - Perhaps, but we are describing as much healthier
[00:21:01.240 --> 00:21:02.880]   sort of giving yourself over.
[00:21:02.880 --> 00:21:05.680]   And you talk about this in the new email book,
[00:21:05.680 --> 00:21:07.200]   the process, I mean, in general,
[00:21:07.200 --> 00:21:09.800]   you talk about it all over is creating a process
[00:21:09.800 --> 00:21:13.420]   and then giving yourself over to the process.
[00:21:13.420 --> 00:21:17.600]   But then you have to be strict with yourself.
[00:21:17.600 --> 00:21:19.640]   - Yeah, but what are the deadlines you're talking about?
[00:21:19.640 --> 00:21:20.600]   It's like with papers,
[00:21:20.600 --> 00:21:22.800]   like what's the main type of deadline work?
[00:21:22.800 --> 00:21:26.240]   - Well, so papers definitely,
[00:21:26.240 --> 00:21:31.240]   but publications like say this podcast,
[00:21:31.240 --> 00:21:35.300]   I have to publish this podcast early next week,
[00:21:35.300 --> 00:21:36.720]   one, because your book is coming out.
[00:21:36.720 --> 00:21:40.080]   I'd love to support this amazing book.
[00:21:40.080 --> 00:21:45.080]   But the other is I have to fly to Vegas on Thursday
[00:21:45.080 --> 00:21:48.540]   to run 40 miles with David Goggins.
[00:21:48.540 --> 00:21:51.620]   And so I want this podcast,
[00:21:51.620 --> 00:21:55.020]   this conversation we're doing now to be out of my life.
[00:21:55.020 --> 00:21:57.340]   Like I don't want to be in a hotel in Vegas,
[00:21:57.340 --> 00:21:59.660]   like editing the, like freaking out
[00:21:59.660 --> 00:22:01.420]   while David Goggins is yelling.
[00:22:01.420 --> 00:22:05.820]   - On hour 43 of your Terrathon thing.
[00:22:05.820 --> 00:22:09.260]   - But actually it's possible that I still will be doing that
[00:22:09.260 --> 00:22:10.340]   because that's not a hard,
[00:22:10.340 --> 00:22:12.040]   that's a softer deadline, right?
[00:22:12.040 --> 00:22:13.180]   But those are sort of,
[00:22:13.180 --> 00:22:15.460]   life imposes these kinds of deadlines.
[00:22:15.460 --> 00:22:18.020]   I'm not, so yeah, papers are nice
[00:22:18.020 --> 00:22:20.580]   because there's an actual deadline.
[00:22:20.580 --> 00:22:24.500]   But I am almost referring to like the pressure
[00:22:24.500 --> 00:22:26.020]   that people put on you.
[00:22:26.020 --> 00:22:28.060]   Hey man, you said you're going to get this done
[00:22:28.060 --> 00:22:30.260]   two months ago, why haven't you gotten it done?
[00:22:30.260 --> 00:22:31.660]   - See, I don't like that pressure.
[00:22:31.660 --> 00:22:32.500]   - Yeah.
[00:22:32.500 --> 00:22:33.340]   - So maybe we, now, first of all, I think we can all--
[00:22:33.340 --> 00:22:34.260]   - I hate it too.
[00:22:34.260 --> 00:22:35.140]   - We can agree by the way,
[00:22:35.140 --> 00:22:37.180]   having David Goggins yell at you
[00:22:37.180 --> 00:22:39.520]   is probably the top productivity technique.
[00:22:39.520 --> 00:22:41.140]   (laughing)
[00:22:41.140 --> 00:22:44.020]   I think we'd all get a lot more done if he was yelling.
[00:22:44.020 --> 00:22:45.080]   But see, I don't like that.
[00:22:45.080 --> 00:22:47.540]   So I will try to get things done early.
[00:22:47.540 --> 00:22:49.040]   I like having flex.
[00:22:49.040 --> 00:22:52.960]   I also don't like the idea of this has to get done today.
[00:22:52.960 --> 00:22:55.060]   Right, like it's due at midnight
[00:22:55.060 --> 00:22:57.180]   and we've got a lot to do as the night before
[00:22:57.180 --> 00:22:59.780]   because then I get in my head about what if I get sick?
[00:22:59.780 --> 00:23:01.420]   Or like, what if, you know,
[00:23:01.420 --> 00:23:03.260]   what if I don't get a bad night's sleep
[00:23:03.260 --> 00:23:04.800]   and I can't think clearly?
[00:23:04.800 --> 00:23:05.740]   So I like to have the flex.
[00:23:05.740 --> 00:23:07.500]   So I'm all process.
[00:23:07.500 --> 00:23:08.980]   And that's like the philosophical aspect
[00:23:08.980 --> 00:23:09.820]   of that book, "Deep Work"
[00:23:09.820 --> 00:23:13.080]   is that there's something very human and deep
[00:23:13.080 --> 00:23:15.100]   about just wrangling with the world of ideas.
[00:23:15.100 --> 00:23:16.540]   I mean, Aristotle talked about this.
[00:23:16.540 --> 00:23:18.380]   If you go back and read the ethics,
[00:23:18.380 --> 00:23:20.300]   he's trying to understand the meaning of life.
[00:23:20.300 --> 00:23:24.720]   And he eventually ends up ultimately at the human capacity
[00:23:24.720 --> 00:23:26.440]   to contemplate deeply.
[00:23:26.440 --> 00:23:27.960]   It's kind of a teleological argument.
[00:23:27.960 --> 00:23:29.520]   It's the things that only humans can do
[00:23:29.520 --> 00:23:31.720]   and therefore it must be somehow connected to our ends.
[00:23:31.720 --> 00:23:34.280]   And he said, ultimately, that's where he found his meaning.
[00:23:34.280 --> 00:23:36.520]   But, you know, he's touching on some sort of intimation
[00:23:36.520 --> 00:23:37.840]   there that's correct.
[00:23:37.840 --> 00:23:39.640]   And so what I try to build my life around
[00:23:39.640 --> 00:23:44.400]   is regularly thinking hard about stuff that's interesting.
[00:23:44.400 --> 00:23:46.400]   Just like if you get a fitness habit going,
[00:23:46.400 --> 00:23:50.080]   you feel off when you don't do it.
[00:23:50.080 --> 00:23:51.620]   I try to get that cognitive habit.
[00:23:51.620 --> 00:23:52.940]   So it's like, I got it.
[00:23:52.940 --> 00:23:54.300]   I mean, look, I have my bag here somewhere.
[00:23:54.300 --> 00:23:56.780]   I have my notebook in it because I was thinking
[00:23:56.780 --> 00:23:58.300]   on the Uber ride over, I was like, you know,
[00:23:58.300 --> 00:24:00.420]   I could get some, I'm working on this new proof.
[00:24:00.420 --> 00:24:02.740]   And it just, so you train yourself.
[00:24:02.740 --> 00:24:04.700]   You train yourself to appreciate certain things.
[00:24:04.700 --> 00:24:08.060]   And then over time, the hope is that it accretes.
[00:24:08.060 --> 00:24:09.740]   - Well, let's talk about some demons
[00:24:09.740 --> 00:24:13.780]   because I wonder, so there's like "Deep Work"
[00:24:13.780 --> 00:24:18.780]   which and the "World Without Email" books
[00:24:19.260 --> 00:24:24.100]   that to me symbolize the life I want to live, okay?
[00:24:24.100 --> 00:24:27.680]   And then there is, I'm like, despite appearances,
[00:24:27.680 --> 00:24:29.860]   an adult at this point.
[00:24:29.860 --> 00:24:32.100]   And this is the life I actually live.
[00:24:32.100 --> 00:24:36.180]   And I'm in constant chaos.
[00:24:36.180 --> 00:24:37.780]   You said you don't like that anxiety.
[00:24:37.780 --> 00:24:41.300]   I hate it too, but it seems like I'm always in it.
[00:24:41.300 --> 00:24:42.980]   It's a giant mess.
[00:24:42.980 --> 00:24:46.580]   It's like, it's almost like whenever I establish,
[00:24:46.580 --> 00:24:49.340]   whenever I have successful processes for doing deep work,
[00:24:49.340 --> 00:24:52.460]   I'll add stuff on top of it just to introduce the chaos.
[00:24:52.460 --> 00:24:55.660]   And like, I don't want to, but you know,
[00:24:55.660 --> 00:24:57.500]   you have to look in the mirror at a certain point
[00:24:57.500 --> 00:25:00.580]   and you have to say like, who the hell am I?
[00:25:00.580 --> 00:25:02.420]   Like, I keep doing this.
[00:25:02.420 --> 00:25:04.660]   Is this something that's fundamental to who I am
[00:25:04.660 --> 00:25:06.180]   or do I really need to fix this?
[00:25:06.180 --> 00:25:07.500]   - What's the chaos right now?
[00:25:07.500 --> 00:25:09.820]   Like I've seen your video about like your routine.
[00:25:09.820 --> 00:25:12.300]   It seemed very structured and deep.
[00:25:12.300 --> 00:25:13.640]   In fact, I was really envious of it.
[00:25:13.640 --> 00:25:17.060]   So like, what's the chaos now that's not in that video?
[00:25:17.060 --> 00:25:19.100]   - Many of those sessions go way longer.
[00:25:19.100 --> 00:25:20.580]   I don't get enough sleep.
[00:25:20.580 --> 00:25:24.080]   And then the main introduction of chaos is,
[00:25:24.080 --> 00:25:26.640]   it's taking on too many things on the to-do list.
[00:25:26.640 --> 00:25:27.800]   - I see.
[00:25:27.800 --> 00:25:29.440]   - It's, I mean, I suppose it's a problem
[00:25:29.440 --> 00:25:33.400]   that everybody deals with, which is saying, not saying no.
[00:25:33.400 --> 00:25:36.880]   But it's not like I have trouble saying no.
[00:25:36.880 --> 00:25:39.280]   It's that there's so much cool shit in my life.
[00:25:39.280 --> 00:25:42.720]   Okay, listen, there's nothing I love more in this world
[00:25:42.720 --> 00:25:45.320]   than the Boston Dynamics robots.
[00:25:45.320 --> 00:25:47.000]   - Spot and the other, yeah.
[00:25:47.000 --> 00:25:48.380]   - And they're giving me spot.
[00:25:48.380 --> 00:25:51.400]   So there's a to-do, what am I gonna say, no?
[00:25:51.400 --> 00:25:52.600]   So they're giving me spot
[00:25:52.600 --> 00:25:54.360]   and I wanna do some computer vision stuff
[00:25:54.360 --> 00:25:55.480]   for the hell of it.
[00:25:55.480 --> 00:25:57.520]   Okay, so that's now a to-do item.
[00:25:57.520 --> 00:25:59.000]   - And then you go to Texas for a while.
[00:25:59.000 --> 00:25:59.840]   - And there's Texas.
[00:25:59.840 --> 00:26:00.840]   - But then everything's happening
[00:26:00.840 --> 00:26:02.400]   to all the interesting people down there.
[00:26:02.400 --> 00:26:03.920]   - And then there's surprises, right?
[00:26:03.920 --> 00:26:05.800]   There are power outages in Texas.
[00:26:05.800 --> 00:26:07.360]   There's constant changes to plans
[00:26:07.360 --> 00:26:08.480]   and all those kinds of things.
[00:26:08.480 --> 00:26:09.880]   And you sleep less.
[00:26:09.880 --> 00:26:11.400]   And then there's personal stuff,
[00:26:11.400 --> 00:26:13.600]   like just people in your life,
[00:26:13.600 --> 00:26:16.520]   sources of stress, all those kinds of things.
[00:26:16.520 --> 00:26:19.960]   But it does feel like if I'm just being introspective
[00:26:19.960 --> 00:26:22.380]   that I bring it onto myself.
[00:26:22.380 --> 00:26:24.640]   I suppose a lot of people do this kind of thing
[00:26:24.640 --> 00:26:30.360]   is they flourish under pressure.
[00:26:30.360 --> 00:26:34.920]   And I wonder if that's just a hack I've developed
[00:26:34.920 --> 00:26:37.600]   as a habit early on in life
[00:26:37.600 --> 00:26:40.720]   that you need to let go of.
[00:26:40.720 --> 00:26:42.080]   You need to fix.
[00:26:42.080 --> 00:26:44.000]   - But it's all interesting things.
[00:26:44.000 --> 00:26:44.960]   - Yeah, it's interesting.
[00:26:44.960 --> 00:26:45.800]   - That's interesting.
[00:26:45.800 --> 00:26:47.520]   Yeah, because these are all interesting things.
[00:26:47.520 --> 00:26:49.720]   - Well, one of the things you talked about in "Deep Work,"
[00:26:49.720 --> 00:26:50.840]   which is really important,
[00:26:50.840 --> 00:26:55.060]   is having an end to the day, putting it down.
[00:26:55.060 --> 00:26:59.040]   I don't think I've ever done that in my life.
[00:26:59.040 --> 00:26:59.880]   - Yeah.
[00:26:59.880 --> 00:27:01.000]   Well, see, I started doing that early
[00:27:01.000 --> 00:27:04.040]   because I got married early.
[00:27:04.040 --> 00:27:05.280]   So I didn't have a real job.
[00:27:05.280 --> 00:27:07.680]   I was a grad student, but my wife had a real job.
[00:27:07.680 --> 00:27:12.200]   And so I just figured I should do my work when she's at work
[00:27:12.200 --> 00:27:14.480]   because, hey, when work's over, she'll be home
[00:27:14.480 --> 00:27:17.760]   and I don't wanna be on campus or whatever.
[00:27:17.760 --> 00:27:19.720]   And so real early on, I just got in that habit
[00:27:19.720 --> 00:27:22.320]   of this is when you end work.
[00:27:22.320 --> 00:27:24.040]   And then when I was a postdoc,
[00:27:24.040 --> 00:27:25.440]   which is kind of an easy job,
[00:27:25.440 --> 00:27:30.020]   I put artificial, I was like, "I wanna train."
[00:27:30.020 --> 00:27:31.600]   I was like, "When I'm a professor, it's gonna be busier
[00:27:31.600 --> 00:27:34.200]   because there's demands that professors have beyond research."
[00:27:34.200 --> 00:27:35.560]   And so as a postdoc,
[00:27:35.560 --> 00:27:37.960]   I added artificial, large, time-consuming things
[00:27:37.960 --> 00:27:38.800]   into the middle of my day.
[00:27:38.800 --> 00:27:40.440]   I'd basically exercise for two hours
[00:27:40.440 --> 00:27:41.260]   in the middle of the day
[00:27:41.260 --> 00:27:44.600]   and do all this productive meditation and stuff like this
[00:27:44.600 --> 00:27:46.720]   while still maintaining the nine to five.
[00:27:46.720 --> 00:27:48.680]   So it's like, okay, I wanna get really good
[00:27:48.680 --> 00:27:51.600]   at putting artificial constraints on so that I stay,
[00:27:51.600 --> 00:27:54.720]   I didn't wanna get flabby when my job was easy
[00:27:54.720 --> 00:27:56.600]   so that when I became a professor,
[00:27:56.600 --> 00:27:57.800]   and now all of that's paying off
[00:27:57.800 --> 00:27:59.760]   because I have a ton of kids.
[00:27:59.760 --> 00:28:01.760]   So now I don't really have a choice.
[00:28:01.760 --> 00:28:04.120]   That's what's probably keeping me away from cool things
[00:28:04.120 --> 00:28:06.280]   is I just don't have time to do them.
[00:28:06.280 --> 00:28:09.240]   And then after a while, people stop bothering.
[00:28:09.240 --> 00:28:13.040]   - But that's how you have a successful life.
[00:28:13.040 --> 00:28:15.240]   Otherwise, it's too easy
[00:28:15.240 --> 00:28:18.000]   to then go into the full Hunter S. Thompson,
[00:28:18.000 --> 00:28:23.000]   like to where nobody functional wants
[00:28:23.000 --> 00:28:25.720]   to be in your vicinity.
[00:28:25.720 --> 00:28:29.240]   Like you're driving, you attract the people
[00:28:29.240 --> 00:28:33.400]   that have a similar behavior pattern as you.
[00:28:33.400 --> 00:28:35.440]   So if you live in chaos,
[00:28:35.440 --> 00:28:37.400]   you're going to attract chaotic people.
[00:28:37.400 --> 00:28:42.040]   And then it becomes like this self-fulfilling prophecy.
[00:28:42.040 --> 00:28:45.920]   And it feels like, I'm not bothered by it,
[00:28:45.920 --> 00:28:48.400]   but I guess this is all coming around
[00:28:48.400 --> 00:28:49.520]   to exactly what you're saying,
[00:28:49.520 --> 00:28:52.240]   which is like, I think one of the big hacks
[00:28:52.240 --> 00:28:54.920]   for productive people that I've met is to get married
[00:28:54.920 --> 00:28:57.600]   and have kids, honestly.
[00:28:57.600 --> 00:29:00.840]   It's very, perhaps counterintuitive,
[00:29:00.840 --> 00:29:05.640]   but it's like the ultimate timetable enforcer.
[00:29:05.640 --> 00:29:08.840]   - Yeah, it enforces a lot of timetables,
[00:29:08.840 --> 00:29:12.240]   though it has a huge, kids have a huge productivity hit.
[00:29:12.240 --> 00:29:13.320]   Those, you gotta weigh it.
[00:29:13.320 --> 00:29:15.440]   But okay, here's the complicated thing, though.
[00:29:15.440 --> 00:29:17.120]   Like you could think about in your own life,
[00:29:17.120 --> 00:29:20.600]   starting the podcast as one of these just cool opportunities
[00:29:20.600 --> 00:29:22.080]   that you put on yourself, right?
[00:29:22.080 --> 00:29:25.040]   Like I could have been talking to you at MIT four years ago
[00:29:25.040 --> 00:29:25.880]   and be like, don't do that.
[00:29:25.880 --> 00:29:28.120]   Like your research is going well, right?
[00:29:28.120 --> 00:29:29.360]   But then everyone who watches you is like,
[00:29:29.360 --> 00:29:31.720]   okay, this podcast is, the direction that's taking you
[00:29:31.720 --> 00:29:33.680]   is like a couple of years from now,
[00:29:33.680 --> 00:29:36.600]   it's gonna, there'll be something really monumental
[00:29:36.600 --> 00:29:38.080]   that you're probably, that's gonna probably lead to, right?
[00:29:38.080 --> 00:29:39.920]   There'll be some really,
[00:29:39.920 --> 00:29:41.800]   it just feels like your life is going somewhere.
[00:29:41.800 --> 00:29:43.240]   - It's going somewhere, it's interesting.
[00:29:43.240 --> 00:29:44.600]   - Yeah. - Unexpected, yeah.
[00:29:44.600 --> 00:29:46.360]   - Yeah, so how do you balance those two things?
[00:29:46.360 --> 00:29:48.920]   And so what I try to throw at it is this motto
[00:29:48.920 --> 00:29:50.920]   of do less, do better, know why, right?
[00:29:50.920 --> 00:29:55.600]   So do less, do better, know why.
[00:29:55.600 --> 00:29:58.760]   It used to be the motto of my website years ago.
[00:29:58.760 --> 00:30:01.920]   So do a few things, but like an interesting array, right?
[00:30:01.920 --> 00:30:06.880]   So I was doing MIT stuff, but I was also writing, you know?
[00:30:06.880 --> 00:30:08.480]   So a couple of things are, you know, they were interesting.
[00:30:08.480 --> 00:30:10.240]   Like I have a couple bets placed
[00:30:10.240 --> 00:30:12.800]   on a couple of different numbers on the roulette table,
[00:30:12.800 --> 00:30:14.120]   but not too many things.
[00:30:14.120 --> 00:30:15.880]   And then really try to do those things really well
[00:30:15.880 --> 00:30:16.840]   and see where it goes.
[00:30:16.840 --> 00:30:17.680]   Like with my writing,
[00:30:17.680 --> 00:30:19.560]   I just spent years and years and years just training.
[00:30:19.560 --> 00:30:20.400]   I was like, I wanna be a better writer,
[00:30:20.400 --> 00:30:21.240]   I wanna be a better writer.
[00:30:21.240 --> 00:30:24.200]   I started writing student books when I was a student.
[00:30:24.200 --> 00:30:25.960]   I really wanted to write hardcover idea books.
[00:30:25.960 --> 00:30:26.800]   I started training.
[00:30:27.480 --> 00:30:30.320]   I would use like New Yorker articles to train myself.
[00:30:30.320 --> 00:30:32.000]   I'd break them down and then I'd get commissions
[00:30:32.000 --> 00:30:34.360]   with much smaller magazines and practice the skills.
[00:30:34.360 --> 00:30:37.040]   And it took forever until, you know, but now today,
[00:30:37.040 --> 00:30:38.360]   like I actually get to write for the New Yorker,
[00:30:38.360 --> 00:30:40.320]   but it took like a decade.
[00:30:40.320 --> 00:30:42.120]   So a small number of things, try to do them really well.
[00:30:42.120 --> 00:30:44.080]   And then the know why is have a connection
[00:30:44.080 --> 00:30:45.800]   to some sort of value.
[00:30:45.800 --> 00:30:48.600]   Like in general, I think this is worth doing
[00:30:48.600 --> 00:30:50.640]   and then seeing where it leads.
[00:30:50.640 --> 00:30:54.840]   - And so the choice of the few things is grounded
[00:30:54.840 --> 00:30:59.200]   in what, like a little flame of passion,
[00:30:59.200 --> 00:31:00.280]   like a love for the thing,
[00:31:00.280 --> 00:31:02.600]   like a sense that you say you wanted to write,
[00:31:02.600 --> 00:31:04.320]   get good at writing.
[00:31:04.320 --> 00:31:07.800]   You had that kind of introspective moment of thinking,
[00:31:07.800 --> 00:31:10.360]   this actually brings me a lot of joy and fulfillment.
[00:31:10.360 --> 00:31:11.600]   - Yeah, I mean, it gets complicated
[00:31:11.600 --> 00:31:13.200]   'cause I wrote a whole book about
[00:31:13.200 --> 00:31:14.680]   following your passion being bad advice,
[00:31:14.680 --> 00:31:18.440]   which is like the first thing I kind of got infamous for.
[00:31:18.440 --> 00:31:20.120]   I wrote that back in 2012.
[00:31:20.120 --> 00:31:23.280]   But the argument there is like passion cultivates, right?
[00:31:23.280 --> 00:31:26.120]   So what I was pushing back on was the myth
[00:31:26.120 --> 00:31:29.880]   that the passion for what you do exists full intensity
[00:31:29.880 --> 00:31:32.600]   before you start, and then that's what propels you.
[00:31:32.600 --> 00:31:35.400]   Where actually the reality is as you get better at something,
[00:31:35.400 --> 00:31:36.680]   as you gain more autonomy, more skill,
[00:31:36.680 --> 00:31:38.600]   and more impact, the passion grows along with it.
[00:31:38.600 --> 00:31:42.080]   So that when people look back later and say,
[00:31:42.080 --> 00:31:43.760]   oh, follow your passion, what they really mean is
[00:31:43.760 --> 00:31:45.200]   I'm very passionate about what I do,
[00:31:45.200 --> 00:31:47.080]   and that's a worthy goal.
[00:31:47.080 --> 00:31:49.320]   But how you actually cultivate that is much more complicated
[00:31:49.320 --> 00:31:51.960]   than just introspection is gonna identify,
[00:31:51.960 --> 00:31:54.080]   like for sure you should be a writer or something like this.
[00:31:54.080 --> 00:31:55.320]   - So I was actually quoting you.
[00:31:55.320 --> 00:32:00.320]   I was on a social network last night in a clubhouse.
[00:32:00.320 --> 00:32:01.720]   I don't know if you've heard of it.
[00:32:01.720 --> 00:32:03.280]   - Wait, I have to ask you about this
[00:32:03.280 --> 00:32:05.560]   because I'm invited to do a clubhouse.
[00:32:05.560 --> 00:32:07.160]   I don't know what that means.
[00:32:07.160 --> 00:32:09.320]   A tech reporter has invited me to do a clubhouse
[00:32:09.320 --> 00:32:10.240]   about my new book.
[00:32:10.240 --> 00:32:12.560]   - That's awesome.
[00:32:12.560 --> 00:32:14.240]   Well, let me know when 'cause I'll show up.
[00:32:14.240 --> 00:32:15.080]   - But what is it?
[00:32:15.080 --> 00:32:16.680]   - Okay, so first of all, let me just mention
[00:32:16.680 --> 00:32:21.120]   that I was in a clubhouse room last night
[00:32:21.120 --> 00:32:24.760]   and I kept plugging exactly what you said about passion.
[00:32:24.760 --> 00:32:25.800]   So we'll talk about it.
[00:32:25.800 --> 00:32:28.400]   It was a room that was focused on burnout.
[00:32:28.400 --> 00:32:29.520]   - Okay.
[00:32:29.520 --> 00:32:34.200]   - But first, clubhouse is a kind of fascinating place
[00:32:34.200 --> 00:32:37.700]   in terms of your mind would be very interesting
[00:32:37.700 --> 00:32:41.640]   to analyze this place because we talk about email,
[00:32:41.640 --> 00:32:43.560]   talk about social networks,
[00:32:43.560 --> 00:32:45.400]   but clubhouse is something very different
[00:32:45.400 --> 00:32:49.080]   and I've encountered in other places, Discord and so on,
[00:32:49.080 --> 00:32:52.320]   that's voice only communication.
[00:32:52.320 --> 00:32:53.720]   So it's a bunch of people in a room,
[00:32:53.720 --> 00:32:56.240]   they're just eyes closed.
[00:32:56.240 --> 00:32:57.640]   All you hear is their voices.
[00:32:57.640 --> 00:32:58.520]   - In real time.
[00:32:58.520 --> 00:32:59.860]   - Real time, live.
[00:32:59.860 --> 00:33:01.040]   It only happens live.
[00:33:01.040 --> 00:33:03.240]   You're technically not allowed to record,
[00:33:03.240 --> 00:33:04.720]   but some people still do,
[00:33:04.720 --> 00:33:07.760]   and especially when it's big conversations.
[00:33:07.760 --> 00:33:09.840]   But the whole point is it's there live.
[00:33:09.840 --> 00:33:10.960]   And there's different structures.
[00:33:10.960 --> 00:33:13.920]   Like on Discord, it was so fascinating.
[00:33:13.920 --> 00:33:16.880]   I have this Discord server that would have
[00:33:16.880 --> 00:33:19.480]   hundreds of people in a room together, right?
[00:33:19.480 --> 00:33:21.000]   We're all just little icons
[00:33:21.000 --> 00:33:22.880]   that can mute and unmute our mics.
[00:33:22.880 --> 00:33:23.700]   - Okay.
[00:33:23.700 --> 00:33:25.840]   - And so you're sitting there,
[00:33:25.840 --> 00:33:30.840]   so it's just voices and you're able with hundreds of people
[00:33:30.840 --> 00:33:33.960]   to not interrupt each other.
[00:33:33.960 --> 00:33:37.400]   Well, first of all, as a dynamic system.
[00:33:37.400 --> 00:33:39.680]   - You see icons, just like mics muted or not muted,
[00:33:39.680 --> 00:33:40.520]   basically.
[00:33:40.520 --> 00:33:42.360]   - Yeah, so everyone's muted and they unmute
[00:33:42.360 --> 00:33:44.120]   and it starts flashing.
[00:33:44.120 --> 00:33:44.960]   - Yeah.
[00:33:45.280 --> 00:33:47.960]   - So you're like, okay, let me get precedence.
[00:33:47.960 --> 00:33:48.800]   - Yeah.
[00:33:48.800 --> 00:33:49.620]   - So it's the digital equivalent
[00:33:49.620 --> 00:33:50.920]   of when you're in a conversation,
[00:33:50.920 --> 00:33:52.360]   like at a faculty meeting,
[00:33:52.360 --> 00:33:54.880]   and you sort of like kind of make some noises
[00:33:54.880 --> 00:33:56.240]   like while the other person's finishing.
[00:33:56.240 --> 00:33:57.620]   And so people realize like,
[00:33:57.620 --> 00:33:58.920]   okay, this person wants to talk next,
[00:33:58.920 --> 00:34:00.240]   but now it's purely digital.
[00:34:00.240 --> 00:34:01.520]   You see a flashing.
[00:34:01.520 --> 00:34:04.320]   - But in a faculty meeting, which is very interesting,
[00:34:04.320 --> 00:34:06.580]   like even as we're talking now,
[00:34:06.580 --> 00:34:09.640]   there's a visual element that seems to increase
[00:34:09.640 --> 00:34:11.360]   the probability of interruption.
[00:34:11.360 --> 00:34:12.200]   - Yeah.
[00:34:12.200 --> 00:34:13.700]   - When it's just darkness,
[00:34:13.700 --> 00:34:17.300]   you actually listen better and you don't interrupt.
[00:34:17.300 --> 00:34:18.760]   So like if you create a culture,
[00:34:18.760 --> 00:34:20.880]   there's always gonna be assholes,
[00:34:20.880 --> 00:34:23.360]   but they're actually exceptions.
[00:34:23.360 --> 00:34:24.680]   Everybody adjusts.
[00:34:24.680 --> 00:34:28.280]   They kind of evolve to the beat of the room.
[00:34:28.280 --> 00:34:30.320]   Okay, that's one fascinating aspect.
[00:34:30.320 --> 00:34:32.240]   It's like, okay, that's weird.
[00:34:32.240 --> 00:34:34.400]   'Cause it's different than like a Zoom call
[00:34:34.400 --> 00:34:35.560]   where there's video.
[00:34:35.560 --> 00:34:36.800]   - Yeah.
[00:34:36.800 --> 00:34:38.560]   - It's just audio.
[00:34:38.560 --> 00:34:40.040]   You think video adds,
[00:34:40.040 --> 00:34:42.760]   but it actually seems like it subtracts.
[00:34:42.760 --> 00:34:45.220]   The second aspect of it that's fascinating
[00:34:45.220 --> 00:34:48.120]   is when it's no video, just audio,
[00:34:48.120 --> 00:34:49.980]   there's an intimacy.
[00:34:49.980 --> 00:34:51.700]   It's weird.
[00:34:51.700 --> 00:34:53.860]   Because with strangers,
[00:34:53.860 --> 00:34:57.220]   you connect in a much more real way.
[00:34:57.220 --> 00:34:59.020]   It's similar to podcasts.
[00:34:59.020 --> 00:34:59.860]   - Yeah.
[00:34:59.860 --> 00:35:00.700]   - But--
[00:35:00.700 --> 00:35:01.660]   - With a lot of people.
[00:35:01.660 --> 00:35:03.700]   - With a lot of people and new people.
[00:35:03.700 --> 00:35:04.540]   - Huh.
[00:35:04.540 --> 00:35:07.560]   - And they bring, okay, first of all,
[00:35:07.560 --> 00:35:10.980]   different voices like low voices and like high voices.
[00:35:11.360 --> 00:35:14.360]   And it's more difficult to judge.
[00:35:14.360 --> 00:35:18.560]   In Discord, you couldn't even see the people.
[00:35:18.560 --> 00:35:21.640]   It was a culture where you do funny profile pictures
[00:35:21.640 --> 00:35:23.160]   as opposed to your actual face.
[00:35:23.160 --> 00:35:24.960]   In Clubhouse, it's your actual face.
[00:35:24.960 --> 00:35:27.640]   So you can tell like as an older person, younger person.
[00:35:27.640 --> 00:35:28.840]   In Discord, you couldn't.
[00:35:28.840 --> 00:35:31.260]   You just have to judge based on the voice.
[00:35:31.260 --> 00:35:35.000]   But there's something about the listening
[00:35:35.000 --> 00:35:39.040]   and the intimacy of being surprised by different strangers.
[00:35:39.040 --> 00:35:43.580]   It feels almost like a party with friends
[00:35:43.580 --> 00:35:45.340]   and friends of friends you haven't met yet,
[00:35:45.340 --> 00:35:47.300]   but you really like.
[00:35:47.300 --> 00:35:49.700]   Now, Clubhouse also has an interesting innovation
[00:35:49.700 --> 00:35:52.340]   where there's a large crowd that just listens
[00:35:52.340 --> 00:35:54.140]   and there's a stage.
[00:35:54.140 --> 00:35:56.340]   And you can bring people up on the stage.
[00:35:56.340 --> 00:35:59.220]   So only people on stage are talking.
[00:35:59.220 --> 00:36:01.340]   And you can have like five, six, seven, eight,
[00:36:01.340 --> 00:36:03.340]   sometimes 20, 30 people on stage.
[00:36:03.340 --> 00:36:04.820]   And then you can also have thousands
[00:36:04.820 --> 00:36:05.980]   of people just listening.
[00:36:05.980 --> 00:36:06.820]   - I see.
[00:36:06.820 --> 00:36:08.900]   - So there's a, I don't know.
[00:36:08.900 --> 00:36:10.760]   A lot of people are being surprised by this.
[00:36:10.760 --> 00:36:12.280]   - Why is it called a social network?
[00:36:12.280 --> 00:36:14.200]   It seems like it doesn't have, there's not social links.
[00:36:14.200 --> 00:36:17.200]   There's not a feed that's trying to harvest attention.
[00:36:17.200 --> 00:36:19.160]   It feels like a communication.
[00:36:19.160 --> 00:36:24.200]   - So the social network aspect is you follow people.
[00:36:24.200 --> 00:36:26.180]   And the people you follow,
[00:36:26.180 --> 00:36:27.600]   now this is like the first social network
[00:36:27.600 --> 00:36:30.460]   that's actually correct use of follow, I think.
[00:36:30.460 --> 00:36:35.120]   You're more likely to see the rooms they're in.
[00:36:35.120 --> 00:36:37.240]   So there's a, your feed is a bunch of rooms
[00:36:37.240 --> 00:36:38.560]   that are going on right now.
[00:36:38.560 --> 00:36:39.400]   - Okay.
[00:36:39.400 --> 00:36:43.380]   - And the people you follow are the ones
[00:36:43.380 --> 00:36:44.820]   that will increase the likelihood
[00:36:44.820 --> 00:36:46.380]   that you'll see the room they're in.
[00:36:46.380 --> 00:36:48.540]   And so the final result is like,
[00:36:48.540 --> 00:36:50.360]   there's a list of really interesting rooms.
[00:36:50.360 --> 00:36:52.460]   Like I have all these,
[00:36:52.460 --> 00:36:54.380]   I've been speaking Russian quite a bit.
[00:36:54.380 --> 00:36:58.260]   There's practicing, but also just like talking politics
[00:36:58.260 --> 00:37:00.100]   and philosophy in Russian.
[00:37:00.100 --> 00:37:01.220]   I've never done that before,
[00:37:01.220 --> 00:37:03.340]   but it allows me to connect with that community.
[00:37:03.340 --> 00:37:07.620]   And then there's a community of like, it's funny,
[00:37:07.620 --> 00:37:09.200]   but like I'll go in a community
[00:37:09.200 --> 00:37:12.200]   of all African-American people talking about race
[00:37:12.200 --> 00:37:13.460]   and I'll be welcomed.
[00:37:13.460 --> 00:37:14.300]   - Yeah.
[00:37:14.300 --> 00:37:17.080]   - I've never had, like, I've literally never been
[00:37:17.080 --> 00:37:20.220]   in a difficult conversation about race,
[00:37:20.220 --> 00:37:22.560]   like with people from all over the place.
[00:37:22.560 --> 00:37:23.480]   It's like fascinating.
[00:37:23.480 --> 00:37:26.440]   And musicians, jazz musicians, I don't know.
[00:37:26.440 --> 00:37:28.460]   You could say that a lot of other places
[00:37:28.460 --> 00:37:29.680]   could have created that culture.
[00:37:29.680 --> 00:37:33.000]   I suppose Twitter and Facebook allow for that culture,
[00:37:33.000 --> 00:37:35.880]   but there's something about this network
[00:37:35.880 --> 00:37:39.200]   as it stands now, 'cause no Android users.
[00:37:39.200 --> 00:37:42.780]   It's probably just because it's iPhone people.
[00:37:42.780 --> 00:37:45.240]   - Yeah, less conspiratorial or something.
[00:37:45.240 --> 00:37:47.300]   - Well, like less, listen, I'm an Android person,
[00:37:47.300 --> 00:37:50.200]   so I got an iPhone just for this network, which is funny.
[00:37:50.200 --> 00:37:55.920]   For now, it's all like, there's very few trolls.
[00:37:55.920 --> 00:37:57.240]   There's very few people that are trying
[00:37:57.240 --> 00:37:59.240]   to manipulate the system and so on.
[00:37:59.240 --> 00:38:00.920]   So I don't know, it's interesting.
[00:38:00.920 --> 00:38:03.620]   Now, the downside, the reason you're going to hate it
[00:38:04.760 --> 00:38:08.520]   is because it's so intimate, because it pulls you in
[00:38:08.520 --> 00:38:11.320]   and pulls in very successful people like you,
[00:38:11.320 --> 00:38:15.960]   just like really successful, productive, very busy people.
[00:38:15.960 --> 00:38:21.360]   It's a huge time sink.
[00:38:21.360 --> 00:38:23.640]   It's very difficult to pull yourself out.
[00:38:23.640 --> 00:38:25.080]   - Interesting, you mean once you're in a room?
[00:38:25.080 --> 00:38:27.760]   - Well, no, leaving the room is actually easy.
[00:38:27.760 --> 00:38:30.480]   The beautiful thing about a stage with multiple people,
[00:38:30.480 --> 00:38:33.600]   there's actually a little button that says leave quietly.
[00:38:33.600 --> 00:38:38.520]   So culture, no, etiquette-wise, it's okay to just leave.
[00:38:38.520 --> 00:38:41.000]   So you and I in a room, when it's just you and I,
[00:38:41.000 --> 00:38:42.200]   it's a little awkward to leave.
[00:38:42.200 --> 00:38:44.240]   - If you're asking questions and I'm just gone.
[00:38:44.240 --> 00:38:46.360]   - But, and actually, if you're being interviewed
[00:38:46.360 --> 00:38:51.040]   for the book, that's weird because you're now in the event
[00:38:51.040 --> 00:38:54.000]   and you're supposed to, but usually the person interviewing
[00:38:54.000 --> 00:38:55.920]   would be like, okay, it's time for you to go.
[00:38:55.920 --> 00:38:59.960]   It's more normal, but the normal way to use the room
[00:38:59.960 --> 00:39:02.820]   is like, you're just opening the app,
[00:39:02.820 --> 00:39:05.480]   and there'll be like, I don't know, Sam Harris,
[00:39:05.480 --> 00:39:11.840]   Eric Weinstein, I think Joe Rogan showed up to the app,
[00:39:11.840 --> 00:39:14.880]   Bill Gates, these people on stage just randomly
[00:39:14.880 --> 00:39:18.020]   just plugged in, and then you'll step up on stage,
[00:39:18.020 --> 00:39:20.360]   listen, maybe you won't contribute at all,
[00:39:20.360 --> 00:39:21.960]   maybe you'll say something funny,
[00:39:21.960 --> 00:39:23.400]   and then you'll just leave.
[00:39:23.400 --> 00:39:26.840]   And there's the addicting aspect to it,
[00:39:26.840 --> 00:39:30.000]   the reason it's a time sink is you don't wanna leave.
[00:39:30.000 --> 00:39:33.420]   - What I've noticed about exceptionally busy people
[00:39:33.420 --> 00:39:36.500]   that they love this, I think might have to do
[00:39:36.500 --> 00:39:38.220]   with the pandemic. - Might be a little bit, yeah.
[00:39:38.220 --> 00:39:40.540]   - There's a loneliness. - They're all starved.
[00:39:40.540 --> 00:39:42.480]   - But also it's really cool people.
[00:39:42.480 --> 00:39:44.980]   - Yeah. - Like when was the last time
[00:39:44.980 --> 00:39:47.020]   you talked to Sam Harris or whoever?
[00:39:47.020 --> 00:39:52.020]   Like think of anybody, Tyler, like any faculty.
[00:39:52.020 --> 00:39:54.860]   - This is like what universities strive to create,
[00:39:54.860 --> 00:39:57.700]   but it's taken hundreds of years of cultural evolution
[00:39:57.700 --> 00:39:59.660]   to try to get a lot of interesting, smart people together
[00:39:59.660 --> 00:40:00.660]   that run into each other.
[00:40:00.660 --> 00:40:04.620]   - We have really strong faculty in a room together
[00:40:04.620 --> 00:40:07.180]   with no scheduling, this is the power of it.
[00:40:07.180 --> 00:40:10.140]   It's like you just show up, there's none of that baggage
[00:40:10.140 --> 00:40:13.380]   of scheduling and so on, and there's no pressure to leave,
[00:40:13.380 --> 00:40:16.700]   sorry, no pressure to stay, it's very easy for you to leave.
[00:40:16.700 --> 00:40:19.220]   You realize that there's a lot of constraints on meetings
[00:40:19.220 --> 00:40:24.220]   and like faculty, like even stopping by before the pandemic,
[00:40:24.220 --> 00:40:28.380]   a friend or faculty or colleague and so on,
[00:40:28.380 --> 00:40:30.140]   there's a weirdness about leaving,
[00:40:30.140 --> 00:40:33.260]   but here there's not a weirdness about leaving.
[00:40:33.260 --> 00:40:36.060]   So they've discovered something interesting,
[00:40:36.060 --> 00:40:38.340]   but the final result when you observe it,
[00:40:38.340 --> 00:40:43.420]   it's very fulfilling, I think it's very beneficial,
[00:40:43.420 --> 00:40:44.860]   but it's very addicting.
[00:40:44.860 --> 00:40:48.540]   So you have to make sure you moderate.
[00:40:48.540 --> 00:40:50.140]   - Yeah, that's interesting.
[00:40:50.140 --> 00:40:52.180]   Okay, well, so maybe I'll try it.
[00:40:52.180 --> 00:40:54.700]   I mean, look, there's no, the things that make me suspicious
[00:40:54.700 --> 00:40:56.660]   about other platforms aren't here.
[00:40:56.660 --> 00:41:00.500]   So the feed is not full of user generated content
[00:41:00.500 --> 00:41:02.340]   that is going through some sort of algorithmic rating
[00:41:02.340 --> 00:41:05.780]   process with all the weird incentives and nudging that does.
[00:41:05.780 --> 00:41:08.900]   And you're not producing content that's being harvested
[00:41:08.900 --> 00:41:11.260]   to be monetized by another company.
[00:41:11.260 --> 00:41:14.420]   I mean, it seems like it's more ephemeral, right?
[00:41:14.420 --> 00:41:15.780]   You're here, you're talking,
[00:41:15.780 --> 00:41:17.780]   the feed is just actually just showing you,
[00:41:17.780 --> 00:41:19.380]   here's interesting things happening, right?
[00:41:19.380 --> 00:41:21.060]   You're not jockeying in the feed for,
[00:41:21.060 --> 00:41:22.420]   look, I'm being clever or something,
[00:41:22.420 --> 00:41:24.700]   and I'm gonna get a like count that goes up
[00:41:24.700 --> 00:41:26.700]   and that's gonna influence.
[00:41:26.700 --> 00:41:27.540]   And there's more friction,
[00:41:27.540 --> 00:41:28.980]   there's more cognitive friction, I guess,
[00:41:28.980 --> 00:41:31.380]   involved in listening to smart people
[00:41:31.380 --> 00:41:33.540]   versus scrolling through.
[00:41:33.540 --> 00:41:34.460]   Yeah, there's something there.
[00:41:34.460 --> 00:41:35.380]   - So there's no--
[00:41:35.380 --> 00:41:37.180]   - Why are people so, I see a lot of,
[00:41:37.180 --> 00:41:39.260]   there's all these articles that seem,
[00:41:39.260 --> 00:41:40.100]   I haven't really read them,
[00:41:40.100 --> 00:41:42.380]   but it seems, why are reporters negative about this?
[00:41:42.380 --> 00:41:43.300]   - Competition.
[00:41:43.300 --> 00:41:44.700]   The New York Times wrote this article
[00:41:44.700 --> 00:41:47.620]   called "Unfettered Conversations Happening on Clubhouse."
[00:41:47.620 --> 00:41:51.180]   - So I'm right in picking up a tone,
[00:41:51.180 --> 00:41:52.020]   even from the headlines,
[00:41:52.020 --> 00:41:55.060]   that there's some negative vibes from the press.
[00:41:55.060 --> 00:41:58.500]   - No, so I can say, let's say,
[00:41:58.500 --> 00:42:00.980]   well, I'll tell you what the article was saying,
[00:42:00.980 --> 00:42:05.780]   which is they're having cancelable conversations,
[00:42:05.780 --> 00:42:07.340]   like the biggest people in the world
[00:42:07.340 --> 00:42:09.500]   almost trolling the press.
[00:42:09.500 --> 00:42:10.340]   And the press is desperate--
[00:42:10.340 --> 00:42:11.540]   - Like 4channing the press.
[00:42:11.540 --> 00:42:13.500]   - Yeah, 4channing the press,
[00:42:13.500 --> 00:42:16.780]   by saying that you guys are looking for clickbait
[00:42:16.780 --> 00:42:19.260]   from our genuine human conversations.
[00:42:19.260 --> 00:42:24.260]   And so I think, honestly, the press is just like,
[00:42:24.260 --> 00:42:25.500]   what do we do with this?
[00:42:25.500 --> 00:42:28.940]   We can't, first of all, it's a lot of work for the,
[00:42:28.940 --> 00:42:31.380]   okay, it's what Naval says,
[00:42:31.380 --> 00:42:34.620]   which is like, this is skipping the journalist.
[00:42:34.620 --> 00:42:37.180]   Like the interview, if you go on Clubhouse,
[00:42:37.180 --> 00:42:39.740]   the interview you might do for the book
[00:42:39.740 --> 00:42:41.460]   would be with somebody who's like a journalist
[00:42:41.460 --> 00:42:43.140]   and interviewing you.
[00:42:43.140 --> 00:42:45.140]   That's more traditional.
[00:42:45.140 --> 00:42:47.280]   It'd be a good introduction for you to try it,
[00:42:47.280 --> 00:42:52.280]   but the way to use Clubhouse is you just show up
[00:42:52.280 --> 00:42:55.620]   and it's like, again, like me, I'm sorry,
[00:42:55.620 --> 00:42:58.980]   I'm like, I keep mentioning Sam Harris
[00:42:58.980 --> 00:43:00.540]   as if it's like the only person I know,
[00:43:00.540 --> 00:43:03.660]   but like a lot of these major faculty,
[00:43:03.660 --> 00:43:06.860]   I don't know, Max Tegmark, just major faculty
[00:43:06.860 --> 00:43:08.620]   just sitting there, and then you show up
[00:43:08.620 --> 00:43:10.940]   and then I'll ask like,
[00:43:10.940 --> 00:43:12.780]   oh, don't you have a book coming out or something?
[00:43:12.780 --> 00:43:14.460]   And then you'll talk about the book
[00:43:14.460 --> 00:43:15.700]   and then you'll leave five minutes later
[00:43:15.700 --> 00:43:18.160]   'cause you have to go get coffee and go to the bathroom.
[00:43:18.160 --> 00:43:21.000]   So like, that's the, it's not the journalistic,
[00:43:21.000 --> 00:43:23.920]   you're not gonna actually enjoy the interview as much
[00:43:23.920 --> 00:43:26.680]   because it'll be like the normal thing.
[00:43:26.680 --> 00:43:28.880]   Like you're there for 40 minutes or an hour
[00:43:28.880 --> 00:43:31.160]   and there'll be questions from the audience.
[00:43:31.160 --> 00:43:33.800]   - Like I'm doing an event next week for the book launch
[00:43:33.800 --> 00:43:37.440]   where it's like Jason Fried and I are talking about email,
[00:43:37.440 --> 00:43:39.080]   but it's using some more,
[00:43:39.080 --> 00:43:40.680]   there'll be like a thousand people who are there
[00:43:40.680 --> 00:43:42.080]   to watch virtually, but it's using
[00:43:42.080 --> 00:43:44.760]   some sort of traditional webinar.
[00:43:44.760 --> 00:43:46.020]   Clubhouse would be a situation
[00:43:46.020 --> 00:43:47.620]   where that could just happen informally.
[00:43:47.620 --> 00:43:49.020]   Like I jump in like Jason's there
[00:43:49.020 --> 00:43:51.940]   and then someone else jumps in and yeah, that's interesting.
[00:43:51.940 --> 00:43:53.940]   - But for now it's still closed.
[00:43:53.940 --> 00:43:56.180]   So even though there's a lot of excitement
[00:43:56.180 --> 00:43:58.940]   and there'll be quite famous people
[00:43:58.940 --> 00:44:00.740]   just sitting there listening to you,
[00:44:00.740 --> 00:44:04.300]   but the numbers aren't exactly high.
[00:44:04.300 --> 00:44:05.940]   So you're talking about rooms,
[00:44:05.940 --> 00:44:09.340]   like even the huge rooms are like just a few thousand.
[00:44:09.340 --> 00:44:11.260]   - Right, and this is probably like Soho
[00:44:11.260 --> 00:44:12.620]   in the fifties or something too,
[00:44:12.620 --> 00:44:15.480]   just because of the exponential growth,
[00:44:15.480 --> 00:44:17.120]   give it seven more months.
[00:44:17.120 --> 00:44:19.480]   And if you let one invite, be gets two invites,
[00:44:19.480 --> 00:44:22.200]   be gets four invites, be pretty soon it'll be everyone.
[00:44:22.200 --> 00:44:25.440]   And then the rooms in your feed are gonna be whatever,
[00:44:25.440 --> 00:44:27.160]   marketing, performance, enhancing drugs
[00:44:27.160 --> 00:44:28.760]   or something like that.
[00:44:28.760 --> 00:44:30.600]   - But then, and a bunch of competitors,
[00:44:30.600 --> 00:44:32.680]   there's already like 30 plus competitors
[00:44:32.680 --> 00:44:34.400]   that sprung up Twitter spaces.
[00:44:34.400 --> 00:44:36.160]   So Twitter is creating a competitor
[00:44:36.160 --> 00:44:38.560]   that's going to likely destroy Clubhouse
[00:44:38.560 --> 00:44:40.480]   because they just have a much larger user base
[00:44:40.480 --> 00:44:42.400]   and they already have a social network.
[00:44:42.400 --> 00:44:46.380]   So I would be very cautious of course
[00:44:46.380 --> 00:44:47.580]   with the addictive element,
[00:44:47.580 --> 00:44:49.300]   but it doesn't just like you said,
[00:44:49.300 --> 00:44:52.100]   this particular implementation in its early stages
[00:44:52.100 --> 00:44:53.340]   doesn't have the like,
[00:44:53.340 --> 00:44:58.500]   it doesn't have the context switching problem.
[00:44:58.500 --> 00:45:01.140]   You'll just switch to it and you'll be stuck.
[00:45:01.140 --> 00:45:02.860]   - Yeah, the keep a context is great.
[00:45:02.860 --> 00:45:04.380]   - Yeah. - Yeah.
[00:45:04.380 --> 00:45:07.660]   - But then I think the best way I've found to use it
[00:45:07.660 --> 00:45:12.660]   is to acknowledge that these things pull you in.
[00:45:12.660 --> 00:45:13.780]   - Yeah.
[00:45:13.780 --> 00:45:17.000]   - So I've used it in the past,
[00:45:17.000 --> 00:45:19.140]   like almost, I'll go get a coffee
[00:45:19.140 --> 00:45:20.880]   and I'll tune into a conversation
[00:45:20.880 --> 00:45:24.360]   as if that's how I use podcasts sometimes.
[00:45:24.360 --> 00:45:26.960]   I'll just like play a little bit of a podcast
[00:45:26.960 --> 00:45:29.600]   and then I can just turn it off.
[00:45:29.600 --> 00:45:31.880]   The problem with these is it pulls you in,
[00:45:31.880 --> 00:45:32.840]   it's really interesting.
[00:45:32.840 --> 00:45:35.420]   And then the other problem that you'll experience
[00:45:35.420 --> 00:45:37.520]   is like somebody will recognize you
[00:45:37.520 --> 00:45:41.320]   and then they'll be like, oh Lex, come on up.
[00:45:41.320 --> 00:45:43.440]   Come on, no way, I had a question for you.
[00:45:43.440 --> 00:45:47.600]   And then it takes a lot for you to go like to ignore that.
[00:45:47.600 --> 00:45:49.280]   - Yeah, yeah.
[00:45:49.280 --> 00:45:51.120]   - And then you pulled in and it's fascinating
[00:45:51.120 --> 00:45:52.320]   and it's really cool people.
[00:45:52.320 --> 00:45:53.880]   So it's like a source of a lot of joy,
[00:45:53.880 --> 00:45:58.160]   but you have to be very, very careful.
[00:45:58.160 --> 00:46:00.920]   The reason I brought it up is there's a room,
[00:46:00.920 --> 00:46:04.120]   there's an entire club actually on burnout
[00:46:04.120 --> 00:46:08.100]   and I brought you up and I brought David Goggins
[00:46:08.100 --> 00:46:09.700]   as the process I go through,
[00:46:09.700 --> 00:46:14.700]   which is my passion goes up and down, it dips.
[00:46:14.700 --> 00:46:17.700]   And I don't think I trust my own mind
[00:46:17.700 --> 00:46:22.620]   to tell me whether I'm getting close to burnout
[00:46:22.620 --> 00:46:24.820]   or exhaustion or not.
[00:46:24.820 --> 00:46:28.260]   I kind of go with the David Goggins model of,
[00:46:28.260 --> 00:46:30.260]   I mean, he's probably more applying it to running,
[00:46:30.260 --> 00:46:35.140]   but when it feels like your mind can't take any more,
[00:46:35.140 --> 00:46:38.900]   that you're just 40% at your capacity.
[00:46:38.900 --> 00:46:41.020]   I mean, it's just like an arbitrary level.
[00:46:41.020 --> 00:46:41.860]   - It's the Navy SEAL thing, right?
[00:46:41.860 --> 00:46:43.060]   - The Navy SEAL thing.
[00:46:43.060 --> 00:46:44.620]   I mean, you could put that at any percent,
[00:46:44.620 --> 00:46:48.100]   but it is remarkable that if you just take it
[00:46:48.100 --> 00:46:49.980]   one step at a time, just keep going,
[00:46:49.980 --> 00:46:53.380]   it's similar to this idea of a process.
[00:46:53.380 --> 00:46:55.860]   If you just trust the process and you just keep following,
[00:46:55.860 --> 00:46:58.420]   even if the passion goes up and down and so on,
[00:46:58.420 --> 00:47:02.740]   then ultimately, if you look in aggregate,
[00:47:02.740 --> 00:47:04.420]   the passion will increase.
[00:47:04.420 --> 00:47:05.820]   Your self-satisfaction will increase.
[00:47:05.820 --> 00:47:08.300]   - Yeah, and if you have two things,
[00:47:08.300 --> 00:47:09.780]   this has been a big strategy of mine,
[00:47:09.780 --> 00:47:12.580]   so that what you hope for is off-phase,
[00:47:12.580 --> 00:47:14.220]   off-phase alignment.
[00:47:14.220 --> 00:47:16.580]   Sometimes it's in-phase and that's a problem,
[00:47:16.580 --> 00:47:18.060]   but off-phase alignment's good.
[00:47:18.060 --> 00:47:20.700]   So, okay, my research, I'm struggling,
[00:47:20.700 --> 00:47:22.740]   but my book stuff is going well, right?
[00:47:22.740 --> 00:47:24.820]   And so when you add those two waves together,
[00:47:24.820 --> 00:47:25.780]   like, oh, we're doing pretty well.
[00:47:25.780 --> 00:47:28.460]   And then in other periods, like on my writing,
[00:47:28.460 --> 00:47:29.940]   I feel like I'm just not getting anywhere,
[00:47:29.940 --> 00:47:31.180]   but oh, I've had some good papers,
[00:47:31.180 --> 00:47:32.420]   I'm feeling good over there.
[00:47:32.420 --> 00:47:35.980]   So having two things that can counteract each other.
[00:47:35.980 --> 00:47:38.980]   Now, sometimes they fall into sync and then it gets rough.
[00:47:38.980 --> 00:47:40.300]   (laughs)
[00:47:40.300 --> 00:47:41.780]   Because everything for me is cyclical,
[00:47:41.780 --> 00:47:43.300]   good periods, bad periods with all this stuff.
[00:47:43.300 --> 00:47:47.620]   So typically they don't coincide, so it helps compensate.
[00:47:47.620 --> 00:47:50.420]   When they do coincide, you get really high highs,
[00:47:50.420 --> 00:47:51.260]   like where everything's clicking,
[00:47:51.260 --> 00:47:52.820]   and then you get these really low lows
[00:47:52.820 --> 00:47:54.740]   where your research is not working,
[00:47:54.740 --> 00:47:56.420]   your program's not clicking,
[00:47:56.420 --> 00:47:59.340]   you feel like you're nowhere with your writing,
[00:47:59.340 --> 00:48:00.740]   and then it's a little rougher.
[00:48:00.740 --> 00:48:04.060]   - Is, do you think about the concept of burnout?
[00:48:04.060 --> 00:48:06.420]   'Cause I, so I personally have never experienced burnout
[00:48:06.420 --> 00:48:08.300]   in the way that folks talk about,
[00:48:08.300 --> 00:48:11.740]   which is like, it's not just the up and down,
[00:48:11.740 --> 00:48:14.780]   it's like, you don't wanna do anything ever again.
[00:48:14.780 --> 00:48:15.620]   - Yeah.
[00:48:15.620 --> 00:48:17.740]   - It's like, for some people it's like physical,
[00:48:17.740 --> 00:48:19.740]   like to the hospital kind of thing.
[00:48:19.740 --> 00:48:22.880]   - Yeah, so I do worry about it.
[00:48:22.880 --> 00:48:24.840]   So when I used to do student writing,
[00:48:24.840 --> 00:48:27.460]   like writing about students and student advice,
[00:48:27.460 --> 00:48:30.440]   it came up a lot with students at elite schools,
[00:48:30.440 --> 00:48:32.420]   and I used to call it deep procrastination,
[00:48:32.420 --> 00:48:35.380]   but it was a real, really vivid,
[00:48:35.380 --> 00:48:37.680]   very replicatable syndrome
[00:48:37.680 --> 00:48:39.700]   where they stop being able to do schoolwork.
[00:48:39.700 --> 00:48:40.540]   - Yeah.
[00:48:40.540 --> 00:48:42.980]   - Like, this is due, and the professor gives you an extension
[00:48:42.980 --> 00:48:44.340]   and the professor gives you an incomplete
[00:48:44.340 --> 00:48:46.140]   and says, "You got it, you were gonna fail the course,
[00:48:46.140 --> 00:48:48.600]   "you have to hand this in," and they can't do it.
[00:48:48.600 --> 00:48:50.680]   Right, it's like a complete stop
[00:48:50.680 --> 00:48:52.460]   on the ability to actually do work.
[00:48:52.460 --> 00:48:54.180]   And so I used to counsel students who had that issue,
[00:48:54.180 --> 00:48:56.500]   and often it was a combination of,
[00:48:56.500 --> 00:48:58.700]   at least this is my best analysis,
[00:48:58.700 --> 00:49:01.380]   is you have just the physical and cognitive difficulties
[00:49:01.380 --> 00:49:03.700]   of they're usually under a very hard load, right?
[00:49:03.700 --> 00:49:05.540]   They're doing too many majors, too many extracurriculars,
[00:49:05.540 --> 00:49:07.500]   just really pushing themselves,
[00:49:07.500 --> 00:49:11.500]   and the motivation is not sufficiently intrinsic.
[00:49:11.500 --> 00:49:12.340]   - Right.
[00:49:12.340 --> 00:49:13.340]   - So if you have a motivational center
[00:49:13.340 --> 00:49:14.460]   that's not completely on board,
[00:49:14.460 --> 00:49:16.740]   so a lot of these kids, like when I'm dealing with MIT kids,
[00:49:16.740 --> 00:49:20.220]   they would be, their whole town was shooting off fireworks
[00:49:20.220 --> 00:49:21.060]   that they got in,
[00:49:21.060 --> 00:49:23.500]   everyone's hoped that they were going there,
[00:49:23.500 --> 00:49:24.620]   and that they're in three majors,
[00:49:24.620 --> 00:49:25.620]   they don't wanna let people down,
[00:49:25.620 --> 00:49:26.580]   but they're not really interested
[00:49:26.580 --> 00:49:28.180]   in being a doctor or whatever.
[00:49:28.180 --> 00:49:30.100]   So your motivation's not in the right place,
[00:49:30.100 --> 00:49:31.580]   the motivational psychologist would say
[00:49:31.580 --> 00:49:33.180]   the locus of control was more towards
[00:49:33.180 --> 00:49:36.660]   the extrinsic end of the spectrum, and you have hardship.
[00:49:36.660 --> 00:49:38.840]   And you could just fritz out the whole system.
[00:49:38.840 --> 00:49:40.540]   And so I would always be very worried about that,
[00:49:40.540 --> 00:49:41.900]   so I think about that a lot.
[00:49:41.900 --> 00:49:45.400]   I do a lot of multi-phase or multi-scale seasonality.
[00:49:45.400 --> 00:49:48.140]   So I'll go hard on something for a while,
[00:49:48.140 --> 00:49:50.300]   and then for a few weeks, go easy.
[00:49:50.300 --> 00:49:51.940]   I'll have semesters that are hard
[00:49:51.940 --> 00:49:53.140]   and semesters that are easier.
[00:49:53.140 --> 00:49:54.140]   I'll take the summer really low.
[00:49:54.140 --> 00:49:55.180]   So on multiple scales,
[00:49:55.180 --> 00:49:56.660]   and in the day I'll go really hard on something,
[00:49:56.660 --> 00:49:57.900]   but then have a hard cutoff at five.
[00:49:57.900 --> 00:50:01.900]   So like every scale, it's all about rest and recovery.
[00:50:01.900 --> 00:50:03.620]   'Cause I really wanna avoid that, and I do burn out.
[00:50:03.620 --> 00:50:06.620]   I burnt out, pretty recently I get minor burnt outs.
[00:50:06.620 --> 00:50:10.060]   I got a couple papers that I was trying to work through
[00:50:10.060 --> 00:50:12.780]   for a deadline a few weeks ago,
[00:50:12.780 --> 00:50:14.660]   and I wasn't sleeping well,
[00:50:14.660 --> 00:50:17.500]   and there's some other things going on,
[00:50:17.500 --> 00:50:20.300]   and it just knocks out, and I get sick usually,
[00:50:20.300 --> 00:50:22.380]   is how I know I've pushed myself too far.
[00:50:22.380 --> 00:50:23.460]   And so I kind of pulled it back.
[00:50:23.460 --> 00:50:24.460]   Now I'm doing this book launch,
[00:50:24.460 --> 00:50:26.700]   then after this book launch, I'm pulling it back again.
[00:50:26.700 --> 00:50:30.140]   So seasonality for rest and recovery, I think is crucial.
[00:50:30.140 --> 00:50:33.700]   And at every scale, daily, monthly,
[00:50:33.700 --> 00:50:34.940]   and then at the annual scale.
[00:50:34.940 --> 00:50:36.260]   An easy summer, for example,
[00:50:36.260 --> 00:50:38.900]   I think is a great idea, if that's possible.
[00:50:38.900 --> 00:50:41.260]   - Okay, you just made me realize
[00:50:41.260 --> 00:50:43.260]   that that's exactly what I do.
[00:50:43.260 --> 00:50:45.160]   'Cause I feel like I'm not even close
[00:50:45.160 --> 00:50:46.100]   to burn out on anything,
[00:50:46.100 --> 00:50:48.800]   even though I'm in chaos.
[00:50:48.800 --> 00:50:53.740]   I feel the right exact ways of seasonality is the,
[00:50:53.740 --> 00:50:55.180]   not even the seasonality,
[00:50:55.180 --> 00:50:59.020]   but you always have multiple seasons operating.
[00:50:59.020 --> 00:51:00.140]   It's like you said,
[00:51:00.140 --> 00:51:02.820]   'cause when you have a lot of cool shit going on,
[00:51:02.820 --> 00:51:05.660]   there's always at least one thing that's a source of joy.
[00:51:05.660 --> 00:51:08.740]   There's always a reason.
[00:51:08.740 --> 00:51:10.900]   I suppose the fundamental thing,
[00:51:10.900 --> 00:51:13.780]   and I've known people that suffer from depression too,
[00:51:13.780 --> 00:51:16.080]   the fundamental problem with the experience
[00:51:16.080 --> 00:51:18.380]   of depression and burnout is like,
[00:51:18.380 --> 00:51:21.900]   why do, like, life is meaningless.
[00:51:21.900 --> 00:51:25.420]   And I always have an answer of like, why?
[00:51:25.420 --> 00:51:26.940]   Why today could be cool.
[00:51:26.940 --> 00:51:29.940]   - And you have to contrive it, right?
[00:51:29.940 --> 00:51:31.780]   If you don't have it, you have to contrive it.
[00:51:31.780 --> 00:51:33.180]   I think it's really important.
[00:51:33.180 --> 00:51:34.860]   Like, okay, well, this is going bad.
[00:51:34.860 --> 00:51:37.300]   So now is the time to start thinking about,
[00:51:37.300 --> 00:51:39.860]   I mean, look, I started a podcast during the pandemic.
[00:51:39.860 --> 00:51:42.300]   It's like, this is going pretty bad,
[00:51:42.300 --> 00:51:43.120]   but you know what?
[00:51:43.180 --> 00:51:46.340]   This could be something really interesting.
[00:51:46.340 --> 00:51:48.140]   - Deep questions with Kyle Newport.
[00:51:48.140 --> 00:51:50.580]   - I do it all in that voice.
[00:51:50.580 --> 00:51:53.500]   - I love the podcast, by the way.
[00:51:53.500 --> 00:51:56.740]   But yeah, I think David Foster Wallace said,
[00:51:56.740 --> 00:51:59.100]   "The key to life is to be unboreable."
[00:51:59.100 --> 00:52:01.840]   I've always kind of taken that to heart,
[00:52:01.840 --> 00:52:04.660]   which is like, you should be able to,
[00:52:04.660 --> 00:52:08.380]   maybe artificially, generate anything.
[00:52:10.780 --> 00:52:14.960]   Find something in your environment, in your surroundings,
[00:52:14.960 --> 00:52:16.080]   that's a source of joy.
[00:52:16.080 --> 00:52:17.400]   Like, everything is fun.
[00:52:17.400 --> 00:52:18.520]   - Yeah.
[00:52:18.520 --> 00:52:20.160]   Did you read "The Pale King"?
[00:52:20.160 --> 00:52:21.540]   It goes deep on boredom.
[00:52:21.540 --> 00:52:22.980]   It's like uncomfortable.
[00:52:22.980 --> 00:52:25.680]   It's like an uncomfortable meditation on boredom.
[00:52:25.680 --> 00:52:27.700]   Like, the characters in that are just driven
[00:52:27.700 --> 00:52:30.000]   to the extremes of...
[00:52:30.000 --> 00:52:33.340]   I just bought three books on boredom the other day.
[00:52:33.340 --> 00:52:35.200]   So now I'm really interested in this topic.
[00:52:35.200 --> 00:52:37.200]   Because I was anxious about my book launch
[00:52:37.200 --> 00:52:38.040]   happening this week.
[00:52:38.040 --> 00:52:39.560]   So I was like, okay, I need something else.
[00:52:39.560 --> 00:52:41.120]   So I have this idea for a...
[00:52:41.120 --> 00:52:43.580]   I might do it as an article first, but as a book.
[00:52:43.580 --> 00:52:46.760]   Like, okay, I need something cool to be thinking about.
[00:52:46.760 --> 00:52:48.800]   Because I was worried about, like, I don't know.
[00:52:48.800 --> 00:52:49.800]   Is the launch gonna work?
[00:52:49.800 --> 00:52:51.200]   The pandemic, what's gonna happen?
[00:52:51.200 --> 00:52:52.140]   I don't know if it's gonna get there.
[00:52:52.140 --> 00:52:54.120]   So this is exactly what we're talking about.
[00:52:54.120 --> 00:52:56.040]   So I went out and I bought a bunch of books,
[00:52:56.040 --> 00:53:00.200]   and I'm beginning a whole sort of intellectual exploration.
[00:53:00.200 --> 00:53:03.000]   - Well, I think that's one of the profound ideas
[00:53:03.000 --> 00:53:08.000]   in deep work that you don't expand on too much is boredom.
[00:53:08.360 --> 00:53:12.520]   - Yeah, well, so deep work had a superficial idea
[00:53:12.520 --> 00:53:13.800]   about boredom, which was...
[00:53:13.800 --> 00:53:16.040]   I had this chapter called "Embrace Boredom."
[00:53:16.040 --> 00:53:19.720]   And a very functionalist idea was basically,
[00:53:19.720 --> 00:53:21.720]   you have to have some boredom in your regular schedule,
[00:53:21.720 --> 00:53:24.960]   or your mind is gonna form a Pavlovian connection
[00:53:24.960 --> 00:53:28.440]   between as soon as I feel boredom, I get stimuli.
[00:53:28.440 --> 00:53:29.600]   And once it forms that connection,
[00:53:29.600 --> 00:53:30.800]   it's never gonna tolerate deep work.
[00:53:30.800 --> 00:53:34.240]   So there's this very pragmatic treatment of boredom
[00:53:34.240 --> 00:53:36.640]   of your mind better be used to the idea
[00:53:36.640 --> 00:53:37.880]   that sometimes you don't get stimuli,
[00:53:37.880 --> 00:53:39.760]   because otherwise you can't write for three hours.
[00:53:39.760 --> 00:53:41.780]   Like it's just not gonna tolerate it.
[00:53:41.780 --> 00:53:44.080]   But more recently, what I'm really interested in boredom
[00:53:44.080 --> 00:53:47.200]   is it as a fundamental human drive, right?
[00:53:47.200 --> 00:53:49.720]   Because it's incredibly uncomfortable.
[00:53:49.720 --> 00:53:50.960]   And think about the other things
[00:53:50.960 --> 00:53:53.240]   that are incredibly uncomfortable, like hunger or thirst.
[00:53:53.240 --> 00:53:56.640]   They serve a really important purpose for our species, right?
[00:53:56.640 --> 00:53:58.720]   Like if something is really distressing, there's a reason.
[00:53:58.720 --> 00:53:59.960]   Pain is really uncomfortable
[00:53:59.960 --> 00:54:02.360]   because we need to worry about getting injured.
[00:54:02.360 --> 00:54:03.640]   Thirst is really uncomfortable
[00:54:03.640 --> 00:54:05.560]   because we need water to survive.
[00:54:05.560 --> 00:54:07.000]   So what's boredom?
[00:54:07.000 --> 00:54:08.680]   Why is that uncomfortable?
[00:54:08.680 --> 00:54:11.480]   And I've been interested in this notion
[00:54:11.480 --> 00:54:16.480]   that boredom is about driving us towards productive action.
[00:54:16.480 --> 00:54:19.160]   Like as a species, I mean, think about it.
[00:54:19.160 --> 00:54:22.320]   Like what got us to actually take advantage of these brains?
[00:54:22.320 --> 00:54:24.400]   What got us to actually work with fire?
[00:54:24.400 --> 00:54:27.660]   What got us to start shaping stones and the hand axes
[00:54:27.660 --> 00:54:29.680]   and figuring out if we could actually sharpen a stick
[00:54:29.680 --> 00:54:32.320]   sharp enough that we could throw it as a melee weapon
[00:54:32.320 --> 00:54:35.240]   or a distance weapon for hunting mammoth, right?
[00:54:35.240 --> 00:54:37.960]   Boredom drives us towards action.
[00:54:37.960 --> 00:54:41.480]   So now I'm fascinated by this fundamental action instinct
[00:54:41.480 --> 00:54:43.560]   because I have this theory that I'm working on
[00:54:43.560 --> 00:54:45.800]   that we're out of sync with it.
[00:54:45.800 --> 00:54:47.940]   Just like we have this drive for hunger,
[00:54:47.940 --> 00:54:49.080]   but then we introduced junk food
[00:54:49.080 --> 00:54:50.300]   and got out of sync with hunger
[00:54:50.300 --> 00:54:52.080]   and it makes us really unhealthy.
[00:54:52.080 --> 00:54:53.420]   We have this drive towards action,
[00:54:53.420 --> 00:54:55.400]   but then we overload ourselves
[00:54:55.400 --> 00:54:56.800]   and we have all of these distractions.
[00:54:56.800 --> 00:54:58.600]   And then that causes,
[00:54:58.600 --> 00:55:01.440]   it's like a cognitive action obesity type things
[00:55:01.440 --> 00:55:02.840]   because it short circuits the system
[00:55:02.840 --> 00:55:03.800]   that wants us to do things,
[00:55:03.800 --> 00:55:04.840]   but we put more things on our plate
[00:55:04.840 --> 00:55:05.720]   than we can possibly do.
[00:55:05.720 --> 00:55:07.480]   And then we're really frustrated we can't do them.
[00:55:07.480 --> 00:55:09.200]   And we're short circuiting all of our wires.
[00:55:09.200 --> 00:55:11.560]   So it all comes back to this question,
[00:55:11.560 --> 00:55:16.560]   well, what would be the ideal amount of stuff to do
[00:55:16.560 --> 00:55:18.400]   and type of things to do?
[00:55:18.400 --> 00:55:19.320]   Like if we wanted to look back
[00:55:19.320 --> 00:55:21.720]   at our ancestral environment and say,
[00:55:21.720 --> 00:55:24.000]   if I could just build from scratch,
[00:55:24.000 --> 00:55:26.520]   how much work I do and what I work on
[00:55:26.520 --> 00:55:28.640]   to be as in touch with that as like paleo people
[00:55:28.640 --> 00:55:30.120]   are trying to get their diets in touch with that.
[00:55:30.120 --> 00:55:31.320]   And so now I'm just,
[00:55:31.320 --> 00:55:34.040]   but see, it's something I made up.
[00:55:34.040 --> 00:55:36.160]   But now I'm going deep on it.
[00:55:36.160 --> 00:55:37.560]   And one of my podcast listeners,
[00:55:37.560 --> 00:55:39.320]   I was talking about on the show and I was like,
[00:55:39.320 --> 00:55:41.440]   well, I keep trying to learn about animals and boredom.
[00:55:41.440 --> 00:55:42.880]   And she sent me this cool article
[00:55:42.880 --> 00:55:44.600]   from an animal behaviorist journal
[00:55:44.600 --> 00:55:48.120]   about what we know about human boredom versus animal boredom.
[00:55:48.120 --> 00:55:50.000]   So trying to figure out that puzzle
[00:55:50.000 --> 00:55:52.280]   is the wave that's high.
[00:55:52.280 --> 00:55:54.160]   So I can get through the wave that's low of like,
[00:55:54.160 --> 00:55:55.700]   I don't know about this pandemic book launch.
[00:55:55.700 --> 00:55:59.800]   And my research is stumbling a little bit
[00:55:59.800 --> 00:56:00.640]   because of the pandemic.
[00:56:00.640 --> 00:56:03.680]   And so I needed a nice high.
[00:56:03.680 --> 00:56:05.360]   So there we go, there's a case study.
[00:56:05.360 --> 00:56:07.520]   - Well, it's both a case study
[00:56:07.520 --> 00:56:09.320]   and a very interesting set of concepts
[00:56:09.320 --> 00:56:12.280]   'cause I didn't even realize that it's so simple.
[00:56:12.280 --> 00:56:17.280]   I'm one of the people that has a interesting
[00:56:17.280 --> 00:56:18.980]   push and pull dynamic with hunger,
[00:56:18.980 --> 00:56:21.200]   trying to understand the hunger with myself.
[00:56:21.200 --> 00:56:24.680]   Like I probably have an unhealthy relationship with food.
[00:56:24.680 --> 00:56:28.480]   I don't know, but there's probably a perfect,
[00:56:28.480 --> 00:56:32.800]   that's a nice way to think about diet as action.
[00:56:32.800 --> 00:56:36.520]   There's probably an optimal diet response
[00:56:36.520 --> 00:56:40.340]   to the experience that our body's telling us,
[00:56:40.340 --> 00:56:43.280]   the signal that our body's sending, which is hunger.
[00:56:43.280 --> 00:56:46.840]   And in that same way, boredom is sending a signal.
[00:56:46.840 --> 00:56:49.960]   And most of our intellectual activities in this world,
[00:56:49.960 --> 00:56:52.680]   our creative activities are essentially
[00:56:52.680 --> 00:56:56.600]   a response to that signal.
[00:56:56.600 --> 00:56:59.640]   - Yeah, and think about this analogy
[00:56:59.640 --> 00:57:01.080]   that we have this hunger instinct
[00:57:01.080 --> 00:57:03.360]   that junk food short circuits.
[00:57:03.360 --> 00:57:04.200]   - Yes.
[00:57:04.200 --> 00:57:06.720]   - Right, it's like, oh, we'll satisfy that hyper-palatably
[00:57:06.720 --> 00:57:08.240]   and it doesn't end up well.
[00:57:08.240 --> 00:57:11.760]   Now think about modern attention engineered,
[00:57:11.760 --> 00:57:14.520]   digitally mediated entertainment.
[00:57:14.520 --> 00:57:16.000]   We have this boredom instinct.
[00:57:16.000 --> 00:57:17.760]   Oh, we can take care of that
[00:57:17.760 --> 00:57:20.500]   with a hyper-palatable alternative.
[00:57:20.500 --> 00:57:22.260]   Is that gonna lead to a similar problem?
[00:57:22.260 --> 00:57:23.720]   - So I've been fasting a lot lately.
[00:57:23.720 --> 00:57:27.660]   Like I'm doing eating once a day.
[00:57:27.660 --> 00:57:29.620]   I've been doing that for over a month.
[00:57:29.620 --> 00:57:33.800]   Just eating one meal a day and primarily meat.
[00:57:33.800 --> 00:57:38.280]   But it's very, fasting has been incredible for me,
[00:57:38.280 --> 00:57:40.880]   for focus, for wellbeing, for a few,
[00:57:40.880 --> 00:57:42.680]   I don't know, just for feeling good.
[00:57:42.680 --> 00:57:45.680]   Okay, we'll put on a chart what makes me feel good.
[00:57:45.680 --> 00:57:50.680]   And that fasting and eating primarily a meat-based diet
[00:57:50.680 --> 00:57:52.500]   makes me feel really good.
[00:57:52.500 --> 00:57:57.500]   And so, but that ultimately, what fasting did,
[00:57:57.800 --> 00:58:00.500]   I haven't fasted super long yet, like a seven-day diet,
[00:58:00.500 --> 00:58:02.140]   which I really like to do.
[00:58:02.140 --> 00:58:05.120]   But even just fasting for a day for 24 hours
[00:58:05.120 --> 00:58:09.980]   gets you in touch with the signal.
[00:58:09.980 --> 00:58:10.820]   It's fascinating.
[00:58:10.820 --> 00:58:12.340]   Like you get to listen to your,
[00:58:12.340 --> 00:58:15.040]   learn to listen to your body that like,
[00:58:15.040 --> 00:58:17.720]   you know, it's okay to be hungry.
[00:58:17.720 --> 00:58:19.780]   It's like a little signal that sends you stuff.
[00:58:19.780 --> 00:58:24.140]   And then I get to listen to how it responds
[00:58:24.140 --> 00:58:27.540]   when I put food in my body.
[00:58:27.540 --> 00:58:30.480]   And I get to like, okay, cool.
[00:58:30.480 --> 00:58:33.760]   So like food is a thing that pacifies the signal.
[00:58:33.760 --> 00:58:35.760]   Like it sounds ridiculous, okay.
[00:58:35.760 --> 00:58:36.600]   You could do that with-
[00:58:36.600 --> 00:58:38.320]   - And do different types of food.
[00:58:38.320 --> 00:58:39.160]   It feels different.
[00:58:39.160 --> 00:58:41.660]   So you learn about what your body wants.
[00:58:41.660 --> 00:58:44.140]   - For some reason, fasting,
[00:58:44.140 --> 00:58:47.360]   it's similar to the deep work embrace boredom.
[00:58:47.360 --> 00:58:50.400]   Fasting allowed me to go into mode of listening,
[00:58:50.400 --> 00:58:52.080]   of trying to understand the signal,
[00:58:52.080 --> 00:58:56.920]   that I could say I have an unhealthy appreciation of fruit.
[00:58:56.920 --> 00:58:57.860]   Okay.
[00:58:57.860 --> 00:58:59.540]   I love apples and cherries.
[00:58:59.540 --> 00:59:01.420]   Like I don't know how to moderate them.
[00:59:01.420 --> 00:59:03.500]   So if you take just same amount of calories,
[00:59:03.500 --> 00:59:04.840]   I don't know, calories matter,
[00:59:04.840 --> 00:59:05.960]   but they say calories,
[00:59:05.960 --> 00:59:10.960]   2000 calories of cherries versus 2000 calories of steak.
[00:59:10.960 --> 00:59:13.300]   If I eat 2000 calories of steak,
[00:59:13.300 --> 00:59:17.180]   maybe just a little bit of like green beans or cauliflower,
[00:59:17.180 --> 00:59:22.180]   I'm going to feel really good, fulfilled, focused, and happy.
[00:59:22.180 --> 00:59:24.460]   If I eat cherries, I'm going to be,
[00:59:24.460 --> 00:59:27.780]   I'm going to wake up behind a dumpster crying with like,
[00:59:27.780 --> 00:59:29.780]   naked and like, it's just-
[00:59:29.780 --> 00:59:30.620]   - Pits all around.
[00:59:30.620 --> 00:59:31.460]   - Yeah, with everything.
[00:59:31.460 --> 00:59:32.300]   - With a face, yeah.
[00:59:32.300 --> 00:59:36.420]   - And just like bloated, just not, and unhappy.
[00:59:36.420 --> 00:59:39.740]   And also the mood swings up and down.
[00:59:39.740 --> 00:59:41.340]   I don't know.
[00:59:41.340 --> 00:59:44.740]   And I'll be much hungrier the next day.
[00:59:44.740 --> 00:59:46.220]   Sometimes it takes a couple of days,
[00:59:46.220 --> 00:59:50.100]   but when I introduce carbs into the system, too many carbs,
[00:59:50.100 --> 00:59:53.020]   it starts, it's just unhealthy.
[00:59:53.020 --> 00:59:56.020]   I go into this roller coaster as opposed to a calm boat ride
[00:59:56.020 --> 00:59:58.540]   along the river in the Amazon or something like that.
[00:59:58.540 --> 01:00:01.140]   And so fasting was the mechanism of,
[01:00:01.140 --> 01:00:03.940]   for me to start listening to the body.
[01:00:03.940 --> 01:00:05.940]   I wonder if you can do that same kind of,
[01:00:05.940 --> 01:00:07.860]   I guess that's what meditation a little bit is.
[01:00:07.860 --> 01:00:10.100]   - A little bit, but yeah, listen to boredom.
[01:00:10.100 --> 01:00:11.020]   But so two years ago,
[01:00:11.020 --> 01:00:13.540]   I had a book out called "Digital Minimalism."
[01:00:13.540 --> 01:00:16.100]   And one of the things I was recommending that people do
[01:00:16.100 --> 01:00:18.300]   is basically a 30-day fast,
[01:00:18.300 --> 01:00:20.420]   but from digital personal entertainment,
[01:00:20.420 --> 01:00:21.900]   social media, online videos,
[01:00:21.900 --> 01:00:26.420]   anything that captures your attention and dispels boredom.
[01:00:26.420 --> 01:00:29.540]   And people were thinking like, oh, this is a detox.
[01:00:29.540 --> 01:00:30.700]   Like I just want to teach your body
[01:00:30.700 --> 01:00:32.780]   not to need the distraction or this or that,
[01:00:32.780 --> 01:00:34.300]   but it really wasn't what I was interested in.
[01:00:34.300 --> 01:00:37.660]   I wanted there to be space
[01:00:37.660 --> 01:00:39.380]   that you could listen to your boredom.
[01:00:39.380 --> 01:00:41.100]   Like, okay, I can't just dispel it.
[01:00:41.100 --> 01:00:42.540]   I can't just look at the screen
[01:00:42.540 --> 01:00:45.140]   and revel in it a little bit and start to listen to it
[01:00:45.140 --> 01:00:48.020]   and say, what is this really pushing me towards?
[01:00:48.020 --> 01:00:49.460]   And you take the new stuff,
[01:00:49.460 --> 01:00:51.460]   the new technology off the table and sort of ask,
[01:00:51.460 --> 01:00:53.220]   what is this, what am I craving?
[01:00:53.220 --> 01:00:57.300]   Like, what's the activity equivalent of 2000 calories of meat
[01:00:57.300 --> 01:00:59.380]   with a little bit of green beans on the side?
[01:00:59.380 --> 01:01:01.620]   And I had 1700 people go through this experiment,
[01:01:01.620 --> 01:01:03.420]   like spend 30 days doing this.
[01:01:03.420 --> 01:01:04.340]   And it's hard at first,
[01:01:04.340 --> 01:01:06.980]   but then they get used to listening to themselves
[01:01:06.980 --> 01:01:07.860]   and sort of seeking out
[01:01:07.860 --> 01:01:09.740]   what is this really pushing me towards?
[01:01:09.740 --> 01:01:12.260]   And it was pushing people towards connection.
[01:01:12.260 --> 01:01:13.460]   It was pushing people towards,
[01:01:13.460 --> 01:01:15.580]   I just want to go be around other people.
[01:01:15.580 --> 01:01:19.340]   It was pushing people towards high quality leisure activities
[01:01:19.340 --> 01:01:21.700]   like I want to go do something that's complicated.
[01:01:21.700 --> 01:01:23.260]   And it took weeks sometimes for them
[01:01:23.260 --> 01:01:25.140]   to get in touch with their boredom,
[01:01:25.140 --> 01:01:28.780]   but then it completely rewired how they thought about
[01:01:28.780 --> 01:01:30.780]   what do I want to do with my time outside of work?
[01:01:30.780 --> 01:01:32.060]   And then the idea is when you're done with that,
[01:01:32.060 --> 01:01:33.340]   then it was much easier to go back
[01:01:33.340 --> 01:01:34.940]   and completely change your digital life
[01:01:34.940 --> 01:01:37.420]   because you have alternatives, right?
[01:01:37.420 --> 01:01:39.860]   You're not just trying to abstain from things you don't like
[01:01:39.860 --> 01:01:42.660]   but that's basically a listening to boredom experiment.
[01:01:42.660 --> 01:01:45.140]   Like just be there with the boredom
[01:01:45.140 --> 01:01:46.380]   and see where it drives you
[01:01:46.380 --> 01:01:48.860]   when you don't have the digital cheese it's.
[01:01:48.860 --> 01:01:52.020]   Okay, so if I can't do that, where is it gonna drive me?
[01:01:52.020 --> 01:01:53.940]   Well, I guess I kind of want to go to the library,
[01:01:53.940 --> 01:01:54.780]   which came up a lot by the way.
[01:01:54.780 --> 01:01:57.460]   A lot of people rediscovered the library.
[01:01:57.460 --> 01:01:58.300]   - With physical books.
[01:01:58.300 --> 01:02:00.460]   - Physical books, so you can just go borrow them.
[01:02:00.460 --> 01:02:03.180]   And there's low pressure and you can explore
[01:02:03.180 --> 01:02:04.860]   and you bring them home and then you read them
[01:02:04.860 --> 01:02:06.620]   and you can sit by the window and read them
[01:02:06.620 --> 01:02:07.660]   and it's nice weather outside.
[01:02:07.660 --> 01:02:09.740]   And I used to do that 20 years ago.
[01:02:09.740 --> 01:02:10.980]   They're listening to boredom.
[01:02:10.980 --> 01:02:12.980]   - So can you maybe elaborate a little bit
[01:02:12.980 --> 01:02:15.620]   on the different experiences that people had
[01:02:15.620 --> 01:02:17.820]   when they quit social media for 30 days?
[01:02:17.820 --> 01:02:20.740]   Like if you were to recommend that process,
[01:02:20.740 --> 01:02:23.020]   what is ultimately the goal?
[01:02:23.020 --> 01:02:24.940]   - Yeah, digital minimalism,
[01:02:24.940 --> 01:02:27.740]   that's my philosophy for all this tech.
[01:02:27.740 --> 01:02:30.700]   And it's working backwards from what's important.
[01:02:30.700 --> 01:02:33.180]   So it's, you figure out what you're actually all about,
[01:02:33.180 --> 01:02:34.140]   like what you want to do,
[01:02:34.140 --> 01:02:35.900]   what you want to spend your time doing.
[01:02:35.900 --> 01:02:37.660]   And then you can ask, okay,
[01:02:37.660 --> 01:02:39.180]   is there a place that tech could amplify
[01:02:39.180 --> 01:02:40.180]   or support some of these things?
[01:02:40.180 --> 01:02:42.860]   And that's how you decide what tech to use.
[01:02:42.860 --> 01:02:45.300]   And so the process is let's actually
[01:02:45.300 --> 01:02:46.500]   get away from everything.
[01:02:46.500 --> 01:02:47.580]   Let's be bored for a while.
[01:02:47.580 --> 01:02:48.980]   Let's really spend a month getting,
[01:02:48.980 --> 01:02:51.060]   really figuring out what do I actually want to do?
[01:02:51.060 --> 01:02:52.180]   What do I want to spend my time doing?
[01:02:52.180 --> 01:02:53.820]   What's important to me?
[01:02:53.820 --> 01:02:54.700]   What makes me feel good?
[01:02:54.700 --> 01:02:55.540]   And then when you're done,
[01:02:55.540 --> 01:02:57.140]   you can bring back in tech very strategically
[01:02:57.140 --> 01:02:58.740]   to help those things, right?
[01:02:58.740 --> 01:02:59.980]   And that was the goal.
[01:02:59.980 --> 01:03:01.860]   That turns out to be much more successful
[01:03:01.860 --> 01:03:05.500]   than when people take a abstention only approach.
[01:03:05.500 --> 01:03:09.060]   So if you come at your tech life and say,
[01:03:09.060 --> 01:03:10.700]   whatever, I look at Instagram too much.
[01:03:10.700 --> 01:03:12.860]   Like I don't like how much I'm on Instagram.
[01:03:12.860 --> 01:03:13.740]   That's a bad thing.
[01:03:13.740 --> 01:03:15.100]   I want to reduce this bad thing.
[01:03:15.100 --> 01:03:16.260]   So here's my new thing.
[01:03:16.260 --> 01:03:18.100]   I'm going to spend less time looking at Instagram,
[01:03:18.100 --> 01:03:20.580]   much less likely to succeed in the longterm.
[01:03:20.580 --> 01:03:23.100]   So we're much less likely at trying to reduce
[01:03:23.100 --> 01:03:25.500]   this sort of amorphous negative because in the moment,
[01:03:25.500 --> 01:03:27.220]   you're like, yeah, but it's not that bad.
[01:03:27.220 --> 01:03:29.140]   And it would be kind of interesting to look at it now.
[01:03:29.140 --> 01:03:30.780]   When you're instead controlling behavior
[01:03:30.780 --> 01:03:32.620]   because you have a positive that you're aiming towards,
[01:03:32.620 --> 01:03:33.620]   it's very powerful for people.
[01:03:33.620 --> 01:03:35.860]   Like I want my life to be like this.
[01:03:35.860 --> 01:03:39.140]   Here's the role that tech plays in that life.
[01:03:39.140 --> 01:03:41.260]   The connection to wanting your life to be like that
[01:03:41.260 --> 01:03:42.460]   is very, very strong.
[01:03:42.460 --> 01:03:43.700]   And then it's much, much easier to say,
[01:03:43.700 --> 01:03:45.780]   yeah, like using Instagram is not part of my plan
[01:03:45.780 --> 01:03:46.660]   for how I have that life.
[01:03:46.660 --> 01:03:47.780]   And I really want to have that life.
[01:03:47.780 --> 01:03:49.060]   So of course I'm not going to use Instagram.
[01:03:49.060 --> 01:03:51.780]   So it turns out to be a much more sustainable way
[01:03:51.780 --> 01:03:53.260]   to tame what's going on.
[01:03:53.260 --> 01:03:55.340]   - So if you quit social media for 30 days,
[01:03:55.340 --> 01:03:58.100]   you kind of have to do the work.
[01:03:58.100 --> 01:03:59.340]   - You have to do the work.
[01:03:59.340 --> 01:04:01.460]   - Of thinking like, what am I actually,
[01:04:01.460 --> 01:04:04.340]   what makes me happy in terms of these tools
[01:04:04.340 --> 01:04:05.620]   that I've previously used?
[01:04:05.620 --> 01:04:08.940]   And when you try to integrate them back,
[01:04:08.940 --> 01:04:11.100]   how can I integrate them to maximize the thing
[01:04:11.100 --> 01:04:11.940]   that actually makes me happy?
[01:04:11.940 --> 01:04:14.500]   - Yeah, or what makes me happy unrelated to technology?
[01:04:14.500 --> 01:04:16.300]   Like, what do I actually, what do I want my life to be like?
[01:04:16.300 --> 01:04:18.660]   Well, maybe what I want to do is be outside of nature
[01:04:18.660 --> 01:04:20.220]   two hours a day and spend a lot more time
[01:04:20.220 --> 01:04:22.020]   like helping my community and sacrificing
[01:04:22.020 --> 01:04:23.180]   on behalf of my connections,
[01:04:23.180 --> 01:04:26.500]   and then have some sort of intellectually engaging
[01:04:26.500 --> 01:04:28.220]   leisure activity, like I'm reading
[01:04:28.220 --> 01:04:29.620]   or trying to read the great books
[01:04:29.620 --> 01:04:31.700]   and having more calm and seeing the sunset.
[01:04:31.700 --> 01:04:35.180]   Like you create this picture and then you go back and say,
[01:04:35.180 --> 01:04:36.580]   well, I still need my Facebook group
[01:04:36.580 --> 01:04:39.820]   because that's how I keep up with my cycling group.
[01:04:39.820 --> 01:04:41.300]   But Twitter is just, you know,
[01:04:41.300 --> 01:04:42.820]   toxic's not helping any of these things.
[01:04:42.820 --> 01:04:45.420]   And well, I'm an artist, so I kind of need Instagram
[01:04:45.420 --> 01:04:46.260]   to get inspiration.
[01:04:46.260 --> 01:04:48.180]   But if I know that's why I'm using Instagram,
[01:04:48.180 --> 01:04:49.980]   I don't need it on my phone, it's just on my computer.
[01:04:49.980 --> 01:04:51.900]   And I just follow 10 artists and check it once a week.
[01:04:51.900 --> 01:04:54.060]   Like you really can start deploying.
[01:04:54.060 --> 01:04:55.820]   It was the number one thing that differentiated
[01:04:55.820 --> 01:04:58.220]   in that experiment, the people who ended up
[01:04:58.220 --> 01:05:00.940]   sustainably making changes and getting through the 30 days
[01:05:00.940 --> 01:05:03.060]   and those who didn't, was the people who did
[01:05:03.060 --> 01:05:04.580]   the experimentation and the reflection.
[01:05:04.580 --> 01:05:07.500]   Like, let me try to figure out what's positive.
[01:05:07.500 --> 01:05:09.180]   They were much more successful than the people
[01:05:09.180 --> 01:05:11.940]   that just said, I'm sick of using my phone so much.
[01:05:11.940 --> 01:05:12.900]   So I'm just gonna white knuckle it.
[01:05:12.900 --> 01:05:14.100]   Just 30 days will be good for me.
[01:05:14.100 --> 01:05:16.540]   I just gotta, I just gotta get away from it or something.
[01:05:16.540 --> 01:05:17.620]   It doesn't last.
[01:05:17.620 --> 01:05:19.820]   - So you don't use social media currently.
[01:05:19.820 --> 01:05:21.060]   - Yeah.
[01:05:21.060 --> 01:05:24.340]   - Do you find that a lot of people going through this process
[01:05:24.340 --> 01:05:29.340]   will seek to basically arrive at a similar place
[01:05:29.340 --> 01:05:30.900]   to not use social media primarily?
[01:05:30.900 --> 01:05:32.460]   - About half, right.
[01:05:32.460 --> 01:05:34.700]   So about half when they went through this exercise,
[01:05:34.700 --> 01:05:36.700]   and these aren't quantified numbers.
[01:05:36.700 --> 01:05:40.100]   You know, this is just, they sent me reports and yeah.
[01:05:40.100 --> 01:05:42.060]   - That's pretty good though, 1,700?
[01:05:42.060 --> 01:05:43.380]   - Yeah, yeah.
[01:05:43.380 --> 01:05:47.220]   So roughly half probably got rid of social media altogether.
[01:05:47.220 --> 01:05:48.260]   Once they did this exercise,
[01:05:48.260 --> 01:05:50.140]   they realized these things I care about,
[01:05:50.140 --> 01:05:53.580]   I don't, social media is not the tools that's really helping.
[01:05:53.580 --> 01:05:56.060]   The other half kept some, there were some things
[01:05:56.060 --> 01:05:59.060]   in their life where some social media was useful.
[01:05:59.060 --> 01:06:01.180]   But the key thing is, if they knew why they were deploying
[01:06:01.180 --> 01:06:04.380]   social media, they could put fences around it.
[01:06:04.380 --> 01:06:07.100]   So for example, of those half that kept some social media,
[01:06:07.100 --> 01:06:09.460]   almost none of them kept it on their phone.
[01:06:09.460 --> 01:06:10.300]   - Oh, interesting.
[01:06:10.300 --> 01:06:12.100]   - Yeah, you can't optimize if you don't know
[01:06:12.100 --> 01:06:13.620]   what the function you're trying to optimize.
[01:06:13.620 --> 01:06:14.660]   So it's like this huge hack.
[01:06:14.660 --> 01:06:16.940]   It's like, once you know this is why I'm using Twitter,
[01:06:16.940 --> 01:06:19.180]   then you can have a lot of rules about how you use Twitter.
[01:06:19.180 --> 01:06:21.660]   And suddenly you take this cost benefit ratio
[01:06:21.660 --> 01:06:24.140]   and it goes like way from the company's advantage
[01:06:24.140 --> 01:06:25.820]   and then way over towards your advantage.
[01:06:25.820 --> 01:06:28.700]   - It's kind of fascinating 'cause I've been torn
[01:06:28.700 --> 01:06:30.580]   with social media, but I did this kind of process.
[01:06:30.580 --> 01:06:32.300]   I haven't actually done it for 30 days,
[01:06:32.300 --> 01:06:33.740]   which I probably should.
[01:06:33.740 --> 01:06:36.100]   I'll do it for like a week at a time and regularly
[01:06:36.100 --> 01:06:41.100]   and thinking what kind of approach to Twitter works for me.
[01:06:41.100 --> 01:06:47.940]   I'm distinctly aware of the fact that I really enjoy
[01:06:47.940 --> 01:06:51.160]   posting once or twice a day.
[01:06:51.160 --> 01:06:55.060]   And at that time checking from the previous post,
[01:06:55.060 --> 01:06:59.780]   it makes me feel, even when there's negative comments,
[01:06:59.780 --> 01:07:01.340]   they go right past me.
[01:07:01.340 --> 01:07:03.500]   And when there's positive comments, it makes you smile.
[01:07:03.500 --> 01:07:06.140]   I feel like love and connection with people,
[01:07:06.140 --> 01:07:08.500]   especially if people I know, but even just in general,
[01:07:08.500 --> 01:07:10.300]   it's like, it makes me feel like the world
[01:07:10.300 --> 01:07:12.220]   is full of awesome people.
[01:07:12.220 --> 01:07:15.380]   Okay, when you increase that from checking from two to,
[01:07:15.380 --> 01:07:17.500]   like, I don't know what the threshold is for me,
[01:07:17.500 --> 01:07:19.900]   but probably like five or six per day,
[01:07:19.900 --> 01:07:21.940]   it starts going to anxiety world,
[01:07:21.940 --> 01:07:25.620]   like where negative comments will actually stick
[01:07:25.620 --> 01:07:30.620]   to me mentally and positive comments will feel more shallow.
[01:07:32.380 --> 01:07:33.660]   - Yeah, yeah.
[01:07:33.660 --> 01:07:34.660]   - It's kind of fascinating.
[01:07:34.660 --> 01:07:39.660]   So I've been trying to, there's been long stretches
[01:07:39.660 --> 01:07:43.300]   of time, I think December and January,
[01:07:43.300 --> 01:07:46.340]   where I did just post and check, post and check.
[01:07:46.340 --> 01:07:49.020]   That makes me really happy.
[01:07:49.020 --> 01:07:52.540]   Most of 2020 I did that, it made me really happy.
[01:07:52.540 --> 01:07:56.060]   Recently I started, like, I'll go, you know,
[01:07:56.060 --> 01:07:57.980]   you go right back in like a drug addict
[01:07:57.980 --> 01:08:00.700]   where you check it like, I don't know what that number is,
[01:08:00.700 --> 01:08:01.660]   but that number is high.
[01:08:01.660 --> 01:08:02.500]   It's not good.
[01:08:02.500 --> 01:08:03.860]   You don't come out happy.
[01:08:03.860 --> 01:08:06.020]   No one comes out of a day full of Twitter
[01:08:06.020 --> 01:08:07.300]   celebrating humanity.
[01:08:07.300 --> 01:08:10.380]   - And it's not even, 'cause I'm very fortunate
[01:08:10.380 --> 01:08:12.900]   to have a lot of just like positivity in the Twitter,
[01:08:12.900 --> 01:08:16.180]   but there's just a general anxiety.
[01:08:16.180 --> 01:08:19.860]   I wouldn't even say, I wouldn't even say it's,
[01:08:19.860 --> 01:08:21.380]   it's probably the thing that you're talking about
[01:08:21.380 --> 01:08:22.620]   with the contact switching.
[01:08:22.620 --> 01:08:25.820]   It's almost like an exhaustion.
[01:08:25.820 --> 01:08:27.900]   I wouldn't even say it's like a negative feeling.
[01:08:27.900 --> 01:08:30.740]   It's almost just an exhaustion to where I'm not creating
[01:08:30.740 --> 01:08:33.820]   anything beautiful in my life, just exhausted.
[01:08:33.820 --> 01:08:35.220]   - Like an existential exhaustion.
[01:08:35.220 --> 01:08:36.900]   - Existential exhaustion.
[01:08:36.900 --> 01:08:39.540]   But I wonder, do you think it's possible to use,
[01:08:39.540 --> 01:08:42.580]   from the people you've seen, from yourself,
[01:08:42.580 --> 01:08:45.500]   to use social media in the way I'm describing, moderation,
[01:08:45.500 --> 01:08:48.060]   or is it always going to become?
[01:08:48.060 --> 01:08:49.220]   - When people do this exercise,
[01:08:49.220 --> 01:08:52.340]   you get lots of configurations.
[01:08:52.340 --> 01:08:56.100]   So for people that have a public presence, for example,
[01:08:56.100 --> 01:08:58.820]   like what you're doing is not that unusual.
[01:08:58.820 --> 01:09:02.220]   Okay, I post one thing a day and my audience likes it
[01:09:02.220 --> 01:09:04.860]   and that's kind of it, which, but you've thought through,
[01:09:04.860 --> 01:09:06.940]   like, okay, this supports something I value,
[01:09:06.940 --> 01:09:09.420]   which is like having a sort of informal connection
[01:09:09.420 --> 01:09:12.140]   with my audience and being exposed
[01:09:12.140 --> 01:09:16.020]   to some sort of positive randomness.
[01:09:16.020 --> 01:09:16.860]   - Yes.
[01:09:16.860 --> 01:09:18.900]   - Okay, then you could say, if that's my goal,
[01:09:18.900 --> 01:09:19.740]   what's the right way to do it?
[01:09:19.740 --> 01:09:21.260]   Well, I don't need to be on Twitter on my phone all day.
[01:09:21.260 --> 01:09:23.180]   Maybe what I do is every day at five,
[01:09:23.180 --> 01:09:25.660]   I do my post and check on the day.
[01:09:25.660 --> 01:09:28.980]   So I have a writer friend, Ryan Holiday,
[01:09:28.980 --> 01:09:30.940]   who writes about the Stoics a lot,
[01:09:30.940 --> 01:09:32.540]   and he has this similar strategy.
[01:09:32.540 --> 01:09:36.900]   He posts one quote every day, usually from a famous Stoic
[01:09:36.900 --> 01:09:38.100]   and sometimes from a contemporary figure,
[01:09:38.100 --> 01:09:38.940]   and that's just what he does.
[01:09:38.940 --> 01:09:41.700]   He just posts it and it's a very positive thing.
[01:09:41.700 --> 01:09:43.000]   Like his readers really love it
[01:09:43.000 --> 01:09:44.740]   because it's just like a dose of inspiration.
[01:09:44.740 --> 01:09:46.440]   He doesn't spend time,
[01:09:46.440 --> 01:09:49.180]   he's never interacting with anyone on social media, right?
[01:09:49.180 --> 01:09:50.820]   But that's an example of,
[01:09:50.820 --> 01:09:52.260]   I figured out what's important to me,
[01:09:52.260 --> 01:09:54.460]   what's the best way to use tools to amplify it,
[01:09:54.460 --> 01:09:56.700]   and then you get advantages out of the tools.
[01:09:56.700 --> 01:09:57.980]   So I like what you're doing.
[01:09:57.980 --> 01:10:00.980]   I looked up your Twitter feed before I came over here.
[01:10:00.980 --> 01:10:02.980]   I was curious, you're not on there a lot.
[01:10:02.980 --> 01:10:03.820]   - No.
[01:10:03.820 --> 01:10:04.640]   - I don't see you yelling at people.
[01:10:04.640 --> 01:10:08.020]   Now, do you think social media as a medium
[01:10:08.020 --> 01:10:09.540]   changed the cultural standards?
[01:10:09.540 --> 01:10:12.340]   And I mean it in a, have you read Neil Postman at all?
[01:10:12.340 --> 01:10:14.420]   Have you read like "Amusing Ourselves to Death"?
[01:10:14.420 --> 01:10:16.520]   He was a social critic, technology critic,
[01:10:16.520 --> 01:10:20.140]   and wrote a lot about sort of technological determinism.
[01:10:20.140 --> 01:10:22.840]   So the ways, which is a really influential idea
[01:10:22.840 --> 01:10:24.620]   to a lot of my work, which is actually a little out
[01:10:24.620 --> 01:10:25.960]   of fashion right now in academia,
[01:10:25.960 --> 01:10:28.900]   but the ways that the properties and presence
[01:10:28.900 --> 01:10:31.540]   of technologies change things about humans
[01:10:31.540 --> 01:10:32.980]   in a way that's not really intended
[01:10:32.980 --> 01:10:34.260]   or planned by the humans themselves.
[01:10:34.260 --> 01:10:35.460]   And he has, that book is all about
[01:10:35.460 --> 01:10:38.460]   how different communication medium,
[01:10:38.460 --> 01:10:39.700]   like fundamentally just changed the way
[01:10:39.700 --> 01:10:42.380]   the human brain understands and operates.
[01:10:42.380 --> 01:10:43.700]   And so he sort of gets into the,
[01:10:43.700 --> 01:10:45.980]   what happened when the printed word was widespread
[01:10:45.980 --> 01:10:47.540]   and how television changed it.
[01:10:47.540 --> 01:10:50.120]   And this was all pre-social media.
[01:10:50.120 --> 01:10:51.700]   But this one of these ideas I'm having is like,
[01:10:51.700 --> 01:10:52.780]   what's the degree to which,
[01:10:52.780 --> 01:10:54.300]   I get into it sometimes on my show,
[01:10:54.300 --> 01:10:55.140]   I get into a little bit,
[01:10:55.140 --> 01:10:58.340]   like the degree to which like Twitter in particular,
[01:10:58.340 --> 01:11:00.780]   just changed the way that people conceptualized what,
[01:11:00.780 --> 01:11:04.180]   for example, debate and discussion was.
[01:11:04.180 --> 01:11:06.380]   Like it introduced a rhetorical dunk culture,
[01:11:06.380 --> 01:11:09.260]   or it's sort of more about tribes not giving ground
[01:11:09.260 --> 01:11:10.580]   to other tribes.
[01:11:10.580 --> 01:11:12.340]   And it's like, it's a complete,
[01:11:12.340 --> 01:11:15.060]   there's different places and times
[01:11:15.060 --> 01:11:18.100]   when that type of discussion was thought of differently.
[01:11:18.100 --> 01:11:19.540]   - Well, yeah, absolutely.
[01:11:19.540 --> 01:11:20.780]   But I tend to believe,
[01:11:20.780 --> 01:11:21.620]   I don't know what you think,
[01:11:21.620 --> 01:11:23.580]   that there's the technological solutions.
[01:11:23.580 --> 01:11:27.860]   Like there's literally different features in Twitter
[01:11:27.860 --> 01:11:29.540]   that could completely reverse that.
[01:11:29.540 --> 01:11:32.980]   There's so much power in the different choices
[01:11:32.980 --> 01:11:33.800]   that are made.
[01:11:33.800 --> 01:11:36.540]   And it could still be highly engaging
[01:11:36.540 --> 01:11:37.740]   and have very different effects,
[01:11:37.740 --> 01:11:38.980]   perhaps more negative,
[01:11:38.980 --> 01:11:40.580]   or hopefully more positive.
[01:11:40.580 --> 01:11:42.740]   - Yeah, so I'm trying to pull these two things apart.
[01:11:42.740 --> 01:11:45.540]   So there's these two ways social media,
[01:11:45.540 --> 01:11:46.940]   let's say could change the experience
[01:11:46.940 --> 01:11:49.460]   of reading a major newspaper today.
[01:11:49.460 --> 01:11:51.300]   One could be a little bit more economic, right?
[01:11:51.300 --> 01:11:53.700]   So the internet made it cheaper to get news.
[01:11:53.700 --> 01:11:55.720]   The newspapers had to retreat to a paywall model
[01:11:55.720 --> 01:11:57.340]   because it was the only way they were gonna survive.
[01:11:57.340 --> 01:11:58.580]   But once you're in a paywall model,
[01:11:58.580 --> 01:12:01.740]   then what you really wanna do is make your tribe,
[01:12:01.740 --> 01:12:04.120]   which is within the paywall, very, very happy with you.
[01:12:04.120 --> 01:12:05.260]   So you wanna work to them.
[01:12:05.260 --> 01:12:07.860]   But then there's the sort of the determinist point of view,
[01:12:07.860 --> 01:12:10.660]   which is the properties of Twitter, which were arbitrary.
[01:12:10.660 --> 01:12:14.220]   Jack and Evan just, whatever, let's just do it this way,
[01:12:14.220 --> 01:12:16.060]   influenced the very way that people now understand
[01:12:16.060 --> 01:12:17.100]   and think about the world.
[01:12:17.100 --> 01:12:19.060]   - So the one influenced the other,
[01:12:19.060 --> 01:12:22.100]   I think they kind of started adjusting together.
[01:12:22.100 --> 01:12:25.380]   I did this thing, I mean, I'm trying to understand this.
[01:12:25.380 --> 01:12:30.380]   Part of the, I've been playing with the entrepreneurial idea
[01:12:30.380 --> 01:12:34.700]   that's a very particular dream I've had of a startup
[01:12:34.700 --> 01:12:37.260]   that this is a longer term thing
[01:12:37.260 --> 01:12:39.580]   that has to do with artificial intelligence.
[01:12:39.580 --> 01:12:43.020]   But more and more, it seems like there's some trajectory
[01:12:43.020 --> 01:12:47.420]   through creating social media type of technologies,
[01:12:47.420 --> 01:12:49.540]   very different than what people are thinking I'm doing.
[01:12:49.540 --> 01:12:54.540]   But it's a kind of challenge to the way that Twitter is done.
[01:12:54.540 --> 01:12:58.740]   But it's not obvious what the best mechanisms are
[01:12:58.740 --> 01:13:01.900]   to still make an exceptionally engaging platform,
[01:13:01.900 --> 01:13:04.020]   like Clubhouse is very engaging,
[01:13:04.020 --> 01:13:06.540]   and not have any of the negative effects.
[01:13:06.540 --> 01:13:08.940]   For example, there's Chrome extensions
[01:13:08.940 --> 01:13:13.660]   that allow you to turn off all likes and dislikes
[01:13:13.660 --> 01:13:15.020]   and all of that from Twitter.
[01:13:15.020 --> 01:13:18.260]   So all you're seeing is just the content.
[01:13:18.260 --> 01:13:21.540]   On Twitter, that to me creates,
[01:13:21.540 --> 01:13:23.620]   that's not a compelling experience at all.
[01:13:23.620 --> 01:13:26.940]   Because I still need, I would argue,
[01:13:26.940 --> 01:13:30.420]   I still need the likes to know what's a tweet worth reading.
[01:13:30.420 --> 01:13:32.220]   'Cause I only have a limited amount of time,
[01:13:32.220 --> 01:13:34.020]   so I need to know what's valuable.
[01:13:34.020 --> 01:13:36.980]   It's like great Yelp reviews on tweets or something.
[01:13:36.980 --> 01:13:40.620]   But I've turned off on, for example,
[01:13:40.620 --> 01:13:42.420]   on my account on YouTube,
[01:13:44.620 --> 01:13:47.380]   I wrote a Chrome extension that turns off
[01:13:47.380 --> 01:13:50.300]   all likes and dislikes and just views.
[01:13:50.300 --> 01:13:53.020]   I don't know how many views a video gets and so on,
[01:13:53.020 --> 01:13:53.860]   unless it's on my phone.
[01:13:53.860 --> 01:13:55.660]   - Do you take off the recommendations?
[01:13:55.660 --> 01:13:58.540]   - No, no.
[01:13:58.540 --> 01:13:59.540]   - On YouTube, some people,
[01:13:59.540 --> 01:14:02.100]   distraction for YouTube is a big one for people.
[01:14:02.100 --> 01:14:04.100]   - No, I'm not worried about the distraction
[01:14:04.100 --> 01:14:06.900]   because I'm able to control myself on YouTube.
[01:14:06.900 --> 01:14:07.900]   - You don't rabbit hole.
[01:14:07.900 --> 01:14:09.060]   - No, I don't rabbit hole.
[01:14:09.060 --> 01:14:10.500]   So you have to know your demons
[01:14:10.500 --> 01:14:11.900]   or your addictions or whatever.
[01:14:11.900 --> 01:14:12.860]   On YouTube, I'm okay.
[01:14:12.860 --> 01:14:14.700]   I don't keep clicking.
[01:14:14.700 --> 01:14:19.340]   The negative feelings come from seeing the views
[01:14:19.340 --> 01:14:22.100]   on stuff you've created.
[01:14:22.100 --> 01:14:24.100]   - Oh, so you don't wanna see your views.
[01:14:24.100 --> 01:14:24.940]   - Yeah.
[01:14:24.940 --> 01:14:26.780]   So I'm just speaking to the things
[01:14:26.780 --> 01:14:29.660]   that I'm aware of myself that are helpful
[01:14:29.660 --> 01:14:31.940]   and things that are not helpful emotionally.
[01:14:31.940 --> 01:14:34.540]   And I feel like there should be,
[01:14:34.540 --> 01:14:37.020]   we need to create actually tooling for ourselves.
[01:14:37.020 --> 01:14:38.860]   That's not me with JavaScript,
[01:14:38.860 --> 01:14:42.340]   but anybody's able to create,
[01:14:42.340 --> 01:14:45.060]   sort of control the experience that they have.
[01:14:45.060 --> 01:14:45.900]   - Yeah.
[01:14:45.900 --> 01:14:48.900]   Well, so my big unified theory on social media
[01:14:48.900 --> 01:14:52.460]   is I'm very bearish on the big platforms
[01:14:52.460 --> 01:14:53.820]   having a long future.
[01:14:53.820 --> 01:14:54.660]   - You are.
[01:14:54.660 --> 01:14:57.940]   - I think the moment of three or four major platforms
[01:14:57.940 --> 01:15:01.020]   is not gonna last.
[01:15:01.020 --> 01:15:03.820]   Right, so I don't, okay, this is just perspective, right?
[01:15:03.820 --> 01:15:07.020]   So you can start shorting these stocks on my,
[01:15:07.020 --> 01:15:08.100]   don't tell Vlad. - It's not financial advice.
[01:15:08.100 --> 01:15:09.980]   - Yeah, don't do it, Robin Hood.
[01:15:09.980 --> 01:15:12.860]   So here's, I think the big mistake
[01:15:12.860 --> 01:15:14.780]   the major platforms made
[01:15:14.780 --> 01:15:19.380]   is when they took out the network effect advantage, right?
[01:15:19.380 --> 01:15:20.980]   So the original pitch,
[01:15:20.980 --> 01:15:23.140]   especially if something like Facebook or Instagram
[01:15:23.140 --> 01:15:26.420]   was the people you know are on here, right?
[01:15:26.420 --> 01:15:27.740]   So like what you use this for
[01:15:27.740 --> 01:15:29.780]   is you can connect to people that you already know.
[01:15:29.780 --> 01:15:31.700]   This is what makes the network useful.
[01:15:31.700 --> 01:15:34.780]   So therefore the value of our network grows quadratically
[01:15:34.780 --> 01:15:35.820]   with the number of users.
[01:15:35.820 --> 01:15:37.780]   And therefore it's such a headstart
[01:15:37.780 --> 01:15:40.340]   that there's no way that someone else can catch up.
[01:15:40.340 --> 01:15:42.660]   But when they shifted and when Facebook took the lead
[01:15:42.660 --> 01:15:45.700]   of say we're gonna shift towards a newsfeed model,
[01:15:45.700 --> 01:15:48.460]   they basically said we're going to try to in the moment
[01:15:48.460 --> 01:15:50.260]   get more data and get more likes.
[01:15:50.260 --> 01:15:51.300]   Like what we're gonna go towards
[01:15:51.300 --> 01:15:54.300]   is actually just seeing interesting stuff,
[01:15:54.300 --> 01:15:55.380]   like seeing diverting information.
[01:15:55.380 --> 01:15:58.260]   So people took this social internet impulse
[01:15:58.260 --> 01:16:00.860]   to connect to people digitally to other tools,
[01:16:00.860 --> 01:16:02.460]   like group text messages and WhatsApp
[01:16:02.460 --> 01:16:03.300]   and stuff like this, right?
[01:16:03.300 --> 01:16:04.420]   So you don't think about these tools
[01:16:04.420 --> 01:16:06.460]   as oh, this is where I connect with people.
[01:16:06.460 --> 01:16:09.140]   Once it's just a feed that's kind of interesting,
[01:16:09.140 --> 01:16:10.780]   now you're competing with everything else
[01:16:10.780 --> 01:16:13.460]   that can produce interesting content that's diverting.
[01:16:13.460 --> 01:16:16.300]   And I think that is a much fiercer competition
[01:16:16.300 --> 01:16:17.140]   because now for example,
[01:16:17.140 --> 01:16:18.620]   you're going up against podcast, right?
[01:16:18.620 --> 01:16:19.980]   I mean like, okay, I guess, you know,
[01:16:19.980 --> 01:16:22.900]   the Twitter feed is interesting right now,
[01:16:22.900 --> 01:16:24.340]   but also a podcast is interesting
[01:16:24.340 --> 01:16:25.580]   or something else could be interesting too.
[01:16:25.580 --> 01:16:27.300]   I think it's a much fiercer competition
[01:16:27.300 --> 01:16:29.900]   when there's no more network effects, right?
[01:16:29.900 --> 01:16:32.220]   And so my sense is we're gonna see a fragmentation
[01:16:32.220 --> 01:16:34.740]   into what I call long tail social media,
[01:16:34.740 --> 01:16:38.460]   where if I don't need everyone I know to be on a platform,
[01:16:38.460 --> 01:16:41.860]   then why not have three or four bespoke platforms I use
[01:16:41.860 --> 01:16:43.980]   where it's a thousand people and it's all,
[01:16:43.980 --> 01:16:46.500]   we're all interested in, you know, whatever,
[01:16:46.500 --> 01:16:50.060]   AI or comedy and we've perfected this interface
[01:16:50.060 --> 01:16:52.060]   and maybe it's like Clubhouse, it's audio or something.
[01:16:52.060 --> 01:16:54.180]   And we all pay $2 so we don't have to worry
[01:16:54.180 --> 01:16:55.780]   about attention harvesting.
[01:16:55.780 --> 01:16:57.740]   And that's gonna be wildly more entertaining.
[01:16:57.740 --> 01:17:00.580]   Like, I mean, I'm thinking about comedians on Twitter.
[01:17:00.580 --> 01:17:04.060]   It's not the best internet possible format
[01:17:04.060 --> 01:17:06.380]   for them expressing themselves and being interesting
[01:17:06.380 --> 01:17:08.100]   that you have all these comedians that are trying to like,
[01:17:08.100 --> 01:17:10.020]   well, I can do like little clips and little whatever.
[01:17:10.020 --> 01:17:13.020]   Like, I don't know if there was a long tail social media.
[01:17:13.020 --> 01:17:14.460]   It's really, this is where the comedians are
[01:17:14.460 --> 01:17:16.500]   and there's podcasts and the comedians run podcasts now.
[01:17:16.500 --> 01:17:19.460]   So this is my thought is that there's really no,
[01:17:19.460 --> 01:17:21.180]   there's really no strong advantage
[01:17:21.180 --> 01:17:25.860]   to having one large platform that everyone is on.
[01:17:25.860 --> 01:17:27.980]   If all you're getting from it is I now have different
[01:17:27.980 --> 01:17:31.180]   options for diversion and like uplifting aspirational
[01:17:31.180 --> 01:17:33.300]   or whatever types of entertainment,
[01:17:33.300 --> 01:17:34.540]   that whole thing could fragment.
[01:17:34.540 --> 01:17:36.100]   And I think the glue that was holding together
[01:17:36.100 --> 01:17:36.940]   was network effects.
[01:17:36.940 --> 01:17:38.620]   I don't think they realized that when network effects
[01:17:38.620 --> 01:17:40.020]   have been destabilized,
[01:17:40.020 --> 01:17:41.900]   they don't have the centrifugal force anymore
[01:17:41.900 --> 01:17:43.340]   and they're spinning faster and faster.
[01:17:43.340 --> 01:17:46.660]   But is a Twitter feed really that much more interesting
[01:17:46.660 --> 01:17:48.060]   than all these streaming services?
[01:17:48.060 --> 01:17:51.060]   Is it really that much more interesting than Clubhouse?
[01:17:51.060 --> 01:17:54.020]   Is it that much more interesting than podcasts?
[01:17:54.020 --> 01:17:56.500]   I feel like they don't realize how unstable
[01:17:56.500 --> 01:17:57.460]   their ground actually is.
[01:17:57.460 --> 01:17:58.420]   - Yeah, that's fascinating.
[01:17:58.420 --> 01:18:03.420]   But the thing that makes Twitter and Facebook work,
[01:18:03.420 --> 01:18:07.100]   I mean, the newsfeed, you're exactly right.
[01:18:07.100 --> 01:18:08.780]   Like you can just duplicate the news.
[01:18:08.780 --> 01:18:12.780]   Like if it's not the social network and it's the newsfeed,
[01:18:12.780 --> 01:18:15.220]   then why not have multiple different feeds
[01:18:15.220 --> 01:18:17.480]   that are more, that are better at satisfying you?
[01:18:17.480 --> 01:18:20.540]   There's a dopamine gamification that they've figured out.
[01:18:20.540 --> 01:18:21.380]   - Yeah.
[01:18:21.380 --> 01:18:24.820]   - And so you have to, whatever you create,
[01:18:24.820 --> 01:18:27.340]   you have to at least provide some pleasure
[01:18:27.340 --> 01:18:29.660]   in that same gamification kind of way.
[01:18:29.660 --> 01:18:32.180]   It doesn't have to have to do with scale
[01:18:32.180 --> 01:18:33.260]   of large social networks.
[01:18:33.260 --> 01:18:35.300]   But I mean, I guess you're implying
[01:18:35.300 --> 01:18:37.300]   that you should be able to design
[01:18:37.300 --> 01:18:40.220]   that kind of mechanism in other forms.
[01:18:40.220 --> 01:18:42.660]   - Or people are turning on that gamification.
[01:18:42.660 --> 01:18:44.500]   I mean, so people are getting wise to it
[01:18:44.500 --> 01:18:46.300]   and are getting uncomfortable about it, right?
[01:18:46.300 --> 01:18:49.060]   So if I'm offering something, these exist out here.
[01:18:49.060 --> 01:18:49.900]   - Like sugar.
[01:18:49.900 --> 01:18:51.060]   People realize sugar's bad for you,
[01:18:51.060 --> 01:18:51.900]   they're gonna stop eating it. - Yeah, sugar's great.
[01:18:51.900 --> 01:18:53.100]   Yeah, drinking a lot's great too,
[01:18:53.100 --> 01:18:56.100]   but also after a while you realize there's problems.
[01:18:56.100 --> 01:18:58.140]   So some of the long tail social media networks
[01:18:58.140 --> 01:18:59.820]   that are out there that I've looked at,
[01:18:59.820 --> 01:19:02.900]   they offer usually like a deeper sense of connection.
[01:19:02.900 --> 01:19:04.940]   Like it's usually interesting people
[01:19:04.940 --> 01:19:06.180]   that you share some affinity
[01:19:06.180 --> 01:19:08.180]   and you have these carefully cultivated.
[01:19:08.180 --> 01:19:09.940]   I wrote this New Yorker piece a couple of years ago
[01:19:09.940 --> 01:19:11.580]   about the indie social media movement
[01:19:11.580 --> 01:19:14.940]   that really got into some of these different technologies.
[01:19:14.940 --> 01:19:17.020]   But I think the technologies are a distraction.
[01:19:17.020 --> 01:19:18.900]   We focus too much on, you know,
[01:19:18.900 --> 01:19:21.180]   Macedon versus, you know, whatever, like forget,
[01:19:21.180 --> 01:19:22.500]   or Discord, like actually let's forget
[01:19:22.500 --> 01:19:23.500]   the protocols right now.
[01:19:23.540 --> 01:19:26.060]   It's the idea of, okay,
[01:19:26.060 --> 01:19:28.500]   and there's a lot of these long tail social media groups,
[01:19:28.500 --> 01:19:29.580]   what people are getting out of it,
[01:19:29.580 --> 01:19:32.900]   which I think can outweigh the dopamine gamification
[01:19:32.900 --> 01:19:35.140]   is strong connection and motivation.
[01:19:35.140 --> 01:19:36.940]   Like you're in a group with other guys
[01:19:36.940 --> 01:19:38.820]   that are all trying to be, you know,
[01:19:38.820 --> 01:19:40.380]   better dads or something like this.
[01:19:40.380 --> 01:19:42.660]   And you talk to them on a regular basis
[01:19:42.660 --> 01:19:43.700]   and you're sharing your stories
[01:19:43.700 --> 01:19:44.780]   and there's interesting talks.
[01:19:44.780 --> 01:19:47.500]   And that's a powerful thing too.
[01:19:47.500 --> 01:19:49.860]   - One interesting thing about scale of Twitter
[01:19:49.860 --> 01:19:53.260]   is you have these viral spread of information.
[01:19:53.260 --> 01:19:57.060]   So sort of Twitter has become a newsmaker in itself.
[01:19:57.060 --> 01:19:58.580]   - Yeah, I think it's a problem.
[01:19:58.580 --> 01:20:01.140]   - Well, yes, but I wonder what replaces that
[01:20:01.140 --> 01:20:03.540]   because then you immediately--
[01:20:03.540 --> 01:20:04.380]   - Reporting?
[01:20:04.380 --> 01:20:05.220]   - Well, no.
[01:20:05.220 --> 01:20:07.140]   - Reporters would have to do some work again, I don't know.
[01:20:07.140 --> 01:20:09.740]   - No, the problem with reporters and journalism
[01:20:09.740 --> 01:20:12.420]   is that they're intermediary.
[01:20:12.420 --> 01:20:14.100]   They have control.
[01:20:14.100 --> 01:20:15.980]   I mean, this is the problem in Russia currently
[01:20:15.980 --> 01:20:17.140]   is that you have,
[01:20:17.140 --> 01:20:22.420]   it creates a shield between the people and the news.
[01:20:22.420 --> 01:20:25.100]   The interesting thing and the powerful thing about Twitter
[01:20:25.100 --> 01:20:28.100]   is that the news originates from the individual
[01:20:28.100 --> 01:20:29.020]   that's creating the news.
[01:20:29.020 --> 01:20:31.820]   Like you have the president of the United States,
[01:20:31.820 --> 01:20:33.700]   the former president of the United States on Twitter
[01:20:33.700 --> 01:20:34.900]   creating news.
[01:20:34.900 --> 01:20:36.780]   You have Elon Musk creating news.
[01:20:36.780 --> 01:20:39.620]   You have people announcing stuff on Twitter
[01:20:39.620 --> 01:20:41.500]   as opposed to talking to a journalist.
[01:20:41.500 --> 01:20:44.020]   And that feels much more genuine
[01:20:44.020 --> 01:20:48.340]   and it feels very powerful,
[01:20:48.340 --> 01:20:49.940]   but actually coming to realize
[01:20:50.380 --> 01:20:53.020]   it doesn't need the social network.
[01:20:53.020 --> 01:20:55.980]   You can just put that announcement on a YouTube type thing.
[01:20:55.980 --> 01:20:56.900]   - This is what I'm thinking, right.
[01:20:56.900 --> 01:20:59.620]   So this is my point about that because that's right.
[01:20:59.620 --> 01:21:02.300]   The democratizing power of the internet is fantastic.
[01:21:02.300 --> 01:21:03.660]   I'm an old school internet nerd,
[01:21:03.660 --> 01:21:05.260]   a guy that was, you know,
[01:21:05.260 --> 01:21:07.100]   telemating in the servers and gophering
[01:21:07.100 --> 01:21:08.700]   before the World Wide Web was around, right?
[01:21:08.700 --> 01:21:10.100]   So I'm a huge internet booster
[01:21:10.100 --> 01:21:12.260]   and that's one of its big power.
[01:21:12.260 --> 01:21:14.340]   But when you put everything on Twitter,
[01:21:14.340 --> 01:21:16.620]   I think the fact that you've taken,
[01:21:16.620 --> 01:21:18.380]   you homogenized everything, right?
[01:21:18.380 --> 01:21:20.140]   So everything looks the same,
[01:21:20.140 --> 01:21:21.580]   moves with the same low friction,
[01:21:21.580 --> 01:21:22.700]   is very difficult.
[01:21:22.700 --> 01:21:25.260]   You have no, what I call distributed curation, right?
[01:21:25.260 --> 01:21:26.980]   The only curation that really happens,
[01:21:26.980 --> 01:21:29.460]   I was a little bit with likes and also the algorithm,
[01:21:29.460 --> 01:21:33.660]   but if you look back to pre-Web 2.0 or early Web 2.0,
[01:21:33.660 --> 01:21:35.940]   when a lot of this was happening, let's say on blogs,
[01:21:35.940 --> 01:21:37.420]   where people own their own servers
[01:21:37.420 --> 01:21:39.180]   and you had your different blogs,
[01:21:39.180 --> 01:21:41.340]   there was this distributed curation that happened
[01:21:41.340 --> 01:21:45.340]   where in order for your blog to get on people's radar,
[01:21:45.340 --> 01:21:47.500]   and this had nothing to do with any gatekeepers
[01:21:47.500 --> 01:21:50.220]   or legacy media, it was over time,
[01:21:50.220 --> 01:21:52.020]   you got more links and people respected you
[01:21:52.020 --> 01:21:53.460]   and you would hear about this blog over here.
[01:21:53.460 --> 01:21:55.220]   And there's this whole distributed curation
[01:21:55.220 --> 01:21:56.740]   and filtering going on.
[01:21:56.740 --> 01:22:00.020]   So if you think like the 2004 presidential election,
[01:22:00.020 --> 01:22:02.220]   most of the information people are getting from the internet
[01:22:02.220 --> 01:22:05.740]   was when the first big internet news driven elections
[01:22:05.740 --> 01:22:09.420]   was from, you had like the daily costs and drudge,
[01:22:09.420 --> 01:22:11.020]   but there was like blogs that were out there.
[01:22:11.020 --> 01:22:13.460]   And this was back, Ezra Klein was just running a blog
[01:22:13.460 --> 01:22:16.260]   out of his dorm room at this point, right?
[01:22:16.260 --> 01:22:20.700]   And you would in a distributed fashion gain credibility
[01:22:20.700 --> 01:22:22.580]   because, okay, people have paid,
[01:22:22.580 --> 01:22:24.020]   it's very hard to get people to pay attention to your blog,
[01:22:24.020 --> 01:22:26.460]   they're paying attention, I get linked to this kid Ezra
[01:22:26.460 --> 01:22:28.060]   or whatever, it seems to be really sharp
[01:22:28.060 --> 01:22:29.740]   and now people are noticing it.
[01:22:29.740 --> 01:22:32.100]   And now you have a distributed curation
[01:22:32.100 --> 01:22:34.100]   that solves a lot of the problems we see
[01:22:34.100 --> 01:22:36.100]   when you have a completely homogenized low friction
[01:22:36.100 --> 01:22:38.260]   environment like friction where, I mean, Twitter,
[01:22:38.260 --> 01:22:41.140]   where any random conspiracy theory or whatever
[01:22:41.140 --> 01:22:44.620]   that people like can just shoot through and spread.
[01:22:44.620 --> 01:22:48.180]   Whereas if you're starting a blog to try to push QAnon
[01:22:48.180 --> 01:22:49.500]   or something like that,
[01:22:49.500 --> 01:22:51.500]   it's probably gonna be a really weird looking blog.
[01:22:51.500 --> 01:22:52.420]   You're gonna have a hard time,
[01:22:52.420 --> 01:22:55.580]   like it's just never gonna show up on people's radar, right?
[01:22:55.580 --> 01:22:58.700]   - So everything you've said up until the very last statement
[01:22:58.700 --> 01:22:59.940]   I would agree with.
[01:22:59.940 --> 01:23:02.740]   - This is a topic I don't know a ton about, I guess.
[01:23:02.740 --> 01:23:07.100]   - So there's, I think, I forget QAnon.
[01:23:07.100 --> 01:23:07.940]   - Yeah, no, we can-
[01:23:07.940 --> 01:23:09.940]   - But QAnon is, QAnon could be that.
[01:23:09.940 --> 01:23:11.900]   I also don't know, I should know more,
[01:23:11.900 --> 01:23:13.620]   I apologize, I don't know more.
[01:23:13.620 --> 01:23:17.220]   I mean, that's a power and the downside.
[01:23:17.220 --> 01:23:21.660]   You can have, I mean, Hitler could have a blog today
[01:23:21.660 --> 01:23:24.180]   and he would have potentially a very large following
[01:23:24.180 --> 01:23:28.180]   if he's charismatic, if he's good with words,
[01:23:28.180 --> 01:23:30.300]   is able to express the ideas of whatever,
[01:23:30.300 --> 01:23:32.700]   maybe he's able to channel the frustration,
[01:23:32.700 --> 01:23:35.220]   the anger that people have about a certain thing.
[01:23:35.220 --> 01:23:37.300]   So I think that's the power of blogs,
[01:23:37.300 --> 01:23:39.700]   but it's also the limitation, but that doesn't,
[01:23:39.700 --> 01:23:40.900]   we're not trying to solve that.
[01:23:40.900 --> 01:23:41.740]   - You can't solve that, yeah.
[01:23:41.740 --> 01:23:43.340]   - The fundamental problem you're saying
[01:23:43.340 --> 01:23:44.940]   is not the problem.
[01:23:44.940 --> 01:23:48.060]   Your thesis is that there's nothing special
[01:23:48.060 --> 01:23:50.980]   about large-scale social networks
[01:23:50.980 --> 01:23:53.500]   that guarantees that they will keep existing.
[01:23:53.500 --> 01:23:54.940]   - And it's important to remember
[01:23:54.940 --> 01:23:58.180]   for a lot of the older generation of internet activists,
[01:23:58.180 --> 01:24:01.180]   so the people who were very pro-internet in the early days,
[01:24:01.180 --> 01:24:03.900]   they were completely flabbergasted
[01:24:03.900 --> 01:24:05.540]   by the rise of these platforms.
[01:24:05.540 --> 01:24:08.540]   Say, why would you take the internet
[01:24:08.540 --> 01:24:10.980]   and then build your own version of the internet
[01:24:10.980 --> 01:24:12.580]   where you own all the servers?
[01:24:12.580 --> 01:24:14.500]   And we built this whole distributed,
[01:24:14.500 --> 01:24:16.580]   the whole thing, we had open protocols.
[01:24:16.580 --> 01:24:17.700]   Everyone anywhere in the world
[01:24:17.700 --> 01:24:18.620]   uses the same protocols.
[01:24:18.620 --> 01:24:19.980]   Your machine can talk to any other machine.
[01:24:19.980 --> 01:24:23.300]   It's the most democratic communication system
[01:24:23.300 --> 01:24:24.260]   that's ever been built.
[01:24:24.260 --> 01:24:25.460]   And then these companies came along and said,
[01:24:25.460 --> 01:24:26.540]   "We're gonna build our own,
[01:24:26.540 --> 01:24:27.500]   we'll just own all the servers
[01:24:27.500 --> 01:24:29.100]   and put them in buildings that we own.
[01:24:29.100 --> 01:24:30.940]   And the internet will just be the first mile
[01:24:30.940 --> 01:24:32.460]   that gets you into our private internet
[01:24:32.460 --> 01:24:33.780]   where we owned the whole thing."
[01:24:33.780 --> 01:24:37.900]   It went completely against the entire motivation
[01:24:37.900 --> 01:24:39.940]   of the internet, was like, "Yes, it's not gonna be
[01:24:39.940 --> 01:24:41.300]   one person owns all the servers
[01:24:41.300 --> 01:24:42.260]   and you pay to access them.
[01:24:42.260 --> 01:24:43.860]   It's any one server that they own
[01:24:43.860 --> 01:24:45.100]   can talk to anyone else's server
[01:24:45.100 --> 01:24:48.380]   because we all agree on a standard set of protocols."
[01:24:48.380 --> 01:24:51.700]   And so the old guard of pro-internet people
[01:24:51.700 --> 01:24:53.940]   never understood this move towards,
[01:24:53.940 --> 01:24:56.540]   "Let's build private versions of the internet.
[01:24:56.540 --> 01:24:59.060]   We'll build three or four private internets
[01:24:59.060 --> 01:25:00.060]   and that's what we'll all use."
[01:25:00.060 --> 01:25:01.740]   It was the opposite, basically.
[01:25:01.740 --> 01:25:03.340]   - Well, it's funny enough, I don't know if you follow,
[01:25:03.340 --> 01:25:07.580]   but Jack Dorsey is also, is a proponent
[01:25:07.580 --> 01:25:11.940]   and is helping to fund, create fully distributed versions
[01:25:11.940 --> 01:25:13.180]   of Twitter, essentially, a thing
[01:25:13.180 --> 01:25:15.940]   that would potentially destroy Twitter.
[01:25:15.940 --> 01:25:18.540]   But I think there might be financial,
[01:25:18.540 --> 01:25:21.900]   like business cases to be made there, I'm not sure.
[01:25:21.900 --> 01:25:23.660]   But that seems to be another alternative
[01:25:23.660 --> 01:25:28.660]   as opposed to creating a bunch of, like the long tail,
[01:25:28.660 --> 01:25:31.500]   creating like the ultimate long tail
[01:25:31.500 --> 01:25:33.220]   of like fully distributed.
[01:25:33.220 --> 01:25:35.060]   - Yeah, which is-- - Which is what the internet is.
[01:25:35.060 --> 01:25:36.620]   - But that's sort of why I'm thinking
[01:25:36.620 --> 01:25:37.860]   about long tail social media,
[01:25:37.860 --> 01:25:40.940]   I'm thinking it's like the text's not so important.
[01:25:40.940 --> 01:25:42.660]   Like there's groups out there, right?
[01:25:42.660 --> 01:25:45.700]   I know where the tech they use to actually implement
[01:25:45.700 --> 01:25:47.620]   their digital only social group, whatever,
[01:25:47.620 --> 01:25:50.020]   they might use Slack, they might use some combination
[01:25:50.020 --> 01:25:51.020]   of Zoom or it doesn't matter.
[01:25:51.020 --> 01:25:52.740]   I think in the tech world,
[01:25:52.740 --> 01:25:54.780]   we wanna build the beautiful protocol
[01:25:54.780 --> 01:25:56.100]   that, okay, everyone's gonna use
[01:25:56.100 --> 01:25:58.620]   as just a federated server protocol
[01:25:58.620 --> 01:25:59.860]   in which we've worked out X, Y, and Z
[01:25:59.860 --> 01:26:00.700]   and no one understands it
[01:26:00.700 --> 01:26:02.260]   because then the engineers need it all to make,
[01:26:02.260 --> 01:26:03.940]   I get it because I'm a nerd like this, like, okay,
[01:26:03.940 --> 01:26:05.700]   every standard has to fit with everything else
[01:26:05.700 --> 01:26:07.420]   and no one understands what's going on.
[01:26:07.420 --> 01:26:10.540]   Meanwhile, you have this group of bike enthusiasts
[01:26:10.540 --> 01:26:12.420]   that are like, yeah, we'll just jump on a Zoom
[01:26:12.420 --> 01:26:14.260]   and have some Slack and put up a blog
[01:26:14.260 --> 01:26:15.740]   and the tech doesn't really matter.
[01:26:15.740 --> 01:26:19.500]   Like we built a world with our own curation, our own rules,
[01:26:19.500 --> 01:26:22.020]   our own sort of social ecosystem
[01:26:22.020 --> 01:26:23.460]   that's generating a lot of value.
[01:26:23.460 --> 01:26:24.940]   I mean, I don't know if it'll happen,
[01:26:24.940 --> 01:26:26.140]   there's a lot of money at stake
[01:26:26.140 --> 01:26:27.340]   with obviously these large,
[01:26:27.340 --> 01:26:29.540]   but I just think they're more,
[01:26:29.540 --> 01:26:30.900]   they're so, I mean, look how quickly
[01:26:30.900 --> 01:26:33.460]   Americans left Facebook, right?
[01:26:33.460 --> 01:26:35.620]   I mean, Facebook was savvy to buy other properties
[01:26:35.620 --> 01:26:36.740]   and to diversify, right?
[01:26:36.740 --> 01:26:37.940]   But how quick did that take
[01:26:37.940 --> 01:26:40.780]   for just standard Facebook newsfeed?
[01:26:40.780 --> 01:26:42.980]   Everyone under the age of something were using it
[01:26:42.980 --> 01:26:44.540]   and no one under a certain age is using it now.
[01:26:44.540 --> 01:26:45.380]   It took like four years.
[01:26:45.380 --> 01:26:47.500]   I mean, this stuff is really-
[01:26:47.500 --> 01:26:50.980]   - I believe people can leave Facebook overnight.
[01:26:50.980 --> 01:26:51.820]   - Yeah.
[01:26:51.820 --> 01:26:55.340]   - Like I think Facebook hasn't actually messed up
[01:26:55.340 --> 01:26:57.940]   like enough to, there's two things.
[01:26:57.940 --> 01:26:58.820]   They haven't messed up enough
[01:26:58.820 --> 01:27:00.620]   for people to really leave aggressively
[01:27:00.620 --> 01:27:03.860]   and there's no good alternative for them to leave.
[01:27:03.860 --> 01:27:06.220]   I think if good alternatives pop up,
[01:27:06.220 --> 01:27:07.620]   it would just immediately happen.
[01:27:07.620 --> 01:27:10.300]   This stuff is a lot more culturally fragile, I think.
[01:27:10.300 --> 01:27:11.460]   I mean, Twitter's having a moment
[01:27:11.460 --> 01:27:13.180]   'cause it was feeding a certain type of,
[01:27:13.180 --> 01:27:14.260]   I mean, there's a lot of anxieties
[01:27:14.260 --> 01:27:16.460]   that was in the sort of political sphere anyways
[01:27:16.460 --> 01:27:19.080]   that Twitter was working with,
[01:27:19.080 --> 01:27:21.700]   but its moment could go too as well.
[01:27:21.700 --> 01:27:23.420]   I mean, it's a really arbitrary thing,
[01:27:23.420 --> 01:27:24.580]   short little things.
[01:27:24.580 --> 01:27:26.340]   And I read a Wired article about this
[01:27:26.340 --> 01:27:27.300]   earlier in the pandemic.
[01:27:27.300 --> 01:27:29.580]   Like this is crazy that the way
[01:27:29.580 --> 01:27:31.420]   that we're trying to communicate information
[01:27:31.420 --> 01:27:34.100]   about the pandemic is all these weird arbitrary rules
[01:27:34.100 --> 01:27:37.100]   where people are screenshotting pictures of articles
[01:27:37.100 --> 01:27:38.380]   that are part of a tweet thread
[01:27:38.380 --> 01:27:40.700]   where you say one slash in under it.
[01:27:40.700 --> 01:27:43.100]   Like we have the technology guys
[01:27:43.100 --> 01:27:45.300]   to like really clearly convey
[01:27:45.300 --> 01:27:47.100]   for long form information to people.
[01:27:47.100 --> 01:27:48.380]   Why do we have these?
[01:27:48.380 --> 01:27:50.420]   And I know it's 'cause it's the gamified dopamine hits,
[01:27:50.420 --> 01:27:52.300]   but what a weird medium.
[01:27:52.300 --> 01:27:55.460]   There's no reason for us to have these threads
[01:27:55.460 --> 01:27:57.860]   that you have to find and pin when you screenshot.
[01:27:57.860 --> 01:27:59.700]   I mean, we have technology to communicate better
[01:27:59.700 --> 01:28:00.540]   using the internet.
[01:28:00.540 --> 01:28:05.100]   I mean, why are epidemiologists having to do tweet threads?
[01:28:05.100 --> 01:28:06.980]   - Because there's mechanisms of publishing
[01:28:06.980 --> 01:28:08.660]   that make it easier on Twitter.
[01:28:08.660 --> 01:28:10.780]   I mean, we're evolving as a species
[01:28:10.780 --> 01:28:12.900]   and the internet is a very fresh thing.
[01:28:12.900 --> 01:28:16.100]   And so it's kind of interesting to think
[01:28:16.100 --> 01:28:18.380]   that as opposed to Twitter,
[01:28:18.380 --> 01:28:20.220]   this is what Jack also complains about
[01:28:20.220 --> 01:28:23.140]   is Twitter's not innovating fast enough.
[01:28:23.140 --> 01:28:26.820]   And so it's almost like the people are innovating
[01:28:26.820 --> 01:28:29.620]   and thinking about their productive life
[01:28:29.620 --> 01:28:31.780]   faster than the platforms
[01:28:31.780 --> 01:28:33.660]   on which they operate can catch up.
[01:28:33.660 --> 01:28:37.460]   And so at the point the gap grows sufficiently,
[01:28:37.460 --> 01:28:39.540]   they'll jump a few people,
[01:28:39.540 --> 01:28:42.580]   a few innovative folks will just create an alternative
[01:28:42.580 --> 01:28:47.580]   and perhaps distributed, perhaps just many little silos
[01:28:47.580 --> 01:28:49.020]   and then people will jump
[01:28:49.020 --> 01:28:50.660]   and then we'll just continue in this kind of way.
[01:28:50.660 --> 01:28:52.340]   - But see, I think like Substack, for example,
[01:28:52.340 --> 01:28:53.660]   what they're gonna pull out of Twitter,
[01:28:53.660 --> 01:28:56.420]   among other things, is the audience that was,
[01:28:56.420 --> 01:28:58.900]   let's say like slightly left of center,
[01:28:58.900 --> 01:29:03.060]   but slightly left of center, don't like Trump,
[01:29:03.060 --> 01:29:05.500]   uncomfortable with like postmodern critical theories
[01:29:05.500 --> 01:29:07.500]   made into political action, right?
[01:29:07.500 --> 01:29:08.460]   And they're like, yeah, Twitter,
[01:29:08.460 --> 01:29:10.180]   there was people on there talking about this
[01:29:10.180 --> 01:29:12.380]   and it made me feel sort of heard
[01:29:12.380 --> 01:29:14.220]   because I was feeling a little bit like a nerd about it.
[01:29:14.220 --> 01:29:16.820]   But honestly, I'd probably rather subscribe to four subs,
[01:29:16.820 --> 01:29:19.180]   you know, I'm gonna have like Barry's and Andrew Sullivan's,
[01:29:19.180 --> 01:29:20.740]   I'll have like a Jesse Signals,
[01:29:20.740 --> 01:29:22.820]   like I'll have a few Substacks I can subscribe to.
[01:29:22.820 --> 01:29:26.780]   And honestly, I'm a knowledge worker who's 32 anyways,
[01:29:26.780 --> 01:29:28.100]   probably that's an email all day.
[01:29:28.100 --> 01:29:30.180]   And so like there's an innovation that's gonna,
[01:29:30.180 --> 01:29:32.460]   that group, you know, it's gonna suck them off.
[01:29:32.460 --> 01:29:34.180]   - Which is actually a very large group.
[01:29:34.180 --> 01:29:36.380]   - Yeah, that's a lot of energy.
[01:29:36.380 --> 01:29:38.700]   And then once Trump's gone, I guess that's probably gonna,
[01:29:38.700 --> 01:29:42.220]   that drove a lot of more like Trump people off Twitter.
[01:29:42.220 --> 01:29:44.380]   Like this stuff is fragile, I think.
[01:29:44.380 --> 01:29:47.020]   - So I, but the fascinating thing to me,
[01:29:47.020 --> 01:29:50.660]   'cause I've hung out on Parler for a short amount enough
[01:29:50.660 --> 01:29:53.580]   to know that the interface matters, it's so fascinating.
[01:29:53.580 --> 01:29:57.220]   Like that it's not just about ideas.
[01:29:57.220 --> 01:30:00.180]   It's about creating like Substack too,
[01:30:01.220 --> 01:30:04.500]   creating a pleasant experience, addicting experience.
[01:30:04.500 --> 01:30:05.340]   - No, you're right about that.
[01:30:05.340 --> 01:30:06.620]   And it's hard.
[01:30:06.620 --> 01:30:08.220]   And it's why the, this is one of the conclusions
[01:30:08.220 --> 01:30:10.100]   from that Indy social media article is,
[01:30:10.100 --> 01:30:12.140]   it's just the ugliness matters.
[01:30:12.140 --> 01:30:13.380]   And I don't mean even just aesthetically,
[01:30:13.380 --> 01:30:15.660]   but just the clunkiness of the interfaces,
[01:30:15.660 --> 01:30:18.860]   and I don't know, it's to some degree,
[01:30:18.860 --> 01:30:21.060]   the social media companies have spent a lot of money on this
[01:30:21.060 --> 01:30:23.940]   and to some degree, it's a survivorship bias, right?
[01:30:23.940 --> 01:30:26.860]   I think Twitter, every time I hear Jack talks about this,
[01:30:26.860 --> 01:30:30.100]   it seems like he's as surprised as anyone else
[01:30:30.100 --> 01:30:31.100]   the way Twitter is being used.
[01:30:31.100 --> 01:30:36.100]   I mean, it's basically the way they had it years ago.
[01:30:36.100 --> 01:30:39.460]   And then it was a great, it'll be statuses, right?
[01:30:39.460 --> 01:30:41.020]   This is what I'm doing,
[01:30:41.020 --> 01:30:42.500]   and my friends can follow me and see it.
[01:30:42.500 --> 01:30:43.620]   And without really changing anything,
[01:30:43.620 --> 01:30:45.980]   it just happened to hit everything right
[01:30:45.980 --> 01:30:47.540]   to support this other type of interaction.
[01:30:47.540 --> 01:30:49.420]   - Well, there's also the JavaScript model,
[01:30:49.420 --> 01:30:51.340]   which Brendan and I talked about.
[01:30:51.340 --> 01:30:53.460]   He just implemented JavaScript,
[01:30:53.460 --> 01:30:55.780]   like the crappy version of JavaScript in 10 days,
[01:30:55.780 --> 01:31:00.340]   threw it out there and just changed it really quickly.
[01:31:00.340 --> 01:31:03.220]   Evolved it really quickly and now has become,
[01:31:03.220 --> 01:31:04.220]   according to Stack Exchange,
[01:31:04.220 --> 01:31:06.140]   the most popular programming language in the world.
[01:31:06.140 --> 01:31:09.340]   It drives most of the internet and even the backend
[01:31:09.340 --> 01:31:10.980]   and now mobile.
[01:31:10.980 --> 01:31:14.180]   And so that's an argument for the kind of thing
[01:31:14.180 --> 01:31:17.300]   you're talking about where the bike club people
[01:31:17.300 --> 01:31:22.060]   could literally create the thing that would run
[01:31:22.060 --> 01:31:24.420]   most of the internet 10 years from now.
[01:31:24.420 --> 01:31:27.780]   So there's something to that.
[01:31:27.780 --> 01:31:29.340]   As opposed to trying to get lucky
[01:31:29.340 --> 01:31:30.500]   or trying to think through stuff,
[01:31:30.500 --> 01:31:33.300]   it's just to solve a particular problem.
[01:31:33.300 --> 01:31:34.140]   - Do stuff, yeah.
[01:31:34.140 --> 01:31:35.460]   - And then do stuff.
[01:31:35.460 --> 01:31:36.820]   - Keep tinkering until you love it.
[01:31:36.820 --> 01:31:37.660]   - Yeah.
[01:31:37.660 --> 01:31:40.100]   And then, and of course the sad thing
[01:31:40.100 --> 01:31:42.420]   is timing and luck matter,
[01:31:42.420 --> 01:31:43.900]   and that you can't really control.
[01:31:43.900 --> 01:31:45.580]   - That's the problem, yeah.
[01:31:45.580 --> 01:31:47.900]   You can't go back to 2007.
[01:31:47.900 --> 01:31:48.740]   - Yeah.
[01:31:48.740 --> 01:31:49.580]   - That's like the number one thing you could do
[01:31:49.580 --> 01:31:51.140]   to have a lot of success with a new platform
[01:31:51.140 --> 01:31:53.500]   is go back in time 14 years.
[01:31:53.500 --> 01:31:55.140]   - So the thing you have to kind of think about
[01:31:55.140 --> 01:31:58.380]   is what's the totally new thing
[01:31:59.260 --> 01:32:03.060]   that 10 years from now would seem obvious?
[01:32:03.060 --> 01:32:05.060]   I mean, some people are saying clubhouse is that.
[01:32:05.060 --> 01:32:08.100]   There's been a lot of stuff like clubhouse before,
[01:32:08.100 --> 01:32:11.280]   but it hit the right kind of thing.
[01:32:11.280 --> 01:32:14.140]   Similar to Tesla, actually.
[01:32:14.140 --> 01:32:16.580]   What clubhouse did is it got a lot of
[01:32:16.580 --> 01:32:19.340]   relatively famous people on there quickly.
[01:32:19.340 --> 01:32:23.160]   And then the other effect is like,
[01:32:23.160 --> 01:32:24.620]   it's invite only, so like,
[01:32:24.620 --> 01:32:27.340]   oh, all the famous people are on there.
[01:32:27.340 --> 01:32:29.100]   I wonder what's, it's the FOMO.
[01:32:29.100 --> 01:32:32.300]   Like, fear that you're missing something really profound
[01:32:32.300 --> 01:32:34.260]   or exciting happening there.
[01:32:34.260 --> 01:32:36.100]   So those social effects.
[01:32:36.100 --> 01:32:37.860]   And then once you actually show up,
[01:32:37.860 --> 01:32:40.180]   I'm a huge fan of this.
[01:32:40.180 --> 01:32:41.340]   It's the JavaScript model.
[01:32:41.340 --> 01:32:44.700]   It's like clubhouse is so dumb,
[01:32:44.700 --> 01:32:46.700]   like so simple in its interface.
[01:32:46.700 --> 01:32:50.460]   Like you literally can't do anything except unmute.
[01:32:50.460 --> 01:32:51.780]   There's a mute button,
[01:32:51.780 --> 01:32:54.700]   and there's a leave quietly button, and that's it.
[01:32:54.700 --> 01:32:56.100]   And it's kind of--
[01:32:56.100 --> 01:32:59.580]   I love single use technology in that sense.
[01:32:59.580 --> 01:33:04.580]   - There's no like, there's no, it's just like trivial.
[01:33:04.580 --> 01:33:08.340]   And Twitter kind of started like that.
[01:33:08.340 --> 01:33:10.260]   Facebook started like that.
[01:33:10.260 --> 01:33:12.460]   But they've evolved quickly to add all these features
[01:33:12.460 --> 01:33:13.420]   and so on.
[01:33:13.420 --> 01:33:16.180]   And I do hope clubhouse stays that way.
[01:33:16.180 --> 01:33:17.180]   - Yeah. - It'd be interesting.
[01:33:17.180 --> 01:33:18.500]   - Or there's alternatives.
[01:33:18.500 --> 01:33:21.100]   I mean, even with clubhouse, though,
[01:33:21.100 --> 01:33:23.020]   so one of the issues with a lot of these platforms,
[01:33:23.020 --> 01:33:26.740]   I think, is bits are cheap enough now
[01:33:26.740 --> 01:33:30.940]   that we don't really need a unicorn investor model.
[01:33:30.940 --> 01:33:32.900]   I mean, the investors need that model.
[01:33:32.900 --> 01:33:37.220]   There's really not really an imperative of,
[01:33:37.220 --> 01:33:39.860]   we need something that can scale
[01:33:39.860 --> 01:33:42.140]   to 100 million plus a year revenue.
[01:33:42.140 --> 01:33:44.820]   So because it was gonna require this much seed
[01:33:44.820 --> 01:33:45.740]   and angel investment,
[01:33:45.740 --> 01:33:48.380]   and you're not gonna get this much seed angel investment
[01:33:48.380 --> 01:33:50.900]   unless you can have a potential exit this wide,
[01:33:50.900 --> 01:33:52.060]   'cause you have to be part of a portfolio
[01:33:52.060 --> 01:33:55.020]   that depends on one out of 10 exiting here.
[01:33:55.020 --> 01:33:57.420]   If you don't actually need that,
[01:33:57.420 --> 01:33:59.900]   and you don't need to satisfy that investor model,
[01:33:59.900 --> 01:34:01.620]   which I think is basically the case.
[01:34:01.620 --> 01:34:02.940]   I mean, bits are so cheap.
[01:34:02.940 --> 01:34:04.300]   Everything is so cheap.
[01:34:04.300 --> 01:34:05.820]   You don't necessarily, so even like with clubhouse,
[01:34:05.820 --> 01:34:07.500]   it's investor backed, right?
[01:34:07.500 --> 01:34:10.620]   So this notion of like, this needs to be a major platform.
[01:34:10.620 --> 01:34:14.140]   But the bike club doesn't necessarily need a major platform.
[01:34:14.140 --> 01:34:14.980]   That's where I'm interested.
[01:34:14.980 --> 01:34:15.900]   I mean, I don't know.
[01:34:15.900 --> 01:34:16.900]   There's so much money.
[01:34:16.900 --> 01:34:18.340]   That's the only problem that bets against me,
[01:34:18.340 --> 01:34:21.460]   is that you can concentrate a lot of capital
[01:34:21.460 --> 01:34:22.540]   if you do these things, right?
[01:34:22.540 --> 01:34:23.860]   I mean, so Facebook was like
[01:34:23.860 --> 01:34:26.260]   a fantastic capital concentration machine.
[01:34:26.260 --> 01:34:27.980]   It's crazy how much,
[01:34:27.980 --> 01:34:30.380]   where it even found that capital in the world
[01:34:30.380 --> 01:34:32.540]   that it could concentrate and ossify in the stock price
[01:34:32.540 --> 01:34:35.220]   that a very small number of people have access to, right?
[01:34:35.220 --> 01:34:37.260]   That's incredibly powerful.
[01:34:37.260 --> 01:34:40.700]   So when there is a possibility to consolidate
[01:34:40.700 --> 01:34:41.980]   and gather a huge amount of capital,
[01:34:41.980 --> 01:34:43.980]   that's a huge imperative that's very hard
[01:34:43.980 --> 01:34:45.860]   for the bike club to go up against, so.
[01:34:45.860 --> 01:34:47.300]   - But there's a lot of money in the bike club,
[01:34:47.300 --> 01:34:50.700]   as you see with the Wall Street bets,
[01:34:50.700 --> 01:34:53.100]   when a bunch of people get together.
[01:34:53.100 --> 01:34:54.780]   I mean, it doesn't have to be a bike,
[01:34:54.780 --> 01:34:56.220]   it could be a bunch of different bike clubs
[01:34:56.220 --> 01:34:59.340]   just kind of team up to overtake.
[01:34:59.340 --> 01:35:00.620]   - That's what we're doing now, yeah.
[01:35:00.620 --> 01:35:02.860]   Or we're gonna repurpose off the shelf stuff.
[01:35:02.860 --> 01:35:03.700]   - Yes, that's good.
[01:35:03.700 --> 01:35:05.980]   - That's not, yeah, we're gonna repurpose whatever.
[01:35:05.980 --> 01:35:07.780]   It was for office productivity or something,
[01:35:07.780 --> 01:35:11.020]   and like the clubs using Slack just to build out these.
[01:35:11.020 --> 01:35:12.580]   Yeah. - Yeah.
[01:35:12.580 --> 01:35:14.020]   Let's talk about email.
[01:35:14.020 --> 01:35:15.740]   - Yeah, that's right.
[01:35:15.740 --> 01:35:16.580]   I wrote a book.
[01:35:17.500 --> 01:35:22.500]   - You wrote yet another amazing book, "A World Without Email."
[01:35:22.500 --> 01:35:24.860]   Maybe one way to enter this discussion
[01:35:24.860 --> 01:35:28.260]   is to ask what is the hyperactive hive mind,
[01:35:28.260 --> 01:35:29.860]   which is the concept you opened the book with.
[01:35:29.860 --> 01:35:31.260]   - Yeah, and the devil.
[01:35:31.260 --> 01:35:32.900]   - And the devil. (laughs)
[01:35:32.900 --> 01:35:35.060]   - It's the scourge of hundreds of millions.
[01:35:35.060 --> 01:35:40.140]   So I think, so I called this book "A World Without Email."
[01:35:40.140 --> 01:35:40.980]   The real title should be
[01:35:40.980 --> 01:35:43.900]   "A World Without the Hyperactive Hive Mind Workflow,"
[01:35:43.900 --> 01:35:45.820]   but my publisher didn't like that, right?
[01:35:45.820 --> 01:35:47.740]   So we had to get a little bit more pithy.
[01:35:47.740 --> 01:35:50.580]   I was trying to answer the question after Deep Work,
[01:35:50.580 --> 01:35:52.780]   why is it so hard to do this?
[01:35:52.780 --> 01:35:55.620]   Like if this is so valuable, if we can produce much higher,
[01:35:55.620 --> 01:35:58.980]   people are much happier, why do we check email a day?
[01:35:58.980 --> 01:36:00.860]   Why are we on Slack all day?
[01:36:00.860 --> 01:36:02.420]   And so I started working on this book
[01:36:02.420 --> 01:36:04.220]   immediately after Deep Work.
[01:36:04.220 --> 01:36:06.780]   And so my initial interviews were done in 2016.
[01:36:06.780 --> 01:36:08.900]   So it took five years to pull the threads together.
[01:36:08.900 --> 01:36:11.340]   I was trying to understand why is it so hard
[01:36:11.340 --> 01:36:14.420]   for most people to actually find any time
[01:36:14.420 --> 01:36:16.500]   to do this stuff that actually moves the needle.
[01:36:16.500 --> 01:36:18.300]   And the story was, and I thought this was,
[01:36:18.300 --> 01:36:19.940]   I hadn't heard this reported anywhere else,
[01:36:19.940 --> 01:36:22.020]   that's why it took me so long to pull it together,
[01:36:22.020 --> 01:36:24.420]   is email arrives on the scene,
[01:36:24.420 --> 01:36:25.980]   email spreads, I trace it,
[01:36:25.980 --> 01:36:28.580]   it really picks up steam in the early 1990s,
[01:36:28.580 --> 01:36:32.300]   between like 1990 and 1995, it makes its move, right?
[01:36:32.300 --> 01:36:34.060]   And it does so for very pragmatic reasons.
[01:36:34.060 --> 01:36:36.820]   It was replacing existing communication technologies
[01:36:36.820 --> 01:36:37.660]   that it was better than.
[01:36:37.660 --> 01:36:39.940]   It was mainly the fax machine, voicemail, and memos, right?
[01:36:39.940 --> 01:36:41.700]   So this was just better, right?
[01:36:41.700 --> 01:36:44.340]   So it was a killer app because it was useful.
[01:36:44.340 --> 01:36:47.820]   In its wake came a new way of collaborating,
[01:36:47.820 --> 01:36:49.540]   and that's the hyperactive hive mind.
[01:36:49.540 --> 01:36:53.460]   So it's like the virus that follows the rats
[01:36:53.460 --> 01:36:55.780]   that went through Western Europe for the black pig.
[01:36:55.780 --> 01:36:57.840]   As email spread through organizations,
[01:36:57.840 --> 01:37:00.500]   in its wake came the hyperactive hive mind workflow,
[01:37:00.500 --> 01:37:01.480]   which says, okay, guys,
[01:37:01.480 --> 01:37:03.500]   here's the way we're gonna collaborate.
[01:37:03.500 --> 01:37:05.340]   We'll just work things out on the fly
[01:37:05.340 --> 01:37:07.300]   with unscheduled back and forth messages.
[01:37:07.300 --> 01:37:08.940]   Just boom, boom, boom, let's go back and forth.
[01:37:08.940 --> 01:37:09.780]   Hey, what about this?
[01:37:09.780 --> 01:37:10.600]   You see this?
[01:37:10.600 --> 01:37:11.440]   What about that client?
[01:37:11.440 --> 01:37:13.140]   What's going on over here?
[01:37:13.140 --> 01:37:14.780]   That followed email.
[01:37:14.780 --> 01:37:18.140]   It completely took over office work.
[01:37:18.140 --> 01:37:22.260]   And the need to keep up with all of these asynchronous
[01:37:22.260 --> 01:37:24.360]   back and forth unscheduled messages,
[01:37:24.360 --> 01:37:25.620]   as those got more and more and more,
[01:37:25.620 --> 01:37:26.700]   we had more of those to service,
[01:37:26.700 --> 01:37:28.140]   the need to service those required us
[01:37:28.140 --> 01:37:30.340]   to check more and more and more and more, right?
[01:37:30.340 --> 01:37:32.000]   And so by the time, and I go through the numbers,
[01:37:32.000 --> 01:37:33.960]   but by the time you get to today,
[01:37:33.960 --> 01:37:34.940]   now the average knowledge worker
[01:37:34.940 --> 01:37:37.420]   has to check one of these channels once every six minutes.
[01:37:37.420 --> 01:37:39.660]   Because every single thing you do in your organization,
[01:37:39.660 --> 01:37:40.640]   how you talk to your colleagues,
[01:37:40.640 --> 01:37:41.580]   how you talk to your vendors,
[01:37:41.580 --> 01:37:42.540]   how you talk to your clients,
[01:37:42.540 --> 01:37:43.740]   how you talk to the HR department,
[01:37:43.740 --> 01:37:44.900]   it's all this asynchronous,
[01:37:44.900 --> 01:37:47.180]   unscheduled back and forth messaging.
[01:37:47.180 --> 01:37:49.540]   And you have to service the conversations.
[01:37:49.540 --> 01:37:51.300]   And it spiraled out of control,
[01:37:51.300 --> 01:37:54.180]   and it has sort of devolved a lot of work in the office now
[01:37:54.180 --> 01:37:58.780]   to all I do is constantly tend communication channels.
[01:37:58.780 --> 01:38:00.740]   - So it's fascinating what you're describing
[01:38:00.740 --> 01:38:05.740]   is nobody ever paused in this whole evolution
[01:38:05.740 --> 01:38:08.740]   to try to create a system that actually works.
[01:38:08.740 --> 01:38:13.060]   That it was kind of like a huge fan of cellular automata.
[01:38:13.060 --> 01:38:17.380]   So it just kind of started a very simple mechanism,
[01:38:17.380 --> 01:38:18.220]   just like cellular automata,
[01:38:18.220 --> 01:38:20.540]   it just kind of grew to overtake
[01:38:20.540 --> 01:38:24.300]   all the fundamental communication of how we do business
[01:38:24.300 --> 01:38:25.300]   and also personal life.
[01:38:25.300 --> 01:38:27.180]   - Yeah, and that's one of the big ideas
[01:38:27.180 --> 01:38:29.740]   is that the unintentionality, right?
[01:38:29.740 --> 01:38:31.860]   So this goes back to technological determinism.
[01:38:31.860 --> 01:38:33.240]   I mean, this is a weird business book
[01:38:33.240 --> 01:38:35.780]   because I go deep on philosophy,
[01:38:35.780 --> 01:38:37.040]   I go deep on for some reason
[01:38:37.180 --> 01:38:39.060]   we get into paleoanthropology for a while,
[01:38:39.060 --> 01:38:40.220]   we do a lot of neuroscience.
[01:38:40.220 --> 01:38:41.540]   It's kind of a weird book,
[01:38:41.540 --> 01:38:44.780]   but I got real into this technological determinism, right?
[01:38:44.780 --> 01:38:46.900]   This notion that just the presence of a technology
[01:38:46.900 --> 01:38:48.740]   can change how people act.
[01:38:48.740 --> 01:38:49.700]   That's my big argument
[01:38:49.700 --> 01:38:51.080]   about what happened with the hive mind.
[01:38:51.080 --> 01:38:54.260]   And I can document specific examples, right?
[01:38:54.260 --> 01:38:59.260]   So I document this example in IBM 1987, maybe 85,
[01:38:59.260 --> 01:39:01.220]   but it's in like the mid to late 80s,
[01:39:01.220 --> 01:39:03.580]   IBM, Armonk headquarters,
[01:39:03.580 --> 01:39:05.580]   we're gonna put an internal email, right?
[01:39:05.580 --> 01:39:07.600]   Because it's convenient.
[01:39:07.600 --> 01:39:09.380]   And so they ran a whole study.
[01:39:09.380 --> 01:39:11.840]   And so I talked to the engineer who ran the study,
[01:39:11.840 --> 01:39:13.120]   Adrian Stundlick, we're gonna run this study
[01:39:13.120 --> 01:39:14.800]   to figure out how much do we communicate
[01:39:14.800 --> 01:39:17.800]   because it was still an era where it's expensive, right?
[01:39:17.800 --> 01:39:19.160]   So you have to provision a mainframe
[01:39:19.160 --> 01:39:20.880]   so you can't over provision.
[01:39:20.880 --> 01:39:22.880]   Like we wanna know how much communication actually happens.
[01:39:22.880 --> 01:39:24.220]   So they went and figured it out.
[01:39:24.220 --> 01:39:26.400]   How many memos, how many calls, how many notes?
[01:39:26.400 --> 01:39:28.760]   Great, we'll provision a mainframe to handle email
[01:39:28.760 --> 01:39:29.940]   that can handle all of that.
[01:39:29.940 --> 01:39:32.660]   So if all of our communication moves to email,
[01:39:32.660 --> 01:39:34.200]   the mainframe will still be fine.
[01:39:34.200 --> 01:39:36.060]   In three days, they had melted it down.
[01:39:36.060 --> 01:39:39.500]   People were communicating six times more than that estimate.
[01:39:39.500 --> 01:39:41.200]   So just in three days,
[01:39:41.200 --> 01:39:44.420]   the presence of a low friction digital communication tool
[01:39:44.420 --> 01:39:46.600]   drastically changed how everyone collaborated.
[01:39:46.600 --> 01:39:49.560]   So that's not enough time for an all hands meeting.
[01:39:49.560 --> 01:39:51.140]   Guys, we figured it out.
[01:39:51.140 --> 01:39:52.760]   This is what we need to communicate a lot more
[01:39:52.760 --> 01:39:54.980]   is what's gonna make us more productive.
[01:39:54.980 --> 01:39:57.540]   We need more emails, it's emergent.
[01:39:57.540 --> 01:40:00.800]   - Isn't that just on the positive end, amazing to you?
[01:40:00.800 --> 01:40:03.240]   Like, isn't email amazing?
[01:40:03.240 --> 01:40:04.720]   Like in those early days,
[01:40:04.720 --> 01:40:07.180]   like just the frictionless communication.
[01:40:07.180 --> 01:40:09.620]   I mean, email is awesome.
[01:40:09.620 --> 01:40:13.500]   Like, people say that there's a lot of problems with emails,
[01:40:13.500 --> 01:40:15.020]   just like people say a lot of problems with Twitter
[01:40:15.020 --> 01:40:15.860]   and so on.
[01:40:15.860 --> 01:40:18.540]   It's kinda cool that you can just send a little note.
[01:40:18.540 --> 01:40:19.940]   - It was a miracle, right?
[01:40:19.940 --> 01:40:23.820]   So I wrote a, this originally was a New Yorker piece
[01:40:23.820 --> 01:40:26.100]   from a year or two ago called "Was Email a Mistake?"
[01:40:26.100 --> 01:40:28.080]   And then it's in the book too.
[01:40:28.080 --> 01:40:31.220]   But I go into the history of email,
[01:40:31.220 --> 01:40:32.740]   like why did it come along?
[01:40:32.740 --> 01:40:34.400]   And it solved a huge problem.
[01:40:34.400 --> 01:40:37.880]   So it was the problem of fast asynchronous communication.
[01:40:37.880 --> 01:40:39.340]   And it was a problem that did not exist
[01:40:39.340 --> 01:40:41.060]   until we got large offices.
[01:40:41.060 --> 01:40:43.400]   We got large offices, synchronous communication,
[01:40:43.400 --> 01:40:44.820]   like let's get on the phone at the same time,
[01:40:44.820 --> 01:40:45.840]   there's too much overhead to it,
[01:40:45.840 --> 01:40:48.160]   there's too many people you might have to talk to.
[01:40:48.160 --> 01:40:50.580]   Asynchronous communication, like let me send you a memo
[01:40:50.580 --> 01:40:52.680]   when I'm ready and you can read it when you're ready,
[01:40:52.680 --> 01:40:53.800]   took too long.
[01:40:53.800 --> 01:40:54.980]   And so it was like a huge problem.
[01:40:54.980 --> 01:40:56.740]   So one of the things I talked about is the way that
[01:40:56.740 --> 01:40:58.940]   when they built the CIA headquarters,
[01:40:58.940 --> 01:41:02.100]   there was such a need for fast asynchronous communication
[01:41:02.100 --> 01:41:05.100]   that they built a pneumatic powered email system.
[01:41:05.100 --> 01:41:07.020]   They had these pneumatic tubes all throughout
[01:41:07.020 --> 01:41:09.660]   the headquarters with electromagnetic routers.
[01:41:09.660 --> 01:41:12.700]   So you would put your message in a plexiglass tube
[01:41:12.700 --> 01:41:15.220]   and you would turn these brass dials about the location,
[01:41:15.220 --> 01:41:17.260]   you would stick it in these things and pneumatic tubes
[01:41:17.260 --> 01:41:19.620]   and it would shoot and sort and work its way
[01:41:19.620 --> 01:41:23.100]   through these tubes to show up in just a minute or something
[01:41:23.100 --> 01:41:24.860]   at the floor and at the general office suite
[01:41:24.860 --> 01:41:25.940]   where you wanted to go.
[01:41:25.940 --> 01:41:28.300]   And my point is the fact that they spent so much money
[01:41:28.300 --> 01:41:31.060]   to make that work show how important
[01:41:31.060 --> 01:41:33.100]   fast asynchronous communication was to large offices.
[01:41:33.100 --> 01:41:35.020]   So when email came along,
[01:41:35.020 --> 01:41:37.060]   it was a productivity silver bullet.
[01:41:37.060 --> 01:41:37.900]   It was a miracle.
[01:41:37.900 --> 01:41:39.980]   I talked to the researchers who were working on
[01:41:39.980 --> 01:41:41.860]   computer supported collaboration in the late 80s,
[01:41:41.860 --> 01:41:44.100]   trying to figure out how are we gonna use computer networks
[01:41:44.100 --> 01:41:44.980]   to be more productive?
[01:41:44.980 --> 01:41:47.180]   And they were building all these systems and tools.
[01:41:47.180 --> 01:41:49.460]   Email showed up, it just wiped all that research
[01:41:49.460 --> 01:41:50.300]   off the map.
[01:41:50.300 --> 01:41:52.100]   There was no need to build these custom
[01:41:52.100 --> 01:41:53.420]   intranet applications.
[01:41:53.420 --> 01:41:56.380]   There was no need to build these communication platforms.
[01:41:56.380 --> 01:41:58.340]   Email could just do everything.
[01:41:58.340 --> 01:42:00.840]   So it was a miracle application,
[01:42:00.840 --> 01:42:02.740]   which is why it spread everywhere.
[01:42:02.740 --> 01:42:04.420]   That's one of these things where, okay,
[01:42:04.420 --> 01:42:05.660]   unintended consequences, right?
[01:42:05.660 --> 01:42:07.700]   You had this miracle productivity silver bullet.
[01:42:07.700 --> 01:42:10.620]   It spread everywhere, but it was so effective.
[01:42:10.620 --> 01:42:12.460]   It just, I don't know, like a drug.
[01:42:12.460 --> 01:42:15.360]   I'm sure there's some pandemic metaphor here,
[01:42:15.360 --> 01:42:17.580]   analogy here of a drug that like is so effective
[01:42:17.580 --> 01:42:19.060]   at treating this, that it also blows up
[01:42:19.060 --> 01:42:21.020]   your whole immune system and then everyone gets sick.
[01:42:21.020 --> 01:42:23.460]   - Well, ultimately it probably significantly increased
[01:42:23.460 --> 01:42:24.680]   the productivity of the world,
[01:42:24.680 --> 01:42:28.060]   but there's a kind of hump that it now has plateaued.
[01:42:28.060 --> 01:42:32.160]   And then the fundamental question you're asking is like,
[01:42:32.160 --> 01:42:33.720]   okay, how do we take the next,
[01:42:33.720 --> 01:42:35.680]   how do we keep increasing the productivity?
[01:42:35.680 --> 01:42:36.840]   - No, I think it brought it down.
[01:42:36.840 --> 01:42:41.320]   So my contention, and so again,
[01:42:41.320 --> 01:42:42.160]   there's a little bit in the book,
[01:42:42.160 --> 01:42:44.080]   but I have a more recent Wired article
[01:42:44.080 --> 01:42:47.000]   that puts some newer numbers to this.
[01:42:47.000 --> 01:42:50.000]   I subscribe to the hypothesis that the hyperactive hive mind
[01:42:50.000 --> 01:42:51.520]   was so detrimental.
[01:42:51.520 --> 01:42:53.840]   So yeah, it helped productivity at first, right?
[01:42:53.840 --> 01:42:56.480]   When you could do fast asynchronous communication,
[01:42:56.480 --> 01:42:58.980]   but very quickly there was a sort of exponential rise
[01:42:58.980 --> 01:43:00.980]   in communication amounts.
[01:43:00.980 --> 01:43:02.660]   Once we got to the point where the hive mind meant
[01:43:02.660 --> 01:43:04.440]   you had to constantly check your email,
[01:43:04.440 --> 01:43:06.860]   I think that made us so unproductive
[01:43:06.860 --> 01:43:08.220]   that it actually was pulling down
[01:43:08.220 --> 01:43:09.400]   non-industrial productivity.
[01:43:09.400 --> 01:43:11.220]   And I think the only reason why,
[01:43:11.220 --> 01:43:12.940]   so it certainly has not been going up.
[01:43:12.940 --> 01:43:15.220]   That metric has been stagnating for a long time now
[01:43:15.220 --> 01:43:16.900]   while all this was going on.
[01:43:16.900 --> 01:43:19.180]   I think the only reason why it hasn't fallen
[01:43:19.180 --> 01:43:22.540]   is that we added these extra shifts off the books.
[01:43:22.540 --> 01:43:23.880]   I'm gonna work for three hours in the morning,
[01:43:23.880 --> 01:43:25.580]   I'm gonna work for three hours at night.
[01:43:25.580 --> 01:43:27.460]   And only that I think has allowed us
[01:43:27.460 --> 01:43:31.620]   to basically maintain a stagnated non-industrial growth.
[01:43:31.620 --> 01:43:32.980]   We should have been shooting up the charts.
[01:43:32.980 --> 01:43:35.580]   I mean, this is miraculous innovations.
[01:43:35.580 --> 01:43:37.000]   Computer networks, and then we built out
[01:43:37.000 --> 01:43:39.740]   these hundred billion dollar ubiquitous worldwide
[01:43:39.740 --> 01:43:42.000]   high-speed wireless internet infrastructure
[01:43:42.000 --> 01:43:43.380]   with supercomputers in our pockets
[01:43:43.380 --> 01:43:44.820]   where we could talk to anyone at any time.
[01:43:44.820 --> 01:43:47.300]   Like, why did our productivity not shoot off the charts?
[01:43:47.300 --> 01:43:48.620]   Because our brain can't context switch
[01:43:48.620 --> 01:43:49.460]   once every six minutes.
[01:43:49.460 --> 01:43:51.500]   - So it's fundamentally back to the context switching.
[01:43:51.500 --> 01:43:53.420]   - Context switching is poison.
[01:43:53.420 --> 01:43:54.740]   - Context switching is poison.
[01:43:54.740 --> 01:43:58.300]   What is it about email that forces context switching?
[01:43:58.300 --> 01:44:00.580]   Is it both our psychology that drags us in?
[01:44:00.580 --> 01:44:02.260]   Or is it the expectation of--
[01:44:02.260 --> 01:44:03.860]   - Yeah, right, right, because it's not,
[01:44:03.860 --> 01:44:06.740]   I think we've seen this through a personal will
[01:44:06.740 --> 01:44:08.980]   or failure lens recently.
[01:44:08.980 --> 01:44:11.820]   Like, oh, am I addicted to email?
[01:44:11.820 --> 01:44:14.100]   I have bad etiquette about my email.
[01:44:14.100 --> 01:44:16.100]   No, it's the underlying workflow.
[01:44:16.100 --> 01:44:19.100]   So the tool itself I will exonerate.
[01:44:19.100 --> 01:44:23.660]   I think I would rather use POP3 than a fax protocol.
[01:44:23.660 --> 01:44:24.980]   I think it's easier.
[01:44:24.980 --> 01:44:27.020]   The issue is the hyperactive hive mind workflow.
[01:44:27.020 --> 01:44:30.900]   So if I am now collaborating with 20 or 30 different people
[01:44:30.900 --> 01:44:33.060]   with back and forth unscheduled messaging,
[01:44:33.060 --> 01:44:35.220]   I have to tend those conversations, right?
[01:44:35.220 --> 01:44:38.180]   It's like you have 30 metaphorical ping pong tables.
[01:44:38.180 --> 01:44:39.380]   And when the balls come back across,
[01:44:39.380 --> 01:44:41.220]   you have to pretty soon hit it back
[01:44:41.220 --> 01:44:43.460]   or stuff actually grinds to a halt.
[01:44:43.460 --> 01:44:45.580]   So it's the workflow that's the problem.
[01:44:45.580 --> 01:44:47.140]   It's not the tool, it's the fact that we use it
[01:44:47.140 --> 01:44:48.400]   to do all of our collaboration.
[01:44:48.400 --> 01:44:49.900]   Let's just send messages back and forth,
[01:44:49.900 --> 01:44:52.780]   which means you can't be far from checking that
[01:44:52.780 --> 01:44:54.820]   'cause if you take a break, if you batch,
[01:44:54.820 --> 01:44:56.780]   if you try to have better habits,
[01:44:56.780 --> 01:44:58.020]   it's gonna slow things down.
[01:44:58.020 --> 01:45:02.420]   So my whole villain is this hyperactive hive mind workflow.
[01:45:02.420 --> 01:45:03.580]   The tool is fine.
[01:45:03.580 --> 01:45:05.380]   I don't want the tool to go away,
[01:45:05.380 --> 01:45:07.260]   but I wanna replace the hyperactive hive mind workflow.
[01:45:07.260 --> 01:45:10.060]   I think this is gonna be one of the biggest
[01:45:10.060 --> 01:45:12.980]   value generating productivity revolutions
[01:45:12.980 --> 01:45:14.180]   of the 21st century.
[01:45:14.180 --> 01:45:16.980]   I quote an anonymous CEO who's pretty well-known
[01:45:16.980 --> 01:45:19.620]   who says this is gonna be the moon shot of the 21st century.
[01:45:19.620 --> 01:45:20.980]   It's gonna be of that importance.
[01:45:20.980 --> 01:45:24.420]   There's so much latent productivity that's being suppressed
[01:45:24.420 --> 01:45:26.340]   because we just figure things out on the fly in email
[01:45:26.340 --> 01:45:27.740]   that as we figure that out,
[01:45:27.740 --> 01:45:32.060]   I think it's gonna be hundreds of billions of dollars.
[01:45:32.060 --> 01:45:35.480]   - You're so absolutely right.
[01:45:35.480 --> 01:45:39.140]   The question is, what does a world without email look like?
[01:45:39.140 --> 01:45:40.900]   How do we fix email?
[01:45:40.900 --> 01:45:44.100]   - So what happens is, at least in my vision,
[01:45:44.100 --> 01:45:46.900]   you identify, well, actually,
[01:45:46.900 --> 01:45:49.220]   there's these different processes that make up my workday.
[01:45:49.220 --> 01:45:52.020]   Like these are things that I do repeatedly,
[01:45:52.020 --> 01:45:53.460]   often in collaboration with other people
[01:45:53.460 --> 01:45:56.460]   that do useful things for my company or whatever.
[01:45:56.460 --> 01:45:57.980]   Right now, most of these processes
[01:45:57.980 --> 01:46:00.660]   are implicitly implemented with the hyperactive hive mind.
[01:46:00.660 --> 01:46:01.580]   How do we do this thing?
[01:46:01.580 --> 01:46:02.660]   Like answering client questions
[01:46:02.660 --> 01:46:03.820]   to shoot messages back and forth.
[01:46:03.820 --> 01:46:05.220]   You know, how do we do this thing?
[01:46:05.220 --> 01:46:06.300]   Posting podcast episodes,
[01:46:06.300 --> 01:46:07.820]   we'll just figure it out on the fly.
[01:46:07.820 --> 01:46:09.340]   My main argument is we actually have to do
[01:46:09.340 --> 01:46:11.340]   like they did in the industrial sector.
[01:46:11.340 --> 01:46:12.860]   Take each of these processes and say,
[01:46:12.860 --> 01:46:14.900]   is there a better way to do this?
[01:46:14.900 --> 01:46:16.980]   And by better, I mean a way that's gonna minimize
[01:46:16.980 --> 01:46:19.220]   the need to have unscheduled back and forth messaging.
[01:46:19.220 --> 01:46:22.020]   So we actually have to do process engineering.
[01:46:22.020 --> 01:46:24.300]   This created a massive growth in productivity
[01:46:24.300 --> 01:46:25.900]   in the industrial sector during the 20th century.
[01:46:25.900 --> 01:46:26.900]   We have to do it in knowledge work.
[01:46:26.900 --> 01:46:28.460]   We can't just rock and roll an inbox
[01:46:28.460 --> 01:46:30.100]   as we actually have to say,
[01:46:30.100 --> 01:46:31.420]   how do we deal with client questions?
[01:46:31.420 --> 01:46:32.620]   Well, let's put in place a process
[01:46:32.620 --> 01:46:35.060]   that doesn't require us to send messages back and forth.
[01:46:35.060 --> 01:46:36.580]   How do we post podcast episodes?
[01:46:36.580 --> 01:46:38.540]   Let's automate this to a degree where
[01:46:38.540 --> 01:46:40.380]   I don't have to just send you a message on the fly.
[01:46:40.380 --> 01:46:43.220]   And you do this process by process
[01:46:43.220 --> 01:46:45.180]   and the pressure on that inbox is released.
[01:46:45.180 --> 01:46:46.980]   And now you don't have to check it every six minutes.
[01:46:46.980 --> 01:46:47.940]   So you still have email.
[01:46:47.940 --> 01:46:49.140]   I mean, like I need to send you a file.
[01:46:49.140 --> 01:46:50.380]   Sure, I'll use email,
[01:46:50.380 --> 01:46:52.700]   but we're not coordinating or collaborating over email
[01:46:52.700 --> 01:46:55.020]   or Slack, which is just a faster way of doing the hive mind.
[01:46:55.020 --> 01:46:57.860]   I mean, Slack doesn't solve anything there.
[01:46:57.860 --> 01:47:00.260]   You have better structured bespoke processes.
[01:47:00.260 --> 01:47:01.780]   I think that's what's gonna unleash
[01:47:01.780 --> 01:47:03.180]   this massive productivity.
[01:47:03.180 --> 01:47:05.580]   - Bespoke, so the interesting thing is like,
[01:47:05.580 --> 01:47:07.620]   if for example, you and I exchanged some emails.
[01:47:07.620 --> 01:47:10.980]   So obviously I, for, let's just say my particular case,
[01:47:10.980 --> 01:47:11.940]   I scheduled podcasts.
[01:47:11.940 --> 01:47:13.900]   There's a bunch of different tasks.
[01:47:13.900 --> 01:47:16.620]   Fascinatingly enough that I do
[01:47:16.620 --> 01:47:19.180]   that could be converted into processes.
[01:47:19.180 --> 01:47:21.820]   Is it up to me to create that process?
[01:47:21.820 --> 01:47:23.740]   Or do you think we also need to build tools
[01:47:23.740 --> 01:47:28.500]   just like email was a protocol for helping us
[01:47:28.500 --> 01:47:31.020]   create processes for the different tasks?
[01:47:31.020 --> 01:47:34.260]   - I mean, I think ultimately the whole organization,
[01:47:34.260 --> 01:47:35.380]   the whole team has to be involved.
[01:47:35.380 --> 01:47:37.740]   I think ultimately there's certainly a lot of investor money
[01:47:37.740 --> 01:47:40.460]   being spent right now to try to figure out those tools.
[01:47:40.460 --> 01:47:42.140]   So I think Silicon Valley has figured this out
[01:47:42.140 --> 01:47:43.140]   in the past couple of years.
[01:47:43.140 --> 01:47:45.060]   This is the difference between
[01:47:45.060 --> 01:47:47.220]   when I was talking to people after Deep Work
[01:47:47.220 --> 01:47:51.580]   and now five years later is this scent is in the air.
[01:47:51.580 --> 01:47:53.260]   Because there's so much latent productivity.
[01:47:53.260 --> 01:47:54.900]   So yes, there are gonna be new tools,
[01:47:54.900 --> 01:47:55.740]   which I think could help.
[01:47:55.740 --> 01:47:57.020]   There are already tools that exist.
[01:47:57.020 --> 01:47:59.620]   I mean, in the different groups I profiled
[01:47:59.620 --> 01:48:03.740]   use things like Trello or Basecamp or Asana or Flow
[01:48:03.740 --> 01:48:06.060]   and our schedule wants and acuity.
[01:48:06.060 --> 01:48:08.740]   Like there's a lot of tools out there.
[01:48:08.740 --> 01:48:10.700]   The key is not to think about it in terms of
[01:48:10.700 --> 01:48:12.500]   what tool do I replace email with?
[01:48:12.500 --> 01:48:14.300]   Instead you think about it with,
[01:48:14.300 --> 01:48:16.180]   I have a pro, we're trying to come up with a process
[01:48:16.180 --> 01:48:17.580]   that reduces back and forth messages.
[01:48:17.580 --> 01:48:21.300]   Oh, what tool might help us do that?
[01:48:21.300 --> 01:48:22.220]   Yeah, and I would push,
[01:48:22.220 --> 01:48:24.020]   it's not about necessarily efficiency.
[01:48:24.020 --> 01:48:26.020]   In fact, some of these things are gonna take more time.
[01:48:26.020 --> 01:48:29.780]   So writing a letter to someone is like a high value activity
[01:48:29.780 --> 01:48:30.940]   it's probably worth doing.
[01:48:30.940 --> 01:48:33.460]   The thing that's killer is the back and forth.
[01:48:33.460 --> 01:48:35.100]   'Cause now I have to keep checking, right?
[01:48:35.100 --> 01:48:36.700]   So we scheduled this together
[01:48:36.700 --> 01:48:38.540]   'cause I knew you from before,
[01:48:38.540 --> 01:48:41.500]   but like most of the interviews I was scheduling for this,
[01:48:41.500 --> 01:48:43.780]   actually I have a process with my publicist
[01:48:43.780 --> 01:48:45.140]   where we use a shared document
[01:48:45.140 --> 01:48:46.460]   and she puts stuff in there
[01:48:46.460 --> 01:48:48.060]   and then I check it twice a week
[01:48:48.060 --> 01:48:50.060]   and there's scheduling options.
[01:48:50.060 --> 01:48:51.180]   I say, here's what I wanna do this one
[01:48:51.180 --> 01:48:52.780]   or this will work for this one or whatever.
[01:48:52.780 --> 01:48:54.980]   And it takes more time in the moment than just,
[01:48:54.980 --> 01:48:58.460]   but it means that we have almost no back and forth messaging
[01:48:58.460 --> 01:49:00.500]   for podcast scheduling, which without this,
[01:49:00.500 --> 01:49:02.460]   so like with my UK publisher,
[01:49:02.460 --> 01:49:03.860]   I didn't put this process into place
[01:49:03.860 --> 01:49:06.420]   'cause we're not doing as many interviews,
[01:49:06.420 --> 01:49:07.420]   but it's all the time.
[01:49:07.420 --> 01:49:08.420]   And I'm like, oh man,
[01:49:08.420 --> 01:49:10.100]   I could really feel the difference, right?
[01:49:10.100 --> 01:49:11.740]   It's the back and forth that's killer.
[01:49:11.740 --> 01:49:15.060]   - I suppose it is up to the individual people involved.
[01:49:15.060 --> 01:49:18.140]   Like you said, knowledge workers,
[01:49:18.140 --> 01:49:21.020]   like they have to carry the responsibility
[01:49:21.020 --> 01:49:23.540]   of creating processes.
[01:49:23.540 --> 01:49:25.940]   Like how, always asking the first principles question,
[01:49:25.940 --> 01:49:28.300]   how can this be converted into a process?
[01:49:28.300 --> 01:49:30.700]   - Yeah, so you can start by doing this yourself,
[01:49:30.700 --> 01:49:32.700]   like just with what you can control.
[01:49:32.700 --> 01:49:34.940]   I think ultimately once the teams are doing that,
[01:49:34.940 --> 01:49:36.460]   I think that's probably the right scale.
[01:49:36.460 --> 01:49:38.180]   If you try to do that at the organizational scale,
[01:49:38.180 --> 01:49:39.860]   you're gonna get bureaucracy, right?
[01:49:39.860 --> 01:49:44.660]   So if Elon Musk is gonna dictate down
[01:49:44.660 --> 01:49:46.900]   to everyone at Tesla or something like this,
[01:49:46.900 --> 01:49:48.620]   that's too much remove and you get bureaucracy.
[01:49:48.620 --> 01:49:50.980]   But if it's, we're a team of six
[01:49:50.980 --> 01:49:55.140]   that's working together on whatever powertrain software,
[01:49:55.140 --> 01:49:56.420]   then we can figure out on our own,
[01:49:56.420 --> 01:49:57.260]   what are our processes?
[01:49:57.260 --> 01:49:58.180]   How do we wanna do this?
[01:49:58.180 --> 01:50:00.020]   - So it's ultimately also creating a culture
[01:50:00.020 --> 01:50:02.020]   where it's saying like in email,
[01:50:02.020 --> 01:50:03.820]   sending an email just for the hell of it,
[01:50:03.820 --> 01:50:04.740]   it should be taboo.
[01:50:05.620 --> 01:50:10.620]   So you are being destructive to the productivity of the team
[01:50:10.620 --> 01:50:12.540]   by sending this email,
[01:50:12.540 --> 01:50:17.060]   as opposed to helping develop a process and so on
[01:50:17.060 --> 01:50:20.780]   that will ultimately automate this.
[01:50:20.780 --> 01:50:22.500]   - That's why I'm trying to spread this message
[01:50:22.500 --> 01:50:24.180]   of the context switches is poison.
[01:50:24.180 --> 01:50:25.460]   I get so much into the science of it.
[01:50:25.460 --> 01:50:28.020]   I think we underestimate how much it kills us
[01:50:28.020 --> 01:50:29.660]   to have to wrench away our context,
[01:50:29.660 --> 01:50:30.820]   look at a message and come back.
[01:50:30.820 --> 01:50:32.820]   And so once you have the mindset of,
[01:50:32.820 --> 01:50:35.180]   it's a huge thing to ask of someone
[01:50:35.180 --> 01:50:37.100]   to have to take their attention off something
[01:50:37.100 --> 01:50:38.180]   and look back at this.
[01:50:38.180 --> 01:50:40.380]   And if they have to do that for three or four times,
[01:50:40.380 --> 01:50:42.140]   like we're just gonna figure this out on the fly
[01:50:42.140 --> 01:50:44.060]   and every message is gonna require five checks
[01:50:44.060 --> 01:50:45.540]   of the inbox while you wait for it.
[01:50:45.540 --> 01:50:47.660]   Now you've created whatever it is at this point,
[01:50:47.660 --> 01:50:50.220]   25 or 30 context shifts.
[01:50:50.220 --> 01:50:52.820]   Like you've just done a huge disservice to someone's day.
[01:50:52.820 --> 01:50:54.620]   This would be like if I had a professional athlete,
[01:50:54.620 --> 01:50:55.900]   like, "Hey, do me a favor.
[01:50:55.900 --> 01:50:57.060]   I need you to go do this press interview."
[01:50:57.060 --> 01:50:59.580]   But to get there, you're gonna have to carry this sandbag
[01:50:59.580 --> 01:51:00.700]   and sprint up this hill,
[01:51:00.700 --> 01:51:02.060]   like completely exhaust your muscles
[01:51:02.060 --> 01:51:03.180]   and then you have to go play a game.
[01:51:03.180 --> 01:51:04.660]   Like, of course I'm not gonna ask an athlete
[01:51:04.660 --> 01:51:07.340]   to do like an incredibly physically demanding thing
[01:51:07.340 --> 01:51:08.660]   right before a game,
[01:51:08.660 --> 01:51:11.780]   but something as easy as thoughts, question mark,
[01:51:11.780 --> 01:51:13.260]   or like, "Hey, do you wanna jump on a call
[01:51:13.260 --> 01:51:14.900]   and it's gonna be six back and forth messages
[01:51:14.900 --> 01:51:15.820]   to figure it out?"
[01:51:15.820 --> 01:51:17.780]   It's kind of the cognitive equivalent, right?
[01:51:17.780 --> 01:51:19.580]   You're taking the wind out of someone.
[01:51:19.580 --> 01:51:22.460]   - Yeah, and by the way, for people who are listening,
[01:51:22.460 --> 01:51:24.660]   'cause I recently posted a few job openings
[01:51:24.660 --> 01:51:26.500]   for so I had to help with this thing.
[01:51:26.500 --> 01:51:28.860]   And one of the things that people are surprised
[01:51:28.860 --> 01:51:29.700]   when they work with me
[01:51:29.700 --> 01:51:32.180]   is how many spreadsheets and processes are involved.
[01:51:32.180 --> 01:51:33.420]   - And it's like Claude Shannon, right?
[01:51:33.420 --> 01:51:36.340]   I talked about communication theory or information theory.
[01:51:36.340 --> 01:51:38.700]   It takes time to come up with a clever code upfront.
[01:51:38.700 --> 01:51:39.740]   So you spend more time upfront
[01:51:39.740 --> 01:51:40.780]   figuring out those spreadsheets
[01:51:40.780 --> 01:51:42.860]   and trying to get people on board with it.
[01:51:42.860 --> 01:51:45.300]   But then your communication going forward
[01:51:45.300 --> 01:51:46.260]   is all much more efficient.
[01:51:46.260 --> 01:51:49.740]   So over time, you're using much less bandwidth, right?
[01:51:49.740 --> 01:51:52.300]   So you do pain upfront.
[01:51:52.300 --> 01:51:54.700]   It's quicker just right now to send an email.
[01:51:54.700 --> 01:51:56.140]   But if I spend a half day to do this
[01:51:56.140 --> 01:52:00.220]   over the next six months, I've saved myself 600 emails.
[01:52:00.220 --> 01:52:02.660]   - Now, here's a tough question for, you know,
[01:52:02.660 --> 01:52:04.660]   from the computer science perspective,
[01:52:04.660 --> 01:52:07.100]   we often over optimize.
[01:52:07.100 --> 01:52:10.940]   So you've create processes and you, okay.
[01:52:10.940 --> 01:52:14.860]   Just like you're saying, it's so pleasurable
[01:52:14.860 --> 01:52:19.700]   to increase in the long-term productivity
[01:52:19.700 --> 01:52:22.900]   that sometimes you just enjoy that process in itself
[01:52:22.900 --> 01:52:25.020]   by just creating processes.
[01:52:25.020 --> 01:52:27.280]   And you actually never,
[01:52:27.280 --> 01:52:31.060]   like it has a negative effect on productivity long-term
[01:52:31.060 --> 01:52:33.500]   because you're too obsessed with the processes.
[01:52:33.500 --> 01:52:37.860]   Is that a nice problem to have essentially?
[01:52:37.860 --> 01:52:38.860]   - I mean, it's a problem.
[01:52:38.860 --> 01:52:41.220]   I mean, because let's look at the one sector
[01:52:41.220 --> 01:52:44.540]   that does do this, which is developers, right?
[01:52:44.540 --> 01:52:47.220]   So agile methodologies like Scrum or Kanban
[01:52:47.220 --> 01:52:49.860]   are basically workflow methodologies
[01:52:49.860 --> 01:52:52.360]   that are much better than the hyperactive hive mind.
[01:52:52.360 --> 01:52:55.740]   But man, some of those programmers get pretty obsessive.
[01:52:55.740 --> 01:52:56.660]   I don't know if you've ever talked
[01:52:56.660 --> 01:52:59.460]   to a whatever level three Scrum master.
[01:52:59.460 --> 01:53:01.980]   They get really obsessive about like,
[01:53:01.980 --> 01:53:04.200]   it has to happen exactly this way.
[01:53:04.200 --> 01:53:05.900]   And it's probably seven times more complex
[01:53:05.900 --> 01:53:07.300]   than it needs to be.
[01:53:07.300 --> 01:53:09.780]   I'm hoping that's just because nerds like me,
[01:53:09.780 --> 01:53:11.020]   you know, like to do that.
[01:53:11.020 --> 01:53:14.220]   But it's a broadly probably an issue, right?
[01:53:14.220 --> 01:53:16.060]   We have to be careful because you can just go down
[01:53:16.060 --> 01:53:17.940]   that fiddling path.
[01:53:17.940 --> 01:53:19.760]   Like, so it needs to be, here's how we do it.
[01:53:19.760 --> 01:53:22.580]   Let's reduce the messages and let's roll, you know?
[01:53:22.580 --> 01:53:26.120]   You can't save yourself through
[01:53:26.120 --> 01:53:28.220]   if you can get the process just right, right?
[01:53:28.220 --> 01:53:30.580]   So I wrote this article kind of recently
[01:53:30.580 --> 01:53:32.820]   called "The Rise and Fall of Getting Things Done."
[01:53:32.820 --> 01:53:37.060]   And I profiled this productivity guru named Merlin Mann.
[01:53:37.060 --> 01:53:39.940]   And I talked about this movement called productivity prong
[01:53:39.940 --> 01:53:42.900]   as like elite speak term in the early 2000s
[01:53:42.900 --> 01:53:44.540]   where people just became convinced
[01:53:44.540 --> 01:53:47.140]   that if they could combine their productivity systems
[01:53:47.140 --> 01:53:50.180]   with software and they could find just the right software,
[01:53:50.180 --> 01:53:51.900]   just the right configuration where they could offload
[01:53:51.900 --> 01:53:53.100]   most of the difficulty of work,
[01:53:53.100 --> 01:53:54.660]   what happened with the machines.
[01:53:54.660 --> 01:53:55.500]   We'd kind of figure it out for them.
[01:53:55.500 --> 01:53:57.060]   And then they could just sort of crank widgets
[01:53:57.060 --> 01:53:58.660]   and it'd be, and the whole thing fell apart
[01:53:58.660 --> 01:54:00.860]   because work is hard and it's hard to do
[01:54:00.860 --> 01:54:03.020]   and making decisions about what to work on is hard
[01:54:03.020 --> 01:54:04.740]   and no system can really do that for you.
[01:54:04.740 --> 01:54:08.820]   So you have to have this sort of balance between,
[01:54:08.820 --> 01:54:10.900]   context switches are poison.
[01:54:10.900 --> 01:54:12.460]   So we got to get rid of the context switches.
[01:54:12.460 --> 01:54:13.900]   Once like something's working good enough
[01:54:13.900 --> 01:54:17.060]   to get rid of the context switches, then get after it.
[01:54:17.060 --> 01:54:19.500]   - Yeah, there's a psychological process there for me.
[01:54:19.500 --> 01:54:23.580]   The OCD nature, like I've literally embarrassing enough
[01:54:23.580 --> 01:54:26.260]   have lost my shit before when,
[01:54:26.260 --> 01:54:30.420]   so in many of the processes that involve Python scripts,
[01:54:30.420 --> 01:54:34.900]   the rule is to not use spaces.
[01:54:34.900 --> 01:54:36.580]   Underscores, there's like rules
[01:54:36.580 --> 01:54:39.300]   for like how you format stuff, okay?
[01:54:39.300 --> 01:54:42.100]   And like, I should not lose my shit
[01:54:42.100 --> 01:54:45.300]   when somebody had a space and maybe capital letters.
[01:54:45.300 --> 01:54:48.020]   Like, it's okay to have a space.
[01:54:48.020 --> 01:54:50.980]   'Cause there's this feeling like something's not perfect.
[01:54:50.980 --> 01:54:51.820]   - Yeah.
[01:54:51.820 --> 01:54:54.620]   - And as opposed to in the Python script,
[01:54:54.620 --> 01:54:56.660]   allowing some flexibility around that,
[01:54:56.660 --> 01:54:59.100]   you create this programmatic way that's flawless.
[01:54:59.100 --> 01:55:01.740]   And when everything's working perfectly, it's perfect.
[01:55:01.740 --> 01:55:06.080]   But actually, if you strive for perfection,
[01:55:06.080 --> 01:55:10.100]   it has the same stress, like has a lot of the stress
[01:55:10.100 --> 01:55:12.940]   that you were seeking to escape with the context switching.
[01:55:12.940 --> 01:55:17.940]   Because you're almost stressing about errors.
[01:55:17.940 --> 01:55:20.820]   Like when the process is functioning,
[01:55:20.820 --> 01:55:23.180]   there's always this anxiety of like,
[01:55:23.180 --> 01:55:25.100]   I wonder if it's gonna succeed.
[01:55:25.100 --> 01:55:25.940]   - Yeah.
[01:55:25.940 --> 01:55:26.820]   - I wonder if it's gonna succeed.
[01:55:26.820 --> 01:55:29.060]   - Yeah, no, I think some of that's just you and I probably.
[01:55:29.060 --> 01:55:30.300]   I mean, it's just our mindset, right?
[01:55:30.300 --> 01:55:32.340]   We're in, we do computer science, right?
[01:55:32.340 --> 01:55:34.500]   So chicken and egg, I guess.
[01:55:34.500 --> 01:55:36.500]   And a lot of the processes end up working here
[01:55:36.500 --> 01:55:37.820]   are much rougher.
[01:55:37.820 --> 01:55:39.440]   It's like, okay, instead of letting clients
[01:55:39.440 --> 01:55:43.700]   just email me all the time, we have a weekly call,
[01:55:43.700 --> 01:55:45.420]   and then we send them a breakdown
[01:55:45.420 --> 01:55:47.600]   of everything we committed to, right?
[01:55:47.600 --> 01:55:48.540]   That's a process that works.
[01:55:48.540 --> 01:55:50.100]   Okay, I get asked a lot of questions
[01:55:50.100 --> 01:55:51.940]   'cause I'm the JavaScript guy in the company.
[01:55:51.940 --> 01:55:53.780]   Instead of doing it by email, I have office hours.
[01:55:53.780 --> 01:55:54.860]   This is what Basecamp does.
[01:55:54.860 --> 01:55:55.940]   All right, so you come to my office hours,
[01:55:55.940 --> 01:55:57.140]   that cuts down a lot of back and forth.
[01:55:57.140 --> 01:55:58.300]   All right, we're gonna, instead of emailing
[01:55:58.300 --> 01:56:02.040]   about this project, we'll have a Trello board,
[01:56:02.040 --> 01:56:04.680]   and we'll do a weekly really structured status meeting
[01:56:04.680 --> 01:56:07.020]   real quick, what's going on, who needs what, let's go.
[01:56:07.020 --> 01:56:09.060]   And now everything's on there and on our inboxes,
[01:56:09.060 --> 01:56:10.180]   we don't have to send as many messages.
[01:56:10.180 --> 01:56:12.460]   So like that rough level of granularity,
[01:56:12.460 --> 01:56:14.540]   that gets you most of the way there.
[01:56:14.540 --> 01:56:17.140]   - So the parts that you can't automate
[01:56:17.140 --> 01:56:18.680]   and turn into a process.
[01:56:19.780 --> 01:56:21.780]   So how many parts like that do you think
[01:56:21.780 --> 01:56:24.660]   should remain in a perfect world?
[01:56:24.660 --> 01:56:29.660]   And for those parts where email is still useful,
[01:56:29.660 --> 01:56:32.660]   what do you recommend those emails look like?
[01:56:32.660 --> 01:56:34.420]   How should you write the emails?
[01:56:34.420 --> 01:56:35.820]   When should you send them?
[01:56:35.820 --> 01:56:40.780]   - Yeah, I think email is good for delivering information.
[01:56:40.780 --> 01:56:42.860]   Right, so I think about like a fax machine or something.
[01:56:42.860 --> 01:56:44.340]   You know, it's a really good fax machine.
[01:56:44.340 --> 01:56:46.260]   So if I need to send you something
[01:56:46.260 --> 01:56:47.100]   and you just send you a file,
[01:56:47.100 --> 01:56:49.460]   I need to broadcast a new policy or something,
[01:56:49.460 --> 01:56:51.340]   like email is a great way to do it.
[01:56:51.340 --> 01:56:53.620]   It's bad for collaboration.
[01:56:53.620 --> 01:56:55.660]   So if you're having a conversation,
[01:56:55.660 --> 01:56:57.480]   like we're trying to reach a decision on something,
[01:56:57.480 --> 01:56:58.860]   I'm trying to learn about something,
[01:56:58.860 --> 01:57:01.220]   I'm trying to clarify what something, what this is,
[01:57:01.220 --> 01:57:04.820]   that's more than just like a one answer type question,
[01:57:04.820 --> 01:57:07.460]   then I think that you shouldn't be doing an email.
[01:57:07.460 --> 01:57:08.960]   - But see, here's the thing,
[01:57:08.960 --> 01:57:11.580]   like you and I don't talk often.
[01:57:11.580 --> 01:57:13.820]   And so we have a kind of new interaction.
[01:57:13.820 --> 01:57:17.500]   It's not, so sure, yeah, you have a book coming out,
[01:57:17.500 --> 01:57:19.060]   so there's a process and so on,
[01:57:19.060 --> 01:57:22.020]   but say there, don't you think there's a lot
[01:57:22.020 --> 01:57:24.300]   of novel interactive experiences?
[01:57:24.300 --> 01:57:25.340]   - Yeah, I think it's fine.
[01:57:25.340 --> 01:57:27.780]   - So you could, just for every novel experience,
[01:57:27.780 --> 01:57:29.980]   it's okay to have a little bit of an exchange.
[01:57:29.980 --> 01:57:30.820]   - I think it's fine.
[01:57:30.820 --> 01:57:33.100]   Like I think it's fine if stuff comes in over the transom
[01:57:33.100 --> 01:57:35.980]   or you hear from someone you haven't heard from in a while.
[01:57:35.980 --> 01:57:37.460]   I think all that's fine.
[01:57:37.460 --> 01:57:39.700]   I mean, that email at its best,
[01:57:39.700 --> 01:57:42.460]   where it starts to kill us is where all of our collaboration
[01:57:42.460 --> 01:57:43.460]   is happening with the back and forth.
[01:57:43.460 --> 01:57:45.700]   So when you've moved the bulk of that out of your inbox,
[01:57:45.700 --> 01:57:47.980]   now you're back in that Meg Ryan movie,
[01:57:47.980 --> 01:57:49.940]   like you got mail or it's like, all right,
[01:57:49.940 --> 01:57:51.580]   load this up and you wait for the vote.
[01:57:51.580 --> 01:57:53.100]   I'm like, oh, we got a message.
[01:57:53.100 --> 01:57:54.740]   Yeah, Lex sent me a message.
[01:57:54.740 --> 01:57:55.580]   This is interesting, right?
[01:57:55.580 --> 01:57:56.900]   You're back to the AOL days.
[01:57:56.900 --> 01:58:00.060]   - So you're talking about the bulk of the business world
[01:58:00.060 --> 01:58:04.060]   where like email has replaced the actual communication,
[01:58:04.060 --> 01:58:05.940]   all of the communication protocols required
[01:58:05.940 --> 01:58:06.780]   to accomplish anything.
[01:58:06.780 --> 01:58:08.060]   - Everything is just happening with messages.
[01:58:08.060 --> 01:58:10.980]   So if you now get most stuff done,
[01:58:10.980 --> 01:58:14.060]   repeatable collaborations with other processes
[01:58:14.060 --> 01:58:15.580]   that don't require you to check these inboxes,
[01:58:15.580 --> 01:58:17.860]   then the inbox can serve like an inbox,
[01:58:17.860 --> 01:58:20.860]   which includes hearing from interesting people, right?
[01:58:20.860 --> 01:58:22.660]   Or sending something, hey, I don't know if you saw this,
[01:58:22.660 --> 01:58:23.500]   I thought you might like it.
[01:58:23.500 --> 01:58:24.700]   I think it's great for that.
[01:58:24.700 --> 01:58:27.580]   - So there's probably a bunch of people listening to this.
[01:58:27.580 --> 01:58:31.100]   They're like, yeah, but I work on a team
[01:58:31.100 --> 01:58:33.100]   and all they use is email.
[01:58:33.100 --> 01:58:35.900]   How do you start the revolution from like the ground up?
[01:58:35.900 --> 01:58:39.020]   - Yeah, well, do it, do asymmetric optimization first.
[01:58:39.020 --> 01:58:40.700]   So identify all your processes
[01:58:40.700 --> 01:58:42.340]   and then change what you can change
[01:58:42.340 --> 01:58:44.340]   and be socially very careful about it.
[01:58:44.340 --> 01:58:46.220]   So don't necessarily say like, okay,
[01:58:46.220 --> 01:58:48.260]   this is a new process we all have to do.
[01:58:48.260 --> 01:58:51.820]   You're just, hey, we gotta get this report ready.
[01:58:51.820 --> 01:58:52.700]   Here's what I think we should do.
[01:58:52.700 --> 01:58:54.660]   Like I'll get a draft into our Dropbox folder
[01:58:54.660 --> 01:58:57.900]   by like noon on Monday, grab it.
[01:58:57.900 --> 01:59:00.100]   I won't touch it again until Tuesday morning.
[01:59:00.100 --> 01:59:01.540]   And then I'll look at your changes.
[01:59:01.540 --> 01:59:03.860]   I have this office hours always scheduled Tuesday afternoon.
[01:59:03.860 --> 01:59:05.780]   So if there's anything that catches your attention,
[01:59:05.780 --> 01:59:06.960]   grab me then.
[01:59:06.960 --> 01:59:09.180]   But I've told the designer who CC'd on this
[01:59:09.180 --> 01:59:13.060]   that by COB Tuesday, the final version will be ready
[01:59:13.060 --> 01:59:14.900]   for them to take and polish or whatever.
[01:59:14.900 --> 01:59:16.100]   Like the person on the other end is like, great,
[01:59:16.100 --> 01:59:18.140]   I'm glad, you know, Cal has a plan.
[01:59:18.140 --> 01:59:19.260]   So I just, what do I need to do?
[01:59:19.260 --> 01:59:21.260]   I need to edit this tomorrow, whatever, right?
[01:59:21.260 --> 01:59:22.740]   But you've actually pulled them into a process.
[01:59:22.740 --> 01:59:24.220]   That means we're gonna get this report together
[01:59:24.220 --> 01:59:25.900]   without having to just go back and forth.
[01:59:25.900 --> 01:59:29.220]   So you just asymmetrically optimize these things
[01:59:29.220 --> 01:59:31.340]   and then you can begin the conversation.
[01:59:31.340 --> 01:59:32.940]   And maybe that's where my book comes in place.
[01:59:32.940 --> 01:59:36.300]   You just sort of slide it, slide it across the desk.
[01:59:36.300 --> 01:59:38.020]   - So buy the book and just leave it.
[01:59:38.020 --> 01:59:38.860]   - Leave it at the, yeah.
[01:59:38.860 --> 01:59:40.220]   - Give it to everybody on your team.
[01:59:40.220 --> 01:59:42.940]   Okay, so we solved the bulk of the email problem with this.
[01:59:42.940 --> 01:59:45.500]   Is there a case to be made that even for like communication
[01:59:45.500 --> 01:59:50.500]   between you and I, we should move away from email?
[01:59:50.500 --> 01:59:52.500]   And for example, there's a guy, I recently,
[01:59:52.500 --> 01:59:53.700]   I don't know if you know comedians,
[01:59:53.700 --> 01:59:55.940]   but there's a guy named Joey Diaz
[01:59:55.940 --> 01:59:57.900]   that I've had an interaction with recently.
[01:59:57.900 --> 02:00:00.460]   And that guy, first of all, the sweetest human,
[02:00:00.460 --> 02:00:02.500]   despite what his comedy sounds like,
[02:00:02.500 --> 02:00:04.300]   is the sweetest human being.
[02:00:04.300 --> 02:00:08.620]   And he's a big proponent of just pick up the phone and call.
[02:00:08.620 --> 02:00:10.900]   And it makes me so uncomfortable when people call me.
[02:00:10.900 --> 02:00:14.080]   It's like, I don't know what to do with this thing.
[02:00:14.080 --> 02:00:17.780]   But it kind of gets everything done quicker, I think,
[02:00:17.780 --> 02:00:19.900]   if I remove the anxiety from that.
[02:00:19.900 --> 02:00:21.220]   Is there a case to be made for that?
[02:00:21.220 --> 02:00:24.260]   Or is email could still be the most efficient way
[02:00:24.260 --> 02:00:25.100]   to do this?
[02:00:25.100 --> 02:00:27.620]   - No, I mean, look, if you have to interact with someone,
[02:00:27.620 --> 02:00:30.020]   there's a lot of efficiency in synchrony, right?
[02:00:30.020 --> 02:00:31.780]   And this is something from distributed system theory
[02:00:31.780 --> 02:00:33.420]   where you know if you go from synchronous
[02:00:33.420 --> 02:00:34.740]   to asynchronous networks,
[02:00:34.740 --> 02:00:36.660]   there's a huge amount of overhead to the asynchrony.
[02:00:36.660 --> 02:00:39.060]   So actually the protocols required to solve things
[02:00:39.060 --> 02:00:42.180]   in asynchronous networks are significantly more complicated
[02:00:42.180 --> 02:00:44.060]   and fragile than synchronous protocols.
[02:00:44.060 --> 02:00:47.020]   So if we can just do real time, it's usually better.
[02:00:47.020 --> 02:00:48.620]   And also from an interaction,
[02:00:48.620 --> 02:00:50.020]   like social connection standpoint,
[02:00:50.020 --> 02:00:51.900]   there's a lot more information in the human voice
[02:00:51.900 --> 02:00:53.820]   and the back and forth.
[02:00:53.820 --> 02:00:56.500]   Yeah, if you just call, so very generational, right?
[02:00:56.500 --> 02:00:59.460]   Like our generation will be comfortable talking on the phone
[02:00:59.460 --> 02:01:01.460]   in a way that like a younger generation isn't,
[02:01:01.460 --> 02:01:03.320]   but an older generation is more comfortable with,
[02:01:03.320 --> 02:01:05.140]   well, you just call people.
[02:01:05.140 --> 02:01:07.060]   Whereas we, so there's a happy medium,
[02:01:07.060 --> 02:01:08.780]   but most of my good friends,
[02:01:08.780 --> 02:01:11.060]   we just talk, we have regular phone calls.
[02:01:11.060 --> 02:01:11.900]   - Okay.
[02:01:11.900 --> 02:01:13.060]   - Yeah, it's not, I don't just call them,
[02:01:13.060 --> 02:01:14.220]   we schedule it, we schedule it.
[02:01:14.220 --> 02:01:15.280]   Yeah, just on text, like, yeah,
[02:01:15.280 --> 02:01:17.980]   you want to talk sometime soon.
[02:01:17.980 --> 02:01:20.860]   - Do you ever have a process around friends?
[02:01:20.860 --> 02:01:22.300]   - Not really, no.
[02:01:22.300 --> 02:01:24.420]   - I feel like I should, I feel like-
[02:01:24.420 --> 02:01:26.020]   - When you have like a lot of interesting
[02:01:26.020 --> 02:01:27.580]   friend possibilities,
[02:01:27.580 --> 02:01:29.180]   is you have like an interesting problem, right?
[02:01:29.180 --> 02:01:32.100]   Like really interesting people you can talk to.
[02:01:32.100 --> 02:01:33.380]   - Well, that's one problem.
[02:01:33.380 --> 02:01:34.740]   And the other one is the introversion
[02:01:34.740 --> 02:01:37.660]   where I'm just afraid of people and get really stressed.
[02:01:37.660 --> 02:01:39.520]   Like I freak out and so-
[02:01:39.520 --> 02:01:41.420]   - You picked a good line of work.
[02:01:41.420 --> 02:01:43.980]   - Yeah, now perhaps it's the Goggins thing.
[02:01:43.980 --> 02:01:46.500]   It's like facing your fears or whatever,
[02:01:46.500 --> 02:01:50.440]   but it's almost like there's a,
[02:01:50.440 --> 02:01:52.020]   it has to do with the timetables thing
[02:01:52.020 --> 02:01:54.660]   and the deep work that the nice thing
[02:01:54.660 --> 02:01:59.660]   about the processes is it not only automates,
[02:01:59.660 --> 02:02:03.380]   sort of automates away the context switching,
[02:02:03.380 --> 02:02:05.740]   it ensures you do the important things too.
[02:02:05.740 --> 02:02:06.580]   - Yeah.
[02:02:06.580 --> 02:02:10.140]   - It's like prioritize, so the thing is with email,
[02:02:10.140 --> 02:02:12.500]   because everything is done over email,
[02:02:12.500 --> 02:02:17.260]   you can be lazy in the same way with like social networks
[02:02:17.260 --> 02:02:21.060]   and do the easy things first that are not that important.
[02:02:21.060 --> 02:02:23.260]   So the process also enforces
[02:02:23.260 --> 02:02:24.900]   that you do the important things.
[02:02:24.900 --> 02:02:28.180]   And for me, the important things is like,
[02:02:28.180 --> 02:02:30.380]   okay, this sounds weird, but like social connection.
[02:02:30.380 --> 02:02:33.020]   - No, that's one of the most important things
[02:02:33.020 --> 02:02:34.340]   in all of human existence.
[02:02:34.340 --> 02:02:35.180]   - Yeah.
[02:02:35.180 --> 02:02:37.620]   - And doing it, the paradoxical thing,
[02:02:37.620 --> 02:02:39.580]   I got into this for digital minimalism,
[02:02:40.340 --> 02:02:42.500]   the more you sacrifice on behalf of the connection,
[02:02:42.500 --> 02:02:44.460]   the stronger the connection feels, right?
[02:02:44.460 --> 02:02:47.180]   So sacrificing non-trivial time and attention
[02:02:47.180 --> 02:02:49.100]   on behalf of someone is what tells your brain
[02:02:49.100 --> 02:02:52.260]   that this is a serious relationship,
[02:02:52.260 --> 02:02:54.860]   which is why social media had this paradoxical effect
[02:02:54.860 --> 02:02:57.100]   of making people feel less social
[02:02:57.100 --> 02:02:58.580]   'cause it took the friction out of it.
[02:02:58.580 --> 02:02:59.860]   And so the brain just doesn't like,
[02:02:59.860 --> 02:03:02.380]   yeah, you've been commenting on this person's whatever,
[02:03:02.380 --> 02:03:05.400]   you've been retweeting them or sending them some texts,
[02:03:05.400 --> 02:03:07.500]   you haven't, it's not hard enough.
[02:03:07.500 --> 02:03:09.700]   And then the perceived strength
[02:03:09.700 --> 02:03:11.100]   of that social connection diminishes,
[02:03:11.100 --> 02:03:13.300]   where if you talk to them or go spend time with them
[02:03:13.300 --> 02:03:16.180]   or whatever, you're gonna feel better about it.
[02:03:16.180 --> 02:03:17.740]   So the friction is good.
[02:03:17.740 --> 02:03:18.920]   I have a thing with some of my friends
[02:03:18.920 --> 02:03:20.700]   where at the end of each call,
[02:03:20.700 --> 02:03:23.100]   we take a couple minutes to schedule the next.
[02:03:23.100 --> 02:03:23.940]   Then you never have to,
[02:03:23.940 --> 02:03:25.340]   it's like I do with haircuts or something, right?
[02:03:25.340 --> 02:03:27.340]   Like if I don't schedule it then,
[02:03:27.340 --> 02:03:29.100]   I'm never gonna get my haircut, right?
[02:03:29.100 --> 02:03:32.200]   And so it's like, okay, when do you wanna talk next?
[02:03:32.200 --> 02:03:34.180]   - Yeah, that's a really good idea.
[02:03:34.180 --> 02:03:38.100]   I just don't call friends and like every 10 years,
[02:03:38.100 --> 02:03:39.660]   I do something dramatic for them
[02:03:39.660 --> 02:03:40.980]   so that we maintain the friendship.
[02:03:40.980 --> 02:03:43.460]   Like I'd murder somebody that they really don't like.
[02:03:43.460 --> 02:03:44.300]   - Yeah, exactly.
[02:03:44.300 --> 02:03:45.420]   - I just don't like that. - Careful, man,
[02:03:45.420 --> 02:03:46.780]   Joey might ask you to do that.
[02:03:46.780 --> 02:03:49.300]   - Yeah, that's why, oh, this is one of my favorite things.
[02:03:49.300 --> 02:03:51.580]   - Lex, you need to come down to New Jersey.
[02:03:51.580 --> 02:03:52.420]   - That's exactly what we're gonna do.
[02:03:52.420 --> 02:03:54.140]   - With that robot dog of yours.
[02:03:54.140 --> 02:03:56.740]   - We're gonna go down to Jersey.
[02:03:56.740 --> 02:03:57.780]   There's a special human.
[02:03:57.780 --> 02:04:00.020]   I love the comedian world.
[02:04:00.020 --> 02:04:01.500]   They've been shaking up,
[02:04:01.500 --> 02:04:04.100]   I don't know if you listen to Joe Rogan, all those folks,
[02:04:04.100 --> 02:04:08.100]   they kind of are doing something interesting
[02:04:08.100 --> 02:04:10.460]   for MIT and academia.
[02:04:10.460 --> 02:04:13.060]   They're shaking up this world a little bit,
[02:04:13.060 --> 02:04:15.520]   like podcasting, because comedians are paving the way
[02:04:15.520 --> 02:04:17.060]   for podcasting.
[02:04:17.060 --> 02:04:18.820]   And so you have like Andrew Huberman,
[02:04:18.820 --> 02:04:21.620]   who's a neuroscientist at Stanford, a friend of mine now.
[02:04:21.620 --> 02:04:25.260]   He's like into podcasting now,
[02:04:25.260 --> 02:04:27.100]   and you're into podcasting.
[02:04:27.100 --> 02:04:29.180]   Of course, you're not necessarily podcasting
[02:04:29.180 --> 02:04:30.960]   about computer science currently, right?
[02:04:30.960 --> 02:04:35.760]   But that, it feels like you could have a lot
[02:04:35.760 --> 02:04:40.520]   of the free spirit of the comedians implemented
[02:04:40.520 --> 02:04:43.920]   by the people who are academically trained.
[02:04:43.920 --> 02:04:46.760]   - Who actually have a niche specialty.
[02:04:46.760 --> 02:04:49.280]   - Yeah, and then that results, I mean,
[02:04:49.280 --> 02:04:51.340]   who knows what the experiment looks like,
[02:04:51.340 --> 02:04:54.200]   but that results me being able to talk about robotics
[02:04:54.200 --> 02:04:56.960]   with Joey Diaz when he says, you know,
[02:04:56.960 --> 02:04:58.680]   drops F-bombs every other sentence.
[02:04:58.680 --> 02:05:02.720]   And I, the world is, like, I've seen actually a shift
[02:05:02.720 --> 02:05:06.240]   within colleagues and friends within MIT
[02:05:06.240 --> 02:05:08.520]   where they're becoming much more accepting
[02:05:08.520 --> 02:05:09.340]   of that kind of thing.
[02:05:09.340 --> 02:05:10.480]   It's very interesting.
[02:05:10.480 --> 02:05:11.320]   - That's interesting.
[02:05:11.320 --> 02:05:12.720]   So you're seeing, okay.
[02:05:12.720 --> 02:05:14.520]   - Because they're seeing how popular it is.
[02:05:14.520 --> 02:05:15.360]   They're like--
[02:05:15.360 --> 02:05:16.180]   - Well, you're really popular.
[02:05:16.180 --> 02:05:17.360]   I don't know how they think about it
[02:05:17.360 --> 02:05:18.880]   at Georgetown, for example.
[02:05:18.880 --> 02:05:19.720]   - I don't know.
[02:05:19.720 --> 02:05:22.080]   It's interesting, but I think what happens
[02:05:22.080 --> 02:05:25.160]   is the popularity of it combined
[02:05:25.160 --> 02:05:28.400]   with just good conversations with people.
[02:05:28.400 --> 02:05:32.720]   They respect, it's like, oh, okay, wait, this is the thing.
[02:05:32.720 --> 02:05:33.560]   - Yeah.
[02:05:33.560 --> 02:05:34.760]   - And this is more fun to listen to
[02:05:34.760 --> 02:05:39.160]   than a shitty Zoom lecture about their work.
[02:05:39.160 --> 02:05:40.000]   - Yeah.
[02:05:40.000 --> 02:05:40.820]   - It's like, there's something here.
[02:05:40.820 --> 02:05:41.660]   - There's something interesting.
[02:05:41.660 --> 02:05:44.000]   - And we don't, nobody actually knows what that is.
[02:05:44.000 --> 02:05:46.440]   Just like with Clubhouse or something,
[02:05:46.440 --> 02:05:49.080]   nobody's figured out, like, where does this medium take?
[02:05:49.080 --> 02:05:51.520]   Is this a legitimate medium of education?
[02:05:51.520 --> 02:05:52.360]   - Yeah.
[02:05:52.360 --> 02:05:54.360]   - Or is this just like a fun--
[02:05:54.360 --> 02:05:55.560]   - Well, that's your innovation, I think,
[02:05:55.560 --> 02:05:58.040]   was we can bring on professors.
[02:05:58.040 --> 02:05:58.880]   - Yeah.
[02:05:58.880 --> 02:06:00.600]   - And I know Joe Rogan did some of that too,
[02:06:00.600 --> 02:06:04.880]   but your professors in your field.
[02:06:04.880 --> 02:06:05.720]   - Yeah, exactly.
[02:06:05.720 --> 02:06:08.280]   - You bring on all these MIT guys who I remember.
[02:06:08.280 --> 02:06:10.120]   - Well, that's been the big challenge for me is,
[02:06:10.120 --> 02:06:13.400]   I don't know, is I feel,
[02:06:13.400 --> 02:06:17.680]   I would ask big philosophical questions
[02:06:17.680 --> 02:06:22.680]   of people like yourself that are really well,
[02:06:22.680 --> 02:06:25.920]   like, so for example, you have a lot of excellent papers
[02:06:25.920 --> 02:06:30.920]   on, you know, that has a lot of theory in it, right?
[02:06:30.920 --> 02:06:35.440]   And there is some temptation to just go through papers.
[02:06:35.440 --> 02:06:37.080]   And I think it's possible to actually do that.
[02:06:37.080 --> 02:06:39.440]   I haven't done that much, but I think it's possible.
[02:06:39.440 --> 02:06:41.720]   It just requires a lot of preparation.
[02:06:41.720 --> 02:06:43.920]   And I can probably only do that with things
[02:06:43.920 --> 02:06:48.080]   that I'm actually, like, in the field I'm aware of.
[02:06:48.080 --> 02:06:51.120]   But there's a dance that I would love to be able
[02:06:51.120 --> 02:06:53.320]   to try to hit right, where it's actually getting
[02:06:53.320 --> 02:06:55.000]   to the core of some interesting ideas,
[02:06:55.000 --> 02:06:56.960]   as opposed to just talking about philosophy.
[02:06:56.960 --> 02:06:59.840]   At the same time, there's a large audience of people
[02:06:59.840 --> 02:07:02.920]   that just wanna be inspired by, like,
[02:07:02.920 --> 02:07:05.960]   by disciplines where they don't necessarily
[02:07:05.960 --> 02:07:07.240]   know the details.
[02:07:07.240 --> 02:07:08.560]   But there's a lot of people that are like,
[02:07:08.560 --> 02:07:10.640]   hmm, I'm really curious.
[02:07:10.640 --> 02:07:13.120]   I've been thinking about pivoting careers
[02:07:13.120 --> 02:07:14.720]   into software engineering.
[02:07:14.720 --> 02:07:16.680]   They would love to hear from people like you
[02:07:16.680 --> 02:07:19.320]   about computer science, even if it's like theory.
[02:07:19.320 --> 02:07:22.200]   - Yeah, but just like the idea that you can have big ideas,
[02:07:22.200 --> 02:07:24.120]   you push them through and it's interesting,
[02:07:24.120 --> 02:07:25.200]   you fight for it, yeah.
[02:07:25.200 --> 02:07:27.720]   - Well, there's some, there's, what is it,
[02:07:27.720 --> 02:07:32.720]   Computerphile and Numberphile, these YouTube channels.
[02:07:32.720 --> 02:07:37.240]   There's channels I watch on chess, exceptionally popular,
[02:07:37.240 --> 02:07:41.480]   where I don't understand maybe 80% of the time
[02:07:41.480 --> 02:07:42.720]   what the hell they're talking about,
[02:07:42.720 --> 02:07:44.600]   'cause they're talking about why this move
[02:07:44.600 --> 02:07:45.560]   is better than this move.
[02:07:45.560 --> 02:07:48.480]   But I love the passion and the genius of those people
[02:07:48.480 --> 02:07:50.200]   and just overhearing it.
[02:07:50.200 --> 02:07:52.080]   I don't know why that's so exciting.
[02:07:52.080 --> 02:07:54.040]   - Do you look at Scott Aronson's blog at all?
[02:07:54.040 --> 02:07:55.200]   The Shuttle to Optimize.
[02:07:55.200 --> 02:07:57.880]   Yeah, it's like hardcore complexity theory,
[02:07:57.880 --> 02:08:01.200]   but it's just an enthusiasm or like Terry Tao's blog.
[02:08:01.200 --> 02:08:02.680]   - A little bit of humor.
[02:08:02.680 --> 02:08:03.920]   Terry Tao has a blog?
[02:08:03.920 --> 02:08:04.840]   - He used to, yeah.
[02:08:04.840 --> 02:08:08.720]   And it would just be, I'm going all in on,
[02:08:08.720 --> 02:08:10.280]   you know, here's the new affine group
[02:08:10.280 --> 02:08:11.680]   with which you can do whatever.
[02:08:11.680 --> 02:08:13.040]   I mean, it was just equations.
[02:08:13.040 --> 02:08:15.360]   - Well, in the case of Scott Aronson, he's good.
[02:08:15.360 --> 02:08:19.360]   He's able to turn on like the inner troll
[02:08:19.360 --> 02:08:20.600]   and comedian and so on.
[02:08:20.600 --> 02:08:22.960]   He keeps the fun, which is the best of kinds of fun.
[02:08:22.960 --> 02:08:24.160]   - He's a philosophical guy.
[02:08:24.160 --> 02:08:25.000]   He wrote that book.
[02:08:25.000 --> 02:08:27.120]   - Yeah, he turns on the philosophy.
[02:08:27.120 --> 02:08:30.400]   Yeah, so, you know, we're exploring these different ways
[02:08:30.400 --> 02:08:33.480]   of communicating science and exciting the world.
[02:08:33.480 --> 02:08:36.160]   Speaking of which, I gotta ask you about computer science.
[02:08:36.160 --> 02:08:37.160]   (laughs)
[02:08:37.160 --> 02:08:39.320]   - That's right, I do some of that.
[02:08:39.320 --> 02:08:43.600]   - So, I mean, a lot of your work is what inspired
[02:08:43.600 --> 02:08:46.480]   this deep thinking about productivity
[02:08:46.480 --> 02:08:48.800]   from all the different angles,
[02:08:48.800 --> 02:08:52.080]   because some of the most rigorous work is mathematical work.
[02:08:52.080 --> 02:08:55.040]   And in computer science, the theoretical computer science.
[02:08:55.040 --> 02:08:57.440]   Let me ask the Scott Aronson question of like,
[02:08:57.440 --> 02:09:00.760]   is there something to you that stands out in particular
[02:09:00.760 --> 02:09:03.440]   that's beautiful or inspiring
[02:09:03.440 --> 02:09:05.920]   or just really insightful about computer science
[02:09:05.920 --> 02:09:08.400]   or maybe mathematics?
[02:09:08.400 --> 02:09:11.240]   - I mean, I like theory.
[02:09:11.240 --> 02:09:13.120]   And in particular, what I've always liked in theory
[02:09:13.120 --> 02:09:14.840]   is the notion of impossibilities.
[02:09:14.840 --> 02:09:16.600]   That's kind of my specialty.
[02:09:16.600 --> 02:09:19.880]   So, within the context of distributed algorithms,
[02:09:19.880 --> 02:09:21.680]   my specialty is impossibility results.
[02:09:21.680 --> 02:09:26.040]   The idea that you can argue nothing exists that solves this
[02:09:26.040 --> 02:09:30.520]   or nothing exists that can solve this faster than this.
[02:09:30.520 --> 02:09:32.000]   And I think that's really interesting.
[02:09:32.000 --> 02:09:34.640]   And that goes all the way back to Turing.
[02:09:34.640 --> 02:09:37.760]   His original paper on computable numbers
[02:09:37.760 --> 02:09:38.760]   with their connection to the,
[02:09:38.760 --> 02:09:40.240]   it's in German, the Eichler-Tung problem,
[02:09:40.240 --> 02:09:41.840]   but basically the German name
[02:09:41.840 --> 02:09:43.400]   that Hilbert called the decision problem.
[02:09:43.400 --> 02:09:46.280]   This was pre-computers, but he's English,
[02:09:46.280 --> 02:09:48.760]   so it's written in English, so it's a very accessible paper.
[02:09:48.760 --> 02:09:50.480]   And it lays the foundation
[02:09:50.480 --> 02:09:51.880]   for all of theoretical computer science.
[02:09:51.880 --> 02:09:53.080]   He just has this insight.
[02:09:53.080 --> 02:09:55.120]   He's like, well, if we think about an algorithm,
[02:09:55.120 --> 02:09:57.680]   I mean, he figures out all effective procedures
[02:09:57.680 --> 02:09:59.560]   or Turing machines are basically algorithms.
[02:09:59.560 --> 02:10:01.960]   We could really describe a Turing machine with a number,
[02:10:01.960 --> 02:10:04.120]   which we can now imagine with computer code,
[02:10:04.120 --> 02:10:05.560]   you could just take a source file
[02:10:05.560 --> 02:10:07.640]   and just treat the binary version of the file
[02:10:07.640 --> 02:10:09.400]   as a really long number, right?
[02:10:09.400 --> 02:10:12.400]   But he's like, every program is just a finite number.
[02:10:12.400 --> 02:10:14.000]   It's a natural number.
[02:10:14.000 --> 02:10:16.520]   And then he realized one way to think about a problem
[02:10:16.520 --> 02:10:19.680]   is you have, and this is kind of the Mike Sipser approach,
[02:10:19.680 --> 02:10:21.760]   but you have a sort of, it's a language.
[02:10:21.760 --> 02:10:23.440]   So an infinite number of strings,
[02:10:23.440 --> 02:10:25.040]   some of them are in the language and some of them aren't,
[02:10:25.040 --> 02:10:26.720]   but basically you can imagine a problem
[02:10:26.720 --> 02:10:29.320]   is represented as an infinite binary string,
[02:10:29.320 --> 02:10:31.040]   where in every position, like a one means
[02:10:31.040 --> 02:10:33.600]   that string is in the language and a zero means it isn't.
[02:10:33.600 --> 02:10:37.480]   And then he applied Cantor from the 19th century and said,
[02:10:37.480 --> 02:10:39.720]   okay, the natural numbers are countable,
[02:10:39.720 --> 02:10:43.720]   so it's countably infinite, and infinite binary strings,
[02:10:43.720 --> 02:10:45.160]   you can use a diagonalization argument
[02:10:45.160 --> 02:10:47.920]   and show they're uncountable.
[02:10:47.920 --> 02:10:50.800]   So there's just vastly more problems
[02:10:50.800 --> 02:10:51.920]   than there are algorithms.
[02:10:51.920 --> 02:10:53.440]   So basically anything you can come up with
[02:10:53.440 --> 02:10:54.480]   for the most part, almost certainly
[02:10:54.480 --> 02:10:56.320]   is not solvable by a computer.
[02:10:56.320 --> 02:10:57.400]   You know, and then he was like,
[02:10:57.400 --> 02:10:58.760]   let me give a particular example,
[02:10:58.760 --> 02:11:00.760]   and he figured out the very first computability proof.
[02:11:00.760 --> 02:11:02.000]   And he said, let's just walk through
[02:11:02.000 --> 02:11:03.720]   with a little bit of simple logic.
[02:11:03.720 --> 02:11:06.040]   The halting problem can't be solved by an algorithm.
[02:11:06.040 --> 02:11:08.840]   And that kicked off the whole enterprise
[02:11:08.840 --> 02:11:12.800]   of some things can't be solved by algorithms,
[02:11:12.800 --> 02:11:14.240]   some things can't be solved by computers.
[02:11:14.240 --> 02:11:16.240]   And we've just been doing theory on that
[02:11:16.240 --> 02:11:18.600]   since that was the '30s he wrote that.
[02:11:18.600 --> 02:11:21.120]   - So proving that something is impossible
[02:11:21.120 --> 02:11:23.400]   is sort of a more, a stricter version of that.
[02:11:23.400 --> 02:11:26.280]   Is it like proving bounds on the performance
[02:11:26.280 --> 02:11:27.200]   of different algorithms?
[02:11:27.200 --> 02:11:28.040]   - Yeah, so those are, yeah,
[02:11:28.040 --> 02:11:29.360]   so bounds are upper bounds, right?
[02:11:29.360 --> 02:11:32.920]   So you say this algorithm does at least this well
[02:11:32.920 --> 02:11:33.760]   and no worse than this,
[02:11:33.760 --> 02:11:35.560]   but you're looking at a particular algorithm.
[02:11:35.560 --> 02:11:39.200]   And possibility proofs say no algorithm ever
[02:11:39.200 --> 02:11:40.120]   could ever solve this problem.
[02:11:40.120 --> 02:11:42.480]   So no algorithm could ever solve the halting problem.
[02:11:42.480 --> 02:11:43.880]   - So it's problem-centric.
[02:11:43.880 --> 02:11:46.120]   It's making something different,
[02:11:46.120 --> 02:11:48.520]   making a conclusive statement about the problem.
[02:11:48.520 --> 02:11:49.360]   - Yes.
[02:11:49.360 --> 02:11:51.800]   - And that's somehow satisfying 'cause it's--
[02:11:51.800 --> 02:11:53.240]   - It's philosophically interesting.
[02:11:53.240 --> 02:11:54.080]   - Yeah.
[02:11:54.080 --> 02:11:54.920]   - I mean, it all goes back to,
[02:11:54.920 --> 02:11:58.360]   you get back to Plato, it's all reducto ad absurdum.
[02:11:58.360 --> 02:11:59.680]   So all these arguments have to start.
[02:11:59.680 --> 02:12:00.520]   The only way to do it is
[02:12:00.520 --> 02:12:01.960]   because there's an infinite number of solutions,
[02:12:01.960 --> 02:12:02.800]   you can't go through them.
[02:12:02.800 --> 02:12:06.320]   You say, let's assume for the sake of contradiction
[02:12:06.320 --> 02:12:09.040]   that there existed something that solves this problem.
[02:12:09.040 --> 02:12:10.280]   And then you turn to crank a logic
[02:12:10.280 --> 02:12:11.720]   until you blow up the universe.
[02:12:11.720 --> 02:12:12.560]   And then you go back and say,
[02:12:12.560 --> 02:12:13.920]   okay, our original assumption
[02:12:13.920 --> 02:12:16.480]   that this solution exists can't be true.
[02:12:16.480 --> 02:12:17.680]   I just think philosophically,
[02:12:17.680 --> 02:12:19.760]   it's like a really exciting kind of beautiful thing.
[02:12:19.760 --> 02:12:22.080]   It's what I specialize in within distributed algorithms
[02:12:22.080 --> 02:12:24.840]   is more like time-bound impossibility results.
[02:12:24.840 --> 02:12:28.680]   Like no algorithm can solve this problem faster than this
[02:12:28.680 --> 02:12:29.680]   in this setting.
[02:12:29.680 --> 02:12:32.080]   Of all the infinite number of ways you might ever do it.
[02:12:32.080 --> 02:12:34.080]   - So you have many papers,
[02:12:34.080 --> 02:12:35.440]   but the one that caught my eye
[02:12:35.440 --> 02:12:38.400]   is "Smooth Analysis of Dynamic Networks,"
[02:12:38.400 --> 02:12:40.640]   in which you write,
[02:12:40.640 --> 02:12:42.720]   "A problem with the worst-case perspective
[02:12:42.720 --> 02:12:45.540]   "is that it often leads to extremely strong lower bounds.
[02:12:45.540 --> 02:12:48.000]   "These strong results motivate a key question.
[02:12:48.000 --> 02:12:49.800]   "Is this bound robust in the sense
[02:12:49.800 --> 02:12:51.940]   "that it captures the fundamental difficulty
[02:12:51.940 --> 02:12:53.840]   "introduced by dynamism?
[02:12:53.840 --> 02:12:56.480]   "Or is the bound fragile in the sense
[02:12:56.480 --> 02:12:58.200]   "that the poor performance it describes
[02:12:58.200 --> 02:13:01.980]   "depends on an exact sequence of adversarial changes?
[02:13:01.980 --> 02:13:05.120]   "Fragile lower bounds leave open the possibility
[02:13:05.120 --> 02:13:08.680]   "of algorithms that might still perform well in practice."
[02:13:08.680 --> 02:13:11.760]   That's in the sense of the impossible
[02:13:11.760 --> 02:13:15.280]   and the bounds discussion presents the interesting question.
[02:13:15.280 --> 02:13:18.440]   I just like the idea of robust and fragile bounds,
[02:13:18.440 --> 02:13:23.000]   but what do you make about this kind of tension
[02:13:23.000 --> 02:13:25.760]   between what's provably,
[02:13:25.760 --> 02:13:30.000]   like what bounds you can prove that are like robust
[02:13:30.000 --> 02:13:32.520]   and something that's a bit more fragile?
[02:13:32.520 --> 02:13:36.040]   And also by way of answering that
[02:13:36.040 --> 02:13:38.120]   for this particular paper,
[02:13:38.120 --> 02:13:41.400]   can you say what the hell are dynamic networks?
[02:13:41.400 --> 02:13:42.240]   - What are distributed algorithms?
[02:13:42.240 --> 02:13:43.680]   - You don't know this, come on now.
[02:13:43.680 --> 02:13:44.880]   - And I have no idea.
[02:13:44.880 --> 02:13:46.160]   And what is smooth analysis?
[02:13:46.160 --> 02:13:47.000]   - Yeah, well, okay.
[02:13:47.000 --> 02:13:49.960]   So smooth analysis, it's, so it wasn't my idea.
[02:13:49.960 --> 02:13:52.400]   So Spielman and Tang came up with this
[02:13:52.400 --> 02:13:54.360]   in the context of sequential algorithms.
[02:13:54.360 --> 02:13:57.040]   So just like the normal world of an algorithm
[02:13:57.040 --> 02:13:58.460]   that runs on a computer.
[02:13:58.460 --> 02:13:59.920]   And they were looking at,
[02:13:59.920 --> 02:14:02.760]   there's a well-known algorithm called the simplex algorithm,
[02:14:02.760 --> 02:14:04.920]   but basically you're trying to, whatever,
[02:14:04.920 --> 02:14:07.480]   find a hole around a group of points.
[02:14:07.480 --> 02:14:08.320]   And there was an algorithm
[02:14:08.320 --> 02:14:10.480]   that worked really well in practice.
[02:14:10.480 --> 02:14:12.040]   But when you analyze it, you would say,
[02:14:12.040 --> 02:14:13.960]   I can't guarantee it's gonna work well in practice
[02:14:13.960 --> 02:14:16.400]   because if you have just the right inputs,
[02:14:16.400 --> 02:14:18.960]   this thing could run really long, right?
[02:14:18.960 --> 02:14:20.480]   But in practice, it seemed to be really fast.
[02:14:20.480 --> 02:14:22.580]   So smooth analysis is they came in and they said,
[02:14:22.580 --> 02:14:26.000]   let's assume that a bad guy chooses the inputs.
[02:14:26.000 --> 02:14:27.880]   It could be anything like really bad ones.
[02:14:27.880 --> 02:14:30.880]   And all we're gonna do, because in simplex, they're numbers.
[02:14:30.880 --> 02:14:33.520]   We're gonna just randomly put a little bit of noise
[02:14:33.520 --> 02:14:34.640]   on each of the numbers.
[02:14:34.640 --> 02:14:36.080]   And they showed if you put a little bit of noise
[02:14:36.080 --> 02:14:39.720]   on the numbers, suddenly simplex algorithm goes really fast.
[02:14:39.720 --> 02:14:41.840]   Like, oh, that explains this lower bound,
[02:14:41.840 --> 02:14:44.280]   this idea that it could sometimes run really long
[02:14:44.280 --> 02:14:46.280]   was a fragile bound because it could only run
[02:14:46.280 --> 02:14:47.840]   a really long time if you had exactly
[02:14:47.840 --> 02:14:49.800]   the worst pathological input.
[02:14:49.800 --> 02:14:51.560]   So then my collaborators and I brought this over
[02:14:51.560 --> 02:14:53.580]   to the world of distributed algorithms.
[02:14:53.580 --> 02:14:55.360]   We brought them over the general lower bounds, right?
[02:14:55.360 --> 02:14:58.160]   So in the world of dynamic networks,
[02:14:58.160 --> 02:15:00.080]   so distributed algorithm is a bunch of algorithms
[02:15:00.080 --> 02:15:02.440]   on different machines talking to each other,
[02:15:02.440 --> 02:15:03.280]   trying to solve a problem.
[02:15:03.280 --> 02:15:05.000]   And sometimes they're in a network.
[02:15:05.000 --> 02:15:07.460]   So you imagine them connected with network links.
[02:15:07.460 --> 02:15:09.980]   And a dynamic network, those can change, right?
[02:15:09.980 --> 02:15:12.300]   So I was talking to you, but now I can't talk to you anymore
[02:15:12.300 --> 02:15:14.020]   and I'm connected to a person over here.
[02:15:14.020 --> 02:15:16.420]   It's a really hard environment, mathematically speaking.
[02:15:16.420 --> 02:15:19.060]   And there's a lot of really strong lower bounds,
[02:15:19.060 --> 02:15:20.700]   which you could imagine if the network can change
[02:15:20.700 --> 02:15:23.520]   all the time and a bad guy is doing it,
[02:15:23.520 --> 02:15:24.960]   it's like hard to do things well.
[02:15:24.960 --> 02:15:26.460]   - So there's an algorithm running
[02:15:26.460 --> 02:15:27.820]   on every single node in the network.
[02:15:27.820 --> 02:15:28.660]   - Yeah.
[02:15:28.660 --> 02:15:30.780]   - And then you're trying to say something of any kind
[02:15:30.780 --> 02:15:32.820]   that makes any kind of definitive sense
[02:15:32.820 --> 02:15:34.780]   about the performance of that algorithm.
[02:15:34.780 --> 02:15:37.300]   - Yeah, so like, so I just submitted a new paper
[02:15:37.300 --> 02:15:38.340]   on this a couple of weeks ago,
[02:15:38.340 --> 02:15:39.860]   and we were looking at a very simple problem.
[02:15:39.860 --> 02:15:42.340]   There's some messages in the network.
[02:15:42.340 --> 02:15:44.220]   We want everyone to get them.
[02:15:44.220 --> 02:15:47.900]   If the network doesn't change, you can do this pretty well.
[02:15:47.900 --> 02:15:48.740]   You can pipeline them.
[02:15:48.740 --> 02:15:50.260]   There's some algorithms that work,
[02:15:50.260 --> 02:15:52.180]   basic algorithms that work really well.
[02:15:52.180 --> 02:15:54.140]   If the network can change every round,
[02:15:54.140 --> 02:15:57.100]   there's these lower bounds that says,
[02:15:57.100 --> 02:15:58.060]   it takes a really long time.
[02:15:58.060 --> 02:15:58.900]   There's a way that like,
[02:15:58.900 --> 02:16:00.200]   no matter what algorithm you come up with,
[02:16:00.200 --> 02:16:02.140]   there's a way the network can change in such a way
[02:16:02.140 --> 02:16:05.640]   that just really slows down your progress basically, right?
[02:16:05.640 --> 02:16:07.100]   So smooth analysis there says,
[02:16:07.100 --> 02:16:09.060]   yeah, but that seems like a really,
[02:16:09.060 --> 02:16:10.180]   you'd have really bad luck
[02:16:10.180 --> 02:16:13.820]   if your network was changing like exactly
[02:16:13.820 --> 02:16:15.940]   in the right way that you needed to screw your algorithm.
[02:16:15.940 --> 02:16:19.180]   So we said, what if we randomly just add
[02:16:19.180 --> 02:16:20.620]   or remove a couple edges in every round?
[02:16:20.620 --> 02:16:21.700]   So the adversary is trying to choose
[02:16:21.700 --> 02:16:22.660]   the worst possible network.
[02:16:22.660 --> 02:16:24.780]   We're just tweaking it a little bit.
[02:16:24.780 --> 02:16:25.980]   And in that case, this is a new paper.
[02:16:25.980 --> 02:16:27.500]   I mean, it's a blinded submission,
[02:16:27.500 --> 02:16:30.340]   so maybe I shouldn't, it's not, whatever.
[02:16:30.340 --> 02:16:31.660]   We basically showed-
[02:16:31.660 --> 02:16:33.620]   - An anonymous friend of yours submitted a paper.
[02:16:33.620 --> 02:16:35.700]   - Anonymous friend of mine, yeah, yeah.
[02:16:35.700 --> 02:16:37.780]   Whose paper should be accepted.
[02:16:37.780 --> 02:16:40.780]   Showed that even just adding like one random edge per round,
[02:16:40.780 --> 02:16:43.260]   and here's the cool thing about it,
[02:16:43.260 --> 02:16:46.020]   the simplest possible solution to this problem
[02:16:46.020 --> 02:16:47.860]   blows away that lower bound and does really well.
[02:16:47.860 --> 02:16:50.100]   So that's like a very fragile lower bound
[02:16:50.100 --> 02:16:53.100]   because we're like, it's almost impossible
[02:16:53.100 --> 02:16:55.340]   to actually keep things slow.
[02:16:55.340 --> 02:16:59.900]   - I wonder how many lower bounds you can smash open
[02:16:59.900 --> 02:17:02.420]   with this kind of analysis and show that they're fragile.
[02:17:02.420 --> 02:17:03.620]   - It's my interest, yeah.
[02:17:03.620 --> 02:17:05.660]   Because in distributed algorithms,
[02:17:05.660 --> 02:17:08.300]   there's a ton of really famous strong lower bounds,
[02:17:08.300 --> 02:17:12.500]   but things have to go wrong, really, really wrong
[02:17:12.500 --> 02:17:14.580]   for these lower bound arguments to work.
[02:17:14.580 --> 02:17:15.660]   And so I like this approach.
[02:17:15.660 --> 02:17:17.940]   So this whole notion of fragile versus robust,
[02:17:17.940 --> 02:17:19.020]   I was like, well, let's go in
[02:17:19.020 --> 02:17:21.100]   and just throw a little noise in there.
[02:17:21.100 --> 02:17:22.980]   And if it becomes solvable,
[02:17:22.980 --> 02:17:24.540]   then maybe that lower bound wasn't really something
[02:17:24.540 --> 02:17:25.540]   we should worry about.
[02:17:25.540 --> 02:17:27.340]   - You know, that's gonna embarrass,
[02:17:27.340 --> 02:17:28.580]   that's really uncomfortable.
[02:17:28.580 --> 02:17:31.500]   That's really embarrassing to a lot of people.
[02:17:31.500 --> 02:17:35.900]   'Cause okay, this is the OCD thing with the spaces
[02:17:35.900 --> 02:17:39.060]   is it feels really good when you can prove a nice bound.
[02:17:39.060 --> 02:17:42.680]   And if you say that that bound is fragile,
[02:17:42.680 --> 02:17:46.100]   that's like, there's gonna be a sad kid
[02:17:46.100 --> 02:17:49.740]   that walks with their lunchbox back home,
[02:17:49.740 --> 02:17:52.340]   like, "Well, my lower bound doesn't matter."
[02:17:52.340 --> 02:17:53.260]   - No, I don't think they care.
[02:17:53.260 --> 02:17:55.380]   It's all, I don't know, it feels like to me
[02:17:55.380 --> 02:17:57.460]   a lot of this theory is just math machismo.
[02:17:57.460 --> 02:18:00.460]   It's like, whatever, this was a hard bound to prove.
[02:18:00.460 --> 02:18:02.260]   - What do you think about that?
[02:18:02.260 --> 02:18:03.740]   So if you show that something is fragile,
[02:18:03.740 --> 02:18:04.700]   that's more important,
[02:18:04.700 --> 02:18:07.200]   that's really important in practice, right?
[02:18:07.200 --> 02:18:10.740]   So do you think kind of theoretical computer science
[02:18:10.740 --> 02:18:13.340]   is living in its own world, just like mathematics,
[02:18:13.340 --> 02:18:16.300]   and their main effort, which I think is very valuable,
[02:18:16.300 --> 02:18:19.340]   is to develop ideas that's not necessarily interesting,
[02:18:19.340 --> 02:18:21.060]   whether it's applicable in the real world?
[02:18:21.060 --> 02:18:23.220]   - Yeah, we don't care about the applicability.
[02:18:23.220 --> 02:18:24.620]   We kind of do, but not really.
[02:18:24.620 --> 02:18:26.060]   And we're terrible with computers.
[02:18:26.060 --> 02:18:27.740]   You can't do anything useful with computers
[02:18:27.740 --> 02:18:28.700]   and we don't know how to code.
[02:18:28.700 --> 02:18:31.860]   And we're not productive members
[02:18:31.860 --> 02:18:33.300]   of like technological society,
[02:18:33.300 --> 02:18:36.260]   but I do think things percolate.
[02:18:36.260 --> 02:18:37.100]   - Exactly.
[02:18:37.100 --> 02:18:38.660]   - You percolate from the world of theory
[02:18:38.660 --> 02:18:40.100]   into the world of algorithm design,
[02:18:40.100 --> 02:18:41.260]   where we'll pull on the theory
[02:18:41.260 --> 02:18:42.980]   and now suddenly it's useful.
[02:18:42.980 --> 02:18:44.840]   And then the algorithm design gets pulled
[02:18:44.840 --> 02:18:46.020]   into the world of practice where they say,
[02:18:46.020 --> 02:18:47.700]   "Well, actually we can make this algorithm a lot better
[02:18:47.700 --> 02:18:50.020]   because in practice, really these servers do XYZ
[02:18:50.020 --> 02:18:51.620]   and now we can make this super efficient."
[02:18:51.620 --> 02:18:53.580]   And so I do think, I mean, I tell my,
[02:18:53.580 --> 02:18:56.500]   I teach theory to the PhD students at Georgetown.
[02:18:56.500 --> 02:18:58.820]   I show them the sort of funnel of like,
[02:18:58.820 --> 02:19:00.060]   "Okay, we're over here doing theory,"
[02:19:00.060 --> 02:19:02.500]   but it eventually some of this stuff will percolate down
[02:19:02.500 --> 02:19:05.540]   in effect at the very end, a phone,
[02:19:05.540 --> 02:19:08.060]   but it's a long tunnel.
[02:19:08.060 --> 02:19:10.020]   - But the very question you're asking
[02:19:10.020 --> 02:19:12.380]   at the highest philosophical level is fascinating.
[02:19:12.380 --> 02:19:14.980]   Like if you take a system, a distributed system
[02:19:14.980 --> 02:19:19.520]   or a network and introduce a little bit of noise into it,
[02:19:19.520 --> 02:19:23.340]   like how many problems of that nature
[02:19:23.340 --> 02:19:25.220]   are fundamentally changed
[02:19:25.220 --> 02:19:27.580]   by that little introduction of noise?
[02:19:27.580 --> 02:19:29.060]   - Yeah, because it's all,
[02:19:29.060 --> 02:19:30.260]   especially in distributed algorithms,
[02:19:30.260 --> 02:19:31.540]   the model is everything.
[02:19:31.540 --> 02:19:33.820]   Like the way we work is we're incredibly precise
[02:19:33.820 --> 02:19:36.020]   about here's exactly, it's mathematical.
[02:19:36.020 --> 02:19:37.540]   Here's exactly how the network works
[02:19:37.540 --> 02:19:40.140]   and it's a state machine, algorithms are state machines.
[02:19:40.140 --> 02:19:41.320]   There's rounds and schedulers.
[02:19:41.320 --> 02:19:44.160]   We're super precise, we can prove lower bounds.
[02:19:44.160 --> 02:19:45.060]   But yeah, often those lower,
[02:19:45.060 --> 02:19:48.780]   those impossibility results really get at the hard edges
[02:19:48.780 --> 02:19:50.540]   of exactly how that model works.
[02:19:50.540 --> 02:19:53.500]   So we'll see if this, so we published a paper on this,
[02:19:53.500 --> 02:19:55.480]   that paper you mentioned,
[02:19:55.480 --> 02:19:56.740]   that kind of introduced the idea
[02:19:56.740 --> 02:19:58.060]   to the distributed algorithms world.
[02:19:58.060 --> 02:20:01.060]   And I think that's got some traction
[02:20:01.060 --> 02:20:02.180]   and there's been some follow-ups.
[02:20:02.180 --> 02:20:05.660]   So we've just submitted our next.
[02:20:05.660 --> 02:20:06.900]   I mean, honestly, the issue with the next
[02:20:06.900 --> 02:20:09.340]   is that like the result fell out so easily,
[02:20:09.340 --> 02:20:11.100]   and this shows the mathematical machismo problem
[02:20:11.100 --> 02:20:13.940]   in these fields, is there's a good chance
[02:20:13.940 --> 02:20:15.020]   the paper won't be accepted
[02:20:15.020 --> 02:20:18.220]   because there wasn't enough mathematical self-flagellation.
[02:20:18.220 --> 02:20:20.260]   - That's such a nice finding.
[02:20:20.260 --> 02:20:22.580]   So even, so showing that very few,
[02:20:22.580 --> 02:20:24.380]   just very little bit of noise,
[02:20:24.380 --> 02:20:27.300]   can have a dramatic, make a dramatic statement
[02:20:27.300 --> 02:20:29.380]   about the-- - It was a big surprise to us,
[02:20:29.380 --> 02:20:33.220]   but once we figured out how to show it, it's not too hard.
[02:20:33.220 --> 02:20:37.740]   - And these are venues that for theoretical,
[02:20:37.740 --> 02:20:38.900]   for theoretical work.
[02:20:38.900 --> 02:20:41.140]   Okay, so the fascinating tension
[02:20:41.140 --> 02:20:42.540]   that exists in other disciplines,
[02:20:42.540 --> 02:20:44.700]   like one of them is machine learning,
[02:20:44.700 --> 02:20:48.780]   which despite the power of machine learning
[02:20:48.780 --> 02:20:52.000]   and deep learning and all, like the impact of it,
[02:20:52.720 --> 02:20:55.960]   in the real world, the main conferences on machine learning
[02:20:55.960 --> 02:20:58.400]   are still resistant to application papers.
[02:20:58.400 --> 02:20:59.240]   - Yeah.
[02:20:59.240 --> 02:21:03.880]   - I'm not, sort of, and application papers broadly defined,
[02:21:03.880 --> 02:21:08.160]   meaning like finding almost like you would,
[02:21:08.160 --> 02:21:12.760]   like Darwin did by like going around,
[02:21:12.760 --> 02:21:14.240]   collecting some information, saying,
[02:21:14.240 --> 02:21:15.680]   "Huh, isn't this interesting?"
[02:21:15.680 --> 02:21:16.760]   - Yeah.
[02:21:16.760 --> 02:21:19.200]   - Like those are some of the most popular blogs,
[02:21:19.200 --> 02:21:21.320]   and yet as a paper, it's not really accepted.
[02:21:21.320 --> 02:21:23.400]   I wonder what you think about this whole world
[02:21:23.400 --> 02:21:27.900]   of deep learning from a perspective of theory.
[02:21:27.900 --> 02:21:31.720]   What do you make of this whole discipline
[02:21:31.720 --> 02:21:33.240]   of the success of neural networks,
[02:21:33.240 --> 02:21:34.960]   of how to do science on them?
[02:21:34.960 --> 02:21:37.960]   Are you excited by the possibilities
[02:21:37.960 --> 02:21:40.080]   of what we might discover about neural networks?
[02:21:40.080 --> 02:21:42.320]   Do you think it's fundamental in engineering discipline,
[02:21:42.320 --> 02:21:44.680]   or is there something theoretical
[02:21:44.680 --> 02:21:47.480]   that we might crack open one of these days
[02:21:47.480 --> 02:21:49.640]   in understanding something deep about how system,
[02:21:49.640 --> 02:21:52.160]   optimization, and how systems learn?
[02:21:52.160 --> 02:21:56.160]   - I am convinced by, is it Tagamart at MIT, who's--
[02:21:56.160 --> 02:21:57.000]   - Tagamart?
[02:21:57.000 --> 02:21:58.000]   - Yeah, Tagamart, right?
[02:21:58.000 --> 02:22:00.240]   So his notion has always been convincing to me
[02:22:00.240 --> 02:22:04.800]   that the fact that some of these models are inscrutable
[02:22:04.800 --> 02:22:06.880]   is not fundamental to them,
[02:22:06.880 --> 02:22:08.600]   and that we can, we're gonna get better and better,
[02:22:08.600 --> 02:22:09.920]   because in the end, you know,
[02:22:09.920 --> 02:22:12.360]   the reason why practicing computer scientists
[02:22:12.360 --> 02:22:15.560]   often who are doing AI, or working in AI industry,
[02:22:15.560 --> 02:22:18.640]   aren't like worried about so much existential threats
[02:22:18.640 --> 02:22:20.320]   is because they see the reality
[02:22:20.320 --> 02:22:22.920]   is they're multiplying matrices with numpy
[02:22:22.920 --> 02:22:23.760]   or something like this, right?
[02:22:23.760 --> 02:22:25.440]   Yeah, and tweaking constants
[02:22:25.440 --> 02:22:27.800]   and hoping that the classifier fitness,
[02:22:27.800 --> 02:22:30.520]   for God's sakes, before the submission deadline
[02:22:30.520 --> 02:22:31.680]   actually like gets above some,
[02:22:31.680 --> 02:22:36.120]   like it feels like it's linear algebra and TDM, right?
[02:22:36.120 --> 02:22:39.280]   But anyways, I'm really convinced with his idea
[02:22:39.280 --> 02:22:40.720]   that once we understand better and better
[02:22:40.720 --> 02:22:42.240]   what's going on from a theory perspective,
[02:22:42.240 --> 02:22:44.680]   it's gonna make it into an engineering discipline.
[02:22:44.680 --> 02:22:47.240]   So in my mind, where we're gonna end up is,
[02:22:47.240 --> 02:22:50.280]   okay, forget these metaphors of neurons,
[02:22:50.280 --> 02:22:52.360]   these things are gonna be put down
[02:22:52.360 --> 02:22:54.880]   into these mathematical kind of elegant equations,
[02:22:54.880 --> 02:22:57.640]   differentiable equations that just kind of work well.
[02:22:57.640 --> 02:23:00.000]   And then it's gonna be when I need a little bit of AI
[02:23:00.000 --> 02:23:02.320]   in this thing, plumbing,
[02:23:02.320 --> 02:23:05.240]   like let's get a little bit of a pattern recognizer
[02:23:05.240 --> 02:23:06.840]   with a noise module and let's connect,
[02:23:06.840 --> 02:23:08.280]   I mean, you know this feel better than me,
[02:23:08.280 --> 02:23:11.240]   so I don't know if this is like a reasonable prediction,
[02:23:11.240 --> 02:23:14.120]   but that we're gonna, it's gonna become less inscrutable,
[02:23:14.120 --> 02:23:16.320]   and then it's gonna become more engineerable,
[02:23:16.320 --> 02:23:18.600]   and then we're gonna have AI and more things
[02:23:18.600 --> 02:23:20.600]   because we're gonna have a little bit more control
[02:23:20.600 --> 02:23:22.560]   over how we piece together
[02:23:22.560 --> 02:23:25.880]   these different classification black boxes.
[02:23:25.880 --> 02:23:26.960]   - So one of the problems,
[02:23:26.960 --> 02:23:29.040]   and there might be some interesting parallels
[02:23:29.040 --> 02:23:31.040]   that you might provide intuition on is,
[02:23:31.040 --> 02:23:32.680]   you know, neural networks are very large
[02:23:32.680 --> 02:23:33.840]   and they have a lot of,
[02:23:33.840 --> 02:23:38.120]   we were talking about, you know,
[02:23:38.120 --> 02:23:41.920]   dynamic networks and distributed algorithms.
[02:23:41.920 --> 02:23:45.520]   One of the problems with the analysis of neural networks
[02:23:45.520 --> 02:23:48.720]   is, you know, you have a lot of nodes
[02:23:48.720 --> 02:23:50.840]   and you have a lot of edges.
[02:23:50.840 --> 02:23:53.160]   To be able to interpret and to control different things
[02:23:53.160 --> 02:23:54.000]   is very difficult.
[02:23:54.000 --> 02:23:58.120]   There's fields in trying to figure out,
[02:23:58.120 --> 02:24:03.120]   like mathematically, how you form clean representations
[02:24:03.120 --> 02:24:07.720]   that are like, like one node contains all the information
[02:24:07.720 --> 02:24:09.800]   about a particular thing and no other nodes
[02:24:09.800 --> 02:24:13.680]   is correlated to it, so like it has unique knowledge.
[02:24:13.680 --> 02:24:15.920]   But that ultimately boils down to trying
[02:24:15.920 --> 02:24:19.000]   to simplify this thing into,
[02:24:19.000 --> 02:24:20.600]   that goes against its very nature,
[02:24:20.600 --> 02:24:25.600]   which is like deeply connected and like dynamic
[02:24:25.600 --> 02:24:30.400]   and just, you know, hundreds of millions, billions of nodes.
[02:24:30.400 --> 02:24:33.960]   And in a distributed sense, like when you zoom out,
[02:24:33.960 --> 02:24:35.440]   the thing has a representation
[02:24:35.440 --> 02:24:36.960]   and understanding of something,
[02:24:36.960 --> 02:24:38.520]   but the individual nodes are just doing
[02:24:38.520 --> 02:24:40.520]   their little exchange thing.
[02:24:40.520 --> 02:24:42.800]   And it's the same thing with Stephen Wolfram
[02:24:42.800 --> 02:24:44.480]   when he talked about cellular automata.
[02:24:44.480 --> 02:24:46.360]   It's very difficult to do math
[02:24:46.360 --> 02:24:48.800]   when you have a huge collection of distributed things,
[02:24:48.800 --> 02:24:50.440]   each acting on their own.
[02:24:50.440 --> 02:24:54.760]   And it's almost like, it feels like it's almost impossible
[02:24:54.760 --> 02:24:58.200]   to do any kind of theoretical work in the traditional sense.
[02:24:58.200 --> 02:25:02.560]   It almost becomes completely like a biology,
[02:25:02.560 --> 02:25:06.080]   you become a biologist as opposed to a theoretician.
[02:25:06.080 --> 02:25:07.720]   You just study it experimentally.
[02:25:07.720 --> 02:25:10.720]   - Yeah, so I think that's the big question, I guess, right?
[02:25:10.720 --> 02:25:15.280]   Yeah, so is the large size and interconnectedness
[02:25:15.280 --> 02:25:17.800]   of the, like a deep learning network,
[02:25:17.800 --> 02:25:19.160]   fundamental to that task,
[02:25:19.160 --> 02:25:20.400]   or are we just not very good at it yet
[02:25:20.400 --> 02:25:23.400]   because we're using the wrong metaphor?
[02:25:23.400 --> 02:25:26.800]   I mean, the human brain learns with much fewer examples
[02:25:26.800 --> 02:25:30.440]   and with much less tuning of the whatever, whatever,
[02:25:30.440 --> 02:25:32.360]   whatever probably that requires to get
[02:25:32.360 --> 02:25:34.480]   those like deep mind networks up and running.
[02:25:34.480 --> 02:25:36.240]   But yeah, so I don't really know,
[02:25:36.240 --> 02:25:38.160]   but the one thing I have observed is that,
[02:25:38.160 --> 02:25:41.280]   yeah, there's the mundane nature
[02:25:41.280 --> 02:25:43.760]   of some of the working with these models
[02:25:43.760 --> 02:25:45.560]   tends to lead people to think that,
[02:25:45.560 --> 02:25:49.200]   to do it like, it could be Skynet,
[02:25:49.200 --> 02:25:52.320]   or it could be like a lot of pain to get,
[02:25:52.320 --> 02:25:54.600]   the thermostat to do what we want it to do.
[02:25:54.600 --> 02:25:56.680]   - And there's a lot of open questions in between there.
[02:25:56.680 --> 02:26:01.680]   And then of course, the distributed network
[02:26:01.680 --> 02:26:04.760]   of humans that use these systems.
[02:26:04.760 --> 02:26:07.520]   So like you can have the system itself,
[02:26:07.520 --> 02:26:08.680]   then you know network,
[02:26:08.680 --> 02:26:10.520]   but you can also have like little algorithms
[02:26:10.520 --> 02:26:11.800]   controlling the behavior of humans,
[02:26:11.800 --> 02:26:14.120]   which is what you have with social networks.
[02:26:14.120 --> 02:26:17.120]   It's possible that a very, what is it a toaster,
[02:26:17.120 --> 02:26:19.560]   or whatever, the opposite of Skynet,
[02:26:19.560 --> 02:26:20.640]   when taken at scale,
[02:26:20.640 --> 02:26:23.320]   but used by individual humans and controlling their behavior
[02:26:23.320 --> 02:26:25.640]   can actually have the Skynet effect.
[02:26:25.640 --> 02:26:27.800]   - Yeah. - So the scale there.
[02:26:27.800 --> 02:26:29.040]   - We might have that now.
[02:26:29.040 --> 02:26:30.720]   - We might have that now, we just don't know.
[02:26:30.720 --> 02:26:32.240]   - Yeah. - As it's happening.
[02:26:32.240 --> 02:26:33.680]   - Is Twitter creating a little mini Skynet?
[02:26:33.680 --> 02:26:35.080]   I mean, because what happens,
[02:26:35.080 --> 02:26:38.000]   it twirls out ramifications in the world.
[02:26:38.000 --> 02:26:40.160]   And is it really that much different
[02:26:40.160 --> 02:26:44.520]   if it's a robot with tentacles or a bunch of servers that.
[02:26:44.520 --> 02:26:47.560]   - Yeah, and the destructive effects could be,
[02:26:47.560 --> 02:26:48.880]   I mean, it could be political,
[02:26:48.880 --> 02:26:51.040]   but it could also be like,
[02:26:51.040 --> 02:26:52.640]   you could probably make an interesting case
[02:26:52.640 --> 02:26:57.640]   that the virus, the coronavirus spread on Twitter too,
[02:26:57.640 --> 02:27:00.800]   in the minds of people,
[02:27:00.800 --> 02:27:03.400]   like the fear and the misinformation
[02:27:03.400 --> 02:27:05.000]   in some very interesting ways.
[02:27:05.000 --> 02:27:06.400]   - Yeah. - Mixed up.
[02:27:06.400 --> 02:27:08.960]   And maybe this pandemic wasn't sufficiently dangerous
[02:27:08.960 --> 02:27:13.000]   to where that could have created a weird instability,
[02:27:13.000 --> 02:27:15.120]   but maybe other things might create instability.
[02:27:15.120 --> 02:27:17.000]   Like somebody, God forbid,
[02:27:17.000 --> 02:27:19.200]   detonates a nuclear weapon somewhere.
[02:27:19.200 --> 02:27:21.800]   And then maybe the destructive aspect of that
[02:27:21.800 --> 02:27:24.920]   would not as much be the military actions,
[02:27:24.920 --> 02:27:27.800]   but the way those news are spread on Twitter.
[02:27:27.800 --> 02:27:29.320]   - Yeah. - And the panic that creates.
[02:27:29.320 --> 02:27:30.440]   - Yeah, yeah.
[02:27:30.440 --> 02:27:32.360]   I mean, I think that's a great case study, right?
[02:27:32.360 --> 02:27:34.320]   Like what happened?
[02:27:34.320 --> 02:27:36.760]   I'm not suggesting that Lexi got let off a nuclear bomb.
[02:27:36.760 --> 02:27:39.280]   I meant the coronavirus, but okay.
[02:27:39.280 --> 02:27:42.120]   But yeah, I think that's a really interesting case study.
[02:27:42.120 --> 02:27:46.040]   I'm interested in the counterfactual of 1995,
[02:27:46.040 --> 02:27:48.360]   like do the same virus in 1995.
[02:27:48.360 --> 02:27:50.440]   So first of all, it would have been,
[02:27:50.440 --> 02:27:53.520]   I get to hear whatever, the nightly news,
[02:27:53.520 --> 02:27:54.480]   we'll talk about it,
[02:27:54.480 --> 02:27:57.280]   and then there'll be my local health board
[02:27:57.280 --> 02:27:58.320]   will talk about it.
[02:27:58.320 --> 02:27:59.880]   That meant mitigation decisions
[02:27:59.880 --> 02:28:04.320]   would probably necessarily be very sort of localized.
[02:28:04.320 --> 02:28:05.480]   Like our community is trying to figure out
[02:28:05.480 --> 02:28:06.320]   what are we gonna do?
[02:28:06.320 --> 02:28:07.160]   What's gonna happen?
[02:28:07.160 --> 02:28:08.080]   Like we see this with schools,
[02:28:08.080 --> 02:28:10.400]   like where I grew up in New Jersey,
[02:28:10.400 --> 02:28:12.800]   there's very localized school districts.
[02:28:12.800 --> 02:28:16.120]   So even though they had sort of really bad viral numbers
[02:28:16.120 --> 02:28:18.360]   there, my school I grew up in has been open since the fall
[02:28:18.360 --> 02:28:20.160]   because it's very localized.
[02:28:20.160 --> 02:28:21.640]   It's like these teachers and these parents,
[02:28:21.640 --> 02:28:22.480]   what do we wanna do?
[02:28:22.480 --> 02:28:23.680]   What are we comfortable with?
[02:28:23.680 --> 02:28:26.440]   I live in a school district right now in Montgomery County
[02:28:26.440 --> 02:28:27.960]   that's a billion dollar a year budget,
[02:28:27.960 --> 02:28:30.000]   150,000 kid school district.
[02:28:30.000 --> 02:28:32.720]   It just can't, it's closed, you know, because it's too.
[02:28:32.720 --> 02:28:33.960]   So I'm interested in that counterfactual.
[02:28:33.960 --> 02:28:36.440]   Yes, you have all this information moving around.
[02:28:36.440 --> 02:28:39.240]   And then you have the effects on discourse
[02:28:39.240 --> 02:28:40.520]   that we were talking about earlier,
[02:28:40.520 --> 02:28:43.520]   that the Neil Postman style effects of Twitter,
[02:28:43.520 --> 02:28:46.200]   which shifts people into a sort of a dunk culture mindset
[02:28:46.200 --> 02:28:49.880]   of don't give an inch to the other team.
[02:28:49.880 --> 02:28:51.720]   And we're used to this and was fired up by politics
[02:28:51.720 --> 02:28:53.480]   and the unique attributes of Twitter.
[02:28:53.480 --> 02:28:54.760]   Now throw in the coronavirus
[02:28:54.760 --> 02:28:57.800]   and suddenly we see decades of public health
[02:28:57.800 --> 02:29:00.960]   knowledge, a lot of which was honed during the HIV epidemic
[02:29:00.960 --> 02:29:02.320]   was thrown out the window
[02:29:02.320 --> 02:29:04.280]   because a lot of this was happening on Twitter.
[02:29:04.280 --> 02:29:06.200]   And suddenly we had public health officials
[02:29:06.200 --> 02:29:08.760]   using a don't give an inch to the other team mindset
[02:29:08.760 --> 02:29:10.600]   of like, well, if we say this,
[02:29:10.600 --> 02:29:13.000]   that might validate something that was wrong over here.
[02:29:13.000 --> 02:29:14.240]   And we need to, if we say this,
[02:29:14.240 --> 02:29:16.440]   then maybe like that'll stop them from doing this.
[02:29:16.440 --> 02:29:19.760]   That's like very Twittery in a way that in 1995
[02:29:19.760 --> 02:29:22.620]   is probably not the way public health officials
[02:29:22.620 --> 02:29:23.460]   would be thinking.
[02:29:23.460 --> 02:29:25.320]   Or now it's like, well, this is,
[02:29:25.320 --> 02:29:26.560]   if we said this about masks,
[02:29:26.560 --> 02:29:28.160]   but the other team said that about masks,
[02:29:28.160 --> 02:29:29.320]   we can't give an inch to this.
[02:29:29.320 --> 02:29:30.280]   So we gotta be careful.
[02:29:30.280 --> 02:29:31.680]   And like, we can't tell people it's okay
[02:29:31.680 --> 02:29:33.640]   after they're vaccinated because that might,
[02:29:33.640 --> 02:29:34.680]   we're giving them an inch on this.
[02:29:34.680 --> 02:29:36.840]   And that's very Twittery in my mind, right?
[02:29:36.840 --> 02:29:39.440]   That is the impact of Twitter
[02:29:39.440 --> 02:29:40.680]   on the way we think about discourse,
[02:29:40.680 --> 02:29:42.480]   which is a dunking culture of don't give any inch
[02:29:42.480 --> 02:29:43.320]   to the other team.
[02:29:43.320 --> 02:29:45.240]   And it's all about slam dunks where you're completely right
[02:29:45.240 --> 02:29:46.480]   and they're completely wrong.
[02:29:46.480 --> 02:29:49.080]   It's as a rhetorical strategy is incredibly simplistic,
[02:29:49.080 --> 02:29:50.840]   but it's also the way that we think right now
[02:29:50.840 --> 02:29:52.540]   about how we do debate.
[02:29:52.540 --> 02:29:56.400]   It combined terribly with a election year pandemic.
[02:29:56.400 --> 02:29:58.240]   - Yeah, election year pandemic.
[02:29:58.240 --> 02:30:00.040]   I wonder if we could do some smooth analysis.
[02:30:00.040 --> 02:30:01.840]   Let's run the simulation over a few times.
[02:30:01.840 --> 02:30:02.680]   - A little bit of noise.
[02:30:02.680 --> 02:30:05.760]   - Yeah, see if it can dramatically change
[02:30:05.760 --> 02:30:07.240]   the behavior of the system.
[02:30:07.240 --> 02:30:10.080]   Okay, we talked about your love for proving
[02:30:10.080 --> 02:30:11.640]   that something is impossible.
[02:30:11.640 --> 02:30:14.360]   So there's quite a few still open problems
[02:30:14.360 --> 02:30:17.440]   and complexity of algorithms.
[02:30:17.440 --> 02:30:20.820]   So let me ask, does P equal NP?
[02:30:20.820 --> 02:30:22.040]   - Probably not.
[02:30:22.040 --> 02:30:23.380]   - Probably not.
[02:30:23.380 --> 02:30:28.380]   If P equals NP, what kind of,
[02:30:28.380 --> 02:30:31.600]   and you'd be really surprised somebody proves it.
[02:30:31.600 --> 02:30:33.960]   What would that proof look like?
[02:30:33.960 --> 02:30:35.080]   And why would that even be?
[02:30:35.080 --> 02:30:36.480]   What would that mean?
[02:30:36.480 --> 02:30:38.320]   What would that proof look like?
[02:30:38.320 --> 02:30:40.800]   And what possible universe could P equals NP?
[02:30:40.800 --> 02:30:43.440]   Is there something insightful you could say there?
[02:30:43.440 --> 02:30:45.440]   - It could be true.
[02:30:45.440 --> 02:30:47.280]   And I mean, I'm not a complexity theorist,
[02:30:47.280 --> 02:30:49.520]   but every complexity theorist I know
[02:30:49.520 --> 02:30:51.440]   is convinced they're not equal
[02:30:51.440 --> 02:30:53.120]   and are basically not working on it anymore.
[02:30:53.120 --> 02:30:54.480]   I mean, there is a million dollars at stake
[02:30:54.480 --> 02:30:55.920]   if you can solve the proof.
[02:30:55.920 --> 02:30:57.800]   It's one of the Millennium Prizes.
[02:30:57.800 --> 02:31:00.840]   Okay, so here's how I think the P not equals NP proof
[02:31:00.840 --> 02:31:02.800]   is gonna eventually happen.
[02:31:02.800 --> 02:31:04.520]   I think it's gonna fall out
[02:31:04.520 --> 02:31:07.360]   and it's gonna be not super simple,
[02:31:07.360 --> 02:31:08.720]   but not as hard as people think.
[02:31:08.720 --> 02:31:11.640]   Because my theory about a lot of theoretical
[02:31:11.640 --> 02:31:14.000]   computer science based on just some results I've done,
[02:31:14.000 --> 02:31:15.440]   so this is a huge extrapolation,
[02:31:15.440 --> 02:31:17.200]   is that a lot of what we're doing
[02:31:17.200 --> 02:31:20.040]   is just obfuscating deeper mathematics.
[02:31:20.040 --> 02:31:22.680]   So like this happens to me a lot, not a lot,
[02:31:22.680 --> 02:31:24.320]   but it's happened to me a few times in my work
[02:31:24.320 --> 02:31:26.240]   where we obfuscate it because we say,
[02:31:26.240 --> 02:31:29.280]   well, there's an algorithm and it has this much memory
[02:31:29.280 --> 02:31:30.640]   and they're connected on a network.
[02:31:30.640 --> 02:31:31.760]   And okay, here's our setup.
[02:31:31.760 --> 02:31:34.480]   And now we're trying to see how fast it can solve a problem.
[02:31:34.480 --> 02:31:35.960]   And people do bounds about it.
[02:31:35.960 --> 02:31:37.040]   And then the end, it turns out
[02:31:37.040 --> 02:31:39.320]   that we were just obfuscating some underlying
[02:31:39.320 --> 02:31:43.320]   mathematical thing that already existed.
[02:31:43.320 --> 02:31:44.360]   Right, so this has happened to me.
[02:31:44.360 --> 02:31:47.400]   I had this paper I was quite fond of a while ago.
[02:31:47.400 --> 02:31:50.580]   It was looking at this problem called contention resolution
[02:31:50.580 --> 02:31:54.520]   where you put an unknown set of people on a shared channel
[02:31:54.520 --> 02:31:55.740]   and they're trying to break symmetry.
[02:31:55.740 --> 02:31:57.720]   So it was like an ethernet, whatever.
[02:31:57.720 --> 02:31:58.920]   Only one person can use it at a time.
[02:31:58.920 --> 02:31:59.760]   You try to break symmetry.
[02:31:59.760 --> 02:32:02.360]   There's all these bounds people have proven over the years
[02:32:02.360 --> 02:32:05.040]   about how long it takes to do this, right?
[02:32:05.040 --> 02:32:07.400]   And like I discovered at some point,
[02:32:07.400 --> 02:32:10.400]   there's this one combinatorial result
[02:32:10.400 --> 02:32:12.600]   from the early 1990s.
[02:32:12.600 --> 02:32:15.400]   All of these lower bound proofs all come from this.
[02:32:15.400 --> 02:32:16.800]   And in fact, it improved a lot of them
[02:32:16.800 --> 02:32:17.640]   and simplified a lot.
[02:32:17.640 --> 02:32:19.760]   You could put it all in one paper.
[02:32:19.760 --> 02:32:20.600]   It was like, are we really?
[02:32:20.600 --> 02:32:21.860]   And then, okay, so this new paper
[02:32:21.860 --> 02:32:24.180]   that I submitted a couple of weeks ago,
[02:32:24.180 --> 02:32:26.380]   I found you could take some of these same lower bound proofs
[02:32:26.380 --> 02:32:28.040]   for this contention resolution problem.
[02:32:28.040 --> 02:32:32.240]   You could reprove them using Shannon's source code theorem.
[02:32:32.240 --> 02:32:34.040]   That actually when you're breaking contention,
[02:32:34.040 --> 02:32:37.320]   what you're really doing is building a code over,
[02:32:37.320 --> 02:32:40.460]   if you have a distribution on the network sizes,
[02:32:40.460 --> 02:32:41.700]   it's a code over that source.
[02:32:41.700 --> 02:32:44.400]   And if you plug in a high entropy information source
[02:32:44.400 --> 02:32:47.460]   and plug in from 1948, the source code theorem
[02:32:47.460 --> 02:32:48.680]   that says on a noiseless channel,
[02:32:48.680 --> 02:32:51.080]   you can't send things at a faster rate
[02:32:51.080 --> 02:32:52.400]   than the entropy allows,
[02:32:52.400 --> 02:32:54.400]   the exact same lower bounds fall back out again.
[02:32:54.400 --> 02:32:55.800]   So like this type of thing happens,
[02:32:55.800 --> 02:32:58.680]   there's some famous lower bounds and distributed algorithms
[02:32:58.680 --> 02:33:01.560]   that turned out to all be algebraic topology
[02:33:01.560 --> 02:33:02.680]   underneath the covers.
[02:33:02.680 --> 02:33:05.480]   And they won the Girdle Prize for working on that.
[02:33:05.480 --> 02:33:08.360]   So my sense is what's gonna happen is at some point,
[02:33:08.360 --> 02:33:10.960]   someone really smart, it's gonna be very exciting,
[02:33:10.960 --> 02:33:14.720]   is gonna realize there's some sort of other representation
[02:33:14.720 --> 02:33:16.800]   of what's going on with these Turing machines
[02:33:16.800 --> 02:33:18.480]   trying to sort of efficiently compute.
[02:33:18.480 --> 02:33:19.320]   - It'll naturally fall out of that.
[02:33:19.320 --> 02:33:22.080]   - And there'll be an existing mathematical result
[02:33:22.080 --> 02:33:23.920]   that applies.
[02:33:23.920 --> 02:33:25.360]   - Someone or something, I guess.
[02:33:25.360 --> 02:33:28.000]   It could be AI theorem provers kind of thing.
[02:33:28.000 --> 02:33:28.840]   - It could be, yeah.
[02:33:28.840 --> 02:33:30.520]   I mean, not a, well, yeah.
[02:33:30.520 --> 02:33:31.760]   I mean, there's theorem provers,
[02:33:31.760 --> 02:33:35.520]   like what that means now, which is not fun.
[02:33:35.520 --> 02:33:37.040]   - It's just a bunch of--
[02:33:37.040 --> 02:33:39.280]   - Very carefully formulated postulates that,
[02:33:39.280 --> 02:33:41.320]   but I take your point, yeah.
[02:33:41.320 --> 02:33:42.700]   - Yeah, so, okay.
[02:33:44.480 --> 02:33:47.480]   On a small tangent, and then you're kind of
[02:33:47.480 --> 02:33:49.840]   implying that mathematics, it almost feels
[02:33:49.840 --> 02:33:52.480]   like a kind of weird evolutionary tree
[02:33:52.480 --> 02:33:55.400]   that ultimately leads back to some kind of ancestral,
[02:33:55.400 --> 02:33:58.820]   few fundamental ideas that all are just like,
[02:33:58.820 --> 02:34:00.320]   they're all somehow connected.
[02:34:00.320 --> 02:34:06.240]   In that sense, do you think math is fundamental
[02:34:06.240 --> 02:34:09.080]   to our universe and we're just like slowly
[02:34:09.080 --> 02:34:12.000]   trying to understand these patterns?
[02:34:12.000 --> 02:34:14.200]   Or is it discovered?
[02:34:14.200 --> 02:34:17.960]   Or is it just a little game that we play
[02:34:17.960 --> 02:34:21.560]   amongst ourselves to try to fit
[02:34:21.560 --> 02:34:23.640]   little patterns to the world?
[02:34:23.640 --> 02:34:24.840]   - Yeah, that's the question, right?
[02:34:24.840 --> 02:34:26.800]   That's the physicist's question.
[02:34:26.800 --> 02:34:29.080]   I mean, I'm probably, I'm in the discovered camp,
[02:34:29.080 --> 02:34:30.680]   but I don't do theoretical physics,
[02:34:30.680 --> 02:34:34.240]   so I know they have a, they feel like
[02:34:34.240 --> 02:34:37.240]   they have a stronger claim to answering that question.
[02:34:37.240 --> 02:34:38.440]   But everything comes back to it.
[02:34:38.440 --> 02:34:39.480]   Everything comes back to it.
[02:34:39.480 --> 02:34:41.360]   I mean, all of physics, the fact that
[02:34:41.360 --> 02:34:43.900]   the universe is, well, okay.
[02:34:43.900 --> 02:34:47.200]   It's a complicated question.
[02:34:47.200 --> 02:34:51.080]   - So how often do you think, how deeply
[02:34:51.080 --> 02:34:55.680]   does this result describe the fundamental reality of nature?
[02:34:55.680 --> 02:35:01.080]   - So the reason I hesitated, because it's something I'm,
[02:35:01.080 --> 02:35:03.080]   I taught this seminar and did a little work
[02:35:03.080 --> 02:35:05.000]   on what are called biological algorithms.
[02:35:05.000 --> 02:35:07.500]   So there's this notion of,
[02:35:09.360 --> 02:35:14.360]   so physicists use mathematics to explain the universe,
[02:35:14.360 --> 02:35:18.200]   and it was unreasonable that mathematics works so well.
[02:35:18.200 --> 02:35:19.760]   All these differential equations,
[02:35:19.760 --> 02:35:21.640]   why does that explain all we need to know
[02:35:21.640 --> 02:35:23.000]   about thermodynamics and gravity
[02:35:23.000 --> 02:35:24.200]   and all these type of things?
[02:35:24.200 --> 02:35:26.400]   Well, there's this movement within
[02:35:26.400 --> 02:35:28.640]   the intersection of computer science and biology.
[02:35:28.640 --> 02:35:31.120]   It's just kind of Wolframium, I guess, really,
[02:35:31.120 --> 02:35:35.000]   that algorithms can be very explanatory.
[02:35:35.000 --> 02:35:38.960]   Like if you're trying to explain parsimoniously
[02:35:39.400 --> 02:35:41.800]   something about like an ant colony or something like this,
[02:35:41.800 --> 02:35:43.640]   you're not going to, ultimately,
[02:35:43.640 --> 02:35:45.800]   it's not gonna be explained as a equation,
[02:35:45.800 --> 02:35:46.800]   like a physics equation.
[02:35:46.800 --> 02:35:48.240]   It's gonna be explained by an algorithm.
[02:35:48.240 --> 02:35:51.880]   So like this algorithm run distributedly
[02:35:51.880 --> 02:35:53.220]   is going to explain the behavior.
[02:35:53.220 --> 02:35:56.120]   So that's mathematical, but not quite mathematical,
[02:35:56.120 --> 02:35:57.620]   but it is if you think about an algorithm
[02:35:57.620 --> 02:35:59.480]   like a lambda calculus, which brings you back
[02:35:59.480 --> 02:36:01.040]   to the world of mathematics.
[02:36:01.040 --> 02:36:03.280]   So I'm thinking out loud here, but basically,
[02:36:04.880 --> 02:36:09.120]   abstract math is sort of like unreasonably effective
[02:36:09.120 --> 02:36:10.400]   at explaining a lot of things.
[02:36:10.400 --> 02:36:11.820]   And that's just what I feel like I glimpse.
[02:36:11.820 --> 02:36:14.960]   I'm not like a super well-known theoretician.
[02:36:14.960 --> 02:36:16.940]   I don't have really famous results.
[02:36:16.940 --> 02:36:21.840]   So even as a sort of middling career theoretician,
[02:36:21.840 --> 02:36:25.440]   I keep encountering this where we think
[02:36:25.440 --> 02:36:28.600]   we're solving some problem about computers and algorithms,
[02:36:28.600 --> 02:36:31.000]   and it's some much deeper underlying math.
[02:36:31.000 --> 02:36:33.200]   It's Shannon, but Shannon is entropy,
[02:36:33.200 --> 02:36:36.800]   but entropy was really, goes all the way back
[02:36:36.800 --> 02:36:39.280]   to whatever it was, Boyle, or all the way back
[02:36:39.280 --> 02:36:40.600]   to looking at the early physics.
[02:36:40.600 --> 02:36:44.920]   And it's, anyways, to me, I think it's amazing.
[02:36:44.920 --> 02:36:47.360]   - Yeah, but it could be the flip side of that
[02:36:47.360 --> 02:36:49.440]   could be just our brains draw so much pleasure
[02:36:49.440 --> 02:36:53.200]   from the deriving generalized theories
[02:36:53.200 --> 02:36:56.640]   and simplifying the universe that we just naturally see
[02:36:56.640 --> 02:36:58.640]   that kind of simplicity and everything.
[02:36:58.640 --> 02:37:01.840]   - Yeah, so that's the whole Newton to Einstein, right?
[02:37:01.840 --> 02:37:03.560]   So you can say this must be right
[02:37:03.560 --> 02:37:04.920]   because it's so predictive.
[02:37:04.920 --> 02:37:06.400]   And well, it's not quite predictive
[02:37:06.400 --> 02:37:07.680]   because Mercury wobbles a little bit,
[02:37:07.680 --> 02:37:08.640]   but I think we have it set.
[02:37:08.640 --> 02:37:10.720]   And then you turn out, no, Einstein.
[02:37:10.720 --> 02:37:13.880]   And then you get Bohr, like, no, not Einstein.
[02:37:13.880 --> 02:37:15.120]   It's actually statistical.
[02:37:15.120 --> 02:37:16.600]   And yeah, so that would say-
[02:37:16.600 --> 02:37:18.960]   - It's hard to also know where a smooth analysis
[02:37:18.960 --> 02:37:21.520]   fits into all that, where a little bit of noise,
[02:37:21.520 --> 02:37:25.680]   like you can say something very clean about a system
[02:37:25.680 --> 02:37:27.400]   and then a little bit of noise,
[02:37:27.400 --> 02:37:29.840]   like the average case is actually very different.
[02:37:29.840 --> 02:37:32.880]   And so, I mean, that's where the quantum mechanics comes in.
[02:37:32.880 --> 02:37:36.160]   It's like, ugh, why does it have to be randomness in this?
[02:37:36.160 --> 02:37:38.680]   - Yeah, it would have to do this complex statistics.
[02:37:38.680 --> 02:37:39.520]   - Yeah. - Yeah.
[02:37:39.520 --> 02:37:42.560]   - So to be determined.
[02:37:42.560 --> 02:37:43.960]   - Yeah, that'll be my next book.
[02:37:43.960 --> 02:37:45.600]   That'd be ambitious.
[02:37:45.600 --> 02:37:50.000]   The fundamental core of reality, comma,
[02:37:50.000 --> 02:37:52.440]   and some advice for being more productive at work.
[02:37:52.440 --> 02:37:53.720]   (both laugh)
[02:37:53.720 --> 02:37:57.080]   - Can I ask you just if it's possible to do an overview
[02:37:57.080 --> 02:38:00.400]   and just some brief comments of wisdom
[02:38:00.400 --> 02:38:02.880]   on the process of publishing a book?
[02:38:02.880 --> 02:38:04.040]   What's that process entail?
[02:38:04.040 --> 02:38:05.400]   What are the different options?
[02:38:05.400 --> 02:38:06.960]   And what's your recommendation
[02:38:06.960 --> 02:38:12.400]   for somebody that wants to write a book like yours,
[02:38:12.400 --> 02:38:15.960]   a nonfiction book that discovers something interesting
[02:38:15.960 --> 02:38:17.760]   about this world?
[02:38:17.760 --> 02:38:22.760]   - So what I usually advise is follow the process as is.
[02:38:25.560 --> 02:38:27.280]   Don't try to reinvent.
[02:38:27.280 --> 02:38:31.480]   I think that happens a lot where you'll try to reinvent
[02:38:31.480 --> 02:38:33.000]   the way the publishing industry should work.
[02:38:33.000 --> 02:38:36.280]   Like this is kind of not like in a business model ways,
[02:38:36.280 --> 02:38:38.200]   but just like, this is what I want to do.
[02:38:38.200 --> 02:38:40.680]   I wanna write a thousand words a day and I wanna do this,
[02:38:40.680 --> 02:38:42.120]   and I'm gonna put it on the internet.
[02:38:42.120 --> 02:38:44.640]   And the publishing industry is very specific
[02:38:44.640 --> 02:38:46.280]   about how it works.
[02:38:46.280 --> 02:38:47.840]   And so like when I got started writing books,
[02:38:47.840 --> 02:38:48.800]   which at a very young age,
[02:38:48.800 --> 02:38:52.280]   so I sold my first book at the age of 21,
[02:38:52.280 --> 02:38:56.280]   the way I did that is I found a family friend
[02:38:56.280 --> 02:38:57.920]   that was an agent.
[02:38:57.920 --> 02:39:00.480]   And I said, "I'm not trying to make you be my agent.
[02:39:00.480 --> 02:39:02.160]   "Just explain to me how this works.
[02:39:02.160 --> 02:39:03.240]   "Not just how the world works,
[02:39:03.240 --> 02:39:06.580]   "but give me the hard truth about how would a 21-year-old,
[02:39:06.580 --> 02:39:08.700]   "under what conditions could a 21-year-old sell a book
[02:39:08.700 --> 02:39:09.540]   "and what would that look like?"
[02:39:09.540 --> 02:39:10.560]   And she just explained it to me.
[02:39:10.560 --> 02:39:11.400]   Well, you'd have to do this
[02:39:11.400 --> 02:39:14.000]   and have to be a subject that it made sense for you to write.
[02:39:14.000 --> 02:39:15.320]   And you would have to do this type of writing
[02:39:15.320 --> 02:39:17.560]   for other publications, the validated and blah, blah, blah.
[02:39:17.560 --> 02:39:18.720]   And you have to get the agent first.
[02:39:18.720 --> 02:39:20.760]   And I learned the whole game plan.
[02:39:20.760 --> 02:39:22.240]   And then I executed.
[02:39:22.240 --> 02:39:24.680]   And so the rough game plan is with nonfiction,
[02:39:24.680 --> 02:39:26.440]   you get the agent first
[02:39:26.440 --> 02:39:28.720]   and the agent's gonna sell it to the publishers.
[02:39:28.720 --> 02:39:30.200]   So you're never sending something directly
[02:39:30.200 --> 02:39:31.040]   to the publishers.
[02:39:31.040 --> 02:39:34.280]   In nonfiction, you're not writing the book first.
[02:39:34.280 --> 02:39:37.260]   You're gonna get an advance from the publisher once sold.
[02:39:37.260 --> 02:39:40.160]   And then you're gonna do the primary writing of the book.
[02:39:40.160 --> 02:39:42.320]   In fact, it will, in most circumstances,
[02:39:42.320 --> 02:39:43.520]   hurt you if you've already written.
[02:39:43.520 --> 02:39:44.360]   - If you've already written.
[02:39:44.360 --> 02:39:45.200]   - Yeah.
[02:39:45.200 --> 02:39:46.020]   - So you're trying to sell,
[02:39:46.020 --> 02:39:47.160]   well, I guess the agent,
[02:39:47.160 --> 02:39:48.000]   first you sell it to the agent
[02:39:48.000 --> 02:39:49.760]   and then the agent sells it to the publishers.
[02:39:49.760 --> 02:39:51.640]   - Yeah, it's much easier to get an agent
[02:39:51.640 --> 02:39:52.480]   than a book deal.
[02:39:52.480 --> 02:39:53.440]   So the thought is,
[02:39:53.440 --> 02:39:55.260]   if you can't get an agent, then why would you?
[02:39:55.260 --> 02:39:56.200]   So you start with,
[02:39:56.200 --> 02:39:59.160]   and also, the way this works with a good agent is,
[02:39:59.160 --> 02:40:00.440]   they know all the editors
[02:40:00.440 --> 02:40:01.480]   and they have lunch with the editors.
[02:40:01.480 --> 02:40:02.320]   And they're always just like,
[02:40:02.320 --> 02:40:03.160]   "Hey, what projects do you have coming?
[02:40:03.160 --> 02:40:04.000]   "What are you looking for?
[02:40:04.000 --> 02:40:04.820]   "Here's one of my authors."
[02:40:04.820 --> 02:40:06.040]   That's the way all these deals happen.
[02:40:06.040 --> 02:40:09.480]   It's not, you're not emailing a manuscript to a slush pile.
[02:40:09.480 --> 02:40:11.120]   - Yeah, and so, so first of all,
[02:40:11.120 --> 02:40:13.200]   the agent takes a percentage and then the publishers,
[02:40:13.200 --> 02:40:14.640]   this is where the process comes in.
[02:40:14.640 --> 02:40:17.840]   They take also a cut that's probably ridiculous.
[02:40:17.840 --> 02:40:20.980]   So if you try to reinvent the system,
[02:40:20.980 --> 02:40:22.760]   you'll probably be frustrated by the percentage
[02:40:22.760 --> 02:40:24.320]   that everyone takes relative to
[02:40:24.320 --> 02:40:26.560]   how much bureaucracy and efficiency,
[02:40:26.560 --> 02:40:28.200]   ridiculousness there is in the system.
[02:40:28.200 --> 02:40:29.960]   Your recommendation is like,
[02:40:29.960 --> 02:40:31.620]   you're just one ant.
[02:40:31.620 --> 02:40:34.360]   Stop trying to build your own ant colony.
[02:40:34.360 --> 02:40:36.800]   - Well, or if you create your own process
[02:40:36.800 --> 02:40:38.400]   for how it should work,
[02:40:38.400 --> 02:40:39.440]   the book's not gonna get published.
[02:40:39.440 --> 02:40:40.880]   So there's the separate question,
[02:40:40.880 --> 02:40:41.920]   the economic question of like,
[02:40:41.920 --> 02:40:43.660]   should I create my own,
[02:40:43.660 --> 02:40:45.400]   like self-publish it or do something like that?
[02:40:45.400 --> 02:40:47.080]   But putting that aside,
[02:40:47.080 --> 02:40:48.860]   there's a lot of people I encounter
[02:40:48.860 --> 02:40:51.320]   that wanna publish a book with a main publisher,
[02:40:51.320 --> 02:40:54.080]   but they invent their own rules for how it works, right?
[02:40:54.080 --> 02:40:56.400]   - So then the alternative though is self-publishing
[02:40:56.400 --> 02:41:00.440]   and the downside, there's a lot of downsides.
[02:41:00.440 --> 02:41:03.080]   It's like, it's almost like publishing an opinion piece
[02:41:03.080 --> 02:41:05.560]   in the New York Times versus writing on a blog.
[02:41:05.560 --> 02:41:09.520]   There's no reason why writing a blog post on Medium
[02:41:09.520 --> 02:41:13.560]   can't get way more attention and legitimacy
[02:41:13.560 --> 02:41:16.800]   and long-lasting prestige than a New York Times article.
[02:41:16.800 --> 02:41:18.900]   But nevertheless, for most people,
[02:41:18.900 --> 02:41:21.220]   writing in a prestigious newspaper,
[02:41:21.220 --> 02:41:26.220]   quote unquote prestigious, is just easier.
[02:41:26.220 --> 02:41:29.060]   - And well, and depends on your goal.
[02:41:29.060 --> 02:41:32.220]   So, you know, like I push you towards a big publisher
[02:41:32.220 --> 02:41:33.980]   because I think your goal is,
[02:41:33.980 --> 02:41:36.060]   it's huge ideas you want to impact.
[02:41:36.060 --> 02:41:37.900]   You're gonna have more impact, you know?
[02:41:37.900 --> 02:41:39.820]   - Even though, like actually,
[02:41:39.820 --> 02:41:41.860]   so there's different ways to measure impact, right?
[02:41:41.860 --> 02:41:42.780]   - In the world of ideas.
[02:41:42.780 --> 02:41:44.180]   - In the world of ideas.
[02:41:44.180 --> 02:41:46.420]   And also, yeah, in the world of ideas,
[02:41:46.420 --> 02:41:48.920]   it's kind of like the clubhouse thing now,
[02:41:48.920 --> 02:41:51.160]   even if the audience is not large,
[02:41:51.160 --> 02:41:53.560]   the people in the audience are very interesting.
[02:41:53.560 --> 02:41:56.860]   It's like the conversation feels like
[02:41:56.860 --> 02:42:01.460]   it has long-lasting impact among the people
[02:42:01.460 --> 02:42:03.720]   who in different and disparate industries
[02:42:03.720 --> 02:42:06.540]   that are also then starting their own conversations
[02:42:06.540 --> 02:42:07.380]   and all that kind of stuff.
[02:42:07.380 --> 02:42:09.020]   - Yeah, because you have other,
[02:42:09.020 --> 02:42:11.080]   so like self-publishing a book,
[02:42:11.080 --> 02:42:13.260]   the goals that would solve,
[02:42:13.260 --> 02:42:15.340]   you have much better ways of getting to those goals
[02:42:15.340 --> 02:42:16.220]   might be part of it, right?
[02:42:16.220 --> 02:42:17.900]   So if there's the financial aspect
[02:42:17.900 --> 02:42:19.580]   of, well, you get to keep more of it,
[02:42:19.580 --> 02:42:22.420]   I mean, the podcast is probably gonna crush
[02:42:22.420 --> 02:42:23.580]   what the book's gonna do anyways, right?
[02:42:23.580 --> 02:42:25.260]   Yeah, if it's,
[02:42:25.260 --> 02:42:28.780]   wanna get directly to certain audiences or crowds,
[02:42:28.780 --> 02:42:30.580]   it might be harder through a traditional publisher.
[02:42:30.580 --> 02:42:32.580]   There's better ways to talk to those crowds.
[02:42:32.580 --> 02:42:33.420]   It could be on clubhouse.
[02:42:33.420 --> 02:42:34.660]   With all these new technologies,
[02:42:34.660 --> 02:42:36.900]   self-published books not gonna be the most effective way
[02:42:36.900 --> 02:42:39.220]   to find your way to a new crowd.
[02:42:39.220 --> 02:42:40.040]   But if the idea is like,
[02:42:40.040 --> 02:42:43.600]   I wanna have a, leave a dent in the world of ideas,
[02:42:43.600 --> 02:42:47.200]   then to have a vulnerable old publisher,
[02:42:47.200 --> 02:42:49.140]   put out your book in a nice hardcover
[02:42:49.140 --> 02:42:51.080]   and do the things they do,
[02:42:51.080 --> 02:42:53.000]   that goes a long way.
[02:42:53.000 --> 02:42:53.820]   And they do do a lot.
[02:42:53.820 --> 02:42:55.660]   I mean, it's very difficult actually.
[02:42:55.660 --> 02:42:57.800]   There's so much involved in putting together a book.
[02:42:57.800 --> 02:42:59.160]   - They get books into bookstores
[02:42:59.160 --> 02:42:59.980]   and all that kind of stuff.
[02:42:59.980 --> 02:43:02.480]   - All this, and from an efficiency standpoint,
[02:43:02.480 --> 02:43:03.720]   I mean, just the time involved
[02:43:03.720 --> 02:43:05.520]   in trying to do this yourself is,
[02:43:05.520 --> 02:43:06.720]   they know people do it. - They have a process, right?
[02:43:06.720 --> 02:43:08.360]   Like you said, they have a process.
[02:43:08.360 --> 02:43:09.200]   - They've got a process.
[02:43:09.200 --> 02:43:10.960]   I mean, I know like Jaco did this recently.
[02:43:10.960 --> 02:43:13.000]   He started his own imprint and I have a couple other,
[02:43:13.000 --> 02:43:14.960]   but it's a huge overhead.
[02:43:14.960 --> 02:43:17.160]   I mean, if you like, if you run a business and you,
[02:43:17.160 --> 02:43:19.220]   so like Jaco is a good case study, right?
[02:43:19.220 --> 02:43:22.360]   So he got fed up with Simon and Schuster
[02:43:22.360 --> 02:43:23.400]   dragging their feet and said,
[02:43:23.400 --> 02:43:25.160]   "I'm gonna start my own imprint then,
[02:43:25.160 --> 02:43:28.040]   if you're not gonna publish my kid's book."
[02:43:28.040 --> 02:43:29.240]   But he, what does he do?
[02:43:29.240 --> 02:43:30.760]   He runs businesses, right?
[02:43:30.760 --> 02:43:32.720]   So I think in his world, like I already run,
[02:43:32.720 --> 02:43:34.880]   I'm a partner in whatever, in Origin,
[02:43:34.880 --> 02:43:35.720]   and I have this and that.
[02:43:35.720 --> 02:43:37.680]   And so it's like, yeah, we can run businesses.
[02:43:37.680 --> 02:43:38.520]   That's what we know how to do.
[02:43:38.520 --> 02:43:39.340]   That's what I do.
[02:43:39.340 --> 02:43:40.500]   I run businesses, I have people,
[02:43:40.500 --> 02:43:43.120]   but for like you or I, we don't run businesses.
[02:43:43.120 --> 02:43:43.960]   It'd be terrible.
[02:43:43.960 --> 02:43:44.800]   - Yeah. - Yeah.
[02:43:44.800 --> 02:43:47.400]   - Well, especially these kinds of businesses, right?
[02:43:47.400 --> 02:43:48.880]   So I do wanna launch a business,
[02:43:48.880 --> 02:43:50.400]   but very different technology business.
[02:43:50.400 --> 02:43:51.240]   It's a very different space. - Very different.
[02:43:51.240 --> 02:43:52.560]   - Very different space. - Very, very different.
[02:43:52.560 --> 02:43:53.400]   Yeah, yeah.
[02:43:53.400 --> 02:43:55.720]   I mean, this is like, okay, I need copy editors
[02:43:55.720 --> 02:43:57.840]   and graphic book binders,
[02:43:57.840 --> 02:43:59.240]   and I need a contract with the printer,
[02:43:59.240 --> 02:44:00.720]   but the printer doesn't have slots.
[02:44:00.720 --> 02:44:02.760]   And so now I have to try to, I mean, it's-
[02:44:02.760 --> 02:44:05.640]   - I get so, I need to shut this off in my brain,
[02:44:05.640 --> 02:44:07.180]   but I get so frustrated when the system
[02:44:07.180 --> 02:44:08.580]   could clearly be improved.
[02:44:08.580 --> 02:44:10.080]   It's the thing that you're mentioning.
[02:44:10.080 --> 02:44:10.920]   - Yeah.
[02:44:10.920 --> 02:44:12.440]   - It's like, this is so inefficient.
[02:44:12.440 --> 02:44:15.640]   Every time I go to the DMV or something like that,
[02:44:15.640 --> 02:44:17.940]   you'd think like, ah, this could be done so much better.
[02:44:17.940 --> 02:44:18.780]   - Yeah.
[02:44:18.780 --> 02:44:22.600]   - But, you know, and the same thing is the worry
[02:44:22.600 --> 02:44:26.080]   with an editor, which I guess would come from the publisher,
[02:44:26.080 --> 02:44:31.080]   like who would, how much supervision on your book
[02:44:31.080 --> 02:44:32.060]   did you receive?
[02:44:32.060 --> 02:44:34.260]   Like, hey, do you think this is too long?
[02:44:34.260 --> 02:44:36.240]   Or do you think the title, like title,
[02:44:36.240 --> 02:44:38.220]   how much choice do you have in the title,
[02:44:38.220 --> 02:44:40.460]   in the cover, in the presentation,
[02:44:40.460 --> 02:44:41.940]   and the branding and all that kind of stuff?
[02:44:41.940 --> 02:44:43.820]   - Yeah, I mean, all of it depends, right?
[02:44:43.820 --> 02:44:47.260]   So when it comes on the relationship
[02:44:47.260 --> 02:44:48.500]   on the, with the editor on the writing,
[02:44:48.500 --> 02:44:51.620]   it depends on the editor and it depends on you.
[02:44:51.620 --> 02:44:54.100]   So like at this point, I'm on my seventh book
[02:44:54.100 --> 02:44:56.860]   and I write for a lot of major publications.
[02:44:56.860 --> 02:45:00.180]   And at this point I have what I feel like is a voice
[02:45:00.180 --> 02:45:01.900]   that I've, and a level of craft
[02:45:01.900 --> 02:45:03.860]   that I'm very comfortable with, right?
[02:45:03.860 --> 02:45:07.060]   So my editor is not gonna be, she kind of is gonna trust me
[02:45:07.060 --> 02:45:08.300]   and it's gonna be more big picture.
[02:45:08.300 --> 02:45:10.860]   Like I'm losing the thread here
[02:45:10.860 --> 02:45:12.820]   or this seems like it could be longer.
[02:45:12.820 --> 02:45:15.380]   Whereas the first book I wrote when I was 21,
[02:45:15.380 --> 02:45:19.500]   I had notes such as, you start a lot of sentences with so,
[02:45:19.500 --> 02:45:20.940]   you don't use any contractions
[02:45:20.940 --> 02:45:22.340]   because I've been doing scientific writing.
[02:45:22.340 --> 02:45:23.180]   We don't use contractions.
[02:45:23.180 --> 02:45:25.220]   Like you should probably use contractions.
[02:45:25.220 --> 02:45:26.940]   I think it was way more, you know,
[02:45:26.940 --> 02:45:29.660]   I had to go back and rewrite the whole thing, yeah.
[02:45:29.660 --> 02:45:31.980]   - But ultimately the recommendation,
[02:45:31.980 --> 02:45:33.740]   I mean, we talked offline and sort of,
[02:45:33.740 --> 02:45:36.860]   I was thinking loosely, not really sure,
[02:45:36.860 --> 02:45:37.940]   but I was thinking of writing a book
[02:45:37.940 --> 02:45:40.620]   and there's a kind of desire to go self-publishing,
[02:45:40.620 --> 02:45:42.060]   not for financial reasons.
[02:45:42.060 --> 02:45:43.380]   - And the money can be good by the way, right?
[02:45:43.380 --> 02:45:48.100]   I mean, it's very power law type distributed, right?
[02:45:48.100 --> 02:45:49.820]   So the money on a hardcover is somewhere
[02:45:49.820 --> 02:45:52.020]   between one or $2 a book.
[02:45:52.020 --> 02:45:53.860]   - So the thing is, I personally don't-
[02:45:53.860 --> 02:45:56.060]   - But then you give up 15% to the agent, so.
[02:45:56.060 --> 02:45:57.420]   - I personally don't care about money
[02:45:57.420 --> 02:45:58.620]   as I've mentioned before,
[02:45:58.620 --> 02:46:02.220]   but I, for some reason, really don't like spending money
[02:46:02.220 --> 02:46:05.800]   on things that are not worth it.
[02:46:05.800 --> 02:46:08.060]   Like I don't care if I get money,
[02:46:08.060 --> 02:46:10.020]   I just don't like spending money on,
[02:46:10.020 --> 02:46:12.820]   like feeding a system that's inefficient.
[02:46:12.820 --> 02:46:14.460]   It's like I'm contributing to the problem.
[02:46:14.460 --> 02:46:16.100]   That's my biggest problem.
[02:46:16.100 --> 02:46:17.620]   - Right, so you think that you're worried
[02:46:17.620 --> 02:46:19.820]   about the inefficiencies of the-
[02:46:19.820 --> 02:46:20.660]   - Yeah, the fact that-
[02:46:20.660 --> 02:46:23.220]   - Like the overheads, the number of people involved or-
[02:46:23.220 --> 02:46:26.120]   - The overhead, the emails again,
[02:46:26.120 --> 02:46:30.300]   the fact that they have this way of speaking,
[02:46:30.300 --> 02:46:32.340]   which I'm allergic to many people,
[02:46:32.340 --> 02:46:34.640]   like that's very marketing speak.
[02:46:34.640 --> 02:46:36.780]   Like you could tell they've been having Zoom meetings
[02:46:36.780 --> 02:46:37.780]   all day.
[02:46:37.780 --> 02:46:42.780]   It's like, as opposed to a sort of creative collaborators
[02:46:42.780 --> 02:46:45.700]   that are like also a little bit crazy.
[02:46:45.700 --> 02:46:46.540]   - Yeah, well-
[02:46:46.540 --> 02:46:48.160]   - And I suppose some of that is finding the right people.
[02:46:48.160 --> 02:46:49.540]   - Finding the right people, that's what I would say.
[02:46:49.540 --> 02:46:52.500]   I say there's definitely, and maybe it's just good fortune,
[02:46:52.500 --> 02:46:55.140]   good fortune in terms of like my agents
[02:46:55.140 --> 02:46:55.980]   and editors I've worked with,
[02:46:55.980 --> 02:47:00.980]   there's really good people who see the vision,
[02:47:00.980 --> 02:47:02.660]   are smart, are incredibly literary.
[02:47:02.660 --> 02:47:03.500]   - And they actually help you.
[02:47:03.500 --> 02:47:04.340]   - Yeah, and like let's-
[02:47:04.340 --> 02:47:05.180]   - Basically.
[02:47:05.180 --> 02:47:06.820]   - Yeah, I had a great editor when I was first moving
[02:47:06.820 --> 02:47:08.740]   into hardcover books, for example.
[02:47:08.740 --> 02:47:12.260]   It was my first big book advance
[02:47:12.260 --> 02:47:14.660]   and my first sort of big deal.
[02:47:14.660 --> 02:47:19.180]   And he was like a senior editor and it was very useful.
[02:47:19.180 --> 02:47:21.340]   He was like, we had a lot of long talks, right?
[02:47:21.340 --> 02:47:23.140]   I was, so this was my fourth book,
[02:47:23.140 --> 02:47:23.980]   "So Good They Can't Ignore You"
[02:47:23.980 --> 02:47:26.600]   was my first big hardcover idea book.
[02:47:26.600 --> 02:47:29.260]   And we had a lot of talks,
[02:47:29.260 --> 02:47:31.020]   like even before I started writing it,
[02:47:31.020 --> 02:47:34.020]   just let's talk about books and his philosophy.
[02:47:34.020 --> 02:47:35.540]   He'd been in the business for a long time.
[02:47:35.540 --> 02:47:37.020]   He was the head of the imprint.
[02:47:37.020 --> 02:47:38.420]   It was useful.
[02:47:38.420 --> 02:47:40.980]   - Yeah, but I mean, the other frustrating thing
[02:47:40.980 --> 02:47:42.540]   is how long the whole thing takes.
[02:47:42.540 --> 02:47:44.220]   - Takes a long time.
[02:47:44.220 --> 02:47:46.300]   - Yeah, I suppose that's, you just have to accept that.
[02:47:46.300 --> 02:47:48.020]   - Well, yeah, I handed in this manuscript
[02:47:48.020 --> 02:47:50.380]   for the book that comes out now,
[02:47:50.380 --> 02:47:53.580]   like when this, I handed it in, I mean, over the summer,
[02:47:53.580 --> 02:47:54.420]   like during the pandemic.
[02:47:54.420 --> 02:47:56.420]   So it's not terrible, right?
[02:47:56.420 --> 02:47:57.940]   And we were editing during the pandemic
[02:47:57.940 --> 02:48:00.460]   and I finished it in the spring.
[02:48:00.460 --> 02:48:02.260]   - We've talked most of today,
[02:48:02.260 --> 02:48:03.620]   except for a little bit computer science,
[02:48:03.620 --> 02:48:05.560]   most of today about a productive life.
[02:48:05.560 --> 02:48:11.200]   How does love, friendship, and family fit into that?
[02:48:11.200 --> 02:48:16.200]   Is there, do you find that there's a tension?
[02:48:16.200 --> 02:48:19.660]   Is it possible for relationships
[02:48:19.660 --> 02:48:22.500]   to energize the whole process, to benefit?
[02:48:22.500 --> 02:48:25.460]   Or is it ultimately a trade-off?
[02:48:25.460 --> 02:48:27.540]   But because life is short
[02:48:27.540 --> 02:48:32.540]   and ultimately we seek happiness, not productivity,
[02:48:32.820 --> 02:48:35.060]   that we have to accept that tension.
[02:48:35.060 --> 02:48:39.220]   - Yeah, I mean, I think relationships is the,
[02:48:39.220 --> 02:48:41.540]   that's the whole deal.
[02:48:41.540 --> 02:48:43.780]   Like I thought about this the other day,
[02:48:43.780 --> 02:48:44.700]   I don't know what the context was.
[02:48:44.700 --> 02:48:46.180]   I was thinking about if I was gonna give
[02:48:46.180 --> 02:48:49.060]   like an advice speech, like a commencement address,
[02:48:49.060 --> 02:48:51.700]   or like giving advice to young people.
[02:48:51.700 --> 02:48:55.260]   And like the big question I have for young people
[02:48:55.260 --> 02:48:58.600]   is if they haven't already, bad things are gonna happen
[02:48:58.600 --> 02:49:01.820]   that you don't control, so what's the plan, right?
[02:49:01.820 --> 02:49:03.580]   Like, let's start figuring that out now
[02:49:03.580 --> 02:49:05.620]   because it's not all good.
[02:49:05.620 --> 02:49:07.460]   Some people get off better than others,
[02:49:07.460 --> 02:49:09.980]   but eventually stuff happens, right?
[02:49:09.980 --> 02:49:11.900]   You get sick, something falls apart,
[02:49:11.900 --> 02:49:16.020]   the economy craters, someone you know dies,
[02:49:16.020 --> 02:49:19.020]   like all sorts of bad stuff is gonna happen, right?
[02:49:19.020 --> 02:49:20.660]   So how are we gonna do this?
[02:49:20.660 --> 02:49:23.100]   Like, how do we like live life when life is hard?
[02:49:23.100 --> 02:49:25.780]   And in ways that is unfair and unpredictable.
[02:49:25.780 --> 02:49:27.860]   Then relationships is the,
[02:49:27.860 --> 02:49:29.560]   that's the buffer for all of that.
[02:49:29.560 --> 02:49:31.540]   'Cause we're wired for it, right?
[02:49:31.540 --> 02:49:34.180]   I went down this rabbit hole with digital minimalism.
[02:49:34.180 --> 02:49:35.460]   I went down this huge rabbit hole
[02:49:35.460 --> 02:49:38.820]   about the human brain and sociality.
[02:49:38.820 --> 02:49:41.300]   It's all rewired, it's like all of our brain is for this.
[02:49:41.300 --> 02:49:43.380]   Like everything, all of our mechanisms,
[02:49:43.380 --> 02:49:46.420]   everything is made to service social connections
[02:49:46.420 --> 02:49:47.460]   because it's what kept you alive.
[02:49:47.460 --> 02:49:49.700]   You know, I mean, you had your tribal connections
[02:49:49.700 --> 02:49:52.060]   is how you didn't starve during a famine,
[02:49:52.060 --> 02:49:53.820]   people would share food, et cetera.
[02:49:53.820 --> 02:49:57.180]   And so you can't neglect that, and it's like everything.
[02:49:57.180 --> 02:49:58.380]   And people feel it, right?
[02:49:58.380 --> 02:50:00.060]   Like there's no, our social networks
[02:50:00.060 --> 02:50:01.740]   are hooked up to the pain center.
[02:50:01.740 --> 02:50:04.020]   It's why it feels so terrible when you miss someone
[02:50:04.020 --> 02:50:05.340]   or like someone dies or something, right?
[02:50:05.340 --> 02:50:07.700]   That's like how seriously we take it.
[02:50:07.700 --> 02:50:09.300]   There's a pretty accepted theory
[02:50:09.300 --> 02:50:10.520]   that the default mode network,
[02:50:10.520 --> 02:50:12.580]   like a lot of what the default mode network is doing.
[02:50:12.580 --> 02:50:15.020]   So the sort of the default state our brain goes into
[02:50:15.020 --> 02:50:16.300]   when we're not doing something in particular
[02:50:16.300 --> 02:50:19.580]   is practicing sociality, practicing interactions,
[02:50:19.580 --> 02:50:22.580]   because it's so crucial to what we do.
[02:50:22.580 --> 02:50:25.140]   It's like at the core of human thriving.
[02:50:25.140 --> 02:50:27.100]   So I've, more recently, the way I think about it
[02:50:27.100 --> 02:50:28.860]   is like relationships first.
[02:50:28.860 --> 02:50:31.620]   Like, okay, given that foundation of putting like,
[02:50:31.620 --> 02:50:33.220]   and I don't think we put nearly enough time into it.
[02:50:33.220 --> 02:50:36.100]   I worry that social media is reducing relationships,
[02:50:36.100 --> 02:50:37.460]   strong relationships.
[02:50:37.460 --> 02:50:39.180]   Strong relationships where you're sacrificing
[02:50:39.180 --> 02:50:42.140]   non-trivial time and attention,
[02:50:42.140 --> 02:50:44.620]   resources, whatever, on behalf of other people.
[02:50:44.620 --> 02:50:46.460]   That's the net that is gonna allow you
[02:50:46.460 --> 02:50:48.260]   to get through anything.
[02:50:48.260 --> 02:50:51.060]   Then, all right, now what do we wanna do
[02:50:51.060 --> 02:50:53.580]   with the surplus that remains?
[02:50:53.580 --> 02:50:55.940]   May I wanna build some fire, build some tools?
[02:50:55.940 --> 02:50:57.740]   - So put relationships first.
[02:50:57.740 --> 02:50:59.300]   I like the worst case analysis
[02:50:59.300 --> 02:51:01.220]   from the computer science perspective.
[02:51:01.220 --> 02:51:03.420]   Put relationships first.
[02:51:03.420 --> 02:51:05.260]   Yeah, because everything else is just
[02:51:05.260 --> 02:51:08.700]   assuming average case,
[02:51:08.700 --> 02:51:10.780]   assuming things kind of keep going as they were going.
[02:51:10.780 --> 02:51:13.420]   - And you're neglecting the fundamental human drive.
[02:51:13.420 --> 02:51:15.380]   Like we have this, we talk about the boredom instinct.
[02:51:15.380 --> 02:51:17.020]   We wanna build things, we wanna have impact,
[02:51:17.020 --> 02:51:17.860]   we wanna do productivity.
[02:51:17.860 --> 02:51:21.460]   That's not nearly as clear cut of a drive of we need people.
[02:51:21.460 --> 02:51:25.660]   - But if we look at the real worst case analysis here
[02:51:25.660 --> 02:51:28.940]   is one day, you're pretty young now,
[02:51:28.940 --> 02:51:31.900]   but that's not gonna last very long.
[02:51:31.900 --> 02:51:33.060]   You're gonna die one day.
[02:51:33.060 --> 02:51:35.260]   Is that something you think about?
[02:51:35.260 --> 02:51:36.420]   - Little bit.
[02:51:36.420 --> 02:51:37.660]   - Are you afraid of death?
[02:51:37.660 --> 02:51:39.460]   - Well, I'm of the mindset of,
[02:51:39.460 --> 02:51:41.980]   let's make that a productivity hack.
[02:51:41.980 --> 02:51:43.260]   I'm of the mindset of,
[02:51:43.260 --> 02:51:47.100]   we need to confront that soon.
[02:51:47.100 --> 02:51:48.700]   So let's do what we can now
[02:51:48.700 --> 02:51:50.620]   so that when we really confront and think about it,
[02:51:50.620 --> 02:51:52.780]   we're more likely to feel better about it.
[02:51:52.780 --> 02:51:54.700]   So in other words, like let's focus now
[02:51:54.700 --> 02:51:57.020]   on living and doing things in such a way
[02:51:57.020 --> 02:51:58.660]   that we're proud of,
[02:51:58.660 --> 02:52:00.740]   so that when it really comes time to confront that,
[02:52:00.740 --> 02:52:02.900]   we're more likely to say like,
[02:52:02.900 --> 02:52:05.100]   okay, I feel kind of good about the situation.
[02:52:05.100 --> 02:52:07.580]   - So what, when you're laying in your deathbed,
[02:52:07.580 --> 02:52:09.700]   would you, in looking back,
[02:52:09.700 --> 02:52:11.060]   what would make you think like,
[02:52:11.060 --> 02:52:13.900]   oh, I did okay, I'm proud of that.
[02:52:13.900 --> 02:52:15.900]   I optimized the hell out of that.
[02:52:15.900 --> 02:52:17.180]   - That's a good, I mean, it's a good question
[02:52:17.180 --> 02:52:19.940]   to go backwards on.
[02:52:19.940 --> 02:52:24.140]   I mean, this is like David Brooks's eulogy,
[02:52:24.140 --> 02:52:26.980]   virtues versus resume virtues, right?
[02:52:26.980 --> 02:52:28.900]   So his argument is that,
[02:52:28.900 --> 02:52:30.700]   and that's another interesting DC area person.
[02:52:30.700 --> 02:52:32.940]   I keep thinking of interesting DC area people.
[02:52:32.940 --> 02:52:34.740]   All right, David Brooks is here too.
[02:52:34.740 --> 02:52:38.300]   His argument, he thinks eulogy virtues is,
[02:52:38.300 --> 02:52:39.740]   so what we eulogize is different
[02:52:39.740 --> 02:52:42.060]   than what we promote on the resume.
[02:52:42.060 --> 02:52:44.100]   That's his whole thing now, right?
[02:52:44.100 --> 02:52:45.980]   His second mountain, the road to character,
[02:52:45.980 --> 02:52:47.460]   both these books are,
[02:52:47.460 --> 02:52:48.700]   he has this whole premise
[02:52:48.700 --> 02:52:50.100]   of there's like this professional phase
[02:52:50.100 --> 02:52:53.260]   and there's a phase of giving of yourself
[02:52:53.260 --> 02:52:55.700]   and sacrificing on behalf of other people.
[02:52:55.700 --> 02:52:57.260]   I don't know, maybe it's all mixed together, right?
[02:52:57.260 --> 02:53:00.260]   You wanna, I think living by a code is important, right?
[02:53:00.260 --> 02:53:03.140]   I mean, this is something that's not emphasized enough.
[02:53:03.140 --> 02:53:03.980]   I always think of advice
[02:53:03.980 --> 02:53:05.700]   that my undergrad should be given,
[02:53:05.700 --> 02:53:06.660]   that they're not given,
[02:53:06.660 --> 02:53:07.740]   especially at a place like Georgetown
[02:53:07.740 --> 02:53:10.260]   that has this like deep history of,
[02:53:10.260 --> 02:53:11.780]   you know, trying to promote human flourishing
[02:53:11.780 --> 02:53:13.420]   because of the Jesuit connection.
[02:53:13.420 --> 02:53:19.300]   There's such resiliency and pride
[02:53:19.300 --> 02:53:21.420]   that comes out of living well,
[02:53:21.420 --> 02:53:22.260]   even when it's hard,
[02:53:22.260 --> 02:53:23.540]   living according to a code,
[02:53:23.540 --> 02:53:25.220]   living accord to, which, you know,
[02:53:25.220 --> 02:53:28.540]   I think religion used to structure this for people,
[02:53:28.540 --> 02:53:30.260]   but in its absence, you need some sort of replacement,
[02:53:30.260 --> 02:53:31.900]   but this, even when things weren't,
[02:53:31.900 --> 02:53:33.540]   soldiers get this a lot, right?
[02:53:33.540 --> 02:53:34.380]   They experience this a lot.
[02:53:34.380 --> 02:53:35.220]   Even when things were tough,
[02:53:35.220 --> 02:53:36.780]   I was able to persist in living in this way
[02:53:36.780 --> 02:53:37.620]   that I knew was right,
[02:53:37.620 --> 02:53:38.500]   even though it wasn't the easiest thing
[02:53:38.500 --> 02:53:39.340]   to do in the moment.
[02:53:39.340 --> 02:53:42.060]   Like fewer things give humans more resiliency.
[02:53:42.060 --> 02:53:43.980]   It's like having done that,
[02:53:43.980 --> 02:53:45.740]   your relationships were strong, right?
[02:53:45.740 --> 02:53:47.500]   Many people coming to your funeral is a standard.
[02:53:47.500 --> 02:53:48.900]   Like a lot of people are gonna come to your funeral,
[02:53:48.900 --> 02:53:51.180]   like that means you matter to a lot of people.
[02:53:51.180 --> 02:53:52.380]   And then maybe having done,
[02:53:52.380 --> 02:53:55.660]   to the extent of whatever capabilities
[02:53:55.660 --> 02:53:57.860]   you happen to be granted, you know,
[02:53:57.860 --> 02:53:59.420]   and they're different for different people,
[02:53:59.420 --> 02:54:00.740]   some people can sprint real fast
[02:54:00.740 --> 02:54:02.860]   and some people can do math problems,
[02:54:02.860 --> 02:54:04.860]   try to actually do something of impact.
[02:54:04.860 --> 02:54:08.540]   - I'll just promise to give gift cards
[02:54:08.540 --> 02:54:10.220]   to anybody who shows up to the funeral.
[02:54:10.220 --> 02:54:11.380]   - You're gonna hack it?
[02:54:11.380 --> 02:54:12.900]   - I'm gonna hack even the funeral.
[02:54:12.900 --> 02:54:14.740]   - There's gonna be a lottery wheel you spin
[02:54:14.740 --> 02:54:17.940]   when you come in and someone goes away with $10,000.
[02:54:17.940 --> 02:54:19.020]   - See, the problem is like,
[02:54:19.340 --> 02:54:22.180]   with all this, the living by principles,
[02:54:22.180 --> 02:54:24.300]   living a principle life, focusing on relationships
[02:54:24.300 --> 02:54:27.780]   and kind of thinking of this life as this perfect thing
[02:54:27.780 --> 02:54:30.220]   kind of forgets the notion that none of it,
[02:54:30.220 --> 02:54:34.060]   you know, makes any sense, right?
[02:54:34.060 --> 02:54:37.980]   Like the, like it kind of implies
[02:54:37.980 --> 02:54:40.340]   that this is like a video game
[02:54:40.340 --> 02:54:42.420]   and you wanna get a high score,
[02:54:42.420 --> 02:54:45.820]   as opposed to none of this even makes sense.
[02:54:45.820 --> 02:54:47.940]   Like why would he, like what the?
[02:54:47.940 --> 02:54:49.780]   (laughing)
[02:54:49.780 --> 02:54:52.700]   Like what does it even mean to die?
[02:54:52.700 --> 02:54:53.740]   It's gonna be over.
[02:54:53.740 --> 02:54:57.820]   It's like everything I do, all of these productivity hacks,
[02:54:57.820 --> 02:55:00.180]   all this life, all these efforts, all this creative efforts,
[02:55:00.180 --> 02:55:02.220]   kind of assume it's gonna go on forever.
[02:55:02.220 --> 02:55:05.180]   There's a kind of a sense of immortality
[02:55:05.180 --> 02:55:06.700]   and I don't even know how to intellectually
[02:55:06.700 --> 02:55:08.380]   make sense that it ends.
[02:55:08.380 --> 02:55:10.820]   Of course, gotta ask you in that context,
[02:55:10.820 --> 02:55:13.220]   what do you think is the meaning of it all?
[02:55:13.220 --> 02:55:14.780]   Especially for a computer scientist,
[02:55:14.780 --> 02:55:17.420]   I mean, there's gotta be some mathematical--
[02:55:17.420 --> 02:55:19.740]   - Yeah, 27 or what's the--
[02:55:19.740 --> 02:55:21.300]   - What's the Douglas Adams?
[02:55:21.300 --> 02:55:23.420]   - Yeah, 42, okay.
[02:55:23.420 --> 02:55:24.820]   - 27 is a better number.
[02:55:24.820 --> 02:55:26.700]   - I should read more sci-fi.
[02:55:26.700 --> 02:55:28.780]   - Maybe you're onto something with a 27.
[02:55:28.780 --> 02:55:30.020]   - I don't wanna give away too much,
[02:55:30.020 --> 02:55:31.940]   but just trust me, 27.
[02:55:31.940 --> 02:55:33.180]   - It's invisible, yeah.
[02:55:33.180 --> 02:55:37.060]   - So, I mean, I don't know, obviously, right?
[02:55:37.060 --> 02:55:38.060]   I mean, I'm a--
[02:55:38.060 --> 02:55:38.900]   - So be it.
[02:55:38.900 --> 02:55:41.860]   - Yeah, I don't know, but going back to what you were saying
[02:55:41.860 --> 02:55:43.460]   about the sort of the existentialist
[02:55:43.460 --> 02:55:47.140]   or sort of the more nihilist style approach,
[02:55:47.140 --> 02:55:50.460]   the one thing that there is are intimations, right?
[02:55:50.460 --> 02:55:53.700]   So there's these intimations that human have
[02:55:53.700 --> 02:55:56.540]   of somehow this feels right and this feels wrong,
[02:55:56.540 --> 02:55:59.180]   this feels good, this feels like I'm doing,
[02:55:59.180 --> 02:56:01.020]   I'm aligned with something, you know,
[02:56:01.020 --> 02:56:03.140]   when I'm acting with courage to save whatever, right?
[02:56:03.140 --> 02:56:05.540]   It's not, these intimations are a grounding
[02:56:05.540 --> 02:56:07.180]   against arbitrariness.
[02:56:07.180 --> 02:56:09.820]   Like one of the ideas I'm really interested in is that
[02:56:09.820 --> 02:56:13.500]   when you look at religion, right?
[02:56:13.500 --> 02:56:15.820]   So I'm interested in world religions.
[02:56:15.820 --> 02:56:18.380]   My grandfather was like a theologian
[02:56:18.380 --> 02:56:19.540]   that studied and wrote all these books,
[02:56:19.540 --> 02:56:21.420]   and I'm very interested in this type of stuff.
[02:56:21.420 --> 02:56:24.620]   And there's this great book that's,
[02:56:24.620 --> 02:56:27.580]   it's not specific to a particular religion,
[02:56:27.580 --> 02:56:29.300]   but it's Karen Armstrong wrote this great book
[02:56:29.300 --> 02:56:30.900]   called "The Case for God."
[02:56:30.900 --> 02:56:31.740]   She's very interesting.
[02:56:31.740 --> 02:56:34.700]   She was a Catholic nun who sort of left that religion,
[02:56:34.700 --> 02:56:36.780]   but one of the smartest thinkers
[02:56:36.780 --> 02:56:40.300]   in terms of like accessible theological thinking
[02:56:40.300 --> 02:56:43.020]   that's not tied to any particular religion.
[02:56:43.020 --> 02:56:45.540]   Her whole argument is that the way to understand religion,
[02:56:45.540 --> 02:56:47.660]   you first of all, you have to go way back pre-enlightenment
[02:56:47.660 --> 02:56:48.540]   where all this was formed.
[02:56:48.540 --> 02:56:51.140]   We got messed up thinking about religion post-enlightenment,
[02:56:51.140 --> 02:56:51.980]   right?
[02:56:51.980 --> 02:56:54.780]   And these were operating systems
[02:56:54.780 --> 02:56:56.420]   for making sense of intimations.
[02:56:56.420 --> 02:57:00.340]   The one thing we had were these different intimations
[02:57:00.340 --> 02:57:03.860]   of this feel like awe and mystical experience.
[02:57:03.860 --> 02:57:05.980]   And this feels, there's something you feel
[02:57:05.980 --> 02:57:07.300]   when you act in a certain way
[02:57:07.300 --> 02:57:08.860]   and don't act in this other way.
[02:57:08.860 --> 02:57:12.060]   And it was like the scientists who were trying to study
[02:57:12.060 --> 02:57:13.620]   and understand the model of the atom
[02:57:13.620 --> 02:57:15.420]   by just looking at experiments
[02:57:15.420 --> 02:57:16.700]   and trying to understand what's going on.
[02:57:16.700 --> 02:57:18.420]   Like the great religions of the world
[02:57:18.420 --> 02:57:19.900]   were basically figuring out
[02:57:19.900 --> 02:57:21.460]   how do we make sense of these intimations
[02:57:21.460 --> 02:57:23.100]   and live in alignment with them
[02:57:23.100 --> 02:57:24.980]   and build a life of meaning around that?
[02:57:24.980 --> 02:57:26.340]   What were the tools they were using?
[02:57:26.340 --> 02:57:28.260]   They were using ritual, they were using belief,
[02:57:28.260 --> 02:57:29.620]   they were using action.
[02:57:29.620 --> 02:57:31.220]   But all of it was like an OS.
[02:57:31.220 --> 02:57:33.900]   It was like a liturgical model of the atom.
[02:57:33.900 --> 02:57:36.020]   - It's hard coded in.
[02:57:36.020 --> 02:57:39.300]   So it did through the evolutionary process.
[02:57:39.300 --> 02:57:41.460]   - I mean, they wouldn't have called it that back then.
[02:57:41.900 --> 02:57:45.500]   Yeah, I mean, they didn't have that as pre-enlightenment.
[02:57:45.500 --> 02:57:46.980]   They just said this is here.
[02:57:46.980 --> 02:57:50.940]   And the directive is to try to live in alignment with that.
[02:57:50.940 --> 02:57:53.460]   - Well, then I wanna ask who wrote the original code.
[02:57:53.460 --> 02:57:54.980]   - Yeah, so-- - That's the open question.
[02:57:54.980 --> 02:57:56.820]   - Yeah, so Armstrong lays out this good argument.
[02:57:56.820 --> 02:57:58.260]   And where it gets really interesting
[02:57:58.260 --> 02:58:00.740]   is that she emphasizes that
[02:58:00.740 --> 02:58:03.260]   all of this was considered ineffable, right?
[02:58:03.260 --> 02:58:04.500]   So the whole notion,
[02:58:04.500 --> 02:58:06.580]   and this is like rich in Jewish tradition in particular
[02:58:06.580 --> 02:58:08.260]   and also in Islamic tradition,
[02:58:08.340 --> 02:58:11.540]   we can't comprehend and understand what's going on here.
[02:58:11.540 --> 02:58:14.020]   And so the best we can do to approximate understanding
[02:58:14.020 --> 02:58:17.020]   and live in alignment is we act as if this is true,
[02:58:17.020 --> 02:58:20.020]   do these rituals, have these actions or whatever.
[02:58:20.020 --> 02:58:21.900]   Post-enlightenment, a lot of that got,
[02:58:21.900 --> 02:58:24.940]   once we learned about enlightenment,
[02:58:24.940 --> 02:58:26.780]   we grew these suspicions around religion
[02:58:26.780 --> 02:58:28.860]   that are very much of the modern era, right?
[02:58:28.860 --> 02:58:30.860]   So like to Karen Armstrong,
[02:58:30.860 --> 02:58:33.740]   like Sam Harris's critique of religion makes no sense.
[02:58:33.740 --> 02:58:36.100]   The critique's based on, well, this is,
[02:58:36.100 --> 02:58:37.700]   you're making the ascent to propositions
[02:58:37.700 --> 02:58:38.660]   that you think are true for which
[02:58:38.660 --> 02:58:39.900]   you do not have evidence that they are true.
[02:58:39.900 --> 02:58:41.500]   She's like, that's an enlightenment thing, right?
[02:58:41.500 --> 02:58:43.660]   This is not the context and this is not,
[02:58:43.660 --> 02:58:46.740]   the religion is the Rutherford model of the atom.
[02:58:46.740 --> 02:58:50.300]   Like it's not actually maybe what is underneath happening,
[02:58:50.300 --> 02:58:52.660]   but this model explains why your chemical equations work.
[02:58:52.660 --> 02:58:54.980]   And so this is like the way religion was.
[02:58:54.980 --> 02:58:57.140]   There's a God, we'll call it this, this is how it works.
[02:58:57.140 --> 02:58:59.500]   We do this ritual, we act in this way, it aligns with it.
[02:58:59.500 --> 02:59:02.020]   Just like the model of the atom predicted why,
[02:59:02.020 --> 02:59:03.580]   Na and Cl is gonna become salt,
[02:59:03.580 --> 02:59:04.940]   this predicts that you're gonna feel
[02:59:04.940 --> 02:59:05.780]   and live in alignment, right?
[02:59:06.060 --> 02:59:08.340]   It's like this beautiful, sophisticated theory,
[02:59:08.340 --> 02:59:11.060]   which actually matches how a lot of great theologians
[02:59:11.060 --> 02:59:13.540]   have thought about it.
[02:59:13.540 --> 02:59:14.860]   But then when you come forward in time,
[02:59:14.860 --> 02:59:15.820]   yeah, maybe it's evolution.
[02:59:15.820 --> 02:59:18.380]   I mean, this is like what Peterson hints at, right?
[02:59:18.380 --> 02:59:23.380]   Like he's basically, he doesn't like to get
[02:59:23.380 --> 02:59:26.660]   super pinned down on this, but it kind of seems where he--
[02:59:26.660 --> 02:59:29.100]   - He's almost like searching for the words.
[02:59:29.100 --> 02:59:30.980]   - He focuses more on like Jung and other people,
[02:59:30.980 --> 02:59:32.860]   but I mean, I know he's very Jungian,
[02:59:33.140 --> 02:59:35.820]   but that same type of analysis, I think, roughly speaking,
[02:59:35.820 --> 02:59:37.780]   like Armstrong is sort of a,
[02:59:37.780 --> 02:59:39.380]   it's kind of like a Petersonian analysis,
[02:59:39.380 --> 02:59:42.700]   but she's looking more at the deep history of religion.
[02:59:42.700 --> 02:59:46.540]   But yeah, he throws in an evolutionary aspect.
[02:59:46.540 --> 02:59:48.500]   - And I wonder what home it finds.
[02:59:48.500 --> 02:59:51.860]   I wonder what the new home is if religion dissipates,
[02:59:51.860 --> 02:59:54.100]   what the new home for these kinds of
[02:59:54.100 --> 02:59:55.580]   natural inclinations are.
[02:59:55.580 --> 02:59:57.500]   - Yeah.
[02:59:57.500 --> 02:59:58.780]   - Whether it's technology, whether--
[02:59:58.780 --> 02:59:59.820]   - And if it's evolution, I mean,
[02:59:59.820 --> 03:00:01.500]   this is Francis Collins' book also.
[03:00:01.500 --> 03:00:04.420]   He's like, well, that's a religious,
[03:00:04.420 --> 03:00:06.500]   that could be a very religious notion.
[03:00:06.500 --> 03:00:07.580]   I don't, I think this stuff is interesting.
[03:00:07.580 --> 03:00:08.700]   I'm not a very religious person,
[03:00:08.700 --> 03:00:11.900]   but I'm thinking it's not a bad idea.
[03:00:11.900 --> 03:00:13.500]   Maybe what replaces, honestly,
[03:00:13.500 --> 03:00:17.220]   like maybe what replaces religion is a return to religion,
[03:00:17.220 --> 03:00:20.060]   but in this sort of more sophisticated,
[03:00:20.060 --> 03:00:22.020]   I mean, if you went back, yeah, I mean,
[03:00:22.020 --> 03:00:24.860]   it's the issue with like a lot of the recent critiques,
[03:00:24.860 --> 03:00:29.460]   I think it's a stronger critique in a complicated way,
[03:00:29.460 --> 03:00:31.060]   right, because the whole way these,
[03:00:31.060 --> 03:00:33.260]   the way this works, I mean, the theologians,
[03:00:33.260 --> 03:00:35.940]   if you're reading Paul Tillich, if you're reading Heschel,
[03:00:35.940 --> 03:00:36.980]   if you're reading these people,
[03:00:36.980 --> 03:00:38.620]   they're thinking very sophisticatedly
[03:00:38.620 --> 03:00:39.860]   about religion in terms of this.
[03:00:39.860 --> 03:00:42.020]   It's ineffable and we're just these things
[03:00:42.020 --> 03:00:44.220]   and this is as deep, it connects us to these things
[03:00:44.220 --> 03:00:46.060]   in a way that puts life in alignment.
[03:00:46.060 --> 03:00:47.180]   We can't really explain what's going on
[03:00:47.180 --> 03:00:50.060]   because our brains can't handle it, right?
[03:00:50.060 --> 03:00:52.380]   For the average person though,
[03:00:52.380 --> 03:00:55.420]   this notion of live as if is kind of how religions work.
[03:00:55.420 --> 03:00:57.620]   Is live as if this is true.
[03:00:57.620 --> 03:01:00.140]   It's like an OS for getting in alignment with,
[03:01:00.140 --> 03:01:02.780]   because through cultural evolution,
[03:01:02.780 --> 03:01:04.220]   like you behave in this way, do these ritual,
[03:01:04.220 --> 03:01:06.300]   live as if this is true,
[03:01:06.300 --> 03:01:10.140]   gives you the goal you're looking for.
[03:01:10.140 --> 03:01:12.380]   But that's a complicated thing, live as if this is true,
[03:01:12.380 --> 03:01:15.020]   because if you, especially if you're not a theologian
[03:01:15.020 --> 03:01:18.620]   to say, yeah, this is not true in an enlightenment sense,
[03:01:18.620 --> 03:01:21.420]   but I'm living as if, it kind of takes the heat out of it.
[03:01:21.420 --> 03:01:22.820]   But of course it's what people are doing
[03:01:22.820 --> 03:01:25.660]   because highly religious people still do bad things
[03:01:25.660 --> 03:01:28.380]   where if you really were, there's absolutely a hell
[03:01:28.380 --> 03:01:30.100]   and I'm definitely gonna go to it if I do this bad thing,
[03:01:30.100 --> 03:01:33.140]   you would never have, no one would ever murder anyone
[03:01:33.140 --> 03:01:34.780]   if they were an evangelical Christian, right?
[03:01:34.780 --> 03:01:37.380]   So it's like what, this is kind of a tangent
[03:01:37.380 --> 03:01:39.540]   that I'm on shaky ground here,
[03:01:39.540 --> 03:01:43.300]   but it's something I've been interested off and on a lot.
[03:01:43.300 --> 03:01:44.220]   - Well, it's fascinating.
[03:01:44.220 --> 03:01:46.620]   I mean, I think we're in some sense searching for,
[03:01:46.620 --> 03:01:48.660]   'cause it does make for a good operating system.
[03:01:48.660 --> 03:01:52.060]   We're searching for a good live as if X is true
[03:01:52.060 --> 03:01:54.260]   and we're searching for a new X.
[03:01:54.260 --> 03:01:58.800]   And maybe artificial intelligence will be the very,
[03:01:58.800 --> 03:02:02.540]   the new gods that we're so desperately looking for.
[03:02:02.540 --> 03:02:04.000]   - Or it'll just spit out 42.
[03:02:04.000 --> 03:02:06.460]   - I thought it was 27.
[03:02:06.460 --> 03:02:09.800]   Cal, this is, as you know, I've been a huge fan.
[03:02:09.800 --> 03:02:14.700]   So are a huge number of people that I've spoken with.
[03:02:14.700 --> 03:02:17.380]   So they keep telling me, I absolutely have to talk to you.
[03:02:17.380 --> 03:02:18.380]   This was a huge honor.
[03:02:18.380 --> 03:02:19.220]   This was really fun.
[03:02:19.220 --> 03:02:20.700]   Thanks for wasting all this time with me.
[03:02:20.700 --> 03:02:21.540]   - Yeah, no, likewise, man.
[03:02:21.540 --> 03:02:22.440]   I've been a long time fan.
[03:02:22.440 --> 03:02:23.500]   So this was a lot of fun.
[03:02:23.500 --> 03:02:24.500]   - Yeah, thanks man.
[03:02:24.500 --> 03:02:27.060]   Thanks for listening to this conversation
[03:02:27.060 --> 03:02:28.140]   with Cal Newport.
[03:02:28.140 --> 03:02:31.860]   A thank you to our sponsors, ExpressVPN,
[03:02:31.860 --> 03:02:34.260]   Linode Linux Virtual Machines,
[03:02:34.260 --> 03:02:36.580]   Sun Basket Meal Delivery Service,
[03:02:36.580 --> 03:02:39.300]   and SimpliSafe Home Security.
[03:02:39.300 --> 03:02:42.180]   Click the sponsor links to get a discount
[03:02:42.180 --> 03:02:44.460]   and to support this podcast.
[03:02:44.460 --> 03:02:47.860]   And now let me leave you with some words from Cal himself.
[03:02:47.860 --> 03:02:49.700]   "Clarity about what matters
[03:02:49.700 --> 03:02:52.960]   "provides clarity about what does not."
[03:02:52.960 --> 03:02:55.800]   Thank you for listening and hope to see you next time.
[03:02:55.800 --> 03:02:58.380]   (upbeat music)
[03:02:58.380 --> 03:03:00.960]   (upbeat music)
[03:03:00.960 --> 03:03:10.960]   [BLANK_AUDIO]

