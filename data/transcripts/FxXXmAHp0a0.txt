
[00:00:00.000 --> 00:00:29.440]   Oh, we're live on YouTube already.
[00:00:29.480 --> 00:00:31.360]   I started the stream five minutes early.
[00:00:31.360 --> 00:00:34.000]   So I'm just going to wait for people to join.
[00:00:34.000 --> 00:00:35.960]   But hey, Vignesh, how are you?
[00:00:35.960 --> 00:00:45.200]   Sorry, I guess I was muted.
[00:00:45.200 --> 00:00:47.920]   Hi, I'm good.
[00:00:47.920 --> 00:00:49.640]   So how are you?
[00:00:49.640 --> 00:00:50.680]   How's the day going?
[00:00:50.680 --> 00:00:52.080]   I'm good as well.
[00:00:52.080 --> 00:00:52.520]   Thank you.
[00:00:52.520 --> 00:00:58.680]   I've done a mistake, which is I was trying to connect Zoom to
[00:00:58.680 --> 00:01:02.320]   YouTube and it started the live stream by default.
[00:01:02.320 --> 00:01:05.640]   So now I have to kill five minutes because I started the
[00:01:05.640 --> 00:01:06.560]   live stream early.
[00:01:06.560 --> 00:01:11.320]   Yeah, no problem.
[00:01:11.320 --> 00:01:13.840]   So we're going to wait for five minutes, right?
[00:01:13.840 --> 00:01:18.960]   Yeah, it's a mistake of mine that I started it early.
[00:01:18.960 --> 00:01:21.480]   So I just want people to join in and then start.
[00:01:21.480 --> 00:01:22.400]   Yeah, that's OK.
[00:01:25.960 --> 00:01:29.840]   Do you have any favorite papers of your own from NewLibs?
[00:01:29.840 --> 00:01:33.720]   Not particular.
[00:01:33.720 --> 00:01:36.840]   This is my first time watching this NewLibs.
[00:01:36.840 --> 00:01:40.200]   So I guess I'm going to pick favorites from this one.
[00:01:40.200 --> 00:01:49.200]   And yeah, I was watching like I used to follow the NewLibs.
[00:01:49.200 --> 00:01:52.200]   This year, I was quite far away from that.
[00:01:52.200 --> 00:01:54.600]   I'm looking forward to this season.
[00:01:54.640 --> 00:01:56.200]   What will be the top five papers?
[00:01:56.200 --> 00:01:59.200]   So how are we going to decide that five papers?
[00:01:59.200 --> 00:02:04.960]   Like, are we going to like select based on any particular
[00:02:04.960 --> 00:02:06.760]   topic or based on the...
[00:02:06.760 --> 00:02:12.240]   Yeah, so I've picked five papers that I found interesting and
[00:02:12.240 --> 00:02:14.120]   I'll be going through those.
[00:02:14.120 --> 00:02:19.400]   These are quite random, but it's just based on my personal
[00:02:19.400 --> 00:02:19.920]   interest.
[00:02:19.920 --> 00:02:22.000]   Yeah, that's OK.
[00:02:24.000 --> 00:02:28.440]   I also see the same question by Aman in the live chat.
[00:02:28.440 --> 00:02:32.840]   Hey, Aman, we're gonna go through a bunch of different
[00:02:32.840 --> 00:02:33.160]   papers.
[00:02:33.160 --> 00:02:34.640]   So there's not a single theme.
[00:02:34.640 --> 00:02:39.280]   I guess I'll just share in like two or three minutes.
[00:02:39.280 --> 00:02:41.440]   So please, please wait for two or three minutes.
[00:02:41.440 --> 00:02:47.080]   But in the meantime, for people joining on YouTube, because I
[00:02:47.080 --> 00:02:50.680]   see it's a larger number, does anyone want to share what
[00:02:51.160 --> 00:02:54.440]   papers have caught their eye from NeurIPS or any other papers
[00:02:54.440 --> 00:02:55.720]   that you all have been interested in?
[00:02:55.720 --> 00:03:05.720]   [BLANK_AUDIO]
[00:03:05.720 --> 00:03:15.720]   [BLANK_AUDIO]
[00:03:15.720 --> 00:03:27.720]   >> Hey, Ming Guo, how are you?
[00:03:27.720 --> 00:03:37.720]   [BLANK_AUDIO]
[00:03:37.720 --> 00:03:47.720]   [BLANK_AUDIO]
[00:03:47.720 --> 00:04:05.720]   Jeff says he's not looked at any papers from NeurIPS.
[00:04:05.720 --> 00:04:09.720]   So he's keen to check out what I've discovered.
[00:04:09.720 --> 00:04:12.720]   I can share the list right now while we wait.
[00:04:12.720 --> 00:04:18.720]   These are the five papers that I will try to summarize or basically walk through.
[00:04:18.720 --> 00:04:22.720]   And it'll be an open-ended discussion, so I invite any questions around it.
[00:04:22.720 --> 00:04:27.720]   [BLANK_AUDIO]
[00:04:27.720 --> 00:04:31.720]   Abranta mentions they're trying to get started with reading papers.
[00:04:31.720 --> 00:04:32.720]   That's awesome.
[00:04:32.720 --> 00:04:36.720]   So this will more be of a summary of different papers.
[00:04:36.720 --> 00:04:39.720]   So this isn't a guided read-through.
[00:04:39.720 --> 00:04:41.720]   This is to give you a taste of five different papers.
[00:04:41.720 --> 00:04:45.720]   And maybe in a future paper reading group, we'll come back,
[00:04:45.720 --> 00:04:50.720]   take a deeper look at any of these papers that I assume everyone from the audience would like.
[00:04:50.720 --> 00:04:55.720]   So I hope you find that useful.
[00:04:55.720 --> 00:04:58.720]   Hey, Ayush, good to see you.
[00:04:58.720 --> 00:05:03.720]   To the people who are trickling in, I'm just waiting another minute,
[00:05:03.720 --> 00:05:05.720]   and then we'll officially get started.
[00:05:05.720 --> 00:05:15.720]   [BLANK_AUDIO]
[00:05:15.720 --> 00:05:20.720]   Does anyone in the Zoom call want to share any papers that they've liked so far
[00:05:20.720 --> 00:05:41.720]   or any things that they have been excited about in the research world?
[00:05:41.720 --> 00:05:43.720]   Okay, amazing. I guess we can get started.
[00:05:43.720 --> 00:05:47.720]   I'll just share this link, which should take you to our forum.
[00:05:47.720 --> 00:05:49.720]   So I'll tell you about that.
[00:05:49.720 --> 00:05:53.720]   If you head over to this link, it should take you to our community
[00:05:53.720 --> 00:05:57.720]   where you can reply with any questions.
[00:05:57.720 --> 00:06:02.720]   Let's say we're going to be talking about revisiting deep learning,
[00:06:02.720 --> 00:06:03.720]   which is one of the papers.
[00:06:03.720 --> 00:06:17.720]   So you can ask about any question based on the paper.
[00:06:17.720 --> 00:06:19.720]   How is this better than great in poster trees?
[00:06:19.720 --> 00:06:20.720]   So on and so forth.
[00:06:20.720 --> 00:06:24.720]   And I'll keep coming back and taking a look at these.
[00:06:24.720 --> 00:06:33.720]   I'll post this link in the Zoom chat and pin this in the YouTube chat.
[00:06:33.720 --> 00:06:36.720]   And as a reminder to everyone who's joined the Zoom call,
[00:06:36.720 --> 00:06:43.720]   please feel free to unmute yourself and ask any question at any point.
[00:06:43.720 --> 00:06:45.720]   So I'll get started now.
[00:06:45.720 --> 00:06:47.720]   Thanks everyone for joining our paper reading group.
[00:06:47.720 --> 00:06:52.720]   This is a special one because it's an official NeurIPS meetup.
[00:06:52.720 --> 00:06:55.720]   And by that I mean we'll be going through a bunch of papers
[00:06:55.720 --> 00:06:57.720]   that are from the NeurIPS conference.
[00:06:57.720 --> 00:07:01.720]   If you are not familiar, I would say I'll point you to a meme
[00:07:01.720 --> 00:07:03.720]   and my downloads folder.
[00:07:03.720 --> 00:07:06.720]   Essentially, this has been going on in my life for a while.
[00:07:06.720 --> 00:07:09.720]   I'd say my teams, the Weights and Biases teams,
[00:07:09.720 --> 00:07:13.720]   meme game has gone up quite a bit.
[00:07:13.720 --> 00:07:16.720]   NeurIPS is one of the top go-to conferences
[00:07:16.720 --> 00:07:19.720]   in the field of AI/machine learning.
[00:07:19.720 --> 00:07:23.720]   And as you can see on the right, I've gone through a lot of papers
[00:07:23.720 --> 00:07:28.720]   from which I have picked a few that I would love to talk about today.
[00:07:28.720 --> 00:07:33.720]   And just to reemphasize this because might as well,
[00:07:33.720 --> 00:07:37.720]   this is the stack of papers that has been living on my desk
[00:07:37.720 --> 00:07:39.720]   for the past two, three days.
[00:07:39.720 --> 00:07:41.720]   Actually, one or two weeks now.
[00:07:41.720 --> 00:07:43.720]   Let me just get this out of the way.
[00:07:43.720 --> 00:07:48.720]   So while we were waiting to get started,
[00:07:48.720 --> 00:07:50.720]   I was just asking the people in the Zoom call
[00:07:50.720 --> 00:07:53.720]   if they have any papers that they have liked.
[00:07:53.720 --> 00:07:55.720]   I didn't get any interesting questions.
[00:07:55.720 --> 00:07:59.720]   So I'll ask that again if anyone wants to share any paper
[00:07:59.720 --> 00:08:01.720]   that they really found interesting from NeurIPS
[00:08:01.720 --> 00:08:04.720]   or they have any comments on that.
[00:08:04.720 --> 00:08:08.720]   I really want to keep this interactive
[00:08:08.720 --> 00:08:12.720]   because that's one thing I really enjoy about our paper reading groups.
[00:08:12.720 --> 00:08:18.720]   So please unmute and keep any comments coming.
[00:08:18.720 --> 00:08:27.720]   Okay, maybe not.
[00:08:27.720 --> 00:08:33.720]   So here's the agenda for today.
[00:08:33.720 --> 00:08:37.720]   I've spoken about the paper reading group in this series.
[00:08:37.720 --> 00:08:43.720]   We meet once every month where I try to talk about a research paper,
[00:08:43.720 --> 00:08:46.720]   summarize the paper, share its implementation,
[00:08:46.720 --> 00:08:49.720]   and we run through how does it implement an idea
[00:08:49.720 --> 00:08:52.720]   or we try to further extend it to different things.
[00:08:52.720 --> 00:08:55.720]   Last time we met, we talked about the unit paper.
[00:08:55.720 --> 00:08:58.720]   Before that, my colleague Aman had been talking about a lot
[00:08:58.720 --> 00:09:03.720]   in the world of transformers, vision transformers specifically.
[00:09:03.720 --> 00:09:06.720]   So you can find all of those papers on our YouTube channel.
[00:09:07.720 --> 00:09:11.720]   My method of selecting papers from this ginormous list
[00:09:11.720 --> 00:09:15.720]   has been just going through abstracts
[00:09:15.720 --> 00:09:18.720]   and selecting any that I really enjoyed
[00:09:18.720 --> 00:09:21.720]   and then further trying to read the papers.
[00:09:21.720 --> 00:09:25.720]   So using that strategy, I basically gradient descended
[00:09:25.720 --> 00:09:28.720]   on these five papers.
[00:09:28.720 --> 00:09:33.720]   And in this meetup or in this live stream Zoom call,
[00:09:33.720 --> 00:09:38.720]   I'll be summarizing all five of these papers bit by bit.
[00:09:38.720 --> 00:09:41.720]   And I'll be taking a pause after every paper summary
[00:09:41.720 --> 00:09:46.720]   to discuss any questions or any comments that you all might have.
[00:09:46.720 --> 00:09:49.720]   So please feel free to interrupt me anytime
[00:09:49.720 --> 00:09:52.720]   or ask any questions as you may like.
[00:09:59.720 --> 00:10:04.720]   And I'll be keeping an eye out on the forums just as a reminder.
[00:10:04.720 --> 00:10:07.720]   Abdus, no, I haven't read all of those papers in two weeks.
[00:10:07.720 --> 00:10:09.720]   I've skimmed a lot of them.
[00:10:09.720 --> 00:10:12.720]   I'm not smart enough to read so many papers in just two weeks.
[00:10:12.720 --> 00:10:15.720]   So I skimmed a lot of them. I won't lie to you.
[00:10:15.720 --> 00:10:22.720]   A few resources I want to mention.
[00:10:22.720 --> 00:10:24.720]   NeurIPS has an amazing website.
[00:10:24.720 --> 00:10:26.720]   This time it's really easy to navigate that.
[00:10:26.720 --> 00:10:29.720]   So I use that to find these papers.
[00:10:29.720 --> 00:10:33.720]   Yannick Kilcher, he's a YouTuber and creates these amazing YouTube videos.
[00:10:33.720 --> 00:10:38.720]   I watched a few from his channel where I basically learned
[00:10:38.720 --> 00:10:40.720]   about one of the papers I'm going to be talking about.
[00:10:40.720 --> 00:10:44.720]   And I've used the open source repositories that were mentioned in the papers.
[00:10:44.720 --> 00:10:47.720]   After this live stream, I'll link off all of these resources.
[00:10:47.720 --> 00:10:51.720]   And I've also, of course, gone through our own fully connected blog
[00:10:51.720 --> 00:10:55.720]   hosted by Weights & Biases where I found a lot of these concepts.
[00:10:55.720 --> 00:10:59.720]   So as a reminder, I'll again post this link in the Zoom chat
[00:10:59.720 --> 00:11:02.720]   for anyone that wants to ask any questions.
[00:11:02.720 --> 00:11:06.720]   Or for the people in the Zoom call, please also feel free to unmute yourself.
[00:11:06.720 --> 00:11:13.720]   No questions are dumb questions, as a reminder.
[00:11:13.720 --> 00:11:20.720]   So the first paper is "Revisiting Deep Learning Models for Tabular Data".
[00:11:20.720 --> 00:11:22.720]   That's the title of the paper.
[00:11:24.720 --> 00:11:27.720]   And to take a step back and start summarizing this paper,
[00:11:27.720 --> 00:11:33.720]   I want to emphasize the point what makes tabular data interesting.
[00:11:33.720 --> 00:11:37.720]   I'll also leave this question to the audience, but while I try to explain
[00:11:37.720 --> 00:11:41.720]   why is it a challenging problem, let me draw and explain that.
[00:11:41.720 --> 00:11:45.720]   Or if anyone wants to jump in, please feel free to do so.
[00:11:45.720 --> 00:11:53.720]   Usually when you're working on images, you just have a homogenous
[00:11:53.720 --> 00:11:57.720]   data set, right? So it's, let's say, n images.
[00:11:57.720 --> 00:12:00.720]   This could be in the scale of millions if you're talking ImageNet.
[00:12:00.720 --> 00:12:07.720]   More than that, for the case of GPT-3, which is a text-based model,
[00:12:07.720 --> 00:12:10.720]   it could be the entire internet if you're looking at such a model.
[00:12:10.720 --> 00:12:17.720]   The challenge with tabular data, you have a lot of columns,
[00:12:17.720 --> 00:12:19.720]   or a few columns.
[00:12:19.720 --> 00:12:22.720]   And these are heterogeneous columns.
[00:12:23.720 --> 00:12:26.720]   So one column could have continuous numbers.
[00:12:26.720 --> 00:12:30.720]   One could have categorical numbers.
[00:12:30.720 --> 00:12:33.720]   So let's say someone likes chai, someone doesn't like chai.
[00:12:33.720 --> 00:12:38.720]   Someone likes chai on a scale of 5 out of 5.
[00:12:38.720 --> 00:12:40.720]   Someone doesn't like it on a scale of 1.
[00:12:40.720 --> 00:12:42.720]   These are redundant columns.
[00:12:42.720 --> 00:12:46.720]   Usually you want to remove such columns, but disregarding that,
[00:12:46.720 --> 00:12:49.720]   you could also have images in here.
[00:12:49.720 --> 00:12:52.720]   We have had Kaggle competitions where that was the case.
[00:12:52.720 --> 00:12:57.720]   So as you can see, we've built models that were really effective
[00:12:57.720 --> 00:13:03.720]   in just images, or in the case of some NLP tasks.
[00:13:03.720 --> 00:13:08.720]   But tabular in itself also starts to become a multimodal problem of sorts.
[00:13:08.720 --> 00:13:13.720]   And in itself, even if it's not multimodal, so let's say it doesn't
[00:13:13.720 --> 00:13:17.720]   have images, it could be a really sparse data set.
[00:13:18.720 --> 00:13:22.720]   One of the other problems that the authors mentioned for images,
[00:13:22.720 --> 00:13:27.720]   everyone just instantly thinks of ImageNet, right?
[00:13:27.720 --> 00:13:33.720]   But there is no such baseline or benchmark for tabular data sets.
[00:13:33.720 --> 00:13:37.720]   So in this paper, the authors start by commenting on that.
[00:13:37.720 --> 00:13:42.720]   And they discuss about two techniques or two architectures
[00:13:42.720 --> 00:13:44.720]   that are really helpful for this.
[00:13:44.720 --> 00:13:56.720]   So they suggest two architectures.
[00:13:56.720 --> 00:14:00.720]   The first one is a ResNet-like framework, and we'll quickly look
[00:14:00.720 --> 00:14:02.720]   at the open source implementation as well.
[00:14:02.720 --> 00:14:06.720]   And the second one is a FT transformer, which is an adaptation
[00:14:06.720 --> 00:14:09.720]   of the classic transformer model.
[00:14:09.720 --> 00:14:13.720]   And then they compare all of these two frameworks.
[00:14:13.720 --> 00:14:18.720]   These two frameworks or architectures against decision trees,
[00:14:18.720 --> 00:14:23.720]   boosted algorithms, CAD boost, XGBoost, and the complete family.
[00:14:23.720 --> 00:14:28.720]   So the ResNet-like architecture looks somewhat like this.
[00:14:28.720 --> 00:14:34.720]   ResNet is defined as a ResNet block, which encapsulates
[00:14:34.720 --> 00:14:35.720]   a bunch of linear blocks.
[00:14:35.720 --> 00:14:40.720]   Inside of a ResNet blocks, you're adding the input that goes
[00:14:40.720 --> 00:14:44.720]   into the block with a pass-through of the following.
[00:14:44.720 --> 00:14:48.720]   So you normalize the input, pass it to a linear layer, pass it
[00:14:48.720 --> 00:14:53.720]   through activation function, apply dropout, apply linear layer again
[00:14:53.720 --> 00:14:54.720]   and apply dropout again.
[00:14:54.720 --> 00:15:01.720]   And to predict from that, you again run it through a normalization layer.
[00:15:01.720 --> 00:15:03.720]   In the case of ResNet, it's batch norm.
[00:15:03.720 --> 00:15:05.720]   You could have weight norm as well.
[00:15:05.720 --> 00:15:10.720]   If you're experimenting, you could have any other function
[00:15:10.720 --> 00:15:13.720]   in here, after which you apply an activation function
[00:15:13.720 --> 00:15:15.720]   and you get your predictions.
[00:15:15.720 --> 00:15:19.720]   So the authors do the same for tabular data and they do it
[00:15:19.720 --> 00:15:22.720]   across a bunch of different data sets, which I'll quickly share.
[00:15:22.720 --> 00:15:30.720]   They also create a feature tokenizer and a transformer model.
[00:15:30.720 --> 00:15:34.720]   So the reason for creating these two architectures, they want
[00:15:34.720 --> 00:15:37.720]   to create two standard architectures and compare how do these
[00:15:37.720 --> 00:15:42.720]   perform against the traditional boosted algorithms or
[00:15:42.720 --> 00:15:43.720]   all other techniques.
[00:15:43.720 --> 00:15:48.720]   So the first one was a ResNet like structure and the other
[00:15:48.720 --> 00:15:52.720]   one is a feature tokenizer and a transformer layer
[00:15:52.720 --> 00:15:53.720]   that sits on top of it.
[00:15:53.720 --> 00:15:57.720]   So in this image, if you pay close attention, what's going on,
[00:15:57.720 --> 00:16:00.720]   we have our input, which gets passed through a feature
[00:16:00.720 --> 00:16:03.720]   tokenizer, which creates all of these tokens.
[00:16:03.720 --> 00:16:09.720]   All of the features, categorical and numerical, basically
[00:16:09.720 --> 00:16:11.720]   get converted into an embedding.
[00:16:11.720 --> 00:16:15.720]   And then the transformer operates on all of these and
[00:16:15.720 --> 00:16:17.720]   gives us predictions.
[00:16:17.720 --> 00:16:20.720]   So that is a simple gist of it.
[00:16:20.720 --> 00:16:25.720]   It's again, a multi-head self-attention layer that gets
[00:16:25.720 --> 00:16:29.720]   applied to all of the embeddings and we get outputs
[00:16:29.720 --> 00:16:30.720]   from all of these.
[00:16:30.720 --> 00:16:38.720]   Now let me switch screen sharing to the paper.
[00:16:38.720 --> 00:16:41.720]   I'll be switching my screen sharing a lot.
[00:16:41.720 --> 00:16:43.720]   It might take a few minutes.
[00:16:43.720 --> 00:16:44.720]   Sorry about that.
[00:16:44.720 --> 00:16:56.720]   I have all of the papers open.
[00:16:56.720 --> 00:16:59.720]   So I'm just trying to point zoom to the correct one and I
[00:16:59.720 --> 00:17:00.720]   managed to do that.
[00:17:00.720 --> 00:17:01.720]   Sorry about the delay.
[00:17:01.720 --> 00:17:07.720]   Oh, looks like I messed up.
[00:17:07.720 --> 00:17:08.720]   Sorry about that.
[00:17:08.720 --> 00:17:09.720]   Let me quickly minimize this.
[00:17:09.720 --> 00:17:10.720]   Sorry about this mess.
[00:17:10.720 --> 00:17:15.720]   I have a lot of papers open today and I'm not used to being
[00:17:15.720 --> 00:17:17.720]   able to manage so many windows.
[00:17:17.720 --> 00:17:22.720]   So these were the two architectures that the authors
[00:17:22.720 --> 00:17:25.720]   have come up and they are also comparing these against some
[00:17:25.720 --> 00:17:29.720]   other architectures that have recently popped up in the world
[00:17:29.720 --> 00:17:30.720]   of deep learning.
[00:17:30.720 --> 00:17:34.720]   Broadly speaking, deep learning has really lagged behind on
[00:17:34.720 --> 00:17:38.720]   tabular datasets, but still a few architectures I want to
[00:17:38.720 --> 00:17:46.720]   call out, TabNet, Node, Grownet, have really shown some
[00:17:46.720 --> 00:17:50.720]   progress on this, on tabular datasets.
[00:17:50.720 --> 00:17:53.720]   The authors compare their approach against all of these.
[00:17:53.720 --> 00:17:56.720]   And how do they do so?
[00:17:56.720 --> 00:18:01.720]   They do it across a bunch of datasets, the California housing
[00:18:01.720 --> 00:18:08.720]   dataset, adult income, Helena anonymized dataset.
[00:18:08.720 --> 00:18:10.720]   I've actually looked at all of these.
[00:18:10.720 --> 00:18:14.720]   They are referenced in the papers, but these are just a
[00:18:14.720 --> 00:18:21.720]   bunch of tabular datasets with a good number of categorical
[00:18:21.720 --> 00:18:24.720]   features in some of them.
[00:18:24.720 --> 00:18:28.720]   Some have a larger number of numerical features and the
[00:18:28.720 --> 00:18:32.720]   metric varies as being RMSE or accuracy, depending on the
[00:18:32.720 --> 00:18:33.720]   dataset.
[00:18:33.720 --> 00:18:38.720]   And the classes vary from two up to a hundred or no class
[00:18:38.720 --> 00:18:40.720]   in case it's a regression problem.
[00:18:40.720 --> 00:18:43.720]   So if you're trying to predict someone's income, that's not
[00:18:43.720 --> 00:18:44.720]   a number of classes problem.
[00:18:44.720 --> 00:18:48.720]   That's a regression problem.
[00:18:48.720 --> 00:18:54.720]   In this approach, the authors apply minimal pre-processing
[00:18:54.720 --> 00:19:01.720]   and they use Optuna to tune all of their algorithms or
[00:19:01.720 --> 00:19:04.720]   basically just those two architectures.
[00:19:04.720 --> 00:19:08.720]   They also ensemble to see how does this fare against other
[00:19:08.720 --> 00:19:09.720]   approaches.
[00:19:09.720 --> 00:19:13.720]   And here is the overview across all of these datasets.
[00:19:13.720 --> 00:19:16.720]   So the best performing are in bold.
[00:19:16.720 --> 00:19:19.720]   And the last two architectures here are by the authors.
[00:19:19.720 --> 00:19:22.720]   As you can see in a good portion of them, they perform
[00:19:22.720 --> 00:19:28.720]   really well against these particular datasets.
[00:19:28.720 --> 00:19:33.720]   But is it true across outside of these or are these datasets
[00:19:33.720 --> 00:19:35.720]   really universal?
[00:19:35.720 --> 00:19:39.720]   So the author's take on this is ResNet.
[00:19:39.720 --> 00:19:45.720]   The one we just looked at is an effective baseline.
[00:19:45.720 --> 00:19:50.720]   The FD transformer performs best on most tasks, but it
[00:19:50.720 --> 00:19:53.720]   takes longer as you would expect from an attention based
[00:19:53.720 --> 00:19:57.720]   model to train.
[00:19:57.720 --> 00:20:02.720]   And if you compare these against boosted algorithms, they
[00:20:02.720 --> 00:20:07.720]   do perform well with tuned hyper parameters, but that means
[00:20:07.720 --> 00:20:13.720]   especially with the transformer model, it takes longer to train.
[00:20:13.720 --> 00:20:17.720]   And in conclusion, even the authors mentioned it's not
[00:20:17.720 --> 00:20:21.720]   clear if there's one standard winner from this.
[00:20:21.720 --> 00:20:26.720]   But the approach taken by the authors to just apply ResNets
[00:20:26.720 --> 00:20:30.720]   and FD transformers without any pre-processing still shows
[00:20:30.720 --> 00:20:32.720]   good enough results.
[00:20:32.720 --> 00:20:36.720]   So in my opinion, with some pre-processing and they haven't
[00:20:36.720 --> 00:20:43.720]   even applied any learning rate warm up or any regularization
[00:20:43.720 --> 00:20:49.720]   techniques even, I think if you just apply those techniques,
[00:20:49.720 --> 00:20:53.720]   these results still could be better, but I'm not too
[00:20:53.720 --> 00:20:56.720]   convinced if they would outperform boosted algorithms.
[00:20:56.720 --> 00:20:58.720]   So that was the first paper summary.
[00:20:58.720 --> 00:21:01.720]   And essentially in this entirety of the call, I'll just be
[00:21:01.720 --> 00:21:04.720]   summarizing different papers and taking breaks to see if
[00:21:04.720 --> 00:21:05.720]   there are any questions.
[00:21:05.720 --> 00:21:11.720]   So any questions on this so far?
[00:21:11.720 --> 00:21:24.720]   Ramesh, please feel free to unmute yourself as well.
[00:21:24.720 --> 00:21:38.720]   Okay. I don't see any questions.
[00:21:38.720 --> 00:21:43.720]   So let me go back to sharing my screen.
[00:21:43.720 --> 00:21:46.720]   Hey, Sandeep, I had a quick question.
[00:21:46.720 --> 00:21:48.720]   Please go ahead.
[00:21:48.720 --> 00:21:53.720]   Can you place the links to these papers on the thing?
[00:21:53.720 --> 00:21:58.720]   I'll just do it after the call.
[00:21:58.720 --> 00:22:00.720]   Okay, sure.
[00:22:00.720 --> 00:22:08.720]   Thank you.
[00:22:08.720 --> 00:22:12.720]   So I always try to suggest quote unquote homework, not that I'm
[00:22:12.720 --> 00:22:17.720]   a qualified lecturer by any means, but Deep Kaggle has been
[00:22:17.720 --> 00:22:22.720]   hosting some tabular competitions which have anonymized
[00:22:22.720 --> 00:22:24.720]   columns.
[00:22:24.720 --> 00:22:29.720]   This approach could be a great place to try that.
[00:22:29.720 --> 00:22:32.720]   And like I said, the authors haven't done much pre-processing,
[00:22:32.720 --> 00:22:36.720]   so maybe we could improve their results further.
[00:22:36.720 --> 00:22:41.720]   And they haven't ensembled this across as a Kaggler, that's
[00:22:41.720 --> 00:22:42.720]   what comes to my brain.
[00:22:42.720 --> 00:22:46.720]   So maybe try also ensembling these against boosted algorithms.
[00:22:46.720 --> 00:22:58.720]   I don't think I want to run through anything here.
[00:22:58.720 --> 00:23:02.720]   But I also wanted to mention that the authors, after a link
[00:23:02.720 --> 00:23:06.720]   of the paper, make this available as an open source package.
[00:23:06.720 --> 00:23:08.720]   So you might also want to check that out.
[00:23:08.720 --> 00:23:12.720]   The interesting bit about this to me, at least, is this create
[00:23:12.720 --> 00:23:16.720]   good enough baselines across all of these datasets that you can
[00:23:16.720 --> 00:23:23.720]   really try and benchmark your models against ASCII.
[00:23:23.720 --> 00:23:25.720]   I have one question, Sanyam.
[00:23:25.720 --> 00:23:26.720]   Please.
[00:23:26.720 --> 00:23:33.720]   So in most tabular datasets, the hardest thing is the data
[00:23:33.720 --> 00:23:35.720]   augmentation.
[00:23:35.720 --> 00:23:38.720]   It's like images, it's very simple to do data augmentation,
[00:23:38.720 --> 00:23:40.720]   the sort of built-in techniques.
[00:23:40.720 --> 00:23:42.720]   It is challenging in text as well.
[00:23:42.720 --> 00:23:47.720]   But usually text, you have unsupervised learning of a very
[00:23:47.720 --> 00:23:48.720]   large dataset.
[00:23:48.720 --> 00:23:51.720]   So all you're doing is just fine-tuning on a pre-trained
[00:23:51.720 --> 00:23:52.720]   model.
[00:23:52.720 --> 00:23:56.720]   So when it comes to tabular data, I don't know the details
[00:23:56.720 --> 00:24:00.720]   in these papers, but many times you are learning from scratch.
[00:24:00.720 --> 00:24:03.720]   You don't have a pre-trained model to learn from.
[00:24:03.720 --> 00:24:07.720]   So neural networks, when you learn something from scratch,
[00:24:07.720 --> 00:24:12.720]   you need to have a lot of data, or you need to somehow trick
[00:24:12.720 --> 00:24:15.720]   the neural network to think there is a lot of data through
[00:24:15.720 --> 00:24:16.720]   data augmentation.
[00:24:16.720 --> 00:24:22.720]   Has the authors talked about any of these challenges?
[00:24:22.720 --> 00:24:24.720]   That's a great question.
[00:24:24.720 --> 00:24:25.720]   And yeah, I agree.
[00:24:25.720 --> 00:24:29.720]   It's quite a challenge with tabular data.
[00:24:29.720 --> 00:24:30.720]   No, they haven't discussed this.
[00:24:30.720 --> 00:24:32.720]   And to the best of my knowledge, they don't apply any
[00:24:32.720 --> 00:24:34.720]   pre-processing techniques as well.
[00:24:34.720 --> 00:24:39.720]   That's what made this paper really interesting to me.
[00:24:39.720 --> 00:24:40.720]   OK, sure.
[00:24:40.720 --> 00:24:44.720]   Yeah, I think it's still a good thing to try it on a few
[00:24:44.720 --> 00:24:48.720]   Kaggle competitions to see whether the model is over-parameterized
[00:24:48.720 --> 00:24:51.720]   and just over-fit to training data.
[00:24:51.720 --> 00:24:58.720]   That's always a good levelizer in my opinion.
[00:24:58.720 --> 00:25:02.720]   OK, I'll continue further, but as a reminder to everyone,
[00:25:02.720 --> 00:25:09.720]   please feel free to interrupt me anytime.
[00:25:09.720 --> 00:25:13.720]   I'll probably also answer this one.
[00:25:13.720 --> 00:25:16.720]   Did the authors say that ResNet and Transformer gave better
[00:25:16.720 --> 00:25:18.720]   performance and boosted algorithms?
[00:25:18.720 --> 00:25:22.720]   Yes, for some datasets, they did.
[00:25:22.720 --> 00:25:26.720]   But again, like Ramesh just mentioned, maybe it's over-fit
[00:25:26.720 --> 00:25:29.720]   to these particular datasets.
[00:25:29.720 --> 00:25:32.720]   I'm not trying to criticize the authors and I'm not trying to
[00:25:32.720 --> 00:25:34.720]   say bad things about the research, but we might want to
[00:25:34.720 --> 00:25:41.720]   try on other datasets just to double-check.
[00:25:41.720 --> 00:25:45.720]   The paper that I want to discuss after this comes from something
[00:25:45.720 --> 00:25:48.720]   that I have personally really disliked, which is video editing.
[00:25:48.720 --> 00:25:53.720]   And if you have ever in your life tried to export a video,
[00:25:53.720 --> 00:25:56.720]   you would know first of all, how long does it take?
[00:25:56.720 --> 00:26:00.720]   It takes hours and hours if you're not on a latest MacBook,
[00:26:00.720 --> 00:26:02.720]   in my opinion.
[00:26:02.720 --> 00:26:05.720]   And what's going on during that time?
[00:26:05.720 --> 00:26:08.720]   Let me switch sharing so that I can explain.
[00:26:08.720 --> 00:26:14.720]   I actually ended up reading a lot about this to my dislike.
[00:26:14.720 --> 00:26:18.720]   But whenever you have any form of media on your computer,
[00:26:18.720 --> 00:26:22.720]   it's usually stored in a certain format.
[00:26:22.720 --> 00:26:23.720]   And how does this happen?
[00:26:23.720 --> 00:26:26.720]   So video, even my video or the video you're seeing right now
[00:26:26.720 --> 00:26:30.720]   is mostly just analog signal.
[00:26:30.720 --> 00:26:35.720]   To store this on your computer's memory, you need a way to
[00:26:35.720 --> 00:26:38.720]   encode it.
[00:26:38.720 --> 00:26:44.720]   And then to be able to read it, you need a way to decode it.
[00:26:44.720 --> 00:26:49.720]   Now, if you've like me painfully tried to export a video,
[00:26:49.720 --> 00:26:55.720]   you would know there are different formats like H.264, H.265.
[00:26:55.720 --> 00:26:58.720]   If you come from the world of images, I'm sure you would
[00:26:58.720 --> 00:27:00.720]   recognize JPEGs.
[00:27:00.720 --> 00:27:09.720]   So all of these are ways of compressing and encoding this
[00:27:09.720 --> 00:27:16.720]   analog signal of audio and images onto your computer's memory.
[00:27:16.720 --> 00:27:19.720]   And then it gets decoded while you're trying to access it.
[00:27:19.720 --> 00:27:22.720]   So while you're watching this live stream, YouTube is encoding
[00:27:22.720 --> 00:27:26.720]   my video, delivering it to you real time.
[00:27:26.720 --> 00:27:31.720]   And your computer is decoding the live stream and showing it to you.
[00:27:31.720 --> 00:27:34.720]   So that's what is happening right now.
[00:27:34.720 --> 00:27:38.720]   As you might expect, you could probably throw neural networks
[00:27:38.720 --> 00:27:42.720]   at this problem and try to improve it.
[00:27:42.720 --> 00:27:46.720]   So here's the traditional technique and it is, as the authors
[00:27:46.720 --> 00:27:52.720]   called, explicit representation, where you just take frame by frame.
[00:27:52.720 --> 00:27:58.720]   So you treat video as a bunch of images and you pass that to the
[00:27:58.720 --> 00:28:02.720]   algorithm. Now, these algorithms aren't necessarily neural network
[00:28:02.720 --> 00:28:07.720]   dependent. They have, especially JPEG encoding and the likes,
[00:28:07.720 --> 00:28:09.720]   don't have a neural network involved.
[00:28:09.720 --> 00:28:14.720]   But they treat a video like a collection of frames.
[00:28:14.720 --> 00:28:19.720]   The authors asked the question of, can we make this an implicit
[00:28:19.720 --> 00:28:21.720]   representation for videos?
[00:28:21.720 --> 00:28:27.720]   And can we apply an interesting architecture to speed up the process
[00:28:27.720 --> 00:28:29.720]   of encoding?
[00:28:29.720 --> 00:28:33.720]   Like I mentioned, if you've ever tried to export a video in any
[00:28:33.720 --> 00:28:38.720]   video editing software, it takes hours at least, even on the best
[00:28:38.720 --> 00:28:39.720]   laptops.
[00:28:39.720 --> 00:28:41.720]   So can we make it faster?
[00:28:41.720 --> 00:28:45.720]   And that's a quite interesting problem in my opinion.
[00:28:45.720 --> 00:28:54.720]   So the authors suggest using an image-wise implicit representation.
[00:28:54.720 --> 00:29:00.720]   And in this study, they showed that NERV outperforms the explicit
[00:29:00.720 --> 00:29:02.720]   representation approach.
[00:29:02.720 --> 00:29:05.720]   From there, they asked the question, of course, this neural network
[00:29:05.720 --> 00:29:09.720]   that you have created will eat up your computer's memory.
[00:29:09.720 --> 00:29:13.720]   So can you further compress this model?
[00:29:13.720 --> 00:29:17.720]   And then they also extend their work to show it works really well
[00:29:17.720 --> 00:29:20.720]   on video denoising.
[00:29:20.720 --> 00:29:23.720]   So here is their architecture.
[00:29:23.720 --> 00:29:27.720]   And I'll also dive into the implementation which I have open in a
[00:29:27.720 --> 00:29:29.720]   different app.
[00:29:29.720 --> 00:29:34.720]   But the approach above is a different neural network based one
[00:29:34.720 --> 00:29:40.720]   where you take the pixel coordinates and you pass them through an
[00:29:40.720 --> 00:29:45.720]   embedding layer followed by a simple MLP.
[00:29:45.720 --> 00:29:52.720]   For NERV, you pass the frame index to the embedding.
[00:29:52.720 --> 00:29:59.720]   And after passing that through a MLP, you apply a NERV block.
[00:29:59.720 --> 00:30:01.720]   So what is a NERV block?
[00:30:01.720 --> 00:30:04.720]   It consists of the following layers.
[00:30:04.720 --> 00:30:11.720]   A convolution followed by a pixel shuffle and an activation layer.
[00:30:11.720 --> 00:30:15.720]   This one is useful for upscaling the video.
[00:30:15.720 --> 00:30:18.720]   So when you're trying to decode them.
[00:30:18.720 --> 00:30:25.720]   And this one is useful for encoding the videos.
[00:30:25.720 --> 00:30:30.720]   And at this point, I should switch sharing to go into the --
[00:30:30.720 --> 00:30:34.720]   talk about how it's implemented.
[00:30:34.720 --> 00:30:40.720]   So the authors implement this custom conf block.
[00:30:40.720 --> 00:30:47.720]   Where they either apply a cont 2D layer followed by a pixel shuffle
[00:30:47.720 --> 00:30:51.720]   if you're trying to upscale it.
[00:30:51.720 --> 00:30:55.720]   Or if you're trying to deconvolve, you'll apply a cont 2D followed
[00:30:55.720 --> 00:31:01.720]   by an identity.
[00:31:01.720 --> 00:31:06.720]   Or you could also apply upsampling followed by cont 2D if you're
[00:31:06.720 --> 00:31:09.720]   trying to apply a bilinear transform to the video.
[00:31:09.720 --> 00:31:12.720]   So these three are different types of transformations that you
[00:31:12.720 --> 00:31:17.720]   usually apply to a video as you're trying to process it or images.
[00:31:17.720 --> 00:31:21.720]   Depending on the particular case, you could apply these following
[00:31:21.720 --> 00:31:28.720]   layers as you can see defined by simple if or else if.
[00:31:28.720 --> 00:31:33.720]   And followed by that, you pass it through MLP.
[00:31:33.720 --> 00:31:36.720]   And then you define the nerve block.
[00:31:36.720 --> 00:31:41.720]   Where you call the custom conf block that we've just defined.
[00:31:41.720 --> 00:31:44.720]   And depending on the case, if you're trying to do bilinear
[00:31:44.720 --> 00:31:49.720]   transformation, if you're trying to encode or upscale, the
[00:31:49.720 --> 00:31:51.720]   particular blocks get called.
[00:31:51.720 --> 00:31:54.720]   Followed by those layers, you pass these through a normalization
[00:31:54.720 --> 00:31:57.720]   layer and an activation layer.
[00:31:57.720 --> 00:32:02.720]   So that is the nerve block that the authors have suggested.
[00:32:02.720 --> 00:32:07.720]   Now let me find the paper and share my screen which will take one minute.
[00:32:08.720 --> 00:32:36.720]   So as I mentioned, all of the open source implementations are
[00:32:36.720 --> 00:32:37.720]   by the authors themselves.
[00:32:37.720 --> 00:32:41.720]   I'm not smart enough to implement so many papers in two weeks.
[00:32:41.720 --> 00:32:45.720]   I want to jump towards the results and showcase something really
[00:32:45.720 --> 00:32:53.720]   interesting, at least to me.
[00:32:53.720 --> 00:32:58.720]   So here basically they've defined the loss objectives mathematically
[00:32:58.720 --> 00:33:02.720]   and the architecture which I just showcased in PyTorch.
[00:33:02.720 --> 00:33:06.720]   I know it was a quick rush through showcase, but I'm also
[00:33:06.720 --> 00:33:08.720]   trying to summarize five papers.
[00:33:08.720 --> 00:33:11.720]   I'll take any questions that you all might have.
[00:33:11.720 --> 00:33:15.720]   After doing this implementation, the authors also take a look at
[00:33:15.720 --> 00:33:20.720]   how can they prune the model, how can then they quantize it, and
[00:33:20.720 --> 00:33:23.720]   then how can they weight encode this.
[00:33:23.720 --> 00:33:28.720]   So to anyone who's not familiar, model pruning is a way of taking
[00:33:28.720 --> 00:33:38.720]   a look at different weights inside of a model and removing the unnecessary ones.
[00:33:38.720 --> 00:33:43.720]   After pruning, you can further quantize it and then encode the weights.
[00:33:43.720 --> 00:33:46.720]   We're doing this to reduce the model size.
[00:33:46.720 --> 00:33:48.720]   And ideally when you're applying these techniques, you would
[00:33:48.720 --> 00:33:53.720]   want to retain the same accuracy.
[00:33:53.720 --> 00:33:58.720]   So after doing all of this, the authors show first of all, compared
[00:33:58.720 --> 00:34:02.720]   to other neural network approaches.
[00:34:02.720 --> 00:34:07.720]   If you look here, so they're comparing even the smallest model.
[00:34:07.720 --> 00:34:10.720]   So nerve S represents small.
[00:34:10.720 --> 00:34:18.720]   It's really faster in training and lower is better for this case.
[00:34:18.720 --> 00:34:25.720]   It encodes videos faster than the other neural network based approaches.
[00:34:25.720 --> 00:34:28.720]   And is also 50x faster.
[00:34:28.720 --> 00:34:32.720]   The smallest model is 50x faster in decoding.
[00:34:32.720 --> 00:34:35.720]   So it decodes at the rate of 50 FPS.
[00:34:35.720 --> 00:34:39.720]   If you're watching this video right now on YouTube, you're watching it at 30 FPS.
[00:34:39.720 --> 00:34:44.720]   So this model quite outperforms the general needs.
[00:34:44.720 --> 00:34:49.720]   And then this basically in these graphs show that you can quantize
[00:34:49.720 --> 00:34:59.720]   the models without losing much accuracy compared to the counterparts.
[00:34:59.720 --> 00:35:05.720]   Further, they show, so there's a case where you want to make sure that
[00:35:05.720 --> 00:35:12.720]   the decoded video is first of all, good enough to be watched on any screen.
[00:35:12.720 --> 00:35:16.720]   So they also visually compare their approach by zooming into different
[00:35:16.720 --> 00:35:23.720]   frames against HEVC, which is one of the standard ways of decoding videos.
[00:35:23.720 --> 00:35:29.720]   And it took me quite a while of studying to realize that it's slightly higher quality.
[00:35:29.720 --> 00:35:33.720]   And in their presentation or in their poster presentation, they've also
[00:35:33.720 --> 00:35:38.720]   showcased a video where it was quite actually visible that these are really
[00:35:38.720 --> 00:35:40.720]   good at video denoising as well.
[00:35:40.720 --> 00:35:44.720]   So video denoising is the process of taking a noisy or jittery video
[00:35:44.720 --> 00:35:48.720]   and clearing the output.
[00:35:48.720 --> 00:35:55.720]   So to summarize, the authors create a way of having an explicit,
[00:35:55.720 --> 00:36:00.720]   sorry, an implicit input to a neural network, which first of all,
[00:36:00.720 --> 00:36:04.720]   I found very interesting because even for anyone that has worked with
[00:36:04.720 --> 00:36:09.720]   videos and neural networks, this could be a great approach to try.
[00:36:09.720 --> 00:36:14.720]   Further, they define this nerve block that is much, much faster at encoding
[00:36:14.720 --> 00:36:16.720]   and decoding.
[00:36:16.720 --> 00:36:20.720]   Now, I'm not sure if there are any problems that would come to mind
[00:36:20.720 --> 00:36:27.720]   when you're working with this, but to me, the encoding speed was quite exciting.
[00:36:27.720 --> 00:36:30.720]   So that was the bits that I wanted to cover for this summary.
[00:36:30.720 --> 00:36:52.720]   Any questions with this?
[00:36:52.720 --> 00:37:01.720]   I'm just looking at the YouTube chart as well just to make sure.
[00:37:01.720 --> 00:37:05.720]   Any information on the training data volumes?
[00:37:05.720 --> 00:37:10.720]   So I'm not sure if you're asking about the first or second data set.
[00:37:10.720 --> 00:37:14.720]   So the first one, the authors have mentioned all of the tabular data sets
[00:37:14.720 --> 00:37:17.720]   for this.
[00:37:17.720 --> 00:37:25.720]   Let's see if I can find it in the paper.
[00:37:25.720 --> 00:37:28.720]   But they've essentially mentioned the data set that they train on.
[00:37:28.720 --> 00:37:32.720]   So I'll share the name after the live stream, but the authors just train
[00:37:32.720 --> 00:37:37.720]   on one standard data set that's used for, I assume, benchmarking, encoding,
[00:37:37.720 --> 00:37:41.720]   decoding.
[00:37:41.720 --> 00:37:44.720]   How are the embeddings generated for the first paper?
[00:37:44.720 --> 00:37:47.720]   The authors discussed this in depth.
[00:37:47.720 --> 00:37:54.720]   In the paper, I wanted to skip it because it was quite straightforward.
[00:37:54.720 --> 00:38:05.720]   I'll try to come back to this question in case we have time left.
[00:38:05.720 --> 00:38:10.720]   Okay, I don't see any other questions, so I'll proceed.
[00:38:10.720 --> 00:38:14.720]   And Ramesh, you might be right, we have more people on YouTube than Zoom
[00:38:14.720 --> 00:38:18.720]   because meetup points to YouTube.
[00:38:18.720 --> 00:38:34.720]   Sorry about that. I think that was a mistake on our part.
[00:38:34.720 --> 00:38:44.720]   So, sorry about that. I've had a quite bad cold that I'm fighting with a lot of chai,
[00:38:44.720 --> 00:38:47.720]   but the cold is winning right now.
[00:38:47.720 --> 00:38:53.720]   So for a few suggested homeworks, I would say, please try comparing performance
[00:38:53.720 --> 00:38:55.720]   on your PC versus Core app.
[00:38:55.720 --> 00:38:59.720]   So since the authors mentioned such exciting speedups, which of course require
[00:38:59.720 --> 00:39:03.720]   you to have a GPU, I would say if you ever have worked with encoding
[00:39:03.720 --> 00:39:08.720]   videos, try exporting them on your PC and then maybe trying NERF
[00:39:08.720 --> 00:39:10.720]   on a Core app instance and see if that outperforms your PC.
[00:39:10.720 --> 00:39:15.720]   If that does, well, you saved a lot of time.
[00:39:15.720 --> 00:39:19.720]   And there was this DFTC Kaggle competition that had a lot of videos
[00:39:19.720 --> 00:39:21.720]   that wasn't the challenge there.
[00:39:21.720 --> 00:39:25.720]   It was trying to detect deep fakes inside of these videos,
[00:39:25.720 --> 00:39:30.720]   but you could try compressing them or picking a few to see if
[00:39:30.720 --> 00:39:39.720]   these speedups actually correlate to that as well.
[00:39:39.720 --> 00:39:44.720]   So the paper I want to discuss after this, this is the third paper.
[00:39:44.720 --> 00:39:47.720]   We've managed to go through two papers already.
[00:39:47.720 --> 00:39:50.720]   It's a Rust explanation, and I'll keep coming back to questions.
[00:39:50.720 --> 00:39:54.720]   So please don't hesitate to ask any questions if anything is not clear.
[00:39:54.720 --> 00:39:57.720]   That's what I wanted to remind you all of.
[00:39:57.720 --> 00:40:03.720]   It's titled Pay Attention to MLPs.
[00:40:03.720 --> 00:40:07.720]   This was quite an interesting pun, made me laugh enough and made me
[00:40:07.720 --> 00:40:08.720]   look at the paper.
[00:40:08.720 --> 00:40:10.720]   It's a simple paper.
[00:40:10.720 --> 00:40:14.720]   The presentation by the authors themselves is smaller than two minutes.
[00:40:14.720 --> 00:40:18.720]   But they look at the question and they ask, is attention really
[00:40:18.720 --> 00:40:20.720]   what you all need?
[00:40:20.720 --> 00:40:23.720]   So attention is what transformers are based on.
[00:40:23.720 --> 00:40:31.720]   And they take up the challenge of making MLPs outperform transformers
[00:40:31.720 --> 00:40:35.720]   in the domain of image classification and marks language modeling,
[00:40:35.720 --> 00:40:40.720]   where they have really, really shown.
[00:40:40.720 --> 00:40:46.720]   And they have also showcased, which has been a hot topic of discussion,
[00:40:46.720 --> 00:40:52.720]   that MLPs, the thing we so fashionably abandoned,
[00:40:52.720 --> 00:40:58.720]   can be scaled while being less resource intensive.
[00:40:58.720 --> 00:41:05.720]   So without adding more fluff to it, this is the definition of GMLPs.
[00:41:05.720 --> 00:41:17.720]   And I'll actually, again, switch to my handy one note to explain this concept.
[00:41:17.720 --> 00:41:25.720]   And let me actually try to clear all of this mess.
[00:41:25.720 --> 00:41:29.720]   So I never thought this concept would come handy later in life,
[00:41:29.720 --> 00:41:31.720]   but surprisingly it did.
[00:41:31.720 --> 00:41:37.720]   I learned about this in electronics, where you have gated,
[00:41:37.720 --> 00:41:41.720]   I think it was diodes, or basically you simply apply a gate
[00:41:41.720 --> 00:41:42.720]   to different problems.
[00:41:42.720 --> 00:41:46.720]   This also actually appears in RNN, so if you're not familiar.
[00:41:46.720 --> 00:41:50.720]   Electronics engineering, you would know of that by RNNs.
[00:41:50.720 --> 00:41:59.720]   The concept of having a gate is when you pass a certain information,
[00:41:59.720 --> 00:42:02.720]   which you want to allow opening the gate.
[00:42:02.720 --> 00:42:05.720]   So let's say you have a specific key to the gate,
[00:42:05.720 --> 00:42:07.720]   only that should allow opening the gate,
[00:42:07.720 --> 00:42:11.720]   and otherwise it should remain zero.
[00:42:11.720 --> 00:42:17.720]   So when that particular signal or information passes through the gate,
[00:42:17.720 --> 00:42:20.720]   it should go through, otherwise it shouldn't.
[00:42:20.720 --> 00:42:24.720]   The authors take this approach and they apply it to MLPs,
[00:42:24.720 --> 00:42:28.720]   or multi-layer perceptrons, simple neural networks.
[00:42:28.720 --> 00:42:31.720]   And with this simple idea, they outperform transformers
[00:42:31.720 --> 00:42:35.720]   on image classification and mass language modeling.
[00:42:35.720 --> 00:42:40.720]   So let's take a look at how do they do so.
[00:42:40.720 --> 00:42:43.720]   This is their architecture, simply summarized,
[00:42:43.720 --> 00:42:47.720]   and it's called a gated MLP.
[00:42:47.720 --> 00:42:51.720]   Here's the pseudocode for gMLP block on the right.
[00:42:51.720 --> 00:42:59.720]   They apply a normalization layer across the axis,
[00:42:59.720 --> 00:43:05.720]   followed by a projection, and a GLUE activation,
[00:43:05.720 --> 00:43:08.720]   followed by which they pass it through a special gating unit.
[00:43:08.720 --> 00:43:12.720]   What is that? That's right underneath.
[00:43:12.720 --> 00:43:17.720]   So they split it across two different columns,
[00:43:17.720 --> 00:43:20.720]   which they normalize,
[00:43:20.720 --> 00:43:23.720]   and they do a special projection,
[00:43:23.720 --> 00:43:27.720]   and then multiply these.
[00:43:27.720 --> 00:43:31.720]   So they split these two columns that are normalized,
[00:43:31.720 --> 00:43:33.720]   multiply them, and then return them,
[00:43:33.720 --> 00:43:37.720]   followed by which they add them to the input.
[00:43:37.720 --> 00:43:40.720]   So think of this, it's quite visible in the diagram,
[00:43:40.720 --> 00:43:43.720]   but they split the input, normalize it,
[00:43:43.720 --> 00:43:49.720]   apply a special projection, and then do a dot product.
[00:43:49.720 --> 00:43:53.720]   And this layer actually gets applied multiple times,
[00:43:53.720 --> 00:44:00.720]   so to be precise, a number of times inside of this architecture.
[00:44:00.720 --> 00:44:04.720]   The input is pre-processed according to how it's pre-processed
[00:44:04.720 --> 00:44:09.720]   for different BERT or VIT architectures,
[00:44:09.720 --> 00:44:13.720]   which are the standard for MLM or Vision.
[00:44:13.720 --> 00:44:17.720]   But after pre-processing, there's no attention being applied here.
[00:44:17.720 --> 00:44:24.720]   And just using this concept of this particular model,
[00:44:24.720 --> 00:44:32.720]   they are able to outperform transformers.
[00:44:32.720 --> 00:44:37.720]   Across ImageNet, the red line is gMLP.
[00:44:37.720 --> 00:44:41.720]   And as you can see, it is quite accurate enough,
[00:44:41.720 --> 00:44:45.720]   not as accurate as VIT, the largest one,
[00:44:45.720 --> 00:44:48.720]   but it has significantly less number of parameters,
[00:44:48.720 --> 00:44:52.720]   so on the order of, I'd say, 20% less,
[00:44:52.720 --> 00:44:55.720]   if I'm trying to eyeball it.
[00:44:55.720 --> 00:44:59.720]   But the authors were trying to showcase that on ImageNet
[00:44:59.720 --> 00:45:02.720]   and on other tasks.
[00:45:02.720 --> 00:45:05.720]   These models eat lesser resources,
[00:45:05.720 --> 00:45:08.720]   both in terms of parameters,
[00:45:08.720 --> 00:45:12.720]   and since these don't involve transformers,
[00:45:12.720 --> 00:45:14.720]   they also train faster.
[00:45:14.720 --> 00:45:16.720]   So they create lesser carbon footprints,
[00:45:16.720 --> 00:45:18.720]   and they're nicer than that.
[00:45:18.720 --> 00:45:25.720]   So faster training, lesser memory footprint,
[00:45:25.720 --> 00:45:32.720]   and also faster inference, because again, it's a smaller model.
[00:45:32.720 --> 00:45:36.720]   And they also showcase that these scale as nicely as a transformer model,
[00:45:36.720 --> 00:45:38.720]   or actually outperform it.
[00:45:38.720 --> 00:45:42.720]   If you look at these images, these are on the log scale.
[00:45:42.720 --> 00:45:45.720]   And they compare the parameters of a transformer
[00:45:45.720 --> 00:45:48.720]   along with a gMLP and an aMLP.
[00:45:48.720 --> 00:45:52.720]   So aMLP has what they call tiny attention.
[00:45:52.720 --> 00:45:57.720]   And if you add tiny attention to the gated MLP,
[00:45:57.720 --> 00:46:02.720]   they even outperform gMLPs further.
[00:46:02.720 --> 00:46:04.720]   I won't talk about what is tiny attention.
[00:46:04.720 --> 00:46:09.720]   It's a simple attention layer that gets applied on top of gMLP.
[00:46:09.720 --> 00:46:12.720]   That's how the authors summarize that in the paper.
[00:46:12.720 --> 00:46:21.720]   Yes, I just wanted to summarize these points.
[00:46:21.720 --> 00:46:25.720]   I know it's a short summary, but the presentation by the authors themselves
[00:46:25.720 --> 00:46:27.720]   was really short as well.
[00:46:27.720 --> 00:46:29.720]   So I don't think I'm missing any points here.
[00:46:29.720 --> 00:46:31.720]   I'll still take any questions if there are any.
[00:46:31.720 --> 00:46:40.720]   So the authors only compared with the transformers,
[00:46:40.720 --> 00:46:46.720]   not with the efficient or any other image net architectures
[00:46:46.720 --> 00:46:49.720]   on these curves and performance.
[00:46:49.720 --> 00:46:53.720]   I'll just double check right now to make sure, but not to my knowledge.
[00:46:53.720 --> 00:47:05.720]   I'm sorry, I have like five papers open, so I have to find the correct screen to share.
[00:47:05.720 --> 00:47:12.720]   Let's take a look.
[00:47:17.720 --> 00:47:19.720]   They actually do, sorry about that.
[00:47:19.720 --> 00:47:29.720]   So they've compared it across a set of continents and transformers
[00:47:29.720 --> 00:47:34.720]   where they take a look at efficient nets and VITs and DITs
[00:47:34.720 --> 00:47:39.720]   with gMLPs and also mixer MLPs.
[00:47:39.720 --> 00:47:45.720]   I think the gist of this was this still performs better than most of them,
[00:47:45.720 --> 00:47:48.720]   not across all of them, but most of them.
[00:47:48.720 --> 00:47:56.720]   I hope that answers your question.
[00:47:56.720 --> 00:47:57.720]   Yeah, thank you.
[00:47:57.720 --> 00:47:58.720]   Actually, it's very interesting.
[00:47:58.720 --> 00:48:01.720]   If you actually compare it with the efficient net,
[00:48:01.720 --> 00:48:08.720]   like efficient net still seems better, at least the B3 and B7 ones.
[00:48:08.720 --> 00:48:11.720]   And if you just look at the millions of parameters there as well,
[00:48:11.720 --> 00:48:21.720]   it's quite small compared to even the efficiencies that gMLP gives it.
[00:48:21.720 --> 00:48:26.720]   Sorry, isn't gMLP smaller?
[00:48:26.720 --> 00:48:32.720]   No, but that is significantly lower performance, 79% versus 84%.
[00:48:32.720 --> 00:48:38.720]   If you compare that with the B0 or B3, even B3,
[00:48:38.720 --> 00:48:44.720]   B3 is probably a fair comparison, that's 12 versus 20.
[00:48:44.720 --> 00:48:47.720]   So that is still efficient, it is better than,
[00:48:47.720 --> 00:48:49.720]   with just a number of parameters.
[00:48:49.720 --> 00:48:55.720]   I don't know other details, but I'm just saying that
[00:48:55.720 --> 00:48:57.720]   if you're comparing with VIT, obviously,
[00:48:57.720 --> 00:48:59.720]   it's a huge jump in number of parameters.
[00:48:59.720 --> 00:49:04.720]   No, thanks. Thanks for pointing that out.
[00:49:04.720 --> 00:49:06.720]   I wasn't paying close attention.
[00:49:07.720 --> 00:49:09.720]   No, this table is good. Thanks for pulling it up.
[00:49:09.720 --> 00:49:13.720]   Yeah, thanks. Thanks for the question.
[00:49:13.720 --> 00:49:21.720]   I'll probably take a deeper look and also post another summary on the forums.
[00:49:21.720 --> 00:49:26.720]   I think what they showcased after this in the paper was,
[00:49:26.720 --> 00:49:29.720]   if you add what they call tiny attention,
[00:49:29.720 --> 00:49:33.720]   it seems to improve the performance a little.
[00:49:33.720 --> 00:49:39.720]   Okay, I'll just double check YouTube if there are any questions.
[00:49:39.720 --> 00:49:42.720]   No, I don't see any questions. So I'll continue further.
[00:49:42.720 --> 00:49:56.720]   Yeah, one thing I liked on the paper is how they put the block
[00:49:56.720 --> 00:50:02.720]   of the architecture and the code next to it, like pseudocode,
[00:50:02.720 --> 00:50:06.720]   which is pretty much translating that diagram,
[00:50:06.720 --> 00:50:10.720]   which is super nice to see, like ResNet style,
[00:50:10.720 --> 00:50:13.720]   where you can actually see the block and you can see the code
[00:50:13.720 --> 00:50:16.720]   and you can easily understand one from the other.
[00:50:16.720 --> 00:50:20.720]   I wish all papers do that.
[00:50:20.720 --> 00:50:24.720]   I wish papers with code does that for most papers.
[00:50:30.720 --> 00:50:36.720]   So this is, I assume most of the people here would have heard about this,
[00:50:36.720 --> 00:50:41.720]   but it's called parameter prediction for unseen deep architectures.
[00:50:41.720 --> 00:50:46.720]   And to give the gist of this, I'll start by asking the question,
[00:50:46.720 --> 00:50:48.720]   what do we do in deep learning?
[00:50:48.720 --> 00:50:54.720]   We try to take an input and we define this architecture around it, right?
[00:50:54.720 --> 00:50:58.720]   We define an input layer, a bunch of hidden layers and an output layer,
[00:50:58.720 --> 00:51:02.720]   and then we train the weights and biases,
[00:51:02.720 --> 00:51:07.720]   pun intended because this is being hosted by weights and biases.
[00:51:07.720 --> 00:51:11.720]   But inside of this neural network, we try to tune these thousands
[00:51:11.720 --> 00:51:17.720]   and millions or sometimes even billions of knobs to get an output.
[00:51:17.720 --> 00:51:20.720]   One of the most interesting questions is always, of course,
[00:51:20.720 --> 00:51:23.720]   how can we decide these architectures?
[00:51:24.720 --> 00:51:31.720]   The authors ask this question, can we train a meta model
[00:51:31.720 --> 00:51:39.720]   on the list of a lot of models or on this dataset that they have created
[00:51:39.720 --> 00:51:46.720]   to generate what we learn inside of a model
[00:51:46.720 --> 00:51:49.720]   and how well can that perform?
[00:51:49.720 --> 00:51:55.720]   So they actually have this Colab notebook inside of their GitHub,
[00:51:55.720 --> 00:51:59.720]   which I'll quickly rerun because I got disconnected.
[00:51:59.720 --> 00:52:04.720]   But the authors have created this graph neural network,
[00:52:04.720 --> 00:52:09.720]   which has been trained on 1 million architectures,
[00:52:09.720 --> 00:52:12.720]   on the CIFAR-10, an ImageNet dataset.
[00:52:12.720 --> 00:52:16.720]   So it just works on those two datasets, that's the caveat.
[00:52:17.720 --> 00:52:22.720]   But if you take these two datasets and take this meta model
[00:52:22.720 --> 00:52:26.720]   of a graph neural network that the authors have created,
[00:52:26.720 --> 00:52:30.720]   and without doing anything at all, if you simply normalize the inputs,
[00:52:30.720 --> 00:52:37.720]   set your data loaders, and ask this meta model
[00:52:37.720 --> 00:52:43.720]   to predict the parameters of any architecture,
[00:52:43.720 --> 00:52:47.720]   it does a surprisingly well job of it.
[00:52:47.720 --> 00:52:54.720]   So you throw any model at the meta model
[00:52:54.720 --> 00:52:57.720]   from these two datasets at least,
[00:52:57.720 --> 00:53:01.720]   and it performs somewhat better than random.
[00:53:01.720 --> 00:53:03.720]   For some cases, it performs really well.
[00:53:03.720 --> 00:53:08.720]   This was a tuned example from the Colab notebook.
[00:53:08.720 --> 00:53:11.720]   The authors start with ResNet-50.
[00:53:11.720 --> 00:53:16.720]   And on CIFAR-10, if I remember correctly,
[00:53:16.720 --> 00:53:18.720]   it gets you around 60% accuracy.
[00:53:18.720 --> 00:53:21.720]   So definitely not the state of the art.
[00:53:21.720 --> 00:53:26.720]   But in under 2 or 3 seconds,
[00:53:26.720 --> 00:53:30.720]   it predicts the weights of this model
[00:53:30.720 --> 00:53:32.720]   that you have just thrown at the meta model.
[00:53:32.720 --> 00:53:36.720]   That performs surprisingly really well on these two datasets.
[00:53:37.720 --> 00:53:42.720]   So it's close to 58%, and we can also try...
[00:53:42.720 --> 00:53:45.720]   Let's try ResNet-18.
[00:53:45.720 --> 00:53:49.720]   So what I'm doing here is,
[00:53:49.720 --> 00:53:53.720]   I'm saying, "Hey, graph model, please take a look at ResNet-18."
[00:53:53.720 --> 00:53:57.720]   So it's being defined as a graph.
[00:53:57.720 --> 00:53:58.720]   It looks like this.
[00:53:58.720 --> 00:54:01.720]   Inside of the paper, the authors tell you what each layer means.
[00:54:01.720 --> 00:54:06.720]   I assume everyone is familiar with the details of a ResNet layer.
[00:54:06.720 --> 00:54:09.720]   So it's being defined as a graph network here.
[00:54:09.720 --> 00:54:15.720]   And we predict the parameters in under a second.
[00:54:15.720 --> 00:54:18.720]   How accurate are these on CIFAR-10?
[00:54:18.720 --> 00:54:19.720]   Let's take a look.
[00:54:19.720 --> 00:54:24.720]   So in this block, we're running inference on these parameters
[00:54:24.720 --> 00:54:30.720]   the meta model has predicted in under a second on CIFAR-10.
[00:54:30.720 --> 00:54:34.720]   Unfortunately, they're not accurate enough.
[00:54:34.720 --> 00:54:36.720]   They're slightly better than random.
[00:54:36.720 --> 00:54:38.720]   I'd say random is 10% accurate.
[00:54:38.720 --> 00:54:43.720]   But it gets us to 90% accuracy.
[00:54:43.720 --> 00:54:46.720]   Which, apart from if you stop criticizing that
[00:54:46.720 --> 00:54:49.720]   this is not a high accuracy, this is not good enough.
[00:54:49.720 --> 00:54:51.720]   I think this is a really good starting point, right?
[00:54:51.720 --> 00:54:54.720]   You could also custom define a model
[00:54:54.720 --> 00:54:58.720]   and throw it at this meta model
[00:54:58.720 --> 00:55:01.720]   which would predict the parameters.
[00:55:01.720 --> 00:55:04.720]   They of course would be slightly better than random
[00:55:04.720 --> 00:55:11.720]   sometimes even just as good as random predictions on the dataset.
[00:55:11.720 --> 00:55:16.720]   But in the broader picture, I think this gives us a really, really good starting point.
[00:55:16.720 --> 00:55:25.720]   So coming back to the paper, this is what the authors do.
[00:55:25.720 --> 00:55:29.720]   They create this dataset called DeepNets1M.
[00:55:29.720 --> 00:55:32.720]   It has 1 million architectures.
[00:55:32.720 --> 00:55:38.720]   A dataset of 1 million architectures that work on CIFAR-10 and ImageNet.
[00:55:38.720 --> 00:55:44.720]   They train a meta model on top of it which predicts parameters.
[00:55:44.720 --> 00:55:49.720]   And they propose this architecture called GHN2.
[00:55:54.720 --> 00:55:59.720]   Let me switch to the paper because I want to point out a few details from there.
[00:55:59.720 --> 00:56:05.720]   But when I discovered this paper, I was quite pleasantly surprised by how well it works.
[00:56:05.720 --> 00:56:16.720]   There we go.
[00:56:22.720 --> 00:56:25.720]   So this is what they're trying to train.
[00:56:25.720 --> 00:56:28.720]   This is the loss function, I believe.
[00:56:28.720 --> 00:56:30.720]   It's a meta learning task.
[00:56:30.720 --> 00:56:36.720]   So they iterate over M tasks, over M training architectures.
[00:56:36.720 --> 00:56:46.720]   And they take a look at pairs of inputs and HD, which is the hyper network,
[00:56:46.720 --> 00:56:49.720]   compared with the label.
[00:56:49.720 --> 00:56:52.720]   So this is how they train the meta model.
[00:56:52.720 --> 00:57:00.720]   The thing I want to point out, and I definitely took a lot of time to digest this equation,
[00:57:00.720 --> 00:57:03.720]   so if it's not instantly clear, I want to mention one thing.
[00:57:03.720 --> 00:57:10.720]   That the meta model is not being trained on the parameters of all of these 1 million models.
[00:57:10.720 --> 00:57:18.720]   It's just learning the structure and then predicting their parameters when required.
[00:57:19.720 --> 00:57:25.720]   Ramesh, I wish they had the code on the side with this as well.
[00:57:25.720 --> 00:57:27.720]   Unfortunately, they do not.
[00:57:27.720 --> 00:57:31.720]   But luckily, they make it open source, so we could go back and take a look at that.
[00:57:31.720 --> 00:57:38.720]   Inside of it, they have this meta batching computational graph that creates these virtual edges,
[00:57:38.720 --> 00:57:43.720]   which get passed through a gated graph neural network.
[00:57:43.720 --> 00:57:49.720]   That gets decoded, and from there the parameters are normalized.
[00:57:49.720 --> 00:57:55.720]   And through this, we get a loss function and we back propagate through that.
[00:57:55.720 --> 00:57:58.720]   So this is how the model is trained.
[00:57:58.720 --> 00:58:07.720]   I want to mention one more interesting bit that I found.
[00:58:07.720 --> 00:58:13.720]   So the authors evaluate, or I think this is a fair criteria for evaluation.
[00:58:13.720 --> 00:58:18.720]   They have a test-train split and then they have an out-of-distribution.
[00:58:18.720 --> 00:58:22.720]   So these are models that are wider than the in-distribution dataset,
[00:58:22.720 --> 00:58:27.720]   or deeper than the in-distribution dataset, or some are batch-norm-free.
[00:58:27.720 --> 00:58:34.720]   Some have transformer models in there, these do not.
[00:58:34.720 --> 00:58:39.720]   And the authors also evaluate their model on this.
[00:58:39.720 --> 00:58:47.720]   I won't go into looking at how accurate in terms of percentage is it across different architectures,
[00:58:47.720 --> 00:58:52.720]   but I wanted to again share this idea which I found really interesting.
[00:58:52.720 --> 00:58:57.720]   You can define any random computer vision model,
[00:58:57.720 --> 00:59:04.720]   throw it at this meta model which would give hopefully better than random initialized parameters
[00:59:04.720 --> 00:59:11.720]   in under a second for at least two datasets right now, that is the catch.
[00:59:11.720 --> 00:59:13.720]   But I found this super fascinating.
[00:59:13.720 --> 00:59:19.720]   So we have this meta model that gives us really nice baseline in my opinion.
[00:59:19.720 --> 00:59:24.720]   And it's without even being trained on the parameters that have learned about CIFAR-10 and ImageNet.
[00:59:24.720 --> 00:59:29.720]   They just know about the architectures.
[00:59:29.720 --> 00:59:34.720]   I think this paper was definitely worth the hype that it got on Twitter.
[00:59:34.720 --> 00:59:38.720]   But that was my summary of this paper and I'll take any questions right now if there are any.
[00:59:38.720 --> 00:59:56.720]   Awesome, I'll continue further.
[00:59:56.720 --> 00:59:59.720]   Sorry about the constant breaks, I have a really bad cold.
[00:59:59.720 --> 01:00:09.720]   So I need to drink water so that I don't sound too crackly.
[01:00:09.720 --> 01:00:18.720]   But I would highly encourage everyone to check this paper out.
[01:00:18.720 --> 01:00:22.720]   This brings me to the final paper.
[01:00:22.720 --> 01:00:25.720]   I think we have five minutes left.
[01:00:25.720 --> 01:00:32.720]   I'll try to summarize it or wrap up really fast.
[01:00:32.720 --> 01:00:34.720]   But it's a straightforward paper.
[01:00:34.720 --> 01:00:38.720]   It's called...
[01:00:38.720 --> 01:00:42.720]   It defines k-nets and it takes a look at image segmentation.
[01:00:42.720 --> 01:00:48.720]   I wanted to give the premise of image segmentation and talk about different types of image segmentations.
[01:00:48.720 --> 01:00:54.720]   And furthermore, the authors talk about there are two different approaches.
[01:00:54.720 --> 01:00:59.720]   So the task of image segmentation, if I stop sharing my screen,
[01:00:59.720 --> 01:01:04.720]   if you're looking at my video right now, image segmentation tells you to
[01:01:04.720 --> 01:01:09.720]   label every single pixel inside of this image.
[01:01:09.720 --> 01:01:12.720]   That is simply image segmentation.
[01:01:12.720 --> 01:01:18.720]   This is useful for self-driving cars, for understanding what's going on inside of the images,
[01:01:18.720 --> 01:01:21.720]   or any similar tasks.
[01:01:21.720 --> 01:01:27.720]   They have these three categories called instance, semantic, and panoptic.
[01:01:27.720 --> 01:01:33.720]   Panoptic was new to me and as I learned it predicts the instance ID.
[01:01:33.720 --> 01:01:38.720]   So if, assuming it doesn't know what's going on inside of an image,
[01:01:38.720 --> 01:01:45.720]   it associates an ID to every single similar object that it detects.
[01:01:45.720 --> 01:01:53.720]   Semantic tries to understand the meaning and instance tries to separate all instances by drawing bounding boxes.
[01:01:53.720 --> 01:01:56.720]   So those are the different types.
[01:01:56.720 --> 01:02:02.720]   And furthermore, the authors classify these as top-down where you have a bounding box
[01:02:02.720 --> 01:02:09.720]   being created, or bottom-up where you group different predictions or you group different pixels to give a prediction.
[01:02:09.720 --> 01:02:14.720]   So in top-down, you have the group pixels and then you draw a bounding box.
[01:02:14.720 --> 01:02:22.720]   In bottom-up, you group the labeled or the outputted pixel and then call them a prediction.
[01:02:22.720 --> 01:02:29.720]   The authors suggest one approach of using dynamic kernels, as they call it,
[01:02:29.720 --> 01:02:35.720]   that works really well across these three tasks.
[01:02:35.720 --> 01:02:39.720]   So one of the challenges right now in the semantic,
[01:02:39.720 --> 01:02:44.720]   or sorry, the image segmentation communities, the architectures that work well,
[01:02:44.720 --> 01:02:51.720]   or one or two of these, doesn't work really well across the other one or two.
[01:02:51.720 --> 01:02:57.720]   The authors claim k-net works fairly good across the multiple tasks.
[01:02:57.720 --> 01:03:06.720]   So they try to combine all of these by using what they call
[01:03:06.720 --> 01:03:10.720]   instance and semantic kernels.
[01:03:10.720 --> 01:03:15.720]   This is how it looks inside of the implementation.
[01:03:15.720 --> 01:03:19.720]   The authors also make it open-sourced.
[01:03:19.720 --> 01:03:23.720]   But you have your input that goes through a backbone and neck.
[01:03:23.720 --> 01:03:30.720]   This is quite similar across most semantic, sorry, I keep saying semantic, most image segmentation models.
[01:03:30.720 --> 01:03:35.720]   And for your feature map, you have a bunch of learned kernels
[01:03:35.720 --> 01:03:38.720]   that derive through semantic and instance kernels,
[01:03:38.720 --> 01:03:42.720]   through which you generate a mask,
[01:03:42.720 --> 01:03:45.720]   after which you apply an attention layer.
[01:03:45.720 --> 01:03:51.720]   The key thing to note here is there are a lot of kernels,
[01:03:51.720 --> 01:03:55.720]   so almost, I think, as many as the number of classes.
[01:03:55.720 --> 01:04:01.720]   You create all of these kernels that give you the output.
[01:04:01.720 --> 01:04:06.720]   These are passed through.
[01:04:06.720 --> 01:04:15.720]   So every kernel's output gets passed through multiple times towards the next kernel layer.
[01:04:15.720 --> 01:04:22.720]   And the final grouped output gives us the nice mask that we expect from a semantic,
[01:04:22.720 --> 01:04:26.720]   sorry, image segmentation model.
[01:04:26.720 --> 01:04:33.720]   So a set of learned kernels, I'm just reading through the subtext,
[01:04:33.720 --> 01:04:38.720]   and this, all images, sorry I didn't mention this earlier, all images come from the papers.
[01:04:38.720 --> 01:04:42.720]   I'll post, I'll paste a link to all of the papers afterwards.
[01:04:42.720 --> 01:04:47.720]   But a set of learned kernels first perform convolution with the feature map
[01:04:47.720 --> 01:04:49.720]   to predict the mask.
[01:04:49.720 --> 01:04:54.720]   Then the kernel update head takes the mask prediction
[01:04:54.720 --> 01:04:58.720]   and produces the class predictions,
[01:04:58.720 --> 01:05:08.720]   followed by which you have group-away dynamic kernels and mask predictions,
[01:05:08.720 --> 01:05:13.720]   which are sent towards the next kernel operation.
[01:05:13.720 --> 01:05:19.720]   So doing this a bunch of multiple times, you get your masked output.
[01:05:19.720 --> 01:05:26.720]   Inside of the paper, the author showed this works well across the three different tasks that I mentioned,
[01:05:26.720 --> 01:05:30.720]   instance semantic and panoptic segmentation.
[01:05:30.720 --> 01:05:34.720]   We're a little over time, so I don't want to dive into the results.
[01:05:34.720 --> 01:05:41.720]   But the authors have showcased that using this method of kernels
[01:05:41.720 --> 01:05:46.720]   instead of just using FCN,
[01:05:46.720 --> 01:05:51.720]   you can work really well across these three different tasks
[01:05:51.720 --> 01:05:55.720]   inside of image segmentation. I'm going to write this down.
[01:05:55.720 --> 01:06:00.720]   So to summarize, using a bunch of learnable kernels,
[01:06:00.720 --> 01:06:04.720]   and these are a lot of kernels as I showcased,
[01:06:04.720 --> 01:06:09.720]   we create this model that works really well across all of these three problems.
[01:06:09.720 --> 01:06:14.720]   So I'll again take a pause for any questions.
[01:06:14.720 --> 01:06:30.720]   I need to go back and look at the paper, but when you say kernel, are you talking about
[01:06:30.720 --> 01:06:37.720]   like the dense-net style, like multiple paths?
[01:06:37.720 --> 01:06:44.720]   Because in UNet, you have only, you know, because of the resolution, you have a single path for each of them.
[01:06:44.720 --> 01:06:50.720]   So I couldn't get a clear picture of what was happening here.
[01:06:50.720 --> 01:06:56.720]   So let me try to bring that up. I have the repository.
[01:07:05.720 --> 01:07:10.720]   I'm just trying to find the right module inside of the repository.
[01:07:10.720 --> 01:07:31.720]   I'll actually post the link on the forums, but as far as I understand, it's the dense-net style kernels.
[01:07:31.720 --> 01:07:35.720]   Okay, thank you. Thanks for doing this. It's been very helpful.
[01:07:35.720 --> 01:07:39.720]   Yeah, just a firehose of all these papers.
[01:07:39.720 --> 01:07:46.720]   Yeah, it was a challenging task. I've gone two minutes over, but I wanted to give five papers that I found interesting and then dive into them.
[01:07:46.720 --> 01:07:53.720]   So with your questions, I have noted them down. I'll come back and try to answer them.
[01:07:53.720 --> 01:07:58.720]   Okay, I'll take one last call for questions if there are any. Let me check.
[01:07:58.720 --> 01:08:10.720]   There's a question in the earlier paper on MLPs. Does the author compare GMLPs with large transformers?
[01:08:10.720 --> 01:08:14.720]   I believe they do. Yes, let me.
[01:08:14.720 --> 01:08:19.720]   Oops, I've closed that, so let me quickly find that paper and open it ASAP.
[01:08:19.720 --> 01:08:30.720]   The shorter answer is yes. Bharat, thanks for that question.
[01:08:46.720 --> 01:08:51.720]   Awesome. I don't see any other questions. Well, thank everyone for joining. I know it was a lot of papers.
[01:08:51.720 --> 01:08:56.720]   I'll probably open a poll on Twitter if we want to dive into any of these papers.
[01:08:56.720 --> 01:09:06.720]   The one I wanted to discuss was the unseen parameters one where we had a meta model, but that has been discussed really in depth on Janik Kilcher's channel.
[01:09:06.720 --> 01:09:10.720]   That's how I also gained my understanding. I'll point everyone to that video.
[01:09:10.720 --> 01:09:18.720]   I'll be opening a poll for the remaining four papers. If there's any interest, we could select a paper from there.
[01:09:18.720 --> 01:09:26.720]   Or we meet again in the first week of January, which will be a regular paper reading group because it won't be a NeurIPS paper.
[01:09:26.720 --> 01:09:32.720]   Thank you everyone for joining today. This will be the last event of the year for Weights and Biases.
[01:09:32.720 --> 01:09:37.720]   I might do a live stream where I read a paper live. But this will be the last planned event.
[01:09:37.720 --> 01:09:45.720]   I look forward to meeting everyone next year. We'll be starting with an AMA with Franois Chollet.
[01:09:45.720 --> 01:09:50.720]   I'll announce that next week. We'll be starting by reading his book on Keras.
[01:09:50.720 --> 01:09:56.720]   I always wanted to get into Keras and I think it's an amazing book. So keep an eye out for that announcement.
[01:09:56.720 --> 01:10:04.720]   I personally am quite excited about that. We'll continue reading papers. We'll continue learning about the Hugging Face API.
[01:10:04.720 --> 01:10:10.720]   We'll continue learning about Keras now and also PyTorch. So it's an exciting year coming up.
[01:10:10.720 --> 01:10:15.720]   But I want to thank everyone who's been joining us every week this year as well. Thanks everyone.
[01:10:15.720 --> 01:10:17.720]   And I hope you enjoy your holidays.
[01:10:17.720 --> 01:10:24.720]   Thank you, Sanyam. It's been a great 2021. But one thing I want to say is at least they are all Python.
[01:10:24.720 --> 01:10:30.720]   It's not like you're going through four different programming languages as you work through them.
[01:10:30.720 --> 01:10:35.720]   Are you suggesting we should consider Julia and Rust?
[01:10:35.720 --> 01:10:43.720]   No, I think Jaxx gives you enough functional programming if you really care for it.
[01:10:43.720 --> 01:10:47.720]   Yep. I forgot Jaxx study group would continue as well.
[01:10:47.720 --> 01:10:53.720]   Awesome. Thank you so much for doing everything. Have a happy...
[01:10:53.720 --> 01:10:58.720]   Thanks to you for your time and for joining us every week. I know you come back to a lot of our events.
[01:10:58.720 --> 01:11:02.720]   So I hope I live up for them to be worth your time.
[01:11:02.720 --> 01:11:08.720]   No, no, it's great. This is probably the easiest way to catch up on things.
[01:11:08.720 --> 01:11:12.720]   People could self-study and that's also that works for some people.
[01:11:12.720 --> 01:11:21.720]   But for me, like attending these ones that you have done and also what Aman has done in the past have been the quickest way to catch up on things.
[01:11:21.720 --> 01:11:27.720]   And then maybe go back and read them later. But this has been amazing. Thank you.
[01:11:27.720 --> 01:11:30.720]   And wait and buy us and everybody else who does this.
[01:11:30.720 --> 01:11:34.720]   It's the complete team. I don't want to steal all the credit. But thanks.
[01:11:34.720 --> 01:11:38.720]   I'll wrap up here. Thanks again, everyone. And we'll see you next year.
[01:11:38.720 --> 01:11:40.720]   See you. Bye.
[01:11:40.720 --> 01:11:48.960]   [BLANK_AUDIO]

