
[00:00:00.000 --> 00:00:07.360]   What I think people don't realize is we're all in it to get better every day.
[00:00:07.360 --> 00:00:12.840]   And we're sharing our skills, whether it's sharing open source, techniques, ideas, technology,
[00:00:12.840 --> 00:00:13.840]   that's where it's coming together.
[00:00:13.840 --> 00:00:20.480]   So in fact, if anything, I think this terminology, this movement is a community-based organization,
[00:00:20.480 --> 00:00:23.220]   just as like Roger said, like open source.
[00:00:23.220 --> 00:00:25.400]   No individual made this happen.
[00:00:25.400 --> 00:00:27.600]   The community owns this collectively.
[00:00:27.600 --> 00:00:31.980]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:00:31.980 --> 00:00:33.680]   And I'm your host, Lukas Biewald.
[00:00:33.680 --> 00:00:38.320]   DJ and Roger are both good friends of mine and have been working in data and ML for the
[00:00:38.320 --> 00:00:40.560]   last 20 to 30 years.
[00:00:40.560 --> 00:00:46.160]   Roger was for a long time, the co-chair of the Strata Conference and VP of Research at
[00:00:46.160 --> 00:00:47.660]   O'Reilly.
[00:00:47.660 --> 00:00:54.320]   And DJ was the head of data science at LinkedIn and the chief data scientist under the Obama
[00:00:54.320 --> 00:00:55.320]   administration.
[00:00:55.320 --> 00:01:00.800]   They both recently worked on the California COVID response using data, and I could not
[00:01:00.800 --> 00:01:02.280]   be more excited to talk to them.
[00:01:02.280 --> 00:01:03.280]   All right.
[00:01:03.280 --> 00:01:06.200]   So I have a whole bunch of really good, thoughtful questions.
[00:01:06.200 --> 00:01:10.280]   And then one question is probably slightly annoying for DJ, so I just thought I'd get
[00:01:10.280 --> 00:01:15.800]   it out of the way and start there, which is I was telling my wife, Noga, about talking
[00:01:15.800 --> 00:01:19.680]   to you this morning and I was saying, "DJ, you're the person that came up with the term
[00:01:19.680 --> 00:01:20.680]   data scientist."
[00:01:20.680 --> 00:01:24.060]   And then my wife was like, "No, no, that's not true."
[00:01:24.060 --> 00:01:28.760]   And then we were discussing it and then I was looking it up and I couldn't figure it
[00:01:28.760 --> 00:01:29.760]   out.
[00:01:29.760 --> 00:01:32.600]   And so I was just wondering if you could let me know what the real story is, because I
[00:01:32.600 --> 00:01:36.360]   feel like at least you made the term popular, right?
[00:01:36.360 --> 00:01:37.360]   Yeah.
[00:01:37.360 --> 00:01:42.940]   So I think the first part to call out here is Roger actually gets credit for making big
[00:01:42.940 --> 00:01:46.880]   data popular versus when people were talking about data.
[00:01:46.880 --> 00:01:50.040]   So Roger deserves credit for big data.
[00:01:50.040 --> 00:01:51.040]   Wow.
[00:01:51.040 --> 00:01:53.840]   I think the part about that, most people don't realize that.
[00:01:53.840 --> 00:01:56.360]   Roger never talks about it, but he's the guy.
[00:01:56.360 --> 00:02:01.360]   And I remember going to an early talk of Roger's where he's like big data.
[00:02:01.360 --> 00:02:03.520]   And I'm like, "Who's talking about big data?
[00:02:03.520 --> 00:02:04.520]   Like, what is it?
[00:02:04.520 --> 00:02:06.160]   Like, is it all big, all data big?"
[00:02:06.160 --> 00:02:09.000]   And he laid out an argument for it and I was like, "Oh yeah."
[00:02:09.000 --> 00:02:11.680]   And then you saw it kind of catch fire afterwards.
[00:02:11.680 --> 00:02:16.280]   And so Roger, you should talk about big data, but I'm happy to talk about where I think
[00:02:16.280 --> 00:02:18.480]   the origin story of data science comes from.
[00:02:18.480 --> 00:02:19.480]   Yeah.
[00:02:19.480 --> 00:02:20.480]   Let's start with that.
[00:02:20.480 --> 00:02:23.720]   And then I want to hear about the origin story of big data and then what I should trademark
[00:02:23.720 --> 00:02:26.520]   and what domain names I should buy.
[00:02:26.520 --> 00:02:27.520]   Totally.
[00:02:27.520 --> 00:02:36.120]   So, you know, part of the thing of data people, especially in that early era of LinkedIn and
[00:02:36.120 --> 00:02:41.760]   Facebook and others, was that there was a community starting to form of people getting
[00:02:41.760 --> 00:02:45.520]   together and what do you call themselves?
[00:02:45.520 --> 00:02:49.880]   And so people had many different versions of names that were going on.
[00:02:49.880 --> 00:02:54.640]   And even going back to the '60s, there's been arguments with people where they found documentation
[00:02:54.640 --> 00:02:58.320]   of people titling things data science.
[00:02:58.320 --> 00:03:03.320]   And so it's been floating around and I wouldn't be surprised if we find a lot more examples
[00:03:03.320 --> 00:03:07.440]   of what people had been calling data science.
[00:03:07.440 --> 00:03:11.440]   What was also going on at the same time as people were trying to figure this out, people
[00:03:11.440 --> 00:03:15.360]   were playing around with the terms like analytics scientists.
[00:03:15.360 --> 00:03:18.640]   And Jonathan Goldman was a guy who came up with that.
[00:03:18.640 --> 00:03:23.400]   Pete Scomarach was talking about the idea of a data artist.
[00:03:23.400 --> 00:03:26.560]   And that actually got raised at a board meeting at LinkedIn.
[00:03:26.560 --> 00:03:28.200]   Was like, are we painting a palette?
[00:03:28.200 --> 00:03:31.920]   Are we creating a palette with that?
[00:03:31.920 --> 00:03:39.120]   What I recall is, and what we put in our book, was when we were getting ready for the IPO
[00:03:39.120 --> 00:03:43.280]   for LinkedIn, Facebook was it, Jeff Hammerbacher and I both got together and we were like,
[00:03:43.280 --> 00:03:45.760]   "Hey, HR is breathing down both of our necks.
[00:03:45.760 --> 00:03:46.760]   What do we kind of call people?"
[00:03:46.880 --> 00:03:48.520]   And he said, "We had too many different job titles."
[00:03:48.520 --> 00:03:51.120]   And so it was like, "Well, what's the listing?"
[00:03:51.120 --> 00:03:56.080]   And what actually went through those is you start to think about the terms like analyst
[00:03:56.080 --> 00:03:58.160]   felt a little too Wall Street.
[00:03:58.160 --> 00:04:03.120]   Research scientist was a title that Yahoo had really popularized for where the data
[00:04:03.120 --> 00:04:06.200]   scientists sat with Kymer Marlow and other people.
[00:04:06.200 --> 00:04:10.400]   But they've kind of always pushed out to the side of the product process or the product
[00:04:10.400 --> 00:04:11.400]   engineering process.
[00:04:11.400 --> 00:04:14.880]   And so there was kind of like a little too researchy.
[00:04:14.880 --> 00:04:21.600]   If you kind of go with some of the things more statistician or economics or any of those,
[00:04:21.600 --> 00:04:24.120]   you're creating a war right off the bat.
[00:04:24.120 --> 00:04:29.480]   But also the term hadn't really quite caught on except for places like at Google with how
[00:04:29.480 --> 00:04:31.320]   variants team.
[00:04:31.320 --> 00:04:34.680]   And so what we did is we kind of went to that list.
[00:04:34.680 --> 00:04:37.280]   Jeff actually was the one who kind of was like, "Well, we're starting to think about
[00:04:37.280 --> 00:04:39.880]   this term data scientist."
[00:04:39.880 --> 00:04:42.960]   And I took it back to, it was like, "Well, that seems plenty reasonable."
[00:04:42.960 --> 00:04:47.800]   I took it back to the team and Monica Ragati actually had the idea of saying, "Well, we're
[00:04:47.800 --> 00:04:48.800]   LinkedIn.
[00:04:48.800 --> 00:04:50.320]   We have all the job postings.
[00:04:50.320 --> 00:04:55.680]   Let's post all the jobs with the different titles and see what everyone applies to."
[00:04:55.680 --> 00:04:57.120]   And so we did that.
[00:04:57.120 --> 00:04:59.520]   Monica actually constructed the test for it.
[00:04:59.520 --> 00:05:00.800]   And guess what?
[00:05:00.800 --> 00:05:04.300]   Everyone we hired was in the term data scientist.
[00:05:04.300 --> 00:05:06.020]   And so that's why it sticks.
[00:05:06.020 --> 00:05:09.880]   And so I think a lot of people have gotten caught up in this origin story, but I think
[00:05:09.880 --> 00:05:11.560]   there's two parts that are important.
[00:05:11.560 --> 00:05:16.120]   One, it exemplifies that this was a team effort.
[00:05:16.120 --> 00:05:19.920]   It's very easy for people to say, "Oh, DJ and Jeff did this."
[00:05:19.920 --> 00:05:22.320]   It's a community-wide thing.
[00:05:22.320 --> 00:05:27.080]   This was a broad, diverse community that was all coming together to make this happen.
[00:05:27.080 --> 00:05:31.360]   The second is, why did it take off?
[00:05:31.360 --> 00:05:35.920]   And not only did we sort of data science our way into this title, but the reason I think
[00:05:35.920 --> 00:05:39.280]   it takes off is because no one knows what the hell it means.
[00:05:39.280 --> 00:05:45.320]   And I say that with great seriousness, and Roger knows, is you kind of watch these fields
[00:05:45.320 --> 00:05:49.520]   evolve, and you've seen this, Lucas, tremendous amount through all your work over the years,
[00:05:49.520 --> 00:05:52.360]   is people like to put people in boxes.
[00:05:52.360 --> 00:05:54.000]   They like to put skill sets in the boxes.
[00:05:54.000 --> 00:05:58.480]   And they're like, "Oh, if you're doing data, you're not supposed to do product.
[00:05:58.480 --> 00:06:00.880]   If you're doing product, you're not supposed to do engineering."
[00:06:00.880 --> 00:06:05.520]   And we're sort of like, "Why can't we do it all if we've got the skill set?"
[00:06:05.520 --> 00:06:10.160]   And the data scientist person, people are kind of like, "Okay, they're smart and they
[00:06:10.160 --> 00:06:11.160]   have superpowers.
[00:06:11.160 --> 00:06:14.720]   We don't understand them, but they really add value."
[00:06:14.720 --> 00:06:18.680]   And if you kind of pull on that string of why do they add value, the reason finally
[00:06:18.680 --> 00:06:22.600]   is because they're allowed in the room and they have context.
[00:06:22.600 --> 00:06:26.980]   And once you have context, you can take your skills and apply it to the problem faster
[00:06:26.980 --> 00:06:28.720]   than other people can.
[00:06:28.720 --> 00:06:31.680]   And so it's the ambiguity that has come out.
[00:06:31.680 --> 00:06:35.960]   And I think that has led to the rise of the title being actually taken over.
[00:06:35.960 --> 00:06:40.680]   If you asked Jeff or me back then, I'm very confident that I would have said, "No way
[00:06:40.680 --> 00:06:42.960]   this is going to be a thing that sticks.
[00:06:42.960 --> 00:06:46.440]   This is going to be really something that we really label our teams."
[00:06:46.440 --> 00:06:51.960]   And I think part of the reason it also took off in there is, frankly, LinkedIn and Facebook
[00:06:51.960 --> 00:06:53.640]   were very successful in their IPOs.
[00:06:53.640 --> 00:06:55.600]   And people said, "What's behind that?"
[00:06:55.600 --> 00:06:59.640]   And people said, "Ah, Roger's term, big data."
[00:06:59.640 --> 00:07:03.880]   And the whole thing that makes big data come alive are these data scientists.
[00:07:03.880 --> 00:07:09.760]   And so I think that in my view is how we should think about where this is coming from and
[00:07:09.760 --> 00:07:12.640]   where it also gives us indication of where we need to go.
[00:07:12.640 --> 00:07:19.200]   But I guess there must've been something new going on with the sort of social network companies
[00:07:19.200 --> 00:07:25.200]   of I guess the mid to late aughts that there was a new need or something.
[00:07:25.200 --> 00:07:30.080]   I run, I think, a pretty standard business model at Weights & Biases, and it's really
[00:07:30.080 --> 00:07:34.120]   hard to imagine operating without a data science team.
[00:07:34.120 --> 00:07:37.080]   There must've been some kind of function before that.
[00:07:37.080 --> 00:07:41.800]   What sort of changed in the requirements that it was needed to make a new role that didn't
[00:07:41.800 --> 00:07:42.800]   exist before?
[00:07:42.800 --> 00:07:50.160]   Well, let me lay out an argument from the late '90s, and then Roger should dovetail
[00:07:50.160 --> 00:07:54.520]   because he's seen the whole evolution of this.
[00:07:54.520 --> 00:08:01.240]   What I think had happened, and this really started around just around 9/11 time period,
[00:08:01.240 --> 00:08:05.700]   is people were like, "Wait a second, there's signal in the noise, but no one's actually
[00:08:05.700 --> 00:08:08.160]   able to capitalize on it.
[00:08:08.160 --> 00:08:09.160]   How do we find the signal?
[00:08:09.160 --> 00:08:10.160]   How do we do something with it?"
[00:08:10.160 --> 00:08:15.520]   And so you did see a lot of the early e-commerce companies like eBay and others actually had
[00:08:15.520 --> 00:08:17.560]   the equivalent of data science team.
[00:08:17.560 --> 00:08:23.040]   They were just analytics functions in those roles, and people were called business analysts
[00:08:23.040 --> 00:08:24.960]   or other different titles.
[00:08:24.960 --> 00:08:29.240]   Google had a lot of these people and it had a lot of impact.
[00:08:29.240 --> 00:08:34.500]   I think the seminal difference that we saw, which was really building on Yahoo's research
[00:08:34.500 --> 00:08:42.760]   team and those kind of groups, is that the data team could actually build products, not
[00:08:42.760 --> 00:08:44.600]   just come up with insights.
[00:08:44.600 --> 00:08:51.440]   And so at LinkedIn, the data science team had one part, which is, "Hey, how are we doing?
[00:08:51.440 --> 00:08:53.840]   Metrics, dashboards, all of that thing."
[00:08:53.840 --> 00:08:58.400]   Had another component, which is, "You're responsible for revenue.
[00:08:58.400 --> 00:09:00.400]   You're responsible for engagement.
[00:09:00.400 --> 00:09:04.940]   Your responsibility is to build things, make stuff happen.
[00:09:04.940 --> 00:09:09.580]   And you're a design team, and a product team, an engineering team, all that comes together."
[00:09:09.580 --> 00:09:13.400]   And then there was another part, which is, "You got to open up new turf and help things
[00:09:13.400 --> 00:09:14.400]   in new ways."
[00:09:14.400 --> 00:09:16.640]   And so that was like security.
[00:09:16.640 --> 00:09:21.420]   Because if you're going to fight bad guys who got super sophisticated data tools, don't
[00:09:21.420 --> 00:09:22.420]   let them get to you.
[00:09:22.420 --> 00:09:26.920]   And the only way to survive that is by bringing increased data science and functionality to
[00:09:26.920 --> 00:09:27.920]   actually to bear to that.
[00:09:27.920 --> 00:09:30.640]   But Roger, let me hand it back to you.
[00:09:30.640 --> 00:09:31.640]   Sure.
[00:09:31.640 --> 00:09:35.200]   I mean, I think what changed, and it was right around the timeframe you're talking about,
[00:09:35.200 --> 00:09:39.400]   is suddenly there were little companies with big data.
[00:09:39.400 --> 00:09:44.040]   And they weren't going to go out and buy Oracle, or I was at Sybase in '99.
[00:09:44.040 --> 00:09:45.040]   They weren't going to go with Sybase.
[00:09:45.040 --> 00:09:46.600]   They needed to come up with their own thing.
[00:09:46.600 --> 00:09:52.140]   And I think those big social media companies had to do was write quickly.
[00:09:52.140 --> 00:09:54.860]   And that meant in a distributed fashion.
[00:09:54.860 --> 00:10:01.120]   And then you've got Jeff Dean doing MapReduce at Google to start that going.
[00:10:01.120 --> 00:10:06.580]   And then you've got the Yahoo people taking that idea and making it out.
[00:10:06.580 --> 00:10:10.100]   And what's interesting is that it kind of dovetails with open source becoming kind of
[00:10:10.100 --> 00:10:11.700]   mainstream.
[00:10:11.700 --> 00:10:16.580]   Because now you've got people who are willing to use open source because their company is
[00:10:16.580 --> 00:10:17.580]   banking on it.
[00:10:17.580 --> 00:10:21.980]   I remember talking to Abder Choudhury, who was the chief data scientist at Twitter.
[00:10:21.980 --> 00:10:24.740]   And this might have been like 2004, 2005.
[00:10:24.740 --> 00:10:30.180]   And he's like, I wouldn't use Oracle because I can't go into the code and fix it if there's
[00:10:30.180 --> 00:10:31.180]   a problem.
[00:10:31.180 --> 00:10:33.100]   And that became a really important thing.
[00:10:33.100 --> 00:10:36.180]   And I also think that that was kind of the era when the best engineers in the world were
[00:10:36.180 --> 00:10:38.380]   really centering on the Bay Area.
[00:10:38.380 --> 00:10:43.580]   And that's kind of where in some ways, when I wrote the essay about big data and stuff,
[00:10:43.580 --> 00:10:45.580]   I was doing those talks where I was using big data.
[00:10:45.580 --> 00:10:51.660]   I was trying to capture this notion of having to store a lot of data, do it in a distributed
[00:10:51.660 --> 00:10:56.660]   way, analyzing big masses of data instead of kind of little or medium sized pieces of
[00:10:56.660 --> 00:10:57.780]   data.
[00:10:57.780 --> 00:11:04.420]   And that this became more the core of what companies were doing.
[00:11:04.420 --> 00:11:06.660]   And trying to get that all in one thing.
[00:11:06.660 --> 00:11:12.380]   That's the way DJ and I met, as I was writing a journal article with Ben Lorica on big data.
[00:11:12.380 --> 00:11:13.380]   And I knew Jonathan Goldman.
[00:11:13.380 --> 00:11:15.860]   And he said, well, you should come and talk to us.
[00:11:15.860 --> 00:11:16.860]   And so we did.
[00:11:16.860 --> 00:11:21.420]   And we really liked the way DJ's team had people arrayed all across these functions that
[00:11:21.420 --> 00:11:23.500]   we used to think of it were in separate pieces.
[00:11:23.500 --> 00:11:27.340]   Like he mentioned the product piece, he had visualization people, and they were all kind
[00:11:27.340 --> 00:11:28.340]   of together.
[00:11:28.340 --> 00:11:31.260]   And we're like, you know, not only is it big data, it's kind of like a big group of people
[00:11:31.260 --> 00:11:38.460]   with these multiple functions that ended up being worth like integrating and worth coordinating
[00:11:38.460 --> 00:11:39.460]   with.
[00:11:39.460 --> 00:11:43.780]   And I think it was just a big thing, because I had been doing data warehousing in the late
[00:11:43.780 --> 00:11:44.860]   '90s.
[00:11:44.860 --> 00:11:49.020]   And that was as siloed a thing as imaginable in most companies.
[00:11:49.020 --> 00:11:51.500]   It was not part of the mainstream.
[00:11:51.500 --> 00:11:55.940]   And I think what happened is all of a sudden you have LinkedIn, Facebook, Google, where
[00:11:55.940 --> 00:12:01.180]   that was what the company did, is capture a lot of data and try to make sense of it
[00:12:01.180 --> 00:12:06.220]   to in some ways improve what they're doing and in some ways to monetize what they were
[00:12:06.220 --> 00:12:07.220]   doing.
[00:12:07.220 --> 00:12:08.220]   So it's a lot of incentive.
[00:12:08.220 --> 00:12:12.020]   And it was just driven in a whole different direction because of the open source piece
[00:12:12.020 --> 00:12:13.020]   of it too.
[00:12:13.020 --> 00:12:14.780]   And I'll just add one thing about big data.
[00:12:14.780 --> 00:12:16.980]   So there's one personal part and then one other part.
[00:12:16.980 --> 00:12:20.980]   So the personal part is I've worked at home for a long time.
[00:12:20.980 --> 00:12:24.460]   And I used to often bike to go a daily shop.
[00:12:24.460 --> 00:12:26.660]   So I'd go get some things.
[00:12:26.660 --> 00:12:31.620]   And so once every other 10 days or so, I had to do a big shop.
[00:12:31.620 --> 00:12:37.940]   And I think this is just like a verbal tick that I use that there's little and big things.
[00:12:37.940 --> 00:12:43.420]   But the other thing is I got access to Simply Hired's data and it was huge for me.
[00:12:43.420 --> 00:12:47.260]   It was two terabytes and two billion rows and I needed help.
[00:12:47.260 --> 00:12:51.900]   And I got introduced to Scott Yair at Greenplum and we started doing that.
[00:12:51.900 --> 00:12:57.860]   So I know the first talk I gave where I officially used big data was around that distributed
[00:12:57.860 --> 00:12:59.820]   data management and doing that.
[00:12:59.820 --> 00:13:04.220]   I guess the last thing I should say is I was at O'Reilly, which is famously meme generating
[00:13:04.220 --> 00:13:05.220]   as a company.
[00:13:05.220 --> 00:13:06.220]   Incredibly successful.
[00:13:06.220 --> 00:13:07.220]   Right.
[00:13:07.220 --> 00:13:10.940]   I had a platform that people actually listened to.
[00:13:10.940 --> 00:13:11.940]   Yeah.
[00:13:11.940 --> 00:13:17.700]   The part of there I think that I think hopefully people are also taking away is this has been
[00:13:17.700 --> 00:13:20.900]   a very big tent phenomena.
[00:13:20.900 --> 00:13:27.040]   And Abder and Scott Yair, Lucas, you, all of us work together.
[00:13:27.040 --> 00:13:33.660]   People don't realize when we were first comparing these ideas of how do we use Mechanical Turk
[00:13:33.660 --> 00:13:38.340]   and then we actually, people probably don't realize, we ran a test head to head with each
[00:13:38.340 --> 00:13:41.060]   other of how could my team do versus you.
[00:13:41.060 --> 00:13:43.220]   And we learned a lot more from each other.
[00:13:43.220 --> 00:13:45.700]   We ended up going with you and using you.
[00:13:45.700 --> 00:13:51.140]   But what I think people don't realize is we're all in it to get better every day.
[00:13:51.140 --> 00:13:56.620]   And we're sharing our skills, whether it's sharing open source techniques, ideas, technology,
[00:13:56.620 --> 00:13:57.620]   that's where it's coming together.
[00:13:57.620 --> 00:14:02.340]   So in fact, if anything, I think, you know, like this terminology, this movement is a
[00:14:02.340 --> 00:14:06.780]   community based organization, just as like Roger said, like open source.
[00:14:06.780 --> 00:14:09.140]   And no individual made this happen.
[00:14:09.140 --> 00:14:12.420]   The community owns this collectively.
[00:14:12.420 --> 00:14:15.660]   You know, I'll bring up an interesting kind of adjunct to that.
[00:14:15.660 --> 00:14:19.300]   I don't know when it was, it was probably around 2010, 2011.
[00:14:19.300 --> 00:14:25.420]   But at the time there was, you know, MapReduce at Google, and then there was Hadoop starting
[00:14:25.420 --> 00:14:28.140]   to make a lot of waves out in the world.
[00:14:28.140 --> 00:14:32.460]   And Google, the people I know at Google were very much in support of Hadoop.
[00:14:32.460 --> 00:14:36.320]   And this was, I think people were evolving their kind of thinking about open source.
[00:14:36.320 --> 00:14:41.420]   The reason Google was so in support of Hadoop is that if you've learned MapReduce on Hadoop,
[00:14:41.420 --> 00:14:45.340]   they could hire you, that it was a way of training people.
[00:14:45.340 --> 00:14:49.180]   And I think now open source is a different dynamic on why people do it.
[00:14:49.180 --> 00:14:51.860]   But back then that ended up being an important dynamic.
[00:14:51.860 --> 00:14:56.900]   But I think when you're ready to ML, where everything is open source now, is that that's
[00:14:56.900 --> 00:15:00.540]   the logical thing is that the ML tools are cool.
[00:15:00.540 --> 00:15:02.180]   They do a lot of stuff.
[00:15:02.180 --> 00:15:03.180]   They're great.
[00:15:03.180 --> 00:15:05.640]   But what you really need is people.
[00:15:05.640 --> 00:15:09.380]   And the more people you get involved, the more likely these things are going to get
[00:15:09.380 --> 00:15:12.420]   traction and become part of the mainstream.
[00:15:12.420 --> 00:15:18.660]   So that is why PyTorch and TensorFlow and those kinds of things are, I think, if you
[00:15:18.660 --> 00:15:22.980]   call it in the public domain, but in an open source way are shareable because what's really
[00:15:22.980 --> 00:15:25.740]   more important is what you do with them than the tools themselves.
[00:15:25.740 --> 00:15:32.820]   You know, it's funny not to turn this into a whole reminiscing session, but DJ, I remember
[00:15:32.820 --> 00:15:37.500]   right after meeting with you, back then, I think you had recently left eBay.
[00:15:37.500 --> 00:15:41.060]   I remember I got a meeting with the eBay CTO, which was a huge deal for me at the time,
[00:15:41.060 --> 00:15:43.420]   because we were selling data products.
[00:15:43.420 --> 00:15:48.620]   And I have this vivid memory of him telling me that he couldn't possibly store all of
[00:15:48.620 --> 00:15:50.260]   the user data.
[00:15:50.260 --> 00:15:56.180]   He basically erased 99.9% of it and just saved the little bits of the rest of it, because
[00:15:56.180 --> 00:15:58.300]   that's kind of all you needed to do anything important.
[00:15:58.300 --> 00:16:03.260]   I remember thinking, wow, that seems so painful to erase that data.
[00:16:03.260 --> 00:16:04.860]   You might want that data.
[00:16:04.860 --> 00:16:09.140]   It's funny because now I feel like no company would dream of erasing data.
[00:16:09.140 --> 00:16:14.300]   It makes me wonder how much of all this was just driven by the ability to actually store
[00:16:14.300 --> 00:16:17.420]   all of this data, right?
[00:16:17.420 --> 00:16:21.060]   Actually people don't always realize, this is one of the reasons I actually moved on
[00:16:21.060 --> 00:16:25.580]   from eBay, is the straw that broke the camel's back.
[00:16:25.580 --> 00:16:29.780]   And eBay obviously recovered from this, but it was that there was a big argument from
[00:16:29.780 --> 00:16:33.520]   a number of us that said, "Hey, every time we want to do something interesting, we have
[00:16:33.520 --> 00:16:38.540]   to go to the lords of the data warehouse and ask permission."
[00:16:38.540 --> 00:16:42.700]   And then to get something done took months.
[00:16:42.700 --> 00:16:45.460]   And it was pretty easy, obvious stuff.
[00:16:45.460 --> 00:16:49.300]   And so one of the things that, and I remember this meeting very clearly, is a number of
[00:16:49.300 --> 00:16:51.460]   us had this technical session.
[00:16:51.460 --> 00:16:55.220]   We basically said, "Look, the bet for us has to be Hadoop.
[00:16:55.220 --> 00:16:56.740]   There's no other way.
[00:16:56.740 --> 00:17:03.060]   We cannot sit on traditional infrastructure and do the problems that we need to compete.
[00:17:03.060 --> 00:17:04.920]   It's business critical."
[00:17:04.920 --> 00:17:07.280]   And those ideas got pushed out.
[00:17:07.280 --> 00:17:13.400]   And effectively all those people that were on that mission of doing it all left to other
[00:17:13.400 --> 00:17:14.400]   things.
[00:17:14.400 --> 00:17:18.720]   Chris Ricamini, one of the kind of key people behind Kafka, was one of them as well.
[00:17:18.720 --> 00:17:24.680]   And what it showed is, and I think this is something that companies need to grok with
[00:17:24.680 --> 00:17:30.380]   respect to machine learning, is that there are paradigm jumping moments.
[00:17:30.380 --> 00:17:35.040]   And if you don't jump, you will have to jump later, but you're going to be so far behind
[00:17:35.040 --> 00:17:36.040]   the curve.
[00:17:36.040 --> 00:17:41.480]   Because eBay obviously adopted one of the biggest Hadoop clusters with Cloudera, five,
[00:17:41.480 --> 00:17:43.320]   seven years later.
[00:17:43.320 --> 00:17:46.480]   But they could have been so much more competitive and done so much more.
[00:17:46.480 --> 00:17:52.320]   And it strikes me that there's a similar moment that is happening around machine learning,
[00:17:52.320 --> 00:18:00.320]   that if you don't get on the bandwagon now, you're late, if not already late.
[00:18:00.320 --> 00:18:01.320]   Interesting.
[00:18:01.320 --> 00:18:03.280]   I mean, of course, I would agree with that.
[00:18:03.280 --> 00:18:08.560]   I actually had a question I wrote down for you, Roger, that's maybe poorly formed, but
[00:18:08.560 --> 00:18:11.000]   I feel like you kind of had front row seats to this.
[00:18:11.000 --> 00:18:15.100]   I'm not even sure if it's completely true, but it feels like there was this sort of massive
[00:18:15.100 --> 00:18:19.300]   shift from Hadoop to Spark maybe five or six years ago.
[00:18:19.300 --> 00:18:21.280]   And it seemed like it was kind of slow.
[00:18:21.280 --> 00:18:26.280]   And then all of a sudden, I was kind of wondering, it seemed like you just really saw that.
[00:18:26.280 --> 00:18:32.360]   And I was wondering what you think about that and if there's some fundamental problem with
[00:18:32.360 --> 00:18:36.520]   Hadoop that they could have fixed or if there's something coming beyond Spark.
[00:18:36.520 --> 00:18:39.160]   I was really curious to get your take on that.
[00:18:39.160 --> 00:18:40.160]   Yeah.
[00:18:40.160 --> 00:18:45.640]   So I actually have kind of strong opinions on this, but they'd be easy to try to puncture
[00:18:45.640 --> 00:18:46.640]   holes in.
[00:18:46.640 --> 00:18:50.880]   I think Hadoop was the right engine and that people needed a read engine.
[00:18:50.880 --> 00:18:54.880]   So the fundamental early problem was the one you guys just talked about with eBay.
[00:18:54.880 --> 00:18:57.000]   How do you get all this stuff to disks?
[00:18:57.000 --> 00:19:01.120]   Well distributed was the way you do it, but it was kind of slow to get stuff out.
[00:19:01.120 --> 00:19:05.760]   And things like no schema, that ends up isn't really very good for if you're trying to do
[00:19:05.760 --> 00:19:09.280]   any analytics on it.
[00:19:09.280 --> 00:19:13.520]   And I remember I got a lot of arguments with people where they were telling me MapReduce
[00:19:13.520 --> 00:19:15.000]   is the way everyone's going to work.
[00:19:15.000 --> 00:19:18.360]   And I'm like, there's just no way that that's going to be the case.
[00:19:18.360 --> 00:19:20.760]   There's just too much embedded SQL.
[00:19:20.760 --> 00:19:25.840]   SQL is very productive and in a place like the Bay Area with its high concentration of
[00:19:25.840 --> 00:19:29.880]   great engineers, yeah, a lot of people were getting MapReduce, but a lot of people weren't
[00:19:29.880 --> 00:19:30.880]   getting MapReduce.
[00:19:30.880 --> 00:19:33.800]   In fact, when I was first learning, I had the lucky thing because I was working with
[00:19:33.800 --> 00:19:39.120]   Greenplum, Joe Hellerstein used to come to my home and we'd go through MapReduce problems
[00:19:39.120 --> 00:19:41.600]   as they were trying to put in a MapReduce part of it.
[00:19:41.600 --> 00:19:46.280]   But my sense was that it was going to be SQL that was going to win and that the analytics,
[00:19:46.280 --> 00:19:49.880]   instead of just storing stuff, which like step one is the kind of the analytics support
[00:19:49.880 --> 00:19:51.960]   that really mattered.
[00:19:51.960 --> 00:19:56.360]   And Spark was just better at that than Hadoop was.
[00:19:56.360 --> 00:20:01.320]   And when I think it was Impala was the first time that really SQL was kind of available.
[00:20:01.320 --> 00:20:05.320]   Well, Spark came right away with SQL.
[00:20:05.320 --> 00:20:09.440]   The other thing that happened, and this is just like just kind of an anomaly of, not
[00:20:09.440 --> 00:20:12.840]   an anomaly, but just one of those harmonic convergences.
[00:20:12.840 --> 00:20:18.960]   Python was starting to become just a de facto language right around when Spark had a Python
[00:20:18.960 --> 00:20:19.960]   binding.
[00:20:19.960 --> 00:20:26.040]   And that meant a lot more people were just able to get into and do the kind of work that
[00:20:26.040 --> 00:20:27.040]   made sense.
[00:20:27.040 --> 00:20:29.360]   Also, just part of it was Spark being in memory.
[00:20:29.360 --> 00:20:31.520]   It was just fast.
[00:20:31.520 --> 00:20:36.200]   So as long as you were able to make your RDSs and eventually the more table-like things
[00:20:36.200 --> 00:20:39.120]   in memory, then you could run really fast queries.
[00:20:39.120 --> 00:20:43.360]   And I think what it comes down to with all this, you had mentioned the kind of disconnect
[00:20:43.360 --> 00:20:47.000]   DJ between you're getting data at eBay and having to wait for it.
[00:20:47.000 --> 00:20:49.760]   So I was running the equivalent organization at Sybase.
[00:20:49.760 --> 00:20:52.480]   And I had the data engineers and the data scientists right together.
[00:20:52.480 --> 00:20:55.080]   Of course, they weren't called data scientists.
[00:20:55.080 --> 00:20:57.140]   And I ran the group.
[00:20:57.140 --> 00:20:58.760]   I did both things.
[00:20:58.760 --> 00:21:02.840]   And the reason we did that is so that no one could complain that it took months to get
[00:21:02.840 --> 00:21:06.680]   anything because I wanted to keep everything kind of tight together.
[00:21:06.680 --> 00:21:11.600]   So I think, go forward to when Spark started coming out, you were able to actually do data
[00:21:11.600 --> 00:21:16.040]   engineering and data science work all in the same platform.
[00:21:16.040 --> 00:21:18.700]   You didn't need someone to pull all this stuff for you.
[00:21:18.700 --> 00:21:23.320]   You could do it yourself because it was SQL and you could just kind of pull it in and
[00:21:23.320 --> 00:21:27.360]   go through the whole thing up to even early ML stuff at the time.
[00:21:27.360 --> 00:21:30.080]   The other thing is Spark is cheap.
[00:21:30.080 --> 00:21:34.640]   So like one of the things that people like, the eBay team had some amazing technologists.
[00:21:34.640 --> 00:21:40.820]   They're sort of all that sun sort of generation of deep, deep infrastructure thinkers.
[00:21:40.820 --> 00:21:44.920]   And so they used to have a tip code bus and they had basically stream processor sitting
[00:21:44.920 --> 00:21:45.920]   on top of it.
[00:21:45.920 --> 00:21:48.560]   Except it's very expensive.
[00:21:48.560 --> 00:21:53.180]   Just because of the structural constraints of the time.
[00:21:53.180 --> 00:21:58.160]   With Spark, one of the things that was beautiful about it that we sort of saw is like, wait,
[00:21:58.160 --> 00:22:00.760]   we could have a stream processor finally?
[00:22:00.760 --> 00:22:07.080]   We can actually do computation without having to wait and doing all sitting behind the ETL
[00:22:07.080 --> 00:22:08.340]   pane.
[00:22:08.340 --> 00:22:14.160]   That gave us a massive leg up on a key set of problems, mostly that were time bound,
[00:22:14.160 --> 00:22:16.880]   like fraud and security kind of issues.
[00:22:16.880 --> 00:22:24.040]   And so that was natural to gravitate to versus the Hadoop frameworks and MapReduce frameworks.
[00:22:24.040 --> 00:22:27.480]   The other part is I think Roger's pointing out, which I still think is there is a lot
[00:22:27.480 --> 00:22:28.480]   of people want to work.
[00:22:28.480 --> 00:22:29.960]   And we saw this for Kafka also.
[00:22:29.960 --> 00:22:34.080]   It was like, we were going to put the logic layer on there, but it just takes so much
[00:22:34.080 --> 00:22:37.760]   time of development, even with the open source community to graduate these things.
[00:22:37.760 --> 00:22:41.180]   And Spark didn't have to worry about the underlying bus.
[00:22:41.180 --> 00:22:48.760]   But the part there that I think that we're seeing is different community, data has moved
[00:22:48.760 --> 00:22:51.840]   into a space of just like the background view.
[00:22:51.840 --> 00:22:55.800]   You've got specialized tools, right?
[00:22:55.800 --> 00:22:58.220]   For depending on the team, you're going to need different things.
[00:22:58.220 --> 00:23:04.800]   Because most people who work in MapReduce, that is a leap way too far for most individuals
[00:23:04.800 --> 00:23:08.920]   and teams, especially when you're bringing in fresh talent from other disciplines or
[00:23:08.920 --> 00:23:09.920]   other areas.
[00:23:09.920 --> 00:23:15.080]   Yeah, I just want to bring up one, and this is a bit of a corollary to this stuff, but
[00:23:15.080 --> 00:23:18.640]   when things first started, I know Lucas, we haven't even brought up Math Club yet, but
[00:23:18.640 --> 00:23:24.920]   you know, Math Club in San Francisco, diversity, cognitive, physical background, all these
[00:23:24.920 --> 00:23:30.080]   kinds, is something that leads to like really a lot better outcomes.
[00:23:30.080 --> 00:23:34.480]   And I think that that's kind of the tension with the things like MapReduce, which kind
[00:23:34.480 --> 00:23:38.520]   of are exclusionary and are really geared towards people who are really technically
[00:23:38.520 --> 00:23:43.360]   adept, that the companies that are really going to do well are the ones who can bring
[00:23:43.360 --> 00:23:44.360]   the tools out.
[00:23:44.360 --> 00:23:47.560]   And I don't talk about just democratizing data because I've got a really clear issue
[00:23:47.560 --> 00:23:53.180]   around too much democratizing data, but getting people who can go into the data and figure
[00:23:53.180 --> 00:23:58.180]   things out and having a lot of different perspectives on that is really going to make a big difference.
[00:23:58.180 --> 00:24:03.160]   And I think that that means having tools that more people can use to get their matters.
[00:24:03.160 --> 00:24:08.180]   And I think when you look back at the aughts and maybe the early tens, that they were still
[00:24:08.180 --> 00:24:09.880]   pretty hard to do.
[00:24:09.880 --> 00:24:14.520]   And that now we're at a place where a lot of people can spin something up and start
[00:24:14.520 --> 00:24:17.400]   to make sense of it from all sorts of different backgrounds.
[00:24:17.400 --> 00:24:20.040]   It's a great point.
[00:24:20.040 --> 00:24:24.320]   I wanted to go back, Roger, to an earlier point that you made before I forget, which
[00:24:24.320 --> 00:24:28.680]   is you sort of made a little bit of a, I don't know, it seemed like you're a little bit dismissive
[00:24:28.680 --> 00:24:34.680]   of kind of NoSQL databases, which is ironic because I learned about NoSQL databases in
[00:24:34.680 --> 00:24:39.560]   your math club and have kind of continued to use them for the last 20 plus years since
[00:24:39.560 --> 00:24:40.560]   then.
[00:24:40.560 --> 00:24:44.560]   So I was actually curious, do you think that it's generally a bad idea?
[00:24:44.560 --> 00:24:47.320]   Of course, everyone uses them now for some functions.
[00:24:47.320 --> 00:24:49.480]   No, I don't think they're a bad idea at all.
[00:24:49.480 --> 00:24:52.640]   I think they were not a replacement.
[00:24:52.640 --> 00:24:53.640]   They're good for what they were.
[00:24:53.640 --> 00:25:00.600]   And I think that the main argument about schema-less was like, that was a really terrible argument
[00:25:00.600 --> 00:25:05.880]   because if you want to make sense of data, you probably want to have it organized in
[00:25:05.880 --> 00:25:06.880]   a way that people know.
[00:25:06.880 --> 00:25:11.160]   So when you think about analytics, it's a combination of things, the combination of
[00:25:11.160 --> 00:25:15.200]   the data, the tools you're using, and the person who's using the data.
[00:25:15.200 --> 00:25:20.280]   So the more that the person can know about the data, the less kind of cognitive load
[00:25:20.280 --> 00:25:23.240]   on them to get into it, the better they're going to do.
[00:25:23.240 --> 00:25:27.080]   And having to deal with different schemas is not a way to promote that.
[00:25:27.080 --> 00:25:31.200]   In fact, what you end up promoting is someone who's got this photographic memory rather
[00:25:31.200 --> 00:25:33.940]   than just kind of broad memory.
[00:25:33.940 --> 00:25:38.520]   So it's kind of like the way JSON is clearly the way that most people move data around.
[00:25:38.520 --> 00:25:44.120]   I much prefer getting CSV data because it's organized in a way.
[00:25:44.120 --> 00:25:46.360]   And you're not having to know the overhead of tags and stuff.
[00:25:46.360 --> 00:25:48.100]   They're telling you what everything is.
[00:25:48.100 --> 00:25:52.120]   And you can move right into what usually I want to do with the data is trying to make
[00:25:52.120 --> 00:25:54.520]   some sense out of it.
[00:25:54.520 --> 00:25:57.600]   So I'm only dismissive in it as a pure replacement.
[00:25:57.600 --> 00:26:01.160]   It's like a lot of things when it's the right tool, like you've got a lot of text.
[00:26:01.160 --> 00:26:04.760]   Yeah, a NoSQL database is great.
[00:26:04.760 --> 00:26:06.680]   But for plenty of things, I want a key.
[00:26:06.680 --> 00:26:07.680]   I want a primary key.
[00:26:07.680 --> 00:26:10.920]   And I want to have something to dedupe against.
[00:26:10.920 --> 00:26:16.680]   I just ran a big deduping project for the state of California around home bases, this
[00:26:16.680 --> 00:26:18.040]   time card data.
[00:26:18.040 --> 00:26:19.040]   And they gave me stuff.
[00:26:19.040 --> 00:26:20.040]   And I had to dedupe it.
[00:26:20.040 --> 00:26:26.820]   It took 19 steps to dedupe it, to try to make a primary key that I could use and pick the
[00:26:26.820 --> 00:26:27.820]   right one.
[00:26:27.820 --> 00:26:29.080]   And I'm going to have multiples and stuff.
[00:26:29.080 --> 00:26:30.080]   So I think that stuff matters.
[00:26:30.080 --> 00:26:35.060]   I think that-- and I'd love to hear someone argue the other point.
[00:26:35.060 --> 00:26:39.120]   But that you end up with things kind of messy, which was maybe OK.
[00:26:39.120 --> 00:26:44.200]   But you end up having to build taxonomies and the kind of things that help you make
[00:26:44.200 --> 00:26:50.440]   sense of data that end up looking a lot more like tables than a schema-less thing.
[00:26:50.440 --> 00:26:54.240]   DJ, do you have thoughts here?
[00:26:54.240 --> 00:26:55.240]   I'm with Roger.
[00:26:55.240 --> 00:27:00.880]   I think one of the things that we've seen with a tool that is being used for many other
[00:27:00.880 --> 00:27:05.680]   things, you end up building a lot of scaffolding or process around it that then suddenly is
[00:27:05.680 --> 00:27:07.720]   like, hey, there's data dictionaries for this.
[00:27:07.720 --> 00:27:14.360]   And there's manuals and wikis to help you get through the schema-less world.
[00:27:14.360 --> 00:27:20.920]   And you're just like, so did we just put a schema structure that's just meta around this?
[00:27:20.920 --> 00:27:28.120]   And Roger and I both had the opportunity and good fortune of working on California on the
[00:27:28.120 --> 00:27:29.840]   COVID response.
[00:27:29.840 --> 00:27:37.800]   And there's a lot of really dumb, boring, unsexy problems that are the real rate limiter
[00:27:37.800 --> 00:27:39.920]   of progress.
[00:27:39.920 --> 00:27:43.240]   People are very apt to saying, oh, there's a note of the data source.
[00:27:43.240 --> 00:27:44.240]   We'll grab it.
[00:27:44.240 --> 00:27:45.240]   We'll put it in.
[00:27:45.240 --> 00:27:46.800]   And then you ask, how many people are actually ever looking at it?
[00:27:46.800 --> 00:27:48.000]   It's like zero.
[00:27:48.000 --> 00:27:51.400]   And you go around, and then you kind of say-- you look at the requests where people have,
[00:27:51.400 --> 00:27:53.700]   and you're like, everyone's requesting this data.
[00:27:53.700 --> 00:27:54.920]   How come no one's looking at it?
[00:27:54.920 --> 00:28:00.680]   And you go, oh, this is actually a comms accessibility problem.
[00:28:00.680 --> 00:28:05.640]   And we're trying to solve this with all this machinery and everything else.
[00:28:05.640 --> 00:28:11.400]   And literally, in the California COVID response, you know the thing that changed the game?
[00:28:11.400 --> 00:28:18.120]   Myself and three other state people, we wrote a data dictionary in Excel for, like, here's
[00:28:18.120 --> 00:28:19.960]   all the data that we have.
[00:28:19.960 --> 00:28:22.600]   And we just sent it around to all the different departments.
[00:28:22.600 --> 00:28:24.160]   And we're like, this is what we have.
[00:28:24.160 --> 00:28:25.160]   Here's where it is.
[00:28:25.160 --> 00:28:28.800]   If you see something new or you want something, here's the new process.
[00:28:28.800 --> 00:28:33.040]   And we're going to go super old school and just-- you can print this out.
[00:28:33.040 --> 00:28:34.680]   You can share it.
[00:28:34.680 --> 00:28:36.480]   Here's all the data that you want.
[00:28:36.480 --> 00:28:41.800]   And so people could flip through it and be like, oh, I need COVID case counts by this.
[00:28:41.800 --> 00:28:42.800]   Oh, great.
[00:28:42.800 --> 00:28:43.800]   It's already in there.
[00:28:43.800 --> 00:28:44.800]   Sweet.
[00:28:44.800 --> 00:28:46.040]   Ready to rock and roll.
[00:28:46.040 --> 00:28:51.340]   Those things move the needle more than just having, like, this brand name data warehouse
[00:28:51.340 --> 00:28:56.440]   or super other cool stuff or dashboards up the wazoo.
[00:28:56.440 --> 00:28:59.280]   Because they don't get looked at or utilized.
[00:28:59.280 --> 00:29:04.780]   And I think one of the things I find myself saying a lot is, what problem are we trying
[00:29:04.780 --> 00:29:06.600]   to solve?
[00:29:06.600 --> 00:29:08.560]   And does this actually solve the problem?
[00:29:08.560 --> 00:29:12.000]   And I suspect this is true for all of us.
[00:29:12.000 --> 00:29:15.480]   We're in plenty of times where people are like, the problem you're trying to solve is
[00:29:15.480 --> 00:29:18.000]   not the problem you really have.
[00:29:18.000 --> 00:29:19.000]   Yeah.
[00:29:19.000 --> 00:29:25.840]   I have this thing where I often tell people, what does paradise look like?
[00:29:25.840 --> 00:29:26.840]   That's the question I ask.
[00:29:26.840 --> 00:29:32.280]   And they give me-- and they're usually not-- paradise isn't clouds and harp playing, but
[00:29:32.280 --> 00:29:33.280]   the business problem they're trying to solve.
[00:29:33.280 --> 00:29:35.480]   And then you go, OK, how do I step through?
[00:29:35.480 --> 00:29:37.880]   How do I get there?
[00:29:37.880 --> 00:29:41.960]   And then that process leads into what kind of data you might use.
[00:29:41.960 --> 00:29:47.240]   And actually, as you were saying that, DJ, about the data dictionary that you did, I
[00:29:47.240 --> 00:29:48.240]   think that's really important.
[00:29:48.240 --> 00:29:51.760]   And I think there's some-- if we want to get into this, there's some fundamentals that
[00:29:51.760 --> 00:29:58.000]   people kind of forgot about that I think are worth reiterating to put things in a more
[00:29:58.000 --> 00:29:59.400]   productive manner.
[00:29:59.400 --> 00:30:04.960]   But at the bottom of this list I prepared for this is put human perspective first.
[00:30:04.960 --> 00:30:07.440]   And maybe I should have made that the top thing.
[00:30:07.440 --> 00:30:13.120]   Because I think what it ends up is, we start thinking about the math and all that, and
[00:30:13.120 --> 00:30:15.400]   biases and everything that's part of this.
[00:30:15.400 --> 00:30:17.480]   But it ends up, it's really a human process.
[00:30:17.480 --> 00:30:21.160]   And what you're really trying to do is get humans to give them the kind of cognitive
[00:30:21.160 --> 00:30:27.880]   capacity to make better decisions, or at least to make decisions that are informed in a way
[00:30:27.880 --> 00:30:33.160]   that they can then learn from what they've done and move forward from there.
[00:30:33.160 --> 00:30:36.560]   Well, I want to hear this list of best practices.
[00:30:36.560 --> 00:30:42.000]   But it reminds me-- that reminds me of one of my favorite memories of you, Roger, which
[00:30:42.000 --> 00:30:44.320]   I don't think you-- I don't know if you remember this.
[00:30:44.320 --> 00:30:46.120]   I don't know if it made such an impact on you.
[00:30:46.120 --> 00:30:53.080]   But I was late to meet you at our first office for Weights and Biases when it was six people.
[00:30:53.080 --> 00:30:58.400]   And I remember you were already telling the head of product, Carrie, you were telling
[00:30:58.400 --> 00:31:02.040]   her basically, nobody wants data visualizations, they want insights.
[00:31:02.040 --> 00:31:05.600]   It's funny because our tool is mainly data visualization tools, one way of looking at
[00:31:05.600 --> 00:31:06.600]   it.
[00:31:06.600 --> 00:31:07.600]   And she was nodding in agreement.
[00:31:07.600 --> 00:31:11.360]   And I was worried she was thinking about taking all the graphs out of our product.
[00:31:11.360 --> 00:31:14.440]   I was like, "Oh my God, I'm five minutes late."
[00:31:14.440 --> 00:31:17.360]   I'm like, "This is like already happening."
[00:31:17.360 --> 00:31:18.840]   Yeah, I remember that.
[00:31:18.840 --> 00:31:20.920]   And that actually is on my note.
[00:31:20.920 --> 00:31:26.680]   And what I probably should-- I think we were just hanging around and trying to make a point.
[00:31:26.680 --> 00:31:31.680]   So one of the points is, when you've got KPIs, when you've got someone who's in the data
[00:31:31.680 --> 00:31:35.240]   every day, and they know what they're looking for, you need a dashboard.
[00:31:35.240 --> 00:31:37.360]   You need this kind of visualization.
[00:31:37.360 --> 00:31:43.480]   But when you want to communicate-- and I like when DJ used the word comms-- you need narration,
[00:31:43.480 --> 00:31:44.480]   you need annotation.
[00:31:44.480 --> 00:31:46.520]   A dashboard won't do it.
[00:31:46.520 --> 00:31:49.160]   So I can just give one example from the state.
[00:31:49.160 --> 00:31:53.520]   They had mobility kind of patterns for every county in California.
[00:31:53.520 --> 00:32:00.400]   So there's 58 counties, so there's 58 little charts arrayed in a lattice.
[00:32:00.400 --> 00:32:04.200]   Alpine County in California has 1,200 people.
[00:32:04.200 --> 00:32:05.840]   There's high schools bigger than that.
[00:32:05.840 --> 00:32:09.840]   And that showed as big as Los Angeles, which is the second biggest city in the country,
[00:32:09.840 --> 00:32:11.960]   second biggest county in the country.
[00:32:11.960 --> 00:32:13.880]   That was not telling a story.
[00:32:13.880 --> 00:32:15.720]   That was just going to confuse people.
[00:32:15.720 --> 00:32:19.960]   And so the point I was trying to make when I was at your office was more around that,
[00:32:19.960 --> 00:32:25.460]   that you need to include the things around narration and annotation.
[00:32:25.460 --> 00:32:29.680]   So that, again, bringing the human part in so that you can make sense of it and to show
[00:32:29.680 --> 00:32:30.680]   what's going on.
[00:32:30.680 --> 00:32:35.760]   If you've ever seen me give a recent presentation, I use the lipstick mode, and I put big red
[00:32:35.760 --> 00:32:38.800]   circles around the things I want you to pay attention to.
[00:32:38.800 --> 00:32:42.840]   And then the slide appears with "Dun-ah," and then that comes on, and then I say it.
[00:32:42.840 --> 00:32:47.880]   So I'm trying to peg it a little bit into your memory.
[00:32:47.880 --> 00:32:49.560]   I think this gets actually to a pet peeve.
[00:32:49.560 --> 00:32:52.520]   Roger and I were talking about this some time ago.
[00:32:52.520 --> 00:32:56.760]   My biggest pet peeve, or I'm curious to hear your reaction, is somebody's like, "As you
[00:32:56.760 --> 00:32:57.760]   can see."
[00:32:57.760 --> 00:33:02.720]   I'm like, "I have no idea what you're talking about.
[00:33:02.720 --> 00:33:03.720]   There's 58 lines.
[00:33:03.720 --> 00:33:04.720]   What are you talking about?"
[00:33:04.720 --> 00:33:09.440]   And then they're like, "As the graph shows."
[00:33:09.440 --> 00:33:13.420]   And you're like, "I don't know what that means even."
[00:33:13.420 --> 00:33:14.680]   And people love these things.
[00:33:14.680 --> 00:33:16.360]   And you're just like, "Where the hell is the end?
[00:33:16.360 --> 00:33:17.360]   Tell me."
[00:33:17.360 --> 00:33:23.320]   We have a staying in national security bluff, bottom line up front.
[00:33:23.320 --> 00:33:26.240]   Tell me what the bottom line up front is, and then I can get there.
[00:33:26.240 --> 00:33:33.360]   But if you're taking me on this journey of literature, I don't have time for that crap.
[00:33:33.360 --> 00:33:35.520]   Help me understand.
[00:33:35.520 --> 00:33:38.920]   Could you go to the president and be like, "Well, let's go on a data journey together
[00:33:38.920 --> 00:33:41.400]   and let's talk about how we got here."
[00:33:41.400 --> 00:33:42.400]   No.
[00:33:42.400 --> 00:33:47.400]   Bottom line up front, and then figure out if they're interested, how do you get them
[00:33:47.400 --> 00:33:52.200]   to the richness that helps get another level of understanding?
[00:33:52.200 --> 00:33:53.200]   Yeah.
[00:33:53.200 --> 00:33:59.720]   There's a thing that I always tell the analysts who work for me, that you can at best communicate
[00:33:59.720 --> 00:34:02.440]   four things plus or minus three.
[00:34:02.440 --> 00:34:05.560]   So you got one to seven things that you can try to communicate.
[00:34:05.560 --> 00:34:10.160]   And you should say them up front, the bluff thing, you can go through it and then say
[00:34:10.160 --> 00:34:13.200]   them again at the end and save the detail for later.
[00:34:13.200 --> 00:34:17.520]   And I think what's hard for a lot of analysts is that for them, the story of how they got
[00:34:17.520 --> 00:34:20.600]   to where they got is pretty interesting to them.
[00:34:20.600 --> 00:34:23.360]   But it's really the insight that you really need to-
[00:34:23.360 --> 00:34:28.840]   I joined this table and then I did this, and then everyone's like, "Oh my God."
[00:34:28.840 --> 00:34:36.960]   And I forgot to do a left join.
[00:34:36.960 --> 00:34:38.520]   We don't want the GitHub repo.
[00:34:38.520 --> 00:34:39.520]   Right, right.
[00:34:39.520 --> 00:34:44.800]   That's what appendixes are made for, you throw that stuff in there.
[00:34:44.800 --> 00:34:50.720]   I wanted to ask you about, you both mentioned your work with the COVID response.
[00:34:50.720 --> 00:34:56.720]   And I was thinking about COVID and I feel like it's maybe the first time in my life
[00:34:56.720 --> 00:35:00.560]   that I feel like I've really consumed data visualizations from my government.
[00:35:00.560 --> 00:35:07.240]   So it does seem like I'm starting to get this communication in graphs and charts that are
[00:35:07.240 --> 00:35:09.960]   reasonably good and seem to be well thought out.
[00:35:09.960 --> 00:35:16.040]   But I was wondering what problems you were trying to solve or what were the big problems
[00:35:16.040 --> 00:35:19.480]   that data could solve with COVID and our government?
[00:35:19.480 --> 00:35:20.480]   Yeah.
[00:35:20.480 --> 00:35:25.040]   Well, maybe I'll start and then Roger, you want to layer in because you picked up a lot
[00:35:25.040 --> 00:35:30.400]   of the baton from me in our first wave and then took it far further.
[00:35:30.400 --> 00:35:37.400]   So the way this happened is our intention wasn't to go up to the Capitol and just be
[00:35:37.400 --> 00:35:38.400]   like, "Look, we're here."
[00:35:38.400 --> 00:35:44.760]   In fact, what it was, was we just happened to be on a phone call with a friend who was
[00:35:44.760 --> 00:35:48.360]   helping out at California, was actually a state employee.
[00:35:48.360 --> 00:35:51.840]   And we're talking and they said, "Here's what we're thinking about data."
[00:35:51.840 --> 00:35:55.560]   And I remember saying, "Well, that's not what I would do if I were you."
[00:35:55.560 --> 00:35:57.040]   And they're like, "Well, what would you do?"
[00:35:57.040 --> 00:35:58.760]   And it's like famous last words.
[00:35:58.760 --> 00:36:02.720]   So in a couple hours, I wrote a memo and I said, "Here's the way I would frame it.
[00:36:02.720 --> 00:36:03.960]   Here's what I think is doable.
[00:36:03.960 --> 00:36:04.960]   Here's what's not.
[00:36:04.960 --> 00:36:05.960]   And here's how I would organize things."
[00:36:05.960 --> 00:36:14.080]   And next thing I knew, 24 hours later, we were driving up to Sacramento at 5 a.m. to
[00:36:14.080 --> 00:36:15.800]   meet the team and start jumping in.
[00:36:15.800 --> 00:36:18.680]   And then we were up there for about 100 days.
[00:36:18.680 --> 00:36:24.400]   And the first part of it was, remember at this time period, there was no data.
[00:36:24.400 --> 00:36:25.800]   People think there was lots of data.
[00:36:25.800 --> 00:36:29.840]   We had data that we weren't sure we could trust out of Wuhan.
[00:36:29.840 --> 00:36:35.600]   We had data off two cruise ships and a little bit of information when we were able to call
[00:36:35.600 --> 00:36:42.360]   our friends who could connect us to other friends who were physicians in Northern Italy.
[00:36:42.360 --> 00:36:44.200]   That's all the data we had.
[00:36:44.200 --> 00:36:46.560]   There was all this talk of epidemiological models, all these things.
[00:36:46.560 --> 00:36:49.600]   There were no models that were like, "This is the gold standard.
[00:36:49.600 --> 00:36:51.720]   There's no weather model for this."
[00:36:51.720 --> 00:36:54.720]   And so we were able, luckily enough, to have...
[00:36:54.720 --> 00:37:00.240]   And this story actually, interestingly enough, is being super detailed by Michael Lewis and
[00:37:00.240 --> 00:37:01.960]   his book that's released today.
[00:37:01.960 --> 00:37:05.360]   And we had this amazing woman, Charity Dean, who is working on things.
[00:37:05.360 --> 00:37:08.840]   We had another guy, Adam Reedhead, who is another public health official.
[00:37:08.840 --> 00:37:14.280]   And then Amy Tong, who's running information technology for the state of California, is
[00:37:14.280 --> 00:37:15.280]   amazing human.
[00:37:15.280 --> 00:37:19.400]   These are people we should be grateful for.
[00:37:19.400 --> 00:37:23.600]   And what they were putting together was like, "Well, what is the model?"
[00:37:23.600 --> 00:37:26.600]   And then we found that one of the models...
[00:37:26.600 --> 00:37:29.440]   Everyone was looking at the models for all of the states.
[00:37:29.440 --> 00:37:33.160]   So the model for Delaware is the same as the model for California.
[00:37:33.160 --> 00:37:34.160]   That doesn't make sense.
[00:37:34.160 --> 00:37:38.440]   And that doesn't help us think about how to think about LA, San Francisco versus Alpine
[00:37:38.440 --> 00:37:41.600]   County or Tahoe area.
[00:37:41.600 --> 00:37:43.640]   And so we needed a more sophisticated one.
[00:37:43.640 --> 00:37:48.720]   Luckily, there was a research scientist named Justin Lessner out of John Hopkins who had
[00:37:48.720 --> 00:37:50.880]   a pretty sophisticated model.
[00:37:50.880 --> 00:37:51.880]   And this model-
[00:37:51.880 --> 00:37:52.880]   Sorry, this is a model of what?
[00:37:52.880 --> 00:37:53.880]   What are you modeling?
[00:37:53.880 --> 00:37:54.880]   Yeah.
[00:37:54.880 --> 00:37:55.960]   So this is modeling...
[00:37:55.960 --> 00:37:58.840]   It's basically a set of differential equations.
[00:37:58.840 --> 00:38:01.240]   And it says basically a person's...
[00:38:01.240 --> 00:38:08.480]   You start with a population, you sprinkle some base conditions of those that are infected.
[00:38:08.480 --> 00:38:17.280]   They at some percentage get other people to be infected, symptomatic, asymptomatic ratios.
[00:38:17.280 --> 00:38:18.920]   Some portion of them will die.
[00:38:18.920 --> 00:38:21.840]   Some portion of them will survive.
[00:38:21.840 --> 00:38:23.320]   And then that's it.
[00:38:23.320 --> 00:38:24.320]   Like super simple.
[00:38:24.320 --> 00:38:26.280]   Now you need other things in there.
[00:38:26.280 --> 00:38:31.760]   Like, well, what about people who commute between the Bay Area and LA?
[00:38:31.760 --> 00:38:34.120]   What about different age demographics?
[00:38:34.120 --> 00:38:35.480]   What about closing schools?
[00:38:35.480 --> 00:38:36.480]   What will that do?
[00:38:36.480 --> 00:38:40.080]   And so they had started to build more and more sophistication in the model.
[00:38:40.080 --> 00:38:43.920]   And so you could run an ensemble, many, many scenarios.
[00:38:43.920 --> 00:38:46.440]   The only problem is this was a research thing.
[00:38:46.440 --> 00:38:49.320]   So it's running under somebody's desk.
[00:38:49.320 --> 00:38:55.000]   And luckily we were able to call on Sam Shaw, who really deserves a lot of the credit for
[00:38:55.000 --> 00:38:57.120]   scaling people you may know at LinkedIn.
[00:38:57.120 --> 00:39:01.480]   Jonathan Goldman and Mike Greenfield really kind of came up with the ideas and Sam Shaw
[00:39:01.480 --> 00:39:04.680]   scaled it and made it really a machine learning platform.
[00:39:04.680 --> 00:39:08.960]   And Josh Wills, who was at Cloudera and then at Slack, figuring out how to make digital
[00:39:08.960 --> 00:39:09.960]   engineering work.
[00:39:09.960 --> 00:39:15.120]   The two of them with Justin Lessiter's team and with a massive help from Werner Vogel
[00:39:15.120 --> 00:39:20.680]   and the Amazon team took that model and ported it over in a matter of days.
[00:39:20.680 --> 00:39:25.240]   And so now we're able to run hundreds of simulations.
[00:39:25.240 --> 00:39:31.320]   Those simulations are what led to those first graphs that people saw of the exponential
[00:39:31.320 --> 00:39:36.280]   curve that were shown on press conferences by Governor Gavin Newsom.
[00:39:36.280 --> 00:39:42.280]   That also led everyone to see like, holy cow, if we don't get this under control, here's
[00:39:42.280 --> 00:39:45.560]   where our bed capacity is right now.
[00:39:45.560 --> 00:39:50.000]   Here's our bed capacity if we put them in parking lots and do everything.
[00:39:50.000 --> 00:39:51.480]   And here's the curve.
[00:39:51.480 --> 00:39:54.360]   And a lot of people at that time were like, this is garbage.
[00:39:54.360 --> 00:39:57.560]   They didn't see what's happening literally in India right now.
[00:39:57.560 --> 00:40:00.840]   They weren't seeing what was happening in New York.
[00:40:00.840 --> 00:40:08.200]   That model, that effort of a combination of data scientists, state experts, and technologists
[00:40:08.200 --> 00:40:14.520]   combined together with the policymakers, that's what led to the state order on saying we need
[00:40:14.520 --> 00:40:15.520]   to stay at home.
[00:40:15.520 --> 00:40:17.480]   Because there's one goal.
[00:40:17.480 --> 00:40:19.880]   One goal is to preserve the healthcare system for tomorrow.
[00:40:19.880 --> 00:40:24.080]   Because if physicians get sick or die, you don't get that back.
[00:40:24.080 --> 00:40:31.360]   That model, those efforts with Governor Newsom and him is what allowed other states to realize
[00:40:31.360 --> 00:40:33.480]   that they need to take action as well.
[00:40:33.480 --> 00:40:39.800]   And that's what led us to the follow on orders and actually being able to make sure that
[00:40:39.800 --> 00:40:44.960]   we didn't have happen what we saw in New York happen in San Francisco or in LA, even though
[00:40:44.960 --> 00:40:47.920]   LA was still hammered from those efforts.
[00:40:47.920 --> 00:40:50.120]   And there was so much more data that we started to realize.
[00:40:50.120 --> 00:40:51.120]   That was just one part.
[00:40:51.160 --> 00:40:54.960]   Because then it was like, hey, how do we help policymakers with richer, deeper understanding
[00:40:54.960 --> 00:40:56.000]   of ideas?
[00:40:56.000 --> 00:40:58.240]   And we had to bring data in and draw the insights.
[00:40:58.240 --> 00:41:05.520]   And luckily for me, one of those people who answered the call on the first ring was Roger.
[00:41:05.520 --> 00:41:07.760]   And Roger said, I'd be happy to volunteer.
[00:41:07.760 --> 00:41:10.280]   And because we were all volunteers, no one was paid.
[00:41:10.280 --> 00:41:12.080]   This is all volunteer all the time.
[00:41:12.080 --> 00:41:14.720]   And we were all just trying to do our best.
[00:41:14.720 --> 00:41:20.160]   And Roger, you should take over because you led the next portion.
[00:41:20.160 --> 00:41:24.480]   Yeah, so one of the interesting things that happened is what came out of that model was
[00:41:24.480 --> 00:41:25.960]   a need to look at mobility data.
[00:41:25.960 --> 00:41:31.200]   And so we started getting a lot of data about how people are moving around.
[00:41:31.200 --> 00:41:36.240]   And we noticed some things about that that ended up leading.
[00:41:36.240 --> 00:41:40.200]   And I think this is what's so interesting about it is the data led to thinking a lot
[00:41:40.200 --> 00:41:41.200]   about ethnography.
[00:41:41.200 --> 00:41:47.400]   It ended up being behavior that mattered and then turning that behavior into something
[00:41:47.400 --> 00:41:48.400]   we could do.
[00:41:48.400 --> 00:41:50.320]   So I'll give you one quick example.
[00:41:50.320 --> 00:41:55.640]   In the spring in Los Angeles, there is a bioluminescent event.
[00:41:55.640 --> 00:42:00.600]   So these algae glow, and people want to go to the beach to look at them.
[00:42:00.600 --> 00:42:03.720]   And the people brought this up.
[00:42:03.720 --> 00:42:05.280]   And we're like, well, what are we going to do about it?
[00:42:05.280 --> 00:42:06.800]   And it's like, we've got to keep people off the beaches.
[00:42:06.800 --> 00:42:07.800]   We've got to keep people off the beaches.
[00:42:07.800 --> 00:42:13.920]   And it's like, at that time-- so this was April, I think late April, early May.
[00:42:13.920 --> 00:42:19.080]   I think enough people on the team knew it was an aerosol and that spreading apart was
[00:42:19.080 --> 00:42:20.080]   OK.
[00:42:20.080 --> 00:42:23.480]   But this is my remembering it.
[00:42:23.480 --> 00:42:25.680]   It might have not been as clear as it seems now.
[00:42:25.680 --> 00:42:28.480]   Now we know for sure it's an aerosol, but that's how I remember it.
[00:42:28.480 --> 00:42:33.160]   And one of the things we did is like, no, we're not going to keep people off the beaches.
[00:42:33.160 --> 00:42:35.960]   Let's keep people safe on the beaches.
[00:42:35.960 --> 00:42:36.960]   So here's what we had.
[00:42:36.960 --> 00:42:39.360]   We knew that people were moving around a little bit.
[00:42:39.360 --> 00:42:41.880]   I think at that time, there was a little upward movement.
[00:42:41.880 --> 00:42:47.120]   And so we kind of told people in Los Angeles that what you need to do is maybe have some
[00:42:47.120 --> 00:42:49.400]   people to keep people spread apart and stuff.
[00:42:49.400 --> 00:42:52.440]   And of course, there weren't going to be as many people anyway as doing that.
[00:42:52.440 --> 00:42:56.360]   And then we started doing stuff like preparing for Thanksgiving in August.
[00:42:56.360 --> 00:42:57.360]   What do you tell people?
[00:42:57.360 --> 00:43:04.000]   The harvest was a big-- the real boost in California's rates came because the harvest,
[00:43:04.000 --> 00:43:09.360]   which was like a perfect equation for how you're going to get infections in a community.
[00:43:09.360 --> 00:43:12.520]   And I don't know if you remember, I think Imperial County at one time had the highest
[00:43:12.520 --> 00:43:13.840]   rates in the world.
[00:43:13.840 --> 00:43:16.160]   And it was totally because of the harvest.
[00:43:16.160 --> 00:43:22.160]   So what we ended up doing is using data to try to communicate to the people in the state
[00:43:22.160 --> 00:43:26.240]   and to think about kind of behavior things that then we would maybe build new-- and they
[00:43:26.240 --> 00:43:27.240]   really weren't models.
[00:43:27.240 --> 00:43:30.840]   They were really just kind of studies about what we could do or what was happening that
[00:43:30.840 --> 00:43:33.000]   we could intervene better.
[00:43:33.000 --> 00:43:38.080]   And so we started going from where everything was kind of statewide at first to talking
[00:43:38.080 --> 00:43:43.840]   about rural versus urban because there's very different things going on depending on the
[00:43:43.840 --> 00:43:47.040]   kind of density and characteristics.
[00:43:47.040 --> 00:43:52.000]   And trying to also learn things like the Bay Area did better than the rest of the state.
[00:43:52.000 --> 00:43:56.120]   And I think the trivial reasons why really ended up not being the reasons why.
[00:43:56.120 --> 00:44:02.080]   The trivial reason was people could remote work easier and an educated population.
[00:44:02.080 --> 00:44:06.480]   And it ends up-- and this isn't something we found, but a lot of it had to do with the
[00:44:06.480 --> 00:44:11.640]   Bay Area's experience with AIDS and having to deal with another pandemic.
[00:44:11.640 --> 00:44:14.680]   And that it was community, kind of community access.
[00:44:14.680 --> 00:44:20.200]   So as we started seeing mobility data showing more movement, we brought in someone from
[00:44:20.200 --> 00:44:24.960]   New Haven who had done some really interesting work around how do you deal with that community
[00:44:24.960 --> 00:44:25.960]   part.
[00:44:25.960 --> 00:44:31.600]   So what I like to think is what happened is the basis was laid with data.
[00:44:31.600 --> 00:44:35.080]   And then we were using that to go to this kind of next level of mixing that data with
[00:44:35.080 --> 00:44:36.880]   some kind of qualitative and behavior.
[00:44:36.880 --> 00:44:40.720]   And we had some ethnographers on the team who started doing a lot of surveys.
[00:44:40.720 --> 00:44:44.520]   And we would use those surveys to-- in the end, Deji, I don't know how much of you were
[00:44:44.520 --> 00:44:48.480]   involved towards the end of this, but really the surveys ended up being the driving force
[00:44:48.480 --> 00:44:51.600]   behind the kind of comms that were going afterwards.
[00:44:51.600 --> 00:44:53.360]   Which is again another kind of qualitative thing.
[00:44:53.360 --> 00:44:59.000]   But we made sure that the surveys were being a better instrument for pulling stuff.
[00:44:59.000 --> 00:45:01.200]   And this is one of those lucky breaks.
[00:45:01.200 --> 00:45:03.360]   I happen to run all the surveys at O'Reilly.
[00:45:03.360 --> 00:45:07.240]   So I had some survey experience.
[00:45:07.240 --> 00:45:10.360]   And we were able to bring that in and improve it.
[00:45:10.360 --> 00:45:11.360]   Yeah.
[00:45:11.360 --> 00:45:15.640]   Kara DeFrias really deserves credit for having this survey idea.
[00:45:15.640 --> 00:45:21.960]   What she did is she basically convinced the state to basically put just an open-ended
[00:45:21.960 --> 00:45:26.880]   kind of set of questions on one of the highly trafficked web pages.
[00:45:26.880 --> 00:45:27.880]   And it just sort of sampled.
[00:45:27.880 --> 00:45:30.480]   And it was just a way of getting feedback.
[00:45:30.480 --> 00:45:37.040]   The problem is it's very hard to get a feedback on a state the size of California, just given
[00:45:37.040 --> 00:45:38.040]   the disparity.
[00:45:38.040 --> 00:45:42.920]   And so one of the things that was prepared every night at that point was basically a
[00:45:42.920 --> 00:45:45.880]   briefing book for the governor and the key staff.
[00:45:45.880 --> 00:45:50.840]   And it had charts and graphs and all sorts of really important key insights.
[00:45:50.840 --> 00:45:59.000]   But also it had snippets of key things that we heard from the real population, real stories.
[00:45:59.000 --> 00:46:01.960]   And so these weren't data points anymore.
[00:46:01.960 --> 00:46:02.960]   They were people.
[00:46:02.960 --> 00:46:03.960]   They had names.
[00:46:03.960 --> 00:46:05.200]   They had ages.
[00:46:05.200 --> 00:46:06.960]   They had stories.
[00:46:06.960 --> 00:46:11.120]   And you read those things and you could feel the fear.
[00:46:11.120 --> 00:46:13.460]   You could feel the pain.
[00:46:13.460 --> 00:46:16.640]   And so no longer could you just be like, well, it's an uptick.
[00:46:16.640 --> 00:46:17.640]   We'll see what happens.
[00:46:17.640 --> 00:46:20.920]   No, that uptick destroyed a family.
[00:46:20.920 --> 00:46:23.880]   Like, oh, it's just a harvest.
[00:46:23.880 --> 00:46:27.400]   No, we're about to destroy a community.
[00:46:27.400 --> 00:46:29.320]   What are we going to do about it?
[00:46:29.320 --> 00:46:34.860]   And so it changes the whole narrative and approach you take from just being a data science
[00:46:34.860 --> 00:46:41.920]   thing and sort of thinking about this in this abstract and playing with graphs to if we
[00:46:41.920 --> 00:46:49.600]   do not act right now with immediate sense of urgency, somebody will die.
[00:46:49.600 --> 00:46:52.520]   It's not an if.
[00:46:52.520 --> 00:46:54.980]   Someone will die.
[00:46:54.980 --> 00:47:03.120]   And our actions directly help the shift in balance of who that is and how do we make
[00:47:03.120 --> 00:47:06.320]   sure that they get the best shot at surviving?
[00:47:06.320 --> 00:47:12.480]   Our job fundamentally was to use data to give everyone a shot at living.
[00:47:12.480 --> 00:47:16.760]   If a hospital doesn't have oxygen, figure out how to get them the oxygen so those people
[00:47:16.760 --> 00:47:17.920]   have a chance.
[00:47:17.920 --> 00:47:22.240]   If people don't realize that the people around them are highly infective, let's give them
[00:47:22.240 --> 00:47:27.920]   a shot to actually be able to take safety measures in their own hands so they can survive
[00:47:27.920 --> 00:47:32.600]   and increase the probability that they are OK or some other family member that they may
[00:47:32.600 --> 00:47:35.120]   expose will be OK.
[00:47:35.120 --> 00:47:40.520]   Yeah, I just want to say one thing that was really striking to me about it.
[00:47:40.520 --> 00:47:42.760]   Obviously, this kind of thing is in politics.
[00:47:42.760 --> 00:47:44.960]   It's in the realm of politics.
[00:47:44.960 --> 00:47:52.080]   Was this group of volunteers went out of their way to always treat every group, every person
[00:47:52.080 --> 00:47:54.400]   as worthwhile?
[00:47:54.400 --> 00:47:59.960]   That there was really no politics in the kind of traditional polarizing way we think about
[00:47:59.960 --> 00:48:00.960]   it going on.
[00:48:00.960 --> 00:48:03.840]   It was always about how to keep people safe.
[00:48:03.840 --> 00:48:10.240]   And mostly, it was about how to tell them the information they need that they can try
[00:48:10.240 --> 00:48:12.560]   to be safer and do the right things.
[00:48:12.560 --> 00:48:19.480]   And it was really quite, I don't know, like enervating to see that going on with it.
[00:48:19.480 --> 00:48:24.880]   But I guess this relates back to my earlier point about the annotation narration is that
[00:48:24.880 --> 00:48:29.820]   we ended up moving from learning from the data and then moving to this alternate approach
[00:48:29.820 --> 00:48:33.720]   that I think ended up being effective downstream.
[00:48:33.720 --> 00:48:36.960]   Do you have a sense of how effective this was?
[00:48:36.960 --> 00:48:40.720]   I mean, it sounds like there's a whole bunch of different kinds of interventions you were
[00:48:40.720 --> 00:48:42.360]   doing.
[00:48:42.360 --> 00:48:48.680]   Is it possible to even know if you hadn't done these things what would have happened?
[00:48:48.680 --> 00:48:52.960]   Do you feel about the overall effectiveness of this response?
[00:48:52.960 --> 00:48:58.120]   Yeah, there's a blog post that Sam Shah wrote that kind of talks about this.
[00:48:58.120 --> 00:48:59.440]   And there's been a whole lot of estimates.
[00:48:59.440 --> 00:49:05.680]   And I think we'll continue to see estimates and sort of a lot of people doing deep analysis
[00:49:05.680 --> 00:49:08.720]   for decades to come.
[00:49:08.720 --> 00:49:14.200]   And I feel, I think I've been received a fair amount of criticism, and it's okay to receive
[00:49:14.200 --> 00:49:19.880]   criticism about what people would describe as a very strong policy response and that
[00:49:19.880 --> 00:49:24.400]   we were too aggressive in shutting down the economy and taking the action we did.
[00:49:24.400 --> 00:49:29.280]   I actually sleep well at night knowing that we took the strong action that we did, because
[00:49:29.280 --> 00:49:34.720]   if we didn't, I mean, I was in contact very regularly with friends who are on the front
[00:49:34.720 --> 00:49:36.160]   lines in New York City.
[00:49:36.160 --> 00:49:43.440]   I was on the calls with people who were in the ER who were showing me how they had literally
[00:49:43.440 --> 00:49:50.640]   just like in a kindergarten, they had a wall with like paper brown bags that you put your
[00:49:50.640 --> 00:49:56.040]   masks in because you just need to come back and reuse them.
[00:49:56.040 --> 00:50:03.360]   And people forget how many physicians and nurses and janitorial staff, the people who
[00:50:03.360 --> 00:50:10.440]   we don't often think about in the healthcare system that died in service.
[00:50:10.440 --> 00:50:17.100]   And when you lose that capacity, you don't have the capacity to get back up.
[00:50:17.100 --> 00:50:19.480]   You don't have, there's just no one else to take care.
[00:50:19.480 --> 00:50:25.360]   And what you're seeing happen in Brazil, what you're seeing happen in India, that could
[00:50:25.360 --> 00:50:27.520]   have easily been us.
[00:50:27.520 --> 00:50:31.600]   People think, oh, we got, remember there was no REN Disappear.
[00:50:31.600 --> 00:50:36.480]   People didn't even know about ventilators and how, like, do we flip a person over?
[00:50:36.480 --> 00:50:37.640]   Do we set someone up?
[00:50:37.640 --> 00:50:39.520]   We had no information.
[00:50:39.520 --> 00:50:46.200]   We had a Slack channel that was created literally for physicians just to share information from
[00:50:46.200 --> 00:50:49.360]   one group to another about what they were learning.
[00:50:49.360 --> 00:50:52.240]   Like, that's how little information we had.
[00:50:52.240 --> 00:50:54.280]   Now what's behind this?
[00:50:54.280 --> 00:50:58.360]   A decade of under-investing in public health.
[00:50:58.360 --> 00:50:59.360]   More than a decade.
[00:50:59.360 --> 00:51:01.000]   Did we have to end up this way?
[00:51:01.000 --> 00:51:02.000]   Absolutely not.
[00:51:02.000 --> 00:51:08.040]   This is an abject failure of literally 20 plus years of not investing.
[00:51:08.040 --> 00:51:16.280]   And Obama called for a massive revamp of this after Ebola.
[00:51:16.280 --> 00:51:17.600]   We saw this with MERS.
[00:51:17.600 --> 00:51:19.600]   We saw this with SARS.
[00:51:19.600 --> 00:51:21.040]   We've seen this many times over.
[00:51:21.040 --> 00:51:23.640]   And people often think, oh, we're through the pandemic.
[00:51:23.640 --> 00:51:25.840]   This is not pandemic flu.
[00:51:25.840 --> 00:51:29.000]   This is not pandemic tuberculosis.
[00:51:29.000 --> 00:51:32.380]   This is not the next coronavirus, which will show up.
[00:51:32.380 --> 00:51:33.880]   We will expect another coronavirus.
[00:51:33.880 --> 00:51:38.200]   And so I don't say that to be a doomsayer.
[00:51:38.200 --> 00:51:44.080]   I say it as these are the systems that we need to get into place now to be ready for
[00:51:44.080 --> 00:51:45.160]   what's next.
[00:51:45.160 --> 00:51:49.640]   So that we don't have to just say one size fits all, shut everything down.
[00:51:49.640 --> 00:51:50.640]   We can be smart.
[00:51:50.640 --> 00:51:54.920]   Because we are going to have to create this as a knob, if you will, of dialing things
[00:51:54.920 --> 00:51:59.400]   open, dialing things back, depending on what we're seeing in which community.
[00:51:59.400 --> 00:52:04.240]   And a lot of this is going to be really tough, because it's socioeconomic also.
[00:52:04.240 --> 00:52:09.960]   And as Roger pointed out, you get very different dynamics from one region to another.
[00:52:09.960 --> 00:52:21.480]   I guess as an outside observer, it hasn't felt like the levels of COVID that different
[00:52:21.480 --> 00:52:27.240]   regions saw was exactly correlated with the thoughtfulness of the COVID response.
[00:52:27.240 --> 00:52:29.440]   Do you think that's fair?
[00:52:29.440 --> 00:52:33.840]   Or is it just the data is noisy?
[00:52:33.840 --> 00:52:37.640]   Or am I missing something there?
[00:52:37.640 --> 00:52:38.640]   Say more.
[00:52:38.640 --> 00:52:39.960]   I don't think I fully understand what you mean.
[00:52:39.960 --> 00:52:44.280]   Well, I guess it seems to me-- and I did not prep by looking deeply at the data.
[00:52:44.280 --> 00:52:50.480]   But I've had this sense that some states that really sort of aggressively put in controls,
[00:52:50.480 --> 00:52:54.080]   maybe-- or the states that put aggressive controls in quickly, sometimes they ended
[00:52:54.080 --> 00:52:57.600]   up having more COVID cases than states that seemed to ignore it.
[00:52:57.600 --> 00:53:01.160]   And some states-- it sounds like California had a really thoughtful response.
[00:53:01.160 --> 00:53:04.320]   It seems like some even governors are kind of like, hey, this isn't even a problem.
[00:53:04.320 --> 00:53:08.640]   So I can't even imagine how that state could have any kind of reasonable response when
[00:53:08.640 --> 00:53:13.640]   the leadership doesn't even believe that there's an issue when there obviously is.
[00:53:13.640 --> 00:53:22.080]   So I guess it seems really hard to know how much the interventions really mattered.
[00:53:22.080 --> 00:53:23.080]   Right.
[00:53:23.080 --> 00:53:24.640]   So I think there's a bunch of stuff.
[00:53:24.640 --> 00:53:28.800]   And then, Raja, I'd love to-- well, let me just be real quick, and then you should go
[00:53:28.800 --> 00:53:29.800]   ahead.
[00:53:29.800 --> 00:53:34.760]   The first is we're still scratching the surface in our understanding of COVID.
[00:53:34.760 --> 00:53:37.240]   And so I think a lot is still going to be learned.
[00:53:37.240 --> 00:53:41.600]   We now know, as we were in the summer and we were really worried about protests that
[00:53:41.600 --> 00:53:45.680]   were happening, we were really worried at Sturgis rally, we thought, oh my gosh, these
[00:53:45.680 --> 00:53:46.840]   are going to be super spreading events.
[00:53:46.840 --> 00:53:48.760]   It turned out we dodged a bullet.
[00:53:48.760 --> 00:53:50.600]   I think people are like, oh, you're wrong.
[00:53:50.600 --> 00:53:53.480]   I could look at it as we dodged a bullet.
[00:53:53.480 --> 00:53:58.800]   Because if that was a highly contagious, more like measles, we'd be in real trouble.
[00:53:58.800 --> 00:54:03.700]   The other part, I think, which is there is one of the things that happened is because
[00:54:03.700 --> 00:54:09.640]   of the actions here, a lot of people did start to take COVID very seriously on a personal
[00:54:09.640 --> 00:54:10.640]   level.
[00:54:10.640 --> 00:54:14.040]   And they go, oh, this isn't just some-- California's taking this action.
[00:54:14.040 --> 00:54:15.680]   Maybe we should take it seriously.
[00:54:15.680 --> 00:54:20.080]   But the other example of this is the version that you're seeing in India, which is they
[00:54:20.080 --> 00:54:22.440]   stopped taking it seriously.
[00:54:22.440 --> 00:54:25.160]   They started a whole big political rallies.
[00:54:25.160 --> 00:54:27.800]   They gave away their vaccines.
[00:54:27.800 --> 00:54:29.440]   And they're not shutting down still.
[00:54:29.440 --> 00:54:31.680]   And people are partying in other aspects.
[00:54:31.680 --> 00:54:42.720]   And that has led to the spike that is-- it's decimating because there's no path out now.
[00:54:42.720 --> 00:54:46.360]   What you see in the COVID numbers today is a reflection of four weeks ago.
[00:54:46.360 --> 00:54:48.920]   But Roger, I'd love your--
[00:54:48.920 --> 00:54:54.000]   There's-- using the machine learning language, there's a lot of features that go into what
[00:54:54.000 --> 00:54:55.360]   goes on with this.
[00:54:55.360 --> 00:54:58.200]   The protests end up not being super spreader.
[00:54:58.200 --> 00:54:59.680]   Sturges was.
[00:54:59.680 --> 00:55:02.080]   Look at what happened in the Dakotas.
[00:55:02.080 --> 00:55:07.160]   Those are places without a lot of cross-mixing of people because they're relatively isolated
[00:55:07.160 --> 00:55:08.160]   and remote.
[00:55:08.160 --> 00:55:11.720]   And they had the worst caseloads in the whole world.
[00:55:11.720 --> 00:55:14.480]   And it's not clear that it was totally Sturges.
[00:55:14.480 --> 00:55:19.840]   But there's a lot of thought that it was Sturges and that their political response was not
[00:55:19.840 --> 00:55:21.360]   very strong.
[00:55:21.360 --> 00:55:24.920]   Now those are states with less than a million people each.
[00:55:24.920 --> 00:55:26.560]   So the impact isn't as great.
[00:55:26.560 --> 00:55:31.640]   But there was different response.
[00:55:31.640 --> 00:55:34.040]   And they had a much worse result.
[00:55:34.040 --> 00:55:38.080]   Now what happened in Florida, I think, can be looked at differently.
[00:55:38.080 --> 00:55:42.040]   And I'll just bring up-- I know this isn't supposed to be a political discussion per
[00:55:42.040 --> 00:55:45.160]   se, but I have enough parents who live in Florida.
[00:55:45.160 --> 00:55:48.520]   And people have self-preservation.
[00:55:48.520 --> 00:55:52.920]   And I think that there was enough knowledge out there that at some point, with or without
[00:55:52.920 --> 00:55:57.320]   these-- whether the government was intervening or not-- enough people were trying to play
[00:55:57.320 --> 00:56:00.880]   it safe and were doing the right things.
[00:56:00.880 --> 00:56:05.600]   And yes, there were people who were in that obnoxious way about individual liberty and
[00:56:05.600 --> 00:56:06.800]   stuff like that.
[00:56:06.800 --> 00:56:10.480]   And I always want to take one of those people and say, you need to talk to my friends from
[00:56:10.480 --> 00:56:11.480]   Taiwan.
[00:56:11.480 --> 00:56:16.480]   Because if you know anyone from Taiwan who went through, I guess, with SARS, they, as
[00:56:16.480 --> 00:56:23.560]   a collective group, kind of knew how to act and do the right things.
[00:56:23.560 --> 00:56:31.240]   And giving up some things that feel like a freedom or whatever seemed kind of worthwhile
[00:56:31.240 --> 00:56:33.280]   in the long run to tamp things down.
[00:56:33.280 --> 00:56:36.800]   And of course, Taiwan had a much better response than others.
[00:56:36.800 --> 00:56:40.840]   So it's hard to take out the American context in how people behave.
[00:56:40.840 --> 00:56:47.840]   And there's plenty of examples that would support one position or the other.
[00:56:47.840 --> 00:56:50.960]   But personally, I'm more comfortable with the intervention.
[00:56:50.960 --> 00:56:54.480]   I mean, I realized that it was really bad for the economy and stuff.
[00:56:54.480 --> 00:56:58.440]   And I think things like maybe the way schools were opened and so forth could maybe have
[00:56:58.440 --> 00:56:59.440]   been handled differently.
[00:56:59.440 --> 00:57:02.800]   But we've got a lot to learn.
[00:57:02.800 --> 00:57:06.880]   Well, I really appreciate all the work that both of you did.
[00:57:06.880 --> 00:57:10.200]   And that's actually a segue to a question I really wanted to-- also wanted to make sure
[00:57:10.200 --> 00:57:17.480]   I asked you, which is for someone just starting their career in data science, maybe most of
[00:57:17.480 --> 00:57:22.040]   the people in that situation that I talk to these days, they really care about doing something
[00:57:22.040 --> 00:57:25.360]   meaningful, maybe getting involved in public sector stuff.
[00:57:25.360 --> 00:57:30.660]   I guess, what advice would you give someone maybe just graduating now who wants to do
[00:57:30.660 --> 00:57:35.120]   interesting work and have exciting careers like both of you have had?
[00:57:35.120 --> 00:57:40.160]   Where would you tell them to start, I guess?
[00:57:40.160 --> 00:57:43.200]   That's a good question.
[00:57:43.200 --> 00:57:48.680]   What I've been telling people is to remember this kind of human side of things and don't
[00:57:48.680 --> 00:57:50.640]   get too lost in the numbers.
[00:57:50.640 --> 00:57:54.760]   And this is more like-- this isn't quite career advice, I think, what you're asking.
[00:57:54.760 --> 00:57:59.640]   But also, you've gotten a bunch of tools that are pretty cool.
[00:57:59.640 --> 00:58:03.880]   But that doesn't mean they're applicable in every case.
[00:58:03.880 --> 00:58:08.280]   It's always like kind of work your way up from, is there a simple thing that will work?
[00:58:08.280 --> 00:58:10.720]   How far does it get you?
[00:58:10.720 --> 00:58:12.280]   And then work from there.
[00:58:12.280 --> 00:58:17.240]   And then when you need this sophisticated tool and when it's worth it to kind of jump
[00:58:17.240 --> 00:58:18.360]   in with that.
[00:58:18.360 --> 00:58:23.480]   So TensorFlow is not the answer to every classification problem.
[00:58:23.480 --> 00:58:26.560]   There's other tools that can work really well.
[00:58:26.560 --> 00:58:29.480]   But also, I mean, just find things.
[00:58:29.480 --> 00:58:35.500]   I think I just saw a tweet today, DJ, from Rick Clow, that the state is hiring.
[00:58:35.500 --> 00:58:40.000]   So if you are trying to do some good things, like I said, I was so impressed with the people
[00:58:40.000 --> 00:58:45.440]   at the state and their attitudes about trying to do the right things and being good for
[00:58:45.440 --> 00:58:46.440]   all Californians.
[00:58:46.440 --> 00:58:51.000]   But that's a great place to start.
[00:58:51.000 --> 00:58:55.840]   The place maybe I would go with, because I agree with Roger on all of this, no surprise,
[00:58:55.840 --> 00:59:01.240]   is-- and hopefully what people have taken away from listening is like, this is a team
[00:59:01.240 --> 00:59:02.240]   sport.
[00:59:02.240 --> 00:59:08.120]   And the amount I've learned and grown from you, Lucas, from Roger, from the people that
[00:59:08.120 --> 00:59:14.460]   you've introduced me to, the people like we've all hung out with, the thing that you want
[00:59:14.460 --> 00:59:19.780]   to do at the early part of your career is be around amazing, awesome people.
[00:59:19.780 --> 00:59:22.080]   Be around awesome.
[00:59:22.080 --> 00:59:25.960]   And if you're around awesome people, you'll become awesome, too.
[00:59:25.960 --> 00:59:28.580]   You may feel like you're an imposter to start.
[00:59:28.580 --> 00:59:31.860]   And you've got to kind of figure out how to shake off that imposter syndrome.
[00:59:31.860 --> 00:59:36.520]   But if you're around amazing people, they're going to carry you.
[00:59:36.520 --> 00:59:40.540]   And one of the things-- and that could be in the public sector, it could be in the private
[00:59:40.540 --> 00:59:42.560]   sector, it could be in the hybrid sector.
[00:59:42.560 --> 00:59:48.260]   But I fundamentally believe that if you're around those great people, that's what carries
[00:59:48.260 --> 00:59:49.260]   forward.
[00:59:49.260 --> 00:59:53.860]   I've been so fortunate early in my career being around amazing people in academia, then
[00:59:53.860 --> 00:59:59.380]   being around amazing people in public service the first time after 9/11, then coming out
[00:59:59.380 --> 01:00:03.340]   here to Silicon Valley meeting all of you and kind of being exposed to that group, and
[01:00:03.340 --> 01:00:07.020]   then going back into government back and forth several times.
[01:00:07.020 --> 01:00:09.860]   Each time, we're able to pull an amazing.
[01:00:09.860 --> 01:00:18.860]   And the thing that people don't realize is-- people ask me all the time, why do people
[01:00:18.860 --> 01:00:21.240]   pick up the phone when you call?
[01:00:21.240 --> 01:00:22.780]   And then why won't they pick up first as others?
[01:00:22.780 --> 01:00:27.740]   And it's because I'm trying to do it for the team.
[01:00:27.740 --> 01:00:29.940]   It's a we approach.
[01:00:29.940 --> 01:00:33.260]   I'm not trying to just further it for one perspective.
[01:00:33.260 --> 01:00:39.460]   And I think we've all had that philosophy that this is kind of a collective movement.
[01:00:39.460 --> 01:00:45.700]   And as much-- and I will go on the record saying this, which is I get way too much credit.
[01:00:45.700 --> 01:00:47.860]   The credit belongs to the community.
[01:00:47.860 --> 01:00:51.300]   It belongs to the teams, all these people.
[01:00:51.300 --> 01:00:55.140]   I've just had the good fortune of being in certain roles that gets to shape certain things,
[01:00:55.140 --> 01:00:57.940]   but those people have also shaped me.
[01:00:57.940 --> 01:01:02.760]   They're the ones that have helped make me into what I am and helped make that happen.
[01:01:02.760 --> 01:01:08.100]   And if you're early in your career and you can find a place where you're learning at
[01:01:08.100 --> 01:01:13.940]   three to seven times the rate of somebody that's just in a regular job, you're going
[01:01:13.940 --> 01:01:16.380]   to do fine.
[01:01:16.380 --> 01:01:18.140]   And seek out those places.
[01:01:18.140 --> 01:01:19.940]   Don't optimize for a salary.
[01:01:19.940 --> 01:01:23.820]   I'm not saying it's not important, but optimize for learning.
[01:01:23.820 --> 01:01:28.860]   Your first derivative, your second derivative should be highly positive on your learning
[01:01:28.860 --> 01:01:29.860]   experience quotients.
[01:01:29.860 --> 01:01:30.860]   Yeah.
[01:01:30.860 --> 01:01:33.820]   I want to focus on a particular part of that because I completely agree.
[01:01:33.820 --> 01:01:37.180]   And this talk that I give is like my general talk about data topics.
[01:01:37.180 --> 01:01:38.180]   Starts off with humility.
[01:01:38.180 --> 01:01:41.940]   And humility is a key to learning.
[01:01:41.940 --> 01:01:45.700]   And I also will tell anyone, like they say, I need to hire a data scientist.
[01:01:45.700 --> 01:01:49.140]   I said, you know, you do that at one, you need to hire two.
[01:01:49.140 --> 01:01:53.620]   And no matter what you're doing, you need to be paired with other people.
[01:01:53.620 --> 01:01:58.060]   And that in terms of finding an opportunity, I think you got to make sure you're not siloed.
[01:01:58.060 --> 01:02:01.660]   And I want to give a particular kind of example of what I think happens.
[01:02:01.660 --> 01:02:06.100]   Because when you get into the data, it's almost like a scientist looking at the universe.
[01:02:06.100 --> 01:02:08.660]   You say, the universe is my data.
[01:02:08.660 --> 01:02:13.060]   And without outside perspective, you don't like learn.
[01:02:13.060 --> 01:02:17.380]   The data almost in a way, like kind of stops your expansion because that's all that you
[01:02:17.380 --> 01:02:18.380]   can see.
[01:02:18.380 --> 01:02:19.460]   You can't go beyond that.
[01:02:19.460 --> 01:02:24.180]   So I know like Ben Lorica, who is one of the people I was lucky enough to work with, who
[01:02:24.180 --> 01:02:25.180]   taught me so much.
[01:02:25.180 --> 01:02:27.780]   And he's a real math PhD.
[01:02:27.780 --> 01:02:29.920]   And I didn't have that kind of background.
[01:02:29.920 --> 01:02:33.700]   We did not release anything without the other looking at it.
[01:02:33.700 --> 01:02:37.140]   And we were on the phone almost every morning talking about what we were doing.
[01:02:37.140 --> 01:02:41.980]   So when you're looking for those career opportunities, make sure that you're not going to be siloed,
[01:02:41.980 --> 01:02:45.900]   that you're going to have as much opportunity to work with other people, like almost like
[01:02:45.900 --> 01:02:47.660]   in a peer programming way.
[01:02:47.660 --> 01:02:49.020]   Like look for that.
[01:02:49.020 --> 01:02:52.540]   And look for companies where you're going to be able to talk to other people in the
[01:02:52.540 --> 01:02:57.540]   organization so that you're getting all these things that DJ was talking about, the opportunities
[01:02:57.540 --> 01:02:59.700]   to learn from amazing people.
[01:02:59.700 --> 01:03:04.340]   And just picking up little things like DJ's story about the data dictionary working so
[01:03:04.340 --> 01:03:05.340]   well.
[01:03:05.340 --> 01:03:08.580]   You know, the next job you go to and there's no data dictionary, you're going to make sure
[01:03:08.580 --> 01:03:11.180]   there's one there.
[01:03:11.180 --> 01:03:15.340]   And picking up on those kind of things because they can be so effective.
[01:03:15.340 --> 01:03:21.520]   So I think just making sure that you're like an octopus and your career move is your tentacles
[01:03:21.520 --> 01:03:22.520]   are all over the place.
[01:03:22.520 --> 01:03:25.700]   It's funny you say you both say that.
[01:03:25.700 --> 01:03:27.420]   I mean, I totally agree with it.
[01:03:27.420 --> 01:03:32.380]   And I think it's one of the reasons that, you know, this is totally shameless self-promotion,
[01:03:32.380 --> 01:03:34.380]   but I really think it's true.
[01:03:34.380 --> 01:03:40.180]   We've really tried to build a friendly, smart, but really like inclusive community at Weights
[01:03:40.180 --> 01:03:44.020]   and Bites with stuff like this where people can kind of meet smart people that they might
[01:03:44.020 --> 01:03:48.020]   not otherwise have access to based on, you know, kind of luck and geography.
[01:03:48.020 --> 01:03:51.980]   And so, you know, I just really encourage people to like engage with our community.
[01:03:51.980 --> 01:03:54.020]   If you're watching this, you're kind of part of it.
[01:03:54.020 --> 01:03:58.740]   And you know, we love answering people's questions and hearing from people and hearing about
[01:03:58.740 --> 01:03:59.740]   what they're working on.
[01:03:59.740 --> 01:04:04.700]   So anyway, I just totally appreciate you guys coming on and talking and answering my open-ended
[01:04:04.700 --> 01:04:07.780]   questions and also appreciate all the work that you've done throughout your career.
[01:04:07.780 --> 01:04:12.180]   It's been inspiring to watch and clearly directly connected to a lot of good in the world.
[01:04:12.180 --> 01:04:13.060]   So thank you.
[01:04:14.060 --> 01:04:15.060]   Thanks.
[01:04:15.060 --> 01:04:17.780]   It's been fun to catch up.
[01:04:17.780 --> 01:04:22.980]   At Weights and Biases, we make this podcast Gradient Dissent to learn about making machine
[01:04:22.980 --> 01:04:25.060]   learning work in the real world.
[01:04:25.060 --> 01:04:27.260]   But we also have a part to play here.
[01:04:27.260 --> 01:04:34.220]   We are building tools to help all the people that are on this podcast make their work better
[01:04:34.220 --> 01:04:38.140]   and make machine learning models actually run in production.
[01:04:38.140 --> 01:04:42.220]   And if you're interested in joining us on this mission, we are hiring and engineering
[01:04:42.220 --> 01:04:46.140]   sales, growth, product, and customer support.
[01:04:46.140 --> 01:04:50.260]   And you should go to WMB.me/hiring and check out our job postings.
[01:04:50.260 --> 01:04:51.260]   We'd love to talk about working with you.
[01:04:51.260 --> 01:04:53.260]   Thanks for watching!

