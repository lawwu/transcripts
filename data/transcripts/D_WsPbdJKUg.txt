
[00:00:00.000 --> 00:00:07.360]   In this incredibly scary late period when AI has really automated research, humans do this function
[00:00:07.360 --> 00:00:14.800]   of like auditing, making it more difficult for the AIs to conspire together and root the servers,
[00:00:14.800 --> 00:00:21.200]   take over the process, and extract information from them within the set of things that we can
[00:00:21.200 --> 00:00:27.840]   verify, like experiments where we can see, oh yeah, this works at stopping an AI trained to
[00:00:27.840 --> 00:00:35.920]   get a fast one past human raters. The reasons why I think we actually have such a relatively good
[00:00:35.920 --> 00:00:48.080]   chance of handling that are twofold. So one is, as we approach that kind of AI capability,
[00:00:48.080 --> 00:00:56.080]   we're approaching that from weaker systems. If the really bad sorts of motivations develop
[00:00:56.080 --> 00:00:59.680]   relatively later in the training process, at least with all our countermeasures,
[00:00:59.680 --> 00:01:09.280]   then by that time we may have plenty of ability to extract AI assistance on further strengthening
[00:01:09.280 --> 00:01:12.880]   the quality of our adversarial examples, the strength of our neural lie detectors,
[00:01:12.880 --> 00:01:18.480]   the experiments that we can use to reveal and elicit and distinguish between different kinds
[00:01:18.480 --> 00:01:23.920]   of reward hacking tendencies and motivations. So yeah, we may have systems that have just not
[00:01:23.920 --> 00:01:30.080]   developed bad motivations in the first place and be able to use them a lot in developing the
[00:01:30.080 --> 00:01:36.160]   incrementally better systems in a safe way. And we may be able to, even if some of the early systems
[00:01:36.160 --> 00:01:40.960]   do develop these bad motivations, if we're able to detect that and experiment and find a way
[00:01:40.960 --> 00:01:48.400]   to get away from that, then we can win, even if these sort of hostile motivations develop early.
[00:01:48.400 --> 00:01:54.480]   When I combine the possibility that we get relatively lucky on the motivations of the
[00:01:54.480 --> 00:01:59.680]   earlier AI systems, systems strong enough that we can use for some alignment research tasks,
[00:01:59.680 --> 00:02:06.160]   and then the possibility of getting that later with AI assistance that we can't trust fully,
[00:02:06.160 --> 00:02:13.040]   where we have to have hard power constraints and a number of things to prevent them from doing this
[00:02:13.040 --> 00:02:19.680]   takeover, it still seems plausible we can get a second saving throw where we're able to extract
[00:02:19.680 --> 00:02:25.360]   work from these AIs on solving the remaining problems of alignment of things like neural lie
[00:02:25.360 --> 00:02:32.720]   detectors faster than they can contribute in their spare time to the project of overthrowing
[00:02:32.720 --> 00:02:37.920]   humanity, hacking their servers, and removing the hard power. And so if we wind up in a situation
[00:02:37.920 --> 00:02:44.640]   where the AIs are misaligned, and then we need to uncover those motivations, change them,
[00:02:44.640 --> 00:02:54.400]   and align them, then we get a very scary situation for us because we need to do this stuff very
[00:02:54.400 --> 00:03:01.520]   quickly. We may fail, but it's a second chance where our work is just evaluating outputs that
[00:03:01.520 --> 00:03:08.240]   the AIs are delivering, having the hard power and supervision to keep them from successfully
[00:03:08.240 --> 00:03:14.320]   rooting the servers, doing a takeover during this process, and have them finish the alignment task
[00:03:14.320 --> 00:03:21.520]   that we sadly failed to invest enough or succeed in doing beforehand. The incredibly juicy ability
[00:03:21.520 --> 00:03:31.360]   that we have working with the AIs is that we can have an invaluable outcome that we can see and
[00:03:31.360 --> 00:03:39.280]   tell whether they got a fast one past us on an identifiable situation. We can have, here's an
[00:03:39.280 --> 00:03:46.480]   air gap computer, you get control of the keyboard, you can input commands, can you root the environment
[00:03:47.920 --> 00:03:54.960]   and make a blue banana appear on the screen. Even if we train the AI to do that and it succeeds,
[00:03:54.960 --> 00:04:02.160]   we see the blue banana, we know it worked. Even if we did not understand and would not have detected
[00:04:02.160 --> 00:04:09.440]   the particular exploit that it used to do it, this can give us a rich empirical feedback where we're
[00:04:09.440 --> 00:04:19.120]   able to identify things that are even an AI using its best efforts to get past our interpretability
[00:04:19.120 --> 00:04:25.360]   methods. At what point would it be the case that the AI is contributing significantly in the sense
[00:04:25.360 --> 00:04:30.080]   that it would almost be the equivalent of having additional researchers to AI progress in software?
[00:04:30.080 --> 00:04:39.120]   The thing to look for is when is it the case that the contributions from AI are starting to
[00:04:39.120 --> 00:04:47.760]   become as large or larger as the contributions from humans. When this is boosting their
[00:04:47.760 --> 00:04:57.200]   effective productivity by 50 or 100% and if you then go from eight months doubling time, say,
[00:04:57.200 --> 00:05:02.560]   for effective compute from software innovations, things like inventing the transformer or discovering
[00:05:02.560 --> 00:05:07.040]   chinchilla scaling and doing your training runs more optimally or creating flash attention,
[00:05:07.040 --> 00:05:14.560]   it doesn't have to have been able to automate everything involved in the process of AI research.
[00:05:14.560 --> 00:05:20.480]   It can be it's automated a bunch of things and then those are being done in extreme profusion
[00:05:21.040 --> 00:05:26.080]   because a thing that AI can do you have it done much more often because it's so cheap.
[00:05:26.080 --> 00:05:34.720]   And so it's not a threshold of this is human level AI, it can do everything a human can do
[00:05:34.720 --> 00:05:42.400]   with no weaknesses in any area. It's that even with its weaknesses it's able to bump up the
[00:05:42.400 --> 00:05:52.080]   performance. Tens of millions of GPUs, each is doing the work of maybe 40 maybe more of these
[00:05:52.080 --> 00:05:57.680]   kind of existing workers. It's like going from a workforce of tens of thousands to hundreds of
[00:05:57.680 --> 00:06:04.400]   millions. You immediately make all kinds of discoveries then, you immediately develop all
[00:06:04.400 --> 00:06:12.800]   sorts of tremendous technologies. So human level AI is deep, deep into an intelligence explosion.
[00:06:12.800 --> 00:06:15.920]   The intelligence explosion has to start with something weaker than that.
[00:06:15.920 --> 00:06:19.840]   But what is the point at which that feedback loop starts where you can even,
[00:06:19.840 --> 00:06:25.520]   you're not just doing the 0.5% increase in productivity that a sort of AI tool might do,
[00:06:25.520 --> 00:06:29.360]   but is actually the equivalent of a researcher or close to it?
[00:06:29.360 --> 00:06:34.640]   So I think maybe a way to look at it is to give some illustrative examples of the kinds
[00:06:34.640 --> 00:06:40.880]   of capabilities that you might see. What we'll have is intense application of the ways in which
[00:06:40.880 --> 00:06:48.480]   AIs have advantages, partly offsetting their weaknesses. And so AIs are cheap, we can call
[00:06:48.480 --> 00:06:56.800]   a lot of them to do many small problems. And so you'll have situations where you have dumber AIs
[00:06:57.600 --> 00:07:04.000]   that are deployed thousands of times to equal, say, one human worker.
[00:07:04.000 --> 00:07:13.440]   And they'll be doing things like these voting algorithms where you, with an LLM, you generate
[00:07:13.440 --> 00:07:18.800]   a bunch of different responses and take a majority vote among them that improves performance sum.
[00:07:18.800 --> 00:07:26.880]   You'll have things like the AlphaGo kind of approach, where you use the neural net to do
[00:07:26.880 --> 00:07:32.640]   search and you go deeper with the search by plowing in more compute, which helps to offset
[00:07:32.640 --> 00:07:36.880]   the inefficiency and weaknesses of the model on its own.
[00:07:36.880 --> 00:07:43.520]   You'll do things that would just be totally impractical for humans because of the sheer
[00:07:43.520 --> 00:07:48.160]   number of steps. And so an example of that would be designing synthetic training data.
[00:07:48.160 --> 00:07:54.720]   So humans do not learn by just going into the library and opening books at random pages.
[00:07:55.680 --> 00:08:02.320]   It's actually much, much more efficient to have things like schools and classes where they teach
[00:08:02.320 --> 00:08:07.040]   you things in an order that makes sense, that's focusing on the skills that are more valuable to
[00:08:07.040 --> 00:08:12.400]   learn. They give you tests and exams that are designed to try and elicit the skill they're
[00:08:12.400 --> 00:08:19.600]   actually trying to teach. And right now we don't bother with that because we can hoover up more
[00:08:19.600 --> 00:08:24.480]   data from the internet. We're getting towards the end of that. But yeah, as the AIs get more
[00:08:24.480 --> 00:08:31.600]   sophisticated, they'll be better able to tell what is a useful kind of skill to practice and
[00:08:31.600 --> 00:08:37.440]   to generate that. And we've done that in other areas. So AlphaGo, the original version of AlphaGo
[00:08:37.440 --> 00:08:44.960]   was booted up with data from human Go play and then improved with reinforcement learning and
[00:08:44.960 --> 00:08:52.480]   Monte Carlo tree search. But then AlphaZero, with a somewhat more sophisticated model,
[00:08:52.480 --> 00:09:00.000]   benefited from some other improvements, but was able to go from scratch. And it generated
[00:09:00.000 --> 00:09:07.440]   its own data through self-play, so getting data of a higher quality than the human data,
[00:09:07.440 --> 00:09:13.600]   because there are no human players that good available in the dataset. And also a curriculum,
[00:09:13.600 --> 00:09:19.440]   so that at any given point, it was playing games against an opponent of equal skill itself.
[00:09:20.480 --> 00:09:25.360]   And so it was always in an area when it was easy to learn. If you're just always losing,
[00:09:25.360 --> 00:09:30.000]   no matter what you do, or always winning, no matter what you do, it's hard to distinguish
[00:09:30.000 --> 00:09:36.240]   which things are better and which are worse. And when we have somewhat more sophisticated AIs that
[00:09:36.240 --> 00:09:41.840]   can generate training data and tasks for themselves, for example, if the AI can generate a
[00:09:41.840 --> 00:09:48.720]   lot of unit tests and then can try and produce programs that pass those unit tests, then the
[00:09:48.720 --> 00:09:55.760]   interpreter is providing a training signal. And the AI can get good at figuring out what's the
[00:09:55.760 --> 00:10:01.120]   kind of programming problem that is hard for AIs right now that will develop more of the skills
[00:10:01.120 --> 00:10:08.800]   that I need and then do them. And now you're not going to have employees at OpenAI write like a
[00:10:08.800 --> 00:10:14.080]   billion programming problems. That's just not going to happen. But you are going to have AIs
[00:10:14.080 --> 00:10:19.600]   given the task of producing those enormous number of programming challenges.

