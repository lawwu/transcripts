
[00:00:00.000 --> 00:00:03.360]   [MUSIC PLAYING]
[00:00:03.360 --> 00:00:08.640]   Now, let's explore sampling methods
[00:00:08.640 --> 00:00:10.120]   for large language models.
[00:00:10.120 --> 00:00:16.200]   LLMs generate text through sampling.
[00:00:16.200 --> 00:00:18.240]   They produce a set of probabilities
[00:00:18.240 --> 00:00:21.000]   across the vocabulary and pick a token
[00:00:21.000 --> 00:00:24.520]   to follow the input sequence.
[00:00:24.520 --> 00:00:27.000]   You might think choosing the token with the highest
[00:00:27.000 --> 00:00:29.080]   probability is best.
[00:00:29.080 --> 00:00:33.120]   This approach is called greedy decoding.
[00:00:33.120 --> 00:00:35.600]   Another option is beam search, which
[00:00:35.600 --> 00:00:38.520]   generates multiple candidate sequences
[00:00:38.520 --> 00:00:42.800]   to maximize the probability of a sequence of tokens.
[00:00:42.800 --> 00:00:47.000]   However, neither method results in natural text.
[00:00:47.000 --> 00:00:49.320]   Text generated via beam search often
[00:00:49.320 --> 00:00:52.800]   has repetitions and lacks meaning.
[00:00:52.800 --> 00:00:56.440]   Researchers compared beam search to human-generated text,
[00:00:56.440 --> 00:00:59.120]   finding significant differences.
[00:00:59.120 --> 00:01:03.120]   Human text appears more interesting and relevant.
[00:01:03.120 --> 00:01:06.760]   So we need alternative sampling methods.
[00:01:06.760 --> 00:01:10.760]   One option is called sampling with temperature.
[00:01:10.760 --> 00:01:14.680]   Adjusting temperature affects token probabilities,
[00:01:14.680 --> 00:01:19.320]   with higher values resulting in more diverse outputs.
[00:01:19.320 --> 00:01:23.000]   Lower temperature makes less probable tokens even less
[00:01:23.000 --> 00:01:24.440]   likely.
[00:01:24.440 --> 00:01:27.280]   As temperature approaches zero, this method
[00:01:27.280 --> 00:01:30.560]   becomes like greedy decoding.
[00:01:30.560 --> 00:01:34.120]   High temperature values might generate less useful text.
[00:01:34.120 --> 00:01:37.040]   And we will experiment with temperature in our next video.
[00:01:37.040 --> 00:01:44.360]   Another technique is top-P sampling,
[00:01:44.360 --> 00:01:49.200]   proposed by researchers studying beam search and human text.
[00:01:49.200 --> 00:01:51.240]   This method cuts off probabilities
[00:01:51.240 --> 00:01:54.480]   at a certain threshold, only considering tokens
[00:01:54.480 --> 00:01:57.160]   with higher probabilities.
[00:01:57.160 --> 00:02:00.240]   It excludes low-probability tokens and samples
[00:02:00.240 --> 00:02:02.960]   from those with higher chances.
[00:02:02.960 --> 00:02:05.120]   Depending on the input, distribution
[00:02:05.120 --> 00:02:09.160]   may be flat or peaked, affecting token choice.
[00:02:09.160 --> 00:02:12.320]   Top-P sampling is popular and results
[00:02:12.320 --> 00:02:14.640]   in high-quality generated text.
[00:02:14.640 --> 00:02:17.120]   And again, that's something that we'll experiment
[00:02:17.120 --> 00:02:19.680]   in the next video.
[00:02:19.920 --> 00:02:23.280]   [MUSIC PLAYING]
[00:02:23.280 --> 00:02:26.640]   [MUSIC ENDS]
[00:02:26.640 --> 00:02:29.140]   (light music)

