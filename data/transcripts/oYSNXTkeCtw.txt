
[00:00:00.000 --> 00:00:02.720]   (upbeat music)
[00:00:02.720 --> 00:00:04.040]   - What we see right now in the field
[00:00:04.040 --> 00:00:07.920]   is that there's lots of interesting reinforcement learning
[00:00:07.920 --> 00:00:10.120]   results that come out of industry labs
[00:00:10.120 --> 00:00:12.300]   that have a lot of computational resources.
[00:00:12.300 --> 00:00:13.960]   And that makes it basically impossible
[00:00:13.960 --> 00:00:16.540]   for anyone outside, specifically in academia,
[00:00:16.540 --> 00:00:17.560]   to reproduce these results.
[00:00:17.560 --> 00:00:19.420]   And that was exactly the kind of motivation
[00:00:19.420 --> 00:00:22.460]   behind that environment in that it's really complex,
[00:00:22.460 --> 00:00:24.840]   but at the same time should be affordable
[00:00:24.840 --> 00:00:27.680]   for grad students and master's students and whatnot
[00:00:27.680 --> 00:00:29.720]   to actually do experiments with.
[00:00:29.720 --> 00:00:31.680]   You're listening to Gradient Dissent,
[00:00:31.680 --> 00:00:34.080]   a show where we learn about making machine learning models
[00:00:34.080 --> 00:00:35.320]   work in the real world.
[00:00:35.320 --> 00:00:37.680]   I'm your host, Lucas Biewald.
[00:00:37.680 --> 00:00:41.160]   Tim is a research scientist at Facebook AI Research
[00:00:41.160 --> 00:00:44.360]   and a lecturer at the University College of London.
[00:00:44.360 --> 00:00:47.520]   Heinrich is a research engineer at Facebook AI Research
[00:00:47.520 --> 00:00:49.520]   and previously worked at DeepMind.
[00:00:49.520 --> 00:00:51.920]   Together, they built the NetHack learning environment,
[00:00:51.920 --> 00:00:54.560]   which is a super exciting project to make it easier
[00:00:54.560 --> 00:00:56.240]   for people to build and experiment
[00:00:56.240 --> 00:00:58.160]   with reinforcement learning algorithms.
[00:00:58.160 --> 00:00:59.960]   It also operates in a game called NetHack
[00:00:59.960 --> 00:01:01.520]   that I've played for the last three decades.
[00:01:01.520 --> 00:01:04.520]   And so I'm especially excited to talk to these guys.
[00:01:04.520 --> 00:01:05.680]   I had been thinking for a while,
[00:01:05.680 --> 00:01:08.880]   I was wondering how well reinforcement learning
[00:01:08.880 --> 00:01:10.160]   would work on the game NetHack.
[00:01:10.160 --> 00:01:11.440]   And then I came across your project
[00:01:11.440 --> 00:01:13.080]   and you're actually making an environment
[00:01:13.080 --> 00:01:15.960]   where people could try out different algorithms in NetHack.
[00:01:15.960 --> 00:01:18.160]   So maybe you could start by kind of telling me
[00:01:18.160 --> 00:01:21.400]   how you came to this idea as a learning environment
[00:01:21.400 --> 00:01:24.160]   and maybe describe what the game NetHack is.
[00:01:24.160 --> 00:01:26.640]   So NetHack is this really old game
[00:01:26.640 --> 00:01:28.520]   that grew out of an even older game
[00:01:28.520 --> 00:01:31.480]   in I think the 80s or thereabouts.
[00:01:31.480 --> 00:01:33.680]   So it's as old as Unix basically.
[00:01:33.680 --> 00:01:36.000]   And it's this like text-based game,
[00:01:36.000 --> 00:01:37.160]   Dungeons and Dragons style.
[00:01:37.160 --> 00:01:39.080]   The object is to go down a dungeon
[00:01:39.080 --> 00:01:43.160]   and retrieve a certain item and then go back up and win.
[00:01:43.160 --> 00:01:45.320]   And that kind of undersells it
[00:01:45.320 --> 00:01:47.880]   because the actual fun is in interacting
[00:01:47.880 --> 00:01:50.240]   with all the monsters or picking up objects.
[00:01:50.240 --> 00:01:52.560]   And then there's loads of in-game jokes.
[00:01:52.560 --> 00:01:55.160]   And it's also generally quite a hard game.
[00:01:55.160 --> 00:01:57.640]   So I've been playing NetHack since I was quite young,
[00:01:57.640 --> 00:01:59.000]   I think about 12.
[00:01:59.000 --> 00:02:00.440]   And it was like some dust box
[00:02:00.440 --> 00:02:02.000]   where someone installed NetHack.
[00:02:02.000 --> 00:02:04.360]   And I didn't really like understand what to do.
[00:02:04.360 --> 00:02:06.760]   And then later on when I had the internet
[00:02:06.760 --> 00:02:09.440]   and there were some so-called spoilers,
[00:02:09.440 --> 00:02:10.640]   and I started being able to look
[00:02:10.640 --> 00:02:12.400]   at this actual source code of NetHack,
[00:02:12.400 --> 00:02:15.040]   I started being able to like do more in the game.
[00:02:15.040 --> 00:02:17.560]   So it's really easy to die in NetHack.
[00:02:17.560 --> 00:02:19.720]   You don't have a second chance.
[00:02:19.720 --> 00:02:21.520]   Like you can save the game, but then it exits.
[00:02:21.520 --> 00:02:23.760]   And when you enter it, go back in,
[00:02:23.760 --> 00:02:26.040]   it's like it's fixed up where you were.
[00:02:26.040 --> 00:02:28.040]   And it's a really hard and really fun game
[00:02:28.040 --> 00:02:30.320]   with a still active community.
[00:02:30.320 --> 00:02:33.480]   And it's text-based, but still pretty complex.
[00:02:33.480 --> 00:02:35.640]   - I feel like what's notable to me about NetHack
[00:02:35.640 --> 00:02:39.320]   is I've probably played it more than maybe any other game.
[00:02:39.320 --> 00:02:40.840]   And yet I'm still kind of surprised
[00:02:40.840 --> 00:02:41.920]   by things that happen in it.
[00:02:41.920 --> 00:02:45.040]   Like I still find myself looking up like what's going on.
[00:02:45.040 --> 00:02:47.960]   And like, it seems like people even will kind of come
[00:02:47.960 --> 00:02:50.840]   to like interesting ideas about how to use the objects
[00:02:50.840 --> 00:02:52.720]   in the game and the game will actually have
[00:02:52.720 --> 00:02:54.040]   kind of supported these sort of
[00:02:54.040 --> 00:02:56.400]   one in a million chance occurrences.
[00:02:56.400 --> 00:02:58.400]   So it seems like incredibly deep.
[00:02:58.400 --> 00:03:00.720]   Like I don't even actually know how deep it goes
[00:03:00.720 --> 00:03:02.880]   for how simple it looks at first.
[00:03:02.880 --> 00:03:03.880]   - Yeah, I fully agree.
[00:03:03.880 --> 00:03:06.120]   I mean, so the reason why we believe
[00:03:06.120 --> 00:03:08.800]   this is an interesting challenge for reinforcement learning
[00:03:08.800 --> 00:03:10.640]   is exactly that kind of depth, right?
[00:03:10.640 --> 00:03:14.240]   It's as Heiner mentioned, from the looks of it,
[00:03:14.240 --> 00:03:17.280]   it's a quite simple game in that it's a terminal based,
[00:03:17.280 --> 00:03:18.680]   right, so everything is rendered
[00:03:18.680 --> 00:03:20.800]   as these ASCII characters in terminal.
[00:03:20.800 --> 00:03:24.400]   But in fact, it's so deep in terms of the number of items
[00:03:24.400 --> 00:03:25.440]   and the number of monsters
[00:03:25.440 --> 00:03:27.200]   that you have to learn to interact with.
[00:03:27.200 --> 00:03:29.360]   There's always new things to discover.
[00:03:29.360 --> 00:03:32.400]   And on top of that, it's procedurally generated.
[00:03:32.400 --> 00:03:34.840]   That means every time you enter the game,
[00:03:34.840 --> 00:03:36.400]   every time you enter the dungeon,
[00:03:36.400 --> 00:03:38.360]   it will be generated in front of you
[00:03:38.360 --> 00:03:41.200]   and it will look different from any other episode
[00:03:41.200 --> 00:03:43.320]   that you have been playing before.
[00:03:43.320 --> 00:03:46.880]   So that gives it also a lot of, I guess, replayability.
[00:03:46.880 --> 00:03:48.720]   And it's much closer, I guess, in spirit
[00:03:48.720 --> 00:03:51.200]   to more modern games like Minecraft, right,
[00:03:51.200 --> 00:03:52.800]   where also every time you play Minecraft,
[00:03:52.800 --> 00:03:53.880]   the world is generated.
[00:03:53.880 --> 00:03:57.160]   That poses very unique challenges to reinforcement learning
[00:03:57.160 --> 00:03:59.280]   because so far, or for a long time,
[00:03:59.280 --> 00:04:03.640]   we've been mostly using games like Atari games
[00:04:03.640 --> 00:04:06.800]   to test the limits of reinforcement learning agents.
[00:04:06.800 --> 00:04:09.960]   And that has been going on for a while and has been good.
[00:04:09.960 --> 00:04:13.040]   But at some point, I think people started to realize
[00:04:13.040 --> 00:04:16.960]   that in Atari, when you, let's say, play Breakout
[00:04:16.960 --> 00:04:18.640]   or you play even Montezuma's Revenge,
[00:04:18.640 --> 00:04:19.880]   which is one of the hardest games
[00:04:19.880 --> 00:04:22.120]   in the arcade learning environment,
[00:04:22.120 --> 00:04:23.920]   every time you play that game, it's the same, right?
[00:04:23.920 --> 00:04:27.880]   I mean, you can basically memorize sequences of actions
[00:04:27.880 --> 00:04:30.320]   through the game that lead you to win the game.
[00:04:30.320 --> 00:04:33.400]   And that's exactly what then approaches like Go Explore
[00:04:33.400 --> 00:04:37.240]   by Ubai have been exploited to win the game.
[00:04:37.240 --> 00:04:40.200]   So I think it started roughly two, three years ago
[00:04:40.200 --> 00:04:41.840]   when people started to look
[00:04:41.840 --> 00:04:44.120]   into these procedurally generated games.
[00:04:44.120 --> 00:04:45.800]   I mean, Minecraft is one example,
[00:04:45.800 --> 00:04:47.880]   but it's very expensive to render
[00:04:47.880 --> 00:04:49.440]   and expensive to simulate.
[00:04:49.440 --> 00:04:51.760]   But also, I guess, the Obstacle Tower Challenge
[00:04:51.760 --> 00:04:53.320]   by Unity AI is another example
[00:04:53.320 --> 00:04:54.960]   of such a procedurally generated environment
[00:04:54.960 --> 00:04:56.520]   for reinforcement learning.
[00:04:56.520 --> 00:04:59.920]   And then more recently, OpenAI's ProcGen Benchmark
[00:04:59.920 --> 00:05:00.960]   is another example.
[00:05:00.960 --> 00:05:03.560]   So people are looking more and more for test beds
[00:05:03.560 --> 00:05:06.560]   where reinforcement learning agents really have to learn,
[00:05:06.560 --> 00:05:09.800]   to generalize to novel situations, novel observations.
[00:05:09.800 --> 00:05:12.720]   And we believe NetHack is a perfect example for that
[00:05:12.720 --> 00:05:14.920]   because it's at the same time also really fast to run
[00:05:14.920 --> 00:05:18.720]   and really deep, much deeper than many of the 3D games
[00:05:18.720 --> 00:05:20.120]   that you could play right now.
[00:05:20.120 --> 00:05:21.280]   - I guess I never thought about this,
[00:05:21.280 --> 00:05:24.080]   but I'm also a huge fan of the game Go.
[00:05:24.080 --> 00:05:26.120]   And that game also feels deep,
[00:05:26.120 --> 00:05:28.960]   but its depth seems to come from a lot of interactions
[00:05:28.960 --> 00:05:30.240]   with a small number of rules.
[00:05:30.240 --> 00:05:32.400]   Whereas I imagine the NetHack code base
[00:05:32.400 --> 00:05:35.200]   just having this massive nest of case statements.
[00:05:35.200 --> 00:05:37.760]   It's almost like the complexity is intrinsic to it.
[00:05:37.760 --> 00:05:40.280]   I wonder, what is it about...
[00:05:40.280 --> 00:05:43.280]   I mean, both Go and NetHack, I think, are kind of deep
[00:05:43.280 --> 00:05:46.560]   in the sense that it's kind of hard for people to do well.
[00:05:46.560 --> 00:05:48.640]   But what is it about reinforcement algorithms
[00:05:48.640 --> 00:05:51.120]   that worked really well for Go
[00:05:51.120 --> 00:05:54.920]   and struggled to do basic things in the NetHack world?
[00:05:54.920 --> 00:05:57.120]   - Yeah, so Go is a really interesting case
[00:05:57.120 --> 00:06:00.200]   because the depth and the complexity of Go
[00:06:00.200 --> 00:06:01.400]   comes from the fact that you're playing
[00:06:01.400 --> 00:06:02.760]   against another player, right?
[00:06:02.760 --> 00:06:04.960]   So we should, first of all, state that NetHack
[00:06:04.960 --> 00:06:06.200]   is a single player game, right?
[00:06:06.200 --> 00:06:07.160]   You play against the game,
[00:06:07.160 --> 00:06:09.400]   you're not playing against another human.
[00:06:09.400 --> 00:06:10.240]   And obviously, right,
[00:06:10.240 --> 00:06:12.920]   if you have a very strong human you play against,
[00:06:12.920 --> 00:06:14.600]   then that's a really hard game.
[00:06:14.600 --> 00:06:17.080]   But what makes this work for reinforcement learning
[00:06:17.080 --> 00:06:18.640]   is the fact that Go, as you mentioned,
[00:06:18.640 --> 00:06:19.680]   has very simple rules.
[00:06:19.680 --> 00:06:23.280]   So it's very clear for a specific action
[00:06:23.280 --> 00:06:25.680]   how the next state will look like, right?
[00:06:25.680 --> 00:06:27.880]   And that allows you to basically
[00:06:27.880 --> 00:06:30.400]   exploit planning mechanisms, right?
[00:06:30.400 --> 00:06:32.640]   That allow you to basically plan ahead,
[00:06:32.640 --> 00:06:35.680]   think through what happens if somebody plays a specific move
[00:06:35.680 --> 00:06:37.800]   and then I play a specific move, what will happen?
[00:06:37.800 --> 00:06:39.360]   And it's still really hard
[00:06:39.360 --> 00:06:42.200]   because there's this humongous observation space
[00:06:42.200 --> 00:06:43.160]   in Go already, right?
[00:06:43.160 --> 00:06:45.680]   Because you have this 90 by 90 board
[00:06:45.680 --> 00:06:48.960]   and on every tile there could be a white mark
[00:06:48.960 --> 00:06:51.040]   or a black mark or a white stone,
[00:06:51.040 --> 00:06:52.920]   a black stone or no stone.
[00:06:52.920 --> 00:06:55.920]   But in NetHack, it's fundamentally different
[00:06:55.920 --> 00:06:58.560]   in that the transition dynamics that govern
[00:06:58.560 --> 00:07:02.120]   how a state evolves from time step T to the next one,
[00:07:02.120 --> 00:07:03.560]   extremely complex.
[00:07:03.560 --> 00:07:06.280]   Because it is, first of all, it's partially observable.
[00:07:06.280 --> 00:07:09.800]   You don't see what's on the entire map, right?
[00:07:09.800 --> 00:07:11.120]   There might be a monster around the corner,
[00:07:11.120 --> 00:07:13.080]   but it's not visible to you.
[00:07:13.080 --> 00:07:14.800]   And on top of that, it's stochastic.
[00:07:14.800 --> 00:07:17.120]   So every time you hit a monster,
[00:07:17.120 --> 00:07:18.200]   just to give an example, right?
[00:07:18.200 --> 00:07:19.520]   There's a die roll in the bag
[00:07:19.520 --> 00:07:22.720]   that determines really how much damage you incur.
[00:07:22.720 --> 00:07:27.520]   And on top of that, there are so many possibilities
[00:07:27.520 --> 00:07:29.480]   in terms of what could actually be on the tile.
[00:07:29.480 --> 00:07:30.960]   So there's like hundreds, as I mentioned,
[00:07:30.960 --> 00:07:32.680]   hundreds of monsters, hundreds of items.
[00:07:32.680 --> 00:07:36.680]   Each of them come with all kinds of specific attributes
[00:07:36.680 --> 00:07:40.280]   and specific mechanisms that you have to learn about
[00:07:40.280 --> 00:07:41.160]   in order to do well.
[00:07:41.160 --> 00:07:43.080]   So it's really, really hard to plan ahead.
[00:07:43.080 --> 00:07:45.120]   It's also really hard over time
[00:07:45.120 --> 00:07:47.680]   to even learn about all of these mechanisms, right?
[00:07:47.680 --> 00:07:49.960]   Whereas in Go, you can write down the rules easily
[00:07:49.960 --> 00:07:52.320]   in a program and you can simulate.
[00:07:52.320 --> 00:07:55.040]   - I think there's another aspect of this comparison
[00:07:55.040 --> 00:07:57.680]   with Go and with MCDS-like algorithms.
[00:07:57.680 --> 00:07:59.560]   The NetHack community actually has done loads
[00:07:59.560 --> 00:08:02.960]   of crazy things outside of research and published papers.
[00:08:02.960 --> 00:08:04.760]   There's a few people in the NetHack community
[00:08:04.760 --> 00:08:07.480]   that have, for instance, there's this alt.org website
[00:08:07.480 --> 00:08:09.480]   where you can have officially recorded games.
[00:08:09.480 --> 00:08:11.800]   And what you could do in the previous version of NetHack
[00:08:11.800 --> 00:08:14.000]   is that you would have your local NetHack
[00:08:14.000 --> 00:08:16.320]   on your own machine and you would try out a few things
[00:08:16.320 --> 00:08:17.960]   and whatever you like best, you would do that
[00:08:17.960 --> 00:08:20.520]   in the actual online running game, right?
[00:08:20.520 --> 00:08:21.960]   Where you basically have this perfect simulator,
[00:08:21.960 --> 00:08:23.560]   which is NetHack itself.
[00:08:23.560 --> 00:08:26.520]   And the only thing that, so what Tim was saying,
[00:08:26.520 --> 00:08:28.360]   that how can that work, right?
[00:08:28.360 --> 00:08:30.640]   It's not deterministic, it's stochastic, right?
[00:08:30.640 --> 00:08:33.560]   So the way people did that is that they hashed,
[00:08:33.560 --> 00:08:36.440]   like had a map from all starting positions of the game
[00:08:36.440 --> 00:08:40.120]   with your inventory and so on to the seed of the RNG,
[00:08:40.120 --> 00:08:43.120]   pre-computed this in a few days and hours of compute,
[00:08:43.120 --> 00:08:46.400]   and then could look up the seed, see a new game,
[00:08:46.400 --> 00:08:49.200]   look at your inventory, and that's enough entropy
[00:08:49.200 --> 00:08:50.840]   to tell you what seed you are in.
[00:08:50.840 --> 00:08:53.800]   And then you know which seed to initialize
[00:08:53.800 --> 00:08:55.840]   your local version, and then you can actually beat the game
[00:08:55.840 --> 00:08:56.960]   in no time, right?
[00:08:56.960 --> 00:09:00.160]   Because you have the perfect simulator.
[00:09:00.160 --> 00:09:03.280]   And then the NetHack dev team produced a new version
[00:09:03.280 --> 00:09:04.480]   of NetHack that makes this impossible,
[00:09:04.480 --> 00:09:06.920]   where you can no longer manipulate the RNG state
[00:09:06.920 --> 00:09:08.360]   by walking against walls or whatever,
[00:09:08.360 --> 00:09:09.600]   the way that these people did it.
[00:09:09.600 --> 00:09:11.640]   But it's comparable in a way to how you would do it
[00:09:11.640 --> 00:09:14.640]   if you were just like to plain like MCTS NetHack, right?
[00:09:14.640 --> 00:09:17.200]   You save the game and you try out what's happening,
[00:09:17.200 --> 00:09:19.240]   and then you'll go back to the position
[00:09:19.240 --> 00:09:20.480]   where you really were.
[00:09:20.480 --> 00:09:22.920]   And you could probably beat NetHack that way pretty easily,
[00:09:22.920 --> 00:09:24.720]   but you would really only beat NetHack.
[00:09:24.720 --> 00:09:27.560]   You wouldn't learn anything about reinforcement learning
[00:09:27.560 --> 00:09:28.680]   at large.
[00:09:28.680 --> 00:09:30.680]   - And it's also really clear that for the community
[00:09:30.680 --> 00:09:33.200]   and also for us, that would be considered cheating, right?
[00:09:33.200 --> 00:09:35.440]   I mean, really you should be developing agents
[00:09:35.440 --> 00:09:37.920]   that can, given a fresh game of NetHack,
[00:09:37.920 --> 00:09:39.320]   solve the game of NetHack.
[00:09:39.320 --> 00:09:41.840]   - That's funny, I think I would be impressed if,
[00:09:41.840 --> 00:09:44.680]   I mean, yes, if you could see the random number generator
[00:09:44.680 --> 00:09:47.320]   and forecast ahead, it would be much easier,
[00:09:47.320 --> 00:09:49.120]   but it still seems a little bit tricky, right?
[00:09:49.120 --> 00:09:51.720]   I feel like there's a fair amount of long range planning
[00:09:51.720 --> 00:09:53.200]   that you need to do, right?
[00:09:53.200 --> 00:09:55.240]   I've actually never won the game, so I don't even know,
[00:09:55.240 --> 00:09:57.480]   but I feel like even if I could see ahead,
[00:09:57.480 --> 00:10:00.400]   it might be hard for me to beat the game.
[00:10:00.400 --> 00:10:03.640]   It's still gonna be super hard for playing like,
[00:10:03.640 --> 00:10:05.560]   learning from scratch reinforcement learning algorithm.
[00:10:05.560 --> 00:10:07.160]   But what these guys did is that they,
[00:10:07.160 --> 00:10:08.640]   you basically can get infinite wishes, right?
[00:10:08.640 --> 00:10:11.320]   There's a thing in NetHack, like in certain situations,
[00:10:11.320 --> 00:10:12.960]   there's a wish and then you can wish for any object
[00:10:12.960 --> 00:10:14.080]   and you can get it.
[00:10:14.080 --> 00:10:17.720]   And if you can force the RNG to always give you a wish,
[00:10:17.720 --> 00:10:19.720]   and then you can get like infinite amounts of wishes,
[00:10:19.720 --> 00:10:22.040]   and you can always like make this mini,
[00:10:22.040 --> 00:10:24.320]   there's actually, like when I played NetHack
[00:10:24.320 --> 00:10:25.440]   when I was very young,
[00:10:25.440 --> 00:10:26.720]   I did this thing called safe scumming,
[00:10:26.720 --> 00:10:28.360]   which you're not supposed to do where like,
[00:10:28.360 --> 00:10:30.120]   you save the game and then you copy the game file,
[00:10:30.120 --> 00:10:32.240]   the safe file, and then when you die, you go back
[00:10:32.240 --> 00:10:34.440]   and you like go back to that point in time.
[00:10:34.440 --> 00:10:36.000]   And what you do with that,
[00:10:36.000 --> 00:10:37.600]   like from a scientific perspective is,
[00:10:37.600 --> 00:10:39.960]   you force a really unlikely trajectory, right?
[00:10:39.960 --> 00:10:41.640]   With all the games where you died and you didn't like it,
[00:10:41.640 --> 00:10:43.800]   threw them out and you go into this more
[00:10:43.800 --> 00:10:45.840]   and more unlikely space.
[00:10:45.840 --> 00:10:48.320]   And at some point you're like,
[00:10:48.320 --> 00:10:50.120]   you really like dodged all the bullets,
[00:10:50.120 --> 00:10:52.560]   but the game will just kill you like a thousand times
[00:10:52.560 --> 00:10:55.400]   per round because you didn't really beat it.
[00:10:55.400 --> 00:10:56.920]   And I think this is what's likely to happen,
[00:10:56.920 --> 00:10:59.080]   what you will likely force when you like can force
[00:10:59.080 --> 00:11:00.680]   your own GDB in a specific state.
[00:11:00.680 --> 00:11:03.800]   You produce this extremely unlikely trajectories
[00:11:03.800 --> 00:11:04.880]   of the game.
[00:11:04.880 --> 00:11:06.920]   - When you take like the sort of basic,
[00:11:06.920 --> 00:11:09.880]   you know, reinforcement algorithm from Go
[00:11:09.880 --> 00:11:12.160]   or just sort of like a vanilla reinforcement learning
[00:11:12.160 --> 00:11:14.520]   algorithm and you train it on NetHack, what happens?
[00:11:14.520 --> 00:11:16.760]   What does the character do?
[00:11:16.760 --> 00:11:19.840]   - Yeah, that's what I guess exactly the thing
[00:11:19.840 --> 00:11:20.960]   that we wanted to see.
[00:11:20.960 --> 00:11:23.640]   I mean, first of all, you couldn't use MCDS from Go
[00:11:23.640 --> 00:11:26.720]   just because you don't have that environment
[00:11:26.720 --> 00:11:27.640]   transition model, right?
[00:11:27.640 --> 00:11:29.880]   You don't know what happens at the next timestamp
[00:11:29.880 --> 00:11:31.280]   given the current timestamp in an action.
[00:11:31.280 --> 00:11:32.840]   - Actually, wait, sorry, I need to step even back
[00:11:32.840 --> 00:11:34.320]   one step further.
[00:11:34.320 --> 00:11:37.120]   What are you actually even optimized for when you,
[00:11:37.120 --> 00:11:38.720]   I mean, Go it's so clear that you're trying to win,
[00:11:38.720 --> 00:11:39.560]   but I don't think that's a good question.
[00:11:39.560 --> 00:11:40.520]   - That's a great question.
[00:11:40.520 --> 00:11:41.920]   - It's a really great question, yes.
[00:11:41.920 --> 00:11:44.080]   So ideally, right, we want to have agents
[00:11:44.080 --> 00:11:46.720]   that can win NetHack and the way to win NetHack
[00:11:46.720 --> 00:11:48.800]   is to ascend to Demi Gohai's fly,
[00:11:48.800 --> 00:11:51.800]   offering the amulet of Yendo to your in-game daily.
[00:11:51.800 --> 00:11:53.880]   But the problem is that that's a really sparse reward,
[00:11:53.880 --> 00:11:54.720]   right?
[00:11:54.720 --> 00:11:55.560]   - Yeah.
[00:11:55.560 --> 00:11:57.480]   - It's like you have to solve it before you get any reward.
[00:11:57.480 --> 00:11:58.560]   So that doesn't work.
[00:11:58.560 --> 00:12:01.800]   Then there's lots of techniques right now
[00:12:01.800 --> 00:12:04.520]   for providing agents with intrinsic motivation.
[00:12:04.520 --> 00:12:07.680]   I mean, that's what basically keeps people like you and me
[00:12:07.680 --> 00:12:10.040]   playing NetHack, although they haven't won NetHack yet.
[00:12:10.040 --> 00:12:12.760]   We're just curious about, you know, finding new quirks
[00:12:12.760 --> 00:12:15.960]   and new interesting situations in NetHack.
[00:12:15.960 --> 00:12:19.200]   But what we basically did is we have an reinforcement agent
[00:12:19.200 --> 00:12:21.680]   that is trying to optimize for in-game score.
[00:12:21.680 --> 00:12:23.560]   And that comes with all kinds of caveats actually,
[00:12:23.560 --> 00:12:27.640]   because you can try to maximize the in-game score
[00:12:27.640 --> 00:12:29.600]   by doing all kinds of things that are unrelated
[00:12:29.600 --> 00:12:31.520]   to actually winning the game.
[00:12:31.520 --> 00:12:33.800]   So for instance, you get score for killing monsters,
[00:12:33.800 --> 00:12:37.160]   you get score for descending deeper down into the dungeon,
[00:12:37.160 --> 00:12:39.520]   but that really doesn't help you to understand
[00:12:39.520 --> 00:12:41.600]   at some point you have to go back up again, right?
[00:12:41.600 --> 00:12:42.600]   Just to give an example.
[00:12:42.600 --> 00:12:46.000]   Also people have been, when they're really good,
[00:12:46.000 --> 00:12:48.360]   so meaning when they already know how to play NetHack
[00:12:48.360 --> 00:12:50.120]   really well and they solve NetHack,
[00:12:50.120 --> 00:12:51.680]   they start to give themselves all kinds
[00:12:51.680 --> 00:12:52.600]   of interesting challenges.
[00:12:52.600 --> 00:12:54.360]   And one is actually to solve NetHack
[00:12:54.360 --> 00:12:56.880]   while minimizing the score.
[00:12:56.880 --> 00:12:58.240]   So you can also do that.
[00:12:58.240 --> 00:13:02.720]   So it's not really a very good reward function in a sense
[00:13:02.720 --> 00:13:04.520]   towards the goal of solving NetHack,
[00:13:04.520 --> 00:13:07.120]   but I think it's still a really good proxy for now
[00:13:07.120 --> 00:13:10.160]   in order to compare how well different models
[00:13:10.160 --> 00:13:11.480]   or different agents do.
[00:13:11.480 --> 00:13:14.160]   So I think for now we're happy with that kind of setup
[00:13:14.160 --> 00:13:16.120]   because we are still in a very early stage
[00:13:16.120 --> 00:13:18.200]   or the community, I guess as a whole,
[00:13:18.200 --> 00:13:20.000]   is in a very early stage when it comes to like
[00:13:20.000 --> 00:13:21.040]   making progress on NetHack,
[00:13:21.040 --> 00:13:22.400]   but I think eventually at some point
[00:13:22.400 --> 00:13:23.600]   we'll have to refine that a bit.
[00:13:23.600 --> 00:13:27.360]   And the winning condition is actually winning the game.
[00:13:27.360 --> 00:13:29.880]   - Got it, so you're optimizing for score?
[00:13:29.880 --> 00:13:31.600]   - You can also optimize for like gold
[00:13:31.600 --> 00:13:33.560]   or dungeon depth of these things,
[00:13:33.560 --> 00:13:36.200]   but typically you do try to optimize for score.
[00:13:36.200 --> 00:13:37.440]   - And it's okay, so what happens
[00:13:37.440 --> 00:13:39.720]   when you put a vanilla agent in there?
[00:13:39.720 --> 00:13:42.680]   - Yeah, so what happens is quite interesting.
[00:13:42.680 --> 00:13:46.080]   So first of all, we thought when we started this project
[00:13:46.080 --> 00:13:50.480]   that just a vanilla agent wouldn't really be doing anything
[00:13:50.480 --> 00:13:53.000]   in NetHack because it's just so complicated.
[00:13:53.000 --> 00:13:56.040]   Just learning to navigate in the first dungeon level
[00:13:56.040 --> 00:13:59.000]   to the next dungeon level is already hard
[00:13:59.000 --> 00:14:00.960]   because there are all kinds of situations
[00:14:00.960 --> 00:14:03.960]   where you are in a room where there might not be any doors
[00:14:03.960 --> 00:14:06.200]   and you have to walk around the walls
[00:14:06.200 --> 00:14:07.640]   to find a secret door,
[00:14:07.640 --> 00:14:09.640]   which is actually quite tricky to learn.
[00:14:09.640 --> 00:14:11.720]   Then you might find doors, but they might be locked
[00:14:11.720 --> 00:14:14.800]   and you don't have any key around, right?
[00:14:14.800 --> 00:14:16.600]   You have to actually kick in the door
[00:14:16.600 --> 00:14:19.520]   to even make it to the next dungeon level.
[00:14:19.520 --> 00:14:20.680]   And we thought this is really hard
[00:14:20.680 --> 00:14:22.080]   for reinforcement learning agents to learn
[00:14:22.080 --> 00:14:26.600]   because there's no reward attached to kicking in a door.
[00:14:26.600 --> 00:14:30.080]   And it actually, it turns out that if you kick a wall,
[00:14:30.080 --> 00:14:31.560]   you hurt yourself and you might die.
[00:14:31.560 --> 00:14:33.520]   So that actually gives you negative reward, right?
[00:14:33.520 --> 00:14:36.440]   Or at least terminates the episode.
[00:14:36.440 --> 00:14:37.560]   But actually what turns out,
[00:14:37.560 --> 00:14:39.080]   and this is really interesting,
[00:14:39.080 --> 00:14:40.240]   is that if you train
[00:14:40.240 --> 00:14:42.960]   in these procedurally generated environments,
[00:14:42.960 --> 00:14:45.440]   what happens is that occasionally
[00:14:45.440 --> 00:14:48.000]   there's an instance generated of this whole problem
[00:14:48.000 --> 00:14:49.280]   that is really simple.
[00:14:49.280 --> 00:14:52.400]   Like the staircase down might be just in a room next to you
[00:14:52.400 --> 00:14:54.840]   and the corridor might already be visible.
[00:14:54.840 --> 00:14:55.880]   So from your starting position,
[00:14:55.880 --> 00:14:58.360]   you might already see where the staircase down is.
[00:14:58.360 --> 00:15:01.040]   So your agent, even when just randomly exploring,
[00:15:01.040 --> 00:15:03.800]   might just bump into that staircase down
[00:15:03.800 --> 00:15:05.960]   and go downstairs and get a reward.
[00:15:05.960 --> 00:15:08.000]   So this is fascinating because it means
[00:15:08.000 --> 00:15:10.160]   with these procedurally generated environments,
[00:15:10.160 --> 00:15:13.400]   if you train for quite a number of episodes,
[00:15:13.400 --> 00:15:16.240]   there will be episodes generated that are quite simple
[00:15:16.240 --> 00:15:18.000]   and where the agent actually can learn
[00:15:18.000 --> 00:15:20.720]   to acquire certain skills to then make also progress
[00:15:20.720 --> 00:15:21.560]   on the harder ones.
[00:15:21.560 --> 00:15:23.440]   So this is one thing that we saw, right?
[00:15:23.440 --> 00:15:26.800]   So our agents right now, just by optimizing for score,
[00:15:26.800 --> 00:15:31.800]   they average at a score of, I think, 750-ish roughly,
[00:15:31.800 --> 00:15:35.160]   which is not bad if you are new to NetHack.
[00:15:35.160 --> 00:15:39.760]   So if I take a random computer scientist in the lab
[00:15:39.760 --> 00:15:41.840]   and I ask them to learn about NetHack and play NetHack,
[00:15:41.840 --> 00:15:44.600]   I think it takes them a good fair amount of time
[00:15:44.600 --> 00:15:47.560]   to reach 750 on average as a score.
[00:15:47.560 --> 00:15:49.280]   I think the maximum score we've seen so far
[00:15:49.280 --> 00:15:51.560]   is maybe something like 4,000 or 5,000.
[00:15:51.560 --> 00:15:53.960]   They descend down to dungeon level,
[00:15:53.960 --> 00:15:55.800]   on average, five or six-ish,
[00:15:55.800 --> 00:15:58.360]   but we also see individual agents sometimes,
[00:15:58.360 --> 00:16:01.320]   luckily, going down even dungeon level 15.
[00:16:01.320 --> 00:16:04.160]   And we see agents killing a lot of monsters on the way
[00:16:04.160 --> 00:16:06.200]   because that gives them immediate reward.
[00:16:06.200 --> 00:16:08.560]   We see them passing by landmarks
[00:16:08.560 --> 00:16:10.800]   like the Oracle or Mine Town even.
[00:16:10.800 --> 00:16:12.360]   So that was actually quite surprising to us
[00:16:12.360 --> 00:16:14.320]   that a vanilla approach can already make
[00:16:14.320 --> 00:16:16.920]   quite steady progress on NetHack.
[00:16:16.920 --> 00:16:18.560]   So yeah, that's quite encouraging, I think,
[00:16:18.560 --> 00:16:20.800]   for them building up other extensions
[00:16:20.800 --> 00:16:22.640]   and more sophisticated models.
[00:16:22.640 --> 00:16:25.040]   - Wow, so it's like a basic model learns that?
[00:16:25.040 --> 00:16:28.080]   You don't have to tweak it at all to get it to that level?
[00:16:28.080 --> 00:16:31.280]   - Yeah, it's a very straightforward model.
[00:16:31.280 --> 00:16:33.400]   Like it's, I mean, the only thing that we do
[00:16:33.400 --> 00:16:35.560]   is that we have basically a convolutional network
[00:16:35.560 --> 00:16:39.280]   that encodes the entire dungeon level
[00:16:39.280 --> 00:16:40.360]   that's visible so far.
[00:16:40.360 --> 00:16:43.040]   We have another convolutional network
[00:16:43.040 --> 00:16:47.200]   that's centered around a seven by seven crop of the agent.
[00:16:47.200 --> 00:16:50.160]   So that gives it basically some inductive bias
[00:16:50.160 --> 00:16:51.400]   that the things that are close to the agent
[00:16:51.400 --> 00:16:52.640]   are more important than, let's say,
[00:16:52.640 --> 00:16:53.960]   things that are very far apart.
[00:16:53.960 --> 00:16:56.680]   We have another feature representation
[00:16:56.680 --> 00:16:58.720]   based on the agent statistics.
[00:16:58.720 --> 00:17:01.000]   And then all of that is mapped down
[00:17:01.000 --> 00:17:02.480]   to a low dimensional representation
[00:17:02.480 --> 00:17:04.840]   that's fed into a recurrent policy parameterized
[00:17:04.840 --> 00:17:07.760]   by an LSTM, and then we get the action distribution
[00:17:07.760 --> 00:17:08.600]   out of that.
[00:17:08.600 --> 00:17:10.400]   So it's really nothing fancy at this point.
[00:17:10.400 --> 00:17:12.000]   - Maybe we should mention that it also does
[00:17:12.000 --> 00:17:12.960]   some bad things.
[00:17:12.960 --> 00:17:15.040]   Like if you optimize for score, for instance,
[00:17:15.040 --> 00:17:17.480]   it quickly notices that it has this pet with it
[00:17:17.480 --> 00:17:18.600]   in the beginning, right?
[00:17:18.600 --> 00:17:22.440]   And if the pet kills a enemy, or kills a monster,
[00:17:22.440 --> 00:17:24.200]   then you don't get the score.
[00:17:24.200 --> 00:17:26.840]   So what it learns, at some point of training,
[00:17:26.840 --> 00:17:29.080]   it starts killing its own pet, which is really bad.
[00:17:29.080 --> 00:17:31.960]   Like you kick your cat in the cat roll.
[00:17:31.960 --> 00:17:33.440]   But it will do that.
[00:17:33.440 --> 00:17:36.440]   And the interesting thing is it starts playing random games,
[00:17:36.440 --> 00:17:39.240]   then it starts killing the pet along training.
[00:17:39.240 --> 00:17:41.760]   But then if you train for longer, it stops killing the pet
[00:17:41.760 --> 00:17:43.400]   because it notices that killing the pet
[00:17:43.400 --> 00:17:46.080]   actually makes an in-game NetEck DT mad at you
[00:17:46.080 --> 00:17:47.720]   and bad things happen.
[00:17:47.720 --> 00:17:49.800]   So it will stop doing this after a while.
[00:17:49.800 --> 00:17:52.320]   So that's kind of an interesting behavior.
[00:17:52.320 --> 00:17:53.720]   - That's really interesting.
[00:17:53.720 --> 00:17:55.160]   - Also, I think we should mention that
[00:17:55.160 --> 00:17:56.600]   from what Tim says right now,
[00:17:56.600 --> 00:17:58.120]   you can kind of, like if you know the game of NetEck,
[00:17:58.120 --> 00:17:59.400]   you notice that we don't actually use
[00:17:59.400 --> 00:18:00.800]   all the inputs yet, right?
[00:18:00.800 --> 00:18:03.040]   So NetEck has like this status bar,
[00:18:03.040 --> 00:18:06.840]   like the stats of your strength and so on.
[00:18:06.840 --> 00:18:09.440]   It has the dungeon, but it also has this message.
[00:18:09.440 --> 00:18:11.520]   And it has these little in-game windows
[00:18:11.520 --> 00:18:13.360]   that can pop up, like your inventory can pop up
[00:18:13.360 --> 00:18:14.600]   and other things can pop up.
[00:18:14.600 --> 00:18:17.080]   And that's actually like a research challenge
[00:18:17.080 --> 00:18:18.680]   of how to make use of all of this.
[00:18:18.680 --> 00:18:20.960]   And the other question is, like, which actions do we,
[00:18:20.960 --> 00:18:22.600]   what's the action space, right?
[00:18:22.600 --> 00:18:24.520]   Like the human can also just play,
[00:18:24.520 --> 00:18:27.120]   press capital S in NetEck and save the game and exit.
[00:18:27.120 --> 00:18:28.240]   And we don't actually want our agents
[00:18:28.240 --> 00:18:29.080]   to be able to do that, right?
[00:18:29.080 --> 00:18:30.840]   So you cannot like give it like access
[00:18:30.840 --> 00:18:32.800]   to all like the full keyboard as it were.
[00:18:32.800 --> 00:18:34.000]   And typically what we do is like,
[00:18:34.000 --> 00:18:36.800]   we restrict the action set and yeah.
[00:18:36.800 --> 00:18:39.160]   - Will the agent know its own inventory?
[00:18:39.160 --> 00:18:41.800]   Like could it pick up some food and eat it later?
[00:18:41.800 --> 00:18:46.080]   - Kind of, yes, but we don't have like a full solution
[00:18:46.080 --> 00:18:48.520]   for that yet because we would need to feed in that
[00:18:48.520 --> 00:18:49.800]   as a constant observation.
[00:18:49.800 --> 00:18:51.800]   And we don't do that presently.
[00:18:51.800 --> 00:18:54.120]   It is hard to exclude the agent from doing this
[00:18:54.120 --> 00:18:56.800]   because different keys like K, J,
[00:18:56.800 --> 00:18:58.440]   like different keys on the keyboard mean different things
[00:18:58.440 --> 00:18:59.680]   in different situations in the game.
[00:18:59.680 --> 00:19:02.400]   And in some situations, if you enable the eat action,
[00:19:02.400 --> 00:19:04.040]   then you can eat some stuff, right?
[00:19:04.040 --> 00:19:06.160]   But maybe only those keys that you already enabled
[00:19:06.160 --> 00:19:07.000]   for the game.
[00:19:07.000 --> 00:19:09.520]   But it gets a little bit technical technically right now,
[00:19:09.520 --> 00:19:12.240]   but our agents can eat certain things in the inventory,
[00:19:12.240 --> 00:19:13.680]   like if it has the right letter,
[00:19:13.680 --> 00:19:15.240]   but not other things, for instance.
[00:19:15.240 --> 00:19:17.880]   - Yeah, also I think maybe it's worth emphasizing
[00:19:17.880 --> 00:19:20.600]   that right now we've been spending most of our time
[00:19:20.600 --> 00:19:22.840]   just building that NetHack learning environment
[00:19:22.840 --> 00:19:24.720]   where you actually do have, if you want,
[00:19:24.720 --> 00:19:27.160]   you have access to the inventory observation.
[00:19:27.160 --> 00:19:29.440]   You can, if you want, use the entire keyboard
[00:19:29.440 --> 00:19:30.560]   as your action space.
[00:19:30.560 --> 00:19:32.800]   So that's out there for everybody
[00:19:32.800 --> 00:19:34.280]   if they want to use, right?
[00:19:34.280 --> 00:19:36.040]   We hope that lots of researchers pick this up
[00:19:36.040 --> 00:19:38.400]   and come up with all kinds of interesting solutions
[00:19:38.400 --> 00:19:40.240]   that make progress on NetHack.
[00:19:40.240 --> 00:19:41.480]   And then on top of that,
[00:19:41.480 --> 00:19:44.440]   we have this really basic agent implementation
[00:19:44.440 --> 00:19:47.080]   that we mentioned here released with that as well,
[00:19:47.080 --> 00:19:48.840]   so that people can piggyback on.
[00:19:48.840 --> 00:19:51.680]   But obviously there are lots of open research questions
[00:19:51.680 --> 00:19:54.760]   of like how to make best use of all these observations
[00:19:54.760 --> 00:19:56.400]   that come from different modalities,
[00:19:56.400 --> 00:19:59.840]   as well as really deal with this really large action space.
[00:19:59.840 --> 00:20:02.000]   One thing that I find super exciting
[00:20:02.000 --> 00:20:03.800]   is the fact that we as humans, right,
[00:20:03.800 --> 00:20:06.120]   we have all kinds of prior knowledge.
[00:20:06.120 --> 00:20:07.240]   Like when you play NetHack,
[00:20:07.240 --> 00:20:09.240]   although you've never heard about that game,
[00:20:09.240 --> 00:20:11.560]   and you bump into a door and you have, let's say,
[00:20:11.560 --> 00:20:13.720]   170 actions that you could apply,
[00:20:13.720 --> 00:20:16.480]   like trying to drink the door, right?
[00:20:16.480 --> 00:20:18.080]   Or trying to sit on the door.
[00:20:18.080 --> 00:20:19.440]   You just don't do that, right?
[00:20:19.440 --> 00:20:20.720]   You won't even try this out.
[00:20:20.720 --> 00:20:22.120]   You know, okay, I can try to open this
[00:20:22.120 --> 00:20:24.200]   maybe if I have a key, or if I don't,
[00:20:24.200 --> 00:20:25.720]   well, oh, there's also this kick action.
[00:20:25.720 --> 00:20:27.600]   So maybe let me try to kick in the door.
[00:20:27.600 --> 00:20:31.080]   So this fact that we as humans are so amazing
[00:20:31.080 --> 00:20:34.520]   at using our prior knowledge, our world knowledge,
[00:20:34.520 --> 00:20:36.120]   our common sense knowledge,
[00:20:36.120 --> 00:20:39.200]   to then really efficiently explore in these environments
[00:20:39.200 --> 00:20:40.800]   is absolutely fascinating to me.
[00:20:40.800 --> 00:20:43.040]   And that's why I also really like NetHack
[00:20:43.040 --> 00:20:45.560]   as a testbed for artificial intelligence,
[00:20:45.560 --> 00:20:47.200]   because I think ultimately we should have agents
[00:20:47.200 --> 00:20:51.160]   that are capable of transferring such domain knowledge
[00:20:51.160 --> 00:20:54.120]   from other sources to then be really efficient
[00:20:54.120 --> 00:20:56.960]   in these hard assimilated environments.
[00:20:56.960 --> 00:20:59.400]   - Like in the, there's a concept in NetHack community
[00:20:59.400 --> 00:21:01.640]   called source diving, where you look at the C code
[00:21:01.640 --> 00:21:04.200]   of NetHack and try to figure out how the game dynamics work.
[00:21:04.200 --> 00:21:06.280]   And ideally our agent should be able to do that, right?
[00:21:06.280 --> 00:21:07.640]   Our agent should look at the source code
[00:21:07.640 --> 00:21:10.520]   and be able to figure out how this game will behave
[00:21:10.520 --> 00:21:12.240]   like generally in certain actions,
[00:21:12.240 --> 00:21:13.320]   and then just do the right thing.
[00:21:13.320 --> 00:21:17.160]   That would be like the perfect research agenda for NetHack.
[00:21:17.160 --> 00:21:18.640]   - I feel like on top of that,
[00:21:18.640 --> 00:21:22.040]   there's this really amazing community created
[00:21:22.040 --> 00:21:24.640]   natural language research, which is the NetHack Wiki.
[00:21:24.640 --> 00:21:29.240]   So almost everybody I know who learned to play NetHack
[00:21:29.240 --> 00:21:32.400]   learned that by also looking up things on the NetHack Wiki.
[00:21:32.400 --> 00:21:34.400]   As you mentioned, right, you started playing NetHack
[00:21:34.400 --> 00:21:36.760]   when you didn't have any internet connection.
[00:21:36.760 --> 00:21:38.920]   So you couldn't look at any of these kind of spoilers.
[00:21:38.920 --> 00:21:40.600]   That makes it almost impossible, I think,
[00:21:40.600 --> 00:21:42.160]   to make progress in NetHack.
[00:21:42.160 --> 00:21:44.360]   And even with the NetHack Wiki, it's really hard.
[00:21:44.360 --> 00:21:46.960]   So people sometimes play NetHack for 20 years
[00:21:46.960 --> 00:21:48.600]   before they first win the game.
[00:21:48.600 --> 00:21:50.320]   But this kind of resource is amazing.
[00:21:50.320 --> 00:21:54.280]   It's like 3000 Wikipedia pages of explaining
[00:21:54.280 --> 00:21:58.080]   like how certain entities, items, monsters work.
[00:21:58.080 --> 00:22:01.520]   And I think one direction that's really exciting to me,
[00:22:01.520 --> 00:22:02.880]   and that's not really very different
[00:22:02.880 --> 00:22:04.080]   from what Heiner just described
[00:22:04.080 --> 00:22:06.160]   by like directly looking at the source code,
[00:22:06.160 --> 00:22:10.040]   but what if we had agents capable of encoding information
[00:22:10.040 --> 00:22:13.600]   in the NetHack Wiki and using that to, for instance,
[00:22:13.600 --> 00:22:15.160]   explore more efficiently
[00:22:15.160 --> 00:22:18.720]   or avoid certain really stupid deaths.
[00:22:18.720 --> 00:22:21.120]   And yeah, just generally, you know,
[00:22:21.120 --> 00:22:23.000]   using that prior domain knowledge
[00:22:23.000 --> 00:22:24.440]   to be much more sample efficient
[00:22:24.440 --> 00:22:27.200]   and generalize better in NetHack.
[00:22:27.200 --> 00:22:28.080]   - It's funny, it's like, actually,
[00:22:28.080 --> 00:22:29.760]   I think it's a kind of a different game.
[00:22:29.760 --> 00:22:31.800]   Like I, you know, in prep for this interview,
[00:22:31.800 --> 00:22:34.240]   I started playing NetHack a little bit again,
[00:22:34.240 --> 00:22:36.200]   and I kind of couldn't believe
[00:22:36.200 --> 00:22:38.600]   that I tolerated this game without the internet.
[00:22:38.600 --> 00:22:40.320]   Like it's just such a frustrating game
[00:22:40.320 --> 00:22:42.480]   with like such little guidance.
[00:22:42.480 --> 00:22:45.040]   And I was reading your paper on reinforcement learning,
[00:22:45.040 --> 00:22:46.400]   where you're talking about building a system
[00:22:46.400 --> 00:22:49.000]   to optimize for learning or optimize,
[00:22:49.000 --> 00:22:49.840]   I forget how you put it,
[00:22:49.840 --> 00:22:52.640]   but like sort of optimize for modifying the state space
[00:22:52.640 --> 00:22:53.880]   of the algorithm.
[00:22:53.880 --> 00:22:55.720]   And then I was thinking of my daughter,
[00:22:55.720 --> 00:22:56.800]   who's clearly doing that.
[00:22:56.800 --> 00:22:57.920]   So she's like nine months old
[00:22:57.920 --> 00:22:59.280]   and I've just been watching her a lot.
[00:22:59.280 --> 00:23:02.280]   And she clearly explores her environment
[00:23:02.280 --> 00:23:03.800]   in a way that she's just totally focused
[00:23:03.800 --> 00:23:04.720]   on whatever is novel.
[00:23:04.720 --> 00:23:06.480]   And there's no question that that's like,
[00:23:06.480 --> 00:23:08.560]   she's completely wired to like, you know,
[00:23:08.560 --> 00:23:10.200]   if I show her a new toy, she loses it
[00:23:10.200 --> 00:23:12.040]   or anything that seems to defy her,
[00:23:12.040 --> 00:23:13.800]   like belief about the laws of physics,
[00:23:13.800 --> 00:23:15.240]   like blows her mind.
[00:23:15.240 --> 00:23:16.640]   So clearly she's doing that.
[00:23:16.640 --> 00:23:18.040]   And then I was wondering if, you know,
[00:23:18.040 --> 00:23:19.440]   maybe myself as a child,
[00:23:19.440 --> 00:23:21.000]   I was kind of more willing
[00:23:21.000 --> 00:23:24.760]   or kind of more enjoyed the exploratory modes necessary
[00:23:24.760 --> 00:23:27.280]   for figuring out NetHack.
[00:23:27.280 --> 00:23:30.640]   - Yeah, so that's a perfect remark.
[00:23:30.640 --> 00:23:33.360]   So in fact, some of the research that we're doing
[00:23:33.360 --> 00:23:36.080]   is really centered around how can we design agents
[00:23:36.080 --> 00:23:39.320]   that are intrinsically motivated to learn in an environment?
[00:23:39.320 --> 00:23:40.880]   Because again, in NetHack,
[00:23:40.880 --> 00:23:42.960]   like any reward function that we come up with,
[00:23:42.960 --> 00:23:44.400]   it's not going to be great, right?
[00:23:44.400 --> 00:23:46.080]   The actual thing we want to optimize for
[00:23:46.080 --> 00:23:46.960]   is solving the game.
[00:23:46.960 --> 00:23:48.720]   And there's just not any reward function,
[00:23:48.720 --> 00:23:51.160]   I think that really can guide an agent's
[00:23:51.160 --> 00:23:53.040]   that first step towards that.
[00:23:53.040 --> 00:23:54.520]   And I have two daughters
[00:23:54.520 --> 00:23:57.280]   and in fact, my youngest daughter as well,
[00:23:57.280 --> 00:23:59.720]   at some point was playing with a toy kitchen
[00:23:59.720 --> 00:24:01.720]   and she was just opening and closing the door, right?
[00:24:01.720 --> 00:24:03.200]   Opening, closing the door.
[00:24:03.200 --> 00:24:04.960]   Until at some point she even
[00:24:04.960 --> 00:24:06.360]   squeezed her finger in the door.
[00:24:06.360 --> 00:24:09.560]   She was crying, it was clearly something really bad, right?
[00:24:09.560 --> 00:24:10.960]   She was actually in pain.
[00:24:10.960 --> 00:24:12.440]   She was crying for a minute
[00:24:12.440 --> 00:24:14.880]   and then she was closing, opening the door
[00:24:14.880 --> 00:24:16.120]   until it became boring.
[00:24:16.120 --> 00:24:19.080]   So this fact that we as humans
[00:24:19.080 --> 00:24:21.480]   are just setting ourselves goals
[00:24:21.480 --> 00:24:23.480]   when we are in an environment, right?
[00:24:23.480 --> 00:24:26.200]   We get bored and then we think of,
[00:24:26.200 --> 00:24:28.600]   "Oh, what happens if we try this or that?"
[00:24:28.600 --> 00:24:30.640]   And then we see, can we actually control this?
[00:24:30.640 --> 00:24:35.120]   Are we empowered to have control over what we want to do?
[00:24:35.120 --> 00:24:36.720]   Are we able to actually predict
[00:24:36.720 --> 00:24:38.080]   what's going to happen next?
[00:24:38.080 --> 00:24:41.200]   And if not, right, then maybe that's really interesting
[00:24:41.200 --> 00:24:42.280]   or maybe it's noise, right?
[00:24:42.280 --> 00:24:45.000]   Maybe it's just the environment being completely stochastic
[00:24:45.000 --> 00:24:47.320]   and there's just nothing I can control.
[00:24:47.320 --> 00:24:49.920]   So how do we design agents that can do this as well?
[00:24:49.920 --> 00:24:52.920]   I think that's a question that's super exciting to me
[00:24:52.920 --> 00:24:54.560]   specifically in the context of NetHack
[00:24:54.560 --> 00:24:56.520]   because it has this stochasticity,
[00:24:56.520 --> 00:25:00.520]   it has this humongous, I guess, internal mechanism
[00:25:00.520 --> 00:25:02.360]   that governs the state transition.
[00:25:02.360 --> 00:25:03.880]   So I think this will lead to lots
[00:25:03.880 --> 00:25:05.600]   of quite interesting research.
[00:25:05.600 --> 00:25:08.120]   - In a sense, NetHack is really like a hard case there,
[00:25:08.120 --> 00:25:08.960]   right?
[00:25:08.960 --> 00:25:11.920]   There's almost no human who plays NetHack unspoiled.
[00:25:11.920 --> 00:25:14.400]   I mean, typically people don't have a good reason to do that
[00:25:14.400 --> 00:25:16.600]   because they need to find out about NetHack first,
[00:25:16.600 --> 00:25:18.440]   but a few people really that weren't,
[00:25:18.440 --> 00:25:19.680]   for certain situations,
[00:25:19.720 --> 00:25:23.320]   were in the situation to try NetHack without any spoilers.
[00:25:23.320 --> 00:25:25.360]   And it takes decades, right?
[00:25:25.360 --> 00:25:27.520]   You die so many deaths and you don't even know what to do.
[00:25:27.520 --> 00:25:29.680]   Like, you don't even know what the exact goal of the game is.
[00:25:29.680 --> 00:25:31.320]   The game that kind of tells you,
[00:25:31.320 --> 00:25:33.560]   if you read enough oracles,
[00:25:33.560 --> 00:25:34.920]   but there's also like,
[00:25:34.920 --> 00:25:36.280]   there's a thing called rumors in the game
[00:25:36.280 --> 00:25:38.320]   where you can read up what you're supposed to do,
[00:25:38.320 --> 00:25:39.440]   but there's also wrong ones.
[00:25:39.440 --> 00:25:41.080]   And if you're unlucky, you get the wrong ones
[00:25:41.080 --> 00:25:42.080]   and it misleads you.
[00:25:42.080 --> 00:25:44.320]   So there's almost no way to find out
[00:25:44.320 --> 00:25:45.480]   how to even beat the game,
[00:25:45.480 --> 00:25:47.760]   let alone get around all the obstacles
[00:25:47.760 --> 00:25:48.920]   if you don't spoil yourself.
[00:25:48.920 --> 00:25:51.680]   And we would like our computers to do that.
[00:25:51.680 --> 00:25:53.520]   But I want to mention another thing that Kim was saying,
[00:25:53.520 --> 00:25:56.040]   there's no reward that leads you to beating the game.
[00:25:56.040 --> 00:25:57.000]   That is true.
[00:25:57.000 --> 00:25:59.240]   But what there is, is recorded games
[00:25:59.240 --> 00:26:00.640]   and say, the NetHack community,
[00:26:00.640 --> 00:26:01.960]   or people can just,
[00:26:01.960 --> 00:26:03.320]   we could just look at what humans do
[00:26:03.320 --> 00:26:05.680]   and try to imitate this, right?
[00:26:05.680 --> 00:26:07.200]   Have all of us play NetHack,
[00:26:07.200 --> 00:26:08.920]   which we do in our lab a lot.
[00:26:08.920 --> 00:26:12.880]   And then try to join an agent that predicts human actions
[00:26:12.880 --> 00:26:13.880]   and then go from there.
[00:26:13.880 --> 00:26:15.040]   That might be one option.
[00:26:15.040 --> 00:26:16.240]   - I was going to ask you about that actually,
[00:26:16.240 --> 00:26:17.720]   because I remember the first version
[00:26:17.720 --> 00:26:20.120]   of the successful Go algorithm
[00:26:20.120 --> 00:26:21.800]   was trained on expert games.
[00:26:21.800 --> 00:26:24.040]   Have you tried to train an algorithm?
[00:26:24.040 --> 00:26:27.520]   I mean, I guess even on an amateur NetHack player
[00:26:27.520 --> 00:26:29.720]   would probably, you could imagine that helps
[00:26:29.720 --> 00:26:31.800]   the algorithm learn some strategy, right?
[00:26:31.800 --> 00:26:34.280]   - So we're definitely thinking about doing that.
[00:26:34.280 --> 00:26:35.640]   The problem is getting the data, right?
[00:26:35.640 --> 00:26:37.280]   Just getting a few games isn't enough
[00:26:37.280 --> 00:26:38.640]   for the methods that we have.
[00:26:38.640 --> 00:26:40.760]   We need enormous amounts of data
[00:26:40.760 --> 00:26:42.480]   and there's no easy way to produce it
[00:26:42.480 --> 00:26:45.280]   unless we pay someone to play NetHack all day.
[00:26:45.280 --> 00:26:47.280]   And even then you have to play for a long time.
[00:26:47.280 --> 00:26:48.480]   Now, interestingly, the NetHack community
[00:26:48.480 --> 00:26:50.680]   actually does have recorded games, say Alt.org,
[00:26:50.680 --> 00:26:52.400]   but unfortunately they basically only record
[00:26:52.400 --> 00:26:53.240]   the outcome of the game,
[00:26:53.240 --> 00:26:55.600]   like a video stream that the game shows.
[00:26:55.600 --> 00:26:56.920]   They don't record the actions
[00:26:56.920 --> 00:26:58.280]   that were put in by the players.
[00:26:58.280 --> 00:27:00.000]   And that's a research question by itself, right?
[00:27:00.000 --> 00:27:02.080]   How to make use of this kind of data.
[00:27:02.080 --> 00:27:03.640]   But yeah, we have certainly something
[00:27:03.640 --> 00:27:05.280]   that we are thinking about.
[00:27:05.280 --> 00:27:07.520]   - Has anyone built a kind of a rule-based system
[00:27:07.520 --> 00:27:08.360]   that can beat NetHack?
[00:27:08.360 --> 00:27:11.080]   That seems like something someone would try at some point.
[00:27:11.080 --> 00:27:12.440]   - People tried, but I don't think
[00:27:12.440 --> 00:27:14.160]   they were super successful.
[00:27:14.160 --> 00:27:16.160]   And I'm not sure what the, I think there was one system
[00:27:16.160 --> 00:27:18.840]   that like maybe ascended in 10% of cases.
[00:27:18.840 --> 00:27:21.240]   Maybe Tim has like the details on that.
[00:27:21.240 --> 00:27:22.800]   - Yeah, so if I vaguely remember,
[00:27:22.800 --> 00:27:25.280]   so there are cases of hard-coded bots
[00:27:25.280 --> 00:27:27.280]   that ascended prior versions of NetHack,
[00:27:27.280 --> 00:27:29.520]   where they, as far as I remember,
[00:27:29.520 --> 00:27:32.240]   they use certain exploits in the game.
[00:27:32.240 --> 00:27:34.360]   There's something called putting farming,
[00:27:34.360 --> 00:27:37.120]   where you can, I think, get a lot of items or whatnot,
[00:27:37.120 --> 00:27:39.520]   and then it makes the game much easier.
[00:27:39.520 --> 00:27:43.400]   But these exploits, they are not in there anymore
[00:27:43.400 --> 00:27:44.960]   in the most current versions of NetHack.
[00:27:44.960 --> 00:27:47.520]   So all of these bots that have been handcrafted
[00:27:47.520 --> 00:27:50.080]   some time ago, they won't work right now.
[00:27:50.080 --> 00:27:53.160]   Also, I think ideally we want to have systems
[00:27:53.160 --> 00:27:56.480]   that are able to ascend, meaning win the game
[00:27:56.480 --> 00:28:00.040]   with all kinds of character combinations, right?
[00:28:00.040 --> 00:28:02.040]   I mean, you have different roles in NetHack,
[00:28:02.040 --> 00:28:05.400]   races and gender and whatnot, right?
[00:28:05.400 --> 00:28:07.760]   So like these bots, as far as I remember,
[00:28:07.760 --> 00:28:09.280]   we're always quite specialized
[00:28:09.280 --> 00:28:11.960]   for one specific role in NetHack.
[00:28:11.960 --> 00:28:14.760]   But ideally we want to have agents similar to humans
[00:28:14.760 --> 00:28:16.680]   that can, in fact, win the game
[00:28:16.680 --> 00:28:19.440]   with all kinds of starting conditions.
[00:28:19.440 --> 00:28:22.080]   - So could you maybe describe your paper
[00:28:22.080 --> 00:28:24.480]   that I sort of alluded to in a little more detail?
[00:28:24.480 --> 00:28:26.120]   I think it was an ICML paper, right?
[00:28:26.120 --> 00:28:28.000]   That on the exploration
[00:28:28.000 --> 00:28:29.560]   and reinforcement learning strategies,
[00:28:29.560 --> 00:28:32.200]   and then maybe sort of say what the results were.
[00:28:32.200 --> 00:28:34.320]   - I guess you're referring to the iClear paper.
[00:28:34.320 --> 00:28:35.160]   - Oh, iClear, sorry.
[00:28:35.160 --> 00:28:36.280]   - Yeah, anyways.
[00:28:36.280 --> 00:28:38.480]   So yeah, so I think that, so first of all,
[00:28:38.480 --> 00:28:41.960]   that was a paper that was not done on NetHack.
[00:28:41.960 --> 00:28:45.200]   So that was at a time where the NetHack learning environment
[00:28:45.200 --> 00:28:46.200]   didn't exist yet.
[00:28:46.200 --> 00:28:48.960]   This is a paper done by Roberta Rellino.
[00:28:48.960 --> 00:28:51.960]   She's a PhD student at New York University,
[00:28:51.960 --> 00:28:53.240]   and she was interning with us
[00:28:53.240 --> 00:28:55.120]   at Facebook Research in London.
[00:28:55.120 --> 00:28:57.800]   And she has been done a really good job
[00:28:57.800 --> 00:29:00.000]   at investigating the current limits
[00:29:00.000 --> 00:29:02.320]   of these intrinsic motivation mechanisms
[00:29:02.320 --> 00:29:03.440]   for reinforcement learning.
[00:29:03.440 --> 00:29:05.840]   So maybe just to give a bit more context,
[00:29:05.840 --> 00:29:08.400]   one really open challenge in reinforcement learning
[00:29:08.400 --> 00:29:10.280]   is how do you learn in environments
[00:29:10.280 --> 00:29:13.120]   where your reward that you get from the environment
[00:29:13.120 --> 00:29:14.680]   is extremely sparse, right?
[00:29:14.680 --> 00:29:16.600]   So reinforcement learning works amazingly
[00:29:16.600 --> 00:29:18.360]   if you get a very dense reward function.
[00:29:18.360 --> 00:29:21.960]   So that means in many steps in the episode,
[00:29:21.960 --> 00:29:23.360]   you actually get a reward from the environment,
[00:29:23.360 --> 00:29:25.800]   but if your reward only comes at the very end
[00:29:25.800 --> 00:29:27.080]   and your episode is quite long,
[00:29:27.080 --> 00:29:29.040]   then it's really hard to learn from that.
[00:29:29.040 --> 00:29:31.680]   And what people have been doing in the past,
[00:29:31.680 --> 00:29:33.040]   developing all kinds of mechanisms
[00:29:33.040 --> 00:29:35.200]   that provide the agent with reward
[00:29:35.200 --> 00:29:37.160]   that's not given by the environment,
[00:29:37.160 --> 00:29:41.320]   but that is basically given to the agent intrinsically.
[00:29:41.320 --> 00:29:44.640]   And one such thing could be how well is the agent
[00:29:44.640 --> 00:29:47.840]   predicting the next time step given the current action?
[00:29:47.840 --> 00:29:48.680]   Right?
[00:29:48.680 --> 00:29:49.520]   So that you could use that, right?
[00:29:49.520 --> 00:29:52.120]   You could use, if your agent makes a big prediction error
[00:29:52.120 --> 00:29:55.080]   in terms of given the current state and the next action,
[00:29:55.080 --> 00:29:56.640]   what the next state is going to be,
[00:29:56.640 --> 00:29:58.280]   then we are rewarding the agent.
[00:29:58.280 --> 00:30:01.640]   The problem with that is that there's this noisy TV problem
[00:30:01.640 --> 00:30:03.000]   where in your environment,
[00:30:03.000 --> 00:30:05.200]   there's some source of stochasticity.
[00:30:05.200 --> 00:30:08.320]   Let's say a television that just shows white noise, right?
[00:30:08.320 --> 00:30:12.400]   So every prediction that you make as an agent will be wrong
[00:30:12.400 --> 00:30:14.240]   because you can't predict what's going to be
[00:30:14.240 --> 00:30:15.840]   on the next screen.
[00:30:15.840 --> 00:30:19.240]   So you just reward the agent continuously for that.
[00:30:19.240 --> 00:30:21.160]   And that means that kind of noisy TV
[00:30:21.160 --> 00:30:22.640]   becomes an attractor to the agent.
[00:30:22.640 --> 00:30:26.000]   So the agent will just stand in front of the noisy TV all day
[00:30:26.000 --> 00:30:28.080]   without actually exploring the environment.
[00:30:28.080 --> 00:30:32.160]   So what Roberta was doing is she was building on top of work
[00:30:32.160 --> 00:30:36.160]   that is trying to predict or calculating intrinsic reward
[00:30:36.160 --> 00:30:37.840]   based on the forward model,
[00:30:37.840 --> 00:30:38.800]   trying to predict the next state,
[00:30:38.800 --> 00:30:41.720]   but also given the representation of the next state
[00:30:41.720 --> 00:30:43.320]   and the representation of the current state,
[00:30:43.320 --> 00:30:45.440]   trying to pick the action that led to that next state.
[00:30:45.440 --> 00:30:46.920]   So that's an inverse model.
[00:30:46.920 --> 00:30:50.920]   And what she basically figured out is how can we make sure
[00:30:50.920 --> 00:30:54.760]   that the agent's internal representation of the state
[00:30:54.760 --> 00:30:57.920]   is only encoding what the agent can actually control
[00:30:57.920 --> 00:30:59.000]   in the environment.
[00:30:59.000 --> 00:31:01.560]   So if there's a noisy TV and the agent over time learns
[00:31:01.560 --> 00:31:04.320]   that its actions don't have any effect on the TV
[00:31:04.320 --> 00:31:05.440]   and the noisy TV,
[00:31:05.440 --> 00:31:08.960]   then it would just ignore that source of stochasticity
[00:31:08.960 --> 00:31:11.400]   in terms of, or with regards to providing
[00:31:11.400 --> 00:31:13.160]   intrinsic motivation to the agent.
[00:31:13.160 --> 00:31:15.840]   And that led to at the time state of the art results
[00:31:15.840 --> 00:31:19.040]   on quite hard exploration problems in mini-grid.
[00:31:19.040 --> 00:31:21.360]   Mini-grid again, being a grid world,
[00:31:21.360 --> 00:31:24.800]   a bit like NAHAG, but just million orders
[00:31:24.800 --> 00:31:26.680]   of magnitude simpler,
[00:31:26.680 --> 00:31:28.560]   but still really hard for contemporary
[00:31:28.560 --> 00:31:30.360]   and possible learning approaches.
[00:31:30.360 --> 00:31:31.560]   So that was that paper.
[00:31:31.560 --> 00:31:32.960]   - I'm not super familiar with the literature.
[00:31:32.960 --> 00:31:34.400]   So let me see if I understood it.
[00:31:34.400 --> 00:31:37.480]   Maybe I'll channel the audience here.
[00:31:37.480 --> 00:31:40.400]   So it sounds like there's sort of a standard pattern
[00:31:40.400 --> 00:31:43.160]   of trying to actually go to environments
[00:31:43.160 --> 00:31:46.120]   where you can't predict what the next thing will happen.
[00:31:46.120 --> 00:31:48.080]   And I thought you were gonna say,
[00:31:48.080 --> 00:31:49.880]   it's like wanting to optimize
[00:31:49.880 --> 00:31:52.520]   for being able to predict the next step,
[00:31:52.520 --> 00:31:55.000]   but that's showing my supervised learning bias, right?
[00:31:55.000 --> 00:31:56.320]   Where you would probably wanna optimize
[00:31:56.320 --> 00:31:58.040]   for good predictions, but you're actually like
[00:31:58.040 --> 00:31:59.360]   kind of trying to go to places
[00:31:59.360 --> 00:32:01.000]   where you can't predict the next step,
[00:32:01.000 --> 00:32:04.000]   which makes sense 'cause that more learning would happen.
[00:32:04.000 --> 00:32:06.640]   - Yeah, so I mean, you should be basically,
[00:32:06.640 --> 00:32:10.160]   hope I get this right because it has been some time ago,
[00:32:10.160 --> 00:32:14.800]   but basically, you should be rewarding yourself
[00:32:14.800 --> 00:32:18.920]   if you find novel mechanisms in the environment
[00:32:18.920 --> 00:32:23.920]   that you can control, but you shouldn't be rewarding yourself
[00:32:23.920 --> 00:32:27.240]   for novel observations in the environment
[00:32:27.240 --> 00:32:28.760]   that you can't control, right?
[00:32:28.760 --> 00:32:31.080]   Because if there's a noisy TV,
[00:32:31.080 --> 00:32:32.360]   you shouldn't be caring about that.
[00:32:32.360 --> 00:32:34.280]   Otherwise you'll be standing in front of that TV
[00:32:34.280 --> 00:32:36.320]   for eternity.
[00:32:36.320 --> 00:32:37.160]   But yeah, you're right.
[00:32:37.160 --> 00:32:38.360]   There's always this kind of tension
[00:32:38.360 --> 00:32:42.040]   between learning the agent to get better
[00:32:42.040 --> 00:32:43.920]   at doing whatever it's doing in the environment.
[00:32:43.920 --> 00:32:47.680]   So that will also lead to better forward predictions,
[00:32:47.680 --> 00:32:49.720]   but at the same time, also rewarding the agent
[00:32:49.720 --> 00:32:53.080]   whenever it encounters a mechanism that it can control,
[00:32:53.080 --> 00:32:55.600]   but that also leads to novel observations.
[00:32:55.600 --> 00:32:58.840]   Now, the problem is that another common approach
[00:32:58.840 --> 00:33:02.720]   is to actually count how often the agent
[00:33:02.720 --> 00:33:04.800]   observes a specific state.
[00:33:04.800 --> 00:33:07.400]   And that has been doing really well in,
[00:33:07.400 --> 00:33:09.120]   for instance, in these Atari games,
[00:33:09.120 --> 00:33:10.920]   where every time you play the game, it's the same,
[00:33:10.920 --> 00:33:13.400]   but in procedurally generated games like NetHack,
[00:33:13.400 --> 00:33:14.240]   that won't work, right?
[00:33:14.240 --> 00:33:18.440]   Because it's just so incredibly unlikely
[00:33:18.440 --> 00:33:20.400]   that you will ever see the same state twice,
[00:33:20.400 --> 00:33:23.040]   that counting them doesn't make any sense if you do.
[00:33:24.880 --> 00:33:26.440]   So basically, if you have a noisy TV
[00:33:26.440 --> 00:33:27.520]   and you can change a channel,
[00:33:27.520 --> 00:33:30.400]   we don't really know what to do with it yet.
[00:33:30.400 --> 00:33:31.240]   - Right.
[00:33:31.240 --> 00:33:32.080]   Do you?
[00:33:32.080 --> 00:33:36.560]   - And honestly, that's how humans behave as well.
[00:33:36.560 --> 00:33:38.720]   So I think we're pretty close to AGI there.
[00:33:38.720 --> 00:33:41.560]   - Yeah, no, I mean, it's funny.
[00:33:41.560 --> 00:33:43.360]   I mean, you alluded to this a little bit in the paper,
[00:33:43.360 --> 00:33:45.160]   but I was thinking,
[00:33:45.160 --> 00:33:48.080]   some of the most joy I've felt is in NetHack.
[00:33:48.080 --> 00:33:50.360]   I remember when you realized that,
[00:33:50.360 --> 00:33:52.320]   you have a throw option
[00:33:52.320 --> 00:33:54.440]   and mostly use that to throw weapons,
[00:33:54.440 --> 00:33:56.120]   and it kind of guides you in that way,
[00:33:56.120 --> 00:33:58.440]   but you can actually throw food at animals
[00:33:58.440 --> 00:33:59.440]   and turn them into pets.
[00:33:59.440 --> 00:34:01.960]   And it's like this incredible joy
[00:34:01.960 --> 00:34:04.760]   of realizing this surprising thing that you can do.
[00:34:04.760 --> 00:34:07.240]   Clearly there's a reward function, at least in my brain,
[00:34:07.240 --> 00:34:09.360]   of kind of discovering something new.
[00:34:09.360 --> 00:34:10.200]   And in your paper,
[00:34:10.200 --> 00:34:12.480]   you kind of allude to some of this coming
[00:34:12.480 --> 00:34:16.280]   from education literature or early psychology literature.
[00:34:16.280 --> 00:34:17.280]   Did you look at any of that
[00:34:17.280 --> 00:34:19.360]   when you were thinking about this?
[00:34:19.360 --> 00:34:22.640]   - I mean, we have a paper together with Josh Tenenbaum,
[00:34:22.640 --> 00:34:26.880]   who is I think really leading in that area at MIT.
[00:34:26.880 --> 00:34:30.920]   I have to say, I'm not very familiar with that literature.
[00:34:30.920 --> 00:34:33.080]   I mean, that's the honest answer to that.
[00:34:33.080 --> 00:34:35.000]   But I think the thing that you just mentioned, right,
[00:34:35.000 --> 00:34:37.520]   in terms of, oh, you know that you can throw
[00:34:37.520 --> 00:34:39.480]   not just weapons, but you, as a human,
[00:34:39.480 --> 00:34:41.080]   you know you can also throw food around, right?
[00:34:41.080 --> 00:34:42.840]   You can throw anything basically around.
[00:34:42.840 --> 00:34:45.520]   And then realizing that actually in NetHack,
[00:34:45.520 --> 00:34:47.600]   the developers of NetHack, they thought of everything.
[00:34:47.600 --> 00:34:49.520]   Like you can actually throw food around.
[00:34:49.520 --> 00:34:51.400]   There was this revelation to me.
[00:34:51.400 --> 00:34:52.240]   I mean, I have to say,
[00:34:52.240 --> 00:34:53.760]   I'm not an expert NetHack player
[00:34:53.760 --> 00:34:56.440]   and our entire team kind of is the only one
[00:34:56.440 --> 00:34:58.320]   who actually ascended in NetHack.
[00:34:58.320 --> 00:35:00.240]   But so I had this revelation the other day
[00:35:00.240 --> 00:35:01.160]   where I was playing NetHack
[00:35:01.160 --> 00:35:04.320]   and then I was always like encountering graves.
[00:35:04.320 --> 00:35:06.360]   And I was like, okay, you go over this grave
[00:35:06.360 --> 00:35:07.640]   and you get like some interesting message
[00:35:07.640 --> 00:35:10.320]   that's engraved on the stone, right?
[00:35:10.320 --> 00:35:11.480]   And okay, fine.
[00:35:11.480 --> 00:35:12.960]   But what do I actually do with graves?
[00:35:12.960 --> 00:35:16.000]   I mean, they didn't seem to be any use to it.
[00:35:16.000 --> 00:35:18.120]   And then the other day I thought at some point,
[00:35:18.120 --> 00:35:20.880]   like actually, you know, there are pickaxes in NetHack.
[00:35:20.880 --> 00:35:23.640]   What if I dig up whatever's lying in that grave?
[00:35:23.640 --> 00:35:25.840]   And you know, there's actually something in that grave.
[00:35:25.840 --> 00:35:27.120]   There's items, there might be,
[00:35:27.120 --> 00:35:28.400]   I mean, there's definitely a corpse,
[00:35:28.400 --> 00:35:29.640]   but there might also be items in there.
[00:35:29.640 --> 00:35:32.800]   And it was again, like for you, right?
[00:35:32.800 --> 00:35:35.160]   It was for me so interesting to see
[00:35:35.160 --> 00:35:37.440]   that my kind of prior knowledge about the world
[00:35:37.440 --> 00:35:38.760]   also applied within NetHack,
[00:35:38.760 --> 00:35:40.640]   although it's this kind of terminal based game.
[00:35:40.640 --> 00:35:42.600]   So that's again, why I believe NetHack
[00:35:42.600 --> 00:35:44.560]   is such an amazing resource
[00:35:44.560 --> 00:35:46.640]   for artificial intelligence research.
[00:35:46.640 --> 00:35:49.400]   - Okay, so we've probably driven away anyone
[00:35:49.400 --> 00:35:51.600]   with any kind of practical mindset,
[00:35:51.600 --> 00:35:53.720]   but this is supposed to be for people
[00:35:53.720 --> 00:35:56.560]   practicing machine learning for real world applications.
[00:35:56.560 --> 00:35:59.280]   I mean, where do you think reinforcement learning goes?
[00:35:59.280 --> 00:36:01.080]   Like, I feel like the knock on it right now
[00:36:01.080 --> 00:36:03.560]   is maybe that it's really just for these kind of
[00:36:03.560 --> 00:36:07.760]   toy environments like Atari games and Dota and NetHack.
[00:36:07.760 --> 00:36:10.760]   Like what, is it being used for things
[00:36:10.760 --> 00:36:11.880]   that we would experience now
[00:36:11.880 --> 00:36:14.560]   or is it on a path to being useful for things?
[00:36:14.560 --> 00:36:16.680]   Where do you think the applications are?
[00:36:16.680 --> 00:36:20.320]   - Yeah, so first of all, I think it's not necessarily fair
[00:36:20.320 --> 00:36:22.200]   to say that the kind of research that's done
[00:36:22.200 --> 00:36:23.400]   in simulated environments
[00:36:23.400 --> 00:36:25.680]   is not with real world applications in mind.
[00:36:25.680 --> 00:36:27.880]   So it's very funny in that, you know,
[00:36:27.880 --> 00:36:29.200]   NetHack is this really old game.
[00:36:29.200 --> 00:36:30.920]   So it feels like a step back, right?
[00:36:30.920 --> 00:36:34.600]   From more 3D visually appealing games like Dota.
[00:36:34.600 --> 00:36:36.560]   But in fact, as we, I guess, discussed now,
[00:36:36.560 --> 00:36:38.600]   like NetHack has a lot of properties
[00:36:38.600 --> 00:36:42.040]   of how you also would try to solve tasks in the real world.
[00:36:42.040 --> 00:36:44.240]   Right, if I try to fix my car engine,
[00:36:44.240 --> 00:36:45.480]   I have no idea how to do this.
[00:36:45.480 --> 00:36:48.320]   Maybe I can look up the information on Wikipedia.
[00:36:48.320 --> 00:36:49.800]   I mean, probably it's not going to work,
[00:36:49.800 --> 00:36:53.280]   but you know, we are so good at, you know,
[00:36:53.280 --> 00:36:55.160]   using world knowledge, common sense knowledge,
[00:36:55.160 --> 00:36:57.800]   and also acquiring a specific domain knowledge
[00:36:57.800 --> 00:36:59.400]   for solving tasks in the real world.
[00:36:59.400 --> 00:37:00.960]   So in some sense, I feel like NetHack
[00:37:00.960 --> 00:37:04.640]   is even a step forward towards actually making progress
[00:37:04.640 --> 00:37:06.280]   in real world tasks with reinforcement learning.
[00:37:06.280 --> 00:37:08.320]   Also given the fact that it's procedurally generated
[00:37:08.320 --> 00:37:10.600]   and every time the observation will look different.
[00:37:10.600 --> 00:37:11.880]   Similarly in the real world, right?
[00:37:11.880 --> 00:37:13.800]   It's like, again, a comp-based approach
[00:37:13.800 --> 00:37:14.920]   won't really help you that much
[00:37:14.920 --> 00:37:16.440]   because like, you know,
[00:37:16.440 --> 00:37:18.240]   the world will look different tomorrow.
[00:37:18.240 --> 00:37:19.560]   And at the same time,
[00:37:19.560 --> 00:37:21.680]   I think there's also more and more applications
[00:37:21.680 --> 00:37:23.280]   of reinforcement learning for the real world.
[00:37:23.280 --> 00:37:26.400]   So for instance, we published a workshop paper
[00:37:26.400 --> 00:37:28.280]   on using reinforcement learning
[00:37:28.280 --> 00:37:32.560]   for learning to control internet traffic.
[00:37:32.560 --> 00:37:34.640]   So there's these handcrafted heuristics
[00:37:34.640 --> 00:37:37.400]   that people have been developing for decades,
[00:37:37.400 --> 00:37:40.440]   TCP protocols and whatnot, right?
[00:37:40.440 --> 00:37:43.560]   That govern how I'm going to,
[00:37:43.560 --> 00:37:46.080]   sorry, congestion control window approaches, right?
[00:37:46.080 --> 00:37:49.520]   That go on how I can maximize my throughput
[00:37:49.520 --> 00:37:51.320]   in a internet network, right?
[00:37:51.320 --> 00:37:53.000]   How can I make sure that I can send
[00:37:53.000 --> 00:37:54.800]   as many packages as possible
[00:37:54.800 --> 00:37:56.560]   without losing too many packages
[00:37:56.560 --> 00:37:58.960]   because of congestion of the other participants
[00:37:58.960 --> 00:38:00.040]   in the internet network.
[00:38:00.040 --> 00:38:01.960]   And we are developing approaches
[00:38:01.960 --> 00:38:03.960]   that allow us to train reinforcement agents
[00:38:03.960 --> 00:38:06.680]   to automatically learn what's a good policy
[00:38:06.680 --> 00:38:07.720]   in terms of, you know,
[00:38:07.720 --> 00:38:10.760]   sending out how many packets per second
[00:38:10.760 --> 00:38:12.760]   so that, you know, they maximize bandwidth.
[00:38:12.760 --> 00:38:15.880]   So there are definitely more and more applications
[00:38:15.880 --> 00:38:17.520]   of reinforcement learning in real world,
[00:38:17.520 --> 00:38:20.240]   like also advertisement is, I think, an example.
[00:38:20.240 --> 00:38:23.920]   So I think we'll see much more of that in the future.
[00:38:23.920 --> 00:38:26.560]   - Yeah, I think like computer systems,
[00:38:26.560 --> 00:38:27.840]   like operating systems and so on,
[00:38:27.840 --> 00:38:30.120]   they have like all kinds of inbuilt heuristics
[00:38:30.120 --> 00:38:32.720]   that are often good, but perhaps not optimal.
[00:38:32.720 --> 00:38:34.160]   And reinforcement learning is one way
[00:38:34.160 --> 00:38:35.720]   to try to optimize these things.
[00:38:35.720 --> 00:38:37.480]   There's a lot, like if you look at the Linux kernel,
[00:38:37.480 --> 00:38:39.440]   by the way, looking at the NetX source code
[00:38:39.440 --> 00:38:41.640]   is a great gateway drug to becoming a kernel developer.
[00:38:41.640 --> 00:38:44.880]   It's basically like a mini Unix in there.
[00:38:44.880 --> 00:38:45.720]   But if you look at the Linux kernel,
[00:38:45.720 --> 00:38:48.520]   there's all kinds of heuristics and like constants
[00:38:48.520 --> 00:38:50.400]   and like wait times and so on.
[00:38:50.400 --> 00:38:52.680]   And potentially you could actually like,
[00:38:52.680 --> 00:38:53.880]   not just hard code these things,
[00:38:53.880 --> 00:38:55.280]   but learn them on the fly.
[00:38:55.280 --> 00:38:58.480]   Of course, you will have a complex system if you do that,
[00:38:58.480 --> 00:38:59.960]   and you may not want to do this at all times,
[00:38:59.960 --> 00:39:01.440]   but it's certainly an option.
[00:39:01.440 --> 00:39:03.920]   And I think this is where the world is going.
[00:39:03.920 --> 00:39:06.520]   I want to wait one more comment about NetX.
[00:39:06.520 --> 00:39:08.120]   We compared NetX to Go early on,
[00:39:08.120 --> 00:39:09.840]   but I think the comparison I like more
[00:39:09.840 --> 00:39:11.720]   is like StarCraft, right?
[00:39:11.720 --> 00:39:14.080]   So StarCraft 2 has famously been like a challenge.
[00:39:14.080 --> 00:39:15.240]   And of course it's a multiplayer game,
[00:39:15.240 --> 00:39:16.760]   so it's different in that sense.
[00:39:16.760 --> 00:39:19.120]   But many of the challenges that StarCraft has
[00:39:19.120 --> 00:39:21.600]   are also in NetX, like big observation space,
[00:39:21.600 --> 00:39:24.400]   complex environment dynamics, big action space,
[00:39:24.400 --> 00:39:26.360]   and all these things that are technically hard.
[00:39:26.360 --> 00:39:29.560]   But on top of that, to actually like solve StarCraft,
[00:39:29.560 --> 00:39:31.880]   you have to use like, you basically use up the energy
[00:39:31.880 --> 00:39:33.400]   of like a small town, right?
[00:39:33.400 --> 00:39:36.360]   And to play NetX, it's very cheap.
[00:39:36.360 --> 00:39:38.240]   And you can do this in your university lab,
[00:39:38.240 --> 00:39:39.360]   on your home computer and so on.
[00:39:39.360 --> 00:39:42.800]   So that's for us, one of the sales pitches for NetX.
[00:39:42.800 --> 00:39:45.440]   It's as an reinforcement learning environment,
[00:39:45.440 --> 00:39:47.440]   NetX is simple where it counts,
[00:39:47.440 --> 00:39:49.400]   but hard where you want it to be.
[00:39:49.400 --> 00:39:51.680]   So it's fast, but hard.
[00:39:51.680 --> 00:39:53.520]   And often we have like the trickiest button
[00:39:53.520 --> 00:39:55.040]   in this two quadrants.
[00:39:55.040 --> 00:39:56.880]   Often there's reinforcement learning environments
[00:39:56.880 --> 00:39:59.720]   that are complex, but easy, right?
[00:39:59.720 --> 00:40:01.880]   So everything is 3D and it's finally rendered,
[00:40:01.880 --> 00:40:03.280]   but the actual policy you need to execute
[00:40:03.280 --> 00:40:05.320]   is like left, left, right, and you're done.
[00:40:05.320 --> 00:40:06.280]   And-
[00:40:06.280 --> 00:40:09.600]   - I guess what makes for a hard reinforcement learning
[00:40:09.600 --> 00:40:13.120]   challenge, like it seems to me like having to sort of
[00:40:13.120 --> 00:40:17.720]   save some state to use a lot later seems to be challenging.
[00:40:17.720 --> 00:40:20.920]   I mean, what else makes, or do you have an intuition
[00:40:20.920 --> 00:40:23.400]   for like what, even what games would be easy for,
[00:40:23.400 --> 00:40:24.720]   it sounds like you do have a good intuition
[00:40:24.720 --> 00:40:27.160]   for what games would be easy for reinforcement learning
[00:40:27.160 --> 00:40:28.800]   and what games would be hard.
[00:40:28.800 --> 00:40:31.080]   - So I, yeah, so the thing that you just mentioned,
[00:40:31.080 --> 00:40:31.920]   that's one, right?
[00:40:31.920 --> 00:40:32.880]   Long range dependencies.
[00:40:32.880 --> 00:40:35.720]   How do you memorize or how do you remember
[00:40:35.720 --> 00:40:37.600]   that maybe on the first level of NetHack,
[00:40:37.600 --> 00:40:39.960]   you dropped a certain item that you've made
[00:40:39.960 --> 00:40:41.880]   like much later or whatnot.
[00:40:41.880 --> 00:40:44.200]   And actually NetHack has these very long range dependencies.
[00:40:44.200 --> 00:40:47.800]   NetHack is also like normal play of NetHack.
[00:40:47.800 --> 00:40:51.240]   If you succeed is maybe on average 50 to a hundred thousand
[00:40:51.240 --> 00:40:53.520]   steps, they're expert players who can solve NetHack
[00:40:53.520 --> 00:40:57.000]   in 20,000 steps, but it's still an order of magnitude
[00:40:57.000 --> 00:41:00.320]   longer than for instance, a normal game of StarCraft II,
[00:41:00.320 --> 00:41:02.080]   which goes on for 15 minutes,
[00:41:02.080 --> 00:41:04.120]   but has only a few actions per second.
[00:41:04.120 --> 00:41:06.800]   So I think average is around 2000 steps.
[00:41:06.800 --> 00:41:08.880]   So yeah, long range dependencies is one.
[00:41:08.880 --> 00:41:10.480]   Then the question of exploration.
[00:41:10.480 --> 00:41:13.760]   So how easy is it for the reinforcement learning agent
[00:41:13.760 --> 00:41:16.400]   to discover what it can do in the environment,
[00:41:16.400 --> 00:41:18.800]   how it can control things in the environment?
[00:41:18.800 --> 00:41:21.440]   How often does it bump into reward?
[00:41:21.440 --> 00:41:24.320]   Another question I guess is,
[00:41:24.320 --> 00:41:29.320]   do you have all the information that you need to
[00:41:29.320 --> 00:41:31.760]   given in the environment itself
[00:41:31.760 --> 00:41:33.040]   in order to do well in the environment
[00:41:33.040 --> 00:41:35.720]   or do you have to have a really strong prior
[00:41:35.720 --> 00:41:37.920]   based on your, as I mentioned, common sense, knowledge,
[00:41:37.920 --> 00:41:40.480]   world knowledge or domain specific knowledge, right?
[00:41:40.480 --> 00:41:42.040]   If you have a very large action space,
[00:41:42.040 --> 00:41:44.400]   that's really problematic for current approaches,
[00:41:44.400 --> 00:41:46.720]   but we as humans do well because we prune away
[00:41:46.720 --> 00:41:48.360]   lots of that action space.
[00:41:48.360 --> 00:41:51.120]   Can you easily plan ahead, right?
[00:41:51.120 --> 00:41:53.720]   Is your environment fully observable
[00:41:53.720 --> 00:41:55.200]   or is it only partially observable
[00:41:55.200 --> 00:41:57.520]   and you have to actually infer what's going on
[00:41:57.520 --> 00:41:59.560]   in the hidden parts of the environment.
[00:41:59.560 --> 00:42:02.520]   So these things make games or environments hard
[00:42:02.520 --> 00:42:04.320]   or easy for reinforcement learning.
[00:42:04.320 --> 00:42:05.880]   - Yeah, it's funny as you're talking,
[00:42:05.880 --> 00:42:08.560]   I mean, did you guys notice how I tried to
[00:42:08.560 --> 00:42:10.920]   kind of steer this towards like general topics,
[00:42:10.920 --> 00:42:14.200]   but I wasn't able to, just pointing that out.
[00:42:14.200 --> 00:42:18.640]   But since we're back in this NetHack topic,
[00:42:18.640 --> 00:42:20.560]   have you thought about my other favorite game,
[00:42:20.560 --> 00:42:21.560]   Kerbal Space Program?
[00:42:21.560 --> 00:42:23.400]   Are you fans at all of you?
[00:42:23.400 --> 00:42:25.480]   Have you played this game?
[00:42:25.480 --> 00:42:27.160]   - I mean, I've seen that on Steam.
[00:42:27.160 --> 00:42:28.160]   I haven't played it myself,
[00:42:28.160 --> 00:42:30.680]   but I think that's a really interesting example.
[00:42:30.680 --> 00:42:32.320]   I mean, again, as I mentioned, I haven't played this.
[00:42:32.320 --> 00:42:33.680]   I've only watched the trailer.
[00:42:33.680 --> 00:42:38.680]   The fact that we, as humans can build mental models, right?
[00:42:38.680 --> 00:42:40.920]   Of what should work and what shouldn't work
[00:42:40.920 --> 00:42:42.360]   and then test them, right?
[00:42:42.360 --> 00:42:43.960]   I mean, that's, I guess what you do in that game, right?
[00:42:43.960 --> 00:42:46.600]   You have an idea of like what might work out
[00:42:46.600 --> 00:42:48.240]   in terms of a rocket that can fly.
[00:42:48.240 --> 00:42:49.640]   You build it, then you see it fails
[00:42:49.640 --> 00:42:51.400]   and then you make modifications.
[00:42:51.400 --> 00:42:52.960]   You have, again, you're planning your hat,
[00:42:52.960 --> 00:42:54.360]   what kind of modifications you want to make.
[00:42:54.360 --> 00:42:55.680]   You make them and you see again.
[00:42:55.680 --> 00:42:58.960]   Like this kind of way of experimenting in an environment.
[00:42:58.960 --> 00:43:01.200]   I think that probably sounds quite interesting
[00:43:01.200 --> 00:43:03.320]   for a reinforcement learning challenge.
[00:43:03.320 --> 00:43:04.400]   That said, I haven't played it myself
[00:43:04.400 --> 00:43:07.400]   and I'm pretty sure current approaches would struggle a lot.
[00:43:07.400 --> 00:43:12.600]   - Can I ask you, is there anything just like practically
[00:43:12.600 --> 00:43:13.960]   that changes when you're trying
[00:43:13.960 --> 00:43:15.800]   to train reinforcement learning algorithms,
[00:43:15.800 --> 00:43:17.760]   if you're kind of used to more supervised
[00:43:17.760 --> 00:43:18.600]   learning algorithms?
[00:43:18.600 --> 00:43:21.000]   Like what's different about that kind of training?
[00:43:21.000 --> 00:43:24.760]   - I think there are some like engineering challenges
[00:43:24.760 --> 00:43:25.800]   to reinforcement learning.
[00:43:25.800 --> 00:43:27.400]   Basically reinforcement learning,
[00:43:27.400 --> 00:43:28.960]   you can make it look like supervised learning,
[00:43:28.960 --> 00:43:30.560]   but the data comes from,
[00:43:30.560 --> 00:43:32.680]   like you generate the data yourself, right?
[00:43:32.680 --> 00:43:35.280]   As opposed to just reading like photos from this,
[00:43:35.280 --> 00:43:37.280]   you generate the data yourself.
[00:43:37.280 --> 00:43:39.320]   And this is actually what modern reinforcement learning
[00:43:39.320 --> 00:43:43.840]   systems like say Impala or like various others do.
[00:43:43.840 --> 00:43:46.040]   They have this part of the system that produces the data
[00:43:46.040 --> 00:43:47.960]   and then a part of the system that learns on the data.
[00:43:47.960 --> 00:43:49.600]   And there's all kinds of like engineering challenges
[00:43:49.600 --> 00:43:51.720]   around this with asynchronous processes
[00:43:51.720 --> 00:43:54.280]   and like data communication and so on.
[00:43:54.280 --> 00:43:57.200]   But apart from that, we use PyTorch.
[00:43:57.200 --> 00:43:59.160]   We use standard tools.
[00:43:59.160 --> 00:44:00.200]   You have to have to compute.
[00:44:00.200 --> 00:44:01.720]   Typically the games run on the CPU.
[00:44:01.720 --> 00:44:03.480]   So you have to have more CPU.
[00:44:03.480 --> 00:44:04.720]   And while the reinforcement,
[00:44:04.720 --> 00:44:07.080]   like the machine learning code runs on accelerators
[00:44:07.080 --> 00:44:08.280]   like GPUs.
[00:44:08.280 --> 00:44:10.120]   But once you have that in place,
[00:44:10.120 --> 00:44:11.360]   it looks pretty similar.
[00:44:11.360 --> 00:44:14.160]   And the output, like the models look similar, right?
[00:44:14.160 --> 00:44:17.640]   They input a picture or like the game observation output
[00:44:17.640 --> 00:44:20.280]   is the probabilities of certain actions.
[00:44:20.280 --> 00:44:21.120]   - Yeah.
[00:44:21.120 --> 00:44:23.560]   So there's one additional thing I would want to mention.
[00:44:23.560 --> 00:44:26.120]   And that also relates, I think, to weights and biases.
[00:44:26.120 --> 00:44:28.400]   And it's that in reinforcement learning generally,
[00:44:28.400 --> 00:44:30.800]   your results have much higher variance.
[00:44:30.800 --> 00:44:32.960]   So you can train an agent once
[00:44:32.960 --> 00:44:35.240]   and then you train it another time
[00:44:35.240 --> 00:44:37.200]   and the results might actually look quite different.
[00:44:37.200 --> 00:44:39.480]   So you have to be careful in terms of
[00:44:39.480 --> 00:44:41.440]   how reliable your results are
[00:44:41.440 --> 00:44:45.320]   when you only train based on one run basically, right?
[00:44:45.320 --> 00:44:47.280]   That makes it interesting in terms of
[00:44:47.280 --> 00:44:50.760]   how you should plot these results in publications, right?
[00:44:50.760 --> 00:44:53.560]   I mean, ideally you should be repeating your experiments
[00:44:53.560 --> 00:44:55.720]   multiple times and you want to plot
[00:44:55.720 --> 00:44:57.800]   maybe the mean of the different runs
[00:44:57.800 --> 00:45:00.680]   and you also want to indicate the variance to some extent.
[00:45:00.680 --> 00:45:02.600]   But I think in publications,
[00:45:02.600 --> 00:45:04.760]   we've seen all kinds of tricks of how people
[00:45:04.760 --> 00:45:07.720]   make results look better than they actually are.
[00:45:07.720 --> 00:45:09.960]   - I mean, how do you even think about reproducibility
[00:45:09.960 --> 00:45:11.640]   of reinforcement learning results
[00:45:11.640 --> 00:45:13.320]   if they're inherently stochastic?
[00:45:13.320 --> 00:45:19.200]   - I think it's fine as long as you make sure you train
[00:45:19.200 --> 00:45:22.240]   with different initializations of your model multiple times.
[00:45:22.240 --> 00:45:24.880]   And then that really comes down to a question of
[00:45:24.880 --> 00:45:28.040]   how expensive is it to run the experiments, right?
[00:45:28.040 --> 00:45:29.560]   What we see right now in the field is that
[00:45:29.560 --> 00:45:33.760]   there's lots of interesting reinforcement learning results
[00:45:33.760 --> 00:45:35.440]   that come out of industry labs
[00:45:35.440 --> 00:45:37.600]   that have a lot of computational resources
[00:45:37.600 --> 00:45:40.840]   and that makes it basically impossible for anyone outside,
[00:45:40.840 --> 00:45:42.880]   specifically in academia to reproduce these results.
[00:45:42.880 --> 00:45:45.160]   And that was, again, sorry to mention that,
[00:45:45.160 --> 00:45:47.000]   but that was exactly the kind of motivation
[00:45:47.000 --> 00:45:50.040]   behind that environment in that it's really complex,
[00:45:50.040 --> 00:45:53.080]   but at the same time should be affordable for,
[00:45:53.080 --> 00:45:54.880]   you know, grad students and master's students
[00:45:54.880 --> 00:45:57.360]   and whatnot to actually do experiments with.
[00:45:57.360 --> 00:45:58.200]   - Yeah, I hadn't thought about that.
[00:45:58.200 --> 00:45:59.800]   That's such a great point.
[00:45:59.800 --> 00:46:01.720]   That's, but still actually,
[00:46:01.720 --> 00:46:03.640]   you do need quite a lot of resources
[00:46:03.640 --> 00:46:05.400]   to even do NetHack, right?
[00:46:05.400 --> 00:46:08.400]   Like I was saying, you built some kind of system
[00:46:08.400 --> 00:46:09.640]   or you're using some kind of system
[00:46:09.640 --> 00:46:11.600]   to train in parallel, right?
[00:46:11.600 --> 00:46:14.560]   - Yeah, but you can run this on a single box
[00:46:14.560 --> 00:46:16.040]   with say two GPUs.
[00:46:16.040 --> 00:46:17.760]   So you'll just wait a little bit longer,
[00:46:17.760 --> 00:46:20.160]   but we don't like, for NetHack,
[00:46:20.160 --> 00:46:22.840]   we don't currently use like hundreds of GPUs in parallel.
[00:46:22.840 --> 00:46:23.680]   We could do that,
[00:46:23.680 --> 00:46:26.000]   but we haven't just haven't invested the engineering,
[00:46:26.000 --> 00:46:29.080]   like hours to do that properly.
[00:46:29.080 --> 00:46:30.520]   But you can actually run this at home.
[00:46:30.520 --> 00:46:32.640]   You can, I mean, you could even run this on your MacBook
[00:46:32.640 --> 00:46:34.280]   if you wanted to wait long enough
[00:46:34.280 --> 00:46:35.480]   and like make it look a little bit hot,
[00:46:35.480 --> 00:46:38.280]   but you can, depends on what kind of neural networks
[00:46:38.280 --> 00:46:39.120]   would apply to NetHack,
[00:46:39.120 --> 00:46:42.200]   but it's actually something you can do at home.
[00:46:42.200 --> 00:46:44.480]   - Yeah, and actually, yeah, and I think you,
[00:46:44.480 --> 00:46:47.000]   I mean, you even can do this really well with just one GPU.
[00:46:47.000 --> 00:46:49.840]   I think we have our implementation of our agents
[00:46:49.840 --> 00:46:51.400]   is based on Torchbeast,
[00:46:51.400 --> 00:46:53.120]   which again is based on Impala.
[00:46:53.120 --> 00:46:54.240]   And we have two versions of that.
[00:46:54.240 --> 00:46:56.120]   One is training based on one GPU.
[00:46:56.120 --> 00:46:59.160]   We have one that's training using two GPUs.
[00:46:59.160 --> 00:47:01.080]   I mean, just with one GPU, you can do experiments.
[00:47:01.080 --> 00:47:03.040]   You can write papers on NetHack with one GPU.
[00:47:03.040 --> 00:47:05.400]   I'm quite certain of that.
[00:47:05.400 --> 00:47:06.240]   - Cool.
[00:47:06.240 --> 00:47:07.680]   And it's basically just playing the game
[00:47:07.680 --> 00:47:11.160]   over and over and over and then updating the model.
[00:47:11.160 --> 00:47:12.920]   - Yeah, we have this line in our paper
[00:47:12.920 --> 00:47:16.360]   where it mentioned how many agents died in the process.
[00:47:16.360 --> 00:47:17.640]   And it's a large number, right?
[00:47:17.640 --> 00:47:20.760]   We're talking about more games,
[00:47:20.760 --> 00:47:22.320]   like probably our algorithm by now
[00:47:22.320 --> 00:47:23.680]   is played by far more games
[00:47:23.680 --> 00:47:25.400]   than the rest of mankind combined.
[00:47:25.400 --> 00:47:29.560]   - Have they really not found any flaw in NetHack to exploit?
[00:47:29.560 --> 00:47:30.480]   It's kind of amazing to me
[00:47:30.480 --> 00:47:31.600]   that there's not some tricky way
[00:47:31.600 --> 00:47:33.800]   that you can live forever or something.
[00:47:33.800 --> 00:47:35.560]   - Well, our agents actually haven't explored
[00:47:35.560 --> 00:47:37.320]   that large part of the game yet.
[00:47:37.320 --> 00:47:40.360]   We are really at the beginning of the research here.
[00:47:40.360 --> 00:47:43.600]   And I'm sure there will be, like, so previous,
[00:47:43.600 --> 00:47:45.360]   people have tried what is called TAS,
[00:47:45.360 --> 00:47:47.840]   tool automated speed runs with NetHack
[00:47:47.840 --> 00:47:49.640]   and found exploits,
[00:47:49.640 --> 00:47:51.480]   like some of the ones that Tim mentioned,
[00:47:51.480 --> 00:47:52.680]   putting farming and so on.
[00:47:52.680 --> 00:47:53.600]   But the dev team, NetHack,
[00:47:53.600 --> 00:47:54.760]   that kind of like keeps track of that
[00:47:54.760 --> 00:47:57.280]   and removes these things one by one from the game.
[00:47:57.280 --> 00:47:59.880]   So NetHack by now is pretty resilient
[00:47:59.880 --> 00:48:02.320]   against these kinds of exploits.
[00:48:02.320 --> 00:48:05.520]   - Are you in communication with the NetHack dev team?
[00:48:05.520 --> 00:48:07.160]   - We did reach out to them at some point, yes.
[00:48:07.160 --> 00:48:08.800]   And they were very kind.
[00:48:08.800 --> 00:48:09.960]   - That's great.
[00:48:09.960 --> 00:48:11.520]   So NetHack has been under development
[00:48:11.520 --> 00:48:13.120]   for over 30 years, right?
[00:48:13.120 --> 00:48:15.600]   So there's been a lot of, as Heiner mentioned,
[00:48:15.600 --> 00:48:16.560]   there's been a lot of effort
[00:48:16.560 --> 00:48:19.720]   in kind of removing all of these kind of exploits.
[00:48:19.720 --> 00:48:22.120]   - Right, right, right, right.
[00:48:22.120 --> 00:48:24.800]   Okay, sorry, one more, just really the weeds question.
[00:48:24.800 --> 00:48:27.160]   Does the agent have a preference?
[00:48:27.160 --> 00:48:29.160]   Like in NetHack, you can kind of go down the normal levels
[00:48:29.160 --> 00:48:30.240]   where you meet the Oracle and stuff,
[00:48:30.240 --> 00:48:32.960]   or you can go down to that like mine town.
[00:48:32.960 --> 00:48:33.960]   Does the agent kind of learn
[00:48:33.960 --> 00:48:36.160]   that one path is safer than the other?
[00:48:36.160 --> 00:48:39.800]   I always kind of wonder which I should go to first.
[00:48:39.800 --> 00:48:40.640]   - It's a great question.
[00:48:40.640 --> 00:48:42.600]   That's exactly the kind of high level planning
[00:48:42.600 --> 00:48:44.360]   that our agents right now are not capable of.
[00:48:44.360 --> 00:48:46.040]   So it's basically by chance.
[00:48:46.040 --> 00:48:48.760]   So sometimes they just follow the main dungeon, Oracle,
[00:48:48.760 --> 00:48:52.560]   they even get to the big room or even further down
[00:48:52.560 --> 00:48:53.480]   and then at some point die,
[00:48:53.480 --> 00:48:55.840]   or they go into mine town and at some point die.
[00:48:55.840 --> 00:48:58.040]   We haven't really seen agents being strategic
[00:48:58.040 --> 00:49:00.600]   about first making some progress in the main dungeons
[00:49:00.600 --> 00:49:03.160]   and then going back up to the fork
[00:49:03.160 --> 00:49:06.880]   to then go down the mine town to get items and whatnot.
[00:49:06.880 --> 00:49:08.400]   But I mean, that's really one of the next,
[00:49:08.400 --> 00:49:11.080]   I think, milestones that we should get to.
[00:49:11.080 --> 00:49:12.880]   - It's also because our agents have a really hard time
[00:49:12.880 --> 00:49:14.600]   remembering things long-term, right?
[00:49:14.600 --> 00:49:17.000]   So they basically, our agents look like,
[00:49:17.000 --> 00:49:18.400]   like the first order of estimation,
[00:49:18.400 --> 00:49:21.360]   our agents optimize the current situation
[00:49:21.360 --> 00:49:23.440]   without any regard for the past.
[00:49:23.440 --> 00:49:25.360]   So going down mine town,
[00:49:25.360 --> 00:49:26.760]   like if you go down any stair
[00:49:26.760 --> 00:49:28.600]   and you happen to enter the Memories Mines,
[00:49:28.600 --> 00:49:31.560]   which is like the special dungeon branch in NetEck,
[00:49:31.560 --> 00:49:33.000]   the logical thing for you to do
[00:49:33.000 --> 00:49:35.040]   is to kill the monsters in the vicinity,
[00:49:35.040 --> 00:49:36.880]   not to go back up where you were
[00:49:36.880 --> 00:49:38.280]   and where you already killed things.
[00:49:38.280 --> 00:49:41.200]   So if you optimize for really short-term things,
[00:49:41.200 --> 00:49:42.760]   that's how you end up playing,
[00:49:42.760 --> 00:49:43.960]   and that's what our agents do.
[00:49:43.960 --> 00:49:45.920]   That said, we have seen our agents go back upstairs
[00:49:45.920 --> 00:49:48.800]   and we're not quite sure if this is just like random chance
[00:49:48.800 --> 00:49:52.400]   or if this is something where it got incentivized
[00:49:52.400 --> 00:49:56.960]   to not play certain levels, but that's where we are.
[00:49:56.960 --> 00:49:59.040]   - All right, well, I'm really excited to play with your,
[00:49:59.040 --> 00:50:01.040]   I'm even more excited to play
[00:50:01.040 --> 00:50:02.480]   with your NetEck dev environment.
[00:50:02.480 --> 00:50:05.640]   I really want to give this a big run myself.
[00:50:05.640 --> 00:50:06.960]   We always end with these two questions.
[00:50:06.960 --> 00:50:09.400]   I kind of wonder how they'll work in this format,
[00:50:09.400 --> 00:50:13.360]   but do you have any kind of underrated aspect
[00:50:13.360 --> 00:50:15.080]   of reinforcement learning or machine learning
[00:50:15.080 --> 00:50:17.800]   that you think people should pay more attention to
[00:50:17.800 --> 00:50:19.480]   than they are right now?
[00:50:19.480 --> 00:50:20.320]   - Yeah, I mean- - Did you mention
[00:50:20.320 --> 00:50:22.000]   really fast environments?
[00:50:22.000 --> 00:50:22.960]   (all laughing)
[00:50:22.960 --> 00:50:26.640]   - No, but I think on top of that, in my view,
[00:50:26.640 --> 00:50:29.160]   people should be looking more into causality.
[00:50:29.160 --> 00:50:31.240]   I mean, it's something that I'm not very familiar with,
[00:50:31.240 --> 00:50:35.560]   but I think in terms of making progress as a community,
[00:50:35.560 --> 00:50:38.080]   we should be looking more into causal models
[00:50:38.080 --> 00:50:41.000]   because essentially that's also what you are learning
[00:50:41.000 --> 00:50:43.160]   when you're playing NetEck over and over again
[00:50:43.160 --> 00:50:46.160]   that at some point you have some mental causal model
[00:50:46.160 --> 00:50:48.080]   in mind, if I do this, then that happens,
[00:50:48.080 --> 00:50:51.080]   or at least with some probabilities, something happens.
[00:50:51.080 --> 00:50:53.960]   And I think that's the only reasonable way
[00:50:53.960 --> 00:50:56.240]   we can go forward in terms of agents
[00:50:56.240 --> 00:50:57.880]   that really can systematically generalize
[00:50:57.880 --> 00:50:59.040]   to novel situations.
[00:50:59.040 --> 00:51:01.720]   You have to have that kind of abstract mental model
[00:51:01.720 --> 00:51:05.080]   in mind that useful planning and for exploration and so on.
[00:51:05.080 --> 00:51:07.760]   - One thing that bugs me a bit about the research
[00:51:07.760 --> 00:51:08.720]   and machine learning at large
[00:51:08.720 --> 00:51:10.760]   is that we make this artificial distinctions
[00:51:10.760 --> 00:51:13.400]   between this is engineering and this is research,
[00:51:13.400 --> 00:51:15.120]   where if you want to fly to the moon,
[00:51:15.120 --> 00:51:17.400]   is that research or is it engineering?
[00:51:17.400 --> 00:51:18.600]   It's kind of both, right?
[00:51:18.600 --> 00:51:21.120]   And I think that in particular,
[00:51:21.120 --> 00:51:23.480]   it's especially true in reinforcement learning,
[00:51:23.480 --> 00:51:25.800]   where the breakthroughs that we saw recently
[00:51:25.800 --> 00:51:28.480]   came to a large extent from engineering breakthroughs.
[00:51:28.480 --> 00:51:31.600]   - Yeah, I totally agree with that.
[00:51:31.600 --> 00:51:33.600]   And that's actually a good segue to the last question
[00:51:33.600 --> 00:51:35.920]   that we always ask, which is, we usually frame it,
[00:51:35.920 --> 00:51:37.640]   what's the biggest challenge of machine learning
[00:51:37.640 --> 00:51:38.480]   in the real world?
[00:51:38.480 --> 00:51:40.040]   But I think maybe for you two, I'd be curious,
[00:51:40.040 --> 00:51:42.960]   what are the surprising engineering challenges
[00:51:42.960 --> 00:51:44.600]   of making reinforcement learning work
[00:51:44.600 --> 00:51:48.040]   that you wouldn't necessarily know as a grad student
[00:51:48.040 --> 00:51:50.920]   doing your first toy reinforcement learning project?
[00:51:50.920 --> 00:51:53.960]   - I think, I mean, maybe we should make this clear, right?
[00:51:53.960 --> 00:51:56.120]   What we do when you're training reinforcement learning agents
[00:51:56.120 --> 00:51:59.440]   in modern approaches is we have dozens or hundreds
[00:51:59.440 --> 00:52:01.520]   of copies of the game running simultaneously,
[00:52:01.520 --> 00:52:02.480]   played by the same agent,
[00:52:02.480 --> 00:52:05.840]   and then something needs to ingest all of this information.
[00:52:05.840 --> 00:52:09.520]   So I'm not sure if people are aware this is how it is.
[00:52:09.520 --> 00:52:11.080]   People used to think of it like, this is the world,
[00:52:11.080 --> 00:52:12.920]   and this is my agent, and my agent interacts with the world,
[00:52:12.920 --> 00:52:14.400]   and there's only one world, obviously,
[00:52:14.400 --> 00:52:15.800]   but things are just so much faster
[00:52:15.800 --> 00:52:17.200]   if you have a batch of worlds,
[00:52:17.200 --> 00:52:19.680]   and you interact with a batch of experience,
[00:52:19.680 --> 00:52:21.840]   although that is kind of bad news
[00:52:21.840 --> 00:52:24.440]   for all the comparisons to how humans learn,
[00:52:24.440 --> 00:52:26.920]   and how real biological systems work.
[00:52:26.920 --> 00:52:28.360]   - Yeah, I think on top of that,
[00:52:28.360 --> 00:52:32.040]   I would encourage people to really look at
[00:52:32.040 --> 00:52:33.960]   what these agents, or what generally
[00:52:33.960 --> 00:52:37.040]   your machine learning model is actually doing on the data.
[00:52:37.040 --> 00:52:39.600]   So it's, I think, quite easy to try to chase
[00:52:39.600 --> 00:52:43.320]   some leaderboard numbers, or try to chase better scores
[00:52:43.320 --> 00:52:45.440]   on NetHack without actually understanding
[00:52:45.440 --> 00:52:47.760]   what your agent is capable of, or not capable of,
[00:52:47.760 --> 00:52:51.400]   and how that informs your modeling choices,
[00:52:51.400 --> 00:52:53.800]   modeling decisions, and generally your research
[00:52:53.800 --> 00:52:56.560]   or engineering work going forward.
[00:52:56.560 --> 00:52:59.760]   - And so one final question, mainly for Heinrich, I think.
[00:52:59.760 --> 00:53:02.680]   So for someone like me who's been playing the game for,
[00:53:02.680 --> 00:53:04.400]   playing NetHack for almost three decades
[00:53:04.400 --> 00:53:06.520]   and never ascended, do you have any tips
[00:53:06.520 --> 00:53:08.720]   on how to improve my NetHack skills?
[00:53:08.720 --> 00:53:11.680]   - I think there's this one game, one point in NetHack
[00:53:11.680 --> 00:53:14.680]   where you ask this special shopkeeper,
[00:53:14.680 --> 00:53:17.360]   like in Mine Town, and it tells you to slow down,
[00:53:17.360 --> 00:53:18.200]   think about it.
[00:53:18.200 --> 00:53:20.520]   You have as much time as you want to do any action,
[00:53:20.520 --> 00:53:22.080]   like NetHack is turn-based, right?
[00:53:22.080 --> 00:53:23.920]   So I think this is the best approach.
[00:53:23.920 --> 00:53:26.440]   Think clearly, but it's really not human, right?
[00:53:26.440 --> 00:53:28.840]   You see this big dragon, bad dragon,
[00:53:28.840 --> 00:53:29.800]   and you want to run away from it,
[00:53:29.800 --> 00:53:33.400]   but there's no need for speed in that sense in NetHack.
[00:53:33.400 --> 00:53:34.840]   Just thinking clearly about every step
[00:53:34.840 --> 00:53:35.960]   is the best approach.
[00:53:36.560 --> 00:53:37.720]   - Yeah, it's so hard to do.
[00:53:37.720 --> 00:53:38.920]   - And read the spoilers.
[00:53:38.920 --> 00:53:41.600]   (laughing)
[00:53:41.600 --> 00:53:42.560]   - Awesome, thanks so much guys.
[00:53:42.560 --> 00:53:43.400]   That was super fun.
[00:53:43.400 --> 00:53:44.240]   - Thank you.
[00:53:44.240 --> 00:53:46.680]   - Yeah, likewise, thanks so much for the invitation.
[00:53:46.680 --> 00:53:48.800]   - Thanks for listening to another episode
[00:53:48.800 --> 00:53:50.520]   of "Gradient Dissent."
[00:53:50.520 --> 00:53:52.320]   Doing these interviews are a lot of fun,
[00:53:52.320 --> 00:53:54.720]   and it's especially fun for me when I can actually hear
[00:53:54.720 --> 00:53:57.040]   from the people that are listening to these episodes.
[00:53:57.040 --> 00:53:59.760]   So if you wouldn't mind leaving a comment
[00:53:59.760 --> 00:54:02.440]   and telling me what you think or starting a conversation,
[00:54:02.440 --> 00:54:04.960]   that would make me inspired to do more of these episodes.
[00:54:04.960 --> 00:54:07.720]   And also if you wouldn't mind liking and subscribing,
[00:54:07.720 --> 00:54:09.020]   I'd appreciate that a lot.

