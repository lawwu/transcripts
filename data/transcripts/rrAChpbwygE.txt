
[00:00:00.000 --> 00:00:06.760]   Generative AI is what many expect to be the next big technology boom.
[00:00:06.760 --> 00:00:14.140]   And being what it is, AI, it could have far-reaching implications that are beyond what we would
[00:00:14.140 --> 00:00:15.140]   imagine today.
[00:00:15.140 --> 00:00:21.920]   That's not to say that we have entered the end game of AI with AGI or anything like that,
[00:00:21.920 --> 00:00:27.300]   but I think that generative AI is a pretty big step forwards.
[00:00:27.300 --> 00:00:30.420]   And it seems that investors are aware of this as well.
[00:00:30.420 --> 00:00:38.320]   We all know that the majority of industries had a very bad 2022, yet generative AI startups
[00:00:38.320 --> 00:00:43.720]   actually received $1.37 billion in funding.
[00:00:43.720 --> 00:00:48.920]   According to New York Times, that's almost as much as they received in the past five
[00:00:48.920 --> 00:00:50.440]   years combined.
[00:00:50.440 --> 00:00:53.000]   However, it's hardly surprising.
[00:00:53.000 --> 00:00:58.940]   There were several wow moments that came from generative AI in 2022.
[00:00:58.940 --> 00:01:06.360]   From generative art tools like OpenAI's DALI 2, Mid-Journey, and Sable Diffusion, to the
[00:01:06.360 --> 00:01:14.180]   next generation of large language models from the likes of OpenAI with the GPT 3.5 models,
[00:01:14.180 --> 00:01:21.080]   the Open Source Bloom project, and the chatbots like Goose Lambda, and of course, the chat
[00:01:21.080 --> 00:01:22.220]   GPT.
[00:01:22.220 --> 00:01:30.020]   All of this together marks just the first year of the widespread adoption of generative
[00:01:30.020 --> 00:01:31.020]   AI.
[00:01:31.020 --> 00:01:37.820]   We're still in the very early days of a technology that is poised to completely change the way
[00:01:37.820 --> 00:01:40.580]   that we interact with the machines.
[00:01:40.580 --> 00:01:46.000]   And one of the most thought provoking use cases in how we interact with machines, I
[00:01:46.000 --> 00:01:50.220]   think belongs to generative question answering or GQA.
[00:01:50.220 --> 00:01:55.980]   Now the most simple GQA pipeline consists of nothing more than a user's question or
[00:01:55.980 --> 00:01:59.060]   query and a large language model.
[00:01:59.060 --> 00:02:03.460]   The query is passed to the large language model and based on what the large language
[00:02:03.460 --> 00:02:09.100]   model has learned during its training, so the knowledge that's stored within the model
[00:02:09.100 --> 00:02:13.540]   parameters, it will output an answer to your question.
[00:02:13.540 --> 00:02:19.180]   And we can see that this works for general knowledge questions pretty well across the
[00:02:19.180 --> 00:02:20.180]   board.
[00:02:20.180 --> 00:02:26.700]   So if we take a look at OpenAI's DaVinci 003 model, Cohere's extra large model behind
[00:02:26.700 --> 00:02:31.380]   the generation endpoint, or even Open Source models that we can access through Hugging
[00:02:31.380 --> 00:02:37.760]   Face Transformers, we will get a good answer for general knowledge questions.
[00:02:37.760 --> 00:02:42.780]   So if we ask, "Who was the first person on the moon?" we will get across the board
[00:02:42.780 --> 00:02:44.600]   the answer, Neil Armstrong.
[00:02:44.600 --> 00:02:49.620]   So we can see that this works incredibly well for things that are within the general knowledge
[00:02:49.620 --> 00:02:51.780]   base of these large language models.
[00:02:51.780 --> 00:02:57.020]   However, if we start asking more specific or advanced questions, these large language
[00:02:57.020 --> 00:02:59.380]   models will begin to fail.
[00:02:59.380 --> 00:03:05.100]   So if we ask it a very specific question about machine learning methods and specifically
[00:03:05.100 --> 00:03:11.460]   NLP and semantic search training methods, like, "Which training method should I use
[00:03:11.460 --> 00:03:17.380]   for training sentence transformers when I have just pairs of positive sentences?"
[00:03:17.380 --> 00:03:22.140]   Now, you don't need to understand what that means if you don't, no problem.
[00:03:22.140 --> 00:03:27.700]   One of the correct answers to this should be multiple noted ranking loss, or even just
[00:03:27.700 --> 00:03:29.420]   ranking loss would be fine as well.
[00:03:29.420 --> 00:03:34.780]   Yeah, if we ask this question, and we'll go ahead and ask what I found to be the best
[00:03:34.780 --> 00:03:38.540]   performing of the large language models so far.
[00:03:38.540 --> 00:03:44.020]   If we ask DaVinci 003 this question, it gives us this answer, and it says, "We need to
[00:03:44.020 --> 00:03:49.420]   use a supervised training method," which, yes, that is correct, but it doesn't really
[00:03:49.420 --> 00:03:51.060]   answer the question.
[00:03:51.060 --> 00:03:55.620]   It doesn't give us a specific method to use, and the reason it doesn't give us that is
[00:03:55.620 --> 00:03:57.980]   because the model doesn't know.
[00:03:57.980 --> 00:04:05.120]   This knowledge has not been encoded into the model weights or parameters during training,
[00:04:05.120 --> 00:04:06.540]   so it can't answer the question.
[00:04:06.540 --> 00:04:12.140]   Now, there are two options we can take in order to help the model answer this question.
[00:04:12.140 --> 00:04:19.660]   The first is we can fine tune the large language model on the text data that would contain
[00:04:19.660 --> 00:04:20.660]   this information.
[00:04:20.660 --> 00:04:22.180]   Now, this can be hard to do.
[00:04:22.180 --> 00:04:28.820]   It can take a lot of computational resources or money, and it also requires a lot of text
[00:04:28.820 --> 00:04:31.940]   data as well, which is not always necessarily available.
[00:04:31.940 --> 00:04:39.620]   If we just mention the answer once in a single sentence of a million sentences, the large
[00:04:39.620 --> 00:04:44.580]   language model might not pick up on that information, and when we ask the question again, it may
[00:04:44.580 --> 00:04:46.340]   not have learned the answer.
[00:04:46.340 --> 00:04:52.300]   We need a lot of text data that mentions this in multiple contexts in order for it to learn
[00:04:52.300 --> 00:04:54.780]   this information well.
[00:04:54.780 --> 00:04:59.740]   Considering that, our second option, which I think is probably the easier option, is
[00:04:59.740 --> 00:05:06.260]   to use something called retrieval augmented generation, or in this case, retrieval augmented
[00:05:06.260 --> 00:05:08.460]   generative Q&A.
[00:05:08.460 --> 00:05:15.060]   This simply means that we add what is called a retrieval component to our GQA pipeline.
[00:05:15.060 --> 00:05:21.500]   Adding this retrieval component allows us to retrieve relevant information.
[00:05:21.500 --> 00:05:25.700]   If we have that sentence within our million sentences, we can retrieve that sentence and
[00:05:25.700 --> 00:05:29.620]   feed it into our large language model alongside our query.
[00:05:29.620 --> 00:05:34.300]   We're essentially creating a secondary source of information.
[00:05:34.300 --> 00:05:42.260]   Going ahead with this second option of retrieval augmented ML, when we apply it to large language
[00:05:42.260 --> 00:05:47.500]   models, we can actually think of it as a form of long-term memory.
[00:05:47.500 --> 00:05:52.980]   To implement this long-term memory, we need to integrate a knowledge base into our GQA
[00:05:52.980 --> 00:05:53.980]   pipeline.
[00:05:53.980 --> 00:05:58.260]   This knowledge base is the retrieval component that we're talking about, and it allows us
[00:05:58.260 --> 00:06:04.140]   to take our query and search through our sentences or paragraphs for relevant information and
[00:06:04.140 --> 00:06:09.640]   return that relevant information that we can then pass to our larger language model.
[00:06:09.640 --> 00:06:14.740]   As you can see, using this approach, we get much better results.
[00:06:14.740 --> 00:06:20.660]   Again, using DaVinci 003 for the generation model here, we get, "You should use natural
[00:06:20.660 --> 00:06:24.540]   language inference NLI with multiple negative ranking loss."
[00:06:24.540 --> 00:06:31.880]   Now, NLI is just one option for the format of the data, essentially, but the answer of
[00:06:31.880 --> 00:06:36.060]   multiple negative ranking loss is definitely what we're looking for.
[00:06:36.060 --> 00:06:42.780]   This much better answer is a direct result of adding more contextual information to our
[00:06:42.780 --> 00:06:47.560]   query, which we would refer to as source knowledge.
[00:06:47.560 --> 00:06:52.940]   Source knowledge is basically any knowledge that gets passed through to the large language
[00:06:52.940 --> 00:06:58.940]   model within the input of whatever we're putting into the model at inference time, so when
[00:06:58.940 --> 00:07:01.700]   we're predicting or generating text.
[00:07:01.700 --> 00:07:09.820]   In this example, what we use is OpenAI with both generation and actually embedding, which
[00:07:09.820 --> 00:07:16.020]   I'll explain in a moment, and also Pinecone Vector Database as our knowledge base.
[00:07:16.020 --> 00:07:21.740]   Both these together are what we would refer to as the OP stack, so OpenAI, Pinecone.
[00:07:21.740 --> 00:07:29.100]   This is a more recently popularized option for building very performant AI apps that
[00:07:29.100 --> 00:07:34.660]   rely on a retrieval component like Retrieval Augmented GQA.
[00:07:34.660 --> 00:07:40.020]   At query time in this scenario, the pipeline consisted of three main steps.
[00:07:40.020 --> 00:07:48.020]   The first one, we use an OpenAI embedding endpoint to encode our query into what we
[00:07:48.020 --> 00:07:55.020]   call dense vector, and step two, we took that encoded query, sent it to our knowledge base,
[00:07:55.020 --> 00:08:02.700]   which returned relevant context or text passages back to us, which then we combined with our
[00:08:02.700 --> 00:08:05.700]   query, and that leads on to step three.
[00:08:05.700 --> 00:08:12.720]   We take our query and that relevant information, relevant context, and push them into our large
[00:08:12.720 --> 00:08:20.420]   language model to generate a natural language answer, and as you can see, adding that extra
[00:08:20.420 --> 00:08:25.620]   context from Pinecone, our knowledge base, allowed the large language model to answer
[00:08:25.620 --> 00:08:28.860]   the question much more accurately.
[00:08:28.860 --> 00:08:34.140]   Even beyond providing more factual, accurate answers, the fact that we can retrieve the
[00:08:34.140 --> 00:08:40.780]   sources of information and actually present them to users using this approach also instills
[00:08:40.780 --> 00:08:46.380]   user trust in the system, allowing users to confirm the reliability of the information
[00:08:46.380 --> 00:08:48.180]   that is being presented to them.
[00:08:48.180 --> 00:08:50.180]   Let's go ahead and try a few more examples.
[00:08:50.180 --> 00:08:54.340]   We're going to use the same pipeline that I've already described.
[00:08:54.340 --> 00:08:59.060]   The knowledge base that we're going to be using, so the data source, is the James Callum
[00:08:59.060 --> 00:09:03.980]   YouTube Transcriptions dataset, which is hosted on Hockeying Face Datasets, which is just
[00:09:03.980 --> 00:09:10.560]   a dataset of transcribed audio from various tech and ML YouTube channels.
[00:09:10.560 --> 00:09:15.820]   If we ask questions around ML and tech, generally speaking, if it's within the knowledge base,
[00:09:15.820 --> 00:09:18.060]   it should be able to answer those questions pretty accurately.
[00:09:18.060 --> 00:09:21.020]   We're going to start with, what is NLI?
[00:09:21.020 --> 00:09:27.300]   Our first answer is NLI stands for Natural Language Interface, which is wrong.
[00:09:27.300 --> 00:09:32.560]   The second is correct, so we get Natural Language Inference, NLI is a test that requires pairs
[00:09:32.560 --> 00:09:38.460]   of sentences to be labeled as either contradictory, neutral, or entailing inferring each other,
[00:09:38.460 --> 00:09:39.900]   which is perfect.
[00:09:39.900 --> 00:09:41.260]   Let's try something else.
[00:09:41.260 --> 00:09:49.060]   How can I use OpenAI's clip easily?
[00:09:49.060 --> 00:09:50.060]   No augmentation.
[00:09:50.060 --> 00:09:53.420]   It looks like we're just getting a description of what clip is, which is, I mean, this is
[00:09:53.420 --> 00:09:54.420]   correct.
[00:09:54.420 --> 00:10:02.500]   It used to classify images and generate natural language descriptions of them, which is not
[00:10:02.500 --> 00:10:04.220]   how I would define it.
[00:10:04.220 --> 00:10:07.380]   In fact, I know that's not what I would go with.
[00:10:07.380 --> 00:10:11.060]   To use clip, you need access to a GPU and the OpenAI clip repository.
[00:10:11.060 --> 00:10:15.940]   Yes, you can do that, and you can use the provided scripts to train and evaluate the
[00:10:15.940 --> 00:10:16.940]   model.
[00:10:16.940 --> 00:10:19.140]   Additionally, you can use a so on and so on.
[00:10:19.140 --> 00:10:20.140]   Okay.
[00:10:20.140 --> 00:10:24.500]   It's mostly correct, except from the start, it's not really how I would describe clip,
[00:10:24.500 --> 00:10:27.340]   but then the rest about using the clip repository is correct.
[00:10:27.340 --> 00:10:33.500]   Now, I got a rate limit error, so let me try and comment this part out and try again.
[00:10:33.500 --> 00:10:34.500]   Okay.
[00:10:34.500 --> 00:10:37.460]   And what I wanted to get is this.
[00:10:37.460 --> 00:10:42.660]   So you can use OpenAI's clip easily by using the Hugging Face Transformers library, which
[00:10:42.660 --> 00:10:47.660]   in my opinion is 100% the easiest way to use the model.
[00:10:47.660 --> 00:10:52.860]   And then we get this, which some library for doing anything with NLP and computer vision.
[00:10:52.860 --> 00:10:57.180]   Not necessarily that standard with computer vision, but I think I know the source of information
[00:10:57.180 --> 00:10:59.620]   that's coming from, which is one of my videos.
[00:10:59.620 --> 00:11:03.180]   And I probably do say something along those lines, because that is what we're using clip
[00:11:03.180 --> 00:11:05.540]   for in this instance.
[00:11:05.540 --> 00:11:10.460]   And then to get started, you should install PyTorch and the Transformers and Datasets
[00:11:10.460 --> 00:11:17.420]   libraries, which is actually usually the case using a Dataset from Datasets.
[00:11:17.420 --> 00:11:20.460]   And you do need to install PyTorch with Transformers.
[00:11:20.460 --> 00:11:21.980]   So that is really cool.
[00:11:21.980 --> 00:11:23.620]   And let's ask one more question.
[00:11:23.620 --> 00:11:31.620]   I want to know what is a good, what is a good de facto model or sentence transformer model
[00:11:31.620 --> 00:11:34.940]   to use in semantic search?
[00:11:34.940 --> 00:11:36.660]   And let's see what we get.
[00:11:36.660 --> 00:11:42.060]   So in no augmentation, we get a popular de facto sentence transformer model for semantic
[00:11:42.060 --> 00:11:43.060]   search.
[00:11:43.060 --> 00:11:44.060]   It's BERT.
[00:11:44.060 --> 00:11:45.060]   It's a deep learning model.
[00:11:45.060 --> 00:11:46.980]   It's been pre-trained and so on and so on.
[00:11:46.980 --> 00:11:47.980]   Not actually.
[00:11:47.980 --> 00:11:53.740]   So here it seems like they're talking about the standard BERT model and not even the sentence
[00:11:53.740 --> 00:11:56.840]   transformer or bi-encoded version of BERT.
[00:11:56.840 --> 00:11:58.980]   So I would say it's definitely wrong.
[00:11:58.980 --> 00:12:00.860]   So I'm hitting a rate limit again.
[00:12:00.860 --> 00:12:04.700]   So let me comment this out and run it again.
[00:12:04.700 --> 00:12:05.700]   Okay.
[00:12:05.700 --> 00:12:06.700]   And here we go.
[00:12:06.700 --> 00:12:12.280]   So the pre-trained universal sentence encoder model is a good de facto sentence transformer
[00:12:12.280 --> 00:12:14.180]   model to use in semantic search.
[00:12:14.180 --> 00:12:16.220]   Now I would disagree with that.
[00:12:16.220 --> 00:12:21.740]   I think there are better models to use, but that is actually, I think one of the most
[00:12:21.740 --> 00:12:28.380]   popular ones to use as the sort of first sentence transformer that people end up using or sentence
[00:12:28.380 --> 00:12:31.420]   encoding model that people end up using.
[00:12:31.420 --> 00:12:37.380]   And this is a much more accurate answer than what we got before without the context, without
[00:12:37.380 --> 00:12:42.100]   the augmentation, which was BERT, which is not even a sentence transformer.
[00:12:42.100 --> 00:12:45.100]   So I think this is still a pretty good answer.
[00:12:45.100 --> 00:12:50.020]   Personally, I would like to see like an MPNet model or something on there, but that's actually
[00:12:50.020 --> 00:12:51.820]   more my personal preference.
[00:12:51.820 --> 00:12:55.940]   So I think this is probably a more broadly accepted answer.
[00:12:55.940 --> 00:13:01.940]   Okay, so as demonstrated, large language models do work incredibly well, particularly for
[00:13:01.940 --> 00:13:07.900]   general knowledge questions, but they definitely struggle with more niche or more specific
[00:13:07.900 --> 00:13:09.820]   pointed questions.
[00:13:09.820 --> 00:13:16.340]   And this typically leads to what we call hallucinations, which is where the model is basically spewing
[00:13:16.340 --> 00:13:19.240]   out things that are not true.
[00:13:19.240 --> 00:13:25.880]   And it's really obvious to the user that these models are being inaccurate in what they are
[00:13:25.880 --> 00:13:33.060]   saying because they can say very untruthful things very convincingly because these models,
[00:13:33.060 --> 00:13:37.740]   we can think of them as essentially masters of linguistic patterns.
[00:13:37.740 --> 00:13:42.980]   So they can say things that are completely false and say them in a way that makes them
[00:13:42.980 --> 00:13:44.500]   just seem true.
[00:13:44.500 --> 00:13:52.220]   So to protect us from this issue, we can add what we call a long term memory component
[00:13:52.220 --> 00:13:54.420]   to our GQA systems.
[00:13:54.420 --> 00:13:59.260]   And through this, we benefit from having an external knowledge base to improve system
[00:13:59.260 --> 00:14:03.820]   factuality and also improve user trust in the system.
[00:14:03.820 --> 00:14:10.340]   Naturally, there is a very vast potential for this type of technology.
[00:14:10.340 --> 00:14:14.900]   And despite being very new, there are already many people using it.
[00:14:14.900 --> 00:14:22.220]   I've seen that you.com have their new YouChat feature, which gives you natural language
[00:14:22.220 --> 00:14:25.160]   responses to your search queries.
[00:14:25.160 --> 00:14:30.300]   I've seen many podcast search apps recently using this technology.
[00:14:30.300 --> 00:14:36.980]   And there are even rumors of Microsoft with Bing using ChatGPT, which is another form
[00:14:36.980 --> 00:14:42.260]   of this technology as a challenger to Google itself.
[00:14:42.260 --> 00:14:49.620]   So as I think we can all see, there's very big potential and opportunity here for disruption
[00:14:49.620 --> 00:14:52.660]   within the space of information retrieval.
[00:14:52.660 --> 00:15:00.000]   Essentially any industry, any company that relies on information in some way and retrieving
[00:15:00.000 --> 00:15:06.100]   that information efficiently can benefit from the use of retrieval augmented generative
[00:15:06.100 --> 00:15:11.580]   question answering and other retrieval augmented generative AI technologies.
[00:15:11.580 --> 00:15:16.380]   So this really represents an opportunity for replacing some of those outdated information
[00:15:16.380 --> 00:15:19.900]   retrieval technologies that we use today.
[00:15:19.900 --> 00:15:22.180]   Now that's it for this video.
[00:15:22.180 --> 00:15:29.140]   I hope all of this has been somewhat thought provoking, interesting, and useful, but that's
[00:15:29.140 --> 00:15:30.140]   it for now.
[00:15:30.140 --> 00:15:35.140]   So thank you very much for watching and I will see you again in the next one.
[00:15:35.140 --> 00:15:35.140]   Bye.
[00:15:35.460 --> 00:15:35.960]   Bye.
[00:15:36.280 --> 00:15:36.780]   Bye.
[00:15:37.100 --> 00:15:37.600]   Bye.
[00:15:37.920 --> 00:15:38.420]   Bye.
[00:15:38.740 --> 00:15:39.240]   Bye.
[00:15:39.560 --> 00:15:40.060]   Bye.
[00:15:40.380 --> 00:15:40.880]   Bye.
[00:15:41.200 --> 00:15:41.700]   Bye.
[00:15:42.020 --> 00:15:43.020]   Bye.
[00:15:43.020 --> 00:15:44.020]   Bye.
[00:15:44.020 --> 00:15:45.020]   Bye.
[00:15:45.020 --> 00:15:46.020]   Bye.
[00:15:46.020 --> 00:15:47.020]   Bye.
[00:15:47.020 --> 00:15:48.020]   Bye.
[00:15:48.020 --> 00:15:49.020]   Bye.
[00:15:49.020 --> 00:15:50.020]   Bye.

