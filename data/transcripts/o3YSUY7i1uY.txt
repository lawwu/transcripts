
[00:00:00.000 --> 00:00:01.440]   I'm Lucas.
[00:00:01.440 --> 00:00:03.720]   So I run this company, Weights and Biases, that's here.
[00:00:03.720 --> 00:00:05.600]   And I've been friends with Josh and Sergey
[00:00:05.600 --> 00:00:07.800]   and some of the other folks for a while.
[00:00:07.800 --> 00:00:10.320]   And I've been really enjoying all the lectures.
[00:00:10.320 --> 00:00:12.720]   I'm enjoying all the projects folks are working on.
[00:00:12.720 --> 00:00:15.840]   And I really appreciate you all coming here weekly.
[00:00:15.840 --> 00:00:18.320]   It's fun to see you and fun to see all the stuff you're
[00:00:18.320 --> 00:00:18.840]   working on.
[00:00:18.840 --> 00:00:21.760]   So I just wanted to make sure you had my contact info.
[00:00:21.760 --> 00:00:23.320]   It's lucas@w&b.com.
[00:00:23.320 --> 00:00:26.680]   So if you have any questions or you
[00:00:26.680 --> 00:00:28.880]   want to talk about any of these topics,
[00:00:28.880 --> 00:00:33.160]   I would love to keep the conversation going.
[00:00:33.160 --> 00:00:34.960]   I want to talk about machine learning tools,
[00:00:34.960 --> 00:00:38.320]   and specifically the experiment management product
[00:00:38.320 --> 00:00:40.360]   that I built. But I know a lot of you
[00:00:40.360 --> 00:00:42.240]   are pretty entrepreneurial.
[00:00:42.240 --> 00:00:44.040]   And we've had some amazing entrepreneurs come in.
[00:00:44.040 --> 00:00:46.040]   So I kind of want to talk a little bit about how
[00:00:46.040 --> 00:00:48.400]   I thought about building this company
[00:00:48.400 --> 00:00:51.520]   and how I have seen the space evolve.
[00:00:51.520 --> 00:00:54.160]   Because my first company was called
[00:00:54.160 --> 00:00:56.440]   Figure Eight that actually started as CrowdFlower
[00:00:56.440 --> 00:01:00.720]   with Chris over there about 12 years ago that collects training
[00:01:00.720 --> 00:01:02.720]   data for machine learning.
[00:01:02.720 --> 00:01:06.160]   And I'm really trying to get you guys credits for Figure Eight.
[00:01:06.160 --> 00:01:07.840]   We actually just got acquired.
[00:01:07.840 --> 00:01:11.120]   So it's been a little tricky to navigate the bureaucracy
[00:01:11.120 --> 00:01:14.080]   that I built, which is painful.
[00:01:14.080 --> 00:01:14.880]   Painful for me.
[00:01:14.880 --> 00:01:15.960]   It kind of breaks my heart.
[00:01:15.960 --> 00:01:18.220]   But if any of you want to collect data on Figure Eight,
[00:01:18.220 --> 00:01:22.080]   I really, really want to help you do it on the platform.
[00:01:22.080 --> 00:01:23.160]   It would make me so happy.
[00:01:23.160 --> 00:01:25.280]   So I have a list of everybody's email address.
[00:01:25.280 --> 00:01:28.320]   And I'm begging the team to get you guys
[00:01:28.320 --> 00:01:31.280]   accounts and some credits in them.
[00:01:31.280 --> 00:01:33.280]   The thing I really can't control is
[00:01:33.280 --> 00:01:36.040]   we can definitely get you access to Weights and Biases, which
[00:01:36.040 --> 00:01:39.320]   is an experiment tracking platform that I built.
[00:01:39.320 --> 00:01:41.520]   Has anyone used it yet?
[00:01:41.520 --> 00:01:42.020]   Awesome.
[00:01:42.020 --> 00:01:42.600]   Thanks, guys.
[00:01:42.600 --> 00:01:43.520]   Awesome.
[00:01:43.520 --> 00:01:46.160]   And if you need any help at all, especially with that,
[00:01:46.160 --> 00:01:48.120]   please, please talk to me.
[00:01:48.120 --> 00:01:52.040]   I would love to help you out there.
[00:01:52.040 --> 00:01:55.360]   I wanted to talk about some of my favorite AI applications.
[00:01:55.360 --> 00:01:57.760]   Maybe Sergey could grade them if he has any.
[00:01:57.760 --> 00:01:58.840]   Sergey snuck off.
[00:01:58.840 --> 00:01:59.720]   Man, I wanted to--
[00:01:59.720 --> 00:02:01.060]   I was kind of thinking, how would
[00:02:01.060 --> 00:02:03.560]   I grade these applications?
[00:02:03.560 --> 00:02:06.880]   One super cool application is this microwave.
[00:02:06.880 --> 00:02:09.520]   [LAUGHTER]
[00:02:09.520 --> 00:02:12.120]   It's controversial in this company, but this microwave--
[00:02:12.120 --> 00:02:13.240]   Sergey, what do you think?
[00:02:13.240 --> 00:02:13.920]   Check this out.
[00:02:13.920 --> 00:02:16.000]   Where does this land on your scale?
[00:02:16.000 --> 00:02:19.040]   So it looks at your food, and then it
[00:02:19.040 --> 00:02:22.320]   cooks it perfectly, supposedly.
[00:02:22.320 --> 00:02:22.820]   So it--
[00:02:22.820 --> 00:02:23.320]   [INAUDIBLE]
[00:02:23.320 --> 00:02:24.700]   Yeah, I've watched on everything.
[00:02:24.700 --> 00:02:25.560]   Yes.
[00:02:25.560 --> 00:02:26.480]   Is that on videotape?
[00:02:26.480 --> 00:02:27.520]   We'll send it-- oh, man.
[00:02:27.520 --> 00:02:29.360]   Stacey's against it.
[00:02:29.360 --> 00:02:32.960]   I think Stacey has some meta marks,
[00:02:32.960 --> 00:02:35.320]   like environmental impact, which is good.
[00:02:35.320 --> 00:02:39.040]   But anyway, if you want to try it, it's in the room in there.
[00:02:39.040 --> 00:02:45.560]   I do think this could still be done.
[00:02:45.560 --> 00:02:47.800]   Here, so everybody talks about self-driving cars,
[00:02:47.800 --> 00:02:49.880]   but nobody talks about self-driving suitcases.
[00:02:49.880 --> 00:02:51.240]   [LAUGHTER]
[00:02:51.240 --> 00:02:53.600]   Which has this additional benefit.
[00:02:53.600 --> 00:02:55.440]   Well, actually, this is not a benefit at all.
[00:02:55.440 --> 00:02:56.560]   So there's this problem.
[00:02:56.560 --> 00:02:59.920]   I tried to get one, even though you actually probably
[00:02:59.920 --> 00:03:02.120]   can't take them--
[00:03:02.120 --> 00:03:04.080]   you can't carry these onto a plane, right?
[00:03:04.080 --> 00:03:05.880]   But it will follow you onto the plane.
[00:03:05.880 --> 00:03:07.840]   [LAUGHTER]
[00:03:07.840 --> 00:03:10.340]   Which I guess-- so yeah, I think this would get higher marks
[00:03:10.340 --> 00:03:12.920]   in the car for if it fails to follow you.
[00:03:12.920 --> 00:03:13.600]   Right?
[00:03:13.600 --> 00:03:16.280]   So you could-- like, human operator override
[00:03:16.280 --> 00:03:18.680]   is probably easier.
[00:03:18.680 --> 00:03:21.000]   If it fails to follow you, you get arrested.
[00:03:21.000 --> 00:03:23.600]   I also just imagine the training data is people's butts,
[00:03:23.600 --> 00:03:26.920]   which just seems kind of amazing.
[00:03:26.920 --> 00:03:28.720]   Like, recognizing you from behind,
[00:03:28.720 --> 00:03:29.880]   from a suitcase angle.
[00:03:29.880 --> 00:03:32.400]   [LAUGHTER]
[00:03:32.400 --> 00:03:34.200]   And then this-- oh, man.
[00:03:34.200 --> 00:03:37.680]   This, I think maybe vaporware breaks my heart,
[00:03:37.680 --> 00:03:38.680]   if this is vaporware.
[00:03:38.680 --> 00:03:40.680]   I ordered this about a year ago and still have not
[00:03:40.680 --> 00:03:42.560]   received it.
[00:03:42.560 --> 00:03:44.360]   But maybe, what if you want to make this?
[00:03:44.360 --> 00:03:45.800]   I would be so-- maybe--
[00:03:45.800 --> 00:03:47.480]   Stacey might prefer this.
[00:03:47.480 --> 00:03:49.040]   We all-- environmental impact.
[00:03:49.040 --> 00:03:52.000]   Although, I question-- I question the environmental
[00:03:52.000 --> 00:03:52.560]   impact.
[00:03:52.560 --> 00:03:53.600]   But it is super cool.
[00:03:53.600 --> 00:03:55.960]   So according to her video, it looks at the trash
[00:03:55.960 --> 00:03:58.260]   as you put it in and then decides to put it in recycling
[00:03:58.260 --> 00:04:00.680]   or not recycling.
[00:04:00.680 --> 00:04:02.800]   So maybe cool.
[00:04:02.800 --> 00:04:05.400]   There's actually another-- this is one that I made recently
[00:04:05.400 --> 00:04:09.000]   that I'm really proud of that only works in my demo video.
[00:04:09.000 --> 00:04:11.080]   Also kind of vaporware, but I'll sell you one.
[00:04:11.080 --> 00:04:13.320]   [LAUGHTER]
[00:04:13.320 --> 00:04:15.520]   So it's one of these--
[00:04:15.520 --> 00:04:16.440]   we have one of these.
[00:04:16.440 --> 00:04:18.320]   So that's like a radiant heater.
[00:04:18.320 --> 00:04:21.280]   And so this is actually-- it's actually--
[00:04:21.280 --> 00:04:24.320]   space heaters, about 10 times as efficient as a radiant heater,
[00:04:24.320 --> 00:04:26.360]   because the radiant heater beams the heat at you.
[00:04:26.360 --> 00:04:28.520]   So sometimes it follows me.
[00:04:28.520 --> 00:04:31.080]   And I cut the video precisely to make sure
[00:04:31.080 --> 00:04:33.280]   that it only shows the part where it follows me.
[00:04:33.280 --> 00:04:35.080]   [LAUGHTER]
[00:04:35.080 --> 00:04:35.580]   What's that?
[00:04:35.580 --> 00:04:36.920]   Does that feel creepy?
[00:04:36.920 --> 00:04:38.320]   No, it doesn't feel creepy at all.
[00:04:38.320 --> 00:04:41.120]   It feels wonderful, because it's warming you, like the sun.
[00:04:41.120 --> 00:04:42.000]   It's amazing.
[00:04:42.000 --> 00:04:44.840]   [LAUGHTER]
[00:04:44.840 --> 00:04:46.460]   I mean, it might feel different to you,
[00:04:46.460 --> 00:04:50.560]   but it's like face recognition for my benefit feels amazing.
[00:04:50.560 --> 00:04:51.360]   For my warmth?
[00:04:51.360 --> 00:04:53.040]   I don't know.
[00:04:53.040 --> 00:04:57.280]   Anyway, I've been really enjoying your projects.
[00:04:57.280 --> 00:04:59.400]   I'm really excited to see them.
[00:04:59.400 --> 00:05:02.080]   I think one of the things that's really cool right now is there's
[00:05:02.080 --> 00:05:06.360]   so many more possible applications than people
[00:05:06.360 --> 00:05:08.040]   to make applications that kind of know
[00:05:08.040 --> 00:05:12.440]   how to use this technology that I think this is the best way
[00:05:12.440 --> 00:05:15.720]   to democratize AI is for you all to go out and make
[00:05:15.720 --> 00:05:16.480]   really cool stuff.
[00:05:16.480 --> 00:05:19.280]   We'd love to support that.
[00:05:19.280 --> 00:05:21.200]   So I want to go through some of the challenges
[00:05:21.200 --> 00:05:23.760]   that I see, because I've been watching this space for 10,
[00:05:23.760 --> 00:05:25.640]   12 years and just seeing the same challenges.
[00:05:25.640 --> 00:05:26.960]   And they don't seem to go away.
[00:05:26.960 --> 00:05:29.600]   And people really work hard to overcome them.
[00:05:29.600 --> 00:05:33.080]   I think one big issue from a bureaucratic perspective
[00:05:33.080 --> 00:05:36.000]   is that as doing machine learning,
[00:05:36.000 --> 00:05:37.800]   you look a lot like engineers.
[00:05:37.800 --> 00:05:40.560]   But in fact, a lot of things are different.
[00:05:40.560 --> 00:05:43.400]   So the code is incomprehensible.
[00:05:43.400 --> 00:05:44.680]   It doesn't version well.
[00:05:44.680 --> 00:05:48.240]   It's much bigger than something you might check into GitHub.
[00:05:48.240 --> 00:05:51.400]   And actually, your whole workflow is different.
[00:05:51.400 --> 00:05:53.560]   And I think everybody's had a different take on it.
[00:05:53.560 --> 00:05:54.640]   Surya had a slide on this.
[00:05:54.640 --> 00:05:56.920]   Almost everybody that has come in and spoke, I think,
[00:05:56.920 --> 00:06:00.440]   unprompted has actually had some variation of this,
[00:06:00.440 --> 00:06:03.600]   where there's clearly a data piece in the beginning,
[00:06:03.600 --> 00:06:06.280]   like a model building piece, and then a deployment piece.
[00:06:06.280 --> 00:06:08.680]   And I think you're kind of underserved
[00:06:08.680 --> 00:06:11.760]   by the different classes that we have, as awesome as--
[00:06:11.760 --> 00:06:14.480]   there's so much good online material about the learning
[00:06:14.480 --> 00:06:15.800]   algorithm piece.
[00:06:15.800 --> 00:06:17.460]   And there's just so much less material.
[00:06:17.460 --> 00:06:20.880]   And I think that the promise of the supplied deep learning
[00:06:20.880 --> 00:06:23.880]   class is to help you with the end-to-end process.
[00:06:23.880 --> 00:06:25.800]   Because if you can't get good data,
[00:06:25.800 --> 00:06:27.340]   you can't do machine learning right.
[00:06:27.340 --> 00:06:30.560]   And if you can't deploy it and monitor it, it's like, what's--
[00:06:30.560 --> 00:06:31.420]   that's just a toy.
[00:06:31.420 --> 00:06:34.240]   It's not a useful thing.
[00:06:34.240 --> 00:06:36.680]   So this workflow, I think, is fundamentally different.
[00:06:36.680 --> 00:06:39.160]   And what this means is that there's a whole new set
[00:06:39.160 --> 00:06:41.000]   of tools needed.
[00:06:41.000 --> 00:06:43.800]   And the tools, I kind of group them around the challenges
[00:06:43.800 --> 00:06:44.440]   that we face.
[00:06:44.440 --> 00:06:46.440]   And so the first one is machine learning
[00:06:46.440 --> 00:06:47.440]   requires training data.
[00:06:47.440 --> 00:06:50.120]   And this is something I noticed many, many years ago.
[00:06:50.120 --> 00:06:52.400]   My first job is just collecting training data.
[00:06:52.400 --> 00:06:55.480]   Actually, Peter Norvig-- this is one of the best papers ever
[00:06:55.480 --> 00:06:58.060]   on machine learning-- I think it's the unreasonable effectiveness
[00:06:58.060 --> 00:06:58.560]   of data.
[00:06:58.560 --> 00:07:00.760]   And it's actually aged beautifully,
[00:07:00.760 --> 00:07:04.480]   unlike some papers, which you'd say, 15 years later,
[00:07:04.480 --> 00:07:05.080]   not relevant.
[00:07:05.080 --> 00:07:06.520]   I think this is even more relevant,
[00:07:06.520 --> 00:07:11.320]   where basically Peter Norvig points out that you basically--
[00:07:11.320 --> 00:07:14.360]   the applications that work are applications
[00:07:14.360 --> 00:07:16.400]   where there's a lot of training data available.
[00:07:16.400 --> 00:07:18.440]   He points out at the time, this was 15 years ago,
[00:07:18.440 --> 00:07:20.680]   I think that's still really true.
[00:07:20.680 --> 00:07:23.160]   He also has been posting on Facebook
[00:07:23.160 --> 00:07:25.960]   how ML tools companies are stupid, which I just totally
[00:07:25.960 --> 00:07:29.360]   don't understand and disagree with.
[00:07:29.360 --> 00:07:31.440]   But he's a smart guy.
[00:07:31.440 --> 00:07:33.320]   So you get these effects, right?
[00:07:33.320 --> 00:07:36.040]   This is what me and Chris were looking at 12 years ago,
[00:07:36.040 --> 00:07:39.520]   when we started a company, which is that better algorithms get
[00:07:39.520 --> 00:07:41.040]   incremental improvements.
[00:07:41.040 --> 00:07:43.920]   Better features, or better data engineering
[00:07:43.920 --> 00:07:46.160]   before you feed into the model, gets more improvement,
[00:07:46.160 --> 00:07:49.400]   but it's really difficult to put a process around.
[00:07:49.400 --> 00:07:53.200]   But then more data often just gets you huge improvements.
[00:07:53.200 --> 00:07:56.760]   And actually, cleaner data, even less studied,
[00:07:56.760 --> 00:07:58.880]   can really get you big improvements.
[00:07:58.880 --> 00:08:01.640]   And I think you'll feel-- anyone who's worked on machine learning
[00:08:01.640 --> 00:08:02.600]   is nodding their head.
[00:08:02.600 --> 00:08:04.880]   But this is actually a surprise, I think,
[00:08:04.880 --> 00:08:06.760]   to everyone when they first go through it.
[00:08:06.760 --> 00:08:08.340]   Because there's not, as far as I know,
[00:08:08.340 --> 00:08:12.400]   a class at Stanford on collecting data well.
[00:08:12.400 --> 00:08:15.120]   And really, if we were optimizing
[00:08:15.120 --> 00:08:17.000]   for high-quality deployed models,
[00:08:17.000 --> 00:08:21.120]   half the classes would be on collecting data well.
[00:08:21.120 --> 00:08:23.200]   Because the other thing that you really can't do
[00:08:23.200 --> 00:08:25.000]   is toss it over the fence to someone else
[00:08:25.000 --> 00:08:27.560]   to collect the data for you.
[00:08:27.560 --> 00:08:31.000]   I think Andrej Karpathy put this best.
[00:08:31.000 --> 00:08:34.320]   So he's the person running machine learning at Tesla,
[00:08:34.320 --> 00:08:36.360]   trying to make the autopilot work well.
[00:08:36.360 --> 00:08:40.240]   And he's saying that he spends most of his time
[00:08:40.240 --> 00:08:42.400]   on collecting data sets.
[00:08:42.400 --> 00:08:43.760]   I actually saw him the other day,
[00:08:43.760 --> 00:08:45.180]   and he was basically saying, I now
[00:08:45.180 --> 00:08:47.920]   spend all my time on data set collection.
[00:08:47.920 --> 00:08:50.280]   I'm not doing any modeling, basically.
[00:08:50.280 --> 00:08:52.600]   There's other folks doing it now.
[00:08:52.600 --> 00:08:56.400]   So I think that this is--
[00:08:56.400 --> 00:08:59.000]   I think that Andrej Karpathy's allocation is actually
[00:08:59.000 --> 00:09:01.160]   a pretty good allocation in most cases,
[00:09:01.160 --> 00:09:02.840]   trying to make models work well.
[00:09:02.840 --> 00:09:04.800]   And it's been really interesting, actually,
[00:09:04.800 --> 00:09:08.520]   to see when I started CrowdFlower,
[00:09:08.520 --> 00:09:11.560]   reading and figuring out that we had basically no competitors.
[00:09:11.560 --> 00:09:13.760]   Or none of our competitors really thought of themselves
[00:09:13.760 --> 00:09:14.640]   as a competitor.
[00:09:14.640 --> 00:09:17.000]   They didn't think of themselves as training data collection
[00:09:17.000 --> 00:09:17.600]   companies.
[00:09:17.600 --> 00:09:18.960]   Now there's actually a whole bunch
[00:09:18.960 --> 00:09:22.280]   of competitors with really interesting takes.
[00:09:22.280 --> 00:09:24.240]   But you should use figure eight in this class
[00:09:24.240 --> 00:09:24.960]   for your projects.
[00:09:24.960 --> 00:09:28.040]   [LAUGHTER]
[00:09:28.040 --> 00:09:32.200]   Another thing is difficulty is super hard to predict.
[00:09:32.200 --> 00:09:33.640]   And I think that this is something
[00:09:33.640 --> 00:09:36.160]   that also everyone's talked about.
[00:09:36.160 --> 00:09:37.660]   It's been really interesting to see.
[00:09:37.660 --> 00:09:40.360]   So I think Josh had some really smart stuff to say on this,
[00:09:40.360 --> 00:09:41.520]   how to predict difficulty.
[00:09:41.520 --> 00:09:43.680]   But I think everyone that's talked to you in different ways
[00:09:43.680 --> 00:09:44.960]   has looked into this.
[00:09:44.960 --> 00:09:47.580]   And I always point to Kaggle competitions,
[00:09:47.580 --> 00:09:48.640]   because it's data.
[00:09:48.640 --> 00:09:51.600]   But I did a competition on data that I knew.
[00:09:51.600 --> 00:09:54.060]   And in the first three days, you get this improvement going
[00:09:54.060 --> 00:09:57.000]   like 35% to 50%.
[00:09:57.000 --> 00:09:58.760]   It feels really good.
[00:09:58.760 --> 00:10:02.560]   And then totally flattened out over the next month or two.
[00:10:02.560 --> 00:10:05.640]   And I was thinking, OK, if I was managing a team,
[00:10:05.640 --> 00:10:07.840]   or even if I was doing this project,
[00:10:07.840 --> 00:10:10.200]   it would be super frustrating, because you get all this
[00:10:10.200 --> 00:10:11.480]   momentum right at the beginning.
[00:10:11.480 --> 00:10:13.040]   And then it completely flattens out.
[00:10:13.040 --> 00:10:15.040]   And the effort was actually skyrocketing.
[00:10:15.040 --> 00:10:18.440]   Because anybody ever do a Kaggle competition?
[00:10:18.440 --> 00:10:19.360]   Yeah?
[00:10:19.360 --> 00:10:20.960]   Yeah, so it's like towards the end,
[00:10:20.960 --> 00:10:22.400]   people get excited because they're like, OK,
[00:10:22.400 --> 00:10:25.080]   we're going to get that prize.
[00:10:25.080 --> 00:10:28.560]   And so more and more people were competing
[00:10:28.560 --> 00:10:32.320]   and doing really smart things over less and less value,
[00:10:32.320 --> 00:10:34.160]   which is interesting.
[00:10:34.160 --> 00:10:36.000]   And then you look at these self-driving cars.
[00:10:36.000 --> 00:10:40.120]   And I think people just can't help themselves,
[00:10:40.120 --> 00:10:44.560]   but look at a graph like this of miles per disengage
[00:10:44.560 --> 00:10:47.160]   and make crazy statements.
[00:10:47.160 --> 00:10:48.520]   Josh had the same slide.
[00:10:48.520 --> 00:10:51.280]   I just think the slide needs to be pounded on.
[00:10:51.280 --> 00:10:53.000]   I mean, this is just--
[00:10:53.000 --> 00:10:54.640]   look, this is 2015.
[00:10:54.640 --> 00:10:57.400]   This is happening.
[00:10:57.400 --> 00:10:59.200]   But it's just such human nature.
[00:10:59.200 --> 00:11:01.080]   Even people have seen this stuff over and over
[00:11:01.080 --> 00:11:03.160]   to get confused by this.
[00:11:03.160 --> 00:11:07.000]   And then people talk about machine learning
[00:11:07.000 --> 00:11:08.440]   is unpredictable and opaque.
[00:11:08.440 --> 00:11:10.440]   And I've had a lot of different examples of this.
[00:11:10.440 --> 00:11:12.720]   But I had a personal example recently
[00:11:12.720 --> 00:11:17.600]   that really brought this home to me, where I--
[00:11:17.600 --> 00:11:19.680]   I write a Medium post about machine learning.
[00:11:19.680 --> 00:11:22.000]   And I read a lot of posts about machine learning.
[00:11:22.000 --> 00:11:24.920]   And this is typical content I write as recommended.
[00:11:24.920 --> 00:11:27.640]   And then I got this recommendation from Medium.
[00:11:27.640 --> 00:11:29.280]   The short dudes need love, too.
[00:11:29.280 --> 00:11:33.360]   And it's just like, man, how did it come up with that?
[00:11:33.360 --> 00:11:34.560]   Of course, I clicked on it.
[00:11:34.560 --> 00:11:36.160]   So I fed the beast.
[00:11:36.160 --> 00:11:40.960]   But it's just like, I really wanted explainable AI.
[00:11:40.960 --> 00:11:43.040]   It's so badly in this moment.
[00:11:43.040 --> 00:11:44.800]   But I still want it.
[00:11:44.800 --> 00:11:51.240]   So another thing is that it can feel super non-deterministic.
[00:11:51.240 --> 00:11:53.920]   And I think that this is another thing that people working on it
[00:11:53.920 --> 00:11:56.360]   really feel, where code feels very deterministic generally.
[00:11:56.360 --> 00:11:58.720]   You can just rerun the same code twice.
[00:11:58.720 --> 00:12:00.880]   And you get the same results.
[00:12:00.880 --> 00:12:03.400]   But then people have talked about this machine learning
[00:12:03.400 --> 00:12:04.400]   reproducibility crisis.
[00:12:04.400 --> 00:12:05.560]   And it's such an issue.
[00:12:05.560 --> 00:12:07.360]   You get code off GitHub.
[00:12:07.360 --> 00:12:08.960]   And you can run it.
[00:12:08.960 --> 00:12:10.560]   If you get machine learning code,
[00:12:10.560 --> 00:12:11.600]   it's going to take you a long time
[00:12:11.600 --> 00:12:13.600]   to even reproduce one thing that the person did.
[00:12:13.600 --> 00:12:16.360]   But to do all these different things will take so much time.
[00:12:16.360 --> 00:12:20.560]   And that's why we really got excited about building
[00:12:20.560 --> 00:12:21.880]   weights and biases.
[00:12:21.880 --> 00:12:23.840]   When we were thinking about--
[00:12:23.840 --> 00:12:27.200]   even going back for ourselves one week or one month earlier
[00:12:27.200 --> 00:12:31.640]   and trying to figure out what we did was incredibly painful.
[00:12:31.640 --> 00:12:33.240]   And my friend actually was showing me
[00:12:33.240 --> 00:12:35.280]   his notes of the different models that he ran.
[00:12:35.280 --> 00:12:36.940]   And he actually had this Google document
[00:12:36.940 --> 00:12:39.440]   that was thousands of pages of this, where it was like,
[00:12:39.440 --> 00:12:40.840]   this is the first thing he tried.
[00:12:40.840 --> 00:12:42.440]   And this is the second thing he tried.
[00:12:42.440 --> 00:12:46.040]   And this is so far from reproducible.
[00:12:46.040 --> 00:12:50.240]   I just-- it blows my mind that he was thinking
[00:12:50.240 --> 00:12:51.360]   he could go back to this.
[00:12:51.360 --> 00:12:55.200]   And rerun something.
[00:12:55.200 --> 00:12:57.400]   So I think what you get from this,
[00:12:57.400 --> 00:13:00.240]   and I think what's really exciting to me
[00:13:00.240 --> 00:13:03.320]   is new workflows and new tools.
[00:13:03.320 --> 00:13:05.920]   And obviously, I'm incredibly biased.
[00:13:05.920 --> 00:13:07.800]   But I'm also looking at other tools.
[00:13:07.800 --> 00:13:10.520]   And I think it's important for you to know what
[00:13:10.520 --> 00:13:12.160]   the good tooling out there is.
[00:13:12.160 --> 00:13:15.440]   So one place that I think you really
[00:13:15.440 --> 00:13:19.360]   should be looking, practicing in the space, is Siobhan's machine
[00:13:19.360 --> 00:13:20.680]   intelligence slide.
[00:13:20.680 --> 00:13:23.320]   So I think it's just interesting to look at her first one, which
[00:13:23.320 --> 00:13:25.440]   I think was very good and a really nice framing
[00:13:25.440 --> 00:13:26.160]   of the industry.
[00:13:26.160 --> 00:13:28.720]   And her second one is getting a little more crowded.
[00:13:28.720 --> 00:13:32.760]   And then her third one is getting intense.
[00:13:32.760 --> 00:13:37.880]   But you can-- her website, siobhanzulis.com,
[00:13:37.880 --> 00:13:39.600]   is actually just an amazing place
[00:13:39.600 --> 00:13:42.760]   to see the entire third-party take
[00:13:42.760 --> 00:13:46.280]   on the taxonomy of tools and then
[00:13:46.280 --> 00:13:47.520]   applications of those tools.
[00:13:47.520 --> 00:13:51.920]   And I even think the diff of her 2.0 versus 3.0
[00:13:51.920 --> 00:13:55.800]   is a good place to see where momentum's happening
[00:13:55.800 --> 00:13:58.520]   and where interesting applications are taking off
[00:13:58.520 --> 00:14:04.040]   and where interesting back-end tools are evolving.
[00:14:04.040 --> 00:14:05.480]   There's actually another-- I think
[00:14:05.480 --> 00:14:07.680]   just if you're interested in tools at all,
[00:14:07.680 --> 00:14:10.920]   there's a really, really smart VC at a Redpoint
[00:14:10.920 --> 00:14:12.280]   that I have no affiliation with.
[00:14:12.280 --> 00:14:15.080]   I just think she's actually writing really excellent stuff
[00:14:15.080 --> 00:14:17.200]   right now, where she has her own--
[00:14:17.200 --> 00:14:19.320]   this is a little bit more of an investor landscape.
[00:14:19.320 --> 00:14:20.740]   But I think for those of you that
[00:14:20.740 --> 00:14:23.360]   are really thinking from an investment perspective,
[00:14:23.360 --> 00:14:26.040]   or if you're thinking of pitching VCs on an AI thing
[00:14:26.040 --> 00:14:31.840]   in any way, I would read her stuff, because it's really good.
[00:14:31.840 --> 00:14:33.800]   But these are the--
[00:14:33.800 --> 00:14:36.280]   I think these folks are trying to give you
[00:14:36.280 --> 00:14:38.560]   a complete picture of everything out there.
[00:14:38.560 --> 00:14:42.200]   I think what I want to do briefly is zoom out a little bit
[00:14:42.200 --> 00:14:44.440]   and just give you my quick taxonomy, which
[00:14:44.440 --> 00:14:48.480]   would be very similar, I think, to the stuff
[00:14:48.480 --> 00:14:49.440]   that Sergey was saying.
[00:14:49.440 --> 00:14:51.840]   So one is that clearly there's a point-solution
[00:14:51.840 --> 00:14:54.400]   versus end-to-end dichotomy.
[00:14:54.400 --> 00:14:57.320]   And I, myself, and I think a lot of people
[00:14:57.320 --> 00:14:59.320]   with a little bit more of a hacker mentality,
[00:14:59.320 --> 00:15:01.600]   I think, tend to prefer point-solutions.
[00:15:01.600 --> 00:15:04.280]   So it's really important to me that Weights and Biases
[00:15:04.280 --> 00:15:08.800]   builds point-solutions, because that's what I would want.
[00:15:08.800 --> 00:15:13.440]   But I do think people in more regulated industries
[00:15:13.440 --> 00:15:16.400]   tend to seem to prefer more end-to-end platforms.
[00:15:16.400 --> 00:15:17.680]   Has anybody used Domino Data?
[00:15:17.680 --> 00:15:18.640]   Any of you?
[00:15:18.640 --> 00:15:19.880]   Yeah?
[00:15:19.880 --> 00:15:21.280]   How do you feel about Domino Data?
[00:15:21.280 --> 00:15:22.240]   Let's crowdsource this.
[00:15:22.240 --> 00:15:24.160]   I used it very briefly.
[00:15:24.160 --> 00:15:25.400]   I know some people there.
[00:15:25.400 --> 00:15:31.400]   One of the concerns that I would have had was not just--
[00:15:31.400 --> 00:15:33.680]   if I were to do a project working by myself,
[00:15:33.680 --> 00:15:34.680]   maybe it would work.
[00:15:34.680 --> 00:15:36.120]   But it would be really hard--
[00:15:36.120 --> 00:15:37.800]   there's no incremental adoption option.
[00:15:37.800 --> 00:15:39.440]   So if I was working with a team that
[00:15:39.440 --> 00:15:41.800]   had a bunch of tools that they were already using,
[00:15:41.800 --> 00:15:42.960]   it would be really hard to convince everyone
[00:15:42.960 --> 00:15:45.000]   to drop all of those things and convert over.
[00:15:45.000 --> 00:15:45.960]   Right, right, right, right.
[00:15:45.960 --> 00:15:49.400]   Yeah, I think that's the trade-off.
[00:15:49.400 --> 00:15:51.200]   I actually really admire Domino Data.
[00:15:51.200 --> 00:15:54.760]   I think the issue, though, is you really, really
[00:15:54.760 --> 00:15:56.320]   have to get everyone to commit.
[00:15:56.320 --> 00:15:58.120]   And that feels really hard to do.
[00:15:58.120 --> 00:16:02.140]   I think even FB Learner, which is Facebook's tool, which
[00:16:02.140 --> 00:16:06.040]   is well-known for having broad adoption inside Facebook,
[00:16:06.040 --> 00:16:08.840]   it's unique in that it's a company where people really
[00:16:08.840 --> 00:16:09.500]   committed to it.
[00:16:09.500 --> 00:16:10.880]   And actually, we were over there,
[00:16:10.880 --> 00:16:14.040]   and it seemed like they were still rounding up people
[00:16:14.040 --> 00:16:15.200]   to get them to use it.
[00:16:15.200 --> 00:16:17.720]   So the problem with these tools is
[00:16:17.720 --> 00:16:19.560]   that when you don't have complete adoption,
[00:16:19.560 --> 00:16:21.400]   it can be really difficult to use them.
[00:16:21.400 --> 00:16:24.920]   Actually, at Figure 8, we used Kubeflow.
[00:16:24.920 --> 00:16:27.320]   And it had that same issue, where it was just like,
[00:16:27.320 --> 00:16:29.800]   if you don't completely buy into it, it can be tough.
[00:16:29.800 --> 00:16:31.720]   So I would not necessarily--
[00:16:31.720 --> 00:16:35.680]   my take would be for your projects,
[00:16:35.680 --> 00:16:37.400]   I would not start with an end-to-end tool
[00:16:37.400 --> 00:16:39.100]   unless you really want to learn about it,
[00:16:39.100 --> 00:16:42.820]   because they're all really designed for companies
[00:16:42.820 --> 00:16:46.260]   where you get complete buy-in for the tool.
[00:16:46.260 --> 00:16:48.100]   And that's where you get the real benefits.
[00:16:48.100 --> 00:16:50.520]   So I think point solutions are going to be better for you.
[00:16:50.520 --> 00:16:52.300]   I think paper space is a useful thing.
[00:16:52.300 --> 00:16:55.860]   SIGOPT is the hyperparameter optimization tool.
[00:16:55.860 --> 00:16:57.540]   Packaderm is a data set versioning tool.
[00:16:57.540 --> 00:17:00.780]   So I think picking and choosing tools makes a ton of sense.
[00:17:00.780 --> 00:17:04.020]   And I always am curious about the stack
[00:17:04.020 --> 00:17:05.780]   that people are actually using, because I
[00:17:05.780 --> 00:17:07.780]   think the space is so fluid right now
[00:17:07.780 --> 00:17:10.900]   that it has not converged around--
[00:17:10.900 --> 00:17:12.780]   it hasn't even converged around frameworks.
[00:17:12.780 --> 00:17:15.660]   So I would love to hear what you use
[00:17:15.660 --> 00:17:18.140]   and what you think is useful.
[00:17:18.140 --> 00:17:19.700]   We actually-- at Figure 8, we put out
[00:17:19.700 --> 00:17:22.960]   a survey of what people are using every year.
[00:17:22.960 --> 00:17:24.380]   And so if you go to that website,
[00:17:24.380 --> 00:17:27.500]   you can actually just see a ranking of Figure 8's customers,
[00:17:27.500 --> 00:17:29.380]   essentially machine learning practitioners
[00:17:29.380 --> 00:17:33.700]   and what has general adoption.
[00:17:33.700 --> 00:17:36.340]   I think what I try to think about for my tools,
[00:17:36.340 --> 00:17:38.860]   I think, is less in the infrastructure and frameworks
[00:17:38.860 --> 00:17:40.580]   realm and more in the dev tools thing.
[00:17:40.580 --> 00:17:42.060]   Figure 8 was a training data thing.
[00:17:42.060 --> 00:17:43.620]   I think weights and biases tries to be
[00:17:43.620 --> 00:17:46.620]   in the more learning algorithm evaluation, basically
[00:17:46.620 --> 00:17:49.380]   it's an experiment tracking thing.
[00:17:49.380 --> 00:17:51.660]   And so that's, I think, where I like to--
[00:17:51.660 --> 00:17:53.900]   I think the infrastructure stuff is really cool
[00:17:53.900 --> 00:17:57.100]   and actually much more evolved and settled.
[00:17:57.100 --> 00:18:03.180]   So I think SageMaker is a great-- anybody use SageMaker?
[00:18:03.180 --> 00:18:04.740]   How do you feel about SageMaker?
[00:18:05.740 --> 00:18:06.740]   [INAUDIBLE]
[00:18:06.740 --> 00:18:13.720]   What's that?
[00:18:13.720 --> 00:18:14.720]   It was easy to set up.
[00:18:14.720 --> 00:18:15.720]   Easy to set up.
[00:18:15.720 --> 00:18:16.720]   [INAUDIBLE]
[00:18:16.720 --> 00:18:21.220]   Cool.
[00:18:21.220 --> 00:18:22.720]   [INAUDIBLE]
[00:18:22.720 --> 00:18:23.220]   Nice.
[00:18:23.220 --> 00:18:23.720]   Yeah, yeah.
[00:18:23.720 --> 00:18:26.700]   I think SageMaker, I would say, of all the--
[00:18:26.700 --> 00:18:28.740]   I mean, they don't pay me.
[00:18:28.740 --> 00:18:30.740]   But I think SageMaker is just-- it's probably
[00:18:30.740 --> 00:18:34.820]   the one kind of ML infrastructure thing
[00:18:34.820 --> 00:18:37.700]   that we probably see the most, just generally going
[00:18:37.700 --> 00:18:38.780]   into companies right now.
[00:18:38.780 --> 00:18:41.140]   But of course, that can change quickly.
[00:18:41.140 --> 00:18:43.260]   And then you see these frameworks like, clearly,
[00:18:43.260 --> 00:18:44.640]   TensorFlow, Python, and Keras are
[00:18:44.640 --> 00:18:49.780]   kind of the top most common frameworks that we encounter.
[00:18:49.780 --> 00:18:53.140]   So I thought the most fun thing to do really since--
[00:18:53.140 --> 00:18:55.940]   I think Sergey gave such a comprehensive overview
[00:18:55.940 --> 00:18:57.440]   of all the different tools would be,
[00:18:57.440 --> 00:18:59.780]   I just want to show you the tools that we built
[00:18:59.780 --> 00:19:02.260]   and what they do and actually play a little bit
[00:19:02.260 --> 00:19:03.260]   with this stuff quickly.
[00:19:03.260 --> 00:19:06.220]   So if you would open up your laptop
[00:19:06.220 --> 00:19:15.460]   and go to app.wmb.ai/settings, that would be fantastic.
[00:19:15.460 --> 00:19:18.940]   And you can maybe even do it on your iPad.
[00:19:18.940 --> 00:19:19.440]   Maybe.
[00:19:19.440 --> 00:19:25.340]   And I got this friend, this co-founder, Chris,
[00:19:25.340 --> 00:19:26.820]   right over there.
[00:19:26.820 --> 00:19:29.220]   And he just loves giving tech support.
[00:19:29.220 --> 00:19:30.740]   It's like his favorite thing to do.
[00:19:30.740 --> 00:19:34.860]   So if you have any problems at all,
[00:19:34.860 --> 00:19:37.500]   like you think you might have typed in the URL wrong,
[00:19:37.500 --> 00:19:40.940]   just raise your hand.
[00:19:40.940 --> 00:19:41.780]   Chris has got you.
[00:19:41.780 --> 00:19:43.620]   You need like antivirus software?
[00:19:43.620 --> 00:19:46.020]   [LAUGHTER]
[00:19:46.020 --> 00:19:52.260]   If you don't have an account, you
[00:19:52.260 --> 00:19:54.420]   just have to sign up with your GitHub or Google
[00:19:54.420 --> 00:19:56.420]   or you create an account.
[00:19:56.420 --> 00:19:59.860]   But then if you put this code in at that URL,
[00:19:59.860 --> 00:20:03.820]   once you do have an account, then that gives you--
[00:20:03.820 --> 00:20:05.740]   keep that code up.
[00:20:05.740 --> 00:20:07.580]   Oh, yeah, I got to keep the code up.
[00:20:07.580 --> 00:20:09.420]   Simpsons Stochastic Base.
[00:20:09.420 --> 00:20:10.960]   Every day we choose a different code
[00:20:10.960 --> 00:20:13.060]   that is a combination of a machine learning
[00:20:13.060 --> 00:20:15.660]   word and a musical instrument.
[00:20:15.660 --> 00:20:18.300]   The Simpsons part tells us to check out
[00:20:18.300 --> 00:20:20.700]   this really sick machine learning repo
[00:20:20.700 --> 00:20:21.900]   that we set up for you.
[00:20:21.900 --> 00:20:24.780]   It's a very basic multi-layer perceptron
[00:20:24.780 --> 00:20:28.540]   to classify images of Simpsons characters.
[00:20:28.540 --> 00:20:31.980]   And we're going to have a brief competition
[00:20:31.980 --> 00:20:36.260]   to see who can make the best model fastest.
[00:20:36.260 --> 00:20:41.940]   We're going to do hyperparameter search by crowdsourcing.
[00:20:41.940 --> 00:20:45.340]   [LAUGHTER]
[00:20:45.340 --> 00:20:47.060]   All right, so here's the code that you run,
[00:20:47.060 --> 00:20:49.260]   Python space train.py.
[00:20:49.260 --> 00:20:51.780]   We'll run this little Keras classifier.
[00:20:51.780 --> 00:20:55.660]   [INAUDIBLE]
[00:20:55.660 --> 00:20:56.340]   So you see that?
[00:20:56.340 --> 00:20:58.980]   So if you go to File, and then New, and then Terminal,
[00:20:58.980 --> 00:21:00.900]   you get a terminal.
[00:21:00.900 --> 00:21:01.540]   And this will--
[00:21:01.540 --> 00:21:05.940]   [INAUDIBLE]
[00:21:05.940 --> 00:21:08.780]   So this is starting to train my--
[00:21:08.780 --> 00:21:12.060]   this is starting to train Chris's little Simpsons
[00:21:12.060 --> 00:21:15.620]   character face recognition classifier.
[00:21:15.620 --> 00:21:22.260]   So the code that ran is here.
[00:21:22.260 --> 00:21:27.060]   And so the first piece is that there's a couple hyperparameters
[00:21:27.060 --> 00:21:31.180]   that we set that you may want to modify.
[00:21:31.180 --> 00:21:32.940]   And then there's downloading, and then we
[00:21:32.940 --> 00:21:34.620]   make these Keras image data generators
[00:21:34.620 --> 00:21:38.260]   to generate examples of Simpsons characters
[00:21:38.260 --> 00:21:42.140]   for both the training and test data.
[00:21:42.140 --> 00:21:46.140]   And then right here is the Keras model.
[00:21:46.140 --> 00:21:47.940]   So if you used Keras before and you
[00:21:47.940 --> 00:21:51.260]   want to play with this model, you
[00:21:51.260 --> 00:21:54.100]   could make it a convolutional model or something
[00:21:54.100 --> 00:21:57.300]   more complicated.
[00:21:57.300 --> 00:22:01.340]   If you haven't done a lot of playing with Keras,
[00:22:01.340 --> 00:22:08.140]   you can actually change these hyperparameters here.
[00:22:08.140 --> 00:22:12.940]   But the goal, I guess, is to make a better model.
[00:22:12.940 --> 00:22:15.540]   All right, so guys, I do want to make sure
[00:22:15.540 --> 00:22:18.660]   that you all get to keep moving your projects forward.
[00:22:18.660 --> 00:22:21.380]   So I think we should probably pause this.
[00:22:21.380 --> 00:22:23.220]   Although, if you want to keep working on it,
[00:22:23.220 --> 00:22:25.820]   maybe we'll pull this up in the next class and--
[00:22:25.820 --> 00:22:26.820]   [LAUGHS]
[00:22:26.820 --> 00:22:28.460]   Yeah, we'll keep it open.
[00:22:28.460 --> 00:22:29.700]   Yeah, we'll keep it open.
[00:22:29.700 --> 00:22:30.660]   I mean, I don't know.
[00:22:30.660 --> 00:22:31.860]   What should we-- next class?
[00:22:31.860 --> 00:22:34.940]   Want to-- I mean, I would prefer that you
[00:22:34.940 --> 00:22:36.940]   worked on your cool projects.
[00:22:36.940 --> 00:22:41.740]   But I would also prefer that someone--
[00:22:41.740 --> 00:22:43.540]   ooh, someone did knock Tom off.
[00:22:43.540 --> 00:22:44.540]   Christina, who are you?
[00:22:44.540 --> 00:22:45.380]   Yeah, I'm Christina.
[00:22:45.380 --> 00:22:46.820]   Christina.
[00:22:46.820 --> 00:22:47.500]   Hey, Tom.
[00:22:47.500 --> 00:22:48.000]   Hey.
[00:22:48.000 --> 00:22:52.260]   Well done, well done.
[00:22:52.260 --> 00:22:53.660]   I guess you teach the next class.
[00:22:53.660 --> 00:23:00.500]   Do you want to say anything about what your approach was?
[00:23:00.500 --> 00:23:03.940]   I just added a convolution and then added two layers to that.
[00:23:03.940 --> 00:23:06.740]   And then it just keeps going.
[00:23:06.740 --> 00:23:07.940]   Interesting.
[00:23:07.940 --> 00:23:12.340]   So one thing just to point out is you actually can go--
[00:23:12.340 --> 00:23:17.260]   you can go into here and get a sense of what
[00:23:17.260 --> 00:23:20.140]   happened with this model.
[00:23:20.140 --> 00:23:22.060]   [INAUDIBLE]
[00:23:22.060 --> 00:23:25.340]   So if you click on the--
[00:23:25.340 --> 00:23:28.340]   if you click on this thing on the left here,
[00:23:28.340 --> 00:23:31.260]   you can kind of dig into what any particular person is
[00:23:31.260 --> 00:23:32.860]   doing in their run.
[00:23:32.860 --> 00:23:33.940]   But we'll keep this--
[00:23:33.940 --> 00:23:36.360]   I mean, just for fun, we'll keep this running for a while.
[00:23:36.360 --> 00:23:37.540]   If you want to keep--
[00:23:37.540 --> 00:23:38.900]   yeah?
[00:23:38.900 --> 00:23:41.980]   How do you get to this?
[00:23:41.980 --> 00:23:46.820]   So when you do this run in the terminal,
[00:23:46.820 --> 00:23:50.420]   it's going to tell you that it's synced to this URL here.
[00:23:50.420 --> 00:23:52.660]   It'll be the first line in the output
[00:23:52.660 --> 00:23:54.660]   or the last line once it finishes.
[00:23:54.660 --> 00:24:00.660]   You can also just go to app.wb.ai and just see everyone's--
[00:24:01.640 --> 00:24:03.120]   [INAUDIBLE]
[00:24:03.120 --> 00:24:12.960]   But anyway, you can also use this tool
[00:24:12.960 --> 00:24:14.640]   for your personal projects.
[00:24:14.640 --> 00:24:15.760]   And that was super fun.
[00:24:15.760 --> 00:24:16.320]   Thank you.
[00:24:16.320 --> 00:24:20.880]   [APPLAUSE]

