
[00:00:00.000 --> 00:00:03.980]   In the 48 hours since I released my first code interpreter video,
[00:00:03.980 --> 00:00:08.840]   I believe I have found another 12 use cases that showcase its power.
[00:00:08.840 --> 00:00:15.040]   From finding errors in datasets, to reading Anna Karenina, ASCII art, to image captioning.
[00:00:15.040 --> 00:00:19.200]   Most of these I haven't seen anyone else mention, so let's begin.
[00:00:19.200 --> 00:00:25.120]   First is creating a 3D surface plot which you can see on the left from the image on the right.
[00:00:25.120 --> 00:00:30.260]   I know I will get to professional uses in a second, but I was personally very impressed
[00:00:30.260 --> 00:00:35.940]   that all of this that you can see can be done through the interface of ChatGPT.
[00:00:35.940 --> 00:00:42.180]   You can even see the little buildings at the bottom left reflected in this 3D surface plot.
[00:00:42.180 --> 00:00:46.500]   To give you an idea of how it works, you click on the button to the left of the chat box,
[00:00:46.500 --> 00:00:48.440]   and then it analyzes whatever you've uploaded.
[00:00:48.440 --> 00:00:54.040]   And all I said was, analyze the RGB of the pixels and output a 3D surface map
[00:00:54.040 --> 00:00:55.100]   of the colors of the image.
[00:00:55.160 --> 00:00:58.640]   Now I will admit it doesn't do a perfect job immediately.
[00:00:58.640 --> 00:01:02.080]   At first it wasn't downloadable, and then it wasn't big enough,
[00:01:02.080 --> 00:01:03.900]   but eventually I got it to work.
[00:01:03.900 --> 00:01:05.540]   But it's time for the next example.
[00:01:05.540 --> 00:01:11.020]   And what I wondered was, what is the biggest document I could upload to get it to analyze?
[00:01:11.020 --> 00:01:14.120]   The longest book that I've ever read is Anna Karenina.
[00:01:14.120 --> 00:01:15.880]   I think it's about a thousand pages long.
[00:01:15.880 --> 00:01:20.060]   And I pasted it into a Word doc, and it's about 340,000 words.
[00:01:20.060 --> 00:01:22.460]   I uploaded it, and then I asked, as you can see,
[00:01:22.460 --> 00:01:24.260]   find all mentions of England,
[00:01:24.260 --> 00:01:25.080]   analyze them,
[00:01:25.180 --> 00:01:28.760]   and then I asked them to discover the tone in which the country is perceived in the book.
[00:01:28.760 --> 00:01:30.760]   Now I know what some of you may be wondering,
[00:01:30.760 --> 00:01:33.200]   is it just using its stored knowledge of the book?
[00:01:33.200 --> 00:01:34.560]   And I'll get to that in a second.
[00:01:34.560 --> 00:01:35.560]   But look at what it did.
[00:01:35.560 --> 00:01:39.200]   It went through and found the relevant quotes.
[00:01:39.200 --> 00:01:40.920]   There are seven of them there.
[00:01:40.920 --> 00:01:44.220]   I checked the document and these were legitimate quotes.
[00:01:44.220 --> 00:01:48.640]   But here's where we get to something that you can't just do with control F in a Word document.
[00:01:48.640 --> 00:01:52.980]   It analyzed the tone and sentiment of each of these passages.
[00:01:52.980 --> 00:01:54.900]   And you can see the analysis here.
[00:01:55.020 --> 00:02:00.120]   Then I asked, drawing on your own knowledge of the 19th century and the findings above,
[00:02:00.120 --> 00:02:05.620]   write a 2,000 word reflection on the presentation of England in Anna Karenina.
[00:02:05.620 --> 00:02:07.920]   Now I know many of you won't be interested in that book,
[00:02:07.920 --> 00:02:09.420]   but imagine your own text.
[00:02:09.420 --> 00:02:12.120]   This is 340,000 words.
[00:02:12.120 --> 00:02:15.120]   It then created a somewhat beautiful essay.
[00:02:15.120 --> 00:02:18.820]   And yes, it did bring up each of those quotes with analysis.
[00:02:18.820 --> 00:02:21.220]   Now here is where things get kind of wild.
[00:02:21.220 --> 00:02:23.420]   Just to demonstrate that it's not using its own knowledge,
[00:02:23.420 --> 00:02:24.920]   I asked the same question.
[00:02:24.980 --> 00:02:28.880]   I asked the same question in a different window without, of course, uploading the file.
[00:02:28.880 --> 00:02:30.880]   And at first I was like, oh, damn, it did it.
[00:02:30.880 --> 00:02:31.880]   Here are the quotes.
[00:02:31.880 --> 00:02:32.880]   Wow, it did the job.
[00:02:32.880 --> 00:02:34.880]   It didn't even need the document.
[00:02:34.880 --> 00:02:38.880]   But that was until I actually checked out whether the quotes were legitimate.
[00:02:38.880 --> 00:02:41.880]   And lo and behold, it had made up the quotes.
[00:02:41.880 --> 00:02:44.880]   I searched far and wide for these quotes.
[00:02:44.880 --> 00:02:47.880]   And unless I'm going completely crazy, they are completely made up.
[00:02:47.880 --> 00:02:51.880]   So when it found those quotes earlier, it wasn't drawing upon its own knowledge.
[00:02:51.880 --> 00:02:54.880]   It was finding them from the document.
[00:02:54.920 --> 00:03:00.820]   And that serves as a warning of the hallucinations that the model can do if it doesn't have enough data.
[00:03:00.820 --> 00:03:03.820]   I'm going to get back to reliability and factuality in a moment.
[00:03:03.820 --> 00:03:05.820]   But just quickly, a bonus.
[00:03:05.820 --> 00:03:10.820]   I got it to write an epilogue to the death of Ivan Ilyich, an incredible short story by Leo Tolstoy.
[00:03:10.820 --> 00:03:17.820]   And as some people had asked, it can indeed output that to a PDF, which is convenient for many people.
[00:03:17.820 --> 00:03:19.820]   Next, what about multiple files?
[00:03:19.820 --> 00:03:24.780]   I didn't actually investigate this in my previous video, which if you haven't watched, by the way, please do check it out.
[00:03:24.860 --> 00:03:27.720]   I've got 73 examples of use cases there.
[00:03:27.720 --> 00:03:32.720]   But anyway, what I wanted to try out was I wanted to upload four datasets.
[00:03:32.720 --> 00:03:37.720]   And then I wanted to get GPT-4 to find any correlations between the datasets.
[00:03:37.720 --> 00:03:41.720]   Also, I was kind of investigating if there was a limit to the number of files you could upload.
[00:03:41.720 --> 00:03:43.720]   And honestly, there doesn't seem to be.
[00:03:43.720 --> 00:03:46.720]   I picked this global data almost at random, to be honest.
[00:03:46.720 --> 00:03:48.720]   It was the amount of sugar consumed per person.
[00:03:48.720 --> 00:03:51.720]   And then the murder rate per 100,000 people.
[00:03:51.720 --> 00:03:54.700]   And then the inequality index of each of those countries.
[00:03:54.800 --> 00:03:57.660]   And then the population aged 20 to 39.
[00:03:57.660 --> 00:03:59.660]   But first, notice how it didn't stop me.
[00:03:59.660 --> 00:04:01.660]   I could just keep uploading files.
[00:04:01.660 --> 00:04:03.660]   And then it would ask me things like:
[00:04:03.660 --> 00:04:09.660]   "Please provide guidance on the kind of analysis or specific questions you would like me to investigate with these four datasets."
[00:04:09.660 --> 00:04:11.660]   So it's still aware of the previous files.
[00:04:11.660 --> 00:04:13.660]   What I asked was this:
[00:04:13.660 --> 00:04:17.660]   "Analyze all four datasets and find five surprising correlations.
[00:04:17.660 --> 00:04:23.660]   Output as many insights as you can, distinguishing between correlation and causation."
[00:04:23.800 --> 00:04:26.660]   This is really pushing the limits of what Code Interpreter can do.
[00:04:26.660 --> 00:04:27.660]   But it did it.
[00:04:27.660 --> 00:04:32.660]   Many of you asked before if it could be lulled with false data into giving bad conclusions.
[00:04:32.660 --> 00:04:34.660]   And it's really hard to get it to do that.
[00:04:34.660 --> 00:04:38.660]   GPT-4 is honestly really smart and increasingly hard to fool.
[00:04:38.660 --> 00:04:40.660]   You can read what it said.
[00:04:40.660 --> 00:04:45.660]   It found a very weak negative correlation, for example, between sugar consumption and murder.
[00:04:45.660 --> 00:04:49.660]   And then just admitted there is probably no significant relationship between these two factors.
[00:04:49.660 --> 00:04:52.660]   But notice it then found a correlation that it found more plausible.
[00:04:52.800 --> 00:05:01.660]   There is a moderate positive correlation, 0.4, between the murder rate per 100,000 people and the Gini Inequality Index.
[00:05:01.660 --> 00:05:06.660]   This suggests that countries with higher income inequality tend to have a higher murder rate.
[00:05:06.660 --> 00:05:08.660]   I then followed up with this:
[00:05:08.660 --> 00:05:13.660]   "Drawing on your own knowledge of the world, which correlation seems the most causally related?"
[00:05:13.660 --> 00:05:20.660]   It then brought in research from the field of social science and gave a plausible explanation about why this correlation might exist.
[00:05:20.660 --> 00:05:22.660]   Obviously this was just my example.
[00:05:22.800 --> 00:05:30.660]   You would have to think about all the different files and data sets that you were willing to upload to find correlations and surprising insights within.
[00:05:30.660 --> 00:05:33.660]   I'm going to try to alternate between fun and serious.
[00:05:33.660 --> 00:05:35.660]   So the next one is going to be kind of fun.
[00:05:35.660 --> 00:05:40.660]   I was surprised by the number of comments asking me to get it to do ASCII art.
[00:05:40.660 --> 00:05:43.660]   Now you may remember from the last video that I got it to analyze this image.
[00:05:43.660 --> 00:05:46.660]   And yes, I asked it to turn it into ASCII art.
[00:05:46.660 --> 00:05:49.660]   And here is what it came up with.
[00:05:49.660 --> 00:05:52.660]   Not bad. Not amazing.
[00:05:52.800 --> 00:05:53.660]   Not bad.
[00:05:53.660 --> 00:05:55.660]   A bit more seriously now for data analytics.
[00:05:55.660 --> 00:06:01.660]   What I wanted to do is test if it could spot an error in a massive CSV or Excel file.
[00:06:01.660 --> 00:06:05.660]   This is a huge data set of population density.
[00:06:05.660 --> 00:06:06.660]   And notice what I did.
[00:06:06.660 --> 00:06:09.660]   I say notice, you almost certainly wouldn't be able to notice.
[00:06:09.660 --> 00:06:17.660]   But basically for the Isle of Man, for 1975, I changed 105, which was the original, to 1500.
[00:06:17.660 --> 00:06:21.660]   And I did something similar for Lichtenstein for a different year.
[00:06:21.800 --> 00:06:29.660]   Then I uploaded the file and said, find any anomalies in the data by looking for implausible percent changes year to year.
[00:06:29.660 --> 00:06:32.660]   Output any data points that look suspicious.
[00:06:32.660 --> 00:06:35.660]   And really interestingly here, the wording does make a difference.
[00:06:35.660 --> 00:06:37.660]   You've got to give it a tiny hint.
[00:06:37.660 --> 00:06:44.660]   If you just say find anything that looks strange, it will find empty cells and say, oh, there's a missing cell here.
[00:06:44.660 --> 00:06:51.660]   But if you give it a tiny nudge and just say that you're looking for anomalies, look out for things like implausible percent changes.
[00:06:51.800 --> 00:06:54.660]   Data that looks suspicious.
[00:06:54.660 --> 00:06:55.660]   Then look what it did.
[00:06:55.660 --> 00:06:59.660]   It did the analysis and you can see the reasoning above.
[00:06:59.660 --> 00:07:02.660]   And it found the Isle of Man and Lichtenstein.
[00:07:02.660 --> 00:07:07.660]   And it said these values are indeed very unusual and may indicate errors in the data.
[00:07:07.660 --> 00:07:14.660]   It said it's also possible that these changes could be due to significant population migration, changes in land area or other factors.
[00:07:14.660 --> 00:07:21.660]   I guess if in one year one of those places was invaded and it was only a city that was left officially as part of the territory,
[00:07:21.800 --> 00:07:23.660]   the population density would skyrocket.
[00:07:23.660 --> 00:07:25.660]   So that's a smart answer.
[00:07:25.660 --> 00:07:29.660]   But it spotted the two changes that I'd made among thousands of data points.
[00:07:29.660 --> 00:07:32.660]   In fact, actually, I'm going to work out how many data points there were in that file.
[00:07:32.660 --> 00:07:37.660]   I used Excel to work it out, of course, and there were 36,000 data points.
[00:07:37.660 --> 00:07:41.660]   And I made two changes and it spotted both of them.
[00:07:41.660 --> 00:07:44.660]   But now it's time to go back to something a bit more fun and creative.
[00:07:44.660 --> 00:07:46.660]   Next, I gave it that same image again.
[00:07:46.660 --> 00:07:51.660]   And I said, write a sonnet about a morpho AI reflecting on a dystopian landscape.
[00:07:51.800 --> 00:07:53.660]   She does look kind of solemn here.
[00:07:53.660 --> 00:07:58.660]   Overlay the poem in the background of this image using edge detector to avoid any objects.
[00:07:58.660 --> 00:08:00.660]   Now, there are different ways of doing it.
[00:08:00.660 --> 00:08:03.660]   It can detect the foreground and background.
[00:08:03.660 --> 00:08:06.660]   So it put the text away from the central character.
[00:08:06.660 --> 00:08:09.660]   I think the final result is really not bad.
[00:08:09.660 --> 00:08:11.660]   And the sonnet is pretty powerful.
[00:08:11.660 --> 00:08:12.660]   I'll read just the ending.
[00:08:12.660 --> 00:08:16.660]   Gone are the humans it once adored, leaving it in silent solitude.
[00:08:16.660 --> 00:08:21.660]   In binary sorrow, it has stored memories of a world it once knew.
[00:08:21.800 --> 00:08:23.660]   In the void, it sends a mournful cry.
[00:08:23.660 --> 00:08:26.660]   A ghost in the machine, left to die.
[00:08:26.660 --> 00:08:33.660]   Anyway, this is a glimpse of the power of merging GPT-4's language abilities with its nascent code interpreter abilities.
[00:08:33.660 --> 00:08:35.660]   Next, people asked about unclean data.
[00:08:35.660 --> 00:08:39.660]   So I tried to find the most unclean data I could find.
[00:08:39.660 --> 00:08:46.660]   What I did is I pasted in directly from a website, RealClearPolitics, a bunch of polls, different polls.
[00:08:46.660 --> 00:08:49.660]   And notice the formatting is quite confusing.
[00:08:49.660 --> 00:08:51.660]   You've got the dates on top.
[00:08:51.800 --> 00:08:54.660]   You've got the matching data, coloured data, all sorts of things.
[00:08:54.660 --> 00:08:55.660]   Then I asked:
[00:08:55.660 --> 00:09:00.660]   Extract out the data for the 2024 Republican presidential nomination.
[00:09:00.660 --> 00:09:05.660]   Analyse the trend in the data and output the results in a new downloadable file.
[00:09:05.660 --> 00:09:10.660]   It sorted through and then found the averages for each of the candidates in that specific race.
[00:09:10.660 --> 00:09:14.660]   And I'm going to get to factuality and accuracy just a bit later on.
[00:09:14.660 --> 00:09:17.660]   The hint is that the accuracy is really surprisingly good.
[00:09:17.660 --> 00:09:21.660]   But I wanted to push it a bit further and do some trend analysis.
[00:09:21.800 --> 00:09:25.660]   First, it analysed some of the other races from that very unclean data set.
[00:09:25.660 --> 00:09:29.660]   And then what I did is I pasted in an article from Politico.
[00:09:29.660 --> 00:09:34.660]   And based on this very messy data, I got it to do some political prognostication.
[00:09:34.660 --> 00:09:40.660]   It analysed the article and the polls and then I think gave quite a smart and nuanced answer.
[00:09:40.660 --> 00:09:42.660]   And what about accuracy?
[00:09:42.660 --> 00:09:44.660]   I know many people had that question.
[00:09:44.660 --> 00:09:49.660]   Well, I uploaded this data and I'm also going to link to it in the description so you can do further checks.
[00:09:49.660 --> 00:09:51.660]   It was based on a fictional food company based in the United States.
[00:09:51.800 --> 00:09:53.660]   It was based in Boston and New York.
[00:09:53.660 --> 00:09:59.660]   And what I asked was: draw six actionable insights that would be relevant for the CEO of this company.
[00:09:59.660 --> 00:10:02.660]   It then gave the insights below.
[00:10:02.660 --> 00:10:07.660]   And even though I didn't actually ask for this, it gave six visualisations.
[00:10:07.660 --> 00:10:09.660]   Let me zoom in here so you can see it.
[00:10:09.660 --> 00:10:12.660]   And then I picked out at random five of those data points.
[00:10:12.660 --> 00:10:15.660]   Obviously, I'm not going to check hundreds of them, but I picked out five.
[00:10:15.660 --> 00:10:19.660]   Then I laboriously checked them in Excel and they were all right.
[00:10:19.660 --> 00:10:21.660]   Obviously, though, I'm not guaranteeing that every single one of them is correct.
[00:10:21.800 --> 00:10:27.660]   And as I say, you can download the file and see if the six visualisations are correct yourself.
[00:10:27.660 --> 00:10:29.660]   So far, honestly, it's looking good.
[00:10:29.660 --> 00:10:36.660]   And then below we have more detail on those insights and then some actions that we could take as a CEO.
[00:10:36.660 --> 00:10:39.660]   Just like I did with Anna Karenina, I then followed up and said:
[00:10:39.660 --> 00:10:45.660]   Use your own knowledge of the world and offer plausible explanations for each of these findings.
[00:10:45.660 --> 00:10:48.660]   This is where GPT-4 becomes your own data analyst assistant.
[00:10:48.660 --> 00:10:51.660]   And it gave plausible explanations for these findings.
[00:10:51.800 --> 00:10:54.660]   And it also gave a full summary of some of the findings.
[00:10:54.660 --> 00:11:01.660]   For example, the higher sales in the East region could be due to a higher population density, better distribution networks, or higher demand for the company's products.
[00:11:01.660 --> 00:11:08.660]   And at this point, you could either use the web browser plugin to do more research on your own, or you could upload more files.
[00:11:08.660 --> 00:11:10.660]   I actually asked, and I think this is a great question:
[00:11:10.660 --> 00:11:16.660]   Suggest six other company datasets you would find helpful to access to test these suppositions.
[00:11:16.660 --> 00:11:19.660]   Now, obviously, a lot is going to come down to privacy and data protection.
[00:11:19.660 --> 00:11:21.660]   But GPT-4 Code Interpreter can solve this.
[00:11:21.800 --> 00:11:25.660]   It can suggest further files that would help it with its analytics.
[00:11:25.660 --> 00:11:26.660]   And it gives those below.
[00:11:26.660 --> 00:11:33.660]   And again, the lazy CEO could just upload those files and get GPT-4 Code Interpreter to do further analysis.
[00:11:33.660 --> 00:11:35.660]   You don't have to think about what to upload.
[00:11:35.660 --> 00:11:37.660]   GPT-4 will suggest it for you.
[00:11:37.660 --> 00:11:40.660]   Whether that's advisable or not, I'll leave you to decide.
[00:11:40.660 --> 00:11:42.660]   The next one is slightly less serious.
[00:11:42.660 --> 00:11:46.660]   And it's that Code Interpreter can output PowerPoint slides directly.
[00:11:46.660 --> 00:11:51.660]   Now, I know when Microsoft 365 Copilot rolls out, this might be a little bit redundant.
[00:11:51.800 --> 00:11:57.660]   But it's cool to know you can output directly into PowerPoint the visualizations and analysis from Code Interpreter.
[00:11:57.660 --> 00:11:59.660]   Now, on to mathematics.
[00:11:59.660 --> 00:12:03.660]   And many people pointed out that I didn't fully test out Wolfram to give it a fair shot.
[00:12:03.660 --> 00:12:07.660]   So I tested both Code Interpreter and Wolfram on differential equations.
[00:12:07.660 --> 00:12:09.660]   And they both got it right.
[00:12:09.660 --> 00:12:13.660]   Interestingly, though, they gave you a link for the step-by-step solutions.
[00:12:13.660 --> 00:12:16.660]   Because this is a paid option on the Wolfram website.
[00:12:16.660 --> 00:12:19.660]   But I did find some other differences between them.
[00:12:19.660 --> 00:12:21.660]   And honestly, it favored Code Interpreter.
[00:12:21.800 --> 00:12:24.660]   So here is a really challenging mathematics question.
[00:12:24.660 --> 00:12:26.660]   And Wolfram can't get it right.
[00:12:26.660 --> 00:12:30.660]   It says that the answer is 40%, even though that's not one of the options.
[00:12:30.660 --> 00:12:33.660]   And yes, it used Wolfram, I think, about five times.
[00:12:33.660 --> 00:12:39.660]   Here was the exact same prompt, except instead of saying "use Wolfram", I said "use Code Interpreter".
[00:12:39.660 --> 00:12:41.660]   And this was not a one-off example.
[00:12:41.660 --> 00:12:43.660]   It fairly consistently got it right.
[00:12:43.660 --> 00:12:47.660]   So Code Interpreter does indeed have some serious oomph behind it.
[00:12:47.660 --> 00:12:49.660]   Just quickly again on the silly stuff.
[00:12:49.660 --> 00:12:51.660]   I uploaded the entire demo.
[00:12:51.860 --> 00:12:54.660]   And I did not even know that I was going to be able to do this.
[00:12:54.660 --> 00:12:56.660]   I did not even know that I was going to be able to do this.
[00:12:56.660 --> 00:12:58.660]   I did not even know that I was going to be able to do this.
[00:12:58.660 --> 00:13:00.660]   I did not even know that I was going to be able to do this.
[00:13:00.660 --> 00:13:02.660]   I did not even know that I was going to be able to do this.
[00:13:02.660 --> 00:13:04.660]   I did not even know that I was going to be able to do this.
[00:13:04.660 --> 00:13:06.660]   I did not even know that I was going to be able to do this.
[00:13:06.660 --> 00:13:08.660]   I did not even know that I was going to be able to do this.
[00:13:08.660 --> 00:13:10.660]   I did not even know that I was going to be able to do this.
[00:13:10.660 --> 00:13:12.660]   I did not even know that I was going to be able to do this.
[00:13:12.660 --> 00:13:14.660]   I did not even know that I was going to be able to do this.
[00:13:14.660 --> 00:13:16.660]   I did not even know that I was going to be able to do this.
[00:13:16.800 --> 00:13:18.800]   I did not even know that I was going to be able to do this.
[00:13:18.800 --> 00:13:20.800]   I did not even know that I was going to be able to do this.
[00:13:20.800 --> 00:13:22.800]   I did not even know that I was going to be able to do this.
[00:13:22.800 --> 00:13:24.800]   I did not even know that I was going to be able to do this.
[00:13:24.800 --> 00:13:26.800]   I did not even know that I was going to be able to do this.
[00:13:26.800 --> 00:13:28.800]   I did not even know that I was going to be able to do this.
[00:13:28.800 --> 00:13:30.800]   I did not even know that I was going to be able to do this.
[00:13:30.800 --> 00:13:32.800]   I did not even know that I was going to be able to do this.
[00:13:32.800 --> 00:13:34.800]   I did not even know that I was going to be able to do this.
[00:13:34.800 --> 00:13:36.800]   I did not even know that I was going to be able to do this.
[00:13:36.800 --> 00:13:38.800]   I did not even know that I was going to be able to do this.
[00:13:38.800 --> 00:13:40.800]   I did not even know that I was going to be able to do this.
[00:13:40.800 --> 00:13:42.800]   I did not even know that I was going to be able to do this.
[00:13:42.800 --> 00:13:44.800]   I did not even know that I was going to be able to do this.
[00:13:44.800 --> 00:13:46.800]   I did not even know that I was going to be able to do this.
[00:13:46.800 --> 00:13:48.800]   I did not even know that I was going to be able to do this.
[00:13:48.800 --> 00:13:50.800]   I did not even know that I was going to be able to do this.
[00:13:50.800 --> 00:13:52.800]   I did not even know that I was going to be able to do this.
[00:13:52.800 --> 00:13:54.800]   I did not even know that I was going to be able to do this.
[00:13:54.800 --> 00:13:56.800]   I did not even know that I was going to be able to do this.
[00:13:56.800 --> 00:13:58.800]   I did not even know that I was going to be able to do this.
[00:13:58.800 --> 00:14:00.800]   I did not even know that I was going to be able to do this.
[00:14:00.800 --> 00:14:02.800]   I did not even know that I was going to be able to do this.
[00:14:02.800 --> 00:14:04.800]   I did not even know that I was going to be able to do this.
[00:14:04.800 --> 00:14:06.800]   I did not even know that I was going to be able to do this.
[00:14:06.800 --> 00:14:08.800]   I did not even know that I was going to be able to do this.
[00:14:08.800 --> 00:14:10.800]   I did not even know that I was going to be able to do this.
[00:14:10.800 --> 00:14:12.800]   I did not even know that I was going to be able to do this.
[00:14:12.800 --> 00:14:14.800]   I did not even know that I was going to be able to do this.
[00:14:14.800 --> 00:14:16.800]   I did not even know that I was going to be able to do this.
[00:14:16.800 --> 00:14:18.800]   I did not even know that I was going to be able to do this.
[00:14:18.800 --> 00:14:20.800]   I did not even know that I was going to be able to do this.
[00:14:20.800 --> 00:14:22.800]   I did not even know that I was going to be able to do this.
[00:14:22.800 --> 00:14:24.800]   I did not even know that I was going to be able to do this.

