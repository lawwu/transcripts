
[00:00:00.000 --> 00:00:04.720]   Okay, looks like we're on our way.
[00:00:04.720 --> 00:00:07.440]   It's about 10 more seconds.
[00:00:07.440 --> 00:00:11.320]   Yeah, I just saw us start to go live there.
[00:00:11.320 --> 00:00:12.320]   Amazing.
[00:00:12.320 --> 00:00:17.040]   All right, we are in fact live and online.
[00:00:17.040 --> 00:00:19.320]   Hey, everybody.
[00:00:19.320 --> 00:00:24.600]   Welcome to the Weights and Biases Deep Learning Salon, the final Weights and Biases Deep Learning
[00:00:24.600 --> 00:00:29.880]   Salon of the year 2020.
[00:00:29.880 --> 00:00:37.000]   The next Weights and Biases Salon will be next month in January, on January 19th.
[00:00:37.000 --> 00:00:42.300]   We're going to be switching over to a monthly format rather than a biweekly format.
[00:00:42.300 --> 00:00:47.640]   And so our January speaker will be Greg Yang of Microsoft Research.
[00:00:47.640 --> 00:00:55.320]   Greg has been doing this really awesome work on infinite width neural networks.
[00:00:55.320 --> 00:00:57.440]   Big networks are better, right?
[00:00:57.440 --> 00:01:00.360]   So why not work with infinitely big networks?
[00:01:00.360 --> 00:01:05.260]   So Greg's been working on this approach called Tensor Programs to try and understand what
[00:01:05.260 --> 00:01:08.200]   infinitely big neural networks can and cannot do.
[00:01:08.200 --> 00:01:11.240]   And there's been some really exciting developments over the last couple of months.
[00:01:11.240 --> 00:01:16.000]   And I'm really excited to hear from Greg on that topic.
[00:01:16.000 --> 00:01:24.080]   So you can RSVP at that wanb.me/salon link starting in just a few minutes.
[00:01:24.080 --> 00:01:29.520]   You can also catch all of our salons on our YouTube channel, including this one and eventually
[00:01:29.520 --> 00:01:32.920]   Greg's, if you go to our YouTube channel and check it out.
[00:01:32.920 --> 00:01:36.400]   We've got a playlist of all of our deep learning salons.
[00:01:36.400 --> 00:01:41.680]   We also, we announce things like the salons and other events that we do on YouTube and
[00:01:41.680 --> 00:01:44.680]   on Twitter and in our Slack community.
[00:01:44.680 --> 00:01:48.800]   So that's the best place to go to know where stuff that's happening with Weights and Biases,
[00:01:48.800 --> 00:01:54.080]   to know about it before it's announced elsewhere and to be able to participate in some of the
[00:01:54.080 --> 00:01:55.080]   special things that we do.
[00:01:55.080 --> 00:02:01.640]   We're doing a reproducibility challenge associated with NeurIPS and we're doing a bunch of collaborative
[00:02:01.640 --> 00:02:04.680]   model training projects over the next couple of months.
[00:02:04.680 --> 00:02:12.280]   So that's all organized via the Slack forum, which you can join at wanb.me/slack.
[00:02:12.280 --> 00:02:17.880]   So that's the Weights and Biases salon background and our community as a whole.
[00:02:17.880 --> 00:02:25.920]   Today, what our salon is going to focus on, we're going to talk about the, or we're going
[00:02:25.920 --> 00:02:31.720]   to hear from folks at ML at Berkeley, an undergraduate student organization about some of the work
[00:02:31.720 --> 00:02:35.360]   that they did over this past semester.
[00:02:35.360 --> 00:02:38.960]   So we're going to hear from three separate teams that worked on three different projects,
[00:02:38.960 --> 00:02:44.360]   each very different from the other, but all attacking a different subfield in deep learning
[00:02:44.360 --> 00:02:46.440]   and machine learning.
[00:02:46.440 --> 00:02:49.280]   And I'm really excited to hear from all of them about their work.
[00:02:49.280 --> 00:02:55.780]   We're going to start with some folks who worked on what was more of an ML engineering project.
[00:02:55.780 --> 00:03:01.080]   So we're going to be hearing from Ashwat and Andre on their design of a computer vision
[00:03:01.080 --> 00:03:08.600]   based video gesture overlay for Zoom, which they will, if the demo gods are with us today,
[00:03:08.600 --> 00:03:11.240]   create live as part of the salon.
[00:03:11.240 --> 00:03:14.000]   So Andre and Ashwat, take it away.
[00:03:14.000 --> 00:03:15.000]   Awesome.
[00:03:15.000 --> 00:03:17.000]   Thank you, Charles.
[00:03:17.000 --> 00:03:20.000]   Give me one sec.
[00:03:20.000 --> 00:03:23.400]   Okay, sweet.
[00:03:23.400 --> 00:03:26.880]   And yeah, okay, awesome.
[00:03:26.880 --> 00:03:31.800]   So hello and welcome to our presentation, which is video gesture recognition and overlay.
[00:03:31.800 --> 00:03:36.440]   This is a presentation, a final product that we did for the new member education program
[00:03:36.440 --> 00:03:39.000]   here at M-Lab, machine learning at Berkeley.
[00:03:39.000 --> 00:03:40.000]   And yeah, we're excited.
[00:03:40.000 --> 00:03:41.000]   We hope you enjoy that presentation.
[00:03:41.000 --> 00:03:45.440]   So first and foremost, I want to introduce the team.
[00:03:45.440 --> 00:03:49.880]   My name is Ashwat and I worked with Arya, Andre and Saurabh this semester on this project.
[00:03:49.880 --> 00:03:54.000]   Andre will be joining us for the second half of this presentation and Arya and Saurabh
[00:03:54.000 --> 00:03:57.400]   will hopefully be online answering questions on the YouTube live stream as well as at the
[00:03:57.400 --> 00:04:00.000]   end to answer questions.
[00:04:00.000 --> 00:04:02.960]   So first and foremost, what was the goal of this entire project?
[00:04:02.960 --> 00:04:03.960]   Why did we do this?
[00:04:03.960 --> 00:04:08.560]   And let me ask you this, how many times this year on Zoom has someone tried to screen share
[00:04:08.560 --> 00:04:12.120]   and they asked you the question, the dreaded question, can you see my screen?
[00:04:12.120 --> 00:04:14.800]   To which the answer is almost 99% yes.
[00:04:14.800 --> 00:04:19.080]   And then you wait this awkward silence until someone finally says, unmute, they say the
[00:04:19.080 --> 00:04:21.680]   one word yes, and then they mute themselves again.
[00:04:21.680 --> 00:04:23.600]   We thought to ourselves, like there's got to be a better way.
[00:04:23.600 --> 00:04:26.600]   There's got to be a way to use machine learning to prevent the need to unmute to just say
[00:04:26.600 --> 00:04:31.240]   a quick word like yes, or I have a question or I have something to say or goodbye.
[00:04:31.240 --> 00:04:34.800]   On top of that, we wanted to make the Zoom equivalent of emoji reacts because emojis
[00:04:34.800 --> 00:04:37.760]   are pretty much like a second language to all of us right now and all of us know it.
[00:04:37.760 --> 00:04:40.400]   So like how do we combine the two things, make it kind of fun.
[00:04:40.400 --> 00:04:44.280]   And last but not least, we said, how do we combine some relatively simple concepts to
[00:04:44.280 --> 00:04:45.520]   solve a real world problem?
[00:04:45.520 --> 00:04:47.400]   In this case, being the convenience on Zoom.
[00:04:47.400 --> 00:04:52.960]   So on the right side, you can see a GIF of me where I'm doing a demo of what we created.
[00:04:52.960 --> 00:04:54.960]   So I'm gone for a second, I'll be back.
[00:04:54.960 --> 00:04:58.080]   But pretty much what I'm trying to do is you can see my hands will make all these gestures
[00:04:58.080 --> 00:05:01.880]   on the camera and an icon will show up because the camera recognizes what I'm trying to say.
[00:05:01.880 --> 00:05:02.880]   So this would be a comment.
[00:05:02.880 --> 00:05:05.840]   I believe the next one would be like a goodbye, a peace sign.
[00:05:05.840 --> 00:05:08.720]   So yeah, we'll get into detail about that.
[00:05:08.720 --> 00:05:12.080]   So now the first step when it comes to trying to do any kind of gesture recognition is to
[00:05:12.080 --> 00:05:15.760]   find out where in the frame your hand is in the first place.
[00:05:15.760 --> 00:05:18.800]   So for us, we decided to use some off the shelf tracking modules.
[00:05:18.800 --> 00:05:21.440]   For hand tracking, we decided to use Victor D'Obia on GitHub.
[00:05:21.440 --> 00:05:24.560]   He had a great GitHub repository about this.
[00:05:24.560 --> 00:05:29.120]   And then for face tracking, we decided to use the good old reliable hard cascades.
[00:05:29.120 --> 00:05:30.960]   So here are some animations showing how this works.
[00:05:30.960 --> 00:05:32.800]   On the bottom left is Victor D'Obia's GitHub.
[00:05:32.800 --> 00:05:36.200]   I believe he trained this using transfer learning on mobile.
[00:05:36.200 --> 00:05:38.360]   And definitely was really good.
[00:05:38.360 --> 00:05:39.360]   And that was a great foundation.
[00:05:39.360 --> 00:05:43.080]   And on the bottom right, we have the hard cascades for facial hard cascades.
[00:05:43.080 --> 00:05:45.720]   And we use both of these as like a foundation to build off of.
[00:05:45.720 --> 00:05:48.920]   And then again, we had to take this kind of modify it up a bit, change the inputs so that
[00:05:48.920 --> 00:05:53.020]   we could make it as tailored as possible for our specific ML model, which Andre will talk
[00:05:53.020 --> 00:05:55.720]   about in the future slides.
[00:05:55.720 --> 00:05:57.560]   So quick detour into how hard cascades work.
[00:05:57.560 --> 00:05:59.760]   I just want to quickly talk about this.
[00:05:59.760 --> 00:06:03.320]   So right off the bat, Har is the name of a Hungarian mathematician.
[00:06:03.320 --> 00:06:06.720]   I believe he came up with something called Har sequences in mathematics, which led to
[00:06:06.720 --> 00:06:07.720]   Har wavelets.
[00:06:07.720 --> 00:06:09.920]   Har wavelets are intuitively similar to Har features.
[00:06:09.920 --> 00:06:11.960]   And that's where we came here today.
[00:06:11.960 --> 00:06:16.640]   So Har features are pretty much very good at identifying edges and lines in an image.
[00:06:16.640 --> 00:06:20.080]   And I think this was proposed by Viola and Jones in the Viola Jones algorithm in a paper
[00:06:20.080 --> 00:06:24.200]   they wrote to CVPR back in 2001, if you know what that is.
[00:06:24.200 --> 00:06:28.280]   But overall, what it does is it takes the average of all the pixels in the white region,
[00:06:28.280 --> 00:06:31.400]   the average of all the pixels in the black region, subtracts the two means.
[00:06:31.400 --> 00:06:34.640]   And if they're around a certain threshold, then you pretty much have a good confidence
[00:06:34.640 --> 00:06:37.920]   that this is an edge or a line in the image.
[00:06:37.920 --> 00:06:41.320]   Now the next natural question is, well, you know, there's edges and lines, but how does
[00:06:41.320 --> 00:06:42.520]   that apply to my face?
[00:06:42.520 --> 00:06:43.520]   Because like everyone's face is different.
[00:06:43.520 --> 00:06:45.360]   There's no edges or lines directly in your face.
[00:06:45.360 --> 00:06:47.720]   And that's what this diagram shows beautifully.
[00:06:47.720 --> 00:06:51.520]   Pretty much your eyes can be captured very well with an edge feature as shown in this.
[00:06:51.520 --> 00:06:54.520]   A nose can be captured and a mouth can be captured with the line features relatively
[00:06:54.520 --> 00:06:55.520]   well.
[00:06:55.520 --> 00:06:59.120]   So in a sense, these Har features will kind of map and really reliably map to parts of
[00:06:59.120 --> 00:07:02.320]   your face where they'll match up closely because the pixel intensity between the luminosity
[00:07:02.320 --> 00:07:05.880]   between like, it's like your nose and the surrounding surroundings.
[00:07:05.880 --> 00:07:08.120]   And pretty much that's how it works.
[00:07:08.120 --> 00:07:10.480]   And next, an animation showing how this all works together.
[00:07:10.480 --> 00:07:14.680]   So what it does is it slides a window across the entire image and it runs these features
[00:07:14.680 --> 00:07:17.800]   across every single thing and gets like some kind of, how many ever hits it can kind of
[00:07:17.800 --> 00:07:18.800]   get.
[00:07:18.800 --> 00:07:21.480]   So the next question naturally is, you know, we have thousands of thousands of these Har
[00:07:21.480 --> 00:07:23.000]   features and we have so many windows.
[00:07:23.000 --> 00:07:26.040]   That's a lot of calculations for a real time face detector.
[00:07:26.040 --> 00:07:27.720]   So how does it work so well?
[00:07:27.720 --> 00:07:29.600]   And that's where the cascading parts comes into play.
[00:07:29.600 --> 00:07:33.280]   So it's called Har cascade because the cascade of Har features.
[00:07:33.280 --> 00:07:37.080]   What it does is pretty much it takes a few low level core features, runs it across a
[00:07:37.080 --> 00:07:38.080]   window in an image.
[00:07:38.080 --> 00:07:41.160]   And again, if it doesn't pass the initial features, like if it doesn't pass the test,
[00:07:41.160 --> 00:07:44.200]   I guess in the beginning, there's no reason it would pass the more intricate or detailed
[00:07:44.200 --> 00:07:45.200]   test later on.
[00:07:45.200 --> 00:07:48.280]   So that's, you can discard an image or as part of the image right away.
[00:07:48.280 --> 00:07:52.480]   And that's at the end, the parts of the image that passed all of these cascading tests will
[00:07:52.480 --> 00:07:56.120]   be classified as a face because it kind of like passed all of the cascade of Har features
[00:07:56.120 --> 00:07:57.280]   that I wanted to find.
[00:07:57.280 --> 00:08:00.880]   And that's what's returned as a bounding box to the user.
[00:08:00.880 --> 00:08:02.540]   Now moving on to gesture classes.
[00:08:02.540 --> 00:08:06.600]   So on the left side, you can see all of the like emojis showing what kind of hand gestures
[00:08:06.600 --> 00:08:08.360]   we use for these classes.
[00:08:08.360 --> 00:08:11.720]   And the middle is the class that we wanted to identify as.
[00:08:11.720 --> 00:08:15.680]   And on the right side is the icon that we wanted to show.
[00:08:15.680 --> 00:08:19.000]   So for us, we thought the most core things people want to say that they don't need to
[00:08:19.000 --> 00:08:23.540]   unmute for is, you know, I have a question, or, you know, yes, sounds good.
[00:08:23.540 --> 00:08:25.200]   You can say no, please stop.
[00:08:25.200 --> 00:08:28.120]   I have something to say a comment, one or two fingers, peace sign, like I'm going to
[00:08:28.120 --> 00:08:30.080]   go goodbye, end of the call.
[00:08:30.080 --> 00:08:33.560]   And last but not least, my personal favorite, and I think one of the most important is the
[00:08:33.560 --> 00:08:34.560]   out of frame.
[00:08:34.560 --> 00:08:38.760]   And the reason for this is because I think that many times I've seen this in calls myself
[00:08:38.760 --> 00:08:42.520]   at university this past semester, when people would leave their camera, but they leave their
[00:08:42.520 --> 00:08:46.560]   video on which leaves their entire background to the viewers, you know, pleasure, whatever
[00:08:46.560 --> 00:08:47.680]   they may be.
[00:08:47.680 --> 00:08:50.960]   And usually it's not a very bad thing, because I mean, I hope it's not a bad thing.
[00:08:50.960 --> 00:08:54.480]   But oftentimes, we've all heard stories of, you know, things going awry when things in
[00:08:54.480 --> 00:08:55.480]   your background are being shown.
[00:08:55.480 --> 00:09:00.240]   So we thought to ourselves, when someone walks away from the camera, this now our software
[00:09:00.240 --> 00:09:02.560]   will automatically cover up the entire video stream.
[00:09:02.560 --> 00:09:03.560]   So it says away from keyboard.
[00:09:03.560 --> 00:09:07.480]   And when you walk back into frame, it'll detect it'll know that you showed back up, and then
[00:09:07.480 --> 00:09:08.620]   it'll remove the mask.
[00:09:08.620 --> 00:09:12.920]   So thus, this entire process is fully automated, no need to click a button, or click a like
[00:09:12.920 --> 00:09:16.920]   a mouse click or a keyboard click, all you got to do is just raise your hands or simply
[00:09:16.920 --> 00:09:20.720]   leave the camera and everything is done completely by itself because it's detecting every step
[00:09:20.720 --> 00:09:22.320]   of the way.
[00:09:22.320 --> 00:09:24.560]   Now I'm gonna pass the mic off to Andre.
[00:09:24.560 --> 00:09:25.560]   And yeah, take it away.
[00:09:25.560 --> 00:09:26.560]   Thanks, Ashwat.
[00:09:26.560 --> 00:09:30.440]   So for the second part of this project, we want to build a gesture classifier.
[00:09:30.440 --> 00:09:34.440]   And our plan was to use a CNN and train on some sort of hand gesture data set.
[00:09:34.440 --> 00:09:38.440]   What we found for this is this American Sign Language Alphabet data set from Kaggle.
[00:09:38.440 --> 00:09:42.060]   It has 29 classes with 3000 images per class.
[00:09:42.060 --> 00:09:45.640]   Now that sounds like a lot of images, but it's actually not in practice because these
[00:09:45.640 --> 00:09:49.400]   are frames to video, meaning adjacent images are almost identical.
[00:09:49.400 --> 00:09:53.340]   So to avoid wasting training time on those, we only use every 10th image from the data
[00:09:53.340 --> 00:09:54.340]   set.
[00:09:54.340 --> 00:09:57.860]   We also do extensive data augmentation because as you can see on the right, there's not a
[00:09:57.860 --> 00:10:02.620]   lot of variation to the shape of the hand and to the to the background that the data
[00:10:02.620 --> 00:10:03.660]   set uses.
[00:10:03.660 --> 00:10:07.940]   So we use color jitter to adjust the contrast, brightness and hue of these images.
[00:10:07.940 --> 00:10:12.140]   We use random rotation to change the orientation by some small random angle.
[00:10:12.140 --> 00:10:15.900]   And we use random resize crop to simulate what the effect when your hand moves further
[00:10:15.900 --> 00:10:19.340]   away or closer to the camera.
[00:10:19.340 --> 00:10:22.140]   Now when we were choosing the architecture, we had two main objectives in mind.
[00:10:22.140 --> 00:10:27.340]   The first is to choose a model with low model complexity and specifically fast feedforward
[00:10:27.340 --> 00:10:31.620]   runs because we want to be classifying on frames of a video in real time.
[00:10:31.620 --> 00:10:34.340]   So the faster the feedforward, the better.
[00:10:34.340 --> 00:10:38.300]   And the second objective was to choose a model that were more easily generalized to this
[00:10:38.300 --> 00:10:44.420]   task because as the problem, as I mentioned before, our main problem was the status of
[00:10:44.420 --> 00:10:45.820]   lacking variation.
[00:10:45.820 --> 00:10:49.660]   That's why we use convolutional filters that are pre-trained on ImageNet for this task
[00:10:49.660 --> 00:10:54.960]   because the images in ImageNet include all kinds of colors, textures and shapes.
[00:10:54.960 --> 00:10:59.340]   So these learned convolution filters should help us avoid overfitting.
[00:10:59.340 --> 00:11:02.420]   And it also helps with training speed because we won't be training all the filters from
[00:11:02.420 --> 00:11:03.420]   scratch.
[00:11:03.420 --> 00:11:04.740]   We use weights and biases to log our training.
[00:11:04.740 --> 00:11:08.940]   And what we found is that rather than doing a standard transfer learning where we freeze
[00:11:08.940 --> 00:11:12.980]   all the convolution filters during training, it was actually better to allow them to tune
[00:11:12.980 --> 00:11:20.360]   themselves a bit to the hand images, probably because this helps them specialize to detecting
[00:11:20.360 --> 00:11:26.460]   the specific shape of the fingers and palm.
[00:11:26.460 --> 00:11:30.580]   Now one technique that MobileNet V2 uses to reduce the number of parameters is what's
[00:11:30.580 --> 00:11:32.700]   called a depth-wise separable convolution.
[00:11:32.700 --> 00:11:36.440]   Now if you look at our standard convolutional layer, we have these kernels that convolve
[00:11:36.440 --> 00:11:38.580]   across all the channels of the input image.
[00:11:38.580 --> 00:11:41.660]   They essentially collapse the input channels into one output channel.
[00:11:41.660 --> 00:11:46.420]   And in order to produce more output channels, we would need more of these K by K by C kernels.
[00:11:46.420 --> 00:11:50.500]   On the other hand, a depth-wise separable convolution breaks down the convolution into
[00:11:50.500 --> 00:11:51.500]   two steps.
[00:11:51.500 --> 00:11:56.100]   The first is a per-channel convolution where you apply separate K by K kernels to each
[00:11:56.100 --> 00:12:01.460]   channel of the input image to produce this intermediate image of the same depth as your
[00:12:01.460 --> 00:12:02.460]   input.
[00:12:02.460 --> 00:12:06.380]   Then we use one-by-one convolutions to combine information across all channels of the intermediate
[00:12:06.380 --> 00:12:08.980]   image to produce the output image.
[00:12:08.980 --> 00:12:13.020]   And same as before, to have more output channels, we need more one-by-one convolutions.
[00:12:13.020 --> 00:12:17.540]   This already makes the advantage very apparent because now to have more output channels,
[00:12:17.540 --> 00:12:21.540]   you only need to make more of these one-by-one convolutions rather than the K by K.
[00:12:21.540 --> 00:12:27.420]   And this also drastically reduces how many multiplications we need to compute this convolution.
[00:12:27.420 --> 00:12:31.400]   And the other advantage is that now we're also adding more non-linear additives.
[00:12:31.400 --> 00:12:35.300]   We add an extra activation step at the one-by-one convolution.
[00:12:35.300 --> 00:12:38.820]   OK, cool.
[00:12:38.820 --> 00:12:43.180]   And so now Andre is going to try to get the live demo set up, which he's going to do from
[00:12:43.180 --> 00:12:45.060]   his live video camera.
[00:12:45.060 --> 00:12:47.140]   And in the meantime, I just want to go again through the results.
[00:12:47.140 --> 00:12:50.820]   So again, this is just to clarify, a fist means no, which you can see on the bottom
[00:12:50.820 --> 00:12:51.820]   left.
[00:12:51.820 --> 00:12:52.820]   A palm means I have a question.
[00:12:52.820 --> 00:12:55.820]   Naturally, people in high school, you raise your hand for a question.
[00:12:55.820 --> 00:13:01.060]   OK, we thought the perfect sign would be great for OK.
[00:13:01.060 --> 00:13:05.700]   And then when it comes to having a comment, we decided to just raise a finger, like I
[00:13:05.700 --> 00:13:08.260]   got something to say, one or two fingers.
[00:13:08.260 --> 00:13:10.980]   Away from keyboard, again, we just showed this so you can see what's happening.
[00:13:10.980 --> 00:13:14.260]   But pretty much this entire camera would be covered, as I showed in the demo in the GIF
[00:13:14.260 --> 00:13:15.540]   that I had.
[00:13:15.540 --> 00:13:18.460]   And then last but not least, we have the peace sign.
[00:13:18.460 --> 00:13:22.020]   And the peace sign, just again, peace sign, goodbye, makes a waving icon.
[00:13:22.020 --> 00:13:23.020]   And that should work.
[00:13:23.020 --> 00:13:25.300]   So yeah, that's the end of our presentation.
[00:13:25.300 --> 00:13:29.020]   I want to thank Weights and Biases for having us on this podcast.
[00:13:29.020 --> 00:13:30.380]   And again, MLAB.
[00:13:30.380 --> 00:13:35.060]   And Andre, are you ready?
[00:13:35.060 --> 00:13:37.180]   I just need to turn off my virtual background.
[00:13:37.180 --> 00:13:38.180]   OK, sweet.
[00:13:38.180 --> 00:13:40.940]   I'll stop sharing.
[00:13:40.940 --> 00:13:43.580]   So the first gesture is a fist.
[00:13:43.580 --> 00:13:44.580]   Should be no.
[00:13:44.580 --> 00:13:46.580]   An OK is yes.
[00:13:46.580 --> 00:13:49.260]   Palm is a question.
[00:13:49.260 --> 00:13:52.140]   And one finger up is comment.
[00:13:52.140 --> 00:13:58.140]   And a peace sign will be goodbye.
[00:13:58.140 --> 00:14:02.860]   Then when I move out of the frame, it should cover my screen with a graphic.
[00:14:02.860 --> 00:14:07.260]   When I come back, it should move away.
[00:14:07.260 --> 00:14:08.260]   Yep.
[00:14:08.260 --> 00:14:11.420]   That was our presentation.
[00:14:11.420 --> 00:14:12.420]   Thank you.
[00:14:12.420 --> 00:14:13.420]   Yeah, thank you.
[00:14:13.420 --> 00:14:14.420]   Great.
[00:14:14.420 --> 00:14:15.700]   Thank you so much, Andre and Ashwat.
[00:14:15.700 --> 00:14:19.020]   It's really impressive work that your team put together.
[00:14:19.020 --> 00:14:22.580]   And so I think it was a pretty short timeline as well.
[00:14:22.580 --> 00:14:26.380]   Can you say how long it took to develop that from concept to execution?
[00:14:26.380 --> 00:14:28.820]   Yeah, that's a great question.
[00:14:28.820 --> 00:14:31.300]   I think we did start-- I forgot.
[00:14:31.300 --> 00:14:33.820]   The semester has been quite a long semester.
[00:14:33.820 --> 00:14:35.940]   But I think we started around the beginning of November.
[00:14:35.940 --> 00:14:37.980]   And it's been about a month or so.
[00:14:37.980 --> 00:14:40.940]   One thing that was difficult, definitely the fact that half of our team was PST and half
[00:14:40.940 --> 00:14:43.260]   of it was in China and Singapore.
[00:14:43.260 --> 00:14:46.300]   So time zone differences, but we still made it work.
[00:14:46.300 --> 00:14:48.100]   And yeah, it was great.
[00:14:48.100 --> 00:14:49.100]   Gotcha.
[00:14:49.100 --> 00:14:54.340]   Yeah, I imagine that's an experience a lot of teams are having in this year.
[00:14:54.340 --> 00:14:56.980]   I've been doing a lot of online teaching.
[00:14:56.980 --> 00:15:03.580]   And some of that has been on Europe time, on Indian and China time.
[00:15:03.580 --> 00:15:08.620]   So yeah, it's been a wild year for time zone.
[00:15:08.620 --> 00:15:11.420]   And yeah, impressive that you put that together in just a few weeks.
[00:15:11.420 --> 00:15:13.580]   I had a couple of questions.
[00:15:13.580 --> 00:15:20.620]   So one about the-- you used a non-deep learning approach at the beginning for selecting out
[00:15:20.620 --> 00:15:23.220]   your bounding box.
[00:15:23.220 --> 00:15:27.820]   And so I wanted to hear a little bit more about that.
[00:15:27.820 --> 00:15:30.700]   How did you choose that particular hard cascade approach?
[00:15:30.700 --> 00:15:31.780]   How did you find it?
[00:15:31.780 --> 00:15:34.020]   What other options did you consider?
[00:15:34.020 --> 00:15:38.780]   And can you talk more about the decisions you made in that part of the process?
[00:15:38.780 --> 00:15:40.700]   Yeah, for sure.
[00:15:40.700 --> 00:15:41.940]   I guess I can take that one.
[00:15:41.940 --> 00:15:45.540]   So I guess in the beginning, yeah, when it came to hand face tracking, we initially--
[00:15:45.540 --> 00:15:49.100]   I think our team wanted to create our own from scratch.
[00:15:49.100 --> 00:15:52.060]   But then again, given the limited time frame, we decided to go off the shelf because the
[00:15:52.060 --> 00:15:53.980]   core of the project came from the ML side.
[00:15:53.980 --> 00:15:58.260]   But with that being said, in terms of alternatives, I definitely will admit that one of the--
[00:15:58.260 --> 00:15:59.260]   the hand tracking was amazing.
[00:15:59.260 --> 00:16:00.260]   I'll just admit that.
[00:16:00.260 --> 00:16:01.980]   Victor Dubia did a great job.
[00:16:01.980 --> 00:16:04.980]   But when it came to face tracking, I think one of the downfalls of hard cascades is,
[00:16:04.980 --> 00:16:08.980]   as I talked about in the slide of the hard features, if you're wearing sunglasses or
[00:16:08.980 --> 00:16:12.420]   maybe even a mask, which is common in this day and age, it blocks your nose, your mouth,
[00:16:12.420 --> 00:16:13.420]   and your eyes.
[00:16:13.420 --> 00:16:14.660]   And that can actually be a downfall.
[00:16:14.660 --> 00:16:19.180]   So we thought some other methods could be-- I believe there's other things like D-LIB
[00:16:19.180 --> 00:16:21.180]   is something in C++ that can work.
[00:16:21.180 --> 00:16:24.020]   There's something called MTCNN.
[00:16:24.020 --> 00:16:25.900]   I forgot exactly what that stood for.
[00:16:25.900 --> 00:16:29.020]   And most importantly, I think YOLOv3 is used for self-driving cars for detecting different
[00:16:29.020 --> 00:16:30.020]   objects.
[00:16:30.020 --> 00:16:31.180]   And that's more deep learning based.
[00:16:31.180 --> 00:16:36.060]   And if it was deep learning based, we could train it so that you could make a YOLOv3 version
[00:16:36.060 --> 00:16:38.660]   for while wearing a mask or while wearing sunglasses.
[00:16:38.660 --> 00:16:41.020]   So it makes it more robust overall.
[00:16:41.020 --> 00:16:42.020]   So yeah.
[00:16:42.020 --> 00:16:43.020]   I see.
[00:16:43.020 --> 00:16:47.300]   Was part of it also an attempt to cut down on the amount of computation?
[00:16:47.300 --> 00:16:52.460]   Is the hard cascade applied to the whole image cheaper than doing something like YOLOv3?
[00:16:52.460 --> 00:16:58.180]   Or has it advanced to the point where the computation difference isn't that big?
[00:16:58.180 --> 00:17:03.020]   So I believe that-- so we haven't exactly empirically tested the differences between,
[00:17:03.020 --> 00:17:05.380]   I guess, YOLO versus hard cascades.
[00:17:05.380 --> 00:17:07.540]   But I do believe hard cascades is pretty fast.
[00:17:07.540 --> 00:17:09.020]   I personally have used it in the past.
[00:17:09.020 --> 00:17:10.140]   And it's definitely an old algorithm.
[00:17:10.140 --> 00:17:14.700]   I think CVPR 2001 was when Mila and Jones made the initial algorithm and proposal on
[00:17:14.700 --> 00:17:15.700]   the paper.
[00:17:15.700 --> 00:17:18.100]   So definitely, we've come quite a far away.
[00:17:18.100 --> 00:17:20.620]   But yeah, I don't really know about whether it'd be faster.
[00:17:20.620 --> 00:17:22.100]   But for our purposes, it works.
[00:17:22.100 --> 00:17:26.860]   We're like, don't break something that doesn't-- keep something that works, I guess.
[00:17:26.860 --> 00:17:27.860]   Yeah.
[00:17:27.860 --> 00:17:28.860]   Yeah, definitely.
[00:17:28.860 --> 00:17:31.260]   There's an appeal to the simplicity of something like hard cascades.
[00:17:31.260 --> 00:17:36.900]   It looks like somebody in the chat is suggesting multitask cascaded convolutional neural networks
[00:17:36.900 --> 00:17:37.900]   as a potential option.
[00:17:37.900 --> 00:17:38.900]   Yes.
[00:17:38.900 --> 00:17:39.900]   Yeah, sorry.
[00:17:39.900 --> 00:17:40.900]   MTCNN, that's what I meant to say.
[00:17:40.900 --> 00:17:41.900]   That was-- yeah, that's one.
[00:17:41.900 --> 00:17:42.900]   Awesome.
[00:17:42.900 --> 00:17:43.900]   Yeah, thank you.
[00:17:43.900 --> 00:17:44.900]   Ah, OK.
[00:17:44.900 --> 00:17:45.900]   That's MTCNN.
[00:17:45.900 --> 00:17:46.900]   Cool.
[00:17:46.900 --> 00:17:47.900]   Cool.
[00:17:47.900 --> 00:17:53.580]   I guess, what would you think-- I don't know what your plans are with this particular project.
[00:17:53.580 --> 00:17:59.500]   But let's pretend that this was a minimum viable product at a company.
[00:17:59.500 --> 00:18:02.740]   What do you think your next steps would be with this project?
[00:18:02.740 --> 00:18:07.980]   What do you think does it need in order to be able to scale or able to be usable in different
[00:18:07.980 --> 00:18:11.020]   domains?
[00:18:11.020 --> 00:18:12.580]   What do you think you would do next?
[00:18:12.580 --> 00:18:20.460]   I think it would be cool if-- so now we have these few gestures that we had in mind when
[00:18:20.460 --> 00:18:21.980]   we were building this project.
[00:18:21.980 --> 00:18:27.340]   I think it would be cool to have a more general gesture recognizer that can perhaps learn
[00:18:27.340 --> 00:18:31.460]   to cast our particular gesture with just a few examples.
[00:18:31.460 --> 00:18:36.540]   So that way, different people using this application for different tasks, they can customize it
[00:18:36.540 --> 00:18:37.820]   to their own needs.
[00:18:37.820 --> 00:18:41.860]   So maybe they want to-- their line of work needs a particular signal.
[00:18:41.860 --> 00:18:44.740]   And they could customize it for that.
[00:18:44.740 --> 00:18:46.140]   Nice.
[00:18:46.140 --> 00:18:50.380]   So I also noticed the way-- it seems like the way you framed it, it's an image convolution
[00:18:50.380 --> 00:18:53.980]   class-- like, image classification task.
[00:18:53.980 --> 00:18:58.860]   Did you think at all about how you might extend this to video?
[00:18:58.860 --> 00:19:01.660]   Yeah.
[00:19:01.660 --> 00:19:03.660]   I had some idea about how to do it for video.
[00:19:03.660 --> 00:19:05.620]   But we didn't really have time to implement it.
[00:19:05.620 --> 00:19:10.180]   So I heard one way to do it is to have a CNN extract the features and then feed that into
[00:19:10.180 --> 00:19:15.180]   an RNN that takes different frames from the video, which then takes the time series and
[00:19:15.180 --> 00:19:18.340]   then produces some temporary result.
[00:19:18.340 --> 00:19:22.060]   Yeah, that's an interesting suggestion.
[00:19:22.060 --> 00:19:28.460]   I think if you had the compute transformer-type architecture, it could probably handle something
[00:19:28.460 --> 00:19:29.460]   like that.
[00:19:29.460 --> 00:19:33.020]   But getting that to work during somebody's live Zoom presentation sounds like a really
[00:19:33.020 --> 00:19:37.100]   tough challenge.
[00:19:37.100 --> 00:19:42.740]   There's a question in the YouTube chat, just the last one here.
[00:19:42.740 --> 00:19:44.460]   They are looking for slides.
[00:19:44.460 --> 00:19:47.020]   Will you be able to share these slides?
[00:19:47.020 --> 00:19:49.020]   Do you mind if I share them on the video?
[00:19:49.020 --> 00:19:50.780]   Yeah, most definitely.
[00:19:50.780 --> 00:19:52.420]   I can send you the link maybe after.
[00:19:52.420 --> 00:19:54.500]   And then we can make it public for all access.
[00:19:54.500 --> 00:19:55.500]   Yeah.
[00:19:55.500 --> 00:19:56.500]   Great.
[00:19:56.500 --> 00:19:58.500]   I'll put it on the video.
[00:19:58.500 --> 00:19:59.500]   Awesome.
[00:19:59.500 --> 00:20:00.500]   All right.
[00:20:00.500 --> 00:20:03.340]   So I'd love to be able to ask more questions of all of these groups.
[00:20:03.340 --> 00:20:07.060]   But we've got three different great presentations to talk about.
[00:20:07.060 --> 00:20:10.140]   So we've got to move on to the next one.
[00:20:10.140 --> 00:20:20.740]   So next up, we have a group that worked on semantic convolutions, on essentially alternatives
[00:20:20.740 --> 00:20:27.300]   to convolutional neural networks for computer vision tasks based, I believe, around capsule
[00:20:27.300 --> 00:20:28.300]   nets.
[00:20:28.300 --> 00:20:33.580]   So we'll be hearing from Domas and Kenny on this project.
[00:20:33.580 --> 00:20:34.580]   So go ahead.
[00:20:34.580 --> 00:20:35.580]   Take it away.
[00:20:35.580 --> 00:20:36.580]   Awesome.
[00:20:36.580 --> 00:20:37.580]   Thank you so much.
[00:20:37.580 --> 00:20:38.580]   Yeah.
[00:20:38.580 --> 00:20:43.380]   So we really explored the idea of semantic interpretation of convolutional networks.
[00:20:43.380 --> 00:20:46.980]   I'll just give you the next slide.
[00:20:46.980 --> 00:20:51.300]   With our objective being vision networks are largely not super interpretable.
[00:20:51.300 --> 00:20:56.180]   And this is more of a research-based project than some of the previous presentation.
[00:20:56.180 --> 00:21:02.540]   The intuition of hierarchical layers of progressively understanding more complicated features--
[00:21:02.540 --> 00:21:06.620]   you have an edge to a part detector to an object detector-- doesn't really empirically
[00:21:06.620 --> 00:21:07.620]   work out in practice.
[00:21:07.620 --> 00:21:09.660]   You end up getting this big mess.
[00:21:09.660 --> 00:21:12.980]   Some of these undesirable properties-- there's a whole laundry list here.
[00:21:12.980 --> 00:21:14.260]   We'll get into them in more detail.
[00:21:14.260 --> 00:21:17.300]   But the number of reliance on texture and a need for data augmentation, as you saw with
[00:21:17.300 --> 00:21:21.660]   the previous group, to show all different orientations of your hands and upside down
[00:21:21.660 --> 00:21:27.100]   and whatnot, waste in order for the network to really learn the full picture.
[00:21:27.100 --> 00:21:29.780]   Next slide.
[00:21:29.780 --> 00:21:37.100]   So here we have a single activated neuron, in a sense, to show how confused it can get.
[00:21:37.100 --> 00:21:40.740]   We have the same neuron looking at the eyes, the whiskers of the cat, the legs, and the
[00:21:40.740 --> 00:21:42.020]   fronts of the car.
[00:21:42.020 --> 00:21:47.820]   And so obviously, with the next slide, it's doing all three at once, which makes absolutely
[00:21:47.820 --> 00:21:53.620]   no sense, hence our quest for semantic meaning.
[00:21:53.620 --> 00:21:55.600]   So CNNs are brittle.
[00:21:55.600 --> 00:21:58.900]   With the Statue of Liberty, you understand that it is the Statue of Liberty, whether
[00:21:58.900 --> 00:22:01.300]   you see it from the back, whether you see it at nighttime with different shades and
[00:22:01.300 --> 00:22:02.300]   textures and colors.
[00:22:02.300 --> 00:22:07.580]   But that's not quite obvious to the network, hence the need for data augmentation, which
[00:22:07.580 --> 00:22:11.380]   really blows open gaping holes in modern architectures.
[00:22:11.380 --> 00:22:16.180]   We look to the kind of biology for some motivation.
[00:22:16.180 --> 00:22:19.580]   Next slide, please.
[00:22:19.580 --> 00:22:23.460]   The way that we do it is we reason in a set of invariant primitives.
[00:22:23.460 --> 00:22:25.500]   We don't think of objects in isolation.
[00:22:25.500 --> 00:22:30.780]   We think of them as things that are kind of detached from the orientation.
[00:22:30.780 --> 00:22:36.220]   And in actual brains, you have circuits that fire whenever the object in any orientation
[00:22:36.220 --> 00:22:37.220]   is present.
[00:22:37.220 --> 00:22:41.660]   If it's up side down, if it's backwards, if it's dark out, the same circuit fires for
[00:22:41.660 --> 00:22:43.660]   the cat.
[00:22:43.660 --> 00:22:45.340]   Next slide.
[00:22:45.340 --> 00:22:48.900]   And we do this by imposing geometric coordinate frames upon the object.
[00:22:48.900 --> 00:22:52.420]   So here's Jeffrey Hinton, the creator of the network we're going to show you.
[00:22:52.420 --> 00:22:56.660]   He walked around the halls of MIT with two blocks and asked professors to create a tetrahedron
[00:22:56.660 --> 00:22:57.900]   with those blocks.
[00:22:57.900 --> 00:23:02.780]   And none of these brilliant professors, leaders in their fields, could do so, even though
[00:23:02.780 --> 00:23:05.420]   they're geniuses, because they think in terms of coordinate frames.
[00:23:05.420 --> 00:23:07.420]   They think in terms of geometry.
[00:23:07.420 --> 00:23:12.180]   And the blocks in isolation look like nothing that can form a tetrahedron, yet when you
[00:23:12.180 --> 00:23:18.020]   make the tetrahedron, it's obvious because you impose a coordinate frame, you impose
[00:23:18.020 --> 00:23:19.020]   geometry.
[00:23:19.020 --> 00:23:20.020]   Next slide.
[00:23:20.020 --> 00:23:23.100]   So the solution is Hinton's Cat's Law Networks impose geometric structure on CNNs.
[00:23:23.100 --> 00:23:27.540]   So you learn object representations invariant to viewpoint.
[00:23:27.540 --> 00:23:30.300]   Your object does not depend on the way that you look at it.
[00:23:30.300 --> 00:23:32.300]   It just exists.
[00:23:32.300 --> 00:23:34.620]   Next slide.
[00:23:34.620 --> 00:23:37.580]   So empirically, these networks are very data efficient.
[00:23:37.580 --> 00:23:39.660]   You don't have to augment your data set.
[00:23:39.660 --> 00:23:42.340]   You look at an object once and you understand the way that it works.
[00:23:42.340 --> 00:23:46.300]   And because of this, it's also robust to adversarial attacks and shifts in the way that you present
[00:23:46.300 --> 00:23:48.020]   your input to your network.
[00:23:48.020 --> 00:23:49.020]   And there's less redundancy.
[00:23:49.020 --> 00:23:53.220]   And you get state of the art performance with an order of magnitude less data.
[00:23:53.220 --> 00:23:58.180]   I'm going to turn it over to Damas to explain some of what we did.
[00:23:58.180 --> 00:23:59.860]   All right.
[00:23:59.860 --> 00:24:04.220]   So here's a bit of an example of what some of the low-level feature maps look like in
[00:24:04.220 --> 00:24:05.220]   CNNs.
[00:24:05.220 --> 00:24:06.620]   And you can see that there's a lot of redundancy.
[00:24:06.620 --> 00:24:13.500]   There's a lot of these same frequency activations, which are just rotations of each other.
[00:24:13.500 --> 00:24:18.060]   And you see similar things in the color spaces as well.
[00:24:18.060 --> 00:24:24.500]   So that's something that we're trying to avoid with our usage of capsule networks.
[00:24:24.500 --> 00:24:29.460]   So the paper that we're re-implementing is stacked capsule autoencoders.
[00:24:29.460 --> 00:24:35.220]   And this architecture is a combination of two submodels, which is a part capsule autoencoder
[00:24:35.220 --> 00:24:37.580]   and an object capsule autoencoder.
[00:24:37.580 --> 00:24:42.700]   The first model, it predicts parts and their poses.
[00:24:42.700 --> 00:24:48.340]   So the parts here are these different little bits that compose the house and tree and such.
[00:24:48.340 --> 00:24:50.580]   And these are the poses that are then predicted.
[00:24:50.580 --> 00:24:54.220]   Then the second part of the model predicts objects.
[00:24:54.220 --> 00:24:59.080]   So it associates each of these parts with an object.
[00:24:59.080 --> 00:25:03.940]   So it can find that this tree is composed of this leaf part as well as a bottom trunk
[00:25:03.940 --> 00:25:04.940]   part.
[00:25:04.940 --> 00:25:10.500]   So I'm going to go into a bit more detail on the part capsule autoencoder.
[00:25:10.500 --> 00:25:13.140]   So this is the first stage, which predicts the positions.
[00:25:13.140 --> 00:25:17.620]   It actually does this using a CNN.
[00:25:17.620 --> 00:25:24.280]   And it also uses a decoder, which learns templates.
[00:25:24.280 --> 00:25:29.640]   So in the forward pass of the encoder, you get these poses as well as some descriptive
[00:25:29.640 --> 00:25:31.920]   features about each of the parts.
[00:25:31.920 --> 00:25:36.960]   And these are reassembled in the decoder with these templates to produce a reconstruction.
[00:25:36.960 --> 00:25:40.600]   And this is optimized with the log likelihood loss.
[00:25:40.600 --> 00:25:45.080]   And here's some visualizations of what the composition looks like.
[00:25:45.080 --> 00:25:51.580]   So these are the learned templates or parts, which are then transformed and then concatenated
[00:25:51.580 --> 00:25:53.800]   together to form this reconstruction.
[00:25:53.800 --> 00:26:02.640]   You can track using this color coding how each thing comes together to have this final
[00:26:02.640 --> 00:26:03.760]   result.
[00:26:03.760 --> 00:26:05.760]   And this is the reconstruction without the colorization.
[00:26:05.760 --> 00:26:08.880]   So you can see it's a very realistic looking tree.
[00:26:08.880 --> 00:26:13.120]   And you can interpret these templates that it learns as strokes.
[00:26:13.120 --> 00:26:16.160]   The transformations are like the strokes on canvas.
[00:26:16.160 --> 00:26:20.600]   And your final thing is the digit, of course.
[00:26:20.600 --> 00:26:25.920]   So during our experimentation, we studied the evolution of these different parts, which
[00:26:25.920 --> 00:26:26.920]   are learned.
[00:26:26.920 --> 00:26:29.440]   So at the very start, it starts with this random noise initialization.
[00:26:29.440 --> 00:26:33.440]   Then it gradually gets chiseled out into more meaningful parts.
[00:26:33.440 --> 00:26:36.080]   So here you can see roughly there's an edge.
[00:26:36.080 --> 00:26:37.760]   Here we have some double edges.
[00:26:37.760 --> 00:26:40.160]   Here we have a more flat looking edge.
[00:26:40.160 --> 00:26:44.120]   And here we have a very circular object.
[00:26:44.120 --> 00:26:46.520]   OK.
[00:26:46.520 --> 00:26:52.840]   And over time, you can see that at the very top, the transformations of each of these
[00:26:52.840 --> 00:26:55.040]   parts are pretty nonsensical and naive.
[00:26:55.040 --> 00:26:58.360]   It's totally random noise just getting warped around.
[00:26:58.360 --> 00:27:03.560]   But gradually, they get positioned more and more sensibly.
[00:27:03.560 --> 00:27:08.320]   And as the templates at the same time are chiseled out, you end up with these very high
[00:27:08.320 --> 00:27:10.120]   quality reconstructions.
[00:27:10.400 --> 00:27:16.720]   On the very left, on the left column, these are the target images.
[00:27:16.720 --> 00:27:18.320]   And the right are the reconstructions.
[00:27:18.320 --> 00:27:22.200]   You can see that they're reconstructed fairly faithfully.
[00:27:22.200 --> 00:27:26.920]   And at the very bottom, you can see the corresponding templates, which are being utilized for this
[00:27:26.920 --> 00:27:28.960]   last reconstruction.
[00:27:28.960 --> 00:27:30.720]   OK.
[00:27:30.720 --> 00:27:35.920]   So the second part of the SCE model is the object capsule autoencoder, which takes each
[00:27:35.920 --> 00:27:43.760]   of these poses of parts and also the feature information and tries to explain the sort
[00:27:43.760 --> 00:27:45.600]   of mess of parts.
[00:27:45.600 --> 00:27:50.680]   So it associates them with objects, learning the relative positioning of each of the parts
[00:27:50.680 --> 00:27:51.940]   in the object.
[00:27:51.940 --> 00:28:01.120]   And then using this model, we want to optimize the likelihood of what it sees.
[00:28:01.120 --> 00:28:10.280]   So in the final model, we train both autoencoders jointly to optimize the task for unsupervised
[00:28:10.280 --> 00:28:12.160]   object classification.
[00:28:12.160 --> 00:28:18.520]   So empirically, the paper gets really good results on the MNIST and SVHN.
[00:28:18.520 --> 00:28:21.880]   There's still quite a bit of room for improvement, which is what we're trying to address with
[00:28:21.880 --> 00:28:23.640]   this project.
[00:28:23.640 --> 00:28:27.840]   So one of the issues is the engineering difficulty.
[00:28:27.840 --> 00:28:31.080]   The original implementation was in TensorFlow.
[00:28:31.080 --> 00:28:34.880]   And it's quite messy, because it's research quality code.
[00:28:34.880 --> 00:28:37.040]   So we ported that to PyTorch.
[00:28:37.040 --> 00:28:42.160]   And so far, we finished the PCAE reimplementation, which you saw the results of earlier.
[00:28:42.160 --> 00:28:47.080]   And we're currently working on the OCE re-implementation.
[00:28:47.080 --> 00:28:52.440]   We're also working on other improvements, such as reducing the training time, the OCE,
[00:28:52.440 --> 00:28:56.520]   as well as improving the depth of the network.
[00:28:56.520 --> 00:29:01.640]   We want to be able to stack multiple OCAEs after the network, so that we can get more
[00:29:01.640 --> 00:29:06.280]   rich compositionality in our network structure.
[00:29:06.280 --> 00:29:08.720]   We also did quite a bit of hyperparameter tuning.
[00:29:08.720 --> 00:29:13.800]   And we found quite a few interesting key engineering decisions that the paper doesn't really talk
[00:29:13.800 --> 00:29:15.360]   much about.
[00:29:15.360 --> 00:29:18.880]   Yeah, so this is the work we did.
[00:29:18.880 --> 00:29:23.040]   The implementation, hyperparams, found some interesting factors.
[00:29:23.040 --> 00:29:27.400]   We also did some experiments on USPS, which is very much like MNIST.
[00:29:27.400 --> 00:29:29.680]   But it's a bit different stylistically.
[00:29:29.680 --> 00:29:31.640]   So we verified that it works there.
[00:29:31.640 --> 00:29:36.280]   And we're also working on extending reconstructions to color images.
[00:29:36.280 --> 00:29:39.520]   They never do anything with color in their work.
[00:29:39.520 --> 00:29:43.720]   OK, so Kenny also did some optimizations.
[00:29:43.720 --> 00:29:45.000]   So let him explain this.
[00:29:45.000 --> 00:29:50.600]   Yeah, so basically, the loss is driven by a closed form log likelihood that's evaluating
[00:29:50.600 --> 00:29:55.280]   the efficacy of capsules and routing their parts to the correct capsule, which is a very
[00:29:55.280 --> 00:29:56.640]   complicated equation.
[00:29:56.640 --> 00:30:01.160]   And as you can see, when calculated in closed form, it's a consider amount of computation.
[00:30:01.160 --> 00:30:04.840]   We can leverage a technique called Markov chain Monte Carlo simulation to sample from
[00:30:04.840 --> 00:30:10.040]   this distribution and create an approximate distribution, from which we can then pick
[00:30:10.040 --> 00:30:16.440]   off the peak of the distribution, the maximum a posteriori, the map, if you will, to route
[00:30:16.440 --> 00:30:21.360]   our capsule to the part from our sample distribution, such that we can use less computation.
[00:30:21.360 --> 00:30:22.720]   This is one of the things we're exploring.
[00:30:22.720 --> 00:30:28.440]   We're also exploring using adjustment argmax value to drive loss in the back propagation
[00:30:28.440 --> 00:30:32.280]   instead of a closed form log likelihood, which we speculate.
[00:30:32.280 --> 00:30:36.080]   And we have synthetic preliminary data to show it's not entirely necessary.
[00:30:36.080 --> 00:30:38.400]   And it would save a lot of time and training.
[00:30:38.400 --> 00:30:44.720]   All right, so some future work that we're looking to do is finishing up our PyTorch
[00:30:44.720 --> 00:30:45.720]   part.
[00:30:45.720 --> 00:30:47.600]   And we have some big ideas which we're shooting for.
[00:30:47.600 --> 00:30:49.840]   So one is the RGB data set performance.
[00:30:49.840 --> 00:30:55.400]   Hopefully, we can get this to speed up to work on the actual OCE model.
[00:30:55.400 --> 00:30:59.800]   We also want to add more capsule layers that we can scale to more complex data sets, like
[00:30:59.800 --> 00:31:03.080]   for object classification in ImageNet or Cypher.
[00:31:03.080 --> 00:31:07.440]   And we also want to modernize the PCAE CNN.
[00:31:07.440 --> 00:31:10.480]   Right now, it's a very simple, small CNN architecture.
[00:31:10.480 --> 00:31:13.760]   So yeah, that's all.
[00:31:13.760 --> 00:31:17.600]   Any questions?
[00:31:17.600 --> 00:31:19.960]   I've got a lot of questions.
[00:31:19.960 --> 00:31:24.640]   So we'll try and squeeze as many of them as we can in the minutes that we have.
[00:31:24.640 --> 00:31:30.960]   So you mentioned that it was hard to train because it had a strange loss dynamics.
[00:31:30.960 --> 00:31:34.400]   Could you unpack that a little bit?
[00:31:34.400 --> 00:31:38.800]   Maybe I'll talk about the actual strain of the computation that Demos can talk about.
[00:31:38.800 --> 00:31:43.280]   It's kind of the dynamics of carrying the encoder likelihoods.
[00:31:43.280 --> 00:31:49.040]   So the likelihood itself is a very-- it's a closed form distribution that you have to
[00:31:49.040 --> 00:31:53.480]   calculate with each forward pass in closed form and then derive the argmax from that
[00:31:53.480 --> 00:31:56.480]   value or from that distribution each time.
[00:31:56.480 --> 00:31:59.800]   In a sense, it's just a cumbersome way.
[00:31:59.800 --> 00:32:04.360]   You can't leverage sort of automatic differentiation techniques to do things speedily.
[00:32:04.360 --> 00:32:06.120]   That's the problem we ran into.
[00:32:06.120 --> 00:32:09.320]   And then Demos also ran into some issues with the PCAE.
[00:32:09.320 --> 00:32:15.960]   Yeah, so what we spent the most time on turning the PCAE is actually getting a nice balance
[00:32:15.960 --> 00:32:22.000]   between the chiseling of these templates.
[00:32:22.000 --> 00:32:28.680]   So some extremes that we had to sort of dig ourselves out of is one, in some cases, the
[00:32:28.680 --> 00:32:31.160]   templates weren't actually in frame.
[00:32:31.160 --> 00:32:36.560]   So it never quite latched on and started training the model to actually make usage of each of
[00:32:36.560 --> 00:32:37.560]   these templates.
[00:32:37.560 --> 00:32:40.280]   So they ended up just being totally unused in some cases.
[00:32:40.280 --> 00:32:46.000]   In other cases, we had way too much transformation and had really weird transform parts.
[00:32:46.000 --> 00:32:50.600]   Like we had super zoomed in templates, which had some interesting behaviors.
[00:32:50.600 --> 00:32:57.720]   Another one was-- the one is just we just didn't have any transformation going on.
[00:32:57.720 --> 00:33:01.560]   So there's a very fine balance to strike in between those two.
[00:33:01.560 --> 00:33:03.040]   I see.
[00:33:03.400 --> 00:33:09.960]   Do you think that just it's a matter of picking the right sort of hyperparameters and defaults
[00:33:09.960 --> 00:33:16.240]   to make these capsule nets behave a little bit better during training?
[00:33:16.240 --> 00:33:20.360]   The way that we've sort of come across the right ways to initialize layers, using things
[00:33:20.360 --> 00:33:23.560]   like batch norm, et cetera, to make confets easy to train?
[00:33:23.560 --> 00:33:27.600]   Or do you think that it's inherently just going to be harder to train something like
[00:33:27.600 --> 00:33:32.440]   a capsule net than something like a feedforward regular convolutional net?
[00:33:32.440 --> 00:33:41.480]   So we found that actually after we made one key discovery, which is that we want to learn
[00:33:41.480 --> 00:33:47.600]   the transformations or the transformations outputted by our CNN need to be an inverse
[00:33:47.600 --> 00:33:48.600]   space.
[00:33:48.600 --> 00:33:52.280]   So basically, we output some 3 by 3 transformation matrix.
[00:33:52.280 --> 00:33:54.720]   We invert that and then use that.
[00:33:54.720 --> 00:34:00.160]   And basically what that means is you can think of it as the space which you're optimizing
[00:34:00.160 --> 00:34:03.360]   over has a totally different morph to it.
[00:34:03.360 --> 00:34:08.080]   So it's going to be expanded in some regions and shrunk on some other regions.
[00:34:08.080 --> 00:34:11.200]   So because of that, you get a much better loss surface to optimize over.
[00:34:11.200 --> 00:34:15.080]   And everything just starts magically working.
[00:34:15.080 --> 00:34:18.920]   And to also answer your question in terms of tooling, I think some things will need
[00:34:18.920 --> 00:34:19.920]   to change a little bit.
[00:34:19.920 --> 00:34:25.320]   You're not building these vector Jacobian product type computations.
[00:34:25.320 --> 00:34:30.880]   You're really looking at this likelihood and there's going to need some more well-defined
[00:34:30.880 --> 00:34:33.200]   computational building blocks to make that happen.
[00:34:33.200 --> 00:34:37.000]   We're going to pursue sampling to see if we can get an approximate answer to that question,
[00:34:37.000 --> 00:34:39.560]   especially because we don't have to be super specific.
[00:34:39.560 --> 00:34:45.000]   When we generate this distribution over capsules which are good, we don't need to be very specific
[00:34:45.000 --> 00:34:47.200]   over the region where the map is.
[00:34:47.200 --> 00:34:51.160]   We can be in a general area to route the correct capsule.
[00:34:51.160 --> 00:34:53.320]   So that suggests that this could work.
[00:34:53.320 --> 00:34:55.560]   But there are other things we need to look at as well.
[00:34:55.560 --> 00:34:56.560]   I see.
[00:34:56.560 --> 00:34:57.560]   Yeah.
[00:34:57.560 --> 00:35:06.200]   To Damas' point, I know that in other domains, actually sampling, the parameterization, it's
[00:35:06.200 --> 00:35:09.480]   very well known that the parameterization of your model is very important.
[00:35:09.480 --> 00:35:14.800]   And there are tricks like taking inverses and taking logarithms that help a lot in getting
[00:35:14.800 --> 00:35:16.680]   a well-conditioned loss surface.
[00:35:16.680 --> 00:35:20.800]   So it could be, yeah, that those are the kinds of things that need to be discovered.
[00:35:20.800 --> 00:35:27.880]   And to your point, yeah, there's a lot more tooling that could be built for probabilistic
[00:35:27.880 --> 00:35:32.560]   programming and for differentiable probabilistic programming that could probably unlock a lot
[00:35:32.560 --> 00:35:38.680]   more capacity in things like capsule nets that might marry those two approaches.
[00:35:38.680 --> 00:35:39.680]   Yeah.
[00:35:39.680 --> 00:35:45.640]   Like we were just using the PyMC3 Bayesian stat package, which had no sort of fine-tuned
[00:35:45.640 --> 00:35:46.640]   computational optimization.
[00:35:46.640 --> 00:35:49.040]   We still got decent results with synthetic data.
[00:35:49.040 --> 00:35:51.600]   So some improvement there.
[00:35:51.600 --> 00:35:52.600]   Definitely.
[00:35:52.600 --> 00:35:55.680]   There's a question in the chat.
[00:35:55.680 --> 00:36:02.720]   Testing semantic correctness can be via saliency on expected inputs, like on MNIST, the difference
[00:36:02.720 --> 00:36:07.200]   between digits 5 and 6, see that the circle of the 6 is complete or not.
[00:36:07.200 --> 00:36:08.880]   Have you looked into this?
[00:36:08.880 --> 00:36:12.720]   I guess they're asking, you mentioned that you wanted to make confidence more interpretable.
[00:36:12.720 --> 00:36:18.000]   Have you looked at the behavior of your capsule net to see if it is more interpretable, if
[00:36:18.000 --> 00:36:24.400]   you see these sorts of semantic distinctions like they're talking about?
[00:36:24.400 --> 00:36:28.600]   So we haven't used any of the techniques which are standard for CNNs.
[00:36:28.600 --> 00:36:33.740]   In fact, in our surveys, we actually found that there's a good amount of controversy
[00:36:33.740 --> 00:36:37.360]   as to what these visualizations are actually doing.
[00:36:37.360 --> 00:36:44.040]   So we wanted to try and move away from these post-hoc attempts to understand what's happening
[00:36:44.040 --> 00:36:47.840]   so that we can build the understanding into the network.
[00:36:47.840 --> 00:36:51.840]   So what the network is doing is clear from the structure of the network.
[00:36:51.840 --> 00:36:57.520]   You don't have to do any inspection afterwards.
[00:36:57.520 --> 00:36:58.520]   I see.
[00:36:58.520 --> 00:37:01.600]   That's an interesting point.
[00:37:01.600 --> 00:37:06.600]   And that actually brings me to the last question, or one of the major questions that I had,
[00:37:06.600 --> 00:37:08.840]   which is also a tough question.
[00:37:08.840 --> 00:37:14.840]   The vibe in the ML community over the last year has shifted really heavily away from
[00:37:14.840 --> 00:37:22.240]   inductive bias with the success of transformer models on multiple domains where they don't
[00:37:22.240 --> 00:37:26.720]   have necessarily a super great inductive bias.
[00:37:26.720 --> 00:37:34.280]   And that it's, which sparked, I believe, Richard Sutton's famous post, The Bitter Lesson, that
[00:37:34.280 --> 00:37:41.400]   in the end, scaling compute tends to beat out these architectures with special things
[00:37:41.400 --> 00:37:42.400]   built into them.
[00:37:42.400 --> 00:37:47.000]   So I'm curious, do you think that that's a fad?
[00:37:47.000 --> 00:37:56.320]   Or how do you think that this taste for transformers and generic architectures dovetails with,
[00:37:56.320 --> 00:38:03.360]   broadly dovetails with your particular interest in these more biased, structured architectures?
[00:38:03.360 --> 00:38:04.560]   I think there's a lot of parallel.
[00:38:04.560 --> 00:38:05.880]   I think these are almost the same thing.
[00:38:05.880 --> 00:38:08.200]   At least they're converging towards the same approach.
[00:38:08.200 --> 00:38:12.200]   Attention mechanisms are taking permutation invariant sets of objects and deriving importance
[00:38:12.200 --> 00:38:16.360]   that are, irrespective of how far away those things are apart from each other.
[00:38:16.360 --> 00:38:19.560]   The same essential mechanisms happening when a capsule is voting on a part.
[00:38:19.560 --> 00:38:25.520]   And in fact, the architecture, it draws a lot of similarities with hierarchical temporal
[00:38:25.520 --> 00:38:26.520]   models.
[00:38:26.520 --> 00:38:28.960]   So those Bayesian inference type models that are biologically inspired.
[00:38:28.960 --> 00:38:33.720]   All three of them seem to be converging towards that general trend of attention.
[00:38:33.720 --> 00:38:37.800]   I think that's where this is all going.
[00:38:37.800 --> 00:38:43.560]   Another counterpoint is that models with really good bias typically have better data efficiency.
[00:38:43.560 --> 00:38:45.320]   So this was actually shown in capsules.
[00:38:45.320 --> 00:38:54.040]   They're actually an order of magnitude better than other typical approaches to MNIST classification.
[00:38:54.040 --> 00:38:55.400]   That's a good point.
[00:38:55.400 --> 00:39:00.000]   And to Kenny's point, so the research group that I worked at when I was a grad student
[00:39:00.000 --> 00:39:03.640]   at Berkeley, the Redwood Center for Theoretical Neuroscience, is very interested in a lot
[00:39:03.640 --> 00:39:07.600]   of the ideas that you've mentioned, dynamic routing, hierarchical temporal memory.
[00:39:07.600 --> 00:39:13.120]   It was originally created by Jeff Hawkins of Numenta.
[00:39:13.120 --> 00:39:17.080]   So if you haven't swung by the Redwood Center's Zoom meetings, you definitely should email
[00:39:17.080 --> 00:39:19.880]   me afterwards if you want to hear more about that.
[00:39:19.880 --> 00:39:23.440]   That's a good point.
[00:39:23.440 --> 00:39:26.400]   The attention mechanism there is very, very similar.
[00:39:26.400 --> 00:39:27.400]   It's all about invariance.
[00:39:27.400 --> 00:39:29.760]   I mean, that's what Hawkins talks about in his book, right?
[00:39:29.760 --> 00:39:33.440]   That's the key to biological interpretation is learning invariant representation of objects.
[00:39:33.440 --> 00:39:36.640]   So all these things are doing.
[00:39:36.640 --> 00:39:38.200]   All right.
[00:39:38.200 --> 00:39:40.720]   Well, that is all the time that we have.
[00:39:40.720 --> 00:39:44.720]   So we'll have to continue that interesting conversation on a later date.
[00:39:44.720 --> 00:39:50.020]   So thanks to Kenny and Damas and their team for presenting that cool work on capsule nets.
[00:39:50.020 --> 00:39:53.720]   And let's go to our final set of speakers.
[00:39:53.720 --> 00:39:57.360]   So we'll have...
[00:39:57.360 --> 00:40:02.640]   So we just had Damas and Kenny, and now we have, I believe, Pedro and Oliver, who will
[00:40:02.640 --> 00:40:08.360]   be presenting their work on adversarial attacks.
[00:40:08.360 --> 00:40:15.760]   So also a little bit more research-oriented than our first video gesture overlay, but
[00:40:15.760 --> 00:40:19.520]   maybe sort of in the intersection of what's of interest to researchers and what's of interest
[00:40:19.520 --> 00:40:21.340]   to folks doing ML engineering.
[00:40:21.340 --> 00:40:23.800]   So Oliver and Pedro, take it away.
[00:40:23.800 --> 00:40:25.400]   Thank you.
[00:40:25.400 --> 00:40:29.240]   Let me share screens.
[00:40:29.240 --> 00:40:32.400]   Okay.
[00:40:32.400 --> 00:40:41.960]   So my name is Oliver, and this semester, we have been...
[00:40:41.960 --> 00:40:47.520]   My team and I, me, Vincent, Nabil, and Pedro, have been working with Nicholas Carlini on
[00:40:47.520 --> 00:40:50.580]   attacking adversarial defenses.
[00:40:50.580 --> 00:40:54.560]   And I know that's very general, if you know anything about the field.
[00:40:54.560 --> 00:40:59.100]   And so we're trying to own a specific type of defense and try and come up with a more
[00:40:59.100 --> 00:41:06.200]   general attack framework so that we can have a better metric in a lot of these papers,
[00:41:06.200 --> 00:41:10.000]   because some of these defenses claim to work really well, but they don't actually attack
[00:41:10.000 --> 00:41:14.980]   themselves very well, because that doesn't really help their claims.
[00:41:14.980 --> 00:41:20.420]   And we really want a better, more principled way to attack these defenses.
[00:41:20.420 --> 00:41:21.880]   So first some preliminaries.
[00:41:21.880 --> 00:41:25.720]   So in case you don't know, an adversarial attack on a neural network is a perturbation
[00:41:25.720 --> 00:41:30.980]   of some input, which causes the network to misclassify the input.
[00:41:30.980 --> 00:41:35.160]   So in this image at the bottom, you have all of these inputs, and the line is splitting
[00:41:35.160 --> 00:41:36.220]   them up well.
[00:41:36.220 --> 00:41:39.160]   And then if you allow some perturbations, you allow them to move around, then you can
[00:41:39.160 --> 00:41:41.440]   get misclassifications.
[00:41:41.440 --> 00:41:44.140]   And then we make sure that the constraint...
[00:41:44.140 --> 00:41:48.720]   We make sure there's a constraint so that the perturbation isn't too big, so the original
[00:41:48.720 --> 00:41:50.620]   images don't get too far away.
[00:41:50.620 --> 00:41:53.960]   And this is a really big vulnerability in safety-critical fields.
[00:41:53.960 --> 00:41:58.720]   For example, self-driving cars, because you can have things like hiding a stop sign from
[00:41:58.720 --> 00:42:02.520]   the computer vision model of the car, and that's really, really dangerous in such a
[00:42:02.520 --> 00:42:03.560]   safety-critical field.
[00:42:03.560 --> 00:42:07.240]   So defending against these kinds of attacks is really important.
[00:42:07.240 --> 00:42:13.600]   So here is a sample attack, just a really simple example that's used for a lot of just
[00:42:13.600 --> 00:42:14.600]   like prototyping.
[00:42:14.600 --> 00:42:17.200]   It's called the fast gradient sign method.
[00:42:17.200 --> 00:42:21.560]   And what you're doing is you take the image, and then instead of doing gradient descent
[00:42:21.560 --> 00:42:25.680]   on the parameters of a neural network like you'd normally do when training it, you do
[00:42:25.680 --> 00:42:27.960]   gradient descent on the image itself.
[00:42:27.960 --> 00:42:31.680]   So you take the gradient with respect to the image, and then you add that instead.
[00:42:31.680 --> 00:42:35.920]   And what this does is it forces your loss to increase when you add this perturbation.
[00:42:35.920 --> 00:42:39.080]   And so as you can see in this example, we take the original image, we add a bunch of
[00:42:39.080 --> 00:42:44.400]   what looks like random noise, but it's actually specifically selected to attack this model.
[00:42:44.400 --> 00:42:48.480]   And it doesn't look like the image has changed at all, but it completely changes the classification
[00:42:48.480 --> 00:42:50.880]   and increases the confidence drastically.
[00:42:50.880 --> 00:42:52.840]   And this is really dangerous.
[00:42:52.840 --> 00:42:59.120]   And so finally, here is an example defense, which was proposed in Madry 2017, which is
[00:42:59.120 --> 00:43:05.080]   one of the most important papers in this field, where you can use FGSM or some other similar
[00:43:05.080 --> 00:43:10.320]   attacks and use it as like a form of data augmentation during the training, where you
[00:43:10.320 --> 00:43:11.320]   do your normal training.
[00:43:11.320 --> 00:43:15.200]   And then in the middle, you take some examples, you take your batch that you're training on,
[00:43:15.200 --> 00:43:16.960]   and then you get some adversary.
[00:43:16.960 --> 00:43:20.720]   You attack them while you're training using the current model, and then you pass those
[00:43:20.720 --> 00:43:22.560]   in as more training examples.
[00:43:22.560 --> 00:43:26.080]   And then you hope that this makes your model more robust and gives you like a more nonlinear
[00:43:26.080 --> 00:43:28.880]   surface like this that fits the perturbations better.
[00:43:28.880 --> 00:43:31.960]   And then I'm going to pass it off to Pedro.
[00:43:31.960 --> 00:43:36.440]   Okay, so now that we see why having adversarial robustness is important, let's talk about
[00:43:36.440 --> 00:43:38.320]   why it's hard to do.
[00:43:38.320 --> 00:43:42.840]   So looking at this from the attacker's perspective, we see that all they really have to do is
[00:43:42.840 --> 00:43:47.800]   to maximize this equation, which really means that they have to maximize the loss, adding
[00:43:47.800 --> 00:43:53.160]   some perturbation onto the original image, the loss compared to some truth value.
[00:43:53.160 --> 00:43:57.720]   And so we have this loss being very high, and the perturbation be very small or within
[00:43:57.720 --> 00:43:58.720]   some bound.
[00:43:58.720 --> 00:44:03.480]   The first perspective here is that it's hard to construct a theoretical model of what the
[00:44:03.480 --> 00:44:07.760]   attacker is doing, and how they actually create these adversarial examples.
[00:44:07.760 --> 00:44:13.000]   This optimization problem is nonlinear and non-convex, so it's hard to generalize overall
[00:44:13.000 --> 00:44:17.520]   to see how different attackers make these adversarial examples.
[00:44:17.520 --> 00:44:21.360]   The second perspective is that adversarial robustness, which means just protecting yourself
[00:44:21.360 --> 00:44:26.360]   against adversarial attacks, requires models to have good outputs for all inputs.
[00:44:26.360 --> 00:44:30.960]   And it's pretty easy for an attacker to make an adversarial attack, but it's very hard
[00:44:30.960 --> 00:44:38.040]   for a defender to defend against all possible adversarial attacks.
[00:44:38.040 --> 00:44:40.920]   And here we have an example of adversarial detection.
[00:44:40.920 --> 00:44:46.320]   What this is, is what if we could have a layer that before we input anything into our model,
[00:44:46.320 --> 00:44:50.680]   we have a layer that tells you that yes, this is an adversarial attack, or no, this is a
[00:44:50.680 --> 00:44:53.120]   good piece of input that we want to continue with.
[00:44:53.120 --> 00:44:56.680]   And so this picture shows just a layer in between the adversarial example and the actual
[00:44:56.680 --> 00:44:59.000]   model that will tell us that information.
[00:44:59.000 --> 00:45:03.080]   And the problem with this is that some models are not able to ignore this input.
[00:45:03.080 --> 00:45:07.540]   And so what this layer would do is, if it's good, pass it on, if it's bad, throw it away.
[00:45:07.540 --> 00:45:12.160]   But if we go back to the self-driving car example, the self-driving cars can't throw away an
[00:45:12.160 --> 00:45:14.140]   image, they have to make a decision.
[00:45:14.140 --> 00:45:17.960]   If they see a stop sign, they can't believe that it's adversarial and just do nothing
[00:45:17.960 --> 00:45:21.040]   about it.
[00:45:21.040 --> 00:45:25.100]   And so talking about some very rudimentary defenses, the idea here is that what if we
[00:45:25.100 --> 00:45:30.500]   have in between the adversarial example and our actual model, a layer that pre-processes
[00:45:30.500 --> 00:45:31.500]   the image.
[00:45:31.500 --> 00:45:35.560]   So this is not exactly throwing away the image, this is just pre-processing it, throwing it
[00:45:35.560 --> 00:45:37.000]   through some function.
[00:45:37.000 --> 00:45:39.320]   And in this case, we'll look at the Gaussian blur.
[00:45:39.320 --> 00:45:42.220]   The idea is just to blur the image, a normal Gaussian blur.
[00:45:42.220 --> 00:45:46.440]   And the intuition is that what if we can smooth out the high frequency noise, and we can hope
[00:45:46.440 --> 00:45:50.200]   that the adversarial component of the input is in that noise.
[00:45:50.200 --> 00:45:56.320]   And we can smooth that out and essentially throw only the adversarial noise away.
[00:45:56.320 --> 00:46:00.800]   It turns out that this is not a very good idea, and we can actually attack this very
[00:46:00.800 --> 00:46:01.800]   easily.
[00:46:01.800 --> 00:46:05.540]   And the idea here is just we have to fool both the original model and the defended model.
[00:46:05.540 --> 00:46:09.280]   If we try and fool any of them individually, let's say we have the defended model, which
[00:46:09.280 --> 00:46:11.560]   has that Gaussian blur pre-processed step.
[00:46:11.560 --> 00:46:16.240]   If we defend that, we might produce adversarial examples that the original model cannot produce
[00:46:16.240 --> 00:46:17.940]   good outputs for.
[00:46:17.940 --> 00:46:21.660]   And so we want to attack both the original model and the defended model.
[00:46:21.660 --> 00:46:25.960]   And we can chop this together just using a linear combination on the losses and take
[00:46:25.960 --> 00:46:29.180]   the gradients with respect to their inputs, and then produce adversarial examples that
[00:46:29.180 --> 00:46:33.760]   work for both the original and the defended model.
[00:46:33.760 --> 00:46:37.360]   A defense that's a little more complicated uses ensemble methods.
[00:46:37.360 --> 00:46:40.840]   And the idea behind the ensemble is just that, OK, you have an input, I'm going to give it
[00:46:40.840 --> 00:46:43.880]   to, in this picture, four different models.
[00:46:43.880 --> 00:46:47.700]   And the models are going to produce some output and vote on the output or have some average
[00:46:47.700 --> 00:46:49.000]   of the output.
[00:46:49.000 --> 00:46:54.960]   So the actual ensemble prediction will be some function of the outputs of those models.
[00:46:54.960 --> 00:46:57.960]   And in this case, we can have two different types of models.
[00:46:57.960 --> 00:47:01.960]   The first is the full precision model, which gives high accuracy but low robustness.
[00:47:01.960 --> 00:47:06.440]   And the second is the low precision model, which has high robustness and low accuracy.
[00:47:06.440 --> 00:47:10.720]   The high robustness factor of the low precision model comes from quantization.
[00:47:10.720 --> 00:47:14.600]   When I say quantization, I just mean chopping off some of the bits of the input so that
[00:47:14.600 --> 00:47:19.920]   essentially we're taking away some of the lower precision parts of the image.
[00:47:19.920 --> 00:47:22.760]   And the idea is that what if we can get the best of both worlds?
[00:47:22.760 --> 00:47:27.540]   We want the high accuracy and the high robustness of the models.
[00:47:27.540 --> 00:47:29.500]   And that's the idea behind the ensemble method.
[00:47:29.500 --> 00:47:34.640]   It turns out that this does boost adversarial accuracy by about 15.3%.
[00:47:34.640 --> 00:47:40.120]   But we'll talk about why it doesn't quite work as well as we want it to on the next slide.
[00:47:40.120 --> 00:47:41.440]   So we can attack this.
[00:47:41.440 --> 00:47:45.520]   The insight here is that this defense just masks the gradient signal.
[00:47:45.520 --> 00:47:48.200]   It doesn't actually really increase the robustness.
[00:47:48.200 --> 00:47:53.960]   A big consequence of ensembles is that it masks the gradient because it uses several
[00:47:53.960 --> 00:47:58.120]   different models that either vote or average on an output.
[00:47:58.120 --> 00:48:03.240]   And so what we found was that the full precision models have a very, very similar loss surface
[00:48:03.240 --> 00:48:05.680]   to the loss surface of the entire ensemble.
[00:48:05.680 --> 00:48:08.780]   And because this is the case, the gradients are not very well hidden.
[00:48:08.780 --> 00:48:12.480]   We can just retrieve them by using the full precision model.
[00:48:12.480 --> 00:48:17.440]   And so what we do is we just take the gradients from the full precision model, find some adversarial
[00:48:17.440 --> 00:48:21.160]   example using methods that are already known like FGSM or PGD.
[00:48:21.160 --> 00:48:24.760]   And then we can plug this in through the actual entire ensemble.
[00:48:24.760 --> 00:48:28.240]   And this works a majority of the time.
[00:48:28.240 --> 00:48:29.240]   Okay.
[00:48:29.240 --> 00:48:37.960]   And then a second paper that we've been working on is there's another proposed defense which
[00:48:37.960 --> 00:48:40.160]   seems quite different from the last one.
[00:48:40.160 --> 00:48:42.080]   It does use ensembles as well.
[00:48:42.080 --> 00:48:46.680]   But the mechanisms underneath rather than doing quantization to increase robustness
[00:48:46.680 --> 00:48:51.960]   is they use some work from error correction codes.
[00:48:51.960 --> 00:48:55.980]   So they try to use error correcting codes to increase the robustness.
[00:48:55.980 --> 00:49:03.640]   And so the key insight is that when you do adversarial perturbations for a neural network,
[00:49:03.640 --> 00:49:07.360]   you're taking your inputs and then you're perturbing it and then you're trying to change
[00:49:07.360 --> 00:49:08.680]   the outputs.
[00:49:08.680 --> 00:49:13.040]   And so when you have the one hot encoding like you would normally do, all that you have
[00:49:13.040 --> 00:49:17.040]   to do is change a single bit to get a new output, right?
[00:49:17.040 --> 00:49:20.920]   If you have one zero zero, all you have to do is change that to zero one zero.
[00:49:20.920 --> 00:49:23.240]   And that technically just requires changing two bits.
[00:49:23.240 --> 00:49:27.320]   You reduce the one to a zero and then you increase the zero to a one.
[00:49:27.320 --> 00:49:31.000]   And so that's called the Hamming distance between those two bit codes.
[00:49:31.000 --> 00:49:34.400]   And what error correcting codes do is they let you have a larger Hamming distance between
[00:49:34.400 --> 00:49:38.320]   these encodings for the outputs.
[00:49:38.320 --> 00:49:42.200]   And what they do is they design a codebook that instead of using the identity function,
[00:49:42.200 --> 00:49:47.720]   which is the one hot encoding code word, where you have just one zero zero zero, zero one
[00:49:47.720 --> 00:49:53.000]   zero zero, et cetera, you use more complicated code words here.
[00:49:53.000 --> 00:49:57.640]   And so here's some pictures from the paper where on the left is the original models just
[00:49:57.640 --> 00:49:58.880]   without this new loss.
[00:49:58.880 --> 00:50:03.880]   And it's pretty easy to go from low probabilities for a class to high probabilities for a different
[00:50:03.880 --> 00:50:04.880]   class.
[00:50:04.880 --> 00:50:06.920]   Like you just have to cross this little boundary in the middle here.
[00:50:06.920 --> 00:50:10.920]   But in the middle one, it's much harder to get from something in the red side.
[00:50:10.920 --> 00:50:16.480]   And you take some harder traversal through the green and then you get to the blue.
[00:50:16.480 --> 00:50:21.360]   And so it makes it so that your model is more robust and it's harder to change codes.
[00:50:21.360 --> 00:50:25.760]   And so in practice, they use Hadamard codes.
[00:50:25.760 --> 00:50:26.760]   And so this is an example.
[00:50:26.760 --> 00:50:30.360]   Instead of using the identity matrix and one hot encoding, you use these codes that have
[00:50:30.360 --> 00:50:32.480]   a larger Hamming distance between them.
[00:50:32.480 --> 00:50:36.840]   And so it makes it harder to flip the bits and it makes it harder to perturb the inputs
[00:50:36.840 --> 00:50:38.940]   to give a fake example.
[00:50:38.940 --> 00:50:44.360]   And then for the loss that they use, as you just take some sigmoid, some increasing function
[00:50:44.360 --> 00:50:48.840]   bounded between zero one of the outputs of the neural network, and then you do the dot
[00:50:48.840 --> 00:50:54.760]   product of that with the corresponding row for whatever class this is an example of.
[00:50:54.760 --> 00:50:58.400]   And then you do like a soft max like loss where you end up getting a probability out
[00:50:58.400 --> 00:50:59.400]   of this.
[00:50:59.400 --> 00:51:02.460]   And then you maximize this loss over the average over the batch.
[00:51:02.460 --> 00:51:04.400]   And so this gives you actually a probability.
[00:51:04.400 --> 00:51:08.200]   And it turns out that this paper, in addition to improving robustness, also increases the
[00:51:08.200 --> 00:51:10.360]   probability estimates.
[00:51:10.360 --> 00:51:14.800]   Because a lot of neural networks, they output the probability of it being a class, but lots
[00:51:14.800 --> 00:51:19.280]   of times we just ignore that and think, oh, it is this class or it isn't this class.
[00:51:19.280 --> 00:51:24.120]   But it turns out that this can give you more robust probability estimates as well.
[00:51:24.120 --> 00:51:27.960]   And then finally, they also just throw in an ensemble for the same reason as the impure
[00:51:27.960 --> 00:51:33.820]   model, because it'll just increase the effective Hamming distance between all of the codes
[00:51:33.820 --> 00:51:38.540]   if we have lots of different codes at the same time, like working together to vote on
[00:51:38.540 --> 00:51:40.260]   a single output.
[00:51:40.260 --> 00:51:43.100]   So when you're training these, each classifier has different parameters.
[00:51:43.100 --> 00:51:44.100]   They don't share parameters.
[00:51:44.100 --> 00:51:47.860]   And then they're just solving a different classification problem because you have different
[00:51:47.860 --> 00:51:51.460]   encodings of the outputs.
[00:51:51.460 --> 00:51:55.620]   And then finally, we were trying to attack this paper and show that it didn't really
[00:51:55.620 --> 00:51:57.040]   work as well as they claimed.
[00:51:57.040 --> 00:51:59.760]   And the reason that we knew that it wouldn't work as well as they claimed is because if
[00:51:59.760 --> 00:52:04.380]   you look at this graph here, the two outputs that are kind of their state of the art methods
[00:52:04.380 --> 00:52:07.900]   are the green and the blue, the light blue over here.
[00:52:07.900 --> 00:52:13.980]   And if you look at epsilon 0.5, they claim that their accuracy is around 40 to 50% for
[00:52:13.980 --> 00:52:18.580]   an epsilon value of 0.5, which is the balance for our perturbations.
[00:52:18.580 --> 00:52:23.680]   But at this bound level, our images, our pixels are mapped between 0 and 1.
[00:52:23.680 --> 00:52:29.000]   And so if you think about it, if you have an epsilon of 0.5, you can turn any image
[00:52:29.000 --> 00:52:33.780]   into just complete grayscale and just take any image and convert it to all 0.5 for every
[00:52:33.780 --> 00:52:35.380]   pixel value in the image.
[00:52:35.380 --> 00:52:39.780]   And that fits within the bounds of this perturbation, which means that's a valid perturbation for
[00:52:39.780 --> 00:52:40.780]   this value.
[00:52:40.780 --> 00:52:44.200]   Like, that's a valid attack for this constraint.
[00:52:44.200 --> 00:52:48.060]   But if you get that and there's 10 classes and every image is exactly the same grayscale
[00:52:48.060 --> 00:52:52.340]   image, there's no way you could get above 10% accuracy.
[00:52:52.340 --> 00:52:55.260]   Yet they report 40 to 50% accuracy.
[00:52:55.260 --> 00:53:01.540]   So something is not working properly with their model or they're not attacking it properly,
[00:53:01.540 --> 00:53:03.160]   which was kind of suspicious.
[00:53:03.160 --> 00:53:07.720]   And so similar to the impure defense, we noticed that the error correcting code defense here
[00:53:07.720 --> 00:53:09.940]   was performing gradient masking.
[00:53:09.940 --> 00:53:15.220]   And the way that we realized this was in this loss function here, you realize that they
[00:53:15.220 --> 00:53:17.120]   take some sigmoid function.
[00:53:17.120 --> 00:53:20.620]   And in the actual code, when you look at this, they take the log of this.
[00:53:20.620 --> 00:53:22.860]   They take a log in the loss somewhere.
[00:53:22.860 --> 00:53:27.060]   And after they take this log, it causes the gradients to get really, really, really small.
[00:53:27.060 --> 00:53:30.140]   And then it causes a vanishing gradient issue.
[00:53:30.140 --> 00:53:34.740]   And so in effect, what that does is it just masks the gradient similar to before, where
[00:53:34.740 --> 00:53:36.300]   it makes the gradients really hard to read.
[00:53:36.300 --> 00:53:38.580]   And there's not very much information from them because of that.
[00:53:38.580 --> 00:53:43.100]   But what you can do is you just take a similar model where you get a smooth approximation
[00:53:43.100 --> 00:53:46.140]   of the gradients and you get rid of this vanishing gradient problem.
[00:53:46.140 --> 00:53:50.500]   And it's not the exact same model, but it's pretty accurate and almost exactly the same,
[00:53:50.500 --> 00:53:52.420]   just without the masking gradients.
[00:53:52.420 --> 00:53:56.820]   And then you just attack that instead and use those attacks for the new model.
[00:53:56.820 --> 00:54:02.100]   And what that does is it reduces the accuracy already from 50% to 15%.
[00:54:02.100 --> 00:54:03.220]   And that was the first thing we noticed.
[00:54:03.220 --> 00:54:06.860]   And we're still working on this, but it was a pretty drastic improvement already from
[00:54:06.860 --> 00:54:08.980]   even the claims in the paper.
[00:54:08.980 --> 00:54:13.620]   And so that's a general theme with a lot of these papers is that they claim that they
[00:54:13.620 --> 00:54:15.780]   work very, very well.
[00:54:15.780 --> 00:54:19.320]   And then you try to attack them using stronger attacks.
[00:54:19.320 --> 00:54:21.660]   And they don't work as well as they originally claimed.
[00:54:21.660 --> 00:54:25.940]   And it seems like they didn't put in all of their effort to doing the attacks, which makes
[00:54:25.940 --> 00:54:28.940]   sense because their focus is the defense.
[00:54:28.940 --> 00:54:34.580]   But it's not really a valid claim that their defense works amazingly if we can attack it
[00:54:34.580 --> 00:54:36.380]   by putting in a little bit more effort.
[00:54:36.380 --> 00:54:39.060]   And so the goal is to generalize these attacks that we've been doing.
[00:54:39.060 --> 00:54:40.700]   There are some similarities.
[00:54:40.700 --> 00:54:45.660]   We plan to generalize these attacks to a larger class of defenses so that we can find the
[00:54:45.660 --> 00:54:51.460]   commonalities between the errors in a lot of these defenses and figure out a more principled
[00:54:51.460 --> 00:54:53.060]   way to attack them.
[00:54:53.060 --> 00:54:57.540]   And in particular, we really want to focus on the class of detection defenses that Pedro
[00:54:57.540 --> 00:55:03.700]   mentioned at the beginning, where you try to either discard or not discard images based
[00:55:03.700 --> 00:55:05.700]   on whether it's an adversarial example or not.
[00:55:05.700 --> 00:55:10.500]   And so that's what we're going to be shifting to soon, now that we have a good amount of
[00:55:10.500 --> 00:55:13.660]   practice attacking some defenses.
[00:55:13.660 --> 00:55:17.780]   So yeah, now if anybody has any questions.
[00:55:17.780 --> 00:55:18.780]   Great.
[00:55:18.780 --> 00:55:21.460]   Yeah, I've got a lot of questions.
[00:55:21.460 --> 00:55:27.420]   I think I'm going to start off with one that's closer to a comment, but I'm hoping to spark
[00:55:27.420 --> 00:55:28.420]   a discussion.
[00:55:28.420 --> 00:55:33.260]   So you mentioned that the error correcting code approach gives you better calibrated
[00:55:33.260 --> 00:55:36.300]   outputs in terms of the class probabilities.
[00:55:36.300 --> 00:55:41.540]   One thing I've noticed when I've done work on adversarial attacks and defenses is that
[00:55:41.540 --> 00:55:46.260]   people's baselines tend to be really badly calibrated, because the default of a neural
[00:55:46.260 --> 00:55:49.060]   network is to be badly calibrated.
[00:55:49.060 --> 00:55:55.060]   Those class probabilities that you get out on held out data are not good.
[00:55:55.060 --> 00:56:00.340]   If it says it's 0.1, that it's some other digit or some other class, it's actually maybe
[00:56:00.340 --> 00:56:03.940]   more like 25% or 30% probability.
[00:56:03.940 --> 00:56:06.900]   And so there are ways to fix that.
[00:56:06.900 --> 00:56:11.140]   And what I found is that if you just make a model that's a little bit better calibrated,
[00:56:11.140 --> 00:56:17.380]   your adversarial defense, you actually become slightly more adversarially robust.
[00:56:17.380 --> 00:56:23.260]   And so it's very easy when you're just designing a defense to accidentally just calibrate your
[00:56:23.260 --> 00:56:24.580]   model better.
[00:56:24.580 --> 00:56:31.700]   And then in the process, that makes it look like you've come up with a new way to have
[00:56:31.700 --> 00:56:32.700]   a better defense.
[00:56:32.700 --> 00:56:35.540]   I'm curious, I guess, to turn that into a question.
[00:56:35.540 --> 00:56:38.820]   Did you notice anything like that when you were looking at all the different defenses
[00:56:38.820 --> 00:56:44.020]   and superior attacks that you've in your review of the literature and the work you did?
[00:56:44.020 --> 00:56:48.420]   So I can ask Pedro if he noticed anything for the impure.
[00:56:48.420 --> 00:56:53.140]   But it seems like something like that is going on here, because we know how to attack lots
[00:56:53.140 --> 00:56:57.140]   of models at the moment, there's lots of defenses that have been published, and Carlini has
[00:56:57.140 --> 00:57:00.020]   been instrumental in attacking lots of them.
[00:57:00.020 --> 00:57:05.060]   And what happens in this one is like, they claim to work really well, but it clearly
[00:57:05.060 --> 00:57:06.060]   doesn't.
[00:57:06.060 --> 00:57:08.700]   So my guess is that for our model, there is something like that going on.
[00:57:08.700 --> 00:57:12.940]   Like, the effect of this paper is just that it gives you better calibration.
[00:57:12.940 --> 00:57:15.580]   And then they throw lots of theory in there.
[00:57:15.580 --> 00:57:18.460]   And in effect, it's really just improving the calibration.
[00:57:18.460 --> 00:57:21.460]   And then as a result, it gives you a little bit better robustness, but then you just have
[00:57:21.460 --> 00:57:26.300]   to like fiddle with a little bit of the attack parameters, and then you can get it to work.
[00:57:26.300 --> 00:57:29.100]   I'm not sure if you noticed anything like that as well, Pedro.
[00:57:29.100 --> 00:57:30.100]   Yes.
[00:57:30.100 --> 00:57:31.580]   So I think that's really a major theme.
[00:57:31.580 --> 00:57:37.180]   We're seeing a lot of these papers from iClear and NeurIPS being broken very easily, because
[00:57:37.180 --> 00:57:38.860]   all they do is mask gradients.
[00:57:38.860 --> 00:57:43.940]   And really what they try to do, they focus on making it hard to retrieve these gradients,
[00:57:43.940 --> 00:57:48.100]   so that we cannot run FGSM or PGD or other attacks on them.
[00:57:48.100 --> 00:57:51.660]   But they're not really making it more robust, they're just making it harder to get those
[00:57:51.660 --> 00:57:52.660]   gradients.
[00:57:52.660 --> 00:57:57.260]   And the problem with that is that they can laser focus on, here's a model that is hard
[00:57:57.260 --> 00:58:02.820]   to get gradients from, but we can just build a substitute model that is on our own accord,
[00:58:02.820 --> 00:58:08.700]   has our own rules, that has smooth gradients that we can attack ourselves.
[00:58:08.700 --> 00:58:12.980]   And those adversarial examples that come out of those attacks from models we build that
[00:58:12.980 --> 00:58:17.460]   are similar to the models that are claimed to be robust, we can just transfer those over
[00:58:17.460 --> 00:58:20.700]   and they work a majority of the time.
[00:58:20.700 --> 00:58:27.940]   On that front, what do you think could be done to make research in adversarial attacks
[00:58:27.940 --> 00:58:31.540]   and defenses better to avoid this problem?
[00:58:31.540 --> 00:58:37.580]   Do you think that there are some institutional changes that could be done that could help
[00:58:37.580 --> 00:58:38.580]   this?
[00:58:38.580 --> 00:58:45.260]   Or is it just we just have to learn to do better when we do this research?
[00:58:45.260 --> 00:58:47.220]   I think it's kind of hard.
[00:58:47.220 --> 00:58:49.700]   Maybe there is institutional stuff you could do.
[00:58:49.700 --> 00:58:50.700]   I'm not sure.
[00:58:50.700 --> 00:58:55.620]   But I think the issue is that a lot of these attacks are very specifically tailored to
[00:58:55.620 --> 00:58:57.820]   every single defense.
[00:58:57.820 --> 00:59:03.420]   And so what happens is people take some general attack that is just very general and you have
[00:59:03.420 --> 00:59:05.420]   to tailor it a lot.
[00:59:05.420 --> 00:59:09.120]   And maybe for these examples, they do gradient masking.
[00:59:09.120 --> 00:59:14.380]   And so we have to do lots of stuff to do, for example, FGSM.
[00:59:14.380 --> 00:59:17.380]   We would do FGSM on a different model and then use those attacks.
[00:59:17.380 --> 00:59:20.500]   And you have to tailor it a lot to every specific defense.
[00:59:20.500 --> 00:59:25.420]   And a lot of the defenses simply just take the attack out of the box and apply it and
[00:59:25.420 --> 00:59:29.820]   say this doesn't work and this is the strongest attack that exists.
[00:59:29.820 --> 00:59:32.100]   Therefore, our model works really well.
[00:59:32.100 --> 00:59:36.100]   And they don't really put in the amount of effort that they should to attack their model
[00:59:36.100 --> 00:59:37.460]   really, really hard.
[00:59:37.460 --> 00:59:39.540]   And I guess it makes sense from their perspective.
[00:59:39.540 --> 00:59:42.980]   If they spend a couple of months working on this defense and then you spend a couple of
[00:59:42.980 --> 00:59:47.700]   months attacking it and then you actually attack it, then all of your time is wasted.
[00:59:47.700 --> 00:59:51.140]   And people don't really post negative results as often.
[00:59:51.140 --> 00:59:53.420]   Maybe that's an issue with the field as a whole.
[00:59:53.420 --> 00:59:55.980]   But negative results aren't as popular to post.
[00:59:55.980 --> 01:00:00.380]   And so it seems like if you did that kind of thing, then there would be a lot less defense
[01:00:00.380 --> 01:00:01.380]   papers.
[01:00:01.380 --> 01:00:04.940]   And so they just end up not attacking them as well as they should.
[01:00:04.940 --> 01:00:08.620]   And so that's kind of one of our goals is that we want to be able to have a more principled
[01:00:08.620 --> 01:00:10.180]   way to attack these methods.
[01:00:10.180 --> 01:00:14.460]   And if we do come up with something like that, then a lot of the claims that these defenses
[01:00:14.460 --> 01:00:20.420]   can make if they do claim to be defendable, or if they do claim to be able to defend such
[01:00:20.420 --> 01:00:24.060]   a principled attack, we would hope that that would improve the field as a whole.
[01:00:24.060 --> 01:00:28.980]   And it makes these attacks, it makes their claims much more accurate when they say that
[01:00:28.980 --> 01:00:31.180]   we are actually robust.
[01:00:31.180 --> 01:00:41.220]   Yeah, I suppose in a lot of in other scientific fields, there's a lot more peer review, and
[01:00:41.220 --> 01:00:46.380]   a lot more rigorous peer review than in ml, which has brought its benefits and its drawbacks.
[01:00:46.380 --> 01:00:51.500]   And perhaps one of the drawbacks is that it's easier to get a paper published that hasn't
[01:00:51.500 --> 01:00:56.460]   been subjected to a rigorous enough, like attempt to counter claim.
[01:00:56.460 --> 01:01:02.900]   Yeah, but I guess, I mean, we still do see the scientific method happening, right?
[01:01:02.900 --> 01:01:07.820]   People post a defense and then Karlene comes and attacks it.
[01:01:07.820 --> 01:01:11.620]   And so that's the peer review in action, I guess.
[01:01:11.620 --> 01:01:16.700]   Yeah, it's slower, or maybe not even slower.
[01:01:16.700 --> 01:01:20.900]   It's just public instead of private, which is maybe.
[01:01:20.900 --> 01:01:21.900]   Yeah.
[01:01:21.900 --> 01:01:22.900]   Great.
[01:01:22.900 --> 01:01:28.420]   That's, it's really great to see the work that you've done and that everybody at the
[01:01:28.420 --> 01:01:33.500]   at M Lab was able to do in this semester, really impressive the amount of work that
[01:01:33.500 --> 01:01:34.900]   you're able to get done.
[01:01:34.900 --> 01:01:40.180]   And so I'm looking forward to seeing more from from you all, both as you stay at M Lab
[01:01:40.180 --> 01:01:48.060]   and as you move on to become ML engineers and researchers and, you know, members of
[01:01:48.060 --> 01:01:50.100]   full full members of the field.
[01:01:50.100 --> 01:01:51.100]   Very exciting.
[01:01:51.100 --> 01:01:52.100]   Thank you.
[01:01:52.100 --> 01:01:53.100]   Thank you.
[01:01:53.100 --> 01:01:54.100]   All right.
[01:01:54.100 --> 01:01:59.100]   So thanks to everybody who came and talked about their work.
[01:01:59.100 --> 01:02:03.060]   I've, yeah, really great discussions.
[01:02:03.060 --> 01:02:06.460]   Thanks to folks who asked questions in the chat as well.
[01:02:06.460 --> 01:02:11.740]   And I will see you all next year at the next Weights and Biases Salon.
[01:02:11.740 --> 01:02:12.740]   All right.
[01:02:12.740 --> 01:02:13.580]   Take care.
[01:02:13.580 --> 01:02:20.580]   Thanks for watching.

