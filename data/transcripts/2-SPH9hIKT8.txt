
[00:00:00.000 --> 00:00:06.560]   Hi everyone. So two weeks ago I gave a graduate class here in Amsterdam to 200 PhD students about
[00:00:06.560 --> 00:00:14.320]   how to build, how to train a large language model from scratch in 2024. I tried in this talk to
[00:00:14.320 --> 00:00:19.600]   highlight the dark secrets, the thing that people don't talk a lot about but are very crucial to
[00:00:19.600 --> 00:00:24.640]   getting good performance large language models, and maybe to also highlight a bit what is more
[00:00:24.640 --> 00:00:30.480]   hype than reality. And when I shared the slides afterwards there was a lot of interest for this
[00:00:30.480 --> 00:00:38.080]   so I decided I would actually re-record the talk and post it on YouTube as well. So here is our
[00:00:38.080 --> 00:00:46.320]   little guide to building a large language model in 2024. In this talk I'm gonna cover three main
[00:00:46.320 --> 00:00:51.520]   parts - training, fine-tuning, inference. I think for fine-tuning and inference you can already find
[00:00:51.520 --> 00:00:57.360]   super good recipes, super good blog posts and explanations online so I really spend most of my
[00:00:57.360 --> 00:01:02.400]   time on training, which is the part that's you know mostly like dark science I would say today.
[00:01:02.400 --> 00:01:07.680]   In training you have three parts - data preparation, efficient training technique,
[00:01:07.680 --> 00:01:12.880]   evaluation. It's the same here, I'll spend most of my time on the first part, data preparation,
[00:01:12.880 --> 00:01:19.200]   because that's really the secret sauce that I want to highlight today. So let's start writing.
