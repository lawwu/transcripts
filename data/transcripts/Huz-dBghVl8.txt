
[00:00:00.000 --> 00:00:10.000]   [BLANK_AUDIO]
[00:00:10.000 --> 00:00:20.000]   All right, hey everybody and
[00:00:20.000 --> 00:00:26.000]   welcome back to week nine of
[00:00:26.000 --> 00:00:32.000]   Fastbook Reading Group at
[00:00:32.000 --> 00:00:38.000]   Wits and Biases.
[00:00:38.000 --> 00:00:41.000]   Today we're gonna be covering off chapter seven,
[00:00:41.000 --> 00:00:44.000]   which is about training state of the art in ImageNet.
[00:00:44.000 --> 00:00:49.000]   So last week we worked on chapter six where we looked at some of
[00:00:49.000 --> 00:00:54.000]   the extra stuff in computer vision that could help train really good
[00:00:54.000 --> 00:00:57.000]   models on using the fast AI library.
[00:00:57.000 --> 00:01:00.000]   And we sort of had a live coding session and
[00:01:00.000 --> 00:01:03.000]   we started working on the Cassava competition.
[00:01:03.000 --> 00:01:05.000]   And many of you have been working and
[00:01:05.000 --> 00:01:08.000]   many of you have been sharing your results on the validation data set.
[00:01:08.000 --> 00:01:09.000]   So thanks for that.
[00:01:09.000 --> 00:01:12.000]   Again, this week has been super good one.
[00:01:12.000 --> 00:01:14.000]   This week we've had blog posts.
[00:01:14.000 --> 00:01:19.000]   So as every week, go to that link 1db.me/fastbook9.
[00:01:19.000 --> 00:01:26.000]   So let me go and show you guys that link, fastbook9.
[00:01:26.000 --> 00:01:30.000]   I'll zoom in a bit.
[00:01:30.000 --> 00:01:34.000]   Feel free to message me on the chat just in case you can't hear me or
[00:01:34.000 --> 00:01:39.000]   you can't just in case the screens to zoom out.
[00:01:39.000 --> 00:01:40.000]   So that's week nine.
[00:01:40.000 --> 00:01:43.000]   So this is where you'll have all your comments.
[00:01:43.000 --> 00:01:45.000]   I'm just gonna have a first comment.
[00:01:45.000 --> 00:01:47.000]   It's asking me to sign in.
[00:01:47.000 --> 00:02:01.000]   Okay, one second, please.
[00:02:01.000 --> 00:02:04.000]   Sorry, I'm just going back to the forum post.
[00:02:04.000 --> 00:02:09.000]   Just give me one second.
[00:02:09.000 --> 00:02:10.000]   Okay, that should work.
[00:02:10.000 --> 00:02:12.000]   So if I go in and I just say test.
[00:02:12.000 --> 00:02:14.000]   So this is where we're gonna have all of our comments.
[00:02:14.000 --> 00:02:18.000]   And this is where we're gonna have all our discussion about week nine and
[00:02:18.000 --> 00:02:20.000]   chapter seven.
[00:02:20.000 --> 00:02:23.000]   So this has been another great week and many of you have been writing about
[00:02:23.000 --> 00:02:25.000]   various things.
[00:02:25.000 --> 00:02:28.000]   I haven't had a chance to read all of them because many of these blogs have
[00:02:28.000 --> 00:02:31.000]   been written very recently in the past few hours.
[00:02:31.000 --> 00:02:35.000]   So Akash, thanks very much for writing about the various loss functions.
[00:02:35.000 --> 00:02:39.000]   I saw you've started your own personal GitHub repository and you have your
[00:02:39.000 --> 00:02:41.000]   own personal research blog.
[00:02:41.000 --> 00:02:44.000]   So that's really great to see and thanks for writing again about the various
[00:02:44.000 --> 00:02:45.000]   loss functions.
[00:02:45.000 --> 00:02:49.000]   So I think I do want to read about it, particularly the KL divergence part,
[00:02:49.000 --> 00:02:53.000]   so I'll definitely read this straight after this session.
[00:02:53.000 --> 00:02:55.000]   Vinayak, thanks for coming back another week.
[00:02:55.000 --> 00:02:58.000]   Again, I haven't had a chance to read your blog post this time.
[00:02:58.000 --> 00:03:02.000]   It's only because I think it got released very recently, but thanks again for
[00:03:02.000 --> 00:03:07.000]   writing about the cassava leaf disease classification prediction competition.
[00:03:07.000 --> 00:03:11.000]   And we will be discussing cassava a little bit today if there's time in the
[00:03:11.000 --> 00:03:15.000]   end, then we're gonna look at more ways on how we can improve our accuracy
[00:03:15.000 --> 00:03:17.000]   and that's something we'll talk about later.
[00:03:17.000 --> 00:03:21.000]   But thanks very much, Vinayak, for writing about cassava.
[00:03:21.000 --> 00:03:26.000]   Ravi has come back again and Ravi, I really enjoyed this example of the pen
[00:03:26.000 --> 00:03:29.000]   and book and you've written about multilabel classification.
[00:03:29.000 --> 00:03:34.000]   So I think all of you guys who are here in week nine, I think that is going
[00:03:34.000 --> 00:03:39.000]   great because it's already two months that -- yeah, it's already been around
[00:03:39.000 --> 00:03:43.000]   two months that we've been doing this every week and I'm so glad to see this
[00:03:43.000 --> 00:03:45.000]   consistency from so many of you.
[00:03:45.000 --> 00:03:50.000]   So guys, I couldn't be more happy and this is really beyond my
[00:03:50.000 --> 00:03:54.000]   imagination that each and every one of you has been writing every week.
[00:03:54.000 --> 00:03:56.000]   So thanks for doing this.
[00:03:56.000 --> 00:04:00.000]   It gives me motivation to keep going every week and I hope that by doing
[00:04:00.000 --> 00:04:04.000]   this, you're also learning a lot just by writing these blogs and just by
[00:04:04.000 --> 00:04:06.000]   working with us and through this consistency.
[00:04:06.000 --> 00:04:08.000]   So thanks for doing that.
[00:04:08.000 --> 00:04:13.000]   Ravi Chandra has come back another week and he's written about -- again,
[00:04:13.000 --> 00:04:16.000]   he's summarized his learnings and he's written about multilabel
[00:04:16.000 --> 00:04:20.000]   classification, binary cross entropy and pretty much the whole of chapter
[00:04:20.000 --> 00:04:21.000]   six.
[00:04:21.000 --> 00:04:23.000]   So thanks, Ravi.
[00:04:23.000 --> 00:04:27.000]   That's been that, but I did want to cover -- one thing I did want to cover
[00:04:27.000 --> 00:04:32.000]   before we start the session today is I just wanted to go to this discussion
[00:04:32.000 --> 00:04:34.000]   forum post on weights and biases.
[00:04:34.000 --> 00:04:43.000]   So let me copy that link in the chat.
[00:04:43.000 --> 00:04:47.000]   Sorry, there's something up with my Internet connection today.
[00:04:47.000 --> 00:04:52.000]   So just in case I jump out, I just want to say just in case I drop off or
[00:04:52.000 --> 00:04:54.000]   something happens, I will come back.
[00:04:54.000 --> 00:04:58.000]   So I just wanted to kind of point it out quickly.
[00:04:58.000 --> 00:05:02.000]   So everybody, you've been working on cassava leaf disease classification
[00:05:02.000 --> 00:05:06.000]   using fast AI and I'm really happy to see each and every one of you going
[00:05:06.000 --> 00:05:07.000]   in.
[00:05:07.000 --> 00:05:11.000]   So Matthew has come in, I think it was six hours ago, and he says he's able
[00:05:11.000 --> 00:05:15.000]   to get -- neither of them allowed me to get more than 86%.
[00:05:15.000 --> 00:05:21.000]   So he's able to get 86% and he's been using VTG16 and ResNet 34.
[00:05:21.000 --> 00:05:24.000]   Matthew, I think one of the key things you could try is having maybe a
[00:05:24.000 --> 00:05:25.000]   bigger model.
[00:05:25.000 --> 00:05:27.000]   That's one thing you could definitely try.
[00:05:27.000 --> 00:05:31.000]   But we're going to cover some new tricks today as part of chapter seven.
[00:05:31.000 --> 00:05:34.000]   And then each of you can go back and try those tricks and hopefully that
[00:05:34.000 --> 00:05:37.000]   accuracy will come up on the validation dataset.
[00:05:37.000 --> 00:05:41.000]   So Vinayak's been able to get up to 87.4 and as already mentioned in his
[00:05:41.000 --> 00:05:45.000]   blog post that I showed you guys previously, he's documented and presented
[00:05:45.000 --> 00:05:48.000]   his observations in that blog post.
[00:05:48.000 --> 00:05:52.000]   Rinda, I really like this idea of having like two separate models and you're
[00:05:52.000 --> 00:05:55.000]   still, I guess, at 87%, is it?
[00:05:55.000 --> 00:05:56.000]   Still not reached 87.
[00:05:56.000 --> 00:05:59.000]   So let's see if there's stuff we can do today.
[00:05:59.000 --> 00:06:04.000]   And I'm really glad to see this kind of, you know, I guess this interaction
[00:06:04.000 --> 00:06:07.000]   between you guys and this is really good to see that everybody's been
[00:06:07.000 --> 00:06:11.000]   helping everybody else and I couldn't be more happy.
[00:06:11.000 --> 00:06:14.000]   Abhishek has again been working.
[00:06:14.000 --> 00:06:16.000]   He says he's getting about 87 accuracy.
[00:06:16.000 --> 00:06:19.000]   Kevin's getting around 87.24.
[00:06:19.000 --> 00:06:21.000]   Ravi is 87.9.
[00:06:21.000 --> 00:06:23.000]   Is that the highest so far?
[00:06:23.000 --> 00:06:25.000]   What did you do for 87.9?
[00:06:25.000 --> 00:06:26.000]   Let's see.
[00:06:26.000 --> 00:06:28.000]   I did not use any augmentations.
[00:06:28.000 --> 00:06:32.000]   I tried some augmentations but ended up with a worse accuracy, which was a
[00:06:32.000 --> 00:06:33.000]   little counterintuitive.
[00:06:33.000 --> 00:06:34.000]   It does seem that way.
[00:06:34.000 --> 00:06:39.000]   I just resized the images to 224 by 224 because I used the pre-trained
[00:06:39.000 --> 00:06:40.000]   ResNet-34.
[00:06:40.000 --> 00:06:42.000]   What else did you do?
[00:06:42.000 --> 00:06:44.000]   Really long training times.
[00:06:44.000 --> 00:06:46.000]   Fit one cycle or fine-tune.
[00:06:46.000 --> 00:06:50.000]   I got better results by using fit one cycle and then confusion matrix.
[00:06:50.000 --> 00:06:51.000]   Okay.
[00:06:51.000 --> 00:06:55.000]   We'll discuss this towards the end about more stuff that we can do with
[00:06:55.000 --> 00:06:56.000]   Cassava.
[00:06:56.000 --> 00:06:59.000]   So I'm really happy to see most of this.
[00:06:59.000 --> 00:07:02.000]   And a special shout out to Kevin.
[00:07:02.000 --> 00:07:09.000]   I think for -- let me bring up the Slack.
[00:07:09.000 --> 00:07:14.000]   So Kevin's been helping -- there was this question asked on CUDA errors
[00:07:14.000 --> 00:07:17.000]   and Kevin's gone in and Kevin's sort of recorded -- where is that
[00:07:17.000 --> 00:07:20.000]   recording?
[00:07:20.000 --> 00:07:22.000]   I'm just trying to find it.
[00:07:22.000 --> 00:07:27.000]   Kevin's gone in and he's shared a recording of the debugging process.
[00:07:27.000 --> 00:07:29.000]   I recorded some of my debugging process.
[00:07:29.000 --> 00:07:30.000]   Oh, here it is.
[00:07:30.000 --> 00:07:34.000]   Kevin's gone in and he's shared his recording of the debugging error
[00:07:34.000 --> 00:07:36.000]   and that's that video.
[00:07:36.000 --> 00:07:39.000]   And I think that's really, really helpful.
[00:07:39.000 --> 00:07:41.000]   So thanks very much, Kevin, for doing this.
[00:07:41.000 --> 00:07:45.000]   Let me post this video chat -- sorry, this video link in the chat as
[00:07:45.000 --> 00:07:47.000]   well.
[00:07:47.000 --> 00:07:49.000]   And keep up the good work, guys.
[00:07:49.000 --> 00:07:52.000]   And I'm really happy to see it's already been helpful.
[00:07:52.000 --> 00:07:56.000]   In fact, even I can come up -- I didn't know what's causing these
[00:07:56.000 --> 00:07:57.000]   errors.
[00:07:57.000 --> 00:08:01.000]   So thanks for sharing this, Kevin.
[00:08:01.000 --> 00:08:05.000]   So with that being said, let's get started with the next chapter, which
[00:08:05.000 --> 00:08:09.000]   is chapter 7.
[00:08:09.000 --> 00:08:10.000]   All right.
[00:08:10.000 --> 00:08:12.000]   Is that good enough?
[00:08:12.000 --> 00:08:16.000]   Could you please just quickly reply in the chat if that's good enough
[00:08:16.000 --> 00:08:18.000]   in terms of zooming in?
[00:08:18.000 --> 00:08:21.000]   And do you guys want me to zoom in a little bit more?
[00:08:21.000 --> 00:08:29.000]   Or is that perfect?
[00:08:29.000 --> 00:08:30.000]   All right.
[00:08:30.000 --> 00:08:31.000]   Excellent.
[00:08:31.000 --> 00:08:32.000]   We'll get started now, then.
[00:08:32.000 --> 00:08:33.000]   Cool.
[00:08:33.000 --> 00:08:35.000]   So let me just restart my notebook.
[00:08:35.000 --> 00:08:38.000]   So the next thing we do -- as part of this, we're going to look at
[00:08:38.000 --> 00:08:40.000]   training state-of-the-art ImageNet.
[00:08:40.000 --> 00:08:43.000]   This is what the title is, training a state-of-the-art image.
[00:08:43.000 --> 00:08:47.000]   Basically training a state-of-the-art model in computer vision.
[00:08:47.000 --> 00:08:50.000]   And as part of this, we're going to look at some tricks.
[00:08:50.000 --> 00:08:52.000]   So we'll look at the ImageNet data set.
[00:08:52.000 --> 00:08:54.000]   So I'll tell you what that is.
[00:08:54.000 --> 00:08:57.000]   It's a smaller version of the bigger ImageNet.
[00:08:57.000 --> 00:08:59.000]   And we'll talk about that just in a moment.
[00:08:59.000 --> 00:09:03.000]   Then we'll look at normalization, which is, again, a technique that
[00:09:03.000 --> 00:09:07.000]   we'll see that helps improve the accuracy for our models or basically
[00:09:07.000 --> 00:09:09.000]   helps improve the performance.
[00:09:09.000 --> 00:09:13.000]   I should stop saying accuracy, because it's just one metric.
[00:09:13.000 --> 00:09:18.000]   But generally, I guess I want to say all of these things that we look
[00:09:18.000 --> 00:09:22.000]   at, all of these techniques, all of these tricks that we'll look at
[00:09:22.000 --> 00:09:25.000]   generally help improve the validation metrics or the performance of
[00:09:25.000 --> 00:09:26.000]   the model.
[00:09:26.000 --> 00:09:28.000]   Next, we'll look at progressive resizing.
[00:09:28.000 --> 00:09:31.000]   We've already talked about this a little bit in the past.
[00:09:31.000 --> 00:09:36.000]   But today we'll look at it more formally and look at it in more detail.
[00:09:36.000 --> 00:09:38.000]   We'll also look at test time augmentation.
[00:09:38.000 --> 00:09:41.000]   And we'll look at mixup and label smoothing.
[00:09:41.000 --> 00:09:44.000]   So that's the main things we're going to cover off today.
[00:09:44.000 --> 00:09:47.000]   And if there's still time, then we could maybe discuss the questions
[00:09:47.000 --> 00:09:52.000]   or we could have a quick chat about Cassava competition and your
[00:09:52.000 --> 00:09:54.000]   findings.
[00:09:54.000 --> 00:09:56.000]   So that's that.
[00:09:56.000 --> 00:09:58.000]   Sorry, I have to zoom out a little bit.
[00:09:58.000 --> 00:09:59.000]   Okay, that should be fine.
[00:09:59.000 --> 00:10:00.000]   All right.
[00:10:00.000 --> 00:10:02.000]   So training a state-of-the-art ImageNet model.
[00:10:02.000 --> 00:10:06.000]   As I said, these are the various things that we're going to be looking
[00:10:06.000 --> 00:10:07.000]   at today.
[00:10:07.000 --> 00:10:10.000]   And the first thing is ImageNet.
[00:10:10.000 --> 00:10:11.000]   So what is this ImageNet?
[00:10:11.000 --> 00:10:16.000]   So if you go and search ImageNet, it's pronounced "Imaj-net," and it's
[00:10:16.000 --> 00:10:17.000]   on Fast.ai.
[00:10:17.000 --> 00:10:20.000]   There's this repository, ImageNet.
[00:10:20.000 --> 00:10:25.000]   And the basic idea is -- let me again copy/paste that in the chat.
[00:10:25.000 --> 00:10:31.000]   And the basic idea for this dataset was that it's a subset of ten
[00:10:31.000 --> 00:10:35.000]   easily classified classes from ImageNet.
[00:10:35.000 --> 00:10:36.000]   So what's ImageNet?
[00:10:36.000 --> 00:10:42.000]   ImageNet, if you search for ImageNet dataset, you'll see that's the
[00:10:42.000 --> 00:10:45.000]   website that will take you to ImageNet.org.
[00:10:45.000 --> 00:10:49.000]   And if you can read about it, I think if you go to "About," you'll be
[00:10:49.000 --> 00:10:52.000]   able to see stuff about this dataset, what is ImageNet.
[00:10:52.000 --> 00:10:55.000]   So it's a really big dataset.
[00:10:55.000 --> 00:11:00.000]   And it consists of about 1.3 million images.
[00:11:00.000 --> 00:11:04.000]   And let's see if there's some more information here.
[00:11:04.000 --> 00:11:08.000]   But basically there's images across a thousand classes, and it's a
[00:11:08.000 --> 00:11:09.000]   really massive dataset.
[00:11:09.000 --> 00:11:12.000]   So until now, there's different versions of dataset, right?
[00:11:12.000 --> 00:11:16.000]   So when you want to prototype or you want to do your experimentation,
[00:11:16.000 --> 00:11:18.000]   then imagine this.
[00:11:18.000 --> 00:11:22.000]   It takes about three or four days for, let's say, on a V100, if you
[00:11:22.000 --> 00:11:26.000]   have four access to -- not even one, but four GPUs, it would still
[00:11:26.000 --> 00:11:30.000]   take around three or four days to train on this 1.5 million images,
[00:11:30.000 --> 00:11:32.000]   which is going to be very costly.
[00:11:32.000 --> 00:11:33.000]   And imagine this.
[00:11:33.000 --> 00:11:37.000]   If you run all your experiments on such a big dataset, then not only
[00:11:37.000 --> 00:11:41.000]   are costs going to be high, but you'll be able to run a lot less
[00:11:41.000 --> 00:11:42.000]   experiments.
[00:11:42.000 --> 00:11:45.000]   And running more experiments is really helpful in deep learning.
[00:11:45.000 --> 00:11:47.000]   So you want to be able to fail quickly.
[00:11:47.000 --> 00:11:50.000]   You want to be able to know, okay, this is a technique that works
[00:11:50.000 --> 00:11:53.000]   for me, this is a technique that doesn't work for me.
[00:11:53.000 --> 00:11:56.000]   And that's the idea behind this ImageNet dataset.
[00:11:56.000 --> 00:12:01.000]   So as mentioned over here, typically there's three main datasets,
[00:12:01.000 --> 00:12:04.000]   the ImageNet, the MNIST, and CIFAR-10.
[00:12:04.000 --> 00:12:10.000]   And what was found by the FastAI team was when you were training
[00:12:10.000 --> 00:12:14.000]   on -- so, for example, models were being trained on smaller datasets
[00:12:14.000 --> 00:12:18.000]   when experimentation was being done on smaller datasets like MNIST
[00:12:18.000 --> 00:12:22.000]   or CIFAR-10, and you'd see, like, okay, maybe this is a trick or
[00:12:22.000 --> 00:12:25.000]   this is a technique that works on these smaller datasets.
[00:12:25.000 --> 00:12:28.000]   And then what you do is you try that trick on ImageNet.
[00:12:28.000 --> 00:12:32.000]   So let's say you're saying -- you come up with a new augmentation
[00:12:32.000 --> 00:12:36.000]   mix-up, and then that works for MNIST and CIFAR-10, but then when
[00:12:36.000 --> 00:12:40.000]   you would try and apply that technique to ImageNet, what would happen
[00:12:40.000 --> 00:12:43.000]   is that it won't really generalize, so you won't really see the same
[00:12:43.000 --> 00:12:46.000]   amount of improvements or all that stuff.
[00:12:46.000 --> 00:12:51.000]   All those improvements or all that -- yeah, basically the model
[00:12:51.000 --> 00:12:52.000]   won't generalize.
[00:12:52.000 --> 00:12:57.000]   That's the reason behind this ImageNet dataset.
[00:12:57.000 --> 00:13:00.000]   And what they saw or what the FastAI team has done is that they
[00:13:00.000 --> 00:13:06.000]   picked up ten of the more difficult classes from the full ImageNet
[00:13:06.000 --> 00:13:09.000]   dataset, and these are the ones that looked very different from
[00:13:09.000 --> 00:13:13.000]   one another, and what they hope is that you want to be able to
[00:13:13.000 --> 00:13:16.000]   quickly and cheaply create a classifier for recognizing these
[00:13:16.000 --> 00:13:20.000]   classes, and what was noticed was that if you have a dataset or
[00:13:20.000 --> 00:13:23.000]   you have a trick that works on this smaller version of the bigger
[00:13:23.000 --> 00:13:26.000]   ImageNet dataset, and this is -- the smaller version is called
[00:13:26.000 --> 00:13:30.000]   ImageNet, and if you have a trick that's working on a smaller
[00:13:30.000 --> 00:13:34.000]   version, then what they saw is that it pretty much generalizes to
[00:13:34.000 --> 00:13:37.000]   that bigger version of the dataset, so we're going to be working
[00:13:37.000 --> 00:13:39.000]   on this smaller version today.
[00:13:39.000 --> 00:13:42.000]   In fact, I run a lot of my experiments on the smaller version of
[00:13:42.000 --> 00:13:46.000]   this dataset as well, so it's very handy for us to know about this
[00:13:46.000 --> 00:13:50.000]   dataset, and one thing you could do with Cassava is you could try
[00:13:50.000 --> 00:13:53.000]   maybe if you want to do hyperparameter tuning or you could try
[00:13:53.000 --> 00:13:57.000]   just a smaller version of the Cassava dataset, so take inspiration
[00:13:57.000 --> 00:14:01.000]   from what's happened here, and then you could take a smaller
[00:14:01.000 --> 00:14:06.000]   version of that Cassava dataset, and you could run experimentation
[00:14:06.000 --> 00:14:09.000]   and hyperparameter tuning on that smaller version and see if that
[00:14:09.000 --> 00:14:13.000]   works for the bigger Cassava dataset.
[00:14:13.000 --> 00:14:14.000]   So that's it.
[00:14:14.000 --> 00:14:18.000]   So we just pass in that URL that will download that dataset from
[00:14:18.000 --> 00:14:20.000]   that particular URL.
[00:14:20.000 --> 00:14:25.000]   It will untar it, and it will give us a path, so the path here is
[00:14:25.000 --> 00:14:30.000]   this is where that dataset is, so if I go path.ls, you can see
[00:14:30.000 --> 00:14:35.000]   there's a train folder, there's a noisy image net CSV, which we
[00:14:35.000 --> 00:14:39.000]   can safely ignore, and then there's a val folder, so let's see
[00:14:39.000 --> 00:14:49.000]   what's in path.train.
[00:14:49.000 --> 00:14:52.000]   So in path.train, you can see there's ten categories, so image
[00:14:52.000 --> 00:14:56.000]   net categories are basically numbers like this, like N03 and then
[00:14:56.000 --> 00:15:00.000]   something, or N01, and then there's a dictionary that gets provided
[00:15:00.000 --> 00:15:03.000]   by image net, which tells you what each of these classes actually
[00:15:03.000 --> 00:15:06.000]   are, but we don't have to worry about any of that right now, but
[00:15:06.000 --> 00:15:09.000]   I just want to show you that this train folder consists of ten
[00:15:09.000 --> 00:15:14.000]   different folders, which consist of these different categories.
[00:15:14.000 --> 00:15:18.000]   So similarly, we can create a data block.
[00:15:18.000 --> 00:15:20.000]   We do the pre-sizing trick.
[00:15:20.000 --> 00:15:24.000]   We resize and pass that to -- we resize to a bigger image during
[00:15:24.000 --> 00:15:27.000]   our item transforms, and then during our batch transforms, we
[00:15:27.000 --> 00:15:30.000]   apply our augmentations and we reduce the size, so let's do that,
[00:15:30.000 --> 00:15:33.000]   and that will create our data loaders, and then from that data
[00:15:33.000 --> 00:15:37.000]   loaders, we can pass in a model, which is the X-BestNet50.
[00:15:37.000 --> 00:15:40.000]   We don't need to worry about what that is right now, but then we
[00:15:40.000 --> 00:15:43.000]   can train our model, and you can see that the model is able to get
[00:15:43.000 --> 00:15:48.000]   82% accuracy on these very difficult ten classes from the full
[00:15:48.000 --> 00:15:49.000]   image net.
[00:15:49.000 --> 00:15:50.000]   So that's that.
[00:15:50.000 --> 00:15:51.000]   So that's ImageNet.
[00:15:51.000 --> 00:15:54.000]   Are there any questions regarding this data set?
[00:15:54.000 --> 00:16:00.000]   I guess the main learning point or the main takeaway is that if
[00:16:00.000 --> 00:16:05.000]   you want to do experimentation, then do it on a smaller data set
[00:16:05.000 --> 00:16:07.000]   than on a bigger data set.
[00:16:07.000 --> 00:16:10.000]   That's the main takeaway, but if there's -- are there any
[00:16:10.000 --> 00:16:12.000]   questions?
[00:16:12.000 --> 00:16:18.000]   Let me just go and quickly check.
[00:16:18.000 --> 00:16:19.000]   Okay.
[00:16:19.000 --> 00:16:21.000]   No questions about this data set, so that's perfect.
[00:16:21.000 --> 00:16:23.000]   I'll move on to normalization.
[00:16:23.000 --> 00:16:28.000]   So the next thing that we're going to look at is normalization.
[00:16:28.000 --> 00:16:35.000]   So something you will see or something that should be known is
[00:16:35.000 --> 00:16:51.000]   that -- sorry, just give me one sec, guys.
[00:16:51.000 --> 00:16:53.000]   Sorry, there was somebody at the door.
[00:16:53.000 --> 00:16:54.000]   Just one second.
[00:16:54.000 --> 00:16:55.000]   All good now.
[00:16:55.000 --> 00:16:58.000]   So then in terms of normalization, what you will see is that it
[00:16:58.000 --> 00:17:02.000]   really helps if your input data has a mean of zero and standard
[00:17:02.000 --> 00:17:04.000]   deviation of one.
[00:17:04.000 --> 00:17:08.000]   That's something -- the reason for that is it's kind of -- I'm
[00:17:08.000 --> 00:17:11.000]   not from a statistics background, but from what I've read and
[00:17:11.000 --> 00:17:14.000]   from what I've seen in the past is that what that means is it's
[00:17:14.000 --> 00:17:17.000]   like a bell curve and then the statistics and it makes it easier
[00:17:17.000 --> 00:17:20.000]   for the model to train on that sort of input data.
[00:17:20.000 --> 00:17:24.000]   So that's the idea of normalizing your inputs.
[00:17:24.000 --> 00:17:28.000]   So what normalizing is, you will see -- actually, there's
[00:17:28.000 --> 00:17:30.000]   different versions of normalization, but basically,
[00:17:30.000 --> 00:17:33.000]   normalization just means I'll show you the batch normal, group
[00:17:33.000 --> 00:17:34.000]   normal.
[00:17:34.000 --> 00:17:36.000]   There's different versions of normalization that are used in
[00:17:36.000 --> 00:17:37.000]   deep learning.
[00:17:37.000 --> 00:17:40.000]   We won't go into the details, but the most common one that you
[00:17:40.000 --> 00:17:42.000]   will find is batch normalization, and I just wanted to point
[00:17:42.000 --> 00:17:46.000]   that out.
[00:17:46.000 --> 00:17:49.000]   It's this paper, batch normalization.
[00:17:49.000 --> 00:17:52.000]   So I'll just copy and paste that in the chat as well.
[00:17:52.000 --> 00:17:55.000]   We will look at this deeply in the coming sections, but I just
[00:17:55.000 --> 00:17:58.000]   wanted to sort of point out that that's the kind of normalization
[00:17:58.000 --> 00:18:02.000]   that does happen and is the most common.
[00:18:02.000 --> 00:18:06.000]   So what we can do is we can -- just to show you what this
[00:18:06.000 --> 00:18:10.000]   normalization does is I can grab my first batch from the data
[00:18:10.000 --> 00:18:16.000]   loader, and I can check the shapes of these -- of my first
[00:18:16.000 --> 00:18:17.000]   batch.
[00:18:17.000 --> 00:18:22.000]   So I get 64 images, which -- with three channel images, and
[00:18:22.000 --> 00:18:25.000]   they're all 2 to 4 by 2 to 4.
[00:18:25.000 --> 00:18:29.000]   And then I can see I have my Y, which are the labels associated
[00:18:29.000 --> 00:18:31.000]   to each of these 64 images.
[00:18:31.000 --> 00:18:32.000]   So let's see Y.
[00:18:32.000 --> 00:18:34.000]   So these are all tensor categories.
[00:18:34.000 --> 00:18:37.000]   So you can say first image belongs to the 0th category, the
[00:18:37.000 --> 00:18:39.000]   last image belongs to the 7th category.
[00:18:39.000 --> 00:18:42.000]   So it's really helpful to know what the shapes of your inputs
[00:18:42.000 --> 00:18:43.000]   are.
[00:18:43.000 --> 00:18:44.000]   So that's what we get.
[00:18:44.000 --> 00:18:48.000]   And then if we check the mean and we check the standard
[00:18:48.000 --> 00:18:51.000]   deviation, so one thing you will see is what we're doing is
[00:18:51.000 --> 00:18:52.000]   we're passing dimensions.
[00:18:52.000 --> 00:18:54.000]   So what's the shape of X?
[00:18:54.000 --> 00:18:57.000]   X is 64, 3, 2 to 4 by 2 to 4.
[00:18:57.000 --> 00:18:59.000]   What does that mean?
[00:18:59.000 --> 00:19:03.000]   That means you have 64 images, three channels, and then your
[00:19:03.000 --> 00:19:06.000]   height and width are 2 to 4 by 2 to 4.
[00:19:06.000 --> 00:19:10.000]   So if you're normalizing across the 0, second, and the third
[00:19:10.000 --> 00:19:13.000]   dimension, which means it's going to give us back something
[00:19:13.000 --> 00:19:16.000]   to do with 3, or the answers basically.
[00:19:16.000 --> 00:19:19.000]   Because we normalize across the nth dimension, we normalize
[00:19:19.000 --> 00:19:22.000]   across the height and width, which means we're going to get
[00:19:22.000 --> 00:19:24.000]   the normalization statistics for each channel.
[00:19:24.000 --> 00:19:26.000]   So we're going to get back three numbers.
[00:19:26.000 --> 00:19:30.000]   So it's really helpful to know when you pass in dimensions
[00:19:30.000 --> 00:19:32.000]   what the expected outputs are going to be.
[00:19:32.000 --> 00:19:35.000]   So this is something you should definitely try in your own
[00:19:35.000 --> 00:19:36.000]   time.
[00:19:36.000 --> 00:19:38.000]   And this is something that you should definitely before
[00:19:38.000 --> 00:19:41.000]   running something, just take maybe 10 seconds, 20 seconds to
[00:19:41.000 --> 00:19:43.000]   look what is my input shape.
[00:19:43.000 --> 00:19:47.000]   And then if I run an operation on that tensor with that input
[00:19:47.000 --> 00:19:49.000]   shape, then what would my output be?
[00:19:49.000 --> 00:19:52.000]   So that's something that should be -- that I just want to
[00:19:52.000 --> 00:19:53.000]   quickly highlight.
[00:19:53.000 --> 00:19:56.000]   So as you can see, then I can check my mean.
[00:19:56.000 --> 00:19:58.000]   So my mean is nowhere close to 0.
[00:19:58.000 --> 00:20:00.000]   My mean is around 0.44.
[00:20:00.000 --> 00:20:03.000]   And then my standard deviation is 0.27 on average.
[00:20:03.000 --> 00:20:07.000]   And what we want to do is we want to be able to normalize --
[00:20:07.000 --> 00:20:10.000]   we want to be able to normalize our input data set.
[00:20:10.000 --> 00:20:14.000]   So in Fast.ai, the easiest way to do that is in your batch
[00:20:14.000 --> 00:20:17.000]   transforms, you just say normalize from stats, and you can
[00:20:17.000 --> 00:20:19.000]   pass in the ImageNet stats.
[00:20:19.000 --> 00:20:23.000]   So ImageNet stats are just your mean and standard deviation.
[00:20:23.000 --> 00:20:26.000]   What the model will do is that the model will use this mean
[00:20:26.000 --> 00:20:29.000]   and use the standard deviation statistics to normalize your
[00:20:29.000 --> 00:20:30.000]   data set.
[00:20:30.000 --> 00:20:33.000]   So let's see what happens if I do this.
[00:20:33.000 --> 00:20:36.000]   So I'm going to grab my one batch again this time.
[00:20:36.000 --> 00:20:39.000]   And then next time we check the mean and standard deviation,
[00:20:39.000 --> 00:20:43.000]   as you can see, the mean is very much closer to 0, and the
[00:20:43.000 --> 00:20:45.000]   standard deviation is closer to 1.
[00:20:45.000 --> 00:20:47.000]   So we've done our job.
[00:20:47.000 --> 00:20:50.000]   And you can see that if you train again with now your
[00:20:50.000 --> 00:20:53.000]   normalized model, it will help the model a little bit.
[00:20:53.000 --> 00:20:56.000]   It won't be like -- you won't go from, say, the validation
[00:20:56.000 --> 00:20:58.000]   metric here is accuracy.
[00:20:58.000 --> 00:21:01.000]   You won't go from 80 to 85, but you will see some slight
[00:21:01.000 --> 00:21:04.000]   improvement, because it's a bit easier for the model then to
[00:21:04.000 --> 00:21:08.000]   work with these 0 mean and 1 standard deviation input data.
[00:21:08.000 --> 00:21:10.000]   So that's the main thing.
[00:21:10.000 --> 00:21:12.000]   And that's normalization.
[00:21:12.000 --> 00:21:16.000]   I guess it's -- in these parts, then, this is just setting us
[00:21:16.000 --> 00:21:19.000]   up for what's coming in the future.
[00:21:19.000 --> 00:21:23.000]   But let me see if there's any question about what is the
[00:21:23.000 --> 00:21:26.000]   meaning of that three numbers after normalization.
[00:21:26.000 --> 00:21:30.000]   I hope those three numbers are the normalization stats for
[00:21:30.000 --> 00:21:32.000]   each channel.
[00:21:32.000 --> 00:21:34.000]   I hope that should be pretty clear.
[00:21:34.000 --> 00:21:37.000]   But if it isn't, I can visualize that for you.
[00:21:37.000 --> 00:21:40.000]   How does normalization help in faster convergence?
[00:21:40.000 --> 00:21:43.000]   So I would refer you to this batch norm paper.
[00:21:43.000 --> 00:21:46.000]   You can have a look at this paper.
[00:21:46.000 --> 00:21:49.000]   But basically, because the statistics are -- it's mean 0
[00:21:49.000 --> 00:21:51.000]   and standard deviation 1.
[00:21:51.000 --> 00:21:53.000]   It really helps the model to converge.
[00:21:53.000 --> 00:21:56.000]   It's easier for the model, then, to train, and it's easier
[00:21:56.000 --> 00:21:58.000]   for the model to converge.
[00:21:58.000 --> 00:22:00.000]   That's the basics of it.
[00:22:00.000 --> 00:22:04.000]   Durga, could you maybe comment after this and maybe reply to
[00:22:04.000 --> 00:22:08.000]   this comment and just let me know if you still have a
[00:22:08.000 --> 00:22:11.000]   question regarding those three numbers?
[00:22:11.000 --> 00:22:15.000]   Is it fine to use ImageNet stats to normalize images from
[00:22:15.000 --> 00:22:18.000]   other datasets, like the cassava leaf disease classification?
[00:22:18.000 --> 00:22:21.000]   Yes, that's the most common way to do this.
[00:22:21.000 --> 00:22:24.000]   Unless your images are completely different, like if you
[00:22:24.000 --> 00:22:28.000]   end up in a medical image or you start working with maybe
[00:22:28.000 --> 00:22:32.000]   some protein data or all that sort of stuff, that's when --
[00:22:32.000 --> 00:22:36.000]   unless your dataset is widely different from the pre-trained
[00:22:36.000 --> 00:22:39.000]   model, it's fine to use those normalization statistics.
[00:22:39.000 --> 00:22:42.000]   In fact, that brings me to a good point.
[00:22:42.000 --> 00:22:48.000]   One thing we should -- as you'll see in part of this, this
[00:22:48.000 --> 00:22:51.000]   section, this line here says, normalization becomes
[00:22:51.000 --> 00:22:54.000]   especially important when using pre-trained models.
[00:22:54.000 --> 00:22:59.000]   So the idea for that is when you use the normalized pre-trained
[00:22:59.000 --> 00:23:03.000]   model, then all your activations are with mean 0 and standard
[00:23:03.000 --> 00:23:07.000]   deviation 1, and what that happens is then the model starts
[00:23:07.000 --> 00:23:10.000]   to learn about that particular dataset.
[00:23:10.000 --> 00:23:13.000]   But if you update the normalization stats, for example,
[00:23:13.000 --> 00:23:16.000]   when you use the pre-trained model, and the pre-trained
[00:23:16.000 --> 00:23:18.000]   model is using ImageNet stats, right?
[00:23:18.000 --> 00:23:20.000]   And the ImageNet stats are this.
[00:23:20.000 --> 00:23:23.000]   These are my means for each of the channels, and these are
[00:23:23.000 --> 00:23:26.000]   the standard deviations for each of the channels.
[00:23:26.000 --> 00:23:29.000]   And the pre-trained models are generally trained with these
[00:23:29.000 --> 00:23:32.000]   stats, which means they're going to use these stats to
[00:23:32.000 --> 00:23:34.000]   normalize your data.
[00:23:34.000 --> 00:23:37.000]   And let's say what happens is you're working with a different
[00:23:37.000 --> 00:23:40.000]   dataset, and you use different mean, and you use a different
[00:23:40.000 --> 00:23:43.000]   standard deviation to then normalize your dataset, then
[00:23:43.000 --> 00:23:45.000]   that's going to be a problem.
[00:23:45.000 --> 00:23:48.000]   Because then the meaning of the data changes, because then
[00:23:48.000 --> 00:23:51.000]   now you're using different normalization stats, and that's
[00:23:51.000 --> 00:23:53.000]   going to cause a whole lot of problems.
[00:23:53.000 --> 00:23:58.000]   There's this idea of blue frogs and red frogs, but that's
[00:23:58.000 --> 00:24:01.000]   something that's been very well explained by Jeremy himself.
[00:24:01.000 --> 00:24:03.000]   I won't go into the details of that.
[00:24:03.000 --> 00:24:08.000]   I can point you to that idea of why using different
[00:24:08.000 --> 00:24:12.000]   normalization statistics can be hurtful, but have a read of
[00:24:12.000 --> 00:24:13.000]   this section.
[00:24:13.000 --> 00:24:15.000]   It's really straightforward, and there shouldn't be any
[00:24:15.000 --> 00:24:17.000]   problems about that.
[00:24:17.000 --> 00:24:20.000]   So that's the idea behind normalization.
[00:24:20.000 --> 00:24:23.000]   And we're going to move on next to progressive resizing.
[00:24:23.000 --> 00:24:28.000]   So with progressive resizing, we've seen this in the past.
[00:24:28.000 --> 00:24:32.000]   With progressive resizing, what happens is you have your
[00:24:32.000 --> 00:24:37.000]   model, and you start with smaller images first.
[00:24:37.000 --> 00:24:41.000]   So basically, remember this getDLs function?
[00:24:41.000 --> 00:24:43.000]   What does this getDLs function do?
[00:24:43.000 --> 00:24:47.000]   It creates your data block, and then in terms -- it's first
[00:24:47.000 --> 00:24:51.000]   resizing your items to 460, which is using pre-sizing, and
[00:24:51.000 --> 00:24:54.000]   then you can say I'm creating my data loaders, okay?
[00:24:54.000 --> 00:24:58.000]   And then you can pass in the size, which gets passed to the
[00:24:58.000 --> 00:24:59.000]   batch transforms.
[00:24:59.000 --> 00:25:04.000]   So when I say getDLs and I say my batch size is 64 or size 2
[00:25:04.000 --> 00:25:08.000]   to 4 by 2 to 4, what that means is first all of my items are
[00:25:08.000 --> 00:25:13.000]   going to be resized to this 460 height width, and then they're
[00:25:13.000 --> 00:25:17.000]   going to be cropped to 2 to 4 by 2 to 4 images.
[00:25:17.000 --> 00:25:22.000]   So when I say getDLs 128 by 128, it means I'm starting with
[00:25:22.000 --> 00:25:24.000]   small, really small images.
[00:25:24.000 --> 00:25:26.000]   So let me show you that.
[00:25:26.000 --> 00:25:29.000]   X and Y equals DLs on one batch.
[00:25:29.000 --> 00:25:32.000]   That should work.
[00:25:32.000 --> 00:25:36.000]   So if I check my input shape, you can see I have -- because
[00:25:36.000 --> 00:25:40.000]   my batch size is 128, I have 128 images, but all of them are
[00:25:40.000 --> 00:25:42.000]   3 by 128 by 128.
[00:25:42.000 --> 00:25:46.000]   And then I can train my model on these smaller images.
[00:25:46.000 --> 00:25:49.000]   So I start with 128 and 128.
[00:25:49.000 --> 00:25:53.000]   And then I can use -- when that model trains, I can use the
[00:25:53.000 --> 00:25:54.000]   pre-trained weights.
[00:25:54.000 --> 00:25:57.000]   I can use the learned weights from the model.
[00:25:57.000 --> 00:25:59.000]   So it starts with smaller images.
[00:25:59.000 --> 00:26:03.000]   And then I can increase the size of my image to 2 to 4 by 2 to
[00:26:03.000 --> 00:26:04.000]   4.
[00:26:04.000 --> 00:26:06.000]   And then I can again train the model.
[00:26:06.000 --> 00:26:10.000]   And this time you can see the model is able to get like 86%
[00:26:10.000 --> 00:26:11.000]   accuracy.
[00:26:11.000 --> 00:26:15.000]   So if you go back and the first time we did it, which is here,
[00:26:15.000 --> 00:26:20.000]   the model was trained on 2 to 4 by 2 to 4, when we introduced
[00:26:20.000 --> 00:26:21.000]   normalization.
[00:26:21.000 --> 00:26:24.000]   On this model, we trained the model for 5 epochs.
[00:26:24.000 --> 00:26:26.000]   We used a learning rate.
[00:26:26.000 --> 00:26:30.000]   And the accuracy the model was able to get was 82 -- or 0.824,
[00:26:30.000 --> 00:26:31.000]   82.4%.
[00:26:31.000 --> 00:26:35.000]   But then when we did progressive resizing, the same model is
[00:26:35.000 --> 00:26:37.000]   able to get to 86.
[00:26:37.000 --> 00:26:41.000]   The only difference is the second model has been trained for
[00:26:41.000 --> 00:26:46.000]   9 epochs, because 4 epochs with size 128 by 128, and then 5
[00:26:46.000 --> 00:26:49.000]   epochs with size 2 to 4 by 2 to 4.
[00:26:49.000 --> 00:26:53.000]   But let me see if I can draw this in OneNote to show you the
[00:26:53.000 --> 00:26:54.000]   idea.
[00:26:54.000 --> 00:26:55.000]   Sorry.
[00:26:55.000 --> 00:26:57.000]   Just give me one second.
[00:26:57.000 --> 00:26:59.000]   I'm just setting up.
[00:26:59.000 --> 00:27:06.000]   I just need to set up OneNote in a way that I can -- I need to
[00:27:06.000 --> 00:27:07.000]   duplicate this.
[00:27:07.000 --> 00:27:10.000]   Sorry, one second.
[00:27:10.000 --> 00:27:11.000]   Okay.
[00:27:11.000 --> 00:27:12.000]   That should work.
[00:27:12.000 --> 00:27:16.000]   Screen sharing has been stopped again.
[00:27:16.000 --> 00:27:20.000]   So that should work, too.
[00:27:20.000 --> 00:27:25.000]   Can you guys now see my screens?
[00:27:25.000 --> 00:27:28.000]   Could you please just quickly reply in the chat if you can see
[00:27:28.000 --> 00:27:29.000]   my screen?
[00:27:29.000 --> 00:27:30.000]   Excellent.
[00:27:30.000 --> 00:27:33.000]   So then the idea is to -- so this is the model that we're
[00:27:33.000 --> 00:27:34.000]   going to use.
[00:27:34.000 --> 00:27:36.000]   Can you guys see my screen?
[00:27:36.000 --> 00:27:37.000]   Excellent.
[00:27:37.000 --> 00:27:41.000]   So then the idea is -- so this is my -- let's say this is my
[00:27:41.000 --> 00:27:42.000]   model.
[00:27:42.000 --> 00:27:46.000]   And then I have my first -- I have my inputs.
[00:27:46.000 --> 00:27:49.000]   So 2 to 4 by 2 to 4.
[00:27:49.000 --> 00:27:51.000]   And then three channel images.
[00:27:51.000 --> 00:27:54.000]   So 3, 2 to 4 by 2 to 4.
[00:27:54.000 --> 00:27:57.000]   So I first input my 2 to 4 by 2 to 4 image.
[00:27:57.000 --> 00:28:00.000]   Then this model is going to get trained, and I have some
[00:28:00.000 --> 00:28:01.000]   weights.
[00:28:01.000 --> 00:28:03.000]   I store them.
[00:28:03.000 --> 00:28:05.000]   And then I use the same model.
[00:28:05.000 --> 00:28:09.000]   So I just pretty much -- I just use the same model, and I
[00:28:09.000 --> 00:28:13.000]   update my, say, inputs to be now instead of being 2 to 4 by
[00:28:13.000 --> 00:28:17.000]   2 to 4, let's say they are 512 by 512.
[00:28:17.000 --> 00:28:23.000]   So what that -- what this is going to do is it's in its way
[00:28:23.000 --> 00:28:25.000]   a kind of a data augmentation.
[00:28:25.000 --> 00:28:28.000]   Because in a way now we're fine tuning a model.
[00:28:28.000 --> 00:28:31.000]   Like we trained it on 2 to 4 by 2 to 4.
[00:28:31.000 --> 00:28:35.000]   But what happens is when you have -- let's say I have --
[00:28:35.000 --> 00:28:37.000]   sorry, one second.
[00:28:37.000 --> 00:28:41.000]   Let's say my first input is 2 to 4 by 2 to 4, and then my
[00:28:41.000 --> 00:28:44.000]   second time my input is 512 by 512.
[00:28:44.000 --> 00:28:48.000]   So in 2 to 4 by 2 to 4, my object sizes are going to be
[00:28:48.000 --> 00:28:49.000]   small.
[00:28:49.000 --> 00:28:52.000]   And then in 512 by 512, the same object is going to be
[00:28:52.000 --> 00:28:53.000]   bigger.
[00:28:53.000 --> 00:28:56.000]   So for the model, it's a new kind of a task.
[00:28:56.000 --> 00:28:58.000]   But it really helps.
[00:28:58.000 --> 00:29:01.000]   By this time, the model has learned the basic idea of,
[00:29:01.000 --> 00:29:04.000]   okay, that's how edges are or that's what eyes look like.
[00:29:04.000 --> 00:29:07.000]   And it's kind of learned a little bit about that model.
[00:29:07.000 --> 00:29:10.000]   And because when we're training on smaller image sizes, what
[00:29:10.000 --> 00:29:13.000]   that means is the model training is going to be really
[00:29:13.000 --> 00:29:14.000]   fast.
[00:29:14.000 --> 00:29:17.000]   So in this way, then, instead of just starting by 512 by
[00:29:17.000 --> 00:29:21.000]   512 straight away, it's much helpful to start with a smaller
[00:29:21.000 --> 00:29:25.000]   image size first, and then you go on to 512 by 512.
[00:29:25.000 --> 00:29:28.000]   And this is something that could be tried and tested for
[00:29:28.000 --> 00:29:29.000]   Cassava as well.
[00:29:29.000 --> 00:29:33.000]   So for Cassava, maybe have a go at starting with 2 to 4 by
[00:29:33.000 --> 00:29:37.000]   2 to 4, and then you start with 512 by 512, and then you
[00:29:37.000 --> 00:29:40.000]   keep increasing the image size, maybe get to 768, and let's
[00:29:40.000 --> 00:29:41.000]   see how that goes.
[00:29:41.000 --> 00:29:44.000]   So that's another idea that you could try.
[00:29:44.000 --> 00:29:47.000]   And this idea is called progressive resizing.
[00:29:47.000 --> 00:29:48.000]   So that's that.
[00:29:48.000 --> 00:29:51.000]   So let me see if there's any questions about progressive
[00:29:51.000 --> 00:29:52.000]   resizing.
[00:29:52.000 --> 00:30:00.000]   I would be interested in getting more resources.
[00:30:00.000 --> 00:30:02.000]   Normalization has always been difficult for me.
[00:30:02.000 --> 00:30:04.000]   Blue frogs and green frogs sounds helpful, maybe.
[00:30:04.000 --> 00:30:05.000]   Yes, it does.
[00:30:05.000 --> 00:30:07.000]   I have to find that video.
[00:30:07.000 --> 00:30:10.000]   Because if I try and explain that, I won't do even half as
[00:30:10.000 --> 00:30:12.000]   good a job as Jeremy himself.
[00:30:12.000 --> 00:30:15.000]   So I'll have to find that video where that is, and I'll share
[00:30:15.000 --> 00:30:16.000]   with everybody.
[00:30:16.000 --> 00:30:20.000]   Because I think that really is an excellent way of explaining
[00:30:20.000 --> 00:30:23.000]   that part of normalization.
[00:30:23.000 --> 00:30:26.000]   I could maybe point you to the blog post.
[00:30:26.000 --> 00:30:27.000]   Actually, never mind.
[00:30:27.000 --> 00:30:28.000]   Leave that for now.
[00:30:28.000 --> 00:30:31.000]   But I'll get back to you on more resources on normalization.
[00:30:31.000 --> 00:30:34.000]   It would be difficult to get straight into the research
[00:30:34.000 --> 00:30:38.000]   paper, but that blue frogs and green frogs idea, it's really
[00:30:38.000 --> 00:30:39.000]   helpful.
[00:30:39.000 --> 00:30:42.000]   I'll have to find that.
[00:30:42.000 --> 00:30:45.000]   What was the reason to give different sizes for item and
[00:30:45.000 --> 00:30:46.000]   batch?
[00:30:46.000 --> 00:30:47.000]   What value does it add?
[00:30:47.000 --> 00:30:51.000]   We've already covered this in the past videos or in the past
[00:30:51.000 --> 00:30:52.000]   lectures.
[00:30:52.000 --> 00:30:54.000]   I think it was in week six.
[00:30:54.000 --> 00:30:56.000]   But the idea is pre-sizing.
[00:30:56.000 --> 00:31:01.000]   And then I'll give a short version or a short answer.
[00:31:01.000 --> 00:31:05.000]   If you have the same image size or the same batch, basically
[00:31:05.000 --> 00:31:09.000]   the same size in item and batch, then when you do augmentations,
[00:31:09.000 --> 00:31:14.000]   then you either cut off the edges or there's some problems
[00:31:14.000 --> 00:31:17.000]   that you'll see in the image, like it's a bit more blurry or
[00:31:17.000 --> 00:31:21.000]   that's the problem that comes with having the same size.
[00:31:21.000 --> 00:31:23.000]   So in Fast.ai, you have this different way, which is called
[00:31:23.000 --> 00:31:26.000]   pre-sizing, that you have a bigger size for item and then you
[00:31:26.000 --> 00:31:28.000]   do the augmentation.
[00:31:28.000 --> 00:31:34.000]   Isn't changing the size will result in an error for pre-trained
[00:31:34.000 --> 00:31:36.000]   model as they require different size input?
[00:31:36.000 --> 00:31:37.000]   Oh, that's a great question.
[00:31:37.000 --> 00:31:38.000]   I should have covered that.
[00:31:38.000 --> 00:31:40.000]   No, it won't.
[00:31:40.000 --> 00:31:43.000]   So, okay.
[00:31:43.000 --> 00:31:47.000]   So what happens typically, I think this needs to be covered a
[00:31:47.000 --> 00:31:52.000]   little bit, is let me show you an example of a ResNet,
[00:31:52.000 --> 00:31:53.000]   actually.
[00:31:53.000 --> 00:31:54.000]   One second.
[00:31:54.000 --> 00:31:56.000]   I'm just bringing that up.
[00:31:56.000 --> 00:32:03.000]   So the ResNet paper is here.
[00:32:03.000 --> 00:32:05.000]   Where's the PDF?
[00:32:05.000 --> 00:32:07.000]   I just want to show you the first layer of that is a
[00:32:07.000 --> 00:32:08.000]   convolution.
[00:32:08.000 --> 00:32:10.000]   It doesn't matter what the size is.
[00:32:10.000 --> 00:32:13.000]   Let me show you something.
[00:32:13.000 --> 00:32:14.000]   Okay.
[00:32:14.000 --> 00:32:16.000]   Have a look at this.
[00:32:16.000 --> 00:32:20.000]   In a ResNet, the first layer or the first input is a 7x7
[00:32:20.000 --> 00:32:21.000]   conv.
[00:32:21.000 --> 00:32:22.000]   Okay.
[00:32:22.000 --> 00:32:24.000]   So let's do a little experiment.
[00:32:24.000 --> 00:32:27.000]   So you have your first input is 7x7 conv.
[00:32:27.000 --> 00:32:29.000]   Then you have a block.
[00:32:29.000 --> 00:32:30.000]   Then you have another block.
[00:32:30.000 --> 00:32:31.000]   Then you have another block.
[00:32:31.000 --> 00:32:32.000]   Then you have another block.
[00:32:32.000 --> 00:32:35.000]   Basically, in your network, it looks like this.
[00:32:35.000 --> 00:32:39.000]   Like this is maybe your 7x7 conv 3.
[00:32:39.000 --> 00:32:42.000]   And then -- sorry.
[00:32:42.000 --> 00:32:44.000]   Your conv 2D.
[00:32:44.000 --> 00:32:46.000]   And then you have another block.
[00:32:46.000 --> 00:32:49.000]   Then you have another block.
[00:32:49.000 --> 00:32:51.000]   Then you have another block and so on.
[00:32:51.000 --> 00:32:54.000]   And what happens is what you're saying, I guess the question
[00:32:54.000 --> 00:32:57.000]   is then our input is 2 to 4 by 2 to 4.
[00:32:57.000 --> 00:33:00.000]   And then it's 512 by 512.
[00:33:00.000 --> 00:33:01.000]   Right?
[00:33:01.000 --> 00:33:04.000]   So why would that not cause an error?
[00:33:04.000 --> 00:33:07.000]   First, convolutions, for convolutions, it doesn't matter
[00:33:07.000 --> 00:33:08.000]   what your input is.
[00:33:08.000 --> 00:33:11.000]   Like they don't -- so let's do a quick experiment.
[00:33:11.000 --> 00:33:15.000]   So let's say import torch.nn as nn.
[00:33:15.000 --> 00:33:20.000]   And then let's create a conv, nn.conv 2D.
[00:33:20.000 --> 00:33:25.000]   Let's create the -- maybe I shouldn't go into very much
[00:33:25.000 --> 00:33:28.000]   details, but I just did want to show case still.
[00:33:28.000 --> 00:33:35.000]   So let's say my out channels are 64 and my kernel size is 7.
[00:33:35.000 --> 00:33:36.000]   Okay.
[00:33:36.000 --> 00:33:39.000]   So that's what the input -- that's the first layer.
[00:33:39.000 --> 00:33:42.000]   Whenever you pass your inputs, that's the first layer that's
[00:33:42.000 --> 00:33:44.000]   going to -- that's your convolution.
[00:33:44.000 --> 00:33:45.000]   Okay?
[00:33:45.000 --> 00:33:47.000]   So that's my conv.
[00:33:47.000 --> 00:33:57.000]   Let's say my inputs right now are one image of 2 to 4 by 2 to 4.
[00:33:57.000 --> 00:33:58.000]   Okay?
[00:33:58.000 --> 00:34:01.000]   And then let's see what my output shapes are going to be.
[00:34:01.000 --> 00:34:04.000]   So conv inputs and then that's my output.
[00:34:04.000 --> 00:34:06.000]   So let's see what my output shape is.
[00:34:06.000 --> 00:34:07.000]   Okay.
[00:34:07.000 --> 00:34:11.000]   When I have an input of 1 by 3 by 2 to 4 by 2 to 4, my output
[00:34:11.000 --> 00:34:14.000]   shape is 1 by 64 by 218 by 218.
[00:34:14.000 --> 00:34:15.000]   Why?
[00:34:15.000 --> 00:34:19.000]   Because I set my conv to go from in channels 3 to 64.
[00:34:19.000 --> 00:34:20.000]   Okay?
[00:34:20.000 --> 00:34:23.000]   So just get -- I just want to show you a basic -- just a basic
[00:34:23.000 --> 00:34:25.000]   idea of why it doesn't work.
[00:34:25.000 --> 00:34:27.000]   Sorry, why it won't fail.
[00:34:27.000 --> 00:34:30.000]   And then if you have an input of 512 by 512.
[00:34:30.000 --> 00:34:32.000]   So let's do that again.
[00:34:32.000 --> 00:34:34.000]   Let's do the same thing here.
[00:34:34.000 --> 00:34:36.000]   And then -- sorry.
[00:34:36.000 --> 00:34:39.000]   This should be 512 by 512.
[00:34:39.000 --> 00:34:42.000]   And then you see that the output is now a different shape.
[00:34:42.000 --> 00:34:46.000]   So for a convolution, your output size varies as your input
[00:34:46.000 --> 00:34:47.000]   size.
[00:34:47.000 --> 00:34:49.000]   But that's not going to be a problem.
[00:34:49.000 --> 00:34:52.000]   Because at the end of a network, what you do is you flatten it
[00:34:52.000 --> 00:34:53.000]   out.
[00:34:53.000 --> 00:34:55.000]   And then you use a global average pooling.
[00:34:55.000 --> 00:34:57.000]   And then that will still work.
[00:34:57.000 --> 00:34:59.000]   So I don't want to go into the details.
[00:34:59.000 --> 00:35:03.000]   But basically what you do is you take the mean of these sections.
[00:35:03.000 --> 00:35:06.000]   So let's -- it's called nn.gap.
[00:35:06.000 --> 00:35:07.000]   Okay.
[00:35:07.000 --> 00:35:08.000]   Never mind.
[00:35:08.000 --> 00:35:10.000]   I'll be digressing quite a bit.
[00:35:10.000 --> 00:35:13.000]   But in the end, what you do is towards the end of the model, say
[00:35:13.000 --> 00:35:15.000]   your feature map, this is the output.
[00:35:15.000 --> 00:35:17.000]   So this is your activation.
[00:35:17.000 --> 00:35:21.000]   What happens is as you go into the model, so let me do it here.
[00:35:21.000 --> 00:35:23.000]   So 512 by 512.
[00:35:23.000 --> 00:35:28.000]   Your output is, say, 1 by 256 by, let's say, 32 by 32.
[00:35:28.000 --> 00:35:30.000]   Let's say that's the output.
[00:35:30.000 --> 00:35:31.000]   Okay.
[00:35:31.000 --> 00:35:36.000]   And then when you start with 512, let's say my output is 1 by,
[00:35:36.000 --> 00:35:38.000]   again, let's say 256.
[00:35:38.000 --> 00:35:41.000]   But this time it's 128 by 128.
[00:35:41.000 --> 00:35:43.000]   Just an example.
[00:35:43.000 --> 00:35:47.000]   What happens is in your last layer, what you do is you
[00:35:47.000 --> 00:35:49.000]   normalize this part.
[00:35:49.000 --> 00:35:52.000]   You take the mean of this part or you sum them up.
[00:35:52.000 --> 00:35:55.000]   And then your output, because you're going to sum this up on
[00:35:55.000 --> 00:35:59.000]   your height and width, your output is going to be like 1 by
[00:35:59.000 --> 00:36:02.000]   26 by -- 1 by 256 by 1 by 1.
[00:36:02.000 --> 00:36:05.000]   And then you do the same thing for here because that's the last
[00:36:05.000 --> 00:36:06.000]   layer.
[00:36:06.000 --> 00:36:10.000]   So your output is going to be 1 by 26 by 1 by 1.
[00:36:10.000 --> 00:36:14.000]   So in any case, in both the cases, it doesn't matter.
[00:36:14.000 --> 00:36:16.000]   For a convolution, it doesn't matter.
[00:36:16.000 --> 00:36:18.000]   It's just a network.
[00:36:18.000 --> 00:36:21.000]   Because you have a global average pooling in the end.
[00:36:21.000 --> 00:36:26.000]   It doesn't matter if your input is 2 to 4 by 2 to 4 or 5 and 2
[00:36:26.000 --> 00:36:28.000]   by 5 and 2.
[00:36:28.000 --> 00:36:31.000]   And therefore you can start with the same pre-trained weights.
[00:36:31.000 --> 00:36:34.000]   It's completely okay if most of that didn't make sense.
[00:36:34.000 --> 00:36:36.000]   We're going to cover all of this.
[00:36:36.000 --> 00:36:40.000]   So in Fastbook, in the later chapters, we're going to look at
[00:36:40.000 --> 00:36:42.000]   all of this inside Learner.
[00:36:42.000 --> 00:36:45.000]   We're going to have a look at that in architecture details.
[00:36:45.000 --> 00:36:48.000]   We're going to have a look at the complete ResNet architecture.
[00:36:48.000 --> 00:36:51.000]   So it's completely okay if you didn't get that part.
[00:36:51.000 --> 00:36:55.000]   And we're going to look at this section in chapter 14, 15, and
[00:36:55.000 --> 00:36:56.000]   19 anyway.
[00:36:56.000 --> 00:36:59.000]   Thanks, Girijesh, for the good question.
[00:36:59.000 --> 00:37:02.000]   How can a model handle multiple input sizes?
[00:37:02.000 --> 00:37:04.000]   I was under the impression -- okay.
[00:37:04.000 --> 00:37:07.000]   I hope now that this should be clear, Ravi.
[00:37:07.000 --> 00:37:11.000]   So if Ravi and Girijesh, could you maybe just reply to these
[00:37:11.000 --> 00:37:14.000]   comments and let me know that what I've explained has been
[00:37:14.000 --> 00:37:15.000]   somewhat helpful.
[00:37:15.000 --> 00:37:19.000]   I don't expect you to get all of it right now, but hopefully
[00:37:19.000 --> 00:37:21.000]   some of it.
[00:37:21.000 --> 00:37:25.000]   What is the difference in item resizing, batch resizing, and
[00:37:25.000 --> 00:37:26.000]   progressive resizing?
[00:37:26.000 --> 00:37:28.000]   Okay.
[00:37:28.000 --> 00:37:30.000]   It's not item resizing.
[00:37:30.000 --> 00:37:31.000]   It's item transforms.
[00:37:31.000 --> 00:37:33.000]   It's not batch resizing.
[00:37:33.000 --> 00:37:34.000]   It's batch transforms.
[00:37:34.000 --> 00:37:37.000]   And then progressive resizing is completely different.
[00:37:37.000 --> 00:37:42.000]   Progressive resizing is this idea of first you start your
[00:37:42.000 --> 00:37:45.000]   model with smaller images, smaller image size.
[00:37:45.000 --> 00:37:50.000]   So you input to the model 2 to 4 by 2 to 4.
[00:37:50.000 --> 00:37:55.000]   Then you train your model for five epochs as we've done, say,
[00:37:55.000 --> 00:37:58.000]   where is that?
[00:37:58.000 --> 00:37:59.000]   Okay.
[00:37:59.000 --> 00:38:03.000]   As you can see over here, we started with 128 by 128.
[00:38:03.000 --> 00:38:04.000]   We trained our model.
[00:38:04.000 --> 00:38:07.000]   Then we used the same model weights and we started training
[00:38:07.000 --> 00:38:10.000]   with -- like we haven't changed anything in the model.
[00:38:10.000 --> 00:38:11.000]   It's the same model.
[00:38:11.000 --> 00:38:14.000]   And then we just started training with 2 to 4 by 2 to 4.
[00:38:14.000 --> 00:38:18.000]   So this idea of starting with a smaller image size first and
[00:38:18.000 --> 00:38:22.000]   then going to a bigger image size is progressive resizing.
[00:38:22.000 --> 00:38:25.000]   So you start with, say, yeah, you start with smaller image
[00:38:25.000 --> 00:38:28.000]   size, you train for five epochs, then you go to bigger image
[00:38:28.000 --> 00:38:31.000]   size, you train for five epochs, then you train for even bigger
[00:38:31.000 --> 00:38:33.000]   image size and you train for five epochs.
[00:38:33.000 --> 00:38:36.000]   This idea of progressively increasing your image size is
[00:38:36.000 --> 00:38:38.000]   progressive resizing.
[00:38:38.000 --> 00:38:42.000]   So that's batch sizing and item transforms or why we do all of
[00:38:42.000 --> 00:38:45.000]   this or what's the difference in these two sizes.
[00:38:45.000 --> 00:38:47.000]   We've covered this in the past.
[00:38:47.000 --> 00:38:50.000]   Again, this is the idea of pre-sizing, so I won't explain
[00:38:50.000 --> 00:38:51.000]   that right now.
[00:38:51.000 --> 00:38:54.000]   But please have a look at those particular videos.
[00:38:54.000 --> 00:39:01.000]   I think that is section -- I think that's section pet breeds.
[00:39:01.000 --> 00:39:03.000]   Should be pet breeds, I believe.
[00:39:03.000 --> 00:39:05.000]   That's where we covered pre-sizing.
[00:39:05.000 --> 00:39:08.000]   But yes, it's in the past that we've already covered that, so I
[00:39:08.000 --> 00:39:12.000]   won't go into the details.
[00:39:12.000 --> 00:39:13.000]   Okay.
[00:39:13.000 --> 00:39:17.000]   That's that.
[00:39:17.000 --> 00:39:18.000]   Yes.
[00:39:18.000 --> 00:39:21.000]   We covered that in section 1.2 of the pet breeds.
[00:39:21.000 --> 00:39:25.000]   So if anybody has questions about this idea of item transforms
[00:39:25.000 --> 00:39:28.000]   and then batch transforms, please go back and have a look at
[00:39:28.000 --> 00:39:31.000]   this part of the video and you'll see what's the difference and
[00:39:31.000 --> 00:39:36.000]   how we have different resizing.
[00:39:36.000 --> 00:39:37.000]   All right.
[00:39:37.000 --> 00:39:45.000]   So next we're going to look at test time augmentation.
[00:39:45.000 --> 00:39:46.000]   Okay.
[00:39:46.000 --> 00:39:52.000]   So so far what we've done is in test time augmentation, so far
[00:39:52.000 --> 00:39:55.000]   what we've done is we've only done augmentation on the
[00:39:55.000 --> 00:39:56.000]   training images.
[00:39:56.000 --> 00:39:58.000]   Right?
[00:39:58.000 --> 00:40:02.000]   We haven't done any augmentation with validation data sets.
[00:40:02.000 --> 00:40:14.000]   So what that means is -- what that basically means is this is
[00:40:14.000 --> 00:40:15.000]   my model.
[00:40:15.000 --> 00:40:17.000]   Right?
[00:40:17.000 --> 00:40:19.000]   I pass in my input data set.
[00:40:19.000 --> 00:40:23.000]   So let's say that's 10,000 10K images.
[00:40:23.000 --> 00:40:24.000]   Okay.
[00:40:24.000 --> 00:40:26.000]   I train my model with this data set.
[00:40:26.000 --> 00:40:28.000]   Now my model is trained.
[00:40:28.000 --> 00:40:29.000]   Right?
[00:40:29.000 --> 00:40:33.000]   Then what we do is we take the same model with the same weights.
[00:40:33.000 --> 00:40:35.000]   So it's the same model anyway.
[00:40:35.000 --> 00:40:39.000]   And then you say you have validation set and it has 1,000
[00:40:39.000 --> 00:40:42.000]   validation images.
[00:40:42.000 --> 00:40:46.000]   And then you pass this and you get some outputs or you get some
[00:40:46.000 --> 00:40:47.000]   predictions.
[00:40:47.000 --> 00:40:49.000]   Right?
[00:40:49.000 --> 00:40:54.000]   What we've done is for each input image -- so let's say this is
[00:40:54.000 --> 00:40:56.000]   my input image.
[00:40:56.000 --> 00:40:58.000]   We're using random cropping.
[00:40:58.000 --> 00:41:02.000]   So it just randomly crops some part of the section or it would
[00:41:02.000 --> 00:41:03.000]   rotate.
[00:41:03.000 --> 00:41:06.000]   So it would be -- it would look like this.
[00:41:06.000 --> 00:41:09.000]   Rotated if it got rotated clockwise.
[00:41:09.000 --> 00:41:11.000]   Or it would do something like blurring.
[00:41:11.000 --> 00:41:15.000]   So your model basically is going to blur this part of the whole
[00:41:15.000 --> 00:41:16.000]   image.
[00:41:16.000 --> 00:41:19.000]   So it's like all these different augmentations that we do during
[00:41:19.000 --> 00:41:21.000]   our training.
[00:41:21.000 --> 00:41:23.000]   And then that makes our model more robust.
[00:41:23.000 --> 00:41:27.000]   So it's able to now -- so in your validation data set, if you have
[00:41:27.000 --> 00:41:31.000]   this object -- so if you have this object, but this time this
[00:41:31.000 --> 00:41:33.000]   object maybe looks rotated leftward.
[00:41:33.000 --> 00:41:35.000]   So that's how you see the same object.
[00:41:35.000 --> 00:41:37.000]   But a different version of that.
[00:41:37.000 --> 00:41:41.000]   Then our model is going to be able to classify this image.
[00:41:41.000 --> 00:41:43.000]   So that's what augmentation does.
[00:41:43.000 --> 00:41:45.000]   It makes your model robust.
[00:41:45.000 --> 00:41:47.000]   You can also flip it.
[00:41:47.000 --> 00:41:49.000]   You can flip it left or right or up or down.
[00:41:49.000 --> 00:41:52.000]   There's all these different augmentations that you can do.
[00:41:52.000 --> 00:41:54.000]   And then you can do that in the training set.
[00:41:54.000 --> 00:41:57.000]   So this is all training.
[00:41:57.000 --> 00:41:59.000]   At the top.
[00:41:59.000 --> 00:42:04.000]   But in validation data set, the typical strategy or what we do is
[00:42:04.000 --> 00:42:06.000]   you do a center crop.
[00:42:06.000 --> 00:42:09.000]   So if your images are being trained, say, let's say they were
[00:42:09.000 --> 00:42:12.000]   trained on 2 to 4 by 2 to 4 image size.
[00:42:12.000 --> 00:42:14.000]   Or whatever the height and width actually.
[00:42:14.000 --> 00:42:17.000]   Don't worry about 2 to 4 by 2 to 4.
[00:42:17.000 --> 00:42:21.000]   What you do is in your validation set, you take a center crop.
[00:42:21.000 --> 00:42:28.000]   And what that does is it works 90% of the time.
[00:42:28.000 --> 00:42:33.000]   But then there's situations where your object could be at basically
[00:42:33.000 --> 00:42:35.000]   the top right.
[00:42:35.000 --> 00:42:37.000]   Or there could be, like, multiple objects in your image when you're
[00:42:37.000 --> 00:42:39.000]   doing multilabel classifications.
[00:42:39.000 --> 00:42:41.000]   Let's say if you're doing a chair.
[00:42:41.000 --> 00:42:43.000]   So I'm going to try and draw a chair.
[00:42:43.000 --> 00:42:47.000]   Or maybe, yeah, let's say that's my chair.
[00:42:47.000 --> 00:42:49.000]   And then there's the TV over here.
[00:42:49.000 --> 00:42:51.000]   I'm just trying to draw a small example.
[00:42:51.000 --> 00:42:56.000]   And when we do a center crop, then we didn't catch the TV in it.
[00:42:56.000 --> 00:43:00.000]   So our model is never actually seeing this part in the validation set.
[00:43:00.000 --> 00:43:04.000]   So when it's trying to predict, it will always predict this one class.
[00:43:04.000 --> 00:43:06.000]   And, of course, this happens very less.
[00:43:06.000 --> 00:43:10.000]   Like, it's not very common for this to happen.
[00:43:10.000 --> 00:43:14.000]   But there's going to be cases when these things do happen.
[00:43:14.000 --> 00:43:20.000]   So what we do instead is now I have my same model.
[00:43:20.000 --> 00:43:21.000]   This is my trained model.
[00:43:21.000 --> 00:43:23.000]   So I keep it here.
[00:43:23.000 --> 00:43:28.000]   And then what I can do is I can rotate my validation images as well.
[00:43:28.000 --> 00:43:30.000]   So I can -- this is my, say, original image.
[00:43:30.000 --> 00:43:32.000]   Then I can apply my augmentation.
[00:43:32.000 --> 00:43:34.000]   So maybe I rotate it.
[00:43:34.000 --> 00:43:37.000]   Then I flip it.
[00:43:37.000 --> 00:43:42.000]   Or I just have different crops of the image.
[00:43:42.000 --> 00:43:44.000]   So that's the first crop.
[00:43:44.000 --> 00:43:46.000]   And then I can have maybe some number.
[00:43:46.000 --> 00:43:49.000]   And then I can have maybe my second crop, which looks like this.
[00:43:49.000 --> 00:43:54.000]   So anyway, so I can now do all of these five different augmentations on my
[00:43:54.000 --> 00:43:56.000]   validation image.
[00:43:56.000 --> 00:44:00.000]   And I can input all of them to my model.
[00:44:00.000 --> 00:44:03.000]   And then let's say -- then you're going to get five outputs, right?
[00:44:03.000 --> 00:44:06.000]   One, two, three, four, five.
[00:44:06.000 --> 00:44:10.000]   So let's say your classes or your output number of classes was 10.
[00:44:10.000 --> 00:44:12.000]   So you're going to get five cross 10.
[00:44:12.000 --> 00:44:14.000]   And so on.
[00:44:14.000 --> 00:44:16.000]   So this is 10 over here.
[00:44:16.000 --> 00:44:18.000]   And then this is five here.
[00:44:18.000 --> 00:44:24.000]   Because for each image -- for each image, you're going to get one row of
[00:44:24.000 --> 00:44:26.000]   output.
[00:44:26.000 --> 00:44:28.000]   We've already covered this when we were doing multilabel classification.
[00:44:28.000 --> 00:44:30.000]   So we already know what the model outputs.
[00:44:30.000 --> 00:44:33.000]   So I'm not going to go into detail on what the output shapes are.
[00:44:33.000 --> 00:44:36.000]   But basically then for each image, you get some output.
[00:44:36.000 --> 00:44:41.000]   And then we could just take -- basically we could ask the model to predict
[00:44:41.000 --> 00:44:43.000]   an augmented version of my validation image.
[00:44:43.000 --> 00:44:46.000]   And then we could take the average of this.
[00:44:46.000 --> 00:44:55.000]   So you get still a single line of output, which is now one row and ten
[00:44:55.000 --> 00:44:57.000]   classes.
[00:44:57.000 --> 00:44:59.000]   But you've taken the average.
[00:44:59.000 --> 00:45:01.000]   So you take average.
[00:45:01.000 --> 00:45:06.000]   And then you get one prediction for your validation image.
[00:45:06.000 --> 00:45:12.000]   And that's this idea of having augmentations in your validation set is
[00:45:12.000 --> 00:45:14.000]   test time augmentation.
[00:45:14.000 --> 00:45:19.000]   So it's called TTA for short.
[00:45:19.000 --> 00:45:21.000]   I can remove this part.
[00:45:21.000 --> 00:45:23.000]   I can remove this part.
[00:45:23.000 --> 00:45:28.000]   And then to apply TTA in Fast.ai, it's very simple.
[00:45:28.000 --> 00:45:30.000]   You can just say learn.TTA.
[00:45:30.000 --> 00:45:33.000]   I'm not sure if it accepts -- okay.
[00:45:33.000 --> 00:45:35.000]   It accepts this number n.
[00:45:35.000 --> 00:45:38.000]   I think you can say how many times you want to augment your image.
[00:45:38.000 --> 00:45:41.000]   Because in this case, I took my n equal to five example.
[00:45:41.000 --> 00:45:45.000]   I showed you five different augmentations of my image.
[00:45:45.000 --> 00:45:48.000]   And then I took the average of those five.
[00:45:48.000 --> 00:45:53.000]   In the default, I think the default for Fast.ai is n equal to four.
[00:45:53.000 --> 00:45:55.000]   You can try different versions of this.
[00:45:55.000 --> 00:46:00.000]   And you can see how a different number changes my output.
[00:46:00.000 --> 00:46:03.000]   So this is something else that you should try is test time augmentation
[00:46:03.000 --> 00:46:05.000]   for Cassava.
[00:46:05.000 --> 00:46:07.000]   So that's the main ideas.
[00:46:07.000 --> 00:46:10.000]   I think for Cassava, so far, make sure you're normalizing.
[00:46:10.000 --> 00:46:14.000]   You can just use ImageNet data sets as long as you're using CNN Learner.
[00:46:14.000 --> 00:46:16.000]   I'm not sure.
[00:46:16.000 --> 00:46:20.000]   If you're using the CNN Learner from Fast.ai to create your learner
[00:46:20.000 --> 00:46:23.000]   object, then that should already have the normalization.
[00:46:23.000 --> 00:46:26.000]   If you're not, then make sure you're normalizing the data set.
[00:46:26.000 --> 00:46:29.000]   Have this idea, have a go at this progressive resizing.
[00:46:29.000 --> 00:46:31.000]   So maybe start with smaller images.
[00:46:31.000 --> 00:46:33.000]   First, train for five epochs.
[00:46:33.000 --> 00:46:35.000]   Then update your data loaders.
[00:46:35.000 --> 00:46:37.000]   Train for five and two by five and two.
[00:46:37.000 --> 00:46:41.000]   Then train for 600 by 600 and 768 by 768.
[00:46:41.000 --> 00:46:44.000]   And see if that helps improve your accuracy.
[00:46:44.000 --> 00:46:47.000]   And then finally, try test time augmentation.
[00:46:47.000 --> 00:46:51.000]   So these are the main things you should play around with.
[00:46:51.000 --> 00:46:55.000]   I'll just have a look if there's any questions so far.
[00:46:55.000 --> 00:46:58.000]   Looks like no questions on these four topics.
[00:46:58.000 --> 00:47:06.000]   Okay, great.
[00:47:06.000 --> 00:47:11.000]   So then the next one is we're going to have a look at Mixup.
[00:47:11.000 --> 00:47:15.000]   Now.
[00:47:20.000 --> 00:47:22.000]   Can you explain T again?
[00:47:22.000 --> 00:47:25.000]   Did you mean to say TTA, test time augmentation?
[00:47:25.000 --> 00:47:27.000]   When you say again, do you mean the whole thing again?
[00:47:27.000 --> 00:47:29.000]   I'll just give you a shorter version.
[00:47:29.000 --> 00:47:31.000]   You have your validation image.
[00:47:31.000 --> 00:47:34.000]   Typically, you take your validation image, you pass it to your model,
[00:47:34.000 --> 00:47:36.000]   and you get your output, right?
[00:47:36.000 --> 00:47:38.000]   That's what you do typically.
[00:47:38.000 --> 00:47:42.000]   What you're now doing is you take your validation image and you
[00:47:42.000 --> 00:47:45.000]   augment it to some number of times.
[00:47:45.000 --> 00:47:49.000]   So you have now four extra versions and your original image if
[00:47:49.000 --> 00:47:51.000]   your TTA is five.
[00:47:51.000 --> 00:47:54.000]   Basically, then you're creating your augmented versions of your
[00:47:54.000 --> 00:47:57.000]   image, then you pass them all in your model.
[00:47:57.000 --> 00:48:02.000]   So you get output and you average the predictions for each of these
[00:48:02.000 --> 00:48:06.000]   different versions of the same image, and then you get your final
[00:48:06.000 --> 00:48:07.000]   prediction.
[00:48:07.000 --> 00:48:10.000]   So basically, now your model is going to be more robust or your
[00:48:10.000 --> 00:48:13.000]   predictions are going to be a bit more robust.
[00:48:13.000 --> 00:48:16.000]   That's the idea behind TTA.
[00:48:18.000 --> 00:48:19.000]   Okay.
[00:48:19.000 --> 00:48:21.000]   So that's TTA.
[00:48:21.000 --> 00:48:23.000]   So now I'm moving on to Mixup.
[00:48:23.000 --> 00:48:24.000]   Okay.
[00:48:24.000 --> 00:48:29.000]   So Mixup was introduced in 2017, I guess in October.
[00:48:29.000 --> 00:48:34.000]   And Mixup introduced this really crazy idea.
[00:48:34.000 --> 00:48:38.000]   I call it crazy because it's very counterintuitive, at least to me
[00:48:38.000 --> 00:48:39.000]   it is.
[00:48:39.000 --> 00:48:45.000]   And what Mixup did was -- let me again show you what Mixup did.
[00:48:45.000 --> 00:48:47.000]   All right.
[00:48:47.000 --> 00:48:50.000]   So this is -- so during my training, so all of this happens during
[00:48:50.000 --> 00:48:51.000]   training, okay?
[00:48:51.000 --> 00:48:53.000]   Mixup is during training.
[00:48:53.000 --> 00:48:57.000]   So I'm just going to say training over here.
[00:48:57.000 --> 00:48:59.000]   So I have my model.
[00:48:59.000 --> 00:49:01.000]   This is what my model is.
[00:49:01.000 --> 00:49:07.000]   During training, what I do is -- because I have -- remember, this
[00:49:07.000 --> 00:49:10.000]   is something that you should already know and this is something we've
[00:49:10.000 --> 00:49:15.000]   covered many times, is that you input your images to your model
[00:49:15.000 --> 00:49:17.000]   batch by batch, right?
[00:49:17.000 --> 00:49:21.000]   So let's say I have ten images in my dataset right now.
[00:49:21.000 --> 00:49:23.000]   Let's just say I have ten images.
[00:49:23.000 --> 00:49:27.000]   So this is my one, two, three, and so on.
[00:49:27.000 --> 00:49:29.000]   And then that's my tenth, right?
[00:49:29.000 --> 00:49:33.000]   Now, typically what you would do is you input this and you get some
[00:49:33.000 --> 00:49:35.000]   outputs and you're training your model, right?
[00:49:35.000 --> 00:49:41.000]   In Mixup, what Mixup said was, okay, you take this image and then
[00:49:41.000 --> 00:49:43.000]   you mix it with another image.
[00:49:43.000 --> 00:49:47.000]   So basically, without worrying about anything else, you have two
[00:49:47.000 --> 00:49:48.000]   images.
[00:49:48.000 --> 00:49:51.000]   So you have your first -- sorry, you have your two images.
[00:49:51.000 --> 00:49:53.000]   You take this as the first image.
[00:49:53.000 --> 00:49:55.000]   You take this as the second image.
[00:49:55.000 --> 00:49:59.000]   And let's say the category or the prediction for this is -- let's say
[00:49:59.000 --> 00:50:01.000]   this is a cat.
[00:50:01.000 --> 00:50:03.000]   And let's say this is a dog.
[00:50:03.000 --> 00:50:06.000]   So without worrying about all the other images right now, let's say I
[00:50:06.000 --> 00:50:09.000]   just have two images, okay?
[00:50:09.000 --> 00:50:12.000]   Because it's going to be the same for multiple numbers anyway.
[00:50:12.000 --> 00:50:16.000]   So let's say I have my first image as cat, my second image as dog.
[00:50:16.000 --> 00:50:21.000]   And now instead of asking the model to predict that this first image is
[00:50:21.000 --> 00:50:27.000]   100% cat and the second image is 100% dog, what we do is we mix our
[00:50:27.000 --> 00:50:29.000]   images.
[00:50:29.000 --> 00:50:31.000]   So let's say the Mixup number is 30%.
[00:50:31.000 --> 00:50:40.000]   So I will say mix my images to say 30% of cat plus 70% of dog.
[00:50:40.000 --> 00:50:43.000]   So how could we do that?
[00:50:43.000 --> 00:50:45.000]   Well, it's very easy.
[00:50:45.000 --> 00:50:48.000]   Because cat and dog or the images underneath are just numbers.
[00:50:48.000 --> 00:50:51.000]   So I could just multiply this tensor by .3.
[00:50:51.000 --> 00:50:55.000]   I could just multiply this tensor by .7, add them up, and I have my
[00:50:55.000 --> 00:50:57.000]   mixed image like this.
[00:50:57.000 --> 00:50:59.000]   So what Mixup said was do this.
[00:50:59.000 --> 00:51:02.000]   Have this Mixup number.
[00:51:02.000 --> 00:51:04.000]   I've chosen this to be 30%.
[00:51:04.000 --> 00:51:06.000]   It could be like a different number.
[00:51:06.000 --> 00:51:14.000]   But have this Mixup -- like mix your images by some number and then
[00:51:14.000 --> 00:51:16.000]   some proportion.
[00:51:16.000 --> 00:51:20.000]   And then now what happens is instead of your output -- so what are your
[00:51:20.000 --> 00:51:22.000]   outputs going to be?
[00:51:22.000 --> 00:51:27.000]   Instead of the model predicting like this image is 100% cat, what the
[00:51:27.000 --> 00:51:33.000]   model needs to predict for what this input image is, what my labels or
[00:51:33.000 --> 00:51:39.000]   what the model needs to predict is that it's 30% cat and it's 70% dog.
[00:51:39.000 --> 00:51:41.000]   Right?
[00:51:41.000 --> 00:51:43.000]   So how could we update my labels?
[00:51:43.000 --> 00:51:45.000]   I could one-hot encode my labels.
[00:51:45.000 --> 00:51:48.000]   So let's say I have maybe three categories.
[00:51:48.000 --> 00:51:51.000]   Cat, horse, and dog.
[00:51:51.000 --> 00:51:53.000]   Let's say I have three categories.
[00:51:53.000 --> 00:51:59.000]   So my cat looks like 100 and my dog is 001.
[00:51:59.000 --> 00:52:05.000]   So if I'm using 30% of this and I'm using 70% of this, then my labels
[00:52:05.000 --> 00:52:12.000]   become .30 and .7.
[00:52:12.000 --> 00:52:16.000]   So I updated my labels and then you can use the cross-entropy law
[00:52:16.000 --> 00:52:19.000]   similarly and then that's the idea.
[00:52:19.000 --> 00:52:28.000]   So this is what Mixup does.
[00:52:28.000 --> 00:52:31.000]   So let me show you what's happening for each image.
[00:52:31.000 --> 00:52:35.000]   So for each image in your batch, select another image from your
[00:52:35.000 --> 00:52:37.000]   dataset at random.
[00:52:37.000 --> 00:52:39.000]   Pick a weight at random.
[00:52:39.000 --> 00:52:41.000]   Take a weighted average of the image.
[00:52:41.000 --> 00:52:43.000]   This will be your independent variable.
[00:52:43.000 --> 00:52:47.000]   That just means you take 30% of the cat image and 70% of the dog
[00:52:47.000 --> 00:52:51.000]   image and then take a weighted average of this image labels with the
[00:52:51.000 --> 00:52:53.000]   other image labels.
[00:52:53.000 --> 00:52:55.000]   This will be your dependent variable.
[00:52:55.000 --> 00:52:58.000]   So let's say my cat looked like this.
[00:52:58.000 --> 00:53:00.000]   Oh, sorry.
[00:53:00.000 --> 00:53:02.000]   One second.
[00:53:02.000 --> 00:53:05.000]   I'm not sure what just happened.
[00:53:05.000 --> 00:53:10.000]   Can you still see my screen or not?
[00:53:10.000 --> 00:53:18.000]   I think my laptop decided to stop duplicating my displays.
[00:53:18.000 --> 00:53:25.000]   One second.
[00:53:25.000 --> 00:53:27.000]   Okay.
[00:53:27.000 --> 00:53:29.000]   Can you guys please see my screen?
[00:53:29.000 --> 00:53:31.000]   Could you please maybe just say yes or no in the chat?
[00:53:31.000 --> 00:53:32.000]   Okay.
[00:53:32.000 --> 00:53:33.000]   Excellent.
[00:53:33.000 --> 00:53:34.000]   Perfect.
[00:53:34.000 --> 00:53:36.000]   I'm really sorry for all these technical errors today.
[00:53:36.000 --> 00:53:39.000]   But then -- so I was here.
[00:53:39.000 --> 00:53:44.000]   So I was just saying then the basic idea is then you take the weighted
[00:53:44.000 --> 00:53:46.000]   average of your labels.
[00:53:46.000 --> 00:53:48.000]   So my cat looks like this.
[00:53:48.000 --> 00:53:50.000]   My dog looks like this.
[00:53:50.000 --> 00:53:52.000]   So I take my weighted average and this is what I get.
[00:53:52.000 --> 00:53:55.000]   So then my input image is this one.
[00:53:55.000 --> 00:53:58.000]   And my labels then are updated to be like this.
[00:53:58.000 --> 00:54:04.000]   And that's the idea.
[00:54:04.000 --> 00:54:07.000]   So let me show you how that looks like.
[00:54:07.000 --> 00:54:09.000]   So how could we do that?
[00:54:09.000 --> 00:54:13.000]   I could just grab a random image of a church.
[00:54:13.000 --> 00:54:15.000]   I could grab an image of a gas station.
[00:54:15.000 --> 00:54:17.000]   So let's plot the image of the church.
[00:54:17.000 --> 00:54:19.000]   That's how that looks like.
[00:54:19.000 --> 00:54:21.000]   Let's plot the image of the gas station.
[00:54:21.000 --> 00:54:23.000]   That's how that looks like.
[00:54:23.000 --> 00:54:27.000]   And then when I'm going to apply mixup,
[00:54:27.000 --> 00:54:31.000]   first let me resize both of them to be 256 by 256.
[00:54:31.000 --> 00:54:33.000]   So that's just resizing.
[00:54:33.000 --> 00:54:35.000]   Sorry, one second.
[00:54:35.000 --> 00:54:41.000]   So I'm just going to resize both of them to be this church and gas.
[00:54:41.000 --> 00:54:47.000]   Then I can convert both of them, because right now the church and gas are images,
[00:54:47.000 --> 00:54:49.000]   or like they're PIL image.
[00:54:49.000 --> 00:54:51.000]   I'm just going to convert them to tensors,
[00:54:51.000 --> 00:54:55.000]   because you can't really do all these operations
[00:54:55.000 --> 00:54:58.000]   or like multiply something by 0.3 unless they're presented as numbers.
[00:54:58.000 --> 00:55:00.000]   So you're just converting them to be tensors.
[00:55:00.000 --> 00:55:02.000]   So let's do that.
[00:55:02.000 --> 00:55:05.000]   So I get my tensor church, I get my tensor gas.
[00:55:05.000 --> 00:55:07.000]   So my image now, the church image,
[00:55:07.000 --> 00:55:10.000]   is just these bunch of random numbers.
[00:55:10.000 --> 00:55:12.000]   Not random numbers, but bunch of numbers.
[00:55:12.000 --> 00:55:16.000]   And then t gas is also again a tensor representing that image.
[00:55:16.000 --> 00:55:20.000]   So I can check the shape for both.
[00:55:20.000 --> 00:55:23.000]   It's 256 by 256 by 3, 256 by 256 by 3,
[00:55:23.000 --> 00:55:26.000]   because 256 by 256 channels and 3,
[00:55:26.000 --> 00:55:31.000]   sorry, 256 by 256 height and width and 3 channels.
[00:55:31.000 --> 00:55:35.000]   So I can then show my image for t church, show my image for t gas,
[00:55:35.000 --> 00:55:39.000]   and then I can show this image of 0.3 times church and 0.7 times gas,
[00:55:39.000 --> 00:55:43.000]   which looks like...
[00:55:43.000 --> 00:55:46.000]   See that? That's the image.
[00:55:46.000 --> 00:55:51.000]   So you can see how that, in that you can still see some part of the church,
[00:55:51.000 --> 00:55:53.000]   and you can see still some part of...
[00:55:53.000 --> 00:55:56.000]   So you can see this tree over here, which is 30%.
[00:55:56.000 --> 00:56:02.000]   So what happens if I do maybe 50% of that and 50% of this?
[00:56:02.000 --> 00:56:04.000]   Would that make things better?
[00:56:04.000 --> 00:56:07.000]   Yes. So you can see now the images are...
[00:56:07.000 --> 00:56:11.000]   This is my mixup image.
[00:56:11.000 --> 00:56:14.000]   So yes, to me, this is very contributive.
[00:56:14.000 --> 00:56:17.000]   Like how is this going to help the model, right?
[00:56:17.000 --> 00:56:21.000]   There's some theory behind on why this is going to help the model,
[00:56:21.000 --> 00:56:24.000]   and I'm going to cover that in a moment.
[00:56:24.000 --> 00:56:26.000]   But basically this is the idea of mixup.
[00:56:26.000 --> 00:56:32.000]   And then as I said, you can have your labels as one-hot encoded,
[00:56:32.000 --> 00:56:35.000]   so I could have my labels as one-hot encoded,
[00:56:35.000 --> 00:56:39.000]   and then my final target just becomes like this 0.3 and then 0.7.
[00:56:39.000 --> 00:56:43.000]   Don't worry about this. This is me playing with the notebook.
[00:56:43.000 --> 00:56:45.000]   And then you just have to then...
[00:56:45.000 --> 00:56:49.000]   In Fast.ai, if you want to apply mixup, you don't have to do anything.
[00:56:49.000 --> 00:56:53.000]   I've just explained what mixup is, and if you want to apply that to Cassava,
[00:56:53.000 --> 00:56:55.000]   do nothing. Just create your learner,
[00:56:55.000 --> 00:56:58.000]   and then just pass in this callback called mixup.
[00:56:58.000 --> 00:57:00.000]   So in Fast.ai, whenever you want to customize things
[00:57:00.000 --> 00:57:04.000]   or you want to basically add more augmentation,
[00:57:04.000 --> 00:57:06.000]   this is the idea of callbacks.
[00:57:06.000 --> 00:57:11.000]   We will cover callbacks in this chapter, chapter Accelerated SGD,
[00:57:11.000 --> 00:57:14.000]   and we will learn how to make our own callbacks as well.
[00:57:14.000 --> 00:57:18.000]   So it's completely okay for you to not worry about them right now.
[00:57:18.000 --> 00:57:22.000]   But all you have to do then to apply mixup to Cassava, do this.
[00:57:22.000 --> 00:57:25.000]   Just update your learner, pass in your CBs with this mixup CB,
[00:57:25.000 --> 00:57:27.000]   and then update your loss function.
[00:57:27.000 --> 00:57:30.000]   And that's it. Fast.ai will take care of that for you.
[00:57:32.000 --> 00:57:36.000]   So now the question is, why does this work?
[00:57:36.000 --> 00:57:38.000]   Okay. So why does that work?
[00:57:38.000 --> 00:57:42.000]   So I'm going to go into a little bit of theory.
[00:57:42.000 --> 00:57:46.000]   But it's okay if you don't get it, so don't worry.
[00:57:46.000 --> 00:57:48.000]   But let me try and explain that.
[00:57:48.000 --> 00:57:51.000]   So what happens is, when you input your images,
[00:57:51.000 --> 00:57:57.000]   so let's say, again, this is my model, right?
[00:57:57.000 --> 00:58:02.000]   And I input a CAD image. So my CAD image is 1.0.0.
[00:58:02.000 --> 00:58:04.000]   Okay? This is my input.
[00:58:04.000 --> 00:58:07.000]   So what happens when I input this image?
[00:58:07.000 --> 00:58:11.000]   Remember, my last layer of this is a Softmax, right?
[00:58:11.000 --> 00:58:13.000]   So what Softmax is going to do,
[00:58:13.000 --> 00:58:16.000]   there's going to be some outputs from the model
[00:58:16.000 --> 00:58:20.000]   that are going to look like -.3, or sorry, let's say,
[00:58:20.000 --> 00:58:25.000]   yeah, they're going to look like -.3, then 1.4, and then 0.0.
[00:58:25.000 --> 00:58:27.000]   This is going to be some random numbers.
[00:58:27.000 --> 00:58:31.000]   Then you apply Softmax to your output logits.
[00:58:31.000 --> 00:58:34.000]   So this is something we've covered already in the past.
[00:58:34.000 --> 00:58:39.000]   So then you get numbers like, let's say, this becomes 0.2,
[00:58:39.000 --> 00:58:43.000]   this becomes 0.6, and this becomes 0. -
[00:58:43.000 --> 00:58:46.000]   this won't be 0.2, this will be 0.3, and this will be, say, 0.1.
[00:58:46.000 --> 00:58:49.000]   So that's your Softmax outputs, right?
[00:58:50.000 --> 00:58:55.000]   For the loss to go down, this number, 0.3,
[00:58:55.000 --> 00:58:57.000]   should be as close to 1,
[00:58:57.000 --> 00:59:00.000]   and these numbers should be as close to 0.
[00:59:00.000 --> 00:59:05.000]   And then any output from the Softmax
[00:59:05.000 --> 00:59:08.000]   is never going to be exactly 1, right?
[00:59:08.000 --> 00:59:12.000]   It's never going to be like 1.0.0.
[00:59:12.000 --> 00:59:15.000]   The model is never going to output something like that.
[00:59:18.000 --> 00:59:20.000]   Sorry, one second.
[00:59:20.000 --> 00:59:25.000]   So the outputs from Softmax are never going to look like 1.0.0, right?
[00:59:25.000 --> 00:59:28.000]   The max, or the model that can try, is what it's going to do,
[00:59:28.000 --> 00:59:33.000]   is it's going to try and push this number up.
[00:59:33.000 --> 00:59:37.000]   So it's going to try and make this as big a number as possible.
[00:59:37.000 --> 00:59:40.000]   So the model is going to try and make this number, let's say, 1,000,
[00:59:40.000 --> 00:59:44.000]   and it's going to try and push these numbers down as much as possible.
[00:59:44.000 --> 00:59:47.000]   So it's going to try and make them, like, -700.
[00:59:47.000 --> 00:59:50.000]   I'm just coming up with random numbers, okay?
[00:59:50.000 --> 00:59:52.000]   But this is what the model is going to try and do.
[00:59:52.000 --> 00:59:58.000]   And what that does, when we work with this kind of labels,
[00:59:58.000 --> 01:00:04.000]   what that does is my model starts to become overly confident.
[01:00:04.000 --> 01:00:06.000]   Like, it becomes really, really confident
[01:00:06.000 --> 01:00:12.000]   in terms of, like, this idea of my model's now very confident
[01:00:12.000 --> 01:00:17.000]   that this image is like the image of a cat, okay?
[01:00:17.000 --> 01:00:19.000]   Which is good for this training image.
[01:00:19.000 --> 01:00:26.000]   But when I come to, say, when I come to, in my validation set,
[01:00:26.000 --> 01:00:31.000]   let's say there's, like, the image has, in my validation set,
[01:00:31.000 --> 01:00:34.000]   let's say there's a cat over here,
[01:00:34.000 --> 01:00:37.000]   and then there's a really small part of the dog, okay?
[01:00:37.000 --> 01:00:40.000]   We don't want, at this stage, we don't want our model
[01:00:40.000 --> 01:00:44.000]   to be just as confident as this, right?
[01:00:44.000 --> 01:00:48.000]   Because if the model's really confident,
[01:00:48.000 --> 01:00:55.000]   it's always going to output this as 0.99 cat and then 0.01 dog.
[01:00:55.000 --> 01:00:57.000]   Which is incorrect.
[01:00:57.000 --> 01:01:01.000]   Like, it should have been something like 0.8 cat or 0.2 dog, right?
[01:01:01.000 --> 01:01:04.000]   That's a more...
[01:01:04.000 --> 01:01:08.000]   That's a better representation of the outputs.
[01:01:08.000 --> 01:01:13.000]   And when that matters is it really matters in medical domain
[01:01:13.000 --> 01:01:16.000]   when your model says, okay, this person has cancer, 99%.
[01:01:16.000 --> 01:01:21.000]   I'm 99% confident that this image has a particular type of cancer
[01:01:21.000 --> 01:01:23.000]   or a particular type of tumor.
[01:01:23.000 --> 01:01:26.000]   But we actually want the confidence, like, we want the model to say,
[01:01:26.000 --> 01:01:31.000]   I'm 60% confident that this model, in medical,
[01:01:31.000 --> 01:01:33.000]   that this image has tumor.
[01:01:33.000 --> 01:01:37.000]   Because then it's a big difference when we're using models
[01:01:37.000 --> 01:01:39.000]   in hospitals.
[01:01:39.000 --> 01:01:43.000]   And in hospital, a model says, oh, this image has 90% tumor
[01:01:43.000 --> 01:01:45.000]   or this image has 60% tumor.
[01:01:45.000 --> 01:01:48.000]   It's a very big difference between the two because it's life and death.
[01:01:48.000 --> 01:01:53.000]   And that's the reason why we don't want our models
[01:01:53.000 --> 01:01:55.000]   to be very overconfident.
[01:01:55.000 --> 01:01:57.000]   So I did a sort of...
[01:01:57.000 --> 01:02:01.000]   I did take a bit of a sidetrack while explaining all of this idea to you.
[01:02:01.000 --> 01:02:06.000]   But the main idea is then when we're doing mix-up, right?
[01:02:06.000 --> 01:02:10.000]   So when I'm doing mix-up, my input labels are never 100.
[01:02:10.000 --> 01:02:14.000]   They become...input labels become something like this, right?
[01:02:14.000 --> 01:02:19.000]   So in this case, the model doesn't really become very overconfident.
[01:02:19.000 --> 01:02:22.000]   Like, that's the main idea or that's the main thing.
[01:02:22.000 --> 01:02:27.000]   So when you read this part, if you can see this section over here,
[01:02:27.000 --> 01:02:31.000]   this means that training pushes our models ever close to the values.
[01:02:31.000 --> 01:02:36.000]   And we...like, our activations basically become very extreme.
[01:02:36.000 --> 01:02:40.000]   With mix-up, we no longer have this problem because the labels are mixed.
[01:02:40.000 --> 01:02:42.000]   They're never going to be like 0 and 1.
[01:02:42.000 --> 01:02:45.000]   They're going to be something like 0.7 and 0.3.
[01:02:45.000 --> 01:02:49.000]   So I hope this kind of helps, this idea of the model becoming
[01:02:49.000 --> 01:02:51.000]   really overconfident help.
[01:02:51.000 --> 01:02:56.000]   And that's just an added advantage of mix-up.
[01:02:56.000 --> 01:02:58.000]   So that's mix-up, guys.
[01:02:58.000 --> 01:03:01.000]   If there's any questions about mix-up.
[01:03:01.000 --> 01:03:12.000]   Okay.
[01:03:12.000 --> 01:03:13.000]   This is a good question.
[01:03:13.000 --> 01:03:16.000]   Like, how does the mix-up loss change?
[01:03:16.000 --> 01:03:22.000]   So what you could do is in mix-up, instead of, like, having a BCE loss,
[01:03:22.000 --> 01:03:25.000]   you could just say 30% cross-entropy loss.
[01:03:25.000 --> 01:03:31.000]   So you could, like...actually, let me show you in code.
[01:03:31.000 --> 01:03:35.000]   Don't kill me for showing you too much code today, Ravi.
[01:03:35.000 --> 01:03:40.000]   But let's see some code.
[01:03:40.000 --> 01:03:43.000]   Here it is, Facebook Research CIFAR 10.
[01:03:43.000 --> 01:03:48.000]   And then if you go to train.py...in fact, I asked myself the exact same question.
[01:03:48.000 --> 01:03:52.000]   And I just want to show you on how I kind of found the answer.
[01:03:52.000 --> 01:03:53.000]   So I looked at this.
[01:03:53.000 --> 01:03:55.000]   I saw, okay, how does mix-up data work?
[01:03:55.000 --> 01:03:57.000]   This is how mix-up data works.
[01:03:57.000 --> 01:03:59.000]   You have a batch size.
[01:03:59.000 --> 01:04:03.000]   Then you get some number, like 0.3, 0.4, whatever that number looks like.
[01:04:03.000 --> 01:04:09.000]   Then you mix your X, which looks like that number, 0.3 times the first image.
[01:04:09.000 --> 01:04:11.000]   And then 0.7 times my second image.
[01:04:11.000 --> 01:04:13.000]   And then you mix your Ys, which is your label.
[01:04:13.000 --> 01:04:18.000]   So you get your mixed X, and you get your basically YA and YB.
[01:04:18.000 --> 01:04:26.000]   But then your mix-up criterion is 0.3 times cross-entropy loss between the first image.
[01:04:26.000 --> 01:04:30.000]   And then 0.7 times the cross-entropy loss between the second image.
[01:04:30.000 --> 01:04:36.000]   So instead of converting that to be binary cross-entropy, I guess what I'm trying to say is
[01:04:36.000 --> 01:04:43.000]   it could just be 30% cross-entropy loss between basically my predictions and YA,
[01:04:43.000 --> 01:04:44.000]   which is my first label.
[01:04:44.000 --> 01:04:50.000]   And then it could just be -- sorry, let me try and explain that.
[01:04:50.000 --> 01:04:52.000]   Where am I?
[01:04:52.000 --> 01:05:01.000]   Mix-up is -- this is not mix-up.
[01:05:01.000 --> 01:05:03.000]   This is not mix-up.
[01:05:03.000 --> 01:05:05.000]   Where is mix-up?
[01:05:05.000 --> 01:05:07.000]   Is it this one?
[01:05:07.000 --> 01:05:09.000]   Yes, here it is.
[01:05:09.000 --> 01:05:11.000]   So this is my labels, right?
[01:05:11.000 --> 01:05:15.000]   So let me do it again.
[01:05:15.000 --> 01:05:19.000]   So this time I have my model.
[01:05:19.000 --> 01:05:21.000]   I have my mix-up image.
[01:05:21.000 --> 01:05:29.000]   So this is my mix-up image, which is 30% cat and 70% dog.
[01:05:29.000 --> 01:05:35.000]   And then my cat label looks like 100, and my dog label looks like 001, right?
[01:05:35.000 --> 01:05:41.000]   So I could still -- you parse in this image, you get some predictions, right?
[01:05:41.000 --> 01:05:42.000]   These are my preds.
[01:05:42.000 --> 01:05:47.000]   Then what I could do is I could take cross-entropy loss between this and that.
[01:05:47.000 --> 01:05:54.000]   So I could take the cross-entropy loss between preds and dog, and then I could take cross-entropy loss.
[01:05:54.000 --> 01:05:57.000]   I'm just going to call this CE, cross-entropy.
[01:05:57.000 --> 01:06:03.000]   And -- sorry, this is cat.
[01:06:03.000 --> 01:06:05.000]   And this is dog.
[01:06:05.000 --> 01:06:07.000]   So I'm again taking cross-entropy loss.
[01:06:07.000 --> 01:06:09.000]   And then you see what I'm trying to do.
[01:06:09.000 --> 01:06:17.000]   You could just say 30% cross-entropy loss of cat plus 70% cross-entropy loss of dog.
[01:06:17.000 --> 01:06:22.000]   So you don't really need to update, like go from cross-entropy to binary cross-entropy.
[01:06:22.000 --> 01:06:24.000]   You don't really need to update your loss function.
[01:06:24.000 --> 01:06:31.000]   So you can just say, okay, 30% times the first label, and then 70% times the second label.
[01:06:31.000 --> 01:06:36.000]   So that's the official implementation from Facebook.
[01:06:36.000 --> 01:06:42.000]   I did kind of ask myself the same question on how it would look like if we updated the labels instead.
[01:06:42.000 --> 01:06:50.000]   I think it's something I do want to do some more research on, but this would make another wonderful, wonderful blog post.
[01:06:50.000 --> 01:06:57.000]   Maybe have a look at how the loss -- like does the model perform better if you try binary cross-entropy loss instead?
[01:06:57.000 --> 01:07:01.000]   And that would make a good experiment.
[01:07:01.000 --> 01:07:03.000]   So that's an idea for a blog post.
[01:07:03.000 --> 01:07:06.000]   Thanks for the great question, Ravi.
[01:07:06.000 --> 01:07:10.000]   Would techniques like mixup and cutmix help in other domains like medical image?
[01:07:10.000 --> 01:07:11.000]   Okay.
[01:07:11.000 --> 01:07:18.000]   Something I want to say again.
[01:07:18.000 --> 01:07:22.000]   Okay.
[01:07:22.000 --> 01:07:23.000]   Here's the answer.
[01:07:23.000 --> 01:07:25.000]   Try it yourself and see.
[01:07:25.000 --> 01:07:27.000]   That's something a part of the Fast.ai book.
[01:07:27.000 --> 01:07:31.000]   And I don't want to be rude by saying, oh, you asked me a question, I want to say try it yourself.
[01:07:31.000 --> 01:07:34.000]   But I guess I want to say something more general.
[01:07:34.000 --> 01:07:48.000]   Like I've seen cutmix and mixup work in medical images for some parts of medical images, and I've seen them not work for some other -- maybe CD scans or some other basically data from a different institution.
[01:07:48.000 --> 01:07:50.000]   But that's just been my example.
[01:07:50.000 --> 01:07:56.000]   If I would say, oh, mixup and cutmix would work in medical image, I don't think I can say that generally.
[01:07:56.000 --> 01:07:59.000]   So I guess the idea is if you have a question, try it.
[01:07:59.000 --> 01:08:02.000]   And then you'll know and you'll have a better answer for that question.
[01:08:02.000 --> 01:08:05.000]   So the first thing should be experiment with it.
[01:08:05.000 --> 01:08:07.000]   For example, like this one.
[01:08:07.000 --> 01:08:10.000]   Even I'm 100% not sure why this won't work.
[01:08:10.000 --> 01:08:14.000]   So I want to try and experiment and then I can have better insights.
[01:08:14.000 --> 01:08:17.000]   But I guess then the answer is just to try it.
[01:08:17.000 --> 01:08:21.000]   Oh, next thing I want to cover off then, okay, so that's mixup.
[01:08:21.000 --> 01:08:28.000]   So we've already looked at ImageNet, we've looked at normalization, progressive resizing, test time augmentation, and mixup so far.
[01:08:28.000 --> 01:08:31.000]   The next thing is just label smoothing.
[01:08:31.000 --> 01:08:36.000]   So label smoothing is really this simple idea.
[01:08:36.000 --> 01:08:45.000]   I mean, I've already explained everything that goes on in label smoothing in this section here.
[01:08:45.000 --> 01:08:47.000]   Not this section.
[01:08:47.000 --> 01:08:49.000]   In this section, not that section.
[01:08:49.000 --> 01:08:51.000]   Okay.
[01:08:51.000 --> 01:08:53.000]   Ah, here it is.
[01:08:53.000 --> 01:08:58.000]   So I've already explained everything you need to know about label smoothing already.
[01:08:58.000 --> 01:09:08.000]   Like, if my inputs are 1, 0, and 0, and then what the model is going to do is it's going to be very overconfident.
[01:09:08.000 --> 01:09:13.000]   Or it's going to predict one number to be really high and then the other numbers to be really low.
[01:09:13.000 --> 01:09:15.000]   Right? That's what's going to happen.
[01:09:15.000 --> 01:09:19.000]   And what we want to do is we want to go away from that.
[01:09:19.000 --> 01:09:24.000]   We want to go away from our models becoming overly confident of themselves.
[01:09:24.000 --> 01:09:35.000]   And then simplest idea is, okay, instead of that being 1, 0, 0, just make it 0.9, 0.05, 0.05.
[01:09:35.000 --> 01:09:40.000]   So now the model is always predicting, like you update all your labels.
[01:09:40.000 --> 01:09:44.000]   Any label, you just make it like, you reduce it by some number.
[01:09:44.000 --> 01:09:51.000]   So now your model is always going to say, okay, it's 0.9% or like 90% that particular label and then it's 5% the other labels.
[01:09:51.000 --> 01:09:54.000]   So this way your model never becomes overly confident.
[01:09:54.000 --> 01:09:59.000]   That's the basic idea behind label smoothing.
[01:09:59.000 --> 01:10:07.000]   But it's the first time I can now start sharing my blogs with everybody.
[01:10:07.000 --> 01:10:11.000]   So I'm going to share this with you.
[01:10:11.000 --> 01:10:13.000]   Have a look at this.
[01:10:13.000 --> 01:10:16.000]   I've just passed in the blog example.
[01:10:16.000 --> 01:10:22.000]   But have a look at this and you'll see I've explained label smoothing in much more detail using Microsoft Excel.
[01:10:22.000 --> 01:10:29.000]   So you'll see if I have my input as these particular labels and you first one-hot encode them.
[01:10:29.000 --> 01:10:35.000]   So this is how my -- for each of my images, this is how my -- basically my labels look like.
[01:10:35.000 --> 01:10:38.000]   And then -- sorry, I keep doing that.
[01:10:38.000 --> 01:10:41.000]   And then in label smoothing, you just update all your labels like this.
[01:10:41.000 --> 01:10:43.000]   I'll show you the images.
[01:10:43.000 --> 01:10:49.000]   But then labels instead of being like 1, 0, 0, 0, they just become something like 0.92 and then 0.02 all the way.
[01:10:49.000 --> 01:10:53.000]   So if you want to look at more math or some experiments in Microsoft Excel, this is the blog.
[01:10:53.000 --> 01:11:00.000]   And then I'll show you how the differences in loss function or all that stuff in more detail in that blog post.
[01:11:00.000 --> 01:11:03.000]   So do have a look at that.
[01:11:03.000 --> 01:11:05.000]   So that's label smoothing.
[01:11:05.000 --> 01:11:10.000]   I guess there wasn't a lot of stuff in this chapter.
[01:11:10.000 --> 01:11:15.000]   Like we had covered most of the ground about computer vision before this.
[01:11:15.000 --> 01:11:22.000]   But in this chapter, I really like this chapter for the fact that we're now going deeper and deeper into computer vision.
[01:11:22.000 --> 01:11:24.000]   Like by this stage, you're ready to start reading papers.
[01:11:24.000 --> 01:11:32.000]   By this stage, you're ready to start reading more complex blog posts or you're ready to start looking at some sections of the paper.
[01:11:32.000 --> 01:11:37.000]   So there's this sidebar in this chapter which says label smoothing.
[01:11:37.000 --> 01:11:43.000]   Oh, that's not being rendered nicely, is it?
[01:11:43.000 --> 01:11:45.000]   Okay. It's fine now.
[01:11:45.000 --> 01:11:47.000]   So you'll see this section of the paper.
[01:11:47.000 --> 01:11:55.000]   When you read this label smoothing paper, you will see like this maximum is not achievable, but for finance ZK and then there's all this math.
[01:11:55.000 --> 01:11:59.000]   And then you'll see, okay, this, however, can cause two problems.
[01:11:59.000 --> 01:12:01.000]   First, it may result in overfitting.
[01:12:01.000 --> 01:12:07.000]   If the model learns to assign full probability to the ground truth label, it is not guaranteed to generalize.
[01:12:07.000 --> 01:12:11.000]   And then you'll see the second idea of the largest logit.
[01:12:11.000 --> 01:12:14.000]   And then you'll see this happens because the model becomes too confident.
[01:12:14.000 --> 01:12:19.000]   Like this is what's written in math or like in the research paper using some math.
[01:12:19.000 --> 01:12:26.000]   And then what this book does is it makes all of that in a simple language.
[01:12:26.000 --> 01:12:34.000]   So you know how I've explained this idea of the model becoming overly confident when my prediction is one over here and zero otherwise.
[01:12:34.000 --> 01:12:41.000]   And then the model is going to try and make that number corresponding to that label one really large and the other numbers really slow.
[01:12:41.000 --> 01:12:51.000]   That's just what's been written and then been explained in this part of the -- that's been explained in this part of the book.
[01:12:51.000 --> 01:13:02.000]   So I think it would make sense to really start playing around with some research papers and start reading some research papers, especially the ones around mixup.
[01:13:02.000 --> 01:13:04.000]   Have a look at the mixup research paper.
[01:13:04.000 --> 01:13:06.000]   So there's this link over here.
[01:13:06.000 --> 01:13:08.000]   Go there, have a read about mixup.
[01:13:08.000 --> 01:13:10.000]   And then it would make a really good plot for us.
[01:13:10.000 --> 01:13:16.000]   So something I've been -- this is something I've really been doing personally is like I read research papers and then I summarize them.
[01:13:16.000 --> 01:13:20.000]   And that's what landed me a job at Weights and Biases in a way.
[01:13:20.000 --> 01:13:29.000]   And I think it's not a bad idea at all to understand the paper and then be able to summarize it and be able to explain it in a simple language.
[01:13:29.000 --> 01:13:31.000]   So that's something you should definitely, definitely try.
[01:13:31.000 --> 01:13:44.000]   So I think as part of this week's work, try all of these experiments of progressive resizing, test time augmentation, mixup, and label smoothing to Cassava and see if your model now generalizes well.
[01:13:44.000 --> 01:13:50.000]   So with mixup, one thing I should point out is that you want your number of epochs to be higher.
[01:13:50.000 --> 01:13:51.000]   Why is that?
[01:13:51.000 --> 01:13:57.000]   Because now your model is not seeing like cats and dogs straightaway, but it's seeing like 30% cat and 70% dog.
[01:13:57.000 --> 01:14:00.000]   So your model needs more epochs to be able to train.
[01:14:00.000 --> 01:14:07.000]   And it's never going to overfit because you never show it like an image of the cat or the same image every time.
[01:14:07.000 --> 01:14:11.000]   So that's -- there's not -- overfitting is not going to be a problem with mixup.
[01:14:11.000 --> 01:14:13.000]   But have a read of the paper.
[01:14:13.000 --> 01:14:17.000]   Try and summarize the mixup paper in your own words.
[01:14:17.000 --> 01:14:23.000]   Show experiments with Fast.ai using the Fast.ai library on mixup.
[01:14:23.000 --> 01:14:25.000]   And then look at label smoothing.
[01:14:25.000 --> 01:14:28.000]   So in fact, I wrote about this label smoothing -- when was that?
[01:14:28.000 --> 01:14:32.000]   When I first started reading around this time in 2020.
[01:14:32.000 --> 01:14:35.000]   So that's when I started reading papers myself in early 2020s.
[01:14:35.000 --> 01:14:38.000]   And I've done the exact same thing.
[01:14:38.000 --> 01:14:40.000]   I was following the Fast.ai course.
[01:14:40.000 --> 01:14:42.000]   I looked at this label smoothing.
[01:14:42.000 --> 01:14:46.000]   I said, okay, I'm going to have a read of this paper, which is over here.
[01:14:46.000 --> 01:14:52.000]   And then I'm going to try and do something different, which is I'm going to try and explain that using Microsoft Excel.
[01:14:52.000 --> 01:14:57.000]   So these are things that really work to your advantage if you can explain things really well.
[01:14:57.000 --> 01:15:04.000]   So please, please, please read the papers and do experiments and then write a good blog post about it.
[01:15:04.000 --> 01:15:06.000]   Because it will be very helpful.
[01:15:06.000 --> 01:15:08.000]   So that's that for today.
[01:15:08.000 --> 01:15:19.000]   I'll just take questions if there are any.
[01:15:19.000 --> 01:15:21.000]   Looks like there's no questions.
[01:15:21.000 --> 01:15:23.000]   So that's going to be it for today, guys.
[01:15:23.000 --> 01:15:26.000]   I guess I will see you next week.
[01:15:26.000 --> 01:15:30.000]   And let's continue the conversation on Cassava on Slack.
[01:15:30.000 --> 01:15:36.000]   So feel free to ping me if you have an idea that you want to ask or feel free to keep that conversation going.
[01:15:36.000 --> 01:15:39.000]   I'll see you guys next week and thank you very much.
[01:15:40.000 --> 01:15:42.000]   [ End ]
[01:15:42.000 --> 01:15:44.000]   [ Session concluded ]
[01:15:44.000 --> 01:15:46.000]   [ End ]
[01:15:46.000 --> 01:15:48.000]   [ Session concluded ]
[01:15:48.000 --> 01:15:58.000]   [BLANK_AUDIO]

