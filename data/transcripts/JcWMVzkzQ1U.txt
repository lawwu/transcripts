
[00:00:00.000 --> 00:00:04.560]   - So let me ask, you've kind of alluded to it,
[00:00:04.560 --> 00:00:07.680]   but let me ask again, what is intelligence?
[00:00:07.680 --> 00:00:10.760]   Underlying the discussions we'll have
[00:00:10.760 --> 00:00:13.800]   with Jeopardy and beyond,
[00:00:13.800 --> 00:00:15.400]   how do you think about intelligence?
[00:00:15.400 --> 00:00:18.080]   Is it a sufficiently complicated problem,
[00:00:18.080 --> 00:00:20.760]   being able to reason your way through solving that problem?
[00:00:20.760 --> 00:00:22.100]   Is that kind of how you think about
[00:00:22.100 --> 00:00:23.720]   what it means to be intelligent?
[00:00:23.720 --> 00:00:27.960]   - So I think of intelligence primarily two ways.
[00:00:27.960 --> 00:00:31.560]   One is the ability to predict.
[00:00:31.560 --> 00:00:34.040]   So in other words, if I have a problem,
[00:00:34.040 --> 00:00:35.840]   can I predict what's gonna happen next?
[00:00:35.840 --> 00:00:39.120]   Whether it's to predict the answer of a question
[00:00:39.120 --> 00:00:42.120]   or to say, look, I'm looking at all the market dynamics
[00:00:42.120 --> 00:00:44.400]   and I'm gonna tell you what's gonna happen next,
[00:00:44.400 --> 00:00:47.600]   or you're in a room and somebody walks in
[00:00:47.600 --> 00:00:49.560]   and you're gonna predict what they're gonna do next
[00:00:49.560 --> 00:00:51.080]   or what they're gonna say next.
[00:00:51.080 --> 00:00:54.780]   - So in a highly dynamic environment full of uncertainty,
[00:00:54.780 --> 00:00:56.880]   be able to-- - Lots of, you know,
[00:00:56.880 --> 00:00:59.720]   the more variables, the more complex,
[00:00:59.720 --> 00:01:02.320]   the more possibilities, the more complex.
[00:01:02.320 --> 00:01:05.960]   But can I take a small amount of prior data
[00:01:05.960 --> 00:01:08.120]   and learn the pattern and then predict
[00:01:08.120 --> 00:01:11.240]   what's gonna happen next accurately and consistently?
[00:01:11.240 --> 00:01:15.160]   That's certainly a form of intelligence.
[00:01:15.160 --> 00:01:16.520]   - What do you need for that, by the way?
[00:01:16.520 --> 00:01:21.100]   You need to have an understanding of the way the world works
[00:01:21.100 --> 00:01:24.600]   in order to be able to unroll it into the future, right?
[00:01:24.600 --> 00:01:26.280]   What do you think is needed to predict--
[00:01:26.280 --> 00:01:27.760]   - Depends what you mean by understanding.
[00:01:27.760 --> 00:01:30.520]   I need to be able to find that function,
[00:01:30.520 --> 00:01:32.560]   and this is very much what-- - What's a function.
[00:01:32.560 --> 00:01:34.360]   - Deep learning does, machine learning does,
[00:01:34.360 --> 00:01:37.280]   is if you give me enough prior data
[00:01:37.280 --> 00:01:40.240]   and you tell me what the output variable is that matters,
[00:01:40.240 --> 00:01:42.760]   I'm gonna sit there and be able to predict it.
[00:01:42.760 --> 00:01:45.560]   And if I can predict it accurately
[00:01:45.560 --> 00:01:49.240]   so that I can get it right more often than not, I'm smart.
[00:01:49.240 --> 00:01:53.080]   If I can do that with less data and less training time,
[00:01:53.080 --> 00:01:54.300]   I'm even smarter.
[00:01:55.300 --> 00:01:58.880]   If I can figure out what's even worth predicting,
[00:01:58.880 --> 00:02:02.140]   I'm smarter, meaning I'm figuring out
[00:02:02.140 --> 00:02:04.620]   what path is gonna get me toward a goal.
[00:02:04.620 --> 00:02:05.820]   - What about picking a goal?
[00:02:05.820 --> 00:02:06.740]   Sorry to interrupt again.
[00:02:06.740 --> 00:02:07.580]   - Well, that's interesting.
[00:02:07.580 --> 00:02:09.300]   About picking a goal is sort of an interesting thing,
[00:02:09.300 --> 00:02:11.460]   and I think that's where you bring in
[00:02:11.460 --> 00:02:13.260]   what are you pre-programmed to do?
[00:02:13.260 --> 00:02:16.300]   We talk about humans, and humans are pre-programmed
[00:02:16.300 --> 00:02:21.300]   to survive, so sort of their primary driving goal.
[00:02:21.300 --> 00:02:22.940]   What do they have to do to do that?
[00:02:22.940 --> 00:02:25.620]   And that can be very complex, right?
[00:02:25.620 --> 00:02:29.940]   So it's not just figuring out that you need to run away
[00:02:29.940 --> 00:02:31.900]   from the ferocious tiger,
[00:02:31.900 --> 00:02:36.900]   but we survive in a social context as an example.
[00:02:36.900 --> 00:02:40.580]   So understanding the subtleties of social dynamics
[00:02:40.580 --> 00:02:43.700]   becomes something that's important for surviving,
[00:02:43.700 --> 00:02:45.460]   finding a mate, reproducing, right?
[00:02:45.460 --> 00:02:47.620]   So we're continually challenged
[00:02:47.620 --> 00:02:52.020]   with complex sets of variables, complex constraints,
[00:02:52.020 --> 00:02:55.140]   rules, if you will, or patterns,
[00:02:55.140 --> 00:02:57.580]   and we learn how to find the functions
[00:02:57.580 --> 00:02:59.420]   and predict the things, in other words,
[00:02:59.420 --> 00:03:01.820]   represent those patterns efficiently,
[00:03:01.820 --> 00:03:03.180]   and be able to predict what's gonna happen,
[00:03:03.180 --> 00:03:04.340]   and that's a form of intelligence.
[00:03:04.340 --> 00:03:09.340]   That doesn't really require anything specific
[00:03:09.340 --> 00:03:11.660]   other than the ability to find that function
[00:03:11.660 --> 00:03:14.100]   and predict that right answer.
[00:03:14.100 --> 00:03:16.700]   It's certainly a form of intelligence.
[00:03:16.700 --> 00:03:21.580]   But then when we say, well, do we understand each other?
[00:03:21.580 --> 00:03:26.580]   In other words, would you perceive me as intelligent
[00:03:26.580 --> 00:03:29.300]   beyond that ability to predict?
[00:03:29.300 --> 00:03:33.500]   So now I can predict, but I can't really articulate
[00:03:33.500 --> 00:03:36.100]   how I'm going through that process,
[00:03:36.100 --> 00:03:39.500]   what my underlying theory is for predicting,
[00:03:39.500 --> 00:03:41.940]   and I can't get you to understand what I'm doing
[00:03:41.940 --> 00:03:43.940]   so that you can follow,
[00:03:43.940 --> 00:03:46.340]   you can figure out how to do this yourself
[00:03:46.340 --> 00:03:49.060]   if you did not have, for example,
[00:03:49.060 --> 00:03:52.100]   the right pattern-managing machinery that I did.
[00:03:52.100 --> 00:03:54.020]   And now we potentially have this breakdown
[00:03:54.020 --> 00:03:57.380]   where, in effect, I'm intelligent,
[00:03:57.380 --> 00:04:00.940]   but I'm sort of an alien intelligence relative to you.
[00:04:00.940 --> 00:04:03.780]   - You're intelligent, but nobody knows about it.
[00:04:03.780 --> 00:04:06.940]   - Well, I can see the output.
[00:04:06.940 --> 00:04:09.980]   - So you're saying, let's sort of separate the two things.
[00:04:09.980 --> 00:04:14.180]   One is you explaining why you were able
[00:04:14.180 --> 00:04:15.540]   to predict the future,
[00:04:17.460 --> 00:04:21.660]   and the second is me being able to,
[00:04:21.660 --> 00:04:23.820]   impressing me that you're intelligent,
[00:04:23.820 --> 00:04:25.580]   me being able to know that you successfully
[00:04:25.580 --> 00:04:26.940]   predicted the future.
[00:04:26.940 --> 00:04:27.900]   Do you think that's--
[00:04:27.900 --> 00:04:29.660]   - Well, it's not impressing you that I'm intelligent.
[00:04:29.660 --> 00:04:31.940]   In other words, you may be convinced
[00:04:31.940 --> 00:04:34.260]   that I'm intelligent in some form.
[00:04:34.260 --> 00:04:35.460]   - So how, what would convince--
[00:04:35.460 --> 00:04:37.140]   - Because of my ability to predict.
[00:04:37.140 --> 00:04:37.980]   - So I would look at the metrics.
[00:04:37.980 --> 00:04:40.380]   - When you can't, I say, wow, you're right all,
[00:04:40.380 --> 00:04:43.260]   you're right more times than I am.
[00:04:43.260 --> 00:04:44.580]   You're doing something interesting.
[00:04:44.580 --> 00:04:47.420]   That's a form of intelligence.
[00:04:47.420 --> 00:04:51.700]   But then what happens is, if I say, how are you doing that?
[00:04:51.700 --> 00:04:53.580]   And you can't communicate with me,
[00:04:53.580 --> 00:04:56.020]   and you can't describe that to me,
[00:04:56.020 --> 00:04:59.020]   now I may label you a savant.
[00:04:59.020 --> 00:05:01.540]   I may say, well, you're doing something weird,
[00:05:01.540 --> 00:05:04.660]   and it's just not very interesting to me,
[00:05:04.660 --> 00:05:07.660]   because you and I can't really communicate.
[00:05:07.660 --> 00:05:10.660]   And so now, so this is interesting, right?
[00:05:10.660 --> 00:05:13.420]   Because now this is, you're in this weird place
[00:05:13.420 --> 00:05:17.620]   where for you to be recognized as intelligent
[00:05:17.620 --> 00:05:20.300]   the way I'm intelligent, then you and I
[00:05:20.300 --> 00:05:22.580]   sort of have to be able to communicate.
[00:05:22.580 --> 00:05:26.820]   And then my, we start to understand each other,
[00:05:26.820 --> 00:05:31.780]   and then my respect and my appreciation,
[00:05:31.780 --> 00:05:35.060]   my ability to relate to you starts to change.
[00:05:35.060 --> 00:05:37.380]   So now you're not an alien intelligence anymore.
[00:05:37.380 --> 00:05:39.340]   You're a human intelligence now,
[00:05:39.340 --> 00:05:42.180]   because you and I can communicate.
[00:05:42.180 --> 00:05:46.380]   And so I think when we look at animals, for example,
[00:05:46.380 --> 00:05:48.980]   animals can do things we can't quite comprehend,
[00:05:48.980 --> 00:05:50.060]   we don't quite know how they do them,
[00:05:50.060 --> 00:05:52.700]   but they can't really communicate with us.
[00:05:52.700 --> 00:05:56.620]   They can't put what they're going through in our terms.
[00:05:56.620 --> 00:05:57.980]   And so we think of them as sort of,
[00:05:57.980 --> 00:05:59.780]   well, they're these alien intelligences,
[00:05:59.780 --> 00:06:01.860]   and they're not really worth necessarily what we're worth.
[00:06:01.860 --> 00:06:04.620]   We don't treat them the same way as a result of that.
[00:06:04.620 --> 00:06:09.620]   But it's hard, because who knows what's going on.
[00:06:09.900 --> 00:06:13.900]   - So just a quick elaboration on that.
[00:06:13.900 --> 00:06:16.220]   The explaining that you're intelligent,
[00:06:16.220 --> 00:06:20.540]   explaining the reasoning that went into the prediction
[00:06:20.540 --> 00:06:25.340]   is not some kind of mathematical proof.
[00:06:25.340 --> 00:06:28.500]   If we look at humans, look at political debates
[00:06:28.500 --> 00:06:33.500]   and discourse on Twitter, it's mostly just telling stories.
[00:06:33.500 --> 00:06:38.600]   So your task is, sorry, your task is not to tell
[00:06:39.340 --> 00:06:43.420]   an accurate depiction of how you reason,
[00:06:43.420 --> 00:06:46.700]   but to tell a story, real or not,
[00:06:46.700 --> 00:06:49.380]   that convinces me that there was a mechanism
[00:06:49.380 --> 00:06:50.220]   by which you--
[00:06:50.220 --> 00:06:51.900]   - Ultimately, that's what a proof is.
[00:06:51.900 --> 00:06:54.500]   I mean, even a mathematical proof is that.
[00:06:54.500 --> 00:06:56.500]   Because ultimately, the other mathematicians
[00:06:56.500 --> 00:06:58.300]   have to be convinced by your proof.
[00:06:58.300 --> 00:07:01.260]   Otherwise, in fact, there have been--
[00:07:01.260 --> 00:07:02.740]   - That's the metric of success, yeah.
[00:07:02.740 --> 00:07:04.340]   - Yeah, there have been several proofs out there
[00:07:04.340 --> 00:07:06.300]   where mathematicians would study for a long time
[00:07:06.300 --> 00:07:07.140]   before they were convinced
[00:07:07.140 --> 00:07:08.860]   that it actually proved anything.
[00:07:08.860 --> 00:07:10.500]   Right, you never know if it proved anything
[00:07:10.500 --> 00:07:13.140]   until the community mathematicians decided that it did.
[00:07:13.140 --> 00:07:16.940]   So I mean, but it's a real thing.
[00:07:16.940 --> 00:07:19.220]   And that's sort of the point, right,
[00:07:19.220 --> 00:07:22.740]   is that ultimately, this notion of understanding,
[00:07:22.740 --> 00:07:26.500]   us understanding something is ultimately a social concept.
[00:07:26.500 --> 00:07:29.000]   In other words, I have to convince enough people
[00:07:29.000 --> 00:07:32.020]   that I did this in a reasonable way.
[00:07:32.020 --> 00:07:34.660]   I did this in a way that other people can understand
[00:07:34.660 --> 00:07:38.100]   and replicate and that it makes sense to them.
[00:07:38.100 --> 00:07:43.100]   So human intelligence is bound together in that way.
[00:07:43.100 --> 00:07:45.740]   We're bound up in that sense.
[00:07:45.740 --> 00:07:47.820]   We sort of never really get away with it
[00:07:47.820 --> 00:07:50.860]   until we can sort of convince others
[00:07:50.860 --> 00:07:54.140]   that our thinking process makes sense.
[00:07:54.140 --> 00:07:57.380]   - Did you think the general question of intelligence
[00:07:57.380 --> 00:07:59.260]   is then also a social construct?
[00:07:59.260 --> 00:08:04.260]   So if we ask questions of an artificial intelligence system,
[00:08:04.260 --> 00:08:06.900]   is this system intelligent?
[00:08:06.900 --> 00:08:10.900]   The answer will ultimately be a socially constructed--
[00:08:10.900 --> 00:08:14.300]   - I think, so I think, I'm making two statements.
[00:08:14.300 --> 00:08:16.260]   I'm saying we can try to define intelligence
[00:08:16.260 --> 00:08:21.260]   in this super objective way that says, here's this data.
[00:08:21.260 --> 00:08:24.020]   I wanna predict this type of thing.
[00:08:24.020 --> 00:08:27.100]   Learn this function, and then if you get it right,
[00:08:27.100 --> 00:08:30.340]   often enough, we consider you intelligent.
[00:08:30.340 --> 00:08:32.700]   - But that's more like a savant.
[00:08:32.700 --> 00:08:34.020]   - I think it is.
[00:08:34.020 --> 00:08:35.860]   It doesn't mean it's not useful.
[00:08:35.860 --> 00:08:36.940]   It could be incredibly useful.
[00:08:36.940 --> 00:08:39.780]   It could be solving a problem we can't otherwise solve
[00:08:39.780 --> 00:08:42.820]   and can solve it more reliably than we can.
[00:08:42.820 --> 00:08:45.260]   But then there's this notion of,
[00:08:45.260 --> 00:08:48.740]   can humans take responsibility
[00:08:48.740 --> 00:08:51.980]   for the decision that you're making?
[00:08:51.980 --> 00:08:54.420]   Can we make those decisions ourselves?
[00:08:54.420 --> 00:08:57.140]   Can we relate to the process that you're going through?
[00:08:57.140 --> 00:08:59.460]   And now, you as an agent,
[00:08:59.460 --> 00:09:02.840]   whether you're a machine or another human, frankly,
[00:09:02.840 --> 00:09:06.960]   are now obliged to make me understand
[00:09:06.960 --> 00:09:09.180]   how it is that you're arriving at that answer
[00:09:09.180 --> 00:09:12.180]   and allow me, me or obviously a community
[00:09:12.180 --> 00:09:15.100]   or a judge of people to decide whether or not
[00:09:15.100 --> 00:09:15.940]   that makes sense.
[00:09:15.940 --> 00:09:18.520]   And by the way, that happens with humans as well.
[00:09:18.520 --> 00:09:20.360]   You're sitting down with your staff, for example,
[00:09:20.360 --> 00:09:23.840]   and you ask for suggestions about what to do next,
[00:09:23.840 --> 00:09:26.880]   and someone says, "Oh, I think you should buy,
[00:09:26.880 --> 00:09:28.880]   "and I think you should buy this much,"
[00:09:28.880 --> 00:09:31.440]   or whatever, or sell, or whatever it is,
[00:09:31.440 --> 00:09:34.000]   or I think you should launch the product today or tomorrow
[00:09:34.000 --> 00:09:35.360]   or launch this product versus that product,
[00:09:35.360 --> 00:09:38.120]   whatever the decision may be, and you ask why,
[00:09:38.120 --> 00:09:41.080]   and the person says, "I just have a good feeling about it."
[00:09:41.080 --> 00:09:42.680]   And you're not very satisfied.
[00:09:42.680 --> 00:09:46.560]   Now, that person could be, you might say,
[00:09:46.560 --> 00:09:49.080]   "Well, you've been right before,
[00:09:49.080 --> 00:09:52.340]   "but I'm gonna put the company on the line.
[00:09:52.340 --> 00:09:55.120]   "Can you explain to me why I should believe this?"
[00:09:55.120 --> 00:09:58.280]   - And that explanation may have nothing to do
[00:09:58.280 --> 00:09:59.120]   with the truth.
[00:10:00.080 --> 00:10:01.760]   It's how to convince the other person.
[00:10:01.760 --> 00:10:03.600]   It could still be wrong.
[00:10:03.600 --> 00:10:04.600]   - It's just gotta be convincing.
[00:10:04.600 --> 00:10:06.120]   - But it's ultimately gotta be convincing.
[00:10:06.120 --> 00:10:10.440]   And that's why I'm saying we're bound together.
[00:10:10.440 --> 00:10:12.440]   Our intelligences are bound together in that sense.
[00:10:12.440 --> 00:10:13.640]   We have to understand each other.
[00:10:13.640 --> 00:10:17.160]   And if, for example, you're giving me an explanation,
[00:10:17.160 --> 00:10:19.280]   and this is a very important point,
[00:10:19.280 --> 00:10:21.340]   you're giving me an explanation,
[00:10:21.340 --> 00:10:26.540]   and I'm not good,
[00:10:27.640 --> 00:10:31.800]   and then I'm not good at reasoning well
[00:10:31.800 --> 00:10:36.320]   and being objective and following logical paths
[00:10:36.320 --> 00:10:37.440]   and consistent paths,
[00:10:37.440 --> 00:10:39.680]   and I'm not good at measuring
[00:10:39.680 --> 00:10:43.800]   and sort of computing probabilities across those paths,
[00:10:43.800 --> 00:10:45.480]   what happens is collectively,
[00:10:45.480 --> 00:10:48.400]   we're not gonna do well.
[00:10:48.400 --> 00:10:51.440]   - How hard is that problem, the second one?
[00:10:51.440 --> 00:10:56.240]   So I think we'll talk quite a bit about the first
[00:10:56.240 --> 00:11:01.240]   on a specific objective metric benchmark performing well.
[00:11:01.240 --> 00:11:07.120]   But being able to explain the steps, the reasoning,
[00:11:07.120 --> 00:11:08.840]   how hard is that problem?
[00:11:08.840 --> 00:11:10.080]   - I think that's very hard.
[00:11:10.080 --> 00:11:11.560]   I mean, I think that that's,
[00:11:11.560 --> 00:11:16.440]   well, it's hard for humans.
[00:11:16.440 --> 00:11:19.240]   - The thing that's hard for humans, as you know,
[00:11:19.240 --> 00:11:21.200]   may not necessarily be hard for computers
[00:11:21.200 --> 00:11:22.720]   and vice versa.
[00:11:22.720 --> 00:11:27.720]   So, sorry, so how hard is that problem for computers?
[00:11:27.720 --> 00:11:30.920]   - I think it's hard for computers,
[00:11:30.920 --> 00:11:32.880]   and the reason why I related to,
[00:11:32.880 --> 00:11:34.700]   or saying that it's also hard for humans
[00:11:34.700 --> 00:11:36.620]   is because I think when we step back
[00:11:36.620 --> 00:11:40.220]   and we say we wanna design computers to do that,
[00:11:40.220 --> 00:11:44.760]   one of the things we have to recognize
[00:11:44.760 --> 00:11:48.800]   is we're not sure how to do it well.
[00:11:48.800 --> 00:11:51.220]   I'm not sure we have a recipe for that,
[00:11:51.220 --> 00:11:53.600]   and even if you wanted to learn it,
[00:11:53.600 --> 00:11:56.680]   it's not clear exactly what data we use
[00:11:56.680 --> 00:12:01.980]   and what judgments we use to learn that well.
[00:12:01.980 --> 00:12:03.720]   And so what I mean by that is,
[00:12:03.720 --> 00:12:07.760]   if you look at the entire enterprise of science,
[00:12:07.760 --> 00:12:11.960]   science is supposed to be about objective reason, right?
[00:12:11.960 --> 00:12:15.940]   So we think about, gee, who's the most intelligent person
[00:12:15.940 --> 00:12:18.780]   or group of people in the world?
[00:12:18.780 --> 00:12:22.320]   Do we think about the savants who can close their eyes
[00:12:22.320 --> 00:12:23.800]   and give you a number?
[00:12:23.800 --> 00:12:25.960]   We think about the think tanks,
[00:12:25.960 --> 00:12:27.760]   or the scientists or the philosophers
[00:12:27.760 --> 00:12:30.960]   who kind of work through the details
[00:12:30.960 --> 00:12:32.560]   and write the papers and come up
[00:12:32.560 --> 00:12:35.360]   with the thoughtful, logical proofs
[00:12:35.360 --> 00:12:36.880]   and use the scientific method,
[00:12:36.880 --> 00:12:38.940]   and I think it's the latter.
[00:12:38.940 --> 00:12:44.040]   And my point is that, how do you train someone to do that?
[00:12:44.040 --> 00:12:45.880]   And that's what I mean by it's hard.
[00:12:45.880 --> 00:12:49.060]   What's the process of training people to do that well?
[00:12:49.060 --> 00:12:50.660]   That's a hard process.
[00:12:50.660 --> 00:12:54.300]   We work, as a society, we work pretty hard
[00:12:54.300 --> 00:12:57.520]   to get other people to understand our thinking
[00:12:57.520 --> 00:13:00.500]   and to convince them of things.
[00:13:00.500 --> 00:13:02.300]   Now we could persuade them,
[00:13:02.300 --> 00:13:03.580]   obviously we talked about this,
[00:13:03.580 --> 00:13:05.780]   like human flaws or weaknesses,
[00:13:05.780 --> 00:13:10.460]   we can persuade them through emotional means,
[00:13:10.460 --> 00:13:14.420]   but to get them to understand and connect to
[00:13:14.420 --> 00:13:18.240]   and follow a logical argument is difficult.
[00:13:18.240 --> 00:13:20.720]   We try it, we do it as scientists,
[00:13:20.720 --> 00:13:22.480]   we try to do it as journalists,
[00:13:22.480 --> 00:13:25.540]   we try to do it as even artists in many forms,
[00:13:25.540 --> 00:13:28.040]   as writers, as teachers.
[00:13:28.040 --> 00:13:31.200]   We go through a fairly significant training process
[00:13:31.200 --> 00:13:33.500]   to do that, and then we could ask,
[00:13:33.500 --> 00:13:36.180]   well, why is that so hard?
[00:13:36.180 --> 00:13:41.220]   But it's hard, and for humans, it takes a lot of work.
[00:13:41.220 --> 00:13:44.240]   And when we step back and say,
[00:13:44.240 --> 00:13:47.420]   well, how do we get a machine to do that?
[00:13:47.420 --> 00:13:48.900]   It's a vexing question.
[00:13:48.900 --> 00:13:53.540]   - How would you begin to try to solve that?
[00:13:53.540 --> 00:13:55.660]   And maybe just a quick pause,
[00:13:55.660 --> 00:13:58.100]   because there's an optimistic notion
[00:13:58.100 --> 00:13:59.300]   in the things you're describing,
[00:13:59.300 --> 00:14:03.360]   which is being able to explain something through reason.
[00:14:03.360 --> 00:14:06.920]   But if you look at algorithms that recommend things
[00:14:06.920 --> 00:14:08.060]   that we'll look at next,
[00:14:08.060 --> 00:14:10.060]   whether it's Facebook, Google,
[00:14:10.060 --> 00:14:11.720]   advertisement-based companies,
[00:14:12.860 --> 00:14:17.660]   you know, their goal is to convince you to buy things
[00:14:17.660 --> 00:14:20.660]   based on anything.
[00:14:20.660 --> 00:14:23.740]   So that could be reason,
[00:14:23.740 --> 00:14:25.460]   'cause the best of advertisement
[00:14:25.460 --> 00:14:27.900]   is showing you things that you really do need
[00:14:27.900 --> 00:14:30.300]   and explain why you need it.
[00:14:30.300 --> 00:14:33.960]   But it could also be through emotional manipulation.
[00:14:33.960 --> 00:14:39.100]   The algorithm that describes why a certain reason,
[00:14:39.100 --> 00:14:42.080]   a certain decision was made,
[00:14:42.080 --> 00:14:46.480]   how hard is it to do it through emotional manipulation?
[00:14:46.480 --> 00:14:50.220]   And why is that a good or a bad thing?
[00:14:50.220 --> 00:14:55.160]   So you've kind of focused on reason, logic,
[00:14:55.160 --> 00:14:59.780]   really showing in a clear way why something is good.
[00:14:59.780 --> 00:15:04.220]   One, is that even a thing that us humans do?
[00:15:04.220 --> 00:15:08.180]   And two, how do you think of the difference
[00:15:08.180 --> 00:15:11.700]   in the reasoning aspect and the emotional manipulation?
[00:15:11.700 --> 00:15:15.620]   - So you call it emotional manipulation,
[00:15:15.620 --> 00:15:18.460]   but more objectively, it's essentially saying,
[00:15:18.460 --> 00:15:20.920]   there are certain features of things
[00:15:20.920 --> 00:15:22.700]   that seem to attract your attention.
[00:15:22.700 --> 00:15:25.080]   I mean, it kind of give you more of that stuff.
[00:15:25.080 --> 00:15:26.540]   - Manipulation is a bad word.
[00:15:26.540 --> 00:15:29.400]   - Yeah, I mean, I'm not saying it's right or wrong.
[00:15:29.400 --> 00:15:31.240]   It works to get your attention,
[00:15:31.240 --> 00:15:32.700]   and it works to get you to buy stuff.
[00:15:32.700 --> 00:15:34.240]   And when you think about algorithms
[00:15:34.240 --> 00:15:38.280]   that look at the patterns of features
[00:15:38.280 --> 00:15:40.200]   that you seem to be spending your money on,
[00:15:40.200 --> 00:15:41.540]   and say, I'm gonna give you something
[00:15:41.540 --> 00:15:43.080]   with a similar pattern,
[00:15:43.080 --> 00:15:44.360]   so I'm gonna learn that function,
[00:15:44.360 --> 00:15:46.480]   because the objective is to get you to click on it
[00:15:46.480 --> 00:15:48.500]   or get you to buy it or whatever it is.
[00:15:48.500 --> 00:15:51.680]   I don't know, I mean, it is what it is.
[00:15:51.680 --> 00:15:54.120]   I mean, that's what the algorithm does.
[00:15:54.120 --> 00:15:55.720]   You can argue whether it's good or bad.
[00:15:55.720 --> 00:15:58.720]   It depends what your goal is.
[00:15:58.720 --> 00:16:02.440]   - I guess this seems to be very useful for convincing,
[00:16:02.440 --> 00:16:03.280]   for telling a story.
[00:16:03.280 --> 00:16:05.960]   - I think for convincing humans, it's good,
[00:16:05.960 --> 00:16:07.880]   because again, this goes back to,
[00:16:07.880 --> 00:16:10.360]   what is the human behavior like?
[00:16:10.360 --> 00:16:15.280]   How does the human brain respond to things?
[00:16:15.280 --> 00:16:17.640]   I think there's a more optimistic view of that, too,
[00:16:17.640 --> 00:16:20.280]   which is that if you're searching
[00:16:20.280 --> 00:16:21.400]   for certain kinds of things,
[00:16:21.400 --> 00:16:24.440]   you've already reasoned that you need them.
[00:16:24.440 --> 00:16:28.320]   And these algorithms are saying, look, that's up to you
[00:16:28.320 --> 00:16:30.460]   to reason whether you need something or not.
[00:16:30.460 --> 00:16:31.960]   That's your job.
[00:16:31.960 --> 00:16:35.200]   You may have an unhealthy addiction to this stuff,
[00:16:35.200 --> 00:16:40.200]   or you may have a reasoned and thoughtful explanation
[00:16:40.200 --> 00:16:42.800]   for why it's important to you,
[00:16:42.800 --> 00:16:45.520]   and the algorithms are saying, hey, that's whatever.
[00:16:45.520 --> 00:16:46.360]   That's your problem.
[00:16:46.360 --> 00:16:48.880]   All I know is you're buying stuff like that,
[00:16:48.880 --> 00:16:50.200]   you're interested in stuff like that.
[00:16:50.200 --> 00:16:52.220]   Could be a bad reason, could be a good reason.
[00:16:52.220 --> 00:16:53.240]   That's up to you.
[00:16:53.240 --> 00:16:55.800]   I'm gonna show you more of that stuff.
[00:16:55.800 --> 00:17:00.520]   And I think that that's, it's not good or bad.
[00:17:00.520 --> 00:17:01.840]   It's not reasoned or not reasoned.
[00:17:01.840 --> 00:17:03.200]   The algorithm is doing what it does,
[00:17:03.200 --> 00:17:05.200]   which is saying, you seem to be interested in this,
[00:17:05.200 --> 00:17:07.640]   I'm gonna show you more of that stuff.
[00:17:07.640 --> 00:17:09.520]   And I think we're seeing this not just in buying stuff,
[00:17:09.520 --> 00:17:10.480]   but even in social media.
[00:17:10.480 --> 00:17:12.280]   You're reading this kind of stuff.
[00:17:12.280 --> 00:17:14.040]   I'm not judging on whether it's good or bad.
[00:17:14.040 --> 00:17:15.240]   I'm not reasoning at all.
[00:17:15.240 --> 00:17:17.500]   I'm just saying, I'm gonna show you other stuff
[00:17:17.500 --> 00:17:19.120]   with similar features.
[00:17:19.120 --> 00:17:21.840]   And that's it, and I wash my hands from it,
[00:17:21.840 --> 00:17:24.240]   and I say, that's all that's going on.
[00:17:24.240 --> 00:17:30.200]   - People are so harsh on AI systems.
[00:17:30.200 --> 00:17:33.200]   So one, the bar of performance is extremely high,
[00:17:33.200 --> 00:17:37.840]   and yet we also ask them to, in the case of social media,
[00:17:37.840 --> 00:17:41.200]   to help find the better angels of our nature,
[00:17:41.200 --> 00:17:44.240]   and help make a better society.
[00:17:44.240 --> 00:17:46.640]   So what do you think about the role of AI there?
[00:17:46.640 --> 00:17:48.240]   - I agree with you.
[00:17:48.240 --> 00:17:49.840]   That's the interesting dichotomy, right?
[00:17:49.840 --> 00:17:52.440]   Because on one hand, we're sitting there,
[00:17:52.440 --> 00:17:54.160]   and we're sort of doing the easy part,
[00:17:54.160 --> 00:17:56.240]   which is finding the patterns.
[00:17:56.240 --> 00:18:00.080]   We're not building, the system's not building a theory
[00:18:00.080 --> 00:18:02.480]   that is consumable and understandable by other humans
[00:18:02.480 --> 00:18:04.640]   that can be explained and justified.
[00:18:04.640 --> 00:18:09.640]   And so on one hand, to say, oh, AI is doing this.
[00:18:09.640 --> 00:18:11.960]   Why isn't it doing this other thing?
[00:18:11.960 --> 00:18:14.560]   Well, this other thing's a lot harder.
[00:18:14.560 --> 00:18:18.440]   And it's interesting to think about why it's harder.
[00:18:18.440 --> 00:18:22.240]   And because you're interpreting the data
[00:18:22.240 --> 00:18:24.520]   in the context of prior models.
[00:18:24.520 --> 00:18:27.200]   In other words, understandings of what's important
[00:18:27.200 --> 00:18:28.480]   in the world, what's not important.
[00:18:28.480 --> 00:18:30.280]   What are all the other abstract features
[00:18:30.280 --> 00:18:33.640]   that drive our decision-making?
[00:18:33.640 --> 00:18:35.680]   What's sensible, what's not sensible, what's good,
[00:18:35.680 --> 00:18:38.280]   what's bad, what's moral, what's valuable, what isn't?
[00:18:38.280 --> 00:18:39.400]   Where is that stuff?
[00:18:39.400 --> 00:18:41.520]   No one's applying the interpretation.
[00:18:41.520 --> 00:18:44.880]   So when I see you clicking on a bunch of stuff,
[00:18:44.880 --> 00:18:48.040]   and I look at these simple features, the raw features,
[00:18:48.040 --> 00:18:49.360]   the features that are there in the data,
[00:18:49.360 --> 00:18:51.600]   like what words are being used,
[00:18:51.600 --> 00:18:55.960]   or how long the material is,
[00:18:55.960 --> 00:18:58.920]   or other very superficial features,
[00:18:58.920 --> 00:19:00.840]   what colors are being used in the material.
[00:19:00.840 --> 00:19:02.200]   Like I don't know why you're clicking
[00:19:02.200 --> 00:19:03.280]   on the stuff you're looking,
[00:19:03.280 --> 00:19:05.920]   or if it's products, what the price is,
[00:19:05.920 --> 00:19:07.880]   or what the category is, and stuff like that.
[00:19:07.880 --> 00:19:09.880]   And I just feed you more of the same stuff.
[00:19:09.880 --> 00:19:12.080]   That's very different than kind of getting in there
[00:19:12.080 --> 00:19:14.320]   and saying, what does this mean?
[00:19:14.320 --> 00:19:18.680]   The stuff you're reading, like why are you reading it?
[00:19:18.680 --> 00:19:22.240]   What assumptions are you bringing to the table?
[00:19:22.240 --> 00:19:24.720]   Are those assumptions sensible?
[00:19:24.720 --> 00:19:27.320]   Does the material make any sense?
[00:19:27.320 --> 00:19:32.320]   Does it lead you to thoughtful, good conclusions?
[00:19:32.320 --> 00:19:35.720]   Again, there's interpretation and judgment involved
[00:19:35.720 --> 00:19:37.240]   in that process.
[00:19:37.240 --> 00:19:41.120]   That isn't really happening in the AI today.
[00:19:41.120 --> 00:19:43.760]   That's harder.
[00:19:43.760 --> 00:19:46.840]   Because you have to start getting at the meaning
[00:19:46.840 --> 00:19:50.320]   of the stuff, of the content.
[00:19:50.320 --> 00:19:54.040]   You have to get at how humans interpret the content
[00:19:54.040 --> 00:19:57.000]   relative to their value system
[00:19:57.000 --> 00:19:58.880]   and deeper thought processes.
[00:19:58.880 --> 00:20:00.440]   - So that's what meaning means,
[00:20:00.440 --> 00:20:05.440]   is not just some kind of deep, timeless, semantic thing
[00:20:05.440 --> 00:20:09.240]   that the statement represents,
[00:20:09.240 --> 00:20:11.680]   but also how a large number of people
[00:20:11.680 --> 00:20:13.520]   are likely to interpret.
[00:20:13.520 --> 00:20:17.380]   So it's again, even meaning is a social construct.
[00:20:17.380 --> 00:20:19.800]   It's so you have to try to predict
[00:20:19.800 --> 00:20:22.800]   how most people would understand this kind of statement.
[00:20:22.800 --> 00:20:25.560]   - Yeah, meaning is often relative,
[00:20:25.560 --> 00:20:28.120]   but meaning implies that the connections
[00:20:28.120 --> 00:20:30.120]   go beneath the surface of the artifacts.
[00:20:30.120 --> 00:20:33.760]   If I show you a painting, it's a bunch of colors on a canvas,
[00:20:33.760 --> 00:20:35.400]   what does it mean to you?
[00:20:35.400 --> 00:20:37.680]   And it may mean different things to different people
[00:20:37.680 --> 00:20:40.520]   because of their different experiences.
[00:20:40.520 --> 00:20:43.000]   It may mean something even different
[00:20:43.000 --> 00:20:44.720]   to the artist who painted it.
[00:20:44.720 --> 00:20:48.980]   As we try to get more rigorous with our communication,
[00:20:48.980 --> 00:20:51.520]   we try to really nail down that meaning.
[00:20:51.520 --> 00:20:56.520]   So we go from abstract art to precise mathematics,
[00:20:56.520 --> 00:20:59.800]   precise engineering drawings and things like that.
[00:20:59.800 --> 00:21:01.680]   We're really trying to say,
[00:21:01.680 --> 00:21:06.560]   I wanna narrow that space of possible interpretations
[00:21:06.560 --> 00:21:09.000]   because the precision of the communication
[00:21:09.000 --> 00:21:11.680]   ends up becoming more and more important.
[00:21:11.680 --> 00:21:16.160]   And so that means that I have to specify,
[00:21:16.160 --> 00:21:19.640]   and I think that's why this becomes really hard.
[00:21:19.640 --> 00:21:22.440]   Because if I'm just showing you an artifact
[00:21:22.440 --> 00:21:24.240]   and you're looking at it superficially,
[00:21:24.240 --> 00:21:26.480]   whether it's a bunch of words on a page
[00:21:26.480 --> 00:21:30.200]   or whether it's brushstrokes on a canvas
[00:21:30.200 --> 00:21:31.880]   or pixels in a photograph,
[00:21:31.880 --> 00:21:33.360]   you can sit there and you can interpret
[00:21:33.360 --> 00:21:36.060]   lots of different ways at many, many different levels.
[00:21:36.060 --> 00:21:43.240]   But when I wanna align our understanding of that,
[00:21:43.240 --> 00:21:46.680]   I have to specify a lot more stuff
[00:21:46.680 --> 00:21:50.600]   that's actually not directly in the artifact.
[00:21:50.600 --> 00:21:54.280]   Now I have to say, well, how are you interpreting
[00:21:54.280 --> 00:21:55.560]   this image and that image?
[00:21:55.560 --> 00:21:57.680]   And what about the colors and what do they mean to you?
[00:21:57.680 --> 00:22:00.880]   What perspective are you bringing to the table?
[00:22:00.880 --> 00:22:03.920]   What are your prior experiences with those artifacts?
[00:22:03.920 --> 00:22:07.120]   What are your fundamental assumptions and values?
[00:22:07.120 --> 00:22:09.160]   What is your ability to kind of reason
[00:22:09.160 --> 00:22:11.940]   to chain together logical implication
[00:22:11.940 --> 00:22:12.780]   as you're sitting there and saying,
[00:22:12.780 --> 00:22:14.840]   well, if this is the case, then I would conclude this.
[00:22:14.840 --> 00:22:17.400]   If that's the case, then I would conclude that.
[00:22:17.400 --> 00:22:20.800]   So your reasoning processes and how they work,
[00:22:20.800 --> 00:22:23.640]   your prior models and what they are,
[00:22:23.640 --> 00:22:25.480]   your values and your assumptions,
[00:22:25.480 --> 00:22:28.900]   all those things now come together into the interpretation.
[00:22:28.900 --> 00:22:32.060]   Getting in sync of that is hard.
[00:22:32.060 --> 00:22:35.880]   - And yet humans are able to intuit some of that
[00:22:35.880 --> 00:22:37.840]   without any pre--
[00:22:37.840 --> 00:22:39.840]   - Because they have the shared experience.
[00:22:39.840 --> 00:22:41.200]   - And we're not talking about shared,
[00:22:41.200 --> 00:22:42.680]   two people having a shared experience.
[00:22:42.680 --> 00:22:43.840]   I mean, as a society--
[00:22:43.840 --> 00:22:44.880]   - That's correct.
[00:22:44.880 --> 00:22:49.500]   We have the shared experience and we have similar brains.
[00:22:49.500 --> 00:22:52.360]   So we tend to, in other words,
[00:22:52.360 --> 00:22:53.400]   part of our shared experience
[00:22:53.400 --> 00:22:54.780]   is our shared local experience.
[00:22:54.780 --> 00:22:56.160]   Like we may live in the same culture,
[00:22:56.160 --> 00:22:57.360]   we may live in the same society,
[00:22:57.360 --> 00:23:00.320]   and therefore we have similar educations.
[00:23:00.320 --> 00:23:02.400]   We have similar, what we like to call prior models
[00:23:02.400 --> 00:23:04.160]   about the prior experiences.
[00:23:04.160 --> 00:23:05.680]   And we use that as a,
[00:23:05.680 --> 00:23:09.240]   think of it as a wide collection of interrelated variables
[00:23:09.240 --> 00:23:11.080]   and they're all bound to similar things.
[00:23:11.080 --> 00:23:13.360]   And so we take that as our background
[00:23:13.360 --> 00:23:15.840]   and we start interpreting things similarly.
[00:23:15.840 --> 00:23:20.160]   But as humans, we have a lot of shared experience.
[00:23:20.160 --> 00:23:23.280]   We do have similar brains, similar goals,
[00:23:23.280 --> 00:23:26.360]   similar emotions under similar circumstances
[00:23:26.360 --> 00:23:27.320]   because we're both humans.
[00:23:27.320 --> 00:23:29.720]   So now one of the early questions you asked,
[00:23:29.720 --> 00:23:34.720]   how is biological and computer information systems
[00:23:34.720 --> 00:23:36.280]   fundamentally different?
[00:23:36.280 --> 00:23:41.280]   Well, one is humans come with a lot of pre-programmed stuff,
[00:23:42.160 --> 00:23:44.240]   a ton of program stuff,
[00:23:44.240 --> 00:23:45.520]   and they're able to communicate
[00:23:45.520 --> 00:23:46.640]   because they have a lot of,
[00:23:46.640 --> 00:23:48.400]   because they share that stuff.
[00:23:48.400 --> 00:23:50.560]   (silence)
[00:23:50.560 --> 00:23:52.720]   (silence)
[00:23:52.720 --> 00:23:54.880]   (silence)
[00:23:54.880 --> 00:23:57.040]   (silence)
[00:23:57.040 --> 00:23:59.200]   (silence)
[00:23:59.200 --> 00:24:01.360]   (silence)
[00:24:01.360 --> 00:24:03.520]   (silence)
[00:24:03.520 --> 00:24:13.520]   [BLANK_AUDIO]

