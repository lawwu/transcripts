
[00:00:00.000 --> 00:00:10.240]   going live. Hello everybody. Testing one, two. Testing one, two. Oh my god. Here we go. Sup,
[00:00:10.240 --> 00:00:18.720]   Kevin. All right. All right. We are live. We're live. Hey to our audience. Live on YouTube.
[00:00:18.720 --> 00:00:29.840]   Let me make sure. Turn off some. One moment while I turn off. Somebody's got their sound on. Okay.
[00:00:29.840 --> 00:00:35.680]   Everybody's got their sound off. And we are live here on the YouTube. Welcome everybody.
[00:00:35.680 --> 00:00:43.040]   Am I hearing somebody? No. Yeah, someone has their sound on. Chamath, is that you, Jason?
[00:00:43.040 --> 00:00:45.200]   Maybe it's my computer. Hold on.
[00:00:45.200 --> 00:00:57.520]   Chamath, you're live. Say hi. Hey, what's up everybody?
[00:00:59.600 --> 00:01:03.280]   I am watching you guys here, and then I have it streamed here.
[00:01:03.280 --> 00:01:09.200]   But I don't see it live. It's not live. It's actually delayed. It's definitely my other
[00:01:09.200 --> 00:01:13.360]   computer. It's on a 20 second delay. Yes, we're on a 20 second delay. Oh, it's on a 20 second
[00:01:13.360 --> 00:01:19.680]   delay. Is that your decision, Nick, for cancellation purposes? No. You can press
[00:01:19.680 --> 00:01:23.920]   the button at any time. If you say something that's worth canceling. All right. Let me open
[00:01:23.920 --> 00:01:32.560]   the show properly. Here we go. In three, two. All right, everybody. Welcome to the all in live
[00:01:32.560 --> 00:01:41.840]   stream, celebrating 500,000 subscribers on the YouTube. Do us a favor. Give us a thumbs up,
[00:01:41.840 --> 00:01:48.160]   and go ahead and tweet and share. We've got about 250 thumbs up. This is how you break
[00:01:48.160 --> 00:01:53.040]   the algorithm. If you give a thumbs up, we're now at 400 thumbs up, 3,000 people watching.
[00:01:53.040 --> 00:01:56.880]   We want to get the number of thumbs up to match the number of people watching live.
[00:01:56.880 --> 00:02:01.040]   What does that do? It breaks the algorithm, and then it shares the show with everybody.
[00:02:01.040 --> 00:02:06.800]   Go ahead and tweet it. We have taken questions for the last, I don't know, three days. Producer
[00:02:06.800 --> 00:02:14.560]   Nick put the stream up early. With me today, of course, your chairman dictator, Chamath
[00:02:14.560 --> 00:02:23.840]   Palihapitiya, and the rain man, yeah, David Sachs. At some point, the queen of quinoa, the prince of
[00:02:23.840 --> 00:02:30.240]   panic attacks, the sultan of science will join us. For now, we're sitting here in the afterglow of
[00:02:30.240 --> 00:02:37.360]   the Trump interview. Where is the quinoa button? Nobody knows. Nobody knows. We've sent a group out
[00:02:37.360 --> 00:02:44.960]   for the queen of quinoa, and nobody knows where he is. Did you send somebody to Sweet Greens
[00:02:44.960 --> 00:02:52.720]   or the organic vegan aisle at Whole Foods? Absolutely. It's starting already, man. You
[00:02:52.720 --> 00:02:58.080]   show up late. You're going to get barbecued. All right. Let's kick us off here with a little
[00:02:58.080 --> 00:03:05.760]   of the afterglow. Sachs, I understand that you got a phone call this weekend.
[00:03:06.320 --> 00:03:11.280]   Fill the audience in. Oh, God, please don't start giving us tips. 100% do not do that. I see
[00:03:11.280 --> 00:03:16.640]   somebody just gave a $10 tip. Do not give us any tips. Don't do that. What is that? That's real
[00:03:16.640 --> 00:03:22.480]   money? Yeah. This is like a thing on YouTube, I guess. Only the people who are subscribers to the
[00:03:22.480 --> 00:03:32.080]   channel can be in the chat, but you can also give what's called a super chat. Dan, Danthacrypto,
[00:03:32.080 --> 00:03:37.440]   gave us a $10 super chat, and then that puts their little logo and their chat up on the top,
[00:03:37.440 --> 00:03:40.800]   but I don't think we want you guys to send us money. Please don't do that.
[00:03:40.800 --> 00:03:47.840]   Oh, top chat. Oh, plus 11 likes. Wow. Yeah. It's just a way for you to get a little bit more play,
[00:03:47.840 --> 00:03:55.920]   but just please don't tip us. Stop. Don't. Sachs, fill us in. Oh, looks like Dave has entered.
[00:03:55.920 --> 00:04:01.680]   Should we let Dave in or no? He's in. We'll admit him. He's in. Okay. Oh, no. Here we go. Stefano
[00:04:01.680 --> 00:04:10.160]   now has given us a $5.99 pound. I guess he's from... Oh my God. Here he is. How are you doing
[00:04:10.160 --> 00:04:16.000]   there, buddy? We are live on YouTube right now, by the way. We are? Yeah, we are. How do I see
[00:04:16.000 --> 00:04:21.520]   what people are saying? Click to go to YouTube. You have to go to YouTube. You have to go to
[00:04:21.520 --> 00:04:27.040]   YouTube and click on the link, but then you have to turn off the volume. Okay. I see. What's going
[00:04:27.040 --> 00:04:38.720]   on? Well, apparently we have 1,600 thumbs up and 4,412 people watching live already. Everybody go
[00:04:38.720 --> 00:04:44.800]   ahead and give us a thumbs up. Please do not do... Hank, I just told you guys, do not give us super
[00:04:44.800 --> 00:04:52.240]   chats. Hank Lieber just gave us a $20 super chat. God damn it. Stop it. Atomics Woosh gave us a $10
[00:04:52.240 --> 00:04:56.000]   super chat. Stop giving super chats. I'm telling you now. No, we're doing that to the Trump
[00:04:56.000 --> 00:05:02.320]   campaign. All tips will go to... No. For the Humane Society of the United States.
[00:05:02.320 --> 00:05:07.520]   Oh, look at this guy trying to score points. How about it goes towards my PC12? How about we do
[00:05:07.520 --> 00:05:12.000]   that? Goes to PC12. All right, everybody. Are you reading questions, Jacob?
[00:05:12.000 --> 00:05:17.520]   Oh God. Bill just gave us a $99 super chat. Stop giving money to us.
[00:05:17.520 --> 00:05:21.360]   Give him a Trump donation link.
[00:05:21.360 --> 00:05:31.120]   All right. So I hope this doesn't blow up in your lap, but Sax, give us what happened post the Trump
[00:05:31.120 --> 00:05:36.960]   thing. You got a special phone call this weekend? Yeah. Yeah. The president called to say that he
[00:05:36.960 --> 00:05:43.280]   enjoyed the experience and to thank us. And he even said, "Even thank your co-hosts who aren't
[00:05:43.280 --> 00:05:48.480]   fans of mine." He wanted to say hi to you guys too. Oh, okay. Is he talking about me or talking
[00:05:48.480 --> 00:05:53.680]   about Freebo? Well, I said to him that even... He said the two liberals.
[00:05:53.680 --> 00:06:01.520]   Well, I said that even my co-hosts who aren't fans of his had to admit he did a good job.
[00:06:02.080 --> 00:06:06.320]   And he said, "Well, tell them I said hi too." Oh, okay. So you're saying that
[00:06:06.320 --> 00:06:11.360]   to us with the president. Got it. Okay. Thank you. Chamath, why did you say that Trump did do a good
[00:06:11.360 --> 00:06:20.320]   job? He and I really like each other. In fact, I think of all of us, he called me last November.
[00:06:20.320 --> 00:06:25.280]   That's how this whole thing started, if you remember. No, he called Chamath a friend.
[00:06:27.200 --> 00:06:32.160]   And well, he said, "One of the other guys is a friend." And then he says,
[00:06:32.160 --> 00:06:36.480]   "Say hi to the other two as well." Oh, all right. Well, thank you, President Trump.
[00:06:36.480 --> 00:06:42.960]   Look, he liked the experience. And what he told me is that he's heard nothing but positive things.
[00:06:42.960 --> 00:06:49.680]   Everyone he runs into or talks to says that they saw him on the pod. And I think it's very weighted
[00:06:49.680 --> 00:06:53.920]   towards business people. So when he meets with business people, they're all like, "Oh, I just
[00:06:53.920 --> 00:07:01.920]   saw you on the All In pod." So he said, "We have a big hit on our hands." He said, "We're like TV
[00:07:01.920 --> 00:07:07.440]   stars now." That's incredible. Imagine if we didn't have to push sand uphill with some of these
[00:07:07.440 --> 00:07:14.880]   folks. Trying to do the obvious things. I mean, it's unbelievable, you guys. Unbelievable. Easy
[00:07:14.880 --> 00:07:21.120]   to admit in hindsight, no? Well, let's go around the horn here. We are now a couple of days post
[00:07:21.120 --> 00:07:27.360]   the interview. I'm curious, Chamath, with a little bit of time between the interview, I'm sure a ton
[00:07:27.360 --> 00:07:34.880]   of feedback, what do you think were the most important moments of that interview for the
[00:07:34.880 --> 00:07:43.360]   audience, for Americans, and for the voting public? Well, I thought you saw somebody that was,
[00:07:44.720 --> 00:07:53.360]   frankly, very presidential. I think he took all questions and internalized what he wanted to say,
[00:07:53.360 --> 00:08:01.040]   and gave his version of the facts where we didn't interrupt, we didn't cajole, we didn't
[00:08:01.040 --> 00:08:07.360]   pander. We asked the question, and then we stopped, and we allowed
[00:08:07.360 --> 00:08:13.600]   the former president and someone running for current president to speak his mind,
[00:08:13.600 --> 00:08:18.880]   and put it on the record in a way that people can listen to and go back to. I think that that's
[00:08:18.880 --> 00:08:25.600]   really the most important thing. The second most important thing is that I really want this
[00:08:25.600 --> 00:08:33.040]   platform to become a place that can convene these kinds of thoughtful conversations. It takes a lot
[00:08:33.040 --> 00:08:41.120]   of investment, frankly, upfront. I think it takes a lot of effort. I'm just glad that we were able
[00:08:41.120 --> 00:08:45.760]   to get it over the finish line. I feel like we really accomplished something important,
[00:08:45.760 --> 00:08:50.400]   so I feel really good about the whole thing. Bruber, I'm sure you got some good feedback.
[00:08:50.400 --> 00:08:55.280]   We've had a little bit of time, and I saw you were reading a lot of the press hits,
[00:08:55.280 --> 00:08:59.280]   and we were talking about that on the back channel. Coming out of it, what do you think
[00:08:59.280 --> 00:09:06.080]   the important moments were, and what do you think the public, what resonated with the public?
[00:09:07.120 --> 00:09:11.920]   I don't know. I think people just like a long-form conversation with all these guys from
[00:09:11.920 --> 00:09:20.560]   Vivek to Ian Phillips to RFK Jr. to Chris Christie having Trump on. I hope Biden will come on. The
[00:09:20.560 --> 00:09:28.480]   long-form format, I think, gives everyone a much more authentic and genuine experience with the
[00:09:28.480 --> 00:09:33.840]   person that they're thinking about or considering in the voting. All the wrappers around who this
[00:09:33.840 --> 00:09:38.720]   person is can be suspended for a minute and maybe just hear a little bit from their mouth directly
[00:09:38.720 --> 00:09:46.320]   about the issues and allow all the judgment to happen outside of that context. Let them speak,
[00:09:46.320 --> 00:09:50.960]   and let's hear the voice of the person that is running. I'd love to do more of that. I think
[00:09:50.960 --> 00:09:55.200]   it's super helpful to hear the long-format discussions. I've heard so many people,
[00:09:55.200 --> 00:09:59.520]   by the way, reach out to me about Jared Kushner's interview on the show that they were so surprised
[00:09:59.520 --> 00:10:03.760]   how different he came across relative to what they've read about him and heard about him.
[00:10:03.760 --> 00:10:10.480]   He's never done much long-form until the show, and that made a real difference for people. I'm not
[00:10:10.480 --> 00:10:14.320]   saying anything positive or negative one way or the other. I just think the format's great. I hope
[00:10:14.320 --> 00:10:18.240]   we can do more of it. Then you can judge if you think they're full of shit or you think they're
[00:10:18.240 --> 00:10:26.320]   legit. Sax, a lot of back and forth and people perhaps saying the Trump administration walked
[00:10:26.320 --> 00:10:35.840]   back President Trump's comments on green cards for anybody with a college degree. He said,
[00:10:35.840 --> 00:10:40.400]   "You staple it right to degree." There was a comment, I guess, from somebody on the staff,
[00:10:40.400 --> 00:10:47.280]   "Hey, but these are going to be super vetted." I guess the cynical folks were saying, "Oh,
[00:10:47.280 --> 00:10:51.280]   he walked it all back." I didn't see it as a full walk back. I saw it as, "Yeah,
[00:10:51.280 --> 00:10:56.800]   probably be thoughtful." We don't want to let somebody hack the education system to get a green
[00:10:56.800 --> 00:11:01.040]   card if they're in fact a terrorist or something. What did you think about that exchange? I think
[00:11:01.040 --> 00:11:06.880]   that's the one that got covered the most in the media. Well, I think that what they were talking
[00:11:06.880 --> 00:11:12.320]   about was adding some sort of vetting process because if you allow every single college or
[00:11:12.320 --> 00:11:17.440]   university, even second or third rate ones to give unlimited green cards, which is basically
[00:11:17.440 --> 00:11:24.560]   citizenship to anybody, they could turn into diploma mills that are used purely to circumvent
[00:11:24.560 --> 00:11:29.120]   the normal citizenship rules. So I get that concern. There has to be some sort of vetting
[00:11:29.120 --> 00:11:35.440]   process. There probably does need to be some sort of limit on the number. There probably does need
[00:11:35.440 --> 00:11:41.280]   to be a real skills requirement. There probably does need to be a limit on which kind of institutions
[00:11:41.280 --> 00:11:47.760]   have this power because you can't just give the entire immigration system over to our colleges
[00:11:47.760 --> 00:11:52.240]   and universities to run. Yeah, maybe not University of Phoenix. Yeah. So look, I think there are
[00:11:52.240 --> 00:11:57.440]   legitimate concerns about the ways in which a proposal like this could be abused. But
[00:11:57.440 --> 00:12:03.360]   does that mean that everything Trump said was invalid? I don't think so. I think that Trump
[00:12:03.360 --> 00:12:09.120]   expressed the right sentiments and the right values, which is we want the best and brightest
[00:12:09.120 --> 00:12:14.400]   to be able to come over to the United States. We want to create the dream team here. We want to
[00:12:14.400 --> 00:12:19.360]   have high skill immigrants. I mean, he said all the right things there. And I think it's just a
[00:12:19.360 --> 00:12:24.480]   matter of figuring out how do you actually implement that policy. But I think he overall
[00:12:24.480 --> 00:12:29.920]   helped himself on that question, especially in the tech community. Yeah, that was the most positive
[00:12:29.920 --> 00:12:36.320]   feedback I got was, hey, thanks for asking that question and pushing him on it because it matters.
[00:12:36.320 --> 00:12:40.080]   And I think there were a couple other questions I thought were really important. You know,
[00:12:40.080 --> 00:12:43.760]   one of the one that you asked that turned out to be very important was the one on abortion.
[00:12:43.760 --> 00:12:49.520]   And he was extremely explicit and clear that he would not support a national abortion ban.
[00:12:49.520 --> 00:12:55.680]   Just yesterday, we got our first, the All in Pod became context for our first community note,
[00:12:55.680 --> 00:13:00.720]   correcting the vice president and maybe the president too, because Kamala Harris was out
[00:13:00.720 --> 00:13:07.440]   on X yesterday claiming that Trump would impose a national abortion ban if he were to win a second
[00:13:07.440 --> 00:13:16.240]   term. I think it's really clear that he does not support that. And so people were taking that clip
[00:13:16.240 --> 00:13:21.440]   from our show and putting it on there. I tweeted that clip. So I think that was very important. I
[00:13:21.440 --> 00:13:25.920]   think it's another example of Trump helping himself with that interview. By the way,
[00:13:25.920 --> 00:13:31.440]   he could have given the wrong answer too, you know. There are people who don't like that answer,
[00:13:31.440 --> 00:13:35.600]   but we think it's the right answer. And I think it's the right answer for him electorally,
[00:13:35.600 --> 00:13:39.360]   but he gave the right answer. Chances of him coming back, Sax?
[00:13:39.360 --> 00:13:44.400]   I think they're pretty good. I'm pretty good. Just one other question I want to point out. So
[00:13:44.400 --> 00:13:48.400]   I think the other piece of news, and I think it really should have been far bigger news, was
[00:13:48.400 --> 00:13:54.000]   when I asked him about Ukraine, and he said that NATO expansion had played a major role in provoking
[00:13:54.000 --> 00:14:01.200]   the war. Nigel Farage in the UK, who's running for prime minister on the Reform Party, came out the
[00:14:01.200 --> 00:14:07.280]   very next day and said the exact same thing. And there was a huge firestorm in the UK press because
[00:14:07.280 --> 00:14:13.680]   they're even more belligerent and bellicose over there. I mean, they are like spoiling for a war.
[00:14:13.680 --> 00:14:20.400]   And there was an absolute pandemonium in the UK press over Farage saying that. But there's
[00:14:20.400 --> 00:14:25.680]   abundant evidence that Farage is correct, and there's abundant evidence that Trump is correct.
[00:14:25.680 --> 00:14:30.960]   And I think that the two of them saying this now, almost at the same time, I think that Trump saying
[00:14:30.960 --> 00:14:37.360]   it first actually helped make it acceptable for Farage to say it. But I think both of them now
[00:14:37.360 --> 00:14:45.360]   saying it after a lot of academics have said it, Jeffrey Sachs, John Mearsheimer. I think that,
[00:14:45.360 --> 00:14:53.040]   just to finish the point, I think that it could now break open this debate over Ukraine and help
[00:14:53.040 --> 00:14:57.360]   us to get to a peace solution. All right. I remain undecided. I would
[00:14:57.360 --> 00:15:01.360]   like to ask Trump a couple more questions. I felt like the interview flowed really nicely,
[00:15:01.360 --> 00:15:06.880]   and I appreciate President Trump coming on. I would love to have President Biden come on as
[00:15:06.880 --> 00:15:11.600]   well. I don't know if that's going to happen. But let's get to your questions. A couple of
[00:15:11.600 --> 00:15:21.520]   housekeeping notes here. We will be having another All In Meetup. You can go to more than 50 meetups
[00:15:21.520 --> 00:15:27.600]   that are happening around the world on Thursday, July 11th. Go to allinpodcast.co/meetups. Allin
[00:15:27.600 --> 00:15:34.400]   podcast.co/meetups to join or host an event. Nick will drop the links in the YouTube chat.
[00:15:35.200 --> 00:15:43.760]   And as for the summit, quick housekeeping before we get to your questions. We are holding back a
[00:15:43.760 --> 00:15:47.120]   couple of hundred tickets for scholarships. Am I correct, Friedberg? Anything you want to
[00:15:47.120 --> 00:15:49.600]   let people know about the summit before we get to questions?
[00:15:49.600 --> 00:15:54.880]   It's going to be amazing. It's going to be awesome. We have some really great speakers. We
[00:15:54.880 --> 00:15:59.120]   want to try and announce everyone together, which we'll do in a couple of weeks. But we did just
[00:15:59.120 --> 00:16:05.760]   open up 200 more tickets, which we were holding back. So the first batch all sold out. We have
[00:16:05.760 --> 00:16:12.000]   200 left. If you go to allinpodcast.co or allinsummit, I actually don't know.
[00:16:12.000 --> 00:16:14.960]   Summit.allinpodcast.co.
[00:16:14.960 --> 00:16:21.040]   Summit.allinpodcast.co. You can submit an application. We are going to have-
[00:16:21.040 --> 00:16:24.320]   Is the scholarship form up yet or we're going to put that up in a couple of weeks?
[00:16:24.320 --> 00:16:30.240]   Okay. So we're going to put that up last minute, I guess. All right. Let's get to your questions,
[00:16:30.240 --> 00:16:34.080]   everybody. Once again, give a thumbs up for your squad. Hit the subscribe button,
[00:16:34.080 --> 00:16:41.920]   share it on your socials, and let's get started. Here's a great question for all the besties.
[00:16:41.920 --> 00:16:48.960]   Dave's Dynamite asks, "Who is your dream guest for the show?" Who is your dream guest
[00:16:48.960 --> 00:16:51.440]   for the show, Chamath Palihapitiya? Dream guest.
[00:16:51.440 --> 00:16:52.720]   Dave Chappelle.
[00:16:53.840 --> 00:17:00.800]   Oh, nice. Really nice. He would be extraordinary. Sax, your dream guest came on last week,
[00:17:00.800 --> 00:17:03.920]   so I'll just ask you, who is your second dream guest to come on the pod?
[00:17:03.920 --> 00:17:09.440]   Well, I'd like Biden to come on the pod and submit to the same type of questioning that Trump did.
[00:17:09.440 --> 00:17:13.680]   Okay. At this moment in time, you say Biden. That was going to be my answer as well.
[00:17:13.680 --> 00:17:20.640]   Freeberg, tell us which fictional science fiction character from Dune you would most
[00:17:20.640 --> 00:17:22.880]   like to see on the program as your guest.
[00:17:22.880 --> 00:17:26.080]   Great question, J. Cal, but my answer is Javier Millet.
[00:17:26.080 --> 00:17:30.400]   Oh, great pull. Great pull. Why?
[00:17:30.400 --> 00:17:41.760]   I think Javier Millet represents the salvation of the West. I think that much of the West is headed
[00:17:41.760 --> 00:17:50.480]   in a direction that he's, I think, described well, which he generally calls socialist policies
[00:17:51.120 --> 00:17:59.680]   that negatively impact productivity and progress. Argentina learned that lesson hard and over a long
[00:17:59.680 --> 00:18:06.960]   period of time, and he has kind of Superman-like gone around the world, flying seven times around
[00:18:06.960 --> 00:18:14.400]   the earth in reverse time, and tried to revitalize that country and that economy. He's gotten foreign
[00:18:14.400 --> 00:18:20.560]   dollars to go back into Argentina. He's run a government surplus. Unemployment is declining.
[00:18:20.560 --> 00:18:26.160]   All the benefits of having free markets and having capitalism drive progress for everyone
[00:18:26.160 --> 00:18:31.520]   seems to be playing out in a very short period of time in Argentina. I think that he kind of
[00:18:31.520 --> 00:18:38.160]   represents the next phase of what could be a nasty decade or two ahead.
[00:18:38.160 --> 00:18:41.040]   You know, now that I think about it that we're talking about world leaders,
[00:18:41.040 --> 00:18:45.760]   MBS, Xi Jinping, should be incredible guests as well, huh, Chamath?
[00:18:47.280 --> 00:18:53.360]   Yeah. I think there's a decent chance we'll get at least one of them.
[00:18:53.360 --> 00:19:00.560]   All right. There you have it, folks. All right. Minesh Deva asks us, "How do you view AI
[00:19:00.560 --> 00:19:08.480]   and how it will influence your kids' education?" Saks, I know you're super involved in this,
[00:19:08.480 --> 00:19:11.600]   so what are your thoughts? You have three kids, by the way.
[00:19:11.600 --> 00:19:17.120]   Well, I think the way that AI is going to influence education is that you could create
[00:19:17.120 --> 00:19:26.000]   highly customized AI tutors. And so I think the revolution could be that every kid can now receive,
[00:19:26.000 --> 00:19:31.760]   again, specialized, bespoke, custom education as opposed to being on an assembly line.
[00:19:31.760 --> 00:19:36.560]   I mean, the way that we make education today is the same way that we make, you know,
[00:19:36.560 --> 00:19:43.440]   Fords or McDonald's hamburgers, right? It's a big assembly line where you create a large batch
[00:19:43.440 --> 00:19:48.800]   and you just move it forward. And some people need to go at a different pace. Some people
[00:19:48.800 --> 00:19:54.240]   need to go slower. Some people need to go faster. Some people need to be on a completely different
[00:19:54.240 --> 00:20:00.800]   type of conveyor belt. And you don't get any of that right now. And with AI, you have the ability
[00:20:00.800 --> 00:20:06.000]   to cost-effectively deliver, again, a highly customized education experience for every
[00:20:06.000 --> 00:20:10.880]   child. So I think that's where it's going, but it's going to take several years to play out.
[00:20:11.520 --> 00:20:15.200]   Yeah. And this is referred to in the industry as adaptive learning, if you want to go
[00:20:15.200 --> 00:20:20.080]   take a look at that. Chamath, your thoughts on AI and our kids' future? I'll expand the question a
[00:20:20.080 --> 00:20:27.520]   bit, not just education, but how will it impact our children's career choices and lives generally?
[00:20:27.520 --> 00:20:38.000]   Wow. I mean, I think that the nature of work is going to change really profoundly.
[00:20:38.000 --> 00:20:46.160]   And I think that right now we have not equipped people to take advantage of it. So
[00:20:46.160 --> 00:20:53.040]   the things that are known knowns, to use the Donald Rumsfeld quote, will be automated away.
[00:20:53.040 --> 00:21:00.880]   Unknown knowns will also be automated away, because that's just about computational horsepower.
[00:21:00.880 --> 00:21:07.680]   So any job that you did that's formulaic will get replaced probably by a robot. And any job
[00:21:08.240 --> 00:21:13.920]   where you're making guesses, but the answer can be inferred or known, that job will go away as
[00:21:13.920 --> 00:21:21.440]   well. So what's left over? I think it's judgment, where you have to express some error rate. And
[00:21:21.440 --> 00:21:25.440]   that's acceptable, because the job requires you to judge something, and people like that.
[00:21:25.440 --> 00:21:30.560]   Now the question is, how do you train kids to have judgment? And this is where I think we
[00:21:30.560 --> 00:21:35.360]   have really let kids down. Where does judgment come from? It comes from experience. Where does
[00:21:35.360 --> 00:21:41.440]   experience comes from? It only comes from failure. Where does failure come from? It comes from a
[00:21:41.440 --> 00:21:48.160]   culture that celebrates resilience and says, go take these risks, go try some stuff. It's actually
[00:21:48.160 --> 00:21:53.360]   really good that it didn't work. Let's talk about why. Instead, if you live in a world that's all
[00:21:53.360 --> 00:22:00.640]   about coddling folks and microaggressions and window dressing, the kid won't be resilient.
[00:22:00.640 --> 00:22:06.160]   The kids don't take risks. And then there's just a parade of terribles that leave these people
[00:22:06.160 --> 00:22:11.760]   inexperienced and without judgment. And then I think the computers just run over these folks.
[00:22:11.760 --> 00:22:17.840]   I have been giving this a lot of thought as well. And I have been having my daughters go to the
[00:22:17.840 --> 00:22:24.320]   store on their own with money to buy stuff and doing all this kind of resilient, adventurous
[00:22:24.320 --> 00:22:29.680]   activities with them, having them take risks, do dangerous things, or what maybe helicopter
[00:22:29.680 --> 00:22:33.680]   parents would think are dangerous things. Because all that's going to be left, if you can get the
[00:22:33.680 --> 00:22:39.760]   answer to any question, if everybody's a good painter, writer, photographer, designer, coder,
[00:22:39.760 --> 00:22:44.480]   what's left? Taking risks, to your point, Shamath, and having grit, and maybe being a great
[00:22:44.480 --> 00:22:49.600]   communicator and a great leader. Freeberg, are you thinking about this at all, AI's impact and
[00:22:50.160 --> 00:23:02.800]   how it's impacting how you parent your children? I think it's just finding leverage. The transitions
[00:23:02.800 --> 00:23:09.200]   that are possible with every technology frontier in human history have been ones of leverage where
[00:23:09.200 --> 00:23:15.200]   the human role can scale up. And does it really matter if you know how to do good handwriting
[00:23:15.200 --> 00:23:20.480]   when everyone's typing? Does it really matter if you know how to type if everyone's dictating?
[00:23:20.480 --> 00:23:27.280]   And what can you really do that differentiates you at that new up level? And I think that's
[00:23:27.280 --> 00:23:32.480]   really important to think about. I think a lot of people transition from a world of
[00:23:32.480 --> 00:23:41.520]   labor to design, like you become architects and what the implications of that and how to
[00:23:41.520 --> 00:23:46.080]   be successful in that environment is how I would think about re-education and how I would think
[00:23:46.080 --> 00:23:51.520]   about motivating success. Fantastic. Okay. Ted Zhang asks, "Been watching the podcast for years
[00:23:51.520 --> 00:23:57.520]   now. As a 23-year-old out of college, what are your pieces of advice for getting into bigger rooms
[00:23:57.520 --> 00:24:02.960]   with more intelligent, smarter, well-connected, and able individual sacks?" You were part of the
[00:24:02.960 --> 00:24:09.840]   PayPal mafia. You got yourself in the room, part of the Stanford community as well. What's your
[00:24:09.840 --> 00:24:13.680]   best advice here for Ted who's looking to get into the room where it happens?
[00:24:13.680 --> 00:24:20.000]   You're on mute, Sax. It's year four of post-pandemic,
[00:24:20.000 --> 00:24:25.120]   kids still on mute. Okay. I think I would just generalize the advice
[00:24:25.120 --> 00:24:33.120]   a little bit more and just say that the advantage that a young person has is that you have the
[00:24:33.120 --> 00:24:39.360]   ability to be obsessed in a way that when you get older, it's harder to be. That as you get older,
[00:24:39.360 --> 00:24:44.480]   you get more responsibilities, you have family, you want more breadth in your career. I think
[00:24:44.480 --> 00:24:49.920]   when you're young, you have the ability just to focus on a particular area, be really obsessed
[00:24:49.920 --> 00:24:57.680]   with it. It's a magical thing. It's a superpower that younger people have that it gets hard to
[00:24:57.680 --> 00:25:05.120]   maintain that level of focus as you get older. I would lean into that. I would say find something
[00:25:05.120 --> 00:25:11.920]   that you're excited enough about to be obsessed with. That's my generic career advice.
[00:25:11.920 --> 00:25:22.720]   Chamath, you came out of nowhere to find yourself in the room where it happens at AOL,
[00:25:22.720 --> 00:25:28.160]   room where it happens in Ventureland and at Facebook. You've done it a couple of times.
[00:25:28.160 --> 00:25:33.520]   How does Ted get into the room where it happens? Well, I think Ted needs to
[00:25:33.520 --> 00:25:45.360]   embrace the idea that his life is going to be made of chapters. I think that some chapters will work,
[00:25:45.360 --> 00:25:52.880]   some chapters won't work. Some chapters will work incredibly well. Some chapters will be benign
[00:25:52.880 --> 00:26:02.960]   in his life. You have to just take a shot as best as you can when the shot presents itself.
[00:26:02.960 --> 00:26:09.840]   The reason why I say this is that if you view your life as a series of chapters,
[00:26:09.840 --> 00:26:14.560]   the real problem is going to be everybody else because they're going to want to define you by
[00:26:14.560 --> 00:26:19.360]   one chapter. That's because it allows them to compartmentalize their shitty life.
[00:26:20.880 --> 00:26:26.480]   You have to be very careful in not allowing the expectations of other people to drag you down.
[00:26:26.480 --> 00:26:35.760]   You should live eight or nine careers. You should try seven or eight or 10 or 15 different things in
[00:26:35.760 --> 00:26:43.520]   your life. I've had many chapters from cashier at Burger King to entrepreneur to executive to
[00:26:44.400 --> 00:26:51.280]   rank and file worker to investor, all of it. Not one is more important than the other.
[00:26:51.280 --> 00:26:57.040]   The only people that will try to make you feel good or bad about any one chapter are the ones
[00:26:57.040 --> 00:27:02.400]   that are just completely impotent and on the sidelines and they can't get out of their own
[00:27:02.400 --> 00:27:07.760]   way. Just view your life as a series of chapters and just try your best. Don't be afraid to take
[00:27:07.760 --> 00:27:12.000]   risk. You're saying also don't over-index on other people's opinion. I think that's something you and
[00:27:12.000 --> 00:27:16.080]   I have talked about. You fell victim to that maybe a little bit in your career, you said?
[00:27:16.080 --> 00:27:20.080]   Well, I just think people will try to pull you in a cul-de-sac because they themselves are frozen.
[00:27:20.080 --> 00:27:28.560]   All right, Friedberg. Ignore the haters. Focus on your craft and your skills.
[00:27:28.560 --> 00:27:34.800]   Friedberg, I know that you've been torn, 1989 era, reputation era, lover era. What's your
[00:27:34.800 --> 00:27:37.760]   favorite Taylor Swift era at this point? I know you've gone back and forth.
[00:27:38.800 --> 00:27:44.160]   What? I'm talking with you. I'm telling you the Taylor Swift era is as if you would know-
[00:27:44.160 --> 00:27:48.240]   I don't know. Well, what is Taylor Swift's era? I don't know this stuff.
[00:27:48.240 --> 00:27:53.920]   Reputation era. People are just obsessed with Taylor Swift. It's crazy how much people are
[00:27:53.920 --> 00:27:56.400]   obsessed with Taylor Swift. Did you guys see that Travis
[00:27:56.400 --> 00:28:00.240]   Kelsey went on stage and Wembley with her? I did see that.
[00:28:00.240 --> 00:28:03.440]   I hope he didn't try to sing. No, but he did dance and it was pretty good,
[00:28:03.440 --> 00:28:07.600]   I thought. That's got to be a lot of pressure to go there in front of 100,000 people and not
[00:28:08.720 --> 00:28:11.120]   totally be two left feet. He did a good job, I thought.
[00:28:11.120 --> 00:28:15.280]   No, but any advice? If you saw what he was wearing at the
[00:28:15.280 --> 00:28:18.160]   Super Bowl, you know that he doesn't mind making a fool of himself.
[00:28:18.160 --> 00:28:24.880]   She's a star. She's such a star. I would like to go to the Aeros store just to
[00:28:24.880 --> 00:28:28.880]   soak it in, right? Just to check it out. Same. I wouldn't mind going to it either.
[00:28:28.880 --> 00:28:33.520]   There's a 50/50 chance I go to it in Milan. Oh, let me know the date.
[00:28:34.960 --> 00:28:38.320]   Doesn't she perform for like three or four hours? I mean, these things are-
[00:28:38.320 --> 00:28:40.800]   That's the thing, it's the Aeros store. It's a whole thing.
[00:28:40.800 --> 00:28:45.360]   That is a long performance. And I guess people don't get sick of it.
[00:28:45.360 --> 00:28:47.760]   What's the total revenue on this thing? I would get sick of four hours.
[00:28:47.760 --> 00:28:51.920]   It's like two billion, three billion, right? It's like an insane haul.
[00:28:51.920 --> 00:28:55.840]   And she owns all the equity? It's like a good quarter.
[00:28:55.840 --> 00:28:59.040]   Multiple of them. Okay, it's a good quarter.
[00:28:59.040 --> 00:29:02.480]   InVideo went up that much since we started the live stream.
[00:29:02.480 --> 00:29:04.560]   It's a good quarter. It's a good quarter.
[00:29:04.560 --> 00:29:08.480]   All right, your donations as you know today, if you do a super chat, all those donations go to
[00:29:08.480 --> 00:29:13.760]   the Haktua Fund to help her find a new job. Thank you to everybody donating.
[00:29:13.760 --> 00:29:16.480]   Is it true the Haktua lady got, or the girl got fired?
[00:29:16.480 --> 00:29:21.920]   The woman? I don't know. This is just a rumor, but I mean, it's kind of taken over everything
[00:29:21.920 --> 00:29:23.120]   for the last 72 hours. Is it true that-
[00:29:23.120 --> 00:29:26.560]   No, I saw something. I don't know if it's true or not, but I saw online that apparently she
[00:29:26.560 --> 00:29:30.720]   was a preschool teacher and they didn't like- I mean, I thought she was funny.
[00:29:31.360 --> 00:29:33.040]   Of course, it's hysterical. Of course.
[00:29:33.040 --> 00:29:34.720]   Hilarious. She's super funny.
[00:29:34.720 --> 00:29:36.960]   What is the big deal? Why are people so like uptight?
[00:29:36.960 --> 00:29:40.480]   I don't know. Apparently some people think she might be a bad influence on their
[00:29:40.480 --> 00:29:43.040]   preschool age kids. I don't buy it. Oh my God. No, I don't buy it.
[00:29:43.040 --> 00:29:46.960]   She's an adult. It's an adult kind of goofy thing. Stop with this pandering. Look at this-
[00:29:46.960 --> 00:29:52.080]   Okay, so here's the deal. My wife was really upset because I made it sound like I wasn't
[00:29:52.080 --> 00:29:57.040]   happy with these two dogs. Here they are when they showed up off the street.
[00:29:57.040 --> 00:29:59.200]   I rescued these. What do you think?
[00:29:59.200 --> 00:30:03.680]   My IQ rating is going up. In Italian, they're called Bastardini.
[00:30:03.680 --> 00:30:06.800]   Yeah, this is a Bastardini. This is a little Italian Bastardini.
[00:30:06.800 --> 00:30:09.600]   No, that's a Bastardino. Two of them are Bastardini.
[00:30:09.600 --> 00:30:12.880]   Oh, is that Bastardino Carpaccio? Is that what you had the other day? The Bastardino?
[00:30:12.880 --> 00:30:14.880]   The Bastardino. Thank you.
[00:30:14.880 --> 00:30:20.400]   So wrong. All right, let's keep going here.
[00:30:20.400 --> 00:30:22.960]   Oh, is there any more of this or no? Next question.
[00:30:22.960 --> 00:30:28.960]   All right. Oh, I'll just give that person, Ted, to Sax's point about getting focused
[00:30:28.960 --> 00:30:34.000]   on your career. When you do find the one thing you love, building your social media around that,
[00:30:34.000 --> 00:30:39.760]   your entire online identity, whether you have a podcast yourself, or you make TikTok videos,
[00:30:39.760 --> 00:30:46.000]   or your socials, blogs, Substack, Beehive, whatever your jam is, just really get focused
[00:30:46.000 --> 00:30:50.480]   on one thing that you can excel in. I've given this advice multiple times, and doing spec work
[00:30:50.480 --> 00:30:55.440]   and showing that you're doing work in the world independent of getting paid is a very powerful,
[00:30:55.440 --> 00:30:59.120]   very powerful tool. We've seen that over and over again with the all-in creators,
[00:30:59.120 --> 00:31:05.120]   so don't be afraid to just do work online and share it with the world around your passion.
[00:31:05.120 --> 00:31:13.520]   Okay, let's take another question from the audience here. We got the 26-year-old. Nick,
[00:31:13.520 --> 00:31:16.640]   you are also pulling in questions. Producer Nick is pulling in questions
[00:31:16.640 --> 00:31:19.920]   from the audience in real time, correct? Yes.
[00:31:19.920 --> 00:31:28.800]   Yes, Nick. Okay. Let's see. This is Dominic McDermott. "What experience and habit systems
[00:31:28.800 --> 00:31:33.280]   from your childhood contributed to making you the skilled individuals you are today?"
[00:31:33.280 --> 00:31:40.400]   Sax, anything from the rough and tumble childhood you had in the South of America?
[00:31:40.400 --> 00:31:44.640]   He's not from the South. He is from the South.
[00:31:44.640 --> 00:31:45.440]   I grew up in Memphis.
[00:31:45.440 --> 00:31:47.440]   That's not true. Yeah, I grew up in Tennessee.
[00:31:47.440 --> 00:31:48.400]   Oh, that's right.
[00:31:48.400 --> 00:31:51.360]   I mean, well, we moved there when I was five years old. I mean, I was originally from South
[00:31:51.360 --> 00:31:56.640]   Africa, and then my family moved to Tennessee when I was five. As I look back, I read a lot
[00:31:56.640 --> 00:32:02.000]   when I was a kid, and I didn't really think of myself as an outlier in that department when I
[00:32:02.000 --> 00:32:08.320]   was younger, but now I realize that I clearly was, and by today's standards, it would be a huge
[00:32:08.320 --> 00:32:13.280]   outlier. Kids today, you just can't get them to read. There's just too many entertainment options
[00:32:13.280 --> 00:32:19.040]   or too addicted to screens. It's actually kind of depressing. I read a lot when I was a kid.
[00:32:19.040 --> 00:32:22.240]   Define a lot, a book a month, a book a week?
[00:32:22.240 --> 00:32:27.840]   Well, it's more like if I got interested in something, I would go down the rabbit hole
[00:32:27.840 --> 00:32:32.240]   and just kind of read about it until I completely exhausted that. I remember,
[00:32:32.240 --> 00:32:38.960]   I think in like 10th or 11th grade, I learned about Darwin, and I had never been exposed to
[00:32:38.960 --> 00:32:48.960]   that before. And so I ended up reading, the teacher recommended Stephen Jay Gould and Richard
[00:32:48.960 --> 00:32:53.440]   Dawkins as other authors that I might want to read if I was interested in the subject matter.
[00:32:53.440 --> 00:32:58.960]   I ended up reading every single book by Richard Dawkins, every single book by Stephen Jay Gould,
[00:32:58.960 --> 00:33:04.480]   not one of them, like all of them. And at the time, this is what I was interested in, so that's
[00:33:04.480 --> 00:33:08.800]   why I read it. But looking back, that was clearly an outlier behavior.
[00:33:08.800 --> 00:33:12.240]   Must have been tough for you when Tucker told you that all that Darwin stuff was
[00:33:12.240 --> 00:33:18.000]   actually not true. It must have been tough. It must have been heartbreaking, in fact.
[00:33:18.000 --> 00:33:23.200]   Chamath, anything from your childhood on the rough and tumble streets in Canada
[00:33:23.200 --> 00:33:26.320]   that put a little fire in your belly today?
[00:33:26.320 --> 00:33:30.800]   Yeah, I mean, the Venn diagram is growing up on welfare and
[00:33:31.920 --> 00:33:37.760]   getting beat mercilessly by my dad. It turned out to be incredible, incredibly valuable.
[00:33:37.760 --> 00:33:42.000]   Okay, there you have it, folks. If you can get your parent to beat you,
[00:33:42.000 --> 00:33:44.240]   beat your ass, you will be successful.
[00:33:44.240 --> 00:33:46.640]   Well, no, and be on welfare. I think those two-
[00:33:46.640 --> 00:33:51.280]   And be on welfare, yeah. Freeberg, you led a charmed Nepo baby life. Tell us,
[00:33:51.280 --> 00:33:53.840]   how did that inform all this great success you've had?
[00:33:53.840 --> 00:33:55.520]   Pass.
[00:33:55.520 --> 00:34:00.000]   You pass? He does not want to talk about his childhood. You did that three hours this week
[00:34:00.000 --> 00:34:01.760]   in therapy. Why do a fourth here?
[00:34:01.920 --> 00:34:06.000]   I left home early. I was 15 years old when I moved out of my home and went to college.
[00:34:06.000 --> 00:34:10.000]   I was on the road, so I was eager to move on.
[00:34:10.000 --> 00:34:15.040]   Okay, so the trauma is real. We'll leave it at that. You know what I learned? I've learned
[00:34:15.040 --> 00:34:21.440]   you've got to run in a pack and your safety in numbers when you grow up in Brooklyn.
[00:34:21.440 --> 00:34:30.080]   I have no fear of getting in any kind of an altercation or fight. I do think
[00:34:30.240 --> 00:34:37.440]   like I did learn to have a thick skin and be tough growing up in a bar in Brooklyn.
[00:34:37.440 --> 00:34:43.200]   I'll leave it at that. All right, let's take another question here. Let's see.
[00:34:43.200 --> 00:34:47.440]   You guys see all these comments coming in?
[00:34:47.440 --> 00:34:50.480]   I mean, I can't follow it. They're too fast. And please, stop giving us superchats.
[00:34:50.480 --> 00:34:55.680]   There's like live trolling. Yeah, live shout outs. It's super interesting.
[00:34:55.680 --> 00:34:58.640]   Stop giving superchats. Stop giving money to us.
[00:34:58.640 --> 00:35:02.400]   How much money have we collected so far? I can't even keep track of it. It's just
[00:35:02.400 --> 00:35:04.800]   there's like a hundred people have given any amount of money.
[00:35:04.800 --> 00:35:06.560]   Where does that money go, J. Cal? How are you going to grift that?
[00:35:06.560 --> 00:35:10.400]   I'm trying to figure that out. I'm trying to figure out how to get that money into my-
[00:35:10.400 --> 00:35:13.440]   Nick, how do you make sure J. Cal can't get his rubby paws on that money?
[00:35:13.440 --> 00:35:15.760]   Nick, can you see how much money it's been total so far?
[00:35:15.760 --> 00:35:18.560]   Not yet. I think I can afterwards.
[00:35:18.560 --> 00:35:24.320]   I just want to know, is it like half an arm of the sweater? Is it like most of the sweater?
[00:35:24.320 --> 00:35:25.520]   Is it all the sweater? I just want to get a sense of it.
[00:35:25.520 --> 00:35:27.200]   Yeah, I think it's like the zipper area.
[00:35:27.200 --> 00:35:29.040]   That's the zipper. Okay.
[00:35:29.040 --> 00:35:32.720]   I think we're about a quarter way through it, but I see a couple of hundies in here.
[00:35:32.720 --> 00:35:36.160]   Some people gave Brandon gave a 50, Bill gave a 99.
[00:35:36.160 --> 00:35:39.520]   I mean, people are giving hundies and stuff like that. It's just absurd, but-
[00:35:39.520 --> 00:35:41.440]   Well, for a thousand dollars, you can buy the sweater.
[00:35:41.440 --> 00:35:45.840]   All right. Here we go.
[00:35:45.840 --> 00:35:47.920]   Wait, you want to prioritize superchat questions?
[00:35:47.920 --> 00:35:49.600]   I mean the zipper. I mean the zipper. Sorry, not the sweater.
[00:35:49.600 --> 00:35:52.320]   Yeah, sure. Yeah, absolutely. We're going to give all this money-
[00:35:52.320 --> 00:35:54.640]   Yeah, get the superchat, guys, to say something.
[00:35:54.640 --> 00:35:56.080]   What is a superchat? What is a superchat?
[00:35:56.080 --> 00:35:58.400]   Anybody who gives money will do their question.
[00:35:58.400 --> 00:35:59.600]   Okay. I got a superchat question.
[00:35:59.600 --> 00:36:02.400]   $1. Frank Jimenez just gave a dollar trying to sneak in.
[00:36:02.400 --> 00:36:04.560]   You can't sneak into the Velvet Rope for a dollar, bro.
[00:36:04.560 --> 00:36:05.280]   $1.
[00:36:05.280 --> 00:36:06.640]   $20 from Alex Thomas.
[00:36:06.640 --> 00:36:07.520]   No, he gave a dollar.
[00:36:07.520 --> 00:36:13.760]   I just gave the woman at Balthazar a $1 tip for a table. It's not how it works, kid.
[00:36:13.760 --> 00:36:17.120]   That's not going to get you a table.
[00:36:17.120 --> 00:36:18.720]   David Sachs, this question is for you.
[00:36:18.720 --> 00:36:19.520]   Oh, here we go.
[00:36:19.520 --> 00:36:23.280]   $20 from Alex Thomas. How is the all in tequila business coming along?
[00:36:23.280 --> 00:36:26.080]   And will you allow your fans and community to invest?
[00:36:26.080 --> 00:36:29.680]   Wait. Hold on. Wait. I think that we should answer this on behalf of Sachs,
[00:36:29.680 --> 00:36:34.000]   because Sachs has been doing this. And every now and then, it's like an Easter egg.
[00:36:34.000 --> 00:36:38.800]   He texts into the group chat an update. I am not a tequila drinker, but I will say
[00:36:38.800 --> 00:36:47.280]   it looks totally fucking amazing. I can't wait. And I don't drink tequila, and I cannot wait.
[00:36:47.280 --> 00:36:48.560]   That's what I'll say.
[00:36:48.560 --> 00:36:50.560]   All I'll say is, I saw the bottle-
[00:36:50.560 --> 00:36:54.400]   It's really incredible. I mean, what you have done is really incredible.
[00:36:54.400 --> 00:36:58.800]   I saw the bottle and the packaging, and my pee pee got hard. I'm just leaving it at that.
[00:36:58.800 --> 00:37:02.560]   It's incredible. I mean, where are you in this whole thing?
[00:37:02.560 --> 00:37:05.040]   It's going to make more money than anything else we're going to do ever.
[00:37:05.040 --> 00:37:10.880]   Is it manufacturable, Sachs? That design? Are you going to be like, "Hey, don't reveal too much."
[00:37:10.880 --> 00:37:15.680]   Yes, it's good. It's good. It's good. It's happening. It's happening. Everything's on track.
[00:37:15.680 --> 00:37:18.080]   It's one of the most beautiful designs I've ever seen.
[00:37:18.080 --> 00:37:22.880]   We're hopefully going to unveil it at the All In Summit.
[00:37:22.880 --> 00:37:28.480]   If you're at the Summit, you'll be able to pre-order it.
[00:37:28.480 --> 00:37:31.920]   I don't know exactly when it'll ship.
[00:37:31.920 --> 00:37:34.560]   Give them the taste of the hierarchy.
[00:37:34.560 --> 00:37:37.760]   There's a super limited bottle.
[00:37:37.760 --> 00:37:42.160]   It tastes better than what is the thing that you said that's really, really good?
[00:37:42.160 --> 00:37:47.440]   Well, we're taste testing against Class Visual Ultra, which is $3,000 a bottle or whatever.
[00:37:48.160 --> 00:37:48.960]   And it's better than that?
[00:37:48.960 --> 00:37:52.880]   Yeah. That's the goal. Yeah.
[00:37:52.880 --> 00:37:56.400]   I am so excited about Sachs's-
[00:37:56.400 --> 00:38:00.160]   Sachs, you just drink, you sip tequila with like an ice cube in it. Is that how you do it?
[00:38:00.160 --> 00:38:00.720]   Yeah.
[00:38:00.720 --> 00:38:03.600]   But you would never use this tequila to make like a margarita, right?
[00:38:03.600 --> 00:38:04.480]   No, no, never.
[00:38:04.480 --> 00:38:06.000]   This is too much.
[00:38:06.000 --> 00:38:07.360]   Margarita is too much.
[00:38:07.360 --> 00:38:11.440]   You'd want to use like a Blanco or something and a margarita.
[00:38:11.440 --> 00:38:13.360]   A margarita.
[00:38:13.360 --> 00:38:15.200]   This is extra on Yeho.
[00:38:15.200 --> 00:38:19.040]   So this has been aged in a barrel for five years.
[00:38:19.040 --> 00:38:21.840]   How's the $3,000 tequila?
[00:38:21.840 --> 00:38:23.680]   I'm blending it up with some sugar water.
[00:38:23.680 --> 00:38:25.040]   Yeah, you just want to drink it over ice.
[00:38:25.040 --> 00:38:28.960]   I mean, some people will drink it neat, but that's way too hot in my opinion.
[00:38:28.960 --> 00:38:29.600]   That's tough.
[00:38:29.600 --> 00:38:32.480]   But you want to water down a little bit with some ice.
[00:38:32.480 --> 00:38:32.880]   Open it up.
[00:38:32.880 --> 00:38:34.000]   That's the right way to drink it.
[00:38:34.000 --> 00:38:34.240]   Yeah.
[00:38:34.240 --> 00:38:35.120]   Open it up a little bit.
[00:38:35.120 --> 00:38:38.000]   That's how Sachs and I, when we would hang at the battery, we'd do that.
[00:38:38.000 --> 00:38:39.040]   I like Sachs's move.
[00:38:39.040 --> 00:38:44.640]   Sachs came in into the whole mashugana of this enterprise.
[00:38:44.640 --> 00:38:46.080]   And he said, "I'll do the tequila.
[00:38:46.080 --> 00:38:50.480]   Give me a quarter million dollars in this LLC and everybody shut the fuck up.
[00:38:50.480 --> 00:38:51.840]   And I'll be back in six months."
[00:38:51.840 --> 00:38:53.040]   And you know what?
[00:38:53.040 --> 00:38:56.480]   Everybody shut the fuck up and he brought it.
[00:38:56.480 --> 00:38:57.600]   He brought the heat.
[00:38:57.600 --> 00:38:59.840]   I am so excited for this.
[00:38:59.840 --> 00:39:01.600]   You want another super chat question?
[00:39:01.600 --> 00:39:02.400]   Yeah, sure.
[00:39:02.400 --> 00:39:04.240]   $50 from Eric.
[00:39:04.240 --> 00:39:05.440]   Okay.
[00:39:05.440 --> 00:39:08.560]   What is one thing that makes each of you most optimistic?
[00:39:08.560 --> 00:39:13.520]   Can we allow, sorry, can we allow anybody that's given beyond a certain, like the 50 bucks,
[00:39:13.520 --> 00:39:15.280]   give them a chance to pre-order a bottle?
[00:39:15.280 --> 00:39:16.720]   I think that's kind of nice.
[00:39:16.720 --> 00:39:18.320]   How many of those guys are there?
[00:39:18.320 --> 00:39:19.120]   No, like-
[00:39:19.120 --> 00:39:20.080]   That's tough to track.
[00:39:20.080 --> 00:39:20.560]   That's tough to track.
[00:39:20.560 --> 00:39:21.600]   It's tough to track.
[00:39:21.600 --> 00:39:24.160]   Why, Nick, can you tell who's given at least 50 bucks or more?
[00:39:24.160 --> 00:39:26.880]   If they've done that, which we don't want them to do, let them at least pre-order a
[00:39:26.880 --> 00:39:27.760]   bottle with it.
[00:39:27.760 --> 00:39:29.040]   Now people are going to do it.
[00:39:29.040 --> 00:39:30.000]   Now people are going to do it.
[00:39:30.000 --> 00:39:35.360]   Yes, if you donate $50, we'll get your name and you'll get to pre-order a bottle.
[00:39:35.360 --> 00:39:38.640]   Someone sent $100 and said, "This is for the zipper."
[00:39:38.640 --> 00:39:39.120]   Oh, okay.
[00:39:39.120 --> 00:39:40.960]   We have a very wealthy audience listening right now.
[00:39:40.960 --> 00:39:42.000]   This is pretty crazy.
[00:39:42.000 --> 00:39:43.600]   Anyway, $50 from Eric.
[00:39:43.600 --> 00:39:45.360]   What is one thing that makes each of you most optimistic?
[00:39:45.360 --> 00:39:47.520]   Nick, get his name so that we can give him a chance to-
[00:39:47.520 --> 00:39:48.800]   Eric Somerville.
[00:39:48.800 --> 00:39:49.040]   Okay.
[00:39:49.040 --> 00:39:52.560]   What is one thing that makes each of you most optimistic and motivated about the rest of
[00:39:52.560 --> 00:39:54.000]   2024 and 2025?
[00:39:54.000 --> 00:39:56.400]   Well, I'll tell you something.
[00:39:56.400 --> 00:40:05.280]   I have my advisory board meeting for my legacy venture funds, and I was going to write this
[00:40:05.280 --> 00:40:05.760]   up.
[00:40:05.760 --> 00:40:10.160]   It's hard actually to post a PDF in an ex post.
[00:40:10.160 --> 00:40:18.720]   You can do it in Substack a lot easier, but this was the first quarter since Q1 of '22
[00:40:18.720 --> 00:40:23.040]   where I have not had a meaningful drawdown in my portfolio.
[00:40:23.040 --> 00:40:24.800]   So I kind of think things have bottomed.
[00:40:24.800 --> 00:40:30.400]   So before I was more optimistic at the beginning of January, not knowing where valuations would
[00:40:30.400 --> 00:40:34.800]   be, because a lot of these valuations in all these private companies are not obviously
[00:40:34.800 --> 00:40:35.360]   set by us.
[00:40:35.360 --> 00:40:40.640]   They're set by third-party firms pricing new rounds or valuation firms that comp it to
[00:40:40.640 --> 00:40:42.240]   the public markets, which are still depressed.
[00:40:42.240 --> 00:40:47.600]   Saks tweeted about this today about, I think, GitLab, which is like a nutty valuation.
[00:40:47.600 --> 00:40:55.760]   But despite all of that, that portfolio, that section of social capital has stabilized.
[00:40:55.760 --> 00:40:58.960]   And that was really heartening for me to see that.
[00:40:58.960 --> 00:41:03.280]   So I'm actually just excited that we're beginning the process of rebounding, which if you think
[00:41:03.280 --> 00:41:06.800]   there's going to be a bunch of cuts in the back half of this year and into next year,
[00:41:06.800 --> 00:41:13.440]   these next 18 months are going to be okay for the private markets.
[00:41:13.440 --> 00:41:17.760]   We tend to price those things forward by that amount, if not more.
[00:41:17.760 --> 00:41:22.400]   So I think that valuations are in a decent place, at least looking at the numbers.
[00:41:22.400 --> 00:41:27.840]   Saks, anything you're looking forward to back half of the year?
[00:41:27.840 --> 00:41:32.240]   Well, I think daddy's coming home to the White House.
[00:41:32.240 --> 00:41:33.760]   That's got me excited.
[00:41:33.760 --> 00:41:41.920]   But on a personal level, I guess I'm excited about working on Glue.
[00:41:41.920 --> 00:41:43.280]   So that's a lot of fun right now.
[00:41:43.280 --> 00:41:52.000]   So we did a limited release behind a waitlist, and we're onboarding,
[00:41:52.000 --> 00:41:54.320]   I don't know, about 40 companies a week now.
[00:41:54.320 --> 00:41:57.440]   And then as we do that, we discover more needs and issues.
[00:41:57.440 --> 00:42:02.240]   We knock off those feature requests, and then we take the next 40.
[00:42:02.240 --> 00:42:06.880]   And so we're just doing this systematically until we can go GA, general availability,
[00:42:06.880 --> 00:42:08.800]   where there'll be no waitlist, everyone can just sign up.
[00:42:08.800 --> 00:42:12.080]   And we're kind of racing towards that date over the next two to three months.
[00:42:12.080 --> 00:42:13.840]   I think that's exciting for me.
[00:42:13.840 --> 00:42:18.880]   Freeberg, anything that you're looking forward to?
[00:42:19.520 --> 00:42:22.880]   I'm looking forward to the election being over so we can stop talking about politics, number one.
[00:42:22.880 --> 00:42:27.200]   Number two is I'm building a business at O'Halo.
[00:42:27.200 --> 00:42:30.800]   I am very excited every day with the progress we make.
[00:42:30.800 --> 00:42:32.960]   We are building incredible products.
[00:42:32.960 --> 00:42:36.720]   We are signing deals with customers and partners.
[00:42:36.720 --> 00:42:40.480]   Every milestone is exciting when you're building something new and novel.
[00:42:40.480 --> 00:42:43.440]   And seeing it work in the market, it's pretty awesome.
[00:42:43.440 --> 00:42:49.760]   So that to me is just like, you know, and there's such a long runway ahead.
[00:42:49.760 --> 00:42:51.360]   And there's all these great problems to solve.
[00:42:51.360 --> 00:42:52.560]   So I love it.
[00:42:52.560 --> 00:42:53.060]   It's great.
[00:42:53.060 --> 00:42:54.800]   Fantastic.
[00:42:54.800 --> 00:42:57.360]   Yeah, I echo Freeberg.
[00:42:57.360 --> 00:43:00.800]   It'll be great for the election to get resolved.
[00:43:00.800 --> 00:43:09.680]   I am super excited about, hmm, am I super excited about back after the year?
[00:43:09.680 --> 00:43:11.760]   I'm excited for ski season again.
[00:43:12.800 --> 00:43:18.160]   And I am super excited about some of the companies we're investing in.
[00:43:18.160 --> 00:43:22.320]   Chamath, to your point, I think the bottoming out has occurred and we're starting to see
[00:43:22.320 --> 00:43:30.640]   a lot of secondary offers come in for shares in some of our Fund 1, 2, and 3 companies,
[00:43:30.640 --> 00:43:32.240]   which are Fund 1 and 2 companies.
[00:43:32.240 --> 00:43:33.920]   Those are the ones that have more time to bake.
[00:43:33.920 --> 00:43:35.440]   So that is actually, I think, a sign.
[00:43:35.440 --> 00:43:36.480]   I don't know if you're starting to see that.
[00:43:36.480 --> 00:43:40.560]   Now, some of them are lowball offers where people are offering half off the peak valuation,
[00:43:40.560 --> 00:43:42.080]   but it's an offer nonetheless.
[00:43:42.080 --> 00:43:46.800]   And there was no bid for, what, 18 months on any of these companies or with the exception
[00:43:46.800 --> 00:43:49.040]   of maybe SpaceX and Stripe.
[00:43:49.040 --> 00:43:50.560]   So yeah, it feels like it's bottoming out.
[00:43:50.560 --> 00:43:52.240]   Well, I don't know.
[00:43:52.240 --> 00:43:55.760]   I mean, so I think there's a huge gap right now between public and private valuations.
[00:43:55.760 --> 00:44:00.720]   So I retweeted Jason Lemkin this morning, which Chamath referred to,
[00:44:00.720 --> 00:44:03.440]   who pointed at GitLab's numbers.
[00:44:03.440 --> 00:44:05.680]   I mean, this is a company that's got $700 million of ARR.
[00:44:05.680 --> 00:44:06.320]   It's public.
[00:44:06.320 --> 00:44:10.000]   It's valued at about $6.9 billion, so 10 times ARR.
[00:44:10.000 --> 00:44:15.600]   Scoring 30% year over year, which means it's going to add over $200 million of ARR over
[00:44:15.600 --> 00:44:16.160]   the next year.
[00:44:16.160 --> 00:44:19.360]   It's got 130% net dollar retention.
[00:44:19.360 --> 00:44:24.400]   So basically, all of its existing customers are expanding about 30% every year, which
[00:44:24.400 --> 00:44:25.040]   is amazing.
[00:44:25.040 --> 00:44:28.960]   Expand that, show more, just for the rest of the stats.
[00:44:28.960 --> 00:44:37.040]   If you click show more, you can see the rest of Jason Lemkin's tweet.
[00:44:37.040 --> 00:44:38.480]   Oh, there it is on the side.
[00:44:38.480 --> 00:44:40.000]   Yeah, anyway, it's actually profitable.
[00:44:40.000 --> 00:44:43.040]   It's generating 20% free cash margins.
[00:44:43.040 --> 00:44:46.960]   And again, this is only worth 10 times ARR.
[00:44:46.960 --> 00:44:52.320]   And then if you look at private markets, the last two companies that I talked to, which
[00:44:52.320 --> 00:44:58.880]   are exciting companies, they're at 10 million of ARR, and they want billion dollar valuations.
[00:44:58.880 --> 00:45:00.720]   100 times.
[00:45:00.720 --> 00:45:01.040]   Yeah.
[00:45:01.040 --> 00:45:02.400]   And I don't know if they're going to get that.
[00:45:02.400 --> 00:45:08.400]   Like, it may land at $800 or $900 million, but still, they want 80, 90, 100 times ARR
[00:45:08.400 --> 00:45:09.360]   valuations.
[00:45:09.360 --> 00:45:13.520]   Now, they're growing a lot faster than 30% year over year, but are they really going
[00:45:13.520 --> 00:45:19.040]   to be growing faster than 30% when they're at $700 million of scale, or even $100 million
[00:45:19.040 --> 00:45:19.760]   of scale?
[00:45:19.760 --> 00:45:20.800]   Pretty hard to do.
[00:45:20.800 --> 00:45:21.760]   It's very hard to do.
[00:45:21.760 --> 00:45:25.520]   So there's still a very big gap between public and private valuations.
[00:45:25.520 --> 00:45:29.040]   Or the question is, which of these would you rather invest in?
[00:45:29.040 --> 00:45:34.400]   I mean, a company that's totally de-risked, it's already public, it's fully liquid, and
[00:45:34.400 --> 00:45:35.600]   it's growing very nicely.
[00:45:35.600 --> 00:45:36.800]   It's weathered the storm.
[00:45:36.800 --> 00:45:38.080]   I don't know.
[00:45:38.080 --> 00:45:46.160]   I mean, buying a basket of pretty fast-growing SaaS companies at 10 times ARR seems like
[00:45:46.160 --> 00:45:48.000]   a fairly attractive strategy to me right now.
[00:45:48.000 --> 00:45:53.760]   We just had our first private equity firm come into one of our companies.
[00:45:53.760 --> 00:45:56.560]   Actually, it's happened twice now in the last year.
[00:45:57.120 --> 00:46:02.240]   Private equity kind of crossover funds, asking to buy a percentage of a company and kind
[00:46:02.240 --> 00:46:03.040]   of take it over.
[00:46:03.040 --> 00:46:09.680]   So I think that's a sign, perhaps, of a healthy market where you have many options of what
[00:46:09.680 --> 00:46:11.280]   to do with your funds.
[00:46:11.280 --> 00:46:16.000]   And some venture funds are choosing to maybe buy out companies and act more like private
[00:46:16.000 --> 00:46:19.840]   equity because they see maybe a better opportunity.
[00:46:19.840 --> 00:46:22.000]   But there's a lot of venture money out there still, huh, Zach?
[00:46:22.000 --> 00:46:24.160]   So that's pushing the bid up.
[00:46:24.160 --> 00:46:30.080]   People are looking to find that 10 million, 20 million ARR company and place a bet.
[00:46:30.080 --> 00:46:31.200]   There's just too many funds.
[00:46:31.200 --> 00:46:33.040]   Let's take another question, Nick, from the audience.
[00:46:33.040 --> 00:46:36.960]   And what's the record right now in terms of donations?
[00:46:36.960 --> 00:46:38.480]   Because this is all going to go to charity, folks.
[00:46:38.480 --> 00:46:38.960]   Dead serious.
[00:46:38.960 --> 00:46:40.560]   We'll figure out the charity later.
[00:46:40.560 --> 00:46:42.000]   I think 200.
[00:46:42.000 --> 00:46:45.760]   There are some coming in in different kinds of currencies that I have to convert manually.
[00:46:45.760 --> 00:46:48.160]   But right now for USD, I see 200.
[00:46:48.160 --> 00:46:50.480]   From Rahil, he said, "I don't want a zipper.
[00:46:50.480 --> 00:46:52.000]   I'm about to deal sleds.
[00:46:52.000 --> 00:46:54.640]   Respectfully, how much for the Laura Piana chamath?"
[00:46:54.640 --> 00:46:55.040]   $200.
[00:46:55.040 --> 00:46:59.920]   No, this is a sweater that I got from the team at Louis Vuitton because they asked me
[00:46:59.920 --> 00:47:00.800]   to wear it.
[00:47:00.800 --> 00:47:01.120]   I don't know.
[00:47:01.120 --> 00:47:02.640]   It's probably a couple thousand bucks.
[00:47:02.640 --> 00:47:02.960]   OK.
[00:47:02.960 --> 00:47:05.360]   Here's a question from Aditya, $50.
[00:47:05.360 --> 00:47:11.200]   "At the level of money and power you all have as a group, how do you think about your
[00:47:11.200 --> 00:47:12.320]   legacies, if at all?"
[00:47:12.320 --> 00:47:15.840]   I don't.
[00:47:17.920 --> 00:47:22.080]   And I think that that is a really, really terrible way to live your life.
[00:47:22.080 --> 00:47:29.040]   There have been in humanity about 100 billion people that have ever lived.
[00:47:29.040 --> 00:47:36.800]   And if you took a sampling of a random thousand people and said, "Who do you remember from
[00:47:36.800 --> 00:47:37.280]   history?"
[00:47:37.280 --> 00:47:43.200]   the overlap of the same five or 10 people would be almost universal, which is to say
[00:47:43.200 --> 00:47:47.200]   that 100 billion minus five people are not going to be remembered.
[00:47:47.920 --> 00:47:52.960]   And so this goal of trying to be remembered, I'm not sure accomplishes much.
[00:47:52.960 --> 00:47:58.800]   I think what's important is just to make sure that you don't have a lot of regrets and that
[00:47:58.800 --> 00:48:04.320]   when you have the chance to try something fun or try something memorable or challenging
[00:48:04.320 --> 00:48:08.960]   because you thought you would get something out of it, you did it, as opposed to, "How
[00:48:08.960 --> 00:48:11.280]   will people judge me for this and how will I be written?"
[00:48:11.280 --> 00:48:13.360]   I just think it's a legacy.
[00:48:13.360 --> 00:48:15.440]   Forget it, man.
[00:48:15.440 --> 00:48:17.760]   Your great grandkids won't know you.
[00:48:17.760 --> 00:48:22.400]   There's a decent chance your grandkids may not even know you.
[00:48:22.400 --> 00:48:26.720]   You got to live to be happy today.
[00:48:26.720 --> 00:48:28.640]   That's my personal view.
[00:48:28.640 --> 00:48:30.400]   I just think this whole legacy thing is nuts.
[00:48:30.400 --> 00:48:31.440]   All right.
[00:48:31.440 --> 00:48:32.960]   8,000 people are watching live.
[00:48:32.960 --> 00:48:34.080]   2,800, thumbs up.
[00:48:34.080 --> 00:48:35.520]   Give us a thumbs up right now.
[00:48:35.520 --> 00:48:40.560]   Let's get those thumbs ups to 4,000 and let's break the algorithm.
[00:48:40.560 --> 00:48:42.640]   Freebird, are you thinking about your legacy much these days?
[00:48:42.640 --> 00:48:43.600]   No.
[00:48:44.560 --> 00:48:47.760]   Sax, any thoughts on legacy?
[00:48:47.760 --> 00:48:48.880]   No.
[00:48:48.880 --> 00:48:49.920]   I mean, honestly, it's not...
[00:48:49.920 --> 00:48:51.920]   I agree with Jamath.
[00:48:51.920 --> 00:48:55.760]   It seems like it's not something you should be preoccupied with too much.
[00:48:55.760 --> 00:48:57.280]   Yeah.
[00:48:57.280 --> 00:48:59.280]   I mean, look, you want your kids to be happy.
[00:48:59.280 --> 00:49:00.880]   You want to have success in business.
[00:49:00.880 --> 00:49:04.640]   There it is.
[00:49:04.640 --> 00:49:05.360]   And then it's done.
[00:49:05.360 --> 00:49:07.200]   You're done and then you die.
[00:49:07.200 --> 00:49:11.600]   So legacy, who cares about legacy?
[00:49:11.600 --> 00:49:12.640]   What does that mean?
[00:49:12.640 --> 00:49:17.200]   Other people's judgment of you, like some article or book or your Wikipedia link?
[00:49:17.200 --> 00:49:17.920]   Who cares?
[00:49:17.920 --> 00:49:19.040]   Yeah.
[00:49:19.040 --> 00:49:21.120]   I mean, I get it as a young person.
[00:49:21.120 --> 00:49:24.160]   Maybe you'd think you might project into older people.
[00:49:24.160 --> 00:49:25.920]   Like they're thinking about their legacy.
[00:49:25.920 --> 00:49:31.840]   I think about having great experiences and optimizing great experiences with my family,
[00:49:31.840 --> 00:49:36.960]   my friends, and then business partners who I kind of optimize for family and friends.
[00:49:36.960 --> 00:49:39.600]   And I had two great experiences this past week.
[00:49:39.600 --> 00:49:47.760]   I went to Disney on Saturday with our friend Jason Chang and went to Club 33,
[00:49:47.760 --> 00:49:50.960]   which was like a wonderful, fun experience to go in that private club.
[00:49:50.960 --> 00:49:55.200]   And then I pinched myself because I could afford to have a VIP tour guide.
[00:49:55.200 --> 00:49:56.640]   I don't know if you've ever done that, guys,
[00:49:56.640 --> 00:50:01.280]   where you just get taken around the park and you jump on the rides.
[00:50:01.280 --> 00:50:03.200]   And man, that was a lifetime experience.
[00:50:03.200 --> 00:50:04.480]   I've been to Disney once.
[00:50:05.280 --> 00:50:07.360]   I stayed at the California Lodge.
[00:50:07.360 --> 00:50:07.840]   Nice.
[00:50:07.840 --> 00:50:11.520]   I think it must have been what hell looks like.
[00:50:11.520 --> 00:50:13.200]   And I've never been back since.
[00:50:13.200 --> 00:50:14.960]   Did you have the VIP tour or not?
[00:50:14.960 --> 00:50:15.200]   Sure.
[00:50:15.200 --> 00:50:16.080]   I had everything.
[00:50:16.080 --> 00:50:19.040]   And I thought it was just the most horrendous, horrible experience ever.
[00:50:19.040 --> 00:50:20.320]   My kids loved it.
[00:50:20.320 --> 00:50:22.080]   We did so many rides and it's so much fun.
[00:50:22.080 --> 00:50:24.000]   But maybe they're of the age.
[00:50:24.000 --> 00:50:25.360]   All right.
[00:50:25.360 --> 00:50:28.240]   Another question from our amazing audience, your donations.
[00:50:28.240 --> 00:50:32.320]   Anybody who donates here is going to have their donation given to charity.
[00:50:33.040 --> 00:50:35.040]   $50 from Tommy Gufano.
[00:50:35.040 --> 00:50:37.680]   Oh, Tom, Big Tommy?
[00:50:37.680 --> 00:50:38.720]   Yeah, Big Tom.
[00:50:38.720 --> 00:50:43.920]   Do you see an opportunity for the intersection of blockchains and AI?
[00:50:43.920 --> 00:50:46.000]   I also want a pre-order for Tequila, please.
[00:50:46.000 --> 00:50:46.560]   Smiley face.
[00:50:46.560 --> 00:50:48.560]   Crypto is a disaster.
[00:50:48.560 --> 00:50:49.200]   Next question.
[00:50:49.200 --> 00:50:52.720]   I think that if you take...
[00:50:52.720 --> 00:50:59.840]   Forget the currencies for a second, but what is a blockchain?
[00:50:59.840 --> 00:51:03.600]   It's an incredibly accurate, highly distributed state machine.
[00:51:03.600 --> 00:51:10.240]   That could be valuable if you wanted to do highly distributed inference.
[00:51:10.240 --> 00:51:14.720]   So if these AI applications get to scale, I think one of the most important problems
[00:51:14.720 --> 00:51:18.400]   is around business risk and continuity.
[00:51:18.400 --> 00:51:22.320]   Because if all those models run on AWS and somehow you can take down AWS,
[00:51:22.320 --> 00:51:24.160]   now all of a sudden your app isn't available.
[00:51:24.160 --> 00:51:27.360]   That's a problem for companies.
[00:51:27.360 --> 00:51:30.480]   It's also a problem for countries that may want to use these clouds.
[00:51:30.480 --> 00:51:34.160]   So then all of a sudden you say, "Well, I need to have it distributed."
[00:51:34.160 --> 00:51:37.200]   Well, then if you can only distribute it across three hyperscalers,
[00:51:37.200 --> 00:51:39.200]   that's not really a lot of risk management.
[00:51:39.200 --> 00:51:42.640]   So then you go to a place which is like it needs to be highly distributed.
[00:51:42.640 --> 00:51:45.920]   But if you run inference in a highly distributed compute environment,
[00:51:45.920 --> 00:51:50.080]   then all of a sudden you need to have an extremely accurate state machine
[00:51:50.080 --> 00:51:52.320]   that can then thread all these things back together.
[00:51:52.320 --> 00:51:54.480]   So that's the intersection of AI and crypto.
[00:51:54.480 --> 00:51:56.160]   That's probably the most logical,
[00:51:56.160 --> 00:51:58.640]   but there hasn't been any real attempts to figure that out yet.
[00:51:58.640 --> 00:52:02.240]   But if there was such a project that did that,
[00:52:02.240 --> 00:52:07.520]   now then it creates a bunch of other issues around like NVIDIA lock-in and stuff,
[00:52:07.520 --> 00:52:08.720]   which you also can't support.
[00:52:08.720 --> 00:52:12.000]   You'd need to be able to extract it so it could run on CPUs in a really powerful way
[00:52:12.000 --> 00:52:14.000]   or other custom silicon.
[00:52:14.000 --> 00:52:16.240]   But that would be my answer to that.
[00:52:16.240 --> 00:52:20.880]   Well, one of the problems that crypto solves with respect to AI
[00:52:20.880 --> 00:52:26.320]   is helping you know what's authentic or real versus what's fake.
[00:52:26.320 --> 00:52:29.920]   Because with AI, you're not going to know what's real anymore.
[00:52:29.920 --> 00:52:33.840]   I mean, AI can create a perfect forgery of everything,
[00:52:33.840 --> 00:52:35.120]   or we'll soon be at that point.
[00:52:35.120 --> 00:52:37.440]   So crypto will be helpful in that regard.
[00:52:37.440 --> 00:52:40.880]   I think crypto will be the way that AIs pay each other.
[00:52:40.880 --> 00:52:43.280]   Clearly, they're going to pay each other in some sort of analog way.
[00:52:43.280 --> 00:52:46.240]   Balaji has some really interesting thoughts about this.
[00:52:46.240 --> 00:52:48.320]   I'd refer you to some of his tweets about it.
[00:52:49.680 --> 00:52:54.160]   Yeah, I'm just hoping we see a great regulatory environment
[00:52:54.160 --> 00:52:57.920]   for these crypto projects in the coming years.
[00:52:57.920 --> 00:53:01.600]   That might be one of Trump's most brilliant maneuvers
[00:53:01.600 --> 00:53:05.600]   in this past election cycle is to pick up the mantle of regulation.
[00:53:05.600 --> 00:53:08.720]   My gosh, Nick, how many bottles of tequila have we just pre-sold here?
[00:53:08.720 --> 00:53:09.840]   This is incredible.
[00:53:09.840 --> 00:53:10.400]   Quite a few.
[00:53:10.400 --> 00:53:12.720]   Well, what's the demand?
[00:53:12.720 --> 00:53:13.920]   How many do people want?
[00:53:13.920 --> 00:53:15.200]   There's like tens of bottles.
[00:53:15.200 --> 00:53:16.320]   You've sold tens of bottles.
[00:53:16.320 --> 00:53:19.200]   You've sold almost two dozen.
[00:53:19.200 --> 00:53:22.560]   Well, I mean, I'm sorry to tell people this, but it's going to be very limited.
[00:53:22.560 --> 00:53:26.080]   I think we're only going to produce about 1,000 cases, which is 6,000 bottles.
[00:53:26.080 --> 00:53:30.000]   8,000 tops, I would say, because there's only so much
[00:53:30.000 --> 00:53:38.480]   five-year-old juice that we can get for the drink, because normally tequila aren't aged.
[00:53:38.480 --> 00:53:42.800]   Well, these folks are taking the time to show up, and they're donating.
[00:53:42.800 --> 00:53:46.640]   But we're going to get first shot attendees at the Olin Summit.
[00:53:46.640 --> 00:53:47.440]   So we're going to unveil it.
[00:53:47.440 --> 00:53:48.880]   And the people that just donated $100.
[00:53:48.880 --> 00:53:51.200]   And the people who just donated $50 or more, I guess.
[00:53:51.200 --> 00:53:52.240]   All right.
[00:53:52.240 --> 00:53:53.360]   Give us another question, Nick.
[00:53:53.360 --> 00:53:54.560]   Let's keep this train moving here.
[00:53:54.560 --> 00:53:56.400]   We've got time for maybe two more questions here.
[00:53:56.400 --> 00:53:56.640]   Okay.
[00:53:56.640 --> 00:53:57.120]   This question is from-
[00:53:57.120 --> 00:53:58.880]   Two more questions on the live stream.
[00:53:58.880 --> 00:54:01.440]   Do I pay on chowdery?
[00:54:01.440 --> 00:54:05.280]   What is one book you would recommend that transformed how you think $100?
[00:54:05.280 --> 00:54:10.880]   I'm going to search for meaning, Victor Frank.
[00:54:10.880 --> 00:54:15.680]   Liars, Poker, Michael Lewis.
[00:54:16.160 --> 00:54:20.480]   "Transformed how I think."
[00:54:20.480 --> 00:54:21.280]   That's a tough one.
[00:54:21.280 --> 00:54:24.960]   I don't know.
[00:54:24.960 --> 00:54:26.720]   I may have to come back on that one.
[00:54:26.720 --> 00:54:30.640]   Somebody in the chat mentioned Ayn Rand.
[00:54:30.640 --> 00:54:35.680]   I did like Atlas Shrugged, but I don't know if it transformed my thinking.
[00:54:35.680 --> 00:54:36.240]   The concept's a bad writer.
[00:54:36.240 --> 00:54:38.640]   Good concepts, but not a great writer.
[00:54:38.640 --> 00:54:40.000]   The prose is so brutal.
[00:54:40.000 --> 00:54:41.280]   The prose is brutal.
[00:54:41.280 --> 00:54:42.880]   I totally agree with you.
[00:54:44.080 --> 00:54:46.160]   I mean, make it a short story.
[00:54:46.160 --> 00:54:48.560]   Freeberg, you got a book that transformed your life?
[00:54:48.560 --> 00:54:53.760]   I put it in the Summit gift bag, I think, last year.
[00:54:53.760 --> 00:55:00.720]   Zen Mind, Beginner's Mind by Suzuki, which is basically a compilation of the Dharma lectures
[00:55:00.720 --> 00:55:03.600]   by the founder of the SF Zen Buddhism Institute.
[00:55:03.600 --> 00:55:12.080]   All the lectures are all about sitting Zen, like Buddhist meditation, sitting Zen meditation,
[00:55:12.080 --> 00:55:18.400]   but the thinking is so broadly applicable around how the human brain has a tendency
[00:55:18.400 --> 00:55:22.640]   to always seek things, this continuing drive of desire.
[00:55:22.640 --> 00:55:27.360]   And if you can let go of that drive of desire, much of your perception of reality changes.
[00:55:27.360 --> 00:55:31.280]   And the second is dualistic thinking, where you think about everything as one or the other.
[00:55:31.280 --> 00:55:36.560]   Everything in the universe, everything that we think about is in or out, this or good or bad.
[00:55:37.280 --> 00:55:44.000]   And I think, for me, changing one's point of view on those two aspects of how the human
[00:55:44.000 --> 00:55:50.720]   brain kind of natively associates the universe within which we live can have a huge impact
[00:55:50.720 --> 00:55:54.560]   in decisions and emotion and so on in life.
[00:55:54.560 --> 00:55:56.960]   So it's a very important book for me.
[00:55:56.960 --> 00:56:00.160]   And you can read it a million times and always get something new out of it.
[00:56:00.160 --> 00:56:02.800]   It's just a copy of his lectures.
[00:56:02.800 --> 00:56:03.760]   All right.
[00:56:03.760 --> 00:56:05.040]   Give us another question, Nick.
[00:56:05.040 --> 00:56:06.560]   We have a new high donation.
[00:56:06.560 --> 00:56:07.360]   Rapid fire.
[00:56:07.360 --> 00:56:07.600]   Oh.
[00:56:07.600 --> 00:56:08.800]   Mark Hossler, $200.
[00:56:08.800 --> 00:56:10.080]   Oh, okay.
[00:56:10.080 --> 00:56:14.000]   On Rogan's podcast, Neil deGrasse Tyson made predictions for 2050.
[00:56:14.000 --> 00:56:17.280]   Examples, the human space program will fully transition to a space industry
[00:56:17.280 --> 00:56:19.840]   supported not by tax dollars, but by tourism.
[00:56:19.840 --> 00:56:22.320]   We develop a perfect antiviral serum and cure cancer.
[00:56:22.320 --> 00:56:25.360]   Do you have any predictions for 2050 plus tequila?
[00:56:25.360 --> 00:56:28.560]   Donations for 2050?
[00:56:28.560 --> 00:56:30.240]   Well, we'll definitely have-
[00:56:30.240 --> 00:56:31.760]   Predictions for 2050.
[00:56:31.760 --> 00:56:32.160]   Yeah.
[00:56:32.160 --> 00:56:34.960]   Artificial superintelligence will be here long before that.
[00:56:34.960 --> 00:56:40.800]   And then the problems and the questions it answers are going to be mind-blowing.
[00:56:40.800 --> 00:56:45.600]   We might figure out the origins of the universe by that time.
[00:56:45.600 --> 00:56:48.240]   We might figure out consciousness.
[00:56:48.240 --> 00:56:51.200]   I mean, there are questions that humans have a really hard time with.
[00:56:51.200 --> 00:56:53.520]   So I would say artificial superintelligence.
[00:56:53.520 --> 00:56:58.640]   Anybody else have a prediction for 2050?
[00:56:58.640 --> 00:57:01.120]   That's like a 25-year prediction window.
[00:57:01.120 --> 00:57:05.680]   Yeah.
[00:57:05.680 --> 00:57:08.080]   Nobody has any predictions?
[00:57:08.080 --> 00:57:09.280]   The guy just spent 200 bucks.
[00:57:09.280 --> 00:57:11.680]   Nobody could give him a prediction for 2050.
[00:57:11.680 --> 00:57:12.560]   There's a prediction.
[00:57:12.560 --> 00:57:17.840]   Your $200 would have been worth 18 million if you put it in the S&P.
[00:57:17.840 --> 00:57:21.760]   When you get out that far, you're basically just talking about science fiction, right?
[00:57:21.760 --> 00:57:22.160]   I mean-
[00:57:22.160 --> 00:57:28.160]   Well, I mean, if we went back to 2000, Sax, if you said we'd have flying cars in 2000,
[00:57:28.160 --> 00:57:28.880]   you would be right.
[00:57:29.520 --> 00:57:33.120]   I mean, Archer and Joby are running sorties right now.
[00:57:33.120 --> 00:57:35.120]   Yeah, but that's not what people meant when they said that.
[00:57:35.120 --> 00:57:41.360]   There's like prototypes, but this hasn't transformed society.
[00:57:41.360 --> 00:57:44.880]   They said we'd have self-driving cars and Waymo just opened up.
[00:57:44.880 --> 00:57:47.120]   We should have companion robots by 2050.
[00:57:47.120 --> 00:57:48.880]   Well, say Marchmont.
[00:57:48.880 --> 00:57:54.320]   Like a robot that works around the house that does kind of the 80% of the human toil.
[00:57:54.320 --> 00:57:56.720]   I actually think that that could be really-
[00:57:56.720 --> 00:57:58.160]   Like the Hawk 2A robot?
[00:57:58.160 --> 00:58:00.720]   You think we'll have a Hawk 2A robot?
[00:58:00.720 --> 00:58:01.280]   No, I mean more like the-
[00:58:01.280 --> 00:58:02.480]   The Hawk 2A 2000?
[00:58:02.480 --> 00:58:04.800]   The Hawk 2A 2000.
[00:58:04.800 --> 00:58:10.320]   I mean more like it does the gardening, it does the dishes, it does the vacuum cleaning.
[00:58:10.320 --> 00:58:11.360]   It does the Hawk 2A.
[00:58:11.360 --> 00:58:13.440]   It does the Hawk 2A.
[00:58:13.440 --> 00:58:13.840]   That's like a really-
[00:58:13.840 --> 00:58:14.800]   Freedberg just ran.
[00:58:14.800 --> 00:58:16.320]   Freedberg ran for it.
[00:58:16.320 --> 00:58:17.600]   He can't even be on air for this.
[00:58:17.600 --> 00:58:21.440]   That's a huge unlock for humans.
[00:58:21.440 --> 00:58:22.960]   My dogs want it to go out.
[00:58:25.440 --> 00:58:29.440]   There are two key technologies that I think are long range that I've talked a lot about.
[00:58:29.440 --> 00:58:36.480]   We had a conversation last year on the summit about fusion, fusion-generated power, which
[00:58:36.480 --> 00:58:43.200]   that's kind of a timeframe where we should be kind of, if it works right at production scale,
[00:58:43.200 --> 00:58:44.720]   seeing production steady state output.
[00:58:44.720 --> 00:58:50.240]   And the other one is aging reversal, which there's a tremendous amount of research.
[00:58:50.240 --> 00:58:52.480]   And we will be talking about at the All In Summit this year.
[00:58:52.480 --> 00:58:56.720]   We're going to have an amazing conversation about the research that's gone on in reversing
[00:58:56.720 --> 00:58:59.840]   aging, which I think we've talked about in the past is fundamentally driven.
[00:58:59.840 --> 00:59:06.240]   It looks like from epigenetic errors in cells, so not DNA errors like was originally theorized,
[00:59:06.240 --> 00:59:11.600]   but like the molecules that sit on top of the DNA that turn genes on and off in a cell.
[00:59:11.600 --> 00:59:15.680]   And when that becomes unbalanced, the cells start to dysfunction, organs dysfunction.
[00:59:15.680 --> 00:59:17.920]   And that is the fundamental driver of aging.
[00:59:17.920 --> 00:59:22.080]   And there's now a set of systems and molecules that we can apply to cells to reverse that and
[00:59:22.080 --> 00:59:24.880]   actually reset the molecules that sit on the DNA.
[00:59:24.880 --> 00:59:26.480]   So these cells start to become young again.
[00:59:26.480 --> 00:59:31.520]   And there are many billions of dollars that have gone into this research.
[00:59:31.520 --> 00:59:37.520]   And what I've heard in non-public discussions is that this is like looking like it could
[00:59:37.520 --> 00:59:38.400]   be very real.
[00:59:38.400 --> 00:59:39.520]   That would be really amazing.
[00:59:39.520 --> 00:59:43.040]   So I imagine taking a fap fap to the e-drug.
[00:59:43.040 --> 00:59:44.080]   Yeah, that's in our window.
[00:59:44.080 --> 00:59:48.400]   One of the things that I'll also just say on this is like, I think that there's this
[00:59:48.400 --> 00:59:55.440]   intersection between AI and biology that unlocks like a really like incredible paradigm for
[00:59:55.440 --> 01:00:02.320]   how humans can create things and make things on earth where you could say, I want a molecule
[01:00:02.320 --> 01:00:07.680]   or protein that will go in and find this specific cancer cell for me.
[01:00:07.680 --> 01:00:12.720]   And then you can actually render that protein, apply it to a patient, and it can immediately
[01:00:12.720 --> 01:00:17.360]   wipe out that particular target where the AI resolves to the molecule that's needed
[01:00:17.360 --> 01:00:20.880]   or the genetic structure that's needed to make, let's say, a plant that can grow on
[01:00:20.880 --> 01:00:21.520]   Mars.
[01:00:21.520 --> 01:00:23.120]   I want to make a plant that can grow on Mars.
[01:00:23.120 --> 01:00:26.000]   The AI renders the genome of that plant.
[01:00:26.000 --> 01:00:29.440]   You produce it, you ship it to Mars, and you start growing plants in the really weird
[01:00:29.440 --> 01:00:31.040]   environment on Mars' surface.
[01:00:31.040 --> 01:00:35.520]   So I think that by 2050, we have these kind of intersections that enable some of that
[01:00:35.520 --> 01:00:37.200]   stuff that could be really powerful.
[01:00:37.200 --> 01:00:40.960]   Hey, give us that McHale question if you can.
[01:00:40.960 --> 01:00:41.920]   That came in for a Hyundai.
[01:00:41.920 --> 01:00:42.480]   That's a good one.
[01:00:42.480 --> 01:00:46.080]   I don't see it.
[01:00:46.080 --> 01:00:47.600]   I just had a $200 question for you, though.
[01:00:47.600 --> 01:00:48.880]   Oh, give us a $200 question.
[01:00:48.880 --> 01:00:51.280]   McHale.
[01:00:51.280 --> 01:00:52.320]   Why are we getting paid money?
[01:00:52.320 --> 01:00:52.560]   I don't know.
[01:00:52.560 --> 01:00:53.680]   How does this get turned on?
[01:00:53.680 --> 01:00:56.800]   It's just like a feature of YouTube?
[01:00:56.800 --> 01:00:58.480]   Yeah, you can do two things in YouTube.
[01:00:58.480 --> 01:01:03.680]   You can do a super chat where you give money, and your chat gets highlighted in the top
[01:01:03.680 --> 01:01:04.720]   of the chat.
[01:01:04.720 --> 01:01:06.560]   And then there's memberships.
[01:01:06.560 --> 01:01:10.640]   So we could, for the next one, turn on memberships, and people will pay $1 a month, $10 a month
[01:01:10.640 --> 01:01:11.280]   to be a member.
[01:01:11.280 --> 01:01:12.000]   It's kind of fun.
[01:01:12.000 --> 01:01:13.360]   It gives you extra features.
[01:01:13.360 --> 01:01:15.280]   All right, I got McHale's.
[01:01:15.280 --> 01:01:16.080]   Oh, OK, good.
[01:01:16.080 --> 01:01:17.760]   $100 from McHale.
[01:01:17.760 --> 01:01:18.720]   I am reading the comments.
[01:01:18.720 --> 01:01:19.520]   That's why I'm laughing.
[01:01:19.520 --> 01:01:24.160]   What are some of the ethical dilemmas that you faced in your careers, and how have you
[01:01:24.160 --> 01:01:24.880]   overcome them?
[01:01:24.880 --> 01:01:28.880]   What about some ethical dilemmas you've seen from the startups you've invested in?
[01:01:28.880 --> 01:01:37.840]   Yeah, I've had maybe two of these situations happen where people were, let's say, I'll
[01:01:37.840 --> 01:01:42.080]   just make this a composite, but over 400 investments we've done.
[01:01:42.080 --> 01:01:50.800]   We had maybe two instances where people were doing things that would be illegal, and we
[01:01:50.800 --> 01:01:57.040]   had to, in both cases, I had to sit with the founders and said, listen, I can't be on the
[01:01:57.040 --> 01:01:58.960]   board of this company if you're going to do this kind of things.
[01:01:58.960 --> 01:02:03.840]   And in both cases, I gave them the option to return our money at par, last valuation.
[01:02:03.840 --> 01:02:08.160]   And yeah, I resigned from the board and took our money back.
[01:02:08.160 --> 01:02:10.240]   So it does happen sometimes.
[01:02:10.240 --> 01:02:17.200]   You got to be very careful because somebody does something where they're not upfront or
[01:02:17.200 --> 01:02:19.840]   completely honest, and they sell shares in a company.
[01:02:19.840 --> 01:02:21.040]   It's called securities fraud.
[01:02:21.040 --> 01:02:24.160]   You got to be really vigilant, especially if you found out about it and you're on the
[01:02:24.160 --> 01:02:24.660]   board.
[01:02:24.660 --> 01:02:30.880]   Somebody asked whether Chamath Hawke-- how do you pronounce this?
[01:02:30.880 --> 01:02:33.120]   You say Hawke Tua.
[01:02:33.120 --> 01:02:34.400]   I think it's Tua.
[01:02:34.400 --> 01:02:36.640]   Hawke Tua, your wine.
[01:02:36.640 --> 01:02:40.320]   Chamath has Hawke Tua'd some of my wine right on the damn floor.
[01:02:40.320 --> 01:02:42.560]   He did Hawke Tua it on the floor.
[01:02:42.560 --> 01:02:43.040]   I saw it.
[01:02:43.040 --> 01:02:47.200]   If he doesn't think the wine that's been served is good enough, he'll just dump it on the
[01:02:47.200 --> 01:02:48.080]   floor.
[01:02:48.080 --> 01:02:50.640]   He took the glass and he poured it out on the floor.
[01:02:50.640 --> 01:02:54.400]   It was a nice aged burgundy, too, if I remember.
[01:02:54.400 --> 01:02:57.760]   He Hawke Tua'd it all over a marble floor.
[01:02:57.760 --> 01:03:01.200]   It was, I mean, it was the height of--
[01:03:01.200 --> 01:03:02.240]   Chamath's not drinking this shit.
[01:03:02.240 --> 01:03:03.540]   Wow.
[01:03:03.540 --> 01:03:06.560]   There's actually a question about wine, Chamath, if you want to answer it.
[01:03:07.040 --> 01:03:08.000]   Oh, here we go.
[01:03:08.000 --> 01:03:12.320]   John Knab asked, and this was a pre-submitted one, "I'd like to know what Old World and
[01:03:12.320 --> 01:03:14.720]   New World wines are each of the besties favorites?"
[01:03:14.720 --> 01:03:15.760]   And then someone else asked--
[01:03:15.760 --> 01:03:16.800]   Great question.
[01:03:16.800 --> 01:03:20.960]   --how do you know what a good glass of wine is or something like that?
[01:03:20.960 --> 01:03:24.160]   Well, I really think it comes down to your own personal taste.
[01:03:24.160 --> 01:03:25.760]   I'll give you a perfect example of this.
[01:03:25.760 --> 01:03:34.480]   Merlot gets shat upon a lot by fake intellectuals, but it creates some of the best wines possible.
[01:03:34.480 --> 01:03:39.600]   As an example, if you were to go into the Old World, there's something you've probably
[01:03:39.600 --> 01:03:41.200]   heard of called Chateau Petrus.
[01:03:41.200 --> 01:03:44.800]   Those are like $5,000 to $50,000 a bottle.
[01:03:44.800 --> 01:03:52.960]   But if you go to Italian old-growth Merlot, like Masetto, that's probably $1,000 to $3,000.
[01:03:52.960 --> 01:03:56.480]   So enormously, just an order of magnitude cheaper.
[01:03:56.480 --> 01:04:00.000]   It is incredible and pound-for-pound better than Petrus.
[01:04:00.000 --> 01:04:05.520]   And if you put it against it in a blind tasting, you would prefer the Masetto over the Petrus.
[01:04:05.520 --> 01:04:07.280]   So what does that mean?
[01:04:07.280 --> 01:04:10.480]   I think it means that there is a lot of people that just want to demonstrate they can spend
[01:04:10.480 --> 01:04:14.800]   money, and that's dumb, because I think there's just a lot of incredible wine that's much
[01:04:14.800 --> 01:04:15.440]   cheaper than that.
[01:04:15.440 --> 01:04:20.560]   Then if you go all the way down to like, for example, last summer, I gave these guys some
[01:04:20.560 --> 01:04:26.960]   of this, but the most incredible white burgundy I have found is Charles Boussaint, 80 euros
[01:04:26.960 --> 01:04:29.520]   a bottle, absolutely incredible.
[01:04:29.520 --> 01:04:32.160]   And it'll run circles around PYCM.
[01:04:32.160 --> 01:04:35.600]   It's running circles around Cocherie, which are thousands of dollars.
[01:04:35.600 --> 01:04:41.120]   So you've got to try a lot, and you've got to go with what tastes good, and who cares
[01:04:41.120 --> 01:04:42.320]   what anybody thinks.
[01:04:42.320 --> 01:04:46.560]   By the way, I will pre-reveal this, but I think I told you guys this, but Joshua and
[01:04:46.560 --> 01:04:49.600]   I, so my sommelier and I, we just filed for a liquor license.
[01:04:49.600 --> 01:04:54.480]   And so when we get it, we will be allowed to buy wine wholesale.
[01:04:55.760 --> 01:05:02.800]   So we are becoming, because honestly, the wine is so marked up, and we were like, well,
[01:05:02.800 --> 01:05:06.320]   if we have our own liquor license, and we have our own storefront, so we got a storefront,
[01:05:06.320 --> 01:05:10.080]   we're going to be able to buy and import wine at wholesale prices.
[01:05:10.080 --> 01:05:13.520]   What's your favorite five to $10 bottle, Chema?
[01:05:13.520 --> 01:05:16.800]   Like when you go to Trader Joe's, what do you really like to explore?
[01:05:16.800 --> 01:05:18.640]   I don't go to Trader Joe's, it's off-brand.
[01:05:18.640 --> 01:05:19.440]   I will never go there.
[01:05:19.440 --> 01:05:24.880]   No, but you have a, you already gave an 80 euro bottle, so it's a $100 bottle.
[01:05:24.880 --> 01:05:26.240]   $80, $80 bottle.
[01:05:26.240 --> 01:05:29.840]   If you can find a Charles Busson white burgundy, it's incredible.
[01:05:29.840 --> 01:05:32.720]   Pallmeyer is a couple of hundred bucks, consistently excellent.
[01:05:32.720 --> 01:05:40.400]   In the new world, if you want to spend sort of between that five to 700, Colgan, Sloan,
[01:05:40.400 --> 01:05:42.080]   Bond, all these are great.
[01:05:42.080 --> 01:05:42.960]   A hundred acres.
[01:05:42.960 --> 01:05:43.600]   A hundred acres.
[01:05:43.600 --> 01:05:47.280]   A hundred acres is probably my favorite Napa wine, and it's not the most expensive.
[01:05:47.280 --> 01:05:51.440]   I mean, it's expensive to be sure, but it's not in the Harlan category or Screaming Eagle.
[01:05:51.440 --> 01:05:52.800]   Harlan's like a thousand bucks a bottle.
[01:05:52.800 --> 01:05:53.440]   Harlan's a thousand.
[01:05:53.440 --> 01:05:56.080]   And Screaming Eagle's like two or three thousand.
[01:05:56.080 --> 01:05:59.520]   A hundred acres is like 500 bucks, and I think it's the best Napa wine.
[01:05:59.520 --> 01:06:02.640]   And I like Colgan too, and I like Schrader as well.
[01:06:02.640 --> 01:06:03.120]   Schrader's excellent.
[01:06:03.120 --> 01:06:12.480]   There's a guy named Sean Thackeray, who is a kind of, he passed away two years ago, I think.
[01:06:12.480 --> 01:06:13.920]   Yeah, two years ago.
[01:06:13.920 --> 01:06:20.560]   He made all these crazy California wines, but he had this theory that it wasn't just about
[01:06:20.560 --> 01:06:26.560]   representing the varietal or representing the particular plot, but he really wanted
[01:06:26.560 --> 01:06:28.240]   to create a story and an essence from the wine.
[01:06:28.240 --> 01:06:30.720]   So every year he had a name for the wine.
[01:06:30.720 --> 01:06:34.080]   He created a custom label, and he would go out and he would source grapes, and he would
[01:06:34.080 --> 01:06:39.280]   test all the grapes at different vineyards, and he would do like a multivariety wine that
[01:06:39.280 --> 01:06:41.520]   he never kind of disclosed what was in it.
[01:06:41.520 --> 01:06:46.480]   And then he threw out all of the conventional French wisdom on how do you make good wine,
[01:06:46.480 --> 01:06:50.480]   and he went back to all of these Franciscan monks books, and he translated them from
[01:06:50.480 --> 01:06:55.120]   Latin on how do you make amazing wine, where you like leave the grapes out and let them
[01:06:55.120 --> 01:06:59.040]   breathe for a night and live under the moon after you harvest them.
[01:06:59.040 --> 01:07:01.440]   The Franciscans have crushed two things.
[01:07:01.440 --> 01:07:07.440]   One is champagne, and if you have a chance to buy an incredibly good bottle of single
[01:07:07.440 --> 01:07:11.120]   grape Chardonnay champagne, I encourage you all to do it.
[01:07:11.120 --> 01:07:17.760]   Don't buy the blends, but if you can get a bottle of Salon or get a bottle of Paul Roger,
[01:07:18.400 --> 01:07:22.720]   these things are unbelievable, and the Franciscans used to make it.
[01:07:22.720 --> 01:07:23.520]   They're incredible.
[01:07:23.520 --> 01:07:26.480]   The methods are different, because in the French model, it's like you crush as soon
[01:07:26.480 --> 01:07:30.240]   as you harvest, and like a lot of the guys you mentioned earlier, they'll harvest at
[01:07:30.240 --> 01:07:33.440]   three in the morning so that the berries are still cool when they start to crush.
[01:07:33.440 --> 01:07:36.880]   But Thackeray and the Franciscans, they'll like let the grapes start to ferment naturally
[01:07:36.880 --> 01:07:38.720]   out in the sun in the open before they even do the crush.
[01:07:38.720 --> 01:07:39.920]   The Italians do the same thing.
[01:07:39.920 --> 01:07:41.600]   Also, that's how Amarone is made.
[01:07:41.600 --> 01:07:43.360]   So anyway, check out Sean Thackeray.
[01:07:43.360 --> 01:07:46.560]   His label is called Pleiades, and he's got like an amazing kind of...
[01:07:46.560 --> 01:07:47.360]   Every year is different.
[01:07:47.360 --> 01:07:49.040]   Every vintage is different.
[01:07:49.040 --> 01:07:49.840]   It's really incredible.
[01:07:49.840 --> 01:07:52.800]   But he's gone now, so all this stuff you got to buy.
[01:07:52.800 --> 01:07:54.320]   Okay.
[01:07:54.320 --> 01:08:00.320]   Well, I'll second just on the cheaper, say under $100 bottles of wine, I'll second Jamal's
[01:08:00.320 --> 01:08:02.160]   recommendation on white burgundies.
[01:08:02.160 --> 01:08:06.000]   These are white wines that come from the burgundy region.
[01:08:06.000 --> 01:08:12.880]   If you drink Napa whites, they have a lot of sugar in them, so they either taste like
[01:08:12.880 --> 01:08:14.640]   honey or sometimes butterscotch.
[01:08:14.640 --> 01:08:16.640]   Those are the two dominant flavors.
[01:08:16.640 --> 01:08:20.880]   When you start drinking white burgundies, they're just much more complex.
[01:08:20.880 --> 01:08:24.400]   They have a lot of minerality from that region.
[01:08:24.400 --> 01:08:25.600]   Great nose.
[01:08:25.600 --> 01:08:26.800]   Yeah, they're less sweet.
[01:08:26.800 --> 01:08:32.640]   They're more floral sometimes and just very crisp and refreshing, especially for the summer.
[01:08:32.640 --> 01:08:36.080]   That's a really nice bottle of wine to drink on a hot summer day.
[01:08:36.080 --> 01:08:36.480]   All right.
[01:08:36.480 --> 01:08:37.360]   That's what we need.
[01:08:37.360 --> 01:08:44.080]   Let's do this next live chat when we're outdoors and we could have some just nice chilled wine.
[01:08:44.720 --> 01:08:49.600]   This has been an amazing hour plus with the besties for hitting 500,000 subs.
[01:08:49.600 --> 01:08:51.520]   We're now at 528,000 subs.
[01:08:51.520 --> 01:08:57.680]   8,000 of you live, thousands of thumbs up, thousands of dollars in donations to charity.
[01:08:57.680 --> 01:08:58.480]   We'll pick a charity.
[01:08:58.480 --> 01:09:02.480]   I think that's going to be the J-Cal PC24 Pilatus Fund.
[01:09:02.480 --> 01:09:06.560]   We will see you all when we hit...
[01:09:06.560 --> 01:09:07.760]   When are we going to do the next one of these?
[01:09:07.760 --> 01:09:09.440]   750, 600,000?
[01:09:09.440 --> 01:09:12.400]   We're going to do a million subscriber party in Vegas.
[01:09:12.400 --> 01:09:14.400]   That's the party in...
[01:09:14.400 --> 01:09:18.800]   We're going to do a million subscriber party in Vegas.
[01:09:18.800 --> 01:09:21.520]   But between then and now, maybe we'll announce another one of these.
[01:09:21.520 --> 01:09:25.360]   Maybe every 100,000 or 200,000 subs, we'll do this again.
[01:09:25.360 --> 01:09:27.040]   Yeah, 750 makes sense, I think, right?
[01:09:27.040 --> 01:09:28.080]   Maybe do a 750.
[01:09:28.080 --> 01:09:29.920]   Yeah, when we hit 750, we'll do this again.
[01:09:29.920 --> 01:09:30.160]   Thank you.
[01:09:30.160 --> 01:09:31.280]   Yeah, there's a recommendation.
[01:09:31.280 --> 01:09:32.800]   Next time we should drink while we do it.
[01:09:32.800 --> 01:09:33.280]   Yeah.
[01:09:33.280 --> 01:09:34.160]   There you go.
[01:09:34.160 --> 01:09:35.440]   It could get really interesting.
[01:09:35.440 --> 01:09:36.160]   All right, everybody.
[01:09:36.160 --> 01:09:40.080]   For your chairman, dictator, Chamath Palihapitiya.
[01:09:40.080 --> 01:09:41.200]   Go get his sub stack.
[01:09:41.200 --> 01:09:41.680]   Really good.
[01:09:42.480 --> 01:09:45.600]   Check out Glue from your boy, David Sachs.
[01:09:45.600 --> 01:09:49.760]   And these giant potatoes that Dave's making.
[01:09:49.760 --> 01:09:51.360]   You get your hiring over at O'Halo?
[01:09:51.360 --> 01:09:53.680]   You need somebody?
[01:09:53.680 --> 01:09:54.720]   Any open positions?
[01:09:54.720 --> 01:09:55.680]   Selectively.
[01:09:55.680 --> 01:09:56.320]   Very selectively.
[01:09:56.320 --> 01:09:56.880]   What are you looking for?
[01:09:56.880 --> 01:09:59.280]   What's the most hardcore position?
[01:09:59.280 --> 01:10:02.880]   I have a chief commercial officer role that I'm spending a lot of time on right now.
[01:10:02.880 --> 01:10:04.240]   Chief commercial officer?
[01:10:04.240 --> 01:10:05.520]   Is that like a chief revenue officer?
[01:10:05.520 --> 01:10:06.080]   Is that different?
[01:10:06.080 --> 01:10:06.960]   Yeah.
[01:10:06.960 --> 01:10:08.720]   It's the same thing?
[01:10:08.720 --> 01:10:09.680]   Yeah.
[01:10:09.680 --> 01:10:10.480]   And it's the same thing.
[01:10:10.480 --> 01:10:14.960]   Why don't you make like a cantaloupe size russet potato for steakhouses?
[01:10:14.960 --> 01:10:16.800]   I think people would go wild for that, you know?
[01:10:16.800 --> 01:10:17.200]   Yeah.
[01:10:17.200 --> 01:10:18.480]   Watermelon size, giant.
[01:10:18.480 --> 01:10:18.880]   Watermelon size, yeah.
[01:10:18.880 --> 01:10:20.320]   Put it in the middle of the table, slice it open.
[01:10:20.320 --> 01:10:21.120]   And it could come loaded.
[01:10:21.120 --> 01:10:22.160]   No, but how do you-
[01:10:22.160 --> 01:10:23.360]   Loaded potato.
[01:10:23.360 --> 01:10:24.640]   How do you cook it evenly?
[01:10:24.640 --> 01:10:25.600]   That's the hard part.
[01:10:25.600 --> 01:10:26.720]   When it's too big, it's hard to cook.
[01:10:26.720 --> 01:10:29.760]   You could also make French fries that are the size of like a lightsaber.
[01:10:29.760 --> 01:10:30.480]   Why don't you do that?
[01:10:30.480 --> 01:10:32.560]   The challenge of the potato-
[01:10:32.560 --> 01:10:35.520]   I would buy lightsaber size French fries.
[01:10:35.520 --> 01:10:36.080]   That would be amazing.
[01:10:36.080 --> 01:10:37.200]   Double with the duck fat?
[01:10:37.200 --> 01:10:38.240]   You cook them in duck fat.
[01:10:38.240 --> 01:10:40.400]   Can you make us duck fat French fries, Dave?
[01:10:40.400 --> 01:10:42.320]   Oh my God, that would be incredible.
[01:10:42.320 --> 01:10:45.280]   Can you make us-
[01:10:45.280 --> 01:10:46.320]   Put it on a spit.
[01:10:46.320 --> 01:10:46.880]   Yeah, absolutely.
[01:10:46.880 --> 01:10:48.800]   Is our audience still there or have they left?
[01:10:48.800 --> 01:10:49.520]   They're still here.
[01:10:49.520 --> 01:10:50.160]   They're still here.
[01:10:50.160 --> 01:10:51.680]   They're winding down because we're winding down.
[01:10:51.680 --> 01:10:55.920]   They are winding down, but it's still 6,700 people in the live.
[01:10:55.920 --> 01:10:58.000]   Don't you guys have anything better to do?
[01:10:58.000 --> 01:10:58.560]   I got to go.
[01:10:58.560 --> 01:11:00.640]   Not you, the audience.
[01:11:00.640 --> 01:11:04.320]   And we'll see you all-
[01:11:04.320 --> 01:11:06.080]   Nick, make sure, do you have everybody?
[01:11:06.080 --> 01:11:07.360]   Yes, I have their information.
[01:11:07.360 --> 01:11:07.600]   I got it.
[01:11:07.600 --> 01:11:08.560]   You have everybody's information?
[01:11:08.560 --> 01:11:11.360]   And if not, just email nick@theallimpod.com.
[01:11:11.360 --> 01:11:15.280]   And if you, yeah, because we'll just make sure that you get on the list.
[01:11:15.280 --> 01:11:16.000]   I got it.
[01:11:16.000 --> 01:11:17.120]   Thanks everybody for your support.
[01:11:17.120 --> 01:11:18.000]   We love you.
[01:11:18.000 --> 01:11:18.500]   Thank you.
[01:11:18.500 --> 01:11:20.500]   - Thanks for tuning in.

