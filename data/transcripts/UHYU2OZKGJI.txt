
[00:00:00.000 --> 00:00:08.720]   Here, we have a pretty typical training script using a modern machine learning library. In
[00:00:08.720 --> 00:00:13.800]   this case, I'm using PyTorch, but it doesn't matter. This could be using plain Python or
[00:00:13.800 --> 00:00:19.160]   any other machine learning library. It might look familiar to you. We have some definition
[00:00:19.160 --> 00:00:24.520]   of the configurations of the model or training script that we're trying. We have some setup
[00:00:24.520 --> 00:00:29.560]   of the data, the model, and some training details, but how the model will be updated
[00:00:29.560 --> 00:00:33.560]   during training. And then we have the part of the code that actually iterates over the
[00:00:33.560 --> 00:00:38.920]   data and updates the model. Because machine learning is such an experimental field, we'll
[00:00:38.920 --> 00:00:44.000]   have to actually run the code before we know how this specific configuration will perform
[00:00:44.000 --> 00:00:49.520]   and keep track of some metrics that we care about. To see this, as we run our code, we're
[00:00:49.520 --> 00:00:54.800]   printing the value of the loss to keep track of it as the model trains. And also because
[00:00:54.800 --> 00:01:01.640]   we might want to use this model for some downstream task, we're saving the model weights locally.
[00:01:01.640 --> 00:01:06.120]   So once we've set up this training script, we might want to run it many times using different
[00:01:06.120 --> 00:01:10.880]   configurations and iterate on the various aspects of the script, whether we're trying
[00:01:10.880 --> 00:01:18.120]   a new model architecture or adding more data, or whether we're changing the configurations
[00:01:18.120 --> 00:01:24.280]   like learning rate or the number of epochs that we train for. And each time we try one
[00:01:24.280 --> 00:01:30.440]   of these changes, we want to note down how that affected our model's performance and
[00:01:30.440 --> 00:01:37.600]   maybe make some plots to visualize that. If you've trained more than a handful of models
[00:01:37.600 --> 00:01:41.440]   and you've been through this process, you'll likely know that it becomes pretty difficult
[00:01:41.440 --> 00:01:46.940]   to keep track of what you've tried and how that affected the metrics that you're trying
[00:01:46.940 --> 00:01:52.040]   to improve. So that's just what Weights & Biases was created for. It was created to solve many
[00:01:52.040 --> 00:01:56.880]   of the issues that arise when running machine learning experiments like this. In this series
[00:01:56.880 --> 00:02:02.700]   of short videos, we'll go through how we refactored this code to use Weights & Biases to automatically
[00:02:02.700 --> 00:02:08.080]   track and organize machine learning projects. By just changing a few lines of code, we'll
[00:02:08.080 --> 00:02:12.960]   log everything to Weights & Biases and we'll get shareable dashboards that'll automatically
[00:02:12.960 --> 00:02:17.520]   visualize what we've tried and how that affected our model performance.
[00:02:17.520 --> 00:02:20.100]   (upbeat music)
[00:02:20.100 --> 00:02:22.680]   (upbeat music)

