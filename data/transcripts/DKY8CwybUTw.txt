
[00:00:00.000 --> 00:00:09.520]   And I'll hop over to make sure with life. It's not unnecessary
[00:00:09.520 --> 00:00:13.120]   because yesterday I did run into an issue I was doing something
[00:00:13.120 --> 00:00:16.320]   stupid. So like yesterday I was in a mood maybe I can just like
[00:00:16.320 --> 00:00:17.520]   I'm good at this I can
[00:00:17.520 --> 00:00:23.680]   you got a little too cocky right there. And I'm gonna humble you
[00:00:23.680 --> 00:00:24.320]   a little bit.
[00:00:24.320 --> 00:00:28.360]   Awesome. I think we're live so I can get started and quickly
[00:00:28.360 --> 00:00:32.160]   introduce the session like always. Welcome back everyone.
[00:00:32.160 --> 00:00:36.040]   We have the real host Wade joining us again, who will be
[00:00:36.040 --> 00:00:40.120]   teaching us how to deploy our transformer model this time. So
[00:00:40.120 --> 00:00:42.960]   he'll be teaching different steps involved in that and
[00:00:42.960 --> 00:00:45.800]   different decisions you can make. This is fourth in our
[00:00:45.800 --> 00:00:49.040]   series and to see later how we'll have more announcements of
[00:00:49.040 --> 00:00:52.760]   what we have planned coming up. Like always, welcome back
[00:00:52.760 --> 00:00:55.440]   Kuri and always great to see you. Like always, please keep
[00:00:55.440 --> 00:00:58.400]   your questions coming in the YouTube chat. Wade always asked
[00:00:58.400 --> 00:01:00.880]   me to keep an eye on there and I don't want to make him upset.
[00:01:00.880 --> 00:01:03.920]   And with that, I'll hand it over to Wade and keep monitoring the
[00:01:03.920 --> 00:01:04.280]   chat.
[00:01:04.280 --> 00:01:08.640]   You don't want to see me angry. I'm like the Hulk. So let me
[00:01:08.640 --> 00:01:10.680]   start my share here.
[00:01:10.680 --> 00:01:11.640]   I'm terrified of that.
[00:01:11.640 --> 00:01:18.480]   All right, let's see here.
[00:01:18.480 --> 00:01:30.600]   All right. Okay, there we go. So this is a session for and just
[00:01:30.600 --> 00:01:34.680]   as an FYI, there'll probably be a reminder via email, but next
[00:01:34.680 --> 00:01:40.880]   week we won't be doing a session next week, but we'll be resuming
[00:01:40.880 --> 00:01:46.040]   the week after that. So just kind of keep that in mind. And
[00:01:46.080 --> 00:01:53.280]   today, the resources here again, I'll include this in the slide
[00:01:53.280 --> 00:01:58.400]   deck. Nothing's changed since last time. So if anybody has any
[00:01:58.400 --> 00:02:02.280]   other interesting links that you think I should include in this
[00:02:02.280 --> 00:02:07.640]   for the study group, all ears. Again, these are the Fast AI
[00:02:07.640 --> 00:02:11.040]   Hugging Face kind of integration libraries that I'm familiar
[00:02:11.040 --> 00:02:14.040]   with. If there's other ones out there, I'd love to include it.
[00:02:14.040 --> 00:02:20.800]   So let me know. Today, we're going to look at extractive
[00:02:20.800 --> 00:02:24.920]   question answering task. And again, we're doing a deep dive
[00:02:24.920 --> 00:02:29.480]   starting last week and for the next several weeks looking at
[00:02:29.480 --> 00:02:34.880]   section seven of the Hugging Face, the official course. And
[00:02:34.880 --> 00:02:39.840]   that's where they cover the main NLP task. And those are tasks
[00:02:39.840 --> 00:02:46.080]   that we want to, at least as the sole developer right now of
[00:02:46.080 --> 00:02:50.280]   Blur to be able to bake into Blur so that Fast AI folks can be
[00:02:50.280 --> 00:02:55.600]   able to train and use those models in a way familiar to Fast
[00:02:55.600 --> 00:03:01.480]   AI developers. And they're pretty dense. So as always, I'll
[00:03:01.480 --> 00:03:06.640]   encourage you to read those and go through the Colab. You can
[00:03:06.640 --> 00:03:12.240]   run each of those sections as a Colab. There's a link actually
[00:03:12.240 --> 00:03:18.240]   on there. And if you do that, you get a better taste of kind
[00:03:18.240 --> 00:03:22.400]   of how you would actually build your data sets and data loaders
[00:03:22.400 --> 00:03:25.960]   and train models with the Hugging Face trainer or
[00:03:25.960 --> 00:03:30.040]   accelerate or just a custom PyTorch loop. But again, we'll
[00:03:30.040 --> 00:03:33.120]   be looking at extractive question answering tasks and
[00:03:33.120 --> 00:03:37.240]   we'll be in particular looking at how to do this with Fast AI
[00:03:37.240 --> 00:03:45.360]   and Blur. So let's go ahead and get started. And first, this is
[00:03:45.360 --> 00:03:48.280]   something I learned. So as I'm building this out the version
[00:03:48.280 --> 00:03:51.720]   two, I have all these ideas about how things work and I, I
[00:03:51.720 --> 00:03:55.840]   tend to make certain assumptions about different approaches I
[00:03:55.840 --> 00:03:58.720]   can use, especially when it comes to pre-processing. So if
[00:03:58.720 --> 00:04:02.160]   you saw the previous pre-processing code, I've
[00:04:02.160 --> 00:04:08.640]   code, it typically tried to, not typically, it built the actual
[00:04:08.640 --> 00:04:13.440]   input IDs, and then we fed those into our data block. And I was
[00:04:13.440 --> 00:04:19.040]   relying on my assumption of how this method prepare for model
[00:04:19.040 --> 00:04:26.320]   works in the Hugging Face tokenizer utils module. And my
[00:04:26.320 --> 00:04:32.560]   assumption was wrong. And so when we were actually using,
[00:04:32.560 --> 00:04:35.160]   when I was actually starting to play with using pre-processed
[00:04:35.160 --> 00:04:41.280]   data and using the input IDs, I found that based on how Blur
[00:04:41.280 --> 00:04:44.560]   works and how the data blocks are constructed and the data
[00:04:44.560 --> 00:04:48.920]   loaders are built, it wasn't faithfully reconstructing the
[00:04:48.920 --> 00:04:53.120]   other inputs associated to your text. So like your attention
[00:04:53.120 --> 00:04:57.160]   mask or models that require, that have token type IDs, it
[00:04:57.160 --> 00:05:01.880]   wasn't building them properly. And to get an explanation of,
[00:05:01.880 --> 00:05:07.280]   or to see like why this was a problem. And again, I spent like
[00:05:07.280 --> 00:05:10.880]   hours trying to figure out like what's going on here. And
[00:05:10.880 --> 00:05:15.640]   finally, I was like, okay, let's go look at the docs, not print
[00:05:15.640 --> 00:05:21.680]   the docs. Let me load this up a little bit. So I was using this
[00:05:21.680 --> 00:05:24.960]   prepare for model. And I don't know if it used to work like
[00:05:24.960 --> 00:05:27.920]   this, or I've always been wrong in my understanding, but I
[00:05:27.920 --> 00:05:31.080]   thought this worked similar to how you could call the
[00:05:31.080 --> 00:05:35.680]   tokenizer pass text, and it would return a batch of data
[00:05:35.680 --> 00:05:42.560]   with all your inputs and have the appropriate padding applied
[00:05:42.560 --> 00:05:49.720]   in the appropriate truncation. But in trying to debug this, I
[00:05:49.760 --> 00:05:54.960]   noticed that this doesn't take a list of a list of ints, right?
[00:05:54.960 --> 00:05:57.920]   So it doesn't take a batch of input IDs, it takes a single
[00:05:57.920 --> 00:06:04.640]   input, it takes a single list of input IDs. And so it really, so
[00:06:04.640 --> 00:06:09.960]   I'm a little bit confused about the purpose of this, at least
[00:06:09.960 --> 00:06:13.840]   that it has options for padding and truncation. And a lot of the
[00:06:13.840 --> 00:06:19.240]   options that the tokenizers call method has to handle a batch of
[00:06:19.240 --> 00:06:22.440]   text at a time. So I'm a little bit confused, I'm probably have
[00:06:22.440 --> 00:06:25.120]   to hit up some of the hugging face folks and ask, well, why
[00:06:25.120 --> 00:06:29.360]   doesn't this take a list of input ID or a list of a list of
[00:06:29.360 --> 00:06:33.360]   input IDs, or a batch of input IDs. But nonetheless, it
[00:06:33.360 --> 00:06:37.120]   doesn't. And that means that the approach I was trying to build
[00:06:37.120 --> 00:06:42.520]   into blur for pre processing data and saving input IDs, and
[00:06:42.520 --> 00:06:46.760]   then using those when we construct our data block isn't
[00:06:46.760 --> 00:06:52.480]   going to work. So that was a painful lesson in reading the
[00:06:52.480 --> 00:06:56.040]   documentation. And also some late nights, I didn't have any
[00:06:56.040 --> 00:07:01.280]   chai. So I had to unfortunately, resort to coffee. So sorry for
[00:07:01.280 --> 00:07:07.640]   that, Sonia. But anyways, so we still need pre processing, and
[00:07:07.640 --> 00:07:10.240]   especially we're going to need it when it comes to question and
[00:07:10.240 --> 00:07:13.120]   answer, but we may want to use it and other tasks as well.
[00:07:13.120 --> 00:07:15.560]   Sanyam Bhutani: What if you had chai while designing the
[00:07:15.560 --> 00:07:17.320]   original API, maybe you wouldn't have
[00:07:17.320 --> 00:07:18.120]   Jason Tucker: run into that.
[00:07:18.120 --> 00:07:22.360]   You know, I've thought about that. I think when I start
[00:07:22.360 --> 00:07:25.560]   doing the summarization of translation bits, I'm gonna, I'm
[00:07:25.560 --> 00:07:29.080]   gonna turn to chai and see if that solves all my issues. If it
[00:07:29.080 --> 00:07:34.400]   does, if it doesn't, you're gonna hear about it. So yeah, so
[00:07:34.400 --> 00:07:38.120]   anyway, yeah, yeah. And this is something like, you know, when
[00:07:38.120 --> 00:07:41.600]   you're developing a library, and there's a lot of complexities
[00:07:41.600 --> 00:07:45.000]   with some of the pre processing when it comes to using
[00:07:45.000 --> 00:07:51.400]   transformers. So you know, this is one of the, the things that
[00:07:51.400 --> 00:07:55.160]   it's good to uncovered now versus having to deal with
[00:07:55.160 --> 00:08:03.840]   later. And let me show you the, the fix here. Or I should say a
[00:08:03.840 --> 00:08:08.800]   potential fix. And as we go through today, I don't have a
[00:08:08.800 --> 00:08:14.520]   collab, I'm actually going to be looking at the code base and the
[00:08:14.520 --> 00:08:19.080]   notebooks that I use to develop blur. And so blur is built using
[00:08:19.080 --> 00:08:25.080]   nbdev. And so nbdev is awesome, because you can iteratively
[00:08:25.080 --> 00:08:31.480]   build a notebook. And it will create your modules, it will
[00:08:31.480 --> 00:08:35.600]   create your documentation, you can bake your test right in. So
[00:08:35.600 --> 00:08:39.360]   it's really nice for not having to jump through a bunch of
[00:08:39.360 --> 00:08:44.360]   different hoops to be able to build a library with tests and
[00:08:44.360 --> 00:08:48.040]   with documentation, you could do it all in a familiar format,
[00:08:48.040 --> 00:08:53.120]   and Jupyter Notebook. So again, we're gonna look real briefly at
[00:08:53.120 --> 00:08:58.920]   the changes happening to pre processing. So essentially, with
[00:08:58.920 --> 00:09:03.400]   a blur, we can't use input IDs. And that means that we're going
[00:09:03.400 --> 00:09:06.720]   to have to our pre processed data is going to have to be the
[00:09:06.720 --> 00:09:11.960]   text that we want to feed into our data blocks. But what
[00:09:12.400 --> 00:09:16.800]   happens when we want to do things like, like credit pre
[00:09:16.800 --> 00:09:22.440]   process data set where we set the max length to 20, right, we
[00:09:22.440 --> 00:09:28.680]   have to actually go through our raw text and figure out, okay,
[00:09:28.680 --> 00:09:35.080]   what are the appropriate 20 tokens to keep, and then we have
[00:09:35.080 --> 00:09:40.280]   to decode those back to text. And so I came up with the
[00:09:40.280 --> 00:09:46.920]   creative way, I think of doing that. And I'm using some of the
[00:09:46.920 --> 00:09:50.680]   capabilities of the fast tokenizer. And we won't spend
[00:09:50.680 --> 00:09:56.280]   too much time really looking at the pre processing in depth for
[00:09:56.280 --> 00:09:59.520]   classification and for token classification. I'm going to
[00:09:59.520 --> 00:10:02.920]   check these bits in within the next 24 hours, and I encourage
[00:10:02.920 --> 00:10:06.680]   you to take a look at it. But essentially, what we can do is
[00:10:06.680 --> 00:10:12.480]   we can apply tokenization to a batch of data right here. And
[00:10:12.480 --> 00:10:17.000]   this is going to give us based on whatever tokenization
[00:10:17.000 --> 00:10:22.200]   arguments we've created, it's going to give us our inputs. And
[00:10:22.200 --> 00:10:25.800]   our inputs are we're going to look at we need to we need to
[00:10:25.800 --> 00:10:30.640]   take those inputs and figure out okay, how do we actually build
[00:10:30.640 --> 00:10:34.160]   the text that we want to ultimately use in our pre
[00:10:34.160 --> 00:10:39.000]   process data set. And to do that, we can actually look at
[00:10:39.000 --> 00:10:43.960]   the offset mappings. And again, the offset mappings are for each
[00:10:43.960 --> 00:10:47.760]   token, it tells you the start and end character for those
[00:10:47.760 --> 00:10:52.920]   tokens. So let's say we're truncating to maximum of 20
[00:10:52.920 --> 00:10:58.680]   tokens. Well, using this particular block of code right
[00:10:58.680 --> 00:11:05.120]   here, what we could do is go through those 20 inputs, or
[00:11:05.120 --> 00:11:13.920]   those 20 token IDs in the inputs. And we can find the
[00:11:13.920 --> 00:11:20.960]   start and the lowest start character and the highest end
[00:11:20.960 --> 00:11:26.760]   character. And that right there is our chunk of text that we
[00:11:26.800 --> 00:11:31.640]   want to eventually use. And then once we have that, we can go
[00:11:31.640 --> 00:11:36.680]   ahead and I'm just using a data frame here, I don't have full
[00:11:36.680 --> 00:11:42.520]   support for data sets yet doing this. But what we can do now is
[00:11:42.520 --> 00:11:47.400]   go through here and, and essentially, add this to our
[00:11:47.400 --> 00:11:51.800]   data set, the start character index and end character index of
[00:11:51.800 --> 00:11:55.000]   our raw text that we're actually going to feed into our data
[00:11:55.000 --> 00:11:59.920]   block. And then we can actually create a proc underscore
[00:11:59.920 --> 00:12:03.560]   whatever our text attribute is here. And you can see all we're
[00:12:03.560 --> 00:12:09.640]   doing is just using really standard string slicing. So this
[00:12:09.640 --> 00:12:15.640]   is our, it's going to represent our text. This is the start and
[00:12:15.640 --> 00:12:19.960]   end. And we do plus one, of course, because we have to go,
[00:12:19.960 --> 00:12:24.720]   we want to get to that, we want to include that last character.
[00:12:25.520 --> 00:12:30.920]   And this allows us to then create a data frame that looks
[00:12:30.920 --> 00:12:34.480]   something like this. So previously, we would have our
[00:12:34.480 --> 00:12:39.800]   input IDs in here. But now, here we are just, you know, for the
[00:12:39.800 --> 00:12:44.920]   sake, as an example, have a max length of 20. And so you can see
[00:12:44.920 --> 00:12:49.440]   that it truncated. And so that each of these, the raw text
[00:12:49.440 --> 00:12:54.920]   right here, it will take 24 tokens to represent max 24 tokens
[00:12:54.920 --> 00:13:00.320]   to represent each bit of text. And then we can use this process
[00:13:00.320 --> 00:13:07.400]   text as inputs into our data block. So that's how we're doing
[00:13:07.400 --> 00:13:11.160]   pre processing right now. And you can see I'm including other
[00:13:11.160 --> 00:13:14.880]   things now, when you pre process, so you're getting the
[00:13:16.120 --> 00:13:21.480]   the start character index and in character index, that the
[00:13:21.480 --> 00:13:28.520]   process text represents. So it starts at zero, and then goes to
[00:13:28.520 --> 00:13:37.400]   102. This is showing a subset. So there's actually more here, I
[00:13:37.400 --> 00:13:43.360]   think. So anyways, that's a major change to just be aware if
[00:13:43.560 --> 00:13:49.080]   in the next day or two, you get the latest v2 bits. The pre
[00:13:49.080 --> 00:13:53.840]   processing has changed. All the examples in the documentation
[00:13:53.840 --> 00:13:58.200]   have been updated. And I'll try to go back into the CoLab and
[00:13:58.200 --> 00:14:06.080]   fix that as well. So that's pre processing. Let's go back to
[00:14:09.520 --> 00:14:14.800]   the slides here. So any questions on that, Sanyam?
[00:14:14.800 --> 00:14:18.880]   Sanyam Bhutani: Um, no, no questions so far. Someone's
[00:14:18.880 --> 00:14:22.760]   asking, will you be receiving the, will they be receiving the
[00:14:22.760 --> 00:14:25.520]   materials? Yes, they'll be emailed to you or you can join
[00:14:25.520 --> 00:14:29.200]   the fast.ai discord to find the materials.
[00:14:29.200 --> 00:14:32.120]   Jason Antic: And that's in the slides too. And I always try to
[00:14:32.120 --> 00:14:38.120]   post the slides and CoLab notebook when we have it on the
[00:14:38.120 --> 00:14:43.040]   discord as well. Okay, so let's go ahead and get into
[00:14:43.040 --> 00:14:45.840]   extractive question answering. We'll look at pre processing a
[00:14:45.840 --> 00:14:51.240]   little bit more since it's more critical here. So for working
[00:14:51.240 --> 00:14:56.680]   with extractive question answering, we are looking for a
[00:14:56.680 --> 00:15:01.800]   data set that includes a question, a context, and
[00:15:01.800 --> 00:15:08.080]   information about where to find the answer in the context. And
[00:15:08.080 --> 00:15:14.520]   so this is a bit different than with open ended question
[00:15:14.520 --> 00:15:21.000]   answering, which is you can have a question asked, and an answer
[00:15:21.000 --> 00:15:27.000]   being given that isn't in a particular context. And for
[00:15:27.000 --> 00:15:30.280]   those will potentially, if there's interest, we can look at
[00:15:30.280 --> 00:15:35.600]   how to actually build one of those models. And we would use
[00:15:35.800 --> 00:15:39.840]   more of an encoder decoder model for that something like Bart or
[00:15:39.840 --> 00:15:43.760]   t five says since the task is very similar to something like
[00:15:43.760 --> 00:15:48.560]   summarization or translation. But here, we're actually looking
[00:15:48.560 --> 00:15:54.800]   in a block of text, the context, and we want to see if the answer
[00:15:54.800 --> 00:15:58.760]   is to whatever the question is posed is in there. And if it is,
[00:15:58.760 --> 00:16:05.120]   we want to be able to extract that. And so for these types of
[00:16:05.120 --> 00:16:08.880]   tasks, our data set, we need to also be able to know what are
[00:16:08.880 --> 00:16:14.120]   the start and end character indices of the answer in the
[00:16:14.120 --> 00:16:22.000]   context. And a good example of this is squad and squad v2. The
[00:16:22.000 --> 00:16:27.760]   the version two squad data set actually also includes
[00:16:27.760 --> 00:16:32.040]   unanswerable questions. So if you wanted to, in addition to
[00:16:32.040 --> 00:16:36.640]   just figuring out the start and end tokens and the answer, you
[00:16:36.640 --> 00:16:41.120]   could also add a third task, which is just a simple
[00:16:41.120 --> 00:16:46.720]   classification is the answer in there or not, is it answerable
[00:16:46.720 --> 00:16:56.600]   or not. So those are two data sets to look at. So just like
[00:16:56.600 --> 00:17:04.280]   usual, after we get our data set, we are going to use blur.
[00:17:04.280 --> 00:17:07.400]   Get HF objects to get our hugging face objects. You can
[00:17:07.400 --> 00:17:13.040]   also do this manually. And the next thing to think about, we
[00:17:13.040 --> 00:17:16.520]   have a good data set is what are the best type of models to use
[00:17:16.520 --> 00:17:19.760]   for extractive question answering? I'm gonna let you
[00:17:19.760 --> 00:17:24.320]   all guess. I'll give you just 15 seconds. And Sonia, you could
[00:17:24.320 --> 00:17:32.280]   tell me what the top vote is. Or if there's no top vote, I'll
[00:17:32.280 --> 00:17:33.200]   work with either.
[00:17:33.200 --> 00:17:37.200]   I'll vote if there are no votes, but I'll wait for it.
[00:17:37.200 --> 00:17:40.800]   All right, we'll see. We'll see how, how much you're paying
[00:17:40.800 --> 00:17:43.000]   attention to these study sessions.
[00:17:43.000 --> 00:17:46.400]   Are you doubting me?
[00:17:46.400 --> 00:17:49.760]   No, I don't know. I'm being cocky. I might be wrong.
[00:17:49.760 --> 00:17:52.280]   We'll see. Chai is on trial right now.
[00:17:52.280 --> 00:17:53.440]   Chai is on trial.
[00:17:53.600 --> 00:17:58.960]   So what do you think, based on what you have learned and know
[00:17:58.960 --> 00:18:02.560]   about the different types of transformer architectures would
[00:18:02.560 --> 00:18:07.040]   be the best architecture to choose for an extractive
[00:18:07.040 --> 00:18:14.280]   question answering task? What do we got Sonia? Anybody brave
[00:18:14.280 --> 00:18:17.200]   enough to throw an answer?
[00:18:17.200 --> 00:18:20.800]   There's like a 30 second delay between when the time you say
[00:18:20.840 --> 00:18:24.040]   someone listens, they type it back. So like, whenever I do
[00:18:24.040 --> 00:18:26.560]   this, there's like an anxious wait that I have to go through.
[00:18:26.560 --> 00:18:27.760]   All right.
[00:18:27.760 --> 00:18:31.480]   Money says Bert Robert Art is still Bert.
[00:18:31.480 --> 00:18:37.480]   All right. Those are all good ones because they're from the
[00:18:37.480 --> 00:18:46.240]   right core architecture, which is the encoder only models. So
[00:18:46.240 --> 00:18:47.320]   I was going to say that one.
[00:18:47.800 --> 00:18:51.760]   Yeah, you got real quiet. You're waiting for the green box over
[00:18:51.760 --> 00:18:56.680]   there. It's like, yeah, that's the one I was thinking of. So
[00:18:56.680 --> 00:18:58.680]   yeah, when you're when you're building these also think about
[00:18:58.680 --> 00:19:02.160]   what's the you know, what are good architectures to use now
[00:19:02.160 --> 00:19:06.480]   will help narrow down at least your your model selection. So
[00:19:06.480 --> 00:19:11.560]   for extractive question answering encoder models that
[00:19:11.560 --> 00:19:16.480]   bake in a really rich representation of the inputs of
[00:19:16.480 --> 00:19:21.240]   the inputs are good for these type of tasks. Whereas decoder
[00:19:21.240 --> 00:19:25.040]   only models are good for text generally typically, like GPT
[00:19:25.040 --> 00:19:29.960]   two are focused more on text generation, and the encoder
[00:19:29.960 --> 00:19:34.880]   decoder models sequence to sequence models like Bart or t5
[00:19:34.880 --> 00:19:42.480]   are good options for things that generative tack tasks that also
[00:19:42.480 --> 00:19:48.680]   require a good understanding of both the the inputs and the
[00:19:48.680 --> 00:19:54.080]   outputs. So things like translation or summarization. So
[00:19:54.080 --> 00:20:01.000]   I'm going to go ahead and we're going to look at the source code
[00:20:01.000 --> 00:20:05.480]   here for blur. And again, this stuff will be checked in in the
[00:20:05.480 --> 00:20:10.440]   next 24 hours. And you can see I'm going to work with the squad
[00:20:10.440 --> 00:20:17.400]   v2 data set here in the data module for question answering.
[00:20:17.400 --> 00:20:23.560]   And I'm going to pull 1000 training examples and a couple
[00:20:23.560 --> 00:20:28.080]   hundred validation examples. And then again, because fast
[00:20:28.080 --> 00:20:32.160]   data likes everything in a single data set, and then where
[00:20:32.160 --> 00:20:36.440]   you define a splitter to dictate which examples are associated
[00:20:36.440 --> 00:20:39.800]   to your training set and which are associated to your
[00:20:40.000 --> 00:20:44.560]   validation. I'll go ahead and just convert the raw training
[00:20:44.560 --> 00:20:48.960]   data set, raw validation data set into data frames. I'm going
[00:20:48.960 --> 00:20:54.920]   to use the column splitter in fast AI and and say that we see
[00:20:54.920 --> 00:20:59.840]   is valid equals true. That is part of our validation set
[00:20:59.840 --> 00:21:03.480]   everything else is part of the training set. And we could take
[00:21:03.480 --> 00:21:09.600]   a look at the raw data. And this is how squad and squad v2 looks
[00:21:09.880 --> 00:21:14.200]   like. You get the context, the question, the answers, there's
[00:21:14.200 --> 00:21:20.320]   our is valid attribute that we added on there. And once we have
[00:21:20.320 --> 00:21:25.480]   that, I'm going to concatenate them into a single what I'm
[00:21:25.480 --> 00:21:30.440]   going to call the squad data frame. And now we need to be
[00:21:30.440 --> 00:21:35.720]   able to essentially flatten this out a little bit. So you can see
[00:21:35.720 --> 00:21:41.840]   that the answers come as a dictionary. So you have text,
[00:21:41.840 --> 00:21:46.160]   and then you have your answer start location. And notice also
[00:21:46.160 --> 00:21:50.200]   I put in these include both of these in the blur docs, because
[00:21:50.200 --> 00:21:54.120]   you can see that the validation data set how it represents
[00:21:54.120 --> 00:21:58.360]   answers is a bit different than the training set. So the
[00:21:58.360 --> 00:22:03.960]   training set is going to have a single answer text and answer
[00:22:03.960 --> 00:22:10.240]   start. Whereas when you're doing validation with question
[00:22:10.240 --> 00:22:16.120]   answering, is that you'll get several options for text, and
[00:22:16.120 --> 00:22:19.320]   each of these with their associated start position. And
[00:22:19.320 --> 00:22:24.040]   this could be one or many, or it could be not it can be in the
[00:22:24.040 --> 00:22:26.800]   case of squad v2, there are examples that don't have an
[00:22:26.800 --> 00:22:31.160]   answer. And so the text is just going to be an empty list, as
[00:22:31.160 --> 00:22:37.200]   will the answer start. And we'll look at how to handle this in
[00:22:37.200 --> 00:22:44.000]   fast.ai, or we start modeling a question answering, or actually
[00:22:44.000 --> 00:22:47.080]   start training a question answering model, how this is
[00:22:47.080 --> 00:22:53.320]   actually used in our validation metrics. So just be aware that
[00:22:53.320 --> 00:22:57.520]   you're going to see a little bit of difference. What we're going
[00:22:57.520 --> 00:23:05.720]   to do now is we're going to use at least as we build our data
[00:23:05.720 --> 00:23:12.440]   frame, we're going to actually use the first for validation,
[00:23:12.440 --> 00:23:18.240]   the first text and the first answer start location. We'll be
[00:23:18.240 --> 00:23:21.760]   going back to this when we do our validation metrics. And
[00:23:21.760 --> 00:23:24.560]   we'll look at how to get back at this information to be able to
[00:23:24.560 --> 00:23:30.240]   use all the potential texts, or answers, but at least for our
[00:23:30.240 --> 00:23:35.720]   training validation set, in order to create our data loaders
[00:23:35.720 --> 00:23:39.640]   to train a model and to be able to get both the training and
[00:23:39.640 --> 00:23:43.640]   validation loss, we're going to go ahead and just use the first
[00:23:43.640 --> 00:23:49.560]   answer and the first text. And so when we flatten this out, you
[00:23:49.560 --> 00:23:54.200]   can see we have our data that you saw before. We have answer
[00:23:54.200 --> 00:23:58.760]   start character index, answer text and answer in character
[00:23:58.760 --> 00:24:03.840]   index split into its own columns. And this is required
[00:24:03.840 --> 00:24:08.440]   for the pre processing functionality that we include in
[00:24:08.440 --> 00:24:14.800]   blur. So and you can see that where there isn't a answer,
[00:24:14.800 --> 00:24:19.200]   there's the empty list. And of course, we set the start
[00:24:19.200 --> 00:24:26.320]   character and in character indices equal to zero. So as I
[00:24:26.320 --> 00:24:31.440]   mentioned, you go ahead and build your face objects, you can
[00:24:31.440 --> 00:24:36.000]   use the blur get HF objects, that makes it pretty easy. One
[00:24:36.000 --> 00:24:38.680]   thing that we're going to do for question answering is we're
[00:24:38.680 --> 00:24:44.120]   going to set a max sequence length. And we're going to make
[00:24:44.120 --> 00:24:50.560]   that our vocab. So in this case, we're going to have sequences
[00:24:50.560 --> 00:24:56.920]   of 128 tokens. And for each token, there is a prediction,
[00:24:56.920 --> 00:25:00.760]   we're going to mask some of those like the tokens that
[00:25:00.760 --> 00:25:04.480]   belong to the question, or that are special tokens like the
[00:25:04.480 --> 00:25:07.760]   class or padding tokens, we're going to mask those so those
[00:25:07.760 --> 00:25:12.480]   don't get predicted or factored into our loss. But we start
[00:25:12.480 --> 00:25:20.720]   with 128 potential labels. And that's going to be our vocab for
[00:25:20.720 --> 00:25:27.720]   both our start and in token indices. So again, with the
[00:25:27.720 --> 00:25:36.920]   pre processing here for blur is that it's required for question
[00:25:36.920 --> 00:25:42.120]   answering. And the reason is, is that our labels need to be the
[00:25:42.120 --> 00:25:45.440]   start and end token indices. And right now what we have is the
[00:25:45.440 --> 00:25:50.640]   start and end character indices. And so we have to pre process
[00:25:50.640 --> 00:25:58.120]   our data, we have to go through tokenize it. And also, in order
[00:25:58.120 --> 00:26:01.880]   to figure out the start and end token indices, so that when we
[00:26:01.880 --> 00:26:05.600]   actually build our data block, that's what we use. Another
[00:26:05.600 --> 00:26:09.760]   reason that we want to pre process the raw data is that we
[00:26:09.760 --> 00:26:12.720]   want to be able to handle documents longer than our max
[00:26:12.720 --> 00:26:15.880]   length. And again, the max length is going to be dictated
[00:26:15.880 --> 00:26:21.080]   by what you set specifically. So in the example that I just
[00:26:21.080 --> 00:26:26.800]   showed you, we're setting it to 128 tokens, or by the max
[00:26:26.800 --> 00:26:31.840]   supported by whatever model you're using. But we have to use
[00:26:31.840 --> 00:26:36.440]   pre processing either way to be able to find those start and
[00:26:36.440 --> 00:26:38.680]   token indices. So make sure you keep that clear because
[00:26:38.920 --> 00:26:41.240]   sometimes it's confusing when you read the documentation, they
[00:26:41.240 --> 00:26:46.000]   talk about start in indices. And it's unclear sometimes is they
[00:26:46.000 --> 00:26:49.400]   talk about the character ones or the token indices. Just remember
[00:26:49.400 --> 00:26:52.680]   that you start with the character indices, but your
[00:26:52.680 --> 00:26:56.440]   targets are the token indices. So we need to get those and we
[00:26:56.440 --> 00:27:00.760]   also need to be able to support chunking if we want to handle
[00:27:00.760 --> 00:27:08.000]   long documents. And with this particular iteration of our pre
[00:27:08.000 --> 00:27:14.680]   processor, it does just that. And it's pretty complex. Some
[00:27:14.680 --> 00:27:19.480]   of the things that the pre processing does for you is just
[00:27:19.480 --> 00:27:27.880]   note that not all architectures pad or or expect the pad on the
[00:27:27.880 --> 00:27:32.480]   right side or expect the question to be to come first and
[00:27:32.480 --> 00:27:36.800]   then the context second, there are architectures, I think it's
[00:27:37.280 --> 00:27:42.280]   maybe XL net or XLM that actually pad the other way
[00:27:42.280 --> 00:27:47.320]   around and switch the context and question. One thing if you
[00:27:47.320 --> 00:27:51.800]   use this particular pre processor is that it will go
[00:27:51.800 --> 00:27:54.640]   ahead and do all that for you just pass in what your question
[00:27:54.640 --> 00:27:58.800]   attribute is what your context attribute and it will flip them
[00:27:58.800 --> 00:28:04.120]   as needed. It will reset the truncation as needed. So you
[00:28:04.120 --> 00:28:06.680]   don't need to worry about that. But if you're doing things on
[00:28:06.680 --> 00:28:10.280]   your own, just be aware that not all architectures operate the
[00:28:10.280 --> 00:28:17.200]   same way. So for the pre processing, we're going to rely
[00:28:17.200 --> 00:28:22.160]   on a lot of the capabilities of the fast tokenizers. And I'm
[00:28:22.160 --> 00:28:25.800]   going to go through this pretty quickly, but I really encourage
[00:28:25.800 --> 00:28:29.080]   you to look through it and make sure you can understand
[00:28:29.080 --> 00:28:32.400]   everything that's going on. As well as if you have
[00:28:32.400 --> 00:28:36.560]   recommendations on to improve it, I am all ears. So we're
[00:28:36.560 --> 00:28:41.280]   going to go ahead and look at essentially a mini batch of our
[00:28:41.280 --> 00:28:48.360]   full data frame at a time. And we are going to go through that
[00:28:48.360 --> 00:28:51.680]   mini batch or mini data frame, right a batch data frame, I call
[00:28:51.680 --> 00:28:56.040]   it, we're going to go through one row at a time. And the
[00:28:56.040 --> 00:28:58.800]   reason we're going through it one row at a time is that when
[00:28:58.800 --> 00:29:04.400]   we actually tokenize our question and context, if we're
[00:29:04.400 --> 00:29:08.240]   dealing with a long document, and we've requested that we want
[00:29:08.240 --> 00:29:14.160]   to have chunking done, and we'll look at how you do that's just a
[00:29:14.160 --> 00:29:18.920]   couple parameters you pass to the tokenizer. We have to go row
[00:29:18.920 --> 00:29:23.720]   by row because there's the potentiality that each example
[00:29:24.560 --> 00:29:29.920]   gets split up into multiple rows. And we want to include all
[00:29:29.920 --> 00:29:35.040]   of them in our data set. So we go through each one, we're going
[00:29:35.040 --> 00:29:41.360]   to tokenize. And then we're going to check if we do have
[00:29:41.360 --> 00:29:47.360]   overflow, which means that we're going to have one to many items
[00:29:47.360 --> 00:29:52.840]   if we're not asking it to handle long documents and chunk long
[00:29:52.840 --> 00:29:58.920]   documents into sections. With the maximum length of the max
[00:29:58.920 --> 00:30:01.760]   sequence length that we specified, we're just going to
[00:30:01.760 --> 00:30:10.640]   have one example. And we're going to use the same process
[00:30:10.640 --> 00:30:14.320]   here, whether it's one example, and we're not handling documents
[00:30:14.320 --> 00:30:20.200]   or we have multiple examples generated from the tokenization
[00:30:20.440 --> 00:30:25.200]   of just that one row. And so again, remember, we're not going
[00:30:25.200 --> 00:30:32.160]   to be able to use input IDs. So what we have to do is use the
[00:30:32.160 --> 00:30:37.280]   offset mappings, use the sequence IDs, in particular for
[00:30:37.280 --> 00:30:42.440]   figuring out what part of our inputs represent the question,
[00:30:42.440 --> 00:30:48.040]   and what part represents the context. And again, this will be
[00:30:48.480 --> 00:30:53.960]   different based on how your tokenizer actually does the
[00:30:53.960 --> 00:30:58.720]   padding. So what we need to do is once we process this, we need
[00:30:58.720 --> 00:31:02.520]   to figure out, okay, what are the question offsets? What are
[00:31:02.520 --> 00:31:06.440]   the context offsets? And then just as I showed you before,
[00:31:06.440 --> 00:31:10.160]   with the sequence classification, is we need to now
[00:31:10.160 --> 00:31:17.280]   figure out, okay, in our question column and in our
[00:31:17.280 --> 00:31:22.640]   context column, what is the span of text that we're actually
[00:31:22.640 --> 00:31:27.600]   wanting to include in our pre-processed data set. And we
[00:31:27.600 --> 00:31:32.440]   do it the same way, but we do it for both our question attribute
[00:31:32.440 --> 00:31:36.480]   and our context attribute. And that gives us the text. So if
[00:31:36.480 --> 00:31:41.120]   we're chunking, let's say we have a document that's like 750
[00:31:41.120 --> 00:31:47.200]   tokens, and we're saying, yeah, we want to chunk that in 128
[00:31:47.200 --> 00:31:52.280]   token chunks. So we're going to have a bunch, we're going to
[00:31:52.280 --> 00:32:00.160]   have, you know, I don't know, six, seven, or maybe less, I
[00:32:00.160 --> 00:32:02.760]   didn't know how much, I'm not good at math. We're going to
[00:32:02.760 --> 00:32:08.120]   have a bunch of examples, right? And so this is going to allow
[00:32:08.120 --> 00:32:13.080]   us for each of those chunks to get the correct piece of the
[00:32:13.080 --> 00:32:17.320]   questions and the context. Once we have that, we have to
[00:32:17.320 --> 00:32:22.520]   remember, this stuff's all been tokenized. And if we just use
[00:32:22.520 --> 00:32:28.720]   this as is, it's not going to give us the raw text that we
[00:32:28.720 --> 00:32:33.400]   need to be able to feed into our data block. And also our token
[00:32:33.400 --> 00:32:37.200]   indices are going to be off. So what we have to do is take
[00:32:37.520 --> 00:32:42.600]   these process question and context from our, these
[00:32:42.600 --> 00:32:47.760]   sections from our raw data. And we have to kind of go through
[00:32:47.760 --> 00:32:52.920]   this process again of tokenizing them, finding the input for the
[00:32:52.920 --> 00:32:58.640]   chunk, the question mask, getting the actual inputs. And
[00:32:58.640 --> 00:33:05.760]   if you look at the course documentation, they have a
[00:33:05.760 --> 00:33:12.120]   pretty complex way of figuring out where the start and end
[00:33:12.120 --> 00:33:16.720]   token indices are. I actually use this approach right here,
[00:33:16.720 --> 00:33:23.000]   which is, has at least proven as far as my testing a little bit
[00:33:23.000 --> 00:33:29.960]   more easier to understand, definitely much briefer, and
[00:33:30.000 --> 00:33:35.480]   also more reliable. So trying some of their notebooks with
[00:33:35.480 --> 00:33:38.160]   with some of the other architectures, some of what they
[00:33:38.160 --> 00:33:40.720]   have in the course documentation, didn't give me
[00:33:40.720 --> 00:33:44.080]   correct results all the time. There were little weird
[00:33:44.080 --> 00:33:48.720]   exceptions here and there. But this is how I am doing
[00:33:48.720 --> 00:33:52.280]   pre-processing, at least in Blur, and finding those start
[00:33:52.280 --> 00:33:55.840]   and end token indices. And you can see that we're iterating
[00:33:55.840 --> 00:34:01.560]   through the tokenized chunked text, right, which is going to
[00:34:01.560 --> 00:34:08.560]   be a potentially be a portion of our question and context in
[00:34:08.560 --> 00:34:13.160]   our original raw data set. And I'm going to make sure that
[00:34:13.160 --> 00:34:17.000]   we're not looking at a question token, because remember, we
[00:34:17.000 --> 00:34:21.240]   want to find the answers in the context. I want to see where
[00:34:21.240 --> 00:34:26.760]   the first token is the starting token for the answer. And if
[00:34:26.760 --> 00:34:32.320]   we actually look at the token input from that place all the
[00:34:32.320 --> 00:34:35.440]   way to the length, right, we can check to see yes, is that
[00:34:35.440 --> 00:34:39.240]   the tokenized answer? And if it is, then we go ahead and
[00:34:39.240 --> 00:34:44.120]   assign the start and end indices, and we're done. And
[00:34:44.120 --> 00:34:49.400]   then we can simply create a new row that again, we're using a
[00:34:49.400 --> 00:34:53.400]   new data frame includes our raw data. And now we're going to
[00:34:53.400 --> 00:35:00.320]   add a processed version of our question attribute, attribute
[00:35:00.320 --> 00:35:02.760]   and our context attribute. And then we're also going to
[00:35:02.760 --> 00:35:08.320]   include the answer start token index, answer end token index.
[00:35:08.320 --> 00:35:13.080]   And we're also going to add a is answerable. So we're not
[00:35:13.080 --> 00:35:16.240]   going to use this today. But if you were actually using
[00:35:16.280 --> 00:35:19.960]   squad V2, and you wanted to both predict, is it an
[00:35:19.960 --> 00:35:24.200]   answerable question as well as the start and end logits, this
[00:35:24.200 --> 00:35:29.440]   is going to be part of your pre processed data frame, once you
[00:35:29.440 --> 00:35:36.480]   run your original data frame through it. So again, look at
[00:35:36.480 --> 00:35:41.320]   that code. But let's focus more here on the output. As an
[00:35:41.320 --> 00:35:46.520]   example, I am going to say I want overflowing tokens, I want
[00:35:46.520 --> 00:35:52.160]   a max length, remember, this is set to 128. Another attribute
[00:35:52.160 --> 00:35:56.360]   you typically want to include when you're chunking examples
[00:35:56.360 --> 00:36:01.200]   is stride. And stride is how many overlapping tokens to
[00:36:01.200 --> 00:36:06.240]   include in each chunk. And this is helpful because it gives us
[00:36:06.240 --> 00:36:11.360]   a little bit more context. When we're actually looking at
[00:36:11.360 --> 00:36:16.600]   answers, so a little bit more context, or repeated context for
[00:36:16.600 --> 00:36:21.680]   the models to actually learn from. And we're also going to
[00:36:21.680 --> 00:36:27.960]   pass in a ID attribute, because when we're chunking, when we go
[00:36:27.960 --> 00:36:30.760]   back to building our metrics and figuring out, you know, how
[00:36:30.760 --> 00:36:34.800]   good our model is, we want to aggregate by example. And as I
[00:36:34.800 --> 00:36:39.680]   mentioned before, one example can be split into multiple
[00:36:39.680 --> 00:36:45.320]   chunks. So how do we get back to that? And the way that we get
[00:36:45.320 --> 00:36:50.080]   back is by using a unique identifier for each example. And
[00:36:50.080 --> 00:36:54.440]   so we have to specify that here. And you can see that when we
[00:36:54.440 --> 00:36:58.880]   pre process it, again, we're not getting input IDs anymore. But
[00:36:58.880 --> 00:37:07.560]   now we're getting just the text. And so you can see here, here is
[00:37:07.560 --> 00:37:11.600]   an example that was chunked. So right, we're looking at this 9063.
[00:37:11.600 --> 00:37:18.360]   This is one example. We keep the original context question
[00:37:18.360 --> 00:37:22.600]   answer, we have our is valid attribute, and we have our
[00:37:22.600 --> 00:37:28.400]   character indices and answer text. But we also have now a
[00:37:28.400 --> 00:37:33.920]   process question process context, our start and end token
[00:37:33.920 --> 00:37:39.080]   indices, and whether or not the question is answerable. And if
[00:37:39.080 --> 00:37:43.200]   you notice, based on this architecture we're using, which
[00:37:43.200 --> 00:37:49.320]   paths to the right, we never want to truncate the questions,
[00:37:49.320 --> 00:37:54.120]   you can see the question in each chunk is still the same. But
[00:37:54.120 --> 00:37:58.280]   you'll notice here, if we look at this portion of the process
[00:37:58.280 --> 00:38:04.160]   context, this is the beginning of our raw context. And then you
[00:38:04.160 --> 00:38:08.880]   can see the next chunk actually includes some padding, and then
[00:38:08.880 --> 00:38:16.560]   is a subset of whatever is in here. So that's how chunking
[00:38:16.560 --> 00:38:22.200]   works. And this is what we're going to pass to our data block
[00:38:22.200 --> 00:38:27.400]   this process question, context, our start and end token
[00:38:27.440 --> 00:38:33.200]   indices. We also have tests in blur. So it's pretty
[00:38:33.200 --> 00:38:36.880]   complicated to code above. Again, I'm open to any
[00:38:36.880 --> 00:38:42.480]   recommendations. But at least it works. And so we actually have
[00:38:42.480 --> 00:38:46.080]   tests that goes through here and for answerable questions
[00:38:46.080 --> 00:38:52.360]   verifies that if we decode our input IDs, and we use the that
[00:38:52.360 --> 00:38:56.600]   answer start and answer end token indices that we created
[00:38:56.600 --> 00:39:02.960]   above that we get the correct answer. Here's another example
[00:39:02.960 --> 00:39:09.520]   of where we don't want to chunk long documents here, we just
[00:39:09.520 --> 00:39:13.280]   want to truncate them. And so you can see that instead of
[00:39:13.280 --> 00:39:18.720]   getting multiple examples with the same ID, here we have unique
[00:39:18.720 --> 00:39:24.680]   IDs, it's just that everything is truncated. And so the
[00:39:24.680 --> 00:39:32.640]   process question is just the first 128 tokens represented by
[00:39:32.640 --> 00:39:36.680]   this context here. And we got the same information. So if you
[00:39:36.680 --> 00:39:42.840]   don't want to use chunking, you don't have to. So for question
[00:39:42.840 --> 00:39:46.520]   answering, we have to pre process the data. And we have to
[00:39:46.520 --> 00:39:51.120]   figure out we have to we have to create process question text
[00:39:51.160 --> 00:39:56.640]   and process context text, and then also find our start and end
[00:39:56.640 --> 00:40:02.440]   indices. So any questions on pre processing?
[00:40:02.440 --> 00:40:09.800]   People are saying no questions, but Korean was critiquing your
[00:40:09.800 --> 00:40:15.520]   code a bit. So he is looking at the code, he was a bit confused
[00:40:15.520 --> 00:40:18.920]   since they were like, three, four loops for calculating start
[00:40:18.920 --> 00:40:22.280]   and end. Korean is a software engineer. So he's upset because
[00:40:22.280 --> 00:40:25.880]   this would have, I think, n cube complexity.
[00:40:25.880 --> 00:40:31.960]   Yeah, I share his upsetness. So I am. Yeah, Korean, take a look
[00:40:31.960 --> 00:40:36.640]   at it. If you have some improvements on this, I am like
[00:40:36.640 --> 00:40:39.520]   I said, all ears. I'll check this, I'll be checked in the
[00:40:39.520 --> 00:40:46.320]   next 24 hours. And so feel free to submit a PR. But yeah, my
[00:40:46.320 --> 00:40:51.120]   goal is essentially to get something working and then go
[00:40:51.120 --> 00:40:54.080]   back and refactor. So I am positive you're right that
[00:40:54.080 --> 00:41:00.080]   there's ways to improve this. So yeah, so let me know. Any
[00:41:00.080 --> 00:41:01.800]   questions? Anything else?
[00:41:01.800 --> 00:41:02.920]   Just that critique.
[00:41:02.920 --> 00:41:12.240]   Yeah, yeah. I appreciate that. Yeah. Okay. So we have a pre
[00:41:12.240 --> 00:41:16.440]   set. And now we are going to build our data blocks, just like
[00:41:16.440 --> 00:41:21.640]   normal. And we have two options is here with question
[00:41:21.640 --> 00:41:24.240]   answering is that we can build our data blocks in the usual
[00:41:24.240 --> 00:41:28.840]   fashion. So kind of use the standard fast AI bits, the item
[00:41:28.840 --> 00:41:36.280]   getter, or call reader for our, our get x and get y. But
[00:41:36.280 --> 00:41:41.160]   remember, if we chunked our data set, and one example is now
[00:41:41.160 --> 00:41:47.760]   chunked into, let's say five chunks, when we do question
[00:41:47.760 --> 00:41:51.920]   answering is we want to build our metric, we want to aggregate
[00:41:51.920 --> 00:41:59.640]   those chunks, and do our metrics on the full example, in order
[00:41:59.640 --> 00:42:04.400]   to calculate how well our models doing at finding the answer. And
[00:42:04.400 --> 00:42:10.160]   so if you look at the course documentation, you'll see ways
[00:42:10.160 --> 00:42:17.400]   that at least they propose to do this, if you're using, for
[00:42:17.400 --> 00:42:23.200]   example, a custom PyTorch loop or a or trainer or accelerate.
[00:42:23.200 --> 00:42:29.320]   But here, with blur, one of the things we can now do is we can
[00:42:29.320 --> 00:42:35.560]   pass other things we want to include in our batch. And then
[00:42:35.560 --> 00:42:44.240]   use those things, for example, in our training, to be able to
[00:42:44.240 --> 00:42:48.280]   get at certain that certain extra information. And so for
[00:42:48.280 --> 00:42:50.360]   question answering, when we're training, and we're actually
[00:42:50.360 --> 00:42:54.440]   building our validation metrics, we want to, for example, have
[00:42:54.440 --> 00:42:59.000]   that ID attribute at the very least. And so we can actually
[00:42:59.000 --> 00:43:04.240]   use a dictionary as our get x, and any extra information you
[00:43:04.240 --> 00:43:08.280]   put in there is going to be included in our inputs
[00:43:08.280 --> 00:43:11.320]   dictionary. So remember, our inputs dictionary has our input
[00:43:11.320 --> 00:43:15.520]   IDs, tension mask, and so forth. If we include if we use a
[00:43:15.520 --> 00:43:20.680]   dictionary as our get x, it will take those other things we pass
[00:43:20.680 --> 00:43:26.240]   and also included in there. So let's go ahead and take a look
[00:43:26.240 --> 00:43:30.040]   at how that works. And now we're going to start looking at
[00:43:31.400 --> 00:43:38.280]   actually training question answering model. And so here,
[00:43:38.280 --> 00:43:43.000]   I'm looking at the modeling module for question answering in
[00:43:43.000 --> 00:43:51.000]   blur, we do our pre processing that we just saw. And now we
[00:43:51.000 --> 00:43:54.600]   figured now we go to Okay, let's go ahead and build our data
[00:43:54.600 --> 00:43:59.240]   block. And so most of this stuff is should be pretty familiar.
[00:43:59.280 --> 00:44:04.200]   Remember, we're going to be predicting a start and end
[00:44:04.200 --> 00:44:09.960]   logic. So we use two category blocks, and they're each going
[00:44:09.960 --> 00:44:15.120]   to have a vocab of 128. And each of those digits is actually
[00:44:15.120 --> 00:44:21.640]   going to be the class. But one thing that's different is here,
[00:44:21.640 --> 00:44:27.320]   we're actually going to create a get x method. And this is going
[00:44:27.320 --> 00:44:33.000]   to pass a dictionary. And the only required attribute here is
[00:44:33.000 --> 00:44:36.120]   text. So you don't have to pass in extra information if you
[00:44:36.120 --> 00:44:43.560]   don't want. But this is our the actual things that we want to
[00:44:43.560 --> 00:44:49.320]   tokenize. And if we have two things, we actually include them
[00:44:49.320 --> 00:44:51.960]   in a tuple. So we're going to include our question and our
[00:44:51.960 --> 00:44:57.480]   context. And then I'm also going to pass along the ID of each of
[00:44:57.480 --> 00:45:02.360]   these items. And again, this is so that we can get to those IDs
[00:45:02.360 --> 00:45:06.640]   to to aggregate our chunks by example, rather than just for a
[00:45:06.640 --> 00:45:10.360]   particular chunk. So we can go ahead and build a data block
[00:45:10.360 --> 00:45:14.760]   like this. Just so you can see, it's always helpful to kind of
[00:45:14.760 --> 00:45:19.320]   look at in this particular type of example, right here, we have
[00:45:19.360 --> 00:45:23.600]   multiple targets, what the vocab looks like. So you can see we
[00:45:23.600 --> 00:45:32.040]   have two items. And here's the vocab for our start logits and
[00:45:32.040 --> 00:45:37.040]   our vocab for our end logits. And again, it's 128 options,
[00:45:37.040 --> 00:45:43.800]   because that's the max length that we specified. Again, we can
[00:45:43.800 --> 00:45:47.880]   go ahead and do a show batch. And if you're interested, you
[00:45:47.880 --> 00:45:51.840]   can go back to the data module for question answering and see
[00:45:51.840 --> 00:45:57.560]   how this is implemented. But what we can see here is the full
[00:45:57.560 --> 00:46:05.400]   text, right? So the full text is our question, plus the context.
[00:46:05.400 --> 00:46:10.640]   Okay. And here we're using chunking. So you can see here,
[00:46:10.640 --> 00:46:16.040]   this is repeated, right? We have the same question. And you can
[00:46:16.040 --> 00:46:25.000]   see the context is a little, has some overlap, right? With this
[00:46:25.000 --> 00:46:29.640]   right here. And then it includes some new information. And we
[00:46:29.640 --> 00:46:33.840]   also in the show batch, have a column for found whether or not
[00:46:33.840 --> 00:46:39.440]   the answer can be found in the context that we're that we have
[00:46:39.440 --> 00:46:45.880]   in the particular row. And if it can be found, what are the start
[00:46:45.880 --> 00:46:51.960]   and end token indices, and also what is the expected answer. And
[00:46:51.960 --> 00:46:59.920]   so that's our data block, right? So we have that we have our data
[00:46:59.920 --> 00:47:08.320]   loaders built. Now, this is where it gets fun. When we
[00:47:08.320 --> 00:47:12.320]   define our learner, nothing's going to look too crazy there.
[00:47:12.960 --> 00:47:17.520]   But our metrics are, this is, are very different because we're
[00:47:17.520 --> 00:47:21.240]   dealing with chunk documents. And if you go through the
[00:47:21.240 --> 00:47:28.080]   course, you'll see it is complex to be able to go through it and
[00:47:28.080 --> 00:47:36.440]   and be able to aggregate by example. And if you don't like
[00:47:36.440 --> 00:47:39.000]   for loops, you're not gonna be happy with that code either. And
[00:47:39.000 --> 00:47:42.720]   it probably could be better as well. And I'm not sure if the
[00:47:42.720 --> 00:47:45.720]   course is implemented a little bit different than the actual
[00:47:45.720 --> 00:47:49.680]   code in the Hugging Face library. But you'll see that
[00:47:49.680 --> 00:47:52.400]   it's equally complex, it's worth going through to kind of get an
[00:47:52.400 --> 00:47:58.480]   idea of why it's complex and what they're trying to do. But
[00:47:58.480 --> 00:48:02.000]   the metrics is going to be tough, because we can't just say,
[00:48:02.000 --> 00:48:07.360]   okay, here's a row from our validation set. What's the
[00:48:07.360 --> 00:48:11.640]   accuracy in terms of the ability to pick the right answer?
[00:48:11.640 --> 00:48:16.160]   Because now we need to aggregate by examples in the case that we
[00:48:16.160 --> 00:48:21.160]   have long documents. And again, if you go to this link here,
[00:48:21.160 --> 00:48:23.920]   there's this task page, which is really helpful for just any
[00:48:23.920 --> 00:48:28.240]   task. And we look at question answering, we'll see like
[00:48:28.240 --> 00:48:33.760]   proposed data sets to use and also what are the recommended
[00:48:33.760 --> 00:48:37.560]   metrics. And so since we already have a dependency on
[00:48:37.560 --> 00:48:44.280]   transformers and data sets, one that we can use is the squad
[00:48:44.280 --> 00:48:48.680]   metric. And just as a note, there's a squad and there's a
[00:48:48.680 --> 00:48:53.520]   squad v2 metric. Right now, we only support the squad metric,
[00:48:53.520 --> 00:49:01.840]   which assumes that there's an answer. The squad v2 also has
[00:49:02.880 --> 00:49:05.480]   basically a little different format that we feed into this
[00:49:05.480 --> 00:49:09.600]   when we compute the metric, because it also wants to know
[00:49:09.600 --> 00:49:13.320]   the probability that we predict for the answer not being there.
[00:49:13.320 --> 00:49:16.000]   And so we don't have that supported, but that will be at
[00:49:16.000 --> 00:49:18.960]   some point. So we're going to use the squad metric, which
[00:49:18.960 --> 00:49:26.840]   again, expects that we're going to have a and every, every
[00:49:26.840 --> 00:49:31.200]   question in our validation set is potential is answerable. And
[00:49:31.200 --> 00:49:34.280]   it's going to return two things. It's going to return the exact
[00:49:34.280 --> 00:49:38.320]   match. And so it's going to look at what we predicted. And
[00:49:38.320 --> 00:49:42.200]   remember, the validation set is going to have one or many
[00:49:42.200 --> 00:49:46.840]   potentially correct answers. It's going to look to see if we
[00:49:46.840 --> 00:49:50.160]   match one of those. If there's an exact match, there will be a
[00:49:50.160 --> 00:49:54.800]   one. If it's not, there will be a zero. So we're looking for
[00:49:54.800 --> 00:49:58.680]   exact matches. The other one that's going to calculate for us
[00:49:58.720 --> 00:50:02.800]   is the F1 score. And again, that's the harmonic mean between
[00:50:02.800 --> 00:50:05.280]   precision and recall. So it kind of balances both of those
[00:50:05.280 --> 00:50:10.200]   metrics out. And it's going to calculate that based on each
[00:50:10.200 --> 00:50:15.040]   word in the predicted sequence against the correct answer. So
[00:50:15.040 --> 00:50:20.000]   here's our question as fast AI developers, how are we going to
[00:50:20.000 --> 00:50:23.840]   properly aggregate the scores by examples when the inputs are
[00:50:23.840 --> 00:50:31.560]   chunked? And what do you guess any ideas? I know Sanyam has
[00:50:31.560 --> 00:50:35.040]   been with fast AI for a long time. And so he probably has a
[00:50:35.040 --> 00:50:39.720]   good idea of what the answer is. But before we look at it, let's
[00:50:39.720 --> 00:50:43.920]   give folks a little bit and also are there any questions before
[00:50:43.920 --> 00:50:48.080]   we look at how we're going to do this in blur at least?
[00:50:48.080 --> 00:50:50.880]   Sanyam Bhutani: Um, no, there was just a discussion everyone
[00:50:51.600 --> 00:50:54.240]   didn't accept your challenge. They agree to the fact that
[00:50:54.240 --> 00:50:57.480]   simple approaches are good. Korean denied your challenge to
[00:50:57.480 --> 00:51:00.600]   raise a PR as well. And he said simple is better than complex.
[00:51:00.600 --> 00:51:01.040]   Yeah,
[00:51:01.040 --> 00:51:03.760]   Brian Smith: yeah, totally agree. Yeah, my background is
[00:51:03.760 --> 00:51:11.720]   full stack web development. And, and so yeah, actually, I don't
[00:51:11.720 --> 00:51:17.880]   like complex code or like overly cute code that does something
[00:51:17.880 --> 00:51:21.960]   but it's hard to read. And at the same time, I know like I got
[00:51:21.960 --> 00:51:26.040]   to get things done. So sometimes I build stuff, I just go, Oh my
[00:51:26.040 --> 00:51:30.760]   gosh, that is so ugly. But at least it works. Once you have
[00:51:30.760 --> 00:51:33.280]   something that works, then you can refactor it and improve it.
[00:51:33.280 --> 00:51:38.320]   So yeah, hardly agree. So any so any guesses on how we're going
[00:51:38.320 --> 00:51:41.240]   to do this? I'm going to show it to you. It's not in GitHub. So
[00:51:41.240 --> 00:51:42.720]   don't look there and
[00:51:42.720 --> 00:51:45.160]   Sanyam Bhutani: give them another second. All right,
[00:51:45.160 --> 00:51:49.320]   give them another few seconds. I always worry like when I ask
[00:51:49.320 --> 00:51:51.840]   questions, like, first of all, there's a delay because of the
[00:51:51.840 --> 00:51:54.920]   live stream. And then people take a while to answer and I'm
[00:51:54.920 --> 00:51:59.160]   like anxiously getting nervous. Should I just move on? And then
[00:51:59.160 --> 00:52:02.160]   I go back and watch the recording is not that long. If
[00:52:02.160 --> 00:52:04.280]   you like see it from an audience perspective.
[00:52:04.280 --> 00:52:05.480]   Brian Smith: Oh, that's funny.
[00:52:05.480 --> 00:52:09.560]   Sanyam Bhutani: Mateo says it should be some sort of a way to
[00:52:09.560 --> 00:52:12.600]   divide it. So Mateo, are you leaning towards F1 by that?
[00:52:14.120 --> 00:52:17.240]   Well, I'm actually thinking kind of more like what is the
[00:52:17.240 --> 00:52:20.040]   mechanism? It's a fast AI mechanism that we're going to
[00:52:20.040 --> 00:52:32.040]   use to be able to aggregate our chunks back by example. So it's
[00:52:32.040 --> 00:52:35.480]   actually a mechanism in fast AI that's going to allow us to do
[00:52:35.480 --> 00:52:36.040]   this.
[00:52:36.040 --> 00:52:43.400]   The safe answer is callbacks, everyone. It's a callback
[00:52:43.400 --> 00:52:48.840]   mechanism. We're going to build a callback. And so I hope this
[00:52:48.840 --> 00:52:51.880]   will be interesting to folks because if you're doing anything
[00:52:51.880 --> 00:52:56.640]   with fast AI and want to move beyond, you know, kind of like
[00:52:56.640 --> 00:52:59.680]   the high level API, or even like some of the basics in the
[00:52:59.680 --> 00:53:03.880]   middle-level API, and you want to do something interesting
[00:53:03.880 --> 00:53:07.560]   that's not in the library. The only two answers are typically
[00:53:07.560 --> 00:53:12.040]   transforms and callbacks. And callbacks are immensely helpful.
[00:53:12.680 --> 00:53:17.000]   I have a link to the documentation, you can see some
[00:53:17.000 --> 00:53:21.200]   of the built in ones there. But I use them all over the place in
[00:53:21.200 --> 00:53:24.880]   blur. And just about anybody else that has created some type
[00:53:24.880 --> 00:53:30.080]   of library on top of fast AI actually uses callbacks to do
[00:53:30.080 --> 00:53:34.440]   just all kinds of interesting things at just all the different
[00:53:34.440 --> 00:53:39.040]   parts that a learner goes through in terms of pulling data,
[00:53:39.240 --> 00:53:43.840]   training, validation, and so forth. So important concept for
[00:53:43.840 --> 00:53:49.880]   anybody that's doing fast AI development to understand. So
[00:53:49.880 --> 00:53:58.560]   let's take a look. So one thing is we have this simple question
[00:53:58.560 --> 00:54:03.760]   answer model callback that extends our base model callback.
[00:54:03.760 --> 00:54:08.880]   And remember, this callback right here simply just gets our
[00:54:09.320 --> 00:54:16.520]   predictions associated to our learner. We set them in a way
[00:54:16.520 --> 00:54:22.880]   based on whatever we get from the HuggingFace model output. So
[00:54:22.880 --> 00:54:29.880]   they actually provide an object. I'm gonna go here to take a look
[00:54:29.880 --> 00:54:36.080]   at it. By the way, using VS Code to do notebook work is like
[00:54:36.120 --> 00:54:39.240]   unbelievable, like the debugging and whatnot, being able to like
[00:54:39.240 --> 00:54:42.440]   find code. So you can see remember that base model
[00:54:42.440 --> 00:54:45.960]   callback that we've probably covered a long time ago allows
[00:54:45.960 --> 00:54:51.240]   us to take what the transformer model gives us in the form of a
[00:54:51.240 --> 00:54:58.200]   model output object and assign that to our predictions. So for
[00:54:58.200 --> 00:55:02.800]   example, if the model outputs included a key called logits,
[00:55:02.920 --> 00:55:06.680]   that's our prediction generally. So we assign that to self.learn.pred
[00:55:06.680 --> 00:55:12.680]   so that fast AI continues to work correctly with it. And in
[00:55:12.680 --> 00:55:16.000]   the case of question answering, we're actually going to get a
[00:55:16.000 --> 00:55:18.680]   couple things back. We're going to get the predicted start
[00:55:18.680 --> 00:55:24.120]   logits and the predicted end logits. So we're going to assign
[00:55:24.120 --> 00:55:30.320]   pred to this tuple right here. Before we get to the really fun
[00:55:30.320 --> 00:55:33.800]   callback, we also got to think about how we're going to
[00:55:33.800 --> 00:55:39.120]   calculate loss. And so when you read the course documentation,
[00:55:39.120 --> 00:55:42.520]   they don't really put a lot of value in calculating a
[00:55:42.520 --> 00:55:47.520]   validation loss, they put more emphasis on the metrics. And you
[00:55:47.520 --> 00:55:52.160]   can see how that plays out. I still think that since we're
[00:55:52.160 --> 00:55:57.880]   actually chunking data, and we have a validation set, I think
[00:55:57.880 --> 00:56:03.040]   that it's worthwhile to calculate a loss for both our
[00:56:03.040 --> 00:56:10.440]   training and our validation. And I have this metric that I
[00:56:10.440 --> 00:56:13.960]   built. So in fast AI, you can also build custom metrics. And
[00:56:13.960 --> 00:56:18.680]   if you want to see an example of what one looks like, you can
[00:56:18.680 --> 00:56:22.360]   look at multi target loss. And essentially what this class
[00:56:22.400 --> 00:56:30.920]   does, it allows you to take for multiple targets, you can define
[00:56:30.920 --> 00:56:34.800]   the different functions you want to apply. So you can actually
[00:56:34.800 --> 00:56:39.160]   combine cross entropy with regression, you could have one
[00:56:39.160 --> 00:56:44.520]   or two targets, you can have three, four, however many more
[00:56:44.520 --> 00:56:50.040]   you want, you can weight the results of each of these. And
[00:56:50.040 --> 00:56:55.840]   then also define how you want to reduce those scores. And so
[00:56:55.840 --> 00:57:01.960]   essentially, what we're going to do in question answering is
[00:57:01.960 --> 00:57:06.880]   we're going to get a loss for both our start and end logic
[00:57:06.880 --> 00:57:09.760]   prediction. And we're going to average that and we're going to
[00:57:09.760 --> 00:57:16.760]   call that the loss. So, so we have a metric that's going to
[00:57:16.760 --> 00:57:23.200]   support both of those categories that we're trying to predict
[00:57:23.200 --> 00:57:26.600]   right, our start and end. And now we're going to get to the
[00:57:26.600 --> 00:57:29.520]   fun stuff. And I'm sure there's going to be all kinds of good
[00:57:29.520 --> 00:57:33.280]   recommendations for me to reevaluate some of this code.
[00:57:33.280 --> 00:57:38.160]   It's not going to be pretty, I'm forewarning you, but it works.
[00:57:38.160 --> 00:57:41.480]   And we're going to start with that squad metric, right. So
[00:57:41.480 --> 00:57:45.600]   that's something that the Hugging Face tasks said to use,
[00:57:45.640 --> 00:57:50.800]   they use this in the course. And just be aware that this is
[00:57:50.800 --> 00:57:55.400]   only going to work where our validation set actually has
[00:57:55.400 --> 00:58:01.800]   answers. So again, if you are using squad V2, there's going to
[00:58:01.800 --> 00:58:04.720]   be some examples in our validation set that don't have
[00:58:04.720 --> 00:58:08.560]   an answer. And so this code right now will not work for
[00:58:08.560 --> 00:58:17.520]   squad V2, but it will in the next few weeks. So question
[00:58:17.520 --> 00:58:23.800]   answer callback. And, and we're going to use this to actually
[00:58:23.800 --> 00:58:29.160]   build our validation metrics. And so what's kind of cool about
[00:58:29.160 --> 00:58:35.480]   fast AI and whatnot, is that all kinds of like ways to be able to
[00:58:35.520 --> 00:58:41.360]   do metrics is one, you can actually build a custom metric
[00:58:41.360 --> 00:58:45.520]   based on some of the base classes for metrics in fast AI.
[00:58:45.520 --> 00:58:50.560]   But I actually created this particular metric, which we just
[00:58:50.560 --> 00:58:55.760]   call value metric. And it's a way for us to calculate metrics
[00:58:55.760 --> 00:58:58.880]   in a callback, where the callback does the work. And then
[00:58:58.880 --> 00:59:03.880]   we just want to tell our learner metrics, here's the metric name
[00:59:03.880 --> 00:59:07.720]   and here's the value. So we'll see how this actually works
[00:59:07.720 --> 00:59:10.200]   right here. And this is essentially if you want to
[00:59:10.200 --> 00:59:13.320]   actually do metrics in a callback, this is the approach
[00:59:13.320 --> 00:59:19.040]   that you're going to want to take. So, so yeah, so the things
[00:59:19.040 --> 00:59:22.640]   that we're going to actually return is that exact match and
[00:59:22.640 --> 00:59:27.440]   F1 score that we talked about before. And we're going to
[00:59:27.440 --> 00:59:31.880]   create a custom metrics dictionary. And we're just going
[00:59:31.880 --> 00:59:37.800]   to set each of these keys up with the value of none. And
[00:59:37.800 --> 00:59:42.560]   then we're also going to include this do setup equal to true.
[00:59:42.560 --> 00:59:45.960]   And essentially what that means is that there's code that we
[00:59:45.960 --> 00:59:50.320]   want to run one time and get some other information from our
[00:59:50.320 --> 00:59:57.320]   learner. We could go ahead and put that in a setup function.
[00:59:57.320 --> 01:00:00.920]   And then again, these callbacks, we have all these different
[01:00:00.920 --> 01:00:07.400]   hooks, right? So, or event handlers. And so the before fit
[01:00:07.400 --> 01:00:11.360]   handler is going to start before we start actually fitting our
[01:00:11.360 --> 01:00:15.560]   models, just about anything. And if you're interested on the
[01:00:15.560 --> 01:00:24.560]   lifecycle, if you go to the fast.ai documentation, you'll
[01:00:24.560 --> 01:00:28.320]   see all the different hooks, all the different events that we
[01:00:28.320 --> 01:00:34.080]   can go ahead and create a handler for to be able to do
[01:00:34.080 --> 01:00:37.440]   whatever. And this gives you kind of a rough idea of the
[01:00:37.440 --> 01:00:45.120]   order. So again, we're going to start our stuff by looking at
[01:00:45.120 --> 01:00:51.640]   the before fit, which happens really early in the learner
[01:00:51.640 --> 01:00:56.320]   lifecycle. And we're going to call our do setup. And then
[01:00:56.320 --> 01:00:59.640]   after we do our setup, we're going to say do setup to false
[01:00:59.640 --> 01:01:03.880]   just so that doesn't get ran anymore. And you can see some
[01:01:03.880 --> 01:01:07.360]   of the things that we're doing in here is actually getting our
[01:01:07.360 --> 01:01:11.720]   our Bluer transform, right? So with question answering, we
[01:01:11.720 --> 01:01:15.000]   have a QA batch tokenized transform. And we're going to
[01:01:15.000 --> 01:01:19.840]   use this to get our tokenizer and our tokenizer quarks to be
[01:01:19.840 --> 01:01:23.480]   used in other parts of the callback. Then we're going to
[01:01:23.480 --> 01:01:29.600]   set our custom metrics. And again, we're going to use this
[01:01:29.600 --> 01:01:33.640]   value metric, which is a way for us to have the callback to
[01:01:33.640 --> 01:01:36.400]   say, here's the metric name, and here's the value I calculated
[01:01:36.400 --> 01:01:42.240]   for you, show it in the progress as you're training. And then
[01:01:42.240 --> 01:01:47.280]   we're going to tell the learner about that and say, learner,
[01:01:47.280 --> 01:01:50.880]   your metrics are now whatever you had before. And also these
[01:01:50.880 --> 01:01:56.520]   are metrics that I'm including here. So because this only makes
[01:01:56.520 --> 01:02:00.800]   sense at for our validation set, right, that's where we want to
[01:02:00.800 --> 01:02:05.840]   calculate our metrics, we need to use the before batch and
[01:02:05.840 --> 01:02:10.000]   after batch hooks, but we don't want them to run when we're
[01:02:10.000 --> 01:02:13.880]   training. And a simple way to do that is we can see if we're
[01:02:13.880 --> 01:02:17.120]   training or we don't want to use them when we're doing inference.
[01:02:17.360 --> 01:02:23.040]   And in that case, there would be no targets. So using this code
[01:02:23.040 --> 01:02:26.840]   right here, we can ensure that this before batch and this
[01:02:26.840 --> 01:02:32.520]   after batch is only going to run when we're looking at our
[01:02:32.520 --> 01:02:38.520]   validation batches. And notice that remember, we talked about
[01:02:38.520 --> 01:02:42.960]   passing extra information in that dictionary. With this
[01:02:42.960 --> 01:02:48.880]   particular callback, we can pass a, our data set, right,
[01:02:48.880 --> 01:02:52.880]   which is going to be our, our data frame. And again, this is
[01:02:52.880 --> 01:02:57.000]   working right now, this is going to be better. There's some
[01:02:57.000 --> 01:02:59.280]   things happening in the code that I kind of have hard coded
[01:02:59.280 --> 01:03:05.360]   that will become more argument driven. But we're going to pass
[01:03:05.360 --> 01:03:09.920]   our, our data set that we're using in our data block, right,
[01:03:09.920 --> 01:03:14.280]   which is that squad data frame, the attribute of where the ID
[01:03:14.280 --> 01:03:18.960]   is, so we can aggregate our chunks by ID. And then we're
[01:03:18.960 --> 01:03:22.720]   going to follow the code from the course documentation pretty
[01:03:22.720 --> 01:03:27.800]   much. And we're going to consider the 20 best predictions
[01:03:27.800 --> 01:03:36.240]   for our, our start and end logits. So we have to capture
[01:03:36.240 --> 01:03:39.880]   this information before batch, this extra information that our
[01:03:39.880 --> 01:03:42.920]   model's not going to use, because this is not going to be
[01:03:42.920 --> 01:03:47.880]   available afterwards. So using the before batch hook, you can
[01:03:47.880 --> 01:03:51.960]   see that I'm actually for every single chunked example coming
[01:03:51.960 --> 01:03:57.560]   in, I'm going to get the ID attribute, and I'm going to get
[01:03:57.560 --> 01:04:05.320]   the inputs. And I am, after every batch, I'm going to
[01:04:05.920 --> 01:04:10.720]   essentially create this results list. And it's going to have a
[01:04:10.720 --> 01:04:15.560]   dictionary for each batch. So each batch right now we have it
[01:04:15.560 --> 01:04:17.680]   considered, we have it set up for a batch size, I think of
[01:04:17.680 --> 01:04:21.600]   four or eight. And we're going to create a dictionary so that
[01:04:21.600 --> 01:04:27.200]   we can do this aggregation that includes the example ID, the
[01:04:27.200 --> 01:04:34.120]   inputs associated to the chunk, the start logits and end logits
[01:04:34.120 --> 01:04:42.760]   for that chunk. Okay, so again, we're going to have, if an
[01:04:42.760 --> 01:04:47.520]   example was broken up into four chunks, we're going to have four
[01:04:47.520 --> 01:04:54.120]   records in here with the same ID attribute. So we're going to do
[01:04:54.120 --> 01:05:00.240]   that in our before batch and after batch to be able to get
[01:05:00.240 --> 01:05:05.840]   that information. And before validate, we're going to make
[01:05:05.840 --> 01:05:10.600]   sure that our self dot results is an empty list. And then the
[01:05:10.600 --> 01:05:16.720]   before and after batch will populate this. And after
[01:05:16.720 --> 01:05:19.920]   validation is all done, there's another convenient hook called
[01:05:19.920 --> 01:05:25.440]   after validate, this is where we want to do our aggregation. And
[01:05:25.440 --> 01:05:28.680]   if you don't like nested for loops, get ready to be very
[01:05:28.680 --> 01:05:34.560]   unhappy. But this is from the course, there's probably ways to
[01:05:34.560 --> 01:05:39.000]   clean this up and make it more efficient and cleaner. But I
[01:05:39.000 --> 01:05:42.760]   just wanted to put something in there that at least initially
[01:05:42.760 --> 01:05:47.120]   was working. And again, this some ugly code, so we just want
[01:05:47.120 --> 01:05:49.640]   to look at the validation set, right. So here, I'm assuming
[01:05:49.640 --> 01:05:53.120]   it's a data frame. And I can do this type of syntax, this isn't
[01:05:53.120 --> 01:05:58.280]   good, this will change. But it's in there right now, just to be
[01:05:58.280 --> 01:06:02.640]   able to test things out and make sure it works. So you can see,
[01:06:02.640 --> 01:06:06.520]   we're going to go through each of the chunks here, and we're
[01:06:06.520 --> 01:06:10.000]   going to create this dictionary. And the dictionary is going to
[01:06:10.000 --> 01:06:16.120]   have a key, which is the example ID. And then it's going to have
[01:06:16.120 --> 01:06:26.240]   the index of each chunk from this self dot results list. So
[01:06:26.240 --> 01:06:29.760]   we can go find for each example, find the chunks that we should
[01:06:29.760 --> 01:06:36.920]   look at. Then what we're going to do is actually go through
[01:06:36.920 --> 01:06:48.480]   each row in our data set, get the example and get the process
[01:06:48.480 --> 01:06:53.200]   context, right, that we created when we are pre processing. And
[01:06:53.200 --> 01:06:58.120]   now for each example, we want to look at all the different
[01:06:58.120 --> 01:07:06.760]   chunks, and see which ones are going to give us the best, which
[01:07:06.760 --> 01:07:10.000]   one's going to give us the best answer prediction, the best
[01:07:10.000 --> 01:07:14.320]   start and end logits prediction. And so we're going to go ahead
[01:07:14.320 --> 01:07:18.800]   and use that information that we actually captured in that self
[01:07:18.800 --> 01:07:22.680]   dot results list, we're going to use the input IDs, the
[01:07:22.800 --> 01:07:27.840]   start logis and the end logis to do this. And we're only going
[01:07:27.840 --> 01:07:32.320]   to look at the end best. And again, this is from the course,
[01:07:32.320 --> 01:07:36.600]   I modified this a little bit, but this is pretty much exactly
[01:07:36.600 --> 01:07:40.240]   what you see in the section seven of the course for question
[01:07:40.240 --> 01:07:44.440]   answering. So we're going to consider the by default, the 20
[01:07:44.440 --> 01:07:49.720]   best start indices, the 20 best end indices. And now we're going
[01:07:49.720 --> 01:07:53.520]   to go through here, we're going to make sure that we're not
[01:07:53.520 --> 01:08:01.520]   adding a potential answer, if there is no answer, or if the
[01:08:01.520 --> 01:08:08.000]   the the answers have a length that is either less than zero,
[01:08:08.000 --> 01:08:10.880]   or it's greater than the max answer length, we're not going
[01:08:10.880 --> 01:08:13.720]   to create a potential answer. So here, we're, we're looping
[01:08:13.720 --> 01:08:18.720]   through all our all our start and end indices. And again,
[01:08:18.720 --> 01:08:21.960]   we're just adding in the two probabilities to give us an
[01:08:21.960 --> 01:08:26.120]   overall probability, which column the logit score. And
[01:08:26.120 --> 01:08:29.080]   again, we're using those input IDs that we captured in our
[01:08:29.080 --> 01:08:33.400]   before batch to do decode and actually find the answer text,
[01:08:33.400 --> 01:08:37.160]   because the metric it needs to, that we're using that squad
[01:08:37.160 --> 01:08:42.400]   metric is going to require the actual text. So we can actually
[01:08:42.400 --> 01:08:48.400]   capture that information here. And what we're at the end result
[01:08:48.480 --> 01:08:52.760]   that we process a single example, we are going to have a
[01:08:52.760 --> 01:08:59.520]   a list of potential answers. And we're going to do that for
[01:08:59.520 --> 01:09:06.280]   every example. But for each one, we're going to, after this, we
[01:09:06.280 --> 01:09:08.800]   have those potential ones is actually figure out what is the
[01:09:08.800 --> 01:09:13.240]   best answer. And we're going to append that to this a predicted
[01:09:13.240 --> 01:09:17.960]   answers list. And it needs to have this format, which is our
[01:09:18.000 --> 01:09:23.600]   example ID that this is referencing and our prediction
[01:09:23.600 --> 01:09:29.160]   text, right, which is going to be the best answer text. And so
[01:09:29.160 --> 01:09:32.760]   the answer that we have with the highest score is going to be the
[01:09:32.760 --> 01:09:36.440]   one that we select here. If there are no answers, we just
[01:09:36.440 --> 01:09:41.080]   sent this equal to an empty string. So once we've done that
[01:09:41.080 --> 01:09:44.480]   for every one of our examples, right, so for every example,
[01:09:44.480 --> 01:09:48.120]   we're looking at those chunks, we're figuring out, okay, for
[01:09:48.120 --> 01:09:52.560]   given all those chunks, what is actually the best result we
[01:09:52.560 --> 01:09:55.800]   have. And we're going to use that for our validation metric
[01:09:55.800 --> 01:10:01.000]   for that particular example, we then need to compare that to the
[01:10:01.000 --> 01:10:05.280]   reference or theoretical answers, which, yes, this will
[01:10:05.280 --> 01:10:09.600]   be changed. But we're looking at the validation set. And we want
[01:10:09.600 --> 01:10:15.840]   to compare that to these particular IDs. And the answers
[01:10:15.840 --> 01:10:20.080]   is going to have that dictionary, right, that has the
[01:10:20.080 --> 01:10:26.480]   one or many text attributes, right for potential answers, as
[01:10:26.480 --> 01:10:29.920]   well as their start characters. So that's going to be our
[01:10:29.920 --> 01:10:34.080]   theoretical answers. And the reason we're doing this is
[01:10:34.080 --> 01:10:38.440]   because this is what this squad metric expects is this type of
[01:10:38.440 --> 01:10:45.400]   format. And so we can actually call the squad to it's really
[01:10:45.400 --> 01:10:50.200]   the squad metric, this will be renamed when this thing is
[01:10:50.200 --> 01:10:53.560]   pushed to GitHub. But we can actually then calculate the
[01:10:53.560 --> 01:10:58.840]   metrics based on this information. And we're gonna put
[01:10:58.840 --> 01:11:02.080]   that into a dictionary. And basically the dictionary, the
[01:11:02.080 --> 01:11:06.160]   keys are going to be exact match and F1. And then the values will
[01:11:06.160 --> 01:11:13.520]   be the actual scores. And we can assign those to our custom
[01:11:13.520 --> 01:11:20.200]   metrics dictionary. And what's nice is we have this metric
[01:11:20.200 --> 01:11:27.800]   value. So the value metric will know to look down here. Notice
[01:11:27.800 --> 01:11:33.080]   this self metric value, and it will pass metric key as K
[01:11:33.080 --> 01:11:37.640]   whatever that is, that could be exact match. And what this does
[01:11:37.640 --> 01:11:42.920]   is when we start training, it will know like these are part of
[01:11:42.920 --> 01:11:47.000]   our validation metrics. And when it's looking for the value, it
[01:11:47.000 --> 01:11:51.520]   will call this particular method. And that will return the
[01:11:51.520 --> 01:11:58.080]   value for that key. So if we train, notice now we have to
[01:11:58.080 --> 01:12:02.360]   include this fit callback. So anything that's metrics oriented
[01:12:02.360 --> 01:12:05.920]   that you don't need to export and have for inference,
[01:12:05.920 --> 01:12:10.200]   probably better as a fit callback. And we're going to
[01:12:10.200 --> 01:12:14.080]   set our loss function to multi target loss, right? Because we
[01:12:14.080 --> 01:12:16.080]   have two things that we're trying to predict, we want to
[01:12:16.080 --> 01:12:19.400]   take the mean of the loss and call that the loss for our
[01:12:19.400 --> 01:12:24.160]   particular training in this task. And once we have all that
[01:12:24.160 --> 01:12:28.200]   information, we could go ahead and work with FastAI just like
[01:12:28.200 --> 01:12:36.920]   usual, run LR finder, actually do a fit one cycle. And this is
[01:12:36.920 --> 01:12:43.080]   one weird thing with VS code is sometimes it doesn't show the
[01:12:43.080 --> 01:12:51.880]   results. So what I'm going to do is restart this. They're all
[01:12:51.880 --> 01:12:55.000]   I'm going to run all so that we can actually see how those
[01:12:55.040 --> 01:13:00.320]   metrics are represented. But as this is working, is there any
[01:13:00.320 --> 01:13:04.440]   questions or did I thoroughly confuse everyone with that
[01:13:04.440 --> 01:13:08.720]   callback code or make everyone angry with the number of loops
[01:13:08.720 --> 01:13:09.680]   and if statements?
[01:13:09.680 --> 01:13:14.000]   No one is upset. I think we'll have to go back and study a bit.
[01:13:14.000 --> 01:13:18.160]   So yeah, this is like this definitely more the advanced
[01:13:18.160 --> 01:13:20.720]   side of FastAI development. So when you're doing like
[01:13:20.720 --> 01:13:26.680]   transform custom transforms or callbacks. And as I said, I'm
[01:13:26.680 --> 01:13:29.080]   gonna check this code in the next 24 hours that you can
[01:13:29.080 --> 01:13:32.120]   actually go through it and step through it and and play with it
[01:13:32.120 --> 01:13:38.120]   and see how it works. And make PR if you got some better ways
[01:13:38.120 --> 01:13:45.960]   of doing what I'm doing there in my ugly fashion. But yeah, so
[01:13:45.960 --> 01:13:49.440]   this is a way for us to essentially, we can use a
[01:13:49.440 --> 01:13:54.160]   callback to so we can essentially our data block we
[01:13:54.160 --> 01:13:57.880]   can or a pre processing, we can break long documents into a
[01:13:57.880 --> 01:14:02.600]   bunch of chunks. But we can use that callback to aggregate
[01:14:02.600 --> 01:14:06.800]   those chunks, for example, to really figure out how well our
[01:14:06.800 --> 01:14:12.840]   model is answering the example, you know, however long it is. So
[01:14:12.840 --> 01:14:17.840]   that's the purpose behind it. And it's nice because hopefully,
[01:14:18.240 --> 01:14:20.720]   you won't have to write your own callback and you can just use it
[01:14:20.720 --> 01:14:24.720]   and get the results and then you're good to go. But it does
[01:14:24.720 --> 01:14:30.680]   show at least kind of one approach for doing this with
[01:14:30.680 --> 01:14:34.280]   FastAI. And if you look at the course documentation, it's it's
[01:14:34.280 --> 01:14:37.000]   kind of weird how how they do it, like they actually train
[01:14:37.000 --> 01:14:40.720]   that the end of training, they run the validation metrics. And
[01:14:40.720 --> 01:14:44.680]   the code again, looks very similar to what I have. But they
[01:14:44.680 --> 01:14:47.520]   don't calculate a loss for the validation set. And it's not
[01:14:47.520 --> 01:14:50.240]   really incorporated in the trains kind of like something
[01:14:50.240 --> 01:14:53.600]   you do after training separately, whereas in FastAI,
[01:14:53.600 --> 01:14:56.240]   you have a training step and then you have the validation
[01:14:56.240 --> 01:14:57.840]   steps all kind of built in.
[01:14:57.840 --> 01:15:04.560]   Sanyam Bhutani: Mani has a relevant question. When are you
[01:15:04.560 --> 01:15:08.920]   when you are writing custom callbacks, how do you test them?
[01:15:10.360 --> 01:15:17.200]   So for testing callbacks, I don't know if there's any like,
[01:15:17.200 --> 01:15:26.760]   I don't know if there's any one best approach I can think of for
[01:15:26.760 --> 01:15:31.160]   something like this, I would probably calculate these metrics
[01:15:31.160 --> 01:15:35.280]   outside of the callback. So once you see this code, things will
[01:15:35.280 --> 01:15:40.200]   probably be broken up a little bit where you can actually the
[01:15:40.200 --> 01:15:43.360]   code that we actually use to figure out the score was
[01:15:43.360 --> 01:15:47.840]   probably broken out into its own function. And then that would
[01:15:47.840 --> 01:15:51.120]   be tested. And I would compare that the results of just running
[01:15:51.120 --> 01:15:58.120]   that function with, you know, the predictions and targets work
[01:15:58.120 --> 01:16:00.920]   the same as when it runs in the callback. I don't know if
[01:16:00.920 --> 01:16:06.080]   there's any like, like one approach I can think of to doing
[01:16:06.080 --> 01:16:08.520]   the callback. And it probably just depends also what you're
[01:16:08.520 --> 01:16:12.960]   doing in the callback, as well. Because I mean, some people are
[01:16:12.960 --> 01:16:17.840]   pulling out like activations, like your the fit one cycle,
[01:16:17.840 --> 01:16:20.600]   that's actually a callback. So all your alert rate schedulers
[01:16:20.600 --> 01:16:27.360]   are callbacks. And I know for those they test them. A lot of
[01:16:27.360 --> 01:16:29.800]   ways, I think it's just a visual test by you can actually plot
[01:16:29.800 --> 01:16:33.840]   the schedule and see if yes, it's working how you expected
[01:16:33.840 --> 01:16:35.480]   based on the parameters you pass to it.
[01:16:37.920 --> 01:16:42.080]   It's like the one batch testing approach, right? Where you just
[01:16:42.080 --> 01:16:44.400]   like check on a batch and see if it goes through.
[01:16:44.400 --> 01:16:48.000]   Yeah, I mean, that's part of it. Yeah, it's kind of just doing
[01:16:48.000 --> 01:16:50.840]   like, yeah, I mean, and that's something I think I've showed.
[01:16:50.840 --> 01:16:53.520]   And like, what you just said is that when you're building fast
[01:16:53.520 --> 01:16:59.480]   at AI models, is, it's always good to look at a batch of data
[01:16:59.480 --> 01:17:03.520]   to pass a batch of data into your model and see what the
[01:17:03.520 --> 01:17:09.000]   output is, you know, make sure it's expected. And then as much
[01:17:09.000 --> 01:17:15.360]   as you can is to just write some tests inside of like, so nb
[01:17:15.360 --> 01:17:18.920]   dev has like a lot of functions from fast core, like tests,
[01:17:18.920 --> 01:17:21.680]   equality and whatnot, to try to include those and create little
[01:17:21.680 --> 01:17:26.240]   mockups. So that might be a good approach. And obviously,
[01:17:26.240 --> 01:17:32.160]   sometimes creating those mockups isn't isn't trivial. So but
[01:17:32.160 --> 01:17:34.920]   yeah, you can see here just so just to I just want to mention
[01:17:34.920 --> 01:17:38.040]   that, remember those metrics, so we define them in our callback,
[01:17:38.040 --> 01:17:41.920]   our exact match metric and our f1, you can see that these are
[01:17:41.920 --> 01:17:48.760]   now included as part of the information in our learner. So
[01:17:48.760 --> 01:17:56.360]   we can actually visualize that and also get those back. For our
[01:17:56.960 --> 01:18:04.720]   show results, looks very similar to the show batch, except now we
[01:18:04.720 --> 01:18:08.640]   also include the predicted start in and the predicted answer.
[01:18:08.640 --> 01:18:12.880]   And you can see that at least with these examples, our model,
[01:18:12.880 --> 01:18:16.320]   even though we're using a really small subset did pretty well.
[01:18:16.320 --> 01:18:24.120]   For prediction, one of the things that I'm working on in
[01:18:25.000 --> 01:18:28.320]   for the next few weeks is to actually have a blur predict QA
[01:18:28.320 --> 01:18:31.240]   function that will return results that look just like what
[01:18:31.240 --> 01:18:35.240]   the pipeline, the hugging face pipeline returns for question
[01:18:35.240 --> 01:18:40.560]   answering. But for right now, we can simply pass in data and we
[01:18:40.560 --> 01:18:43.520]   have to include an ID attribute, you can call it whatever you
[01:18:43.520 --> 01:18:49.040]   want. Because that's what we trained our model to look at
[01:18:49.120 --> 01:18:54.920]   examples with these particular attributes. And we can just run
[01:18:54.920 --> 01:18:57.880]   blur predict to get our results. And the results will have our
[01:18:57.880 --> 01:19:02.080]   labels, prediction indices and probabilities for each
[01:19:02.080 --> 01:19:07.120]   prediction. And again, these are the start and end logits. And
[01:19:07.120 --> 01:19:12.880]   with a little bit of post processing, we can actually
[01:19:12.880 --> 01:19:17.200]   iterate through our predictions and see for this question right
[01:19:17.200 --> 01:19:22.840]   here, what did George Lucas make? We correctly get Star
[01:19:22.840 --> 01:19:31.040]   Wars. And we can also pass in multiple examples, multiple
[01:19:31.040 --> 01:19:34.320]   things that we want to do inference on. And we can iterate
[01:19:34.320 --> 01:19:38.440]   those over those as well. And we can see that we get Star Wars
[01:19:38.440 --> 01:19:42.320]   when was Star Wars when did it come out 1977? What did George
[01:19:42.320 --> 01:19:46.520]   Lucas do directed and produced it. So all got good results.
[01:19:46.960 --> 01:19:55.320]   There. We can also, if we wanted to do batch inferencing, so
[01:19:55.320 --> 01:19:59.480]   let's say we had a lot of examples, we can use our data
[01:19:59.480 --> 01:20:03.840]   loaders to create a test data loader of our inference data.
[01:20:03.840 --> 01:20:10.360]   And here I'm throwing in a little tricky one. You play
[01:20:10.360 --> 01:20:14.400]   Spock in the movie, obviously, Spock is not in Star Wars, at
[01:20:14.400 --> 01:20:19.960]   least that I'm aware of. And you would hope that we would get no
[01:20:19.960 --> 01:20:26.360]   answer for this one. And fortunately, our model, even
[01:20:26.360 --> 01:20:30.640]   with the very small subset that we're training on, we get no
[01:20:30.640 --> 01:20:35.520]   answer, right? So it's empty. Because this question is not
[01:20:35.520 --> 01:20:38.800]   answerable based on this context. But this shows us
[01:20:38.800 --> 01:20:42.840]   another, this is just another way that you can get
[01:20:42.840 --> 01:20:46.160]   predictions and then do post processing to figure out the
[01:20:46.160 --> 01:20:50.880]   appropriate answers. If we want to, again, work with the FASTI
[01:20:50.880 --> 01:20:57.120]   app model, so we could unfreeze, we can fit some more and show
[01:20:57.120 --> 01:21:03.200]   results again, getting pretty good predictions for this
[01:21:03.200 --> 01:21:08.200]   particular model. I'm going to try to trick it again, who plays
[01:21:08.200 --> 01:21:17.720]   Spock, and okay, still good, getting an empty answer. And we
[01:21:17.720 --> 01:21:23.200]   can go ahead and do our deployer model, just like we deploy any
[01:21:23.200 --> 01:21:26.880]   other FASTI model, we can export it. The only thing we got to do
[01:21:26.880 --> 01:21:31.640]   is, when we export it, we pick it, it has problems with this
[01:21:31.640 --> 01:21:36.680]   multi target loss. But fortunately, we don't need the
[01:21:36.680 --> 01:21:40.800]   loss when we're doing inference. So what I'm doing here is just
[01:21:40.800 --> 01:21:44.680]   setting the loss function to cross entropy loss, and then
[01:21:44.680 --> 01:21:51.960]   doing the export so that it works successfully. Once we
[01:21:51.960 --> 01:21:57.160]   actually load the learner for inference, we can set it back to
[01:21:57.160 --> 01:22:00.080]   this and we want to set it back to this because this actually,
[01:22:00.080 --> 01:22:04.320]   even though it, we're not calculating a loss here, it
[01:22:04.320 --> 01:22:09.360]   actually does some decoding of the predictions. And these are
[01:22:09.360 --> 01:22:15.640]   just how loss functions work in FASTI. So we include that back,
[01:22:15.640 --> 01:22:23.400]   we can go ahead and pass a bunch of rows in here for inference.
[01:22:23.400 --> 01:22:26.760]   I'm going to really try to confuse it, who plays Spock in
[01:22:26.760 --> 01:22:33.520]   1977. We can do our inference. And you can see at least still
[01:22:33.520 --> 01:22:38.680]   did a pretty good job, except I was able to fool this
[01:22:38.680 --> 01:22:41.520]   particular example into predicting that George Lucas
[01:22:41.520 --> 01:22:45.360]   plays Spock in 1977. And you can kind of see that maybe with
[01:22:45.360 --> 01:22:50.040]   more training, or with more examples, this would learn that
[01:22:50.040 --> 01:22:57.200]   this is not the answer. But it got this one wrong. The high
[01:22:57.200 --> 01:23:02.040]   level API is something that I'm working on. It doesn't quite
[01:23:02.040 --> 01:23:06.960]   work with some of the new bits in V2. But it will. So we won't
[01:23:06.960 --> 01:23:10.560]   look at that. And that's basically it. So that's how we
[01:23:10.560 --> 01:23:20.360]   do training and inference. And let's see if there's anything
[01:23:20.360 --> 01:23:29.120]   else. So again, there will be a blur predict QA method. And
[01:23:29.520 --> 01:23:32.640]   that's basically it. And so these bits will be checked in
[01:23:32.640 --> 01:23:36.840]   in the next 24 hours. And really encourage you all to go back
[01:23:36.840 --> 01:23:40.760]   through the course, look at the relevant question answering
[01:23:40.760 --> 01:23:44.840]   information. So I have links to that in the slides here to where
[01:23:44.840 --> 01:23:48.640]   that exists in the course. And try training your own question
[01:23:48.640 --> 01:23:53.400]   answering model. And if you do something interesting, or it
[01:23:53.400 --> 01:23:58.880]   breaks, or you can refactor my code and make it look pretty,
[01:23:59.200 --> 01:24:03.280]   write a blog explaining to it, let us know and we will promote
[01:24:03.280 --> 01:24:07.920]   it on the Twittersphere. And as a third thing, if you really
[01:24:07.920 --> 01:24:13.000]   want to kind of test your fast AI skills, look at the
[01:24:13.000 --> 01:24:15.800]   documentation for a callback and try to create even just a
[01:24:15.800 --> 01:24:19.720]   trivial callback that you can plug into your learner just like
[01:24:19.720 --> 01:24:25.360]   the callback that I created for handling aggregating the chunks
[01:24:25.360 --> 01:24:28.360]   by example. See if you can do something simple just to
[01:24:28.360 --> 01:24:32.880]   demonstrate that you understand the mechanics and, and, and how
[01:24:32.880 --> 01:24:36.000]   you can use it. And there's some examples of some trivial basic
[01:24:36.000 --> 01:24:40.440]   ones in the documentation, if you click on this, this link. So
[01:24:40.440 --> 01:24:41.840]   I really encourage you to try to do that, because it's
[01:24:41.840 --> 01:24:46.000]   definitely one of the more sophisticated mechanisms and
[01:24:46.000 --> 01:24:51.440]   fast AI, and something you're going to want to probably build
[01:24:51.440 --> 01:24:57.600]   as you start doing more complex things. And wanting to get
[01:24:57.600 --> 01:25:02.680]   information in or out of your model throughout the entire
[01:25:02.680 --> 01:25:08.720]   like learner training validation lifecycle. So that's it for me.
[01:25:08.720 --> 01:25:12.760]   Any other final questions? I know we're getting late here.
[01:25:12.760 --> 01:25:18.440]   No worries about it at all. I wanted to say people are always
[01:25:18.440 --> 01:25:22.400]   happy to learn from you. I don't. I don't see any
[01:25:22.400 --> 01:25:25.160]   questions. So we can wrap up with the announcement that will
[01:25:25.160 --> 01:25:27.440]   be going on a break. Wait, do you want to share that?
[01:25:27.440 --> 01:25:33.120]   Yep. Yeah. So next week, we'll be taking a break. So we'll give
[01:25:33.120 --> 01:25:37.000]   you plenty of time to look at the token classification
[01:25:37.000 --> 01:25:44.080]   question answering bits. And, and also go through the course
[01:25:44.080 --> 01:25:46.080]   if you can. So you have a couple weeks to at least go through
[01:25:46.080 --> 01:25:48.960]   those particular bits and that are in this slide and the slide
[01:25:48.960 --> 01:25:53.160]   before. And in a couple weeks, we'll pick up and look at
[01:25:53.160 --> 01:25:58.800]   summarization and translation. And after that, we'll explore
[01:25:58.800 --> 01:26:01.960]   kind of the really the bedrock of all this, which is language
[01:26:01.960 --> 01:26:06.160]   modeling, and talk about competitions and prizes and all
[01:26:06.160 --> 01:26:07.080]   that other good stuff.
[01:26:07.080 --> 01:26:14.360]   I have two quick announcements to tell everyone let me quickly
[01:26:14.360 --> 01:26:18.520]   share my screen once again, I find the correct window here, I
[01:26:18.520 --> 01:26:22.320]   was able to grab it. So I'm we're putting good use to the
[01:26:22.320 --> 01:26:24.960]   time while we will be away. So for the next two weeks, we
[01:26:24.960 --> 01:26:28.960]   won't have the hugging face session like we'd said, but next
[01:26:28.960 --> 01:26:33.960]   week, and early so at 8am Pacific, we'll be having hosting
[01:26:33.960 --> 01:26:37.920]   someone from the tracks framework, they'll be telling us
[01:26:37.920 --> 01:26:40.680]   more about it. So this is part of the JAG series if you all are
[01:26:40.680 --> 01:26:45.880]   interested. And we're having a fight between coffee and tea. So
[01:26:45.880 --> 01:26:50.800]   the Sunday after that we'll be hosting Leticia, who has this
[01:26:51.000 --> 01:26:55.000]   fantastic YouTube channel. I would highly encourage everyone
[01:26:55.000 --> 01:27:00.840]   to check this out. Our head of data science, Brian Bischoff,
[01:27:00.840 --> 01:27:04.080]   his name is not Dr. Donut will be hosting this. So as you can
[01:27:04.080 --> 01:27:08.160]   see, we're putting good use of the time while we're away. So if
[01:27:08.160 --> 01:27:11.160]   you're interested in joining those sessions, please keep an
[01:27:11.160 --> 01:27:12.920]   eye out on Twitter, we'll announce them there.
[01:27:12.920 --> 01:27:17.040]   Awesome. I'm already a fan of that YouTube. I will have to
[01:27:17.040 --> 01:27:20.400]   subscribe just based on the name of the channel.
[01:27:20.400 --> 01:27:25.840]   Maybe maybe we should ask the loser to rename their channel.
[01:27:25.840 --> 01:27:32.560]   Yeah, you see, we'll see how that goes over there. AI which
[01:27:32.560 --> 01:27:33.960]   has got a nice rhyme. So
[01:27:33.960 --> 01:27:38.200]   I would agree with that. Awesome. Everyone, please
[01:27:38.200 --> 01:27:42.720]   connect with Wade on Twitter. And we'll see Wade in two weeks
[01:27:42.720 --> 01:27:46.080]   from now for those two weeks, we'll have those sessions. And
[01:27:46.160 --> 01:27:49.440]   thanks again for joining. We'll see you in there. Thanks.
[01:27:49.440 --> 01:27:51.000]   Yeah, thank you.
[01:27:51.000 --> 01:28:01.000]   [BLANK_AUDIO]

