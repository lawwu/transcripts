
[00:00:00.000 --> 00:00:05.100]   When using Jupyter Notebooks, once I'm finished an experiment, I need to tell
[00:00:05.100 --> 00:00:09.840]   Weights & Biases by calling wandb.finish(). If we were just running a
[00:00:09.840 --> 00:00:14.420]   Python script, the Weights & Biases run would be automatically finished and stop
[00:00:14.420 --> 00:00:19.000]   tracking metrics when the Python script has finished running. Because notebooks
[00:00:19.000 --> 00:00:24.400]   behave a bit differently, we need to call wandb.finish() to explicitly finish
[00:00:24.400 --> 00:00:30.340]   the run. Here we are starting the run with wandb.init() and logging data with
[00:00:30.340 --> 00:00:35.320]   wandb.log() and then calling wandb.finish() to end the run. We can do the same
[00:00:35.320 --> 00:00:39.920]   thing by changing it to use a Python feature called context managers where we
[00:00:39.920 --> 00:00:45.280]   use the with keyword and indent the code. This will call wandb.finish() when the
[00:00:45.280 --> 00:00:49.280]   code within the context manager is finished running. You might recognize
[00:00:49.280 --> 00:00:56.400]   this if you've ever opened a file with Python by calling with open. Okay so
[00:00:56.400 --> 00:01:00.560]   let's start experimenting. I'm going to go off and try a few things that I think
[00:01:00.560 --> 00:01:07.760]   might improve my model. So I'm back in my Weights & Biases workspace after trying
[00:01:07.760 --> 00:01:11.720]   a few things to improve my model. We can see any log metrics are shown on the
[00:01:11.720 --> 00:01:15.720]   same charts and we can hover over each line and see which run produced that
[00:01:15.720 --> 00:01:20.300]   data. We can navigate over to the runs table and see all of the different runs
[00:01:20.300 --> 00:01:25.380]   data in the one table. We can also pin columns to the runs tab on the left and
[00:01:25.380 --> 00:01:30.440]   navigate back to our workspace and see that metric persists alongside our run
[00:01:30.440 --> 00:01:38.600]   names. We can also interactively organize these runs, in this case I'm sorting by
[00:01:38.600 --> 00:01:53.160]   my loss. Also like we did in the runs page we can interactively edit these
[00:01:53.160 --> 00:02:05.840]   plots or create new ones. We can also delete panels if they're not useful. We
[00:02:05.840 --> 00:02:09.580]   can also do custom analysis within Weights & Biases. Say we want to see a
[00:02:09.580 --> 00:02:15.400]   plot to analyze how training time is affecting the model performance. We can
[00:02:15.400 --> 00:02:18.720]   do this analysis to try and discover additional ways we can optimize our
[00:02:18.720 --> 00:02:23.200]   models. Being able to rapidly iterate on experiments is a huge advantage when it
[00:02:23.200 --> 00:02:26.240]   comes to being an efficient machine learning practitioner. Although we
[00:02:26.240 --> 00:02:30.280]   encourage rapid experimentation and quick feedback loops, there's no
[00:02:30.280 --> 00:02:34.240]   substitute for thinking deeply about what you want to experiment with. Now you
[00:02:34.240 --> 00:02:37.200]   can see how you can use Weights & Biases to make sure you're productive and
[00:02:37.200 --> 00:02:41.080]   efficient while doing so. Now that you're up and running tracking your
[00:02:41.080 --> 00:02:45.780]   experiments with Weights & Biases, what next? Beyond experiment tracking, Weights &
[00:02:45.780 --> 00:02:49.320]   Biases has other features that help with different problems when trying to work
[00:02:49.320 --> 00:02:53.720]   on machine learning projects. From collaborative features for teams to
[00:02:53.720 --> 00:02:57.120]   evaluation tools when it comes to understanding your model predictions. In
[00:02:57.120 --> 00:03:01.680]   the next video I'll touch on some of the things that are possible so that you
[00:03:01.680 --> 00:03:04.120]   know to learn more about them whenever you need them.

