
[00:00:00.000 --> 00:00:04.800]   So because there doesn't seem to be a model really since GPT-4 that seems to be significantly better,
[00:00:04.800 --> 00:00:10.240]   there seems to be the hypothesis that potentially we're hitting some sort of plateau
[00:00:10.240 --> 00:00:15.920]   and that these models aren't actually generalizing that well, and we're going to hit some sort of data wall,
[00:00:15.920 --> 00:00:21.760]   beyond which point the abilities that are unlocked by memorizing a vast corpus of pre-training data
[00:00:21.760 --> 00:00:25.120]   won't actually help you get something much smarter than GPT-4.
[00:00:25.120 --> 00:00:34.320]   I mean, I wouldn't draw too much from the time since GPT-4 was released because, I mean, it does,
[00:00:34.320 --> 00:00:42.960]   yeah, it takes a while to, like, train these models and to, like, get all the, do all the prep to
[00:00:42.960 --> 00:00:50.240]   train a new generation of models. So, yeah, I wouldn't draw too much from that fact.
[00:00:50.240 --> 00:00:54.720]   I would say there are definitely some challenges from the limited amount of data,
[00:00:54.720 --> 00:01:01.520]   but I wouldn't expect us to immediately hit the data wall. But I would expect the nature of
[00:01:01.520 --> 00:01:06.080]   pre-training to somewhat change over time as we get closer to it.
[00:01:06.080 --> 00:01:09.680]   I think we've talked about some examples generically about generalization.
[00:01:09.680 --> 00:01:15.360]   One example I was thinking of was the idea that there is transfer from
[00:01:15.360 --> 00:01:21.120]   reasoning and code. If you train a bunch of code, it gets better at reasoning and language.
[00:01:21.120 --> 00:01:26.160]   And if that's the, is that actually the case? Do you see things like that, which suggests that
[00:01:26.160 --> 00:01:30.080]   there's all this credit positive transfer between different modalities. So once you charge training
[00:01:30.080 --> 00:01:34.080]   on a bunch of videos and images, it'll get smarter and it'll get smarter from synthetic data.
[00:01:34.080 --> 00:01:39.520]   Or does it seem like the abilities that are unlocked are extremely local to
[00:01:39.520 --> 00:01:44.720]   the exact kind of labels and data you put into the training corpus?
[00:01:44.720 --> 00:01:50.080]   In terms of generalization from different types of pre-training data,
[00:01:50.080 --> 00:01:58.320]   I would say it's pretty hard to do science on this type of question because you can't
[00:01:58.320 --> 00:02:04.800]   do that, create that many pre-trained models. So maybe you can't train a GPT-4 size model.
[00:02:04.800 --> 00:02:10.800]   You can't do ablation studies at GPT-4 scale. Maybe you can do, train a ton of
[00:02:11.360 --> 00:02:16.640]   GPT-2 size models or maybe even a GPT-3 size model with different data blends and see what you get.
[00:02:16.640 --> 00:02:24.960]   So I'm not aware of any results or public results on ablations
[00:02:24.960 --> 00:02:28.640]   involving code data and reasoning performance and so forth.
[00:02:28.640 --> 00:02:33.280]   So that would be, I'd be very interested to know about those results.
[00:02:33.280 --> 00:02:39.840]   With regards to the sort of plateau narrative, one of the things I've heard is that a lot of
[00:02:39.840 --> 00:02:43.280]   the abilities these models have to help you with specific things is related to
[00:02:43.280 --> 00:02:51.920]   the having very closely matched labels within the supervised fine-tuning data set.
[00:02:51.920 --> 00:02:57.600]   Is that true? If it can teach me how to use FFmpeg correctly, there's somebody
[00:02:57.600 --> 00:03:03.280]   who's doing, figuring out, seeing the inputs and seeing what flags you need to add.
[00:03:03.280 --> 00:03:10.240]   And some human is figuring that out and smashing to that. And is it, yeah. Do you need to hire
[00:03:10.240 --> 00:03:13.760]   like all these laborers who have domain expertise in all these different domains?
[00:03:13.760 --> 00:03:18.000]   Because if that's the case, it seems like it would be a much bigger slog to get these models to be
[00:03:18.000 --> 00:03:21.520]   smarter and smarter over time. Right. You don't exactly need that
[00:03:21.520 --> 00:03:25.840]   because, yeah, you can get quite a bit out of generalization.
[00:03:27.040 --> 00:03:35.040]   So if you like, like the base model has already been trained on tons of documentation, tons of
[00:03:35.040 --> 00:03:42.720]   code with shell scripts and so forth. So it's already seen all the FFmpeg man pages and lots
[00:03:42.720 --> 00:03:50.880]   of bash scripts and everything. And it's so like the base, even just giving the base model a good
[00:03:50.880 --> 00:03:57.280]   few-shot prompt, you can get it to answer queries like this. And just training a preference model
[00:03:57.280 --> 00:04:05.200]   like for helpfulness will, even if you don't train it on, probably even if you don't train
[00:04:05.200 --> 00:04:11.840]   it on any STEM, it'll somewhat generalize to STEM. So not only do you not need like examples of how
[00:04:11.840 --> 00:04:17.600]   to use FFmpeg, you might not even need anything with programming to get some reasonable behavior
[00:04:17.600 --> 00:04:21.120]   in the programming domain.
[00:04:21.120 --> 00:04:31.120]   [BLANK_AUDIO]

