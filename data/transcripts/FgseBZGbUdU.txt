
[00:00:00.000 --> 00:00:10.400]   All right, we are live on YouTube now. So, hey and welcome everybody to another paper
[00:00:10.400 --> 00:00:16.360]   reading group at Fates and Biases. And today we're going to be covering one that I believe
[00:00:16.360 --> 00:00:22.080]   is a really, one paper that was really influential and one paper that was really, really changed
[00:00:22.080 --> 00:00:29.240]   the game of computer vision. And that paper is EfficientNet. And the reason I say so is
[00:00:29.240 --> 00:00:36.200]   that EfficientNet, as far as I know, since EfficientNet came in 2020, actually it was
[00:00:36.200 --> 00:00:42.760]   2019, was it? Let me just quickly share my screen so you guys can see what I'm doing
[00:00:42.760 --> 00:00:47.160]   on the side. I mean, if you haven't heard of Semantic Scholar, by the way, I would totally
[00:00:47.160 --> 00:00:51.800]   recommend to check it out. So, if you go to Semantic Scholar, it's a really, really nice
[00:00:51.800 --> 00:01:00.600]   website. And if you then press and pass in this EfficientNet, the name of the paper,
[00:01:00.600 --> 00:01:06.440]   copy paste, and then that brings up the results. And you can see how helpful this is. It actually
[00:01:06.440 --> 00:01:11.600]   says published 2019. And the good thing about this paper, sorry, the good thing about this
[00:01:11.600 --> 00:01:17.840]   website is it gives you lots of information around the number of citations. It gives you
[00:01:17.840 --> 00:01:23.200]   like highly influential citations. Like there's information around about the paper as well,
[00:01:23.200 --> 00:01:27.280]   alongside just in this one place. And the thing I really like about this, it gives you
[00:01:27.280 --> 00:01:33.320]   like lots of blog posts, it gives you news articles. And then you could pretty much find
[00:01:33.320 --> 00:01:37.960]   similar papers once you look at go through the citations. So, if you go through the citations,
[00:01:37.960 --> 00:01:44.280]   and you could say, pretty much you could select sort by citation count. So, you get different
[00:01:44.280 --> 00:01:49.560]   papers, for example, you get Arcface, you get YOLOv4, you get EfficientNet. So, you
[00:01:49.560 --> 00:01:54.320]   take a pick. And this is how I pretty much explore papers. I find similar papers using
[00:01:54.320 --> 00:01:59.800]   ATwitter, and then also through Semantic Scholar. So, I'm sorry if I took a little bit extra
[00:01:59.800 --> 00:02:04.720]   of your time before getting straight into the paper. But this is Semantic Scholar is
[00:02:04.720 --> 00:02:09.760]   definitely the place to go. Another place I would recommend is this blog post. Like
[00:02:09.760 --> 00:02:16.280]   I brought this blog post back in August 13, 2020 about the EfficientNet paper. And everything
[00:02:16.280 --> 00:02:21.200]   that I'm going to explain today is going to be like it's already written in this blog
[00:02:21.200 --> 00:02:31.360]   post. Let me just share that in the chat, if I can find it. Okay, so I've shared that
[00:02:31.360 --> 00:02:35.480]   in the chat. Basically, you'll see in this blog post, you'll see everything that you
[00:02:35.480 --> 00:02:39.960]   need to know about EfficientNet. If you don't want to read the paper, you can see how there's
[00:02:39.960 --> 00:02:43.920]   like lots of different things. There's the why, why is this paper important? You can
[00:02:43.920 --> 00:02:49.240]   see how compared to other architectures, EfficientNet pretty much broke the record. And at the same
[00:02:49.240 --> 00:02:54.520]   time, the number of parameters or basically the model sizes were really small. So, that's
[00:02:54.520 --> 00:02:59.200]   one of the biggest things about EfficientNet. If somebody was to ask me, it's like, why
[00:02:59.200 --> 00:03:04.840]   do you love EfficientNet? I would say, A, they really have good performance on ImageNet.
[00:03:04.840 --> 00:03:10.600]   So, they pretty much transfer really well. So, if I pick up an EfficientNet model architecture
[00:03:10.600 --> 00:03:15.480]   and apply it to my own dataset, I'd be really happy because I would see that it transfers
[00:03:15.480 --> 00:03:19.360]   really well. And at the same time, it will be really fast because the models are pretty
[00:03:19.360 --> 00:03:24.200]   much small. So, I could fit an EfficientNet B0 on say a really small chip and I could
[00:03:24.200 --> 00:03:29.280]   serve it in production or I could pretty much have like a really small computer, use just
[00:03:29.280 --> 00:03:35.960]   CPUs to serve this model in production. So, it's really good for companies that have resource
[00:03:35.960 --> 00:03:42.280]   constraints or you care about latency or you care about basically models that are really
[00:03:42.280 --> 00:03:50.520]   fast and accurate. So, EfficientNets are definitely the way to go. And if you don't know it already,
[00:03:50.520 --> 00:03:57.600]   EfficientNet V2 is out already. So, EfficientNet V2, if you search for that, you will see there's
[00:03:57.600 --> 00:04:03.960]   this paper, EfficientNet V2, and you can see again, read about this. It's like 57 citations
[00:04:03.960 --> 00:04:08.360]   published in 2021. You can find the exact date and then you can find, again, what you
[00:04:08.360 --> 00:04:13.720]   can do is you can go to this number of citations and you can say sort by citation count. And
[00:04:13.720 --> 00:04:17.800]   then you can say pay attention to MLPC. I've already done this once. So, you already know
[00:04:17.800 --> 00:04:24.000]   how to train your bit. It's like all of these great papers, CodeNet, and you'll start recognizing
[00:04:24.000 --> 00:04:30.680]   authors. So, Kwakweli and Ming-Ting Tan were the original authors of EfficientNet. And
[00:04:30.680 --> 00:04:37.040]   then they've done that again with EfficientNet V2. And another thing I'd like to share is
[00:04:37.040 --> 00:04:47.840]   that we already have, if I go to Weights and Biases, so if I search EfficientNet V2 PRG,
[00:04:47.840 --> 00:04:53.240]   I think that should come up as at Weights and Biases. There it is, Weights and Biases
[00:04:53.240 --> 00:04:59.480]   paper reading group, EfficientNet. So, you can see we already have a paper reading where
[00:04:59.480 --> 00:05:04.040]   we discussed EfficientNet V2. So, we're kind of doing the reverse. We discussed V2 three
[00:05:04.040 --> 00:05:09.920]   months ago and now we're coming back to the basic papers such as EfficientNet. So, that's
[00:05:09.920 --> 00:05:15.880]   all that I had to say in terms of like the basic introduction. Another thing is this
[00:05:15.880 --> 00:05:21.240]   is going to be my last paper reading group. And from going forward, my colleague, Sanyam
[00:05:21.240 --> 00:05:27.920]   Bhutani, he's going to pick up the paper reading groups. And he's going to start, I believe,
[00:05:27.920 --> 00:05:32.720]   with UNet. But that's just some background information before we get straight into the
[00:05:32.720 --> 00:05:38.840]   paper. Okay. So, with EfficientNet, what I'm going to do, I'm going to reference my blog,
[00:05:38.840 --> 00:05:44.680]   I'm going to reference the paper, and I'm going to explain the EfficientNet architecture
[00:05:44.680 --> 00:05:49.160]   to everybody. So, if you have questions, please feel free to post them in the chat and I'll
[00:05:49.160 --> 00:05:55.840]   continue to respond as we continue going through this paper reading group. Cool. So, let's
[00:05:55.840 --> 00:06:01.200]   get started. So, then again, why are EfficientNets, like why do you care about EfficientNets?
[00:06:01.200 --> 00:06:07.120]   As I've already said, they've got really good performance. And the number of parameters
[00:06:07.120 --> 00:06:12.040]   at the time of their release was really small compared to other papers. So, they're really,
[00:06:12.040 --> 00:06:17.760]   really efficient, if I would say. But there's like some, there's still some things that
[00:06:17.760 --> 00:06:24.000]   they lacked, which EfficientNet redo covers. So, in EfficientNet redo, the reason why I'm
[00:06:24.000 --> 00:06:30.200]   also sort of referencing to this is like EfficientNet redo achieves 87.3 top one accuracy, whereas
[00:06:30.200 --> 00:06:36.440]   EfficientNet at the time did 84.3. So, that's how much has changed in like a couple years,
[00:06:36.440 --> 00:06:43.560]   in two years in deep learning. And this EfficientNets have some shortcomings, but as part of this
[00:06:43.560 --> 00:06:47.640]   paper reading group, we're not going to discuss those shortcomings, or we're not going to
[00:06:47.640 --> 00:06:52.880]   discuss like, like what went wrong, we slightly touched base on it. But if you want to know
[00:06:52.880 --> 00:06:58.200]   more about like why an EfficientNet redo was needed, I would definitely recommend checking
[00:06:58.200 --> 00:07:04.000]   out this paper reading. I'm just going to post this in the chat.
[00:07:04.000 --> 00:07:13.960]   Cool. So, that's been posted in the chat. So, now let's go with EfficientNet. So, let's
[00:07:13.960 --> 00:07:18.720]   see this. So, then I'm just going to read through this paper. I'm going to do exactly
[00:07:18.720 --> 00:07:23.280]   as I do it always. I spent a lot of my time in reading the abstract, and I spent a lot
[00:07:23.280 --> 00:07:27.280]   of my time reading the introduction. So, my theory is like if I'm reading a paper and
[00:07:27.280 --> 00:07:31.560]   I don't understand the paper by finishing abstract and introduction, there's something
[00:07:31.560 --> 00:07:35.480]   either wrong with me or there's something either wrong with the paper. But that being
[00:07:35.480 --> 00:07:40.160]   said, that doesn't mean you have to follow that too. But that's just how I like to do
[00:07:40.160 --> 00:07:46.520]   things. So, let's start with the abstract first. So, then again, you know, this part
[00:07:46.520 --> 00:07:51.240]   of the abstract is basically just saying like there's always like companies have fixed resources,
[00:07:51.240 --> 00:07:58.800]   they have fixed budgets. And what you want to do is like you scale up a model architecture.
[00:07:58.800 --> 00:08:03.480]   Like you could have a really small model architecture, but for companies that have like more resources,
[00:08:03.480 --> 00:08:08.360]   you can have a bigger architecture. So, what that means is like, what does it mean? What's
[00:08:08.360 --> 00:08:13.920]   the difference in like a small architecture and a big architecture? So, how do you scale
[00:08:13.920 --> 00:08:18.920]   up a model? And there's like three different ways how you scale up a model. Either you
[00:08:18.920 --> 00:08:26.640]   make it deeper, which means you add more layers. So, let me go to OneNote and start doing what
[00:08:26.640 --> 00:08:36.520]   I do. And basically, say, let's say this is my model, right? This is say, block zero,
[00:08:36.520 --> 00:08:43.200]   block one, block three, and block four. Let's say my model consists of four blocks. This
[00:08:43.200 --> 00:08:47.480]   is a typical, that's what our ResNet kind of comes. And let's say this is say two layers
[00:08:47.480 --> 00:08:51.880]   in this, two layers in this, two layers in this, and two layers in this. And then the
[00:08:51.880 --> 00:08:59.440]   channels here are say 32, 64, 128, and 256, right? So, what does it mean to say, to scale
[00:08:59.440 --> 00:09:04.040]   an architecture and to say that it's going to be deeper? What that means is you make
[00:09:04.040 --> 00:09:07.840]   this like the number of layers, because it's only like two layers in this. What you do
[00:09:07.840 --> 00:09:12.440]   is you add more layers. So, now one, two, three, instead of having like two layers,
[00:09:12.440 --> 00:09:18.000]   you could have four layers in B0. You could again have four layers in B1. You could have
[00:09:18.000 --> 00:09:23.200]   four layers in B2, and you could have four layers in B3. So, that just basically means
[00:09:23.200 --> 00:09:28.520]   like you just make the model deeper, like you just add the number of layers. So, what
[00:09:28.520 --> 00:09:33.320]   this does is it gives you more computation, right? You keep the number of channels the
[00:09:33.320 --> 00:09:38.760]   same. So, you're just making it deeper for now. The other thing you could do is, it's
[00:09:38.760 --> 00:09:43.160]   called making a model wider. So, basically you just increase the number of channels instead
[00:09:43.160 --> 00:09:49.320]   of having like 22, you could have your first one at 64, second one at 128, 256, 512, and
[00:09:49.320 --> 00:09:55.440]   so on. The basic idea is you're increasing computation. And when you're increasing computation,
[00:09:55.440 --> 00:10:00.360]   you're basically saying, okay, deep learning model, you have a, now you have a better chance
[00:10:00.360 --> 00:10:05.360]   of learning this complex problem that I'm trying to solve. So, the more, it's considered
[00:10:05.360 --> 00:10:11.280]   the more computation you have, basically the more con, fad, non-reliable operations that
[00:10:11.280 --> 00:10:17.640]   you do, the understanding is like, it's possible then to understand more complex problems or
[00:10:17.640 --> 00:10:23.480]   deeper, wider models compared to like the shallower counterparts. And then the last
[00:10:23.480 --> 00:10:29.880]   one is you increase your input image size. So, if your input image size is at 224, if
[00:10:29.880 --> 00:10:36.480]   you make it either 256 or 384, basically you increase your input resolution. Then that
[00:10:36.480 --> 00:10:42.360]   also makes the model performance go up. So, there's these three things in efficient ed
[00:10:42.360 --> 00:10:48.720]   that you see here. So, you could see you have a baseline. So, you have your input resolution
[00:10:48.720 --> 00:10:53.640]   at H by W and you have your layer one and so on. So, if you want to increase the width,
[00:10:53.640 --> 00:10:57.080]   that basically means you increase the channels. So, instead of like this being 64 channels,
[00:10:57.080 --> 00:11:01.400]   now you have 128 channels. This being 128, this is two, it's basically increasing the
[00:11:01.400 --> 00:11:05.680]   number of channels. Depth means you make your layers deeper. So, you add more layers instead
[00:11:05.680 --> 00:11:09.840]   of just having one blue layer, you have two blue layers. Instead of having one green layer,
[00:11:09.840 --> 00:11:14.960]   you have two green layers, two yellow layers and so on. And then the last one is resolution
[00:11:14.960 --> 00:11:20.200]   scaling, which means you basically your input, you have a higher resolution. So, instead
[00:11:20.200 --> 00:11:26.840]   of it being like 224 by 224, which is a really, really small input, you would just increase
[00:11:26.840 --> 00:11:33.000]   the input resolution. So, that's the basic, like those are the basic ways of how you scale
[00:11:33.000 --> 00:11:38.880]   deep learning networks. I repeat, it's either by depth, it's either by width or it's by
[00:11:38.880 --> 00:11:46.520]   resolution. So, those are the three main ways of increasing or like scaling up your deep
[00:11:46.520 --> 00:11:53.720]   learning models. But as part of this efficient ed paper, what they say is they propose something
[00:11:53.720 --> 00:12:00.920]   called compound scaling. So, they figured out that they said that basically as we read
[00:12:00.920 --> 00:12:06.720]   through this paper, you'll see what's the theory or what the hypothesis was. The researchers
[00:12:06.720 --> 00:12:12.200]   felt that these three things don't have to be separate. Like you don't have to just either
[00:12:12.200 --> 00:12:18.820]   increase width or increase depth or increase resolution. What you could do instead is increase
[00:12:18.820 --> 00:12:23.040]   all three at the same time. Like you could have a model now instead of like just getting
[00:12:23.040 --> 00:12:27.400]   the depth, you could have a model that's deeper, you could have a model that's wider, and you
[00:12:27.400 --> 00:12:32.160]   could also have a model that's being trained on a higher input resolution. So, that's the
[00:12:32.160 --> 00:12:40.160]   basic idea. If I'm to introduce EfficientNets to you, this is the basic idea. And basically
[00:12:40.160 --> 00:12:46.840]   then what happened is they developed this architecture called B0. How they developed
[00:12:46.840 --> 00:12:51.960]   it, that's something we'll look into it as we go through the paper. But for now, just
[00:12:51.960 --> 00:12:57.360]   understand that then what happened is they developed a baseline architecture, B0, and
[00:12:57.360 --> 00:13:01.680]   then they scaled it up using this compound scaling technique. So, those are the two main
[00:13:01.680 --> 00:13:06.280]   things. So, if you even have a look at my blog, you will see like particularly if you
[00:13:06.280 --> 00:13:09.680]   look at the two main contributions from the research paper, one of them is the compound
[00:13:09.680 --> 00:13:15.520]   scaling and the second is the architecture itself. So, they introduced like this B0 architecture
[00:13:15.520 --> 00:13:21.360]   was found using neural architecture search. So, what that is, I will introduce it to you.
[00:13:21.360 --> 00:13:26.360]   And the second thing is then compound scaling. So, combining these two things together gives
[00:13:26.360 --> 00:13:31.440]   you this EfficientNet family. So, I'll repeat, they found this B0 architecture, then they
[00:13:31.440 --> 00:13:36.400]   use this compound scaling technique that you can see in this figure two below. And based
[00:13:36.400 --> 00:13:42.320]   on that, you went from B0 to B1 to B2, B3, B4, B5, B6, and all the way to EfficientNet
[00:13:42.320 --> 00:13:54.320]   B7. Ramesh is asking, does it help to increase resolution more than the original size? I
[00:13:54.320 --> 00:14:00.160]   don't understand. When I say resolution, I pretty much mean like instead of it being
[00:14:00.160 --> 00:14:06.280]   two to four by two to four, it being five and two by five and two. So, that's just higher
[00:14:06.280 --> 00:14:16.360]   resolution anyway. So, I think there's, both of the things mean the same thing in my head.
[00:14:16.360 --> 00:14:23.480]   Like it's basically having a bigger image, if that helps. Cool. So, then now let's just
[00:14:23.480 --> 00:14:28.200]   quickly read through the abstract. So, let's see here in this paper, we systematically
[00:14:28.200 --> 00:14:33.720]   study model scaling and identify the carefully balancing network depth, width, and resolution
[00:14:33.720 --> 00:14:38.280]   that can lead to better performance. So, basically they're saying all three things are what we
[00:14:38.280 --> 00:14:48.520]   need instead of just the one. So, based on this, we propose a new scaling method. And
[00:14:48.520 --> 00:14:54.200]   the reason why I'm reading this paper alongside the explanation is that I want everybody who's
[00:14:54.200 --> 00:14:59.120]   part of this paper reading group to get into this habit of reading papers. And what that
[00:14:59.120 --> 00:15:03.440]   does is like the more papers you read, the more comfortable you become with reading more
[00:15:03.440 --> 00:15:10.960]   papers. So, I'm just reading this abstract just so we all lose, like we all stop being
[00:15:10.960 --> 00:15:15.520]   afraid of reading papers. That's another thing we want to promote with paper reading group.
[00:15:15.520 --> 00:15:21.120]   So, cool. So, then based on this observation, we propose a new scaling method that uniformly
[00:15:21.120 --> 00:15:27.000]   scales all dimensions using a simple yet highly effective method, compound coefficient. So,
[00:15:27.000 --> 00:15:31.640]   that's that. And then they even tried it on MobileNet and ResNet. Remember MobileNet and
[00:15:31.640 --> 00:15:37.000]   ResNet are just architectures, like you can still scale them. So, if you have a ResNet
[00:15:37.000 --> 00:15:45.560]   34, it goes from ResNet 34 to ResNet 50, ResNet all the way down to 101 and then so on. But
[00:15:45.560 --> 00:15:51.800]   basically what you have is it even has like a 152 and so on. So, basically what that means
[00:15:51.800 --> 00:15:57.640]   is there's like the original ResNet authors had their own way of scaling this architecture
[00:15:57.640 --> 00:16:03.760]   up. But what they say is we what they say is they tried this compound scaling method
[00:16:03.760 --> 00:16:09.720]   even on ResNets and MobileNets and they found that it was successful. So, then what they
[00:16:09.720 --> 00:16:14.640]   say is we use neural. So, now what they're saying is how did we come up with the efficient
[00:16:14.640 --> 00:16:19.480]   net architecture? They say we use neural architecture search to design a new baseline network and
[00:16:19.480 --> 00:16:23.800]   scale it up to obtain a family of models called efficient net. So, they use neural architecture
[00:16:23.800 --> 00:16:28.520]   search to find this B0 and then they use compound scaling to get efficient nets. Here's the
[00:16:28.520 --> 00:16:35.040]   main contributions. This is important is that their efficient net B7 achieves state of the
[00:16:35.040 --> 00:16:39.440]   art performance. The state of the art just basically is a fancy way of saying like the
[00:16:39.440 --> 00:16:45.440]   best performance at the time. So, it achieved 84.3 top one accuracy on ImageNet while being
[00:16:45.440 --> 00:16:51.520]   8.4x smaller and 6.1x faster. So, that's the main thing I was saying. It's like why I like
[00:16:51.520 --> 00:16:58.320]   efficient nets is that they have good performance, they're smaller and they're faster. So, then
[00:16:58.320 --> 00:17:02.160]   this is again similar thing that they transfer bail on these other data sets. So, if you
[00:17:02.160 --> 00:17:06.600]   have your own data sets that you want to work with, definitely I would recommend you use
[00:17:06.600 --> 00:17:11.560]   efficient nets because they really make things really easy. So, that's the main abstract.
[00:17:11.560 --> 00:17:16.920]   Again, the abstract mostly contains everything that you need to know about the paper and
[00:17:16.920 --> 00:17:24.000]   then they say source code is available to you. Cool. That's that. We're progressing
[00:17:24.000 --> 00:17:29.560]   nicely in terms of time. And then introduction in this you can see there's not much. They
[00:17:29.560 --> 00:17:34.040]   just said like there was this recent paper, G5 at the time and we don't need to worry
[00:17:34.040 --> 00:17:39.800]   about what that paper is. But basically, it scaled the baseline model and made it four
[00:17:39.800 --> 00:17:46.120]   times larger. So, efficient net strategy was very, very different. So, then they say the
[00:17:46.120 --> 00:17:51.120]   most common way to scale up conf is by the depth. Of course, we know you make them deeper
[00:17:51.120 --> 00:17:57.160]   or width. Another less common way is using image resolution. So, in previous work, it
[00:17:57.160 --> 00:18:02.920]   is common to either use one of the three, depth, width, or image size. But in this paper,
[00:18:02.920 --> 00:18:08.240]   what they say is we want to study and retrain the process of scaling up conf nets. So, in
[00:18:08.240 --> 00:18:14.560]   particular, then they introduce compound scaling, which is this image here. What else? There's
[00:18:14.560 --> 00:18:22.280]   a bit of math over here. All this says if you scale width by alpha, depth by beta, and
[00:18:22.280 --> 00:18:27.200]   then resolution by gamma, basically, you could have things by alpha to the power n, beta
[00:18:27.200 --> 00:18:31.400]   to the power n, gamma to the power n if you want to use two to the power n more computational
[00:18:31.400 --> 00:18:37.480]   resources. We don't need to worry too much about this bit of math. As long as we get
[00:18:37.480 --> 00:18:44.400]   the main message, that's completely fine. So, again, they're saying this here intuitively,
[00:18:44.400 --> 00:18:49.920]   this compound scaling makes sense because if the input image is bigger, then the network
[00:18:49.920 --> 00:18:54.400]   needs more layers to increase the receptive field and more channels to capture more fine
[00:18:54.400 --> 00:18:58.560]   grain patterns on the bigger image. This is one of my favorite sentences from this whole
[00:18:58.560 --> 00:19:04.520]   research paper because it explains the paper intuitively. So, I think it's really clear.
[00:19:04.520 --> 00:19:11.320]   I don't need to explain this in simple English. It's already written in a very simple language.
[00:19:11.320 --> 00:19:15.240]   Apart from that, then they say we also demonstrate that our scaling method works really well
[00:19:15.240 --> 00:19:20.480]   on existing mobile nets and breast nets. So, that's the main things. Figure 1 summarizes
[00:19:20.480 --> 00:19:24.400]   the image net performance. We've already had a look at Figure 1. So, you can see how efficient
[00:19:24.400 --> 00:19:32.480]   nets are right at the top if you compare accuracy versus number of parameters. What else? I
[00:19:32.480 --> 00:19:37.520]   guess not really. Compared to widely used, ResNet-50 are efficient at B4, improves the
[00:19:37.520 --> 00:19:45.800]   top-end accuracy from 76.3 to 83%. Although, another thing I'd like to say, the 76.3% take
[00:19:45.800 --> 00:19:51.520]   this number with a grain of salt. There was this recent paper by Ross Whitman. I think
[00:19:51.520 --> 00:20:00.280]   it's called Revisiting ResNets. No, sorry. It's called ResNet Strikes Back. So, in that
[00:20:00.280 --> 00:20:07.000]   paper, this is again a recent paper. It came out in 2021. What the authors say is that
[00:20:07.000 --> 00:20:14.560]   instead of the accuracy, like lots of research papers undercut the accuracy of ResNets. Like,
[00:20:14.560 --> 00:20:19.600]   if you use the latest techniques, then actually you can have even the ResNet have a really
[00:20:19.600 --> 00:20:26.840]   high accuracy of like 80.4. That's just, again, a bit of a side track for those interested
[00:20:26.840 --> 00:20:32.240]   if you're interested in other research papers. So, that's that. Again, that's pretty much
[00:20:32.240 --> 00:20:37.000]   it, I guess, in terms of the introduction. So, if there's any questions, I'm going to
[00:20:37.000 --> 00:20:42.000]   take like, I'm going to give 30 seconds to everybody to put their questions in the chat
[00:20:42.000 --> 00:20:51.840]   if you have any questions. There's one question from Ramesh. If your data is only 2-4 by 2-4,
[00:20:51.840 --> 00:20:56.560]   any more information gain in making it bigger? Yeah, absolutely. Basically, there's this
[00:20:56.560 --> 00:21:02.360]   also paper called FixRes that trains your model at 2-4 by 2-4 and makes an inference
[00:21:02.360 --> 00:21:08.240]   at 2-56 by 2-56. But that's a different thing. But again, if you will see, you almost always
[00:21:08.240 --> 00:21:15.360]   see that your model performance becomes better. So, we can see that in, actually, there's
[00:21:15.360 --> 00:21:20.320]   more analysis coming on this. I will cover that up in figure three. But I will cover
[00:21:20.320 --> 00:21:26.240]   that analysis when we get to this compound scaling method. But are there any questions
[00:21:26.240 --> 00:21:40.160]   so far on just the basics? Yeah, Ramesh is asking if the compound scaling works for ResNet
[00:21:40.160 --> 00:21:44.320]   and other networks, why do we not see it in more use? Is it because they don't have pre-trained
[00:21:44.320 --> 00:21:50.360]   for them? Basically, the weights that we get, we have ResNet weights, we have ResNext, or
[00:21:50.360 --> 00:21:55.760]   we have other weights. Again, different researchers have a different take on different things.
[00:21:55.760 --> 00:22:02.240]   So, this is more to say that the efficient net compound scaling works. The point that
[00:22:02.240 --> 00:22:06.640]   the researchers are trying to make in this is that compound scaling works, not just for
[00:22:06.640 --> 00:22:10.640]   efficient net, but also other architectures. That doesn't mean all the other authors start
[00:22:10.640 --> 00:22:15.440]   using compound scaling. That's a trend that I didn't see. But it's a very valid question.
[00:22:15.440 --> 00:22:20.680]   I guess it just comes down to what the pre-trained weights that we have. So, we have ResNet pre-trained
[00:22:20.680 --> 00:22:25.720]   weights that have been scaled their own way. It just comes down to pre-trained weights.
[00:22:25.720 --> 00:22:31.080]   We just don't have enough pre-trained weights. Could you please explain neural architecture
[00:22:31.080 --> 00:22:36.560]   search used to find v0? Absolutely. We'll get there when we come to it. So, we don't
[00:22:36.560 --> 00:22:42.000]   have to worry about it now. I guess then in terms of the introduction, I guess, things
[00:22:42.000 --> 00:22:47.720]   are pretty clear with everybody. So, now we'll just go into the details of model scaling.
[00:22:47.720 --> 00:22:52.760]   So, we're still progressing good with time. So, let's just get into the details of compound
[00:22:52.760 --> 00:22:58.480]   model scaling. So, what this compound model scaling says, again, there's a lot of math
[00:22:58.480 --> 00:23:05.040]   in this. You don't have to worry about the math. It's basically saying, if I'm to distill,
[00:23:05.040 --> 00:23:10.080]   if I'm to take all of this information in and if I'm to distill it for you, it's basically
[00:23:10.080 --> 00:23:17.560]   saying if you want to scale your model, there's three different ways of doing it. Scale your
[00:23:17.560 --> 00:23:23.400]   depth, scale your width, scale your channels. Sorry, scale your input resolution. So, let's
[00:23:23.400 --> 00:23:29.240]   just say your continent, this basically math, input is this tensor shape. Your continent
[00:23:29.240 --> 00:23:34.400]   has that many layers. So, by the end, you can formulate your problem like this where
[00:23:34.400 --> 00:23:38.400]   you have some budget. So, in the end, you get something like what we care about is we
[00:23:38.400 --> 00:23:44.400]   want to maximize the accuracy where the memory should be less than equal to the target memory
[00:23:44.400 --> 00:23:48.040]   and the flop should be less than equal to the target flops. So, if I'm to explain all
[00:23:48.040 --> 00:23:54.240]   of this in plain English, it just basically means a company or yourself as a researcher
[00:23:54.240 --> 00:24:00.000]   have some resource constraints. Like, you only have access to say one AWS machine that
[00:24:00.000 --> 00:24:06.280]   has a weave in a hundred. Now, that means you can only train a certain, like a big enough
[00:24:06.280 --> 00:24:12.480]   model that fits into those memory and that fits into that, that doesn't take a lot of
[00:24:12.480 --> 00:24:17.040]   time for you to train, right? So, that's your constraint. So, in that time, what you want
[00:24:17.040 --> 00:24:23.840]   to do is you want to scale your, say, B0, efficient B0, you want to scale your model
[00:24:23.840 --> 00:24:29.760]   in a way such that it fits that constraint really nicely. And at the same time, you don't
[00:24:29.760 --> 00:24:34.920]   have to, like, it still gets, it still maximizes the accuracy. So, you want to be mindful of
[00:24:34.920 --> 00:24:41.840]   resources and not just worry about accuracy because most real world use cases have constraints.
[00:24:41.840 --> 00:24:46.000]   So, I really like this paper because they really do address or they really do look at
[00:24:46.000 --> 00:24:51.640]   resource constraints. And again, they, so this is the problem formulation. Like, this
[00:24:51.640 --> 00:24:55.600]   is just the basic, if I'm to explain everything in English, that's what they say in this.
[00:24:55.600 --> 00:24:59.000]   You can go through the math, you'll find, once you read through the math, you'll find
[00:24:59.000 --> 00:25:06.080]   that what I've explained does, it's exactly what they're saying. So, then now it comes
[00:25:06.080 --> 00:25:11.920]   to scaling dimensions. So, this is where they say, okay, now we know that we have a depth
[00:25:11.920 --> 00:25:20.280]   W, we have a depth D with W and resolution R. So, what they're saying is the possible
[00:25:20.280 --> 00:25:26.000]   ways of scaling your model is either making it deeper. So, scaling network depth is the
[00:25:26.000 --> 00:25:30.840]   most common way used by many continents. And then they provide preferences. So, the intuition
[00:25:30.840 --> 00:25:35.920]   is that the deeper continent can capture richer and more complex features and thus generalize
[00:25:35.920 --> 00:25:39.400]   well on new tasks. So, that's just like the basic way of doing it.
[00:25:39.400 --> 00:25:43.040]   The next thing they say is, okay, width is also commonly used. So, then they provide
[00:25:43.040 --> 00:25:48.320]   references on those research papers that use width. And then this explain the reason that
[00:25:48.320 --> 00:25:52.480]   wider networks tend to be able to capture more fine-grained features and are easier
[00:25:52.480 --> 00:25:56.960]   to train. So, that's just basically saying why you want to have width as well. And then
[00:25:56.960 --> 00:26:00.480]   this is to answer, like, you could start with two to four by two to four, but you could
[00:26:00.480 --> 00:26:04.800]   also train your networks in two to nine, two nine nine by two nine nine for better accuracy.
[00:26:04.800 --> 00:26:11.080]   And like some recent papers have also used 480 by 480 resolution or 600 by 600 is widely
[00:26:11.080 --> 00:26:15.880]   used in object detection. So, again, there's like these three different ways. But I really,
[00:26:15.880 --> 00:26:21.440]   really like this figure three. What it does is it starts with a baseline architecture.
[00:26:21.440 --> 00:26:26.040]   I don't think they explain what that baseline architecture is. They basically say baseline
[00:26:26.040 --> 00:26:31.000]   network is described in table one. So, okay, so this is EfficientNet B0. So, they start
[00:26:31.000 --> 00:26:35.920]   with EfficientNet B0 and they start to scale it. So, what they do, the first thing they
[00:26:35.920 --> 00:26:40.480]   try and do is they just increase the width. They don't worry about the depth, which is
[00:26:40.480 --> 00:26:44.240]   like having more deeper layers. They don't worry about the resolution. They just start
[00:26:44.240 --> 00:26:49.040]   with the network and they just start adding more number of channels at each layer. Right?
[00:26:49.040 --> 00:26:54.360]   So, if you remember this, having width means instead of having 32, you have 64 channels,
[00:26:54.360 --> 00:26:58.400]   120. And so, you just increase the number of channels. And if I'm to explain that using
[00:26:58.400 --> 00:27:03.200]   this figure, it basically just means like a wider network. And why do you want a wider
[00:27:03.200 --> 00:27:09.680]   network? That's been explained here where it says wider networks tend to be able to
[00:27:09.680 --> 00:27:15.360]   capture more fine-grained features. So, what they did is they just start to scale the model
[00:27:15.360 --> 00:27:20.800]   using width. And the thing that they observed is like initially, if your model is really
[00:27:20.800 --> 00:27:26.360]   small and you start to scale it using width, it will start to perform really well. Initially,
[00:27:26.360 --> 00:27:31.080]   your performance goes up really good. It goes from 76 all the way to 77.5 or something and
[00:27:31.080 --> 00:27:37.840]   then goes to 78.5. But as you keep increasing the depth, the effect of increasing the network
[00:27:37.840 --> 00:27:44.840]   width starts to plateau. You start to see... You don't see as good a performance gain in
[00:27:44.840 --> 00:27:50.320]   basically very big models. And then they observed a very similar pattern with depth. So, they
[00:27:50.320 --> 00:27:56.400]   saw that in this case, all they do is they increase the network depth. So, they start
[00:27:56.400 --> 00:28:01.600]   with 1.0. Of course, that's just a reference. But then they make it twice as deep, or thrice
[00:28:01.600 --> 00:28:06.400]   as deep, or four times as deep, or six times as deep, or eight times as deep. And they
[00:28:06.400 --> 00:28:13.040]   start to see the similar trend that by very big models, the trend tends to plateau. So,
[00:28:13.040 --> 00:28:18.800]   you start to see like the performance doesn't really go up. So, you can see increasing the
[00:28:18.800 --> 00:28:25.080]   width has a very similar... You can't say increasing width is more important than increasing
[00:28:25.080 --> 00:28:29.520]   depth, because you can see making it eight times, you still get around the 80 point something
[00:28:29.520 --> 00:28:34.160]   accuracy. And then it's the same thing with input resolutions. They keep the model as
[00:28:34.160 --> 00:28:41.200]   efficient at B0, but they try to now make your... They try to make the input resolution
[00:28:41.200 --> 00:28:46.760]   bigger. So, if your input resolution is 2 to 4 by 2 to 4, multiply that by 1.5, so you
[00:28:46.760 --> 00:28:51.920]   get a some number, which is then you multiply that twice. So, that 2.2 would be around,
[00:28:51.920 --> 00:28:59.480]   say 480, I think. But anyway, that's just saying you just keep increasing your input
[00:28:59.480 --> 00:29:08.360]   resolution. And you can see now, again, it gets you around 79.6. So, you could say having
[00:29:08.360 --> 00:29:15.320]   a deeper or a wider network helps more than just having a really high input image. But
[00:29:15.320 --> 00:29:22.280]   you can see all three of these tend to better performance on ImageNet. And what they figured,
[00:29:22.280 --> 00:29:28.080]   again, what they started to do with their experiment is they started experimenting,
[00:29:28.080 --> 00:29:33.280]   scaling the network with different depth and with different resolution. So, the first thing
[00:29:33.280 --> 00:29:37.840]   they did was, with this blue line, your depth remains one and your resolution remains one,
[00:29:37.840 --> 00:29:43.200]   and they're just making it wider. But we can see with this red line, you can see they're
[00:29:43.200 --> 00:29:46.840]   increasing the depth, they've increased the resolution, and then they're increasing the
[00:29:46.840 --> 00:29:52.680]   slope. So, what you can see is like what works best is that if you increase your width, if
[00:29:52.680 --> 00:29:56.600]   you increase your depth, and if you increase your resolution. So, that's the best thing.
[00:29:56.600 --> 00:30:01.480]   Like, that's what you see. This is where those hints start to come in that compound scaling
[00:30:01.480 --> 00:30:08.760]   really, really works. Compound scaling is really, really useful. And then this is how
[00:30:08.760 --> 00:30:12.680]   they get the idea is like, okay, if you want to scale up efficient nets, we are going to
[00:30:12.680 --> 00:30:18.240]   use compound scaling. So, this is where... So, by using all of this analysis, like using
[00:30:18.240 --> 00:30:24.480]   this figure three analysis, they come to the first observation is like scaling up any dimension
[00:30:24.480 --> 00:30:31.240]   of the network with depth or resolution improves accuracy, but the accuracy gain diminishes
[00:30:31.240 --> 00:30:35.320]   for bigger networks. So, this again, what they're saying is like the effect starts to
[00:30:35.320 --> 00:30:41.600]   plateau as you go deeper. Initially, it's really good increasing the accuracy, but then
[00:30:41.600 --> 00:30:47.160]   it starts to plateau. And then trying this compound scaling, which is in this figure,
[00:30:47.160 --> 00:30:51.960]   which is like what they do is we empirically observe, empirically means experimentally,
[00:30:51.960 --> 00:30:56.280]   basically. What they're saying is like they've run lots of experiments and they started scaling
[00:30:56.280 --> 00:31:02.080]   different dimensions. And what they say is like all of these scalings are not independent
[00:31:02.080 --> 00:31:06.240]   from each other. So, like the common belief until now was you either scale one of the
[00:31:06.240 --> 00:31:11.320]   three ways, but in this paper, what they said is they're not independent. Actually, you
[00:31:11.320 --> 00:31:16.240]   can do it all together. So, again, they're saying intuitively high resolution images
[00:31:16.240 --> 00:31:21.200]   would... We should increase depth and we should have... Such that they can capture larger
[00:31:21.200 --> 00:31:26.480]   receptive fields. And then we should also like increase the width. So, network... Basically,
[00:31:26.480 --> 00:31:30.880]   when the resolution is higher in order to capture more fine-grained patterns with more
[00:31:30.880 --> 00:31:35.440]   pixels. So, it's just saying like it intuitively makes sense to have increase all three together.
[00:31:35.440 --> 00:31:39.680]   And then to validate our intuition, they run these numbers in this chart.
[00:31:39.680 --> 00:31:45.800]   So, observation two is that in order to pursue better accuracy and efficiency, it is critical
[00:31:45.800 --> 00:31:50.360]   to balance all dimensions of network with depth and resolution during continent scaling.
[00:31:50.360 --> 00:31:54.720]   So, those are the two main observations and this is how they say compound scaling is the
[00:31:54.720 --> 00:32:01.000]   way to go. And then when we come down there, there's more bit of math. It's basically saying
[00:32:01.000 --> 00:32:07.280]   like if you have a depth D with W and resolution R of your network, and you scale it by say
[00:32:07.280 --> 00:32:14.680]   alpha to the bar five, beta to the bar five, gamma to the bar five. Basically, it's just
[00:32:14.680 --> 00:32:19.080]   saying... It's just a bit of math. It's just a mathematical notation. What they're saying
[00:32:19.080 --> 00:32:26.120]   is basically is that when you're scaling, you make your depth alpha times D, you make
[00:32:26.120 --> 00:32:32.480]   your width beta times W, you make your resolution gamma to the bar five times R. Right? And
[00:32:32.480 --> 00:32:36.640]   then they're basically just constants that can be determined using a small grid set.
[00:32:36.640 --> 00:32:41.720]   So, the grid set is just basically... You run lots of experiments and you pick up the
[00:32:41.720 --> 00:32:47.600]   best... You pick up the best parameters that give you the best performance. So, this is
[00:32:47.600 --> 00:32:52.920]   like... There's not much to worry about it. They just add a constant. It's like on these
[00:32:52.920 --> 00:32:57.880]   alpha, beta and gamma, it's like alpha times beta squared times gamma squared should be
[00:32:57.880 --> 00:33:07.840]   approximately equal to two. And based on this, where are the values that they find? Oh, here
[00:33:07.840 --> 00:33:14.200]   it is. Okay. I'll get to that when it comes to efficient. Basically, this is all about
[00:33:14.200 --> 00:33:18.440]   compound scaling. So, all of this is compound scaling. And the reason why you have alpha
[00:33:18.440 --> 00:33:23.560]   and beta squared and gamma squared is the answer is here. Notably, the flops. Flops
[00:33:23.560 --> 00:33:29.160]   just means float addition and multiplication operations. I don't remember the exact full
[00:33:29.160 --> 00:33:33.920]   form, but it's basically addition and multiplication. What they're saying is if you increase the
[00:33:33.920 --> 00:33:39.320]   depth, if you make it twice as big, then you're doubling... It's been written here. Doubling
[00:33:39.320 --> 00:33:43.800]   network depth will double flops. But doubling network width or resolution will increase
[00:33:43.800 --> 00:33:48.640]   the flops by four times. So, it will make your flops are just like floating point operations.
[00:33:48.640 --> 00:33:57.920]   I guess that's what they are. But basically what they're saying is if you double the depth,
[00:33:57.920 --> 00:34:02.040]   then that will double the flops. But if you double the network width or resolution, it
[00:34:02.040 --> 00:34:06.520]   will make the flops four times. So, your model becomes like four times as bigger, intuitively
[00:34:06.520 --> 00:34:11.120]   speaking. So, that's why you have an alpha, but beta squared and gamma squared should
[00:34:11.120 --> 00:34:18.160]   be approximately equal to two. And that's my understanding of this bit. But that's just
[00:34:18.160 --> 00:34:23.640]   there. So, in this paper, we constrain this such that for any new phi, the total flops
[00:34:23.640 --> 00:34:28.720]   will approximately increase by two to the power phi, which is correct. See, this is
[00:34:28.720 --> 00:34:34.240]   up to the researchers. It's not up to me. This is what they thought, and this is how
[00:34:34.240 --> 00:34:38.720]   they did the research, and this is how they set up the constraint. So, this is the basic
[00:34:38.720 --> 00:34:44.880]   about compound scaling. If you have any questions about compound scaling, please ask me now.
[00:34:44.880 --> 00:34:50.200]   I'm again going to give you everybody. We are going well in terms of time. So, I'm going
[00:34:50.200 --> 00:34:54.440]   to give everybody 30 to 45 seconds. Please ask your questions now.
[00:34:54.440 --> 00:35:02.800]   Cuttable flops, is it because of the multiplication? So, there's a question by Srinivas. I'm trying
[00:35:02.800 --> 00:35:07.800]   to gain an intuitive understanding on why doubling network width or cuttable flops.
[00:35:07.800 --> 00:35:12.960]   Actually, I don't have an answer for this. I'm really sorry. I think it was part, I think
[00:35:12.960 --> 00:35:19.600]   this has been explained in my blog. I don't remember. But I don't have an answer for this.
[00:35:19.600 --> 00:35:22.720]   Let's take this off finally. We could take this to the Weights and Biases forums, and
[00:35:22.720 --> 00:35:34.680]   I'll definitely get back to you on this one. Cool. That being said, then, that's compound
[00:35:34.680 --> 00:35:39.440]   scaling. That's the resource constraint. And now we are down to efficient architecture.
[00:35:39.440 --> 00:35:46.720]   So, there's something called this neural architecture search. It's basically using neural networks
[00:35:46.720 --> 00:35:53.160]   to find a neural network architecture. And there was this paper, if I remember correctly,
[00:35:53.160 --> 00:36:01.320]   it was called MNASNet. There it is, which is a platform of a neural architecture search
[00:36:01.320 --> 00:36:07.880]   for mobile. So, Kwaku Lee, the author of EfficientNet and MixingTan were also part of this research
[00:36:07.880 --> 00:36:13.040]   paper. It's basically, again, I'll have to go back to the blog in this one, because in
[00:36:13.040 --> 00:36:17.320]   the actual EfficientNet paper, they don't explain this really well. They just say like
[00:36:17.320 --> 00:36:21.680]   two paragraphs, and then they give you the EfficientNet v0 architecture. But I'll have
[00:36:21.680 --> 00:36:27.360]   to go back to the blog to explain this better. So, you can see what is neural architecture
[00:36:27.360 --> 00:36:32.920]   search. I'm just actually, I mean, keep going down, keep going down. So, here it is. It's
[00:36:32.920 --> 00:36:39.320]   basically like you're using this. This is what this MNASNet paper was doing when I last
[00:36:39.320 --> 00:36:44.360]   read it. It's like you have a controller. This controller samples models from search
[00:36:44.360 --> 00:36:48.520]   space. So, you have like a lot of different model. Like your model could be any model,
[00:36:48.520 --> 00:36:52.360]   right? It could be structured, like it could be BatchNorm, Conv, Relu. Like you could have
[00:36:52.360 --> 00:36:56.640]   a lots of different combinations, right? So, you train your model. Basically, you sample
[00:36:56.640 --> 00:37:01.400]   your model from a search space, then you get basically one sample. Then you train it. You
[00:37:01.400 --> 00:37:06.120]   use it on mobile phones to get the latency and accuracy, which gives you like your reward,
[00:37:06.120 --> 00:37:10.240]   which is like it tells you how the performance is. And this performance of that model goes
[00:37:10.240 --> 00:37:14.560]   back to this controller. So, then it's able to like basically pick up the best performing
[00:37:14.560 --> 00:37:19.520]   model in terms of accuracy and latency on mobile phones. So, this was the MNASNet approach.
[00:37:19.520 --> 00:37:22.800]   Again, we're not going to go very deep into this because it's just neural architecture
[00:37:22.800 --> 00:37:27.840]   search, which is a completely, completely different, completely, completely different
[00:37:27.840 --> 00:37:31.600]   field when it comes to computer vision. It's completely different. So, we're not going
[00:37:31.600 --> 00:37:37.200]   to go much into the detail, but just want to provide some introduction. So, like this
[00:37:37.200 --> 00:37:41.440]   is how they found the EfficientNet B0. So, basically when they were finding the EfficientNet
[00:37:41.440 --> 00:37:46.840]   B0 architecture, they set up some like our search produces an efficient network, which
[00:37:46.840 --> 00:37:53.120]   we call EfficientNet B0. So, this is how, this is the answer, I think, inspired by Tan
[00:37:53.120 --> 00:38:00.200]   et al. 2019. So, Tan et al. 2019 is this MNASNet paper. And what they're saying is, specifically,
[00:38:00.200 --> 00:38:05.320]   we use the same search space. So, same search space basically means like you have the same
[00:38:05.320 --> 00:38:10.320]   possible options of creating your model. And then this becomes your optimization goal.
[00:38:10.320 --> 00:38:16.960]   So, you have basically, I'm saying multi-objective reward would come based on this optimization
[00:38:16.960 --> 00:38:23.120]   goal. So, they're trying to optimize accuracy and also like keep the flops to a minimum.
[00:38:23.120 --> 00:38:27.320]   And then based on this, they get some hyper parameters. And based on this, they get this
[00:38:27.320 --> 00:38:35.560]   EfficientNet B0 architecture. Like that's the basics of it. What you'll see here is
[00:38:35.560 --> 00:38:41.280]   like another of the things you'll see is like, what is this NBConf? So, NBConf, again, it's
[00:38:41.280 --> 00:38:46.400]   very different from, it's called, it's using something called depth-wise convolution. Now,
[00:38:46.400 --> 00:38:50.840]   again, I won't go into the details of everything over here because, again, this is a paper
[00:38:50.840 --> 00:38:55.160]   that's dependent on, say, 10 different papers. So, it would be very difficult to cover all
[00:38:55.160 --> 00:38:59.680]   of the, everything in detail. But I just want to provide enough information so that you
[00:38:59.680 --> 00:39:03.280]   can go back and then you can read through all of these. So, like you understand, at
[00:39:03.280 --> 00:39:07.520]   least, from a holistic view that this is what's happening or this is what's happened. So,
[00:39:07.520 --> 00:39:13.200]   again, going into this, this gives you my architecture, which says, okay, in my stage
[00:39:13.200 --> 00:39:17.200]   one, I have a conf three by three with 32 channels. So, input resolution is two to four
[00:39:17.200 --> 00:39:22.160]   by two to four. That becomes the number of layers is one. The next thing you have, you
[00:39:22.160 --> 00:39:27.640]   have an NBConf one with a kernel size of three by three. Your resolution now will be half
[00:39:27.640 --> 00:39:31.200]   because the conf three by three will make like resolution from two to four go down to
[00:39:31.200 --> 00:39:35.480]   one, one, two by one, one, two. And you have 16 channels and you have one layer of this.
[00:39:35.480 --> 00:39:39.280]   Then you have NBConf six, you have two layers of this and 24 channels and so on. So, you
[00:39:39.280 --> 00:39:43.640]   can see how the number of channels are going up. But the question that you might want to
[00:39:43.640 --> 00:39:50.680]   ask is what is this? What is this NBConf? Let me try and explain. Basically, there's
[00:39:50.680 --> 00:39:56.040]   like these three different versions, NBConf three, NBConf six. So, this comes down to
[00:39:56.040 --> 00:40:02.680]   the bottleneck design. Again, I'm not going to go much, much into the detail. So, it is
[00:40:02.680 --> 00:40:07.160]   nothing but an, again, if you need more detail, I would definitely recommend going through
[00:40:07.160 --> 00:40:12.400]   this blog post. So, you can see this has been further explained. So, basically, your inverted
[00:40:12.400 --> 00:40:18.680]   bottleneck is this NBConf. So, you have your conf, and then you have a depth-wise convolution.
[00:40:18.680 --> 00:40:23.760]   So, if you're looking at this NBConf three with a kernel size of five by five, you have
[00:40:23.760 --> 00:40:28.160]   something called a depth-wise convolution. Now, a depth-wise convolution is different
[00:40:28.160 --> 00:40:35.960]   than a convolution. I do have, let me see if I have a really good article on depth-wise
[00:40:35.960 --> 00:40:40.880]   convolution. I think I remember. Yeah, this is the one. This is the one I really, really
[00:40:40.880 --> 00:40:46.360]   like. So, this is, this will explain, this is some, like, I covered depth-wise convolution
[00:40:46.360 --> 00:40:56.320]   as part of when I was doing Fastbook, but let me copy/paste this, copy/paste this in
[00:40:56.320 --> 00:41:03.200]   the chat for everybody. So, what this depth-wise convolution does is basically, like, if you
[00:41:03.200 --> 00:41:08.400]   have your input image, and you have your convolution, in a common convolution, what you have is
[00:41:08.400 --> 00:41:13.480]   you have a kernel. So, if you have three channels, then you pretty much multiply and add all
[00:41:13.480 --> 00:41:17.320]   three channels at one to get one single value. Like, this is the way this normal convolution
[00:41:17.320 --> 00:41:22.520]   works. If you haven't looked at this, or if you don't understand convolution, I've actually
[00:41:22.520 --> 00:41:26.920]   covered convolutions in a lot of detail in Fastbook. So, for those of you who don't know
[00:41:26.920 --> 00:41:36.760]   what Fastbook is, at Weights & Biases, we've been doing Fastbook. I think that should give
[00:41:36.760 --> 00:41:44.840]   me a playlist. Yeah, there it is. View full playlist. And then there's one chapter on
[00:41:44.840 --> 00:41:52.080]   introduction to convolutions. Let me copy/paste this. I've pasted that in the chat for everybody
[00:41:52.080 --> 00:41:56.920]   as well. So, if you go and have a look at this introduction to convolutions, you'll
[00:41:56.920 --> 00:42:03.040]   understand exactly what's going on in a convolution. But I'm just gonna do a very quick introduction
[00:42:03.040 --> 00:42:08.200]   over here. So, in convolution, basically, you multiply and add all three channels at
[00:42:08.200 --> 00:42:13.520]   once. In depth-wise convolution, what you do is you have the red channel, the green
[00:42:13.520 --> 00:42:17.640]   channel, and the blue channel. But your filter, instead of it being three by three, what you
[00:42:17.640 --> 00:42:24.080]   do is you separate it. So, the red gets multiplied by this. And then, basically, you have the
[00:42:24.080 --> 00:42:28.920]   convolution operation on the second row with this green channel. And then the third part
[00:42:28.920 --> 00:42:32.240]   of the filter with this blue channel. So, you finally get this. And then you add them
[00:42:32.240 --> 00:42:36.800]   together. Basically, it's kind of doing the same thing. But it's just breaking the operation.
[00:42:36.800 --> 00:42:43.360]   So, it's more in terms like the theory behind this is that it produces the same effect as
[00:42:43.360 --> 00:42:50.160]   a normal convolution. But the theory behind it is that it uses less operations. So, that's
[00:42:50.160 --> 00:42:55.560]   the basic understanding of depth-wise. But if you go into, again, nbconv, what it does
[00:42:55.560 --> 00:42:59.520]   is it's using this depth-wise convolution. So, you have a normal convolution, then you
[00:42:59.520 --> 00:43:03.960]   have a depth-wise convolution, and then you have a squeeze excitation and fully connected
[00:43:03.960 --> 00:43:07.880]   basically all of this. And then you have a one by one batch number again. So, this is
[00:43:07.880 --> 00:43:13.600]   the nbconv3 at the top and nbconv6 at the bottom. So, you can now see what this nbconv
[00:43:13.600 --> 00:43:18.920]   layer is. If you need more detail, I would recommend having a look at the EfficientNV2
[00:43:18.920 --> 00:43:24.160]   paper because that also using this depth-wise convolution was actually a shortcoming. So,
[00:43:24.160 --> 00:43:29.480]   I think it says that here it is. The model was searched from the search space with new
[00:43:29.480 --> 00:43:35.080]   ops as fused nbconv. So, what they're saying is actually when they come up with a new architecture
[00:43:35.080 --> 00:43:41.480]   called EfficientNV2, they actually say like we made a mistake by using nbconv1. So, we
[00:43:41.480 --> 00:43:46.520]   instead use something called fused nbconv2. So, I'm just providing you enough context
[00:43:46.520 --> 00:43:55.120]   on how the research has progressed and what's going on here. So, that's the basics of EfficientNets
[00:43:55.120 --> 00:44:01.640]   really. I hope that I've been able to explain, like provide enough introduction to EfficientNets
[00:44:01.640 --> 00:44:06.280]   that now it's possible for you to go back and like pretty much read the paper. After
[00:44:06.280 --> 00:44:11.720]   this is just experiments like this scale up mobile nets and res nets. You can see, I think
[00:44:11.720 --> 00:44:15.960]   it's all at the bottom. So, here it is. Like they're just, it's basically just like flops
[00:44:15.960 --> 00:44:21.000]   versus image net accuracy. They plot the image net results for EfficientNet. They plot the
[00:44:21.000 --> 00:44:26.440]   Cypher 10. You can see how Cypher 10 is doing in terms of accuracy, like all of these different
[00:44:26.440 --> 00:44:31.880]   data sets. And then this is just plotting the cams. So, which is a class activation
[00:44:31.880 --> 00:44:36.520]   map. So, it's just like, this is pretty much all of this is like just more experimentation.
[00:44:36.520 --> 00:44:41.560]   So, then it says EfficientNets are great for transfer learning. And then finally, the conclusion
[00:44:41.560 --> 00:44:46.800]   is like in this paper, we systematically study convnet scaling and they propose a new technique
[00:44:46.800 --> 00:44:51.520]   called compound scaling. And based on that, based on neural architecture skirts, they
[00:44:51.520 --> 00:44:57.840]   found EfficientNet B0 and then use compound scaling to scale this EfficientNet B0 upwards.
[00:44:57.840 --> 00:45:05.760]   That's really it in terms of EfficientNets. In terms of source code, I would recommend
[00:45:05.760 --> 00:45:09.560]   going through the source code through team. I do plan on going through the source code,
[00:45:09.560 --> 00:45:17.400]   but it basically depends on time. But that's all that I have for EfficientNet. I'll give
[00:45:17.400 --> 00:45:31.120]   everybody like 30 to 60 seconds if they have any questions. But
[00:45:31.120 --> 00:45:34.640]   is the kernel, there's a question Manishan, is the kernel used for convolution same for
[00:45:34.640 --> 00:45:40.120]   all three channels? No, in depthwise it's different. If that's the question you're asking.
[00:45:40.120 --> 00:45:54.080]   This is a different kernel. Cool. I guess then that's pretty much it. Thanks, everybody
[00:45:54.080 --> 00:45:59.960]   for joining me today. And hopefully you'll enjoy the paper used by my colleague, Sanyam.
[00:45:59.960 --> 00:46:07.640]   So, if you don't know Sanyam, let me, Sanyam has his own, I think, website, pulsayampadani.com.
[00:46:07.640 --> 00:46:13.760]   He hosts a data science show. He's part of various different things. He's got like a
[00:46:13.760 --> 00:46:18.520]   blog post that got over 1 million blog views. He does a podcast. He's on Twitter. So, if
[00:46:18.520 --> 00:46:26.680]   I am trying to, let me try and find Sanyam on Twitter and share his Twitter with everybody.
[00:46:26.680 --> 00:46:41.040]   I'm sorry, one sec. Twitter. Sanyam. That's his Twitter. So, I'm just going to paste this
[00:46:41.040 --> 00:46:48.160]   for everybody. So, that's Sanyam. So, everybody say hi to Sanyam. He's a lovely, lovely person.
[00:46:48.160 --> 00:46:52.880]   And I hope you enjoy the paper reading groups going forward. So, thanks, everybody. That's
[00:46:52.880 --> 00:46:56.880]   all from my side. I do want to thank everybody who's been part of the paper reading groups
[00:46:56.880 --> 00:47:02.560]   thus far. And ciao from my side. Bye.
[00:47:02.560 --> 00:47:03.560]   - Thank you.
[00:47:03.560 --> 00:47:04.560]   - Thank you.
[00:47:04.560 --> 00:47:05.560]   - Thank you.
[00:47:05.560 --> 00:47:06.560]   - Thank you.
[00:47:06.560 --> 00:47:06.560]   - Thank you.
[00:47:06.560 --> 00:47:07.560]   - Thank you.
[00:47:07.560 --> 00:47:07.560]   - Thank you.
[00:47:07.560 --> 00:47:08.560]   - Thank you.
[00:47:08.560 --> 00:47:08.560]   - Thank you.
[00:47:08.560 --> 00:47:09.560]   - Thank you.
[00:47:09.560 --> 00:47:09.560]   - Thank you.
[00:47:09.560 --> 00:47:10.560]   - Thank you.
[00:47:10.560 --> 00:47:10.560]   - Thank you.
[00:47:10.560 --> 00:47:11.560]   - Thank you.
[00:47:11.560 --> 00:47:11.560]   - Thank you.
[00:47:11.560 --> 00:47:16.560]   - Thank you.
[00:47:16.560 --> 00:47:21.560]   - Thank you.
[00:47:21.560 --> 00:47:26.560]   - Thank you.
[00:47:26.560 --> 00:47:31.560]   - Thank you.
[00:47:31.560 --> 00:47:41.560]   [BLANK_AUDIO]

