
[00:00:00.000 --> 00:00:05.000]   [music playing]
[00:00:05.000 --> 00:00:11.500]   Hi, all. My name is Alex Avrila.
[00:00:11.500 --> 00:00:13.960]   I am co-director at Carper AI,
[00:00:13.960 --> 00:00:16.260]   and today I will be talking about TRLX,
[00:00:16.260 --> 00:00:18.600]   a framework for open source RLHF.
[00:00:18.600 --> 00:00:20.220]   So what is RLHF?
[00:00:20.220 --> 00:00:22.960]   RLHF is an acronym standing for Reinforcement Learning
[00:00:22.960 --> 00:00:24.260]   from Human Feedback,
[00:00:24.260 --> 00:00:25.800]   and you can think of it as a technique
[00:00:25.800 --> 00:00:29.700]   for fine-tuning language models to incorporate human preferences.
[00:00:29.820 --> 00:00:32.460]   And it can be roughly broken down into three stages.
[00:00:32.460 --> 00:00:35.220]   Number one, first you collect pairwise comparisons
[00:00:35.220 --> 00:00:37.660]   among model responses to a given task.
[00:00:37.660 --> 00:00:40.360]   So for example, let's say I want to write happy stories.
[00:00:40.360 --> 00:00:42.260]   Look at the right-hand side.
[00:00:42.260 --> 00:00:44.460]   And the model generates two potential responses,
[00:00:44.460 --> 00:00:47.320]   one about a happy goose and one about a sad goose.
[00:00:47.320 --> 00:00:50.720]   Since I want to write stories which are happy,
[00:00:50.720 --> 00:00:54.320]   I'm going to select the response which is about a happy goose
[00:00:54.320 --> 00:00:57.060]   as more preferable for the response which is about a sad goose.
[00:00:57.760 --> 00:01:02.400]   These pairwise comparisons can then be used to train a reward model,
[00:01:02.400 --> 00:01:04.640]   which you can think of as an adapted language model
[00:01:04.640 --> 00:01:09.440]   which assigns a higher score or reward to the response
[00:01:09.440 --> 00:01:11.900]   which is preferred by the user.
[00:01:11.900 --> 00:01:14.440]   You can think of this reward model as an offline proxy
[00:01:14.440 --> 00:01:17.000]   for the human preferences which were captured in stage one.
[00:01:17.000 --> 00:01:20.060]   Then once this reward model has been learned,
[00:01:20.060 --> 00:01:23.940]   we optimize against it using a reinforcement learning algorithm,
[00:01:23.940 --> 00:01:27.200]   most commonly proximal policy optimization or PPO.
[00:01:27.880 --> 00:01:30.700]   And this results in a downstream student model
[00:01:30.700 --> 00:01:33.940]   which is able to write happy stories or more generally,
[00:01:33.940 --> 00:01:38.000]   it's aligned to the preferences which were collected in stage one
[00:01:38.000 --> 00:01:40.240]   and distilled into a reward model in stage two.
[00:01:40.240 --> 00:01:42.940]   And this makes up the RLHF pipeline.
[00:01:42.940 --> 00:01:47.900]   So some examples of popular RLHF models in the wild.
[00:01:47.900 --> 00:01:49.800]   TATPT, I think we're all familiar with.
[00:01:49.800 --> 00:01:52.100]   Most people have heard it by now.
[00:01:52.100 --> 00:01:55.780]   And this is an example of what we believe to be an RLHF model.
[00:01:55.900 --> 00:01:58.300]   It's been trained using reinforcement learning
[00:01:58.300 --> 00:01:59.940]   and optimized against a reward model
[00:01:59.940 --> 00:02:02.300]   which incorporates human preferences.
[00:02:02.300 --> 00:02:06.300]   However, there are other examples of RLHF models in the wild as well.
[00:02:06.300 --> 00:02:09.540]   So Anthropic AI, a competitor to TATPT,
[00:02:09.540 --> 00:02:12.980]   has their own model called CLOD, which is trained in a similar way.
[00:02:12.980 --> 00:02:15.080]   And then the research group DeepMind
[00:02:15.080 --> 00:02:17.840]   also has their own version of this called Sparrow,
[00:02:17.840 --> 00:02:19.540]   which optimizes against a reward model
[00:02:19.540 --> 00:02:22.440]   specifically for web-based retrieval.
[00:02:22.440 --> 00:02:24.340]   However, one important caveat here
[00:02:24.480 --> 00:02:28.420]   is that these are all examples of closed source RLHF models.
[00:02:28.420 --> 00:02:31.180]   We don't have access to the underlying model weights.
[00:02:31.180 --> 00:02:33.360]   We don't have access to the training data
[00:02:33.360 --> 00:02:35.160]   which was used to produce these models,
[00:02:35.160 --> 00:02:37.480]   or the training framework that was used to produce these models,
[00:02:37.480 --> 00:02:38.480]   for that matter.
[00:02:38.480 --> 00:02:41.280]   And as a result, it's very hard to reproduce them
[00:02:41.280 --> 00:02:45.060]   or even go about studying them in any relatively rigorous way.
[00:02:45.060 --> 00:02:48.820]   In order to do this, we need open source RLHF models.
[00:02:48.820 --> 00:02:51.320]   But the problem here is that really currently
[00:02:51.460 --> 00:02:55.360]   there are no good large-scale open source RLHF models,
[00:02:55.360 --> 00:02:56.700]   yet at least.
[00:02:56.700 --> 00:02:59.800]   So at Carpre AI, I, along with many others,
[00:02:59.800 --> 00:03:03.660]   are working to produce one of the first open source RLHF models.
[00:03:03.660 --> 00:03:06.500]   We plan to open source the model itself,
[00:03:06.500 --> 00:03:08.360]   as well as the training framework
[00:03:08.360 --> 00:03:11.640]   and the data that was used to train the model.
[00:03:11.640 --> 00:03:15.340]   And we anticipate this to be hugely impactful
[00:03:15.340 --> 00:03:18.700]   to a bunch of people, developers, researchers,
[00:03:19.440 --> 00:03:21.720]   and even everyday folk who are interested
[00:03:21.720 --> 00:03:23.680]   in trying out some of these models.
[00:03:23.680 --> 00:03:26.880]   Probably researchers and developers for the most part.
[00:03:26.880 --> 00:03:28.820]   So why does open source RLHF matter?
[00:03:28.820 --> 00:03:32.180]   I mean, I kind of already touched on some of these points,
[00:03:32.180 --> 00:03:36.720]   but I think there are many people who feel that equal access for all
[00:03:36.720 --> 00:03:40.440]   to this kind of potentially world-changing technology is crucial.
[00:03:40.440 --> 00:03:44.120]   Carpre AI is fortunate enough to be supported by Stability AI
[00:03:44.120 --> 00:03:47.520]   who provides compute for us to train these RLHF models.
[00:03:47.640 --> 00:03:51.180]   And democratization of AI is central to Stability's mission.
[00:03:51.180 --> 00:03:54.380]   Giving people access to this kind of technology is crucial,
[00:03:54.380 --> 00:03:57.140]   I believe, and should not be left in the hands of the few.
[00:03:57.140 --> 00:04:00.640]   But perhaps most importantly, in my belief,
[00:04:00.640 --> 00:04:05.640]   is closed source RLHF makes good science fundamentally impossible.
[00:04:05.640 --> 00:04:09.240]   If researchers don't have access to the underlying model weights
[00:04:09.240 --> 00:04:10.720]   or to the underlying data,
[00:04:10.720 --> 00:04:14.080]   then we really have no idea how these models were really trained.
[00:04:14.080 --> 00:04:17.080]   And so it is hard for us to draw meaningful comparisons
[00:04:17.220 --> 00:04:19.420]   or study these models in any rigorous way.
[00:04:19.420 --> 00:04:21.940]   And due to the huge impact these models have been having,
[00:04:21.940 --> 00:04:25.680]   and media in general, it's very important to be able to study them
[00:04:25.680 --> 00:04:27.680]   and understand them in a deeper way.
[00:04:27.680 --> 00:04:31.440]   So blockers to open source RLHF, what makes this problem difficult?
[00:04:31.440 --> 00:04:35.020]   So first, number one, high quality supervised fine tuning data
[00:04:35.020 --> 00:04:36.740]   is generally difficult to find.
[00:04:36.740 --> 00:04:40.240]   There's kind of a step zero, which I didn't mention beforehand,
[00:04:40.240 --> 00:04:44.020]   before you collect preferences among various model outputs,
[00:04:44.140 --> 00:04:49.800]   which is you collect a bunch of high quality responses,
[00:04:49.800 --> 00:04:53.500]   kind of telling your model, what you want it to roughly do.
[00:04:53.500 --> 00:04:55.360]   And then you fine tune it on these responses.
[00:04:55.360 --> 00:04:57.800]   And this kind of gets the model more in distribution.
[00:04:57.800 --> 00:05:00.060]   If you are from the RL domain,
[00:05:00.060 --> 00:05:01.840]   and are familiar with the concept of behavior planning,
[00:05:01.840 --> 00:05:03.660]   you can think of it as analogous to that.
[00:05:03.660 --> 00:05:06.840]   Number two, once you have a supervised fine tune model,
[00:05:06.840 --> 00:05:09.940]   you still need access to high quality preference data,
[00:05:10.060 --> 00:05:14.280]   which is, you present various responses to users
[00:05:14.280 --> 00:05:16.060]   and then have the users rank or choose
[00:05:16.060 --> 00:05:17.920]   which responses are most preferable.
[00:05:17.920 --> 00:05:19.980]   This is also relatively expensive to collect.
[00:05:19.980 --> 00:05:23.080]   And there are not that many examples out there in the wild.
[00:05:23.080 --> 00:05:25.160]   Number three, and perhaps most underrated,
[00:05:25.160 --> 00:05:27.820]   you need a scalable training framework for language model
[00:05:27.820 --> 00:05:29.820]   fine tuning with reinforcement learning.
[00:05:29.820 --> 00:05:32.660]   The dominant language model training paradigm
[00:05:32.660 --> 00:05:35.460]   over the last couple of years has been a supervised objective,
[00:05:35.460 --> 00:05:39.180]   such as, or semi-supervised objective,
[00:05:39.320 --> 00:05:40.980]   such as next token prediction,
[00:05:40.980 --> 00:05:45.220]   where you don't have to query an outside reward model
[00:05:45.220 --> 00:05:46.460]   or something like that.
[00:05:46.460 --> 00:05:49.380]   And so for particularly training models at scale,
[00:05:49.380 --> 00:05:52.520]   this makes adapting these scaling frameworks
[00:05:52.520 --> 00:05:54.220]   difficult to the RL domain,
[00:05:54.220 --> 00:05:56.820]   since you have many more moving parts of reinforcement learning.
[00:05:56.820 --> 00:06:00.020]   You need to host the reward model, you need to host the student model.
[00:06:00.020 --> 00:06:02.580]   And for online RL, you need to host other things as well.
[00:06:02.580 --> 00:06:05.120]   And this makes the compute and memory requirements
[00:06:05.120 --> 00:06:07.080]   that much more difficult to satisfy,
[00:06:07.220 --> 00:06:10.220]   which were already difficult for 70 billion parameter models.
[00:06:10.220 --> 00:06:12.180]   So where does TRLX come in?
[00:06:12.180 --> 00:06:16.280]   So TRLX addresses number three, a scalable RL training framework.
[00:06:16.280 --> 00:06:19.460]   This was a training framework originally developed by myself
[00:06:19.460 --> 00:06:21.180]   and a couple others at Copper AI.
[00:06:21.180 --> 00:06:25.560]   And as of right now, it is the leader in open source
[00:06:25.560 --> 00:06:27.760]   RLHF training at scale.
[00:06:27.760 --> 00:06:29.180]   It's an actively developed library,
[00:06:29.180 --> 00:06:30.780]   which gets regular updates every week.
[00:06:30.780 --> 00:06:37.180]   And we hope for this repo to continue to be pushing forward
[00:06:37.280 --> 00:06:38.980]   open source RLHF training at scale,
[00:06:38.980 --> 00:06:40.520]   and we're excited to see where it goes.
[00:06:40.520 --> 00:06:43.720]   So what are some of the capabilities that TRLX supports?
[00:06:43.720 --> 00:06:45.320]   When we were designing the library,
[00:06:45.320 --> 00:06:48.120]   we were cognizant of the fact that we would have users
[00:06:48.120 --> 00:06:51.480]   with a wide range of various user profiles using it.
[00:06:51.480 --> 00:06:54.240]   And so we tried to build in various training frameworks,
[00:06:54.240 --> 00:06:56.440]   which accommodate each of these users separately.
[00:06:56.440 --> 00:06:58.620]   These are roughly divided into three cases.
[00:06:58.620 --> 00:07:00.780]   So first of all, you have your independent researchers
[00:07:00.780 --> 00:07:03.020]   with access to maybe a single GPU.
[00:07:03.020 --> 00:07:06.140]   And these can train up to about a billion parameter model
[00:07:06.280 --> 00:07:07.280]   using native pipeline.
[00:07:07.280 --> 00:07:08.820]   But this is still pretty decent,
[00:07:08.820 --> 00:07:10.380]   you can get pretty good RLHF results
[00:07:10.380 --> 00:07:11.880]   with a one billion parameter model.
[00:07:11.880 --> 00:07:13.280]   If you have some more resources,
[00:07:13.280 --> 00:07:15.620]   so maybe multiple GPUs on a single node,
[00:07:15.620 --> 00:07:19.720]   you can consider training using Hugging Face Accelerate integration
[00:07:19.720 --> 00:07:21.820]   plus Deep Speed, which allows you to train
[00:07:21.820 --> 00:07:23.740]   about up to 20 billion parameter models.
[00:07:23.740 --> 00:07:25.180]   And this is quite good.
[00:07:25.180 --> 00:07:27.180]   And if you are particularly well endowed user
[00:07:27.180 --> 00:07:30.140]   with multiple nodes of eight, eight and 100s or something,
[00:07:30.140 --> 00:07:33.020]   then we have tested our integration
[00:07:33.020 --> 00:07:35.340]   with the NVIDIA NEMO training framework,
[00:07:35.480 --> 00:07:38.120]   fine tuning models up to 70 billion parameters.
[00:07:38.120 --> 00:07:40.780]   And this is competitive with the size of models
[00:07:40.780 --> 00:07:43.980]   you see in many of these Anthropx papers, for example.
[00:07:43.980 --> 00:07:46.080]   And we're excited to see where this takes us.
[00:07:46.080 --> 00:07:48.380]   So what are some features that TRLX supports?
[00:07:48.380 --> 00:07:49.680]   We have a good number.
[00:07:49.680 --> 00:07:52.720]   So first of all, we're compatible with most encoder decoders,
[00:07:52.720 --> 00:07:56.040]   such as T5 model, and decoder only models,
[00:07:56.040 --> 00:07:59.280]   such as DPT NeoX, which are supported on Hugging Face.
[00:07:59.280 --> 00:08:02.740]   This makes loading and saving models relatively easy and efficient.
[00:08:02.740 --> 00:08:05.080]   Also, like I said, we were cognizant of the fact
[00:08:05.220 --> 00:08:07.580]   that we want this library to be accessible
[00:08:07.580 --> 00:08:10.180]   to many different users and use cases,
[00:08:10.180 --> 00:08:13.580]   particularly independent researchers who have limited resources.
[00:08:13.580 --> 00:08:15.880]   So we've incorporated several memory saving
[00:08:15.880 --> 00:08:17.620]   slash compute efficient features,
[00:08:17.620 --> 00:08:20.140]   including low rank adaptation or Laura,
[00:08:20.140 --> 00:08:22.680]   8-bit atom, layer freezing and Hydra.
[00:08:22.680 --> 00:08:24.240]   And we'll go into these in more detail later.
[00:08:24.240 --> 00:08:27.140]   But in general, you can think of these as reducing the memory
[00:08:27.140 --> 00:08:29.440]   or compile requirements to do our laptop.
[00:08:29.440 --> 00:08:32.820]   We've also incorporated various online
[00:08:32.940 --> 00:08:35.340]   and offline reinforcement learning algorithms,
[00:08:35.340 --> 00:08:37.240]   which have their various trade offs and benefits,
[00:08:37.240 --> 00:08:38.840]   which I'll talk about later.
[00:08:38.840 --> 00:08:41.140]   And perhaps most importantly,
[00:08:41.140 --> 00:08:44.900]   we have support for multi GPU hyper parameter sweeps,
[00:08:44.900 --> 00:08:47.180]   which are very important for reinforcement learning,
[00:08:47.180 --> 00:08:50.180]   which are integrated with weights and biases,
[00:08:50.180 --> 00:08:51.600]   which we use for experiment tracking.
[00:08:51.600 --> 00:08:54.000]   And this is very useful. We use the feature every day.
[00:08:54.000 --> 00:08:57.400]   Additionally, we have 10 plus built in examples of varying complexity,
[00:08:57.400 --> 00:09:01.280]   which users can run and get started with and start building their own.
[00:09:01.400 --> 00:09:05.060]   So let's dive in a little more and look at the anatomy of language model
[00:09:05.060 --> 00:09:07.800]   fine tuning via online reinforcement learning at scale.
[00:09:07.800 --> 00:09:10.200]   So this is kind of a complex process.
[00:09:10.200 --> 00:09:14.140]   And at training time, you have many different models,
[00:09:14.140 --> 00:09:15.840]   which you need to hold in memory.
[00:09:15.840 --> 00:09:19.300]   So first, you need the model that you're fine tuning the student model,
[00:09:19.300 --> 00:09:21.560]   which you can think of as your policy.
[00:09:21.560 --> 00:09:23.960]   And you also need the value head of this policy,
[00:09:23.960 --> 00:09:27.600]   which is used for proximal policy optimization or PPO.
[00:09:27.600 --> 00:09:29.540]   And if you're not familiar, you can think of this value head
[00:09:29.660 --> 00:09:33.060]   as kind of trying to learn and approximate the reward
[00:09:33.060 --> 00:09:35.600]   that a given response will generate.
[00:09:35.600 --> 00:09:39.360]   Additionally, we have this reference model frozen reference model,
[00:09:39.360 --> 00:09:43.420]   which is used to kind of produce some kind of KL control penalty,
[00:09:43.420 --> 00:09:45.520]   enforcing the student model,
[00:09:45.520 --> 00:09:48.760]   it doesn't kind of overfit to the reward too hard.
[00:09:48.760 --> 00:09:50.620]   And additionally, we need to hold in memory
[00:09:50.620 --> 00:09:53.700]   the reward models themselves, which can be quite large.
[00:09:53.700 --> 00:09:56.220]   And so altogether, if you were to treat each of these models
[00:09:56.360 --> 00:09:59.420]   as separate value network separate from a student network,
[00:09:59.420 --> 00:10:02.120]   separate from a reference network, separate from a reward network,
[00:10:02.120 --> 00:10:05.660]   that's a high bar requires a lot of compute and a lot of memory.
[00:10:05.660 --> 00:10:11.560]   However, TROx leverages a recent architecture introduced by Sparrow,
[00:10:11.560 --> 00:10:14.320]   DeepMind's paper Sparrow, called the Hydra architecture,
[00:10:14.320 --> 00:10:17.120]   which fuses together all these models
[00:10:17.120 --> 00:10:20.060]   into kind of one shared trunk with shared layers.
[00:10:20.060 --> 00:10:22.900]   And this is made feasible by the fact that most of these models
[00:10:22.900 --> 00:10:25.260]   have their layers frozen already.
[00:10:25.400 --> 00:10:27.000]   For example, the reference model is frozen,
[00:10:27.000 --> 00:10:29.600]   it's not being optimized, the reward models are frozen,
[00:10:29.600 --> 00:10:30.900]   they're just being inferenced.
[00:10:30.900 --> 00:10:32.840]   Really, it's just the student that we're training.
[00:10:32.840 --> 00:10:34.640]   And in many cases, it doesn't even make sense
[00:10:34.640 --> 00:10:36.360]   to train the majority of the student anyway.
[00:10:36.360 --> 00:10:39.860]   We want to freeze most of the layers to provide more stability.
[00:10:39.860 --> 00:10:41.440]   And we find that in some cases,
[00:10:41.440 --> 00:10:44.600]   this can save up to 75% of the memory required.
[00:10:44.600 --> 00:10:47.260]   Otherwise, if we were to treat these all in separate networks,
[00:10:47.260 --> 00:10:48.700]   and this really makes it more accessible
[00:10:48.700 --> 00:10:50.800]   to all our different user groups.
[00:10:50.800 --> 00:10:52.200]   And we definitely recommend this feature.
[00:10:52.340 --> 00:10:55.340]   To demo the TRLX library, let's do a warm up.
[00:10:55.340 --> 00:10:58.000]   Let's say we're fine tuning GPTJ,
[00:10:58.000 --> 00:11:01.580]   which is a 6 billion parameter model to produce positive movie review.
[00:11:01.580 --> 00:11:05.280]   How would we do this? Well, we're going to use a reward model,
[00:11:05.280 --> 00:11:08.500]   which is distilbert IMDB, 55 million parameters,
[00:11:08.500 --> 00:11:10.600]   and it's been trained to classify movie reviews
[00:11:10.600 --> 00:11:12.540]   into positive negative sentiment.
[00:11:12.540 --> 00:11:14.640]   And then we're going to use proximal policy optimization,
[00:11:14.640 --> 00:11:17.100]   or PPO, which is an online RL algorithm.
[00:11:17.640 --> 00:11:22.700]   The environment we'll be training in is using 40 gigabyte 8A100s.
[00:11:22.700 --> 00:11:25.040]   And we'll be using DSP 0.2 as our trainer.
[00:11:25.040 --> 00:11:28.440]   And what are the results? So we see that things look pretty good.
[00:11:28.440 --> 00:11:31.480]   After a couple 100 steps, the reward that we get is quite high.
[00:11:31.480 --> 00:11:34.840]   Note the reward from distilbert is between zero and one.
[00:11:34.840 --> 00:11:37.340]   It is effectively just calculating the probability
[00:11:37.340 --> 00:11:39.540]   that the result of text is positive or negative.
[00:11:39.540 --> 00:11:44.680]   And yeah, the reward looks stable, things increase quickly, life is good.
[00:11:46.040 --> 00:11:49.040]   Now, drawing your attention to the graphs on the right hand side,
[00:11:49.040 --> 00:11:51.340]   we have the top, which is explore time,
[00:11:51.340 --> 00:11:53.580]   and we have the bottom, which is backward time.
[00:11:53.580 --> 00:11:56.780]   And these kind of represent the time that the algorithm takes
[00:11:56.780 --> 00:12:00.080]   to do the two main components of the,
[00:12:00.080 --> 00:12:03.280]   I guess, reinforcement learning training loop with language model.
[00:12:03.280 --> 00:12:06.780]   Explore time represents the time it takes to generate rollouts
[00:12:06.780 --> 00:12:10.140]   and allow the model to generate new experiences,
[00:12:10.140 --> 00:12:13.500]   which are then stored in a replay buffer and used at train time.
[00:12:14.100 --> 00:12:15.940]   And then we see that this takes a while.
[00:12:15.940 --> 00:12:21.100]   I mean, a 6 billion parameter model is relatively expensive to infer, right?
[00:12:21.100 --> 00:12:22.500]   And we're trying to generate a story,
[00:12:22.500 --> 00:12:27.400]   which means it needs to be inferred up to 40 times for each movie review.
[00:12:27.400 --> 00:12:30.100]   And this is expensive, can take up to three seconds.
[00:12:30.100 --> 00:12:32.360]   And it's definitely the bottleneck of the computation.
[00:12:32.360 --> 00:12:35.960]   So reducing or eliminating entirely the rollout time is very ideal
[00:12:35.960 --> 00:12:37.300]   if we're trying to speed up training.
[00:12:37.300 --> 00:12:39.300]   Compare this to the amount of time it takes
[00:12:39.300 --> 00:12:41.140]   to do a backward pass on the model.
[00:12:41.260 --> 00:12:43.220]   And we see that this is the backward time
[00:12:43.220 --> 00:12:45.720]   is a fraction of the time that rollout takes.
[00:12:45.720 --> 00:12:47.700]   So optimizing rollout is quite ideal.
[00:12:47.700 --> 00:12:50.400]   This slide is just to allow you to qualitatively evaluate
[00:12:50.400 --> 00:12:53.300]   the results of the optimization.
[00:12:53.300 --> 00:12:55.900]   The reward that you see on the right, like I said,
[00:12:55.900 --> 00:12:58.260]   is between zero and one, so a reward of 0.99
[00:12:58.260 --> 00:13:00.260]   is basically as good as you can get.
[00:13:00.260 --> 00:13:03.160]   So pause here if you want to examine some of the qualitative results.
[00:13:03.160 --> 00:13:05.200]   So some takeaways from fine tuning for Sunsman.
[00:13:05.200 --> 00:13:08.260]   Pros, relatively sample efficient.
[00:13:08.400 --> 00:13:12.000]   Like I said, you see that reward goes up pretty fast, right?
[00:13:12.000 --> 00:13:16.800]   Also, the reward is stable, which means that reward is going up reliably.
[00:13:16.800 --> 00:13:19.260]   You don't see it fluctuating up and down stochastically.
[00:13:19.260 --> 00:13:24.300]   But cons, so online RL is pretty difficult to scale.
[00:13:24.300 --> 00:13:29.500]   You have to have several different copies of the model in memory at times.
[00:13:29.500 --> 00:13:31.540]   You need the value network, you need the policy,
[00:13:31.540 --> 00:13:34.460]   you need the reference model, and you need the reward model.
[00:13:34.460 --> 00:13:36.960]   And this is somewhat alleviated by the Hydra architecture,
[00:13:37.100 --> 00:13:38.740]   like I said, but still expensive.
[00:13:38.740 --> 00:13:41.640]   And then it's also somewhat prone to reward overfitting.
[00:13:41.640 --> 00:13:43.900]   And what I mean when I say this is,
[00:13:43.900 --> 00:13:45.560]   if you look at a couple of these samples here,
[00:13:45.560 --> 00:13:48.600]   you notice that just even of these six samples,
[00:13:48.600 --> 00:13:51.460]   the first generated token is about music, right?
[00:13:51.460 --> 00:13:55.060]   And this is kind of representative of an underlying phenomena
[00:13:55.060 --> 00:13:58.400]   you will see, which is the more you optimize against a reward,
[00:13:58.400 --> 00:13:59.740]   the more overfit you will become
[00:13:59.740 --> 00:14:02.740]   and the less diversity of generated text you will see.
[00:14:02.740 --> 00:14:05.840]   And this is particularly bad with online algorithms
[00:14:05.960 --> 00:14:07.500]   versus offline algorithm.
[00:14:07.500 --> 00:14:09.440]   I would say it's a significant drawback.
[00:14:09.440 --> 00:14:12.000]   So, you want your reward to be pretty high
[00:14:12.000 --> 00:14:14.200]   so that you know you've done a good job optimizing,
[00:14:14.200 --> 00:14:15.960]   but you don't want your reward to be so high
[00:14:15.960 --> 00:14:17.540]   that it's obvious that you've overfit.
[00:14:17.540 --> 00:14:20.300]   And the resulting text is obviously wrong,
[00:14:20.300 --> 00:14:22.240]   but rated highly by the reward model
[00:14:22.240 --> 00:14:25.400]   because the reward model is an imperfect representation
[00:14:25.400 --> 00:14:27.540]   of the human preferences it's trying to capture.
[00:14:27.540 --> 00:14:29.260]   So now with that in mind,
[00:14:29.260 --> 00:14:31.740]   let's take a look at a more advanced case study
[00:14:31.740 --> 00:14:34.360]   where we are training a 20 billion parameter model,
[00:14:34.500 --> 00:14:38.140]   GPT-NeoX to generate summarizations of some Reddit stories.
[00:14:38.140 --> 00:14:41.600]   The reward model will be a fine-tuned GPT-J,
[00:14:41.600 --> 00:14:44.300]   and we'll be using instead an offline algorithm here,
[00:14:44.300 --> 00:14:46.340]   which is implicit language Q-learning.
[00:14:46.340 --> 00:14:49.740]   The environment we're in will be a 40 gigabyte A100 again,
[00:14:49.740 --> 00:14:52.440]   and the trainer we'll be using is DeepSpeedZero2.
[00:14:52.440 --> 00:14:56.000]   Well, we see that the reward that we compute goes up pretty quickly,
[00:14:56.000 --> 00:14:57.440]   looks like training is still stable
[00:14:57.440 --> 00:15:00.940]   and we converge to a reward of about 2.5.
[00:15:01.800 --> 00:15:04.640]   And perhaps more notably, the training time,
[00:15:04.640 --> 00:15:07.300]   which it takes to do a full forward-backward pass,
[00:15:07.300 --> 00:15:08.680]   is only three seconds,
[00:15:08.680 --> 00:15:11.500]   which is when you compare it to the amount of time
[00:15:11.500 --> 00:15:15.340]   it takes to generate rollouts in the online case, very competitive,
[00:15:15.340 --> 00:15:19.280]   especially when you consider the fact that the model we're fine-tuning here
[00:15:19.280 --> 00:15:23.240]   is three times the size of the student model in the sentiments case,
[00:15:23.240 --> 00:15:25.000]   and with a much larger context.
[00:15:25.000 --> 00:15:28.700]   And I just want to point out here that because we're doing offline RL,
[00:15:28.700 --> 00:15:30.240]   no rollouts are required,
[00:15:30.380 --> 00:15:32.280]   which is a component of the fine-tuning procedure,
[00:15:32.280 --> 00:15:35.360]   which takes, it's the bottleneck, it takes the most amount of time.
[00:15:35.360 --> 00:15:38.180]   And so because we've managed to eliminate that bottleneck,
[00:15:38.180 --> 00:15:42.560]   we find that offline reinforcement learning is much more efficient
[00:15:42.560 --> 00:15:44.720]   in terms of compute and wall clock time.
[00:15:44.720 --> 00:15:49.760]   And I let you survey some qualitative results here.
[00:15:49.760 --> 00:15:52.520]   Not going to read you the story, but you can pause if you want to.
[00:15:52.520 --> 00:15:55.520]   And if you look at the summary, I'd say the summary is pretty decent.
[00:15:55.520 --> 00:15:57.860]   If in order to properly evaluate these models,
[00:15:57.980 --> 00:15:59.880]   you would need to deploy them to users,
[00:15:59.880 --> 00:16:04.220]   have them rank their responses, and then compute win rates.
[00:16:04.220 --> 00:16:06.920]   But just looking at the reward here
[00:16:06.920 --> 00:16:10.780]   and examining doing a qualitative analysis is pretty good as well.
[00:16:10.780 --> 00:16:13.780]   And it lets you get a feel for how well these models are performing.
[00:16:13.780 --> 00:16:17.720]   So some takeaways for the second fine-tuning for summarization example.
[00:16:17.720 --> 00:16:23.020]   Pros, relative to online RL, offline RL is compute efficient,
[00:16:23.020 --> 00:16:24.860]   training time is much faster.
[00:16:24.980 --> 00:16:29.560]   It's easier to scale, meaning that the offline RL objective
[00:16:29.560 --> 00:16:31.620]   and training framework is much more similar
[00:16:31.620 --> 00:16:35.260]   to the kind of training that you see doing supervised fine-tuning.
[00:16:35.260 --> 00:16:38.520]   And so it's much easier to implement code-wise
[00:16:38.520 --> 00:16:42.160]   and architecture-wise than online.
[00:16:42.160 --> 00:16:44.980]   And it's also relatively robust to overfitting,
[00:16:44.980 --> 00:16:46.960]   because we're doing offline and we're not actively
[00:16:46.960 --> 00:16:49.660]   interacting with the reward model.
[00:16:49.660 --> 00:16:52.620]   However, cons are, if you're really trying to get a high reward
[00:16:52.760 --> 00:16:55.060]   and optimize your reward model as far as you can,
[00:16:55.060 --> 00:16:57.500]   online is going to do better than offline.
[00:16:57.500 --> 00:16:59.060]   And to decide between the two,
[00:16:59.060 --> 00:17:02.000]   you're just going to have to judge yourself and mix and match, perhaps.
[00:17:02.000 --> 00:17:03.660]   So kind of summarizing the takeaways here,
[00:17:03.660 --> 00:17:05.300]   we have this nice little table.
[00:17:05.300 --> 00:17:07.840]   On the left, PPO and IoQL.
[00:17:07.840 --> 00:17:10.160]   And then on the top, we have ease to scale,
[00:17:10.160 --> 00:17:11.940]   one is better, one is not.
[00:17:11.940 --> 00:17:14.200]   Compute efficiency, offline is a little better,
[00:17:14.200 --> 00:17:16.900]   online is a little worse. Robustness to overfitting,
[00:17:16.900 --> 00:17:19.100]   we have observed online is a little worse,
[00:17:19.100 --> 00:17:22.260]   offline is a little better. And then really maximizing the reward.
[00:17:22.400 --> 00:17:24.660]   So in this case, online does pretty well,
[00:17:24.660 --> 00:17:26.260]   offline not quite as good.
[00:17:26.260 --> 00:17:29.560]   I do want to say here, though, that these are just the results of our study.
[00:17:29.560 --> 00:17:32.400]   And this isn't very much so an active area of research.
[00:17:32.400 --> 00:17:35.860]   So these could be domain-specific phenomena,
[00:17:35.860 --> 00:17:37.200]   which we're discovering.
[00:17:37.200 --> 00:17:40.560]   And so I would definitely recommend doing your own experiments as well
[00:17:40.560 --> 00:17:41.860]   to get a sense of what works well.
[00:17:41.860 --> 00:17:45.640]   So in conclusion, TRLX provides a scalable,
[00:17:45.640 --> 00:17:49.300]   flexible framework for training models up to 70 billion parameters
[00:17:49.300 --> 00:17:51.040]   with reinforcement learning.
[00:17:51.560 --> 00:17:55.840]   An online and an offline trainer for various use cases
[00:17:55.840 --> 00:17:57.400]   in different scenarios.
[00:17:57.400 --> 00:17:59.460]   10 plus examples of varying complexity
[00:17:59.460 --> 00:18:01.540]   ranging from beginner to research level,
[00:18:01.540 --> 00:18:04.040]   which users can get started with and start plugging in.
[00:18:04.040 --> 00:18:07.240]   And finally, integration with weights and biases
[00:18:07.240 --> 00:18:09.740]   for experiment tracking and hyperparameter sweeps,
[00:18:09.740 --> 00:18:13.200]   which I again want to emphasize are crucial for reinforcement learning.
[00:18:13.200 --> 00:18:15.140]   Hyperparameters are quite finicky.
[00:18:15.140 --> 00:18:17.040]   And to get really the maximum performance,
[00:18:17.040 --> 00:18:20.360]   you need to do a thorough search space of your hyperparameters.
[00:18:20.960 --> 00:18:22.560]   So this concludes the presentation.
[00:18:22.560 --> 00:18:25.100]   I attached some resources here at the end.
[00:18:25.100 --> 00:18:26.840]   Otherwise, thank you for listening.
[00:18:26.840 --> 00:18:29.340]   (light music)
[00:18:29.340 --> 00:18:31.920]   (gentle music)
[00:18:31.920 --> 00:18:33.860]   (bright upbeat music)

