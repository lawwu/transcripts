
[00:00:00.640 --> 00:00:01.040]   Cool.
[00:00:01.040 --> 00:00:02.460]   I'm going to record.
[00:00:02.460 --> 00:00:04.540]   You're welcome to set some context.
[00:00:04.540 --> 00:00:05.200]   Yeah, go ahead.
[00:00:05.200 --> 00:00:06.400]   Cool.
[00:00:06.400 --> 00:00:08.640]   And everyone can see my slides and everything.
[00:00:08.640 --> 00:00:11.540]   Well, I can see your page.
[00:00:11.540 --> 00:00:13.400]   It looks like a paper, but I guess it's a slide.
[00:00:13.400 --> 00:00:13.700]   Yeah.
[00:00:13.700 --> 00:00:17.620]   Yeah, this is just cut and paste to the first slide.
[00:00:17.620 --> 00:00:18.960]   Cool.
[00:00:18.960 --> 00:00:20.080]   Well, thank you all for being here.
[00:00:20.080 --> 00:00:20.960]   This is really exciting.
[00:00:20.960 --> 00:00:25.140]   And yeah, super excited to chat about this.
[00:00:25.140 --> 00:00:27.180]   So I'm Shelly.
[00:00:27.180 --> 00:00:29.640]   I'm one of our AI engineers and researchers
[00:00:29.640 --> 00:00:30.160]   at Ryder.
[00:00:30.160 --> 00:00:33.940]   And one of the things we've been talking
[00:00:33.940 --> 00:00:37.360]   and thinking about a lot in the last six to nine months
[00:00:37.360 --> 00:00:40.140]   has been self-evolving large language models,
[00:00:40.140 --> 00:00:41.640]   self-improving large language models.
[00:00:41.640 --> 00:00:43.300]   And I think this is a term or phrase
[00:00:43.300 --> 00:00:44.960]   that's been coming up more and more
[00:00:44.960 --> 00:00:50.060]   across the board for lots of different companies, right?
[00:00:50.060 --> 00:00:54.040]   How can we functionally fine-tune small, large language models
[00:00:54.040 --> 00:00:57.360]   for specific customers in a way that adapts
[00:00:57.360 --> 00:00:59.360]   to their use cases so that they can learn
[00:00:59.360 --> 00:01:03.260]   from their mistakes and become more and more useful?
[00:01:03.260 --> 00:01:05.080]   And I have a couple of canonical examples
[00:01:05.080 --> 00:01:07.280]   that I always use when I'm doing less technical talks
[00:01:07.280 --> 00:01:10.120]   about the subject, or I talk about these sorts of tasks
[00:01:10.120 --> 00:01:14.060]   that you would expect a large language model
[00:01:14.060 --> 00:01:16.140]   to be able to do based on knowing that,
[00:01:16.140 --> 00:01:17.860]   you know, they've been trained on reasoning
[00:01:17.860 --> 00:01:19.640]   and math and coding and all of these things,
[00:01:19.640 --> 00:01:21.540]   but there are still so many gaps
[00:01:21.540 --> 00:01:23.200]   that enterprise customers,
[00:01:23.200 --> 00:01:26.500]   but then also like us in our everyday lives feel.
[00:01:26.920 --> 00:01:30.000]   So like, for example, one of the examples I give,
[00:01:30.000 --> 00:01:33.120]   I recently moved and so I have a new apartment
[00:01:33.120 --> 00:01:35.240]   and the leasing office gave me a floor plan.
[00:01:35.240 --> 00:01:38.520]   And so I asked Claude Sonnet 4 to basically take,
[00:01:38.520 --> 00:01:40.780]   I was like, okay, if I have this floor plan,
[00:01:40.780 --> 00:01:44.260]   can Claude put furniture onto the floor plan for me, right?
[00:01:44.260 --> 00:01:46.840]   Like put a bed here, that's the right size and better that.
[00:01:46.840 --> 00:01:48.860]   And like, you know, the bed ends up in the bathroom
[00:01:48.860 --> 00:01:50.160]   and the couch is like sideways
[00:01:50.160 --> 00:01:51.420]   and like all these crazy things,
[00:01:51.420 --> 00:01:52.960]   like all the walls are moving around.
[00:01:52.960 --> 00:01:54.400]   And it's like one of those things where it's like,
[00:01:54.400 --> 00:01:56.920]   you know, it's not necessarily an intuitive
[00:01:56.920 --> 00:01:59.560]   that this doesn't work, like it should work.
[00:01:59.560 --> 00:02:02.040]   And so, you know, when we think about self-evolving
[00:02:02.040 --> 00:02:03.040]   or self-improving models,
[00:02:03.040 --> 00:02:05.200]   what we're talking about is how can we,
[00:02:05.200 --> 00:02:07.360]   between big model releases,
[00:02:07.360 --> 00:02:08.920]   create large language models
[00:02:08.920 --> 00:02:10.680]   that can learn and adapt to these use cases
[00:02:10.680 --> 00:02:11.980]   that should be in distribution,
[00:02:11.980 --> 00:02:13.260]   but for whatever reason aren't.
[00:02:13.260 --> 00:02:16.840]   So I'm going to spend maybe 20 minutes
[00:02:16.840 --> 00:02:17.680]   walking through the paper
[00:02:17.680 --> 00:02:18.820]   and then hopefully we can just kind of
[00:02:18.820 --> 00:02:21.260]   have a greater discussion about self-improvement,
[00:02:21.260 --> 00:02:22.260]   about GRPO,
[00:02:22.260 --> 00:02:23.840]   like lots of cool research in this space
[00:02:23.840 --> 00:02:25.000]   in the last couple of months,
[00:02:25.000 --> 00:02:27.380]   especially after sort of the May conference
[00:02:27.380 --> 00:02:28.120]   submission deadlines.
[00:02:28.120 --> 00:02:30.520]   So, yeah, I'll just kind of get into it.
[00:02:30.520 --> 00:02:32.860]   So this is the paper that we released
[00:02:32.860 --> 00:02:34.460]   in the late May, early June
[00:02:34.460 --> 00:02:37.540]   called Reflect, Retry, Reward,
[00:02:37.540 --> 00:02:39.200]   Self-improving Large Language Models
[00:02:39.200 --> 00:02:40.240]   to be in Reinforcement Learning.
[00:02:40.240 --> 00:02:42.980]   And it's kind of motivated
[00:02:42.980 --> 00:02:44.280]   by what I was speaking about before,
[00:02:44.400 --> 00:02:47.320]   this desire to improve the performance
[00:02:47.320 --> 00:02:48.320]   of large language models
[00:02:48.320 --> 00:02:49.540]   under certain constraints.
[00:02:49.540 --> 00:02:52.000]   We were focusing on use cases
[00:02:52.000 --> 00:02:52.720]   where, as I said,
[00:02:52.720 --> 00:02:55.140]   tasks that all models do poorly on,
[00:02:55.140 --> 00:02:55.760]   generally speaking,
[00:02:55.760 --> 00:02:57.120]   even very, very large models.
[00:02:57.120 --> 00:02:57.720]   So, you know,
[00:02:57.720 --> 00:03:00.020]   you don't necessarily have other models
[00:03:00.020 --> 00:03:01.280]   that you can use as a judge
[00:03:01.280 --> 00:03:02.580]   or for synthetic data.
[00:03:02.580 --> 00:03:03.800]   So that's kind of how we ended up
[00:03:03.800 --> 00:03:04.980]   at Reinforcement Learning, right?
[00:03:04.980 --> 00:03:07.240]   We want just instead tasks
[00:03:07.240 --> 00:03:08.400]   that are easily verifiable,
[00:03:08.400 --> 00:03:09.360]   but that we don't necessarily
[00:03:09.360 --> 00:03:10.920]   have large ground truth data sets for
[00:03:10.920 --> 00:03:13.400]   or ways to use judge models
[00:03:13.400 --> 00:03:15.360]   or other synthetic data techniques.
[00:03:15.360 --> 00:03:17.940]   And we also stuck to
[00:03:17.940 --> 00:03:20.140]   binary reward settings, right?
[00:03:20.140 --> 00:03:21.020]   We wanted to keep the reward
[00:03:21.020 --> 00:03:21.940]   as simple as possible.
[00:03:21.940 --> 00:03:24.840]   And so we kind of ended up on tasks
[00:03:24.840 --> 00:03:27.480]   that have very simple expressions
[00:03:27.480 --> 00:03:28.000]   of reward.
[00:03:28.000 --> 00:03:31.820]   Let me see.
[00:03:31.820 --> 00:03:32.320]   Okay, right.
[00:03:32.320 --> 00:03:33.980]   So this is like, again,
[00:03:33.980 --> 00:03:34.940]   the sort of setting of
[00:03:34.940 --> 00:03:36.600]   what is it that we're trying
[00:03:36.600 --> 00:03:37.720]   to achieve with this paper?
[00:03:38.080 --> 00:03:39.620]   when you have this standard flow,
[00:03:39.620 --> 00:03:42.020]   you generate some output
[00:03:42.020 --> 00:03:42.780]   given a task.
[00:03:42.780 --> 00:03:44.600]   And if you succeed, that's great.
[00:03:44.600 --> 00:03:45.220]   But when you fail,
[00:03:45.220 --> 00:03:46.180]   you don't really have anything
[00:03:46.180 --> 00:03:47.840]   we can do to get better.
[00:03:47.840 --> 00:03:49.120]   Aside from collecting
[00:03:49.120 --> 00:03:49.900]   large amounts of data,
[00:03:49.900 --> 00:03:51.300]   generating large amounts of data
[00:03:51.300 --> 00:03:52.320]   and doing an SFT run,
[00:03:52.320 --> 00:03:53.100]   we want to think about
[00:03:53.100 --> 00:03:54.760]   how can we incrementally do better
[00:03:54.760 --> 00:03:56.780]   and have models that learn
[00:03:56.780 --> 00:03:57.460]   from their mistakes
[00:03:57.460 --> 00:03:59.640]   and are adaptive and evolve.
[00:04:01.300 --> 00:04:03.000]   So we were quite inspired
[00:04:03.000 --> 00:04:05.680]   by a 2023 work
[00:04:05.680 --> 00:04:08.120]   called Self-Refine.
[00:04:08.120 --> 00:04:10.660]   I believe it was like CMU
[00:04:10.660 --> 00:04:12.360]   and maybe the University of Washington
[00:04:12.360 --> 00:04:14.340]   or I'm probably getting that wrong,
[00:04:14.340 --> 00:04:16.240]   but a couple of organizations
[00:04:16.240 --> 00:04:17.580]   came together to do this work,
[00:04:17.580 --> 00:04:19.660]   which the premise is,
[00:04:19.660 --> 00:04:20.460]   well, so they showed,
[00:04:20.460 --> 00:04:22.260]   and this is part of a larger narrative
[00:04:22.260 --> 00:04:23.860]   around self-reflection
[00:04:23.860 --> 00:04:25.040]   with large language models,
[00:04:25.040 --> 00:04:27.800]   but they were one of the seminal papers
[00:04:27.800 --> 00:04:29.800]   that showed that if you ask a model
[00:04:29.800 --> 00:04:30.620]   to self-critique
[00:04:30.620 --> 00:04:31.740]   or to provide feedback
[00:04:31.740 --> 00:04:32.720]   on its own output
[00:04:32.720 --> 00:04:34.600]   and then use that feedback
[00:04:34.600 --> 00:04:36.900]   to refine its answer,
[00:04:36.900 --> 00:04:38.800]   you can see like up to 20%
[00:04:38.800 --> 00:04:39.620]   performance gains.
[00:04:39.620 --> 00:04:40.860]   And I think a lot of their experiments
[00:04:40.860 --> 00:04:42.340]   were on whatever chatGPT,
[00:04:42.340 --> 00:04:45.220]   whatever was powering chatGPT
[00:04:45.220 --> 00:04:45.800]   at the time.
[00:04:45.800 --> 00:04:48.260]   And so at the bottom,
[00:04:48.260 --> 00:04:50.020]   this is an illustrative example
[00:04:50.020 --> 00:04:52.340]   of how that would look in practice.
[00:04:52.340 --> 00:04:53.800]   So you have a user
[00:04:53.800 --> 00:04:54.980]   talking about table tennis.
[00:04:54.980 --> 00:04:57.060]   There's like kind of a mediocre response
[00:04:57.060 --> 00:04:59.020]   from the model,
[00:04:59.020 --> 00:05:00.900]   but then you tell the model,
[00:05:00.900 --> 00:05:02.920]   you prompt the model to say,
[00:05:02.920 --> 00:05:04.360]   what are the issues with this?
[00:05:04.360 --> 00:05:07.500]   Like what is missing?
[00:05:07.500 --> 00:05:08.840]   And it says,
[00:05:08.840 --> 00:05:10.100]   oh, there's no information
[00:05:10.100 --> 00:05:11.120]   about how to play table tennis
[00:05:11.120 --> 00:05:12.580]   and there's a lack of user understanding.
[00:05:12.580 --> 00:05:14.380]   And so then the refined response
[00:05:14.380 --> 00:05:15.720]   is qualitatively much better.
[00:05:15.720 --> 00:05:17.180]   So we're definitely inspired
[00:05:17.180 --> 00:05:18.640]   by this known property
[00:05:18.640 --> 00:05:19.540]   of large language models
[00:05:19.540 --> 00:05:20.880]   that they can to some degree,
[00:05:20.880 --> 00:05:22.380]   like provide self-feedback
[00:05:22.380 --> 00:05:23.260]   or self-critiques.
[00:05:25.160 --> 00:05:26.420]   So this is what that flow
[00:05:26.420 --> 00:05:27.040]   would look like
[00:05:27.040 --> 00:05:28.960]   if we were just prompting
[00:05:28.960 --> 00:05:30.040]   for self-reflection, right?
[00:05:30.040 --> 00:05:31.240]   So on the left side,
[00:05:31.240 --> 00:05:32.280]   if you succeed, great.
[00:05:32.280 --> 00:05:33.140]   On the right side,
[00:05:33.140 --> 00:05:34.920]   if you fail, right?
[00:05:34.920 --> 00:05:36.320]   If we detected a failure,
[00:05:36.320 --> 00:05:37.080]   as I talked about,
[00:05:37.080 --> 00:05:40.280]   with some sort of verification,
[00:05:40.280 --> 00:05:42.380]   then we could generate
[00:05:42.380 --> 00:05:43.760]   a self-reflection in this style,
[00:05:43.760 --> 00:05:44.400]   like ask the model
[00:05:44.400 --> 00:05:44.980]   to provide feedback
[00:05:44.980 --> 00:05:46.140]   and then have the model
[00:05:46.140 --> 00:05:47.360]   retry the task, right?
[00:05:47.360 --> 00:05:48.340]   And we see pretty much
[00:05:48.340 --> 00:05:49.800]   like pretty immediate
[00:05:49.800 --> 00:05:51.700]   improvements from this
[00:05:51.700 --> 00:05:52.220]   on the task
[00:05:52.220 --> 00:05:53.240]   that we're talking about, right?
[00:05:53.240 --> 00:05:54.360]   So this is cool and all.
[00:05:54.360 --> 00:05:56.640]   That's how reflection actually works.
[00:05:56.640 --> 00:05:59.360]   But then sort of comes
[00:05:59.360 --> 00:06:00.320]   in the learning aspect
[00:06:00.320 --> 00:06:01.940]   or the evolution aspect, right?
[00:06:01.940 --> 00:06:03.120]   The self-reflection prompting
[00:06:03.120 --> 00:06:03.600]   is static
[00:06:03.600 --> 00:06:05.100]   and the model doesn't learn anything.
[00:06:05.100 --> 00:06:06.060]   And so we're using
[00:06:06.060 --> 00:06:07.520]   reinforcement learning again,
[00:06:07.520 --> 00:06:09.560]   which is why the verifiable
[00:06:09.560 --> 00:06:10.320]   rewards are lovely
[00:06:10.320 --> 00:06:11.840]   to teach the model
[00:06:11.840 --> 00:06:12.860]   how to reflect better.
[00:06:12.860 --> 00:06:14.960]   And our specific formulation
[00:06:14.960 --> 00:06:18.440]   is that we specifically rewarded
[00:06:18.440 --> 00:06:19.700]   the self-reflection tokens.
[00:06:19.700 --> 00:06:20.700]   So what we were trying
[00:06:20.700 --> 00:06:21.980]   to do here with this approach
[00:06:21.980 --> 00:06:23.640]   is incentivize the model
[00:06:23.640 --> 00:06:26.340]   to learn how to self-reflect better,
[00:06:26.340 --> 00:06:27.840]   not to get the right answer.
[00:06:27.840 --> 00:06:30.060]   So there was no reward
[00:06:30.060 --> 00:06:31.460]   for the answer tokens.
[00:06:31.460 --> 00:06:32.460]   There was only reward
[00:06:32.460 --> 00:06:33.640]   for the self-reflection tokens.
[00:06:33.640 --> 00:06:37.660]   And so the flow on the right now
[00:06:37.660 --> 00:06:38.700]   is when we fail,
[00:06:38.700 --> 00:06:39.920]   we generate a self-reflection,
[00:06:39.920 --> 00:06:40.980]   we retry the task.
[00:06:40.980 --> 00:06:41.840]   And then if we are
[00:06:41.840 --> 00:06:43.180]   on that specific fail,
[00:06:43.180 --> 00:06:44.940]   retry success path,
[00:06:44.940 --> 00:06:45.940]   just that pathway
[00:06:45.940 --> 00:06:47.080]   where a self-reflection
[00:06:47.080 --> 00:06:50.420]   led to success,
[00:06:50.420 --> 00:06:51.360]   then we reward
[00:06:51.360 --> 00:06:52.360]   the self-reflection tokens
[00:06:52.360 --> 00:06:53.020]   because that means
[00:06:53.020 --> 00:06:53.880]   that is a high-quality
[00:06:53.880 --> 00:06:54.480]   self-reflection
[00:06:54.480 --> 00:06:56.020]   that led to a success
[00:06:56.020 --> 00:06:56.720]   when there was previously
[00:06:56.720 --> 00:06:57.060]   a failure.
[00:06:57.060 --> 00:07:00.240]   Any questions at this point?
[00:07:00.240 --> 00:07:01.560]   I don't know
[00:07:01.560 --> 00:07:01.980]   if there's somewhere
[00:07:01.980 --> 00:07:02.600]   I can see.
[00:07:02.600 --> 00:07:07.860]   There's...
[00:07:07.860 --> 00:07:08.820]   No, no, no,
[00:07:08.820 --> 00:07:10.240]   not specifically.
[00:07:10.240 --> 00:07:10.580]   Okay, perfect.
[00:07:10.580 --> 00:07:11.480]   Okay, okay.
[00:07:11.480 --> 00:07:12.560]   I'll keep moving.
[00:07:12.560 --> 00:07:13.500]   Okay, awesome.
[00:07:13.600 --> 00:07:14.220]   I'll just keep moving
[00:07:14.220 --> 00:07:15.020]   and Sam,
[00:07:15.020 --> 00:07:15.940]   if you want to jump in
[00:07:15.940 --> 00:07:16.260]   on the chat,
[00:07:16.260 --> 00:07:16.680]   you're great.
[00:07:16.680 --> 00:07:18.280]   Awesome, okay.
[00:07:18.280 --> 00:07:19.080]   So that's like
[00:07:19.080 --> 00:07:20.100]   the basic formulation, right?
[00:07:20.100 --> 00:07:20.660]   It's a modification
[00:07:20.660 --> 00:07:22.360]   to a sort of standard
[00:07:22.360 --> 00:07:24.820]   reinforcement learning approach
[00:07:24.820 --> 00:07:25.640]   with this extra
[00:07:25.640 --> 00:07:27.060]   self-reflection
[00:07:27.060 --> 00:07:27.660]   and rewarding
[00:07:27.660 --> 00:07:28.540]   self-reflection step.
[00:07:28.540 --> 00:07:29.400]   And so we're incentivizing
[00:07:29.400 --> 00:07:29.740]   the model
[00:07:29.740 --> 00:07:30.600]   to self-reflection better.
[00:07:30.600 --> 00:07:32.840]   Okay, so in the paper itself,
[00:07:32.840 --> 00:07:33.980]   we focused on two tasks
[00:07:33.980 --> 00:07:34.860]   that again fit
[00:07:34.860 --> 00:07:35.660]   that description
[00:07:35.660 --> 00:07:36.580]   that I've spoken about
[00:07:36.580 --> 00:07:37.020]   previously,
[00:07:37.020 --> 00:07:37.600]   two things,
[00:07:37.600 --> 00:07:38.780]   one, verifiable reward
[00:07:38.780 --> 00:07:40.020]   into like tasks
[00:07:40.020 --> 00:07:40.640]   that you would think
[00:07:40.640 --> 00:07:41.520]   should be in distribution
[00:07:41.520 --> 00:07:42.140]   but aren't.
[00:07:42.140 --> 00:07:42.960]   So the first one
[00:07:42.960 --> 00:07:43.660]   is function calling.
[00:07:43.660 --> 00:07:46.760]   We use the API gen,
[00:07:46.760 --> 00:07:47.420]   like the Salesforce
[00:07:47.420 --> 00:07:48.560]   function calling data set.
[00:07:48.560 --> 00:07:49.820]   It's about 60,000 data points
[00:07:49.820 --> 00:07:51.020]   that came out
[00:07:51.020 --> 00:07:51.820]   mid last year
[00:07:51.820 --> 00:07:52.520]   and that everyone
[00:07:52.520 --> 00:07:53.160]   has been using.
[00:07:53.160 --> 00:07:55.920]   And then we also use
[00:07:55.920 --> 00:07:57.200]   this task called countdown,
[00:07:57.200 --> 00:07:58.240]   which I actually
[00:07:58.240 --> 00:07:59.680]   recently learned
[00:07:59.680 --> 00:08:01.120]   is based on
[00:08:01.120 --> 00:08:02.020]   a British game show,
[00:08:02.020 --> 00:08:02.880]   which I didn't realize.
[00:08:02.880 --> 00:08:03.260]   I didn't know
[00:08:03.260 --> 00:08:04.080]   where that name came from.
[00:08:04.120 --> 00:08:05.520]   but this is a task
[00:08:05.520 --> 00:08:07.340]   that got pretty popular
[00:08:07.340 --> 00:08:08.400]   in January of this year
[00:08:08.400 --> 00:08:08.920]   because there was
[00:08:08.920 --> 00:08:09.500]   like a project
[00:08:09.500 --> 00:08:10.000]   that showed
[00:08:10.000 --> 00:08:11.480]   that RL can like
[00:08:11.480 --> 00:08:13.340]   dramatically improve
[00:08:13.340 --> 00:08:14.280]   ability to do
[00:08:14.280 --> 00:08:15.140]   this particular task.
[00:08:15.140 --> 00:08:15.480]   And it was like
[00:08:15.480 --> 00:08:16.360]   a GRPO experiment.
[00:08:16.360 --> 00:08:18.120]   But models are
[00:08:18.120 --> 00:08:18.920]   surprisingly bad
[00:08:18.920 --> 00:08:19.580]   at this task.
[00:08:19.580 --> 00:08:20.180]   The formulation
[00:08:20.180 --> 00:08:22.220]   is that you get
[00:08:22.220 --> 00:08:23.420]   a list of numbers
[00:08:23.420 --> 00:08:24.240]   between three
[00:08:24.240 --> 00:08:24.880]   to four numbers
[00:08:24.880 --> 00:08:25.820]   and you have to create
[00:08:25.820 --> 00:08:26.260]   an equation
[00:08:26.260 --> 00:08:26.720]   that equals
[00:08:26.720 --> 00:08:27.760]   some other target number
[00:08:27.760 --> 00:08:29.160]   and you can only use
[00:08:29.160 --> 00:08:30.420]   like basic arithmetic
[00:08:30.420 --> 00:08:31.460]   operations plus
[00:08:31.460 --> 00:08:32.780]   minus times divide
[00:08:32.780 --> 00:08:34.100]   and you can only use
[00:08:34.100 --> 00:08:34.780]   each number once.
[00:08:34.780 --> 00:08:36.780]   And so all of the questions
[00:08:36.780 --> 00:08:37.560]   are of this format
[00:08:37.560 --> 00:08:38.380]   where like the numbers
[00:08:38.380 --> 00:08:38.880]   are different,
[00:08:38.880 --> 00:08:39.600]   but like it's always
[00:08:39.600 --> 00:08:40.620]   just create this equation.
[00:08:40.620 --> 00:08:43.200]   It's actually really hard.
[00:08:43.200 --> 00:08:43.780]   Yeah.
[00:08:43.780 --> 00:08:45.840]   Again, there's a British game show
[00:08:45.840 --> 00:08:46.940]   where the entire premise
[00:08:46.940 --> 00:08:47.800]   is that people go
[00:08:47.800 --> 00:08:49.020]   and try to do this quickly
[00:08:49.020 --> 00:08:50.100]   and it's very difficult.
[00:08:50.100 --> 00:08:52.460]   So these are the two tasks
[00:08:52.460 --> 00:08:53.380]   that we experimented on
[00:08:53.380 --> 00:08:54.080]   because again,
[00:08:54.080 --> 00:08:55.100]   like large models
[00:08:55.100 --> 00:08:55.720]   are surprisingly
[00:08:55.720 --> 00:08:57.980]   not as high accuracy
[00:08:57.980 --> 00:08:58.660]   on these tasks
[00:08:58.660 --> 00:08:59.280]   as you would think.
[00:09:02.720 --> 00:09:04.340]   So we did very standard
[00:09:04.340 --> 00:09:05.600]   GRPO training for this,
[00:09:05.600 --> 00:09:05.900]   of course,
[00:09:05.900 --> 00:09:06.640]   with that modification
[00:09:06.640 --> 00:09:07.560]   of self-reflection
[00:09:07.560 --> 00:09:08.980]   and I'm excited to see
[00:09:08.980 --> 00:09:10.520]   like when we get
[00:09:10.520 --> 00:09:11.460]   to the end of this,
[00:09:11.460 --> 00:09:12.600]   like what people think
[00:09:12.600 --> 00:09:13.480]   about, you know,
[00:09:13.480 --> 00:09:15.000]   all of the recent advancements
[00:09:15.000 --> 00:09:16.600]   and all of the building
[00:09:16.600 --> 00:09:17.340]   upon GRPO
[00:09:17.340 --> 00:09:19.060]   and I have a couple papers
[00:09:19.060 --> 00:09:20.320]   that I want to talk about too.
[00:09:20.320 --> 00:09:23.620]   But throughout our training process,
[00:09:23.620 --> 00:09:24.420]   when we started,
[00:09:24.420 --> 00:09:26.360]   we saw qualitatively
[00:09:26.360 --> 00:09:27.400]   that these self-reflections
[00:09:27.400 --> 00:09:28.720]   were very specific, right?
[00:09:28.720 --> 00:09:29.560]   They're very long,
[00:09:29.680 --> 00:09:31.400]   they're very like verbose
[00:09:31.400 --> 00:09:33.120]   and they kind of repeat
[00:09:33.120 --> 00:09:33.580]   the same thing
[00:09:33.580 --> 00:09:34.160]   over and over again
[00:09:34.160 --> 00:09:35.280]   and they're very like specific
[00:09:35.280 --> 00:09:36.720]   to a specific problem, right?
[00:09:36.720 --> 00:09:38.040]   And then we saw
[00:09:38.040 --> 00:09:38.980]   this really cool thing happen
[00:09:38.980 --> 00:09:39.660]   where as you get
[00:09:39.660 --> 00:09:40.700]   to like a thousand steps,
[00:09:40.700 --> 00:09:41.680]   you start to see
[00:09:41.680 --> 00:09:42.420]   these much shorter,
[00:09:42.420 --> 00:09:45.460]   clearer and general examples
[00:09:45.460 --> 00:09:47.400]   or these better,
[00:09:47.400 --> 00:09:48.480]   like they're just higher quality
[00:09:48.480 --> 00:09:49.320]   self-reflections, right?
[00:09:49.320 --> 00:09:50.120]   They're very specific,
[00:09:50.120 --> 00:09:51.380]   they're very effective,
[00:09:51.380 --> 00:09:52.500]   they're very short.
[00:09:52.500 --> 00:09:53.620]   So it was a cool
[00:09:53.620 --> 00:09:54.340]   qualitative result.
[00:09:54.340 --> 00:09:55.120]   Like we can see
[00:09:55.120 --> 00:09:55.680]   that the models
[00:09:55.680 --> 00:09:57.160]   are sort of becoming
[00:09:57.160 --> 00:09:57.580]   quote unquote
[00:09:57.580 --> 00:09:58.800]   better at self-reflection
[00:09:58.800 --> 00:09:59.660]   in our opinion.
[00:09:59.660 --> 00:10:02.120]   Okay.
[00:10:02.120 --> 00:10:03.480]   And then I'll talk
[00:10:03.480 --> 00:10:05.000]   through actual results.
[00:10:05.000 --> 00:10:06.920]   So this is on
[00:10:06.920 --> 00:10:07.880]   the function calling task.
[00:10:07.880 --> 00:10:08.840]   So it's a pretty standard
[00:10:08.840 --> 00:10:09.820]   function calling task, right?
[00:10:09.820 --> 00:10:10.760]   You provide some tools,
[00:10:10.760 --> 00:10:12.840]   you have a user query
[00:10:12.840 --> 00:10:13.840]   and then the model
[00:10:13.840 --> 00:10:15.080]   has to pick a tool
[00:10:15.080 --> 00:10:17.280]   and provide the right parameters
[00:10:17.280 --> 00:10:17.760]   to that tool.
[00:10:17.760 --> 00:10:18.380]   So pretty standard
[00:10:18.380 --> 00:10:18.960]   function calling.
[00:10:18.960 --> 00:10:19.840]   And we stuck
[00:10:19.840 --> 00:10:20.840]   to pretty small models,
[00:10:20.840 --> 00:10:21.720]   particularly since
[00:10:21.720 --> 00:10:22.600]   we started this research
[00:10:22.600 --> 00:10:23.960]   around March
[00:10:23.960 --> 00:10:24.680]   when, you know,
[00:10:24.680 --> 00:10:26.020]   most of the GRPO
[00:10:26.020 --> 00:10:27.640]   research was being done
[00:10:27.640 --> 00:10:28.720]   on like under 10 billion
[00:10:28.720 --> 00:10:29.540]   parameter models.
[00:10:29.540 --> 00:10:31.660]   So the top half
[00:10:31.660 --> 00:10:32.300]   of this table
[00:10:32.300 --> 00:10:34.920]   is the models
[00:10:34.920 --> 00:10:35.840]   that we actually trained.
[00:10:35.840 --> 00:10:37.640]   We stuck to like
[00:10:37.640 --> 00:10:38.640]   a diverse set
[00:10:38.640 --> 00:10:39.760]   of open source models
[00:10:39.760 --> 00:10:41.680]   between one and a half
[00:10:41.680 --> 00:10:42.420]   billion and eight billion
[00:10:42.420 --> 00:10:42.840]   parameters.
[00:10:42.840 --> 00:10:44.720]   And just to walk you
[00:10:44.720 --> 00:10:45.100]   through what the
[00:10:45.100 --> 00:10:46.520]   different columns here mean,
[00:10:46.520 --> 00:10:48.020]   vanilla first try
[00:10:48.020 --> 00:10:49.120]   is like pass at one
[00:10:49.120 --> 00:10:50.200]   on function column,
[00:10:50.200 --> 00:10:50.460]   right?
[00:10:50.460 --> 00:10:51.180]   So you can see
[00:10:51.180 --> 00:10:52.380]   surprisingly low
[00:10:52.380 --> 00:10:53.140]   and even with like
[00:10:53.140 --> 00:10:53.960]   the very big models
[00:10:53.960 --> 00:10:54.620]   towards the bottom,
[00:10:54.620 --> 00:10:56.760]   72 billion
[00:10:56.760 --> 00:10:57.360]   or 70 billion
[00:10:57.360 --> 00:10:57.940]   parameter models,
[00:10:57.940 --> 00:10:58.880]   like they're only
[00:10:58.880 --> 00:10:59.940]   around like 70%
[00:10:59.940 --> 00:11:00.680]   accuracy, right?
[00:11:00.680 --> 00:11:02.660]   And then the second
[00:11:02.660 --> 00:11:03.140]   column,
[00:11:03.140 --> 00:11:04.040]   plus reflection,
[00:11:04.040 --> 00:11:04.720]   second try
[00:11:04.720 --> 00:11:05.300]   is with that
[00:11:05.300 --> 00:11:06.020]   prompting technique,
[00:11:06.020 --> 00:11:06.300]   right?
[00:11:06.300 --> 00:11:07.220]   So how much of a raise
[00:11:07.220 --> 00:11:08.000]   do you get just by
[00:11:08.000 --> 00:11:09.340]   asking for self-reflection
[00:11:09.340 --> 00:11:10.440]   and then giving the models
[00:11:10.440 --> 00:11:11.060]   a second try
[00:11:11.060 --> 00:11:11.840]   at the task, right?
[00:11:11.840 --> 00:11:13.480]   And then for the top
[00:11:13.480 --> 00:11:14.480]   half of these models,
[00:11:14.480 --> 00:11:15.180]   the smaller models,
[00:11:15.180 --> 00:11:15.880]   we trained them
[00:11:15.880 --> 00:11:18.560]   with this approach
[00:11:18.560 --> 00:11:19.060]   I've been talking
[00:11:19.060 --> 00:11:20.020]   about of incentivizing
[00:11:20.020 --> 00:11:21.280]   better self-reflection.
[00:11:21.600 --> 00:11:22.500]   and we see
[00:11:22.500 --> 00:11:23.380]   pretty immediate
[00:11:23.380 --> 00:11:26.620]   performance gains
[00:11:26.620 --> 00:11:27.580]   both on
[00:11:27.580 --> 00:11:29.340]   pass at one,
[00:11:29.340 --> 00:11:30.520]   so trained first try
[00:11:30.520 --> 00:11:31.220]   and then also
[00:11:31.220 --> 00:11:32.200]   if you give it
[00:11:32.200 --> 00:11:32.700]   a prompt
[00:11:32.700 --> 00:11:33.640]   and a retry,
[00:11:33.640 --> 00:11:35.560]   you see even more
[00:11:35.560 --> 00:11:37.400]   more of a performance
[00:11:37.400 --> 00:11:37.700]   jump.
[00:11:37.700 --> 00:11:39.720]   So the bottom half
[00:11:39.720 --> 00:11:40.160]   of the table
[00:11:40.160 --> 00:11:42.000]   is our
[00:11:42.000 --> 00:11:43.320]   sort of baseline.
[00:11:43.320 --> 00:11:44.420]   The way that we
[00:11:44.420 --> 00:11:44.920]   wanted to think
[00:11:44.920 --> 00:11:45.360]   about this
[00:11:45.360 --> 00:11:46.700]   was how effective
[00:11:46.700 --> 00:11:48.340]   can we be
[00:11:48.340 --> 00:11:48.900]   at bringing
[00:11:48.900 --> 00:11:49.540]   small models
[00:11:49.540 --> 00:11:50.320]   up to the performance
[00:11:50.320 --> 00:11:51.020]   of large models?
[00:11:51.020 --> 00:11:53.060]   And one thing
[00:11:53.060 --> 00:11:53.500]   I want to highlight
[00:11:53.500 --> 00:11:54.260]   here is if you look
[00:11:54.260 --> 00:11:54.880]   at, for example,
[00:11:54.880 --> 00:11:56.240]   the Quen 2 7 billion
[00:11:56.240 --> 00:11:57.020]   instruct rows,
[00:11:57.020 --> 00:11:57.640]   so that's the second
[00:11:57.640 --> 00:11:58.340]   row of the table,
[00:11:58.340 --> 00:12:00.560]   you see the model
[00:12:00.560 --> 00:12:01.620]   vanilla first try
[00:12:01.620 --> 00:12:02.940]   you're at 66.4%
[00:12:02.940 --> 00:12:04.140]   and then with the training
[00:12:04.140 --> 00:12:05.020]   and then with giving
[00:12:05.020 --> 00:12:05.640]   you a second try
[00:12:05.640 --> 00:12:06.180]   you bring it all
[00:12:06.180 --> 00:12:06.680]   the way up
[00:12:06.680 --> 00:12:07.800]   past 77%.
[00:12:07.800 --> 00:12:09.040]   And if you compare
[00:12:09.040 --> 00:12:10.980]   this to plus reflection
[00:12:10.980 --> 00:12:11.720]   second try
[00:12:11.720 --> 00:12:12.740]   of the large models,
[00:12:12.740 --> 00:12:14.380]   the Quen 2 72 billion
[00:12:14.380 --> 00:12:16.300]   and the Llama 70 billion,
[00:12:16.300 --> 00:12:17.860]   like that 77%
[00:12:17.860 --> 00:12:18.800]   is actually higher
[00:12:18.800 --> 00:12:20.160]   than when you
[00:12:20.160 --> 00:12:21.480]   prompt Quen and Llama
[00:12:21.480 --> 00:12:22.500]   very large models
[00:12:22.500 --> 00:12:23.340]   for self-reflection,
[00:12:23.340 --> 00:12:23.580]   right?
[00:12:23.580 --> 00:12:24.100]   So what we're showing
[00:12:24.100 --> 00:12:24.800]   is that you can use
[00:12:24.800 --> 00:12:25.420]   this training recipe
[00:12:25.420 --> 00:12:26.340]   to bring very small
[00:12:26.340 --> 00:12:27.740]   models past the
[00:12:27.740 --> 00:12:28.580]   performance of models
[00:12:28.580 --> 00:12:29.520]   10 times their size
[00:12:29.520 --> 00:12:30.900]   just by like using
[00:12:30.900 --> 00:12:31.560]   a particular training
[00:12:31.560 --> 00:12:32.780]   recipe, which is
[00:12:32.780 --> 00:12:33.340]   really cool, right?
[00:12:33.340 --> 00:12:34.080]   Because I think that
[00:12:34.080 --> 00:12:35.940]   there has been a
[00:12:35.940 --> 00:12:37.760]   movement that I
[00:12:37.760 --> 00:12:38.580]   really believe in
[00:12:38.580 --> 00:12:39.300]   about like, you know,
[00:12:39.300 --> 00:12:39.940]   the power of small
[00:12:39.940 --> 00:12:40.660]   models and why
[00:12:40.660 --> 00:12:41.640]   personalization can be
[00:12:41.640 --> 00:12:42.220]   really powerful.
[00:12:42.220 --> 00:12:43.520]   And I think this idea
[00:12:43.520 --> 00:12:44.280]   that you can bring
[00:12:44.280 --> 00:12:45.100]   a small model up
[00:12:45.100 --> 00:12:45.960]   past the performance
[00:12:45.960 --> 00:12:47.620]   of a very general
[00:12:47.620 --> 00:12:48.980]   out-of-the-box model
[00:12:48.980 --> 00:12:50.100]   is a cool thing
[00:12:50.100 --> 00:12:50.480]   to see.
[00:12:50.480 --> 00:12:51.640]   Cool.
[00:12:51.640 --> 00:12:52.240]   So this is the
[00:12:52.240 --> 00:12:52.800]   function coin.
[00:12:52.800 --> 00:12:53.760]   Oh, sorry.
[00:12:53.760 --> 00:12:54.280]   Do you mind if I ask
[00:12:54.280 --> 00:12:54.520]   a question?
[00:12:54.520 --> 00:12:55.320]   Yeah, I noticed
[00:12:55.320 --> 00:12:56.000]   that you had this
[00:12:56.000 --> 00:12:57.340]   sentence in your
[00:12:57.340 --> 00:12:58.680]   write-up as well
[00:12:58.680 --> 00:12:59.600]   that small models
[00:12:59.600 --> 00:13:00.480]   can outperform
[00:13:00.480 --> 00:13:00.980]   larger models.
[00:13:00.980 --> 00:13:02.640]   I guess I just
[00:13:02.640 --> 00:13:03.060]   want to clarify.
[00:13:03.060 --> 00:13:03.760]   This means that
[00:13:03.760 --> 00:13:04.360]   what you're saying
[00:13:04.360 --> 00:13:05.120]   is that small models
[00:13:05.120 --> 00:13:05.620]   can outperform
[00:13:05.620 --> 00:13:06.360]   larger models.
[00:13:06.360 --> 00:13:08.660]   Small models
[00:13:08.660 --> 00:13:10.780]   that are fine-tuned
[00:13:10.780 --> 00:13:11.360]   can outperform
[00:13:11.360 --> 00:13:12.000]   larger models
[00:13:12.000 --> 00:13:12.720]   without fine-tuning.
[00:13:13.500 --> 00:13:13.940]   exactly.
[00:13:13.940 --> 00:13:14.840]   Thank you.
[00:13:14.840 --> 00:13:18.720]   Cool.
[00:13:18.720 --> 00:13:19.820]   And then these
[00:13:19.820 --> 00:13:20.440]   are the countdown
[00:13:20.440 --> 00:13:20.820]   results.
[00:13:20.820 --> 00:13:21.700]   So that second sort
[00:13:21.700 --> 00:13:22.320]   of math equation
[00:13:22.320 --> 00:13:23.080]   writing task.
[00:13:23.080 --> 00:13:24.500]   There's a few more
[00:13:24.500 --> 00:13:25.400]   models on here,
[00:13:25.400 --> 00:13:26.060]   just a slightly
[00:13:26.060 --> 00:13:26.640]   different set.
[00:13:26.640 --> 00:13:28.000]   One thing that we
[00:13:28.000 --> 00:13:29.220]   did for sort of
[00:13:29.220 --> 00:13:29.980]   academic integrity
[00:13:29.980 --> 00:13:31.260]   is like, based on
[00:13:31.260 --> 00:13:32.880]   when the data set
[00:13:32.880 --> 00:13:33.700]   was released,
[00:13:33.700 --> 00:13:34.620]   we only chose
[00:13:34.620 --> 00:13:35.280]   models to train
[00:13:35.280 --> 00:13:35.880]   that were released
[00:13:35.880 --> 00:13:36.700]   prior to that data
[00:13:36.700 --> 00:13:38.000]   set being released
[00:13:38.000 --> 00:13:38.660]   because obviously
[00:13:38.660 --> 00:13:39.940]   with models like
[00:13:39.940 --> 00:13:40.340]   Quan and LLAMA,
[00:13:40.340 --> 00:13:40.920]   you don't know
[00:13:40.920 --> 00:13:41.500]   exactly what they're
[00:13:41.500 --> 00:13:42.000]   trained upon.
[00:13:42.000 --> 00:13:43.560]   And so we wanted
[00:13:43.560 --> 00:13:44.080]   to make sure that
[00:13:44.080 --> 00:13:45.240]   this data wasn't
[00:13:45.240 --> 00:13:47.560]   already in their
[00:13:47.560 --> 00:13:49.780]   training data, right?
[00:13:49.780 --> 00:13:50.600]   Because that would
[00:13:50.600 --> 00:13:51.500]   definitely see results.
[00:13:51.500 --> 00:13:52.420]   So that's why
[00:13:52.420 --> 00:13:53.200]   there's a slightly
[00:13:53.200 --> 00:13:54.280]   different model set
[00:13:54.280 --> 00:13:55.000]   here and then also
[00:13:55.000 --> 00:13:56.580]   some older models,
[00:13:56.580 --> 00:13:56.860]   right?
[00:13:56.860 --> 00:14:00.860]   But we, again,
[00:14:00.860 --> 00:14:01.620]   see some really
[00:14:01.620 --> 00:14:02.300]   similar results.
[00:14:02.300 --> 00:14:03.080]   You can see the
[00:14:03.080 --> 00:14:03.760]   prompting technique
[00:14:03.760 --> 00:14:04.480]   helps and then the
[00:14:04.480 --> 00:14:05.080]   training helps.
[00:14:05.080 --> 00:14:06.960]   And again, you can
[00:14:06.960 --> 00:14:08.820]   bring Quan 2.57
[00:14:08.820 --> 00:14:11.140]   billion with the
[00:14:11.140 --> 00:14:11.780]   training, with the
[00:14:11.780 --> 00:14:12.920]   reflection up to
[00:14:12.920 --> 00:14:14.620]   over 50%, which
[00:14:14.620 --> 00:14:15.900]   again, like handily
[00:14:15.900 --> 00:14:17.000]   beats Quan 72
[00:14:17.000 --> 00:14:19.060]   billion and gets
[00:14:19.060 --> 00:14:20.260]   close, like, yeah,
[00:14:20.260 --> 00:14:21.380]   it just gets close
[00:14:21.380 --> 00:14:21.960]   to the performance
[00:14:21.960 --> 00:14:24.300]   of home RRX4,
[00:14:24.300 --> 00:14:24.780]   which is cool.
[00:14:24.780 --> 00:14:26.280]   Awesome.
[00:14:26.280 --> 00:14:29.260]   So another thing
[00:14:29.260 --> 00:14:29.860]   we wanted to look
[00:14:29.860 --> 00:14:30.700]   at as sort of a
[00:14:30.700 --> 00:14:31.620]   side effect or a
[00:14:31.620 --> 00:14:32.860]   desirable property
[00:14:32.860 --> 00:14:33.880]   was catastrophic
[00:14:33.880 --> 00:14:34.800]   forgetting and
[00:14:34.800 --> 00:14:36.460]   investigating how much
[00:14:36.460 --> 00:14:37.300]   we saw and we
[00:14:37.300 --> 00:14:37.900]   luckily saw very
[00:14:37.900 --> 00:14:38.220]   little.
[00:14:38.220 --> 00:14:40.260]   This is a lot of
[00:14:40.260 --> 00:14:41.000]   numbers and I'm not
[00:14:41.000 --> 00:14:41.340]   going to walk
[00:14:41.340 --> 00:14:41.760]   through them too
[00:14:41.760 --> 00:14:43.060]   much, but just at a
[00:14:43.060 --> 00:14:43.780]   high level, like we
[00:14:43.780 --> 00:14:44.700]   were seeing that there
[00:14:44.700 --> 00:14:45.520]   was not much of a
[00:14:45.520 --> 00:14:46.240]   performance drop,
[00:14:46.240 --> 00:14:47.060]   particularly around a
[00:14:47.060 --> 00:14:47.900]   statistically significant
[00:14:47.900 --> 00:14:48.920]   one across a wide
[00:14:48.920 --> 00:14:50.500]   variety of tasks, even
[00:14:50.500 --> 00:14:50.920]   though you're doing
[00:14:50.920 --> 00:14:51.440]   this approach to
[00:14:51.440 --> 00:14:51.820]   fine-tuning.
[00:14:51.820 --> 00:14:53.180]   And we sort of credit
[00:14:53.180 --> 00:14:54.080]   that a little bit to
[00:14:54.080 --> 00:14:55.660]   how generally we're
[00:14:55.660 --> 00:14:56.320]   fine-tuning, right?
[00:14:56.320 --> 00:14:56.960]   Because again, we're
[00:14:56.960 --> 00:14:58.320]   incentivizing self-reflection
[00:14:58.320 --> 00:14:59.560]   and reasoning in general
[00:14:59.560 --> 00:15:00.820]   as opposed to
[00:15:00.820 --> 00:15:03.180]   specific answers on a
[00:15:03.180 --> 00:15:03.880]   specific task.
[00:15:04.000 --> 00:15:04.500]   we were seeing
[00:15:04.500 --> 00:15:05.140]   relatively low
[00:15:05.140 --> 00:15:05.940]   catastrophic forgetting,
[00:15:05.940 --> 00:15:06.660]   which is cool and
[00:15:06.660 --> 00:15:07.440]   kind of evidence is
[00:15:07.440 --> 00:15:09.860]   that you can train
[00:15:09.860 --> 00:15:10.900]   models in this way and
[00:15:10.900 --> 00:15:11.820]   still use them for
[00:15:11.820 --> 00:15:12.320]   lots of different
[00:15:12.320 --> 00:15:12.740]   things.
[00:15:12.740 --> 00:15:16.080]   Cool.
[00:15:16.080 --> 00:15:17.800]   This is my last slide.
[00:15:17.800 --> 00:15:18.460]   I guess I moved to
[00:15:18.460 --> 00:15:19.180]   this actually quite
[00:15:19.180 --> 00:15:19.440]   quickly.
[00:15:19.440 --> 00:15:21.700]   But I just, I think
[00:15:21.700 --> 00:15:24.280]   in, you know, as I
[00:15:24.280 --> 00:15:25.000]   mentioned in May and
[00:15:25.000 --> 00:15:25.980]   June, there has been a
[00:15:25.980 --> 00:15:26.820]   lot of work that has
[00:15:26.820 --> 00:15:28.660]   been released on GRPO
[00:15:28.660 --> 00:15:29.640]   and adjacent methods
[00:15:29.640 --> 00:15:32.000]   and has kind of given
[00:15:32.000 --> 00:15:33.180]   us a lens for like
[00:15:33.180 --> 00:15:34.840]   where collectively we
[00:15:34.840 --> 00:15:35.540]   should go from here.
[00:15:35.540 --> 00:15:36.200]   And I just wanted to
[00:15:36.200 --> 00:15:37.200]   highlight some papers that
[00:15:37.200 --> 00:15:37.980]   came out in the last
[00:15:37.980 --> 00:15:38.940]   couple months since this
[00:15:38.940 --> 00:15:39.860]   paper was released that I
[00:15:39.860 --> 00:15:40.320]   thought were really
[00:15:40.320 --> 00:15:41.100]   interesting and cool.
[00:15:41.100 --> 00:15:44.880]   So the first Spurious
[00:15:44.880 --> 00:15:46.940]   Rewards is, I believe,
[00:15:46.940 --> 00:15:47.980]   Allen AI.
[00:15:49.380 --> 00:15:50.680]   and they showed that
[00:15:50.680 --> 00:15:51.380]   for Quen models
[00:15:51.380 --> 00:15:54.800]   specifically, when you
[00:15:54.800 --> 00:15:56.180]   do RL training, it's
[00:15:56.180 --> 00:15:57.540]   like the reward function
[00:15:57.540 --> 00:15:58.960]   doesn't necessarily need
[00:15:58.960 --> 00:15:59.800]   to be correlated for the
[00:15:59.800 --> 00:16:00.740]   right answer in order to
[00:16:00.740 --> 00:16:01.740]   get strong mathematical
[00:16:01.740 --> 00:16:02.160]   reasoning.
[00:16:02.160 --> 00:16:03.500]   And this was kind of
[00:16:03.500 --> 00:16:04.500]   speaking, I think, to a
[00:16:04.500 --> 00:16:07.820]   general sort of potential
[00:16:07.820 --> 00:16:08.800]   under-training of Quen
[00:16:08.800 --> 00:16:10.420]   models and then also to
[00:16:10.420 --> 00:16:12.960]   this idea that we need to
[00:16:12.960 --> 00:16:13.900]   be investigating more
[00:16:13.900 --> 00:16:16.160]   carefully, like what it
[00:16:16.160 --> 00:16:17.340]   is or how we're
[00:16:17.340 --> 00:16:17.920]   surfacing.
[00:16:17.920 --> 00:16:20.320]   aspects of model
[00:16:20.320 --> 00:16:21.460]   performance through RL.
[00:16:21.460 --> 00:16:22.420]   Like what, like I think
[00:16:22.420 --> 00:16:23.080]   it really speaks to like
[00:16:23.080 --> 00:16:24.000]   what is actually going on
[00:16:24.000 --> 00:16:24.740]   here because there's this
[00:16:24.740 --> 00:16:27.340]   one, you know, research
[00:16:27.340 --> 00:16:28.420]   angle, which is like let's
[00:16:28.420 --> 00:16:29.340]   design really good reward
[00:16:29.340 --> 00:16:30.200]   functions and they kind of
[00:16:30.200 --> 00:16:31.280]   showed like you actually
[00:16:31.280 --> 00:16:32.160]   maybe don't need to
[00:16:32.160 --> 00:16:33.080]   design really good reward
[00:16:33.080 --> 00:16:34.620]   functions, especially for
[00:16:34.620 --> 00:16:35.120]   certain models.
[00:16:35.120 --> 00:16:36.440]   And it's worth noting that
[00:16:36.440 --> 00:16:37.540]   these results primarily
[00:16:37.540 --> 00:16:38.440]   held for Quen and didn't
[00:16:38.440 --> 00:16:39.400]   really hold for Lama and
[00:16:39.400 --> 00:16:39.860]   other models.
[00:16:39.860 --> 00:16:41.180]   So, you know, potentially
[00:16:41.180 --> 00:16:41.900]   it's something specific
[00:16:41.900 --> 00:16:43.140]   that Quen is doing, but
[00:16:43.140 --> 00:16:44.480]   yeah, it's cool paper.
[00:16:44.480 --> 00:16:47.040]   The next one.
[00:16:47.040 --> 00:16:48.460]   I thought was really
[00:16:48.460 --> 00:16:48.960]   interesting.
[00:16:48.960 --> 00:16:51.760]   The next two are basically
[00:16:51.760 --> 00:16:52.740]   about alternate
[00:16:52.740 --> 00:16:54.100]   approaches to reward.
[00:16:54.100 --> 00:16:57.960]   And the second one is
[00:16:57.960 --> 00:16:59.140]   about using self-certainty
[00:16:59.140 --> 00:16:59.920]   as a reward signal.
[00:16:59.920 --> 00:17:05.260]   So you can basically get
[00:17:05.260 --> 00:17:07.420]   rid of verifiers by simply
[00:17:07.420 --> 00:17:08.760]   just using, and this is
[00:17:08.760 --> 00:17:09.920]   like very related to what
[00:17:09.920 --> 00:17:10.460]   we did, right?
[00:17:10.460 --> 00:17:11.360]   So I thought it was really
[00:17:11.360 --> 00:17:11.820]   cool paper.
[00:17:12.020 --> 00:17:13.340]   And then the last paper
[00:17:13.340 --> 00:17:15.380]   is using RLU directly
[00:17:15.380 --> 00:17:16.420]   maximize the probability
[00:17:16.420 --> 00:17:17.420]   of generating a reference
[00:17:17.420 --> 00:17:17.700]   answer.
[00:17:17.700 --> 00:17:18.520]   So again, they have a
[00:17:18.520 --> 00:17:19.860]   technique where they're
[00:17:19.860 --> 00:17:21.040]   not using a verifier at
[00:17:21.040 --> 00:17:21.300]   all.
[00:17:21.300 --> 00:17:22.440]   And so I think it's
[00:17:22.440 --> 00:17:23.500]   showcasing that like we
[00:17:23.500 --> 00:17:25.200]   can build upon GRPO where
[00:17:25.200 --> 00:17:26.220]   we got to cut out a model
[00:17:26.220 --> 00:17:27.360]   and like cut out even more
[00:17:27.360 --> 00:17:28.360]   models and cut out even
[00:17:28.360 --> 00:17:29.520]   more like sort of
[00:17:29.520 --> 00:17:30.760]   verification steps.
[00:17:30.760 --> 00:17:32.140]   And I think that's
[00:17:32.140 --> 00:17:33.140]   really promising and
[00:17:33.140 --> 00:17:33.460]   interesting.
[00:17:33.460 --> 00:17:36.000]   Yeah, I think that's
[00:17:36.000 --> 00:17:37.180]   everything I had for you
[00:17:37.180 --> 00:17:37.340]   all.
[00:17:37.340 --> 00:17:38.420]   Happy to take questions.
[00:17:38.420 --> 00:17:41.160]   Happy to for just a
[00:17:41.160 --> 00:17:42.260]   greater discussion about
[00:17:42.260 --> 00:17:43.800]   GRPO, reinforcement
[00:17:43.800 --> 00:17:44.880]   learning, self-improvement,
[00:17:44.880 --> 00:17:45.560]   all of the things we
[00:17:45.560 --> 00:17:46.000]   mentioned here.
[00:17:46.000 --> 00:17:47.220]   So yeah, thanks.
[00:17:47.220 --> 00:17:52.820]   Yes, Chris?
[00:17:53.920 --> 00:17:54.960]   Yeah, I found it
[00:17:54.960 --> 00:17:55.680]   interesting how you
[00:17:55.680 --> 00:17:56.540]   also looked at the
[00:17:56.540 --> 00:17:57.580]   Spurious Reward
[00:17:57.580 --> 00:17:58.800]   Rethinking Training
[00:17:58.800 --> 00:18:00.300]   Signals RLVR
[00:18:00.300 --> 00:18:00.740]   paper.
[00:18:00.740 --> 00:18:04.480]   I saw how when you
[00:18:04.480 --> 00:18:05.440]   did the benchmarks that
[00:18:05.440 --> 00:18:06.780]   you did models other
[00:18:06.780 --> 00:18:08.540]   than QN for the
[00:18:08.540 --> 00:18:09.920]   improvements.
[00:18:09.920 --> 00:18:11.140]   Was that to account for
[00:18:11.140 --> 00:18:12.820]   that error from like the
[00:18:12.820 --> 00:18:13.560]   previous one where they
[00:18:13.560 --> 00:18:14.520]   showed how the QN
[00:18:14.520 --> 00:18:15.980]   models were mostly
[00:18:15.980 --> 00:18:16.960]   focused on in these
[00:18:16.960 --> 00:18:18.360]   RLVR papers to prevent
[00:18:18.360 --> 00:18:19.000]   that mistake?
[00:18:20.360 --> 00:18:22.380]   Yeah, so this
[00:18:22.380 --> 00:18:23.400]   Spurious Reward
[00:18:23.400 --> 00:18:24.300]   paper actually came
[00:18:24.300 --> 00:18:26.160]   out around slash
[00:18:26.160 --> 00:18:26.980]   after when our paper
[00:18:26.980 --> 00:18:27.420]   came out.
[00:18:27.420 --> 00:18:28.500]   So I wasn't aware of
[00:18:28.500 --> 00:18:29.760]   this research, but I
[00:18:29.760 --> 00:18:30.560]   do think that in
[00:18:30.560 --> 00:18:31.660]   general, we wanted to
[00:18:31.660 --> 00:18:32.940]   show that our
[00:18:32.940 --> 00:18:34.240]   technique holds across
[00:18:34.240 --> 00:18:35.720]   different model
[00:18:35.720 --> 00:18:36.420]   families, different
[00:18:36.420 --> 00:18:37.380]   sizes of models, all
[00:18:37.380 --> 00:18:38.100]   of those things, right?
[00:18:38.100 --> 00:18:39.380]   Just for like rigor and
[00:18:39.380 --> 00:18:40.220]   like to sort of prove
[00:18:40.220 --> 00:18:40.820]   out the technique.
[00:18:40.820 --> 00:18:41.460]   And I think in
[00:18:41.460 --> 00:18:44.880]   general, I appreciate
[00:18:44.880 --> 00:18:46.000]   a lot when papers do
[00:18:46.000 --> 00:18:47.480]   this to just show that
[00:18:47.480 --> 00:18:48.300]   it's not something
[00:18:48.300 --> 00:18:49.040]   specific to a
[00:18:49.040 --> 00:18:49.540]   particular model
[00:18:49.540 --> 00:18:50.160]   family or a
[00:18:50.160 --> 00:18:50.740]   particular recipe
[00:18:50.740 --> 00:18:51.720]   that one company
[00:18:51.720 --> 00:18:52.180]   was using.
[00:18:52.180 --> 00:18:53.640]   But yeah, we
[00:18:53.640 --> 00:18:54.080]   weren't aware of
[00:18:54.080 --> 00:18:54.700]   this at the time.
[00:18:54.700 --> 00:19:01.020]   I think Ted had
[00:19:01.020 --> 00:19:01.440]   a question.
[00:19:01.440 --> 00:19:02.340]   I don't know, Ted,
[00:19:02.340 --> 00:19:02.860]   if you want to come
[00:19:02.860 --> 00:19:03.540]   on camera and ask
[00:19:03.540 --> 00:19:03.760]   it.
[00:19:03.760 --> 00:19:06.160]   Mark can read
[00:19:06.160 --> 00:19:06.740]   it all for you.
[00:19:06.740 --> 00:19:08.880]   Yeah, so
[00:19:08.880 --> 00:19:10.240]   specifically, you
[00:19:10.240 --> 00:19:11.680]   mentioned this
[00:19:11.680 --> 00:19:12.720]   other Spurious Rewards
[00:19:12.720 --> 00:19:13.580]   paper, and I can't
[00:19:13.580 --> 00:19:14.200]   remember where, but
[00:19:14.200 --> 00:19:14.980]   I thought I saw
[00:19:14.980 --> 00:19:17.240]   somebody was posting
[00:19:17.240 --> 00:19:19.360]   different papers
[00:19:19.360 --> 00:19:21.680]   RL papers, RL papers
[00:19:21.680 --> 00:19:23.160]   had baselines
[00:19:23.160 --> 00:19:24.360]   that were lower
[00:19:24.360 --> 00:19:26.380]   than the model
[00:19:26.380 --> 00:19:27.440]   authors were able
[00:19:27.440 --> 00:19:28.460]   to achieve.
[00:19:28.460 --> 00:19:29.920]   So the baselines
[00:19:29.920 --> 00:19:30.740]   used in the paper
[00:19:30.740 --> 00:19:31.800]   were suboptimal.
[00:19:31.800 --> 00:19:33.420]   And that basically,
[00:19:33.420 --> 00:19:36.480]   if you suboptimally
[00:19:36.480 --> 00:19:36.980]   use the model
[00:19:36.980 --> 00:19:37.780]   and then use RL,
[00:19:37.780 --> 00:19:38.380]   you can sort of
[00:19:38.380 --> 00:19:39.960]   correct for your
[00:19:39.960 --> 00:19:41.100]   suboptimality, but
[00:19:41.100 --> 00:19:41.940]   you're not actually
[00:19:41.940 --> 00:19:44.040]   improving the performance.
[00:19:44.040 --> 00:19:46.740]   this one, which I
[00:19:46.740 --> 00:19:47.360]   thought was a very
[00:19:47.360 --> 00:19:50.260]   interesting conclusion
[00:19:50.260 --> 00:19:51.180]   saying that you don't
[00:19:51.180 --> 00:19:52.060]   really need great
[00:19:52.060 --> 00:19:54.200]   rewards, that maybe
[00:19:54.200 --> 00:19:55.140]   that, in particular,
[00:19:55.140 --> 00:19:56.940]   that result was more
[00:19:56.940 --> 00:19:58.380]   about fixing
[00:19:58.380 --> 00:20:00.200]   suboptimality versus
[00:20:00.200 --> 00:20:01.240]   improving reasoning.
[00:20:01.240 --> 00:20:02.820]   I don't know if you've
[00:20:02.820 --> 00:20:04.940]   seen that discussion.
[00:20:06.720 --> 00:20:08.100]   Yeah, I haven't, but
[00:20:08.100 --> 00:20:08.820]   that sounds
[00:20:08.820 --> 00:20:10.380]   interesting, and
[00:20:10.380 --> 00:20:11.860]   yeah, I would love
[00:20:11.860 --> 00:20:12.740]   to get a link to
[00:20:12.740 --> 00:20:12.980]   that.
[00:20:12.980 --> 00:20:14.160]   Yeah, sorry, I
[00:20:14.160 --> 00:20:14.800]   don't have the
[00:20:14.800 --> 00:20:16.140]   reference at my
[00:20:16.140 --> 00:20:16.640]   fingertips.
[00:20:16.640 --> 00:20:17.400]   You're good.
[00:20:17.400 --> 00:20:30.880]   with the TRL
[00:20:30.880 --> 00:20:31.820]   framework that
[00:20:31.820 --> 00:20:32.780]   applied the
[00:20:32.780 --> 00:20:34.260]   GRPO to the
[00:20:34.260 --> 00:20:34.640]   model.
[00:20:34.640 --> 00:20:36.560]   So is that
[00:20:36.560 --> 00:20:38.120]   like fine-tuning?
[00:20:38.120 --> 00:20:39.140]   Is that like
[00:20:39.140 --> 00:20:40.080]   directly updating
[00:20:40.080 --> 00:20:40.820]   the weights, or is
[00:20:40.820 --> 00:20:41.440]   it some kind of
[00:20:41.440 --> 00:20:42.400]   wrapper on the
[00:20:42.400 --> 00:20:43.600]   model, or how
[00:20:43.600 --> 00:20:44.820]   does that GRPO
[00:20:44.820 --> 00:20:46.440]   trainer work?
[00:20:46.440 --> 00:20:48.360]   Yeah, so we
[00:20:48.360 --> 00:20:49.020]   are directly
[00:20:49.020 --> 00:20:49.600]   updating the
[00:20:49.600 --> 00:20:49.880]   weights.
[00:20:49.880 --> 00:20:50.980]   Okay.
[00:20:50.980 --> 00:20:52.300]   So, yeah,
[00:20:52.300 --> 00:20:52.940]   generally speaking,
[00:20:52.940 --> 00:20:53.500]   you do need an
[00:20:53.500 --> 00:20:54.160]   open-source model
[00:20:54.160 --> 00:20:55.060]   because you are
[00:20:55.060 --> 00:20:56.720]   directly updating
[00:20:56.720 --> 00:20:57.000]   weights.
[00:20:57.000 --> 00:20:58.440]   Got it.
[00:20:58.440 --> 00:20:58.980]   Thank you.
[00:20:58.980 --> 00:21:02.100]   Hey, Shelly,
[00:21:02.100 --> 00:21:03.400]   thanks for the
[00:21:03.400 --> 00:21:04.400]   great presentation.
[00:21:04.400 --> 00:21:05.160]   Thanks for joining
[00:21:05.160 --> 00:21:05.420]   us.
[00:21:05.420 --> 00:21:07.580]   I was curious,
[00:21:07.580 --> 00:21:09.540]   I don't think I
[00:21:09.540 --> 00:21:10.060]   saw anything about
[00:21:10.060 --> 00:21:10.520]   this in the paper,
[00:21:10.520 --> 00:21:11.760]   but maybe you've
[00:21:11.760 --> 00:21:12.240]   thought about this.
[00:21:12.240 --> 00:21:14.480]   You know, it
[00:21:14.480 --> 00:21:15.060]   seems like the
[00:21:15.060 --> 00:21:16.420]   larger goal is to
[00:21:16.420 --> 00:21:17.260]   be able to
[00:21:17.260 --> 00:21:17.620]   sort of
[00:21:17.620 --> 00:21:18.140]   continuously
[00:21:18.140 --> 00:21:19.280]   update the
[00:21:19.280 --> 00:21:20.560]   model as
[00:21:20.560 --> 00:21:21.540]   new information
[00:21:21.540 --> 00:21:23.220]   comes in and
[00:21:23.220 --> 00:21:24.040]   maybe distribution
[00:21:24.040 --> 00:21:24.820]   shift and so
[00:21:24.820 --> 00:21:25.060]   forth.
[00:21:25.060 --> 00:21:26.140]   Have you thought
[00:21:26.140 --> 00:21:26.760]   about, and
[00:21:26.760 --> 00:21:27.360]   you know, the
[00:21:27.360 --> 00:21:27.880]   catastrophic
[00:21:27.880 --> 00:21:29.060]   forgetting part is
[00:21:29.060 --> 00:21:30.000]   maybe addressing
[00:21:30.000 --> 00:21:30.560]   that to some
[00:21:30.560 --> 00:21:30.920]   extent.
[00:21:30.920 --> 00:21:31.780]   Have you thought
[00:21:31.780 --> 00:21:32.440]   about like what
[00:21:32.440 --> 00:21:33.160]   happens when you
[00:21:33.160 --> 00:21:34.140]   do many rounds
[00:21:34.140 --> 00:21:35.860]   of this kind of
[00:21:35.860 --> 00:21:37.420]   like self-reinforced
[00:21:37.420 --> 00:21:38.020]   training?
[00:21:38.020 --> 00:21:40.200]   And maybe you
[00:21:40.200 --> 00:21:40.500]   did some
[00:21:40.500 --> 00:21:40.980]   experiments.
[00:21:40.980 --> 00:21:41.800]   I'm curious to
[00:21:41.800 --> 00:21:42.380]   hear your thoughts.
[00:21:42.380 --> 00:21:44.120]   Yeah, that's a
[00:21:44.120 --> 00:21:44.800]   really good question.
[00:21:45.040 --> 00:21:45.540]   I do think that
[00:21:45.540 --> 00:21:45.920]   is a very
[00:21:45.920 --> 00:21:46.680]   natural extension
[00:21:46.680 --> 00:21:47.360]   of this work.
[00:21:47.360 --> 00:21:47.940]   And one of the
[00:21:47.940 --> 00:21:48.560]   ideas that we
[00:21:48.560 --> 00:21:48.980]   talked about
[00:21:48.980 --> 00:21:49.820]   internally was
[00:21:49.820 --> 00:21:50.820]   swishing between
[00:21:50.820 --> 00:21:51.680]   rounds of RL and
[00:21:51.680 --> 00:21:52.400]   SFT, right?
[00:21:52.400 --> 00:21:52.920]   Which is kind of
[00:21:52.920 --> 00:21:53.540]   a proven thing
[00:21:53.540 --> 00:21:54.340]   like do some
[00:21:54.340 --> 00:21:55.380]   SFT, do some
[00:21:55.380 --> 00:21:56.600]   RL, then you
[00:21:56.600 --> 00:21:57.300]   keep doing that
[00:21:57.300 --> 00:21:57.700]   over and over
[00:21:57.700 --> 00:21:58.220]   again because you
[00:21:58.220 --> 00:21:59.220]   kind of elicit
[00:21:59.220 --> 00:21:59.800]   slightly new
[00:21:59.800 --> 00:22:00.460]   behavior with
[00:22:00.460 --> 00:22:01.140]   each round that
[00:22:01.140 --> 00:22:01.460]   way.
[00:22:01.460 --> 00:22:05.700]   What we were
[00:22:05.700 --> 00:22:06.340]   seeing, we were
[00:22:06.340 --> 00:22:07.220]   training, so we
[00:22:07.220 --> 00:22:09.080]   kind of let the
[00:22:09.080 --> 00:22:09.940]   models over-train a
[00:22:09.940 --> 00:22:10.560]   little bit, right?
[00:22:10.560 --> 00:22:12.160]   like we let
[00:22:12.160 --> 00:22:13.680]   them take a
[00:22:13.680 --> 00:22:14.440]   data set and run
[00:22:14.440 --> 00:22:14.860]   until they
[00:22:14.860 --> 00:22:15.580]   converge and we
[00:22:15.580 --> 00:22:16.820]   were seeing that
[00:22:16.820 --> 00:22:17.400]   they kind of
[00:22:17.400 --> 00:22:18.260]   leveled out and
[00:22:18.260 --> 00:22:18.860]   potentially started
[00:22:18.860 --> 00:22:19.240]   to get a little
[00:22:19.240 --> 00:22:21.280]   bit worse, usually
[00:22:21.280 --> 00:22:24.020]   around like, I
[00:22:24.020 --> 00:22:24.480]   mean, there's like
[00:22:24.480 --> 00:22:25.140]   sample numbers in
[00:22:25.140 --> 00:22:25.920]   the paper, like
[00:22:25.920 --> 00:22:28.840]   about 20 to 25,000
[00:22:28.840 --> 00:22:30.460]   samples, but probably
[00:22:30.460 --> 00:22:30.980]   we could have been
[00:22:30.980 --> 00:22:31.600]   even more sample
[00:22:31.600 --> 00:22:32.140]   efficient if we
[00:22:32.140 --> 00:22:34.000]   tried, and about
[00:22:34.000 --> 00:22:34.800]   a thousand steps.
[00:22:35.200 --> 00:22:36.980]   So my sense is
[00:22:36.980 --> 00:22:37.560]   that you need to
[00:22:37.560 --> 00:22:39.240]   interject probably
[00:22:39.240 --> 00:22:40.600]   something more in
[00:22:40.600 --> 00:22:41.520]   between those rounds
[00:22:41.520 --> 00:22:42.580]   of RL in order to
[00:22:42.580 --> 00:22:43.100]   squeeze out more
[00:22:43.100 --> 00:22:43.940]   performance because I
[00:22:43.940 --> 00:22:44.460]   feel like I've seen
[00:22:44.460 --> 00:22:45.140]   some reports where
[00:22:45.140 --> 00:22:46.380]   it's like GRPO will
[00:22:46.380 --> 00:22:47.260]   like indefinitely go
[00:22:47.260 --> 00:22:48.320]   up, and I haven't
[00:22:48.320 --> 00:22:48.860]   seen that be the
[00:22:48.860 --> 00:22:49.180]   case.
[00:22:49.180 --> 00:22:53.620]   Yeah, and in terms
[00:22:53.620 --> 00:22:54.080]   of distribution
[00:22:54.080 --> 00:22:55.000]   shift in general or
[00:22:55.000 --> 00:22:55.740]   like how catastrophic
[00:22:55.740 --> 00:22:56.460]   we're getting changes,
[00:22:56.460 --> 00:22:57.460]   we weren't able to
[00:22:57.460 --> 00:22:58.480]   run any super long-term
[00:22:58.480 --> 00:22:59.320]   experiments, but I
[00:22:59.320 --> 00:22:59.940]   think that would be
[00:22:59.940 --> 00:23:00.580]   very interesting and
[00:23:00.580 --> 00:23:01.000]   very interesting
[00:23:01.000 --> 00:23:01.420]   extension.
[00:23:01.420 --> 00:23:04.280]   Great, thank you
[00:23:04.280 --> 00:23:04.780]   very much.
[00:23:04.780 --> 00:23:10.560]   I think we have a
[00:23:10.560 --> 00:23:11.180]   question from
[00:23:11.180 --> 00:23:11.620]   Xiaobai.
[00:23:11.620 --> 00:23:12.280]   I don't know, Xiaobai,
[00:23:12.280 --> 00:23:13.520]   if you want to
[00:23:13.520 --> 00:23:14.320]   voice over your
[00:23:14.320 --> 00:23:14.620]   question?
[00:23:14.620 --> 00:23:18.800]   Yeah, thanks.
[00:23:18.800 --> 00:23:19.480]   Yeah, I think the
[00:23:19.480 --> 00:23:20.320]   question I have is
[00:23:20.320 --> 00:23:22.420]   like, it seems like
[00:23:22.420 --> 00:23:23.560]   multiple tries are
[00:23:23.560 --> 00:23:24.900]   very dependent on
[00:23:24.900 --> 00:23:25.800]   the temperature.
[00:23:25.800 --> 00:23:27.600]   So I guess like
[00:23:27.600 --> 00:23:28.060]   when temperature
[00:23:28.060 --> 00:23:29.480]   zero, no matter
[00:23:29.480 --> 00:23:30.200]   how many you
[00:23:30.200 --> 00:23:31.140]   retry, you won't
[00:23:31.140 --> 00:23:32.320]   be actually successful
[00:23:32.320 --> 00:23:33.000]   rate, more
[00:23:33.000 --> 00:23:33.660]   successful rate.
[00:23:33.920 --> 00:23:34.780]   So I wonder, have
[00:23:34.780 --> 00:23:35.740]   you done any
[00:23:35.740 --> 00:23:36.800]   analysis on how
[00:23:36.800 --> 00:23:37.620]   actually temperature
[00:23:37.620 --> 00:23:38.740]   is going to
[00:23:38.740 --> 00:23:39.900]   impact your
[00:23:39.900 --> 00:23:40.880]   experiment result?
[00:23:40.880 --> 00:23:43.580]   Yeah, that's a
[00:23:43.580 --> 00:23:44.240]   really good point.
[00:23:44.240 --> 00:23:46.820]   I believe I would
[00:23:46.820 --> 00:23:47.360]   have to go back.
[00:23:47.360 --> 00:23:48.840]   It's been a couple
[00:23:48.840 --> 00:23:49.400]   months since I
[00:23:49.400 --> 00:23:49.960]   looked at the code.
[00:23:49.960 --> 00:23:50.820]   I believe I set
[00:23:50.820 --> 00:23:51.480]   temperature to like
[00:23:51.480 --> 00:23:53.780]   0.9 or something
[00:23:53.780 --> 00:23:54.140]   like that.
[00:23:54.200 --> 00:23:54.840]   so you get
[00:23:54.840 --> 00:24:00.300]   maybe a little
[00:24:00.300 --> 00:24:00.940]   lower than that.
[00:24:00.940 --> 00:24:01.560]   But like, yeah, you
[00:24:01.560 --> 00:24:02.300]   definitely need some
[00:24:02.300 --> 00:24:04.820]   variance in your
[00:24:04.820 --> 00:24:05.900]   results because you
[00:24:05.900 --> 00:24:07.820]   need, like especially
[00:24:07.820 --> 00:24:08.680]   in the countdown case,
[00:24:08.680 --> 00:24:09.280]   like you need your
[00:24:09.280 --> 00:24:10.020]   model to explore
[00:24:10.020 --> 00:24:10.860]   multiple paths so
[00:24:10.860 --> 00:24:11.460]   that you can reward
[00:24:11.460 --> 00:24:12.480]   the right one, right?
[00:24:12.480 --> 00:24:14.180]   Like you need it to
[00:24:14.180 --> 00:24:16.900]   kind of, you don't
[00:24:16.900 --> 00:24:17.600]   want it to fail the
[00:24:17.600 --> 00:24:18.460]   same way multiple
[00:24:18.460 --> 00:24:19.320]   times because it
[00:24:19.320 --> 00:24:20.100]   always goes down the
[00:24:20.100 --> 00:24:20.760]   exact same reasoning
[00:24:20.760 --> 00:24:21.660]   path no matter what
[00:24:21.660 --> 00:24:23.360]   because you need
[00:24:23.360 --> 00:24:24.240]   to hopefully like
[00:24:24.240 --> 00:24:26.000]   elicit enough
[00:24:26.000 --> 00:24:28.300]   branches such that
[00:24:28.300 --> 00:24:28.940]   like something ends
[00:24:28.940 --> 00:24:29.660]   up in a success.
[00:24:29.660 --> 00:24:30.680]   We didn't do any
[00:24:30.680 --> 00:24:32.120]   specific experiments on
[00:24:32.120 --> 00:24:33.380]   like playing with the
[00:24:33.380 --> 00:24:34.080]   temperature a lot and
[00:24:34.080 --> 00:24:34.520]   seeing how that
[00:24:34.520 --> 00:24:35.160]   changes things.
[00:24:35.160 --> 00:24:36.520]   But intuitively we
[00:24:36.520 --> 00:24:37.940]   did agree that if you
[00:24:37.940 --> 00:24:38.660]   set temperature to
[00:24:38.660 --> 00:24:39.440]   zero and it's like
[00:24:39.440 --> 00:24:40.480]   very deterministic all
[00:24:40.480 --> 00:24:41.940]   the time, that isn't
[00:24:41.940 --> 00:24:42.400]   going to work.
[00:24:42.400 --> 00:24:43.980]   But yeah, it would
[00:24:43.980 --> 00:24:45.020]   be interested in
[00:24:45.020 --> 00:24:45.740]   in general, like
[00:24:45.740 --> 00:24:47.140]   temperature with RL
[00:24:47.140 --> 00:24:48.000]   seems like something
[00:24:48.000 --> 00:24:48.500]   that we could all
[00:24:48.500 --> 00:24:49.140]   investigate a little
[00:24:49.140 --> 00:24:49.500]   bit more.
[00:24:49.500 --> 00:24:52.560]   That makes sense.
[00:24:52.560 --> 00:24:53.280]   Yeah, thanks.
[00:24:53.280 --> 00:24:58.300]   Yikes.
[00:24:58.300 --> 00:24:59.500]   Do you want to go
[00:24:59.500 --> 00:25:00.840]   ahead with your
[00:25:00.840 --> 00:25:01.920]   chain of thought?
[00:25:01.920 --> 00:25:03.000]   A slew of
[00:25:03.000 --> 00:25:03.680]   questions?
[00:25:03.680 --> 00:25:04.140]   Yeah.
[00:25:04.140 --> 00:25:05.720]   Let me finish
[00:25:05.720 --> 00:25:08.460]   clicking this button
[00:25:08.460 --> 00:25:08.880]   maybe.
[00:25:08.880 --> 00:25:10.320]   Okay, that should do
[00:25:10.320 --> 00:25:10.700]   that.
[00:25:10.700 --> 00:25:12.520]   And then that.
[00:25:12.520 --> 00:25:13.340]   And then this one.
[00:25:13.340 --> 00:25:13.600]   Okay.
[00:25:13.600 --> 00:25:17.300]   Yeah, so let's
[00:25:17.300 --> 00:25:17.460]   see.
[00:25:17.460 --> 00:25:21.580]   for the technique
[00:25:21.580 --> 00:25:22.720]   that you guys did
[00:25:22.720 --> 00:25:24.980]   to find the thing
[00:25:24.980 --> 00:25:25.940]   that you're training
[00:25:25.940 --> 00:25:26.840]   for, I didn't quite
[00:25:26.840 --> 00:25:28.440]   like catch it all
[00:25:28.440 --> 00:25:28.980]   the way, but it
[00:25:28.980 --> 00:25:30.700]   sounds like what
[00:25:30.700 --> 00:25:31.820]   we were doing, the
[00:25:31.820 --> 00:25:34.240]   basic principle is
[00:25:34.240 --> 00:25:37.640]   you could just
[00:25:37.640 --> 00:25:39.300]   continue thinking
[00:25:39.300 --> 00:25:40.260]   forever and ever.
[00:25:40.520 --> 00:25:42.100]   and it turns out
[00:25:42.100 --> 00:25:43.760]   that the more
[00:25:43.760 --> 00:25:44.760]   length we train
[00:25:44.760 --> 00:25:45.400]   for, the better
[00:25:45.400 --> 00:25:46.540]   performance we tend
[00:25:46.540 --> 00:25:48.540]   to get, as so
[00:25:48.540 --> 00:25:49.820]   far kind of been
[00:25:49.820 --> 00:25:50.500]   the verdict.
[00:25:50.500 --> 00:25:53.960]   And so I'm
[00:25:53.960 --> 00:25:55.240]   curious for this
[00:25:55.240 --> 00:25:57.080]   technique, I guess
[00:25:57.080 --> 00:25:57.760]   I should like go
[00:25:57.760 --> 00:25:59.040]   back with a paper a
[00:25:59.040 --> 00:25:59.260]   little bit.
[00:25:59.260 --> 00:25:59.660]   Oh, yeah.
[00:25:59.660 --> 00:26:01.000]   Can we, how do we
[00:26:01.000 --> 00:26:01.800]   do better if we fail?
[00:26:01.800 --> 00:26:02.640]   So for the self
[00:26:02.640 --> 00:26:03.460]   reflection thing,
[00:26:04.000 --> 00:26:04.860]   um, the sort
[00:26:04.860 --> 00:26:06.320]   of like basic
[00:26:06.320 --> 00:26:07.760]   principle here, let's
[00:26:07.760 --> 00:26:08.120]   see, generate
[00:26:08.120 --> 00:26:09.340]   output, success, do
[00:26:09.340 --> 00:26:10.000]   nothing, fail,
[00:26:10.000 --> 00:26:10.920]   generate self
[00:26:10.920 --> 00:26:12.620]   reflection, retry
[00:26:12.620 --> 00:26:13.660]   tasks, same
[00:26:13.660 --> 00:26:14.460]   conversation.
[00:26:14.460 --> 00:26:15.440]   Okay.
[00:26:15.440 --> 00:26:16.160]   So you do a
[00:26:16.160 --> 00:26:18.020]   think block and
[00:26:18.020 --> 00:26:19.520]   then a task
[00:26:19.520 --> 00:26:20.040]   attempt.
[00:26:20.040 --> 00:26:21.000]   And then if the
[00:26:21.000 --> 00:26:22.020]   task is insufficient,
[00:26:22.020 --> 00:26:22.560]   then you do
[00:26:22.560 --> 00:26:23.860]   another think block
[00:26:23.860 --> 00:26:25.200]   and another task
[00:26:25.200 --> 00:26:25.540]   attempt.
[00:26:25.540 --> 00:26:26.440]   And that's all in
[00:26:26.440 --> 00:26:27.740]   the same window.
[00:26:27.740 --> 00:26:28.100]   Yeah.
[00:26:28.140 --> 00:26:28.600]   and then you
[00:26:28.600 --> 00:26:30.380]   reward until you
[00:26:30.380 --> 00:26:31.560]   get it or do you
[00:26:31.560 --> 00:26:31.820]   do it?
[00:26:31.820 --> 00:26:34.140]   So, so yeah, it's
[00:26:34.140 --> 00:26:35.220]   just one, one extra
[00:26:35.220 --> 00:26:35.560]   step.
[00:26:35.560 --> 00:26:36.660]   So it's two task
[00:26:36.660 --> 00:26:37.620]   retries total.
[00:26:37.620 --> 00:26:39.080]   Um, and there's
[00:26:39.080 --> 00:26:39.740]   actually not, we
[00:26:39.740 --> 00:26:40.540]   weren't specifically
[00:26:40.540 --> 00:26:43.480]   using, uh, reasoning
[00:26:43.480 --> 00:26:43.980]   models.
[00:26:43.980 --> 00:26:45.800]   Uh, so, so there
[00:26:45.800 --> 00:26:46.620]   isn't an explicit
[00:26:46.620 --> 00:26:47.740]   think block that this
[00:26:47.740 --> 00:26:50.880]   is just like for, uh,
[00:26:50.880 --> 00:26:51.540]   I mean, yeah, this
[00:26:51.540 --> 00:26:52.440]   was a lot of these
[00:26:52.440 --> 00:26:53.140]   models were released
[00:26:53.140 --> 00:26:54.220]   like early 2024,
[00:26:54.220 --> 00:26:54.920]   like reasoning
[00:26:54.920 --> 00:26:55.640]   models weren't as
[00:26:55.640 --> 00:26:56.540]   much of a thing at
[00:26:56.540 --> 00:26:57.200]   that point in time.
[00:26:57.200 --> 00:27:00.080]   So, um, yeah.
[00:27:00.080 --> 00:27:01.220]   Do you have any
[00:27:01.220 --> 00:27:02.760]   intuition on how, or
[00:27:02.760 --> 00:27:03.940]   if this kind of
[00:27:03.940 --> 00:27:05.060]   technique might apply
[00:27:05.060 --> 00:27:05.900]   to reasoning models
[00:27:05.900 --> 00:27:06.460]   and kind of the
[00:27:06.460 --> 00:27:07.620]   reasoning RL paradigm?
[00:27:07.620 --> 00:27:10.260]   Yeah, that's a good
[00:27:10.260 --> 00:27:10.640]   question.
[00:27:10.640 --> 00:27:14.000]   I mean, my, my intuition
[00:27:14.000 --> 00:27:16.020]   is that these sort
[00:27:16.020 --> 00:27:17.860]   like this similar
[00:27:17.860 --> 00:27:18.620]   approaches are already
[00:27:18.620 --> 00:27:19.680]   being used to trade
[00:27:19.680 --> 00:27:20.440]   good reasoning.
[00:27:20.440 --> 00:27:20.840]   Right.
[00:27:20.840 --> 00:27:21.460]   And so I don't know
[00:27:21.460 --> 00:27:22.060]   how much this would
[00:27:22.060 --> 00:27:22.980]   necessarily build upon
[00:27:22.980 --> 00:27:23.600]   a reasoning model
[00:27:23.600 --> 00:27:25.480]   that has already, uh,
[00:27:26.740 --> 00:27:28.220]   been hyper-optimized
[00:27:28.220 --> 00:27:29.560]   to, for example,
[00:27:29.560 --> 00:27:31.180]   probably like self-reflect
[00:27:31.180 --> 00:27:31.740]   really well.
[00:27:31.740 --> 00:27:31.960]   Right.
[00:27:31.960 --> 00:27:32.920]   That's probably a side,
[00:27:32.920 --> 00:27:34.840]   uh, quest or an
[00:27:34.840 --> 00:27:37.940]   adjacent RL target to
[00:27:37.940 --> 00:27:39.080]   what we were shooting
[00:27:39.080 --> 00:27:39.480]   for.
[00:27:39.480 --> 00:27:39.780]   Right.
[00:27:39.780 --> 00:27:40.960]   Like it is like the
[00:27:40.960 --> 00:27:42.800]   reasoning rewards that
[00:27:42.800 --> 00:27:43.700]   generally speaking,
[00:27:43.700 --> 00:27:44.720]   these large lots use.
[00:27:44.720 --> 00:27:46.940]   So my sense is it will
[00:27:46.940 --> 00:27:48.580]   be less effective on
[00:27:48.580 --> 00:27:49.360]   reasoning models because
[00:27:49.360 --> 00:27:50.640]   has this is almost an
[00:27:50.640 --> 00:27:51.200]   approach to like
[00:27:51.200 --> 00:27:51.920]   incentivizing good
[00:27:51.920 --> 00:27:53.040]   reasoning, pre-reasoning
[00:27:53.040 --> 00:27:53.380]   models.
[00:27:53.380 --> 00:27:57.360]   Let's yeah.
[00:27:57.360 --> 00:27:58.540]   Well, so yeah, that's,
[00:27:58.540 --> 00:27:59.220]   I think that's the
[00:27:59.220 --> 00:28:00.800]   interesting part to me,
[00:28:00.800 --> 00:28:03.600]   the largely because the
[00:28:03.600 --> 00:28:04.680]   reasoning models, or
[00:28:04.680 --> 00:28:05.440]   at least from what I've
[00:28:05.440 --> 00:28:07.120]   seen, if you just train
[00:28:07.120 --> 00:28:08.120]   them to think longer,
[00:28:08.120 --> 00:28:09.220]   like that's really all
[00:28:09.220 --> 00:28:10.540]   we're doing is, is the,
[00:28:10.540 --> 00:28:11.580]   the more performance we
[00:28:11.580 --> 00:28:12.780]   get is just like more
[00:28:12.780 --> 00:28:13.880]   think tokens or less
[00:28:13.880 --> 00:28:15.060]   think tokens essentially.
[00:28:15.760 --> 00:28:16.820]   Um, and then there's
[00:28:16.820 --> 00:28:18.040]   very, or like there
[00:28:18.040 --> 00:28:19.000]   seems to be some
[00:28:19.000 --> 00:28:23.380]   contention on, um, does
[00:28:23.380 --> 00:28:24.880]   the content of those
[00:28:24.880 --> 00:28:26.320]   think tokens actually
[00:28:26.320 --> 00:28:26.860]   matter?
[00:28:26.860 --> 00:28:27.920]   There's like a couple
[00:28:27.920 --> 00:28:30.760]   of, um, papers that
[00:28:30.760 --> 00:28:32.400]   sort of indicate like,
[00:28:32.400 --> 00:28:33.540]   no, it actually like
[00:28:33.540 --> 00:28:34.820]   these reasoning traces can
[00:28:34.820 --> 00:28:36.340]   be completely incoherent.
[00:28:36.340 --> 00:28:37.880]   Um, but you still get
[00:28:37.880 --> 00:28:39.100]   the performance increase.
[00:28:39.100 --> 00:28:41.480]   Um, and then some papers
[00:28:41.480 --> 00:28:42.740]   that are like, oh, if
[00:28:42.740 --> 00:28:44.000]   the, if the reasoning
[00:28:44.000 --> 00:28:45.420]   traces change in this
[00:28:45.420 --> 00:28:47.340]   particular way, then you
[00:28:47.340 --> 00:28:48.600]   see a more significant,
[00:28:48.600 --> 00:28:50.280]   um, performance increase
[00:28:50.280 --> 00:28:51.620]   than you otherwise would.
[00:28:51.620 --> 00:28:53.360]   Um, and I'm sort of like
[00:28:53.360 --> 00:28:54.400]   trying to dial in the
[00:28:54.400 --> 00:28:55.240]   bottom of that.
[00:28:55.240 --> 00:28:57.100]   So yeah, I guess like it
[00:28:57.100 --> 00:28:58.960]   doesn't quite apply here
[00:28:58.960 --> 00:29:00.140]   since we don't have a
[00:29:00.140 --> 00:29:01.600]   reasoning model to work
[00:29:01.600 --> 00:29:02.840]   with, but I'm, I would
[00:29:02.840 --> 00:29:06.140]   be interested to see the
[00:29:06.140 --> 00:29:08.640]   I would, I would be
[00:29:08.640 --> 00:29:09.460]   interested to sit and to
[00:29:09.460 --> 00:29:10.620]   like basically just run
[00:29:10.620 --> 00:29:13.100]   this as identical,
[00:29:13.100 --> 00:29:15.080]   identical pipeline, except
[00:29:15.080 --> 00:29:16.620]   with a reasoning model
[00:29:16.620 --> 00:29:17.540]   because you have, you
[00:29:17.540 --> 00:29:19.060]   would have like reflect
[00:29:19.060 --> 00:29:22.220]   task, output, fail, like
[00:29:22.220 --> 00:29:24.640]   then the second self
[00:29:24.640 --> 00:29:25.860]   reflection there might
[00:29:25.860 --> 00:29:27.000]   have like some weird
[00:29:27.000 --> 00:29:28.000]   stuff in the reasoning
[00:29:28.000 --> 00:29:29.060]   block is like where I'm,
[00:29:29.060 --> 00:29:30.360]   where am I, where I'm
[00:29:30.360 --> 00:29:31.000]   headed, I think.
[00:29:31.000 --> 00:29:34.600]   Um, but yeah, no, I
[00:29:34.600 --> 00:29:35.560]   think that, that, that
[00:29:35.560 --> 00:29:36.500]   does it for me basically.
[00:29:36.500 --> 00:29:39.300]   Um, other than, well, and
[00:29:39.300 --> 00:29:40.100]   then I guess it also
[00:29:40.100 --> 00:29:41.100]   applies to reasoning, but
[00:29:41.100 --> 00:29:42.000]   there's, there's been some
[00:29:42.000 --> 00:29:42.880]   interesting stuff.
[00:29:42.880 --> 00:29:44.720]   Um, namely the paper
[00:29:44.720 --> 00:29:45.860]   like absolute zero.
[00:29:45.860 --> 00:29:46.960]   And then I think Sakana
[00:29:46.960 --> 00:29:48.960]   has another, um, like,
[00:29:48.960 --> 00:29:49.940]   Hey, we secretly actually
[00:29:49.940 --> 00:29:50.800]   don't even need teacher
[00:29:50.800 --> 00:29:52.120]   models kind of thing.
[00:29:52.120 --> 00:29:53.840]   Um, but I think those all
[00:29:53.840 --> 00:29:54.940]   technically apply to
[00:29:54.940 --> 00:29:55.920]   reasoning models.
[00:29:55.920 --> 00:29:59.460]   So that's, um, um, did
[00:29:59.460 --> 00:30:01.080]   you see any like, uh,
[00:30:01.080 --> 00:30:04.220]   unexpected behavior in the
[00:30:04.220 --> 00:30:06.200]   self reflection block or did
[00:30:06.200 --> 00:30:07.140]   it like when you're reading
[00:30:07.140 --> 00:30:08.240]   it, does it all like make
[00:30:08.240 --> 00:30:08.580]   sense?
[00:30:08.580 --> 00:30:09.220]   Like, Oh, okay.
[00:30:09.220 --> 00:30:10.020]   It looks like the model
[00:30:10.020 --> 00:30:11.240]   is self-reflecting on this
[00:30:11.240 --> 00:30:12.160]   and then it generates a new
[00:30:12.160 --> 00:30:13.720]   output that is correct kind
[00:30:13.720 --> 00:30:14.020]   of thing.
[00:30:14.020 --> 00:30:16.420]   Yeah.
[00:30:16.420 --> 00:30:17.420]   Um, yeah.
[00:30:17.420 --> 00:30:19.000]   To your first point, like
[00:30:19.000 --> 00:30:20.060]   before I answer this
[00:30:20.060 --> 00:30:20.900]   question, like to your
[00:30:20.900 --> 00:30:24.340]   first point, I think this
[00:30:24.340 --> 00:30:25.920]   falls very neatly into like
[00:30:25.920 --> 00:30:27.620]   this greater research area
[00:30:27.620 --> 00:30:28.360]   of meta prompting.
[00:30:28.360 --> 00:30:29.020]   And I think that's really
[00:30:29.020 --> 00:30:29.280]   cool.
[00:30:29.280 --> 00:30:29.560]   Right.
[00:30:29.560 --> 00:30:30.660]   Because we're telling the
[00:30:30.660 --> 00:30:33.160]   model a very specific way
[00:30:33.160 --> 00:30:34.280]   to use its tokens where
[00:30:34.280 --> 00:30:35.120]   we're saying like create a
[00:30:35.120 --> 00:30:35.700]   self-reflection.
[00:30:35.700 --> 00:30:37.180]   but I think a lot of where
[00:30:37.180 --> 00:30:37.980]   I would love to head with
[00:30:37.980 --> 00:30:40.640]   this is what if you just
[00:30:40.640 --> 00:30:42.460]   give the models more tokens
[00:30:42.460 --> 00:30:43.200]   in general, right.
[00:30:43.200 --> 00:30:43.980]   And give it lots of
[00:30:43.980 --> 00:30:44.680]   different ways that it can
[00:30:44.680 --> 00:30:45.480]   prompt, like, what is it
[00:30:45.480 --> 00:30:46.260]   about self-reflection
[00:30:46.260 --> 00:30:47.520]   specifically, or is it, if
[00:30:47.520 --> 00:30:48.500]   there are nothing specific
[00:30:48.500 --> 00:30:49.500]   about self-reflection, it's
[00:30:49.500 --> 00:30:50.340]   just, yeah, more thinking
[00:30:50.340 --> 00:30:50.720]   spaces.
[00:30:50.720 --> 00:30:52.140]   What is what gets the job
[00:30:52.140 --> 00:30:52.880]   done period, right.
[00:30:52.880 --> 00:30:53.700]   You just give it more
[00:30:53.700 --> 00:30:54.100]   tokens.
[00:30:54.100 --> 00:30:57.540]   Um, so yeah, curious to, I
[00:30:57.540 --> 00:30:58.500]   think like over time,
[00:30:58.500 --> 00:30:59.480]   hopefully like have some
[00:30:59.480 --> 00:31:00.200]   bandwidth to pull that
[00:31:00.200 --> 00:31:01.340]   apart, um, and, and
[00:31:01.340 --> 00:31:03.340]   think about, uh, how
[00:31:03.340 --> 00:31:04.900]   that relates.
[00:31:04.900 --> 00:31:08.840]   I think your question
[00:31:08.840 --> 00:31:11.200]   was, um, sorry, I'm
[00:31:11.200 --> 00:31:12.180]   blanking on your question.
[00:31:12.180 --> 00:31:13.980]   Uh, no, you're good.
[00:31:13.980 --> 00:31:16.220]   The, um, it was, uh, uh,
[00:31:16.220 --> 00:31:18.220]   uh, I don't remember it
[00:31:18.220 --> 00:31:18.660]   now either.
[00:31:18.660 --> 00:31:19.380]   I just had a different,
[00:31:19.380 --> 00:31:20.520]   a different thought.
[00:31:20.520 --> 00:31:23.300]   The, um, did you try, so
[00:31:23.300 --> 00:31:24.440]   task, generate, output,
[00:31:24.440 --> 00:31:25.500]   succeed, do nothing.
[00:31:25.500 --> 00:31:27.480]   And then other side, we
[00:31:27.480 --> 00:31:28.260]   have the chain.
[00:31:28.260 --> 00:31:32.480]   Um, and then for the, for
[00:31:32.480 --> 00:31:33.560]   the benchmarking that you
[00:31:33.560 --> 00:31:36.320]   did after, did you, did you
[00:31:36.320 --> 00:31:37.260]   have the model that you
[00:31:37.260 --> 00:31:38.440]   were benchmarking against
[00:31:38.440 --> 00:31:41.920]   try twice or did you, did
[00:31:41.920 --> 00:31:44.220]   you have the, the, let's
[00:31:44.220 --> 00:31:46.220]   see, uh, performance on
[00:31:46.220 --> 00:31:46.780]   both first.
[00:31:46.860 --> 00:31:47.300]   Okay.
[00:31:47.300 --> 00:31:48.400]   So you did have it just
[00:31:48.400 --> 00:31:50.100]   like fire two attempts and
[00:31:50.100 --> 00:31:52.000]   like without the training and
[00:31:52.000 --> 00:31:52.920]   as opposed to the other
[00:31:52.920 --> 00:31:56.480]   model who got trained to
[00:31:56.480 --> 00:31:58.060]   have two, my ASML.
[00:31:58.060 --> 00:32:00.820]   Oh, wait, go to 900.
[00:32:00.820 --> 00:32:02.080]   Uh, we'll see.
[00:32:02.080 --> 00:32:04.700]   Uh, what, huh?
[00:32:04.700 --> 00:32:09.160]   Um, okay.
[00:32:09.160 --> 00:32:09.760]   Oh, okay.
[00:32:09.760 --> 00:32:10.060]   Got it.
[00:32:10.060 --> 00:32:12.860]   Uh, did the, the, so for the
[00:32:12.860 --> 00:32:13.780]   model, you're benchmarking
[00:32:13.780 --> 00:32:16.120]   against a, does it have both of
[00:32:16.120 --> 00:32:17.320]   its attempts in the con?
[00:32:17.320 --> 00:32:18.160]   Like is the, is the
[00:32:18.160 --> 00:32:19.300]   experiment identical?
[00:32:19.300 --> 00:32:21.580]   Okay.
[00:32:21.580 --> 00:32:22.980]   So, so the, the model that
[00:32:22.980 --> 00:32:25.160]   doesn't have training has two
[00:32:25.160 --> 00:32:26.100]   attempts in the window.
[00:32:26.100 --> 00:32:26.980]   The model that does have
[00:32:26.980 --> 00:32:28.520]   training also has two attempts
[00:32:28.520 --> 00:32:29.140]   in the window.
[00:32:29.140 --> 00:32:30.640]   Model number two.
[00:32:31.080 --> 00:32:35.140]   Was trained, um, to have
[00:32:35.140 --> 00:32:36.000]   two attempts.
[00:32:36.000 --> 00:32:42.080]   And so is being rewarded for
[00:32:42.080 --> 00:32:43.020]   getting it right on the
[00:32:43.020 --> 00:32:43.840]   second attempt.
[00:32:43.840 --> 00:32:46.360]   Does it, did you, did you
[00:32:46.360 --> 00:32:47.840]   benchmark or like see any
[00:32:47.840 --> 00:32:48.980]   meaningful difference in
[00:32:48.980 --> 00:32:50.320]   attempt number one?
[00:32:50.320 --> 00:32:51.680]   Like it's, it's like, it
[00:32:51.680 --> 00:32:52.740]   makes sense to me that like
[00:32:52.740 --> 00:32:53.620]   if you're training for the
[00:32:53.620 --> 00:32:55.520]   shape of two attempts and
[00:32:55.520 --> 00:32:58.760]   succeed, then like it should
[00:32:58.760 --> 00:32:59.600]   be better at that.
[00:33:00.060 --> 00:33:02.720]   Um, yeah, so, so just to
[00:33:02.720 --> 00:33:03.880]   like unravel this table a
[00:33:03.880 --> 00:33:04.480]   little bit more.
[00:33:04.480 --> 00:33:06.720]   So the, the, cause it's,
[00:33:06.720 --> 00:33:08.300]   it's, it's, it's a, it's a
[00:33:08.300 --> 00:33:08.880]   lot of numbers.
[00:33:08.880 --> 00:33:10.380]   Um, okay.
[00:33:10.380 --> 00:33:11.700]   So if we compare vanilla
[00:33:11.700 --> 00:33:13.120]   first try to trained first
[00:33:13.120 --> 00:33:15.880]   try, that's seeing how much
[00:33:15.880 --> 00:33:17.680]   better the model got at pass
[00:33:17.680 --> 00:33:18.660]   at one through this training
[00:33:18.660 --> 00:33:19.060]   process.
[00:33:19.060 --> 00:33:19.480]   Right.
[00:33:19.480 --> 00:33:21.220]   So like first line when to
[00:33:21.220 --> 00:33:24.540]   1.5 billion goes from 32.6%
[00:33:24.540 --> 00:33:27.500]   on a first task try after
[00:33:27.500 --> 00:33:29.640]   training, it's at 48.6%, right?
[00:33:29.640 --> 00:33:30.820]   So ignore the reflection, second
[00:33:30.820 --> 00:33:32.100]   try columns, just look at like
[00:33:32.100 --> 00:33:33.040]   first and third column.
[00:33:33.040 --> 00:33:34.400]   That's how much it got better
[00:33:34.400 --> 00:33:35.920]   at the task itself, which is
[00:33:35.920 --> 00:33:36.820]   actually a really interesting
[00:33:36.820 --> 00:33:37.860]   result because we never
[00:33:37.860 --> 00:33:39.180]   directly incentivize the model
[00:33:39.180 --> 00:33:40.260]   to get better at the task.
[00:33:40.260 --> 00:33:47.120]   We just, uh, we explicitly only
[00:33:47.120 --> 00:33:48.380]   rewarded the self-reflection
[00:33:48.380 --> 00:33:49.160]   tokens, right?
[00:33:49.160 --> 00:33:50.520]   Like, so we were never directly
[00:33:50.520 --> 00:33:53.020]   rewarding the model's answers
[00:33:53.020 --> 00:33:53.860]   and being like, you know, get
[00:33:53.860 --> 00:33:54.480]   better answers.
[00:33:54.480 --> 00:33:56.960]   I think this, there is some
[00:33:56.960 --> 00:33:59.440]   stuff that falls out of like
[00:33:59.440 --> 00:34:00.700]   the spurious rewards paper that
[00:34:00.700 --> 00:34:02.040]   helps kind of explain this, right?
[00:34:02.040 --> 00:34:03.080]   A little bit where it's like,
[00:34:03.080 --> 00:34:04.700]   Hey, like to some extent
[00:34:04.700 --> 00:34:07.120]   exposing the model to data is
[00:34:07.120 --> 00:34:07.600]   what matters.
[00:34:07.600 --> 00:34:08.580]   the reward actually matters
[00:34:08.580 --> 00:34:09.600]   way less than we think it does
[00:34:09.600 --> 00:34:10.800]   specifically for client models.
[00:34:10.800 --> 00:34:15.720]   Um, and then another way of,
[00:34:15.720 --> 00:34:17.640]   or another sort of thing about
[00:34:17.640 --> 00:34:18.920]   to think about what this table,
[00:34:18.920 --> 00:34:21.220]   right, is another column we
[00:34:21.220 --> 00:34:21.940]   could have had here.
[00:34:21.940 --> 00:34:23.080]   We could have had two more
[00:34:23.080 --> 00:34:24.040]   columns, which is basically the
[00:34:24.040 --> 00:34:26.220]   diff, um, between the vanilla
[00:34:26.220 --> 00:34:27.220]   first try and the reflection
[00:34:27.220 --> 00:34:28.680]   second try before training and
[00:34:28.680 --> 00:34:29.500]   after training, right?
[00:34:29.500 --> 00:34:31.080]   So if you look at, again, the
[00:34:31.080 --> 00:34:32.540]   first row, you can see that we
[00:34:32.540 --> 00:34:37.500]   go, um, so 32.6% pass at one
[00:34:37.500 --> 00:34:38.960]   on the vanilla model.
[00:34:38.960 --> 00:34:39.840]   And then when you give it that
[00:34:39.840 --> 00:34:42.040]   second try, it goes up to 34.8%,
[00:34:42.040 --> 00:34:42.320]   right?
[00:34:42.320 --> 00:34:45.200]   So that's 2.2% better, right?
[00:34:45.200 --> 00:34:48.140]   And if we go to the two columns
[00:34:48.140 --> 00:34:50.540]   on the right, we go from 48.6%
[00:34:50.540 --> 00:34:53.120]   to 52.9%, which is more than 2.2.
[00:34:53.120 --> 00:34:56.180]   It's, uh, 4.3%, right?
[00:34:56.180 --> 00:34:57.920]   So the model has gotten better at
[00:34:57.920 --> 00:34:59.620]   utilizing that self, like the
[00:34:59.620 --> 00:35:00.640]   self-reflections are better.
[00:35:00.640 --> 00:35:05.300]   And so that second try is right
[00:35:05.300 --> 00:35:07.000]   now, more of the time, right?
[00:35:07.000 --> 00:35:09.040]   You have like, you, we went from
[00:35:09.040 --> 00:35:12.160]   reflection and a second try gives
[00:35:12.160 --> 00:35:14.480]   us 2.2% improvement to that, that
[00:35:14.480 --> 00:35:15.760]   prompt by the end of training now
[00:35:15.760 --> 00:35:17.780]   gives us 4.3% improvement on the
[00:35:17.780 --> 00:35:18.080]   metrics.
[00:35:18.080 --> 00:35:19.780]   So that's kind of how you could, you
[00:35:19.780 --> 00:35:22.300]   could see that, uh, the self-reflections
[00:35:22.300 --> 00:35:23.400]   are quote unquote better, right?
[00:35:23.400 --> 00:35:24.200]   Uh, objectively.
[00:35:24.200 --> 00:35:27.040]   I'm curious, did you evaluate what is
[00:35:27.040 --> 00:35:28.100]   good self-reflection?
[00:35:28.440 --> 00:35:29.680]   How did you induce that?
[00:35:29.680 --> 00:35:31.600]   Yeah.
[00:35:31.600 --> 00:35:34.640]   So I think for us, we really looked
[00:35:34.640 --> 00:35:37.880]   at it qualitatively, um, cause it is
[00:35:37.880 --> 00:35:39.320]   interesting to like, sort of see
[00:35:39.320 --> 00:35:43.300]   what self-reflections lead to that
[00:35:43.300 --> 00:35:44.060]   improvement, right?
[00:35:44.060 --> 00:35:46.520]   And we didn't encourage any specific
[00:35:46.520 --> 00:35:47.300]   format or anything.
[00:35:47.300 --> 00:35:50.500]   We just, uh, rewarded it when a
[00:35:50.500 --> 00:35:52.080]   self-reflection led to success and
[00:35:52.080 --> 00:35:52.700]   saw what happened.
[00:35:52.700 --> 00:35:54.400]   And actually, I think this was one of
[00:35:54.400 --> 00:35:56.540]   the, like earlier questions, uh, the
[00:35:56.540 --> 00:35:58.340]   self-reflections of selves in
[00:35:58.340 --> 00:36:00.080]   kind of clean language mixes a lot,
[00:36:00.080 --> 00:36:00.980]   like a lot, a lot.
[00:36:00.980 --> 00:36:02.340]   And sometimes it was just like pure
[00:36:02.340 --> 00:36:03.900]   gibberish for some models sometimes.
[00:36:03.900 --> 00:36:05.740]   Um, so there is like definitely
[00:36:05.740 --> 00:36:08.040]   evidence that like more thinking space
[00:36:08.040 --> 00:36:09.300]   of kind of what the models need as
[00:36:09.300 --> 00:36:10.140]   opposed to these like very
[00:36:10.140 --> 00:36:13.340]   parsable human, uh, legible self-
[00:36:13.340 --> 00:36:14.000]   reflections.
[00:36:14.000 --> 00:36:17.080]   Um, but qualitatively they do sort of
[00:36:17.080 --> 00:36:18.880]   seem like quote unquote better self-
[00:36:18.880 --> 00:36:20.680]   reflections or like more effective,
[00:36:20.680 --> 00:36:23.200]   more efficient over time, um, in many
[00:36:23.200 --> 00:36:23.600]   cases.
[00:36:23.600 --> 00:36:25.040]   So that was kind of cool to see, but
[00:36:25.040 --> 00:36:26.200]   because we didn't have any format
[00:36:26.200 --> 00:36:28.540]   constraints, um, yeah.
[00:36:28.540 --> 00:36:29.840]   Sometimes it was gibberish,
[00:36:29.840 --> 00:36:31.800]   particularly again, when models and
[00:36:31.800 --> 00:36:33.200]   the language stuff is very
[00:36:33.200 --> 00:36:33.740]   interesting.
[00:36:33.740 --> 00:36:39.800]   I think Frankie has their hand up.
[00:36:43.400 --> 00:37:01.200]   I think it'd be helpful to go back to tasks.
[00:37:01.200 --> 00:37:02.540]   um, the success field.
[00:37:02.540 --> 00:37:05.460]   Um, what, what's actually put there?
[00:37:05.460 --> 00:37:06.800]   Yeah.
[00:37:06.800 --> 00:37:10.260]   So, um, I think it'd be helpful to go back to tasks.
[00:37:10.260 --> 00:37:19.200]   So for the function calling dataset, uh, we cheated a little bit in that we like, you theoretically don't need a ground truth dataset, right? You could do something where it's a simple as like, did this function call, uh, create a request that when it hits an API, it gets you back like a two-one.
[00:37:19.200 --> 00:37:32.540]   Uh, like those sorts of things, but in our case, we did actually check to see if the correct answer is the answer from the ground truth dataset because we were like using a, uh, SFT dataset.
[00:37:32.540 --> 00:37:56.780]   But generally speaking, for function calling, if you have any sort of binary reward checker, like any way of saying, like, I think this was a good function call versus a bad function call, you should be able to like, um, do this with countdown.
[00:37:56.780 --> 00:38:14.240]   With countdown, this was a little bit more of like a true verifier, um, because, you know, like many math questions, it's very easy to check if a particular equation that the model has generated evaluates to the right number, but it's like hard to generate all the answers, right?
[00:38:14.240 --> 00:38:15.560]   Like there's many possible answers.
[00:38:15.560 --> 00:38:24.400]   So what we did here is quite literally ran, like, so we checked to make sure like the numbers that were allowed were the numbers in the equation that the model wrote.
[00:38:24.400 --> 00:38:28.060]   Um, and then we just ran eval on it and like, saw if it hit the target number.
[00:38:28.060 --> 00:38:32.500]   So it was just like a very basic, like evaluate the function and see if there's success.
[00:38:32.500 --> 00:38:33.040]   Yeah.
[00:38:33.040 --> 00:38:34.120]   Go back to the background.
[00:38:34.120 --> 00:38:34.400]   Sorry.
[00:38:34.400 --> 00:38:36.520]   I just wanted to follow along just couple of questions there.
[00:38:36.520 --> 00:38:40.480]   So on that first block to the right of fail, generate self-reflection.
[00:38:40.480 --> 00:38:43.480]   What are you adding directly there when you get a failure?
[00:38:43.480 --> 00:38:47.960]   So we have a prompt that is in the paper.
[00:38:47.960 --> 00:38:51.200]   There's prompt templates at the bottom, um, just prompt.
[00:38:51.340 --> 00:38:55.300]   And then it generates a self-reflection and then you, uh, prompt the question.
[00:38:55.300 --> 00:38:58.540]   You like put the, the original question back in to retry.
[00:38:58.540 --> 00:38:58.900]   Got you.
[00:38:58.900 --> 00:38:59.380]   Okay.
[00:38:59.380 --> 00:39:12.240]   So then, um, since you follow the success path of the second retry, that is, you got the correct, your, uh, your verifier knows that you got the correct answer.
[00:39:12.360 --> 00:39:15.300]   And then you're going to say reward the self-reflection tokens.
[00:39:15.300 --> 00:39:18.240]   So which tokens specifically are you rewarding?
[00:39:18.240 --> 00:39:23.340]   Just the fact that on that first fail, you generated some new tokens from that.
[00:39:23.340 --> 00:39:26.940]   That's, um, that's what you're rewarding for that particular path.
[00:39:26.940 --> 00:39:28.800]   Yeah, exactly.
[00:39:28.800 --> 00:39:39.340]   So after the failure, we prompt for self-reflection and the prompt is something like try again, like you got the wrong answer, please reflect on what you did wrong.
[00:39:39.340 --> 00:39:42.360]   So you can get the right answer next time or something like the prompt is something like that.
[00:39:42.360 --> 00:39:47.120]   And then whatever the model answers directly after that, that's exactly what we reward.
[00:39:48.080 --> 00:39:48.360]   Okay.
[00:39:48.360 --> 00:39:51.460]   And, uh, and you did not do anything with the failed path.
[00:39:51.460 --> 00:39:53.300]   That is not at all.
[00:39:53.300 --> 00:39:53.720]   Yeah.
[00:39:53.720 --> 00:39:54.860]   Why not?
[00:39:54.860 --> 00:40:00.560]   Because you feel like that's not useful to say negative reward or something.
[00:40:00.560 --> 00:40:03.140]   Yeah, pretty much, pretty much.
[00:40:03.140 --> 00:40:08.000]   I mean, I think we found pretty early on that this very simplistic reward formulation worked quite well.
[00:40:08.000 --> 00:40:12.380]   And so we didn't do a lot of work on like exploring alternate reward formulations.
[00:40:12.380 --> 00:40:17.360]   Um, cause I think this one also feels very, I think it's a team.
[00:40:17.360 --> 00:40:20.720]   And we just like really like these like simple intuitive approaches, right?
[00:40:20.720 --> 00:40:24.080]   Like fail retry success is a good thing, right?
[00:40:24.080 --> 00:40:25.640]   Um, fail retry fail.
[00:40:25.640 --> 00:40:30.140]   Isn't necessarily a bad thing because maybe the question is impossible for everyone.
[00:40:30.140 --> 00:40:30.480]   Right.
[00:40:30.480 --> 00:40:32.760]   Like, um, yes.
[00:40:32.760 --> 00:40:44.240]   So I just, I'm curious to think about the logic here, meaning that you're rewarding something that comes as a result of the follow-up prompt that says you failed, right?
[00:40:44.240 --> 00:40:46.220]   Please try this problem again.
[00:40:46.220 --> 00:40:48.620]   And you're rewarding that particular path.
[00:40:48.620 --> 00:40:51.380]   So I'm trying to understand what is it?
[00:40:51.380 --> 00:40:51.580]   Sorry.
[00:40:51.580 --> 00:40:52.880]   Maybe I don't understand.
[00:40:52.880 --> 00:40:58.920]   Like you did this, but then on your, your table, you said, Oh, we did this training.
[00:40:59.300 --> 00:41:04.520]   And that first train pass kind of like got 32 to 48%, right?
[00:41:04.520 --> 00:41:08.460]   So I'm trying to understand what is actually, what do you think is going on there, right?
[00:41:08.460 --> 00:41:11.800]   Because it's a very, you're rewarding something that doesn't quite.
[00:41:11.800 --> 00:41:12.020]   Yeah.
[00:41:12.020 --> 00:41:13.740]   And I want them to understand.
[00:41:13.740 --> 00:41:14.240]   Yeah.
[00:41:14.240 --> 00:41:16.380]   Yeah, that's a really good question.
[00:41:16.380 --> 00:41:21.680]   I think that, uh, we don't super know.
[00:41:21.680 --> 00:41:29.440]   And I think like some of the new papers that have been coming out recently also speak to like in general, when we do RL, we don't super know what's going on.
[00:41:29.440 --> 00:41:40.840]   Um, and in particular, I think this first paper starts to unlock partially what might be going on in that interestingly, like where word functions don't have a lot to do with, uh,
[00:41:43.840 --> 00:41:45.760]   whether or not these models get better, right?
[00:41:45.760 --> 00:41:58.840]   Like the exact formulation of reward has, even if it has very little correlation with the right answer and like, just the fact that we're rewarding in the space of the right answer, just because we're wearing other tokens, like it sort of doesn't matter.
[00:41:58.840 --> 00:42:04.480]   It's just kind of just exposure to the data and any sort of reward sometimes is leading to pass at one improvement as well.
[00:42:04.480 --> 00:42:12.820]   So can I conjecture that, uh, really when you're rewarding that second attempt, you're just rewarding a better response, right?
[00:42:12.820 --> 00:42:15.280]   Because you actually put in the same question to it again.
[00:42:15.280 --> 00:42:17.620]   So you're just rewarding the fact that
[00:42:17.620 --> 00:42:22.000]   They also want to survive just in order to achieve the goals we set.
[00:42:22.000 --> 00:42:34.960]   So here's an example from Apollo research of a super intelligent, not a super intelligent, so, um, very unsuper intelligence, but something that's moderately intelligent.
[00:42:34.960 --> 00:42:48.880]   Sorry, maybe I, maybe I've misunderstood that you're, uh, the main point that you're bringing up. Can you repeat that again?
[00:43:01.960 --> 00:43:06.760]   That sounded like a TTS thing. Like, I think you guys can continue with whatever conversation was going on before.
[00:43:06.760 --> 00:43:08.920]   Because it's like unmuted. Yeah.
[00:43:08.920 --> 00:43:15.480]   Okay, sorry. Yeah. So going back to my question, it feels like you're just rewarding for a better answer in some sense, because
[00:43:17.080 --> 00:43:26.380]   I guess having the fact that, you know, the same question, right, was posed on the retry and it gave a response and you like that response better than the first one.
[00:43:27.220 --> 00:43:33.480]   Uh, you're kind of like rewarding that, uh, the fact that it got closer to the answer because you only be working on the success path.
[00:43:33.480 --> 00:43:35.860]   Right. So, yeah.
[00:43:36.660 --> 00:43:53.940]   Yeah. And I think, and I think, and I think generally speaking, so, so yes, although I think that a thing that is still kind of interesting is that we are not rewarding the answer token directly, but I do think in practice what happens is often self-reflections have the right answer somewhere in them.
[00:43:54.340 --> 00:44:07.220]   Um, and so the answer is leaking into the self-reflection. And so then when we reward the self-reflection token, at times we are rewarding the answer. Um, because a lot of, a lot of the self-reflections
[00:44:07.220 --> 00:44:10.420]   answer the question. Yeah.
[00:44:10.980 --> 00:44:26.820]   Have you noticed like, sorry, sorry to interrupt again. Uh, have you noticed, have you noticed that the response got actually maybe longer or, or have you had just a metrics that to, to figure it out? Like after you did your training, what's the quality of the responses that that 48% column?
[00:44:26.820 --> 00:44:30.540]   Uh, have you like done any like simple metrics on those?
[00:44:30.540 --> 00:44:40.820]   Honestly, I haven't, but I should, there is an error analysis, um, section of the paper that mostly discusses like how errors have changed pre and post-training.
[00:44:40.820 --> 00:44:55.220]   Like what the sorts of mistakes models make are, um, that I found pretty interesting. But, but no, like, I think like speed to response, like how many tokens does it take to get the right answer? Hopefully we would see, it would be lower, uh, over time. So yeah, that'd be cool to look at.
[00:44:55.220 --> 00:45:10.180]   Well, yeah, I, I, if you saved that information, like if you, if you still have the traces of the runs, I would be quite interested to see. Cause I, what I, what I would be, would be inclined to check is just the raw number of tokens count. Cause my suspicion would be the, like,
[00:45:10.660 --> 00:45:16.100]   if that number is larger than you are going to see the perf increase. If it's lower than you don't would be my, my guess.
[00:45:16.100 --> 00:45:19.700]   Yeah. It makes sense.
[00:45:19.700 --> 00:45:32.820]   I mean, but, uh, an alternative hypothesis here is that the self-reflection induces the model to early on in the, in the answer process to use.
[00:45:33.780 --> 00:45:42.260]   Like sort of that, the language from the self-reflections in its initial response. Right. And so they then, and then induces
[00:45:42.260 --> 00:45:45.060]   a better reasoning process about the answer.
[00:45:45.940 --> 00:45:53.780]   So that, I mean, that would be the, like what you would hope would happen. So like maybe that is an, I think a credible alternative hypothesis here.
[00:45:56.500 --> 00:46:01.940]   for the initial prompt, did you ask for a chain of thought prior to, or just say like one shot this for
[00:46:01.940 --> 00:46:07.300]   me kind of thing. Yeah. These were all one shot because a lot of these models were early enough that
[00:46:07.300 --> 00:46:14.020]   like, yeah, chain of thought prompting or like models specifically optimized for train of thought prompting
[00:46:14.020 --> 00:46:19.620]   wasn't as much of a thing yet. Like, because our, um, the function calling data set was June, 2024.
[00:46:19.620 --> 00:46:25.300]   And so we were using Quintu models, which are not necessarily super optimized for reasoning because
[00:46:25.300 --> 00:46:30.020]   the issue is that a lot of these models were trained on this data set, right? Like when you have a really
[00:46:30.020 --> 00:46:35.140]   high quality data set and it's open source and it's public, like model companies just swallow it right
[00:46:35.140 --> 00:46:38.340]   pretty quickly. And then all of your results are skewed because it's like, okay, we're training on data
[00:46:38.340 --> 00:46:41.860]   that it's already seen. And so like, how much is this training recipe actually valid versus just like
[00:46:41.860 --> 00:46:47.300]   reinforcing something that already exists that you SFT on. So we wanted to keep the data really pure.
[00:46:47.300 --> 00:46:49.060]   And so, yeah, there, there are slightly older models.
[00:46:49.060 --> 00:47:00.340]   So, Shelley, I had another question, um, directly from the paper. So, um, there's a, in section 4.3,
[00:47:00.340 --> 00:47:10.740]   and you're talking about the, um, you know, sort of decision to, to, um, emphasize, uh, um, you know,
[00:47:10.740 --> 00:47:19.060]   sort of failure only path. And, and you say in the last sentence, it, um, it is other function, otherwise
[00:47:19.060 --> 00:47:24.500]   functionally equivalent to learn from a real world scenario where we believe we receive both successes
[00:47:24.500 --> 00:47:31.540]   and failed responses. And I, I, that, that seems unintuitive to me because it seems like if you use the
[00:47:31.540 --> 00:47:38.340]   successes that you're going to be like maybe overtraining on the successful response and therefore,
[00:47:38.340 --> 00:47:46.500]   um, you might have more catastrophic forgetting. So I wanted to, I wanted to hear what you have to say
[00:47:46.500 --> 00:47:52.580]   about that. Um, is section 4.3, the part that discusses the failure data set?
[00:47:52.580 --> 00:48:00.900]   Yeah, yeah, exactly. Okay, cool. Yeah. So for context for everyone here, because I didn't really
[00:48:00.900 --> 00:48:08.180]   talk about this. One of the things we did to make GRPO training a lot more tractable was we pre-generated
[00:48:08.180 --> 00:48:18.420]   a data set of failures, right? So like this whole top half, you can do offline and then you just do
[00:48:18.420 --> 00:48:25.700]   GRPO on the, on the second half. Right. Um, and the reason that we did that is because we were seeing,
[00:48:25.700 --> 00:48:31.380]   if you run full-scale GRPO with this entire pathway, it was very, very, it was a very low number of
[00:48:31.380 --> 00:48:37.860]   trajectories that specifically hit failure retry success initially. And so it was really
[00:48:37.860 --> 00:48:42.180]   incredibly resource intensive for, for what it is. Right. And so instead of what we did was we just
[00:48:42.180 --> 00:48:53.700]   like offline stored, um, task, task prompt output, like pairs in the case of failure. Um, and, and what
[00:48:53.700 --> 00:48:59.300]   we gauged is basically like, yeah, this is functionally equivalent in our opinion to, uh, or the way we
[00:48:59.300 --> 00:49:05.140]   were training this was, was very similar to if you didn't do this offline thing, but you're right, RJ.
[00:49:05.140 --> 00:49:09.540]   And that like, there is actually differences because you could, the model could drift over
[00:49:09.540 --> 00:49:15.300]   time. And like, we're anchoring on this offline dataset that was generated from like model at step
[00:49:15.300 --> 00:49:22.580]   zero. Right. We're not adapting to models over time, potentially having new failures. Right. And so there
[00:49:22.580 --> 00:49:33.700]   isn't a lot of preventative stuff in place here to prevent catastrophic forgetting with respect to other
[00:49:33.700 --> 00:49:41.060]   data points in the training dataset. Right. I think our sense was that that wasn't as big of an, of a problem,
[00:49:41.060 --> 00:49:46.900]   especially since we saw low catastrophic forgetting in general. Um, and then of course, like when you evaluate,
[00:49:46.900 --> 00:49:50.420]   you see that like, no matter what, you are strictly better at the task than you were before. But I
[00:49:50.420 --> 00:49:53.780]   think it's definitely possible that for certain tasks, you could see this thing happen where like,
[00:49:53.780 --> 00:49:59.140]   as you train things that you were succeeding at before have somehow you started failing at. And this,
[00:49:59.140 --> 00:50:04.420]   this offline dataset, you're correct. When you capture that, I actually, um, that's a good point.
[00:50:04.420 --> 00:50:09.460]   I was actually thinking the opposite that if it seems like this is an important feature of your
[00:50:09.460 --> 00:50:15.540]   methodology and not just a functional, I, it seems like a functional feature of the methodology
[00:50:15.540 --> 00:50:21.860]   because you're really only you're, you're basically saying things that I used to get wrong. I get like,
[00:50:21.860 --> 00:50:28.180]   I, and now I got right by re-prompting. Right. And so that, that, and you're basically identifying
[00:50:28.180 --> 00:50:33.460]   that it's very specific subset as the important thing to train on. Whereas if you were to use the
[00:50:34.340 --> 00:50:39.780]   the successes as well, then you wouldn't have honed in on that specific subset.
[00:50:39.780 --> 00:50:44.980]   Yeah. That makes a lot of sense. Like we could be rewarding first try success as well, pretty
[00:50:44.980 --> 00:50:53.060]   continuously. Um, yeah, I think our, our approach, like we were really keen on this idea that we can
[00:50:53.060 --> 00:50:58.820]   do this like meta learning, like don't specialize for a specific task, like just incentivize self-reflection.
[00:50:58.820 --> 00:51:02.980]   And so if we were rewarding initial successes, there's, we're rewarding the task. We're not
[00:51:02.980 --> 00:51:09.060]   rewarding self-reflection ability. Um, but I agree that there are like a lot of ways to extend this,
[00:51:09.060 --> 00:51:14.180]   to like both reward the task and self-reflection capability, and hopefully see both things get
[00:51:14.180 --> 00:51:20.660]   better and potentially like you get better at the task faster. Cool. Um, Ted, go ahead.
[00:51:20.660 --> 00:51:26.580]   Hey, thanks again, Shelley, for, for joining and, and coming and discussing this. I hope this is a
[00:51:26.580 --> 00:51:34.500]   quick question. Um, can you say how you formed your batches when you were doing GRPO? Did you like mix
[00:51:35.060 --> 00:51:44.100]   the original and the new success or, or, and did you just randomly permute them or shuffle them or do
[00:51:44.100 --> 00:51:51.380]   anything special? It was pretty random, honestly. Um, I think we stuck to like between eight and 14
[00:51:51.380 --> 00:51:58.980]   generations per failure. So, um, tasks to generate output to fail, what would happen is for each task.
[00:51:59.780 --> 00:52:05.620]   I want to say we generated it. So the number of times we generated pathways for that first attempt
[00:52:05.620 --> 00:52:12.340]   to vary depending on model, um, capability, right? Smaller models, you give them less tries because
[00:52:12.340 --> 00:52:16.420]   actually more of them are failures. So we gathered the failure data, data, data set by just, um,
[00:52:16.420 --> 00:52:20.820]   generating a bunch of times and saving the ones that were failures. And then, yeah, with the actual
[00:52:20.820 --> 00:52:26.820]   GRPO training, um, yeah, I would say nothing special, like between 18, eight and 14 generations,
[00:52:26.820 --> 00:52:32.340]   not a lot of mixing, um, pretty, pretty standard GRPO training, especially since like, again, this
[00:52:32.340 --> 00:52:35.860]   was like February, March. And I feel like we, as a community, we're still like figuring out GRPO,
[00:52:35.860 --> 00:52:41.140]   um, or at least I personally was. Um, and so like, there's also this thing in the paper where it's like,
[00:52:41.140 --> 00:52:44.980]   oh, and we kind of period on less than 10 billion parameters. And like, there was an infrastructure to
[00:52:44.980 --> 00:52:51.700]   train on more than like, there was no multi-node implementation of GRPO publicly available until
[00:52:52.500 --> 00:52:57.940]   after I ran these experiments. So yeah, it's, uh, it's a process for sure. I'm sure that there are
[00:52:57.940 --> 00:53:03.620]   many papers that have come out since that would optimize specifically the GRPO approach. Yeah.
[00:53:03.620 --> 00:53:05.220]   Cool. Thanks.
[00:53:05.220 --> 00:53:07.860]   Okay. Yeah, of course we have, we have one more question.
[00:53:07.860 --> 00:53:09.860]   Vishvesh?
[00:53:09.860 --> 00:53:14.980]   Uh, hi, Shelly. Yeah. Thank you for the presentation. Uh, I was just thinking about, uh,
[00:53:14.980 --> 00:53:22.500]   uh, uh, I was thinking about your motivation that you want to learn self-reflection process rather
[00:53:22.500 --> 00:53:34.980]   than the specific task. So, looking at this particular experiment in this setup, this would be a very good,
[00:53:34.980 --> 00:53:45.060]   uh, uh, like, uh, like, do you think that this is a good ablation would be to mix and rejected pairs
[00:53:45.060 --> 00:53:50.740]   for the particular setup that now, now the first field like success that I can make a chosen repair
[00:53:50.740 --> 00:53:59.460]   and then can do prep, a direct prep optimization to compare my, it is focusing on self-reflection and
[00:53:59.460 --> 00:54:05.700]   not on the task, like some, some form of ablation where I can also have post-training, uh, to compare
[00:54:05.700 --> 00:54:12.740]   whether just, uh, just, uh, rewarding my self-reflection token is, uh, is the best way forward.
[00:54:12.740 --> 00:54:18.020]   Yeah. I, okay. So I didn't quite catch all of that because I think maybe your connection or my
[00:54:18.020 --> 00:54:23.140]   connection wasn't amazing for a bit there, but what I caught was basically ablation studies around
[00:54:23.140 --> 00:54:29.380]   comparing, uh, rewarding self-reflection specifically to rewarding both self-reflection
[00:54:29.380 --> 00:54:34.500]   and the task or just rewarding the task itself and seeing how performance changes. Um, yeah,
[00:54:34.500 --> 00:54:37.940]   super agree that that would be an interesting ablation. I think like we pretty intentionally
[00:54:37.940 --> 00:54:43.300]   in this paper stepped away from directly comparing to things like, uh, like other reward functions and
[00:54:43.300 --> 00:54:47.860]   instead approach it as let's compare to larger models, right? Let's use our baselines to be like,
[00:54:47.860 --> 00:54:53.860]   how much can this training recipe bring us up to, um, bigger models. But I super agree that like,
[00:54:53.860 --> 00:55:02.660]   head-to-head this approach versus standard GP, GRPO where you reward the answer or other, other like,
[00:55:02.660 --> 00:55:06.260]   combine it with both or whatever. Like, yeah, that would be very interesting and, and, um,
[00:55:06.260 --> 00:55:10.900]   hopefully something the writer team can get to, uh, in the next few months or anyone else.
[00:55:10.900 --> 00:55:18.500]   Cool. Awesome. Just one more, one more point. Uh, do you plan to make the code open source anytime
[00:55:18.500 --> 00:55:22.980]   after like maybe you've submitted somewhere and then you plan to do it? If you. Yes. The,
[00:55:22.980 --> 00:55:29.540]   the goal is to definitely make the code open source. Um, yeah, we are waiting, um, on a few things before
[00:55:29.540 --> 00:55:37.780]   we do that. Um, but we did actually document, in my opinion, like relatively well, hopefully, uh, within
[00:55:37.780 --> 00:55:41.940]   the paper, how we did this is actually a pretty straightforward modification. And so, um, to like
[00:55:41.940 --> 00:55:47.220]   open source libraries. And so, um, definitely encourage people to just try it out and implement
[00:55:47.220 --> 00:55:52.420]   it. And of course, email me if they have questions and I'm happy to help, but, um, yeah, super happy to,
[00:55:52.420 --> 00:55:59.780]   to answer. Um, but hopefully all the pieces should be there, uh, where if you would like to implement on
[00:55:59.780 --> 00:56:10.660]   your own, you can, but yeah, eventually, hopefully we will release the code as well.
[00:56:10.660 --> 00:56:13.060]   Thank you so much. Awesome.
[00:56:13.060 --> 00:56:17.620]   Sam, do you want to close this out?
[00:56:17.620 --> 00:56:24.180]   Uh, I don't have much to say. Just thanks. Thanks a lot, Shelly. I really appreciate it. And
[00:56:24.180 --> 00:56:26.980]   thanks everybody for joining and asking such great questions.
[00:56:26.980 --> 00:56:31.220]   And also vote on hugging face apparently. Yes. And also vote on hugging face.
[00:56:31.220 --> 00:56:34.740]   I didn't know. I didn't know they had voting. That's, uh,
[00:56:34.740 --> 00:56:38.820]   Yeah. They have paper of the day. It's run. Yeah. It's paper of the day and then paper of the week
[00:56:38.820 --> 00:56:41.780]   and then paper of the month. So shameless self-promotion.
[00:56:41.780 --> 00:56:47.140]   Yeah. It's still time. Still got time. All right. You got two or nine votes now.
[00:56:47.140 --> 00:56:53.540]   Okay. Well, um, I'll drop the links in the YouTube. Thanks everyone. Thanks everybody.
[00:56:54.660 --> 00:56:55.500]   Thank you, everyone.
[00:56:55.500 --> 00:56:56.680]   Bye.
[00:56:56.680 --> 00:56:56.840]   Bye.
[00:56:56.840 --> 00:56:57.980]   Bye.

