
[00:00:00.000 --> 00:00:03.000]   we had our first beginner-friendly paper reading group.
[00:00:03.000 --> 00:00:06.000]   So again, if you go to the Weights & Biases forum,
[00:00:06.000 --> 00:00:09.000]   so this is where I am, if you just search for community...
[00:00:09.000 --> 00:00:11.000]   By the way, everyone can see my screen, right?
[00:00:11.000 --> 00:00:13.000]   So I'm not doing anything wrong.
[00:00:13.000 --> 00:00:17.000]   If you could please just share in the chat, that would be wonderful.
[00:00:17.000 --> 00:00:18.000]   OK, perfect.
[00:00:18.000 --> 00:00:23.000]   So if I go to the community.mondv.ai, just that link,
[00:00:23.000 --> 00:00:25.000]   I'll paste that in the chat as well.
[00:00:28.000 --> 00:00:31.000]   So if I go to that forums link, then you'll see...
[00:00:31.000 --> 00:00:34.000]   So now we're at that stage where the Fastbook reading group
[00:00:34.000 --> 00:00:37.000]   and the paper reading group are kind of very...
[00:00:37.000 --> 00:00:40.000]   at that point where they both kind of collide with each other.
[00:00:40.000 --> 00:00:43.000]   So if I go to paper reading group and I go...
[00:00:43.000 --> 00:00:46.000]   There's two... You'll find these two threads, which is ResNet.
[00:00:46.000 --> 00:00:50.000]   So just as of yesterday, we discussed the ResNet paper in detail.
[00:00:50.000 --> 00:00:54.000]   So we went through the whole paper, we looked at and read the research paper
[00:00:54.000 --> 00:00:57.000]   and kind of understood what's going on in the ResNet architecture.
[00:00:57.000 --> 00:01:01.000]   And there were a few people from Fastbook who attended that session.
[00:01:01.000 --> 00:01:03.000]   So I'm really happy for those that did.
[00:01:03.000 --> 00:01:10.000]   And kind of now my appeal to everybody who's been part of Fastbook is this.
[00:01:10.000 --> 00:01:14.000]   Now we're at that stage where we can all start reading papers.
[00:01:14.000 --> 00:01:16.000]   We already have a good platform set.
[00:01:16.000 --> 00:01:19.000]   So this is kind of how my journey also started,
[00:01:19.000 --> 00:01:23.000]   or this is how I kind of progressed in my journey as well,
[00:01:23.000 --> 00:01:28.000]   is like Fast.ai gave me that solid baseline and that solid platform.
[00:01:28.000 --> 00:01:30.000]   And then after that, I started reading papers.
[00:01:30.000 --> 00:01:34.000]   I started trying to code a bit more things from scratch.
[00:01:34.000 --> 00:01:37.000]   And this is how then you just start developing things on top
[00:01:37.000 --> 00:01:39.000]   and you start developing that knowledge.
[00:01:39.000 --> 00:01:43.000]   And I think that's the same advice that I would have for everybody
[00:01:43.000 --> 00:01:45.000]   who's part of the Fastbook reading group,
[00:01:45.000 --> 00:01:49.000]   is that now you're at that stage where you can start reading papers.
[00:01:49.000 --> 00:01:54.000]   And something I do want to point out is the next Wednesday,
[00:01:54.000 --> 00:01:58.000]   so Wednesday the -- yes, Wednesday 15th, not next,
[00:01:58.000 --> 00:02:00.000]   but just the coming Wednesday 15th,
[00:02:00.000 --> 00:02:04.000]   what we're doing is we're doing ResNet live coding in PyTorch.
[00:02:04.000 --> 00:02:07.000]   So something I'm going to do, today we're going to look at ResNet
[00:02:07.000 --> 00:02:10.000]   and we're going to look at ResNet in Fast.ai,
[00:02:10.000 --> 00:02:13.000]   and we are going to code it from scratch,
[00:02:13.000 --> 00:02:15.000]   but then we're still using the Fast.ai framework.
[00:02:15.000 --> 00:02:19.000]   And something I do want to do on the side as well is that I want to take
[00:02:19.000 --> 00:02:24.000]   the Fast.ai folks to the paper reading groups and then also show how
[00:02:24.000 --> 00:02:26.000]   these things can be done in pure PyTorch.
[00:02:26.000 --> 00:02:28.000]   So then that just helps with the -- you know,
[00:02:28.000 --> 00:02:31.000]   when you do things from scratch, like PyTorch is like this scratch,
[00:02:31.000 --> 00:02:35.000]   Fast.ai is this layer on top that makes things a lot, lot easier.
[00:02:35.000 --> 00:02:36.000]   And then you have these papers.
[00:02:36.000 --> 00:02:38.000]   So you can implement the papers in Fast.ai,
[00:02:38.000 --> 00:02:40.000]   you can implement the papers in PyTorch.
[00:02:40.000 --> 00:02:43.000]   But I think if you can do it both,
[00:02:43.000 --> 00:02:47.000]   that's where you understand everything about -- everything there is to
[00:02:47.000 --> 00:02:50.000]   know about a particular training loop, a particular paper.
[00:02:50.000 --> 00:02:54.000]   So this is something I want to try in -- like between our paper reading
[00:02:54.000 --> 00:02:57.000]   groups now, we will have these live coding sessions where I'll be live
[00:02:57.000 --> 00:02:59.000]   coding things from scratch.
[00:02:59.000 --> 00:03:03.000]   It will be -- it won't be as perfect because it's a live coding session,
[00:03:03.000 --> 00:03:06.000]   so I'm never perfect when it comes to live coding.
[00:03:06.000 --> 00:03:09.000]   But I guess that's the point.
[00:03:09.000 --> 00:03:11.000]   Like you'll see me making errors.
[00:03:11.000 --> 00:03:14.000]   And when you do see me making those errors,
[00:03:14.000 --> 00:03:17.000]   you will also see me how I try and fix those errors.
[00:03:17.000 --> 00:03:23.000]   So I think that will help everybody in their journeys when you also hit
[00:03:23.000 --> 00:03:26.000]   similar errors in your -- when you're trying to replicate things from
[00:03:26.000 --> 00:03:27.000]   scratch.
[00:03:27.000 --> 00:03:33.000]   And so to summarize, I guess please start reading more papers.
[00:03:33.000 --> 00:03:37.000]   And then we're also doing these live coding sessions that will help you.
[00:03:37.000 --> 00:03:43.000]   As usual, as every week, if you go to 1db.me/fastbook14.
[00:03:43.000 --> 00:03:51.000]   So let me go 1db.me/fastbook14.
[00:03:51.000 --> 00:03:54.000]   So that brings me to this discussion thread for today.
[00:03:54.000 --> 00:03:56.000]   So we're going to have all our discussions today.
[00:03:56.000 --> 00:03:57.000]   Somebody is already typing.
[00:03:57.000 --> 00:03:59.000]   Great to see that always.
[00:03:59.000 --> 00:04:03.000]   So everything that we discuss in our fastbook reading group today is going
[00:04:03.000 --> 00:04:05.000]   to be in this thread.
[00:04:05.000 --> 00:04:08.000]   And Angelica has already added the blog post from last week.
[00:04:08.000 --> 00:04:11.000]   And then we'll add links from this week as we go on.
[00:04:11.000 --> 00:04:15.000]   So this is how, like, we've been managing the fastbook reading group.
[00:04:15.000 --> 00:04:20.000]   You'll see, like, there's a week 11, week 12, 13, 14, and so on.
[00:04:20.000 --> 00:04:23.000]   And then there's a weekly discussion master thread that you can see where
[00:04:23.000 --> 00:04:25.000]   all the other weeks are.
[00:04:25.000 --> 00:04:29.000]   So I think that's really helpful and really nicely structured.
[00:04:29.000 --> 00:04:32.000]   And we should make the most of this course.
[00:04:32.000 --> 00:04:36.000]   So I'm coming back to week 14, and this is where we'll start our journey
[00:04:36.000 --> 00:04:38.000]   on ResNet.
[00:04:38.000 --> 00:04:44.000]   Something I'm really, really excited to see, and Ravi Mushroo and Vinayak,
[00:04:44.000 --> 00:04:49.000]   they've been -- and even, like, I think Ravi Chandra and Niyasi, like,
[00:04:49.000 --> 00:04:51.000]   you've all been blogging week after week.
[00:04:51.000 --> 00:04:54.000]   And something I'm really, really excited to see -- I'm sorry if I missed the
[00:04:54.000 --> 00:04:59.000]   name, but it's not so much to highlight, like, the name, but just highlight
[00:04:59.000 --> 00:05:01.000]   the work and the effort.
[00:05:01.000 --> 00:05:04.000]   So something I'm really excited to share and see week after week is that
[00:05:04.000 --> 00:05:08.000]   Ravi Mushroo has been coming in and he's been writing wonderful, wonderful
[00:05:08.000 --> 00:05:10.000]   blog posts about fast.ai.
[00:05:10.000 --> 00:05:13.000]   He's been writing wonderful blog posts about everything that we've been
[00:05:13.000 --> 00:05:15.000]   doing week after week.
[00:05:15.000 --> 00:05:19.000]   And I'm seeing more and more that Jeremy started retweeting Ravi's tweets.
[00:05:19.000 --> 00:05:23.000]   And that's really -- like, I was talking to Ravi today, and he said it's
[00:05:23.000 --> 00:05:26.000]   like a fan moment for him every time Jeremy retweets his tweet.
[00:05:26.000 --> 00:05:30.000]   And I can completely relate to that, because I think that's how I felt when
[00:05:30.000 --> 00:05:34.000]   Jeremy retweeted my tweets for the first time or the earlier tweets.
[00:05:34.000 --> 00:05:37.000]   So I think -- sorry, I pressed something.
[00:05:37.000 --> 00:05:41.000]   So I think that's really, really nice to see how Ravi's journey has
[00:05:41.000 --> 00:05:43.000]   progressed.
[00:05:43.000 --> 00:05:47.000]   And he's -- the best thing is, like, he started reading papers now.
[00:05:47.000 --> 00:05:50.000]   So he's already written about the one cycle LR.
[00:05:50.000 --> 00:05:52.000]   So we keep talking about one cycle LR.
[00:05:52.000 --> 00:05:54.000]   We keep talking about what it is in fast.ai.
[00:05:54.000 --> 00:05:57.000]   But he's actually went in and he's written this really nice blog post about
[00:05:57.000 --> 00:05:59.000]   understanding superconvergence.
[00:05:59.000 --> 00:06:04.000]   And you can find the link of those -- sorry, I shouldn't have pressed that.
[00:06:04.000 --> 00:06:05.000]   That was weird.
[00:06:05.000 --> 00:06:06.000]   I could hear myself.
[00:06:06.000 --> 00:06:09.000]   So you can go in and you can click this Ravi Mashroor's post on
[00:06:09.000 --> 00:06:11.000]   superconvergence.
[00:06:11.000 --> 00:06:14.000]   That will take you to that blog post I'm highlighting.
[00:06:14.000 --> 00:06:17.000]   And Vinayak, again, similar journey.
[00:06:17.000 --> 00:06:19.000]   Very similar path.
[00:06:19.000 --> 00:06:24.000]   And I'm so excited to see that he's also grown and he's also -- like,
[00:06:24.000 --> 00:06:27.000]   just reading his blog post, I see a lot more detail.
[00:06:27.000 --> 00:06:30.000]   I particularly love how Vinayak has these handwritten notes.
[00:06:30.000 --> 00:06:32.000]   I used to have them on iPad.
[00:06:32.000 --> 00:06:35.000]   But he's come up and, like, put them on paper.
[00:06:35.000 --> 00:06:37.000]   And it's really nice to see.
[00:06:37.000 --> 00:06:40.000]   It's a really good feeling to see, like, what worked for me is kind of
[00:06:40.000 --> 00:06:42.000]   working for others, too.
[00:06:42.000 --> 00:06:45.000]   So it's really, really nice to see that happening to both Vinayak and
[00:06:45.000 --> 00:06:47.000]   Ravi Mashroor.
[00:06:47.000 --> 00:06:50.000]   And, again, Jeremy's been retweeting Vinayak's tweets as well more and
[00:06:50.000 --> 00:06:53.000]   more, which is perfect and which is, like, the best outcome you could
[00:06:53.000 --> 00:06:55.000]   expect.
[00:06:55.000 --> 00:06:58.000]   Because if, like, if Jeremy's highlighting your work, then it's got to
[00:06:58.000 --> 00:07:00.000]   be good work.
[00:07:00.000 --> 00:07:02.000]   So that's how I would say.
[00:07:02.000 --> 00:07:05.000]   Otherwise, yeah, like, I mean, now you're, like, writing those blogs
[00:07:05.000 --> 00:07:07.000]   that are helpful to more people.
[00:07:07.000 --> 00:07:10.000]   And once you start getting at that stage, you're just getting better and
[00:07:10.000 --> 00:07:13.000]   better at your craft, which is wonderful and which is all that we
[00:07:13.000 --> 00:07:17.000]   expected when I first started and when we first started the Fastbook
[00:07:17.000 --> 00:07:20.000]   reading group at Weights and Biases.
[00:07:20.000 --> 00:07:25.000]   So, again, I really like these Excel, Microsoft Excel explanations, and I
[00:07:25.000 --> 00:07:27.000]   really like these handwritten notes.
[00:07:27.000 --> 00:07:36.000]   So thanks, Vinayak, and thanks, Ravi, for doing this week after week.
[00:07:36.000 --> 00:07:37.000]   Okay.
[00:07:37.000 --> 00:07:42.000]   So now going back to -- I will answer this.
[00:07:42.000 --> 00:07:45.000]   So we're just going to get started with ResNet today.
[00:07:45.000 --> 00:07:47.000]   But let me just quickly have a look at this question.
[00:07:47.000 --> 00:07:51.000]   Is it recommended to dedicate some time learning framework or try to find
[00:07:51.000 --> 00:07:55.000]   repositories from papers with code and use Stack Overflow?
[00:07:55.000 --> 00:07:58.000]   Asking this because I'm not good with coding part of ML.
[00:07:58.000 --> 00:08:00.000]   Also, should we follow PyTorch reading group?
[00:08:00.000 --> 00:08:02.000]   A lot of implementation of old papers are in TF.
[00:08:02.000 --> 00:08:04.000]   How to go around that?
[00:08:04.000 --> 00:08:05.000]   Okay.
[00:08:05.000 --> 00:08:06.000]   I'll go one by one.
[00:08:06.000 --> 00:08:10.000]   So is it -- do you want to find repositories with papers and code?
[00:08:10.000 --> 00:08:14.000]   If you're starting fresh with reading papers and you want to see how they
[00:08:14.000 --> 00:08:17.000]   are implemented in code, so I think starting with ResNet would be a great
[00:08:17.000 --> 00:08:18.000]   point.
[00:08:18.000 --> 00:08:23.000]   But also, again, I'm not -- like this is not just -- I know I'm a little
[00:08:23.000 --> 00:08:26.000]   biased in highlighting my own work, but I do feel it's going to be really
[00:08:26.000 --> 00:08:27.000]   helpful.
[00:08:27.000 --> 00:08:29.000]   Because when I started, things like this didn't exist.
[00:08:29.000 --> 00:08:33.000]   Like I didn't know anybody doing live coding sessions on papers.
[00:08:33.000 --> 00:08:37.000]   And, like, there were still groups that did discuss papers, but there
[00:08:37.000 --> 00:08:40.000]   weren't any groups that would show how things are written from scratch.
[00:08:40.000 --> 00:08:43.000]   And that's how kind of even my own blog picked off.
[00:08:43.000 --> 00:08:48.000]   Like that's how my kind of blog was so different from the others is how I
[00:08:48.000 --> 00:08:53.000]   had code for every piece of paper or every section of the paper.
[00:08:53.000 --> 00:08:56.000]   And, again, this is something that will be really helpful to those
[00:08:56.000 --> 00:09:00.000]   starting -- and to those who are beginners and want to start reading
[00:09:00.000 --> 00:09:05.000]   papers, is that from this week onwards and from the ResNet, basically we're
[00:09:05.000 --> 00:09:08.000]   starting with ResNet, because that's what we'll discuss today.
[00:09:08.000 --> 00:09:11.000]   But from that, we're going from ResNet to DenseNet, then we're going to
[00:09:11.000 --> 00:09:14.000]   go to EfficientNet, then we're going to go -- like, we're going to keep
[00:09:14.000 --> 00:09:15.000]   leveling up.
[00:09:15.000 --> 00:09:19.000]   So I would really, really recommend all beginners, like, if you're new to
[00:09:19.000 --> 00:09:21.000]   coding, start here.
[00:09:21.000 --> 00:09:28.000]   And then from there on, you could find papers that already have
[00:09:28.000 --> 00:09:29.000]   implementation.
[00:09:29.000 --> 00:09:33.000]   So I think the next great place would be team PyTorch image models.
[00:09:33.000 --> 00:09:36.000]   So you can see PyTorch image models team is here.
[00:09:36.000 --> 00:09:38.000]   And then you can go team.
[00:09:38.000 --> 00:09:41.000]   You can go models.
[00:09:41.000 --> 00:09:44.000]   And then you can see the implementation of, like, so many different
[00:09:44.000 --> 00:09:45.000]   models over here.
[00:09:45.000 --> 00:09:49.000]   Like, you can see ResNet, you can see ResNext, you can see SENet.
[00:09:49.000 --> 00:09:53.000]   And then, again, there's, like, lots of these different resources that
[00:09:53.000 --> 00:09:54.000]   could be followed.
[00:09:54.000 --> 00:09:57.000]   But this, again, is a great, great place to get started.
[00:09:57.000 --> 00:09:59.000]   And then what were the other questions?
[00:09:59.000 --> 00:10:01.000]   Like, should we follow the PyTorch reading group?
[00:10:01.000 --> 00:10:02.000]   100%.
[00:10:02.000 --> 00:10:06.000]   I think being a good deep learning practitioner is all about, like, being
[00:10:06.000 --> 00:10:08.000]   a good coder as well.
[00:10:08.000 --> 00:10:11.000]   And in that PyTorch reading group, you'll understand the basics about
[00:10:11.000 --> 00:10:13.000]   PyTorch.
[00:10:13.000 --> 00:10:16.000]   And when we go to these, like, live coding sessions and we're going to
[00:10:16.000 --> 00:10:19.000]   start doing things from scratch and we're going to start building those
[00:10:19.000 --> 00:10:23.000]   training loops from scratch, and we -- even Fast.ai is built all on top of
[00:10:23.000 --> 00:10:24.000]   PyTorch.
[00:10:24.000 --> 00:10:27.000]   So, like, if you want to understand and dig deeper into Fast.ai, we need
[00:10:27.000 --> 00:10:28.000]   to know PyTorch.
[00:10:28.000 --> 00:10:32.000]   So I think that would be really, really helpful if everybody got a good
[00:10:32.000 --> 00:10:35.000]   understanding of what PyTorch is, if everybody got a good understanding
[00:10:35.000 --> 00:10:39.000]   of, like, tensors, good understanding of training loops.
[00:10:39.000 --> 00:10:42.000]   And, again, we're here at Weights and Biases to help you along those
[00:10:42.000 --> 00:10:44.000]   journeys.
[00:10:44.000 --> 00:10:49.000]   So, yeah, so keep following along and we will keep sharing more resources.
[00:10:49.000 --> 00:10:53.000]   And then a lot of implementation of old papers are in TensorFlow, how to
[00:10:53.000 --> 00:10:54.000]   go around that.
[00:10:54.000 --> 00:10:56.000]   So, again, there's different ways to go about it.
[00:10:56.000 --> 00:10:59.000]   So I think I will share -- there's one on ResNet RS.
[00:10:59.000 --> 00:11:04.000]   I think there was a pull request that I once made on ResNet RS to Tim, to
[00:11:04.000 --> 00:11:06.000]   PyTorch image models.
[00:11:06.000 --> 00:11:11.000]   And something to highlight is, like, the more proficient you get at one
[00:11:11.000 --> 00:11:14.000]   framework, the better you're becoming at the other framework, too.
[00:11:14.000 --> 00:11:17.000]   Because it's not like PyTorch versus TensorFlow.
[00:11:17.000 --> 00:11:20.000]   So if you understand the basics of, like, okay, we're going to -- a basic
[00:11:20.000 --> 00:11:23.000]   training loop consists of, like, getting a model, getting the output
[00:11:23.000 --> 00:11:27.000]   activations from a model, calculating loss, doing the loss of backward.
[00:11:27.000 --> 00:11:30.000]   And once you, like, start understanding how these things are done in
[00:11:30.000 --> 00:11:33.000]   PyTorch, you're automatically understanding those things in TensorFlow
[00:11:33.000 --> 00:11:36.000]   because then all you have to do is just understand the syntax.
[00:11:36.000 --> 00:11:39.000]   And same for, like, okay, if you understand what a convolution is, how
[00:11:39.000 --> 00:11:43.000]   you implement a convolution in PyTorch, or how you implement basically any
[00:11:43.000 --> 00:11:47.000]   architecture in PyTorch, you'll also have a better understanding of things
[00:11:47.000 --> 00:11:48.000]   in TensorFlow.
[00:11:48.000 --> 00:11:51.000]   So when I was first -- I'll share an example.
[00:11:51.000 --> 00:11:55.000]   When I was first doing ResNet RS, we did a paper reading group on ResNet
[00:11:55.000 --> 00:11:56.000]   RS as well.
[00:11:56.000 --> 00:12:01.000]   So if I go here and I go paper reading group, I go master biweekly list.
[00:12:01.000 --> 00:12:04.000]   I think there's a ResNet RS somewhere here.
[00:12:04.000 --> 00:12:05.000]   There isn't.
[00:12:05.000 --> 00:12:06.000]   Okay.
[00:12:06.000 --> 00:12:08.000]   Because we didn't YouTube -- because we didn't livestream it on YouTube, I
[00:12:08.000 --> 00:12:09.000]   remember.
[00:12:09.000 --> 00:12:12.000]   But anyway, when I was doing the ResNet RS paper reading group and I was
[00:12:12.000 --> 00:12:16.000]   reading that paper for the first time, the implementation and the widths were
[00:12:16.000 --> 00:12:17.000]   in TensorFlow.
[00:12:17.000 --> 00:12:21.000]   But it wasn't much of a problem because I understood the basics of it.
[00:12:21.000 --> 00:12:25.000]   And so when I went and I started looking at code in TensorFlow, it was just
[00:12:25.000 --> 00:12:29.000]   very easy and, like, it was still an easier part.
[00:12:29.000 --> 00:12:35.000]   And I hope that as you get better in PyTorch, you're also getting better in
[00:12:35.000 --> 00:12:36.000]   TensorFlow.
[00:12:36.000 --> 00:12:41.000]   So I hope that answers, like, all three questions that have been asked.
[00:12:41.000 --> 00:12:42.000]   Great blog posts.
[00:12:42.000 --> 00:12:43.000]   I saw reviews.
[00:12:43.000 --> 00:12:46.000]   Thanks so much for highlighting and thanks for the high-quality work.
[00:12:46.000 --> 00:12:47.000]   Yes, that's lovely.
[00:12:47.000 --> 00:12:50.000]   Thanks, Kevin.
[00:12:50.000 --> 00:12:53.000]   And again, okay, there's more questions.
[00:12:53.000 --> 00:12:57.000]   I will -- how about I'll come back to these questions in, say, a while.
[00:12:57.000 --> 00:13:00.000]   Because I'm also conscious of time.
[00:13:00.000 --> 00:13:04.000]   And we want to wrap up ResNet today as well.
[00:13:04.000 --> 00:13:26.000]   So I'll come back to these questions on the forums.
[00:13:26.000 --> 00:13:27.000]   Okay.
[00:13:27.000 --> 00:13:30.000]   So we are at ResNet.
[00:13:30.000 --> 00:13:34.000]   So the ResNet YouTube video for the paper reading group that we did
[00:13:34.000 --> 00:13:36.000]   yesterday should be up very soon.
[00:13:36.000 --> 00:13:40.000]   I think that would also be a really good place to get started with ResNet.
[00:13:40.000 --> 00:13:44.000]   Because it's mostly going to be a repeat of what we discussed yesterday.
[00:13:44.000 --> 00:13:48.000]   But I'm going to try and, like, include all the information needed in today's
[00:13:48.000 --> 00:13:50.000]   session as well.
[00:13:50.000 --> 00:13:53.000]   But I would highly, highly recommend for everybody doing the session today,
[00:13:53.000 --> 00:13:57.000]   please have a look at the ResNet paper reading group YouTube video
[00:13:57.000 --> 00:13:58.000]   yesterday.
[00:13:58.000 --> 00:14:00.000]   And we will share that link on the forums.
[00:14:00.000 --> 00:14:02.000]   So, okay.
[00:14:02.000 --> 00:14:03.000]   So ResNet.
[00:14:03.000 --> 00:14:06.000]   So then ResNet, again, most of this is same.
[00:14:06.000 --> 00:14:09.000]   So deep residual learning for image recognition.
[00:14:09.000 --> 00:14:13.000]   That was the paper that came out in 2015 by Microsoft Research.
[00:14:13.000 --> 00:14:17.000]   And Kaiming He has been -- he's been, like, a pioneer.
[00:14:17.000 --> 00:14:19.000]   And he's been at the top of, like,
[00:14:19.000 --> 00:14:22.000]   so many nice research papers that I love.
[00:14:22.000 --> 00:14:27.000]   Including, like, some of them were, like -- excuse me.
[00:14:27.000 --> 00:14:30.000]   Some of them were, like, vocal loss or just others.
[00:14:30.000 --> 00:14:33.000]   But I think -- sorry.
[00:14:33.000 --> 00:14:34.000]   Yeah.
[00:14:34.000 --> 00:14:40.000]   So I think there's these -- I think these papers have been wonderful.
[00:14:40.000 --> 00:14:43.000]   Like, ResNet is the most used model architecture.
[00:14:43.000 --> 00:14:49.000]   And I showed a fact yesterday, like, that ResNet has been cited 75,000
[00:14:49.000 --> 00:14:50.000]   times.
[00:14:50.000 --> 00:14:52.000]   About 74,500 times.
[00:14:52.000 --> 00:14:56.000]   So you can imagine how impactful this paper has been.
[00:14:56.000 --> 00:15:00.000]   And you can imagine, like, how much the impact this paper has had on the
[00:15:00.000 --> 00:15:03.000]   computer vision field in general.
[00:15:03.000 --> 00:15:07.000]   So, like, all the recent papers that have come out have kind of built on
[00:15:07.000 --> 00:15:09.000]   top of ResNet.
[00:15:09.000 --> 00:15:13.000]   And I think we're getting to a point here today that even, like -- you can
[00:15:13.000 --> 00:15:17.000]   even compare Vision Transformer in some ways to ResNet because even that is
[00:15:17.000 --> 00:15:19.000]   using Skid Connections.
[00:15:19.000 --> 00:15:20.000]   And all of that.
[00:15:20.000 --> 00:15:23.000]   So, like, there's really a lot of good things that have come out from the
[00:15:23.000 --> 00:15:24.000]   ResNet paper.
[00:15:24.000 --> 00:15:26.000]   And we're discussing that today.
[00:15:26.000 --> 00:15:28.000]   So going back to ImageNet.
[00:15:28.000 --> 00:15:31.000]   ImageNet was that smaller subset of the ImageNet that we keep talking
[00:15:31.000 --> 00:15:32.000]   about.
[00:15:32.000 --> 00:15:35.000]   So this is just getting the data, untying the data, and creating a data
[00:15:35.000 --> 00:15:36.000]   block.
[00:15:36.000 --> 00:15:39.000]   And I'm not going to go into the details of this function because we've
[00:15:39.000 --> 00:15:41.000]   looked at this many, many times.
[00:15:41.000 --> 00:15:44.000]   So this just gets the data, creates data loaders.
[00:15:44.000 --> 00:15:48.000]   And I could click -- I could press DLs.showbatch.
[00:15:48.000 --> 00:15:52.000]   So DLs.showbatch shows, like, these four images with four categories.
[00:15:52.000 --> 00:15:54.000]   So this is what we have to do.
[00:15:54.000 --> 00:15:58.000]   In this case, we have ten -- basically we have ten categories.
[00:15:58.000 --> 00:16:01.000]   And also we're using, like, 128 by 128 pixel sizes.
[00:16:01.000 --> 00:16:05.000]   Because when we said -- I think we said somewhere -- oh, we're using
[00:16:05.000 --> 00:16:07.000]   ResNet 160.
[00:16:07.000 --> 00:16:09.000]   But we're using a size as resize.
[00:16:09.000 --> 00:16:12.000]   So see the size being parsed here is 128.
[00:16:12.000 --> 00:16:14.000]   So we do presizing at 160.
[00:16:14.000 --> 00:16:16.000]   Then we do the data augmentations.
[00:16:16.000 --> 00:16:18.000]   And we resize every image to 128.
[00:16:18.000 --> 00:16:21.000]   So we're using 128 by 128 image sizes in this.
[00:16:21.000 --> 00:16:24.000]   I will continue to skip through most of this.
[00:16:24.000 --> 00:16:28.000]   Because if you remember, like, in MNIST, we got to a point where the
[00:16:28.000 --> 00:16:30.000]   final width size was a one.
[00:16:30.000 --> 00:16:32.000]   And then we just flattened it.
[00:16:32.000 --> 00:16:38.000]   But imagine if we start doing that to, like -- let me highlight what
[00:16:38.000 --> 00:16:41.000]   we -- let me bring that up in OneNote quickly.
[00:16:41.000 --> 00:16:43.000]   So we had, like, MNIST.
[00:16:43.000 --> 00:16:45.000]   We had 28 by 28.
[00:16:45.000 --> 00:16:50.000]   And when we did a convolution, right, the size got down to 14 by 14.
[00:16:50.000 --> 00:16:52.000]   Then it got down to 7 by 7.
[00:16:52.000 --> 00:16:56.000]   Then it got down to, like -- I think it got down to 4 by 4.
[00:16:56.000 --> 00:16:58.000]   Then it got down to 2 by 2.
[00:16:58.000 --> 00:17:00.000]   Until it got down to 1 by 1.
[00:17:00.000 --> 00:17:05.000]   So, like, for each image, then, we had kind of, like, just a 1 by 1
[00:17:05.000 --> 00:17:09.000]   output, which we could then flatten to see, like, okay, what are the
[00:17:09.000 --> 00:17:11.000]   class outputs.
[00:17:11.000 --> 00:17:15.000]   So that's something that we did in -- when we were doing MNIST.
[00:17:15.000 --> 00:17:19.000]   But imagine, like, if this size is, say, 128 by 128, you would need a
[00:17:19.000 --> 00:17:23.000]   lot many of these convolutions about, like -- you would have to keep
[00:17:23.000 --> 00:17:25.000]   going half and half and half.
[00:17:25.000 --> 00:17:28.000]   So it's 2 to the power of 7, I think.
[00:17:28.000 --> 00:17:31.000]   Yeah, I think 128 is 2 to the power of 7.
[00:17:31.000 --> 00:17:34.000]   So instead of needing just, like, these four or five, you would probably
[00:17:34.000 --> 00:17:36.000]   need seven or eight.
[00:17:36.000 --> 00:17:39.000]   And if you go even bigger size, that's even more, and so on.
[00:17:39.000 --> 00:17:43.000]   So if you go 512, you can see how this is a problem.
[00:17:43.000 --> 00:17:47.000]   So one thing you could do is, like, we could use something called
[00:17:47.000 --> 00:17:49.000]   average pooling.
[00:17:49.000 --> 00:17:53.000]   And what average pooling does -- this is really interesting -- is that
[00:17:53.000 --> 00:17:57.000]   -- so what average pooling does, like, the problem is now what you want
[00:17:57.000 --> 00:18:05.000]   to do is you want a network -- basically, you want your network that
[00:18:05.000 --> 00:18:11.000]   can take 28 by 28, or it can take 128 by 128, or it can take 256 by
[00:18:11.000 --> 00:18:17.000]   256 as input, and still give me, like, ten outputs.
[00:18:17.000 --> 00:18:18.000]   Right?
[00:18:18.000 --> 00:18:19.000]   Why ten outputs?
[00:18:19.000 --> 00:18:21.000]   Because you have ten classes.
[00:18:21.000 --> 00:18:23.000]   So you need something like that, right?
[00:18:23.000 --> 00:18:31.000]   So if you pass one image, one image of any of these sizes, what you need
[00:18:31.000 --> 00:18:34.000]   is a network that can give me, like, ten outputs, which you can then do
[00:18:34.000 --> 00:18:38.000]   softmax and you can check, okay, what is the probability of, like, this
[00:18:38.000 --> 00:18:41.000]   image belonging to one of those ten categories?
[00:18:41.000 --> 00:18:43.000]   So that's what we want to do.
[00:18:43.000 --> 00:18:48.000]   And to do that, you can't really have, like -- you can't really have a
[00:18:48.000 --> 00:18:50.000]   network architecture like this.
[00:18:50.000 --> 00:18:54.000]   Because if you pass something -- because right now, you're -- after four
[00:18:54.000 --> 00:18:57.000]   convolution layers, you have one by one output, but if you have, like,
[00:18:57.000 --> 00:19:00.000]   something like 128 by 128, you would need seven, which means you can't
[00:19:00.000 --> 00:19:03.000]   really pre-train the network, which means you can't really pre-train this
[00:19:03.000 --> 00:19:06.000]   network with ImageNet and you can't really use those weights.
[00:19:06.000 --> 00:19:11.000]   So you need a network architecture that has the same structure.
[00:19:11.000 --> 00:19:15.000]   So you can't -- you don't want to change the structure or the architecture
[00:19:15.000 --> 00:19:18.000]   of the network every time there's a new image size.
[00:19:18.000 --> 00:19:21.000]   So you need a network architecture that can handle multiple image sizes.
[00:19:21.000 --> 00:19:27.000]   So what you do, there's something called global pooling.
[00:19:27.000 --> 00:19:29.000]   Average pooling.
[00:19:29.000 --> 00:19:31.000]   Or max pooling.
[00:19:31.000 --> 00:19:34.000]   Basically, it's called -- yeah, it's just basically called pooling
[00:19:34.000 --> 00:19:36.000]   operation.
[00:19:36.000 --> 00:19:42.000]   So what you do in this case is let's say you have a network, right, and
[00:19:42.000 --> 00:19:49.000]   you input 128 by 128 image, right, and this gives you an output of, say,
[00:19:49.000 --> 00:19:54.000]   128 channels by -- so this is, say, three channels by 128 by 128.
[00:19:54.000 --> 00:19:58.000]   And remember, as we go down the network, this is something we discussed
[00:19:58.000 --> 00:20:02.000]   in our last chapter, is, like, you keep increasing the number of channels
[00:20:02.000 --> 00:20:06.000]   and you keep decreasing the number of basically the spatial dimension.
[00:20:06.000 --> 00:20:10.000]   So maybe you get, like, 128 channels and you get, say, I don't know,
[00:20:10.000 --> 00:20:12.000]   14 by 14, maybe, right?
[00:20:12.000 --> 00:20:16.000]   So let's say you get something like this.
[00:20:16.000 --> 00:20:17.000]   Okay?
[00:20:17.000 --> 00:20:22.000]   But then, again, if you get, say -- again, if you have something like 256
[00:20:22.000 --> 00:20:28.000]   by 256, three by 256, the same network would now produce an output of 128
[00:20:28.000 --> 00:20:32.000]   channels, but this time, instead of this being 14 by 14, it's going to be
[00:20:32.000 --> 00:20:34.000]   28 by 28, right?
[00:20:34.000 --> 00:20:37.000]   Can you guys guess why it would be 28 by 28?
[00:20:37.000 --> 00:20:40.000]   I guess the question is why it would be double of 14.
[00:20:40.000 --> 00:20:44.000]   The answer is because the image size is double of 128.
[00:20:44.000 --> 00:20:47.000]   So if the image size is double, then the output size changes.
[00:20:47.000 --> 00:20:49.000]   That's what a convolution does.
[00:20:49.000 --> 00:20:51.000]   Remember what a convolution was?
[00:20:51.000 --> 00:20:56.000]   A three by three convolution, if you have an input of, say -- if you had
[00:20:56.000 --> 00:21:01.000]   an input of, say, 128 by 128, it gives you an output of 127 by 127 if it's
[00:21:01.000 --> 00:21:05.000]   not padded, and if you had, like, 256 by 256, then it gives you an output
[00:21:05.000 --> 00:21:08.000]   of 255 by 255, and so on.
[00:21:08.000 --> 00:21:12.000]   So, like, the -- in the convolution neural networks, the output size is
[00:21:12.000 --> 00:21:14.000]   different based on the input size.
[00:21:14.000 --> 00:21:19.000]   So now what you want is you want to take, like -- you want to be able to
[00:21:19.000 --> 00:21:23.000]   take both of these, and you want to still be able to classify them into
[00:21:23.000 --> 00:21:24.000]   ten layers.
[00:21:24.000 --> 00:21:27.000]   So what you add is you add a pooling layer after.
[00:21:27.000 --> 00:21:30.000]   Pooling layer.
[00:21:30.000 --> 00:21:32.000]   You add a pooling layer.
[00:21:32.000 --> 00:21:34.000]   So what does this pooling layer do?
[00:21:34.000 --> 00:21:38.000]   It takes the average on the second and the third dimension.
[00:21:38.000 --> 00:21:41.000]   Because remember, the first dimension is pad size.
[00:21:41.000 --> 00:21:44.000]   So you're going to -- the zeroth one is pad size.
[00:21:44.000 --> 00:21:46.000]   So you're going to skip over that.
[00:21:46.000 --> 00:21:49.000]   So what it does is you take the average of these spatial dimensions.
[00:21:49.000 --> 00:21:54.000]   So if you look at this output, it looks something like 14 by 14 and 128
[00:21:54.000 --> 00:21:56.000]   by 128, right?
[00:21:56.000 --> 00:21:58.000]   So what you do, this is 14 by 14, right?
[00:21:58.000 --> 00:22:01.000]   You can just take the average, then it becomes one number.
[00:22:01.000 --> 00:22:04.000]   And you can just take the average of this part, it becomes one number.
[00:22:04.000 --> 00:22:08.000]   So you get a 128-long vector, right?
[00:22:08.000 --> 00:22:10.000]   The same thing you do for this one.
[00:22:10.000 --> 00:22:14.000]   You have something called 28 by 28 all the way to 128.
[00:22:14.000 --> 00:22:17.000]   So you have 128 channels.
[00:22:17.000 --> 00:22:19.000]   And 28 by 28 as my spatial dimension.
[00:22:19.000 --> 00:22:23.000]   So then I can just take this, take the average, one pixel, and so on.
[00:22:23.000 --> 00:22:26.000]   So you get, again, a 128-long vector here as well.
[00:22:26.000 --> 00:22:32.000]   So now you're getting the same output because of this pooling layer,
[00:22:32.000 --> 00:22:35.000]   irrespective of the input size.
[00:22:35.000 --> 00:22:39.000]   So what you can do now is like now that you're getting 128,
[00:22:39.000 --> 00:22:43.000]   you always know that you're going to get an output of 128-long vector length,
[00:22:43.000 --> 00:22:45.000]   irrespective of the input image size.
[00:22:45.000 --> 00:22:51.000]   You could just have an nn.linear in the end that goes from 128 to however
[00:22:51.000 --> 00:22:53.000]   many your classes are.
[00:22:53.000 --> 00:22:55.000]   So you could just go from 128 to 10.
[00:22:55.000 --> 00:22:59.000]   So then every time for every image, you get 10 outputs.
[00:22:59.000 --> 00:23:04.000]   I hope this is clear because this is a really, really important part.
[00:23:04.000 --> 00:23:08.000]   And if this is clear, then we're at that point where --
[00:23:08.000 --> 00:23:16.000]   We're at the point where -- Sorry, just give me one sec.
[00:23:16.000 --> 00:23:35.000]   Okay.
[00:23:35.000 --> 00:23:39.000]   I'm back.
[00:23:39.000 --> 00:23:41.000]   So question.
[00:23:41.000 --> 00:23:44.000]   Using global pooling allows you to use any image size as input.
[00:23:44.000 --> 00:23:45.000]   Yes, that's correct.
[00:23:45.000 --> 00:23:47.000]   However, during training, is the network going to perform better at
[00:23:47.000 --> 00:23:48.000]   particular image size?
[00:23:48.000 --> 00:23:49.000]   Yes.
[00:23:49.000 --> 00:23:52.000]   So it's like -- That's a good question.
[00:23:52.000 --> 00:23:57.000]   So different networks, depending on like how many data set sizes you have.
[00:23:57.000 --> 00:24:02.000]   So if you have like 10 million -- Like ImageNet starts with 2 to 4 by 2 to 4,
[00:24:02.000 --> 00:24:08.000]   but it's not to say that it won't work better on 348 by 348 or 512 by 512.
[00:24:08.000 --> 00:24:11.000]   So I think it just comes down to like trying different image sizes.
[00:24:11.000 --> 00:24:16.000]   Generally, the bigger image size you go, the better it is.
[00:24:16.000 --> 00:24:19.000]   So you start with, say, 2 to 4 by 2 to 4.
[00:24:19.000 --> 00:24:23.000]   Then you go 384 by 384.
[00:24:23.000 --> 00:24:25.000]   Then you go 512 by 512.
[00:24:25.000 --> 00:24:28.000]   And this is this idea of progressive resizing that we discussed before as
[00:24:28.000 --> 00:24:30.000]   well.
[00:24:30.000 --> 00:24:33.000]   So I hope that that helps.
[00:24:33.000 --> 00:24:34.000]   Okay.
[00:24:34.000 --> 00:24:37.000]   So all of that just to explain this one line of code.
[00:24:37.000 --> 00:24:39.000]   Wow.
[00:24:39.000 --> 00:24:42.000]   So now you know like why we have this average pool.
[00:24:42.000 --> 00:24:46.000]   So what you can do -- Basically, this idea of like pooling or taking the
[00:24:46.000 --> 00:24:51.000]   average in PyTorch is just called nn.adaptiveaveragepool2d.
[00:24:51.000 --> 00:24:58.000]   So you can see -- So if you search adaptive pool average 2D, you can see
[00:24:58.000 --> 00:25:01.000]   like applies a 2D adaptive average pooling over an input, single size
[00:25:01.000 --> 00:25:03.000]   composed of several input planes.
[00:25:03.000 --> 00:25:06.000]   The output is a size h by w for any input size.
[00:25:06.000 --> 00:25:09.000]   That's the key point.
[00:25:09.000 --> 00:25:16.000]   So like if you have -- I'm just going to check.
[00:25:16.000 --> 00:25:17.000]   Yeah.
[00:25:17.000 --> 00:25:18.000]   I'm going to skip over that.
[00:25:18.000 --> 00:25:20.000]   But go through the documentation.
[00:25:20.000 --> 00:25:22.000]   I think it will be really clear on what this does.
[00:25:22.000 --> 00:25:25.000]   But you can say like if you say 1, that just means that it's going to
[00:25:25.000 --> 00:25:29.000]   flatten my input image size to basically be that 1 by 1 pixel.
[00:25:29.000 --> 00:25:32.000]   And then you can still apply the flatten after.
[00:25:32.000 --> 00:25:37.000]   So let's -- To basically make things more clear, you could actually now
[00:25:37.000 --> 00:25:41.000]   go through this network one by one, and we could actually step through
[00:25:41.000 --> 00:25:45.000]   this network and we could check what the output image size is.
[00:25:45.000 --> 00:25:47.000]   But I'm going to skip over that now.
[00:25:47.000 --> 00:25:50.000]   But the thing that I'm trying to say is like create this network.
[00:25:50.000 --> 00:25:55.000]   Like say class model, and it's an nn.module.
[00:25:55.000 --> 00:26:00.000]   So I'm just -- This is where I'm saying -- This is where the knowledge
[00:26:00.000 --> 00:26:02.000]   of PyTorch will come in handy.
[00:26:02.000 --> 00:26:12.000]   So you could just say like define thunder init, so nothing, and then
[00:26:12.000 --> 00:26:21.000]   you could create like self.blocks equals this.
[00:26:21.000 --> 00:26:33.000]   And you could say self.pool equals nn.averagePool.
[00:26:33.000 --> 00:26:46.000]   And then you could say self.linear equals nn.linear256 by dl.c.
[00:26:46.000 --> 00:26:47.000]   Okay.
[00:26:47.000 --> 00:26:49.000]   And then I could just create my forward.
[00:26:49.000 --> 00:26:50.000]   I will explain what I'm doing.
[00:26:50.000 --> 00:26:52.000]   Just give me one sec.
[00:26:52.000 --> 00:27:03.000]   And you could say out equals self.blocksX and then out equals self.poolX.
[00:27:03.000 --> 00:27:05.000]   Oh, this should be out.
[00:27:05.000 --> 00:27:12.000]   And then out equals flatten.
[00:27:12.000 --> 00:27:14.000]   Flatten out.
[00:27:14.000 --> 00:27:15.000]   Is that how that would work?
[00:27:15.000 --> 00:27:16.000]   Let's see.
[00:27:16.000 --> 00:27:22.000]   And then out equals self.linear out and you return this out.
[00:27:22.000 --> 00:27:23.000]   Okay.
[00:27:23.000 --> 00:27:25.000]   So let's see if that works.
[00:27:25.000 --> 00:27:27.000]   Let me just quickly go through that.
[00:27:27.000 --> 00:27:30.000]   My input say is torch.random.
[00:27:30.000 --> 00:27:34.000]   So I'm just creating a random input of say 128 by 128.
[00:27:34.000 --> 00:27:36.000]   So this just means single image.
[00:27:36.000 --> 00:27:40.000]   So I've basically just tried to replicate this model just using like a
[00:27:40.000 --> 00:27:45.000]   more -- like just using a more classical approach of defining this as
[00:27:45.000 --> 00:27:46.000]   a product module.
[00:27:46.000 --> 00:27:47.000]   So I just take my input.
[00:27:47.000 --> 00:27:50.000]   I define my model as like that.
[00:27:50.000 --> 00:27:53.000]   And then let's see if this creates an output.
[00:27:53.000 --> 00:27:54.000]   It doesn't.
[00:27:54.000 --> 00:27:55.000]   Block is not defined.
[00:27:55.000 --> 00:28:00.000]   Of course it's not defined because it's defined in this.
[00:28:00.000 --> 00:28:02.000]   And it says there's another problem.
[00:28:02.000 --> 00:28:06.000]   It says cannot assign before module init call.
[00:28:06.000 --> 00:28:12.000]   So what does that mean?
[00:28:12.000 --> 00:28:16.000]   Cannot assign.
[00:28:16.000 --> 00:28:19.000]   It's giving me an error here.
[00:28:19.000 --> 00:28:25.000]   Oh, because I didn't do super.init.
[00:28:25.000 --> 00:28:26.000]   Okay.
[00:28:26.000 --> 00:28:27.000]   How about that?
[00:28:27.000 --> 00:28:28.000]   Oh, there we go.
[00:28:28.000 --> 00:28:29.000]   See, that works.
[00:28:29.000 --> 00:28:36.000]   So now what we could do is I could just create a breakpoint here.
[00:28:36.000 --> 00:28:40.000]   And now if I do that, I can check what my output shape is.
[00:28:40.000 --> 00:28:44.000]   So you can see -- so you have basically what you have here.
[00:28:44.000 --> 00:28:48.000]   What I'm trying to show you is like just a way to do this experimentation
[00:28:48.000 --> 00:28:52.000]   and just a way to do these different things is you could actually have
[00:28:52.000 --> 00:28:57.000]   like -- I've just broken down this whole network into like -- I broke it
[00:28:57.000 --> 00:29:00.000]   down into blocks being at the top, then having the average pooling,
[00:29:00.000 --> 00:29:03.000]   then having the flattened layer, and then finally the linear.
[00:29:03.000 --> 00:29:05.000]   So that's how my outputs are going through.
[00:29:05.000 --> 00:29:08.000]   So I could put a breakpoint after the blocks.
[00:29:08.000 --> 00:29:12.000]   Because like this is just saying that my network is always going to be
[00:29:12.000 --> 00:29:13.000]   like this.
[00:29:13.000 --> 00:29:15.000]   Like there's no change in the network.
[00:29:15.000 --> 00:29:18.000]   Remember, my network is the same.
[00:29:18.000 --> 00:29:21.000]   The only thing that changes is this output size, right, based on the
[00:29:21.000 --> 00:29:23.000]   input size.
[00:29:23.000 --> 00:29:25.000]   So I just wanted to show that in practice.
[00:29:25.000 --> 00:29:27.000]   And that's why I put this breakpoint.
[00:29:27.000 --> 00:29:31.000]   So I could now go -- if I put that breakpoint, you can see like if my
[00:29:31.000 --> 00:29:36.000]   input is 1 by 3 by 128 by 128, the output is 1 by 256 by 4 by 4.
[00:29:36.000 --> 00:29:39.000]   And what happens if I pool it?
[00:29:39.000 --> 00:29:42.000]   So let's see the shape of that.
[00:29:42.000 --> 00:29:45.000]   And you can see now it's 1 by 256 by 1 by 1.
[00:29:45.000 --> 00:29:48.000]   So it's doing the exact same thing that I've just explained to you.
[00:29:48.000 --> 00:29:52.000]   It's now taking like -- it's now going to this -- instead of this being
[00:29:52.000 --> 00:29:55.000]   like 4 by 4 -- sorry.
[00:29:55.000 --> 00:29:56.000]   What did I press?
[00:29:56.000 --> 00:29:57.000]   Something.
[00:29:57.000 --> 00:30:03.000]   So if you have like this 4 by 4, it just makes that to be a single
[00:30:03.000 --> 00:30:06.000]   pixel because you just take the average and you just do the same thing
[00:30:06.000 --> 00:30:08.000]   for all of those planes.
[00:30:08.000 --> 00:30:10.000]   So I hope that makes sense.
[00:30:10.000 --> 00:30:15.000]   And then I could just update my input image size to be 256 by 256.
[00:30:15.000 --> 00:30:18.000]   And now let's check what my output shape is.
[00:30:18.000 --> 00:30:23.000]   So instead of it being 1 by 256 by 4 by 4, this time it is 1 by 256 by
[00:30:23.000 --> 00:30:26.000]   8 by 8, just exactly as I've explained here.
[00:30:26.000 --> 00:30:30.000]   And then what you could do next is you could now still, if you take
[00:30:30.000 --> 00:30:37.000]   the pooling, you check the shape, you can see how it's now 1 by 256 by
[00:30:37.000 --> 00:30:39.000]   1 by 1.
[00:30:39.000 --> 00:30:41.000]   So it doesn't matter what the input size is.
[00:30:41.000 --> 00:30:44.000]   What I want to say is like this pooling layer -- sorry, give me one
[00:30:44.000 --> 00:30:45.000]   sec.
[00:30:45.000 --> 00:30:48.000]   So what I want to say is like what this pooling layer does is it makes
[00:30:48.000 --> 00:30:52.000]   sure that your output, irrespective of the input size, is then just
[00:30:52.000 --> 00:30:55.000]   flattened -- oh, sorry, it's just then you just take the average and
[00:30:55.000 --> 00:30:59.000]   you always get like this 1 by 256 by 1 by 1 output.
[00:30:59.000 --> 00:31:02.000]   So I hope that is clear.
[00:31:02.000 --> 00:31:07.000]   So you can see if I remove that break point, you can see I'm still
[00:31:07.000 --> 00:31:11.000]   getting -- it doesn't matter now what input image size I pass in.
[00:31:11.000 --> 00:31:14.000]   So you can see I'm always getting like 10 outputs for my one input
[00:31:14.000 --> 00:31:15.000]   image.
[00:31:15.000 --> 00:31:20.000]   And if I pass in, say, 10 images, I get 10 outputs for each of those
[00:31:20.000 --> 00:31:22.000]   10 images.
[00:31:22.000 --> 00:31:25.000]   And if I pass in, say, one image, I don't know, any different size,
[00:31:25.000 --> 00:31:31.000]   you can just go 1024 by 1024 and this model will still work.
[00:31:31.000 --> 00:31:33.000]   And the reason why this model works is just because of this pooling
[00:31:33.000 --> 00:31:34.000]   layer.
[00:31:34.000 --> 00:31:36.000]   That's the magic.
[00:31:36.000 --> 00:31:41.000]   So let me see if there's any questions about this.
[00:31:41.000 --> 00:31:43.000]   Yes, that's a great point.
[00:31:43.000 --> 00:31:46.000]   Definitely, if you're using pre-trained weights, most of the images
[00:31:46.000 --> 00:31:49.000]   have been trained on 2 to 4 by 2 to 4 -- sorry, most of the models
[00:31:49.000 --> 00:31:52.000]   have been trained on 2 to 4 by 2 to 4 image net.
[00:31:52.000 --> 00:31:54.000]   That's the image size you want to start with.
[00:31:54.000 --> 00:31:57.000]   And then you can just increase it to be 384 by 384.
[00:31:57.000 --> 00:32:03.000]   But another trick is called test time -- I can't remember the -- I
[00:32:03.000 --> 00:32:06.000]   think it's called fix res net or something.
[00:32:06.000 --> 00:32:12.000]   Basically what you do, fixing the test time resolution.
[00:32:12.000 --> 00:32:13.000]   There's another paper.
[00:32:13.000 --> 00:32:15.000]   Yes, there it is.
[00:32:15.000 --> 00:32:20.000]   Fixing the trained test resolution discrepancy.
[00:32:20.000 --> 00:32:24.000]   Yes, I think this is where they basically increase the image size
[00:32:24.000 --> 00:32:26.000]   during inference.
[00:32:26.000 --> 00:32:29.000]   So you train a model on 2 to 4 by 2 to 4 and then you inference the
[00:32:29.000 --> 00:32:31.000]   model on 2 to 6 by 2 to 6.
[00:32:31.000 --> 00:32:34.000]   That's something I want you guys to try.
[00:32:34.000 --> 00:32:36.000]   I'm just going to put this.
[00:32:36.000 --> 00:32:39.000]   Please try this.
[00:32:39.000 --> 00:32:48.000]   Train the model on 2 to 4 by 2 to 4 and run inference on, say, 384 --
[00:32:48.000 --> 00:32:50.000]   384 by 384.
[00:32:50.000 --> 00:32:52.000]   Try this out.
[00:32:52.000 --> 00:32:54.000]   For those of you still participating on Cassava, try this simple
[00:32:54.000 --> 00:32:55.000]   technique out.
[00:32:55.000 --> 00:32:56.000]   It might work.
[00:32:56.000 --> 00:32:57.000]   Okay.
[00:32:57.000 --> 00:32:59.000]   So that's that.
[00:32:59.000 --> 00:33:02.000]   So I've just explained what pooling does.
[00:33:02.000 --> 00:33:05.000]   I haven't gone much into detail into res net yet.
[00:33:05.000 --> 00:33:09.000]   But we could now just pass that model to a learner.
[00:33:09.000 --> 00:33:11.000]   I could grab my learner.
[00:33:11.000 --> 00:33:14.000]   I could say learn.LR find, which will give me my -- basically it
[00:33:14.000 --> 00:33:18.000]   will give me the learning rate that I should use.
[00:33:18.000 --> 00:33:22.000]   Then we can pick up the learning rate from this -- from that
[00:33:22.000 --> 00:33:24.000]   architect -- from that curve.
[00:33:24.000 --> 00:33:27.000]   And you can see you want to pick either the steepest point or you
[00:33:27.000 --> 00:33:29.000]   pick 3E negative 3, which is somewhere here.
[00:33:29.000 --> 00:33:31.000]   That's completely fine.
[00:33:31.000 --> 00:33:33.000]   And we say learn to fit one cycle and that will train.
[00:33:33.000 --> 00:33:35.000]   And that's it.
[00:33:35.000 --> 00:33:38.000]   So we just trained -- we just trained a neural architecture.
[00:33:38.000 --> 00:33:41.000]   Until now we haven't really trained a res net because in res net,
[00:33:41.000 --> 00:33:46.000]   as some of you might know from yesterday, the trick or technique
[00:33:46.000 --> 00:33:48.000]   was that there was a skip connection.
[00:33:48.000 --> 00:33:52.000]   So that's something we're just going to discuss next.
[00:33:52.000 --> 00:33:59.000]   But are there any questions for me?
[00:33:59.000 --> 00:34:02.000]   Thanks for sharing, Kevin.
[00:34:02.000 --> 00:34:03.000]   Cool.
[00:34:03.000 --> 00:34:05.000]   Looks like there's no questions so far.
[00:34:05.000 --> 00:34:08.000]   So I'm just going to assume everybody understood what I said.
[00:34:08.000 --> 00:34:22.000]   Is that correct?
[00:34:22.000 --> 00:34:25.000]   Okay.
[00:34:25.000 --> 00:34:32.000]   So now what's next is -- now we're ready to start creating a res net.
[00:34:32.000 --> 00:34:36.000]   So as most of you might know from yesterday, basically before res
[00:34:36.000 --> 00:34:40.000]   net, training deeper architectures or deeper layers was really hard.
[00:34:40.000 --> 00:34:44.000]   So a 20-layer network did better than a 56-layer network.
[00:34:44.000 --> 00:34:48.000]   And, like, there was this idea of, like, there was these different
[00:34:48.000 --> 00:34:50.000]   problems.
[00:34:50.000 --> 00:34:53.000]   One of the problems was a vanishing gradient problem.
[00:34:53.000 --> 00:34:55.000]   One of the problems was a degradation problem.
[00:34:55.000 --> 00:34:58.000]   And, again, these are things we discussed in the paper reading group
[00:34:58.000 --> 00:35:00.000]   yesterday.
[00:35:00.000 --> 00:35:04.000]   But as part of fastbook, I won't go into all the details of the paper,
[00:35:04.000 --> 00:35:07.000]   but some of the details were, like, there were problems before res net.
[00:35:07.000 --> 00:35:11.000]   And then what res net tried to do is it tried to fix those problems.
[00:35:11.000 --> 00:35:15.000]   And in trying to fix those problems, what it said is, like, until now,
[00:35:15.000 --> 00:35:18.000]   you just have your input, which goes through the network, and then you
[00:35:18.000 --> 00:35:20.000]   get an output.
[00:35:20.000 --> 00:35:22.000]   What if you added an identity layer?
[00:35:22.000 --> 00:35:26.000]   So what identity layer means is, like, you take the input and you add it
[00:35:26.000 --> 00:35:28.000]   as is to the output.
[00:35:28.000 --> 00:35:31.000]   So instead of, like, having something, like, if in mathematical terms,
[00:35:31.000 --> 00:35:36.000]   instead of having my output as f of x, you have the output as f of x plus
[00:35:36.000 --> 00:35:38.000]   x.
[00:35:38.000 --> 00:35:40.000]   Again, this is something that I explained in a lot, lot more detail in
[00:35:40.000 --> 00:35:42.000]   the paper reading group yesterday.
[00:35:42.000 --> 00:35:46.000]   But I've just repeated that bit just so it's easy for everyone to follow
[00:35:46.000 --> 00:35:50.000]   along today in case you didn't join the paper reading group.
[00:35:50.000 --> 00:35:52.000]   So, again, this is the idea.
[00:35:52.000 --> 00:35:55.000]   Like, you take the input and you just add it to the output.
[00:35:55.000 --> 00:35:58.000]   So you get your output as, like, f of x plus x.
[00:35:58.000 --> 00:36:03.000]   So you can see -- I won't go up, but basically, as you can see, in this
[00:36:03.000 --> 00:36:06.000]   network, you just have block after block after block.
[00:36:06.000 --> 00:36:10.000]   But you don't really have any skip connections, which is this part, this
[00:36:10.000 --> 00:36:12.000]   part that adds the identity.
[00:36:12.000 --> 00:36:16.000]   So having those skip connections is what was the main idea behind our
[00:36:16.000 --> 00:36:18.000]   ResNet research paper.
[00:36:18.000 --> 00:36:23.000]   And then you can see now we can create, like, my ResNet block like this.
[00:36:23.000 --> 00:36:26.000]   So what this does, what's a conf layer?
[00:36:26.000 --> 00:36:31.000]   Let's have a look.
[00:36:31.000 --> 00:36:39.000]   So a conf layer is a fast AI -- it's implemented in fast AI layers.py.
[00:36:39.000 --> 00:36:44.000]   And it creates a sequence of convolution, number of input channels to
[00:36:44.000 --> 00:36:49.000]   number of final channels, and then value and then norm, if there is.
[00:36:49.000 --> 00:36:50.000]   Okay.
[00:36:50.000 --> 00:36:52.000]   So it's basically conf batch norm value.
[00:36:52.000 --> 00:36:54.000]   It's basically just that block.
[00:36:54.000 --> 00:36:57.000]   So a conf layer just implements conf batch norm value, right?
[00:36:57.000 --> 00:36:58.000]   So let's see.
[00:36:58.000 --> 00:37:04.000]   You either have batch norm, then you init your batch norm, then you create
[00:37:04.000 --> 00:37:09.000]   your convolution function, and you add them to the list.
[00:37:09.000 --> 00:37:15.000]   You add your activation batch norm, if there is any.
[00:37:15.000 --> 00:37:19.000]   If you're using instance norm, then you add instance norm instead of batch
[00:37:19.000 --> 00:37:21.000]   norm.
[00:37:21.000 --> 00:37:24.000]   You add that to your list.
[00:37:24.000 --> 00:37:26.000]   You add them to extra.
[00:37:26.000 --> 00:37:28.000]   So extra, is that an nn.sequential?
[00:37:28.000 --> 00:37:30.000]   Where is extra?
[00:37:30.000 --> 00:37:32.000]   Oh, this is already an nn.sequential.
[00:37:32.000 --> 00:37:33.000]   Okay.
[00:37:33.000 --> 00:37:35.000]   That is great.
[00:37:35.000 --> 00:37:40.000]   And it delegates nn.cont today, which is all good.
[00:37:40.000 --> 00:37:42.000]   And you go super.initlist.
[00:37:42.000 --> 00:37:43.000]   I got it.
[00:37:43.000 --> 00:37:44.000]   Okay.
[00:37:44.000 --> 00:37:46.000]   That's really helpful.
[00:37:46.000 --> 00:37:48.000]   So can you see what's happening in this piece of code?
[00:37:48.000 --> 00:37:50.000]   Basically what's happening -- actually, instead of doing this, I should
[00:37:50.000 --> 00:37:52.000]   have gone doc, right?
[00:37:52.000 --> 00:37:54.000]   Doc or docs?
[00:37:54.000 --> 00:37:55.000]   Doc.
[00:37:55.000 --> 00:37:57.000]   I should have gone here.
[00:37:57.000 --> 00:37:59.000]   I should have clicked on that.
[00:37:59.000 --> 00:38:02.000]   Which gives me -- and I should have clicked -- actually, sorry.
[00:38:02.000 --> 00:38:04.000]   I should have clicked on this, and then I should have clicked on
[00:38:04.000 --> 00:38:06.000]   showing docs.
[00:38:06.000 --> 00:38:09.000]   So it will take me to the fast.ai docs, and you can see all the things
[00:38:09.000 --> 00:38:13.000]   that Conflare can do, and you can see more examples on what Conflare
[00:38:13.000 --> 00:38:15.000]   can do.
[00:38:15.000 --> 00:38:18.000]   So I think it's really, really helpful to just go through -- I won't
[00:38:18.000 --> 00:38:21.000]   go into the much details, but for those interested, just go through
[00:38:21.000 --> 00:38:25.000]   this layer, go through the documentation, understand, look at the
[00:38:25.000 --> 00:38:27.000]   source code.
[00:38:27.000 --> 00:38:30.000]   But essentially it's just creating a con, batch norm, relu layer.
[00:38:30.000 --> 00:38:32.000]   That's all it is doing.
[00:38:32.000 --> 00:38:34.000]   So what we can do is we can create our two cons.
[00:38:34.000 --> 00:38:37.000]   See how you have in ResNet, you have first weight layer, then you
[00:38:37.000 --> 00:38:40.000]   have relu, then you have the second weight layer, but you don't have
[00:38:40.000 --> 00:38:42.000]   a batch norm after the second weight layer.
[00:38:42.000 --> 00:38:46.000]   So what it's doing is, like, it's creating a first layer going from
[00:38:46.000 --> 00:38:49.000]   number of input channels to number of final channels, and then it's
[00:38:49.000 --> 00:38:52.000]   creating the second Conflare, which is going from number of final
[00:38:52.000 --> 00:38:55.000]   channels to number of final channels, and then you finally set up the
[00:38:55.000 --> 00:38:58.000]   norm type as batch zero, which just says, okay, please don't set up
[00:38:58.000 --> 00:39:00.000]   batch norm.
[00:39:00.000 --> 00:39:02.000]   So this is where it goes.
[00:39:02.000 --> 00:39:05.000]   Where norm type equals this causes fast.ai to init the gamma of the
[00:39:05.000 --> 00:39:08.000]   weight of the last batch norm layer to zero.
[00:39:08.000 --> 00:39:12.000]   That just means, like, it's going to make -- every time you multiply
[00:39:12.000 --> 00:39:16.000]   in batch norm, remember, you're just trying to normalize things.
[00:39:16.000 --> 00:39:20.000]   So it's like -- I think -- I can't remember the exact formula, but it's
[00:39:20.000 --> 00:39:23.000]   like X star gamma plus beta, something like that.
[00:39:23.000 --> 00:39:26.000]   I can't remember exactly.
[00:39:26.000 --> 00:39:29.000]   But basically you just init that to zero for your second layer.
[00:39:29.000 --> 00:39:31.000]   I'm just sad.
[00:39:31.000 --> 00:39:35.000]   But the problem with this Conv is, again, it can't handle a stride
[00:39:35.000 --> 00:39:38.000]   other than one, and it requires that nr equals nf.
[00:39:38.000 --> 00:39:42.000]   Can you guess why?
[00:39:42.000 --> 00:39:47.000]   I will ask, like -- oh, sorry.
[00:39:47.000 --> 00:39:49.000]   So let's -- let me show you.
[00:39:49.000 --> 00:39:52.000]   Actually, the best way to, like, understand these things -- and, like,
[00:39:52.000 --> 00:39:56.000]   I'm not someone who understands things in theory better, but I'm
[00:39:56.000 --> 00:39:58.000]   someone who understands things in code better.
[00:39:58.000 --> 00:40:02.000]   So what I'm going to do is I'm just going to create a rest block, which
[00:40:02.000 --> 00:40:05.000]   goes from three channels to 32 channels.
[00:40:05.000 --> 00:40:10.000]   And then what I could do is I could pass in my block -- I create my
[00:40:10.000 --> 00:40:19.000]   input as nn -- oh, sorry, torch of Brandon 1, 3, 128 by 128.
[00:40:19.000 --> 00:40:22.000]   And then I could pass in this, and I could check the shape.
[00:40:22.000 --> 00:40:24.000]   And it's giving me an error.
[00:40:24.000 --> 00:40:29.000]   The size must match the size at non-singleton, of course.
[00:40:29.000 --> 00:40:31.000]   So I gave you a hint.
[00:40:31.000 --> 00:40:35.000]   So just by doing this simple example, I gave you a hint on why the
[00:40:35.000 --> 00:40:39.000]   number of input channels needs to be equal to the number of output
[00:40:39.000 --> 00:40:41.000]   channels in this case.
[00:40:41.000 --> 00:40:43.000]   Can you guys guess why?
[00:40:43.000 --> 00:40:46.000]   And I'm just going to see -- is con fashion value a standard?
[00:40:46.000 --> 00:40:49.000]   Yes, it is.
[00:40:49.000 --> 00:40:51.000]   It's definitely a standard.
[00:40:51.000 --> 00:40:53.000]   It's like the standard block.
[00:40:53.000 --> 00:40:56.000]   You have con fashion value, then you have another block, con fashion
[00:40:56.000 --> 00:40:59.000]   value, then you have another block, con fashion value.
[00:40:59.000 --> 00:41:03.000]   So let's say in a deep learning network you have these layers after
[00:41:03.000 --> 00:41:05.000]   layers.
[00:41:05.000 --> 00:41:07.000]   So on like that.
[00:41:07.000 --> 00:41:11.000]   Mostly, like this standard thing is like a con fashion value.
[00:41:11.000 --> 00:41:16.000]   And like in ResNet, each block consists of like -- there's like four
[00:41:16.000 --> 00:41:18.000]   blocks in ResNet.
[00:41:18.000 --> 00:41:20.000]   So this is the first block, third block, fourth block.
[00:41:20.000 --> 00:41:23.000]   And for ResNet, I think 34, it's 3, 4, 6, 3.
[00:41:23.000 --> 00:41:27.000]   Which means you have three con fashion value, you have four con
[00:41:27.000 --> 00:41:31.000]   fashion value, six con fashion value, and then three con fashion value.
[00:41:31.000 --> 00:41:35.000]   So that's just like different versions of the ResNet architecture.
[00:41:35.000 --> 00:41:41.000]   But con fashion value is pretty much -- the con fashion value is pretty
[00:41:41.000 --> 00:41:44.000]   much a standard, Ravi.
[00:41:44.000 --> 00:41:47.000]   The reason you need the input and output is for the identity addition
[00:41:47.000 --> 00:41:49.000]   of the input.
[00:41:49.000 --> 00:41:51.000]   Absolutely.
[00:41:51.000 --> 00:41:53.000]   Thanks very much for answering that question.
[00:41:54.000 --> 00:41:56.000]   So if I go -- so look at this, right?
[00:41:56.000 --> 00:42:01.000]   If I go to -- say if I want my rest block to go from input channels
[00:42:01.000 --> 00:42:04.000]   3 to 32, then what's that going to do?
[00:42:04.000 --> 00:42:09.000]   When I do self.conv x, that's going to be -- the output is going to
[00:42:09.000 --> 00:42:11.000]   be of 32 channels, right?
[00:42:11.000 --> 00:42:13.000]   But my input is still three channels.
[00:42:13.000 --> 00:42:17.000]   So I can't really add the three channels with the 32 channels, which
[00:42:17.000 --> 00:42:21.000]   is the error that we saw before, which says the size of tensor 3 must
[00:42:21.000 --> 00:42:25.000]   match the size of tensor B with 32 channels.
[00:42:25.000 --> 00:42:28.000]   So it's basically saying, like, this is my tensor A, which has three
[00:42:28.000 --> 00:42:30.000]   channels, and this is my tensor B.
[00:42:30.000 --> 00:42:33.000]   The output of this is tensor B, which has 32 channels.
[00:42:33.000 --> 00:42:36.000]   So you can't really add them.
[00:42:36.000 --> 00:42:41.000]   So that's the reason why you need to have the same -- you need to have
[00:42:41.000 --> 00:42:43.000]   the same.
[00:42:43.000 --> 00:42:45.000]   And similarly, you also need to have the same stride.
[00:42:45.000 --> 00:42:49.000]   Because if you change the stride -- because remember, my input is 1
[00:42:49.000 --> 00:42:52.000]   by 3 by 128 by 128, right?
[00:42:52.000 --> 00:42:57.000]   And if you remember what stride and what pooling and what all of these
[00:42:57.000 --> 00:43:00.000]   operations were, which we've discussed in our previous chapters, then
[00:43:00.000 --> 00:43:04.000]   what stride did, stride would go, like, instead of having a single
[00:43:04.000 --> 00:43:07.000]   stride, it would jump and it would have a double stride.
[00:43:07.000 --> 00:43:13.000]   So if I made this a stride 2-conv, so if I did stride equals 2, then
[00:43:13.000 --> 00:43:15.000]   what's that going to do?
[00:43:15.000 --> 00:43:19.000]   Instead of my output being 1 by 3 by 128 by 128, the output is going
[00:43:19.000 --> 00:43:22.000]   to be 1 by 3 by 64 by 64.
[00:43:22.000 --> 00:43:24.000]   So let me run this.
[00:43:24.000 --> 00:43:26.000]   And you can see it gives me an error again.
[00:43:26.000 --> 00:43:30.000]   It says the size of tensor 128 must match the size of tensor B, which
[00:43:30.000 --> 00:43:32.000]   is 64.
[00:43:32.000 --> 00:43:37.000]   So you can see how you can't really have stride 2 and you can't really
[00:43:37.000 --> 00:43:40.000]   have -- you can't really change the number of channels.
[00:43:40.000 --> 00:43:42.000]   Why?
[00:43:42.000 --> 00:43:45.000]   Because we haven't really laid this yet.
[00:43:45.000 --> 00:43:50.000]   So I hope, like, everything that I've explained so far now fits in and
[00:43:50.000 --> 00:43:53.000]   like kind of comes together.
[00:43:53.000 --> 00:43:55.000]   It's like this is why we need pooling.
[00:43:55.000 --> 00:43:59.000]   Because what pooling will do is instead of -- like, we could just not
[00:43:59.000 --> 00:44:01.000]   worry about these things anymore.
[00:44:01.000 --> 00:44:04.000]   And pooling will help us with these things.
[00:44:04.000 --> 00:44:08.000]   So I can say -- here's a rest block using this fix.
[00:44:08.000 --> 00:44:12.000]   And here's -- these are the two issues that I mentioned.
[00:44:12.000 --> 00:44:16.000]   Like this particular rest block, which adds -- so this particular rest
[00:44:16.000 --> 00:44:19.000]   block is just a copy of this one.
[00:44:19.000 --> 00:44:22.000]   The first weight layer is this conf layer.
[00:44:22.000 --> 00:44:25.000]   The second weight layer is this second conf layer.
[00:44:25.000 --> 00:44:28.000]   And then what you do is you take the output of cells.cons and then you
[00:44:28.000 --> 00:44:30.000]   just add it to X.
[00:44:30.000 --> 00:44:34.000]   So you see your input goes to the weight layer, then goes to the second
[00:44:34.000 --> 00:44:36.000]   weight layer.
[00:44:36.000 --> 00:44:39.000]   And then you just add X to F of X.
[00:44:39.000 --> 00:44:44.000]   So I could just have -- so this self.cons of X is my F of X.
[00:44:44.000 --> 00:44:46.000]   And this is just my X.
[00:44:46.000 --> 00:44:49.000]   So I'm still doing X plus F of X.
[00:44:49.000 --> 00:44:54.000]   I hope that explains things.
[00:44:54.000 --> 00:44:57.000]   You would have to use interplay to re-land them up.
[00:44:57.000 --> 00:44:59.000]   Yes, that's generally the idea.
[00:44:59.000 --> 00:45:02.000]   Like you do want to interplay them, but there's different ways.
[00:45:02.000 --> 00:45:06.000]   You could use average pooling with a bigger grid size.
[00:45:06.000 --> 00:45:09.000]   So, again, that's something that I will get into.
[00:45:09.000 --> 00:45:11.000]   I will explain that, what's happening there.
[00:45:11.000 --> 00:45:13.000]   Just give me a tick.
[00:45:13.000 --> 00:45:15.000]   But are there any other questions?
[00:45:15.000 --> 00:45:20.000]   Are there any questions on, like, what we've discussed so far and before
[00:45:20.000 --> 00:45:22.000]   this?
[00:45:22.000 --> 00:45:25.000]   Because it's really important in understanding rest nets that we go
[00:45:25.000 --> 00:45:28.000]   step by step.
[00:45:28.000 --> 00:45:30.000]   So, again, so here's a rest block.
[00:45:30.000 --> 00:45:34.000]   So, again, it says to fix this, we need to change the shape of X to
[00:45:34.000 --> 00:45:37.000]   match the shape of self.cons, correct?
[00:45:37.000 --> 00:45:45.000]   Because let me actually call this as F of X, right?
[00:45:45.000 --> 00:45:47.000]   So F of X is this.
[00:45:47.000 --> 00:45:52.000]   And I'm just going to -- all I'm doing here is, like, I'm making this
[00:45:52.000 --> 00:45:56.000]   look exactly like this.
[00:45:56.000 --> 00:45:58.000]   Right?
[00:45:58.000 --> 00:46:01.000]   So let me see now how both of these are the same.
[00:46:01.000 --> 00:46:05.000]   So I'm just going to call this actually -- let me call this self.weight
[00:46:05.000 --> 00:46:10.000]   layer one.
[00:46:10.000 --> 00:46:15.000]   I'm just trying to make this exactly as the image.
[00:46:15.000 --> 00:46:27.000]   So I'm just going to make this -- self.weight layer two.
[00:46:27.000 --> 00:46:29.000]   And, like, these are things you could do as well.
[00:46:29.000 --> 00:46:34.000]   Because I think it really helps with the understanding.
[00:46:34.000 --> 00:46:39.000]   Self.weight layer two.
[00:46:39.000 --> 00:46:42.000]   Self.weight layer one.
[00:46:42.000 --> 00:46:44.000]   And you have X.
[00:46:44.000 --> 00:46:46.000]   And then you do this.
[00:46:46.000 --> 00:46:48.000]   So let's see, does that work?
[00:46:48.000 --> 00:46:50.000]   Yes, it does.
[00:46:50.000 --> 00:46:52.000]   So see what I did?
[00:46:52.000 --> 00:46:54.000]   I just kind of restructured my code so the code matches exactly this
[00:46:54.000 --> 00:46:56.000]   image.
[00:46:56.000 --> 00:46:58.000]   So I created my self.weight layer.
[00:46:58.000 --> 00:47:00.000]   I created my self.weight layer two.
[00:47:00.000 --> 00:47:04.000]   And finally, my F of X is this output from the two weight layers.
[00:47:04.000 --> 00:47:06.000]   So F of X is what?
[00:47:06.000 --> 00:47:08.000]   Self.weight layer one, which is this output.
[00:47:08.000 --> 00:47:10.000]   And then you do the output of the self.weight layer two.
[00:47:10.000 --> 00:47:13.000]   So you get your final output, which is F of X.
[00:47:13.000 --> 00:47:17.000]   And your final result that you return is X plus F of X.
[00:47:17.000 --> 00:47:22.000]   Now, what you could do is you could add stuff, like, print.
[00:47:22.000 --> 00:47:26.000]   It's really, really bad to use print here.
[00:47:26.000 --> 00:47:28.000]   Or, like, print in a forward.
[00:47:28.000 --> 00:47:30.000]   Please don't do it anywhere outside.
[00:47:30.000 --> 00:47:33.000]   And don't tell anybody that you learned this from me.
[00:47:33.000 --> 00:47:35.000]   Don't take my name.
[00:47:35.000 --> 00:47:38.000]   Because it will -- it's really -- people will kill me.
[00:47:38.000 --> 00:47:42.000]   But what you could do, just the easiest way that I do, is just do,
[00:47:42.000 --> 00:47:46.000]   like, F of X shape.
[00:47:46.000 --> 00:47:49.000]   Is F of X.
[00:47:49.000 --> 00:47:52.000]   Shape.
[00:47:52.000 --> 00:47:55.000]   And then X shape is X.shape.
[00:47:55.000 --> 00:48:01.000]   So now you can see it's telling me that for the input, when I go 3 by 3,
[00:48:01.000 --> 00:48:03.000]   my F of X shape is this.
[00:48:03.000 --> 00:48:05.000]   And my X shape is this.
[00:48:05.000 --> 00:48:07.000]   So I can add my input to the F of X.
[00:48:07.000 --> 00:48:13.000]   But when I go 3 by 32, it's telling me my F of X shape is 1 by 32 by 128
[00:48:13.000 --> 00:48:15.000]   by 128.
[00:48:15.000 --> 00:48:17.000]   But my X shape is 1 by 3.
[00:48:17.000 --> 00:48:19.000]   So I can pass it.
[00:48:19.000 --> 00:48:21.000]   And I can add another parameter.
[00:48:21.000 --> 00:48:23.000]   So I can pass it.
[00:48:23.000 --> 00:48:29.000]   Or if I go stride equals -- I'll just add this.
[00:48:29.000 --> 00:48:31.000]   Another parameter called stride.
[00:48:31.000 --> 00:48:33.000]   So I can pass it.
[00:48:33.000 --> 00:48:36.000]   And if I say stride equals 2.
[00:48:36.000 --> 00:48:39.000]   Now you can see why this is a problem.
[00:48:39.000 --> 00:48:43.000]   Because my input was 128 by 128.
[00:48:43.000 --> 00:48:46.000]   But my F of X is 64 by 64.
[00:48:46.000 --> 00:48:48.000]   So I'm adding X to F of X.
[00:48:48.000 --> 00:48:51.000]   But both X and F of X are of the same shape.
[00:48:51.000 --> 00:48:57.000]   That's essentially all I'm trying to explain here.
[00:48:57.000 --> 00:48:59.000]   Okay.
[00:48:59.000 --> 00:49:03.000]   So I'm just going to call this stride 1.
[00:49:03.000 --> 00:49:07.000]   So this is exactly what's being mentioned in this part here.
[00:49:07.000 --> 00:49:11.000]   To fix this, like to fix all of these problems, we need to change a way -- we
[00:49:11.000 --> 00:49:15.000]   need to change the shape of X to match the result of cell.cons.
[00:49:15.000 --> 00:49:18.000]   Like we can't really half the grid size or do all of those things.
[00:49:18.000 --> 00:49:22.000]   So in that case, what we could do is we could create an updated rest block
[00:49:22.000 --> 00:49:25.000]   that looks something like this.
[00:49:25.000 --> 00:49:28.000]   You create your cell.cons as your cons block.
[00:49:28.000 --> 00:49:30.000]   Right?
[00:49:30.000 --> 00:49:36.000]   You have your -- basically, in your -- if the number of channels is
[00:49:36.000 --> 00:49:43.000]   different -- actually, let me try and explain this in here.
[00:49:43.000 --> 00:49:50.000]   So I have, say, my input, 3 channel, 128 by 128.
[00:49:50.000 --> 00:49:52.000]   And this is my conf layer.
[00:49:52.000 --> 00:49:55.000]   So you have your input, and then this is my skip connection.
[00:49:55.000 --> 00:49:57.000]   Right?
[00:49:57.000 --> 00:49:59.000]   And you want to be able to add both of them.
[00:49:59.000 --> 00:50:07.000]   So if this is -- if this becomes stride 2, right, then my output becomes
[00:50:07.000 --> 00:50:11.000]   1 by 3 by 64 by 64.
[00:50:11.000 --> 00:50:14.000]   I hope there's no confusion, like, why stride 2 does 64 by 64.
[00:50:14.000 --> 00:50:17.000]   Because stride 2 is basically, when you're doing convolution with
[00:50:17.000 --> 00:50:19.000]   stride 2, it's just skipping one step.
[00:50:19.000 --> 00:50:23.000]   So instead of, like, getting the same output as 128 by 128, you get half
[00:50:23.000 --> 00:50:25.000]   the output.
[00:50:25.000 --> 00:50:28.000]   And this is something we discussed in our last chapter.
[00:50:28.000 --> 00:50:32.000]   So now you can't really add this to 128 by 128.
[00:50:32.000 --> 00:50:36.000]   So what you could do is to your skip branch, you could add something
[00:50:36.000 --> 00:50:38.000]   called a pooling operation.
[00:50:38.000 --> 00:50:42.000]   So you have a pooling operation that averages a 2 by 2 grid to give you
[00:50:42.000 --> 00:50:44.000]   one pixel.
[00:50:44.000 --> 00:50:47.000]   So it averages a 2 by 2 grid to give you one pixel.
[00:50:47.000 --> 00:50:51.000]   So when you average 128 by 128 using this pooling operation, you get an
[00:50:51.000 --> 00:50:55.000]   output of, like, 1 by 3 by 64 by 64.
[00:50:55.000 --> 00:50:57.000]   And then you could add the two.
[00:50:57.000 --> 00:50:59.000]   So let me show you that.
[00:50:59.000 --> 00:51:05.000]   Ignore this for now, because that's not something we are worried about.
[00:51:05.000 --> 00:51:08.000]   So see how I'm, like, I constantly change and update code.
[00:51:08.000 --> 00:51:11.000]   Like, I don't take the code as is.
[00:51:11.000 --> 00:51:14.000]   If I want to increase my understanding, I'm constantly changing the way I
[00:51:14.000 --> 00:51:17.000]   code.
[00:51:17.000 --> 00:51:19.000]   So you have your conf block.
[00:51:19.000 --> 00:51:23.000]   And now let me create my block as rest block.
[00:51:23.000 --> 00:51:27.000]   And I could pass in, say, 3 by 32.
[00:51:27.000 --> 00:51:30.000]   Actually, let's keep it 3 by 3.
[00:51:30.000 --> 00:51:34.000]   My input, again, is torsh.brandon.
[00:51:34.000 --> 00:51:37.000]   I know today is a slightly bit more complex than all the other ones, but I
[00:51:37.000 --> 00:51:42.000]   hope I'm able to explain it in a way that doesn't look that complex.
[00:51:42.000 --> 00:51:48.000]   So you can see my output now, like, it's able to add both of these up.
[00:51:48.000 --> 00:51:55.000]   So I'm just going to call this as f of x again is this.
[00:51:55.000 --> 00:52:05.000]   And now my shortcut output, or should I call it skip, is this.
[00:52:05.000 --> 00:52:13.000]   And what I'm doing is f of x plus skip instead of f of x plus x.
[00:52:13.000 --> 00:52:14.000]   Okay.
[00:52:14.000 --> 00:52:16.000]   This should be like that.
[00:52:16.000 --> 00:52:17.000]   Cool.
[00:52:17.000 --> 00:52:24.000]   So now what I can do, again, is use that stupid print statement, which I
[00:52:24.000 --> 00:52:29.000]   shouldn't have, which you shouldn't do it publicly outside of here.
[00:52:29.000 --> 00:52:33.000]   But feel free to do it in Fastbook.
[00:52:33.000 --> 00:52:35.000]   I can just say my output is this.
[00:52:35.000 --> 00:52:41.000]   And I can say my skip shape is skip.shape.
[00:52:41.000 --> 00:52:45.000]   Now you can see, okay, for 3 by 3, you're getting, like, my input shape is
[00:52:45.000 --> 00:52:49.000]   1 by 3 by 128 and my output shape is 1 by 3 by -- but what happens if I
[00:52:49.000 --> 00:52:51.000]   pass now stride equals 2?
[00:52:51.000 --> 00:52:52.000]   Okay.
[00:52:52.000 --> 00:52:57.000]   In that case, the f of x shape becomes 1 by 3 by 64 by 64 and my skip
[00:52:57.000 --> 00:53:00.000]   shape becomes also 1 by 3 by 64 by 64.
[00:53:00.000 --> 00:53:02.000]   So we fixed this problem, remember?
[00:53:02.000 --> 00:53:16.000]   So if I had -- if I did the same thing with this instead, this was a
[00:53:16.000 --> 00:53:17.000]   problem, right?
[00:53:17.000 --> 00:53:22.000]   Because my f of x output was 1 by 3 by 64 by 64 any time I did a
[00:53:22.000 --> 00:53:28.000]   stride 2 and my input was still 128 by 128.
[00:53:28.000 --> 00:53:32.000]   So we fixed -- so to fix that, all we did was we added a pooling
[00:53:32.000 --> 00:53:33.000]   operation.
[00:53:33.000 --> 00:53:34.000]   That's it.
[00:53:34.000 --> 00:53:38.000]   And in the pooling operation, when we say nn.average.pool2d, stride,
[00:53:38.000 --> 00:53:42.000]   basically we just pass in 2, because that's what the stride is in this
[00:53:42.000 --> 00:53:43.000]   case.
[00:53:43.000 --> 00:53:48.000]   What that's going to do is it's going to make sure -- so if you have
[00:53:48.000 --> 00:53:54.000]   -- what that's going to do is if you have an input of 128 by 128 and so
[00:53:54.000 --> 00:53:58.000]   on, it's going to go through like 2 by 2 blocks and just create one
[00:53:58.000 --> 00:53:59.000]   pixel.
[00:53:59.000 --> 00:54:02.000]   2 by 2 blocks and just create one pixel.
[00:54:02.000 --> 00:54:05.000]   2 by 2 blocks and just create one pixel.
[00:54:05.000 --> 00:54:12.000]   So in the end what you get is you get a 64 by 64 instead of 128 by 128.
[00:54:12.000 --> 00:54:17.000]   So now you can add the two together, because you're doing pooling on
[00:54:17.000 --> 00:54:19.000]   your skip, so you can add them two together.
[00:54:19.000 --> 00:54:22.000]   And now -- so we fixed the one problem.
[00:54:22.000 --> 00:54:23.000]   So we had two problems, right?
[00:54:23.000 --> 00:54:26.000]   One, that the channels could not -- the channels had to be the same,
[00:54:26.000 --> 00:54:29.000]   and the second problem was that the stride had to be the same.
[00:54:29.000 --> 00:54:32.000]   So we've just fixed the problem with stride by adding the pooling
[00:54:32.000 --> 00:54:33.000]   layer.
[00:54:33.000 --> 00:54:36.000]   I'll go and check again if there's any questions.
[00:54:36.000 --> 00:54:38.000]   No questions yet.
[00:54:38.000 --> 00:54:42.000]   So then the next problem that we have to fix is the number of channels.
[00:54:42.000 --> 00:54:45.000]   Because right now, even with this implementation, I can't go from 3 to
[00:54:45.000 --> 00:54:46.000]   32.
[00:54:46.000 --> 00:54:47.000]   It's going to give me an error.
[00:54:47.000 --> 00:55:01.000]   It's going to say, oh, the channels -- so see how this is the problem?
[00:55:01.000 --> 00:55:25.000]   So I can't go from 3 to 32 channels in this rest block.
[00:55:25.000 --> 00:55:28.000]   I can't still have the different number of channels in this block.
[00:55:28.000 --> 00:55:34.000]   We're still trying to replicate this ResNet block, but even in this
[00:55:34.000 --> 00:55:37.000]   block now, I can't have a different number of channels in my output
[00:55:37.000 --> 00:55:38.000]   than the input.
[00:55:38.000 --> 00:55:39.000]   Why is that?
[00:55:39.000 --> 00:55:45.000]   That's because if I do that, the output is 1 by 32 by 64 by 64, but
[00:55:45.000 --> 00:55:48.000]   the input is still 1 by 3 by 64 by 64.
[00:55:48.000 --> 00:55:52.000]   So even though we fixed this stride, we still haven't fixed the --
[00:55:52.000 --> 00:55:54.000]   being able to change the number of convolutions.
[00:55:54.000 --> 00:56:03.000]   So the first thing that we added here was pooling to fix the stride.
[00:56:03.000 --> 00:56:07.000]   And now what we want to do is we want our channels to be like 32 or
[00:56:07.000 --> 00:56:09.000]   some different number of channels.
[00:56:09.000 --> 00:56:12.000]   And for that, what we're going to add is conf 1 by 1.
[00:56:12.000 --> 00:56:16.000]   And what conf 1 by 1 will do is you can use it to change the number of
[00:56:16.000 --> 00:56:17.000]   channels.
[00:56:17.000 --> 00:56:25.000]   So instead of like you have your input, you have your f of x.
[00:56:25.000 --> 00:56:27.000]   So you get your f of x.
[00:56:27.000 --> 00:56:31.000]   And instead of your identity being just like that, you add two things
[00:56:31.000 --> 00:56:32.000]   to it.
[00:56:32.000 --> 00:56:36.000]   The first thing that you add is pooling.
[00:56:36.000 --> 00:56:41.000]   And the second thing that you add is conf 1 by 1.
[00:56:41.000 --> 00:56:45.000]   So now this is something that can deal with a different number of
[00:56:45.000 --> 00:56:46.000]   channels.
[00:56:46.000 --> 00:56:48.000]   It can deal with a different spatial size.
[00:56:48.000 --> 00:56:51.000]   And if you don't have different number of channels, if you don't have
[00:56:51.000 --> 00:56:54.000]   stride, then you don't add this to your skip connection.
[00:56:54.000 --> 00:56:56.000]   So see how it's doing this?
[00:56:56.000 --> 00:57:02.000]   So now I can add self.idconv or should I just call it self.skipconv
[00:57:02.000 --> 00:57:06.000]   is a no operation if the number of channels are same.
[00:57:06.000 --> 00:57:11.000]   Otherwise, you go conf layer of 1 by 1 kernel and you just update the
[00:57:11.000 --> 00:57:13.000]   number of channels.
[00:57:13.000 --> 00:57:19.000]   So I could just do my skip now becomes self.skipconv output.
[00:57:19.000 --> 00:57:22.000]   And now I should be able to add them two together.
[00:57:22.000 --> 00:57:23.000]   So and there we go.
[00:57:23.000 --> 00:57:24.000]   That fixed it.
[00:57:24.000 --> 00:57:27.000]   Now my f of x is 1 by 32 by 64 by 64.
[00:57:27.000 --> 00:57:31.000]   And my skip shape is also 1 by 32 by 64 by 64.
[00:57:31.000 --> 00:57:34.000]   So now at this point, this is something that works perfectly,
[00:57:34.000 --> 00:57:36.000]   perfectly well.
[00:57:36.000 --> 00:57:44.000]   Let me paste this in the forums.
[00:57:44.000 --> 00:57:45.000]   There you go.
[00:57:45.000 --> 00:57:46.000]   So what's the question?
[00:57:46.000 --> 00:57:47.000]   Sorry.
[00:57:47.000 --> 00:57:52.000]   If the number of channels is different, we could also have concatenated
[00:57:52.000 --> 00:58:02.000]   x instead of using and adding them.
[00:58:02.000 --> 00:58:05.000]   But if you're concatenating them and you're not adding them, then you're
[00:58:05.000 --> 00:58:07.000]   not really following ResNet.
[00:58:07.000 --> 00:58:09.000]   Right?
[00:58:09.000 --> 00:58:15.000]   And what ResNet does is it adds the inputs to the outputs.
[00:58:15.000 --> 00:58:20.000]   So by concatenating, you're actually doing something completely
[00:58:20.000 --> 00:58:23.000]   different than what we're supposed to do.
[00:58:23.000 --> 00:58:26.000]   Because in ResNet, what we have to do is we want to be able to add
[00:58:26.000 --> 00:58:27.000]   them together.
[00:58:27.000 --> 00:58:30.000]   And the only way we can add them together is if the spatial dimensions
[00:58:30.000 --> 00:58:33.000]   are the same and if the number of channels are the same.
[00:58:33.000 --> 00:58:37.000]   The number of channels can be made the same by adding a convolution
[00:58:37.000 --> 00:58:42.000]   layer and the number of -- the spatial dimension can be made the same
[00:58:42.000 --> 00:58:44.000]   by adding a stride.
[00:58:44.000 --> 00:58:47.000]   Sorry, by adding a pooling layer.
[00:58:47.000 --> 00:58:51.000]   So I hope that answers that question.
[00:58:51.000 --> 00:58:54.000]   Because if you're concatenating, you're pretty much doing something
[00:58:54.000 --> 00:58:56.000]   similar to DenseNet instead of ResNet.
[00:58:56.000 --> 00:58:59.000]   But I'll skip on that.
[00:58:59.000 --> 00:59:03.000]   So see on that, so now you can like -- now we can use this res block.
[00:59:03.000 --> 00:59:05.000]   I'll take this away.
[00:59:05.000 --> 00:59:08.000]   So now you can use this res block and you can create your learner.
[00:59:08.000 --> 00:59:12.000]   You can even pass any stride that you want, even stride 3, but pretty
[00:59:12.000 --> 00:59:14.000]   much 2.
[00:59:14.000 --> 00:59:18.000]   You can pass in your NINF, basically a different -- basically any
[00:59:18.000 --> 00:59:20.000]   number of channels.
[00:59:20.000 --> 00:59:22.000]   And now remember our get model function.
[00:59:22.000 --> 00:59:28.000]   So our get model function was -- where are you, get model function?
[00:59:28.000 --> 00:59:30.000]   Here it is.
[00:59:30.000 --> 00:59:37.000]   So now we can still use this -- there we go.
[00:59:37.000 --> 00:59:39.000]   So you can still use this get model function.
[00:59:39.000 --> 00:59:43.000]   But in this case, our block is this res block.
[00:59:43.000 --> 00:59:45.000]   Right?
[00:59:45.000 --> 00:59:47.000]   Because that's what this block function has been defined now, is this
[00:59:47.000 --> 00:59:49.000]   res block.
[00:59:49.000 --> 00:59:53.000]   So instead of like what we just did was like instead of creating our
[00:59:53.000 --> 00:59:57.000]   block as something which is just one single conv layer, now we've just
[00:59:57.000 --> 01:00:01.000]   updated that block to be two conv layers with a res net or a skip
[01:00:01.000 --> 01:00:04.000]   connection in it, a residual path and a skip path.
[01:00:04.000 --> 01:00:08.000]   So we've just implemented res net block from scratch.
[01:00:08.000 --> 01:00:12.000]   And I could just then do this, I could get my model and then I could
[01:00:12.000 --> 01:00:14.000]   train it.
[01:00:14.000 --> 01:00:16.000]   And that's it.
[01:00:16.000 --> 01:00:19.000]   And we've implemented res net from scratch.
[01:00:19.000 --> 01:00:23.000]   And then you'll see one thing that was mentioned in this visualizing
[01:00:23.000 --> 01:00:27.000]   loss landscape of neural nets paper is that what adding a skip
[01:00:27.000 --> 01:00:31.000]   connection actually does is that without the skip connection, your
[01:00:31.000 --> 01:00:35.000]   loss function looks really, really bumpy and it looks really, really
[01:00:35.000 --> 01:00:38.000]   -- you know, now the model has to like go through, jump from here to
[01:00:38.000 --> 01:00:41.000]   here, it could get stuck in this local minima, it could get stuck in
[01:00:41.000 --> 01:00:45.000]   that local minima, but adding a skip connection actually makes the
[01:00:45.000 --> 01:00:47.000]   loss landscape a lot more smoother.
[01:00:47.000 --> 01:00:51.000]   So you can see now, instead of like having an accuracy of I think --
[01:00:51.000 --> 01:00:53.000]   what was the accuracy before?
[01:00:53.000 --> 01:00:55.000]   About 30s?
[01:00:55.000 --> 01:00:57.000]   Oh, 65.
[01:00:57.000 --> 01:00:59.000]   Okay.
[01:00:59.000 --> 01:01:01.000]   Am I doing this correctly?
[01:01:01.000 --> 01:01:03.000]   Yeah.
[01:01:03.000 --> 01:01:05.000]   But anyway, we are making good progress.
[01:01:05.000 --> 01:01:08.000]   I must have made some mistake in training this network, but we're
[01:01:08.000 --> 01:01:11.000]   actually now training a deeper network, we're actually now training a
[01:01:11.000 --> 01:01:14.000]   network with -- oh, actually, we're still training five epochs and we
[01:01:14.000 --> 01:01:16.000]   can now make this even deeper.
[01:01:16.000 --> 01:01:18.000]   I guess is the point.
[01:01:18.000 --> 01:01:21.000]   So we're just left with a state of the art network, which is a state
[01:01:21.000 --> 01:01:23.000]   of the art ResNet.
[01:01:23.000 --> 01:01:25.000]   We're just left with -- sorry.
[01:01:25.000 --> 01:01:29.000]   We're just left with not much, because in this part, we've just
[01:01:29.000 --> 01:01:31.000]   implemented ResNet from scratch already.
[01:01:31.000 --> 01:01:35.000]   Next week when you guys come back, we are going to look at the state
[01:01:35.000 --> 01:01:38.000]   of the art ResNet, which is like more stuff that you can do, things
[01:01:38.000 --> 01:01:42.000]   like mixup, and we will see what a stem is and we will see like how
[01:01:42.000 --> 01:01:46.000]   the ResNet architecture, how like this is the implementation of the
[01:01:46.000 --> 01:01:49.000]   ResNet architecture, and then we should be able to implement like all
[01:01:49.000 --> 01:01:52.000]   of these different architectures, we will be able to -- we will look at
[01:01:52.000 --> 01:01:54.000]   a bottleneck layer and what that means.
[01:01:54.000 --> 01:01:58.000]   But feel free to read -- feel free to read after, because there's not
[01:01:58.000 --> 01:02:00.000]   much left.
[01:02:00.000 --> 01:02:07.000]   Everything that I've explained today until 1.2.1, we're left with 1.2.2
[01:02:07.000 --> 01:02:09.000]   and 1.2.3, and that's pretty much it.
[01:02:09.000 --> 01:02:11.000]   But feel free to read through that.
[01:02:11.000 --> 01:02:15.000]   I think it would really be easy for you guys from here on to understand
[01:02:15.000 --> 01:02:17.000]   those things.
[01:02:17.000 --> 01:02:21.000]   But with that being said, thanks, everybody, for joining me this week.
[01:02:21.000 --> 01:02:26.000]   Please, please join me for this paper reading group.
[01:02:26.000 --> 01:02:30.000]   So we're going -- we're doing the ResNet live coding from scratch in
[01:02:30.000 --> 01:02:34.000]   PyTorch as well, so please, please join me for this live coding in
[01:02:34.000 --> 01:02:36.000]   PyTorch session as well next week.
[01:02:36.000 --> 01:02:39.000]   But apart from that, thanks very much for joining me today, and I'll
[01:02:39.000 --> 01:02:41.000]   see you guys next week.
[01:02:41.000 --> 01:02:43.000]   And bye.
[01:02:43.000 --> 01:02:45.000]   [end]
[01:02:45.000 --> 01:02:47.000]   [end]
[01:02:47.000 --> 01:02:49.000]   [end]
[01:02:49.000 --> 01:02:51.000]   [end]
[01:02:51.000 --> 01:02:53.000]   [end]
[01:02:53.000 --> 01:02:55.000]   [end]
[01:02:55.000 --> 01:02:57.000]   [end]
[01:02:57.000 --> 01:02:59.000]   [end]
[01:02:59.000 --> 01:03:09.000]   [BLANK_AUDIO]

