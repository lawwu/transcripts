
[00:00:00.000 --> 00:00:03.280]   The following is a conversation with Dan Kokorov,
[00:00:03.280 --> 00:00:08.280]   VP of Engineering at Rev.ai, which is by many metrics,
[00:00:08.280 --> 00:00:12.380]   the best speech to text AI engine in the world.
[00:00:12.380 --> 00:00:15.440]   Rev in general is a company that does captioning
[00:00:15.440 --> 00:00:20.020]   and transcription of audio by humans and by AI.
[00:00:20.020 --> 00:00:22.680]   I've been using their services for a couple of years now
[00:00:22.680 --> 00:00:26.960]   and planning to use Rev to add both captions and transcripts
[00:00:26.960 --> 00:00:30.520]   to some of the previous and future episodes of this podcast
[00:00:30.520 --> 00:00:33.160]   to make it easier for people to read through
[00:00:33.160 --> 00:00:36.480]   the conversation or reference various parts of the episode,
[00:00:36.480 --> 00:00:39.800]   since that's something that quite a few people requested.
[00:00:39.800 --> 00:00:41.380]   I'll probably do a separate video on that
[00:00:41.380 --> 00:00:45.480]   with links on the podcast website
[00:00:45.480 --> 00:00:48.440]   so people can provide suggestions and improvements there.
[00:00:48.440 --> 00:00:50.360]   Quick mention of our sponsors,
[00:00:50.360 --> 00:00:53.440]   Athletic Greens, all-in-one nutrition drink,
[00:00:53.440 --> 00:00:56.520]   Blinkist app that summarizes books,
[00:00:56.520 --> 00:01:00.160]   Business Wars podcast and Cash App.
[00:01:00.160 --> 00:01:03.440]   So the choice is health, wisdom or money.
[00:01:03.440 --> 00:01:04.960]   Choose wisely my friends.
[00:01:04.960 --> 00:01:07.840]   And if you wish, click the sponsor links below
[00:01:07.840 --> 00:01:10.880]   to get a discount and to support this podcast.
[00:01:10.880 --> 00:01:13.240]   As a side note, let me say that I reached out to Dan
[00:01:13.240 --> 00:01:15.080]   and the Rev team for a conversation
[00:01:15.080 --> 00:01:19.940]   because I've been using and genuinely loving their service
[00:01:19.940 --> 00:01:22.480]   and really curious about how it works.
[00:01:22.480 --> 00:01:24.980]   I previously talked to the head of Adobe Research
[00:01:24.980 --> 00:01:26.440]   for the same reason.
[00:01:26.440 --> 00:01:28.720]   For me, there's a bunch of products,
[00:01:28.720 --> 00:01:31.120]   usually it's software that comes along
[00:01:31.120 --> 00:01:33.160]   and just makes my life way easier.
[00:01:33.160 --> 00:01:36.080]   Examples are Adobe Premiere for video editing,
[00:01:36.080 --> 00:01:38.680]   iZotope RX for cleaning up audio,
[00:01:38.680 --> 00:01:41.160]   AutoHotKey on Windows for automating keyboard
[00:01:41.160 --> 00:01:43.480]   and mouse tasks,
[00:01:43.480 --> 00:01:46.080]   Emacs as an ID for everything,
[00:01:46.080 --> 00:01:48.280]   including the universe itself.
[00:01:48.280 --> 00:01:50.720]   I can keep on going, but you get the idea.
[00:01:50.720 --> 00:01:52.860]   I just like talking to people who create things
[00:01:52.860 --> 00:01:54.420]   I'm a big fan of.
[00:01:54.420 --> 00:01:56.480]   That said, after doing this conversation,
[00:01:56.480 --> 00:02:01.120]   the folks at Rev.ai offered to sponsor this podcast
[00:02:01.120 --> 00:02:02.720]   in the coming months.
[00:02:02.720 --> 00:02:06.440]   This conversation is not sponsored by the guest.
[00:02:06.440 --> 00:02:08.280]   It probably goes without saying,
[00:02:08.280 --> 00:02:10.020]   but I should say it anyway,
[00:02:10.020 --> 00:02:13.240]   that you cannot buy your way onto this podcast.
[00:02:13.240 --> 00:02:15.480]   I don't know why you would want to.
[00:02:15.480 --> 00:02:18.320]   I wanted to bring this up to make a specific point
[00:02:18.320 --> 00:02:20.920]   that no sponsor will ever influence
[00:02:20.920 --> 00:02:22.740]   what I do on this podcast,
[00:02:22.740 --> 00:02:23.920]   or to the best of my ability,
[00:02:23.920 --> 00:02:25.600]   influence what I think.
[00:02:25.600 --> 00:02:27.800]   I wasn't really thinking about this,
[00:02:27.800 --> 00:02:30.200]   for example, when I interviewed Jack Dorsey,
[00:02:30.200 --> 00:02:32.340]   who is the CEO of Square
[00:02:32.340 --> 00:02:35.100]   that happens to be sponsoring this podcast,
[00:02:35.100 --> 00:02:37.080]   but I should really make it explicit.
[00:02:37.080 --> 00:02:40.100]   I will never take money for bringing a guest on.
[00:02:40.100 --> 00:02:43.040]   Every guest on this podcast is someone
[00:02:43.040 --> 00:02:44.920]   I genuinely am curious to talk to,
[00:02:44.920 --> 00:02:48.200]   or just genuinely love something they've created.
[00:02:48.200 --> 00:02:50.720]   As I sometimes get criticized for,
[00:02:50.720 --> 00:02:52.460]   I'm just a fan of people,
[00:02:52.460 --> 00:02:54.280]   and that's who I talk to.
[00:02:54.280 --> 00:02:56.260]   As I also talk about way too much,
[00:02:56.260 --> 00:02:58.880]   money is really never a consideration.
[00:02:58.880 --> 00:03:03.000]   In general, no amount of money can buy my integrity.
[00:03:03.000 --> 00:03:04.800]   That's true for this podcast,
[00:03:04.800 --> 00:03:06.800]   and that's true for anything else I do.
[00:03:06.800 --> 00:03:10.320]   If you enjoy this thing, subscribe on YouTube,
[00:03:10.320 --> 00:03:12.600]   review on Apple Podcast,
[00:03:12.600 --> 00:03:13.920]   follow on Spotify,
[00:03:13.920 --> 00:03:15.360]   support on Patreon,
[00:03:15.360 --> 00:03:18.240]   or connect with me on Twitter @LexFriedman.
[00:03:18.240 --> 00:03:22.000]   And now, here's my conversation with Dan Kokorov.
[00:03:22.820 --> 00:03:25.620]   You mentioned science fiction on the phone,
[00:03:25.620 --> 00:03:28.060]   so let's go with the ridiculous first.
[00:03:28.060 --> 00:03:31.060]   What's the greatest sci-fi novel of all time,
[00:03:31.060 --> 00:03:32.380]   in your view?
[00:03:32.380 --> 00:03:35.980]   And maybe, what ideas do you find
[00:03:35.980 --> 00:03:37.860]   philosophically fascinating about it?
[00:03:37.860 --> 00:03:41.220]   - The greatest sci-fi novel of all time is Dune,
[00:03:41.220 --> 00:03:44.140]   and the second greatest is the Children of Dune,
[00:03:44.140 --> 00:03:47.820]   and the third greatest is the God Emperor of Dune, so.
[00:03:47.820 --> 00:03:50.360]   I'm a huge fan of the whole series.
[00:03:50.360 --> 00:03:53.980]   I mean, it's just an incredible world that he created.
[00:03:53.980 --> 00:03:55.740]   And I don't know if you've read the book or not.
[00:03:55.740 --> 00:03:56.580]   - No, I have not.
[00:03:56.580 --> 00:03:58.820]   It's one of my biggest regrets,
[00:03:58.820 --> 00:04:01.740]   especially 'cause the new movie is coming out.
[00:04:01.740 --> 00:04:03.940]   Everyone's super excited about it.
[00:04:03.940 --> 00:04:06.300]   It's ridiculous to say,
[00:04:06.300 --> 00:04:07.460]   and sorry to interrupt,
[00:04:07.460 --> 00:04:10.280]   is that I used to play the video game.
[00:04:10.280 --> 00:04:11.940]   It used to be Dune.
[00:04:11.940 --> 00:04:14.340]   I guess you would call that real-time strategy.
[00:04:14.340 --> 00:04:15.900]   - Right, right, I think I remember that game.
[00:04:15.900 --> 00:04:18.060]   - Yeah, it was kind of awesome, '90s or something.
[00:04:18.060 --> 00:04:20.480]   I think I played it, actually, when I was in Russia.
[00:04:20.480 --> 00:04:21.960]   - I definitely remember it.
[00:04:21.960 --> 00:04:23.580]   I was not in Russia anymore.
[00:04:23.580 --> 00:04:26.080]   I think at the time that I used to live in Russia,
[00:04:26.080 --> 00:04:29.760]   I think video games were about the suspicion of Pong.
[00:04:29.760 --> 00:04:32.400]   I think Pong was pretty much the greatest game
[00:04:32.400 --> 00:04:33.960]   I ever got to play in Russia,
[00:04:33.960 --> 00:04:35.920]   which was still a privilege in that age.
[00:04:35.920 --> 00:04:37.240]   - So you didn't get color?
[00:04:37.240 --> 00:04:38.600]   You didn't get like a--
[00:04:38.600 --> 00:04:40.880]   - Well, so I left Russia in 1991, right?
[00:04:40.880 --> 00:04:41.720]   - '91, okay.
[00:04:41.720 --> 00:04:43.960]   - So I always wanted to feel like a kid
[00:04:43.960 --> 00:04:45.200]   'cause my mom was a programmer,
[00:04:45.200 --> 00:04:47.160]   so I would go to her work.
[00:04:47.160 --> 00:04:49.180]   I would take the Metro.
[00:04:49.180 --> 00:04:51.260]   I'd go to her work and play on, I guess,
[00:04:51.260 --> 00:04:53.860]   the equivalent of a 286 PC, you know?
[00:04:53.860 --> 00:04:56.140]   - Nice, with floppy disks.
[00:04:56.140 --> 00:04:56.980]   - Yes, yes.
[00:04:56.980 --> 00:04:58.660]   - So okay, but back to Dune, what do you get?
[00:04:58.660 --> 00:04:59.980]   - Back to Dune.
[00:04:59.980 --> 00:05:02.780]   And by the way, the new movie I'm pretty interested in,
[00:05:02.780 --> 00:05:04.260]   but the original--
[00:05:04.260 --> 00:05:05.300]   - You're skeptical?
[00:05:05.300 --> 00:05:06.580]   - I'm a little skeptical.
[00:05:06.580 --> 00:05:07.420]   I'm a little skeptical.
[00:05:07.420 --> 00:05:08.260]   I saw the trailer.
[00:05:08.260 --> 00:05:11.580]   I don't know, so there's a David Lynch movie, Dune,
[00:05:11.580 --> 00:05:12.580]   as you may know.
[00:05:12.580 --> 00:05:14.300]   I'm a huge David Lynch fan, by the way.
[00:05:14.300 --> 00:05:17.280]   So the movie is somewhat controversial,
[00:05:17.280 --> 00:05:19.920]   but it's a little confusing,
[00:05:19.920 --> 00:05:22.640]   but it captures kind of the mood of the book
[00:05:22.640 --> 00:05:25.640]   better than I would say most any adaptation.
[00:05:25.640 --> 00:05:27.320]   And like, Dune is so much about kind of mood
[00:05:27.320 --> 00:05:28.720]   and the world, right?
[00:05:28.720 --> 00:05:30.000]   But back to the philosophical point.
[00:05:30.000 --> 00:05:34.200]   So in the fourth book, "God, Emperor of Dune,"
[00:05:34.200 --> 00:05:38.640]   there's a sort of setting where Leto,
[00:05:38.640 --> 00:05:39.480]   one of the characters,
[00:05:39.480 --> 00:05:41.920]   he's become this weird sort of god-emperor.
[00:05:41.920 --> 00:05:43.320]   He's turned into a gigantic worm,
[00:05:43.320 --> 00:05:44.500]   and you kind of have to read the book
[00:05:44.500 --> 00:05:45.340]   to understand what that means.
[00:05:45.340 --> 00:05:46.980]   - So the worms are involved.
[00:05:46.980 --> 00:05:47.820]   - Worms are involved.
[00:05:47.820 --> 00:05:49.900]   You probably saw the worms in the trailer, right?
[00:05:49.900 --> 00:05:50.740]   - And in the video game.
[00:05:50.740 --> 00:05:53.060]   - So he kind of like merges with this worm
[00:05:53.060 --> 00:05:55.100]   and becomes this tyrant of the world,
[00:05:55.100 --> 00:05:57.500]   and he like oppresses the people for a long time, right?
[00:05:57.500 --> 00:05:58.700]   But he has a purpose,
[00:05:58.700 --> 00:06:01.900]   and the purpose is to kind of break through
[00:06:01.900 --> 00:06:05.580]   kind of a stagnation period in civilization, right?
[00:06:05.580 --> 00:06:07.240]   But people have gotten too comfortable, right?
[00:06:07.240 --> 00:06:11.620]   And so he kind of oppresses them so that they explode
[00:06:11.620 --> 00:06:14.240]   and go on to colonize new worlds
[00:06:14.240 --> 00:06:17.800]   and kind of renew the forward momentum of humanity, right?
[00:06:17.800 --> 00:06:19.780]   And so to me, that's kind of fascinating, right?
[00:06:19.780 --> 00:06:22.880]   You need a little bit of pressure and suffering, right,
[00:06:22.880 --> 00:06:27.000]   to kind of make progress, not get too comfortable.
[00:06:27.000 --> 00:06:29.540]   (laughing)
[00:06:29.540 --> 00:06:33.700]   Maybe that's a bit of a cruel philosophy to take away, but.
[00:06:33.700 --> 00:06:36.820]   - That seems to be the case, unfortunately.
[00:06:36.820 --> 00:06:40.240]   Obviously, I'm a huge fan of suffering.
[00:06:40.240 --> 00:06:43.740]   So one of the reasons we're talking today
[00:06:43.740 --> 00:06:47.560]   is that a bunch of people requested
[00:06:47.560 --> 00:06:52.460]   that I do transcripts for this podcast and do captioning.
[00:06:52.460 --> 00:06:54.580]   I used to make all kinds of YouTube videos,
[00:06:54.580 --> 00:06:58.360]   and I would go on Upwork, I think,
[00:06:58.360 --> 00:07:00.580]   and I would hire folks to do transcription,
[00:07:00.580 --> 00:07:04.620]   and it was always a pain in the ass, if I'm being honest.
[00:07:04.620 --> 00:07:07.560]   And then I don't know how I discovered Rev,
[00:07:08.540 --> 00:07:13.060]   but when I did, it was this feeling of like,
[00:07:13.060 --> 00:07:15.380]   holy shit, somebody figured out
[00:07:15.380 --> 00:07:17.280]   how to do it just really easily.
[00:07:17.280 --> 00:07:21.080]   I'm such a fan of just,
[00:07:21.080 --> 00:07:26.860]   when people take a problem and they just make it easy.
[00:07:26.860 --> 00:07:27.700]   - Right.
[00:07:27.700 --> 00:07:28.520]   - Like just,
[00:07:28.520 --> 00:07:34.380]   there's so many, it's like there's so many things in life
[00:07:34.380 --> 00:07:37.740]   that you might not even be aware of that are painful,
[00:07:37.740 --> 00:07:42.740]   and then Rev, you just like give the audio, give the video,
[00:07:42.740 --> 00:07:45.060]   you can actually give a YouTube link,
[00:07:45.060 --> 00:07:49.580]   and then it comes back like a day later,
[00:07:49.580 --> 00:07:53.340]   or two days later, whatever the hell it is,
[00:07:53.340 --> 00:07:56.500]   with the captions, all in a standardized format.
[00:07:56.500 --> 00:08:00.380]   I don't know, it was truly a joy.
[00:08:00.380 --> 00:08:05.020]   So I thought I had, just for the hell of it, talk to you.
[00:08:05.020 --> 00:08:08.260]   One other product, it just made my soul feel good.
[00:08:08.260 --> 00:08:10.900]   One other product I've used like that
[00:08:10.900 --> 00:08:12.980]   is for people who might be familiar,
[00:08:12.980 --> 00:08:17.340]   is called iZotope RX, it's for audio editing.
[00:08:17.340 --> 00:08:22.360]   And that's another one where it was like,
[00:08:22.360 --> 00:08:25.940]   you just drop it, I dropped into the audio,
[00:08:25.940 --> 00:08:28.500]   and it just cleans everything up really nicely.
[00:08:28.500 --> 00:08:32.100]   All the stupid, like the mouth sounds,
[00:08:32.100 --> 00:08:37.100]   and sometimes there's background like sounds
[00:08:37.100 --> 00:08:39.220]   due to the malfunction of the equipment,
[00:08:39.220 --> 00:08:40.620]   it can clean that stuff up.
[00:08:40.620 --> 00:08:43.300]   It has like general voice denoising,
[00:08:43.300 --> 00:08:46.060]   it has like automation capabilities
[00:08:46.060 --> 00:08:47.660]   where you can do batch processing,
[00:08:47.660 --> 00:08:49.820]   and you can put a bunch of effects.
[00:08:49.820 --> 00:08:53.900]   I mean, it just, I don't know, everything else sucked
[00:08:53.900 --> 00:08:58.060]   for like voice-based cleanup that I've ever used.
[00:08:58.060 --> 00:09:00.100]   I've used Audition, Adobe Audition,
[00:09:00.100 --> 00:09:02.460]   I've used all kinds of other things with plugins,
[00:09:02.460 --> 00:09:04.700]   and you have to kind of figure it all out,
[00:09:04.700 --> 00:09:07.740]   you have to do it manually, here it just worked.
[00:09:07.740 --> 00:09:09.860]   So that's another one in this whole pipeline
[00:09:09.860 --> 00:09:12.860]   that just brought joy to my heart.
[00:09:12.860 --> 00:09:14.800]   Anyway, all that to say is,
[00:09:14.800 --> 00:09:18.980]   Rev put a smile to my face.
[00:09:18.980 --> 00:09:22.180]   So can you maybe take a step back and say,
[00:09:22.180 --> 00:09:24.780]   what is Rev, and how does it work?
[00:09:24.780 --> 00:09:27.060]   And Rev or Rev.com?
[00:09:27.060 --> 00:09:28.100]   - Rev, Rev.com.
[00:09:28.100 --> 00:09:29.060]   (laughing)
[00:09:29.060 --> 00:09:30.460]   Same thing, I guess.
[00:09:30.460 --> 00:09:32.220]   Though we do have Rev.ai now as well,
[00:09:32.220 --> 00:09:34.620]   which we can talk about later.
[00:09:34.620 --> 00:09:37.260]   - Like, do you have the actual domain, or is it just--
[00:09:37.260 --> 00:09:39.820]   - The actual domain, but we also use it
[00:09:39.820 --> 00:09:42.540]   kind of as a sub-brand.
[00:09:42.540 --> 00:09:46.860]   So we use Rev.ai to denote our ASR services, right?
[00:09:46.860 --> 00:09:48.900]   And Rev.com is kind of our more human
[00:09:48.900 --> 00:09:50.620]   and to the end user services.
[00:09:50.620 --> 00:09:53.180]   - So it's like wordpress.com and wordpress.org,
[00:09:53.180 --> 00:09:55.860]   they actually have separate brands that like,
[00:09:55.860 --> 00:09:57.580]   I don't know if you're familiar with what those are.
[00:09:57.580 --> 00:09:58.420]   - Yeah, yeah, yeah.
[00:09:58.420 --> 00:10:01.020]   They provide almost like a separate branch of--
[00:10:01.020 --> 00:10:02.940]   - A little bit, I think with that, it's like,
[00:10:02.940 --> 00:10:05.020]   wordpress.org is kind of their open source, right?
[00:10:05.020 --> 00:10:07.100]   And wordpress.com is sort of their
[00:10:07.100 --> 00:10:08.660]   hosted commercial offering.
[00:10:08.660 --> 00:10:09.500]   - Yes.
[00:10:09.500 --> 00:10:11.180]   - And with us, the differentiation is a little bit different,
[00:10:11.180 --> 00:10:12.780]   but maybe a similar idea.
[00:10:12.780 --> 00:10:13.700]   - Yeah.
[00:10:13.700 --> 00:10:14.820]   Okay, so what is Rev?
[00:10:14.820 --> 00:10:17.420]   - Before I launch into what is Rev,
[00:10:17.420 --> 00:10:18.980]   I was gonna say, you know, like you were talking about,
[00:10:18.980 --> 00:10:20.740]   like Rev was music to your ears.
[00:10:20.740 --> 00:10:22.900]   Your spiel was music to my ears,
[00:10:22.900 --> 00:10:25.620]   and to us, the founders of Rev,
[00:10:25.620 --> 00:10:28.620]   because Rev was kind of founded
[00:10:28.620 --> 00:10:30.620]   to improve on the model of Upwork.
[00:10:30.620 --> 00:10:32.180]   That was kind of the original,
[00:10:32.180 --> 00:10:35.180]   or part of their original impetus.
[00:10:35.180 --> 00:10:39.700]   Like our CEO, Jason, was a early employee of Upwork,
[00:10:39.700 --> 00:10:40.700]   so he's very familiar with their--
[00:10:40.700 --> 00:10:41.620]   - Upwork the company.
[00:10:41.620 --> 00:10:42.660]   - Upwork the company.
[00:10:42.660 --> 00:10:45.540]   And so he was very familiar with that model,
[00:10:45.540 --> 00:10:48.580]   and he wanted to make the whole experience better,
[00:10:48.580 --> 00:10:50.140]   because he knew like, when you go,
[00:10:50.140 --> 00:10:52.460]   at that time, Upwork was primarily programmers.
[00:10:52.460 --> 00:10:54.860]   So the main thing they offered is,
[00:10:54.860 --> 00:10:57.780]   if you wanna hire someone to help you code a little site,
[00:10:57.780 --> 00:11:00.580]   you could go on Upwork,
[00:11:00.580 --> 00:11:03.060]   and you could like browse through a list of freelancers,
[00:11:03.060 --> 00:11:05.420]   pick a programmer, have a contract with them,
[00:11:05.420 --> 00:11:07.260]   and have them do some work.
[00:11:07.260 --> 00:11:09.740]   But it was kind of a difficult experience,
[00:11:09.740 --> 00:11:14.140]   because for you, you would kind of have to browse
[00:11:14.140 --> 00:11:14.980]   through all these people, right?
[00:11:14.980 --> 00:11:16.140]   And you have to decide, okay, like,
[00:11:16.140 --> 00:11:20.260]   well, is this guy good, or is somebody else better?
[00:11:20.260 --> 00:11:22.900]   And naturally, you're going to Upwork
[00:11:22.900 --> 00:11:24.540]   because you're not an expert, right?
[00:11:24.540 --> 00:11:25.700]   If you're an expert, you probably wouldn't be
[00:11:25.700 --> 00:11:27.780]   like getting a programmer from Upwork.
[00:11:27.780 --> 00:11:29.820]   So how can you really tell?
[00:11:29.820 --> 00:11:33.060]   So there's kind of like a lot of potential regret, right?
[00:11:33.060 --> 00:11:34.780]   What if I choose a bad person?
[00:11:34.780 --> 00:11:36.460]   They're like gonna be late on the work.
[00:11:36.460 --> 00:11:38.180]   It's gonna be a painful experience.
[00:11:38.180 --> 00:11:40.700]   And for the freelancer, it was also painful,
[00:11:40.700 --> 00:11:42.500]   because half the time, they spent not
[00:11:42.500 --> 00:11:43.540]   on actually doing the work,
[00:11:43.540 --> 00:11:46.820]   but kind of figuring out how can I make my profile
[00:11:46.820 --> 00:11:49.020]   most attractive to the buyer, right?
[00:11:49.020 --> 00:11:51.340]   And they're not an expert on that either.
[00:11:51.340 --> 00:11:54.220]   So like, Rob's idea was, let's remove the barrier, right?
[00:11:54.220 --> 00:11:55.340]   Like, let's make it simple.
[00:11:55.340 --> 00:12:00.180]   We'll pick a few verticals that are fairly standardizable.
[00:12:00.180 --> 00:12:02.580]   Now, we actually started with translation,
[00:12:02.580 --> 00:12:05.380]   and then we added audio transcription a bit later.
[00:12:05.380 --> 00:12:06.900]   And we'll just make it a website.
[00:12:06.900 --> 00:12:08.660]   You go, give us your files.
[00:12:08.660 --> 00:12:13.660]   We'll give you back the results as soon as possible.
[00:12:13.660 --> 00:12:15.580]   Originally, maybe it was 48 hours,
[00:12:15.580 --> 00:12:18.180]   then we made it shorter and shorter and shorter.
[00:12:18.180 --> 00:12:19.660]   - Yeah, there's a rush processing too.
[00:12:19.660 --> 00:12:21.780]   - There's a rush processing now.
[00:12:21.780 --> 00:12:25.340]   And we'll hide all the details from you, right?
[00:12:25.340 --> 00:12:26.180]   - Yeah.
[00:12:26.180 --> 00:12:27.940]   - And like, that's kind of exactly
[00:12:27.940 --> 00:12:28.940]   what you're experiencing, right?
[00:12:28.940 --> 00:12:30.620]   You don't need to worry about the details
[00:12:30.620 --> 00:12:31.820]   of how the sausage is made.
[00:12:31.820 --> 00:12:32.940]   - That's really cool.
[00:12:32.940 --> 00:12:34.980]   So you picked like a vertical.
[00:12:34.980 --> 00:12:37.220]   By vertical, you mean basically a--
[00:12:37.220 --> 00:12:39.300]   - A service, a service category.
[00:12:39.300 --> 00:12:41.220]   - Why translation?
[00:12:41.220 --> 00:12:43.220]   Is Rev thinking of potentially going
[00:12:43.220 --> 00:12:45.420]   into other verticals in the future?
[00:12:45.420 --> 00:12:47.980]   Or is this like the focus now is translation,
[00:12:47.980 --> 00:12:50.180]   transcription, like language?
[00:12:50.180 --> 00:12:54.580]   - The focus now is language or speech services generally,
[00:12:54.580 --> 00:12:56.340]   speech to text, language services.
[00:12:56.340 --> 00:12:58.540]   You can kind of group them however you want.
[00:12:58.540 --> 00:13:02.620]   So, but we originally,
[00:13:02.620 --> 00:13:05.100]   the categorization was work from home.
[00:13:05.100 --> 00:13:08.220]   And so we wanted work that was done by people on a computer.
[00:13:08.220 --> 00:13:11.020]   You know, we weren't trying to get into, you know,
[00:13:11.020 --> 00:13:13.220]   task rabbit type of things.
[00:13:13.220 --> 00:13:16.100]   And something that could be relatively standard,
[00:13:16.100 --> 00:13:17.100]   not a lot of options.
[00:13:17.100 --> 00:13:20.020]   So we could kind of present the simplified interface, right?
[00:13:20.020 --> 00:13:21.460]   So programming wasn't like a good fit
[00:13:21.460 --> 00:13:24.740]   because each programming project is kind of unique, right?
[00:13:24.740 --> 00:13:28.300]   We're looking for something that transcription is,
[00:13:28.300 --> 00:13:29.500]   you know, you have five hours of audio,
[00:13:29.500 --> 00:13:30.860]   it's five hours of audio, right?
[00:13:30.860 --> 00:13:33.460]   Translation is somewhat similar in that, you know,
[00:13:33.460 --> 00:13:36.660]   you can have a five page document, you know,
[00:13:36.660 --> 00:13:38.420]   and then you just can price it by that.
[00:13:38.420 --> 00:13:40.220]   And then you pick the language you want,
[00:13:40.220 --> 00:13:42.620]   and that's mostly all that is to it.
[00:13:42.620 --> 00:13:43.980]   So those were a few criteria.
[00:13:43.980 --> 00:13:47.020]   We started with translation because we saw the need
[00:13:48.340 --> 00:13:53.340]   and we picked up kind of a specialty of translation
[00:13:53.340 --> 00:13:57.420]   where we would translate things like birth certificates,
[00:13:57.420 --> 00:14:01.540]   immigration documents, things like that.
[00:14:01.540 --> 00:14:06.020]   And so they were fairly, even more well-defined
[00:14:06.020 --> 00:14:08.260]   and easy to kind of tell if we did a good job.
[00:14:08.260 --> 00:14:10.740]   - So you can literally charge per type of document?
[00:14:10.740 --> 00:14:14.100]   Was that the, so what is it now?
[00:14:14.100 --> 00:14:15.700]   Is it per word or something like that?
[00:14:15.700 --> 00:14:20.220]   Like how do you measure the effort involved
[00:14:20.220 --> 00:14:21.500]   in a particular thing?
[00:14:21.500 --> 00:14:23.420]   - So now it looks like for audio transcription, right?
[00:14:23.420 --> 00:14:24.980]   It's per audio unit.
[00:14:24.980 --> 00:14:26.580]   - Well, that, yes.
[00:14:26.580 --> 00:14:27.420]   - For our translation,
[00:14:27.420 --> 00:14:30.580]   we don't really actually focus on that anymore.
[00:14:30.580 --> 00:14:33.140]   But, you know, back when it was still a main business
[00:14:33.140 --> 00:14:35.260]   of Revit was per page, right?
[00:14:35.260 --> 00:14:36.940]   Or per word, depending on the kind of--
[00:14:36.940 --> 00:14:38.900]   - 'Cause you can also do translation now
[00:14:38.900 --> 00:14:40.660]   on the audio, right?
[00:14:40.660 --> 00:14:41.980]   - Mm-hmm, like subtitles.
[00:14:41.980 --> 00:14:45.060]   So it would be both transcription and translation.
[00:14:45.060 --> 00:14:45.900]   - That's right.
[00:14:45.900 --> 00:14:48.500]   - I wanted to test the system to see how good it is,
[00:14:48.500 --> 00:14:51.980]   to see like how, well, is Russian supported?
[00:14:51.980 --> 00:14:54.420]   - I think so, yeah.
[00:14:54.420 --> 00:14:55.860]   - It'd be interesting to try it out.
[00:14:55.860 --> 00:14:56.700]   I mean, one of the--
[00:14:56.700 --> 00:14:58.180]   - But now it's only in like the one direction, right?
[00:14:58.180 --> 00:14:59.260]   So you start with English
[00:14:59.260 --> 00:15:00.980]   and then you can have subtitles in Russian.
[00:15:00.980 --> 00:15:01.820]   - In Russian.
[00:15:01.820 --> 00:15:02.900]   - Not really the other way.
[00:15:02.900 --> 00:15:06.140]   - Got it, because I'm deeply curious about this.
[00:15:06.140 --> 00:15:07.860]   When COVID opens up a little bit,
[00:15:07.860 --> 00:15:10.900]   when the economy, when the world opens up a little bit.
[00:15:10.900 --> 00:15:12.820]   - You wanna build your brand in Russia?
[00:15:12.820 --> 00:15:14.020]   - No, I don't.
[00:15:14.020 --> 00:15:15.900]   First of all, I'm allergic to the word brand.
[00:15:15.900 --> 00:15:17.980]   (laughing)
[00:15:17.980 --> 00:15:21.140]   I'm definitely not building any brands in Russia.
[00:15:21.140 --> 00:15:24.540]   But I'm going to Paris to talk to the translators
[00:15:24.540 --> 00:15:26.660]   of Dostoevsky and Tolstoy.
[00:15:26.660 --> 00:15:29.820]   There's this famous couple that does translation.
[00:15:29.820 --> 00:15:34.820]   And I'm more and more thinking of how is it possible
[00:15:34.820 --> 00:15:37.860]   to have a conversation with a Russian speaker?
[00:15:37.860 --> 00:15:42.660]   'Cause I have just some number of famous Russian speakers
[00:15:42.660 --> 00:15:44.940]   that I'm interested in talking to.
[00:15:44.940 --> 00:15:49.780]   And my Russian is not strong enough to be witty and funny.
[00:15:49.780 --> 00:15:51.980]   I'm already an idiot in English.
[00:15:51.980 --> 00:15:56.300]   I'm an extra level of like awkward idiot in Russian,
[00:15:56.300 --> 00:15:58.260]   but I can understand it, right?
[00:15:58.260 --> 00:16:01.940]   And I also like wonder how can I create
[00:16:01.940 --> 00:16:05.220]   a compelling English-Russian experience
[00:16:05.220 --> 00:16:06.540]   for an English speaker?
[00:16:06.540 --> 00:16:09.260]   Like if I, there's a guy named Grigori Perlman,
[00:16:09.260 --> 00:16:11.340]   who's a mathematician,
[00:16:11.340 --> 00:16:14.540]   who obviously doesn't speak any English.
[00:16:14.540 --> 00:16:17.220]   So I would probably incorporate
[00:16:17.220 --> 00:16:21.460]   like a Russian translator into the picture.
[00:16:21.460 --> 00:16:24.380]   And then it would be like a, not to use a weird term,
[00:16:24.380 --> 00:16:28.100]   but like a three person thing,
[00:16:28.100 --> 00:16:33.020]   where it's like a dance of, like I understand it one way,
[00:16:33.020 --> 00:16:34.780]   they don't understand the other way,
[00:16:34.780 --> 00:16:38.220]   but I'll be asking questions in English.
[00:16:38.220 --> 00:16:39.060]   I don't know.
[00:16:39.060 --> 00:16:39.900]   I don't know the right way.
[00:16:39.900 --> 00:16:40.740]   - It's complicated.
[00:16:40.740 --> 00:16:41.580]   - It's complicated,
[00:16:41.580 --> 00:16:42.940]   but I feel like it's worth the effort
[00:16:42.940 --> 00:16:45.100]   for certain kinds of people.
[00:16:45.100 --> 00:16:48.140]   One of whom I'm confident is Vladimir Putin,
[00:16:48.140 --> 00:16:49.220]   I'm for sure talking to.
[00:16:49.220 --> 00:16:50.820]   I really want to make it happen
[00:16:50.820 --> 00:16:52.420]   'cause I think I could do a good job with it.
[00:16:52.420 --> 00:16:54.620]   But the right, you know,
[00:16:54.620 --> 00:16:57.460]   understanding the fundamentals of translation
[00:16:57.460 --> 00:16:59.300]   is something I'm really interested in.
[00:16:59.300 --> 00:17:02.780]   So that's why I'm starting with the actual translators
[00:17:02.780 --> 00:17:04.740]   of like Russian literature,
[00:17:04.740 --> 00:17:06.260]   because they understand the nuance
[00:17:06.260 --> 00:17:07.540]   and the beauty of the language
[00:17:07.540 --> 00:17:09.780]   and how it goes back and forth.
[00:17:09.780 --> 00:17:11.980]   But I also want to see like in speech,
[00:17:11.980 --> 00:17:14.180]   how can we do it in real time?
[00:17:14.180 --> 00:17:17.860]   So that's like a little bit of a baby project
[00:17:17.860 --> 00:17:19.060]   that I hope to push forward.
[00:17:19.060 --> 00:17:19.900]   But anyway.
[00:17:19.900 --> 00:17:20.820]   - It's a challenging thing.
[00:17:20.820 --> 00:17:22.980]   So just to share,
[00:17:22.980 --> 00:17:26.300]   my dad actually does translation.
[00:17:26.300 --> 00:17:28.980]   Not professionally, he writes poetry.
[00:17:28.980 --> 00:17:30.420]   That was kind of always his,
[00:17:30.420 --> 00:17:35.860]   not a hobby, but he had a job, like a day job,
[00:17:35.860 --> 00:17:38.700]   but his passion was always writing poetry.
[00:17:38.700 --> 00:17:40.700]   And then we got to America
[00:17:40.700 --> 00:17:42.620]   and like he started also translating.
[00:17:42.620 --> 00:17:46.140]   First he was translating English poetry to Russian.
[00:17:46.140 --> 00:17:49.060]   Now he also like goes the other way.
[00:17:49.060 --> 00:17:52.620]   You kind of gain some small fame in that world anyways,
[00:17:52.620 --> 00:17:56.340]   because recently this poet, like Louise Clark,
[00:17:56.340 --> 00:17:58.180]   I don't know if you know of,
[00:17:58.180 --> 00:17:59.620]   some American poet,
[00:17:59.620 --> 00:18:01.980]   she was awarded the Nobel Prize for literature.
[00:18:01.980 --> 00:18:04.740]   And so my dad had translated
[00:18:04.740 --> 00:18:07.380]   one of her books of poetry into Russian.
[00:18:07.380 --> 00:18:08.660]   He was like one of the few.
[00:18:08.660 --> 00:18:10.180]   So he kind of like, they asked him
[00:18:10.180 --> 00:18:13.220]   and gave an interview to Radio Svoboda,
[00:18:13.220 --> 00:18:14.060]   if you know what that is.
[00:18:14.060 --> 00:18:16.780]   And he kind of talked about some of the intricacies
[00:18:16.780 --> 00:18:18.060]   of translating poetry.
[00:18:18.060 --> 00:18:19.700]   So that's like an extra level of difficulty, right?
[00:18:19.700 --> 00:18:22.420]   Because translating poetry is even more challenging
[00:18:22.420 --> 00:18:25.460]   than translating just, you know, interviews.
[00:18:25.460 --> 00:18:28.500]   - Do you remember any experiences and challenges
[00:18:28.500 --> 00:18:32.420]   to having to do the translation that stuck out to you?
[00:18:32.420 --> 00:18:34.380]   Like something he's talked about?
[00:18:34.380 --> 00:18:36.540]   - I mean, a lot of it I think is word choice, right?
[00:18:36.540 --> 00:18:38.380]   It's the way Russian is structured
[00:18:38.380 --> 00:18:39.620]   is first of all quite different
[00:18:39.620 --> 00:18:41.620]   than the way English is structured, right?
[00:18:41.620 --> 00:18:44.100]   Just there's inflections in Russian and genders
[00:18:44.100 --> 00:18:46.020]   and they don't exist in English.
[00:18:46.020 --> 00:18:48.900]   One of the reasons actually why machine translation
[00:18:48.900 --> 00:18:50.860]   is quite difficult for English to Russian
[00:18:50.860 --> 00:18:51.820]   and Russian to English,
[00:18:51.820 --> 00:18:53.980]   because they're such different languages.
[00:18:53.980 --> 00:18:57.140]   But then English has like a huge number of words,
[00:18:57.140 --> 00:18:58.500]   many more than Russian actually, I think.
[00:18:58.500 --> 00:19:01.500]   So it's often difficult to find the right word
[00:19:01.500 --> 00:19:04.020]   to convey the same emotional meaning.
[00:19:04.020 --> 00:19:07.620]   - Yeah, Russian language, they play with words much more.
[00:19:07.620 --> 00:19:11.740]   So you were mentioning that Rev was kind of born
[00:19:11.740 --> 00:19:15.220]   out of trying to take a vertical on Upwork
[00:19:15.220 --> 00:19:17.380]   and then standardize it.
[00:19:17.380 --> 00:19:18.220]   So--
[00:19:18.220 --> 00:19:19.060]   - We're just trying to make
[00:19:19.060 --> 00:19:23.060]   the freelancer marketplace idea better, right?
[00:19:23.060 --> 00:19:26.100]   Better for both customers
[00:19:26.100 --> 00:19:28.780]   and better for the freelancers themselves.
[00:19:28.780 --> 00:19:31.300]   - Is there something else to the story of Rev,
[00:19:31.300 --> 00:19:32.700]   finding Rev?
[00:19:32.700 --> 00:19:35.740]   Like what did it take to bring it to actually to life?
[00:19:35.740 --> 00:19:37.180]   Was there any pain points?
[00:19:37.180 --> 00:19:39.860]   - Plenty of pain points.
[00:19:39.860 --> 00:19:44.020]   I mean, as often the case, it's with scaling it up, right?
[00:19:44.020 --> 00:19:47.060]   And in this case, the scaling is kind of scaling
[00:19:47.060 --> 00:19:49.300]   the marketplace, so to speak, right?
[00:19:49.300 --> 00:19:51.620]   Rev is essentially a two-sided marketplace, right?
[00:19:51.620 --> 00:19:55.700]   Because there's the customers and then there's the Revvers.
[00:19:55.700 --> 00:19:57.540]   If there's not enough Revvers,
[00:19:57.540 --> 00:19:59.260]   Revvers are what we call our freelancers.
[00:19:59.260 --> 00:20:01.260]   So if there's not enough Revvers,
[00:20:01.260 --> 00:20:04.020]   then customers have a bad experience, right?
[00:20:04.020 --> 00:20:07.580]   Takes longer to get your work done, things like that.
[00:20:07.580 --> 00:20:10.620]   If there's too many, then Revvers have a bad experience
[00:20:10.620 --> 00:20:13.100]   because they might log on to see what work is available
[00:20:13.100 --> 00:20:15.460]   and there's not very much work, right?
[00:20:15.460 --> 00:20:17.140]   So kind of keeping that balance
[00:20:17.140 --> 00:20:20.220]   is a quite challenging problem.
[00:20:20.220 --> 00:20:22.740]   And that's like a problem we've been working on
[00:20:22.740 --> 00:20:23.740]   for many years.
[00:20:23.740 --> 00:20:26.020]   We're still refining our methods, right?
[00:20:26.020 --> 00:20:29.660]   - If you can kind of talk to this gig economy idea,
[00:20:29.660 --> 00:20:31.780]   I did a bunch of different psychology experiments
[00:20:31.780 --> 00:20:33.540]   on Mechanical Turk, for example.
[00:20:33.540 --> 00:20:36.300]   I've asked to do different kinds of very tricky
[00:20:36.300 --> 00:20:38.580]   computer vision annotation on Mechanical Turk
[00:20:38.580 --> 00:20:43.580]   and it's connecting people in a more systematized way.
[00:20:43.580 --> 00:20:48.180]   I would say, you know, between task and,
[00:20:48.180 --> 00:20:51.340]   what would you call that, worker,
[00:20:51.340 --> 00:20:53.740]   is what Mechanical Turk calls it.
[00:20:53.740 --> 00:20:57.180]   What do you think about this world of gig economies,
[00:20:57.180 --> 00:21:02.180]   of there being a service that connects customers to workers
[00:21:02.180 --> 00:21:07.980]   in a way that's like massively distributed,
[00:21:07.980 --> 00:21:10.300]   like potentially scaling to,
[00:21:10.300 --> 00:21:13.220]   it could be scaled to like tens of thousands of people,
[00:21:13.220 --> 00:21:14.060]   right?
[00:21:14.060 --> 00:21:17.100]   Is there something interesting about that world
[00:21:17.100 --> 00:21:18.260]   that you can speak to?
[00:21:18.260 --> 00:21:21.380]   - Yeah, well, we don't think of it as kind of gig economy,
[00:21:21.380 --> 00:21:22.700]   but like to some degree,
[00:21:22.700 --> 00:21:24.420]   I don't like the word gig that much, right?
[00:21:24.420 --> 00:21:26.700]   Because to some degree it diminishes
[00:21:26.700 --> 00:21:27.940]   the work being done, right?
[00:21:27.940 --> 00:21:30.100]   It sounds kind of like almost amateurish.
[00:21:30.100 --> 00:21:32.820]   Well, maybe in like music industry,
[00:21:32.820 --> 00:21:34.180]   like gig is the standard term,
[00:21:34.180 --> 00:21:39.180]   but in work, it kind of sounds like it's frivolous.
[00:21:39.180 --> 00:21:45.220]   To us, it's improving the nature of working from home
[00:21:45.220 --> 00:21:48.060]   on your own time and on your own terms, right?
[00:21:48.060 --> 00:21:52.020]   And kind of taking away geographical limitations
[00:21:52.020 --> 00:21:54.220]   and time limitations, right?
[00:21:54.220 --> 00:21:58.740]   So, many of our freelancers are maybe work from home moms,
[00:21:58.740 --> 00:21:59.580]   right?
[00:21:59.580 --> 00:22:02.540]   And they don't want the traditional nine to five job,
[00:22:02.540 --> 00:22:04.620]   but they wanna make some income
[00:22:04.620 --> 00:22:06.260]   and Rev kind of like allows them to do that
[00:22:06.260 --> 00:22:09.720]   and decide like exactly how much to work and when to work.
[00:22:09.720 --> 00:22:13.460]   Or by the same token, maybe someone is,
[00:22:13.460 --> 00:22:18.020]   someone wants to live the mountain top,
[00:22:18.020 --> 00:22:18.860]   life, right?
[00:22:18.860 --> 00:22:20.220]   You know, cabin in the woods,
[00:22:20.220 --> 00:22:22.500]   but they still wanna make some money.
[00:22:22.500 --> 00:22:25.020]   And like, generally that wouldn't be compatible
[00:22:25.020 --> 00:22:28.620]   before this new world, you kind of had to choose.
[00:22:28.620 --> 00:22:30.380]   But like with Rev, like you feel like
[00:22:30.380 --> 00:22:31.500]   you don't have to choose.
[00:22:31.500 --> 00:22:33.300]   - Can you speak to like,
[00:22:33.300 --> 00:22:38.220]   what's the demographics like distribution,
[00:22:38.220 --> 00:22:40.780]   like where do Revvers live?
[00:22:40.780 --> 00:22:42.420]   Is it from all over the world?
[00:22:42.420 --> 00:22:43.260]   Like, what is it?
[00:22:43.260 --> 00:22:46.420]   Do you have a sense of what's out there?
[00:22:46.420 --> 00:22:48.220]   - We're all over the world.
[00:22:48.220 --> 00:22:51.500]   Most of them are in the US, that's the majority.
[00:22:51.500 --> 00:22:54.900]   Yeah, because most of our work is audio transcription
[00:22:54.900 --> 00:22:57.660]   and so you have to speak pretty good English.
[00:22:57.660 --> 00:22:59.140]   So, the majority of them are from the US,
[00:22:59.140 --> 00:23:00.940]   so we have people in some other
[00:23:00.940 --> 00:23:03.220]   of the English speaking countries.
[00:23:03.220 --> 00:23:06.060]   And as far as like US, it's really all over the place.
[00:23:06.060 --> 00:23:09.220]   You know, for some of the years now,
[00:23:09.220 --> 00:23:10.460]   we've been doing these little meetings
[00:23:10.460 --> 00:23:12.300]   where the management team will go to some place
[00:23:12.300 --> 00:23:13.660]   and we'll try to meet Revvers.
[00:23:13.660 --> 00:23:15.820]   And, you know, pretty much wherever we go,
[00:23:15.820 --> 00:23:17.860]   it's pretty easy to find, you know,
[00:23:17.860 --> 00:23:19.140]   a large number of Revvers.
[00:23:19.140 --> 00:23:21.580]   You know, the most recent one we did is in Utah.
[00:23:21.580 --> 00:23:25.260]   But anywhere really.
[00:23:25.260 --> 00:23:26.700]   - Are they from all walks of life?
[00:23:26.700 --> 00:23:28.900]   Are these young folks, older folks?
[00:23:28.900 --> 00:23:30.220]   - Yeah, all walks of life really.
[00:23:30.220 --> 00:23:32.580]   Like I said, you know, one category is, you know,
[00:23:32.580 --> 00:23:34.860]   the work from home, students, you know,
[00:23:34.860 --> 00:23:37.100]   who wanna make some extra income.
[00:23:37.100 --> 00:23:40.060]   There are some people who maybe, you know,
[00:23:40.060 --> 00:23:42.260]   maybe they have some social anxiety,
[00:23:42.260 --> 00:23:43.700]   so they don't wanna be in the office, right?
[00:23:43.700 --> 00:23:45.260]   And this is one way for them to make a living.
[00:23:45.260 --> 00:23:47.420]   So it's really pretty wide variety.
[00:23:47.420 --> 00:23:49.060]   But like on the flip side, for example,
[00:23:49.060 --> 00:23:52.860]   one Revver we were talking to was a person
[00:23:52.860 --> 00:23:54.620]   who had a fairly high-powered career before
[00:23:54.620 --> 00:23:57.260]   and was kind of like taking a break
[00:23:57.260 --> 00:23:59.260]   and just wanted, she was almost doing this
[00:23:59.260 --> 00:24:01.300]   just to explore and learn about, you know,
[00:24:01.300 --> 00:24:03.420]   the gig economy, quote unquote, right?
[00:24:03.420 --> 00:24:06.300]   So it really is a pretty wide variety of folks.
[00:24:06.300 --> 00:24:08.380]   - Yeah, it's kind of interesting
[00:24:08.380 --> 00:24:10.460]   through the captioning process
[00:24:10.460 --> 00:24:13.100]   for me to learn about the Revvers
[00:24:13.100 --> 00:24:18.100]   because like some are clearly like weirdly knowledgeable
[00:24:18.100 --> 00:24:22.940]   about technical concepts.
[00:24:22.940 --> 00:24:25.260]   Like you can tell by how good they are
[00:24:25.260 --> 00:24:29.060]   at like capitalizing stuff, like technical terms,
[00:24:29.060 --> 00:24:30.660]   like in machine learning and deep learning.
[00:24:30.660 --> 00:24:32.140]   - Right.
[00:24:32.140 --> 00:24:35.460]   - I've used Rev to annotate, to caption
[00:24:35.460 --> 00:24:37.980]   the deep learning lectures or machine learning lectures
[00:24:37.980 --> 00:24:39.860]   I did at MIT.
[00:24:39.860 --> 00:24:44.500]   And it's funny, like a large number of them were like,
[00:24:44.500 --> 00:24:45.820]   I don't know if they looked it up
[00:24:45.820 --> 00:24:47.260]   or were already knowledgeable,
[00:24:47.260 --> 00:24:50.380]   but they do a really good job at like, I don't know.
[00:24:50.380 --> 00:24:52.340]   - They invest time into these things.
[00:24:52.340 --> 00:24:54.980]   They will like do research, they will Google things,
[00:24:54.980 --> 00:24:57.340]   you know, to kind of make sure they get it right.
[00:24:57.340 --> 00:24:59.060]   But to some of them, it's like,
[00:24:59.060 --> 00:25:01.580]   it's actually part of the enjoyment of the work.
[00:25:01.580 --> 00:25:03.300]   Like they'll tell us, you know,
[00:25:03.300 --> 00:25:05.740]   I love doing this because I get paid
[00:25:05.740 --> 00:25:07.380]   to watch a documentary on something, right?
[00:25:07.380 --> 00:25:10.060]   And I learned something while I'm transcribing, right?
[00:25:10.060 --> 00:25:10.900]   Pretty cool.
[00:25:10.900 --> 00:25:11.740]   - Yeah.
[00:25:11.740 --> 00:25:14.660]   So what's that captioning transcription process
[00:25:14.660 --> 00:25:16.180]   look like for the Revver?
[00:25:16.180 --> 00:25:18.940]   Can you maybe speak to that to give people a sense,
[00:25:18.940 --> 00:25:22.020]   like how much is automated, how much is manual?
[00:25:22.020 --> 00:25:25.140]   What's the actual interface look like?
[00:25:25.140 --> 00:25:26.300]   All that kind of stuff.
[00:25:26.300 --> 00:25:28.380]   - Yeah, so, you know, we've invested
[00:25:28.380 --> 00:25:31.020]   a pretty good amount of time to give like our Revvers
[00:25:31.020 --> 00:25:33.020]   the best tools possible.
[00:25:33.020 --> 00:25:34.740]   You know, so typical day for Revver,
[00:25:34.740 --> 00:25:37.100]   they might log into their workspace,
[00:25:37.100 --> 00:25:41.380]   they'll see a list of audios that need to be transcribed.
[00:25:41.380 --> 00:25:43.380]   And we try to give them tools to pick specifically
[00:25:43.380 --> 00:25:44.460]   the ones they want to do, you know?
[00:25:44.460 --> 00:25:47.740]   So maybe some people like to do longer audios
[00:25:47.740 --> 00:25:49.060]   or shorter audios.
[00:25:49.060 --> 00:25:52.300]   People have their preferences.
[00:25:52.300 --> 00:25:55.020]   Some people like to do audios in a particular subject
[00:25:55.020 --> 00:25:55.980]   or from a particular country.
[00:25:55.980 --> 00:25:58.100]   So we try to give people, you know,
[00:25:58.100 --> 00:26:01.060]   the tools to control things like that.
[00:26:01.060 --> 00:26:04.580]   And then when they pick what they want to do,
[00:26:04.580 --> 00:26:07.460]   we'll launch a specialized editor that we've built
[00:26:07.460 --> 00:26:10.180]   to make transcription as efficient as possible.
[00:26:10.180 --> 00:26:12.340]   They'll start with a speech rec draft.
[00:26:12.340 --> 00:26:15.140]   So, you know, we have our machine learning model
[00:26:15.140 --> 00:26:17.260]   for automated speech recognition.
[00:26:17.260 --> 00:26:18.500]   They'll start with that.
[00:26:18.500 --> 00:26:22.740]   And then our tools are optimized to help them correct that.
[00:26:22.740 --> 00:26:24.940]   - So it's basically a process of correction.
[00:26:24.940 --> 00:26:29.500]   - Yeah, it depends on, you know, I would say the audio.
[00:26:29.500 --> 00:26:31.340]   If audio itself is pretty good,
[00:26:31.340 --> 00:26:33.140]   like probably like our podcast right now
[00:26:33.140 --> 00:26:34.100]   would be quite good.
[00:26:34.100 --> 00:26:36.780]   So they would do a fairly good job.
[00:26:36.780 --> 00:26:41.380]   But if you imagine someone recorded a lecture, you know,
[00:26:41.380 --> 00:26:45.700]   in the back of a auditorium, right?
[00:26:45.700 --> 00:26:47.340]   Where like the speaker is really far away
[00:26:47.340 --> 00:26:49.940]   and there's maybe a lot of crosstalk and things like that,
[00:26:49.940 --> 00:26:52.300]   then maybe they wouldn't do a good job.
[00:26:52.300 --> 00:26:53.700]   So the person might say like, you know what,
[00:26:53.700 --> 00:26:55.060]   I'm just gonna do it from scratch.
[00:26:55.060 --> 00:26:56.260]   - Do it from scratch, yeah.
[00:26:56.260 --> 00:26:57.620]   - So it kind of really depends.
[00:26:57.620 --> 00:27:00.540]   - What would you say is the speed that you can possibly get?
[00:27:00.540 --> 00:27:02.100]   Like what's the fastest?
[00:27:02.820 --> 00:27:05.220]   Is it possible to get real time or no?
[00:27:05.220 --> 00:27:09.140]   As you're like listening, can you write as fast as-
[00:27:09.140 --> 00:27:10.420]   - Real time would be pretty difficult.
[00:27:10.420 --> 00:27:12.940]   It's actually a pretty, it's not an easy job.
[00:27:12.940 --> 00:27:16.260]   You know, we actually encourage everyone at the company
[00:27:16.260 --> 00:27:17.660]   to try to be a transcriber for a day,
[00:27:17.660 --> 00:27:19.020]   transcriptionist for a day.
[00:27:19.020 --> 00:27:24.060]   And it's way harder than you might think it is, right?
[00:27:24.060 --> 00:27:28.260]   Because people talk fast and people have accents
[00:27:28.260 --> 00:27:29.180]   and all this kind of stuff.
[00:27:29.180 --> 00:27:30.940]   So real time is pretty difficult.
[00:27:30.940 --> 00:27:32.580]   - Is it possible?
[00:27:32.580 --> 00:27:35.100]   Like there's somebody, we're probably gonna use Rev
[00:27:35.100 --> 00:27:37.340]   to caption this.
[00:27:37.340 --> 00:27:39.300]   They're listening to this right now.
[00:27:39.300 --> 00:27:42.380]   What do you think is the fastest
[00:27:42.380 --> 00:27:44.860]   you could possibly get on this right now?
[00:27:44.860 --> 00:27:46.380]   - I think on a good audio,
[00:27:46.380 --> 00:27:49.820]   maybe two to three X, I would say, real time.
[00:27:49.820 --> 00:27:51.620]   - Meaning it takes two to three times longer
[00:27:51.620 --> 00:27:55.500]   than the actual audio of the podcast.
[00:27:55.500 --> 00:27:56.740]   This is so meta.
[00:27:56.740 --> 00:27:59.660]   I could just imagine the Revvers working on this right now.
[00:27:59.660 --> 00:28:01.020]   Like you're way wrong.
[00:28:01.020 --> 00:28:03.540]   - You're way wrong, this takes way longer.
[00:28:03.540 --> 00:28:04.380]   But yeah, it definitely works.
[00:28:04.380 --> 00:28:06.380]   - Or you doubted me, I could do real time.
[00:28:06.380 --> 00:28:08.620]   (both laughing)
[00:28:08.620 --> 00:28:11.180]   - Okay, so you mentioned ASR.
[00:28:11.180 --> 00:28:15.460]   Can you speak to what is ASR, automatic speech recognition?
[00:28:15.460 --> 00:28:19.300]   How much, like what is the gap
[00:28:19.300 --> 00:28:22.020]   between perfect human performance
[00:28:22.020 --> 00:28:26.660]   and perfect or pretty damn good ASR?
[00:28:26.660 --> 00:28:28.780]   - Yeah, so ASR, automatic speech recognition,
[00:28:28.780 --> 00:28:31.820]   it's a class of machine learning problem, right?
[00:28:31.820 --> 00:28:34.220]   To take speech like we're talking
[00:28:34.220 --> 00:28:37.060]   and transform it into a sequence of words, essentially.
[00:28:37.060 --> 00:28:38.780]   - Audio of people talking.
[00:28:38.780 --> 00:28:40.540]   - Audio to words.
[00:28:40.540 --> 00:28:44.900]   And there's a variety of different approaches and techniques
[00:28:44.900 --> 00:28:47.100]   which we could talk about later if you want.
[00:28:47.100 --> 00:28:51.860]   So we think we have pretty much the world's best ASR
[00:28:51.860 --> 00:28:54.020]   for this kind of speech, right?
[00:28:54.020 --> 00:28:56.940]   So there's different kinds of domains, right, for ASR.
[00:28:56.940 --> 00:29:00.220]   Like one domain might be voice assistance, right?
[00:29:00.220 --> 00:29:04.220]   So Siri, very different than what we're doing, right?
[00:29:04.220 --> 00:29:06.820]   Because Siri, there's fairly limited vocabulary.
[00:29:06.820 --> 00:29:09.860]   You might ask Siri to play a song
[00:29:09.860 --> 00:29:11.900]   or order a pizza or whatever.
[00:29:11.900 --> 00:29:13.540]   And it's very good at doing that.
[00:29:13.540 --> 00:29:16.220]   Very different from when we're talking
[00:29:16.220 --> 00:29:18.220]   in a very unstructured way.
[00:29:18.220 --> 00:29:20.180]   And Siri will also generally adapt to your voice
[00:29:20.180 --> 00:29:21.420]   and stuff like this.
[00:29:21.420 --> 00:29:24.420]   So for this kind of audio, we think we have the best.
[00:29:24.420 --> 00:29:29.420]   And our accuracy, right now it's, I think,
[00:29:29.420 --> 00:29:34.420]   it's maybe 14% word error rate on our test suite
[00:29:34.420 --> 00:29:35.340]   that we generally use to measure.
[00:29:35.340 --> 00:29:38.180]   So word error rate is like one way to measure
[00:29:38.180 --> 00:29:39.580]   accuracy for ASR, right?
[00:29:39.580 --> 00:29:41.420]   - So what's 14% word error rate mean?
[00:29:41.420 --> 00:29:44.860]   - So 14% means across this test suite
[00:29:44.860 --> 00:29:46.740]   of a variety of different audios,
[00:29:46.740 --> 00:29:52.900]   it would be, it would get in some way 14%
[00:29:53.500 --> 00:29:56.860]   of the words wrong, 14% of the words wrong.
[00:29:56.860 --> 00:29:57.700]   - Yeah.
[00:29:57.700 --> 00:30:01.260]   - So the way you kind of calculate it is,
[00:30:01.260 --> 00:30:03.620]   you might add up insertions, deletions,
[00:30:03.620 --> 00:30:04.700]   and substitutions, right?
[00:30:04.700 --> 00:30:07.420]   So insertions is like extra words,
[00:30:07.420 --> 00:30:08.940]   deletions are words that we said,
[00:30:08.940 --> 00:30:12.300]   but weren't in the transcript, right?
[00:30:12.300 --> 00:30:15.740]   Substitutions is, you said Apple, but I said,
[00:30:15.740 --> 00:30:18.440]   but the ASR thought it was Able, something like this.
[00:30:18.440 --> 00:30:23.060]   Human accuracy, most people think realistically,
[00:30:23.060 --> 00:30:26.740]   it's like 3%, 2% word error rate
[00:30:26.740 --> 00:30:28.540]   would be like the max achievable.
[00:30:28.540 --> 00:30:31.500]   So there's still quite a gap, right?
[00:30:31.500 --> 00:30:33.380]   - Would you say that, so YouTube,
[00:30:33.380 --> 00:30:36.980]   when I upload videos often generates automatic captions.
[00:30:36.980 --> 00:30:39.740]   Are you sort of from a company perspective,
[00:30:39.740 --> 00:30:44.260]   from a tech perspective, are you trying to beat YouTube?
[00:30:44.260 --> 00:30:47.180]   Google, it's a hell of a, so Google,
[00:30:47.180 --> 00:30:49.740]   I mean, I don't know how seriously they take this task,
[00:30:49.740 --> 00:30:51.860]   but I imagine it's quite serious.
[00:30:51.860 --> 00:30:56.260]   And they, you know, Google is probably up there
[00:30:56.260 --> 00:31:01.260]   in terms of their teams on ASR,
[00:31:01.260 --> 00:31:03.180]   or just NLP, natural language processing,
[00:31:03.180 --> 00:31:04.440]   different technologies.
[00:31:04.440 --> 00:31:06.660]   So do you think you can beat Google?
[00:31:06.660 --> 00:31:08.980]   - On this kind of stuff, yeah, we think so.
[00:31:08.980 --> 00:31:10.580]   Google just woke up on my phone.
[00:31:10.580 --> 00:31:12.980]   - This is hilarious, okay.
[00:31:12.980 --> 00:31:14.700]   - Now Google is listening,
[00:31:14.700 --> 00:31:16.400]   sending it back to headquarters.
[00:31:16.400 --> 00:31:19.540]   Who are these rough people?
[00:31:19.540 --> 00:31:20.580]   - But that's the goal?
[00:31:20.580 --> 00:31:23.140]   - Yeah, I mean, we measure ourselves against like Google,
[00:31:23.140 --> 00:31:26.900]   Amazon, Microsoft, you know, some smaller competitors.
[00:31:26.900 --> 00:31:30.300]   And we use like our internal tests with it.
[00:31:30.300 --> 00:31:33.060]   We try to compose it of a pretty representative set of
[00:31:33.060 --> 00:31:36.380]   audios, maybe it's some podcasts, some videos,
[00:31:36.380 --> 00:31:39.700]   some interviews, some lectures, things like that, right?
[00:31:39.700 --> 00:31:42.780]   And we beat them in our own testing.
[00:31:42.780 --> 00:31:45.940]   - And actually Rev offers automated,
[00:31:45.940 --> 00:31:49.300]   like you can actually just do the automated captioning.
[00:31:49.300 --> 00:31:52.700]   So like, I guess it's like way cheaper, whatever it is,
[00:31:52.700 --> 00:31:54.180]   whatever the rates are.
[00:31:54.180 --> 00:31:55.660]   - Yeah, yeah.
[00:31:55.660 --> 00:31:57.900]   - By the way, it used to be a dollar per minute
[00:31:57.900 --> 00:32:00.100]   for captioning and transcription.
[00:32:00.100 --> 00:32:02.340]   I think it's like a dollar 15 or something like that.
[00:32:02.340 --> 00:32:03.180]   - Dollar 25.
[00:32:03.180 --> 00:32:04.260]   - Dollar 25.
[00:32:04.260 --> 00:32:07.380]   Dollar 25, no.
[00:32:07.380 --> 00:32:09.260]   Yeah, it's pretty cool.
[00:32:09.260 --> 00:32:10.940]   That was the other thing that was surprising to me.
[00:32:10.940 --> 00:32:15.940]   It was actually like the cheapest thing you could,
[00:32:15.940 --> 00:32:18.420]   I mean, I don't remember it being cheaper.
[00:32:18.420 --> 00:32:20.940]   You could on Upwork get cheaper,
[00:32:20.940 --> 00:32:22.500]   but it was clear to me that this,
[00:32:22.500 --> 00:32:23.980]   that's going to be really shitty.
[00:32:23.980 --> 00:32:24.820]   - Yeah.
[00:32:24.820 --> 00:32:26.900]   - So like, you're also competing on price.
[00:32:26.900 --> 00:32:30.900]   I think there were services that you can get like similar
[00:32:30.900 --> 00:32:35.820]   to Rev kind of feel to it, but it wasn't as automated.
[00:32:35.820 --> 00:32:37.900]   Like the drag and drop, the entirety of the interface.
[00:32:37.900 --> 00:32:39.580]   It's like the thing we're talking about.
[00:32:39.580 --> 00:32:41.700]   I'm such a huge fan of like frictionless,
[00:32:41.700 --> 00:32:46.700]   like Amazon's single buy button, whatever.
[00:32:47.700 --> 00:32:48.540]   - Yeah, yeah.
[00:32:48.540 --> 00:32:52.340]   - That one click, that's genius right there.
[00:32:52.340 --> 00:32:54.940]   Like that is so important for services.
[00:32:54.940 --> 00:32:55.780]   - Yeah.
[00:32:55.780 --> 00:32:56.620]   - That simplicity.
[00:32:56.620 --> 00:33:00.460]   And I mean, Rev is almost there.
[00:33:00.460 --> 00:33:04.380]   I mean, there's like some, trying to think.
[00:33:04.380 --> 00:33:09.380]   So I think I've, I stopped using this pipeline,
[00:33:09.380 --> 00:33:12.460]   but Rev offers it and I like it,
[00:33:12.460 --> 00:33:16.220]   but it was causing me some issues on my side,
[00:33:16.220 --> 00:33:20.340]   which is you can connect it to like Dropbox
[00:33:20.340 --> 00:33:22.780]   and it generates the files in Dropbox.
[00:33:22.780 --> 00:33:26.460]   So like it closes the loop to where I don't have to go
[00:33:26.460 --> 00:33:29.100]   to Rev at all and I can download it.
[00:33:29.100 --> 00:33:32.700]   Sorry, I don't have to go to Rev at all
[00:33:32.700 --> 00:33:34.220]   and to download the files.
[00:33:34.220 --> 00:33:36.300]   It could just like automatically copy them.
[00:33:36.300 --> 00:33:38.580]   - Right, you put in your Dropbox and you know,
[00:33:38.580 --> 00:33:41.060]   a day later or maybe a few hours later.
[00:33:41.060 --> 00:33:41.900]   - Yeah, it just shows up.
[00:33:41.900 --> 00:33:43.620]   - Depending on if you're in a rush, it just shows up, yeah.
[00:33:44.100 --> 00:33:46.540]   I was trying to do it programmatically too.
[00:33:46.540 --> 00:33:48.940]   Is there an API interface you can,
[00:33:48.940 --> 00:33:51.540]   I was trying to through like through Python
[00:33:51.540 --> 00:33:53.460]   to download stuff automatically,
[00:33:53.460 --> 00:33:56.180]   but then I realized this is the programmer in me.
[00:33:56.180 --> 00:33:58.700]   Like, dude, you don't need to automate everything
[00:33:58.700 --> 00:34:01.140]   like in life, like flawlessly.
[00:34:01.140 --> 00:34:04.100]   'Cause I wasn't doing enough captions to justify
[00:34:04.100 --> 00:34:05.660]   to myself the time investment
[00:34:05.660 --> 00:34:07.820]   into automating everything perfectly.
[00:34:07.820 --> 00:34:10.060]   - Yeah, I would say if you're doing so many interviews
[00:34:10.060 --> 00:34:13.300]   that your biggest roadblock is clicking
[00:34:13.300 --> 00:34:15.500]   on the Rev download button.
[00:34:15.500 --> 00:34:18.980]   Now you're talking about Elon Musk levels of business.
[00:34:18.980 --> 00:34:22.020]   - But for sure we have like a variety of ways
[00:34:22.020 --> 00:34:22.860]   to make it easy.
[00:34:22.860 --> 00:34:24.180]   You know, there's the integration.
[00:34:24.180 --> 00:34:26.220]   You mentioned, I think it's through a company called Zapier,
[00:34:26.220 --> 00:34:31.140]   which kind of can connect Dropbox to Rev and vice versa.
[00:34:31.140 --> 00:34:33.460]   We have an API if you wanna really like customize it,
[00:34:33.460 --> 00:34:37.140]   you know, if you wanna create the Lex Friedman,
[00:34:37.140 --> 00:34:40.900]   you know, CMS or whatever.
[00:34:40.900 --> 00:34:42.300]   - For this whole thing, okay, cool.
[00:34:42.300 --> 00:34:46.460]   So can you speak to the ASR a little bit more?
[00:34:46.460 --> 00:34:51.460]   Like, what does it take like approach-wise,
[00:34:51.460 --> 00:34:54.980]   machine learning-wise, how hard is this problem?
[00:34:54.980 --> 00:34:57.700]   How do you get to the 3% error rate?
[00:34:57.700 --> 00:34:59.340]   Like, what's your vision of all of this?
[00:34:59.340 --> 00:35:03.180]   - Yeah, well, the 3% error rate is definitely,
[00:35:03.180 --> 00:35:05.220]   that's the grand vision.
[00:35:05.220 --> 00:35:07.940]   We'll see what it takes to get there.
[00:35:09.860 --> 00:35:13.060]   But we believe, you know, in ASR,
[00:35:13.060 --> 00:35:15.220]   the biggest thing is the data, right?
[00:35:15.220 --> 00:35:16.420]   Like, that's true of like a lot
[00:35:16.420 --> 00:35:18.340]   of machine learning problems today, right?
[00:35:18.340 --> 00:35:21.060]   The more data you have and the higher quality of the data,
[00:35:21.060 --> 00:35:22.820]   the better labeled the data.
[00:35:22.820 --> 00:35:26.540]   Yeah, that's how you get good results.
[00:35:26.540 --> 00:35:29.820]   And we at Rev have kind of like the best data, like we have.
[00:35:29.820 --> 00:35:32.460]   - Like you're literally, your business model
[00:35:32.460 --> 00:35:34.020]   is annotating the data.
[00:35:34.020 --> 00:35:36.780]   - Our business model is being paid to annotate the data.
[00:35:36.780 --> 00:35:39.140]   - Being paid to annotate the data.
[00:35:39.140 --> 00:35:42.060]   - So it's kind of like a pretty magical flywheel.
[00:35:42.060 --> 00:35:42.900]   - Yeah.
[00:35:42.900 --> 00:35:44.540]   - And so we've kind of like ridden this flywheel
[00:35:44.540 --> 00:35:47.060]   to this point.
[00:35:47.060 --> 00:35:50.540]   And we think we're still kind of in the early stages
[00:35:50.540 --> 00:35:53.100]   of figuring out all the parts of the flywheel to use,
[00:35:53.100 --> 00:35:56.420]   you know, because we have the final transcripts
[00:35:56.420 --> 00:36:01.660]   and we have the audios and we train on that.
[00:36:01.660 --> 00:36:05.060]   But we, in principle, also have all the edits
[00:36:05.060 --> 00:36:06.500]   that the Revvers make, right?
[00:36:06.500 --> 00:36:08.700]   - Oh, that's interesting.
[00:36:08.700 --> 00:36:10.540]   How can you use that as data?
[00:36:10.540 --> 00:36:12.580]   - We basically, that's something for us to figure out
[00:36:12.580 --> 00:36:14.380]   in the future, but you know,
[00:36:14.380 --> 00:36:16.300]   we feel like we're only in the early stages, right?
[00:36:16.300 --> 00:36:18.620]   - So the data is there, that'd be interesting,
[00:36:18.620 --> 00:36:20.820]   like almost like a recurrent neural net
[00:36:20.820 --> 00:36:23.380]   for fixing transcripts.
[00:36:23.380 --> 00:36:28.380]   I always remember we did a segmentation annotation
[00:36:28.380 --> 00:36:33.220]   for driving data, so segmenting the scene, like visual data.
[00:36:33.220 --> 00:36:35.980]   And you can get all, so it was drawing,
[00:36:35.980 --> 00:36:38.940]   people were drawing polygons around different objects
[00:36:38.940 --> 00:36:40.060]   and so on.
[00:36:40.060 --> 00:36:42.860]   And it feels like, it always felt like there was a lot
[00:36:42.860 --> 00:36:45.220]   of information in the clicking,
[00:36:45.220 --> 00:36:47.020]   the sequence of clicking that people do,
[00:36:47.020 --> 00:36:49.460]   the kind of fixing of the polygons that they do.
[00:36:49.460 --> 00:36:54.860]   Now there's a few papers written about how to draw polygons,
[00:36:54.860 --> 00:36:59.220]   like with recurrent neural nets to try to learn
[00:36:59.220 --> 00:37:03.560]   from the human clicking, but it was just like experimental,
[00:37:04.380 --> 00:37:06.660]   you know, it was one of those like CVPR type papers
[00:37:06.660 --> 00:37:08.980]   that people do like a really tiny data set.
[00:37:08.980 --> 00:37:13.100]   It didn't feel like people really tried to do it seriously.
[00:37:13.100 --> 00:37:15.140]   And I wonder, I wonder if there's information
[00:37:15.140 --> 00:37:20.140]   in the fixing that provides deeper set of signal
[00:37:20.140 --> 00:37:24.460]   than just like the raw data.
[00:37:24.460 --> 00:37:26.220]   - The intuition is for sure there must be, right?
[00:37:26.220 --> 00:37:27.060]   - There must be.
[00:37:27.060 --> 00:37:29.700]   - And in all kinds of signals and how long you took
[00:37:29.700 --> 00:37:32.700]   to make that edit and stuff like that.
[00:37:32.700 --> 00:37:34.140]   - Yeah, it's gonna be like up to us.
[00:37:34.140 --> 00:37:36.820]   That's why like the next couple of years
[00:37:36.820 --> 00:37:38.340]   is like super exciting for us, right?
[00:37:38.340 --> 00:37:40.340]   - So that's what like the focus is now.
[00:37:40.340 --> 00:37:43.340]   You mentioned Rev.ai, that's where you want to.
[00:37:43.340 --> 00:37:48.340]   - Yeah, so Rev.ai is kind of our way of bringing this ASR
[00:37:48.340 --> 00:37:51.580]   to the rest of the world, right?
[00:37:51.580 --> 00:37:55.660]   So when we started, we were human only,
[00:37:55.660 --> 00:37:59.220]   then we kind of created this TEMI service,
[00:37:59.220 --> 00:38:00.660]   I think you might've used it,
[00:38:00.660 --> 00:38:02.580]   which was kind of ASR for the consumer, right?
[00:38:02.580 --> 00:38:04.580]   So if you don't want to pay $1.25,
[00:38:04.580 --> 00:38:08.100]   but you want to pay, now it's 25 cents a minute, I think.
[00:38:08.100 --> 00:38:10.700]   And you get the transcript,
[00:38:10.700 --> 00:38:12.740]   the machine generated transcript,
[00:38:12.740 --> 00:38:17.460]   you get an editor and you can kind of fix it up yourself.
[00:38:17.460 --> 00:38:21.980]   Then we started using ASR for human transcriptionists.
[00:38:21.980 --> 00:38:23.460]   And then the kind of Rev.ai is the final step
[00:38:23.460 --> 00:38:27.100]   of the journey, which is, we have this amazing engine.
[00:38:27.100 --> 00:38:28.860]   What can people build with it, right?
[00:38:28.860 --> 00:38:32.340]   What kind of new applications could be enabled
[00:38:32.340 --> 00:38:36.340]   if you have SpeedTrack that's that accurate?
[00:38:36.340 --> 00:38:37.580]   - Do you have ideas for this
[00:38:37.580 --> 00:38:39.300]   or is it just providing it as a service
[00:38:39.300 --> 00:38:40.660]   and seeing what people come up with?
[00:38:40.660 --> 00:38:41.980]   - It's providing it as a service
[00:38:41.980 --> 00:38:43.500]   and seeing what people come up with
[00:38:43.500 --> 00:38:45.580]   and kind of learning from what people do with it.
[00:38:45.580 --> 00:38:47.180]   And we have ideas of our own as well, of course,
[00:38:47.180 --> 00:38:49.220]   but it's a little bit like,
[00:38:49.220 --> 00:38:52.580]   when AWS provided the building blocks, right?
[00:38:52.580 --> 00:38:53.940]   And they saw what people built with it
[00:38:53.940 --> 00:38:57.020]   and they try to make it easier to build those things, right?
[00:38:57.020 --> 00:38:59.180]   And we kind of hope to do the same thing.
[00:38:59.180 --> 00:39:02.860]   - Although AWS kind of does a shitty job of like,
[00:39:02.860 --> 00:39:05.060]   I'm continually surprised, like Mechanical Turk,
[00:39:05.060 --> 00:39:07.780]   for example, how shitty the interface is.
[00:39:07.780 --> 00:39:11.140]   We're talking about like Rev.ai making me feel good.
[00:39:11.140 --> 00:39:14.180]   Like when I first discovered Mechanical Turk,
[00:39:14.180 --> 00:39:18.260]   the initial idea of it was like,
[00:39:18.260 --> 00:39:19.660]   it made me feel like Rev.ai does,
[00:39:19.660 --> 00:39:22.820]   but then the interface is like, come on.
[00:39:22.820 --> 00:39:24.180]   - Yeah, it's horrible.
[00:39:24.740 --> 00:39:27.740]   - Why is it so painful?
[00:39:27.740 --> 00:39:32.500]   Does nobody at Amazon wanna like seriously invest in it?
[00:39:32.500 --> 00:39:34.980]   It felt like you can make so much money
[00:39:34.980 --> 00:39:37.260]   if you took this effort seriously.
[00:39:37.260 --> 00:39:40.220]   And it feels like they have a committee of like two people
[00:39:40.220 --> 00:39:42.900]   just sitting back, like a meeting,
[00:39:42.900 --> 00:39:43.980]   they meet once a month,
[00:39:43.980 --> 00:39:46.540]   like what are we gonna do with Mechanical Turk?
[00:39:46.540 --> 00:39:49.260]   It's like two websites make me feel like this,
[00:39:49.260 --> 00:39:53.620]   that and craiglist.org, whatever the hell it is.
[00:39:53.620 --> 00:39:55.940]   - It feels like it's designed in the 90s.
[00:39:55.940 --> 00:39:59.140]   - Well, craiglist basically hasn't been updated
[00:39:59.140 --> 00:39:59.980]   pretty much since the guy originally built.
[00:39:59.980 --> 00:40:01.860]   - Do you seriously think there's a team,
[00:40:01.860 --> 00:40:04.220]   like how big is the team working on Mechanical Turk?
[00:40:04.220 --> 00:40:06.820]   - I don't know, there's some team, right?
[00:40:06.820 --> 00:40:09.460]   - I feel like there isn't, I'm skeptical.
[00:40:09.460 --> 00:40:13.500]   - Yeah, well, if nothing else, they benefit from,
[00:40:13.500 --> 00:40:16.380]   you know, the other teams like moving things forward,
[00:40:16.380 --> 00:40:18.380]   right, in a small way.
[00:40:18.380 --> 00:40:19.740]   But no, I know what you mean,
[00:40:19.740 --> 00:40:22.260]   we use Mechanical Turk for a couple of things as well,
[00:40:22.260 --> 00:40:24.340]   and yeah, it's painful.
[00:40:24.340 --> 00:40:25.700]   - But yeah, it works.
[00:40:25.700 --> 00:40:27.540]   - I think most people, the thing is most people
[00:40:27.540 --> 00:40:29.140]   don't really use the UI, right?
[00:40:29.140 --> 00:40:33.580]   Like, so like we, for example, we use it through the API.
[00:40:33.580 --> 00:40:36.100]   - But even the API documentation and so on,
[00:40:36.100 --> 00:40:37.540]   like it's super outdated.
[00:40:37.540 --> 00:40:45.100]   I don't even know what to, I mean, same criticism,
[00:40:45.100 --> 00:40:49.060]   as long as we're ranting, my same criticism goes
[00:40:49.060 --> 00:40:50.940]   to the APIs of most of these companies,
[00:40:50.940 --> 00:40:55.180]   like Google, for example, the API for the different services
[00:40:55.180 --> 00:40:58.980]   is just the documentation is so shitty.
[00:40:58.980 --> 00:41:04.860]   Like, it's not so shitty, I should actually be,
[00:41:04.860 --> 00:41:08.380]   I should exhibit some gratitude.
[00:41:08.380 --> 00:41:10.900]   Okay, let's practice some gratitude.
[00:41:10.900 --> 00:41:14.340]   The, you know, the documentation is pretty good.
[00:41:14.340 --> 00:41:18.820]   Like most of the things that the API makes available
[00:41:18.820 --> 00:41:19.700]   is pretty good.
[00:41:19.700 --> 00:41:23.100]   It's just that in the sense that it's accurate,
[00:41:23.100 --> 00:41:27.260]   sometimes outdated, but like the degree of explanations
[00:41:27.260 --> 00:41:32.260]   with examples is only covering, I would say like 50%
[00:41:32.260 --> 00:41:33.900]   of what's possible.
[00:41:33.900 --> 00:41:36.300]   And it just feels a little bit like there's a lot
[00:41:36.300 --> 00:41:38.980]   of natural questions that people would wanna ask
[00:41:38.980 --> 00:41:41.660]   that doesn't get covered.
[00:41:41.660 --> 00:41:44.580]   And it feels like it's almost there.
[00:41:44.580 --> 00:41:48.540]   Like it's such a magical thing, like the Maps API,
[00:41:48.540 --> 00:41:51.180]   YouTube API, there's a bunch of stuff.
[00:41:51.180 --> 00:41:52.780]   - I gotta imagine it's like, you know,
[00:41:52.780 --> 00:41:55.580]   there's probably some team at Google, right,
[00:41:55.580 --> 00:41:57.500]   responsible for writing this documentation.
[00:41:57.500 --> 00:42:00.300]   That's probably not the engineers, right?
[00:42:00.300 --> 00:42:03.260]   And probably this team is not, you know,
[00:42:03.260 --> 00:42:04.460]   where you wanna be.
[00:42:04.460 --> 00:42:05.860]   - Well, it's a weird thing.
[00:42:05.860 --> 00:42:09.620]   I sometimes think about this for somebody who wants
[00:42:09.620 --> 00:42:12.180]   to also build the company.
[00:42:12.180 --> 00:42:14.980]   I think about this a lot.
[00:42:15.820 --> 00:42:20.820]   You know, YouTube, the service is one of the most magical,
[00:42:20.820 --> 00:42:24.500]   like I'm so grateful that YouTube exists.
[00:42:24.500 --> 00:42:29.500]   And yet they seem to be quite clueless on so many things
[00:42:29.500 --> 00:42:33.420]   like that everybody's screaming them at.
[00:42:33.420 --> 00:42:38.100]   Like it feels like whatever the mechanism that you use
[00:42:38.100 --> 00:42:40.100]   to listen to your quote unquote customers,
[00:42:40.100 --> 00:42:44.820]   which is like the creators is not very good.
[00:42:44.820 --> 00:42:47.300]   Like there's literally people that are like screaming,
[00:42:47.300 --> 00:42:51.060]   like their new YouTube studio, for example.
[00:42:51.060 --> 00:42:55.180]   There's like features that were like begged for,
[00:42:55.180 --> 00:42:56.940]   for a really long time,
[00:42:56.940 --> 00:43:00.180]   like being able to upload multiple videos at the same time.
[00:43:00.180 --> 00:43:03.940]   That was missing for a really, really long time.
[00:43:03.940 --> 00:43:08.020]   Now, like there's probably things that I don't know,
[00:43:08.020 --> 00:43:10.980]   which is maybe for that kind of huge infrastructure,
[00:43:10.980 --> 00:43:13.780]   it's actually very difficult to build some of these features.
[00:43:13.780 --> 00:43:15.580]   But the fact that that wasn't communicated
[00:43:15.580 --> 00:43:19.180]   and it felt like you're not being heard.
[00:43:19.180 --> 00:43:21.580]   Like I remember this experience for me
[00:43:21.580 --> 00:43:23.860]   and it's not a pleasant experience.
[00:43:23.860 --> 00:43:26.780]   And it feels like the company doesn't give a damn about you.
[00:43:26.780 --> 00:43:28.220]   And that's something to think about.
[00:43:28.220 --> 00:43:30.020]   I'm not sure what that is.
[00:43:30.020 --> 00:43:32.540]   That might have to do with just like small groups
[00:43:32.540 --> 00:43:35.940]   working on these small features and these specific features.
[00:43:35.940 --> 00:43:40.340]   And there's no overarching like dictator type of human
[00:43:40.340 --> 00:43:42.460]   that says like, why the hell are we neglecting
[00:43:42.460 --> 00:43:43.940]   like Steve Jobs type of characters?
[00:43:43.940 --> 00:43:48.940]   Like there's people that we need to speak to the people
[00:43:48.940 --> 00:43:51.700]   that like wanna love our product and they don't.
[00:43:51.700 --> 00:43:52.540]   Let's fix this shit. - Yeah, I mean,
[00:43:52.540 --> 00:43:54.900]   at some point you just get so fixated on the numbers.
[00:43:54.900 --> 00:43:56.980]   And it's like, well, the numbers are pretty great.
[00:43:56.980 --> 00:43:58.660]   Like people are watching,
[00:43:58.660 --> 00:44:01.060]   doesn't seem to be a problem.
[00:44:01.060 --> 00:44:01.980]   - Doesn't seem to be a problem.
[00:44:01.980 --> 00:44:04.260]   - And you're not like the person that like build this thing.
[00:44:04.260 --> 00:44:05.860]   So you really care about it.
[00:44:05.860 --> 00:44:09.140]   You're just there, you came in as a product manager.
[00:44:09.140 --> 00:44:10.700]   You got hired sometime later,
[00:44:10.700 --> 00:44:15.700]   your mandate is like increase this number like 10%, right?
[00:44:15.700 --> 00:44:17.540]   And you just-- - That's brilliantly put.
[00:44:17.540 --> 00:44:21.380]   Like if you, this is, okay, if there's a lesson in this,
[00:44:21.380 --> 00:44:25.340]   is don't reduce your company into a metric of like,
[00:44:25.340 --> 00:44:27.820]   how much, like you said,
[00:44:27.820 --> 00:44:31.020]   how much people watching the videos and so on,
[00:44:31.020 --> 00:44:33.860]   and like convince yourself that everything is working
[00:44:33.860 --> 00:44:36.220]   just because the numbers are going up.
[00:44:36.220 --> 00:44:39.140]   There's something, you have to have a vision.
[00:44:39.140 --> 00:44:43.420]   You have to want people to love your stuff
[00:44:43.420 --> 00:44:46.140]   because love is ultimately the beginning
[00:44:46.140 --> 00:44:49.260]   of like a successful long-term company
[00:44:49.260 --> 00:44:51.340]   is that they always should love your product.
[00:44:51.340 --> 00:44:52.620]   - You have to be like a creator
[00:44:52.620 --> 00:44:55.420]   and have that like creator's love for your own thing, right?
[00:44:55.420 --> 00:44:59.580]   Like, and you paint by, you know, these comments, right?
[00:44:59.580 --> 00:45:02.460]   And probably like, Apple, I think did this generally
[00:45:02.460 --> 00:45:03.860]   like really well. - Yes, really well.
[00:45:03.860 --> 00:45:06.860]   - They're well known for kind of keeping teams small,
[00:45:06.860 --> 00:45:08.220]   even when they were big, right?
[00:45:08.220 --> 00:45:10.380]   And, you know, he was an engineer,
[00:45:10.380 --> 00:45:12.700]   like there's that book, "Creative Selection."
[00:45:12.700 --> 00:45:15.460]   I don't know if you read it by an Apple engineer
[00:45:15.460 --> 00:45:17.300]   named Ken Kosienda.
[00:45:17.300 --> 00:45:18.300]   It's kind of a great book actually,
[00:45:18.300 --> 00:45:21.420]   because unlike most of these business books where it's,
[00:45:21.420 --> 00:45:24.580]   you know, here's how Steve Jobs ran the company.
[00:45:24.580 --> 00:45:26.940]   It's more like, here's how life was like for me,
[00:45:26.940 --> 00:45:27.780]   you know, an engineer.
[00:45:27.780 --> 00:45:29.020]   Here are the projects I worked on
[00:45:29.020 --> 00:45:31.660]   and here what it was like to pitch Steve Jobs, you know,
[00:45:31.660 --> 00:45:34.620]   on like, you know, I think it was in charge of like
[00:45:34.620 --> 00:45:36.860]   the keyboard and the auto correction, right?
[00:45:36.860 --> 00:45:39.420]   And at Apple, like Steve Jobs reviewed everything.
[00:45:39.420 --> 00:45:41.140]   And so he was like, this is what it was like
[00:45:41.140 --> 00:45:43.740]   to show my demos to Steve Jobs and, you know,
[00:45:43.740 --> 00:45:46.580]   to change them because like Steve Jobs didn't like how,
[00:45:46.580 --> 00:45:48.780]   you know, the shape of the little key was off
[00:45:48.780 --> 00:45:50.900]   because the rounding of the corner was like not quite right
[00:45:50.900 --> 00:45:53.380]   or something like this, but he was famously a stickler
[00:45:53.380 --> 00:45:54.620]   for this kind of stuff.
[00:45:54.620 --> 00:45:55.820]   But because the teams were small,
[00:45:55.820 --> 00:45:56.900]   he really owned this stuff, right?
[00:45:56.900 --> 00:45:58.660]   So he really cared.
[00:45:58.660 --> 00:46:01.620]   - Yeah, Elon Musk does that similar kind of thing with Tesla,
[00:46:01.620 --> 00:46:03.380]   which is really interesting.
[00:46:03.380 --> 00:46:05.900]   There's another lesson in leadership in that
[00:46:05.900 --> 00:46:07.660]   is to be obsessed with the details.
[00:46:07.660 --> 00:46:11.300]   And like, he talks to like the lowest level engineers.
[00:46:11.300 --> 00:46:14.620]   Okay, so we're talking about ASR.
[00:46:14.620 --> 00:46:17.660]   And so this is basically where I was saying,
[00:46:17.660 --> 00:46:20.380]   we're gonna take this like ultra seriously.
[00:46:20.380 --> 00:46:22.660]   And then what's the mission?
[00:46:22.660 --> 00:46:24.980]   To try to keep pushing towards the 3%?
[00:46:24.980 --> 00:46:30.340]   - Yeah, and kind of try to build this platform
[00:46:30.340 --> 00:46:33.940]   where all of your, you know, all of your meetings,
[00:46:33.940 --> 00:46:38.460]   you know, they're as easily accessible as your notes, right?
[00:46:38.460 --> 00:46:41.380]   Like, so like imagine all the meetings
[00:46:41.380 --> 00:46:42.780]   a company might have, right?
[00:46:42.780 --> 00:46:46.340]   Now that I'm like no longer a programmer, right?
[00:46:46.340 --> 00:46:48.100]   And I'm a quote unquote manager,
[00:46:48.100 --> 00:46:51.460]   that's less like my day is in meetings, right?
[00:46:51.460 --> 00:46:54.860]   And, you know, pretty often I wanna like see what was said,
[00:46:54.860 --> 00:46:57.100]   right, who said it, you know, what's the context.
[00:46:57.100 --> 00:46:59.420]   But it's generally not really something
[00:46:59.420 --> 00:47:00.500]   that you can easily retrieve, right?
[00:47:00.500 --> 00:47:03.220]   Like imagine if all of those meetings
[00:47:03.220 --> 00:47:05.780]   were indexed, archived, you know, you could go back,
[00:47:05.780 --> 00:47:08.300]   you could share a clip like really easily, right?
[00:47:08.300 --> 00:47:10.060]   - So that might change completely.
[00:47:10.060 --> 00:47:12.940]   Like everything that's said converted to text
[00:47:12.940 --> 00:47:14.860]   might change completely the dynamics
[00:47:14.860 --> 00:47:16.340]   of what we do in this world.
[00:47:16.340 --> 00:47:18.180]   Especially now with remote work, right?
[00:47:18.180 --> 00:47:19.980]   - Exactly, exactly.
[00:47:19.980 --> 00:47:21.460]   - With Zoom and so on.
[00:47:21.460 --> 00:47:22.740]   That's fascinating to think about.
[00:47:22.740 --> 00:47:25.580]   I mean, for me, I care about podcasts, right?
[00:47:25.580 --> 00:47:30.580]   And one of the things that was, you know, I'm torn.
[00:47:31.140 --> 00:47:33.580]   I know a lot of the engineers at Spotify.
[00:47:33.580 --> 00:47:38.580]   So I love them very much because they dream big
[00:47:38.580 --> 00:47:43.340]   in terms of like, they wanna empower creators.
[00:47:43.340 --> 00:47:45.020]   So one of my hopes was with Spotify
[00:47:45.020 --> 00:47:46.780]   that they would use a technology like Rev
[00:47:46.780 --> 00:47:51.660]   or something like that to start converting everything
[00:47:51.660 --> 00:47:55.180]   into text and make it indexable.
[00:47:55.180 --> 00:47:59.500]   Like one of the things that sucks with podcasts
[00:47:59.500 --> 00:48:01.780]   is like, it's hard to find stuff.
[00:48:01.780 --> 00:48:04.460]   Like the model is basically subscription.
[00:48:04.460 --> 00:48:09.460]   Like you find, it's similar to what YouTube used to be like,
[00:48:09.460 --> 00:48:14.220]   which is you basically find a creator that you enjoy
[00:48:14.220 --> 00:48:16.420]   and you subscribe to them and like, you just,
[00:48:16.420 --> 00:48:19.700]   you just kind of follow what they're doing.
[00:48:19.700 --> 00:48:24.260]   But the search and discovery wasn't a big part of YouTube
[00:48:24.260 --> 00:48:25.500]   like in the early days.
[00:48:25.500 --> 00:48:28.500]   But that's what currently with podcasts,
[00:48:28.500 --> 00:48:33.500]   like is the search and discovery is like non-existent.
[00:48:33.500 --> 00:48:35.260]   You're basically searching for like
[00:48:35.260 --> 00:48:36.420]   the dumbest possible thing,
[00:48:36.420 --> 00:48:39.660]   which is like keywords in the titles of episodes.
[00:48:39.660 --> 00:48:41.220]   - Yeah, but even aside from searching,
[00:48:41.220 --> 00:48:42.180]   it's kind of like all the time.
[00:48:42.180 --> 00:48:44.140]   So I listened to like a number of podcasts
[00:48:44.140 --> 00:48:46.860]   and there's something sad
[00:48:46.860 --> 00:48:48.580]   and I wanna like go back to that later
[00:48:48.580 --> 00:48:49.820]   because I was trying to, I'm trying to remember,
[00:48:49.820 --> 00:48:50.660]   what do you say?
[00:48:50.660 --> 00:48:52.180]   Like maybe like recommend some cool product
[00:48:52.180 --> 00:48:53.460]   that I wanna try out.
[00:48:53.460 --> 00:48:54.700]   And like, it's basically impossible.
[00:48:54.700 --> 00:48:56.780]   Maybe like some people have pretty good show notes.
[00:48:56.780 --> 00:48:59.020]   So maybe you'll get lucky and you can find it, right?
[00:48:59.020 --> 00:49:01.580]   But I mean, if everyone had transcripts
[00:49:01.580 --> 00:49:03.340]   and it was all searchable, it would be--
[00:49:03.340 --> 00:49:04.300]   - It's a game changer.
[00:49:04.300 --> 00:49:05.300]   - It'd be so much better.
[00:49:05.300 --> 00:49:08.460]   - I mean, that's one of the things that I wanted to,
[00:49:08.460 --> 00:49:11.020]   I mean, one of the reasons we're talking today
[00:49:11.020 --> 00:49:13.380]   is I wanted to take this quite seriously,
[00:49:13.380 --> 00:49:15.700]   the rev thing, I've just been lazy.
[00:49:15.700 --> 00:49:19.460]   So because I'm very fortunate
[00:49:19.460 --> 00:49:21.220]   that a lot of people support this podcast,
[00:49:21.220 --> 00:49:24.140]   that there's enough money now to do a transcription
[00:49:24.140 --> 00:49:28.300]   and so on, it seemed clear to me,
[00:49:28.300 --> 00:49:32.940]   especially like CEOs and sort of like PhDs,
[00:49:32.940 --> 00:49:36.420]   like people write to me
[00:49:36.420 --> 00:49:38.340]   who are like graduate students in computer science
[00:49:38.340 --> 00:49:41.140]   or graduate students in whatever the heck field.
[00:49:41.140 --> 00:49:43.140]   It's clear that their mind,
[00:49:43.140 --> 00:49:45.220]   like they enjoy podcasts when they're doing laundry
[00:49:45.220 --> 00:49:48.780]   or whatever, but they wanna revisit the conversation
[00:49:48.780 --> 00:49:50.780]   in a much more rigorous way.
[00:49:50.780 --> 00:49:53.340]   And they really wanna transcript.
[00:49:53.340 --> 00:49:56.780]   It's clear that they want to like analyze conversations.
[00:49:56.780 --> 00:49:59.300]   So many people wrote to me about a transcript
[00:49:59.300 --> 00:50:01.060]   for Yosha Bach conversation.
[00:50:01.060 --> 00:50:03.740]   I had just a bunch of conversations.
[00:50:03.740 --> 00:50:05.820]   And then on the Elon Musk side,
[00:50:05.820 --> 00:50:08.260]   like reporters want like,
[00:50:08.260 --> 00:50:10.820]   they wanna write a blog post about your conversation.
[00:50:10.820 --> 00:50:13.060]   So they wanna be able to pull stuff.
[00:50:13.060 --> 00:50:15.500]   And it's like, they're essentially doing
[00:50:15.500 --> 00:50:18.340]   on your conversation transcription privately.
[00:50:18.340 --> 00:50:21.780]   They're doing it for themselves and then starting to pick,
[00:50:21.780 --> 00:50:23.940]   but it's so much easier when you can actually do it
[00:50:23.940 --> 00:50:26.180]   as a reporter, just look at the transcript.
[00:50:26.180 --> 00:50:28.140]   - Yeah, and you can like embed a little thing,
[00:50:28.140 --> 00:50:29.580]   you know, like into your article, right?
[00:50:29.580 --> 00:50:30.500]   Here's what they said.
[00:50:30.500 --> 00:50:33.580]   You can go listen to like this clip from the section.
[00:50:33.580 --> 00:50:35.940]   - I'm actually trying to figure out,
[00:50:35.940 --> 00:50:40.260]   I'll probably on the website create like a place
[00:50:40.260 --> 00:50:42.460]   where the transcript goes like as a webpage
[00:50:42.460 --> 00:50:44.340]   so that people can reference it,
[00:50:44.340 --> 00:50:46.700]   like reporters can reference it and so on.
[00:50:46.700 --> 00:50:49.660]   I mean, most of the reporters probably
[00:50:49.660 --> 00:50:52.740]   have wanted right clickbait articles
[00:50:52.740 --> 00:50:55.380]   that are complete falsifying, which I'm fine with.
[00:50:55.380 --> 00:50:57.740]   It's the way of journalism, I don't care.
[00:50:57.740 --> 00:51:01.660]   Like I've had this conversation with a friend of mine,
[00:51:01.660 --> 00:51:03.920]   a mixed martial artist, Ryan Hall.
[00:51:03.920 --> 00:51:07.100]   And we talked about, you know,
[00:51:07.100 --> 00:51:09.620]   as I've been reading the rise and fall of the Third Reich
[00:51:09.620 --> 00:51:11.820]   and a bunch of books on Hitler.
[00:51:11.820 --> 00:51:16.360]   And we brought up Hitler and he made some kind of comment
[00:51:16.360 --> 00:51:19.580]   where like we should be able to forgive Hitler.
[00:51:19.580 --> 00:51:23.700]   And, you know, like we were talking about forgiveness
[00:51:23.700 --> 00:51:24.700]   and we're bringing that up
[00:51:24.700 --> 00:51:26.760]   as like the worst case possible things.
[00:51:26.760 --> 00:51:31.760]   Like even, you know, for people who are Holocaust survivors,
[00:51:31.760 --> 00:51:34.700]   one of the ways to let go of the suffering
[00:51:34.700 --> 00:51:38.060]   they've been through is to forgive.
[00:51:38.060 --> 00:51:39.780]   And he brought up like Hitler is somebody
[00:51:39.780 --> 00:51:42.460]   that would potentially be the hardest thing
[00:51:42.460 --> 00:51:43.340]   to possibly forgive,
[00:51:43.340 --> 00:51:47.080]   but it might be a worthwhile pursuit psychologically.
[00:51:47.080 --> 00:51:48.560]   So on, blah, blah, blah, it doesn't matter.
[00:51:48.560 --> 00:51:50.860]   It was very eloquent, very powerful words.
[00:51:50.860 --> 00:51:53.160]   I think people should go back and listen to it.
[00:51:53.160 --> 00:51:54.000]   It's powerful.
[00:51:54.000 --> 00:51:55.680]   And then all these journalists,
[00:51:55.680 --> 00:51:58.960]   all these articles written about like MMA fight,
[00:51:58.960 --> 00:51:59.800]   UFC fight.
[00:51:59.800 --> 00:52:01.920]   - MMA fighter loves Hitler.
[00:52:01.920 --> 00:52:05.720]   - No, like, well, no, they were somewhat accurate.
[00:52:05.720 --> 00:52:07.120]   They didn't say like loves Hitler.
[00:52:07.120 --> 00:52:12.120]   They said, thinks that if Hitler came back to life
[00:52:13.280 --> 00:52:14.440]   we should forgive him.
[00:52:14.440 --> 00:52:18.520]   Like they kind of, it's kind of accurate-ish,
[00:52:18.520 --> 00:52:23.520]   but it, the headline made it sound a lot worse
[00:52:23.520 --> 00:52:27.820]   than it was, but I'm fine with it.
[00:52:27.820 --> 00:52:29.740]   That's the way the world,
[00:52:29.740 --> 00:52:32.680]   I wanna almost make it easier for those journalists
[00:52:32.680 --> 00:52:34.880]   and make it easier for people who actually care
[00:52:34.880 --> 00:52:37.320]   about the conversation to go and look and see.
[00:52:37.320 --> 00:52:38.560]   - Right, they can see it for themselves.
[00:52:38.560 --> 00:52:39.400]   - For themselves, full context.
[00:52:39.400 --> 00:52:41.680]   - There's the headline, but now you can go.
[00:52:41.680 --> 00:52:42.960]   - There's something about podcasts,
[00:52:42.960 --> 00:52:46.680]   like the audio that makes it difficult to go,
[00:52:46.680 --> 00:52:50.720]   to jump to a spot and to look for that,
[00:52:50.720 --> 00:52:53.200]   for that particular information.
[00:52:53.200 --> 00:52:56.920]   I think some of it, you know, I'm interested in creating
[00:52:56.920 --> 00:53:00.320]   like myself experimenting with stuff.
[00:53:00.320 --> 00:53:03.400]   So like taking Rev and creating a transcript
[00:53:03.400 --> 00:53:05.160]   and then people can go to it.
[00:53:05.160 --> 00:53:09.320]   I do dream that like, I'm not in the loop anymore,
[00:53:09.320 --> 00:53:12.960]   that like, you know, Spotify does it, right?
[00:53:12.960 --> 00:53:16.320]   Like automatically for everybody,
[00:53:16.320 --> 00:53:19.600]   because ultimately that one-click purchase
[00:53:19.600 --> 00:53:21.680]   needs to be there, like, you know.
[00:53:21.680 --> 00:53:22.760]   - I mean, like you kind of want support
[00:53:22.760 --> 00:53:24.080]   from the entire ecosystem, right?
[00:53:24.080 --> 00:53:24.920]   - Exactly.
[00:53:24.920 --> 00:53:27.920]   - Like from the tool makers and the podcast creators,
[00:53:27.920 --> 00:53:28.760]   even clients, right?
[00:53:28.760 --> 00:53:33.760]   I mean, imagine if like most podcast apps,
[00:53:33.760 --> 00:53:35.800]   you know, if it was a standard, right?
[00:53:35.800 --> 00:53:38.520]   Here's how you include a transcript into a podcast, right?
[00:53:38.520 --> 00:53:40.680]   Podcast is just an RSS feed ultimately.
[00:53:40.680 --> 00:53:44.480]   And actually just yesterday I saw this company called
[00:53:44.480 --> 00:53:46.640]   Buzzsprout, I think they're called.
[00:53:46.640 --> 00:53:48.320]   So they're trying to do this.
[00:53:48.320 --> 00:53:53.320]   They proposed a spec, an extension to their RSS format
[00:53:53.320 --> 00:53:56.560]   to reference podcasts, reference transcripts
[00:53:56.560 --> 00:53:58.080]   in a standard way.
[00:53:58.080 --> 00:53:59.080]   And they're talking about like,
[00:53:59.080 --> 00:54:02.160]   there's one client dimension that will support it,
[00:54:02.160 --> 00:54:04.040]   but imagine like more clients support it, right?
[00:54:04.040 --> 00:54:08.440]   So any podcast you could go and see the transcripts, right?
[00:54:08.440 --> 00:54:10.480]   On your like normal podcast app.
[00:54:10.480 --> 00:54:11.840]   - Yeah, I mean, somebody,
[00:54:11.840 --> 00:54:14.200]   so I have somebody who works with me,
[00:54:14.200 --> 00:54:19.160]   works with, helps with advertising, Matt,
[00:54:19.160 --> 00:54:20.240]   this awesome guy.
[00:54:20.240 --> 00:54:21.600]   He mentioned Buzzsprout to me,
[00:54:21.600 --> 00:54:23.120]   but he says it's really annoying
[00:54:23.120 --> 00:54:24.920]   'cause they want exclusive,
[00:54:24.920 --> 00:54:26.320]   they wanna host the podcast.
[00:54:26.320 --> 00:54:27.160]   - Right.
[00:54:27.160 --> 00:54:29.280]   - This is the problem with Spotify too.
[00:54:29.280 --> 00:54:33.960]   This is where I'd like to say like F Spotify.
[00:54:33.960 --> 00:54:37.560]   There's a magic to RSS with podcasts.
[00:54:37.560 --> 00:54:40.360]   It can be made available to everyone.
[00:54:40.360 --> 00:54:41.320]   And then there's all,
[00:54:41.320 --> 00:54:44.560]   there's this ecosystem of different podcast players
[00:54:44.560 --> 00:54:47.120]   that emerge and they compete freely.
[00:54:47.120 --> 00:54:48.960]   And that's a beautiful thing.
[00:54:48.960 --> 00:54:50.440]   That's why I go on exclusive,
[00:54:50.440 --> 00:54:52.020]   like Joe Rogan went exclusive.
[00:54:52.020 --> 00:54:54.800]   I'm not sure if you're familiar with,
[00:54:54.800 --> 00:54:56.400]   he went to just Spotify.
[00:54:56.400 --> 00:54:59.600]   As a huge fan of Joe Rogan,
[00:54:59.600 --> 00:55:01.400]   I've been kind of nervous about the whole thing,
[00:55:01.400 --> 00:55:03.000]   but let's see.
[00:55:03.000 --> 00:55:05.040]   I hope that Spotify steps up.
[00:55:05.040 --> 00:55:06.000]   They've added video,
[00:55:06.000 --> 00:55:07.760]   which is very surprising that they were able to--
[00:55:07.760 --> 00:55:10.440]   - So exclusive meaning you can't subscribe
[00:55:10.440 --> 00:55:11.760]   to his RSS feed anymore.
[00:55:11.760 --> 00:55:12.720]   It's only in Spotify.
[00:55:12.720 --> 00:55:15.760]   - For now, you can until December 1st.
[00:55:15.760 --> 00:55:17.760]   And December 1st, it's all,
[00:55:17.760 --> 00:55:20.180]   everything disappears and it's Spotify only.
[00:55:20.180 --> 00:55:25.840]   I, you know, and Spotify gave him $100 million for that.
[00:55:25.840 --> 00:55:28.360]   So it's an interesting deal,
[00:55:28.360 --> 00:55:31.200]   but I, you know, I did some soul searching
[00:55:31.200 --> 00:55:34.520]   and I'm glad he's doing it.
[00:55:34.520 --> 00:55:37.160]   But if Spotify came to me with $100 million,
[00:55:37.160 --> 00:55:40.120]   I wouldn't do it.
[00:55:40.120 --> 00:55:40.960]   I wouldn't do, well,
[00:55:40.960 --> 00:55:42.680]   I have a very different relationship with money.
[00:55:42.680 --> 00:55:46.120]   I hate money, but I just think,
[00:55:46.120 --> 00:55:48.960]   I believe in the pirate radio aspect of podcasting,
[00:55:48.960 --> 00:55:51.080]   the freedom and that there's something--
[00:55:51.080 --> 00:55:52.320]   - The open source spirit.
[00:55:52.320 --> 00:55:54.720]   - The open source spirit, it just doesn't seem right.
[00:55:54.720 --> 00:55:55.820]   It doesn't feel right.
[00:55:55.820 --> 00:55:57.320]   That said, you know,
[00:55:57.320 --> 00:56:00.480]   because so many people care about Joe Rogan's program,
[00:56:00.480 --> 00:56:02.980]   they're gonna hold Spotify's feet to the fire.
[00:56:02.980 --> 00:56:05.040]   Like one of the cool things,
[00:56:05.040 --> 00:56:10.040]   what Joe told me is the reason he likes working with Spotify
[00:56:10.040 --> 00:56:15.440]   is that they're like ride or die together, right?
[00:56:15.440 --> 00:56:19.160]   So they want him to succeed.
[00:56:19.160 --> 00:56:22.080]   So that's why they're not actually telling him what to do,
[00:56:22.080 --> 00:56:23.800]   despite what people think.
[00:56:23.800 --> 00:56:25.000]   They don't tell him,
[00:56:25.000 --> 00:56:26.940]   they don't give him any notes on anything.
[00:56:26.940 --> 00:56:28.520]   They want him to succeed.
[00:56:28.520 --> 00:56:30.920]   And that's the cool thing about exclusivity
[00:56:30.920 --> 00:56:33.440]   with a platform is like,
[00:56:33.440 --> 00:56:36.800]   you kind of want each other to succeed.
[00:56:36.800 --> 00:56:39.720]   And that process can actually be very fruitful.
[00:56:39.720 --> 00:56:43.400]   Like YouTube, it goes back to my criticism.
[00:56:43.400 --> 00:56:47.840]   YouTube generally, no matter how big the creator,
[00:56:47.840 --> 00:56:50.200]   maybe for PewDiePie, something like that,
[00:56:50.200 --> 00:56:51.700]   they want you to succeed.
[00:56:51.700 --> 00:56:52.760]   But for the most part,
[00:56:52.760 --> 00:56:54.680]   from all the big creators I've spoken with,
[00:56:54.680 --> 00:56:57.080]   Veritasium, all those folks,
[00:56:57.080 --> 00:56:59.000]   you know, they get some basic assistance,
[00:56:59.000 --> 00:57:02.800]   but it's not like YouTube doesn't care
[00:57:02.800 --> 00:57:03.840]   if you succeed or not.
[00:57:03.840 --> 00:57:04.680]   They have so many creators.
[00:57:04.680 --> 00:57:06.520]   - They have like a hundred other.
[00:57:06.520 --> 00:57:07.600]   - They don't care.
[00:57:07.600 --> 00:57:12.600]   So, and especially with somebody like Joe Rogan,
[00:57:12.600 --> 00:57:15.080]   who YouTube sees Joe Rogan,
[00:57:15.080 --> 00:57:19.760]   not as a person who might revolutionize the nature of news
[00:57:19.760 --> 00:57:23.900]   and idea space and nuanced conversations.
[00:57:23.900 --> 00:57:26.280]   They see him as a potential person
[00:57:26.280 --> 00:57:30.240]   who has racist guests on,
[00:57:30.240 --> 00:57:31.800]   or like, you know,
[00:57:31.800 --> 00:57:34.440]   they see him as like a headache potentially.
[00:57:34.440 --> 00:57:37.960]   So, you know, a lot of people talk about this.
[00:57:37.960 --> 00:57:40.600]   It's a hard place to be for YouTube actually,
[00:57:40.600 --> 00:57:45.600]   is figuring out with the search and discovery process
[00:57:45.600 --> 00:57:49.040]   of how do you filter out conspiracy theories
[00:57:49.040 --> 00:57:53.360]   and which conspiracy theories represent dangerous untruths
[00:57:53.360 --> 00:57:58.080]   and which conspiracy theories are like vanilla untruths.
[00:57:58.080 --> 00:58:00.620]   And then even when you start having meetings
[00:58:00.620 --> 00:58:03.560]   and discussions about what is true or not,
[00:58:03.560 --> 00:58:05.080]   it starts getting weird.
[00:58:05.080 --> 00:58:05.920]   - Yeah.
[00:58:05.920 --> 00:58:06.760]   - It starts getting weird.
[00:58:06.760 --> 00:58:07.800]   - It's difficult these days, right?
[00:58:07.800 --> 00:58:09.720]   I worry more about the other side, right?
[00:58:09.720 --> 00:58:13.240]   Of too much, you know, too much not censorship.
[00:58:13.240 --> 00:58:14.640]   Well, maybe censorship is the right word.
[00:58:14.640 --> 00:58:17.960]   I mean, censorship is usually government censorship,
[00:58:17.960 --> 00:58:21.980]   but still, yeah, putting yourself in a position of arbiter
[00:58:21.980 --> 00:58:22.920]   for these kinds of things.
[00:58:22.920 --> 00:58:23.760]   - Yeah.
[00:58:23.760 --> 00:58:24.580]   - It's very difficult.
[00:58:24.580 --> 00:58:25.420]   And people think it's so easy, right?
[00:58:25.420 --> 00:58:27.840]   Like, it's like, well, you know, like no Nazis, right?
[00:58:27.840 --> 00:58:29.100]   What a simple principle.
[00:58:29.100 --> 00:58:32.760]   But, you know, yes, I mean, no one likes Nazis.
[00:58:32.760 --> 00:58:33.600]   - Yeah.
[00:58:33.600 --> 00:58:35.240]   - But there's like many shades of gray,
[00:58:35.240 --> 00:58:37.400]   like very soon after that.
[00:58:37.400 --> 00:58:39.440]   - Yeah, and then, you know, of course everybody,
[00:58:39.440 --> 00:58:40.800]   you know, there's some people that call
[00:58:40.800 --> 00:58:42.240]   our current president a Nazi.
[00:58:42.240 --> 00:58:45.720]   And then there's like, so you start getting Sam Harris.
[00:58:45.720 --> 00:58:49.680]   I don't know if you know that is wasted, in my opinion,
[00:58:49.680 --> 00:58:51.640]   his conversation with Jack Dorsey.
[00:58:51.640 --> 00:58:54.200]   Now, I spoke with Jack before on this podcast,
[00:58:54.200 --> 00:58:55.680]   and we'll talk again.
[00:58:55.680 --> 00:59:00.080]   But Sam brought up, Sam Harris does not like Donald Trump.
[00:59:00.080 --> 00:59:03.760]   - I do listen to his podcast.
[00:59:03.760 --> 00:59:06.480]   I'm familiar with his views on the matter.
[00:59:06.480 --> 00:59:08.960]   - And he asked Jack Dorsey, he's like,
[00:59:08.960 --> 00:59:12.280]   how can you not ban Donald Trump from Twitter?
[00:59:12.280 --> 00:59:13.920]   And so, you know, there's a set,
[00:59:13.920 --> 00:59:15.980]   you have that conversation.
[00:59:15.980 --> 00:59:18.240]   You have a conversation where some number,
[00:59:18.240 --> 00:59:20.520]   some significant number of people
[00:59:20.520 --> 00:59:22.920]   think that the current president of the United States
[00:59:22.920 --> 00:59:24.920]   should not be on your platform.
[00:59:24.920 --> 00:59:28.060]   And it's like, okay, so if that's even on the table
[00:59:28.060 --> 00:59:31.360]   as a conversation, then everything's on the table
[00:59:31.360 --> 00:59:32.760]   for conversation.
[00:59:32.760 --> 00:59:34.680]   And yeah, it's tough.
[00:59:34.680 --> 00:59:37.040]   I'm not sure where I land on it.
[00:59:37.040 --> 00:59:39.480]   I'm with you, I think that censorship is bad,
[00:59:39.480 --> 00:59:41.840]   but I also think--
[00:59:41.840 --> 00:59:44.000]   - Ultimately, I just also think, you know,
[00:59:44.000 --> 00:59:46.560]   if you're the kind of person that's gonna be convinced,
[00:59:46.560 --> 00:59:49.460]   you know, by some YouTube video, you know,
[00:59:49.460 --> 00:59:53.160]   that, I don't know, our government's been taken over
[00:59:53.160 --> 00:59:56.120]   by aliens, it's unlikely that, like, you know,
[00:59:56.120 --> 00:59:59.000]   you'll be returned to sanity simply because, you know,
[00:59:59.000 --> 01:00:02.040]   that video is not available on YouTube, right?
[01:00:02.040 --> 01:00:02.860]   - Yeah, I'm with you.
[01:00:02.860 --> 01:00:04.560]   I tend to believe in the intelligence of people
[01:00:04.560 --> 01:00:07.000]   and we should trust them.
[01:00:07.000 --> 01:00:10.980]   But I also do think it's the responsibility of platforms
[01:00:10.980 --> 01:00:12.640]   to encourage more love in the world,
[01:00:12.640 --> 01:00:14.160]   more kindness to each other.
[01:00:14.160 --> 01:00:16.960]   And I don't always think that they're great
[01:00:16.960 --> 01:00:19.280]   at doing that particular thing.
[01:00:19.280 --> 01:00:24.280]   So that, there's a nice balance there.
[01:00:24.280 --> 01:00:28.280]   And I think philosophically, I think about that a lot.
[01:00:28.280 --> 01:00:31.240]   Where's the balance between free speech
[01:00:31.240 --> 01:00:35.040]   and like encouraging people,
[01:00:35.040 --> 01:00:37.940]   even though they have the freedom of speech
[01:00:37.940 --> 01:00:39.560]   to not be an asshole.
[01:00:39.560 --> 01:00:41.060]   - Yeah, right.
[01:00:41.060 --> 01:00:42.860]   - That's not a constitutional, like,
[01:00:42.860 --> 01:00:48.140]   so you have the right for free speech,
[01:00:48.140 --> 01:00:50.700]   but like, just don't be an asshole.
[01:00:50.700 --> 01:00:52.680]   Like, you can't really put that in the constitution
[01:00:52.680 --> 01:00:54.480]   that the Supreme Court can't be like,
[01:00:54.480 --> 01:00:56.100]   just don't be a dick.
[01:00:56.100 --> 01:00:59.580]   But I feel like platforms have a role to be like,
[01:00:59.580 --> 01:01:00.860]   just be nicer.
[01:01:00.860 --> 01:01:04.220]   Maybe do the carrot, like encourage people to be nicer
[01:01:04.220 --> 01:01:06.820]   as opposed to the stake of censorship.
[01:01:06.820 --> 01:01:11.060]   But I think it's an interesting machine learning problem.
[01:01:11.060 --> 01:01:12.040]   Just be nicer.
[01:01:12.040 --> 01:01:15.800]   - Machine, yeah, machine learning for niceness.
[01:01:15.800 --> 01:01:16.640]   - It is, I mean--
[01:01:16.640 --> 01:01:20.160]   - Responsible AI, I mean, it is a thing for sure.
[01:01:20.160 --> 01:01:23.760]   - Jack Dorsey kind of talks about it as a vision for Twitter
[01:01:23.760 --> 01:01:26.840]   is how do we increase the health of conversations?
[01:01:26.840 --> 01:01:28.120]   I don't know how seriously
[01:01:28.120 --> 01:01:30.800]   they're actually trying to do that though,
[01:01:30.800 --> 01:01:35.800]   which is one of the reasons I am in part considering
[01:01:35.800 --> 01:01:37.320]   entering that space a little bit.
[01:01:37.320 --> 01:01:38.560]   - It's difficult for them, right?
[01:01:38.560 --> 01:01:41.080]   Because it's kind of like well known that,
[01:01:42.000 --> 01:01:47.000]   people are kind of driven by rage and outrage maybe
[01:01:47.000 --> 01:01:49.440]   is a better word, right?
[01:01:49.440 --> 01:01:53.080]   Outrage drives engagement and well,
[01:01:53.080 --> 01:01:56.040]   these companies are judged by engagement, right?
[01:01:56.040 --> 01:01:58.200]   - In the short term, but this goes to the metrics thing
[01:01:58.200 --> 01:01:59.360]   that we were talking about earlier.
[01:01:59.360 --> 01:02:03.480]   I do believe, I have a fundamental belief that
[01:02:03.480 --> 01:02:07.960]   if you have a metric of long-term happiness of your users,
[01:02:07.960 --> 01:02:09.560]   like not short-term engagement,
[01:02:09.560 --> 01:02:11.480]   but long-term happiness and growth
[01:02:11.480 --> 01:02:15.520]   and both like intellectual, emotional health of your users,
[01:02:15.520 --> 01:02:17.600]   you're going to make a lot more money.
[01:02:17.600 --> 01:02:18.800]   You're going to have long,
[01:02:18.800 --> 01:02:21.360]   like you should be able to optimize for that.
[01:02:21.360 --> 01:02:24.240]   You don't need to necessarily optimize for engagement.
[01:02:24.240 --> 01:02:25.080]   - Yeah. - And that'll be good
[01:02:25.080 --> 01:02:26.360]   for society too.
[01:02:26.360 --> 01:02:28.760]   - Yeah, no, I mean, I generally agree with you,
[01:02:28.760 --> 01:02:31.960]   but it requires a patient person with,
[01:02:31.960 --> 01:02:36.000]   trust from Wall Street to be able to carry out
[01:02:36.000 --> 01:02:36.840]   such a strategy.
[01:02:36.840 --> 01:02:39.200]   - This is what I believe the Steve Jobs character
[01:02:39.200 --> 01:02:41.920]   and Elon Musk character is like,
[01:02:41.920 --> 01:02:45.240]   you basically have to be so good at your job.
[01:02:45.240 --> 01:02:46.960]   - Right, you got to pass for anything.
[01:02:46.960 --> 01:02:48.680]   - That you can hold the board
[01:02:48.680 --> 01:02:52.000]   and all the investors hostage by saying like,
[01:02:52.000 --> 01:02:56.360]   either we do it my way or I leave.
[01:02:56.360 --> 01:02:59.120]   And everyone is too afraid of you to leave
[01:02:59.120 --> 01:03:00.480]   'cause they believe in your vision.
[01:03:00.480 --> 01:03:02.720]   So that, but that requires being really good
[01:03:02.720 --> 01:03:04.320]   at what you do.
[01:03:04.320 --> 01:03:06.680]   - Requires being Steve Jobs and Elon Musk.
[01:03:06.680 --> 01:03:08.520]   - There's kind of a reason why like a third name
[01:03:08.520 --> 01:03:10.840]   doesn't come immediately to mind, right?
[01:03:10.840 --> 01:03:12.360]   Like there's maybe a handful of other people,
[01:03:12.360 --> 01:03:13.400]   but it's not that many.
[01:03:13.400 --> 01:03:14.240]   - It's not many.
[01:03:14.240 --> 01:03:15.480]   I mean, people say like, why,
[01:03:15.480 --> 01:03:18.320]   like people say that I'm like a fan of Elon Musk.
[01:03:18.320 --> 01:03:20.960]   I'm not, I'm a fan of anybody
[01:03:20.960 --> 01:03:23.080]   who's like Steve Jobs and Elon Musk.
[01:03:23.080 --> 01:03:26.320]   And there's just not many of those folks.
[01:03:26.320 --> 01:03:27.640]   - It's the guy that made us believe
[01:03:27.640 --> 01:03:31.040]   that like we can get to Mars, you know, in 10 years, right?
[01:03:31.040 --> 01:03:32.480]   I mean, that's kind of awesome.
[01:03:32.480 --> 01:03:35.220]   - And it's kind of making it happen, which is like.
[01:03:36.640 --> 01:03:37.480]   - It's great.
[01:03:37.480 --> 01:03:40.520]   - It's kind of gone like that kind of like spirit, right?
[01:03:40.520 --> 01:03:42.280]   Like from a lot of our society, right?
[01:03:42.280 --> 01:03:44.680]   You know, like we can get to the moon in 10 years
[01:03:44.680 --> 01:03:45.720]   and like we did it, right?
[01:03:45.720 --> 01:03:50.720]   - Yeah, especially in this time of so much kind of
[01:03:50.720 --> 01:03:53.840]   existential dread that people are going through
[01:03:53.840 --> 01:03:56.680]   because of COVID, like having rockets
[01:03:56.680 --> 01:04:00.440]   that just keep going out there now with humans.
[01:04:00.440 --> 01:04:03.240]   I don't know that it's just like you said,
[01:04:03.240 --> 01:04:05.560]   I mean, it gives you a reason to wake up in the morning
[01:04:05.560 --> 01:04:08.620]   and dream, for us engineers too.
[01:04:08.620 --> 01:04:13.200]   It is inspiring as hell, man.
[01:04:13.200 --> 01:04:17.160]   Well, let me ask you this, the worst possible question,
[01:04:17.160 --> 01:04:21.400]   which is, so you're like at the core, you're a programmer,
[01:04:21.400 --> 01:04:26.400]   you're an engineer, but now you made the unfortunate choice
[01:04:26.400 --> 01:04:30.760]   or maybe that's the way life goes
[01:04:30.760 --> 01:04:35.160]   of basically moving away from the low level work
[01:04:35.160 --> 01:04:38.120]   and becoming a manager, becoming an executive,
[01:04:38.120 --> 01:04:43.120]   having meetings, what's that transition been like?
[01:04:43.120 --> 01:04:44.920]   - It's been interesting, it's been a journey.
[01:04:44.920 --> 01:04:47.120]   Maybe a couple of things to say about that.
[01:04:47.120 --> 01:04:49.320]   I got into this, right?
[01:04:49.320 --> 01:04:54.320]   Because as a kid, I just remember this like incredible
[01:04:54.320 --> 01:04:57.400]   amazement at being able to write a program, right?
[01:04:57.400 --> 01:05:01.280]   And something comes to life that kind of didn't exist before.
[01:05:01.280 --> 01:05:03.960]   I don't think you have that in like many other fields.
[01:05:03.960 --> 01:05:07.920]   Like you have that with some other kinds of engineering,
[01:05:07.920 --> 01:05:09.680]   but you may be a little bit more limited
[01:05:09.680 --> 01:05:10.720]   with what you can do, right?
[01:05:10.720 --> 01:05:12.600]   But with a computer, you can literally imagine
[01:05:12.600 --> 01:05:14.800]   any kind of program, right?
[01:05:14.800 --> 01:05:17.000]   So it's a little bit God-like what you do
[01:05:17.000 --> 01:05:18.200]   like when you create it.
[01:05:18.200 --> 01:05:21.360]   And so, I mean, that's why I got into it.
[01:05:21.360 --> 01:05:23.240]   - Do you remember like first program you wrote
[01:05:23.240 --> 01:05:25.840]   or maybe the first program that like made you fall in love
[01:05:25.840 --> 01:05:28.040]   with computer science?
[01:05:28.040 --> 01:05:29.440]   - I don't know if it was the first program.
[01:05:29.440 --> 01:05:31.880]   It's probably like trying to write one of those games
[01:05:31.880 --> 01:05:35.400]   and basic, you know, like emulate the snake game or whatever.
[01:05:35.400 --> 01:05:37.840]   I don't remember to be honest, but I enjoyed like,
[01:05:37.840 --> 01:05:40.000]   that's why I always loved about, you know,
[01:05:40.000 --> 01:05:41.840]   being a programmer is just the creation process.
[01:05:41.840 --> 01:05:45.160]   And it's a little bit different when you're not the one
[01:05:45.160 --> 01:05:46.200]   doing the creating.
[01:05:46.200 --> 01:05:50.520]   And, you know, another aspect to it I would say is,
[01:05:50.520 --> 01:05:52.080]   you know, when you're a programmer,
[01:05:52.080 --> 01:05:54.200]   when you're an individual contributor,
[01:05:54.200 --> 01:05:57.840]   it's kind of very easy to know when you're doing a good job,
[01:05:57.840 --> 01:05:58.680]   when you're not doing a good job,
[01:05:58.680 --> 01:05:59.520]   when you're being productive,
[01:05:59.520 --> 01:06:00.400]   when you're not being productive, right?
[01:06:00.400 --> 01:06:03.000]   You can kind of see like you trying to make something
[01:06:03.000 --> 01:06:05.560]   and it's like slowly coming together, right?
[01:06:05.560 --> 01:06:08.880]   And when you're a manager, you know, it's more diffuse,
[01:06:08.880 --> 01:06:09.720]   right?
[01:06:09.720 --> 01:06:12.760]   Like, well, you hope, you know, you're motivating your team
[01:06:12.760 --> 01:06:15.920]   and making them more productive and inspiring them, right?
[01:06:15.920 --> 01:06:18.920]   But it's not like you get some kind of like dopamine signal
[01:06:18.920 --> 01:06:22.440]   because you like completed X lines of code, you know, today.
[01:06:22.440 --> 01:06:25.240]   So kind of like you missed that dopamine rush a little bit
[01:06:25.240 --> 01:06:28.480]   when you first become, but then, you know,
[01:06:28.480 --> 01:06:30.640]   slowly you kind of see, yes,
[01:06:30.640 --> 01:06:32.320]   your teams are doing amazing work, right?
[01:06:32.320 --> 01:06:34.640]   And you can take pride in that.
[01:06:34.640 --> 01:06:38.200]   - You can get like, what is it?
[01:06:38.200 --> 01:06:41.600]   Like a ripple effect of somebody else's dopamine rush.
[01:06:41.600 --> 01:06:42.680]   - Yeah, yeah, yeah.
[01:06:42.680 --> 01:06:44.560]   You live off other people's dopamine.
[01:06:44.560 --> 01:06:50.760]   - So is there pain points and challenges you had to overcome
[01:06:50.760 --> 01:06:54.880]   from going to a programmer to becoming a programmer
[01:06:54.880 --> 01:06:55.960]   of humans?
[01:06:55.960 --> 01:06:57.480]   - Programmer of humans.
[01:06:58.360 --> 01:07:01.520]   I don't know, humans are difficult to understand, you know?
[01:07:01.520 --> 01:07:03.680]   It's like one of those things,
[01:07:03.680 --> 01:07:06.760]   like trying to understand other people's motivations
[01:07:06.760 --> 01:07:08.240]   and what really drives them.
[01:07:08.240 --> 01:07:10.880]   It's difficult, maybe like never really know, right?
[01:07:10.880 --> 01:07:13.360]   - Do you find that people are different?
[01:07:13.360 --> 01:07:14.200]   - Yeah.
[01:07:14.200 --> 01:07:15.840]   - Like I, one of the things,
[01:07:15.840 --> 01:07:21.560]   like I had a group at MIT that, you know,
[01:07:21.560 --> 01:07:26.560]   I found that like some people I could like scream at
[01:07:28.280 --> 01:07:30.920]   and criticize like hard,
[01:07:30.920 --> 01:07:33.600]   and that made them do like much better work
[01:07:33.600 --> 01:07:35.960]   and really push them to their limit.
[01:07:35.960 --> 01:07:39.840]   And there's some people that I had to nonstop compliment
[01:07:39.840 --> 01:07:43.520]   because like they're so already self-critical,
[01:07:43.520 --> 01:07:45.240]   like about everything they do,
[01:07:45.240 --> 01:07:47.300]   that I have to be constantly like,
[01:07:47.300 --> 01:07:51.520]   like I cannot criticize them at all
[01:07:51.520 --> 01:07:53.480]   because they're already criticizing themselves.
[01:07:53.480 --> 01:07:55.320]   And you have to kind of encourage
[01:07:55.320 --> 01:07:58.720]   and like celebrate their little victories.
[01:07:58.720 --> 01:07:59.960]   And it's kind of fascinating,
[01:07:59.960 --> 01:08:04.120]   like how that, the complete difference in people.
[01:08:04.120 --> 01:08:07.040]   - Definitely people will respond to different motivations
[01:08:07.040 --> 01:08:08.280]   and different modes of feedback,
[01:08:08.280 --> 01:08:11.360]   and you kind of have to figure it out.
[01:08:11.360 --> 01:08:13.720]   It was like a pretty good book,
[01:08:13.720 --> 01:08:16.080]   which for some reason now the name escapes me,
[01:08:16.080 --> 01:08:18.880]   about management, "First Break All the Rules."
[01:08:18.880 --> 01:08:19.720]   - "First Break All the Rules?"
[01:08:19.720 --> 01:08:20.920]   - "First Break All the Rules."
[01:08:20.920 --> 01:08:24.000]   It's a book that we generally like ask a lot of
[01:08:24.000 --> 01:08:26.400]   like first time managers to read a ref.
[01:08:26.400 --> 01:08:28.800]   Like one of the kind of philosophies
[01:08:28.800 --> 01:08:31.120]   is managed by exception, right?
[01:08:31.120 --> 01:08:34.480]   Which is, don't like have some standard template,
[01:08:34.480 --> 01:08:38.560]   like here's how I tell this person to do this
[01:08:38.560 --> 01:08:40.040]   or the other thing, here's how I get feedback,
[01:08:40.040 --> 01:08:41.280]   like managed by exception, right?
[01:08:41.280 --> 01:08:42.800]   Every person is a little bit different,
[01:08:42.800 --> 01:08:45.360]   you have to try to understand what drives them
[01:08:45.360 --> 01:08:47.240]   and tailor it to them.
[01:08:47.240 --> 01:08:48.920]   - Since you mentioned books,
[01:08:48.920 --> 01:08:50.840]   I don't know if you can answer this question,
[01:08:50.840 --> 01:08:52.480]   but people love it when I ask it,
[01:08:52.480 --> 01:08:55.880]   which is, are there books, technical, fiction,
[01:08:55.880 --> 01:08:58.560]   or philosophical that you enjoyed
[01:08:58.560 --> 01:09:01.360]   or had an impact on your life that you would recommend?
[01:09:01.360 --> 01:09:04.440]   You already mentioned "Dune," like all of the "Dune."
[01:09:04.440 --> 01:09:05.400]   - All of the "Dune."
[01:09:05.400 --> 01:09:06.760]   The second one was probably the weakest,
[01:09:06.760 --> 01:09:09.800]   but anyway, so yeah, all of the "Dune" is good.
[01:09:09.800 --> 01:09:13.320]   - I mean, yeah, can you just slow little tangent on that?
[01:09:13.320 --> 01:09:16.320]   How many "Dune" books are there?
[01:09:16.320 --> 01:09:18.480]   Like, do you recommend people start with the first one
[01:09:18.480 --> 01:09:19.920]   if that was--
[01:09:19.920 --> 01:09:21.080]   - Yeah, you kind of have to read them all.
[01:09:21.080 --> 01:09:23.280]   I mean, it is a complete story, right?
[01:09:23.280 --> 01:09:25.520]   So you start with the first one,
[01:09:25.520 --> 01:09:27.560]   you gotta read all of them.
[01:09:27.560 --> 01:09:28.840]   - There's not like a tree,
[01:09:28.840 --> 01:09:32.360]   like a creation of the universe
[01:09:32.360 --> 01:09:33.920]   that you should go and sequence?
[01:09:33.920 --> 01:09:35.240]   - You should go and sequence, yeah.
[01:09:35.240 --> 01:09:38.040]   It's kind of a chronological storyline.
[01:09:38.040 --> 01:09:39.280]   There's six books in all.
[01:09:39.280 --> 01:09:43.760]   Then there's like many kind of books
[01:09:43.760 --> 01:09:47.900]   that were written by Frank Herbert's son,
[01:09:47.900 --> 01:09:48.880]   but those are not as good,
[01:09:48.880 --> 01:09:50.920]   so you don't have to bother with those.
[01:09:50.920 --> 01:09:51.760]   - Shots fired.
[01:09:51.760 --> 01:09:52.960]   - Shots fired.
[01:09:52.960 --> 01:09:53.800]   - Okay.
[01:09:53.800 --> 01:09:56.320]   - But the main sequence is good.
[01:09:56.320 --> 01:09:57.800]   So what are some other books?
[01:09:57.800 --> 01:09:59.720]   Maybe there's a few.
[01:09:59.720 --> 01:10:01.400]   So I don't know that like I would say
[01:10:01.400 --> 01:10:04.640]   there's a book that kind of, I don't know,
[01:10:04.640 --> 01:10:06.280]   turned my life around or anything like that,
[01:10:06.280 --> 01:10:09.000]   but here's a couple that I really love.
[01:10:09.000 --> 01:10:13.440]   So one is "Brave New World" by Aldous Huxley.
[01:10:13.440 --> 01:10:20.200]   And it's kind of incredible how prescient he was
[01:10:20.200 --> 01:10:25.200]   about what a brave new world might be like.
[01:10:25.200 --> 01:10:28.440]   You kind of see a genetic sorting in this book,
[01:10:28.440 --> 01:10:30.760]   where there's like these alphas and epsilons
[01:10:30.760 --> 01:10:34.960]   and from like the earliest time of society,
[01:10:34.960 --> 01:10:35.800]   like they're sorted.
[01:10:35.800 --> 01:10:39.080]   Like you can kind of see it in a slightly similar way today
[01:10:39.080 --> 01:10:42.120]   where, well, one of the problems with society
[01:10:42.120 --> 01:10:46.000]   is people are kind of genetically sorting a little bit.
[01:10:46.000 --> 01:10:49.040]   Like there's much less, like most marriages
[01:10:49.040 --> 01:10:53.400]   between people of similar kind of intellectual level
[01:10:53.400 --> 01:10:55.920]   or socioeconomic status, more so these days
[01:10:55.920 --> 01:10:57.600]   than in the past.
[01:10:57.600 --> 01:10:59.120]   And you kind of see some effects of it
[01:10:59.120 --> 01:11:01.720]   in stratifying society and kind of,
[01:11:01.720 --> 01:11:05.880]   he illustrated what that could be like in the extreme.
[01:11:05.880 --> 01:11:07.960]   - Different versions of it on social media as well.
[01:11:07.960 --> 01:11:09.880]   It's not just like marriages and so on.
[01:11:09.880 --> 01:11:12.560]   Like it's genetic sorting in terms of
[01:11:12.560 --> 01:11:15.000]   what Dawkins called memes, his ideas.
[01:11:15.000 --> 01:11:15.840]   - Right, right.
[01:11:15.840 --> 01:11:17.360]   - Being put into these bins
[01:11:17.360 --> 01:11:20.040]   or these little echo chambers and so on.
[01:11:20.040 --> 01:11:21.920]   - Yeah, and that's the book that's,
[01:11:21.920 --> 01:11:23.600]   I think a worthwhile read for everyone.
[01:11:23.600 --> 01:11:25.280]   In 1984 is good, of course, as well.
[01:11:25.280 --> 01:11:26.560]   Like if you're talking about,
[01:11:26.560 --> 01:11:28.240]   dystopian novels of the future.
[01:11:28.240 --> 01:11:30.520]   - Yeah, it's a slightly different view of the future, right?
[01:11:30.520 --> 01:11:32.200]   - But I kind of like identify with
[01:11:32.200 --> 01:11:33.660]   Brave New World a bit more.
[01:11:33.660 --> 01:11:39.920]   Speaking of, not a book, but my favorite kind of
[01:11:39.920 --> 01:11:42.600]   dystopian science fiction is a movie called "Brazil,"
[01:11:42.600 --> 01:11:44.160]   which I don't know if you've heard of.
[01:11:44.160 --> 01:11:46.360]   - I've heard of it and I know I need to watch it,
[01:11:46.360 --> 01:11:50.480]   but yeah, 'cause it's in, is it in English or no?
[01:11:50.480 --> 01:11:52.080]   - It's an English movie, yeah.
[01:11:52.080 --> 01:11:55.780]   And it's a sort of like dystopian movie
[01:11:55.780 --> 01:11:58.600]   of authoritarian incompetence, right?
[01:11:58.600 --> 01:12:03.640]   It's like nothing really works very well.
[01:12:03.640 --> 01:12:05.720]   The system is creaky,
[01:12:05.720 --> 01:12:08.200]   but no one is kind of like willing to challenge it.
[01:12:08.200 --> 01:12:10.040]   Just things kind of amble along.
[01:12:10.040 --> 01:12:13.680]   It kind of strikes me as like a very plausible future
[01:12:13.680 --> 01:12:16.840]   of like, you know, what authoritarians might look like.
[01:12:16.840 --> 01:12:19.240]   It's not like this, you know,
[01:12:19.240 --> 01:12:21.880]   super efficient evil dictatorship of 1984.
[01:12:21.880 --> 01:12:25.240]   It's just kind of like this badly functioning, you know,
[01:12:25.240 --> 01:12:30.080]   but it's status quo, so it just goes on.
[01:12:30.080 --> 01:12:33.520]   - Yeah, that's one funny thing that stands out to me
[01:12:33.520 --> 01:12:37.140]   is in what is this, authoritarian dystopian stuff,
[01:12:37.140 --> 01:12:39.480]   or just basic like, you know,
[01:12:39.480 --> 01:12:42.400]   if you look at the movie "Contagion,"
[01:12:42.400 --> 01:12:44.480]   it seems in the movies,
[01:12:44.480 --> 01:12:48.140]   government is almost always exceptionally competent.
[01:12:48.140 --> 01:12:53.200]   Like, it's like used as a storytelling tool
[01:12:53.200 --> 01:12:55.480]   of like extreme competence.
[01:12:55.480 --> 01:12:58.360]   Like, you know, you use it whether it's good or evil,
[01:12:58.360 --> 01:12:59.680]   but it's competent.
[01:12:59.680 --> 01:13:01.840]   It's very interesting to think about
[01:13:01.840 --> 01:13:06.440]   where much more realistically is incompetence,
[01:13:06.440 --> 01:13:11.280]   and that incompetence is itself has consequences
[01:13:11.280 --> 01:13:13.200]   that are difficult to predict.
[01:13:13.200 --> 01:13:18.000]   Like, bureaucracy has a very boring way of being evil.
[01:13:18.000 --> 01:13:21.400]   Of just, you know, if you look at the show,
[01:13:21.400 --> 01:13:24.560]   HBO show "Chernobyl," it's a really good story
[01:13:24.560 --> 01:13:28.120]   of how bureaucracy, you know,
[01:13:28.120 --> 01:13:32.760]   leads to catastrophic events,
[01:13:32.760 --> 01:13:34.280]   but not through any kind of evil
[01:13:34.280 --> 01:13:37.680]   in any one particular place, but more just like the--
[01:13:37.680 --> 01:13:39.120]   - It's just the system, kind of.
[01:13:39.120 --> 01:13:41.240]   - The system, distorting information
[01:13:41.240 --> 01:13:43.240]   as it travels up the chain,
[01:13:43.240 --> 01:13:46.040]   that people unwilling to take responsibility for things,
[01:13:46.040 --> 01:13:50.960]   and just kind of like this laziness resulting in evil.
[01:13:50.960 --> 01:13:52.320]   - There's a comedic version of this,
[01:13:52.320 --> 01:13:53.680]   I don't know if you've seen this movie,
[01:13:53.680 --> 01:13:55.040]   it's called "The Death of Stalin."
[01:13:55.040 --> 01:13:55.880]   - Yeah.
[01:13:55.880 --> 01:13:56.840]   - All right.
[01:13:56.840 --> 01:13:58.160]   - I like that.
[01:13:58.160 --> 01:14:00.160]   I wish it wasn't so,
[01:14:00.160 --> 01:14:02.440]   there's a movie called "Inglorious Bastards"
[01:14:02.440 --> 01:14:06.340]   about, you know, Hitler and, you know, so on.
[01:14:06.340 --> 01:14:09.720]   For some reason, those movies piss me off.
[01:14:09.720 --> 01:14:11.200]   I know a lot of people love them,
[01:14:11.200 --> 01:14:16.200]   but like, I just feel like there's not enough good movies,
[01:14:16.200 --> 01:14:18.620]   even about Hitler.
[01:14:18.620 --> 01:14:21.480]   There's good movies about the Holocaust,
[01:14:21.480 --> 01:14:23.720]   but even Hitler, there's a movie called "Downfall"
[01:14:23.720 --> 01:14:24.560]   that people should watch,
[01:14:24.560 --> 01:14:26.120]   I think it's the last few days of Hitler,
[01:14:26.120 --> 01:14:28.860]   that's a good movie, turned into a meme.
[01:14:28.860 --> 01:14:29.840]   - Mm-hmm, mm-hmm.
[01:14:29.840 --> 01:14:31.720]   - But it's good, but on Stalin,
[01:14:31.720 --> 01:14:33.840]   I feel like I may be wrong on this,
[01:14:33.840 --> 01:14:35.600]   but at least in the English-speaking world,
[01:14:35.600 --> 01:14:38.820]   there's not good movies about the evil of Stalin.
[01:14:38.820 --> 01:14:40.720]   - That's true, I was trying to say that.
[01:14:40.720 --> 01:14:43.360]   I actually, so I agree with you on "Inglorious Bastards",
[01:14:43.360 --> 01:14:44.560]   I didn't love the movie,
[01:14:44.560 --> 01:14:50.040]   because I felt like kind of the stylizing of it, right?
[01:14:50.040 --> 01:14:54.160]   The whole like Tarantino kind of Tarantinoism,
[01:14:54.160 --> 01:14:56.160]   if you will, kind of detracted from it
[01:14:56.160 --> 01:14:58.440]   and made it seem like unserious a little bit.
[01:14:58.440 --> 01:15:02.280]   But "Death of Stalin", I felt differently.
[01:15:02.280 --> 01:15:03.880]   Maybe it's because it's a comedy to begin with,
[01:15:03.880 --> 01:15:06.600]   so it's not like I'm expecting, you know, seriousness,
[01:15:06.600 --> 01:15:10.800]   but it kind of depicted the absurdity
[01:15:10.800 --> 01:15:13.360]   of the whole situation in a way, right?
[01:15:13.360 --> 01:15:15.320]   I mean, it was funny, so maybe it does make light of it,
[01:15:15.320 --> 01:15:18.240]   but it, some degree, it's probably like this, right?
[01:15:18.240 --> 01:15:21.400]   Like a bunch of kind of people that are like, oh shit,
[01:15:21.400 --> 01:15:22.480]   right, like--
[01:15:22.480 --> 01:15:25.480]   - You're right, but like the thing is,
[01:15:25.480 --> 01:15:30.480]   it was so close to like what probably was reality,
[01:15:30.480 --> 01:15:35.520]   it was caricaturing reality,
[01:15:35.520 --> 01:15:39.360]   to where I think an observer might think that this is not,
[01:15:39.360 --> 01:15:41.680]   like they might think it's a comedy,
[01:15:41.680 --> 01:15:45.600]   when in reality, that's the absurdity
[01:15:45.600 --> 01:15:48.840]   of how people act with dictators.
[01:15:48.840 --> 01:15:53.840]   I mean, I guess it was too close to reality for me.
[01:15:53.840 --> 01:15:57.760]   - The kind of banality of like what were eventually
[01:15:57.760 --> 01:15:59.520]   like fairly evil acts, right?
[01:15:59.520 --> 01:16:02.320]   But like, yeah, they're just a bunch of people
[01:16:02.320 --> 01:16:03.240]   trying to survive.
[01:16:04.480 --> 01:16:05.520]   'Cause I think there's a good,
[01:16:05.520 --> 01:16:07.640]   I haven't watched it yet, the good movie on,
[01:16:07.640 --> 01:16:12.520]   the movie on Churchill with Gary Oldman,
[01:16:12.520 --> 01:16:15.480]   I think it's Gary Oldman, I may be making that up,
[01:16:15.480 --> 01:16:16.320]   but I think he won,
[01:16:16.320 --> 01:16:18.040]   like he was nominated for an Oscar or something.
[01:16:18.040 --> 01:16:21.040]   So I like, I love these movies about these humans
[01:16:21.040 --> 01:16:24.880]   and Stalin, like Chernobyl made me realize,
[01:16:24.880 --> 01:16:28.840]   the HBO show that there's not enough movies about Russia
[01:16:28.840 --> 01:16:33.120]   that capture that spirit.
[01:16:33.120 --> 01:16:35.720]   I'm sure it might be in Russian, there is,
[01:16:35.720 --> 01:16:39.400]   but the fact that some British dude that like did comedy,
[01:16:39.400 --> 01:16:42.240]   I feel like he did like "Hangover" or some shit like that.
[01:16:42.240 --> 01:16:43.200]   I don't know if you're familiar
[01:16:43.200 --> 01:16:44.480]   with the person who created "Chernobyl,"
[01:16:44.480 --> 01:16:45.760]   but he was just like some guy
[01:16:45.760 --> 01:16:47.400]   that doesn't know anything about Russia.
[01:16:47.400 --> 01:16:49.840]   And he just went in and just studied it,
[01:16:49.840 --> 01:16:51.960]   like did a good job of creating it
[01:16:51.960 --> 01:16:56.160]   and then got it so accurate, like poetically.
[01:16:56.160 --> 01:16:58.960]   And the facts that you need to get accurate,
[01:16:58.960 --> 01:17:01.200]   he got accurate, just the spirit of it
[01:17:01.200 --> 01:17:03.880]   down to like the bowls that pets use,
[01:17:03.880 --> 01:17:05.200]   just the whole feel of it.
[01:17:05.200 --> 01:17:07.560]   - It was good, yeah, I saw the series.
[01:17:07.560 --> 01:17:08.720]   - Yeah, it's incredible.
[01:17:08.720 --> 01:17:11.000]   It made me wish that somebody did a good,
[01:17:11.000 --> 01:17:16.000]   like 1930s, like starvation at Stalin,
[01:17:16.000 --> 01:17:20.240]   like leading up to World War II
[01:17:20.240 --> 01:17:23.800]   and in World War II itself, like Stalingrad and so on.
[01:17:23.800 --> 01:17:27.560]   Like, I feel like that story needs to be told.
[01:17:27.560 --> 01:17:29.280]   Millions of people died.
[01:17:30.160 --> 01:17:32.880]   And to me, it's so much more fascinating than Hitler
[01:17:32.880 --> 01:17:37.640]   'cause Hitler is like a caricature of evil almost
[01:17:37.640 --> 01:17:41.920]   that it's so, especially with the Holocaust,
[01:17:41.920 --> 01:17:44.080]   it's so difficult to imagine
[01:17:44.080 --> 01:17:47.640]   that something like that is possible ever again.
[01:17:47.640 --> 01:17:52.640]   Stalin to me represents something that is possible.
[01:17:52.640 --> 01:17:56.800]   Like the so interesting, like the bureaucracy of it,
[01:17:56.800 --> 01:18:01.240]   it's so fascinating that it potentially might be happening
[01:18:01.240 --> 01:18:03.240]   in the world now, like that we're not aware of,
[01:18:03.240 --> 01:18:06.160]   like with North Korea, another one that,
[01:18:06.160 --> 01:18:08.320]   like there should be a good film on
[01:18:08.320 --> 01:18:10.680]   and like the possible things that could be happening
[01:18:10.680 --> 01:18:13.160]   in China with overreach of government.
[01:18:13.160 --> 01:18:15.600]   I don't know, there's a lot of possibilities there,
[01:18:15.600 --> 01:18:16.480]   I suppose.
[01:18:16.480 --> 01:18:18.360]   - Yeah, I wonder how much,
[01:18:18.360 --> 01:18:20.480]   I guess the archives should be maybe more open nowadays.
[01:18:20.480 --> 01:18:23.880]   I mean, for a long time, they just, we didn't know, right?
[01:18:23.880 --> 01:18:25.960]   Anyways, no one in the West knew for sure.
[01:18:25.960 --> 01:18:27.640]   - Well, there's a, I don't know if you know him,
[01:18:27.640 --> 01:18:29.520]   there's a guy named Stephen Kotkin.
[01:18:29.520 --> 01:18:33.160]   He is a historian of Stalin that I spoke to on this podcast.
[01:18:33.160 --> 01:18:34.840]   I'll speak to him again.
[01:18:34.840 --> 01:18:38.120]   The guy knows his shit on Stalin.
[01:18:38.120 --> 01:18:41.240]   He like read everything.
[01:18:41.240 --> 01:18:46.240]   And it's so fascinating to talk to somebody,
[01:18:46.240 --> 01:18:51.760]   like he knows Stalin better than Stalin knew himself.
[01:18:51.760 --> 01:18:53.080]   It's crazy.
[01:18:53.080 --> 01:18:55.520]   Like you have, so I think he's at Princeton.
[01:18:55.520 --> 01:18:58.960]   He is basically, his whole life is Stalin.
[01:18:58.960 --> 01:18:59.840]   - Studying Stalin.
[01:18:59.840 --> 01:19:01.000]   - Yeah, it's great.
[01:19:01.000 --> 01:19:03.720]   And in that context, he also talks about
[01:19:03.720 --> 01:19:06.040]   and writes about Putin a little bit.
[01:19:06.040 --> 01:19:07.920]   I've also read at this point,
[01:19:07.920 --> 01:19:09.800]   I think every biography of Putin,
[01:19:09.800 --> 01:19:14.040]   English biography of Putin, I need to read some Russians.
[01:19:14.040 --> 01:19:15.440]   Obviously, I'm mentally preparing
[01:19:15.440 --> 01:19:17.560]   for a possible conversation with Putin.
[01:19:17.560 --> 01:19:19.400]   - What is your first question to Putin
[01:19:19.400 --> 01:19:22.400]   when you have him on the podcast?
[01:19:24.360 --> 01:19:26.400]   - It's interesting you bring that up.
[01:19:26.400 --> 01:19:28.000]   First of all, I wouldn't tell you, but.
[01:19:28.000 --> 01:19:28.960]   (laughing)
[01:19:28.960 --> 01:19:30.720]   - Can't give it away now.
[01:19:30.720 --> 01:19:34.400]   - But I actually haven't even thought about that.
[01:19:34.400 --> 01:19:35.600]   So my current approach,
[01:19:35.600 --> 01:19:38.520]   and I do this with interviews often,
[01:19:38.520 --> 01:19:40.200]   but obviously that's a special one,
[01:19:40.200 --> 01:19:44.280]   but I try not to think about questions until last minute.
[01:19:44.280 --> 01:19:48.680]   I'm trying to sort of get into the mindset.
[01:19:48.680 --> 01:19:52.320]   And so that's why I'm soaking in a lot of stuff,
[01:19:52.320 --> 01:19:53.960]   not thinking about questions,
[01:19:53.960 --> 01:19:56.120]   just learning about the man.
[01:19:56.120 --> 01:19:59.680]   But in terms of like human to human,
[01:19:59.680 --> 01:20:01.400]   it's like, I would say it's,
[01:20:01.400 --> 01:20:03.440]   I don't know if you're a fan of mob movies,
[01:20:03.440 --> 01:20:05.840]   but like the mafia, which I am,
[01:20:05.840 --> 01:20:07.000]   like Goodfellas and so on,
[01:20:07.000 --> 01:20:12.000]   he's much closer to like mob morality, which is like.
[01:20:12.000 --> 01:20:14.000]   - Mob morality, maybe, I could see that.
[01:20:14.000 --> 01:20:16.600]   But I like your approach anyways of this,
[01:20:16.600 --> 01:20:18.160]   the extreme empathy, right?
[01:20:18.160 --> 01:20:21.360]   It's a little bit like, you know, Hannibal, right?
[01:20:21.360 --> 01:20:22.960]   Like if you ever watched the show Hannibal, right?
[01:20:22.960 --> 01:20:23.960]   They had that guy,
[01:20:23.960 --> 01:20:27.600]   we know Hannibal, of course, like.
[01:20:27.600 --> 01:20:30.280]   - Yeah, sounds like the lamb.
[01:20:30.280 --> 01:20:31.840]   - But there's TV shows as well,
[01:20:31.840 --> 01:20:34.080]   and they focused on this guy, Will Durant,
[01:20:34.080 --> 01:20:36.320]   who's a character like extreme empath, right?
[01:20:36.320 --> 01:20:38.200]   So in the way he like catches all these killers,
[01:20:38.200 --> 01:20:39.280]   is he pretty much,
[01:20:39.280 --> 01:20:42.600]   he can empathize with them, right?
[01:20:42.600 --> 01:20:44.120]   Like he can understand why they're doing
[01:20:44.120 --> 01:20:44.960]   the things they're doing, right?
[01:20:44.960 --> 01:20:48.200]   And it's a pretty excruciating thing, right?
[01:20:48.200 --> 01:20:49.440]   Like, because you're pretty much like
[01:20:49.440 --> 01:20:52.440]   spending half your time in the head of evil people, right?
[01:20:52.440 --> 01:20:53.280]   - Yeah.
[01:20:53.280 --> 01:20:54.240]   - But.
[01:20:54.240 --> 01:20:57.040]   - I mean, I definitely try to do that with other,
[01:20:57.040 --> 01:20:59.120]   so you should do that in moderation,
[01:20:59.120 --> 01:21:04.120]   but I think it's a pretty safe place to be.
[01:21:04.120 --> 01:21:06.520]   One of the cool things with this podcast,
[01:21:06.520 --> 01:21:08.840]   and I know you didn't sign up to hear me
[01:21:08.840 --> 01:21:10.240]   listen to this bullshit, but.
[01:21:10.240 --> 01:21:11.080]   (laughing)
[01:21:11.080 --> 01:21:12.240]   - No, it's interesting.
[01:21:12.240 --> 01:21:17.680]   - What's his name, Chris Latner, who's a Google,
[01:21:17.680 --> 01:21:19.160]   oh, he's not Google anymore, Sci-Fi.
[01:21:19.160 --> 01:21:21.760]   He's one of the most legit engineers I've talked with.
[01:21:21.760 --> 01:21:23.400]   I talked with him again on this podcast,
[01:21:23.400 --> 01:21:26.280]   and one of the, he gives me private advice a lot,
[01:21:26.280 --> 01:21:31.240]   and he said for this podcast, I should like interview,
[01:21:31.240 --> 01:21:34.640]   like I should widen the range of people,
[01:21:34.640 --> 01:21:38.200]   because that gives you much more freedom to do stuff.
[01:21:38.200 --> 01:21:41.560]   Like, so his idea, which I think I agree with Chris,
[01:21:41.560 --> 01:21:44.040]   is that you go to the extremes.
[01:21:44.040 --> 01:21:46.080]   You just like cover every extreme base,
[01:21:46.080 --> 01:21:47.960]   and then it gives you freedom to then
[01:21:47.960 --> 01:21:50.440]   go to the more nuanced conversations.
[01:21:50.440 --> 01:21:53.960]   And it's kinda, I think there's a safe place for that.
[01:21:53.960 --> 01:21:56.680]   There's certainly a hunger for that nuanced conversation,
[01:21:56.680 --> 01:22:00.440]   I think, amongst people, where like on social media,
[01:22:00.440 --> 01:22:04.080]   you get canceled for anything slightly tense,
[01:22:04.080 --> 01:22:06.040]   that there's a hunger to go full.
[01:22:06.040 --> 01:22:08.440]   - Right, you go so far to the opposite side.
[01:22:08.440 --> 01:22:10.760]   And it's like demystifies it a little bit, right?
[01:22:10.760 --> 01:22:11.600]   - Yeah, yeah.
[01:22:11.600 --> 01:22:15.120]   - There is a person behind all of these things.
[01:22:15.120 --> 01:22:17.360]   - And that's the cool thing about podcasting,
[01:22:17.360 --> 01:22:19.320]   like three, four hour conversations
[01:22:19.320 --> 01:22:24.120]   that it's very different than a clickbait journalism.
[01:22:24.120 --> 01:22:26.720]   It's like the opposite, that there's a hunger for that.
[01:22:26.720 --> 01:22:28.080]   There's a willingness for that.
[01:22:28.080 --> 01:22:29.480]   - Yeah, especially now, I mean,
[01:22:29.480 --> 01:22:31.720]   how many people do you even see face to face anymore?
[01:22:31.720 --> 01:22:33.320]   - Right. - Right, like this, you know?
[01:22:33.320 --> 01:22:36.080]   It's like not that many people, like in my day to day,
[01:22:36.080 --> 01:22:39.240]   aside from my own family, that like I sit across.
[01:22:39.240 --> 01:22:41.520]   - It's sad, but it's also beautiful.
[01:22:41.520 --> 01:22:43.280]   Like I've gotten the chance to,
[01:22:43.280 --> 01:22:47.120]   like our conversation now, there's somebody,
[01:22:47.120 --> 01:22:50.040]   I guarantee you there's somebody in Russia
[01:22:50.040 --> 01:22:52.280]   listening to this now like jogging.
[01:22:52.280 --> 01:22:55.320]   There's somebody who is just smoked some weed,
[01:22:55.320 --> 01:22:58.480]   sit back on a couch and just like enjoying.
[01:22:58.480 --> 01:23:00.760]   I guarantee you that we'll write in the comments right now
[01:23:00.760 --> 01:23:05.040]   that yes, I'm in St. Petersburg, I'm in Moscow, I'm whatever.
[01:23:05.040 --> 01:23:10.040]   And we're in their head and they have a friendship with us.
[01:23:10.040 --> 01:23:13.320]   I'm the same way, I'm a huge fan of podcasting.
[01:23:13.320 --> 01:23:15.600]   It's a beautiful thing.
[01:23:15.600 --> 01:23:18.200]   I mean, it's a weird one way human connection.
[01:23:18.200 --> 01:23:20.360]   Like before I went on Joe Rogan,
[01:23:20.360 --> 01:23:24.160]   and still I'm just a huge fan of his.
[01:23:24.160 --> 01:23:25.800]   So it was like surreal.
[01:23:25.800 --> 01:23:29.000]   I've been a friend with Joe Rogan for 10 years, but one way.
[01:23:29.000 --> 01:23:31.760]   - Yeah, from this way, from the St. Petersburg way.
[01:23:31.760 --> 01:23:32.960]   - Yeah, the St. Petersburg way.
[01:23:32.960 --> 01:23:35.000]   And it's a real friendship.
[01:23:35.000 --> 01:23:38.840]   I mean, now it's like two way, but it's still surreal.
[01:23:38.840 --> 01:23:40.520]   And that's the magic of podcasting.
[01:23:40.520 --> 01:23:42.080]   I'm not sure what to make of it.
[01:23:42.080 --> 01:23:45.360]   That voice, it's not even the video part.
[01:23:45.360 --> 01:23:48.560]   It's the audio that's magical.
[01:23:48.560 --> 01:23:50.200]   I don't know what to do with it,
[01:23:50.200 --> 01:23:53.080]   but it's people listen to three, four hours.
[01:23:53.080 --> 01:23:57.440]   - Yeah, we evolved over millions of years, right?
[01:23:57.440 --> 01:24:00.480]   To be very fine tuned to things like that, right?
[01:24:00.480 --> 01:24:02.520]   Oh, expressions as well, of course, right?
[01:24:02.520 --> 01:24:06.960]   But back in the day on the Savannah,
[01:24:06.960 --> 01:24:09.360]   you had to be very attuned to whether
[01:24:09.360 --> 01:24:11.840]   you had a good relationship with the rest of your tribe
[01:24:11.840 --> 01:24:13.440]   or a very bad relationship, right?
[01:24:13.440 --> 01:24:15.160]   Because if you had a very bad relationship,
[01:24:15.160 --> 01:24:17.400]   you were probably gonna be left behind
[01:24:17.400 --> 01:24:18.840]   and eaten by the lions.
[01:24:18.840 --> 01:24:22.600]   - Yeah, but it's weird that the tribe is different now.
[01:24:22.600 --> 01:24:26.040]   Like you could have a one way connection with Joe Rogan
[01:24:26.040 --> 01:24:30.560]   as opposed to the tribe of your physical vicinity.
[01:24:30.560 --> 01:24:33.360]   - But that's why it works with the podcasting,
[01:24:33.360 --> 01:24:35.960]   but it's the opposite of what happens on Twitter, right?
[01:24:35.960 --> 01:24:38.080]   Because all those nuances are removed, right?
[01:24:38.080 --> 01:24:40.760]   You're not connecting with the person
[01:24:40.760 --> 01:24:42.320]   'cause you don't hear the voice.
[01:24:42.320 --> 01:24:44.400]   You're connecting with like an abstraction, right?
[01:24:44.400 --> 01:24:48.400]   It's like some stream of tweets, right?
[01:24:48.400 --> 01:24:52.560]   And it's very easy to assign to them
[01:24:52.560 --> 01:24:55.040]   any kind of like evil intent, you know,
[01:24:55.040 --> 01:24:58.120]   or dehumanize them, which is much harder to do
[01:24:58.120 --> 01:24:59.160]   when it's a real voice, right?
[01:24:59.160 --> 01:25:02.720]   Because you realize it's a real person behind the voice.
[01:25:02.720 --> 01:25:05.000]   - Let me try this out on you.
[01:25:05.000 --> 01:25:07.160]   I sometimes ask about the meaning of life.
[01:25:07.160 --> 01:25:12.160]   Do you, your father now, an engineer,
[01:25:12.160 --> 01:25:14.040]   you're building up a company.
[01:25:14.040 --> 01:25:16.840]   Do you ever zoom out and think like,
[01:25:16.840 --> 01:25:19.360]   what the hell is this whole thing for?
[01:25:19.360 --> 01:25:23.960]   Like why are we descended to vapes even on this planet?
[01:25:23.960 --> 01:25:26.120]   What's the meaning of it all?
[01:25:26.120 --> 01:25:28.120]   - That's a pretty big question.
[01:25:28.120 --> 01:25:32.240]   I think I don't allow myself to think about it too often,
[01:25:32.240 --> 01:25:34.320]   or maybe like life doesn't allow me
[01:25:34.320 --> 01:25:35.800]   to think about it too often.
[01:25:35.800 --> 01:25:39.080]   But in some ways, I guess, the meaning of life
[01:25:39.080 --> 01:25:44.040]   is kind of contributing to this kind of weird thing
[01:25:44.040 --> 01:25:45.320]   we call humanity, right?
[01:25:45.320 --> 01:25:47.640]   Like it's in a way, you can think of humanity
[01:25:47.640 --> 01:25:50.240]   as like a living and evolving organism, right?
[01:25:50.240 --> 01:25:52.520]   That like we all contribute in a slight way,
[01:25:52.520 --> 01:25:55.640]   but just by existing, by having our own unique set
[01:25:55.640 --> 01:25:57.320]   of desires and drives, right?
[01:25:57.320 --> 01:26:01.640]   And maybe that means like creating something great,
[01:26:01.640 --> 01:26:04.640]   and it's bringing up kids who, you know,
[01:26:04.640 --> 01:26:07.800]   are unique and different and seeing like, you know,
[01:26:07.800 --> 01:26:09.720]   they can join what they do.
[01:26:09.720 --> 01:26:11.040]   But I mean, to me, that's pretty much it.
[01:26:11.040 --> 01:26:13.200]   I mean, if you're not a religious person, right,
[01:26:13.200 --> 01:26:16.440]   which I guess I'm not, that's the meaning of life.
[01:26:16.440 --> 01:26:20.920]   It's in the living and in the creation.
[01:26:20.920 --> 01:26:22.440]   - Yeah, there's something magical
[01:26:22.440 --> 01:26:24.200]   about that engine of creation.
[01:26:24.200 --> 01:26:27.280]   Like you said, programming, I would say,
[01:26:27.280 --> 01:26:29.420]   I mean, it's even just actually what you said
[01:26:29.420 --> 01:26:31.000]   with even just programs, I don't care
[01:26:31.000 --> 01:26:32.800]   if it's like some JavaScript thing,
[01:26:32.800 --> 01:26:37.240]   a button on the website, it's like magical
[01:26:37.240 --> 01:26:39.040]   that you brought that to life.
[01:26:39.040 --> 01:26:41.580]   I don't know what that is in there, but that seems,
[01:26:41.580 --> 01:26:46.580]   that's probably some version of like reproduction
[01:26:46.580 --> 01:26:49.800]   and sex, whatever that's in evolution.
[01:26:49.800 --> 01:26:54.800]   But like creating that HTML button has echoes
[01:26:54.800 --> 01:26:57.800]   of that feeling and it's magical.
[01:26:57.800 --> 01:27:00.760]   - Right, well, I mean, if you're a religious person,
[01:27:00.760 --> 01:27:01.800]   maybe you could even say, right,
[01:27:01.800 --> 01:27:04.440]   like we were created in God's image, right?
[01:27:04.440 --> 01:27:07.240]   Well, I mean, I guess part of that is the drive
[01:27:07.240 --> 01:27:09.200]   to create something ourselves, right?
[01:27:09.200 --> 01:27:11.760]   I mean, that's part of it.
[01:27:11.760 --> 01:27:14.840]   - Yeah, that HTML button is the creation in God's image.
[01:27:14.840 --> 01:27:18.880]   - So maybe hopefully it'll be something a little more--
[01:27:18.880 --> 01:27:20.960]   - So dynamic, maybe some JavaScript.
[01:27:20.960 --> 01:27:25.400]   - Yeah, maybe some JavaScript, some React and so on.
[01:27:25.400 --> 01:27:29.400]   But no, I mean, I think that's what differentiates us
[01:27:29.400 --> 01:27:32.140]   from the apes, so to speak.
[01:27:32.140 --> 01:27:34.220]   - Yeah, we did a pretty good job.
[01:27:34.220 --> 01:27:36.960]   Dan, it was an honor to talk to you.
[01:27:36.960 --> 01:27:38.760]   Thank you so much for being part of creating
[01:27:38.760 --> 01:27:42.000]   one of my favorite services and products.
[01:27:42.000 --> 01:27:45.080]   This is actually a little bit of an experiment,
[01:27:45.080 --> 01:27:48.520]   allowing me to sort of fanboy over some of the things
[01:27:48.520 --> 01:27:52.280]   I love, so thanks for wasting your time with me today.
[01:27:52.280 --> 01:27:53.120]   It was really fun. - It was awesome.
[01:27:53.120 --> 01:27:55.520]   Thanks for having me on and giving me a chance
[01:27:55.520 --> 01:27:57.200]   to try this out. (laughs)
[01:27:57.200 --> 01:27:58.020]   - Awesome.
[01:27:58.020 --> 01:28:00.720]   Thanks for listening to this conversation
[01:28:00.720 --> 01:28:03.560]   with Dan Kokorov and thank you to our sponsors,
[01:28:03.560 --> 01:28:06.200]   Athletic Greens, All-in-One Nutrition Drink,
[01:28:06.200 --> 01:28:09.080]   Blinkist app that summarizes books,
[01:28:09.080 --> 01:28:13.020]   Business Wars podcast, and Cash App.
[01:28:13.020 --> 01:28:16.800]   So the choice is health, wisdom, or money.
[01:28:16.800 --> 01:28:19.340]   Choose wisely, my friends, and if you wish,
[01:28:19.340 --> 01:28:23.000]   click the sponsor links below to get a discount
[01:28:23.000 --> 01:28:25.280]   and to support this podcast.
[01:28:25.280 --> 01:28:27.320]   And now let me leave you with some words
[01:28:27.320 --> 01:28:29.800]   from Ludwig Wittgenstein.
[01:28:29.800 --> 01:28:33.840]   "The limits of my language means the limits of my world."
[01:28:33.840 --> 01:28:37.700]   Thank you for listening and hope to see you next time.
[01:28:37.700 --> 01:28:40.280]   (upbeat music)
[01:28:40.280 --> 01:28:42.860]   (upbeat music)
[01:28:42.860 --> 01:28:52.860]   [BLANK_AUDIO]

