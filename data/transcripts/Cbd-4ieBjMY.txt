
[00:00:00.000 --> 00:00:09.920]   Okay, we should be live on YouTube now and hey everybody, welcome back to another paper
[00:00:09.920 --> 00:00:14.160]   reading group and this is our second beginner friendly paper reading group.
[00:00:14.160 --> 00:00:20.800]   So in our last paper reading group, we looked at ResNet and this time we're looking at DenseNet.
[00:00:20.800 --> 00:00:24.800]   So I think it's a good progression from ResNet onward.
[00:00:24.800 --> 00:00:30.640]   So we're going to look at DenseNet this week and let me just quickly share my screen.
[00:00:30.640 --> 00:00:32.840]   So I guess I'm just sharing PowerPoint right now.
[00:00:32.840 --> 00:00:35.280]   Please let me know in case you can't see.
[00:00:35.280 --> 00:00:36.600]   I'll just switch to screen now.
[00:00:36.600 --> 00:00:41.040]   Please let me know if there's any issues or you can't see my screen.
[00:00:41.040 --> 00:00:44.840]   Just close this off.
[00:00:44.840 --> 00:00:45.840]   You don't need that.
[00:00:45.840 --> 00:00:48.400]   You don't need that.
[00:00:48.400 --> 00:00:49.520]   Okay.
[00:00:49.520 --> 00:00:55.920]   So just as usual, again, this week, because we're live on YouTube this time and I don't
[00:00:55.920 --> 00:01:02.720]   usually monitor the YouTube chat or the Zoom chat.
[00:01:02.720 --> 00:01:11.920]   So if we go to 1db.me/densenet, I'll just paste that in, you know, Zoom as well.
[00:01:11.920 --> 00:01:16.540]   So if I just go 1db.me/densenet, let's go over.
[00:01:16.540 --> 00:01:17.540]   That brings me to the forum.
[00:01:17.540 --> 00:01:19.900]   So that's this forum here.
[00:01:19.900 --> 00:01:22.240]   And as you can see, this is the paper link.
[00:01:22.240 --> 00:01:23.460]   This is the blog post.
[00:01:23.460 --> 00:01:27.460]   So I wrote a nice blog post about DenseNet quite some time ago.
[00:01:27.460 --> 00:01:30.340]   And that's actually in August, on August 2, 2020.
[00:01:30.340 --> 00:01:33.580]   That's quite some time ago, more than a year.
[00:01:33.580 --> 00:01:36.300]   And it's going to come in very handy today.
[00:01:36.300 --> 00:01:42.380]   But if you're looking for a nice introduction, I guess everything I do share today, if you
[00:01:42.380 --> 00:01:47.300]   do go through this blog post, it will just make understanding DenseNet a whole lot easier.
[00:01:47.300 --> 00:01:52.180]   And you'll find in this blog post that there's also the code in PyTorch, which is the Torch
[00:01:52.180 --> 00:01:54.320]   Vision implementation of DenseNet.
[00:01:54.320 --> 00:01:57.380]   So that's been explained very nicely in this blog post as well.
[00:01:57.380 --> 00:02:03.140]   But of course, as part of Wix and Myosys, next week, we'll be looking at DenseNet live
[00:02:03.140 --> 00:02:04.140]   coding.
[00:02:04.140 --> 00:02:09.700]   So last week, we did ResNet live coding from scratch, which was basically creating the
[00:02:09.700 --> 00:02:15.740]   ResNet architecture in PyTorch in scratch.
[00:02:15.740 --> 00:02:21.180]   And I saw there was a nice comment from Vinayak.
[00:02:21.180 --> 00:02:23.180]   So let me just see if I can find it.
[00:02:23.180 --> 00:02:25.180]   Give me one sec.
[00:02:25.180 --> 00:02:28.180]   Oh yeah, there it is.
[00:02:28.180 --> 00:02:33.420]   So Vinayak is someone I know from Fastbook as well.
[00:02:33.420 --> 00:02:36.460]   And he's implemented the bottleneck blocks.
[00:02:36.460 --> 00:02:41.180]   Last week, we looked at the custom ResNet implementation and we looked at the bottleneck
[00:02:41.180 --> 00:02:42.180]   block.
[00:02:42.180 --> 00:02:44.580]   We didn't look at the bottleneck block, but now it's on the forums.
[00:02:44.580 --> 00:02:47.380]   So it's on the Wix and Myosys forums.
[00:02:47.380 --> 00:02:51.840]   Anybody wants to have a look at this problem, he's posted a problem.
[00:02:51.840 --> 00:02:53.340]   So feel free to go in there.
[00:02:53.340 --> 00:02:55.100]   But that's not what we're looking at today.
[00:02:55.100 --> 00:02:56.460]   We're going into DenseNet.
[00:02:56.460 --> 00:03:00.700]   Let me see if I can add a tag.
[00:03:00.700 --> 00:03:03.460]   Oh, I can't edit this.
[00:03:03.460 --> 00:03:04.460]   Interesting.
[00:03:04.460 --> 00:03:07.620]   Nevermind for now, I won't add the tag.
[00:03:07.620 --> 00:03:09.100]   I'll add that later.
[00:03:09.100 --> 00:03:12.540]   But then again, we will start with DenseNet today.
[00:03:12.540 --> 00:03:14.180]   And I want to do things a bit differently.
[00:03:14.180 --> 00:03:18.660]   Like instead of just reading through the paper, I will try and explain the DenseNet architecture
[00:03:18.660 --> 00:03:20.780]   first in OneNote.
[00:03:20.780 --> 00:03:25.900]   And then when we get to the paper, things will be really easy, hopefully, and should
[00:03:25.900 --> 00:03:28.700]   be an easy breeze from there on to read the paper.
[00:03:28.700 --> 00:03:29.700]   So let's get started.
[00:03:29.700 --> 00:03:34.740]   As I said, this is the DenseNet paper.
[00:03:34.740 --> 00:03:37.260]   This is not something we're not going to start and look from here.
[00:03:37.260 --> 00:03:42.180]   But instead, DenseNet, I'm just going to call this DenseNet 2.
[00:03:42.180 --> 00:03:43.700]   This is where I'm going to scribble.
[00:03:43.700 --> 00:03:48.700]   So everybody's really quiet today.
[00:03:48.700 --> 00:03:52.340]   But if you could please post in here that you can hear me well and you can see my screen
[00:03:52.340 --> 00:03:56.020]   well, so I'll be at peace or just post in the Zoom chat.
[00:03:56.020 --> 00:04:01.460]   I'll be at peace and feel very confident that you guys can hear me well.
[00:04:01.460 --> 00:04:10.260]   So that being said, again, so last week, we were looking at when we looked at ResNet,
[00:04:10.260 --> 00:04:16.340]   we saw so the one thing we saw with ResNet is like if you have actually, that's not how
[00:04:16.340 --> 00:04:17.340]   I should draw it.
[00:04:17.340 --> 00:04:21.940]   Let's say if I have some input, right, that's my input.
[00:04:21.940 --> 00:04:29.420]   And we saw in ResNet, what happens is this input at this point goes like this and you
[00:04:29.420 --> 00:04:36.300]   have and this is again, I'm just this is I'll look at that.
[00:04:36.300 --> 00:04:41.380]   That's very tiny bit bigger.
[00:04:41.380 --> 00:04:42.380]   This is all ResNet.
[00:04:42.380 --> 00:04:43.380]   Okay.
[00:04:43.380 --> 00:04:47.820]   We aren't looking at we are not looking at DenseNet so far.
[00:04:47.820 --> 00:04:54.220]   So we saw this like a residual path, residual path.
[00:04:54.220 --> 00:04:58.500]   And there's the skip connection and these two things get added.
[00:04:58.500 --> 00:05:04.620]   So if my input is X and this is called as F, let's say I denote that by F, then what's
[00:05:04.620 --> 00:05:05.880]   the output going to be here?
[00:05:05.880 --> 00:05:08.140]   It's called as F of X.
[00:05:08.140 --> 00:05:11.460]   Basically when you pass X residual path, that's the output.
[00:05:11.460 --> 00:05:13.180]   And then this is again, this is just X.
[00:05:13.180 --> 00:05:15.060]   There's no change in the input.
[00:05:15.060 --> 00:05:20.260]   So your final output in a ResNet block becomes F of X plus X.
[00:05:20.260 --> 00:05:25.460]   That's just saying that that's just a fancy way of saying that my output is two things.
[00:05:25.460 --> 00:05:29.780]   One the output of residual path and added to the input.
[00:05:29.780 --> 00:05:34.900]   So we're just basically adding the input to the output.
[00:05:34.900 --> 00:05:36.820]   So let me just give you an example again.
[00:05:36.820 --> 00:05:41.060]   I'll go to my really nice written blog post from before.
[00:05:41.060 --> 00:05:43.180]   Oh, I already have it here.
[00:05:43.180 --> 00:05:47.060]   And I just want to give you an example on how this is different.
[00:05:47.060 --> 00:05:50.900]   Where's that VGG example?
[00:05:50.900 --> 00:05:51.900]   Okay.
[00:05:51.900 --> 00:05:57.780]   So there was this VGG architecture that you can see here.
[00:05:57.780 --> 00:06:03.540]   And what happens is in this network, there's only a forward pass, but in ResNet, there's
[00:06:03.540 --> 00:06:04.820]   these skip connections.
[00:06:04.820 --> 00:06:07.740]   So let me copy paste this.
[00:06:07.740 --> 00:06:09.140]   And I just want to show you the difference.
[00:06:09.140 --> 00:06:13.260]   So everything's clear first about ResNet.
[00:06:13.260 --> 00:06:16.500]   Sorry, just give me one sec.
[00:06:16.500 --> 00:06:22.980]   I think that speakers, headphones, speakers, that should be fine.
[00:06:22.980 --> 00:06:25.500]   I always say that sound.
[00:06:25.500 --> 00:06:30.140]   Anyway, so in VGG, if you have an input image, it just goes all the way through.
[00:06:30.140 --> 00:06:31.500]   There's no skip connections.
[00:06:31.500 --> 00:06:34.380]   But what ResNet does is it adds the inputs after each box.
[00:06:34.380 --> 00:06:36.860]   It adds the inputs, adds the inputs.
[00:06:36.860 --> 00:06:40.500]   Basically like you're adding, if there's some output at this point, then you're adding that
[00:06:40.500 --> 00:06:42.620]   to the blocks output.
[00:06:42.620 --> 00:06:44.460]   And then you're doing the same thing over and over again.
[00:06:44.460 --> 00:06:47.860]   And that's the difference in ResNet and VGG.
[00:06:47.860 --> 00:06:52.180]   So let me just quickly remove that.
[00:06:52.180 --> 00:06:53.980]   We don't need that anymore.
[00:06:53.980 --> 00:06:57.140]   And now, what did the DenseNet people do?
[00:06:57.140 --> 00:07:02.220]   So DenseNet, again, is this architecture by Hong et al.
[00:07:02.220 --> 00:07:05.140]   And they're super amazing researchers.
[00:07:05.140 --> 00:07:11.020]   And they thought, well, I guess instead of just having like-- overall, if you think,
[00:07:11.020 --> 00:07:16.220]   in ResNet, you have basically lots of convolution blocks.
[00:07:16.220 --> 00:07:17.900]   These lines are just blocks.
[00:07:17.900 --> 00:07:20.500]   And you just have skip connections.
[00:07:20.500 --> 00:07:25.100]   This is how this ResNet architecture is in a way.
[00:07:25.100 --> 00:07:27.940]   You just have skip connections.
[00:07:27.940 --> 00:07:29.300]   That's just this.
[00:07:29.300 --> 00:07:34.380]   And what the DenseNet people thought would be really amazing or what we could do is instead
[00:07:34.380 --> 00:07:39.260]   of just having like these-- instead of just having like the previous layer, like just
[00:07:39.260 --> 00:07:43.140]   the input being added to the output of the layer, you have something like this.
[00:07:43.140 --> 00:07:44.140]   And what is that?
[00:07:44.140 --> 00:07:49.140]   That's something like you have your input.
[00:07:49.140 --> 00:07:51.620]   Then you have your stem, the usual stem.
[00:07:51.620 --> 00:07:55.700]   So stem is just something that will-- because if the input image is too big, the stem is
[00:07:55.700 --> 00:08:00.500]   just something that will perform the initial convolution, value batch norm, and then just
[00:08:00.500 --> 00:08:01.900]   reduce the size of the input.
[00:08:01.900 --> 00:08:07.340]   So if this is, say, 2 to 4 by 2 to 4, stem would make something to be like-- if it's
[00:08:07.340 --> 00:08:12.780]   3-channel, 64 by 2 to 4 by-- basically 3-channel, 2 to 4 by 2 to 4, it makes something like
[00:08:12.780 --> 00:08:13.780]   64 by 56 by 56.
[00:08:13.780 --> 00:08:18.860]   So it just makes-- stem is just like common in all architectures.
[00:08:18.860 --> 00:08:24.260]   We saw a stem was there in ResNet as well, which was a con 7 by 7.
[00:08:24.260 --> 00:08:26.740]   But anyway, we have a stem.
[00:08:26.740 --> 00:08:33.420]   And then what you have is you have these convolution blocks.
[00:08:33.420 --> 00:08:36.020]   And in ResNet, you're just pretty much adding the outputs.
[00:08:36.020 --> 00:08:37.420]   That's what you're doing.
[00:08:37.420 --> 00:08:42.460]   But in DenseNet, what you're doing is you're adding-- for a specific layer, let's say this
[00:08:42.460 --> 00:08:47.260]   one, the inputs are the outputs from all the previous layers.
[00:08:47.260 --> 00:08:50.900]   So if this is like-- for this layer, what are the previous layers?
[00:08:50.900 --> 00:08:52.780]   It's this one and this one.
[00:08:52.780 --> 00:08:57.580]   So the input to this layer is the output of this layer and the output from the previous
[00:08:57.580 --> 00:08:58.580]   layer.
[00:08:58.580 --> 00:09:03.500]   And then similarly, the output of this layer goes to both layers.
[00:09:03.500 --> 00:09:07.620]   Because for this block-- that's bad.
[00:09:07.620 --> 00:09:09.980]   Because for this block, what are the inputs going to be?
[00:09:09.980 --> 00:09:11.260]   They're going to come from all the layers.
[00:09:11.260 --> 00:09:15.420]   It's going to come from the first one, the second one, third one, and then finally the
[00:09:15.420 --> 00:09:16.420]   fourth one.
[00:09:16.420 --> 00:09:19.260]   So basically, there's four inputs coming instead of one.
[00:09:19.260 --> 00:09:20.860]   Because in ResNet, what did you have?
[00:09:20.860 --> 00:09:25.660]   You had-- if you have something like this, in ResNet, it's pretty much just going like
[00:09:25.660 --> 00:09:26.700]   a forward pass.
[00:09:26.700 --> 00:09:28.580]   So you have some output.
[00:09:28.580 --> 00:09:29.580]   You add the input.
[00:09:29.580 --> 00:09:30.580]   You add them.
[00:09:30.580 --> 00:09:33.220]   Then this goes to the next block.
[00:09:33.220 --> 00:09:34.220]   Then you add the two together.
[00:09:34.220 --> 00:09:35.820]   Then this goes to the next block.
[00:09:35.820 --> 00:09:37.060]   You add the two together.
[00:09:37.060 --> 00:09:39.060]   You go pretty much just like that.
[00:09:39.060 --> 00:09:41.260]   But in DenseNet, this is the main difference.
[00:09:41.260 --> 00:09:45.420]   And this is why these are called densely connected neural networks.
[00:09:45.420 --> 00:09:49.140]   Because you can see how the connections are really, really dense.
[00:09:49.140 --> 00:09:56.100]   Because in a way, for-- basically, there's this-- a bit of math is in the paper is where
[00:09:56.100 --> 00:10:00.940]   they say, whereas traditional convolutional networks with L layers have L connections,
[00:10:00.940 --> 00:10:06.060]   because there's just going to be one connection between each layer and its next layer, our
[00:10:06.060 --> 00:10:09.420]   network has L, L plus 1 by 2 direct connections.
[00:10:09.420 --> 00:10:12.900]   So that's just basically a bit of arithmetic.
[00:10:12.900 --> 00:10:16.420]   And you can easily find out why there's that many connections.
[00:10:16.420 --> 00:10:21.740]   It's not important to know that there's L, L plus 1 by 2 direct connections.
[00:10:21.740 --> 00:10:28.380]   But what's important to know about DenseNet is that for each layer, basically, you add
[00:10:28.380 --> 00:10:31.460]   all the inputs from the previous layers.
[00:10:31.460 --> 00:10:37.380]   And that's just the main important difference in DenseNet.
[00:10:37.380 --> 00:10:44.540]   So this figure here is the main thing.
[00:10:44.540 --> 00:10:52.220]   Basically, you have-- so this is how it looks like.
[00:10:52.220 --> 00:10:54.540]   Basically, you have your input.
[00:10:54.540 --> 00:10:56.740]   Then that input is called x0.
[00:10:56.740 --> 00:11:00.460]   When it goes to the second block-- so let's say this is the 0th block.
[00:11:00.460 --> 00:11:04.780]   This is the first one, second, third block, fourth block, and so on.
[00:11:04.780 --> 00:11:09.300]   So when the 0th block goes to the first one, not only are the inputs-- like, this is--
[00:11:09.300 --> 00:11:12.140]   the first block will just get the inputs from the 0th block.
[00:11:12.140 --> 00:11:13.700]   But what about the second one?
[00:11:13.700 --> 00:11:18.900]   It's going to get input from this block and also from this block, as you can see.
[00:11:18.900 --> 00:11:24.060]   So this thing is getting two inputs, one from the previous block and then from the block
[00:11:24.060 --> 00:11:25.060]   before.
[00:11:25.060 --> 00:11:29.860]   And similarly, this block is going to get three inputs, one from the previous block,
[00:11:29.860 --> 00:11:33.140]   one from the block before, and then one from this block.
[00:11:33.140 --> 00:11:34.900]   So it's like three inputs over here.
[00:11:34.900 --> 00:11:35.900]   And same thing over here.
[00:11:35.900 --> 00:11:39.620]   This is like-- it's going to get four inputs.
[00:11:39.620 --> 00:11:45.460]   So this is the main, main, main, main point of NSTACK.
[00:11:45.460 --> 00:11:49.940]   And if there's any questions on this-- because now we're just going to go and dig deeper
[00:11:49.940 --> 00:11:50.940]   into the details.
[00:11:50.940 --> 00:11:53.980]   And we're just going to look at, OK, how do these connections work?
[00:11:53.980 --> 00:11:54.980]   Why are there no errors?
[00:11:54.980 --> 00:11:57.220]   And why does this network still train very well?
[00:11:57.220 --> 00:12:01.180]   So we're just going to go into the details from this point onwards.
[00:12:01.180 --> 00:12:06.020]   But if there's any points, please let me know.
[00:12:06.020 --> 00:12:11.300]   Hi, I have a question here.
[00:12:11.300 --> 00:12:13.220]   Sure, shoot.
[00:12:13.220 --> 00:12:19.380]   So say you have like 50 convolutional blocks.
[00:12:19.380 --> 00:12:24.980]   And in a dense net, as you're saying, you will get the connection-- the 50th convolutional
[00:12:24.980 --> 00:12:28.740]   block will get the connection from the first convolutional block.
[00:12:28.740 --> 00:12:33.300]   I can imagine that not being very useful.
[00:12:33.300 --> 00:12:39.140]   So has someone looked into how many layers you should go back?
[00:12:39.140 --> 00:12:47.700]   What's the optimum number that you can go back to get the inputs from?
[00:12:47.700 --> 00:12:48.700]   That's a good point.
[00:12:48.700 --> 00:12:52.820]   I think I see your point when you're saying in the 50th block, if you have the first block,
[00:12:52.820 --> 00:12:56.860]   which is the really early layers.
[00:12:56.860 --> 00:13:01.340]   But this architecture, this network architecture, just says the complete opposite to what you're
[00:13:01.340 --> 00:13:02.340]   mentioning.
[00:13:02.340 --> 00:13:06.500]   So the situation in this architecture is that it is useful to use the first block.
[00:13:06.500 --> 00:13:10.820]   So now, I guess what you're trying to say is, in a way, you're trying to disagree with
[00:13:10.820 --> 00:13:11.820]   the researchers.
[00:13:11.820 --> 00:13:14.980]   Or not quite, but then this is, again, a new line of research that I don't know of.
[00:13:14.980 --> 00:13:20.180]   I don't think-- at least that I don't know of is like, there's no such hybrid architecture
[00:13:20.180 --> 00:13:27.340]   that looks like dense net where the connections go only like 10 blocks before, 15 blocks before.
[00:13:27.340 --> 00:13:31.260]   So if there is any, I don't know about it.
[00:13:31.260 --> 00:13:36.140]   But again, the dense net architecture just makes the completely opposite point where
[00:13:36.140 --> 00:13:40.860]   they say that it's actually very helpful to have all of these connections.
[00:13:40.860 --> 00:13:49.580]   But again, the way the dense net architecture has been designed is different to this image.
[00:13:49.580 --> 00:13:56.060]   So in this image, what you're thinking is like, OK, if you have this 50th block, then
[00:13:56.060 --> 00:13:57.420]   all of these get added.
[00:13:57.420 --> 00:14:02.500]   But when you look at the actual implementation of dense net, you'll see that there's some
[00:14:02.500 --> 00:14:03.940]   slight differences.
[00:14:03.940 --> 00:14:08.580]   And those slight differences actually make a big difference.
[00:14:08.580 --> 00:14:15.900]   So as we go deeper, I hope that this thing will become clearer on how dense net is still
[00:14:15.900 --> 00:14:16.900]   a good architecture.
[00:14:16.900 --> 00:14:17.900]   I see.
[00:14:17.900 --> 00:14:18.900]   Thanks.
[00:14:18.900 --> 00:14:19.900]   No worries.
[00:14:19.900 --> 00:14:24.980]   Actually, I would like to add one thought in it.
[00:14:24.980 --> 00:14:29.780]   Again, from my understanding of this paper, it doesn't connect 50 blocks.
[00:14:29.780 --> 00:14:31.980]   It is only within the dense block.
[00:14:31.980 --> 00:14:36.740]   So there is transition blocks right after these dense blocks, because you cannot have
[00:14:36.740 --> 00:14:39.020]   50 blocks with same feature map.
[00:14:39.020 --> 00:14:40.020]   That will kill your memory.
[00:14:40.020 --> 00:14:41.020]   Ramesh, we're jumping forward.
[00:14:41.020 --> 00:14:42.020]   That's something I was going to come to.
[00:14:42.020 --> 00:14:48.940]   I think that's a great point, but I was going to come to that slowly.
[00:14:48.940 --> 00:14:52.820]   So let's not go down that road just yet.
[00:14:52.820 --> 00:14:53.820]   Hello?
[00:14:53.820 --> 00:14:54.820]   Yeah, that makes sense.
[00:14:54.820 --> 00:15:00.740]   I was just trying to say that you don't have to worry about 50 blocks.
[00:15:00.740 --> 00:15:02.780]   Nobody does 50 blocks of dense net.
[00:15:02.780 --> 00:15:03.780]   Absolutely.
[00:15:03.780 --> 00:15:09.380]   So I guess that was just the next point, but now that Ramesh has mentioned it, the way
[00:15:09.380 --> 00:15:15.860]   this dense architecture, but actually before we go to that point, something I do want each
[00:15:15.860 --> 00:15:21.980]   and every one of us to think and maybe just come to the forums and point an answer, think
[00:15:21.980 --> 00:15:26.300]   of an answer, at least in your head and just spend like 30 seconds thinking about this
[00:15:26.300 --> 00:15:31.580]   is like my input is going to be something like, usually it's like one by three by two
[00:15:31.580 --> 00:15:33.140]   to four by two to four.
[00:15:33.140 --> 00:15:34.140]   What does that mean?
[00:15:34.140 --> 00:15:37.940]   Two to four high, two to four width, three channel and single image.
[00:15:37.940 --> 00:15:43.180]   And then by the time you go down and down, you end up coming to a point, for example,
[00:15:43.180 --> 00:15:45.220]   where this becomes the last output.
[00:15:45.220 --> 00:15:55.220]   Because if you see VGG, which is here, as you can see, when you start with two to four,
[00:15:55.220 --> 00:15:58.900]   when you start with two to four by two to four, and you keep going down and down, you
[00:15:58.900 --> 00:16:03.060]   end up reaching a point where your feature map size goes down to seven by seven.
[00:16:03.060 --> 00:16:04.900]   But the number of channels goes up.
[00:16:04.900 --> 00:16:07.500]   The number of channels goes up to 512.
[00:16:07.500 --> 00:16:14.020]   So now the question is, I guess, how do you make sure that when you get to like from when
[00:16:14.020 --> 00:16:20.060]   you get to that point from like three by two to four by two to four, and you get to this
[00:16:20.060 --> 00:16:27.020]   point of like 512 by seven by seven.
[00:16:27.020 --> 00:16:30.500]   So like, how do you make sure that these two can be added to each other?
[00:16:30.500 --> 00:16:31.900]   Because that's what they're doing, right?
[00:16:31.900 --> 00:16:33.780]   That's like, this is the connection.
[00:16:33.780 --> 00:16:37.580]   And that connection basically just means like adding or concatenation or like some kind
[00:16:37.580 --> 00:16:39.580]   of joining of the features.
[00:16:39.580 --> 00:16:43.460]   So I guess the question that I want every one of us to think and spend 30 seconds on
[00:16:43.460 --> 00:16:46.180]   is like, how is this even possible?
[00:16:46.180 --> 00:16:51.580]   Like, how do you join this with 512 by seven by seven?
[00:16:51.580 --> 00:16:56.740]   So that's just something I want each one of us to think for a second.
[00:16:56.740 --> 00:17:00.660]   And then I will share the solution.
[00:17:00.660 --> 00:17:06.140]   So then what they actually let me just bring up the let me just bring up what they do differently
[00:17:06.140 --> 00:17:11.780]   to make sure that something like this happens is.
[00:17:11.780 --> 00:17:20.220]   Okay, so what do you then so then if we go through the let me just go back to the paper.
[00:17:20.220 --> 00:17:28.780]   So then what you see, in fact, is and we're going back to what Ramesh was mentioning,
[00:17:28.780 --> 00:17:34.020]   is that the way the dense net architecture has been implemented is slightly different
[00:17:34.020 --> 00:17:39.100]   from how we understand it in that one figure right now.
[00:17:39.100 --> 00:17:44.300]   Simon cited using one by one cons, which is definitely a good point, because we are going
[00:17:44.300 --> 00:17:48.780]   to use one by one cons to make that make sure that the number of channels is the same or
[00:17:48.780 --> 00:17:50.700]   the feature map size is the same.
[00:17:50.700 --> 00:17:54.540]   But the way the researchers do this is using this figure too.
[00:17:54.540 --> 00:18:01.100]   So let me just paste this in one note.
[00:18:01.100 --> 00:18:09.020]   Okay, so sorry, give me one sec.
[00:18:09.020 --> 00:18:14.300]   So then the way the dense net architecture has been implemented is different.
[00:18:14.300 --> 00:18:20.620]   Sorry about that is different from from this image that we saw before.
[00:18:20.620 --> 00:18:24.140]   The actual implementation looks something like this.
[00:18:24.140 --> 00:18:29.580]   You pretty much have in the dense net architecture, you pretty much have something called a dense
[00:18:29.580 --> 00:18:30.780]   block.
[00:18:30.780 --> 00:18:36.540]   And when you start with, say, three by two to four by two to four, and you do your stem,
[00:18:36.540 --> 00:18:41.780]   which is your initial convolution and say you end up at something like 64 by 56 by 56,
[00:18:41.780 --> 00:18:43.300]   because that's what the stem does.
[00:18:43.300 --> 00:18:47.740]   The stem basically increase the number of channels and decreases the spatial dimensions,
[00:18:47.740 --> 00:18:48.740]   right.
[00:18:48.740 --> 00:18:54.580]   And then what happens is inside the dense block, which is this dense block here, inside
[00:18:54.580 --> 00:18:59.500]   the dense block, the size of the feature map in a way remains the same, but like it remains
[00:18:59.500 --> 00:19:01.260]   56 by 56.
[00:19:01.260 --> 00:19:04.660]   But what you do is this is where all the concatenation can then happen, right?
[00:19:04.660 --> 00:19:08.540]   Because now if the feature map size is the same, then you can concatenate features to
[00:19:08.540 --> 00:19:09.540]   one another, right?
[00:19:09.540 --> 00:19:14.420]   So if it's all 56 by 56, and then basically you have some number of channels, then you
[00:19:14.420 --> 00:19:18.240]   can concatenate with one another inside the dense block.
[00:19:18.240 --> 00:19:23.260]   So this is the actual concatenation just happens inside dense blocks.
[00:19:23.260 --> 00:19:28.700]   And this feature map size remains the same, only the number of channels goes up and it
[00:19:28.700 --> 00:19:34.080]   grows exponentially, not exponentially, basically just grows by a factor called growth rate.
[00:19:34.080 --> 00:19:40.060]   So what happens is, let's say we go from dense block one, and now the output is something
[00:19:40.060 --> 00:19:43.620]   like 256 by 56 by 56.
[00:19:43.620 --> 00:19:45.120]   Let's say that's the output.
[00:19:45.120 --> 00:19:50.940]   Then you have this convolution and pooling, which is what is called something like a transition
[00:19:50.940 --> 00:19:55.420]   block, which is exactly something that Ramesh already mentioned as well.
[00:19:55.420 --> 00:19:57.820]   It's like there's this thing called the transition block.
[00:19:57.820 --> 00:20:06.700]   And what transition block does is it does something very similar to this VGT architecture.
[00:20:06.700 --> 00:20:12.020]   Because as you can see, in convolution neural networks, what happens is you start with a
[00:20:12.020 --> 00:20:14.220]   big spatial size.
[00:20:14.220 --> 00:20:18.340]   And as you keep going forward and forward into your network, the number of channels
[00:20:18.340 --> 00:20:22.220]   keeps on going up, but your spatial dimensions keeps on going down.
[00:20:22.220 --> 00:20:26.260]   So basically what that means is when you start by two to four by two to four, which is like
[00:20:26.260 --> 00:20:29.500]   you have a big image, but you only have three channels.
[00:20:29.500 --> 00:20:33.340]   By the time you're towards the end of the network, you end up being like seven by seven
[00:20:33.340 --> 00:20:38.180]   by five, which means you have 512 channels, but your image size has just gone from like
[00:20:38.180 --> 00:20:40.740]   two to four by two to four to seven by seven.
[00:20:40.740 --> 00:20:41.980]   So this is now the feature map.
[00:20:41.980 --> 00:20:47.600]   So the feature map has reduced in spatial dimensions, but it has increased in the number
[00:20:47.600 --> 00:20:48.700]   of channels.
[00:20:48.700 --> 00:20:52.380]   And what this transition block does is it does something very similar.
[00:20:52.380 --> 00:20:58.660]   It's like it will, because it's a convolution and pooling, what it does is it halves my
[00:20:58.660 --> 00:21:00.020]   spatial dimension size.
[00:21:00.020 --> 00:21:06.500]   So what you get from here is something like, I can't remember if it changes the number
[00:21:06.500 --> 00:21:07.500]   of channels.
[00:21:07.500 --> 00:21:12.380]   That's something I'll have to just confirm, but it definitely makes my spatial size half.
[00:21:12.380 --> 00:21:13.900]   So then you have 28 by 28.
[00:21:13.900 --> 00:21:16.980]   Actually, it doesn't change the number of channels as far as I remember.
[00:21:16.980 --> 00:21:19.780]   So it's like 256 by 28 by 28.
[00:21:19.780 --> 00:21:22.940]   And then again, in this block, you just keep going and going onwards.
[00:21:22.940 --> 00:21:26.700]   And then you get to a point where you have like something like 512 channels this time,
[00:21:26.700 --> 00:21:27.980]   28 by 28.
[00:21:27.980 --> 00:21:33.660]   So as you can see, as we are progressing through this dense net architecture, we're also following
[00:21:33.660 --> 00:21:37.580]   this common approach of like, we started with three channels.
[00:21:37.580 --> 00:21:40.600]   We started by two to four by two to four spatial dimensions.
[00:21:40.600 --> 00:21:44.240]   Then our channels got up to 64, spatial dimensions got reduced.
[00:21:44.240 --> 00:21:47.180]   Then this transition block reduced my spatial dimension further.
[00:21:47.180 --> 00:21:49.440]   So it got down to 28 by 28.
[00:21:49.440 --> 00:21:52.880]   And as I keep going through the dense block, my number of channels is going up.
[00:21:52.880 --> 00:21:57.720]   So as I'm going deeper and deeper into the network, instead of having three channels,
[00:21:57.720 --> 00:21:59.280]   I have something like 512 channels.
[00:21:59.280 --> 00:22:03.920]   And instead of having a spatial dimension size of two to four, I have a spatial dimension
[00:22:03.920 --> 00:22:05.240]   size of 28.
[00:22:05.240 --> 00:22:11.840]   And this is a good and important point to keep in mind, that dense net does something
[00:22:11.840 --> 00:22:15.240]   like completely the opposite of what the common norm was.
[00:22:15.240 --> 00:22:20.960]   And remember, again, I think this was a paper written in 2015 or 2016.
[00:22:20.960 --> 00:22:23.560]   This says the first version came in 2016.
[00:22:23.560 --> 00:22:26.840]   So again, that's like five years ago.
[00:22:26.840 --> 00:22:29.640]   And a lot of things have changed in five years.
[00:22:29.640 --> 00:22:33.240]   But again, because this is a beginner friendly paper reading group series, what we want to
[00:22:33.240 --> 00:22:36.820]   do is we want to get the basics right first.
[00:22:36.820 --> 00:22:43.020]   So the main thing to note here, or the main thing to understand here, is the common architecture
[00:22:43.020 --> 00:22:48.820]   design is you increase the number of channels and you decrease the spatial dimensions.
[00:22:48.820 --> 00:22:52.940]   And that's something that's happening inside the dense net architecture as well.
[00:22:52.940 --> 00:22:58.940]   And then another thing that's happening is the dense block, the dense block in itself
[00:22:58.940 --> 00:23:05.460]   is where all the concatenation of the main point in the paper was mentioned is for each
[00:23:05.460 --> 00:23:08.620]   layer you have the outputs from each of the previous layers.
[00:23:08.620 --> 00:23:12.140]   So all of that is happening inside the dense block.
[00:23:12.140 --> 00:23:17.740]   But otherwise, the architecture is like the features go from here, you pretty much get
[00:23:17.740 --> 00:23:21.740]   the output at this point, then you get the output at this point.
[00:23:21.740 --> 00:23:25.620]   So you get the output here, you get the output here, then you get the output here, then you
[00:23:25.620 --> 00:23:27.780]   get the output here, and the output here.
[00:23:27.780 --> 00:23:31.580]   So in a way, this is again a very forward pass.
[00:23:31.580 --> 00:23:36.580]   Things are just being passed from previous layers to next layers one by one.
[00:23:36.580 --> 00:23:40.620]   But inside the dense block, things are a bit different.
[00:23:40.620 --> 00:23:44.860]   Inside the dense block is where all the concatenation of the features, inside the dense block is
[00:23:44.860 --> 00:23:51.620]   where your last layer basically has all the features from the previous layers.
[00:23:51.620 --> 00:23:59.220]   So this is how this is a bit different from figure one, which was this one.
[00:23:59.220 --> 00:24:04.140]   So this figure essentially gets the main idea of the dense net architecture.
[00:24:04.140 --> 00:24:08.420]   But this figure is where the implementation details start to come in.
[00:24:08.420 --> 00:24:13.100]   So if there's any questions, please also keep posting on the forums.
[00:24:13.100 --> 00:24:17.780]   I'm assuming there's no questions so far.
[00:24:17.780 --> 00:24:24.420]   So that's the main implementation thing that we need to be careful of when we look at dense
[00:24:24.420 --> 00:24:25.420]   net.
[00:24:25.420 --> 00:24:27.420]   And we will look at the implementation next week.
[00:24:27.420 --> 00:24:30.500]   And when we look at next week, we're going to build a dense block.
[00:24:30.500 --> 00:24:32.220]   We're going to build the dense layers.
[00:24:32.220 --> 00:24:36.300]   We're going to build all these connections from scratch in PyTorch.
[00:24:36.300 --> 00:24:40.300]   But if you want to skip ahead, if you go to that again, if you go to that block first,
[00:24:40.300 --> 00:24:43.060]   you'll see all these things that we mentioned.
[00:24:43.060 --> 00:24:52.820]   And something that's now I'm going into basically just words or basically what researchers call
[00:24:52.820 --> 00:25:00.940]   these different things is, again, this has been mentioned that this is dense blocks.
[00:25:00.940 --> 00:25:04.900]   So that's something we know that the blocks where all the concatenation of the previous
[00:25:04.900 --> 00:25:07.620]   features happens have been called dense blocks.
[00:25:07.620 --> 00:25:18.060]   And then these blocks or like these bits are called transition blocks.
[00:25:18.060 --> 00:25:23.660]   So overall, if you look at the dense net architecture, it looks something like this.
[00:25:23.660 --> 00:25:27.740]   If I call them layers, like instead of calling them blocks, if I just call them as layers,
[00:25:27.740 --> 00:25:29.260]   you have something like this.
[00:25:29.260 --> 00:25:32.900]   Your input image, which is this one, passes through a stem.
[00:25:32.900 --> 00:25:39.260]   So I've missed a stem in this simple, this is just a simple view inside my, oh, sorry.
[00:25:39.260 --> 00:25:43.140]   This is just a simple view inside my dense block.
[00:25:43.140 --> 00:25:46.760]   So I have my dense block like this.
[00:25:46.760 --> 00:25:50.180]   So in this dense block, you can see, depends on how many layers there are.
[00:25:50.180 --> 00:25:56.180]   So in this dense block, you have one, two, three, four, five layers, just in this example.
[00:25:56.180 --> 00:26:02.020]   So what you can see is like, now again, the question is like, okay, I understand that
[00:26:02.020 --> 00:26:07.620]   dense blocks is where the concatenation of the features happens and dense block is where,
[00:26:07.620 --> 00:26:11.540]   and transition blocks is where the transition happens, such as the spatial dimensions become
[00:26:11.540 --> 00:26:12.900]   half.
[00:26:12.900 --> 00:26:17.920]   But what exactly goes on inside like this dense block is basically, as you can see here.
[00:26:17.920 --> 00:26:22.040]   So you have an input image, as you can see here, you have an input image, you go through
[00:26:22.040 --> 00:26:23.040]   layer zero.
[00:26:23.040 --> 00:26:27.280]   So the input image is input into layer zero, and it produces an output, which is this gray
[00:26:27.280 --> 00:26:28.280]   output.
[00:26:28.280 --> 00:26:31.400]   Oh, sorry, the gray output is the input image and produces an output, which is this purple
[00:26:31.400 --> 00:26:32.400]   output.
[00:26:32.400 --> 00:26:38.240]   And then the layer one accepts not only this purple output, which is from layer zero, but
[00:26:38.240 --> 00:26:43.480]   it also basically accepts or gets this gray input image.
[00:26:43.480 --> 00:26:48.100]   So you can see like layer one is not only getting layer zero's output, but also the
[00:26:48.100 --> 00:26:49.460]   one layer before.
[00:26:49.460 --> 00:26:53.940]   And similarly, when we come into layer two, it gets layer one's output, which is an orange,
[00:26:53.940 --> 00:26:58.080]   it gets the purple output, which is layer zero's output, and gets this gray output,
[00:26:58.080 --> 00:26:59.200]   which is again, the input image.
[00:26:59.200 --> 00:27:05.800]   So that's basically what's happening is like you're concatenating, like you're not inside
[00:27:05.800 --> 00:27:08.720]   the dense block, you're providing information.
[00:27:08.720 --> 00:27:14.920]   So the way the researchers actually quote this or explain this is like ResNet is taught
[00:27:14.920 --> 00:27:21.260]   to be in a way, if you think of it, ResNet is something that just passes state.
[00:27:21.260 --> 00:27:30.120]   So in ResNet, what you have, could I please ask everybody to mute themselves if you're
[00:27:30.120 --> 00:27:31.120]   not...
[00:27:31.120 --> 00:27:54.840]   My Zoom decided to quit unexpectedly.
[00:27:54.840 --> 00:27:55.840]   Can you guys hear me still?
[00:27:55.840 --> 00:27:56.840]   I'm really sorry about that.
[00:27:56.840 --> 00:27:57.840]   Could you please just post in the chat?
[00:27:57.840 --> 00:28:01.320]   Okay, excellent.
[00:28:01.320 --> 00:28:07.440]   There is no screen being shared.
[00:28:07.440 --> 00:28:09.680]   Yes, I think I'm not the host anymore.
[00:28:09.680 --> 00:28:10.680]   Am I?
[00:28:10.680 --> 00:28:12.200]   It's just giving me some stupid errors.
[00:28:12.200 --> 00:28:14.840]   One sec, let me just make sure I correct this.
[00:28:14.840 --> 00:28:17.840]   There's something weird going on.
[00:28:17.840 --> 00:28:18.840]   Okay, we're back.
[00:28:18.840 --> 00:28:21.840]   Can you guys see my screen now?
[00:28:21.840 --> 00:28:24.840]   Yes, it's visible.
[00:28:24.840 --> 00:28:33.760]   I was just explaining like this dense block architecture and I've actually lost my train
[00:28:33.760 --> 00:28:36.920]   of thought on where I was.
[00:28:36.920 --> 00:28:39.560]   But let's now see the exact specifics.
[00:28:39.560 --> 00:28:41.480]   So I will go into the paper at the very last.
[00:28:41.480 --> 00:28:46.120]   I just want to spend the next 10 minutes just explaining all of this, just explaining like
[00:28:46.120 --> 00:28:47.120]   how this could be.
[00:28:47.120 --> 00:28:52.200]   Like, I want you all to like, because this is a beginner-friendly, I want to be as beginner-friendly
[00:28:52.200 --> 00:28:53.200]   as possible.
[00:28:53.200 --> 00:28:54.880]   So I want to explain the shapes.
[00:28:54.880 --> 00:28:59.400]   I want to explain like how these things go on or like what's actually going on.
[00:28:59.400 --> 00:29:01.800]   So let's first understand.
[00:29:01.800 --> 00:29:04.040]   So let's just draw semantics.
[00:29:04.040 --> 00:29:06.680]   Let's just draw blocks to explain things, right?
[00:29:06.680 --> 00:29:09.560]   So you have the overall architecture.
[00:29:09.560 --> 00:29:12.200]   We'll start again in a top-down approach.
[00:29:12.200 --> 00:29:15.280]   We'll start understanding the dense net architecture first.
[00:29:15.280 --> 00:29:18.000]   Then we'll understand the dense block and then we'll look at the dense net.
[00:29:18.000 --> 00:29:19.000]   So let's just start.
[00:29:19.000 --> 00:29:20.880]   So you have an input image.
[00:29:20.880 --> 00:29:26.840]   Let's say this is my input image, 1 by 3, by 2 to 4, by 2 to 4.
[00:29:26.840 --> 00:29:30.800]   Again, it goes through the stem first, right?
[00:29:30.800 --> 00:29:35.640]   The stem, which is again a very common thing in most architectures.
[00:29:35.640 --> 00:29:40.200]   The stem just does, it just reduces the number, it just increases the number of channels and
[00:29:40.200 --> 00:29:42.240]   reduces the spatial dimensions.
[00:29:42.240 --> 00:29:45.600]   And then it goes into like what I like to call dense net.
[00:29:45.600 --> 00:29:49.680]   So let me just call that dense net here.
[00:29:49.680 --> 00:29:56.720]   Now this dense net architecture, if you think of it like overall, all of this is the dense
[00:29:56.720 --> 00:30:02.360]   net architecture, but I'm just trying to like, in a way, I'm just like trying to make sure
[00:30:02.360 --> 00:30:07.800]   like you think a little differently from the paper, but I'm just trying to explain things
[00:30:07.800 --> 00:30:09.640]   in a nicer way.
[00:30:09.640 --> 00:30:12.800]   Then this architecture, actually instead of calling it dense net, let me just call them
[00:30:12.800 --> 00:30:21.440]   dense blocks, or let's just call them blocks.
[00:30:21.440 --> 00:30:24.200]   So this overall thing is the dense net.
[00:30:24.200 --> 00:30:29.480]   All of this in like dotted lines is the dense net architecture.
[00:30:29.480 --> 00:30:33.120]   So you have your input going through the dense net, you get some output, which is like say
[00:30:33.120 --> 00:30:38.200]   1 by 1000, which just means like it's able to classify into 1000 classes.
[00:30:38.200 --> 00:30:47.640]   But then these blocks, and these blocks are, these blocks look something like, these blocks
[00:30:47.640 --> 00:30:52.760]   look something like, I'm just trying to show you this image again, so you remember, these
[00:30:52.760 --> 00:30:55.280]   blocks look something like this.
[00:30:55.280 --> 00:31:04.040]   Here you have, you have basically dense block followed by a transition block, then you have
[00:31:04.040 --> 00:31:07.160]   a dense block followed by a transition block and so on.
[00:31:07.160 --> 00:31:15.680]   So you have dense block one, transition block one, then you have dense block two, transition
[00:31:15.680 --> 00:31:16.680]   block two.
[00:31:16.680 --> 00:31:19.160]   Does that make sense so far?
[00:31:19.160 --> 00:31:27.280]   And basically you have like four of these, so all the way to something like DB, dense
[00:31:27.280 --> 00:31:34.600]   block four, transition block four.
[00:31:34.600 --> 00:31:41.520]   And inside the dense block, the dense block, it looks something like this.
[00:31:41.520 --> 00:31:47.120]   So inside the dense block is where you have, let's say, however many layers you want, one,
[00:31:47.120 --> 00:31:51.240]   two, three, four, five, six, let's say.
[00:31:51.240 --> 00:31:53.080]   This is where all of this is happening.
[00:31:53.080 --> 00:31:58.040]   So you have your input being passed here, then this goes to this one, this goes to this
[00:31:58.040 --> 00:32:06.440]   one, this one, this one, and this one, and then this one from second one.
[00:32:06.440 --> 00:32:14.840]   Basically this is where all the concatenation is happening.
[00:32:14.840 --> 00:32:16.600]   And like all of that, you get the point.
[00:32:16.600 --> 00:32:19.560]   Like inside the dense block is where this concatenation is happening.
[00:32:19.560 --> 00:32:26.120]   So as you can see, the overall architecture can be thought of to be in like three levels.
[00:32:26.120 --> 00:32:29.480]   This is the first level, this is the second, and this is the third.
[00:32:29.480 --> 00:32:34.240]   So this is the way how I like to think of architectures from a top-down perspective,
[00:32:34.240 --> 00:32:39.560]   because now I think dense net becomes a lot more clearer in our heads.
[00:32:39.560 --> 00:32:44.300]   So you have your input, you get your output, it passes through a stem.
[00:32:44.300 --> 00:32:45.300]   Now you have blocks.
[00:32:45.300 --> 00:32:48.320]   Blocks means dense blocks and transition blocks.
[00:32:48.320 --> 00:32:50.560]   So dense block is here, transition block is here.
[00:32:50.560 --> 00:32:52.980]   I haven't explained what a transition block is.
[00:32:52.980 --> 00:32:58.920]   So a transition block is just a one by one conf followed by a pooling layer.
[00:32:58.920 --> 00:33:01.820]   So what it does is, what does a transition block do?
[00:33:01.820 --> 00:33:07.400]   If you have, say, if you have a transition block here and you input something like one
[00:33:07.400 --> 00:33:12.200]   by three by two to four, by two to four to a transition block, it will give you an output
[00:33:12.200 --> 00:33:15.800]   of one by three of by one, one, two, by one, two.
[00:33:15.800 --> 00:33:23.140]   So it basically halves your, it basically makes your feature, your spatial dimensions
[00:33:23.140 --> 00:33:24.140]   half.
[00:33:24.140 --> 00:33:26.740]   So it makes your feature map be the half size.
[00:33:26.740 --> 00:33:31.180]   So that's what transition blocks are doing over here.
[00:33:31.180 --> 00:33:33.040]   And what's happening in dense blocks?
[00:33:33.040 --> 00:33:38.840]   In dense blocks is where all this, there's like these dense layers inside, depending
[00:33:38.840 --> 00:33:40.460]   on however many you have.
[00:33:40.460 --> 00:33:47.080]   So if you look at the various architectures of DenseNet, I just want to show you how this
[00:33:47.080 --> 00:33:48.080]   looks like.
[00:33:48.080 --> 00:33:53.440]   So you can see, you have, as I said, you have four dense blocks and four, three transition
[00:33:53.440 --> 00:33:54.440]   layers, sorry.
[00:33:54.440 --> 00:34:00.720]   So you don't have, you don't have this one, right?
[00:34:00.720 --> 00:34:03.280]   You have four dense blocks and three transition layers.
[00:34:03.280 --> 00:34:10.360]   And you can see like each architecture, like DenseNet 121, DenseNet 169, DenseNet 201,
[00:34:10.360 --> 00:34:11.920]   DenseNet 264.
[00:34:11.920 --> 00:34:16.920]   What's the difference in them is the number of layers inside these dense blocks.
[00:34:16.920 --> 00:34:21.560]   So dense block one has six of these in all of them.
[00:34:21.560 --> 00:34:27.120]   So dense block one has six layers, as you can see, cross six, cross six, cross six.
[00:34:27.120 --> 00:34:33.920]   But in DenseNet 169, dense block two has, again, all of them have 12, 12, 12.
[00:34:33.920 --> 00:34:38.760]   But you can see in dense block three, this is where the difference is in 121, 121, and
[00:34:38.760 --> 00:34:48.800]   124, is that you have 24 layers in dense block three, 32 layers in DenseNet 169, 48 in 201,
[00:34:48.800 --> 00:34:51.000]   and 64 in 264.
[00:34:51.000 --> 00:34:55.960]   So you can see how like these architectures, now you can see like, okay, 264 just means
[00:34:55.960 --> 00:35:03.480]   that DenseBlock three has more number of layers compared to DenseNet 121.
[00:35:03.480 --> 00:35:07.640]   And then I've already told you what a, what a transition block looks like.
[00:35:07.640 --> 00:35:11.400]   A transition block is just a one by one conf followed by a pooling.
[00:35:11.400 --> 00:35:14.440]   I haven't told you what this dense layer looks like.
[00:35:14.440 --> 00:35:18.200]   So if you look at the dense layer, it just looks something like this.
[00:35:18.200 --> 00:35:23.920]   It's a one by one conf followed by a three by three conf.
[00:35:23.920 --> 00:35:24.920]   That's it.
[00:35:24.920 --> 00:35:25.920]   It's like a bottleneck.
[00:35:25.920 --> 00:35:26.920]   It's like a bottleneck.
[00:35:26.920 --> 00:35:30.140]   It's like a bottleneck design.
[00:35:30.140 --> 00:35:36.240]   So basically, if your inputs are, say, let's just take this example.
[00:35:36.240 --> 00:35:40.080]   What are the inputs going to be for my DB1?
[00:35:40.080 --> 00:35:45.360]   So you pass in, I'm just also showing you like what the feature map shapes and sizes
[00:35:45.360 --> 00:35:46.360]   would be.
[00:35:46.360 --> 00:35:51.100]   So let's say if my input is one by three by two to four by two to four, what's the stem
[00:35:51.100 --> 00:35:52.100]   going to do?
[00:35:52.100 --> 00:35:58.920]   It's going to give me an output of one by say, 64 by 56 by 56.
[00:35:58.920 --> 00:35:59.920]   That's this.
[00:35:59.920 --> 00:36:08.500]   So my input to the dense block one is one by 64 by 56 by 56, which means the input over
[00:36:08.500 --> 00:36:14.300]   here is one by 64 by 56 by 56.
[00:36:14.300 --> 00:36:15.300]   So that's this.
[00:36:15.300 --> 00:36:22.180]   So then in my dense layer again, my input is one by 64 by 56 by 56.
[00:36:22.180 --> 00:36:27.780]   And then what this one by one conf does is that it first changes like this number of
[00:36:27.780 --> 00:36:30.900]   channels to some, because it's the bottleneck design.
[00:36:30.900 --> 00:36:36.060]   So what's different in bottleneck designs is that you first increase the number of channels
[00:36:36.060 --> 00:36:40.260]   or like set them to some number, and then you put them back to what the number they're
[00:36:40.260 --> 00:36:41.400]   supposed to be.
[00:36:41.400 --> 00:36:49.940]   So this will go from one by 64 by 56 by 56 to one by 128 by 56 by 56.
[00:36:49.940 --> 00:36:56.140]   And then this three by three con will put it back to one by 32 by 56 by 66.
[00:36:56.140 --> 00:37:01.740]   So what's happening is like each of these layers are adding plus 32 features.
[00:37:01.740 --> 00:37:07.300]   Each of these layers are adding plus 32 features.
[00:37:07.300 --> 00:37:13.880]   So by the time I will again explain how they're adding plus 32 features, but by the time you
[00:37:13.880 --> 00:37:21.420]   get your output from my dense block one, instead of like this being one by 64 by 56 by 56,
[00:37:21.420 --> 00:37:27.340]   it's something like one by 256 by 56 by 56.
[00:37:27.340 --> 00:37:30.580]   Now why have I made all of this so complex?
[00:37:30.580 --> 00:37:34.580]   So like, why am I showing you all of these shapes or numbers?
[00:37:34.580 --> 00:37:38.980]   It's like, if you really want to understand an architecture, you really need to understand
[00:37:38.980 --> 00:37:42.900]   what's the input shape, what's the output shape, and like how do these things look like.
[00:37:42.900 --> 00:37:48.500]   Like what I could have done in an easy way is like just gone through this research paper
[00:37:48.500 --> 00:37:53.460]   and then shown you, okay, dense net, like dense blocks are basically like dense net
[00:37:53.460 --> 00:37:57.940]   is just this with inside the boxes where you can get near the features and then transitioning
[00:37:57.940 --> 00:38:00.100]   boxes where you reduce the feature size.
[00:38:00.100 --> 00:38:06.220]   But you would never understand like the devil is in the details and complexity is in the
[00:38:06.220 --> 00:38:07.220]   details.
[00:38:07.220 --> 00:38:12.660]   So you would never understand to look at the dense net architecture in this way.
[00:38:12.660 --> 00:38:18.420]   And what would be missing are the details on like how do basically these features get
[00:38:18.420 --> 00:38:20.660]   concatenated in all those details.
[00:38:20.660 --> 00:38:23.660]   So I'm just going to take questions now.
[00:38:23.660 --> 00:38:31.660]   Hi, so can you hear me?
[00:38:31.660 --> 00:38:36.660]   Yeah, could you please post them on the forums if that's okay?
[00:38:36.660 --> 00:38:37.660]   Okay, sure.
[00:38:37.660 --> 00:38:40.660]   But just put it verbose.
[00:38:40.660 --> 00:38:44.900]   Just your voice is cracking a lot.
[00:38:44.900 --> 00:38:46.900]   That's why I just said post on the forums.
[00:38:46.900 --> 00:38:52.100]   Thanks, I'll answer this one, Durga, how 32 features are getting added.
[00:38:52.100 --> 00:38:57.540]   So let me answer this one.
[00:38:57.540 --> 00:39:04.380]   So where are we?
[00:39:04.380 --> 00:39:08.660]   We need to keep size impact with padding while doing convolution.
[00:39:08.660 --> 00:39:10.620]   Yes, that's correct.
[00:39:10.620 --> 00:39:15.780]   Inside the dense, basically inside the dense blocks, you do have enough padding to make
[00:39:15.780 --> 00:39:21.860]   sure like if the feature map size is 56 by 56, it doesn't go up or down.
[00:39:21.860 --> 00:39:25.660]   Stride with the re-strided one by one convolution.
[00:39:25.660 --> 00:39:32.300]   Okay, so you were just trying to answer, I guess.
[00:39:32.300 --> 00:39:37.660]   What's the reason intuition is just a different like difference of the intuition is like deeper
[00:39:37.660 --> 00:39:43.500]   architectures are, if you, when we get to the efficient net, you will see like this
[00:39:43.500 --> 00:39:47.500]   different ways of scaling architectures.
[00:39:47.500 --> 00:39:53.540]   And one of the easiest ways to scale architectures is just to make them deeper.
[00:39:53.540 --> 00:39:57.500]   And that's just the intuition is like if you have deeper architecture and they can learn
[00:39:57.500 --> 00:40:01.740]   more complex patterns, or they can learn more complex things.
[00:40:01.740 --> 00:40:05.780]   And that's why they can perform better at ImageNet or they can perform better at different
[00:40:05.780 --> 00:40:06.780]   data sets.
[00:40:06.780 --> 00:40:08.060]   That's just the intuition here.
[00:40:08.060 --> 00:40:13.060]   So in DenseStock 3 and 4, you just have more number of layers because what we want to say
[00:40:13.060 --> 00:40:22.860]   is that DenseNet, is that DenseNet 264 is deeper than DenseNet 121.
[00:40:22.860 --> 00:40:28.460]   So I was taking this question of how 32 features are getting added in DB1.
[00:40:28.460 --> 00:40:34.180]   So as you can see, what happens is, I'm just going to go into the details, is like when
[00:40:34.180 --> 00:40:41.540]   you start with 1 by 64 by 56 by 56, and dense layers are all 1 by 1 conv followed by 3 by
[00:40:41.540 --> 00:40:48.940]   3 conv, what it does is, this is pretty much just outputting things of 1 by 32 by 56 by
[00:40:48.940 --> 00:40:49.940]   56.
[00:40:49.940 --> 00:40:54.460]   So this, the first layer outputs, this is all a mess.
[00:40:54.460 --> 00:41:00.260]   So the first dense layer outputs are this shape, 1 by 32 by 56 by 56.
[00:41:00.260 --> 00:41:03.860]   And then what you could do is you could concatenate them together.
[00:41:03.860 --> 00:41:10.340]   So you have 1 by 96 by 56 by 56, which is what is going into the next layer.
[00:41:10.340 --> 00:41:12.540]   So this is what goes into the second layer.
[00:41:12.540 --> 00:41:18.860]   And then in the second layer, again, goes from 96 to 128 channels back to 32 channels.
[00:41:18.860 --> 00:41:24.220]   So the output from the second layer is also 1 by 32 by 56 by 56.
[00:41:24.220 --> 00:41:25.500]   Then you concatenate the two together.
[00:41:25.500 --> 00:41:31.460]   So you have something like 1 by 128 by 56 by 56, and so on.
[00:41:31.460 --> 00:41:36.260]   By the time you reach the end of the dense block with six layers, your output shape becomes
[00:41:36.260 --> 00:41:42.980]   this 1 by 256 by 56 by 56, because you're just concatenating 32 channels every time
[00:41:42.980 --> 00:41:47.340]   from the outputs of the dense layers.
[00:41:47.340 --> 00:41:53.460]   No, this architecture actually does not overfit.
[00:41:53.460 --> 00:41:58.020]   I think in a way it does overfit slightly more than ResNet, but I haven't done any research
[00:41:58.020 --> 00:41:59.020]   on.
[00:41:59.020 --> 00:42:10.700]   And from the paper, there isn't any-- as far as I know, there isn't any straight connection.
[00:42:10.700 --> 00:42:14.700]   Of course, it has-- I will go into the details of this one.
[00:42:14.700 --> 00:42:18.860]   I will just share some text that has been mentioned in the paper.
[00:42:18.860 --> 00:42:22.780]   I'm confused on 1 by 1 with 128 filters as bottling.
[00:42:22.780 --> 00:42:25.940]   Is that the same across all box and all the layers in each box?
[00:42:25.940 --> 00:42:26.940]   Yes.
[00:42:26.940 --> 00:42:32.460]   So all layers, at least from an implementation perspective, when you see-- again, this is
[00:42:32.460 --> 00:42:36.740]   something that's not mentioned when you read through the paper.
[00:42:36.740 --> 00:42:39.140]   Let's keep going, and I will show you in the paper.
[00:42:39.140 --> 00:42:41.060]   But this is the bottleneck layer.
[00:42:41.060 --> 00:42:44.660]   Although each layer only produces k output feature maps.
[00:42:44.660 --> 00:42:47.900]   So k, in this case, is 32.
[00:42:47.900 --> 00:42:50.420]   So k is this growth rate.
[00:42:50.420 --> 00:42:56.980]   So what they say is basically, because in a dense block, what you're doing is your features
[00:42:56.980 --> 00:43:00.860]   are growing by some factor, which is plus 32.
[00:43:00.860 --> 00:43:06.580]   So this plus 32, I just said 32 because I know that the growth rate that they've used
[00:43:06.580 --> 00:43:08.900]   in dense net implementation is 32.
[00:43:08.900 --> 00:43:11.460]   But actually, they just say it's called k.
[00:43:11.460 --> 00:43:15.780]   And then it says, it has been noted that a 1 by 1 convolution can be introduced as a
[00:43:15.780 --> 00:43:20.700]   bottleneck layer for each 3 by 3 to reduce the number of input feature maps.
[00:43:20.700 --> 00:43:28.660]   So basically, what they do is they-- so the 3 by 3 convolution, that has more computation
[00:43:28.660 --> 00:43:30.580]   than a 1 by 1 convolution.
[00:43:30.580 --> 00:43:35.740]   So the fact that 3 by 3 does not act on a feature map which is too large, what they
[00:43:35.740 --> 00:43:37.500]   do is you have a bottleneck design.
[00:43:37.500 --> 00:43:41.100]   So the 1 by 1 reduces the feature map size.
[00:43:41.100 --> 00:43:45.860]   And then this reduced feature map size is what gets passed to 3 by 3.
[00:43:45.860 --> 00:43:50.180]   So you'll find those details in this bottleneck layers.
[00:43:50.180 --> 00:43:55.120]   So in our experiments, we let each 1 by 1 convolution produce 4k feature maps.
[00:43:55.120 --> 00:43:59.260]   What that means is-- because I told you already, k is 32.
[00:43:59.260 --> 00:44:09.340]   What that means is that for all of these layers, for all of these layers, 1, 2, 3, 4, 5, which
[00:44:09.340 --> 00:44:14.700]   look like this, for 1 by 1 conv followed by 3 by 3 conv, for all of these layers, the
[00:44:14.700 --> 00:44:17.220]   1 by 1 conv has 128 channels.
[00:44:17.220 --> 00:44:22.400]   So that's fixed.
[00:44:22.400 --> 00:44:28.900]   So more questions?
[00:44:28.900 --> 00:44:36.100]   Ramesh, could you maybe reply to this comment and just make sure-- I just want to make sure
[00:44:36.100 --> 00:44:39.060]   that this has been answered.
[00:44:39.060 --> 00:44:42.980]   In the transition block, would we only have the pooling layer and avoid the 1 by 1?
[00:44:42.980 --> 00:44:48.500]   What is the advantage of where we go from 64 by 56 by 56 to 128?
[00:44:48.500 --> 00:44:51.140]   In the-- oh, that's a good point.
[00:44:51.140 --> 00:44:53.500]   But basically, these are all these experiments.
[00:44:53.500 --> 00:44:57.660]   And when I do live coding next week, I do want to answer all of these experiments.
[00:44:57.660 --> 00:45:04.340]   What we can do is we can try-- I guess the question-- the way I answer these questions
[00:45:04.340 --> 00:45:06.660]   is by trying and testing.
[00:45:06.660 --> 00:45:10.740]   So try removing the 1 by 1 layer.
[00:45:10.740 --> 00:45:14.740]   Because this is like-- there's a reason the researchers would have-- so going through
[00:45:14.740 --> 00:45:21.060]   the experiments section over here, and basically, they also ran a lot of experiments, like different
[00:45:21.060 --> 00:45:25.540]   learning rates on different things and different stuff.
[00:45:25.540 --> 00:45:32.260]   But try-- when I do show you how to implement the dense architecture from scratch next week,
[00:45:32.260 --> 00:45:40.260]   then take that architecture, implement it as is, and then remove the 1 by 1 convolution
[00:45:40.260 --> 00:45:46.100]   and see if it makes a difference.
[00:45:46.100 --> 00:45:49.940]   Fundamental doubt-- how do you differentiate between filters?
[00:45:49.940 --> 00:45:52.100]   How do you differentiate between filters?
[00:45:52.100 --> 00:45:54.340]   Each kernel is unique in a stack of kernels and filters.
[00:45:54.340 --> 00:46:00.420]   So when the channels depends on number of filters we apply, how exactly we differentiate?
[00:46:00.420 --> 00:46:04.860]   This number of kernels will be equal across filters.
[00:46:04.860 --> 00:46:08.580]   I'm not sure I understand.
[00:46:08.580 --> 00:46:12.780]   Yeah, I'm not sure I understand this, Durga.
[00:46:12.780 --> 00:46:13.780]   I'm really sorry.
[00:46:13.780 --> 00:46:14.780]   I'll read it again.
[00:46:14.780 --> 00:46:30.220]   Yeah, I'm not-- I'm not sure.
[00:46:30.220 --> 00:46:31.220]   I really don't know.
[00:46:31.220 --> 00:46:32.220]   Awesome.
[00:46:32.220 --> 00:46:33.220]   Great, you like that advice.
[00:46:33.220 --> 00:46:41.460]   Fundamental question-- when we go deeper with layers, the features that get extracted are
[00:46:41.460 --> 00:46:44.060]   called low-level features or high-level features?
[00:46:44.060 --> 00:46:45.740]   They're called high-level features.
[00:46:45.740 --> 00:46:48.860]   So the low-level features are the ones that are in the early layers, and they're called
[00:46:48.860 --> 00:46:58.860]   low-level because it's just basically like-- this is paper visualizing-- I think.
[00:46:58.860 --> 00:47:02.980]   Zyla and Ferguson.
[00:47:02.980 --> 00:47:15.380]   Oh, yeah, here it is.
[00:47:15.380 --> 00:47:19.300]   So just to answer this question on what is a low-level feature and high-level feature,
[00:47:19.300 --> 00:47:24.780]   if you look at this paper visualizing-- and this is something we've discussed in one of
[00:47:24.780 --> 00:47:31.860]   our past sessions.
[00:47:31.860 --> 00:47:35.500]   And when that opens, I'll come back to that question when that opens.
[00:47:35.500 --> 00:47:37.660]   I'll just take more questions so far.
[00:47:37.660 --> 00:47:45.060]   OK, here's the question from-- the ResNet architecture also adds skip connections.
[00:47:45.060 --> 00:47:49.900]   DenseNet also adds connections from early layers to later layers inside the DenseNet
[00:47:49.900 --> 00:47:50.900]   blocks.
[00:47:51.060 --> 00:47:55.780]   How do you form your intuition about how this is helping the overall performance?
[00:47:55.780 --> 00:48:00.020]   How do these skip connections improve performance over the [INAUDIBLE] ResNets?
[00:48:00.020 --> 00:48:00.940]   That's a good point.
[00:48:00.940 --> 00:48:06.780]   So the point that the researchers make-- actually, let's read through the paper now.
[00:48:06.780 --> 00:48:11.980]   We're at that point where, instead of asking me, let's read through the paper.
[00:48:11.980 --> 00:48:18.740]   So the point that-- where is it written?
[00:48:18.740 --> 00:48:20.380]   These are the points that they make.
[00:48:20.380 --> 00:48:24.500]   So this kind of architecture has several compelling advantages.
[00:48:24.500 --> 00:48:30.420]   They alleviate the vanishing gradient problem, strengthen feature propagation, encourage
[00:48:30.420 --> 00:48:33.820]   feature reuse, and substantially reduce the number of parameters.
[00:48:33.820 --> 00:48:37.980]   So I know that's very counterintuitive, like how is this reducing the number of parameters?
[00:48:37.980 --> 00:48:44.060]   But something that they're saying is it encourages feature reuse.
[00:48:44.060 --> 00:48:50.500]   So what that means is once you've learned-- once your earlier layers have-- for example,
[00:48:50.500 --> 00:48:58.500]   once this x1-- and this is my intuition of reading this-- is that once this x1 has learned
[00:48:58.500 --> 00:49:05.340]   the features from x0, then these features in green don't have to be relearned.
[00:49:05.340 --> 00:49:07.920]   They get passed to all the later layers.
[00:49:07.920 --> 00:49:13.560]   So these later layers don't need to worry about the features that are being learned
[00:49:13.560 --> 00:49:14.560]   in the earlier layers.
[00:49:14.560 --> 00:49:19.100]   They just have them as part of the concatenated input.
[00:49:19.100 --> 00:49:22.460]   So this is, in a way, encouraging feature reuse.
[00:49:22.460 --> 00:49:26.980]   It's much easier now for the network because, again, these are low-level features in the
[00:49:26.980 --> 00:49:29.920]   beginning, and then these are high-level features at the end.
[00:49:29.920 --> 00:49:35.980]   So at some places, it might be helpful to know what the low-level features are.
[00:49:35.980 --> 00:49:42.300]   And my intuition is in this network, what you have is if the network basically at x4
[00:49:42.300 --> 00:49:47.980]   wants to look back in the previous layers at x2, x1, x3, it has the ability to do so
[00:49:47.980 --> 00:49:48.980]   very easily.
[00:49:48.980 --> 00:49:54.140]   Whereas in ResNet, because you keep adding, you keep adding your outputs in a way-- it's
[00:49:54.140 --> 00:49:56.500]   like a state.
[00:49:56.500 --> 00:50:00.740]   So think of it as five people are standing in a queue.
[00:50:00.740 --> 00:50:04.980]   So think of this like the best example is five people are standing in a queue.
[00:50:04.980 --> 00:50:06.380]   This is your deep learning network.
[00:50:06.380 --> 00:50:11.180]   Let me just try and make this example very clear.
[00:50:11.180 --> 00:50:12.180]   This is really bad.
[00:50:12.180 --> 00:50:19.740]   So let's say five people are standing-- or let's just say five people are standing in
[00:50:19.740 --> 00:50:20.740]   a queue.
[00:50:20.740 --> 00:50:21.860]   And you have a ball here.
[00:50:21.860 --> 00:50:26.700]   Now what each person does in ResNet is that they update this ball.
[00:50:26.700 --> 00:50:33.300]   So it goes from red to blue to black to some other color, green.
[00:50:33.300 --> 00:50:39.500]   And then it goes to maybe orange in the end, and finally some other color, light blue.
[00:50:39.500 --> 00:50:41.500]   And this becomes your output in the end.
[00:50:41.500 --> 00:50:46.780]   But what each person is doing is they're adding their own information on top of the previous
[00:50:46.780 --> 00:50:47.780]   person's information.
[00:50:47.780 --> 00:50:57.140]   Whereas in DenseNet, again, if these five people were to pass some information, if the
[00:50:57.140 --> 00:51:03.860]   later layers need to look at the first layers, then they need to find-- that information
[00:51:03.860 --> 00:51:09.460]   is still in this orange ball in a way because it got updated from red to orange as a result.
[00:51:09.460 --> 00:51:10.700]   As a process.
[00:51:10.700 --> 00:51:15.380]   But for this person in DenseNet, it already gets the red ball here.
[00:51:15.380 --> 00:51:18.300]   That red ball gets passed.
[00:51:18.300 --> 00:51:24.660]   So this is the main intuition-- intuitive difference in DenseNet and ResNet is you can
[00:51:24.660 --> 00:51:29.260]   always look back and you know what the earlier layers have learned.
[00:51:29.260 --> 00:51:35.940]   And I think in that way it makes the overall architecture an advancement over ResNet.
[00:51:35.940 --> 00:51:36.940]   But that's just my intuition.
[00:51:36.940 --> 00:51:42.860]   I could be completely wrong.
[00:51:42.860 --> 00:51:56.420]   Could you please explain quickly how a one-by-one-- oh, you're saying that also happens in one-by-one
[00:51:56.420 --> 00:51:57.420]   skip connections.
[00:51:57.420 --> 00:51:59.220]   But I'm talking over the overall architecture.
[00:51:59.220 --> 00:52:03.940]   In skip connections, if you have a network that's just being skipped, it's just around
[00:52:03.940 --> 00:52:04.940]   that person.
[00:52:04.940 --> 00:52:09.660]   So the skip connection in this case is just from here to here.
[00:52:09.660 --> 00:52:13.780]   But all the way-- this is then going to pass and then there's a skip connection.
[00:52:13.780 --> 00:52:16.340]   And then this is going to get passed and then there's a skip connection.
[00:52:16.340 --> 00:52:20.340]   But in DenseNet, you have this input being passed all the way.
[00:52:20.340 --> 00:52:28.780]   The connection is much easier is the point I'm making.
[00:52:28.780 --> 00:52:31.140]   Are there any application studies done for DenseNet?
[00:52:31.140 --> 00:52:33.780]   How do we know which connection is inside the DenseBlock?
[00:52:33.780 --> 00:52:39.180]   I'm not sure of are there any application studies.
[00:52:39.180 --> 00:52:44.980]   But I guess as part of this big reading group, my point is-- my main goal was to make sure
[00:52:44.980 --> 00:52:49.380]   that the audience understands what DenseNet architecture is and how it is different from
[00:52:49.380 --> 00:52:50.380]   ResNet.
[00:52:50.380 --> 00:52:54.940]   And as far as that's been accomplished, I'm going to be a happy person that that's happened.
[00:52:54.940 --> 00:52:58.740]   So in terms of application studies now, there's a whole lot of application studies that we
[00:52:58.740 --> 00:52:59.740]   can do.
[00:52:59.740 --> 00:53:09.700]   So before answering more questions-- yes, pretty much.
[00:53:09.700 --> 00:53:11.460]   That's my intuition, Vinayak.
[00:53:11.460 --> 00:53:15.260]   That's pretty much my intuition on this.
[00:53:15.260 --> 00:53:18.740]   Absolutely.
[00:53:18.740 --> 00:53:23.540]   This is exactly the point that I'm making as well.
[00:53:23.540 --> 00:53:28.220]   So the summing and the concatenation is the difference, which is exactly as I was saying
[00:53:28.220 --> 00:53:32.460]   in the red ball being updated, because each time you sum something, like add something,
[00:53:32.460 --> 00:53:34.500]   it gets updated to a new value.
[00:53:34.500 --> 00:53:38.540]   But if you concatenate it, then that previous value just remains all the way towards the
[00:53:38.540 --> 00:53:39.540]   end.
[00:53:39.540 --> 00:53:44.620]   Now, let's just quickly-- I've spent all the time in explaining DenseNet paper without
[00:53:44.620 --> 00:53:45.620]   reading the DenseNet paper.
[00:53:45.620 --> 00:53:52.100]   So let me just go through the paper and show you how now this discussion is going to make--
[00:53:52.100 --> 00:53:56.340]   the ideal thing that you could do now is go back and read the Densely Connected Network,
[00:53:56.340 --> 00:53:58.340]   and hopefully it will make things very easy.
[00:53:58.340 --> 00:54:02.700]   So the main points here are, OK, this is what a dense block looks like.
[00:54:02.700 --> 00:54:07.260]   And then they say, instead of having l connections, you have l, l plus 1.
[00:54:07.260 --> 00:54:11.500]   Actually, let me just go into the-- go here.
[00:54:11.500 --> 00:54:15.700]   So the main points in-- I'm just going to go skip through so you understand this paper.
[00:54:15.700 --> 00:54:20.460]   It's like the main point that they make in introduction is instead of having l connections,
[00:54:20.460 --> 00:54:22.380]   you have l, l plus 1 by 2.
[00:54:22.380 --> 00:54:27.580]   And then there's several advantages, like vanishing gradient problem, and create feature
[00:54:27.580 --> 00:54:31.220]   reuse, and reduce the number of parameters.
[00:54:31.220 --> 00:54:36.900]   And then we know the dense block looks something like this, is where-- and the overall architecture.
[00:54:36.900 --> 00:54:42.580]   So if you keep reading through here, they say deep learning in 2016 came a long way
[00:54:42.580 --> 00:54:47.740]   from these previous papers, Linnet, which consisted of five layers, VGG, which consisted
[00:54:47.740 --> 00:54:49.020]   of 19 layers.
[00:54:49.020 --> 00:54:55.060]   And then they say, OK, ResNets were introduced, which could take as up to 100 layers.
[00:54:55.060 --> 00:55:00.620]   And then now they're just talking about the ResNets, and they say, something that's really
[00:55:00.620 --> 00:55:04.740]   common is that they create short paths from early layers to later layers.
[00:55:04.740 --> 00:55:07.300]   And this is like the researchers trying to develop that intuition.
[00:55:07.300 --> 00:55:11.220]   It's like they're looking at things, and they're like, OK, skip connection is basically just
[00:55:11.220 --> 00:55:14.220]   a short path from input to output.
[00:55:14.220 --> 00:55:16.660]   So you basically just add the input to the output.
[00:55:16.660 --> 00:55:20.460]   So what you're trying to do is you're adding short paths.
[00:55:20.460 --> 00:55:23.260]   And that's something exactly that stochastic depth does as well.
[00:55:23.260 --> 00:55:34.940]   So if you don't know what stochastic depth is, just go to 1db.ai/aarora, which is A-Aurora.
[00:55:34.940 --> 00:55:41.420]   And you'll find this article revisiting ResNets, which I've written about NF, which basically
[00:55:41.420 --> 00:55:48.060]   ResNet-RS, and in here, I think there should be something about stochastic depth.
[00:55:48.060 --> 00:55:49.740]   There it is.
[00:55:49.740 --> 00:55:53.740]   So I've explained exactly what stochastic depth is over here, and that's not something
[00:55:53.740 --> 00:55:56.180]   I'll discuss as part of the paper reading today.
[00:55:56.180 --> 00:56:01.420]   But please feel free to look at that there.
[00:56:01.420 --> 00:56:06.060]   And going back to the paper, so the main point that the researcher is trying to make is like,
[00:56:06.060 --> 00:56:10.300]   OK, what they're trying to do is they're trying to create paths from early layers to later
[00:56:10.300 --> 00:56:11.300]   layers.
[00:56:11.300 --> 00:56:15.260]   So this is the key point to note, paths from early layers to later layers.
[00:56:15.260 --> 00:56:19.100]   And then this is where they say, we propose an architecture that distills the insight
[00:56:19.100 --> 00:56:21.260]   into a simple connectivity pattern.
[00:56:21.260 --> 00:56:26.700]   So to ensure maximum information flow between layers, we connect all the layers.
[00:56:26.700 --> 00:56:31.740]   So this is the main intuition.
[00:56:31.740 --> 00:56:32.740]   This is how the researchers thought.
[00:56:32.740 --> 00:56:37.580]   It's like, if we connect all the layers, then it's going to be the maximum information flow.
[00:56:37.580 --> 00:56:42.060]   And to preserve the feedforward nature, which as I've said, it just means like the spatial
[00:56:42.060 --> 00:56:47.060]   dimensions come slower and the number of channels goes up.
[00:56:47.060 --> 00:56:51.660]   It just says like, you have things divided in dense blocks, and then you have LL plus
[00:56:51.660 --> 00:56:54.020]   1 by 2 connections.
[00:56:54.020 --> 00:56:58.540]   And therefore, this is called as densely connected network.
[00:56:58.540 --> 00:56:59.540]   And keep going.
[00:56:59.540 --> 00:57:03.560]   I guess I just want to highlight the main thing so you can go back and read this paper.
[00:57:03.560 --> 00:57:05.380]   And it should be all very easy for you.
[00:57:05.380 --> 00:57:10.380]   I usually skip over the related work, but if you're new to deep learning, do go over
[00:57:10.380 --> 00:57:12.260]   through the related work as well.
[00:57:12.260 --> 00:57:14.460]   And then this is where they start explaining dense net.
[00:57:14.460 --> 00:57:18.700]   So they go, OK, resnet looks something like this, which is f of x plus x.
[00:57:18.700 --> 00:57:26.060]   So as you can see, if x is xL, so my output is just my previous output and then f of x
[00:57:26.060 --> 00:57:31.740]   of previous, basically input passed through the residual path plus the input.
[00:57:31.740 --> 00:57:36.360]   And then I've already said, OK, what's the difference in dense connectivity?
[00:57:36.360 --> 00:57:41.460]   So in dense connectivity, you have something like this, where you have the final layer
[00:57:41.460 --> 00:57:43.740]   has all the inputs concatenated.
[00:57:43.740 --> 00:57:48.500]   So that's why it's represented in maths like this.
[00:57:48.500 --> 00:57:55.300]   So instead of having just one layer before, you have all the layers before.
[00:57:55.300 --> 00:57:59.020]   It says pooling layers, which is they're just saying, OK, pooling layers, I've already told
[00:57:59.020 --> 00:58:00.020]   you what to do.
[00:58:00.020 --> 00:58:03.860]   So this is pretty much produced the feature map size growth rate.
[00:58:03.860 --> 00:58:10.180]   So as you can see, what's going to happen in this case is like, actually, this is explained
[00:58:10.180 --> 00:58:15.460]   better in that image that I have.
[00:58:15.460 --> 00:58:21.020]   So as you can see, each layer in a dense block adds its own features.
[00:58:21.020 --> 00:58:23.580]   So the number of features are growing.
[00:58:23.580 --> 00:58:28.700]   The rate by which these features grow, or basically the number of channel outputs from
[00:58:28.700 --> 00:58:31.060]   each layer is referred to as growth rate.
[00:58:31.060 --> 00:58:34.460]   So don't be confused by growth rate.
[00:58:34.460 --> 00:58:36.460]   It's basically just 30.
[00:58:36.460 --> 00:58:39.180]   It's just basically 32.
[00:58:39.180 --> 00:58:40.180]   Yes.
[00:58:40.180 --> 00:58:44.660]   And then again, we've already looked at this.
[00:58:44.660 --> 00:58:49.020]   Now you know what the differences in various dense-thin architectures look like.
[00:58:49.020 --> 00:58:56.580]   So basically, in 24, there's this basic differences in dense block 3 and 4, and dense-thin 201,
[00:58:56.580 --> 00:59:00.380]   264, all of these are just deeper versions of this.
[00:59:00.380 --> 00:59:02.060]   So it's just scaling, basically.
[00:59:02.060 --> 00:59:04.700]   Again, bottleneck layers, this is something I mentioned.
[00:59:04.700 --> 00:59:09.060]   So in bottleneck layers, you have a 1 by 1 followed by a 3 by 3.
[00:59:09.060 --> 00:59:12.020]   Bottleneck layers were first introduced in ResNet.
[00:59:12.020 --> 00:59:17.340]   And you can say, here it is, a 1 by 1 convolution can be introduced as bottleneck before the
[00:59:17.340 --> 00:59:19.060]   3 by 3.
[00:59:19.060 --> 00:59:20.100]   And that's pretty much it.
[00:59:20.100 --> 00:59:21.100]   That's just dense-thin.
[00:59:21.100 --> 00:59:23.000]   And then there's like implementation details.
[00:59:23.000 --> 00:59:25.220]   So I've kind of explained everything.
[00:59:25.220 --> 00:59:28.980]   And they go, OK, we use a kernel size 3 by 3.
[00:59:28.980 --> 00:59:33.780]   We use 1 by 1 convolution followed by 2 by 2 pooling as transition layers.
[00:59:33.780 --> 00:59:39.500]   So this is just everything that I've explained in-- this is everything that I've explained
[00:59:39.500 --> 00:59:43.200]   in this diagram.
[00:59:43.200 --> 00:59:48.460]   So when I was explaining like blocks, dense block, dense layers, this is just pretty much
[00:59:48.460 --> 00:59:49.460]   it.
[00:59:49.460 --> 00:59:54.560]   And I guess that should be it.
[00:59:54.560 --> 00:59:58.020]   And then there's just some experiments that you can read through on the experiments that
[00:59:58.020 --> 01:00:00.540]   the researchers tried.
[01:00:00.540 --> 01:00:06.540]   The training pipeline, basically, that's however many epochs we trained, what was the learning
[01:00:06.540 --> 01:00:10.480]   rate, 90 epochs, what was the momentum.
[01:00:10.480 --> 01:00:13.300]   So these are things that are just very common.
[01:00:13.300 --> 01:00:17.380]   And finally, this share, like which data sets they've got.
[01:00:17.380 --> 01:00:19.900]   They share the classification results on ImageNet.
[01:00:19.900 --> 01:00:22.620]   So there's not a lot going on.
[01:00:22.620 --> 01:00:26.100]   I do like the discussion posts here.
[01:00:26.100 --> 01:00:27.100]   But I'm going to skip.
[01:00:27.100 --> 01:00:30.420]   So if you go back and you read this, and there's like this really good discussion on feature
[01:00:30.420 --> 01:00:34.860]   reuse, which you would just say is, OK, why having these dense connections is actually
[01:00:34.860 --> 01:00:35.860]   better.
[01:00:35.860 --> 01:00:40.260]   So for everybody asking me those questions, have a read of this feature reuse part.
[01:00:40.260 --> 01:00:41.980]   And then finally, there's a conclusion.
[01:00:41.980 --> 01:00:46.140]   So in this, they say, OK, we propose a new convolutional neural architecture, which we
[01:00:46.140 --> 01:00:49.940]   refer to as DenseNet.
[01:00:49.940 --> 01:00:52.160]   And that's time, basically.
[01:00:52.160 --> 01:01:02.660]   This is all that I wanted to cover today.
[01:01:02.660 --> 01:01:05.660]   Could you please-- oh, crap, I missed this.
[01:01:05.660 --> 01:01:08.180]   Sorry, Sam.
[01:01:08.180 --> 01:01:09.180]   I missed the zooming in.
[01:01:09.180 --> 01:01:14.260]   OK, I guess thanks very much.
[01:01:14.260 --> 01:01:17.860]   I hope I have explained DenseNets well.
[01:01:17.860 --> 01:01:21.540]   And I hope you do understand DenseNets now.
[01:01:21.540 --> 01:01:25.780]   So question is, why does increasing the number of layers help?
[01:01:25.780 --> 01:01:35.180]   It's just, as I said, for the whys, try it.
[01:01:35.180 --> 01:01:36.180]   Try it.
[01:01:36.180 --> 01:01:37.180]   Experiment with it.
[01:01:37.180 --> 01:01:39.180]   Use weights and biases for your experiments.
[01:01:39.180 --> 01:01:40.640]   And experiment.
[01:01:40.640 --> 01:01:42.980]   You will see why.
[01:01:42.980 --> 01:01:44.340]   Develop that intuition on why.
[01:01:44.340 --> 01:01:47.060]   I don't have an answer, because these are things like this.
[01:01:47.060 --> 01:01:50.940]   If I want to give you an answer, I'll probably point you to a research paper.
[01:01:50.940 --> 01:01:54.620]   Or I'll mostly just say, OK, this is how things are done.
[01:01:54.620 --> 01:01:55.620]   And this is how it works.
[01:01:55.620 --> 01:01:59.420]   Because if you do it the other way, accuracy actually decreases.
[01:01:59.420 --> 01:02:02.660]   But for those things, these kinds of things, just try.
[01:02:02.660 --> 01:02:04.220]   Just remove the convolution.
[01:02:04.220 --> 01:02:05.540]   And just use a pooling layer.
[01:02:05.540 --> 01:02:11.540]   Just try going from-- I guess the question you're asking is, why do the early layers
[01:02:11.540 --> 01:02:14.740]   have to be 6 and 12, and then followed by that?
[01:02:14.740 --> 01:02:18.820]   The answer is, in my head, the answer is, because in the earlier layers, the feature
[01:02:18.820 --> 01:02:20.100]   map size is too big.
[01:02:20.100 --> 01:02:27.100]   So if you have too many number of layers early on, then that just means that a lot of computation
[01:02:27.100 --> 01:02:28.100]   is already happening.
[01:02:28.100 --> 01:02:30.100]   And you're just adding more and more computation.
[01:02:30.100 --> 01:02:32.940]   So your network has actually become massive.
[01:02:32.940 --> 01:02:35.380]   And it's just becoming massive in terms of memory.
[01:02:35.380 --> 01:02:42.060]   But by the time you reach 10-spot 3 or 10-spot 4, your feature map size is reduced in size.
[01:02:42.060 --> 01:02:45.860]   So it's gone from 224 to something like, I don't know, 14 by 14.
[01:02:45.860 --> 01:02:50.540]   And then it's much easier to have a lot more number of layers then, as opposed to having
[01:02:50.540 --> 01:02:55.740]   a lot more number of layers when your feature map size is 80 by 80 or something like that.
[01:02:55.740 --> 01:02:56.860]   So that's just my answer.
[01:02:56.860 --> 01:03:01.100]   But have a look and try and finding this answer for yourself.
[01:03:01.100 --> 01:03:03.700]   Oh, yes, of course.
[01:03:03.700 --> 01:03:09.620]   And the paper life coding session for DenseTent will happen next week.
[01:03:09.620 --> 01:03:10.620]   Excellent.
[01:03:10.620 --> 01:03:18.220]   I'm glad, Ravi, that you're keen.
[01:03:18.220 --> 01:03:20.900]   The first layer has only 32 outputs.
[01:03:20.900 --> 01:03:25.060]   So the one by one is not a bottleneck as it increases.
[01:03:25.060 --> 01:03:29.380]   But the-- oh, yes, that's the point from me.
[01:03:29.380 --> 01:03:31.180]   And that's something I have my forms against.
[01:03:31.180 --> 01:03:33.260]   And that's something I was against.
[01:03:33.260 --> 01:03:36.700]   So when I was implementing-- actually, let me just show you this.
[01:03:36.700 --> 01:03:38.980]   Where is this?
[01:03:38.980 --> 01:03:42.060]   Why is this-- no, it's not open.
[01:03:42.060 --> 01:03:45.940]   So just before this, I was actually implementing things from scratch.
[01:03:45.940 --> 01:03:47.700]   And this is how a DenseBlock looks like.
[01:03:47.700 --> 01:03:50.220]   It goes from 64 to 128.
[01:03:50.220 --> 01:03:52.940]   That's actually increasing the number of layers.
[01:03:52.940 --> 01:03:58.260]   And by the time you reach-- as you can see, this 128 is pretty much fixed.
[01:03:58.260 --> 01:04:01.260]   But for the later layers, you go from 352 to 128.
[01:04:01.260 --> 01:04:02.780]   So that's a bottleneck.
[01:04:02.780 --> 01:04:06.500]   You go from 288 to 128, and so on.
[01:04:06.500 --> 01:04:09.220]   So I don't like this number of 128 being fixed.
[01:04:09.220 --> 01:04:10.540]   That's something I don't like.
[01:04:10.540 --> 01:04:13.340]   But again, there must be a good reason on why researchers
[01:04:13.340 --> 01:04:14.580]   have done something like this.
[01:04:14.580 --> 01:04:18.180]   So going from 992 to 128 channels, you probably
[01:04:18.180 --> 01:04:22.220]   would be losing a lot of information in doing something like this.
[01:04:22.220 --> 01:04:25.140]   And then you go from 128 to 32.
[01:04:25.140 --> 01:04:28.460]   So that's a really, really good point that you've raised.
[01:04:28.460 --> 01:04:34.460]   But I hope me showing you this little implementation--
[01:04:34.460 --> 01:04:36.340]   these are things-- look out for these things.
[01:04:36.340 --> 01:04:38.900]   I'm so happy that you are looking out for these things,
[01:04:38.900 --> 01:04:43.180]   because I also try and look out for these things.
[01:04:43.180 --> 01:04:43.940]   Excellent, Vinayak.
[01:04:43.940 --> 01:04:45.900]   I'm so happy to hear that.
[01:04:45.900 --> 01:04:51.460]   That's great.
[01:04:51.460 --> 01:04:52.460]   Thanks for that.
[01:04:52.460 --> 01:04:53.860]   Sorry, I don't know what you--
[01:04:53.860 --> 01:04:55.180]   Naveen, thanks for that, Naveen.
[01:04:55.180 --> 01:04:59.340]   Cool.
[01:04:59.340 --> 01:05:00.420]   Let's stop it here, then.
[01:05:00.420 --> 01:05:01.580]   Thanks very much, everybody.
[01:05:01.580 --> 01:05:07.660]   And I will see you next week when I live code Dead Stand from Scratch.
[01:05:07.660 --> 01:05:11.100]   Bye, and hope everybody has a good week.
[01:05:11.220 --> 01:05:14.060]   [NO SPEECH]
[01:05:14.060 --> 01:05:16.500]   [NO SPEECH]
[01:05:16.500 --> 01:05:19.460]   [NO SPEECH]
[01:05:19.460 --> 01:05:22.420]   [NO SPEECH]
[01:05:22.420 --> 01:05:25.420]   [NO SPEECH]
[01:05:25.420 --> 01:05:28.420]   [NO SPEECH]
[01:05:28.420 --> 01:05:31.420]   [NO SPEECH]
[01:05:31.420 --> 01:05:41.420]   [BLANK_AUDIO]

