
[00:00:00.000 --> 00:00:04.320]   OK, hi everybody and welcome to practical deep learning for
[00:00:04.320 --> 00:00:05.400]   coders lesson 5.
[00:00:05.400 --> 00:00:11.120]   We're at a stage now where we're going to be getting
[00:00:11.120 --> 00:00:14.520]   deeper and deeper into the details of how these.
[00:00:14.520 --> 00:00:17.840]   Networks actually work last week we saw how to use
[00:00:17.840 --> 00:00:22.040]   a slightly lower level library than fast AI being
[00:00:22.040 --> 00:00:25.920]   hugging first transformers to train a pretty.
[00:00:27.000 --> 00:00:32.400]   Nice NLP model and today we're going to be.
[00:00:32.400 --> 00:00:35.280]   Going back to tabular data and we're going to be
[00:00:35.280 --> 00:00:38.640]   trying to build a tabular model actually from scratch.
[00:00:38.640 --> 00:00:40.960]   We're going to be able to couple of different types
[00:00:40.960 --> 00:00:42.560]   of tabular model from scratch.
[00:00:42.560 --> 00:00:45.580]   So the problem that I'm going to be working through
[00:00:45.580 --> 00:00:47.080]   is the Titanic problem,
[00:00:47.080 --> 00:00:50.280]   which if you remember back a couple of weeks is
[00:00:50.280 --> 00:00:53.720]   the data set that we looked at on Microsoft Excel
[00:00:53.800 --> 00:00:58.480]   and it has each row is one passenger on the Titanic.
[00:00:58.480 --> 00:01:00.960]   So this is a real world data set.
[00:01:00.960 --> 00:01:04.960]   Historic data set tells you both of that passengers survived.
[00:01:04.960 --> 00:01:06.760]   What class they were on in the ship,
[00:01:06.760 --> 00:01:08.680]   their sex, age, how many siblings,
[00:01:08.680 --> 00:01:10.280]   how many other family members,
[00:01:10.280 --> 00:01:12.200]   how much they spent in the fair,
[00:01:12.200 --> 00:01:15.840]   and whereabouts they embarked one of three different cities.
[00:01:15.840 --> 00:01:20.240]   And you might remember that we built a linear model.
[00:01:20.240 --> 00:01:25.280]   We then did the same thing using matrix multiplication,
[00:01:25.280 --> 00:01:29.760]   and we also created a very very simple neural network.
[00:01:29.760 --> 00:01:34.800]   Um. You know, Excel can do.
[00:01:34.800 --> 00:01:37.800]   Nearly everything we need as you saw to build
[00:01:37.800 --> 00:01:38.840]   a neural network,
[00:01:38.840 --> 00:01:43.240]   but it starts to get unwieldy and so that's why.
[00:01:43.240 --> 00:01:46.640]   People don't use Excel for neural networks in practice.
[00:01:46.640 --> 00:01:49.280]   Instead, we use the programming language like Python.
[00:01:50.280 --> 00:01:53.680]   So what we're going to do today is we're going to do
[00:01:53.680 --> 00:01:55.040]   the same thing.
[00:01:55.040 --> 00:01:57.800]   With Python.
[00:01:57.800 --> 00:02:01.280]   So we're going to start working through the linear model
[00:02:01.280 --> 00:02:04.360]   and neural net from scratch notebook.
[00:02:04.360 --> 00:02:08.120]   Which you can find on Kaggle.
[00:02:08.120 --> 00:02:10.400]   Or on the course repository.
[00:02:10.400 --> 00:02:12.840]   And today what we're going to do is we're going
[00:02:12.840 --> 00:02:15.400]   to work through the one in the clean folder,
[00:02:15.400 --> 00:02:17.360]   so both for fast book the book.
[00:02:17.360 --> 00:02:23.580]   And course 22 these lessons the clean folder.
[00:02:23.580 --> 00:02:29.480]   Contains all of our notebooks, but without.
[00:02:29.480 --> 00:02:32.860]   Any pros or any outputs, so here's what it looks
[00:02:32.860 --> 00:02:36.160]   like when I open up the linear model and neural
[00:02:36.160 --> 00:02:37.560]   net from scratch.
[00:02:37.560 --> 00:02:43.360]   In Jupiter. What I'm using here is.
[00:02:43.360 --> 00:02:47.240]   Paper space gradient, just I mentioned a couple of weeks ago,
[00:02:47.240 --> 00:02:49.740]   is what I'm going to be doing most things in.
[00:02:49.740 --> 00:02:52.200]   It looks a little bit different to the normal paper
[00:02:52.200 --> 00:02:52.560]   space gradient.
[00:02:52.560 --> 00:03:02.920]   Because the the default view for paper space gradient.
[00:03:02.920 --> 00:03:09.400]   At least as I do this course is there rather awkward.
[00:03:09.400 --> 00:03:10.520]   Notebook editor.
[00:03:14.080 --> 00:03:17.720]   Which at first glance has the same features as the
[00:03:17.720 --> 00:03:23.400]   the real Jupiter notebooks and Jupiter lab environments.
[00:03:23.400 --> 00:03:25.920]   But in practice are actually missing lots of things,
[00:03:25.920 --> 00:03:28.760]   so this is the. The normal paper space.
[00:03:28.760 --> 00:03:31.100]   So remember you have to click this button.
[00:03:31.100 --> 00:03:34.840]   Right and the only reason you might keep this window
[00:03:34.840 --> 00:03:38.060]   running is then you might go over here to the
[00:03:38.060 --> 00:03:41.760]   machine to remind yourself when you close the other tab
[00:03:41.760 --> 00:03:43.120]   to click stop machine.
[00:03:43.120 --> 00:03:44.640]   If you're using the free one,
[00:03:44.640 --> 00:03:46.640]   it doesn't matter too much.
[00:03:46.640 --> 00:03:48.200]   And also when I started,
[00:03:48.200 --> 00:03:51.120]   I make sure I've got something to set to shut
[00:03:51.120 --> 00:03:52.240]   down automatically. If case I forget.
[00:03:52.240 --> 00:03:58.120]   So other than that, we're going to.
[00:03:58.120 --> 00:04:01.060]   We can stay in this tab and because Jupiter,
[00:04:01.060 --> 00:04:05.100]   this is Jupiter lab that that that runs and you
[00:04:05.100 --> 00:04:07.800]   can always switch over.
[00:04:07.800 --> 00:04:11.960]   To classic Jupiter. Notebook if you want to.
[00:04:12.960 --> 00:04:16.060]   So given that they're kind of got tabs inside
[00:04:16.060 --> 00:04:16.640]   tabs,
[00:04:16.640 --> 00:04:20.000]   I normally maximize it at this point.
[00:04:20.000 --> 00:04:22.100]   And it's really good at it.
[00:04:22.100 --> 00:04:26.160]   Really helpful to know the keyboard shortcuts so control shift square bracket,
[00:04:26.160 --> 00:04:28.120]   right and left switch between tabs.
[00:04:28.120 --> 00:04:30.800]   That's one of the key things to know about.
[00:04:30.800 --> 00:04:36.000]   OK. So I've opened up the clean version of the.
[00:04:36.000 --> 00:04:39.000]   Linear model and neural net from scratch notebook.
[00:04:39.000 --> 00:04:42.360]   And so remember when you.
[00:04:42.360 --> 00:04:45.360]   Go back through the video kind of the second time
[00:04:45.360 --> 00:04:47.520]   or through the notebook a second time.
[00:04:47.520 --> 00:04:50.200]   This is generally what you want to be doing is
[00:04:50.200 --> 00:04:53.040]   going through the clean notebook and before you run
[00:04:53.040 --> 00:04:55.080]   each cell try to think about like,
[00:04:55.080 --> 00:04:56.880]   oh, what Jeremy say? Why are we doing this?
[00:04:56.880 --> 00:04:58.200]   What output would I expect?
[00:04:58.200 --> 00:05:01.160]   Make sure you get the output you'd expect and if
[00:05:01.160 --> 00:05:03.600]   you're not sure why something is the way it is.
[00:05:03.600 --> 00:05:05.760]   Try changing it and see what happens and then
[00:05:05.820 --> 00:05:09.360]   if you're still not sure why did that thing not work the
[00:05:09.360 --> 00:05:10.160]   way I expect.
[00:05:10.160 --> 00:05:13.640]   Search the forum. See if anybody's asked that question before and you
[00:05:13.640 --> 00:05:16.760]   can ask the question on the forum yourself if you're still not sure.
[00:05:16.760 --> 00:05:21.480]   Um. So as I think we've mentioned briefly before.
[00:05:21.480 --> 00:05:24.120]   I find it really nice to be able to use the
[00:05:24.120 --> 00:05:26.640]   same notebook both on Kaggle and off Kaggle.
[00:05:26.640 --> 00:05:29.500]   So most of my notebooks start with basically the same
[00:05:29.500 --> 00:05:30.040]   cell,
[00:05:30.040 --> 00:05:32.760]   which is something that just checks whether we're on Kaggle.
[00:05:32.760 --> 00:05:35.760]   So Kaggle sets an environment variable.
[00:05:35.760 --> 00:05:37.880]   So we can just check for it in that way.
[00:05:37.880 --> 00:05:39.200]   We know if we're on Kaggle.
[00:05:39.200 --> 00:05:41.400]   And so then if we are on Kaggle.
[00:05:41.400 --> 00:05:44.280]   You know a notebook that's part of a competition will
[00:05:44.280 --> 00:05:47.160]   already have the data downloaded and unzipped for you.
[00:05:47.160 --> 00:05:50.800]   Otherwise. If I haven't downloaded the data before,
[00:05:50.800 --> 00:05:55.760]   then I need to download it and unzip it OK so.
[00:05:55.760 --> 00:05:58.680]   Kaggle is a pip installable module,
[00:05:58.680 --> 00:06:01.600]   so you would type pip install Kaggle.
[00:06:01.600 --> 00:06:05.720]   Um. If you're not sure how to do that,
[00:06:05.720 --> 00:06:08.280]   you should check our.
[00:06:08.280 --> 00:06:10.600]   Deep dive lessons to see exactly the steps,
[00:06:10.600 --> 00:06:14.880]   but roughly speaking. You can use your console.
[00:06:14.880 --> 00:06:18.680]   Pip install and whatever you want to install.
[00:06:18.680 --> 00:06:20.440]   Or as we've seen before,
[00:06:20.440 --> 00:06:23.520]   you can do it directly in a notebook.
[00:06:23.520 --> 00:06:26.960]   By putting an explanation mark at the start.
[00:06:26.960 --> 00:06:28.840]   So that's going to run not Python,
[00:06:28.840 --> 00:06:32.720]   but a shell command.
[00:06:32.720 --> 00:06:36.520]   OK, so that's enough to ensure that we have the
[00:06:36.520 --> 00:06:40.040]   data downloaded and a variable called path that's pointing at it.
[00:06:40.040 --> 00:06:46.560]   Um, most of the time we're going to be using
[00:06:46.560 --> 00:06:48.320]   at least PyTorch and NumPy,
[00:06:48.320 --> 00:06:51.880]   so we import those so that they're available to Python,
[00:06:51.880 --> 00:06:53.720]   and when we're working with tabular data,
[00:06:53.720 --> 00:06:56.360]   as we talked about before.
[00:06:56.360 --> 00:06:58.680]   We're generally also going to want to use pandas,
[00:06:58.680 --> 00:07:02.160]   and it's really important that you're somewhat familiar with
[00:07:02.220 --> 00:07:05.140]   the kind of basic API of these.
[00:07:05.140 --> 00:07:08.840]   Three libraries and I've recommended Wes McKinney's book before,
[00:07:08.840 --> 00:07:11.220]   particularly for these ones.
[00:07:11.220 --> 00:07:13.940]   Um, one thing just by the way is that these things
[00:07:13.940 --> 00:07:16.540]   tend to assume you've got a very narrow screen,
[00:07:16.540 --> 00:07:18.200]   which is really annoying 'cause it always wraps things.
[00:07:18.200 --> 00:07:20.120]   So if you want to put these three lines as well,
[00:07:20.120 --> 00:07:22.620]   then it just makes sure that everything is going to use
[00:07:22.620 --> 00:07:24.020]   up the screen properly.
[00:07:24.020 --> 00:07:26.260]   OK, so as we've seen before,
[00:07:26.260 --> 00:07:30.160]   you can read a common separated values file with pandas.
[00:07:30.160 --> 00:07:32.860]   And you can take a look at the first few lines and
[00:07:32.860 --> 00:07:34.400]   last few lines and how big it is.
[00:07:34.400 --> 00:07:37.920]   And so here's the same thing as our spreadsheet.
[00:07:37.920 --> 00:07:44.360]   OK, so there's our data from the spreadsheet,
[00:07:44.360 --> 00:07:45.760]   and here it is as a data frame.
[00:07:45.760 --> 00:07:49.440]   So.
[00:07:49.440 --> 00:07:56.120]   If we go. Data frame dot is.
[00:07:58.040 --> 00:08:03.560]   NA. That returns a new data frame in which every
[00:08:03.560 --> 00:08:07.400]   column it tells us whether or not.
[00:08:07.400 --> 00:08:09.680]   That particular value is.
[00:08:09.680 --> 00:08:13.480]   Nan so Nan is not a number and most written norm.
[00:08:13.480 --> 00:08:16.480]   The most common reason you get that is because it
[00:08:16.480 --> 00:08:16.920]   was missing.
[00:08:16.920 --> 00:08:21.520]   OK, so a missing value is obviously not a number.
[00:08:21.520 --> 00:08:25.520]   So we in the Excel version we did something you should
[00:08:25.580 --> 00:08:29.200]   never usually do. We deleted all the rows with missing
[00:08:29.200 --> 00:08:33.020]   data. Just because in Excel it's a little bit harder
[00:08:33.020 --> 00:08:34.840]   to work with in pandas,
[00:08:34.840 --> 00:08:37.120]   it's very easy to work with. First of all.
[00:08:37.120 --> 00:08:41.520]   We can just sum up what I just showed you.
[00:08:41.520 --> 00:08:44.000]   Now if you call some on a data frame,
[00:08:44.000 --> 00:08:47.200]   it sums up each column, right? So you can see that
[00:08:47.200 --> 00:08:48.200]   there's kind of some.
[00:08:48.200 --> 00:08:54.040]   Small foundational concepts in pandas,
[00:08:54.040 --> 00:08:56.240]   which when you put them together, take you a long way.
[00:08:56.240 --> 00:08:58.280]   So one idea is this idea that you can park.
[00:08:58.280 --> 00:09:00.740]   You can call a method on a data frame and it
[00:09:00.740 --> 00:09:02.240]   calls it on every row.
[00:09:02.240 --> 00:09:06.040]   And then you can call a reduction on that and it
[00:09:06.040 --> 00:09:10.840]   reduces. Each column and so now we've got the total and.
[00:09:10.840 --> 00:09:13.840]   In Python and pandas and NumPy and PyTorch,
[00:09:13.840 --> 00:09:16.920]   you can treat a Boolean as a number and true
[00:09:16.920 --> 00:09:18.380]   will be one. False will be zero.
[00:09:18.380 --> 00:09:21.720]   So this is the number of missing values in each
[00:09:21.780 --> 00:09:26.200]   column. So we can see that cabin at 891 rows.
[00:09:26.200 --> 00:09:29.720]   It's nearly always empty. Age is empty a bit of
[00:09:29.720 --> 00:09:32.520]   the time, but it's almost never empty.
[00:09:32.520 --> 00:09:36.400]   So. If you remember from Excel.
[00:09:36.400 --> 00:09:39.040]   We need to multiply a coefficient.
[00:09:39.040 --> 00:09:41.680]   By each column, that's how we create a linear model.
[00:09:41.680 --> 00:09:45.140]   So how would you multiply a coefficient by a missing value?
[00:09:45.140 --> 00:09:49.180]   You can't. There's lots of ways of it's called imputing missing
[00:09:49.240 --> 00:09:53.480]   values, so replacing missing value with a number.
[00:09:53.480 --> 00:09:56.960]   There is yes, which always works is to replace missing
[00:09:56.960 --> 00:09:59.560]   values with the mode of a column.
[00:09:59.560 --> 00:10:02.620]   The mode is the most common value that works both
[00:10:02.620 --> 00:10:07.080]   the categorical variables is the most common category and continuous variables.
[00:10:07.080 --> 00:10:09.640]   That's the most common.
[00:10:09.640 --> 00:10:13.800]   Number so you can get the mode.
[00:10:13.800 --> 00:10:18.200]   By calling. DF dot mode, one thing that's a bit
[00:10:18.260 --> 00:10:21.480]   awkward is that if there's a tie for the mode,
[00:10:21.480 --> 00:10:24.760]   so there's more than one thing that's that's the most
[00:10:24.760 --> 00:10:27.280]   common, it's going to return multiple rows,
[00:10:27.280 --> 00:10:30.320]   so I need to return the zeroth row.
[00:10:30.320 --> 00:10:33.000]   So here is the mode of every column,
[00:10:33.000 --> 00:10:37.000]   so we can replace the missing values for age with 24
[00:10:37.000 --> 00:10:39.600]   and the missing values for cabin with.
[00:10:39.600 --> 00:10:43.980]   B96 be 98 and embarked with S.
[00:10:43.980 --> 00:10:47.880]   Um, I'll just mention in passing.
[00:10:47.880 --> 00:10:51.080]   I am not going to describe every single method we
[00:10:51.080 --> 00:10:54.240]   call in every single function we use.
[00:10:54.240 --> 00:10:56.240]   And that is not because.
[00:10:56.240 --> 00:10:59.360]   You're an idiot if you don't already know them.
[00:10:59.360 --> 00:11:03.060]   Nobody knows them all right, but I don't know which
[00:11:03.060 --> 00:11:06.240]   particular subset of them you don't know right?
[00:11:06.240 --> 00:11:09.880]   So let's assume just to pick a number at random
[00:11:09.880 --> 00:11:15.360]   that the average fast AI student knows 80% of the functions we call.
[00:11:15.360 --> 00:11:19.500]   And then. I could tell you.
[00:11:19.500 --> 00:11:22.160]   What every function is, in which case 80% of the
[00:11:22.160 --> 00:11:25.580]   time I'm wasting your time to already know.
[00:11:25.580 --> 00:11:28.020]   Um, or I could pick 20% of them at random,
[00:11:28.020 --> 00:11:30.400]   in which case I'm still not helping 'cause most of the
[00:11:30.400 --> 00:11:32.000]   time it's not the ones you don't know.
[00:11:32.000 --> 00:11:34.400]   My approach is that for the ones that are pretty common,
[00:11:34.400 --> 00:11:36.720]   I'm just not going to mention it at all because I'm
[00:11:36.720 --> 00:11:38.180]   assuming that you'll Google it right.
[00:11:38.180 --> 00:11:39.500]   So it's really important to know.
[00:11:39.500 --> 00:11:41.800]   So for example, if you don't know what I lock is.
[00:11:41.800 --> 00:11:43.920]   That's not a problem. It doesn't mean you're stupid,
[00:11:43.920 --> 00:11:45.960]   right? It just means you haven't used it yet.
[00:11:45.960 --> 00:11:48.660]   And you should Google it, right?
[00:11:48.660 --> 00:11:51.220]   So I mentioned in this particular case, you know,
[00:11:51.220 --> 00:11:53.800]   this is one of the most important pandas,
[00:11:53.800 --> 00:11:58.120]   methods because it gives you the row located at this index.
[00:11:58.120 --> 00:12:01.080]   I for index and lock for location.
[00:12:01.080 --> 00:12:04.280]   So this is a zero through.
[00:12:04.280 --> 00:12:07.180]   But yeah, I did kind of go through things a
[00:12:07.180 --> 00:12:08.200]   little bit quickly.
[00:12:08.200 --> 00:12:11.640]   On the assumption that students fast AI students are,
[00:12:11.640 --> 00:12:15.000]   you know, proactive curious people.
[00:12:15.000 --> 00:12:17.280]   And if you're not a proactive curious person,
[00:12:17.280 --> 00:12:19.820]   then you could either decide to become one for the
[00:12:19.820 --> 00:12:23.640]   purpose of this course or maybe this course isn't for you.
[00:12:23.640 --> 00:12:27.080]   Alright, so a data frame.
[00:12:27.080 --> 00:12:30.600]   Has a very convenient method called fill NA and that's
[00:12:30.600 --> 00:12:34.080]   going to replace the not a numbers with whatever I
[00:12:34.080 --> 00:12:35.240]   put here.
[00:12:35.240 --> 00:12:38.080]   And the nice thing about pandas is it kind of has
[00:12:38.080 --> 00:12:40.760]   this understanding that columns match to columns,
[00:12:40.760 --> 00:12:45.160]   it's going to take the the mode from each column.
[00:12:45.160 --> 00:12:48.300]   And match it to the same column in the data
[00:12:48.300 --> 00:12:51.560]   frame and fill in those missing values.
[00:12:51.560 --> 00:12:55.080]   Normally that would return a new data frame.
[00:12:55.080 --> 00:12:57.160]   Many things, including this one in pandas,
[00:12:57.160 --> 00:13:00.720]   have an in place argument that says actually modify the
[00:13:00.720 --> 00:13:01.500]   original one.
[00:13:01.500 --> 00:13:07.800]   And so if I run that. Now if I call .isna.sum.
[00:13:07.800 --> 00:13:11.880]   They're all zero, so that's like the world's simplest way.
[00:13:11.880 --> 00:13:15.480]   To get rid of missing values, OK, so.
[00:13:15.480 --> 00:13:19.200]   Why did we do it the world's simplest way?
[00:13:19.200 --> 00:13:24.400]   Because honestly. This doesn't make much difference most of
[00:13:24.400 --> 00:13:28.620]   the time, and so I'm not going to spend time the
[00:13:28.620 --> 00:13:33.200]   first time I go through and build a baseline model.
[00:13:33.200 --> 00:13:37.480]   Doing complicated things when I don't necessarily know that
[00:13:37.560 --> 00:13:39.920]   we need complicated things.
[00:13:39.920 --> 00:13:42.680]   And so imputing missing values is an example of
[00:13:42.680 --> 00:13:45.680]   something that most of the time this dumb way,
[00:13:45.680 --> 00:13:48.400]   which always works without even thinking about it,
[00:13:48.400 --> 00:13:51.760]   will be quite good enough, you know, for nearly all
[00:13:51.760 --> 00:13:52.560]   the time.
[00:13:52.560 --> 00:13:54.920]   So we keep things simple where we can.
[00:13:54.920 --> 00:13:57.200]   John, question.
[00:13:57.200 --> 00:14:00.320]   Jeremy, we've got a question on this topic.
[00:14:00.320 --> 00:14:03.880]   Javier is sort of commenting on the assumption involved in
[00:14:03.880 --> 00:14:07.440]   substituting with the mode and he's asking in your experience,
[00:14:07.440 --> 00:14:10.760]   what are the pros and cons of doing this versus,
[00:14:10.760 --> 00:14:14.120]   for example, discarding cabin or age as fields that we
[00:14:14.120 --> 00:14:15.620]   even train the model?
[00:14:15.620 --> 00:14:20.000]   Yeah, so I would certainly never throw him out, right?
[00:14:20.000 --> 00:14:22.880]   That there's just no reason to throw away data,
[00:14:22.880 --> 00:14:26.080]   and there's lots of reasons to not throw away data.
[00:14:26.080 --> 00:14:29.240]   So for example. When we use the fast AI library,
[00:14:29.240 --> 00:14:32.000]   which we'll use later, one of the things it does,
[00:14:32.000 --> 00:14:33.800]   which is actually a really good idea,
[00:14:33.800 --> 00:14:35.080]   is it creates a new column.
[00:14:35.080 --> 00:14:38.160]   For everything that's got missing values, which is Boolean,
[00:14:38.160 --> 00:14:41.000]   which is did that column have a missing value for
[00:14:41.000 --> 00:14:41.520]   this row?
[00:14:41.520 --> 00:14:44.440]   And so maybe it turns out that.
[00:14:44.440 --> 00:14:47.400]   Cabin being empty is a great predictor.
[00:14:47.400 --> 00:14:51.040]   So yeah, I don't throw out rows and I don't throw
[00:14:51.040 --> 00:14:51.600]   out columns.
[00:14:51.600 --> 00:14:55.800]   OK, so.
[00:14:55.800 --> 00:15:01.520]   It's helpful to understand a bit more about our data
[00:15:01.520 --> 00:15:03.960]   set and a really helpful.
[00:15:03.960 --> 00:15:05.460]   I've already imported this.
[00:15:05.460 --> 00:15:10.000]   A really helpful, you know, quick method and again,
[00:15:10.000 --> 00:15:12.400]   it's kind of nice to know like a few.
[00:15:12.400 --> 00:15:15.040]   Quick things you can do to get a picture of
[00:15:15.040 --> 00:15:18.040]   what's happening in your data is described.
[00:15:18.040 --> 00:15:20.440]   And so describe you can say OK,
[00:15:20.440 --> 00:15:23.520]   describe all the numeric variables.
[00:15:23.520 --> 00:15:25.960]   And that gives me a quick sense.
[00:15:25.960 --> 00:15:28.860]   Of what's going on here so we can see
[00:15:28.860 --> 00:15:31.400]   survived clearly is just zeros and ones.
[00:15:31.400 --> 00:15:34.080]   'cause all of the quartiles are zeros and ones.
[00:15:34.080 --> 00:15:36.080]   Looks like P class is 123.
[00:15:36.080 --> 00:15:43.040]   What else do we see fairs and interesting one,
[00:15:43.040 --> 00:15:45.880]   right? Lots of smallish numbers and one really big
[00:15:45.880 --> 00:15:48.040]   numbers are probably long tailed.
[00:15:48.040 --> 00:15:50.840]   So yeah, good to have a look at this to see
[00:15:50.840 --> 00:15:53.280]   what's what's going on for your numeric variables.
[00:15:53.280 --> 00:15:56.640]   So as I said, fair looks kind of interesting.
[00:15:56.640 --> 00:15:58.640]   To find out what's going on there,
[00:15:58.640 --> 00:16:00.720]   I would generally go with a histogram.
[00:16:00.720 --> 00:16:03.840]   So if you can't quite remember histogram is again,
[00:16:03.840 --> 00:16:07.040]   Google it, but in short it shows you for each
[00:16:07.040 --> 00:16:08.200]   amount of fair.
[00:16:08.200 --> 00:16:10.400]   How often does that fair appear?
[00:16:10.400 --> 00:16:12.920]   And it shows me here that the vast majority of
[00:16:12.920 --> 00:16:14.240]   fairs are less than $50,
[00:16:14.240 --> 00:16:17.080]   but there's a few right up here to 500.
[00:16:17.080 --> 00:16:20.480]   So this is what we call a long tailed distribution.
[00:16:20.480 --> 00:16:22.800]   A small number of really big.
[00:16:22.800 --> 00:16:24.800]   Values and lots of small ones.
[00:16:27.640 --> 00:16:31.440]   There are some types of model which do not like
[00:16:31.440 --> 00:16:33.600]   long tail distributions.
[00:16:33.600 --> 00:16:37.400]   Linear models are certainly one of them and neural nets
[00:16:37.400 --> 00:16:40.800]   are generally better behaved without them as well.
[00:16:40.800 --> 00:16:44.640]   Luckily there's a almost surefire way to turn a long tail
[00:16:44.640 --> 00:16:47.720]   distribution into a more reasonably centered distribution,
[00:16:47.720 --> 00:16:51.040]   and that is to take the log.
[00:16:51.040 --> 00:16:55.720]   We use logs a lot in machine learning.
[00:16:55.720 --> 00:16:59.920]   For those of you that haven't touched them since year
[00:16:59.920 --> 00:17:02.320]   10 math.
[00:17:02.320 --> 00:17:05.080]   It would be a very good time to like go to Khan
[00:17:05.080 --> 00:17:08.280]   Academy or something and remind yourself about what logs are
[00:17:08.280 --> 00:17:09.600]   and what they look like,
[00:17:09.600 --> 00:17:13.000]   'cause they're actually really, really important.
[00:17:13.000 --> 00:17:16.720]   But the basic shape of the log curve causes it to
[00:17:16.720 --> 00:17:20.340]   make you know really big numbers less really big and
[00:17:20.340 --> 00:17:24.020]   doesn't change really small numbers very much at all.
[00:17:24.020 --> 00:17:28.320]   So if we take the log now log of 0 is nan.
[00:17:28.320 --> 00:17:32.880]   So a useful trick is to just do log plus one.
[00:17:32.880 --> 00:17:36.240]   And in fact, there is a log P1 if you want to
[00:17:36.240 --> 00:17:38.440]   do that does the same thing.
[00:17:38.440 --> 00:17:41.040]   So if we look at the histogram of that,
[00:17:41.040 --> 00:17:42.320]   you can see it's much more.
[00:17:42.320 --> 00:17:46.080]   You know. Sensible now it's kind of centered and it
[00:17:46.080 --> 00:17:48.640]   doesn't have this big long tail,
[00:17:48.640 --> 00:17:50.980]   so that's pretty good so.
[00:17:50.980 --> 00:17:53.920]   We'll be using that column in the future.
[00:17:53.920 --> 00:17:57.300]   As a rule of thumb.
[00:17:57.300 --> 00:18:01.800]   Stuff like money or population things that kind of can grow
[00:18:01.800 --> 00:18:02.440]   exponentially,
[00:18:02.440 --> 00:18:05.160]   you very often want to take the log of.
[00:18:05.160 --> 00:18:07.640]   So if you have a column with $1 sign on it,
[00:18:07.640 --> 00:18:08.600]   that's a good sign.
[00:18:08.600 --> 00:18:12.040]   It might be something to take the log of.
[00:18:12.040 --> 00:18:13.120]   So there was another one here,
[00:18:13.120 --> 00:18:15.160]   which is we had a numeric.
[00:18:15.160 --> 00:18:16.880]   Which actually doesn't look numeric at all.
[00:18:16.880 --> 00:18:20.480]   It looks like it's actually categories.
[00:18:20.480 --> 00:18:24.480]   So pandas gives us a dot unique.
[00:18:24.480 --> 00:18:26.680]   And so we can see, yeah, they're just one,
[00:18:26.680 --> 00:18:28.240]   two and three or the levels of.
[00:18:28.240 --> 00:18:29.880]   P class that's their first class,
[00:18:29.880 --> 00:18:33.060]   second class or third class.
[00:18:33.060 --> 00:18:37.440]   We can also describe all the non numeric variables.
[00:18:37.440 --> 00:18:40.380]   And so we can see here that not surprisingly names
[00:18:40.380 --> 00:18:43.960]   are unique 'cause the count of names is the same
[00:18:43.960 --> 00:18:46.400]   as count unique is two sexes.
[00:18:46.400 --> 00:18:49.200]   681 different tickets.
[00:18:49.200 --> 00:18:54.400]   147 different cabins and three levels of embarked.
[00:18:54.400 --> 00:19:05.220]   So. We cannot multiply the letter S.
[00:19:05.220 --> 00:19:06.200]   By a coefficient.
[00:19:06.200 --> 00:19:12.000]   Or the word male.
[00:19:12.000 --> 00:19:13.040]   By a coefficient.
[00:19:13.040 --> 00:19:18.800]   So what do we do?
[00:19:19.800 --> 00:19:24.800]   What we do is we create something called dummy variables.
[00:19:24.800 --> 00:19:28.120]   Dummy variables are.
[00:19:28.120 --> 00:19:32.400]   And we can just go get dummies.
[00:19:32.400 --> 00:19:37.440]   A column that says, for example, is sex female,
[00:19:37.440 --> 00:19:40.120]   is sex male, is P class one, is P class two,
[00:19:40.120 --> 00:19:41.160]   is P class three.
[00:19:41.160 --> 00:19:43.480]   So for every possible level of every possible
[00:19:43.480 --> 00:19:45.720]   categorical variable, it's a Boolean column
[00:19:45.720 --> 00:19:48.920]   of did that row have that value of that column?
[00:19:48.920 --> 00:19:52.840]   So I think we've briefly talked about this before,
[00:19:52.840 --> 00:19:55.120]   that there's a couple of different ways we can do this.
[00:19:55.120 --> 00:19:58.440]   One is that for an N level categorical variable,
[00:19:58.440 --> 00:20:00.320]   we could use N minus one levels.
[00:20:00.320 --> 00:20:05.200]   In which case we also need a constant term in our model.
[00:20:05.200 --> 00:20:10.200]   Pandas by default shows all N levels,
[00:20:10.200 --> 00:20:13.080]   although you can pass an argument
[00:20:13.080 --> 00:20:14.380]   to change that if you want.
[00:20:15.220 --> 00:20:17.420]   Yeah, drop first.
[00:20:17.420 --> 00:20:21.260]   I kind of like having all of them sometimes
[00:20:21.260 --> 00:20:23.220]   'cause then you don't have to put in a constant term
[00:20:23.220 --> 00:20:24.940]   and it's a bit less annoying
[00:20:24.940 --> 00:20:28.500]   and it can be a bit easier to interpret,
[00:20:28.500 --> 00:20:31.580]   but I don't feel strongly about it either way.
[00:20:31.580 --> 00:20:35.660]   Okay, so here's a list of all of the columns
[00:20:35.660 --> 00:20:38.500]   that pandas added.
[00:20:38.500 --> 00:20:40.340]   I guess strictly speaking,
[00:20:40.340 --> 00:20:42.380]   I probably should have automated that, but nevermind.
[00:20:42.380 --> 00:20:43.900]   I just copied and pasted them.
[00:20:44.900 --> 00:20:49.900]   And so here are a few examples of the added columns.
[00:20:49.900 --> 00:20:52.620]   In Unix, pandas, lots of things like that.
[00:20:52.620 --> 00:20:56.100]   Head means the first few rows or the first few lines.
[00:20:56.100 --> 00:20:59.220]   So five by default and pandas.
[00:20:59.220 --> 00:21:02.980]   So here you can see they're never both male and female.
[00:21:02.980 --> 00:21:04.020]   They're never neither.
[00:21:04.020 --> 00:21:05.580]   They're always one or the other.
[00:21:05.580 --> 00:21:11.340]   All right, so with that now we've got numbers
[00:21:11.340 --> 00:21:13.260]   which we can multiply by coefficients.
[00:21:14.260 --> 00:21:18.260]   It's not gonna work for name, obviously,
[00:21:18.260 --> 00:21:23.180]   'cause we'd have 891 columns
[00:21:23.180 --> 00:21:25.420]   and all of them would be unique.
[00:21:25.420 --> 00:21:28.300]   So we'll ignore that for now.
[00:21:28.300 --> 00:21:31.860]   That doesn't mean it's have to always ignore it.
[00:21:31.860 --> 00:21:33.540]   And in fact, something I did do,
[00:21:33.540 --> 00:21:39.420]   something I did do on the forum topic
[00:21:39.420 --> 00:21:42.500]   'cause I made a list of some nice Titanic notebooks
[00:21:42.500 --> 00:21:43.340]   that I found.
[00:21:43.340 --> 00:21:49.340]   And quite a few of them really go hard on this name column.
[00:21:49.340 --> 00:21:53.340]   And in fact, one of them, yeah, this one,
[00:21:53.340 --> 00:21:58.680]   in what I believe is, yes,
[00:21:58.680 --> 00:22:00.660]   Christiot's first ever Kaggle notebook.
[00:22:00.660 --> 00:22:03.620]   He's now the number one ranked Kaggle notebook person
[00:22:03.620 --> 00:22:04.740]   in the world.
[00:22:04.740 --> 00:22:06.400]   So this is a very good start.
[00:22:06.400 --> 00:22:09.300]   He got a much better score than any model
[00:22:09.300 --> 00:22:11.180]   that we're gonna create in this course
[00:22:11.180 --> 00:22:13.100]   using only that column name.
[00:22:13.100 --> 00:22:17.300]   And basically, yeah, he came up with this
[00:22:17.300 --> 00:22:21.180]   simple little decision tree
[00:22:21.180 --> 00:22:24.700]   by recognizing all of the information
[00:22:24.700 --> 00:22:26.760]   that's in a name column.
[00:22:26.760 --> 00:22:29.980]   So yeah, we don't have to
[00:22:29.980 --> 00:22:35.900]   treat a big string of letters like this
[00:22:35.900 --> 00:22:37.220]   as a random big string of letters.
[00:22:37.220 --> 00:22:39.100]   We can use our domain expertise
[00:22:39.100 --> 00:22:40.940]   to recognize that things like Mr.
[00:22:41.500 --> 00:22:44.700]   have meaning and that people with the same surname
[00:22:44.700 --> 00:22:45.940]   might be in the same family
[00:22:45.940 --> 00:22:51.100]   and actually figure out quite a lot from that.
[00:22:51.100 --> 00:22:54.100]   But that's not something I'm gonna do.
[00:22:54.100 --> 00:22:55.600]   I'll let you look at those notebooks
[00:22:55.600 --> 00:22:57.860]   if you're interested in the feature engineering.
[00:22:57.860 --> 00:22:59.700]   And I do think that they're very interesting.
[00:22:59.700 --> 00:23:02.020]   So do check them out.
[00:23:02.020 --> 00:23:05.600]   Our focus today is on building a linear model
[00:23:05.600 --> 00:23:07.260]   and a neural net from scratch,
[00:23:07.260 --> 00:23:09.360]   not on tabular feature engineering,
[00:23:09.360 --> 00:23:11.860]   even though that's also a very important subject.
[00:23:11.860 --> 00:23:17.980]   Okay, so we talked about how matrix multiplication
[00:23:17.980 --> 00:23:20.460]   makes linear models much easier.
[00:23:20.460 --> 00:23:24.860]   And the other thing we did in Excel
[00:23:24.860 --> 00:23:27.060]   was element-wise multiplication.
[00:23:27.060 --> 00:23:29.940]   Both of those things are much easier
[00:23:29.940 --> 00:23:32.860]   if we use PyTorch instead of plain Python,
[00:23:32.860 --> 00:23:33.860]   or we could use NumPy.
[00:23:33.860 --> 00:23:35.860]   But I tend to just stick with PyTorch when I can
[00:23:35.860 --> 00:23:38.700]   'cause it's easier to learn one library than two.
[00:23:38.700 --> 00:23:40.360]   So I just do everything in PyTorch.
[00:23:40.360 --> 00:23:44.100]   I almost never touch NumPy nowadays.
[00:23:44.100 --> 00:23:48.020]   They're both great, but they do everything each other does
[00:23:48.020 --> 00:23:50.980]   except PyTorch also does differentiation and GPUs.
[00:23:50.980 --> 00:23:52.540]   So why not just learn PyTorch?
[00:23:52.540 --> 00:23:59.040]   So to turn a column into something that I can do
[00:23:59.040 --> 00:24:06.800]   PyTorch calculations on, I have to turn it into a tensor.
[00:24:06.800 --> 00:24:11.400]   So a tensor is just what NumPy calls an array.
[00:24:11.400 --> 00:24:15.780]   It's what mathematicians will call either a vector
[00:24:15.780 --> 00:24:19.820]   or a matrix, or once we go to higher ranks,
[00:24:19.820 --> 00:24:22.420]   mathematicians and physicists just call them tensors.
[00:24:22.420 --> 00:24:27.540]   In fact, this idea originally in computer science
[00:24:27.540 --> 00:24:31.860]   came from a notation developed in the '50s called APL,
[00:24:31.860 --> 00:24:34.580]   which was turned into a programming language in the '60s
[00:24:34.580 --> 00:24:35.940]   by a guy called Ken Iverson.
[00:24:35.940 --> 00:24:39.420]   And Ken Iverson actually came up with this idea
[00:24:39.420 --> 00:24:44.740]   from, he said, his time doing tensor analysis in physics.
[00:24:44.740 --> 00:24:48.740]   So these areas are very related.
[00:24:48.740 --> 00:24:52.020]   So we can turn the survived column into a tensor
[00:24:52.020 --> 00:24:54.260]   and we'll call that tensor our dependent variable.
[00:24:54.260 --> 00:24:56.740]   That's the thing we're trying to predict.
[00:24:56.740 --> 00:24:59.500]   Okay, so now we need some independent variables.
[00:24:59.500 --> 00:25:04.420]   So our independent variables are age, siblings,
[00:25:05.420 --> 00:25:10.420]   that one is, oh yeah, number of other family members,
[00:25:10.420 --> 00:25:17.060]   the log affair that we just created
[00:25:17.060 --> 00:25:19.140]   plus all of those dummy columns we added.
[00:25:19.140 --> 00:25:25.500]   And so we can now grab those values
[00:25:25.500 --> 00:25:28.820]   and turn them into a tensor
[00:25:28.820 --> 00:25:31.340]   and we have to make sure they're floats.
[00:25:31.340 --> 00:25:33.380]   We want them all to be the same data type
[00:25:33.380 --> 00:25:36.740]   and PyTorch wants things to be floats
[00:25:36.740 --> 00:25:38.740]   if you're gonna multiply things together.
[00:25:38.740 --> 00:25:40.840]   So there we are.
[00:25:40.840 --> 00:25:49.740]   And so one of the most important attributes of a tensor,
[00:25:49.740 --> 00:25:52.140]   probably the most important attribute is its shape,
[00:25:52.140 --> 00:25:54.500]   which is how many rows does it have
[00:25:54.500 --> 00:25:56.300]   and how many columns does it have?
[00:25:56.300 --> 00:26:00.260]   The length of the shape
[00:26:00.260 --> 00:26:05.260]   is called its rank.
[00:26:05.260 --> 00:26:06.660]   That's the rank of the tensor.
[00:26:06.660 --> 00:26:10.980]   It's the number of dimensions or axes that it has.
[00:26:10.980 --> 00:26:15.980]   So a vector is rank one, a matrix is rank two,
[00:26:15.980 --> 00:26:22.300]   a scalar is rank zero and so forth.
[00:26:22.300 --> 00:26:28.300]   I try not to use too much jargon,
[00:26:28.300 --> 00:26:31.180]   but there's some pieces of jargon that are really important
[00:26:31.180 --> 00:26:34.260]   'cause like otherwise you're gonna have to say
[00:26:34.260 --> 00:26:36.100]   the length of the shape again and again.
[00:26:36.100 --> 00:26:37.700]   It's much easier to say rank.
[00:26:37.700 --> 00:26:40.020]   So we'll use that word a lot.
[00:26:40.020 --> 00:26:45.060]   So a table is a rank two tensor.
[00:26:45.060 --> 00:26:51.260]   Okay, so we've now got the data in good shape.
[00:26:51.260 --> 00:26:53.380]   Here's our independent variables
[00:26:53.380 --> 00:26:55.980]   and we've got our dependent variable.
[00:26:55.980 --> 00:26:59.860]   So we can now go ahead and do exactly what we did in Excel,
[00:26:59.860 --> 00:27:05.460]   which is to multiply our rows of data
[00:27:05.460 --> 00:27:09.260]   by some coefficients.
[00:27:09.260 --> 00:27:13.380]   And remember to start with we create random coefficients.
[00:27:13.380 --> 00:27:18.180]   So we're gonna need one coefficient for each column.
[00:27:18.180 --> 00:27:20.740]   Now in Excel we also had a constant,
[00:27:20.740 --> 00:27:24.580]   but in our case now we've got every column,
[00:27:24.580 --> 00:27:26.060]   every level in our dummy variables,
[00:27:26.060 --> 00:27:27.660]   so we don't need a constant.
[00:27:27.660 --> 00:27:30.460]   So the number of coefficients we need
[00:27:30.460 --> 00:27:32.980]   is equal to the shape of the independent variables
[00:27:32.980 --> 00:27:36.260]   and it's the index one element.
[00:27:36.260 --> 00:27:37.780]   That's the number of columns.
[00:27:37.780 --> 00:27:40.460]   So that's how many coefficients we want.
[00:27:40.460 --> 00:27:45.100]   So we can now ask PyTorch to give us some random numbers
[00:27:45.100 --> 00:27:46.300]   and cof of them.
[00:27:46.300 --> 00:27:49.580]   They're between zero and one.
[00:27:49.580 --> 00:27:52.020]   So if we subtract a half then they'll be centered.
[00:27:53.580 --> 00:27:54.420]   And there we go.
[00:27:54.420 --> 00:27:59.100]   Before I do that, I set the seed.
[00:27:59.100 --> 00:28:03.700]   What that means is in computers,
[00:28:03.700 --> 00:28:07.460]   computers in general cannot create truly random numbers.
[00:28:07.460 --> 00:28:12.580]   Instead they can calculate a sequence of numbers
[00:28:12.580 --> 00:28:14.420]   that behave in a random like way.
[00:28:14.420 --> 00:28:17.140]   That's actually good for us
[00:28:17.140 --> 00:28:19.540]   because often in my teaching I like to be able to say,
[00:28:19.540 --> 00:28:22.060]   you know, in the pros, oh look, that was two,
[00:28:22.060 --> 00:28:23.780]   now it's three or whatever.
[00:28:23.780 --> 00:28:26.860]   And if I was using really random numbers,
[00:28:26.860 --> 00:28:29.500]   then I couldn't do that because it'd be different each time.
[00:28:29.500 --> 00:28:33.540]   So this makes my results reproducible.
[00:28:33.540 --> 00:28:34.700]   That means if you run it,
[00:28:34.700 --> 00:28:37.540]   you'll get the same random numbers as I do
[00:28:37.540 --> 00:28:42.540]   by saying start the pseudo random sequence with this number.
[00:28:42.540 --> 00:28:46.780]   I mentioned in passing,
[00:28:46.780 --> 00:28:51.420]   a lot of people are very, very into reproducible results.
[00:28:51.420 --> 00:28:54.380]   They think it's really important to always do this.
[00:28:54.380 --> 00:28:58.020]   I strongly disagree with that.
[00:28:58.020 --> 00:29:01.420]   In my opinion, an important part of understanding your data
[00:29:01.420 --> 00:29:04.820]   is understanding how much it varies from run to run.
[00:29:04.820 --> 00:29:09.540]   So if I'm not teaching and wanting to be able to write things
[00:29:09.540 --> 00:29:11.900]   about these pseudo random numbers,
[00:29:11.900 --> 00:29:15.740]   I almost never use a manual seed.
[00:29:15.740 --> 00:29:17.620]   Instead I like to run things a few times
[00:29:17.620 --> 00:29:19.140]   and get an intuitive sense of like,
[00:29:19.140 --> 00:29:21.700]   oh, this is like very, very stable.
[00:29:21.700 --> 00:29:23.500]   Or, oh, this is all over the place.
[00:29:23.500 --> 00:29:25.100]   I'm getting an intuitive understanding
[00:29:25.100 --> 00:29:26.820]   of how your data behaves
[00:29:26.820 --> 00:29:29.020]   and your model behaves is really important.
[00:29:29.020 --> 00:29:34.020]   Now here's one of the coolest lines of code
[00:29:34.020 --> 00:29:36.300]   you'll ever see.
[00:29:36.300 --> 00:29:39.380]   I know it doesn't look like much,
[00:29:39.380 --> 00:29:43.220]   but think about what it's doing.
[00:29:43.220 --> 00:29:45.620]   Yeah, that'll do.
[00:29:47.140 --> 00:29:52.140]   Okay, so we've multiplied a matrix by a vector.
[00:29:52.140 --> 00:29:56.260]   Now that's pretty interesting.
[00:29:56.260 --> 00:29:58.020]   Now mathematicians amongst you will know
[00:29:58.020 --> 00:30:02.380]   that you can certainly do a matrix vector product,
[00:30:02.380 --> 00:30:05.380]   but that's not what we've done here at all.
[00:30:05.380 --> 00:30:09.020]   We've used element-wise multiplication.
[00:30:09.020 --> 00:30:11.980]   So normally if we did the element-wise multiplication
[00:30:11.980 --> 00:30:15.940]   of two vectors, it would multiply element one
[00:30:15.940 --> 00:30:18.340]   with element one, element two with element two,
[00:30:18.340 --> 00:30:22.260]   and so forth and create a vector of the same size output.
[00:30:22.260 --> 00:30:26.780]   But here we've done a matrix times a vector.
[00:30:26.780 --> 00:30:27.700]   How does that work?
[00:30:27.700 --> 00:30:32.220]   This is using the incredibly powerful technique
[00:30:32.220 --> 00:30:33.660]   of broadcasting.
[00:30:33.660 --> 00:30:36.780]   And broadcasting again comes from APL,
[00:30:36.780 --> 00:30:38.540]   a notation invented in the '50s
[00:30:38.540 --> 00:30:40.980]   and a programming language developed in the '60s.
[00:30:40.980 --> 00:30:45.140]   And it's got a number of benefits.
[00:30:45.140 --> 00:30:47.580]   Basically what it's gonna do is it's gonna take
[00:30:47.580 --> 00:30:51.540]   each coefficient and multiply them in turn
[00:30:51.540 --> 00:30:55.380]   by every row in our matrix.
[00:30:55.380 --> 00:31:00.380]   So if you look at the shape of our independent variable
[00:31:00.380 --> 00:31:06.060]   and the shape of our coefficients,
[00:31:06.060 --> 00:31:08.620]   you can see that each one of these coefficients
[00:31:08.620 --> 00:31:13.260]   can be multiplied by each of these 891 values in turn.
[00:31:14.020 --> 00:31:17.260]   And so the reason we call it broadcasting
[00:31:17.260 --> 00:31:20.460]   is it's as if this is 891 columns by 12,
[00:31:20.460 --> 00:31:22.060]   rows by 12 columns.
[00:31:22.060 --> 00:31:27.060]   It's as if this was broadcast 891 times.
[00:31:27.060 --> 00:31:30.900]   It's as if we had a loop, looping 891 times
[00:31:30.900 --> 00:31:33.220]   and doing coefficients times row zero,
[00:31:33.220 --> 00:31:34.580]   coefficients times row one,
[00:31:34.580 --> 00:31:37.300]   coefficients times row zero, two, and so forth,
[00:31:37.300 --> 00:31:38.860]   which is exactly what we want.
[00:31:39.900 --> 00:31:44.340]   Now, reasons to use broadcasting,
[00:31:44.340 --> 00:31:47.040]   obviously the code is much more concise.
[00:31:47.040 --> 00:31:52.340]   It looks more like math rather than clunky programming
[00:31:52.340 --> 00:31:54.660]   with lots of boilerplate, so that's good.
[00:31:54.660 --> 00:32:00.700]   Also, that broadcasting all happened in optimized C code.
[00:32:00.700 --> 00:32:04.180]   And if in fact it's being done on a GPU,
[00:32:04.180 --> 00:32:08.260]   it's being done in optimized GPU assembler, CUDA code.
[00:32:09.260 --> 00:32:12.140]   It's gonna run very, very fast indeed.
[00:32:12.140 --> 00:32:14.020]   And this is a trick of why we can use
[00:32:14.020 --> 00:32:16.580]   a so-called slow language like Python
[00:32:16.580 --> 00:32:19.380]   to do very fast big models
[00:32:19.380 --> 00:32:21.580]   is because a single line of code like this
[00:32:21.580 --> 00:32:26.100]   can run very quickly on optimized hardware
[00:32:26.100 --> 00:32:27.340]   on lots and lots of data.
[00:32:27.340 --> 00:32:34.100]   The rules of broadcasting are a little bit subtle
[00:32:34.100 --> 00:32:35.780]   and important to know.
[00:32:35.780 --> 00:32:37.780]   And so I would strongly encourage you
[00:32:37.780 --> 00:32:42.780]   to Google NumPy broadcasting rules
[00:32:42.780 --> 00:32:52.840]   and see exactly how they work.
[00:32:52.840 --> 00:32:54.940]   But the kind of intuitive understanding of them,
[00:32:54.940 --> 00:32:56.220]   hopefully you'll get pretty quickly,
[00:32:56.220 --> 00:32:57.900]   which is generally speaking,
[00:32:57.900 --> 00:33:02.220]   you can kind of, as long as the last axes match,
[00:33:02.220 --> 00:33:04.100]   it'll broadcast over those axes.
[00:33:04.100 --> 00:33:06.980]   You can broadcast a rank three thing
[00:33:06.980 --> 00:33:10.660]   with a rank one thing or a simple version
[00:33:10.660 --> 00:33:15.660]   would be tensor one, two, three times two.
[00:33:15.660 --> 00:33:23.380]   So broadcast a scalar over a vector.
[00:33:23.380 --> 00:33:33.620]   That's exactly what you would expect.
[00:33:33.620 --> 00:33:35.540]   So it's copying effectively that two
[00:33:35.540 --> 00:33:38.420]   into each of these spots, multiplying them together,
[00:33:38.420 --> 00:33:40.940]   but it doesn't use it up any memory to do that.
[00:33:40.940 --> 00:33:43.060]   It's kind of a virtual copying if you like.
[00:33:43.060 --> 00:33:46.600]   So this line of code independence by coefficients
[00:33:46.600 --> 00:33:47.740]   is very, very important.
[00:33:47.740 --> 00:33:51.240]   And it's the key step that we wanted to take,
[00:33:51.240 --> 00:33:54.160]   which is now we know exactly what happens
[00:33:54.160 --> 00:33:56.060]   when we multiply the coefficients in.
[00:33:56.060 --> 00:33:58.900]   And if you remember back to Excel,
[00:33:58.900 --> 00:34:03.900]   we did that product and then in Excel,
[00:34:03.900 --> 00:34:12.820]   there's a sum product, we then added it all together
[00:34:12.820 --> 00:34:14.180]   because that's what a linear model is.
[00:34:14.180 --> 00:34:18.620]   It's the coefficients times the values added together.
[00:34:18.620 --> 00:34:21.120]   So we're now gonna need to add those together.
[00:34:21.120 --> 00:34:26.740]   But before we do that, if we did add up this row,
[00:34:28.260 --> 00:34:30.500]   you can see that the very first value
[00:34:30.500 --> 00:34:33.780]   has a very large magnitude and all the other ones are small.
[00:34:33.780 --> 00:34:37.660]   Same with row two, same with row three, same with row four.
[00:34:37.660 --> 00:34:39.100]   What's going on here?
[00:34:39.100 --> 00:34:43.240]   Well, what's going on is that the very first column was age.
[00:34:43.240 --> 00:34:47.500]   And age is much bigger than any of the other columns.
[00:34:47.500 --> 00:34:52.700]   It's not the end of the world, but it's not ideal, right?
[00:34:52.700 --> 00:34:57.700]   Because it means that a coefficient of say 0.5 times age
[00:34:58.060 --> 00:35:00.020]   means something very different to a coefficient
[00:35:00.020 --> 00:35:04.240]   of say 0.5 times log fair, right?
[00:35:04.240 --> 00:35:07.820]   And that means that that random coefficient we start with,
[00:35:07.820 --> 00:35:09.100]   it's gonna mean very different things
[00:35:09.100 --> 00:35:09.940]   for very different columns.
[00:35:09.940 --> 00:35:12.580]   And that's gonna make it really hard to optimize.
[00:35:12.580 --> 00:35:13.860]   So we would like all the columns
[00:35:13.860 --> 00:35:15.300]   to have about the same range.
[00:35:15.300 --> 00:35:21.500]   So what we could do as we did in Excel
[00:35:21.500 --> 00:35:26.020]   is to divide them by the maximum.
[00:35:26.020 --> 00:35:27.780]   So the maximum, so we did it for age
[00:35:27.780 --> 00:35:29.040]   and we also did it for fair.
[00:35:29.040 --> 00:35:30.540]   In this case, I didn't use log.
[00:35:30.540 --> 00:35:39.540]   So we can get the max of each row by calling .max.
[00:35:39.540 --> 00:35:41.640]   And you can pass in a dimension.
[00:35:41.640 --> 00:35:42.900]   Do you want the maximum of the rows
[00:35:42.900 --> 00:35:45.040]   or the maximum of the columns?
[00:35:45.040 --> 00:35:47.540]   We want the maximum over the rows.
[00:35:47.540 --> 00:35:49.080]   So we pass in dimension zero.
[00:35:49.080 --> 00:35:53.580]   So those different parts of the shape
[00:35:53.580 --> 00:35:56.580]   are called either axes or dimensions.
[00:35:56.580 --> 00:35:58.340]   PyTorch calls them dimensions.
[00:35:58.340 --> 00:36:01.180]   So that's gonna give us the maximum of each row.
[00:36:01.180 --> 00:36:04.620]   And if you look at the docs for PyTorch's max function,
[00:36:04.620 --> 00:36:06.500]   it'll tell you it returns two things,
[00:36:06.500 --> 00:36:08.900]   the actual value of each maximum
[00:36:08.900 --> 00:36:12.360]   and the index of which row it was.
[00:36:12.360 --> 00:36:14.220]   We want the values.
[00:36:14.220 --> 00:36:16.120]   So now, thanks to broadcasting,
[00:36:16.120 --> 00:36:19.140]   we can just say take the independent variables
[00:36:19.140 --> 00:36:21.660]   and divide them by the vector of values.
[00:36:21.660 --> 00:36:25.220]   Again, we've got a matrix and a vector.
[00:36:25.220 --> 00:36:28.380]   And so this is gonna do an element-wise division
[00:36:28.380 --> 00:36:33.380]   of each row of this divided by this vector.
[00:36:33.380 --> 00:36:38.060]   Again, in a very optimized way.
[00:36:38.060 --> 00:36:42.900]   So if we now look at our normalized independent variables
[00:36:42.900 --> 00:36:44.620]   by the coefficients,
[00:36:44.620 --> 00:36:47.660]   you can see they're all pretty similar values.
[00:36:47.660 --> 00:36:49.340]   So that's good.
[00:36:49.340 --> 00:36:50.980]   There's lots of different ways of normalizing,
[00:36:50.980 --> 00:36:52.300]   but the main ones you'll come across
[00:36:52.300 --> 00:36:55.940]   is either dividing by the maximum or subtracting the mean
[00:36:55.940 --> 00:36:57.860]   and dividing by the standard deviation.
[00:36:57.860 --> 00:37:00.660]   It normally doesn't matter too much.
[00:37:00.660 --> 00:37:04.500]   'Cause I'm lazy, I just picked the easier one.
[00:37:04.500 --> 00:37:06.280]   And being lazy and picking the easier one
[00:37:06.280 --> 00:37:07.980]   is a very good plan in my opinion.
[00:37:07.980 --> 00:37:10.500]   So now that we've conceived that
[00:37:10.500 --> 00:37:12.900]   multiplying them together is working pretty well,
[00:37:12.900 --> 00:37:14.740]   we can now add them up.
[00:37:14.740 --> 00:37:17.780]   And now we want to add up over the columns.
[00:37:20.900 --> 00:37:22.700]   And that would give us predictions.
[00:37:22.700 --> 00:37:24.260]   Now obviously, just like in Excel,
[00:37:24.260 --> 00:37:26.900]   when we started out, they're not useful predictions
[00:37:26.900 --> 00:37:28.220]   'cause they're random coefficients,
[00:37:28.220 --> 00:37:30.500]   but they are predictions nonetheless.
[00:37:30.500 --> 00:37:32.100]   And here's the first 10 of them.
[00:37:32.100 --> 00:37:39.220]   So then remember, we want to use gradient descent
[00:37:39.220 --> 00:37:42.240]   to try to make these better.
[00:37:42.240 --> 00:37:45.700]   So to do gradient descent, we need a loss.
[00:37:45.700 --> 00:37:48.780]   The loss is the measure of how good or bad
[00:37:48.780 --> 00:37:50.060]   are these coefficients.
[00:37:50.900 --> 00:37:55.980]   My favorite loss function as a kind of like,
[00:37:55.980 --> 00:37:58.220]   don't think about it, just chuck something out there,
[00:37:58.220 --> 00:38:00.020]   is the mean absolute value.
[00:38:00.020 --> 00:38:05.020]   And here it is, torch.absoluteValue of the error,
[00:38:05.020 --> 00:38:07.300]   difference, take the mean.
[00:38:07.300 --> 00:38:12.420]   And often stuff like this, you'll see people
[00:38:12.420 --> 00:38:17.340]   will use pre-written mean absolute error functions,
[00:38:17.340 --> 00:38:20.620]   which is also fine, but I quite like to write it out
[00:38:20.620 --> 00:38:22.880]   'cause I can see exactly what's going on.
[00:38:22.880 --> 00:38:26.140]   No confusion, no chance of misunderstanding.
[00:38:26.140 --> 00:38:29.680]   So those are all the steps I'm gonna need
[00:38:29.680 --> 00:38:34.680]   to create coefficients, run a linear model,
[00:38:34.680 --> 00:38:37.420]   and get its loss.
[00:38:37.420 --> 00:38:39.700]   So what I like to do in my notebooks,
[00:38:39.700 --> 00:38:41.980]   like not just for teaching, but all the time,
[00:38:41.980 --> 00:38:45.080]   is to like do everything step-by-step manually,
[00:38:45.080 --> 00:38:48.380]   and then just copy and paste the steps into a function.
[00:38:48.380 --> 00:38:50.020]   So here's my calc_pretz function,
[00:38:50.020 --> 00:38:52.480]   is exactly what I just did, right?
[00:38:52.480 --> 00:38:55.740]   Here's my calc_loss function, exactly what I just did.
[00:38:55.740 --> 00:39:00.980]   And that way, a lot of people like go back
[00:39:00.980 --> 00:39:03.740]   and delete all their explorations,
[00:39:03.740 --> 00:39:05.580]   or they like do them in a different notebook,
[00:39:05.580 --> 00:39:07.220]   or they're like working in an IDE,
[00:39:07.220 --> 00:39:11.620]   they'll go and do it in some line-oriented repo, whatever.
[00:39:11.620 --> 00:39:14.520]   But if you think about the benefits of keeping it here,
[00:39:15.060 --> 00:39:16.580]   when you come back to it in six months,
[00:39:16.580 --> 00:39:18.260]   you'll see exactly why you did what you did
[00:39:18.260 --> 00:39:19.100]   and how we got there,
[00:39:19.100 --> 00:39:21.580]   or if you're showing it to your boss or your colleague,
[00:39:21.580 --> 00:39:24.300]   you can see exactly what's happening,
[00:39:24.300 --> 00:39:26.120]   what does each step look like.
[00:39:26.120 --> 00:39:28.460]   I think this is really very helpful indeed.
[00:39:28.460 --> 00:39:32.800]   I know not many people code that way,
[00:39:32.800 --> 00:39:37.460]   but I feel strongly that it's a huge productivity win
[00:39:37.460 --> 00:39:39.060]   to individuals and teams.
[00:39:40.700 --> 00:39:45.260]   So remember from our gradient descent from scratch
[00:39:45.260 --> 00:39:48.300]   that the one bit we don't wanna do from scratch
[00:39:48.300 --> 00:39:49.380]   is calculating derivatives,
[00:39:49.380 --> 00:39:51.820]   'cause it's just menial and boring.
[00:39:51.820 --> 00:39:53.540]   So to get PyTorch to do it for us,
[00:39:53.540 --> 00:39:55.800]   you have to say, well, what things do you want derivatives for?
[00:39:55.800 --> 00:39:58.400]   And of course, we want it for the coefficients.
[00:39:58.400 --> 00:40:00.940]   So then we have to say requires_grad.
[00:40:00.940 --> 00:40:03.380]   And remember, very important, in PyTorch,
[00:40:03.380 --> 00:40:05.200]   if there's an underscore at the end,
[00:40:05.200 --> 00:40:06.700]   that's an in-place operation.
[00:40:06.700 --> 00:40:08.900]   So this is actually gonna change coefficients.
[00:40:10.420 --> 00:40:12.220]   It also returns them, right?
[00:40:12.220 --> 00:40:14.260]   But it also changes them in place.
[00:40:14.260 --> 00:40:16.380]   So now we've got exactly the same numbers as before,
[00:40:16.380 --> 00:40:18.740]   but with requires_grad turned on.
[00:40:18.740 --> 00:40:20.640]   So now when we calculate our loss,
[00:40:20.640 --> 00:40:23.240]   that doesn't do any other calculations,
[00:40:23.240 --> 00:40:26.400]   but what it does store is a gradient function.
[00:40:26.400 --> 00:40:29.400]   It's the function that Python has remembered
[00:40:29.400 --> 00:40:31.900]   that it would have to do to undo those steps
[00:40:31.900 --> 00:40:33.540]   to get back to the gradient.
[00:40:33.540 --> 00:40:36.260]   And to say, oh, please actually call
[00:40:36.260 --> 00:40:40.060]   that backward gradient function, you call backward.
[00:40:40.060 --> 00:40:45.060]   And at that point, it sticks into a .grad attribute,
[00:40:45.060 --> 00:40:49.900]   the coefficients gradients.
[00:40:49.900 --> 00:40:54.900]   So this tells us that if we increased the age coefficient,
[00:40:54.900 --> 00:40:57.780]   the loss would go down.
[00:40:57.780 --> 00:41:00.740]   So therefore we should do that, right?
[00:41:00.740 --> 00:41:07.460]   So since negative means increasing this
[00:41:07.460 --> 00:41:09.540]   would decrease the loss, that means we need to,
[00:41:09.540 --> 00:41:11.460]   if you remember back to the gradient descent
[00:41:11.460 --> 00:41:12.700]   from scratch notebook,
[00:41:12.700 --> 00:41:17.700]   we need to subtract the coefficients times the learning rate.
[00:41:17.700 --> 00:41:22.420]   So we haven't got any particular ideas yet
[00:41:22.420 --> 00:41:23.580]   of how to set the learning rate.
[00:41:23.580 --> 00:41:25.620]   So for now, I just pick a, just try a few
[00:41:25.620 --> 00:41:27.580]   and still find out what works best.
[00:41:27.580 --> 00:41:29.820]   In this case, I found .1 worked pretty well.
[00:41:29.820 --> 00:41:34.140]   So I now subtract.
[00:41:34.140 --> 00:41:38.540]   So again, this is sub_, so subtract in place
[00:41:38.540 --> 00:41:42.060]   from the coefficients, the gradient times the learning rate.
[00:41:42.060 --> 00:41:45.740]   And so the loss has gone down.
[00:41:45.740 --> 00:41:49.260]   That's great from .54 to .52.
[00:41:49.260 --> 00:41:53.300]   So there is one step.
[00:41:53.300 --> 00:41:56.980]   So we've now got everything we need
[00:41:56.980 --> 00:41:59.380]   to train a linear model.
[00:41:59.380 --> 00:42:06.780]   So let's do it.
[00:42:06.780 --> 00:42:10.260]   Now, as we discussed last week,
[00:42:10.260 --> 00:42:12.140]   to see whether your model's any good,
[00:42:12.140 --> 00:42:14.980]   it's important that you split your data
[00:42:14.980 --> 00:42:16.580]   into training and validation.
[00:42:16.580 --> 00:42:19.340]   For the Titanic dataset,
[00:42:19.340 --> 00:42:22.660]   it's actually pretty much fine to use a random split
[00:42:22.660 --> 00:42:24.980]   because back when my friend Margit and I
[00:42:24.980 --> 00:42:26.380]   actually created this competition
[00:42:26.380 --> 00:42:27.780]   for Kaggle many years ago,
[00:42:27.780 --> 00:42:31.300]   that's basically what we did, if I remember correctly.
[00:42:31.300 --> 00:42:32.860]   So we can split them randomly
[00:42:34.460 --> 00:42:37.260]   into a training set and a validation set.
[00:42:37.260 --> 00:42:40.220]   So we're just gonna use fast_ai for that.
[00:42:40.220 --> 00:42:43.260]   It's very easy to do it manually
[00:42:43.260 --> 00:42:45.420]   with NumPy or PyTorch.
[00:42:45.420 --> 00:42:48.020]   You can use scikit_learns, train_test_split.
[00:42:48.020 --> 00:42:51.060]   I'm using fast_ai's here,
[00:42:51.060 --> 00:42:53.380]   partly because it's easy just to remember
[00:42:53.380 --> 00:42:55.820]   one way to do things and this works everywhere.
[00:42:55.820 --> 00:42:57.460]   And partly because in the next notebook,
[00:42:57.460 --> 00:42:59.700]   we're gonna be seeing how to do more stuff in fast_ai.
[00:42:59.700 --> 00:43:02.260]   So I wanna make sure we have exactly the same split.
[00:43:02.460 --> 00:43:07.460]   So those are a list of the indexes of the rows
[00:43:07.460 --> 00:43:15.300]   that will be, for example, in the validation set.
[00:43:15.300 --> 00:43:17.300]   That's why I call it validation split.
[00:43:17.300 --> 00:43:19.580]   So to create the validation independent variables,
[00:43:19.580 --> 00:43:21.260]   you have to use those to index
[00:43:21.260 --> 00:43:24.300]   into the independent variables
[00:43:24.300 --> 00:43:26.260]   and ditto for the dependent variables.
[00:43:27.900 --> 00:43:32.900]   And so now we've got our independent variable training set
[00:43:32.900 --> 00:43:37.180]   and our validation set,
[00:43:37.180 --> 00:43:39.740]   and we've also got the same for the dependent variables.
[00:43:39.740 --> 00:43:45.020]   So like I said before, I normally take stuff
[00:43:45.020 --> 00:43:46.700]   that I've already done in a notebook,
[00:43:46.700 --> 00:43:49.140]   seems to be working and put them into functions.
[00:43:49.140 --> 00:43:52.420]   So here's the step which actually updates coefficients.
[00:43:52.420 --> 00:43:54.220]   So let's chuck that into a function.
[00:43:54.220 --> 00:43:56.780]   And then the steps that go cat_loss,
[00:43:56.780 --> 00:43:58.660]   start_backward, update_coefficients,
[00:43:58.660 --> 00:44:01.420]   and then print the loss or chuck that in one function.
[00:44:01.420 --> 00:44:04.020]   So just copying and pasting stuff into cells here.
[00:44:04.020 --> 00:44:08.340]   And then the bit on the very top of the previous section
[00:44:08.340 --> 00:44:11.420]   that got the random numbers, minus 0.5 requires grad,
[00:44:11.420 --> 00:44:13.620]   chuck that in the function.
[00:44:13.620 --> 00:44:16.460]   So here we've got something that initializes coefficients,
[00:44:16.460 --> 00:44:19.340]   something that does one epoch by updating coefficients.
[00:44:19.340 --> 00:44:22.060]   So we can put that together into something
[00:44:22.060 --> 00:44:24.700]   that trains the model for any epochs
[00:44:24.700 --> 00:44:28.060]   with some learning rate by setting the manual seed,
[00:44:28.060 --> 00:44:29.740]   initializing the coefficients,
[00:44:29.740 --> 00:44:33.820]   doing one epoch in a loop,
[00:44:33.820 --> 00:44:35.500]   and then return the coefficients.
[00:44:35.500 --> 00:44:39.340]   So let's go ahead and run that function.
[00:44:39.340 --> 00:44:44.300]   So it's printing at the end of each one the loss,
[00:44:44.300 --> 00:44:47.140]   and you can see the loss going down from 0.53,
[00:44:47.140 --> 00:44:50.620]   down, down, down, down, down, to a bit under 0.3.
[00:44:50.620 --> 00:44:51.460]   So that's good.
[00:44:51.460 --> 00:44:56.420]   We have successfully built and trained a linear model
[00:44:56.420 --> 00:44:58.060]   on a real dataset.
[00:44:58.060 --> 00:44:59.340]   I mean, it's a Kaggle dataset,
[00:44:59.340 --> 00:45:02.740]   but it's important to not underestimate
[00:45:02.740 --> 00:45:05.740]   how real Kaggle datasets are, they're real data.
[00:45:05.740 --> 00:45:08.100]   And this one's a playground dataset,
[00:45:08.100 --> 00:45:09.740]   so it's not like anybody actually cares
[00:45:09.740 --> 00:45:12.220]   about predicting who survived the Titanic,
[00:45:12.220 --> 00:45:13.380]   'cause we already know.
[00:45:13.380 --> 00:45:17.660]   But it has all the same features of different data types
[00:45:17.660 --> 00:45:19.980]   and missing values and normalization and so forth.
[00:45:19.980 --> 00:45:22.620]   So it's a good playground.
[00:45:22.620 --> 00:45:27.940]   So it'd be nice to see what the coefficients are
[00:45:27.940 --> 00:45:29.380]   attached to each variable.
[00:45:29.380 --> 00:45:32.100]   So if we just zip together the independent variables
[00:45:32.100 --> 00:45:34.660]   and the coefficients,
[00:45:34.660 --> 00:45:36.700]   and we don't need the regret anymore,
[00:45:36.700 --> 00:45:38.100]   and create a dict of that,
[00:45:38.100 --> 00:45:41.220]   there we go.
[00:45:41.220 --> 00:45:46.220]   So it looks like older people had less chance of surviving.
[00:45:46.220 --> 00:45:48.500]   That makes sense.
[00:45:49.460 --> 00:45:52.100]   Males had less chance of surviving.
[00:45:52.100 --> 00:45:53.180]   Also makes sense.
[00:45:53.180 --> 00:45:56.460]   So it's good to kind of eyeball these
[00:45:56.460 --> 00:45:59.460]   and check that they seem reasonable.
[00:45:59.460 --> 00:46:06.460]   Now the metric for this Kaggle competition
[00:46:06.460 --> 00:46:11.460]   is not main absolute error, it's accuracy.
[00:46:11.460 --> 00:46:14.180]   Now, of course, we can't use accuracy as a loss function
[00:46:14.180 --> 00:46:16.780]   'cause it doesn't have a sensible gradient, really.
[00:46:17.860 --> 00:46:20.820]   But we should measure accuracy to see how we're doing
[00:46:20.820 --> 00:46:22.740]   because that's gonna tell us
[00:46:22.740 --> 00:46:23.780]   how we're going against the thing
[00:46:23.780 --> 00:46:26.460]   that the Kaggle competition cares about.
[00:46:26.460 --> 00:46:28.340]   So we can calculate our predictions.
[00:46:28.340 --> 00:46:30.620]   And we'll just say, okay,
[00:46:30.620 --> 00:46:32.900]   well, any times the predictions over 0.5,
[00:46:32.900 --> 00:46:37.460]   we'll say that's predicting survival.
[00:46:37.460 --> 00:46:39.580]   So that's our predictors of survival.
[00:46:39.580 --> 00:46:42.160]   This is the actual in a validation set.
[00:46:42.160 --> 00:46:45.440]   So if they're the same, then we predicted it correctly.
[00:46:46.660 --> 00:46:51.660]   So here's, are we right or wrong for the first 16 rows?
[00:46:51.660 --> 00:46:53.500]   We're right more often than not.
[00:46:53.500 --> 00:46:56.460]   So if we take the mean of those,
[00:46:56.460 --> 00:47:00.360]   remember true equals one, then that's our accuracy.
[00:47:00.360 --> 00:47:03.420]   So we are right about 79% of the time.
[00:47:03.420 --> 00:47:04.500]   That's not bad, okay?
[00:47:04.500 --> 00:47:06.260]   So we've successfully created something
[00:47:06.260 --> 00:47:11.060]   that's actually predicting who survived the Titanic.
[00:47:11.060 --> 00:47:12.400]   That's cool, from scratch.
[00:47:12.400 --> 00:47:16.060]   So let's create a function for that,
[00:47:16.060 --> 00:47:18.660]   an accuracy function that just does what I showed.
[00:47:18.660 --> 00:47:21.780]   And there it is.
[00:47:21.780 --> 00:47:25.180]   Now, I'll say another thing like,
[00:47:25.180 --> 00:47:29.620]   my weird coding thing for me,
[00:47:29.620 --> 00:47:31.540]   weird as in not that common,
[00:47:31.540 --> 00:47:35.060]   is I use less comments than most people
[00:47:35.060 --> 00:47:36.940]   because all of my code lives in notebooks.
[00:47:36.940 --> 00:47:39.540]   And of course, in the real version of this notebook
[00:47:39.540 --> 00:47:44.780]   is full of pros, right?
[00:47:44.780 --> 00:47:48.540]   So when I've taken people through a whole journey
[00:47:48.540 --> 00:47:51.020]   about what I've built here and why I've built it
[00:47:51.020 --> 00:47:52.740]   and what intermediate results are
[00:47:52.740 --> 00:47:54.580]   and check them along the way,
[00:47:54.580 --> 00:47:57.020]   the function itself, for me,
[00:47:57.020 --> 00:47:59.940]   doesn't need extensive comments.
[00:47:59.940 --> 00:48:03.180]   I'd rather explain the thinking of how I got there
[00:48:03.180 --> 00:48:05.620]   and show examples of how to use it and so forth.
[00:48:05.620 --> 00:48:09.620]   Okay, now,
[00:48:09.620 --> 00:48:14.420]   here's the first few predictions we made.
[00:48:14.420 --> 00:48:18.420]   And some of the time,
[00:48:18.420 --> 00:48:22.260]   we're predicting negatives for survival
[00:48:22.260 --> 00:48:24.260]   and greater than one for survival,
[00:48:24.260 --> 00:48:28.180]   which doesn't really make much sense, right?
[00:48:28.180 --> 00:48:30.580]   People either survived one or they didn't, zero.
[00:48:30.580 --> 00:48:36.180]   It would be nice if we had a way
[00:48:36.180 --> 00:48:39.740]   to automatically squish everything between zero and one.
[00:48:39.740 --> 00:48:42.580]   That's gonna make it much easier to optimize.
[00:48:43.580 --> 00:48:46.100]   The optimizer doesn't have to try hard
[00:48:46.100 --> 00:48:48.180]   to hit exactly one or hit exactly zero,
[00:48:48.180 --> 00:48:50.660]   but it can just try to create a really big number
[00:48:50.660 --> 00:48:55.660]   to mean survive to a really small number to mean perished.
[00:48:55.660 --> 00:48:58.660]   Here's a great function.
[00:48:58.660 --> 00:49:08.020]   Here's a function that as I increase,
[00:49:08.020 --> 00:49:10.300]   let's make it even bigger, range.
[00:49:10.300 --> 00:49:15.300]   As my numbers get beyond four or five,
[00:49:15.300 --> 00:49:20.300]   it's asymptoting to one.
[00:49:20.300 --> 00:49:21.340]   And on the negative side,
[00:49:21.340 --> 00:49:23.020]   as they get beyond negative four or five,
[00:49:23.020 --> 00:49:24.580]   they asymptote to zero.
[00:49:24.580 --> 00:49:27.020]   Or to zoom in a bit.
[00:49:27.020 --> 00:49:32.860]   But then around about zero,
[00:49:32.860 --> 00:49:36.020]   it's pretty much a straight line.
[00:49:36.020 --> 00:49:37.020]   This is actually perfect.
[00:49:37.020 --> 00:49:38.420]   This is exactly what we want.
[00:49:39.500 --> 00:49:42.500]   So here is the equation,
[00:49:42.500 --> 00:49:45.140]   one over one plus eight of the negative minus X.
[00:49:45.140 --> 00:49:48.020]   And this is called the sigmoid function.
[00:49:48.020 --> 00:49:53.780]   By the way, if you haven't checked out SymPy before,
[00:49:53.780 --> 00:49:54.980]   definitely do so.
[00:49:54.980 --> 00:49:58.060]   This is the symbolic Python package,
[00:49:58.060 --> 00:50:01.340]   which can do, it's kind of like Mathematica
[00:50:01.340 --> 00:50:05.380]   or Wolfram style symbolic calculations,
[00:50:05.380 --> 00:50:09.620]   including the ability to plot symbolic expressions,
[00:50:09.620 --> 00:50:10.660]   which is pretty nice.
[00:50:10.660 --> 00:50:15.620]   PyTorch already has a sigmoid function.
[00:50:15.620 --> 00:50:16.900]   I mean, it just calculates this,
[00:50:16.900 --> 00:50:19.580]   but it does it in an all optimized way.
[00:50:19.580 --> 00:50:22.540]   So what if we replaced calc preds?
[00:50:22.540 --> 00:50:26.860]   Remember before calc preds was just this.
[00:50:26.860 --> 00:50:31.340]   What if we took that and then put it through a sigmoid?
[00:50:31.340 --> 00:50:36.340]   So calc preds were now basically the bigger, oops.
[00:50:36.340 --> 00:50:41.420]   The bigger this number is,
[00:50:41.420 --> 00:50:43.020]   the closer it's gonna get to one.
[00:50:43.020 --> 00:50:45.540]   And the smaller it is, the closer it's gonna get to zero.
[00:50:45.540 --> 00:50:47.420]   There should be a much easier thing to optimize
[00:50:47.420 --> 00:50:49.980]   and ensures that all of our values
[00:50:49.980 --> 00:50:52.180]   are in a sensible range.
[00:50:52.180 --> 00:50:55.260]   Now, here's another cool thing
[00:50:55.260 --> 00:50:59.900]   about using Jupyter plus Python.
[00:50:59.900 --> 00:51:03.260]   Python is a dynamic language.
[00:51:03.260 --> 00:51:05.580]   Even though I called calc preds,
[00:51:05.580 --> 00:51:09.700]   train model calls one epoch,
[00:51:09.700 --> 00:51:15.940]   which calls calc loss, which calls calc preds.
[00:51:15.940 --> 00:51:19.860]   I can redefine calc preds now,
[00:51:19.860 --> 00:51:22.460]   and I don't have to do anything.
[00:51:22.460 --> 00:51:26.100]   That's now inserted into Python symbol table,
[00:51:26.100 --> 00:51:27.300]   and that's the calc preds
[00:51:27.300 --> 00:51:29.460]   that train model will eventually call.
[00:51:29.460 --> 00:51:31.980]   So if I now call train model,
[00:51:31.980 --> 00:51:36.900]   that's actually gonna call my new version of calc preds.
[00:51:36.900 --> 00:51:38.460]   So that's a really neat way
[00:51:38.460 --> 00:51:42.980]   of doing exploratory programming in Python.
[00:51:42.980 --> 00:51:47.980]   I wouldn't release a library
[00:51:47.980 --> 00:51:51.460]   that redefines calc preds multiple times.
[00:51:51.460 --> 00:51:54.380]   When I'm done, I would just keep the final version, of course.
[00:51:54.380 --> 00:51:56.820]   But it's a great way to try things, as you'll see.
[00:51:56.820 --> 00:51:59.380]   And so look what's happened.
[00:51:59.380 --> 00:52:01.260]   I found I was able to increase the learning rate
[00:52:01.260 --> 00:52:02.900]   from 0.1 to two.
[00:52:02.900 --> 00:52:05.580]   Yeah, it was much easier to optimise, as I guessed.
[00:52:05.580 --> 00:52:10.580]   And the loss has improved from 0.295 to 0.197.
[00:52:10.580 --> 00:52:20.340]   The accuracy has improved from 0.79 to 0.82, nearly 0.83.
[00:52:20.340 --> 00:52:26.540]   So as a rule,
[00:52:26.540 --> 00:52:29.380]   this is something that we're pretty much always gonna do
[00:52:29.380 --> 00:52:31.580]   when we have a binary dependent variable.
[00:52:31.580 --> 00:52:33.980]   So dependent variable that's one or zero.
[00:52:33.980 --> 00:52:37.020]   Is the very last step is chuck it through a sigmoid.
[00:52:37.020 --> 00:52:41.740]   Generally speaking, if you're wondering,
[00:52:41.740 --> 00:52:44.300]   why is my model with a binary dependent variable
[00:52:44.300 --> 00:52:45.740]   not training very well?
[00:52:45.740 --> 00:52:48.340]   This is the thing you wanna check.
[00:52:48.340 --> 00:52:50.700]   Oh, are you chucking it through a sigmoid?
[00:52:50.700 --> 00:52:52.100]   Or is the thing you're calling
[00:52:52.100 --> 00:52:53.940]   chucking it through a sigmoid or not?
[00:52:55.260 --> 00:52:58.060]   It can be surprisingly hard to find out if that's happening.
[00:52:58.060 --> 00:53:00.060]   So for example, with hugging face transformers,
[00:53:00.060 --> 00:53:03.220]   I actually found I had to look in their source code
[00:53:03.220 --> 00:53:04.060]   to find out.
[00:53:04.060 --> 00:53:06.900]   And I discovered that something I was doing wasn't
[00:53:06.900 --> 00:53:09.740]   and didn't seem to be documented anywhere.
[00:53:09.740 --> 00:53:13.060]   But it is important to find these things out.
[00:53:13.060 --> 00:53:16.720]   As we'll discuss in the next lesson,
[00:53:16.720 --> 00:53:23.060]   we'll talk a lot about neural net architecture details,
[00:53:23.060 --> 00:53:25.580]   but the details we'll focus on what happens
[00:53:25.580 --> 00:53:27.420]   to the inputs at the very first stage
[00:53:27.420 --> 00:53:30.580]   and what happens to the outputs at the very last stage.
[00:53:30.580 --> 00:53:32.060]   We'll talk a bit about what happens in the middle,
[00:53:32.060 --> 00:53:33.220]   but a lot less.
[00:53:33.220 --> 00:53:35.900]   And the reason why is it's the things that you put
[00:53:35.900 --> 00:53:38.300]   into the inputs that's gonna change
[00:53:38.300 --> 00:53:40.580]   for every single data set you do.
[00:53:40.580 --> 00:53:43.020]   And what do you wanna happen to the outputs?
[00:53:43.020 --> 00:53:45.860]   It's just gonna happen for every different target
[00:53:45.860 --> 00:53:47.460]   that you're trying to hit.
[00:53:47.460 --> 00:53:49.500]   So those are the things that you actually
[00:53:49.500 --> 00:53:50.740]   need to know about.
[00:53:50.740 --> 00:53:52.100]   So for example, this thing of like,
[00:53:52.100 --> 00:53:54.700]   well, you need to know about the sigmoid function
[00:53:54.700 --> 00:53:56.820]   and you need to know that you need to use it.
[00:53:56.820 --> 00:54:01.860]   Fast AI is very good at handling this for you.
[00:54:01.860 --> 00:54:04.340]   That's why we haven't had to talk about it much until now.
[00:54:04.340 --> 00:54:07.980]   If you say, oh, it's a category block dependent variable,
[00:54:07.980 --> 00:54:11.640]   you know, it's gonna use the right kind of thing for you.
[00:54:11.640 --> 00:54:15.780]   But most things are not so convenient.
[00:54:15.780 --> 00:54:17.020]   John's question.
[00:54:17.020 --> 00:54:19.660]   - Yes, there is.
[00:54:19.660 --> 00:54:23.900]   It's back in the sort of the feature engineering topic,
[00:54:23.900 --> 00:54:26.180]   but a couple of people have liked it.
[00:54:26.180 --> 00:54:28.300]   So I thought we'd put it out there.
[00:54:28.300 --> 00:54:32.780]   So Shivam says one concern I have while using get dummies,
[00:54:32.780 --> 00:54:35.060]   so it's in that get dummies phase,
[00:54:35.060 --> 00:54:38.060]   is what happens while using test data?
[00:54:38.060 --> 00:54:41.740]   I have a new category, let's say male, female and other.
[00:54:41.740 --> 00:54:44.100]   And this will have an extra column missing
[00:54:44.100 --> 00:54:46.140]   from the training data.
[00:54:46.140 --> 00:54:48.060]   How do you take care of that?
[00:54:48.060 --> 00:54:49.420]   - That's a great question.
[00:54:49.420 --> 00:54:50.260]   Yeah, so,
[00:54:50.260 --> 00:54:55.980]   normally you've got to think about this pretty carefully
[00:54:55.980 --> 00:54:59.620]   and check pretty carefully, unless you use fast AI.
[00:54:59.620 --> 00:55:04.620]   So fast AI always creates an extra category called other,
[00:55:04.620 --> 00:55:08.100]   and at test time, inference time,
[00:55:08.100 --> 00:55:11.260]   if you have some level that didn't exist before,
[00:55:11.260 --> 00:55:13.420]   we put it into the other category for you.
[00:55:13.420 --> 00:55:17.860]   Otherwise, you basically have to do that yourself,
[00:55:17.860 --> 00:55:19.380]   or at least check.
[00:55:19.380 --> 00:55:26.500]   Generally speaking, it's pretty likely that otherwise,
[00:55:26.500 --> 00:55:31.500]   your extra level will be silently ignored,
[00:55:31.500 --> 00:55:33.380]   'cause it's gonna be in the dataset,
[00:55:33.380 --> 00:55:36.020]   but it's not gonna be matched to a column.
[00:55:36.020 --> 00:55:37.860]   So yeah, it's a good point,
[00:55:37.860 --> 00:55:39.480]   and definitely worth checking.
[00:55:39.480 --> 00:55:43.140]   For categorical variables with lots of levels,
[00:55:43.140 --> 00:55:45.660]   I actually normally like to put the less common ones
[00:55:45.660 --> 00:55:47.500]   into an other category.
[00:55:47.500 --> 00:55:48.340]   And again, that's something
[00:55:48.340 --> 00:55:50.380]   that fast AI will do for you automatically.
[00:55:50.380 --> 00:55:56.520]   But yeah, definitely something to keep an eye out for.
[00:55:56.520 --> 00:55:58.820]   Good question.
[00:55:58.820 --> 00:56:04.680]   Okay, so before we take our break,
[00:56:04.680 --> 00:56:06.060]   we'll just do one last thing,
[00:56:06.060 --> 00:56:07.660]   which is we will submit this to Kaggle,
[00:56:07.660 --> 00:56:09.180]   because I think it's quite cool
[00:56:09.180 --> 00:56:13.440]   that we have successfully built a model from scratch.
[00:56:13.440 --> 00:56:15.820]   So Kaggle provides us with a test.csv,
[00:56:15.820 --> 00:56:19.680]   which is exactly the same structure as the training CSV,
[00:56:19.680 --> 00:56:23.220]   except that it doesn't have a survived column.
[00:56:23.220 --> 00:56:27.020]   Now interestingly, when I tried to submit to Kaggle,
[00:56:27.020 --> 00:56:30.100]   I got an error in my code saying that,
[00:56:30.100 --> 00:56:33.780]   oh, one of my fares is empty.
[00:56:33.780 --> 00:56:34.740]   So that was interesting,
[00:56:34.740 --> 00:56:37.380]   because the training set doesn't have any empty fares.
[00:56:37.380 --> 00:56:39.700]   So sometimes this will happen,
[00:56:39.700 --> 00:56:41.620]   that the training set and the test set
[00:56:41.620 --> 00:56:44.100]   have different things to deal with.
[00:56:44.100 --> 00:56:45.060]   So in this case, I just said,
[00:56:45.060 --> 00:56:47.020]   oh, there's only one row, I don't care.
[00:56:47.020 --> 00:56:50.760]   So I just replaced the empty one with a zero for fare.
[00:56:50.760 --> 00:56:57.820]   So then I just copied and pasted the pre-processing steps
[00:56:57.820 --> 00:57:01.340]   from my training data frame,
[00:57:01.340 --> 00:57:03.260]   and stuck them here for the test data frame
[00:57:03.260 --> 00:57:04.880]   and the normalization as well.
[00:57:04.880 --> 00:57:08.700]   And so now I just call countPrets,
[00:57:08.700 --> 00:57:10.100]   is it greater than 0.5?
[00:57:10.100 --> 00:57:13.200]   Turn it into a zero or one,
[00:57:13.200 --> 00:57:15.060]   because that's what Kaggle expects,
[00:57:15.060 --> 00:57:17.140]   and put that into the survived column,
[00:57:17.140 --> 00:57:19.360]   which previously, remember, didn't exist.
[00:57:19.360 --> 00:57:22.180]   So then finally I created data frame
[00:57:22.180 --> 00:57:24.320]   with just the two columns ID and survived.
[00:57:24.320 --> 00:57:29.300]   Stick it in a CSV file,
[00:57:29.300 --> 00:57:31.740]   and then I can call the Unix command head,
[00:57:31.740 --> 00:57:33.780]   just to look at the first few rows.
[00:57:33.780 --> 00:57:38.020]   And if you look at the Kaggle competition's data page,
[00:57:38.020 --> 00:57:40.060]   you'll see this is what the submission file
[00:57:40.060 --> 00:57:42.320]   is expected to look like, so that made me feel good.
[00:57:42.320 --> 00:57:44.700]   So I went ahead and submitted it.
[00:57:44.700 --> 00:57:47.180]   Oh, I didn't mention it.
[00:57:47.180 --> 00:57:49.160]   Okay, so anyway, I submitted it,
[00:57:49.160 --> 00:57:50.860]   and I remember I got like,
[00:57:50.860 --> 00:57:53.340]   I think I was basically right in the middle,
[00:57:53.340 --> 00:57:56.180]   about 50%, you know, better than half the people
[00:57:56.180 --> 00:57:57.140]   who have entered the competition,
[00:57:57.140 --> 00:57:58.640]   worse than half the people.
[00:57:58.640 --> 00:58:00.860]   So, you know, solid middle of the pack result
[00:58:00.860 --> 00:58:03.980]   for a linear model from scratch,
[00:58:03.980 --> 00:58:06.140]   I think's a pretty good result.
[00:58:06.140 --> 00:58:07.620]   So that's a great place to start.
[00:58:07.620 --> 00:58:09.520]   So let's take a 10-minute break.
[00:58:09.520 --> 00:58:14.520]   We'll come back at 7.17 and continue on our journey.
[00:58:14.520 --> 00:58:25.600]   All right, welcome back.
[00:58:25.600 --> 00:58:31.820]   You might remember from Excel
[00:58:31.820 --> 00:58:36.820]   that after we did the some product version,
[00:58:37.280 --> 00:58:42.280]   we then replaced it with a matrix model play.
[00:58:42.280 --> 00:58:48.860]   Wait, not there, must be here.
[00:58:48.860 --> 00:58:52.920]   Here we are, with a matrix model play.
[00:58:52.920 --> 00:58:54.840]   So let's do that step now.
[00:58:54.840 --> 00:58:59.840]   So matrix times vector dot sum over axis equals one
[00:58:59.840 --> 00:59:04.800]   is the same thing as matrix model play.
[00:59:06.000 --> 00:59:10.080]   So here is the times dot sum version.
[00:59:10.080 --> 00:59:13.880]   Now we can't use this character for a matrix model play
[00:59:13.880 --> 00:59:16.260]   'cause it means element wise operation.
[00:59:16.260 --> 00:59:19.400]   All of the times plus minus divide
[00:59:19.400 --> 00:59:22.520]   in PyTorch NumPy mean element wise.
[00:59:22.520 --> 00:59:26.400]   So corresponding elements.
[00:59:26.400 --> 00:59:29.400]   So in Python instead, we use this character.
[00:59:29.400 --> 00:59:31.080]   As far as I know, it's pretty arbitrary.
[00:59:31.080 --> 00:59:32.880]   It's one of the ones that wasn't used.
[00:59:34.280 --> 00:59:37.600]   So that is an official Python, it's a bit unusual.
[00:59:37.600 --> 00:59:38.960]   It's an official Python operator,
[00:59:38.960 --> 00:59:40.680]   it means matrix model play,
[00:59:40.680 --> 00:59:43.360]   but Python doesn't come with an implementation of it.
[00:59:43.360 --> 00:59:46.080]   So because we've imported, because these are tensors
[00:59:46.080 --> 00:59:49.280]   and in PyTorch it will use PyTorches.
[00:59:49.280 --> 00:59:52.000]   And as you can see, they're exactly the same.
[00:59:52.000 --> 00:59:54.440]   So we can now just simplify a little bit
[00:59:54.440 --> 00:59:55.400]   what we had before.
[00:59:55.400 --> 00:59:58.200]   Calc preds is now torch dot sigmoid
[00:59:58.200 --> 00:59:59.480]   of the matrix model play.
[01:00:01.280 --> 01:00:04.760]   Now there is one thing I'd like to move towards now
[01:00:04.760 --> 01:00:08.160]   is that we're gonna try to create a neural net in a moment.
[01:00:08.160 --> 01:00:10.820]   And so that means rather than treat this
[01:00:10.820 --> 01:00:14.720]   as a matrix times a vector,
[01:00:14.720 --> 01:00:18.160]   I wanna treat this as a matrix times a matrix
[01:00:18.160 --> 01:00:20.120]   because we're about to add some more
[01:00:20.120 --> 01:00:23.520]   columns of coefficients.
[01:00:23.520 --> 01:00:26.720]   So we're gonna change in a coefs
[01:00:26.720 --> 01:00:30.680]   so that rather than creating an n coef vector,
[01:00:31.520 --> 01:00:36.400]   we're gonna create an n coef by one matrix.
[01:00:36.400 --> 01:00:40.440]   So in math, we would probably call that a column vector,
[01:00:40.440 --> 01:00:43.400]   but I think that's a kind of a dumb name in some ways
[01:00:43.400 --> 01:00:44.960]   'cause it's a matrix, right?
[01:00:44.960 --> 01:00:46.500]   It's a rank two tensor.
[01:00:46.500 --> 01:00:56.640]   So the matrix model play will work fine either way,
[01:00:56.640 --> 01:00:59.520]   but the key difference is that if we do it this way,
[01:01:00.600 --> 01:01:02.920]   then the result of the matrix model play
[01:01:02.920 --> 01:01:05.080]   will also be a matrix.
[01:01:05.080 --> 01:01:09.760]   It'll be again, a n rows by one matrix.
[01:01:09.760 --> 01:01:13.200]   That means when we compare it to the dependent variable,
[01:01:13.200 --> 01:01:15.480]   we need the dependent variable to be an n rows
[01:01:15.480 --> 01:01:17.200]   by one matrix as well.
[01:01:17.200 --> 01:01:21.000]   So effectively, we need to take the n rows long vector
[01:01:21.000 --> 01:01:24.280]   and turn it into an n rows by one matrix.
[01:01:24.280 --> 01:01:29.640]   So there's some useful, very useful,
[01:01:29.640 --> 01:01:33.400]   and at first maybe a bit weird notation
[01:01:33.400 --> 01:01:35.560]   in PyTorch NumPy for this,
[01:01:35.560 --> 01:01:39.400]   which is if I take my training dependent variables vector,
[01:01:39.400 --> 01:01:45.440]   I index into it and colon means every row, right?
[01:01:45.440 --> 01:01:48.000]   So in other words,
[01:01:48.000 --> 01:01:54.960]   that just means the whole vector, right?
[01:01:54.960 --> 01:01:56.660]   It's the same basically as that.
[01:01:56.660 --> 01:02:01.660]   And then I index into a second dimension.
[01:02:01.660 --> 01:02:08.900]   Now this doesn't have a second dimension.
[01:02:08.900 --> 01:02:10.300]   So there's a special thing you can do,
[01:02:10.300 --> 01:02:12.740]   which is if you index into a second dimension
[01:02:12.740 --> 01:02:17.740]   with a special value none, it creates that dimension.
[01:02:17.740 --> 01:02:23.820]   So this has the effect of adding
[01:02:24.060 --> 01:02:28.180]   an extra trailing dimension to train dependence.
[01:02:28.180 --> 01:02:33.180]   So it turns it from a vector to a matrix with one column.
[01:02:33.180 --> 01:02:37.860]   So if we look at the shape after that,
[01:02:37.860 --> 01:02:46.300]   as you see, it's now got, we call this a unit axis.
[01:02:46.300 --> 01:02:50.280]   It's got a trailing unit axis, 713 rows and one column.
[01:02:52.300 --> 01:02:54.060]   So now if we train our model,
[01:02:54.060 --> 01:02:57.660]   we'll get coefficients just like before,
[01:02:57.660 --> 01:03:03.660]   except that it's now a column vector,
[01:03:03.660 --> 01:03:14.420]   also known as a rank two matrix with a trailing unit axis.
[01:03:14.420 --> 01:03:18.620]   Okay, so that hasn't changed anything.
[01:03:18.620 --> 01:03:20.840]   It's just repeated what we did in the previous section,
[01:03:20.840 --> 01:03:23.700]   but it's kind of set us up to expand
[01:03:23.700 --> 01:03:26.860]   because now that we've done this using matrix multiply,
[01:03:26.860 --> 01:03:28.860]   we can go crazy and we can go ahead
[01:03:28.860 --> 01:03:31.020]   and create a neural network.
[01:03:31.020 --> 01:03:34.180]   So with our neural network,
[01:03:34.180 --> 01:03:37.860]   remember back to the Excel days,
[01:03:37.860 --> 01:03:40.800]   notice here it's the same thing, right?
[01:03:40.800 --> 01:03:45.420]   We created a column vector,
[01:03:45.420 --> 01:03:46.620]   but we didn't create a column vector,
[01:03:46.620 --> 01:03:50.720]   we actually created a matrix with kind of two sets
[01:03:50.720 --> 01:03:52.180]   of coefficients.
[01:03:52.180 --> 01:03:54.060]   So when we did our matrix multiply,
[01:03:54.060 --> 01:03:57.700]   every row gave us two sets of outputs,
[01:03:57.700 --> 01:04:02.740]   which we then chucked through value, right?
[01:04:02.740 --> 01:04:05.540]   Which remember we just used an if statement
[01:04:05.540 --> 01:04:08.300]   and we added them together.
[01:04:08.300 --> 01:04:13.300]   So our co-effs now to make a proper neural net,
[01:04:13.300 --> 01:04:19.580]   we need one set of co-effs here,
[01:04:20.480 --> 01:04:25.480]   and so here they are, torch.rand and co-eff by what?
[01:04:25.480 --> 01:04:28.080]   Well, in Excel, we just did two
[01:04:28.080 --> 01:04:29.320]   because I kind of got bored
[01:04:29.320 --> 01:04:32.000]   of getting everything working properly,
[01:04:32.000 --> 01:04:35.320]   but you don't have to worry about feeling rash
[01:04:35.320 --> 01:04:37.080]   and creating columns and blah, blah, blah,
[01:04:37.080 --> 01:04:40.000]   and in PyTorch, you can create as many as you like.
[01:04:40.000 --> 01:04:42.320]   So I made it something you can change,
[01:04:42.320 --> 01:04:45.200]   I called it nhidden, number of hidden activations,
[01:04:45.200 --> 01:04:46.360]   and I just set it to 20.
[01:04:47.520 --> 01:04:50.040]   And as before, we centralized them
[01:04:50.040 --> 01:04:52.480]   by making them go from minus 0.5 to 0.5.
[01:04:52.480 --> 01:04:56.340]   Now, when you do stuff by hand,
[01:04:56.340 --> 01:04:59.280]   everything does get more fiddly.
[01:04:59.280 --> 01:05:01.440]   If our coefficients aren't,
[01:05:01.440 --> 01:05:04.880]   if they're too big or too small,
[01:05:04.880 --> 01:05:06.600]   it's not gonna train at all.
[01:05:06.600 --> 01:05:09.160]   Basically, the gradients will kind of vaguely point
[01:05:09.160 --> 01:05:10.000]   in the right direction,
[01:05:10.000 --> 01:05:12.920]   but you'll jump too far or not far enough or whatever.
[01:05:12.920 --> 01:05:15.640]   So I want my gradients to be about the same
[01:05:15.640 --> 01:05:17.160]   as they were before.
[01:05:17.160 --> 01:05:20.240]   So I divide by nhidden,
[01:05:20.240 --> 01:05:21.680]   because otherwise at the next step,
[01:05:21.680 --> 01:05:25.200]   when I add up the next matrix multiply,
[01:05:25.200 --> 01:05:28.560]   it's gonna be much bigger than it was before.
[01:05:28.560 --> 01:05:31.600]   So it's all very fiddly.
[01:05:31.600 --> 01:05:33.520]   So then I wanna take,
[01:05:33.520 --> 01:05:35.820]   so that's gonna give me, for every row,
[01:05:35.820 --> 01:05:40.640]   it's gonna give me 20 activations, 20 values, right?
[01:05:40.640 --> 01:05:43.920]   Just like in Excel, we had two values
[01:05:43.920 --> 01:05:46.160]   because we had two sets of coefficients.
[01:05:47.160 --> 01:05:48.600]   And so to create a neural net,
[01:05:48.600 --> 01:05:51.520]   I now need to multiply each of those 20 things
[01:05:51.520 --> 01:05:55.080]   by a coefficient.
[01:05:55.080 --> 01:05:59.320]   And this time it's gonna be a column vector,
[01:05:59.320 --> 01:06:01.880]   'cause I wanna create one output predictor of survival.
[01:06:01.880 --> 01:06:04.000]   So again, torch.rand,
[01:06:04.000 --> 01:06:06.520]   and this time the nhidden
[01:06:06.520 --> 01:06:09.280]   will be the number of coefficients by one.
[01:06:09.280 --> 01:06:12.600]   And again, like try to find something
[01:06:12.600 --> 01:06:14.920]   that actually trains properly,
[01:06:14.920 --> 01:06:16.440]   required me some fiddling around
[01:06:16.440 --> 01:06:18.440]   to figure out how much to subtract.
[01:06:18.440 --> 01:06:21.160]   And I found if I subtract 0.3, I could get it to train.
[01:06:21.160 --> 01:06:26.120]   And then finally,
[01:06:26.120 --> 01:06:28.440]   I didn't need a constant term for the first layer
[01:06:28.440 --> 01:06:32.840]   as we discussed because our dummy variables
[01:06:32.840 --> 01:06:36.920]   have n columns rather than n minus one columns.
[01:06:36.920 --> 01:06:41.200]   But layer two absolutely needs a constant term, okay?
[01:06:41.200 --> 01:06:43.280]   And we could do that as we discussed last time
[01:06:43.280 --> 01:06:45.360]   by having a column of ones.
[01:06:45.360 --> 01:06:47.360]   Although in practice, I actually find it's just easier
[01:06:47.360 --> 01:06:49.640]   just to create a constant term, okay?
[01:06:49.640 --> 01:06:53.440]   So here is a single scalar random number.
[01:06:53.440 --> 01:06:57.120]   So those are the coefficients we need.
[01:06:57.120 --> 01:06:59.760]   One set of coefficients to go from input to hidden,
[01:06:59.760 --> 01:07:03.760]   one goes from hidden to a single output and a constant.
[01:07:03.760 --> 01:07:05.320]   So they're all gonna need grab.
[01:07:05.320 --> 01:07:10.320]   And so now we can change how we calculate predictions.
[01:07:11.680 --> 01:07:14.080]   So we're gonna pass in all of our coefficients.
[01:07:14.080 --> 01:07:17.720]   So a nice thing in Python is if you've got a list
[01:07:17.720 --> 01:07:21.080]   or a tuple of values, on the left-hand side,
[01:07:21.080 --> 01:07:23.680]   you can expand them out into variables.
[01:07:23.680 --> 01:07:26.400]   So this is gonna be a list of three things.
[01:07:26.400 --> 01:07:29.760]   So call them L1, layer one, layer two
[01:07:29.760 --> 01:07:31.560]   and the constant term
[01:07:31.560 --> 01:07:34.440]   'cause those are the list of three things we returned.
[01:07:34.440 --> 01:07:37.880]   So in Python, if you just chuck things
[01:07:37.880 --> 01:07:40.600]   with commas between them like this, it creates a tuple.
[01:07:40.600 --> 01:07:43.040]   A tuple is a list, isn't a mutable list.
[01:07:43.040 --> 01:07:47.520]   So now we're gonna grab those three things.
[01:07:47.520 --> 01:07:50.440]   So step one is to do our matrix bottle play.
[01:07:50.440 --> 01:07:52.480]   And as we discussed,
[01:07:52.480 --> 01:07:55.080]   we then have to replace the negatives with zeros.
[01:07:55.080 --> 01:07:58.640]   And then we put that through our second matrix bottle play.
[01:07:58.640 --> 01:08:01.600]   So our second layer and add the constant term.
[01:08:01.600 --> 01:08:04.000]   And remember, of course, at the end,
[01:08:04.000 --> 01:08:05.240]   chuck it through a sigmoid.
[01:08:05.240 --> 01:08:07.920]   So here is a neural network.
[01:08:10.520 --> 01:08:14.960]   Now update_coefs previously subtracted the coefficients,
[01:08:14.960 --> 01:08:17.000]   the gradients times the learning rate from the coefficients.
[01:08:17.000 --> 01:08:20.120]   But now we've got three sets of those.
[01:08:20.120 --> 01:08:22.320]   So we have to just chuck that in a for loop.
[01:08:22.320 --> 01:08:25.520]   So change that as well.
[01:08:25.520 --> 01:08:28.040]   And now we can go ahead and train our model.
[01:08:28.040 --> 01:08:29.440]   Ta-da!
[01:08:29.440 --> 01:08:31.280]   We just trained a model.
[01:08:31.280 --> 01:08:32.880]   And how does that compare?
[01:08:32.880 --> 01:08:37.080]   So the loss function's a little better than before.
[01:08:38.600 --> 01:08:43.600]   Accuracy, exactly the same as before.
[01:08:43.600 --> 01:08:51.080]   And I will say it was very annoying to get to this point,
[01:08:51.080 --> 01:08:54.840]   trying to get these constants right
[01:08:54.840 --> 01:08:56.560]   and find a learning rate that worked.
[01:08:56.560 --> 01:08:58.960]   Like it was super fiddly.
[01:08:58.960 --> 01:09:01.800]   But we got there.
[01:09:01.800 --> 01:09:04.720]   We got there and it's a very small test set.
[01:09:04.720 --> 01:09:06.160]   I don't know if this is necessarily better
[01:09:06.160 --> 01:09:07.360]   or worse than the linear model,
[01:09:07.360 --> 01:09:09.120]   but it's certainly fine.
[01:09:09.120 --> 01:09:11.480]   And I think that's pretty cool
[01:09:11.480 --> 01:09:14.840]   that we were able to build a neural net from scratch.
[01:09:14.840 --> 01:09:18.320]   That's doing pretty well.
[01:09:18.320 --> 01:09:20.720]   But I hear that all the cool kids nowadays
[01:09:20.720 --> 01:09:22.920]   are doing deep learning, not just neural nets.
[01:09:22.920 --> 01:09:24.440]   So we better make this deep learning.
[01:09:24.440 --> 01:09:28.720]   So this one only has one hidden layer.
[01:09:28.720 --> 01:09:31.080]   So let's create one with n hidden layers.
[01:09:31.080 --> 01:09:36.600]   So for example, let's say we want two hidden layers.
[01:09:36.600 --> 01:09:38.240]   10 activations in each.
[01:09:38.240 --> 01:09:40.440]   You can put as many as you like here, right?
[01:09:40.440 --> 01:09:45.440]   So init_coefs now is going to have to create a torch.rand
[01:09:45.440 --> 01:09:51.160]   for every one of those hidden layers.
[01:09:51.160 --> 01:09:56.160]   And then another torch.rand for your constant terms.
[01:09:56.160 --> 01:10:00.240]   Stick requires gradient all of them.
[01:10:00.240 --> 01:10:03.280]   And then we can return that.
[01:10:03.280 --> 01:10:06.320]   So that's how we can just initialize
[01:10:06.320 --> 01:10:09.520]   as many layers as we want of coefficients.
[01:10:09.520 --> 01:10:11.560]   So the first one, the first layer,
[01:10:11.560 --> 01:10:13.760]   so the sizes of each one,
[01:10:13.760 --> 01:10:17.080]   the first layer will go from n_coefs to 10.
[01:10:17.080 --> 01:10:19.600]   The second matrix will go from 10 to 10.
[01:10:19.600 --> 01:10:21.920]   And the third matrix will go from 10 to one.
[01:10:21.920 --> 01:10:26.160]   So it's worth working through these matrix multipliers
[01:10:26.160 --> 01:10:28.480]   on a spreadsheet or a piece of paper or something
[01:10:28.480 --> 01:10:29.720]   to kind of convince yourself
[01:10:29.720 --> 01:10:32.520]   that there's a right number of activations at each point.
[01:10:35.560 --> 01:10:39.600]   And so then we need to update calc_preds
[01:10:39.600 --> 01:10:42.680]   so that rather than doing each of these steps manually,
[01:10:42.680 --> 01:10:45.680]   we now need to loop through all the layers.
[01:10:45.680 --> 01:10:51.920]   Do the matrix multiply at the constant.
[01:10:51.920 --> 01:10:56.000]   And as long as it's not the last layer, do the value.
[01:10:56.000 --> 01:10:57.160]   Why not the last layer?
[01:10:57.160 --> 01:11:00.040]   Because remember the last layer has sigmoid.
[01:11:00.040 --> 01:11:02.160]   So these things about like,
[01:11:02.160 --> 01:11:05.160]   remember what happens on the last layer.
[01:11:05.160 --> 01:11:07.360]   This is an important thing you need to know about
[01:11:07.360 --> 01:11:10.320]   and you need to kind of check if things aren't working.
[01:11:10.320 --> 01:11:13.440]   What's your, this thing here is called
[01:11:13.440 --> 01:11:17.280]   the activation function, torch.sigmoid and f.relu.
[01:11:17.280 --> 01:11:20.480]   They're the activation functions for these layers.
[01:11:20.480 --> 01:11:24.120]   One of the most common mistakes
[01:11:24.120 --> 01:11:26.560]   amongst people trying to kind of create their own
[01:11:26.560 --> 01:11:31.160]   architectures or kind of variants of architectures
[01:11:31.160 --> 01:11:35.200]   is to mess up their final activation function
[01:11:35.200 --> 01:11:37.280]   and that makes things very hard to train.
[01:11:37.280 --> 01:11:39.480]   So make sure we've got a torch.sigmoid at the end
[01:11:39.480 --> 01:11:41.240]   and no relu at the end.
[01:11:41.240 --> 01:11:45.640]   So there's our deep learning calc_preds.
[01:11:45.640 --> 01:11:48.760]   And then just one last change
[01:11:48.760 --> 01:11:50.400]   is now when we update our coefficients,
[01:11:50.400 --> 01:11:53.000]   we go through all the layers and all the constants.
[01:11:53.000 --> 01:11:57.760]   And again, there was so much messing around here
[01:11:57.760 --> 01:12:00.680]   with trying to find like exact ranges of random numbers
[01:12:00.680 --> 01:12:02.040]   that end up training okay.
[01:12:02.040 --> 01:12:05.240]   But eventually I found some
[01:12:05.240 --> 01:12:07.320]   and as you can see it gets to about the same
[01:12:07.320 --> 01:12:13.480]   loss and about the same accuracy.
[01:12:13.480 --> 01:12:18.240]   This code is worth spending time with
[01:12:18.240 --> 01:12:23.040]   and when the code's inside a function,
[01:12:23.040 --> 01:12:25.040]   it can be a little difficult to experiment with.
[01:12:25.040 --> 01:12:27.240]   So you know what I would be inclined to do
[01:12:27.240 --> 01:12:31.080]   to understand this code is to kind of copy and paste this cell,
[01:12:31.080 --> 01:12:33.840]   make it so it's not in a function anymore
[01:12:33.840 --> 01:12:36.480]   and then use control shift dash to separate these out
[01:12:36.480 --> 01:12:38.600]   into separate cells, right.
[01:12:38.600 --> 01:12:40.000]   And then try to kind of set it up
[01:12:40.000 --> 01:12:42.120]   so you can run a single layer at a time
[01:12:42.120 --> 01:12:45.440]   or a single coefficient, like make sure you can see
[01:12:45.440 --> 01:12:48.720]   what's going on, okay.
[01:12:48.720 --> 01:12:50.680]   And that's why we use notebooks
[01:12:50.680 --> 01:12:53.640]   and so that we can experiment.
[01:12:53.640 --> 01:12:55.800]   And it's only through experimenting like that
[01:12:55.800 --> 01:12:59.440]   that at least to me, I find that I can really understand
[01:12:59.440 --> 01:13:00.280]   what's going on.
[01:13:00.280 --> 01:13:03.360]   Nobody can look at this code and immediately say,
[01:13:03.360 --> 01:13:05.880]   I don't think anybody can, I get it.
[01:13:05.880 --> 01:13:08.080]   That all makes perfect sense.
[01:13:08.080 --> 01:13:11.160]   But once you try running through it yourself,
[01:13:11.160 --> 01:13:14.000]   you'll be like, oh, I see why that's as it is.
[01:13:14.000 --> 01:13:20.040]   So you know, one thing to point out here
[01:13:20.040 --> 01:13:25.040]   is that our neural nets and deep neural nets
[01:13:25.440 --> 01:13:28.720]   and deep learning models didn't particularly seem to help.
[01:13:28.720 --> 01:13:34.400]   So does that mean that deep learning is a waste of time
[01:13:34.400 --> 01:13:39.040]   and you just did five lessons that you shouldn't have done?
[01:13:39.040 --> 01:13:41.160]   No, not necessarily.
[01:13:41.160 --> 01:13:42.720]   This is a playground competition.
[01:13:42.720 --> 01:13:46.200]   We're doing it because it's easy to get your head around.
[01:13:46.200 --> 01:13:48.160]   But for very small data sets like this
[01:13:48.160 --> 01:13:49.560]   with very, very few columns
[01:13:49.560 --> 01:13:51.240]   and the columns are really simple.
[01:13:54.000 --> 01:13:56.040]   You know, deep learning is not necessarily
[01:13:56.040 --> 01:13:57.440]   gonna give you the best result.
[01:13:57.440 --> 01:14:00.120]   In fact, as I mentioned,
[01:14:00.120 --> 01:14:07.240]   nothing we do is gonna be as good
[01:14:07.240 --> 01:14:11.320]   as a carefully designed model
[01:14:11.320 --> 01:14:12.880]   that uses just the name column.
[01:14:12.880 --> 01:14:18.440]   So, you know, I think that's an interesting insight, right?
[01:14:18.440 --> 01:14:22.120]   Is that the kind of data types
[01:14:22.120 --> 01:14:25.440]   which have a very consistent structure,
[01:14:25.440 --> 01:14:26.960]   like for example, images
[01:14:26.960 --> 01:14:30.560]   or natural language text documents,
[01:14:30.560 --> 01:14:35.040]   quite often you can somewhat brainlessly
[01:14:35.040 --> 01:14:38.520]   chuck a deep learning neural net at it
[01:14:38.520 --> 01:14:39.680]   and get a great result.
[01:14:39.680 --> 01:14:44.640]   Generally, for tabular data, I find that's not the case.
[01:14:44.640 --> 01:14:47.920]   I find I normally have to think pretty long and hard
[01:14:47.920 --> 01:14:52.120]   about the feature engineering in order to get good results.
[01:14:52.120 --> 01:14:53.560]   But once you've got good features,
[01:14:53.560 --> 01:14:55.080]   you then want a good model.
[01:14:55.080 --> 01:14:56.760]   And so you, you know,
[01:14:56.760 --> 01:15:00.680]   and generally like the more features you have
[01:15:00.680 --> 01:15:02.720]   and the more levels in your categorical features
[01:15:02.720 --> 01:15:03.680]   and stuff like that, you know,
[01:15:03.680 --> 01:15:07.520]   the more value you'll get from more sophisticated models.
[01:15:07.520 --> 01:15:11.400]   But yeah, I definitely would say a,
[01:15:11.400 --> 01:15:13.800]   an insight here is that, you know,
[01:15:13.800 --> 01:15:16.720]   you wanna include simple baselines as well.
[01:15:16.720 --> 01:15:19.200]   And we're gonna be seeing even more of that
[01:15:19.200 --> 01:15:22.880]   in a couple of notebooks time.
[01:15:22.880 --> 01:15:33.640]   So we've just seen how you can build stuff from scratch.
[01:15:33.640 --> 01:15:36.520]   We'll now see why you shouldn't.
[01:15:36.520 --> 01:15:38.080]   I mean, I say you shouldn't.
[01:15:38.080 --> 01:15:40.960]   You should to learn that why you probably won't want to
[01:15:40.960 --> 01:15:42.320]   in real life.
[01:15:42.320 --> 01:15:44.840]   When you're doing stuff in real life,
[01:15:44.840 --> 01:15:46.200]   you don't wanna be fiddling around
[01:15:46.200 --> 01:15:49.560]   with all this annoying initialization stuff
[01:15:49.560 --> 01:15:53.600]   and learning rate stuff and dummy variable stuff
[01:15:53.600 --> 01:15:56.080]   and normalization stuff and so forth,
[01:15:56.080 --> 01:15:59.880]   because we can do it for you.
[01:15:59.880 --> 01:16:02.080]   And it's not like everything's so automated
[01:16:02.080 --> 01:16:03.400]   that you don't get to make choices,
[01:16:03.400 --> 01:16:05.040]   but you want like,
[01:16:05.040 --> 01:16:08.840]   you wanna make the choice not to do things the obvious way
[01:16:08.840 --> 01:16:12.080]   and have everything else done the obvious way for you.
[01:16:12.080 --> 01:16:14.040]   So that's why we're gonna look at this,
[01:16:14.040 --> 01:16:16.960]   why you should use a framework notebook.
[01:16:16.960 --> 01:16:19.480]   And again, I'm gonna look at the clean version of it.
[01:16:19.480 --> 01:16:20.800]   And again, in the clean version of it,
[01:16:20.800 --> 01:16:25.040]   step one is to download the data as appropriate
[01:16:25.040 --> 01:16:27.640]   for the Kaggle or non Kaggle environment
[01:16:27.640 --> 01:16:31.800]   and set the display options and set the random seed
[01:16:31.800 --> 01:16:33.600]   and read the data frame.
[01:16:33.600 --> 01:16:39.680]   All right, now there was so much fussing around
[01:16:39.680 --> 01:16:42.320]   with the doing it from scratch version
[01:16:42.320 --> 01:16:44.360]   that I did not wanna do any feature engineering
[01:16:44.360 --> 01:16:46.120]   'cause every column I added was another thing
[01:16:46.120 --> 01:16:48.600]   I had to think about dummy variables and normalization
[01:16:48.600 --> 01:16:53.600]   and random coefficient initialization and blah, blah, blah.
[01:16:53.600 --> 01:16:58.240]   But with a framework, everything's so easy,
[01:16:58.240 --> 01:17:01.660]   you can do all the feature engineering you want.
[01:17:01.660 --> 01:17:05.080]   Because this isn't a lesson about feature engineering,
[01:17:05.080 --> 01:17:08.080]   instead, I plagiarized entirely
[01:17:08.080 --> 01:17:12.160]   from this fantastic advanced feature engineering tutorial
[01:17:12.160 --> 01:17:13.000]   on Kaggle.
[01:17:13.000 --> 01:17:22.560]   And what this tutorial found was that in addition
[01:17:22.560 --> 01:17:24.200]   to the log fair we've already done,
[01:17:24.200 --> 01:17:26.700]   that you can do cool stuff with the deck,
[01:17:26.700 --> 01:17:28.480]   with adding up the number of family members,
[01:17:28.480 --> 01:17:29.920]   with the people who are traveling alone,
[01:17:29.920 --> 01:17:31.940]   how many people are on each ticket.
[01:17:31.940 --> 01:17:33.760]   And finally, we're gonna do stuff with the name,
[01:17:33.760 --> 01:17:37.700]   which is we're gonna grab the Mr, Mrs, Master, whatever.
[01:17:40.360 --> 01:17:42.720]   So we're gonna create a function to do
[01:17:42.720 --> 01:17:43.640]   some feature engineering.
[01:17:43.640 --> 01:17:47.280]   And if you wanna learn a bit of pandas,
[01:17:47.280 --> 01:17:52.880]   here's some great lines of code to step through one by one.
[01:17:52.880 --> 01:17:55.080]   And again, take this out of a function,
[01:17:55.080 --> 01:17:57.600]   put them into individual cells, run each one,
[01:17:57.600 --> 01:18:02.580]   look up the tutorials, what does str do, what does map do,
[01:18:02.580 --> 01:18:04.680]   what does group by and transform do,
[01:18:04.680 --> 01:18:07.520]   what does value counts do.
[01:18:07.520 --> 01:18:10.640]   These are all like, part of the reason I put this here
[01:18:10.640 --> 01:18:13.160]   was the folks that haven't done much of any pandas
[01:18:13.160 --> 01:18:16.800]   to have some examples of functions that I think are useful
[01:18:16.800 --> 01:18:19.640]   and actually refactored this code quite a bit
[01:18:19.640 --> 01:18:22.700]   to try to show off some features of pandas
[01:18:22.700 --> 01:18:23.800]   I think are really nice.
[01:18:23.800 --> 01:18:27.700]   So we'll do the same random split as before,
[01:18:27.700 --> 01:18:29.360]   so passing in the same seed.
[01:18:29.360 --> 01:18:34.360]   And so now we're gonna do the same set of steps
[01:18:34.360 --> 01:18:36.000]   that we did manually with fast AI.
[01:18:37.440 --> 01:18:42.080]   So we wanna create a tabular model data set
[01:18:42.080 --> 01:18:43.880]   based on a pandas data frame.
[01:18:43.880 --> 01:18:46.760]   And here is the data frame.
[01:18:46.760 --> 01:18:50.640]   These are the train versus validation splits I wanna use.
[01:18:50.640 --> 01:18:55.240]   Here's a list of all the stuff I want done, please.
[01:18:55.240 --> 01:18:58.120]   Deal with dummy variables for me,
[01:18:58.120 --> 01:19:01.360]   deal with missing values for me,
[01:19:01.360 --> 01:19:03.540]   normalize continuous variables for me.
[01:19:05.200 --> 01:19:08.360]   I'm gonna tell you which ones are the categorical variables.
[01:19:08.360 --> 01:19:10.520]   So here's, for example, pre-class was a number,
[01:19:10.520 --> 01:19:13.120]   but I'm telling fast AI to treat it as categorical.
[01:19:13.120 --> 01:19:15.960]   Here's all the continuous variables.
[01:19:15.960 --> 01:19:19.040]   Here's my dependent variable,
[01:19:19.040 --> 01:19:21.040]   and the dependent variable is a category.
[01:19:21.040 --> 01:19:26.040]   So create data loaders from that place
[01:19:26.040 --> 01:19:29.820]   and save models right here in this directory.
[01:19:29.820 --> 01:19:32.880]   That's it.
[01:19:32.880 --> 01:19:35.000]   That's all the pre-processing I need to do.
[01:19:35.000 --> 01:19:38.040]   Even with all those extra engineered features.
[01:19:38.040 --> 01:19:40.520]   Create a learner.
[01:19:40.520 --> 01:19:43.880]   Okay, so this remember is something
[01:19:43.880 --> 01:19:48.240]   that contains a model and data.
[01:19:48.240 --> 01:19:51.040]   And I want you to put in two hidden layers
[01:19:51.040 --> 01:19:53.300]   with 10 units and 10 units,
[01:19:53.300 --> 01:19:55.880]   just like we did in our final example.
[01:19:55.880 --> 01:19:58.760]   What learning rate should I use?
[01:19:58.760 --> 01:20:01.720]   Make a suggestion for me, please.
[01:20:01.720 --> 01:20:03.240]   So call it LR find.
[01:20:03.240 --> 01:20:05.760]   You can use this for any fast AI model.
[01:20:05.760 --> 01:20:10.240]   Now, what this does is it starts at a learning rate
[01:20:10.240 --> 01:20:13.080]   that's very, very small, 10 to the negative seven,
[01:20:13.080 --> 01:20:15.620]   and it puts in one batch of data,
[01:20:15.620 --> 01:20:16.960]   and it calculates the loss.
[01:20:16.960 --> 01:20:21.280]   And then it increases the learning rate slightly
[01:20:21.280 --> 01:20:23.200]   and puts through another batch of data.
[01:20:23.200 --> 01:20:25.760]   And it keeps doing that for higher and higher learning rates,
[01:20:25.760 --> 01:20:27.160]   and it keeps track of the loss
[01:20:27.160 --> 01:20:29.120]   as it increases the learning rate.
[01:20:29.120 --> 01:20:30.840]   Just one batch of data at a time.
[01:20:31.680 --> 01:20:34.520]   And what happens is for the very small learning rates,
[01:20:34.520 --> 01:20:35.520]   nothing happens.
[01:20:35.520 --> 01:20:38.560]   But then once you get high enough,
[01:20:38.560 --> 01:20:39.800]   the loss starts improving.
[01:20:39.800 --> 01:20:43.440]   And then as it gets higher, it improves faster.
[01:20:43.440 --> 01:20:45.080]   Until you make the learning rate so big
[01:20:45.080 --> 01:20:47.640]   that it overshoots and then it kills it.
[01:20:47.640 --> 01:20:50.140]   And so generally somewhere around here
[01:20:50.140 --> 01:20:51.640]   is the learning rate you want.
[01:20:51.640 --> 01:20:54.320]   Fast AI has a few different ways
[01:20:54.320 --> 01:20:55.920]   of recommending a learning rate.
[01:20:55.920 --> 01:20:57.920]   You can look up the docs to see what they mean.
[01:20:57.920 --> 01:21:01.120]   I generally find if you choose slide and valley
[01:21:01.120 --> 01:21:03.480]   and pick one between the two,
[01:21:03.480 --> 01:21:05.580]   you get a pretty good learning rate.
[01:21:05.580 --> 01:21:10.280]   So here we've got about 0.01 and about 0.08.
[01:21:10.280 --> 01:21:11.560]   So I picked 0.03.
[01:21:11.560 --> 01:21:17.420]   So just run a bunch of epochs, away it goes.
[01:21:17.420 --> 01:21:21.400]   This is a bit crazy.
[01:21:21.400 --> 01:21:24.200]   After all that, we've ended up exactly the same accuracy
[01:21:24.200 --> 01:21:25.640]   as the last two models.
[01:21:25.640 --> 01:21:27.020]   That's just a coincidence, right?
[01:21:27.020 --> 01:21:29.720]   I mean, there's nothing particularly about that accuracy.
[01:21:30.400 --> 01:21:34.640]   And so at this point, we can now submit that to Kaggle.
[01:21:34.640 --> 01:21:37.840]   Now, remember with the linear model,
[01:21:37.840 --> 01:21:41.320]   we had to repeat all of the pre-processing steps
[01:21:41.320 --> 01:21:43.320]   on the test set in exactly the same way.
[01:21:43.320 --> 01:21:46.640]   Don't have to worry about it with fast AI.
[01:21:46.640 --> 01:21:48.820]   And fast AI, I mean, we still have to deal
[01:21:48.820 --> 01:21:52.680]   with the fill missing for fair, 'cause that's that.
[01:21:52.680 --> 01:21:56.160]   We have to add our feature engineering features.
[01:21:56.160 --> 01:21:58.360]   But all the pre-processing, we just have to use
[01:21:58.360 --> 01:22:00.840]   this one function called test_dl.
[01:22:00.840 --> 01:22:03.400]   That says create a data loader that contains
[01:22:03.400 --> 01:22:06.500]   exactly the same pre-processing steps that our learner used.
[01:22:06.500 --> 01:22:10.000]   And that's it, that's all you need.
[01:22:10.000 --> 01:22:14.160]   So just 'cause you wanna make sure that your inference time,
[01:22:14.160 --> 01:22:16.880]   transformations, pre-processing are exactly the same
[01:22:16.880 --> 01:22:17.880]   as a training time.
[01:22:17.880 --> 01:22:21.260]   So this is the magic method which does that,
[01:22:21.260 --> 01:22:22.400]   just one line of code.
[01:22:22.400 --> 01:22:25.880]   And then to get your predictions,
[01:22:25.880 --> 01:22:29.240]   you just say get preds and pass in
[01:22:29.240 --> 01:22:30.800]   that data loader I just built.
[01:22:30.800 --> 01:22:33.800]   And so then these three lines of code
[01:22:33.800 --> 01:22:36.820]   are the same as the previous notebook.
[01:22:36.820 --> 01:22:39.360]   And we can take a look at the top
[01:22:39.360 --> 01:22:42.520]   and as you can see, there it is.
[01:22:42.520 --> 01:22:46.460]   So how did that go?
[01:22:46.460 --> 01:22:53.760]   I don't remember.
[01:22:55.600 --> 01:22:57.000]   Oh, I didn't say.
[01:22:57.000 --> 01:23:00.520]   I think it was again, basically middle of the pack,
[01:23:00.520 --> 01:23:01.640]   if I remember correctly.
[01:23:01.640 --> 01:23:06.480]   So one of the nice things about,
[01:23:06.480 --> 01:23:13.640]   now that it's so easy to add features and build models
[01:23:13.640 --> 01:23:18.080]   is we can experiment with things much more quickly.
[01:23:18.080 --> 01:23:21.880]   So I'm gonna show you how easy it is to experiment with,
[01:23:21.880 --> 01:23:24.080]   what's often considered a fairly advanced idea,
[01:23:24.080 --> 01:23:25.400]   which is called Ensembling.
[01:23:25.400 --> 01:23:28.320]   There's lots of ways of doing Ensembling,
[01:23:28.320 --> 01:23:32.800]   but basically Ensembling is about creating multiple models
[01:23:32.800 --> 01:23:36.520]   and combining their predictions.
[01:23:36.520 --> 01:23:39.500]   And the easiest kind of ensemble to do
[01:23:39.500 --> 01:23:44.500]   is just to literally just build multiple models.
[01:23:44.500 --> 01:23:47.340]   And so each one is gonna have a different set
[01:23:47.340 --> 01:23:50.340]   of randomly initialized coefficients.
[01:23:50.340 --> 01:23:51.860]   And therefore each one's gonna end up
[01:23:51.860 --> 01:23:54.360]   with a different set of predictions.
[01:23:54.360 --> 01:23:56.120]   So I just create a function called Ensemble,
[01:23:56.120 --> 01:24:00.040]   which creates a learner, exactly the same as before,
[01:24:00.040 --> 01:24:04.600]   fits exactly the same as before and returns the predictions.
[01:24:04.600 --> 01:24:06.900]   And so we'll just use a list comprehension
[01:24:06.900 --> 01:24:08.120]   to do that five times.
[01:24:08.120 --> 01:24:14.020]   So that's gonna create a set of five predictions.
[01:24:14.020 --> 01:24:20.400]   Done.
[01:24:20.400 --> 01:24:22.880]   So now we can take all those predictions
[01:24:22.880 --> 01:24:27.880]   and stack them together and take the mean over the rows.
[01:24:27.880 --> 01:24:29.440]   So that's gonna give us the,
[01:24:29.440 --> 01:24:33.640]   well it's actually sorry, the mean over the first dimension.
[01:24:33.640 --> 01:24:36.620]   So the mean over the sets of predictions.
[01:24:36.620 --> 01:24:39.080]   And so that will give us the average prediction
[01:24:39.080 --> 01:24:40.340]   of our five models.
[01:24:40.340 --> 01:24:43.440]   And again, we can turn that into a CSV
[01:24:43.440 --> 01:24:46.520]   and submit it to cattle.
[01:24:46.520 --> 01:24:50.040]   And that one, I think that went a bit better, let's check.
[01:24:50.040 --> 01:24:52.120]   (sniffs)
[01:24:52.120 --> 01:24:58.120]   Yeah, okay, so that one actually finally gets
[01:24:58.120 --> 01:25:01.560]   into the top 20%, 25% in the competition.
[01:25:01.560 --> 01:25:03.480]   So I mean, not amazing by any means,
[01:25:03.480 --> 01:25:06.560]   but you can see that this simple step
[01:25:06.560 --> 01:25:10.120]   of creating five independently trained models
[01:25:10.120 --> 01:25:11.840]   just starting from different starting points
[01:25:11.840 --> 01:25:16.000]   in terms of random coefficients actually improved us
[01:25:16.000 --> 01:25:19.720]   from top 50% to top 25%.
[01:25:19.720 --> 01:25:20.560]   John.
[01:25:20.560 --> 01:25:23.480]   - Is there an argument
[01:25:23.480 --> 01:25:25.440]   because you've got a categorical result,
[01:25:25.440 --> 01:25:26.760]   you're zero one effectively.
[01:25:26.760 --> 01:25:28.360]   Is there an argument that you might use the mode
[01:25:28.360 --> 01:25:30.840]   of the ensemble rather than the numerical mean?
[01:25:30.840 --> 01:25:36.680]   - I mean, yes, there's an argument that's been made.
[01:25:36.680 --> 01:25:43.000]   And yeah, something I would just try.
[01:25:43.000 --> 01:25:48.000]   I generally find it's less good, but not always.
[01:25:49.360 --> 01:25:52.320]   And I don't feel like I've got a great intuition as to why.
[01:25:52.320 --> 01:25:56.240]   And I don't feel like I've seen any studies as to why.
[01:25:56.240 --> 01:25:59.080]   You could predict, like there's a few,
[01:25:59.080 --> 01:26:01.080]   there's at least three things you could do, right?
[01:26:01.080 --> 01:26:03.800]   You could take the, is it greater or less than 0.5?
[01:26:03.800 --> 01:26:08.120]   Ones and zeros and average them.
[01:26:08.120 --> 01:26:10.080]   Or you could take the mode of them.
[01:26:10.080 --> 01:26:12.760]   Or you could take the actual probability predictions
[01:26:12.760 --> 01:26:15.560]   and take the average of those and then threshold that.
[01:26:15.560 --> 01:26:18.680]   And I've seen examples where certainly
[01:26:18.680 --> 01:26:20.320]   both of the different averaging versions,
[01:26:20.320 --> 01:26:22.120]   each of them has been better.
[01:26:22.120 --> 01:26:23.720]   I don't think I've seen one with the modes better,
[01:26:23.720 --> 01:26:28.720]   but that was very popular back in the '90s, so.
[01:26:28.720 --> 01:26:35.640]   Yeah, so it'd be so easy to try.
[01:26:35.640 --> 01:26:37.200]   You might as well give it a go.
[01:26:37.200 --> 01:26:45.120]   Okay, we don't have time to finish the next notebook,
[01:26:45.120 --> 01:26:46.600]   but let's make a start on it.
[01:26:46.600 --> 01:26:51.600]   So the next notebook is random forests.
[01:26:51.600 --> 01:26:57.400]   How random forests really work.
[01:26:57.400 --> 01:27:02.120]   Who here has heard of random forests before?
[01:27:02.120 --> 01:27:04.720]   Nearly everybody, okay.
[01:27:04.720 --> 01:27:09.720]   So very popular, developed I think initially in 1999,
[01:27:09.720 --> 01:27:14.760]   but gradually improved in popularity during the 2000s.
[01:27:14.760 --> 01:27:18.000]   I was like, everybody kind of knew me
[01:27:18.000 --> 01:27:20.960]   as Mr. Random Forests for years.
[01:27:20.960 --> 01:27:26.320]   I implemented them like a couple of days
[01:27:26.320 --> 01:27:28.400]   after the original technical report came out.
[01:27:28.400 --> 01:27:30.360]   I was such a fan.
[01:27:30.360 --> 01:27:33.840]   All of my early Kaggle results for random forests.
[01:27:33.840 --> 01:27:34.660]   I love them.
[01:27:34.660 --> 01:27:39.280]   And I think hopefully you'll see why I'm such a fan of them
[01:27:39.280 --> 01:27:42.640]   because they're so elegant
[01:27:42.640 --> 01:27:44.760]   and they're almost impossible to mess up.
[01:27:44.760 --> 01:27:48.960]   A lot of people will say like,
[01:27:48.960 --> 01:27:50.740]   oh, why are you using machine learning?
[01:27:50.740 --> 01:27:55.740]   Why don't you use something simple like logistic regression?
[01:27:55.740 --> 01:27:59.160]   And I think like, oh gosh, in industry,
[01:27:59.160 --> 01:28:00.920]   I've seen far more examples of people
[01:28:00.920 --> 01:28:02.440]   screwing up logistic regression
[01:28:02.440 --> 01:28:04.200]   than successfully using logistic regression
[01:28:04.200 --> 01:28:07.800]   'cause it's very, very, very, very difficult to do correctly.
[01:28:07.800 --> 01:28:11.060]   You've got to make sure you've got the correct transformations
[01:28:11.060 --> 01:28:12.620]   and the correct interactions
[01:28:12.620 --> 01:28:15.240]   and the correct outlier handling and blah, blah, blah.
[01:28:15.240 --> 01:28:18.360]   And anything you get wrong, the entire thing falls apart.
[01:28:18.360 --> 01:28:24.480]   Random Forests, it's very rare
[01:28:24.480 --> 01:28:25.960]   that I've seen somebody screw up
[01:28:25.960 --> 01:28:27.240]   a random forest in industry.
[01:28:27.240 --> 01:28:28.760]   They're very hard to screw up
[01:28:28.760 --> 01:28:33.180]   because they're so resilient and you'll see why.
[01:28:33.180 --> 01:28:36.440]   So in this notebook,
[01:28:39.760 --> 01:28:42.360]   just by the way, rather than importing numpy and pandas
[01:28:42.360 --> 01:28:44.040]   and matplotlib and blah, blah, blah,
[01:28:44.040 --> 01:28:45.660]   there's a little handy shortcut,
[01:28:45.660 --> 01:28:49.080]   which is if you just import everything from fastai.imports,
[01:28:49.080 --> 01:28:52.200]   that imports all the things that you normally want.
[01:28:52.200 --> 01:28:54.480]   So, I mean, it doesn't do anything special,
[01:28:54.480 --> 01:28:57.240]   but it's just saved some messing around.
[01:28:57.240 --> 01:28:59.840]   So again, we've got our cell here to grab the data.
[01:28:59.840 --> 01:29:06.080]   And I'm just gonna do some basic pre-processing here
[01:29:09.120 --> 01:29:11.960]   with my fill-in-A for the fair,
[01:29:11.960 --> 01:29:13.860]   only needed for the test set, of course.
[01:29:13.860 --> 01:29:20.480]   Grab the modes and do the fill-in-A on the modes,
[01:29:20.480 --> 01:29:22.520]   take the log fair.
[01:29:22.520 --> 01:29:24.800]   And then I've got a couple of new steps here,
[01:29:24.800 --> 01:29:29.400]   which is converting embarked insects
[01:29:29.400 --> 01:29:33.160]   into categorical variables.
[01:29:33.160 --> 01:29:34.560]   What does that mean?
[01:29:34.560 --> 01:29:36.400]   Well, let's just run this on both the data frame
[01:29:36.400 --> 01:29:38.320]   and the test data frame.
[01:29:39.320 --> 01:29:42.560]   Split things into categories and continuous.
[01:29:42.560 --> 01:29:49.640]   And sex is a categorical variable, so let's look at it.
[01:29:49.640 --> 01:29:51.700]   Well, that's interesting.
[01:29:51.700 --> 01:29:54.840]   It looks exactly the same as before, male and female,
[01:29:54.840 --> 01:29:56.860]   but now it's got a category
[01:29:56.860 --> 01:29:59.040]   and it's got a list of categories.
[01:29:59.040 --> 01:30:00.360]   What's happened here?
[01:30:00.360 --> 01:30:04.080]   Well, what's happened is pandas has made a list
[01:30:04.080 --> 01:30:06.200]   of all of the unique values of this field.
[01:30:07.460 --> 01:30:09.520]   And behind the scenes,
[01:30:09.520 --> 01:30:12.960]   if you look at the cat codes you can see behind the scenes,
[01:30:12.960 --> 01:30:14.860]   it's actually turned them into numbers.
[01:30:14.860 --> 01:30:20.660]   It looks up this one into this list to get male,
[01:30:20.660 --> 01:30:23.360]   looks up this zero into this list to get female.
[01:30:23.360 --> 01:30:27.160]   So when it printed out, it prints out the friendly version,
[01:30:27.160 --> 01:30:29.800]   but it stores it as numbers.
[01:30:29.800 --> 01:30:34.800]   Now, you'll see in a moment why this is helpful,
[01:30:35.660 --> 01:30:37.480]   but a key thing to point out
[01:30:37.480 --> 01:30:41.480]   is we're not gonna have to create any dummy variables.
[01:30:41.480 --> 01:30:46.960]   And even that first, second or third class,
[01:30:46.960 --> 01:30:50.080]   we're not gonna consider that categorical at all.
[01:30:50.080 --> 01:30:52.620]   And you'll see why in a moment.
[01:30:52.620 --> 01:31:00.200]   A random forest is an ensemble of trees.
[01:31:00.200 --> 01:31:03.540]   A tree is an ensemble of binary splits.
[01:31:03.540 --> 01:31:05.000]   And so we're gonna work from the bottom up.
[01:31:05.000 --> 01:31:06.520]   We're gonna first work,
[01:31:06.520 --> 01:31:09.440]   we're gonna first learn about what is a binary split.
[01:31:09.440 --> 01:31:16.960]   And we're gonna do it by looking at an example.
[01:31:16.960 --> 01:31:19.960]   Let's consider what would happen if we took
[01:31:19.960 --> 01:31:22.120]   all the passengers on the Titanic
[01:31:22.120 --> 01:31:25.620]   and grouped them into males and females.
[01:31:25.620 --> 01:31:26.680]   And let's look at two things.
[01:31:26.680 --> 01:31:28.960]   The first is let's look at their survival rate.
[01:31:28.960 --> 01:31:32.560]   So about 20% survival rate for males
[01:31:32.560 --> 01:31:34.880]   and about 75% for females.
[01:31:34.880 --> 01:31:37.440]   And let's look at the histogram.
[01:31:37.440 --> 01:31:38.960]   How many of them are there?
[01:31:38.960 --> 01:31:41.000]   About twice as many males as females.
[01:31:41.000 --> 01:31:43.600]   Consider what would happen if you created
[01:31:43.600 --> 01:31:47.580]   the world's simplest model, which was what sex are they?
[01:31:47.580 --> 01:31:51.760]   That wouldn't be bad, would it?
[01:31:51.760 --> 01:31:53.400]   Because there's a big difference
[01:31:53.400 --> 01:31:54.640]   between the males and the females.
[01:31:54.640 --> 01:31:57.160]   A huge difference in survival rate.
[01:31:57.160 --> 01:32:01.400]   So if we said, oh, if you're a man, you probably died.
[01:32:01.400 --> 01:32:03.680]   If you're a woman, you probably survived.
[01:32:03.680 --> 01:32:06.240]   Or not just a man or a boy, so a male or a female.
[01:32:06.240 --> 01:32:09.120]   That would be a pretty good model
[01:32:09.120 --> 01:32:11.900]   because it's done a good job of splitting it
[01:32:11.900 --> 01:32:14.600]   into two groups that have very different survival rates.
[01:32:14.600 --> 01:32:19.180]   This is called a binary split.
[01:32:19.180 --> 01:32:22.080]   A binary split is something that splits
[01:32:22.080 --> 01:32:25.540]   the rows into two groups, hence binary.
[01:32:25.540 --> 01:32:29.260]   Let's talk about another example of a binary split.
[01:32:30.860 --> 01:32:32.720]   I'm getting ahead of myself.
[01:32:32.720 --> 01:32:35.760]   Before we do that, let's look at what would happen
[01:32:35.760 --> 01:32:37.400]   if we used this model.
[01:32:37.400 --> 01:32:41.400]   So if we created a model which just looked at sex,
[01:32:41.400 --> 01:32:42.800]   how good would it be?
[01:32:42.800 --> 01:32:44.600]   So to figure that out, we first have to split
[01:32:44.600 --> 01:32:46.680]   into training and test sets.
[01:32:46.680 --> 01:32:49.560]   So let's go ahead and do that.
[01:32:49.560 --> 01:32:55.280]   And then let's convert all of our categorical variables
[01:32:55.280 --> 01:32:56.680]   into their codes.
[01:32:56.680 --> 01:32:59.560]   So we've now got zero, one, two, whatever.
[01:32:59.560 --> 01:33:01.720]   We don't have male or female there anymore.
[01:33:01.720 --> 01:33:08.920]   And let's also create something that returns
[01:33:08.920 --> 01:33:13.840]   the independent variables, which we'll call the X's,
[01:33:13.840 --> 01:33:18.040]   and the dependent variable, which we'll call Y.
[01:33:18.040 --> 01:33:20.800]   And so we can now get the X's and the Y
[01:33:20.800 --> 01:33:23.960]   for each of the training set and the validation set.
[01:33:23.960 --> 01:33:26.320]   And so now let's create some predictions.
[01:33:26.320 --> 01:33:28.520]   We'll predict that they survived
[01:33:28.520 --> 01:33:32.800]   if their sex is zero, so if they're female.
[01:33:32.800 --> 01:33:34.100]   So how good is that model?
[01:33:34.100 --> 01:33:39.880]   Remember I told you that to calculate mean absolute error,
[01:33:39.880 --> 01:33:42.800]   we can get psychic learn or PyTorch,
[01:33:42.800 --> 01:33:45.040]   whatever we do it for us, instead of doing it ourselves.
[01:33:45.040 --> 01:33:46.880]   So just showing you.
[01:33:46.880 --> 01:33:49.940]   Here's how you do it just by importing it directly.
[01:33:49.940 --> 01:33:53.080]   This is exactly the same as the one we did manually
[01:33:53.080 --> 01:33:54.080]   in the last notebook.
[01:33:55.040 --> 01:33:58.640]   So that's a 21 and a half percent error.
[01:33:58.640 --> 01:34:00.040]   So that's a pretty good model.
[01:34:00.040 --> 01:34:04.800]   Could we do better?
[01:34:04.800 --> 01:34:08.200]   Well, here's another example.
[01:34:08.200 --> 01:34:09.360]   What about fair?
[01:34:09.360 --> 01:34:13.680]   So fair's different to sex because fair is continuous,
[01:34:13.680 --> 01:34:15.680]   or log fair, I'll take.
[01:34:15.680 --> 01:34:18.360]   But we could still split it into two groups.
[01:34:18.360 --> 01:34:23.240]   So here's, for all the people that didn't survive,
[01:34:23.240 --> 01:34:25.380]   this is their median fair here,
[01:34:25.380 --> 01:34:29.440]   and then this is their quartiles,
[01:34:29.440 --> 01:34:33.020]   for bigger fairs and quartiles for smaller fairs.
[01:34:33.020 --> 01:34:37.440]   And here's the median fair for those that survived,
[01:34:37.440 --> 01:34:38.520]   and their quartiles.
[01:34:38.520 --> 01:34:40.540]   So you can see the median fair for those that survived
[01:34:40.540 --> 01:34:43.580]   is higher than the median fair for those that didn't.
[01:34:43.580 --> 01:34:49.480]   We can't create a histogram exactly for fair
[01:34:49.480 --> 01:34:51.480]   because it's continuous.
[01:34:51.480 --> 01:34:53.840]   We could back it into groups to create a histogram,
[01:34:53.840 --> 01:34:56.760]   so I guess we can create a histogram, that wasn't true.
[01:34:56.760 --> 01:34:59.480]   What I should say is we could create something better,
[01:34:59.480 --> 01:35:01.120]   which is a kernel density plot,
[01:35:01.120 --> 01:35:03.040]   which is just like a histogram,
[01:35:03.040 --> 01:35:05.240]   but it's like with infinitely small bins.
[01:35:05.240 --> 01:35:08.040]   So we can see most people have a log fair of about two.
[01:35:08.040 --> 01:35:15.780]   So what if we split on about a bit under three?
[01:35:15.780 --> 01:35:21.120]   That seems to be a point at which there's a difference
[01:35:21.120 --> 01:35:24.100]   in survival between people that are greater than
[01:35:24.100 --> 01:35:25.780]   or less than that amount.
[01:35:25.780 --> 01:35:30.300]   So here's another model, log fair greater than 0.2.7.
[01:35:30.300 --> 01:35:37.280]   Much worse, 0.336 versus 0.215.
[01:35:37.280 --> 01:35:41.240]   Well, I don't know, maybe is there something better?
[01:35:41.240 --> 01:35:46.300]   We could create a little interactive tool.
[01:35:47.760 --> 01:35:52.760]   So what I want is something that can give us
[01:35:52.760 --> 01:35:58.120]   a quick score of how good a binary split is,
[01:35:58.120 --> 01:36:00.000]   and I want it to be able to work regardless
[01:36:00.000 --> 01:36:01.960]   of whether we're dealing with categorical
[01:36:01.960 --> 01:36:04.400]   or continuous or whatever data.
[01:36:04.400 --> 01:36:10.540]   So I just came up with a simple little way of scoring,
[01:36:10.540 --> 01:36:13.400]   which is I said, okay, if you split your data
[01:36:13.400 --> 01:36:17.320]   into two groups, a good split would be one
[01:36:17.320 --> 01:36:21.840]   in which all of the values of the dependent variable
[01:36:21.840 --> 01:36:24.680]   on one side are all pretty much the same,
[01:36:24.680 --> 01:36:26.280]   and all of the dependent variables on the other side
[01:36:26.280 --> 01:36:27.620]   are all pretty much the same.
[01:36:27.620 --> 01:36:32.000]   For example, if pretty much all the males
[01:36:32.000 --> 01:36:35.800]   had the same survival outcome, which didn't survive,
[01:36:35.800 --> 01:36:38.540]   and all the females had about the same survival outcome,
[01:36:38.540 --> 01:36:41.880]   which is they did survive, that would be a good split.
[01:36:41.880 --> 01:36:43.800]   It doesn't just work for categorical variables,
[01:36:43.800 --> 01:36:45.320]   it would work if your dependent variable
[01:36:45.320 --> 01:36:47.760]   was continuous as well.
[01:36:47.760 --> 01:36:50.960]   You basically want each of your groups within group
[01:36:50.960 --> 01:36:54.320]   to be as similar as possible on the dependent variable,
[01:36:54.320 --> 01:36:55.880]   and then the other group, you want them to be
[01:36:55.880 --> 01:36:59.080]   as similar as possible on the dependent variable.
[01:36:59.080 --> 01:37:01.520]   So how similar is all the things in a group?
[01:37:01.520 --> 01:37:05.160]   That's the standard deviation.
[01:37:05.160 --> 01:37:07.320]   So what I want to do is basically add the standard
[01:37:07.320 --> 01:37:10.320]   deviations of the two groups of the dependent variable,
[01:37:11.400 --> 01:37:16.200]   and then if there's a really small standard deviation,
[01:37:16.200 --> 01:37:19.360]   but it's a really small group, that's not very interesting,
[01:37:19.360 --> 01:37:21.740]   so I'll multiply it by the size.
[01:37:21.740 --> 01:37:23.280]   So this is something which says,
[01:37:23.280 --> 01:37:26.720]   what's the score for one of my groups, one of my sides?
[01:37:26.720 --> 01:37:28.660]   It's the standard deviation multiplied
[01:37:28.660 --> 01:37:30.520]   by how many things are in that group.
[01:37:30.520 --> 01:37:36.640]   So the total score is the score for the left-hand side,
[01:37:36.640 --> 01:37:38.560]   so all the things in one group,
[01:37:38.560 --> 01:37:40.840]   plus the score for the right-hand side,
[01:37:40.840 --> 01:37:42.440]   which is, tilde means not,
[01:37:42.440 --> 01:37:45.800]   so not left-hand side is right-hand side,
[01:37:45.800 --> 01:37:47.800]   and then we'll just take the average of that.
[01:37:47.800 --> 01:37:52.680]   So for example, if we split by sex
[01:37:52.680 --> 01:37:57.000]   is greater than or less than 0.5,
[01:37:57.000 --> 01:37:59.000]   that'll create two groups, males and females,
[01:37:59.000 --> 01:38:00.440]   and that gives us this score,
[01:38:00.440 --> 01:38:04.840]   and if we do log fair greater than or less than 2.7,
[01:38:04.840 --> 01:38:07.600]   it gives us this score, and lower score is better,
[01:38:07.600 --> 01:38:10.920]   so sex is better than log fair.
[01:38:10.920 --> 01:38:11.960]   So now that we've got that,
[01:38:11.960 --> 01:38:14.440]   we can use our favorite interact tool
[01:38:14.440 --> 01:38:19.960]   to create our little GUI,
[01:38:19.960 --> 01:38:22.160]   and so we can say, let's try like,
[01:38:22.160 --> 01:38:26.060]   oh, what about this one, can we, oops.
[01:38:26.060 --> 01:38:31.640]   Can we find something that's a bit better, 4.8, 0.485?
[01:38:31.640 --> 01:38:32.960]   No, not very good.
[01:38:32.960 --> 01:38:35.780]   What about P class?
[01:38:37.200 --> 01:38:42.000]   0.468, 0.460.
[01:38:42.000 --> 01:38:43.800]   So we can fiddle around with these.
[01:38:43.800 --> 01:38:48.200]   We could do the same thing for the categorical variables,
[01:38:48.200 --> 01:38:50.160]   so we already know that sex,
[01:38:50.160 --> 01:38:55.320]   we can get to 0.407, what about embarked?
[01:38:55.320 --> 01:39:01.240]   All right, so it looks like sex might be our best.
[01:39:01.240 --> 01:39:04.480]   Well, that was pretty inefficient, right?
[01:39:04.480 --> 01:39:05.560]   It would be nice if we could find
[01:39:05.560 --> 01:39:07.320]   some automatic way to do all that.
[01:39:07.320 --> 01:39:10.040]   Well, of course we can.
[01:39:10.040 --> 01:39:11.840]   For example, if we wanted to find
[01:39:11.840 --> 01:39:13.960]   what's the best split point for age,
[01:39:13.960 --> 01:39:16.840]   then we just have to create,
[01:39:16.840 --> 01:39:19.760]   let's do this again.
[01:39:19.760 --> 01:39:24.860]   If we want to find the best split point for age,
[01:39:24.860 --> 01:39:26.000]   we could just create a list
[01:39:26.000 --> 01:39:28.840]   of all of the unique values of age,
[01:39:28.840 --> 01:39:32.640]   and try H1, in turn, and see what score we get
[01:39:32.640 --> 01:39:36.440]   if we made a binary split on that level of age.
[01:39:36.440 --> 01:39:38.360]   So here's a list of all of the possible
[01:39:38.360 --> 01:39:41.000]   binary split thresholds for age.
[01:39:41.000 --> 01:39:44.960]   Let's go through all of them for each of them.
[01:39:44.960 --> 01:39:48.160]   Calculate the score.
[01:39:48.160 --> 01:39:54.600]   And then NumPy and PyTorch have an argmin function
[01:39:54.600 --> 01:39:57.600]   which tells you what index into that list is the smallest.
[01:39:57.600 --> 01:40:01.140]   So just to show you, here's the scores.
[01:40:01.140 --> 01:40:06.140]   And zero, one, two, three, four, five, six.
[01:40:06.140 --> 01:40:16.900]   Oh, sorry, zero, one, two, three, four, five, six.
[01:40:16.900 --> 01:40:21.900]   So apparently that value has the smallest score.
[01:40:21.900 --> 01:40:27.820]   So that tells us that for age,
[01:40:27.820 --> 01:40:30.240]   the threshold of six would be best.
[01:40:30.240 --> 01:40:35.100]   So here's something that just calculates that for a column.
[01:40:35.100 --> 01:40:37.200]   It calculates the best split point.
[01:40:37.200 --> 01:40:41.260]   So here's six, right?
[01:40:41.260 --> 01:40:44.060]   And it also tells us what the score is at that point,
[01:40:44.060 --> 01:40:46.380]   which is 0.478.
[01:40:46.380 --> 01:40:48.540]   So now we can just go through and
[01:40:48.540 --> 01:40:55.620]   calculates the score for the best split point
[01:40:55.620 --> 01:40:56.540]   for each column.
[01:40:58.240 --> 01:41:03.240]   And if we do that, we find that the lowest score is six.
[01:41:03.240 --> 01:41:13.300]   So that is how we calculate the best binary split.
[01:41:13.300 --> 01:41:19.820]   So we now know that the model that we created earlier,
[01:41:19.820 --> 01:41:25.860]   this one, is the best single binary split model
[01:41:25.860 --> 01:41:26.700]   we can find.
[01:41:28.060 --> 01:41:31.700]   So next week, we're gonna learn how we can recursively
[01:41:31.700 --> 01:41:33.920]   do this to create a decision tree,
[01:41:33.920 --> 01:41:37.500]   and then do that multiple times to create a random forest.
[01:41:37.500 --> 01:41:42.420]   But before we do, I wanna point something out,
[01:41:42.420 --> 01:41:44.780]   which is this ridiculously simple thing,
[01:41:44.780 --> 01:41:47.460]   which is find a single binary split in stock
[01:41:47.460 --> 01:41:49.660]   is a type of model.
[01:41:49.660 --> 01:41:51.840]   It has a name, it's called 1R.
[01:41:51.840 --> 01:41:55.740]   And the 1R model, it turned out in a review
[01:41:55.740 --> 01:41:59.100]   of machine learning methods in the 90s,
[01:41:59.100 --> 01:42:03.220]   turned out to be one of the best, if not the best,
[01:42:03.220 --> 01:42:04.780]   machine learning classifiers
[01:42:04.780 --> 01:42:07.060]   for a wide range of real world data sets.
[01:42:07.060 --> 01:42:10.940]   So that is to say, don't assume
[01:42:10.940 --> 01:42:15.700]   that you have to go complicated.
[01:42:15.700 --> 01:42:18.060]   It's not a bad idea to always start creating
[01:42:18.060 --> 01:42:22.300]   a baseline of 1R, a decision tree
[01:42:22.300 --> 01:42:24.940]   with a single binary split.
[01:42:24.940 --> 01:42:27.540]   And in fact, for the titanic competition,
[01:42:27.540 --> 01:42:28.580]   that's exactly what we do.
[01:42:28.580 --> 01:42:30.980]   If you look at the titanic competition on Kaggle,
[01:42:30.980 --> 01:42:34.020]   you'll find that what we did is our sample submission
[01:42:34.020 --> 01:42:37.980]   is one that just splits into male versus female.
[01:42:37.980 --> 01:42:41.100]   All right, thanks everybody.
[01:42:41.100 --> 01:42:42.420]   Hope you found that interesting,
[01:42:42.420 --> 01:42:44.460]   and I will see you next lesson.
[01:42:44.460 --> 01:42:45.300]   Bye.
[01:42:45.300 --> 01:42:55.300]   [BLANK_AUDIO]

