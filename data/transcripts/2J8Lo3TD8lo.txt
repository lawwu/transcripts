
[00:00:00.000 --> 00:00:05.000]   We can no longer kind of rely on things just getting better.
[00:00:05.000 --> 00:00:06.800]   Sort of every two or three years,
[00:00:06.800 --> 00:00:09.920]   we'll get another 50% or 2x energy efficiency,
[00:00:09.920 --> 00:00:12.080]   whatever the scaling is.
[00:00:12.080 --> 00:00:13.400]   That's kind of really slowing down.
[00:00:13.400 --> 00:00:15.840]   So the specialization of the processes
[00:00:15.840 --> 00:00:17.520]   is being driven by that.
[00:00:17.520 --> 00:00:20.680]   So we need an architecture that is more memory.
[00:00:20.680 --> 00:00:25.200]   And if you go back to the kind of fundamental processor,
[00:00:25.200 --> 00:00:27.640]   we don't move data very far.
[00:00:27.640 --> 00:00:30.360]   So the whole architecture is geared around data
[00:00:30.360 --> 00:00:32.480]   staying local to the process.
[00:00:32.480 --> 00:00:35.920]   And the physics of moving data is one of the things
[00:00:35.920 --> 00:00:37.160]   that really drives power.
[00:00:37.160 --> 00:00:39.960]   So there's kind of doing the actual operations,
[00:00:39.960 --> 00:00:42.560]   so driving the computation.
[00:00:42.560 --> 00:00:44.760]   And then there's moving data to and from
[00:00:44.760 --> 00:00:46.240]   your memory subsystem.
[00:00:46.240 --> 00:00:48.000]   So if your memory is very close,
[00:00:48.000 --> 00:00:50.040]   the cost of moving data there is a lot lower,
[00:00:50.040 --> 00:00:52.560]   energy costs, compared to if you got it,
[00:00:52.560 --> 00:00:54.240]   the cost is a lot higher.
[00:00:54.240 --> 00:00:56.560]   And this kind of goes into the power consumption
[00:00:56.560 --> 00:00:59.240]   of the device, where are you spending your power?
[00:00:59.240 --> 00:01:01.160]   - You're listening to Gradient Dissent,
[00:01:01.160 --> 00:01:03.640]   a show about machine learning in the real world.
[00:01:03.640 --> 00:01:05.800]   And I'm your host, Lukas Biewald.
[00:01:05.800 --> 00:01:08.880]   Phil Brown leads Graphcore's applications team,
[00:01:08.880 --> 00:01:11.840]   building high-performance machine learning applications
[00:01:11.840 --> 00:01:16.000]   for their intelligence processing units, or IPUs.
[00:01:16.000 --> 00:01:18.240]   Phil's background is in computational chemistry,
[00:01:18.240 --> 00:01:19.680]   which is maybe one of the topics
[00:01:19.680 --> 00:01:21.520]   I really wish I knew more about.
[00:01:21.520 --> 00:01:24.600]   And what he works on now is hardware for machine learning,
[00:01:24.600 --> 00:01:26.240]   which is the other topic that I really wish
[00:01:26.240 --> 00:01:27.080]   I knew more about.
[00:01:27.080 --> 00:01:29.480]   So I always say this, but I could not be more excited
[00:01:29.480 --> 00:01:31.240]   to talk to him today.
[00:01:31.240 --> 00:01:32.960]   I really want to talk about Graphcore
[00:01:32.960 --> 00:01:34.440]   and kind of what it does broadly
[00:01:34.440 --> 00:01:35.280]   and what you're doing there,
[00:01:35.280 --> 00:01:37.080]   but I thought it might be fun to start off with,
[00:01:37.080 --> 00:01:38.280]   I was looking at your background
[00:01:38.280 --> 00:01:40.040]   and I saw that you were originally trained
[00:01:40.040 --> 00:01:44.280]   as a computational chemist and then was working at Cray.
[00:01:44.280 --> 00:01:46.080]   And we've actually noticed that Weights & Biases
[00:01:46.080 --> 00:01:48.320]   is a whole bunch of computational chemists
[00:01:48.320 --> 00:01:50.880]   using our software, which has been intriguing.
[00:01:50.880 --> 00:01:53.200]   I kind of wanted to hear your career path
[00:01:53.200 --> 00:01:55.200]   and how you ended up at Graphcore.
[00:01:56.040 --> 00:01:56.880]   - Yeah, certainly.
[00:01:56.880 --> 00:01:59.440]   So it's been a bit of an interesting journey.
[00:01:59.440 --> 00:02:03.280]   And I would be interested to know what they were doing,
[00:02:03.280 --> 00:02:05.240]   whether they, I mean, I guess running sets
[00:02:05.240 --> 00:02:08.200]   of molecular dynamics or sort of quantum chemistry
[00:02:08.200 --> 00:02:10.320]   kind of calculations.
[00:02:10.320 --> 00:02:12.800]   - It seems like there's a lot of kind of drug discovery
[00:02:12.800 --> 00:02:14.800]   and some material science, yeah.
[00:02:14.800 --> 00:02:17.400]   - Okay, yeah, that's pretty much what I used to do
[00:02:17.400 --> 00:02:18.240]   a long time ago.
[00:02:18.240 --> 00:02:20.600]   So running sort of computational simulations
[00:02:20.600 --> 00:02:21.800]   of various different states.
[00:02:21.800 --> 00:02:24.560]   The way I ended up in sort of machine learning space
[00:02:24.560 --> 00:02:27.960]   was kind of via the high-performance computing arena.
[00:02:27.960 --> 00:02:31.400]   And actually my PhD was writing computational chemistry code.
[00:02:31.400 --> 00:02:33.640]   So quantum chemistry, density functional theory
[00:02:33.640 --> 00:02:36.560]   embedded inside a molecular dynamics simulation
[00:02:36.560 --> 00:02:38.440]   and actually looking to try and accelerate
[00:02:38.440 --> 00:02:39.600]   the density functional theory,
[00:02:39.600 --> 00:02:42.400]   the quantum chemistry bit of that
[00:02:42.400 --> 00:02:44.960]   using very early accelerators.
[00:02:44.960 --> 00:02:48.880]   So actually I did a PhD at the University of Bristol
[00:02:48.880 --> 00:02:51.520]   and there was a company in Bristol called ClearSpeed
[00:02:51.520 --> 00:02:54.320]   that were building an early numerical accelerator.
[00:02:54.320 --> 00:02:58.120]   I think before we had GPGPUs, before that kind of period,
[00:02:58.120 --> 00:03:00.440]   or right when the first GPUs were coming out,
[00:03:00.440 --> 00:03:02.440]   that same kind of cell processor came out
[00:03:02.440 --> 00:03:04.360]   if there were any other people who were playing around,
[00:03:04.360 --> 00:03:05.960]   so PS1, that kind of era.
[00:03:05.960 --> 00:03:08.440]   And this company was trying to build dull precision,
[00:03:08.440 --> 00:03:10.240]   so HPC accelerators.
[00:03:10.240 --> 00:03:13.040]   And so I was actually writing, trying to use those
[00:03:13.040 --> 00:03:15.280]   for these kind of computational chemistry simulations.
[00:03:15.280 --> 00:03:19.000]   So about 2005, six, seven kind of timeframes.
[00:03:19.000 --> 00:03:21.720]   And actually as it happens, my boss today
[00:03:21.720 --> 00:03:23.600]   is somebody who worked at ClearSpeed
[00:03:23.600 --> 00:03:25.560]   and was building those kinds of things.
[00:03:25.560 --> 00:03:26.480]   And a number of these,
[00:03:26.480 --> 00:03:28.280]   particularly the software team at Graphcore
[00:03:28.280 --> 00:03:30.320]   have kind of heritage going back to that.
[00:03:30.320 --> 00:03:32.360]   There's a bit of a kind of a Bristol group
[00:03:32.360 --> 00:03:33.840]   of hardware and software engineers
[00:03:33.840 --> 00:03:36.680]   who have done various kinds of things over the years.
[00:03:36.680 --> 00:03:39.520]   So that was kind of, and that was really what got me in
[00:03:39.520 --> 00:03:40.680]   from being a chemist,
[00:03:40.680 --> 00:03:43.440]   I am not a computer scientist in any sense,
[00:03:43.440 --> 00:03:44.840]   I kind of dabble a little bit,
[00:03:44.840 --> 00:03:47.000]   but I am not a kind of a software developer
[00:03:47.000 --> 00:03:48.400]   or a computer scientist.
[00:03:48.400 --> 00:03:51.280]   And that took me from a pure kind of chemist
[00:03:51.280 --> 00:03:55.720]   into the HPC and the kind of computational science domain
[00:03:55.720 --> 00:03:59.000]   of high-performance computing and building systems.
[00:03:59.000 --> 00:04:01.360]   And then I spent a couple of years
[00:04:01.360 --> 00:04:03.360]   in a consultancy that was specializing
[00:04:03.360 --> 00:04:05.480]   in helping people buy these systems actually.
[00:04:05.480 --> 00:04:08.080]   And then went to Cray and was helping
[00:04:08.080 --> 00:04:09.440]   sort of design and build these systems.
[00:04:09.440 --> 00:04:11.400]   And actually I did a variety of different things
[00:04:11.400 --> 00:04:12.600]   for a couple of years,
[00:04:12.600 --> 00:04:14.080]   focusing actually on weather forecasting.
[00:04:14.080 --> 00:04:17.480]   So the numerical science and how you build
[00:04:17.480 --> 00:04:20.600]   large production systems for weather forecasting.
[00:04:20.600 --> 00:04:22.400]   And particularly in the US,
[00:04:22.400 --> 00:04:25.720]   NOAA and the National Weather Service here in the UK,
[00:04:25.720 --> 00:04:27.960]   the Met Office and actually around the world.
[00:04:27.960 --> 00:04:29.640]   At that time, Cray were building systems
[00:04:29.640 --> 00:04:32.560]   for 80% of the large weather centers.
[00:04:32.560 --> 00:04:33.920]   So the kind of the national weather centers
[00:04:33.920 --> 00:04:34.760]   and those kinds of things.
[00:04:34.760 --> 00:04:36.680]   So that was kind of great fun.
[00:04:36.680 --> 00:04:40.320]   But as kind of the machine learning domain
[00:04:40.320 --> 00:04:41.840]   started taking off,
[00:04:41.840 --> 00:04:44.720]   I was kind of quite interested in that as a field.
[00:04:44.720 --> 00:04:46.600]   It was clear actually that supercomputing
[00:04:46.600 --> 00:04:48.960]   for the high end wasn't gonna continue growing
[00:04:48.960 --> 00:04:50.520]   at a particularly interesting rate.
[00:04:50.520 --> 00:04:52.400]   And there happened to be this kind of little company
[00:04:52.400 --> 00:04:53.800]   in Bristol called Graphcore
[00:04:53.800 --> 00:04:56.680]   that had a really interesting technology,
[00:04:56.680 --> 00:04:59.640]   was just sort of starting to make some waves.
[00:04:59.640 --> 00:05:02.960]   And so I sort of got in touch with Matt actually
[00:05:02.960 --> 00:05:05.360]   and a few other people and ended up sort of coming
[00:05:05.360 --> 00:05:08.400]   to join Graphcore as actually working
[00:05:08.400 --> 00:05:10.880]   as part of the leading field engineering group,
[00:05:10.880 --> 00:05:13.040]   so customer facing and technical teams
[00:05:13.040 --> 00:05:15.400]   were actually working directly with our customers
[00:05:15.400 --> 00:05:16.600]   to build applications.
[00:05:16.600 --> 00:05:17.760]   - Could you talk a little bit about,
[00:05:17.760 --> 00:05:19.120]   I've always been fascinated by this,
[00:05:19.120 --> 00:05:21.080]   about how weather prediction works?
[00:05:21.080 --> 00:05:26.960]   - So weather prediction is an interesting field.
[00:05:26.960 --> 00:05:29.240]   Sort of fundamentally, it's actually quite simple.
[00:05:29.240 --> 00:05:34.240]   The atmosphere is a set of sort of fluids interacting.
[00:05:34.240 --> 00:05:38.400]   And so you could describe that with a set of equations
[00:05:38.400 --> 00:05:41.440]   and you can just kind of solve using those equations.
[00:05:41.440 --> 00:05:43.480]   And so in some senses,
[00:05:43.480 --> 00:05:46.120]   it's just a giant fluid dynamics simulation,
[00:05:46.120 --> 00:05:47.560]   but it's also a bit more complicated than that
[00:05:47.560 --> 00:05:49.200]   because you have the particles,
[00:05:49.200 --> 00:05:52.840]   you've got lots of interesting sort of surface effects.
[00:05:52.840 --> 00:05:53.960]   You've got the Coriolis effect
[00:05:53.960 --> 00:05:55.840]   where the Earth is actually rotating.
[00:05:55.840 --> 00:05:58.400]   You've also got quite an interesting initialization problem
[00:05:58.400 --> 00:06:00.600]   in that space because you don't,
[00:06:00.600 --> 00:06:03.760]   I mean, climate simulations are much longer duration.
[00:06:03.760 --> 00:06:05.480]   Weather forecasting simulations are typically,
[00:06:05.480 --> 00:06:08.280]   I mean, you might only care about the next 10 hours,
[00:06:08.280 --> 00:06:09.640]   12 hours, two weeks.
[00:06:09.640 --> 00:06:12.160]   But so your initialization is actually critical.
[00:06:12.160 --> 00:06:14.040]   So actually the data assimilation
[00:06:14.040 --> 00:06:16.120]   where they take the sort of the global set
[00:06:16.120 --> 00:06:17.440]   of satellite observations
[00:06:17.440 --> 00:06:19.680]   and other kinds of weather observations
[00:06:19.680 --> 00:06:21.440]   and integrate those into the model
[00:06:21.440 --> 00:06:22.640]   as the starting position,
[00:06:22.640 --> 00:06:24.960]   they're really critically important part of that.
[00:06:24.960 --> 00:06:28.280]   So there's lots of quite hairy maths
[00:06:28.280 --> 00:06:32.080]   and lots of big computers to try and scale these systems.
[00:06:32.080 --> 00:06:34.560]   The other thing that's quite close to machine learning
[00:06:34.560 --> 00:06:38.320]   or certainly common is this idea of sort of time to train,
[00:06:38.320 --> 00:06:40.480]   time to get to a solution is quite important.
[00:06:40.480 --> 00:06:42.760]   If you're running a big simulation,
[00:06:42.760 --> 00:06:46.080]   then actually if you're gonna have to wait three weeks
[00:06:46.080 --> 00:06:48.000]   for it, pointless actually running it,
[00:06:48.000 --> 00:06:50.640]   you need your experimental cycle has to be manageable.
[00:06:50.640 --> 00:06:52.280]   And in weather forecasting,
[00:06:52.280 --> 00:06:54.480]   a weather forecasting has to be,
[00:06:54.480 --> 00:06:56.680]   you will be able to predict two weeks in two hours
[00:06:56.680 --> 00:06:57.520]   or something like that.
[00:06:57.520 --> 00:06:59.880]   So actually being able to meet that kind
[00:06:59.880 --> 00:07:03.400]   of operational deadline for delivery was quite important.
[00:07:03.400 --> 00:07:04.960]   - How does the physics,
[00:07:04.960 --> 00:07:07.240]   like the kind of physics simulations compare
[00:07:07.240 --> 00:07:09.600]   to like a more sort of machine learning approach
[00:07:09.600 --> 00:07:13.120]   where you make less assumptions about the underlying physics
[00:07:13.120 --> 00:07:16.160]   and just try to treat it as a standard prediction problem?
[00:07:16.160 --> 00:07:19.600]   - In NWP and in most of the computational sciences
[00:07:19.600 --> 00:07:23.280]   in general, you're building a kind of a simulation based
[00:07:23.280 --> 00:07:28.080]   on some set of physics or chemistry or material science
[00:07:28.080 --> 00:07:30.240]   or whatever, particularly disciplinary in biology.
[00:07:30.240 --> 00:07:33.520]   There will be some set of fundamental principles
[00:07:33.520 --> 00:07:35.880]   that you are modeling in your system.
[00:07:35.880 --> 00:07:39.320]   And so it's very much a science-based,
[00:07:39.320 --> 00:07:41.200]   sort of first principle based approach
[00:07:41.200 --> 00:07:42.440]   to solving these problems.
[00:07:42.440 --> 00:07:45.720]   I mean, they typically do have approximations in them.
[00:07:45.720 --> 00:07:48.560]   And so actually there's quite a bit of interest,
[00:07:48.560 --> 00:07:50.520]   I think in the, particularly in the climate field,
[00:07:50.520 --> 00:07:53.280]   but also in the weather field of replacing some
[00:07:53.280 --> 00:07:55.280]   of their parameterizations of systems
[00:07:55.280 --> 00:07:57.560]   where the physics is too expensive to run.
[00:07:57.560 --> 00:08:00.240]   So the particle interactions are too expensive
[00:08:00.240 --> 00:08:02.600]   to model directly at large scale.
[00:08:02.600 --> 00:08:05.200]   And so up till now, they have used approximations
[00:08:05.200 --> 00:08:06.400]   for that, actually trying to replace
[00:08:06.400 --> 00:08:09.960]   their basic approximations with machine learning models
[00:08:09.960 --> 00:08:13.200]   that will be cheaper or more accurate or both.
[00:08:13.200 --> 00:08:15.760]   And so there is that kind of interaction
[00:08:15.760 --> 00:08:17.880]   where with everything, the entire world,
[00:08:17.880 --> 00:08:20.920]   you could technically simulate everything right down
[00:08:20.920 --> 00:08:23.040]   to the lowest kind of quantum dynamical,
[00:08:23.040 --> 00:08:25.280]   sort of quantum interaction state level,
[00:08:25.280 --> 00:08:26.800]   but that would be phenomenally expensive.
[00:08:26.800 --> 00:08:29.040]   You wouldn't necessarily want to do that.
[00:08:29.040 --> 00:08:30.520]   - Also you can't observe it, right?
[00:08:30.520 --> 00:08:33.000]   I mean, I would think the observations would be messy.
[00:08:33.000 --> 00:08:35.320]   - Well, so I mean, if you're going right down
[00:08:35.320 --> 00:08:36.800]   to sort of an individual electron,
[00:08:36.800 --> 00:08:39.560]   yes, you wouldn't be able to observe that state,
[00:08:39.560 --> 00:08:42.480]   but you could, the sort of quantum interactions,
[00:08:42.480 --> 00:08:44.920]   the way that the difference between sort of the biology
[00:08:44.920 --> 00:08:47.560]   and the chemistry or the molecular dynamical sphere
[00:08:47.560 --> 00:08:50.920]   and the quantum mechanics there is that
[00:08:50.920 --> 00:08:52.480]   where you've got the sort of binding energies
[00:08:52.480 --> 00:08:54.280]   where you're actually making and breaking bonds,
[00:08:54.280 --> 00:08:56.720]   those are sort of the quantum mechanical effects
[00:08:56.720 --> 00:08:59.240]   starting to come in as you're making those bonds.
[00:08:59.240 --> 00:09:02.920]   And so you can accurately sort of simulate those things.
[00:09:02.920 --> 00:09:05.000]   It's just, you can't observe the individual particles
[00:09:05.000 --> 00:09:05.840]   at that level.
[00:09:05.840 --> 00:09:07.960]   So the simulation of the kind of binding energy
[00:09:07.960 --> 00:09:09.440]   is still possible at that level.
[00:09:09.440 --> 00:09:11.400]   But I mean, that's phenomenally expensive.
[00:09:11.400 --> 00:09:13.920]   I mean, you can't, at the time I was doing it,
[00:09:13.920 --> 00:09:16.640]   it was difficult to model sort of water
[00:09:16.640 --> 00:09:19.000]   and maybe sort of small groups of water
[00:09:19.000 --> 00:09:20.960]   where you've got the hydrogen bonds.
[00:09:20.960 --> 00:09:22.840]   That was getting a little bit expensive.
[00:09:22.840 --> 00:09:24.720]   I suspect a decade on,
[00:09:24.720 --> 00:09:26.200]   we're probably a bit further than that now,
[00:09:26.200 --> 00:09:28.080]   but it's still, you won't be able to monitor.
[00:09:28.080 --> 00:09:30.040]   Well, they might just about be able to do,
[00:09:30.040 --> 00:09:32.200]   say, a full protein or something like that.
[00:09:32.200 --> 00:09:33.640]   But it's also a question of,
[00:09:33.640 --> 00:09:35.840]   is it meaningful to actually,
[00:09:35.840 --> 00:09:38.280]   you don't need that level of fidelity
[00:09:38.280 --> 00:09:40.440]   or you don't need that level of modeling.
[00:09:40.440 --> 00:09:42.680]   Where do you want to spend your complete time?
[00:09:42.680 --> 00:09:44.880]   - Or even your observation time.
[00:09:44.880 --> 00:09:47.240]   When I'm imagining the modeling the weather
[00:09:47.240 --> 00:09:48.160]   on the planet Earth,
[00:09:48.160 --> 00:09:51.120]   I'm sure you can't get very fine grained at all, right?
[00:09:51.120 --> 00:09:54.000]   From observing the state of Earth, right?
[00:09:54.000 --> 00:09:56.640]   - Well, that used to be the challenge.
[00:09:56.640 --> 00:09:58.200]   I mean, it very much used to be the challenge.
[00:09:58.200 --> 00:10:00.960]   It's a lot better now that they've got satellites
[00:10:00.960 --> 00:10:03.880]   that give them sort of complete world coverage.
[00:10:03.880 --> 00:10:05.800]   And the challenge before that was
[00:10:05.800 --> 00:10:08.120]   that you didn't have observations.
[00:10:08.120 --> 00:10:09.880]   And actually the metrophists do a great,
[00:10:09.880 --> 00:10:12.640]   or they have an interesting set of observations
[00:10:12.640 --> 00:10:17.640]   and an analysis of around D-Day when in 1944,
[00:10:17.640 --> 00:10:19.760]   when the invasion of Europe,
[00:10:19.760 --> 00:10:21.880]   the prediction of that weather window
[00:10:21.880 --> 00:10:23.640]   when they actually launched the invasion,
[00:10:23.640 --> 00:10:25.520]   the Germans did not think
[00:10:25.520 --> 00:10:27.360]   that there was going to be a weather window.
[00:10:27.360 --> 00:10:29.160]   Their analysis of the weather,
[00:10:29.160 --> 00:10:31.480]   because they didn't have observations in the North Atlantic,
[00:10:31.480 --> 00:10:33.200]   they had much, much faster observations
[00:10:33.200 --> 00:10:34.040]   in the North Atlantic.
[00:10:34.040 --> 00:10:35.640]   And so at that point in time,
[00:10:35.640 --> 00:10:38.160]   there was a real lack of observational information.
[00:10:38.160 --> 00:10:40.200]   I think that's being closed.
[00:10:40.200 --> 00:10:42.000]   I mean, satellites today give you
[00:10:42.000 --> 00:10:44.440]   kind of full globe coverage for a lot of things.
[00:10:44.440 --> 00:10:46.880]   They maybe don't give you the sort of vertical profile
[00:10:46.880 --> 00:10:48.960]   in the atmosphere that you might want in some places,
[00:10:48.960 --> 00:10:51.720]   but they also sort of have observations from aircraft
[00:10:51.720 --> 00:10:53.520]   and a range of other things as well.
[00:10:53.520 --> 00:10:56.800]   - This is kind of a naive question, I guess,
[00:10:56.800 --> 00:11:00.920]   but when I look online or go into dark sky or something
[00:11:00.920 --> 00:11:03.120]   and get the seven day forecast,
[00:11:03.120 --> 00:11:06.560]   are those meaningfully improving over my lifetime?
[00:11:06.560 --> 00:11:07.960]   - Yes, yes.
[00:11:07.960 --> 00:11:09.880]   I mean, it depends if you're looking at these things
[00:11:09.880 --> 00:11:12.280]   emotively or if you look at the analysis,
[00:11:12.280 --> 00:11:14.440]   yes, they're measurably getting better.
[00:11:14.440 --> 00:11:16.000]   - Wait, what does it mean to look at it emotively?
[00:11:16.000 --> 00:11:17.960]   I just, this sort of feeling that's wrong?
[00:11:17.960 --> 00:11:19.960]   - To people, I don't know.
[00:11:19.960 --> 00:11:21.560]   It's a, we're British,
[00:11:21.560 --> 00:11:23.040]   that we're always complaining about the weather
[00:11:23.040 --> 00:11:24.360]   and we're always complaining about the way,
[00:11:24.360 --> 00:11:26.440]   the predictability of the weather here,
[00:11:26.440 --> 00:11:28.360]   but it's raining here at the moment.
[00:11:28.360 --> 00:11:31.200]   But the improvement in these kind of forecasts
[00:11:31.200 --> 00:11:35.480]   are incremental, but I mean, over a decade,
[00:11:35.480 --> 00:11:39.240]   the ability, sort of the accuracy of a forecast out a day
[00:11:39.240 --> 00:11:41.160]   has improved quite significantly.
[00:11:41.160 --> 00:11:42.400]   - This is probably location dependent,
[00:11:42.400 --> 00:11:44.720]   but at what point can you,
[00:11:44.720 --> 00:11:46.840]   does like forecasting out based on sort of the physics
[00:11:46.840 --> 00:11:50.720]   of what's going on stop being meaningfully better
[00:11:50.720 --> 00:11:53.360]   than forecasting based on climate?
[00:11:53.360 --> 00:11:54.200]   Or just sort of like,
[00:11:54.200 --> 00:11:56.640]   what's the sort of average state of weather?
[00:11:56.640 --> 00:11:58.240]   Like, can you predict out like three weeks
[00:11:58.240 --> 00:12:01.560]   and have like a meaningful gain with a physics based model?
[00:12:01.560 --> 00:12:04.360]   - So the numerical systems,
[00:12:04.360 --> 00:12:08.160]   and this is getting to the edge of my knowledge now,
[00:12:08.160 --> 00:12:13.160]   the, I think the numerical systems are good out to two weeks
[00:12:13.160 --> 00:12:15.440]   so they're kind of the long range forecast
[00:12:15.440 --> 00:12:19.440]   are typically out to the kind of two to three week window.
[00:12:19.440 --> 00:12:23.160]   And then they're now starting to do sort of seasonal,
[00:12:23.160 --> 00:12:25.240]   sort of the bridging the gap between climate,
[00:12:25.240 --> 00:12:30.120]   which is sort of multi-year and sort of decadal
[00:12:30.120 --> 00:12:32.760]   and the short term NWP.
[00:12:32.760 --> 00:12:34.560]   They're starting to do seasonal predictions
[00:12:34.560 --> 00:12:36.200]   and they are showing skill,
[00:12:36.200 --> 00:12:38.360]   i.e. prediction above a random,
[00:12:38.360 --> 00:12:41.080]   a prediction above the sort of the climatology,
[00:12:41.080 --> 00:12:43.200]   the just look at history and base it
[00:12:43.200 --> 00:12:44.600]   on the average of history.
[00:12:44.600 --> 00:12:46.480]   So they're starting to show skill beyond that.
[00:12:46.480 --> 00:12:49.040]   And things like El Nino prediction and this kind of thing,
[00:12:49.040 --> 00:12:51.720]   they are starting to show skill out in that kind of time.
[00:12:51.720 --> 00:12:54.640]   But it's a, it's very much a kind of the mean,
[00:12:54.640 --> 00:12:57.800]   are we going to have a wet summer or a dry summer?
[00:12:57.800 --> 00:13:00.280]   The challenge I think for those kinds of organizations
[00:13:00.280 --> 00:13:02.160]   when they're articulating that is,
[00:13:02.160 --> 00:13:04.400]   so the Met Office had a wonderful thing
[00:13:04.400 --> 00:13:06.440]   where they said it was going to be a barbecue summer
[00:13:06.440 --> 00:13:10.240]   and all the headlines were barbecue summer
[00:13:10.240 --> 00:13:11.920]   that was picked up by the press.
[00:13:11.920 --> 00:13:13.400]   And what they, what actually turned out
[00:13:13.400 --> 00:13:15.400]   was it was a bit warmer and a bit wetter,
[00:13:15.400 --> 00:13:17.920]   but a little bit warmer, little bit wetter.
[00:13:17.920 --> 00:13:20.640]   And so people's kind of perceptions
[00:13:20.640 --> 00:13:22.280]   of what barbecue summer means,
[00:13:22.280 --> 00:13:25.440]   that means it's going to be nice and dry the entire time.
[00:13:25.440 --> 00:13:28.720]   And that's not necessarily what the prediction saying
[00:13:28.720 --> 00:13:30.320]   it's going to be slightly warmer than average
[00:13:30.320 --> 00:13:32.040]   and it's going to be slightly wetter than average
[00:13:32.040 --> 00:13:33.960]   really translates to the summer experience.
[00:13:33.960 --> 00:13:36.440]   So that's the kind of thing, the challenge with a lot of it
[00:13:36.440 --> 00:13:40.160]   to be interpreting the information can be quite challenging,
[00:13:40.160 --> 00:13:43.200]   but making it sort of generally understandable.
[00:13:43.200 --> 00:13:44.280]   - Sorry, we should talk about chips,
[00:13:44.280 --> 00:13:45.120]   but I have one more question.
[00:13:45.120 --> 00:13:45.960]   - We should, yeah.
[00:13:45.960 --> 00:13:46.800]   - I can't help myself.
[00:13:46.800 --> 00:13:47.640]   One last question.
[00:13:47.640 --> 00:13:50.000]   What is the function that you're trying to optimize
[00:13:50.000 --> 00:13:51.760]   when you predict weather?
[00:13:51.760 --> 00:13:55.520]   - Ooh, well, so they're not trying to optimize.
[00:13:55.520 --> 00:13:57.760]   - Or how do you measure success, I guess?
[00:13:57.760 --> 00:14:00.160]   - Okay, so yeah, that's a better,
[00:14:00.160 --> 00:14:03.960]   so they have a very wide range of metrics.
[00:14:03.960 --> 00:14:07.120]   So they're looking at sort of sea surface temperature,
[00:14:07.120 --> 00:14:09.720]   the, I mean, you're comparing the state of the atmosphere
[00:14:09.720 --> 00:14:11.960]   that you predict against the state of the atmosphere
[00:14:11.960 --> 00:14:13.000]   that actually exists.
[00:14:13.000 --> 00:14:14.560]   So you'll have a set of observations.
[00:14:14.560 --> 00:14:18.160]   And so the temperatures, the atmospheric pressures,
[00:14:18.160 --> 00:14:20.400]   the amounts of precipitation,
[00:14:20.400 --> 00:14:22.160]   there's a huge range of skill scores
[00:14:22.160 --> 00:14:23.680]   that these organizations generate.
[00:14:23.680 --> 00:14:24.840]   I mean, if you're really,
[00:14:24.840 --> 00:14:27.240]   if you're interested in this, ECMWF,
[00:14:27.240 --> 00:14:28.720]   which is the European Center
[00:14:28.720 --> 00:14:30.720]   for Medium-Range Weather Forecasting,
[00:14:30.720 --> 00:14:33.240]   has quite a detailed set of kind of,
[00:14:33.240 --> 00:14:35.080]   if you go and dig into their webpage,
[00:14:35.080 --> 00:14:39.280]   quite a detailed set of analysis on their forecasts.
[00:14:39.280 --> 00:14:41.240]   And as they're producing new forecasts,
[00:14:41.240 --> 00:14:44.600]   they're producing analyses of where is it improving
[00:14:44.600 --> 00:14:46.680]   and where is it sort of degrading
[00:14:46.680 --> 00:14:48.000]   relative to what they had before.
[00:14:48.000 --> 00:14:50.000]   And ideally you want all of the numbers to be green
[00:14:50.000 --> 00:14:53.480]   and so they're doing quite a lot of work there.
[00:14:53.480 --> 00:14:55.200]   And you can actually see the evolutions.
[00:14:55.200 --> 00:14:58.160]   And I mean, going back to kind of computing,
[00:14:58.160 --> 00:14:59.440]   you can actually see the evolutions
[00:14:59.440 --> 00:15:00.800]   in computers there as well,
[00:15:00.800 --> 00:15:02.960]   because they step up the resolutions
[00:15:02.960 --> 00:15:04.240]   as they're getting better systems,
[00:15:04.240 --> 00:15:08.880]   as they work as a software team to develop their software,
[00:15:08.880 --> 00:15:11.680]   that they're delivering high resolution forecasts,
[00:15:11.680 --> 00:15:14.200]   which tends to translate to better accuracy
[00:15:14.200 --> 00:15:15.760]   in the models that they're doing.
[00:15:15.760 --> 00:15:16.600]   - Cool.
[00:15:16.600 --> 00:15:17.440]   Well, thanks for the digression.
[00:15:17.440 --> 00:15:18.280]   That was fun.
[00:15:18.280 --> 00:15:20.000]   If anyone's listening or watching this
[00:15:20.000 --> 00:15:21.800]   and knows more about this, let us know.
[00:15:21.800 --> 00:15:22.720]   I'd love to-
[00:15:22.720 --> 00:15:25.000]   - Yeah, I mean, I will have to admit my knowledge
[00:15:25.000 --> 00:15:27.720]   is very much, it's probably five years old.
[00:15:27.720 --> 00:15:30.520]   And even at that point, I was not an expert in this space.
[00:15:30.520 --> 00:15:34.200]   So I will apologize if I've got anything massively wrong.
[00:15:34.200 --> 00:15:35.400]   Please correct me.
[00:15:35.400 --> 00:15:36.240]   - Well, it's okay.
[00:15:36.240 --> 00:15:37.400]   So I guess you sort of mentioned
[00:15:37.400 --> 00:15:39.920]   that you felt like high performance computing
[00:15:39.920 --> 00:15:43.480]   like wasn't growing as fast as what Graphcore is doing.
[00:15:43.480 --> 00:15:44.920]   I guess, what is the difference
[00:15:44.920 --> 00:15:47.160]   between kind of high performance computing
[00:15:47.160 --> 00:15:48.640]   and Graphcore?
[00:15:48.640 --> 00:15:51.160]   Like why isn't it sort of the same kind of problem
[00:15:51.160 --> 00:15:53.960]   with the same kind of hardware solution?
[00:15:53.960 --> 00:15:55.360]   - Well, so I would say that-
[00:15:55.360 --> 00:15:56.200]   - And I should say,
[00:15:56.200 --> 00:15:57.680]   I don't really know what high performance computing is.
[00:15:57.680 --> 00:15:59.040]   So I think I need some definitions here
[00:15:59.040 --> 00:16:01.560]   to even understand the point.
[00:16:01.560 --> 00:16:02.680]   - Sort of high performance computing
[00:16:02.680 --> 00:16:05.120]   in the sense of numerical simulation,
[00:16:05.120 --> 00:16:07.840]   where you're using a set of physics or chemistry
[00:16:07.840 --> 00:16:11.560]   to create a model of a system
[00:16:11.560 --> 00:16:14.040]   and generate some kind of prediction of behavior.
[00:16:14.040 --> 00:16:17.120]   That's or generate some kind of output.
[00:16:17.120 --> 00:16:21.760]   And typically those systems are relatively input light.
[00:16:21.760 --> 00:16:23.760]   So you'll be inputting a small amount of information,
[00:16:23.760 --> 00:16:26.040]   a model of a, or the structure of a protein
[00:16:26.040 --> 00:16:27.720]   that you want to have an elegant
[00:16:27.720 --> 00:16:28.960]   that you've got an interaction with
[00:16:28.960 --> 00:16:32.200]   or a description of a furnace or something like that.
[00:16:32.200 --> 00:16:33.160]   And a flame.
[00:16:33.160 --> 00:16:35.640]   And you want to understand how that system behaves.
[00:16:35.640 --> 00:16:37.480]   And so you actually generate huge amounts of information
[00:16:37.480 --> 00:16:38.480]   out of those kind of systems.
[00:16:38.480 --> 00:16:40.440]   So, and there's a huge space
[00:16:40.440 --> 00:16:42.160]   and it's been going for many decades.
[00:16:42.160 --> 00:16:46.680]   And has in the past 20 or 30 years
[00:16:46.680 --> 00:16:48.600]   and sort of been growing moderately fast.
[00:16:48.600 --> 00:16:51.480]   The machine learning space is really geared around
[00:16:51.480 --> 00:16:54.520]   taking very large amounts of data,
[00:16:54.520 --> 00:16:56.400]   very large amounts of data
[00:16:56.400 --> 00:17:00.080]   and using that to kind of build
[00:17:00.080 --> 00:17:04.400]   rather than apply a set of rules to that data.
[00:17:04.400 --> 00:17:06.040]   You're using the data itself
[00:17:06.040 --> 00:17:08.280]   to kind of build the model system
[00:17:08.280 --> 00:17:10.040]   and to learn the rules itself.
[00:17:10.040 --> 00:17:11.760]   The learning system rather than a system
[00:17:11.760 --> 00:17:14.600]   that you're designing to solve a problem.
[00:17:14.600 --> 00:17:17.320]   I think that's probably the easiest at a high level.
[00:17:17.320 --> 00:17:19.040]   - But then I guess, what does that translate to?
[00:17:19.040 --> 00:17:21.440]   So I can kind of see how those are different
[00:17:21.440 --> 00:17:22.520]   but I'm sort of imagining,
[00:17:22.520 --> 00:17:25.880]   that's probably a bunch of linear algebra
[00:17:25.880 --> 00:17:27.640]   underneath both of those problems.
[00:17:27.640 --> 00:17:29.680]   Like why do you need different types of hardware
[00:17:29.680 --> 00:17:31.360]   to solve them well?
[00:17:31.360 --> 00:17:33.800]   - So, well, so the differences
[00:17:33.800 --> 00:17:37.120]   from a kind of a computational science perspective
[00:17:37.120 --> 00:17:42.120]   are generally the HPC kind of simulations
[00:17:42.120 --> 00:17:44.440]   require quite high precision.
[00:17:44.440 --> 00:17:46.080]   And there is a bit of debate in that community
[00:17:46.080 --> 00:17:48.760]   about whether you really need 64 bit everywhere,
[00:17:48.760 --> 00:17:51.240]   whether you should really be doing 32 bit in some places.
[00:17:51.240 --> 00:17:53.960]   But generally you need quite high precision
[00:17:53.960 --> 00:17:55.840]   for most of that field.
[00:17:55.840 --> 00:17:59.440]   And 90% of it, I'm guessing today
[00:17:59.440 --> 00:18:01.160]   is probably done in double precision.
[00:18:01.160 --> 00:18:03.040]   What we, with machine learning,
[00:18:03.040 --> 00:18:04.520]   you're trying to sort of learn
[00:18:04.600 --> 00:18:07.280]   from a very large volume of data
[00:18:07.280 --> 00:18:10.520]   and make actually a relatively sort of limited
[00:18:10.520 --> 00:18:12.880]   set of kind of predictions out of it.
[00:18:12.880 --> 00:18:14.400]   But it's the learning process.
[00:18:14.400 --> 00:18:16.240]   And what become very clear is that
[00:18:16.240 --> 00:18:17.920]   you don't need high precision
[00:18:17.920 --> 00:18:20.080]   when you're in this kind of learning process.
[00:18:20.080 --> 00:18:22.440]   So, I mean, Nvidia started out,
[00:18:22.440 --> 00:18:25.520]   or the people when they were leveraging the Nvidia GPUs
[00:18:25.520 --> 00:18:27.880]   started out using single precision.
[00:18:27.880 --> 00:18:29.760]   GPUs were good at single precision
[00:18:29.760 --> 00:18:32.480]   and it was much faster than double precision.
[00:18:32.480 --> 00:18:33.560]   And then people discovered,
[00:18:33.560 --> 00:18:35.520]   well, you don't actually need single precision,
[00:18:35.520 --> 00:18:37.360]   you can do it in half precision.
[00:18:37.360 --> 00:18:38.600]   And so somebody built some hardware
[00:18:38.600 --> 00:18:40.440]   that was better at half precision.
[00:18:40.440 --> 00:18:42.840]   And so they started leveraging sort of 16 bits
[00:18:42.840 --> 00:18:44.600]   and people, when they're doing inference,
[00:18:44.600 --> 00:18:47.240]   they're using eight bit ints and four bit ints
[00:18:47.240 --> 00:18:48.240]   and they're looking,
[00:18:48.240 --> 00:18:50.600]   people are even playing around with binary kind of format.
[00:18:50.600 --> 00:18:53.280]   So it's very clear that this domain has,
[00:18:53.280 --> 00:18:54.680]   from a computational perspective,
[00:18:54.680 --> 00:18:57.320]   at that level, a very, very different character
[00:18:57.320 --> 00:18:58.960]   from a numerical precision perspective,
[00:18:58.960 --> 00:19:00.320]   different requirements.
[00:19:00.320 --> 00:19:02.680]   And then the other thing that's kind of quite clear
[00:19:02.680 --> 00:19:05.720]   and actually quite interesting about this space is that,
[00:19:05.720 --> 00:19:08.480]   so today we treat everything that we work with,
[00:19:08.480 --> 00:19:10.120]   or almost everything that we work with,
[00:19:10.120 --> 00:19:11.320]   as dense linear algebra.
[00:19:11.320 --> 00:19:15.800]   So if you look at a classic CNN model, like a ResNet,
[00:19:15.800 --> 00:19:20.440]   that convolutional network is typically translated
[00:19:20.440 --> 00:19:22.440]   when you're actually doing the maths with it
[00:19:22.440 --> 00:19:24.960]   on the computer into some kind of dense structure
[00:19:24.960 --> 00:19:26.160]   that you're working with,
[00:19:26.160 --> 00:19:27.920]   even though that convolution could be looked at
[00:19:27.920 --> 00:19:30.320]   as a relatively sparsely connected pattern, actually.
[00:19:30.320 --> 00:19:31.880]   And if you look at transformers
[00:19:32.040 --> 00:19:34.120]   and these kinds of systems that we're using
[00:19:34.120 --> 00:19:36.040]   and have seemed to be eating the world
[00:19:36.040 --> 00:19:38.520]   in natural language processing,
[00:19:38.520 --> 00:19:42.720]   they are big math models, big dense matrix objects.
[00:19:42.720 --> 00:19:47.040]   What we also know is that if we train a model at the end,
[00:19:47.040 --> 00:19:49.200]   we can then prune it quite aggressively
[00:19:49.200 --> 00:19:51.000]   and not lose very much fidelity,
[00:19:51.000 --> 00:19:52.760]   particularly if you go through a human training cycle
[00:19:52.760 --> 00:19:54.240]   afterwards as well.
[00:19:54.240 --> 00:19:56.080]   And there have been a number of papers,
[00:19:56.080 --> 00:19:59.000]   I'm rigging the lottery and a number of other ones
[00:19:59.000 --> 00:20:03.320]   that are sort of theorizing that actually
[00:20:03.320 --> 00:20:05.280]   what we're looking for, the systems we're interested in
[00:20:05.280 --> 00:20:07.760]   are actually fundamentally sparse.
[00:20:07.760 --> 00:20:09.960]   So we want to be able to train sparse systems.
[00:20:09.960 --> 00:20:13.560]   We think if we could train these systems in a sparse way,
[00:20:13.560 --> 00:20:15.400]   it would save a huge amount of blocks.
[00:20:15.400 --> 00:20:19.560]   If we only had 10% or 1% of the parameters in the system,
[00:20:19.560 --> 00:20:22.040]   we wouldn't be calculating all of these other numbers.
[00:20:22.040 --> 00:20:24.520]   And so there's a real interest in these systems
[00:20:24.520 --> 00:20:28.240]   in actually being able to do sparse algebra efficiently
[00:20:28.240 --> 00:20:31.640]   and not just for inference, but for training as well.
[00:20:31.640 --> 00:20:35.760]   And we also are in a place where OpenAI
[00:20:35.760 --> 00:20:38.520]   and some of the very large organizations in this space
[00:20:38.520 --> 00:20:41.600]   or organizations with access to very significant compute power
[00:20:41.600 --> 00:20:43.360]   are building huge models.
[00:20:43.360 --> 00:20:47.040]   It'd be really nice to not have to quite go as far as that.
[00:20:47.040 --> 00:20:49.240]   And so if I didn't have to build
[00:20:49.240 --> 00:20:50.960]   a 5 trillion parameter model,
[00:20:50.960 --> 00:20:54.120]   and I only had to build a 500 million parameter model,
[00:20:54.120 --> 00:20:55.840]   that would save me a lot of compute.
[00:20:55.840 --> 00:21:00.120]   It would reduce the cost of using that model.
[00:21:00.120 --> 00:21:01.680]   It would reduce the cost of training that model.
[00:21:01.680 --> 00:21:03.840]   I might still have to train it over a very big data set,
[00:21:03.840 --> 00:21:06.920]   but it would make it a lot cheaper to do iterations.
[00:21:06.920 --> 00:21:09.400]   So that's the other thing I think that fundamentally
[00:21:09.400 --> 00:21:13.040]   differentiates the machine learning space
[00:21:13.040 --> 00:21:14.480]   and the problems that we're trying to solve.
[00:21:14.480 --> 00:21:18.040]   And that's not to say there aren't sparse problems in HPT.
[00:21:18.040 --> 00:21:19.000]   There definitely are.
[00:21:19.000 --> 00:21:22.360]   But that combination of sparse and low precision,
[00:21:22.360 --> 00:21:25.400]   and particularly the sparse is not something we tackle today.
[00:21:25.400 --> 00:21:29.160]   Well, the sparse bit is not something that's really supported
[00:21:29.160 --> 00:21:31.560]   in general practice.
[00:21:31.560 --> 00:21:36.920]   I mean, is there ways to take advantage of that sparseness now
[00:21:36.920 --> 00:21:39.760]   with existing hardware to train faster?
[00:21:39.760 --> 00:21:43.440]   So today, or as of...
[00:21:43.440 --> 00:21:45.160]   Not really.
[00:21:45.160 --> 00:21:47.520]   And so this is one of these kind of chicken and egg problems
[00:21:47.520 --> 00:21:49.720]   where somebody needs to go and build some hardware
[00:21:49.720 --> 00:21:51.680]   that allows you to solve these kinds of problems.
[00:21:51.680 --> 00:21:54.160]   And then they also...
[00:21:54.160 --> 00:21:57.000]   But nobody builds the hardware until the problem's there
[00:21:57.000 --> 00:21:58.160]   that really justifies it.
[00:21:58.160 --> 00:22:01.120]   And so we are starting to see these kind of things evolve.
[00:22:01.120 --> 00:22:03.480]   And so actually one of the things that I'm really excited about
[00:22:03.480 --> 00:22:08.000]   with our next software release is that we're including
[00:22:08.000 --> 00:22:10.000]   both static sparse libraries,
[00:22:10.000 --> 00:22:12.760]   the ability to work with static sparsity.
[00:22:12.760 --> 00:22:14.720]   So you know the sparsity pattern up front.
[00:22:14.720 --> 00:22:17.080]   This might be the attention matrix in a system,
[00:22:17.080 --> 00:22:19.840]   or it might be a mask or something like that.
[00:22:19.840 --> 00:22:22.360]   You can typically know some of these things up front,
[00:22:22.360 --> 00:22:24.040]   as well as dynamic sparsity,
[00:22:24.040 --> 00:22:25.800]   where you don't know the sparsity pattern.
[00:22:25.800 --> 00:22:26.800]   So you can have a change.
[00:22:26.800 --> 00:22:29.000]   And we can deliver this with actually
[00:22:29.000 --> 00:22:32.000]   very, very significant performance on our architecture.
[00:22:32.000 --> 00:22:33.920]   That's one of the things about the IPU
[00:22:33.920 --> 00:22:36.360]   is that it was specifically designed
[00:22:36.360 --> 00:22:39.680]   to be a very fine-grained processing system
[00:22:39.680 --> 00:22:41.560]   and to be able to target these problems,
[00:22:41.560 --> 00:22:44.760]   as well as being fast and good at the data stuff as well.
[00:22:44.760 --> 00:22:46.480]   And so it kind of gives you a...
[00:22:46.480 --> 00:22:47.760]   And this is the thing,
[00:22:47.760 --> 00:22:50.240]   you can build sparse computing systems,
[00:22:50.240 --> 00:22:52.800]   but they typically go so much slower
[00:22:52.800 --> 00:22:54.440]   than the dense computing system
[00:22:54.440 --> 00:22:57.960]   that actually just running sparse on the dense system,
[00:22:57.960 --> 00:23:00.560]   so filling it full of zeros, makes more sense.
[00:23:00.560 --> 00:23:01.800]   - What's funny, I was going to mention that.
[00:23:01.800 --> 00:23:05.400]   I mean, I am decades out of date on this,
[00:23:05.400 --> 00:23:07.640]   but I remember doing a little bit of work on this
[00:23:07.640 --> 00:23:09.920]   in grad school and being surprised that...
[00:23:09.920 --> 00:23:11.440]   I mean, I would predict, I guess,
[00:23:11.440 --> 00:23:14.280]   based on my incredibly old experience
[00:23:14.280 --> 00:23:17.000]   that a sparsity factor of 1%,
[00:23:17.000 --> 00:23:18.520]   you might as well just fill in all the zeros,
[00:23:18.520 --> 00:23:19.920]   like you were saying.
[00:23:19.920 --> 00:23:22.840]   And not even worry about the sparseness.
[00:23:22.840 --> 00:23:24.320]   - Yeah, and this is,
[00:23:24.320 --> 00:23:26.080]   going back to kind of the HPC space,
[00:23:26.080 --> 00:23:28.760]   this is, people never use the sparse solve
[00:23:28.760 --> 00:23:30.320]   within the HPC space because they're so slow
[00:23:30.320 --> 00:23:32.280]   or the sparse linear algebra,
[00:23:32.280 --> 00:23:35.560]   unless they've got a 99.9% sparse problem,
[00:23:35.560 --> 00:23:36.880]   in which case they start making sense.
[00:23:36.880 --> 00:23:38.840]   So some of the interesting things
[00:23:38.840 --> 00:23:41.200]   about the characteristics we have in machine learning
[00:23:41.200 --> 00:23:43.760]   is that they aren't that sparse, actually.
[00:23:43.760 --> 00:23:45.440]   So they're dense enough so that doing
[00:23:45.440 --> 00:23:47.560]   the kind of the pure sparse arithmetic
[00:23:47.560 --> 00:23:48.800]   doesn't necessarily make sense.
[00:23:48.800 --> 00:23:52.480]   But we also believe that some of our structures
[00:23:52.480 --> 00:23:54.200]   are big enough that you can get away
[00:23:54.200 --> 00:23:56.920]   with having sort of small, dense blocks within them.
[00:23:56.920 --> 00:23:58.440]   And so the thing that's really difficult
[00:23:58.440 --> 00:24:00.560]   with 100% sparse systems is,
[00:24:00.560 --> 00:24:02.280]   well, there are a couple of things that are difficult.
[00:24:02.280 --> 00:24:04.240]   The access patterns moving around a lot
[00:24:04.240 --> 00:24:06.120]   is something that's quite difficult to handle.
[00:24:06.120 --> 00:24:09.240]   But from a really low level computational perspective,
[00:24:09.240 --> 00:24:11.400]   the way we get efficiency on all of these
[00:24:11.400 --> 00:24:13.160]   compute architectures is by having kind of
[00:24:13.160 --> 00:24:15.360]   dense block structures that we work with.
[00:24:15.360 --> 00:24:18.120]   And particularly two-dimensional functional units.
[00:24:18.120 --> 00:24:20.000]   And so if you want to keep those busy,
[00:24:20.000 --> 00:24:21.360]   you need sort of a block of work
[00:24:21.360 --> 00:24:23.400]   that's about the same kind of size as those units.
[00:24:23.400 --> 00:24:25.520]   And so for us, those are quite small.
[00:24:25.520 --> 00:24:27.080]   They might be 16 by 16.
[00:24:27.080 --> 00:24:29.760]   And so actually in big structures,
[00:24:29.760 --> 00:24:33.840]   the accuracy degradation you get over a pure sparse system
[00:24:33.840 --> 00:24:38.600]   going to one of these kind of small block sparse systems
[00:24:38.600 --> 00:24:40.000]   isn't too much.
[00:24:40.000 --> 00:24:43.000]   I mean, this is, and I say that there's been
[00:24:43.000 --> 00:24:45.040]   a very limited amount of work on this
[00:24:45.040 --> 00:24:47.160]   because the hardware just hasn't existed.
[00:24:47.160 --> 00:24:49.720]   But the indications are that there looks like
[00:24:49.720 --> 00:24:50.880]   there's a really nice compromise
[00:24:50.880 --> 00:24:53.200]   where you can get really great performance
[00:24:53.200 --> 00:24:54.520]   with a relatively,
[00:24:54.520 --> 00:24:58.080]   and whilst leveraging this kind of big sparse system.
[00:24:58.080 --> 00:25:02.080]   So I think, I mean, I would say we're right on the kind of
[00:25:02.080 --> 00:25:04.600]   the cusp of people starting to be able
[00:25:04.600 --> 00:25:06.240]   to really use these systems
[00:25:06.240 --> 00:25:10.120]   and sort of fundamentally explore and develop
[00:25:10.120 --> 00:25:12.120]   the algorithms both for sparse training,
[00:25:12.120 --> 00:25:13.800]   as well as understand the sort of
[00:25:13.800 --> 00:25:15.560]   where the breakpoints are.
[00:25:15.560 --> 00:25:17.200]   I mean, it may be that we discover actually,
[00:25:17.200 --> 00:25:19.440]   no, no, 16 by 16 is too big.
[00:25:19.440 --> 00:25:20.800]   What we really want is a four by four,
[00:25:20.800 --> 00:25:22.040]   or we want an eight by eight.
[00:25:22.040 --> 00:25:24.920]   Or we actually need, we're going to need a 16 by 16
[00:25:24.920 --> 00:25:27.000]   works great if you're doing DT3
[00:25:27.000 --> 00:25:28.320]   and you've got really big matrices,
[00:25:28.320 --> 00:25:30.000]   but it doesn't work so well if you're doing BERT
[00:25:30.000 --> 00:25:31.360]   and you've got slightly smaller matrices.
[00:25:31.360 --> 00:25:32.960]   And so there's a sort of a trade off
[00:25:32.960 --> 00:25:36.040]   in terms of relationship versus the hidden side.
[00:25:36.040 --> 00:25:38.120]   So I think we don't know.
[00:25:38.120 --> 00:25:40.040]   That's what's kind of so exciting at the moment
[00:25:40.040 --> 00:25:42.920]   is that there is some really sort of new ground.
[00:25:42.920 --> 00:25:46.160]   And I would say the one thing that attracted me
[00:25:46.160 --> 00:25:48.760]   to this space was a growing clearly
[00:25:48.760 --> 00:25:50.720]   and really interesting field,
[00:25:50.720 --> 00:25:54.920]   but also virtually, I wouldn't say completely greenfield,
[00:25:54.920 --> 00:25:57.160]   but there's so much that we don't know.
[00:25:57.160 --> 00:25:59.760]   I mean, the evolution over the past five years
[00:25:59.760 --> 00:26:02.080]   has been astonishingly fast
[00:26:02.080 --> 00:26:05.840]   and it's been really exciting to be part of.
[00:26:05.840 --> 00:26:07.560]   - Can I just, I'm trying to picture this
[00:26:07.560 --> 00:26:11.880]   and I'm not an expert at all on the space,
[00:26:11.880 --> 00:26:15.560]   but does sparsity help with something simple?
[00:26:15.560 --> 00:26:17.000]   Like for example, a convolution,
[00:26:17.000 --> 00:26:17.840]   like I'm trying to picture
[00:26:17.840 --> 00:26:21.800]   what even a sparse convolution would mean.
[00:26:21.800 --> 00:26:24.240]   Does it mean like a lot of the parameters are zero?
[00:26:24.240 --> 00:26:26.000]   And then my input data is certainly
[00:26:26.000 --> 00:26:27.800]   probably not going to be sparse, right?
[00:26:27.800 --> 00:26:30.680]   So the stuff that's going to pass through is dense.
[00:26:30.680 --> 00:26:33.440]   - It possibly doesn't make sense to think of it
[00:26:33.440 --> 00:26:35.040]   in a convolution,
[00:26:35.040 --> 00:26:38.840]   although you could clearly maybe have a larger,
[00:26:38.840 --> 00:26:40.080]   so typically in a convolution,
[00:26:40.080 --> 00:26:44.560]   you have a small mass that you're moving across your image.
[00:26:44.560 --> 00:26:45.800]   You could potentially think about
[00:26:45.800 --> 00:26:48.320]   having a slightly bigger mass that had some holes in it.
[00:26:48.320 --> 00:26:50.240]   That would be an interesting sparse pattern.
[00:26:50.240 --> 00:26:52.400]   And we've gone to small mass, I think,
[00:26:52.400 --> 00:26:54.640]   because partly they give you a nice characteristic
[00:26:54.640 --> 00:26:56.360]   in that they allow you to apply
[00:26:56.360 --> 00:26:58.120]   the same kind of transformation everywhere.
[00:26:58.120 --> 00:27:00.760]   And we seem to have standardized around three by three
[00:27:00.760 --> 00:27:01.600]   in a lot of places,
[00:27:01.600 --> 00:27:02.800]   whereas some of the early CNNs
[00:27:02.800 --> 00:27:04.400]   were playing around with bigger mass
[00:27:04.400 --> 00:27:06.120]   and seeing where the sweep spot was.
[00:27:06.120 --> 00:27:07.440]   But I don't know,
[00:27:08.600 --> 00:27:10.920]   so I don't know whether the standardization
[00:27:10.920 --> 00:27:13.480]   around three by three was a performance,
[00:27:13.480 --> 00:27:15.600]   as in the accuracy of the model you were making,
[00:27:15.600 --> 00:27:17.280]   whether it was a computational compromise
[00:27:17.280 --> 00:27:19.040]   in that it was a lot cheaper
[00:27:19.040 --> 00:27:21.240]   and it didn't cost you that much in terms of accuracy,
[00:27:21.240 --> 00:27:23.440]   but whether actually there's a better sweep spot
[00:27:23.440 --> 00:27:24.840]   with a bigger sparse mass.
[00:27:24.840 --> 00:27:27.120]   - I mean, it does feel like there's some intuition that,
[00:27:27.120 --> 00:27:29.320]   like, for example, if you're matching images,
[00:27:29.320 --> 00:27:30.600]   like pixels close to each other
[00:27:30.600 --> 00:27:33.480]   would be more relevant to each other than-
[00:27:33.480 --> 00:27:35.840]   - Yeah, and I think that that's certainly,
[00:27:35.840 --> 00:27:39.280]   and you would be picking up the kind of sort of edges
[00:27:39.280 --> 00:27:40.120]   and those kinds of things
[00:27:40.120 --> 00:27:41.640]   if you're thinking about actually sort of
[00:27:41.640 --> 00:27:43.680]   going through an image processing process,
[00:27:43.680 --> 00:27:45.040]   that I think there is some logic there.
[00:27:45.040 --> 00:27:47.000]   So in the image context,
[00:27:47.000 --> 00:27:49.840]   I'm not actually very sure
[00:27:49.840 --> 00:27:52.160]   where, how we might be able to use this,
[00:27:52.160 --> 00:27:53.680]   other than it's a new toy,
[00:27:53.680 --> 00:27:54.960]   I'm sure somebody is going to go and play
[00:27:54.960 --> 00:27:57.120]   and find somewhere where it's interesting.
[00:27:57.120 --> 00:28:00.080]   I think the area that we're seeing
[00:28:00.080 --> 00:28:03.320]   probably the most interest is in the places
[00:28:03.320 --> 00:28:04.880]   where you're currently using
[00:28:04.880 --> 00:28:06.600]   sort of fully connected layers,
[00:28:06.600 --> 00:28:08.760]   and you don't want to have to keep paying the cost
[00:28:08.760 --> 00:28:10.040]   of having a fully connected layer.
[00:28:10.040 --> 00:28:13.360]   And so sort of a stacking multiple
[00:28:13.360 --> 00:28:14.920]   partially connected layers together
[00:28:14.920 --> 00:28:17.280]   looks like quite an interesting approach
[00:28:17.280 --> 00:28:19.040]   and an area that we know,
[00:28:19.040 --> 00:28:20.560]   and I mean, you see this with CNN as well,
[00:28:20.560 --> 00:28:22.560]   you can prune CNN really quite heavily
[00:28:22.560 --> 00:28:23.760]   after you've trained them
[00:28:23.760 --> 00:28:26.120]   and still maintain past performance.
[00:28:26.120 --> 00:28:29.080]   So can we train those fully pruned CNN?
[00:28:29.080 --> 00:28:32.160]   Can we train these fully pruned language models
[00:28:32.160 --> 00:28:35.080]   from scratch in a faster, more efficient way?
[00:28:35.080 --> 00:28:36.760]   So can we rig the lottery
[00:28:36.760 --> 00:28:38.480]   and find that lottery ticket
[00:28:38.480 --> 00:28:40.640]   within that large dense model
[00:28:40.640 --> 00:28:41.600]   by a training process
[00:28:41.600 --> 00:28:42.840]   rather than doing that from scratch?
[00:28:42.840 --> 00:28:44.360]   And if we could do that,
[00:28:44.360 --> 00:28:46.320]   and it's efficient,
[00:28:46.320 --> 00:28:48.800]   then we might be able to access an even bigger model
[00:28:48.800 --> 00:28:50.600]   because one of the things that limits
[00:28:50.600 --> 00:28:52.200]   my ability to train a model is,
[00:28:52.200 --> 00:28:55.760]   do I want to spend a month waiting for it to train?
[00:28:55.760 --> 00:28:56.600]   Probably not,
[00:28:56.600 --> 00:28:58.240]   because I'm going to have to do this 50 times
[00:28:58.240 --> 00:29:01.040]   knowing the ML cycles we go through.
[00:29:01.040 --> 00:29:04.080]   If I could do that in a tenth of the time
[00:29:04.080 --> 00:29:06.480]   or even half the time, a quarter of the time,
[00:29:06.480 --> 00:29:07.840]   it maybe gives me access to something
[00:29:07.840 --> 00:29:09.200]   that's four times as big,
[00:29:09.200 --> 00:29:10.440]   and it might be better.
[00:29:10.440 --> 00:29:12.160]   And so that's the other interesting thing
[00:29:12.160 --> 00:29:15.080]   is if you want to keep going up the curve
[00:29:15.080 --> 00:29:18.480]   of model size and try and drive the accuracy higher,
[00:29:18.480 --> 00:29:21.320]   having something that gives us more flexibility
[00:29:21.320 --> 00:29:23.080]   is another lever that we can pull
[00:29:23.080 --> 00:29:24.840]   in the tool in the toolbox
[00:29:24.840 --> 00:29:27.560]   for exploring this space.
[00:29:28.600 --> 00:29:31.720]   - So I could see how at inference time,
[00:29:31.720 --> 00:29:33.600]   a sparse fully connected layer,
[00:29:33.600 --> 00:29:35.520]   you could do a sparse operation.
[00:29:35.520 --> 00:29:36.920]   That seems quite clear,
[00:29:36.920 --> 00:29:38.840]   but the training seems tricky, right?
[00:29:38.840 --> 00:29:40.720]   If you don't know a priori
[00:29:40.720 --> 00:29:43.440]   where the zeros are and the non-zeros,
[00:29:43.440 --> 00:29:45.120]   how do you figure that out?
[00:29:45.120 --> 00:29:47.800]   Am I asking a deep question that's hard to answer?
[00:29:47.800 --> 00:29:49.120]   Do you think you can explain that to me?
[00:29:49.120 --> 00:29:53.000]   - Well, I mean, that I think is one of the known spaces
[00:29:53.000 --> 00:29:55.160]   because people have not explored this.
[00:29:55.160 --> 00:30:00.160]   So DeepMind, I think, published a paper called "Riggle,"
[00:30:00.160 --> 00:30:03.440]   and which is rigging the lottery.
[00:30:03.440 --> 00:30:06.440]   And so, which was a way that they proposed
[00:30:06.440 --> 00:30:11.040]   to try and sort of discover the right sparsity pattern,
[00:30:11.040 --> 00:30:14.040]   where you wanted your parameters to be.
[00:30:14.040 --> 00:30:16.040]   And so I think it's, I mean,
[00:30:16.040 --> 00:30:17.360]   we train these systems
[00:30:17.360 --> 00:30:19.520]   through an iterative search effectively
[00:30:19.520 --> 00:30:21.960]   where we're learning the parameters.
[00:30:21.960 --> 00:30:24.480]   It's another parameter you learn, the sparsity pattern.
[00:30:24.480 --> 00:30:26.600]   So you'll be adding parameters in,
[00:30:26.600 --> 00:30:28.600]   you'll be taking parameters in elsewhere.
[00:30:28.600 --> 00:30:30.880]   You might have information in the backwards parse
[00:30:30.880 --> 00:30:32.800]   about where, I mean,
[00:30:32.800 --> 00:30:34.320]   so one thing you have to be careful of
[00:30:34.320 --> 00:30:35.760]   is you probably don't want to calculate
[00:30:35.760 --> 00:30:37.840]   the full set of gradients for that,
[00:30:37.840 --> 00:30:39.440]   the dense equivalent space,
[00:30:39.440 --> 00:30:42.040]   because, well, it depends what you're targeting.
[00:30:42.040 --> 00:30:44.840]   But if you're targeting something that is very big,
[00:30:44.840 --> 00:30:47.160]   that could get very, very expensive.
[00:30:47.160 --> 00:30:48.840]   And so you might want to,
[00:30:48.840 --> 00:30:50.200]   for how you get the signal
[00:30:50.200 --> 00:30:52.320]   for where you should be adding and removing parameters,
[00:30:52.320 --> 00:30:54.120]   maybe it's something goes to zero
[00:30:54.120 --> 00:30:55.560]   and you randomly add it somewhere else.
[00:30:55.560 --> 00:31:00.560]   Maybe you're trying to come up with some other method
[00:31:00.560 --> 00:31:01.440]   for adding these in.
[00:31:01.440 --> 00:31:02.440]   But I mean, that's, I think,
[00:31:02.440 --> 00:31:03.760]   one of the things we're going to find out
[00:31:03.760 --> 00:31:05.440]   is can we do this efficiently?
[00:31:05.440 --> 00:31:07.240]   I mean, it might not work, you never know.
[00:31:07.240 --> 00:31:08.080]   - Cool.
[00:31:08.080 --> 00:31:10.680]   So is this like the main thrust
[00:31:10.680 --> 00:31:13.400]   of kind of Graphcore's point of view on the hardware,
[00:31:13.400 --> 00:31:15.960]   that sparsity is important or are there kind of other-
[00:31:15.960 --> 00:31:19.160]   - I think that this is one of the things
[00:31:19.160 --> 00:31:20.680]   we're very excited about.
[00:31:20.680 --> 00:31:22.560]   Actually, one of the really interesting things
[00:31:22.560 --> 00:31:24.600]   right from the start of Graphcore
[00:31:24.600 --> 00:31:27.920]   is the founders, Simon Nolden and Nigel Toom,
[00:31:27.920 --> 00:31:29.920]   they didn't set out to,
[00:31:29.920 --> 00:31:32.800]   oh, we're just going to go and solve deep learning
[00:31:32.800 --> 00:31:34.320]   at the time that it was described.
[00:31:34.320 --> 00:31:35.440]   They set out to say,
[00:31:35.440 --> 00:31:37.840]   we want to try and build a computer architecture
[00:31:37.840 --> 00:31:41.480]   that's designed for machine learning as a general problem.
[00:31:41.480 --> 00:31:43.920]   And so what are the kind of computational characteristics
[00:31:43.920 --> 00:31:47.440]   of this problem and what do we need to do to solve that?
[00:31:47.440 --> 00:31:49.720]   And to an extent, take a punt
[00:31:49.720 --> 00:31:51.440]   at where they thought it was going to go.
[00:31:51.440 --> 00:31:52.640]   And they got a few things right,
[00:31:52.640 --> 00:31:54.560]   I think they probably got a few things wrong as well,
[00:31:54.560 --> 00:31:57.840]   but what we've built is what's designed
[00:31:57.840 --> 00:32:00.560]   as a general purpose architecture for machine learning.
[00:32:00.560 --> 00:32:03.720]   And so it is very, very good that Densely Interaldera,
[00:32:03.720 --> 00:32:06.880]   and we're showing in the benchmark results
[00:32:06.880 --> 00:32:08.360]   that I believe will be published
[00:32:08.360 --> 00:32:10.240]   by the time we go to the world,
[00:32:10.240 --> 00:32:12.320]   that we're showing sort of world-leading performance
[00:32:12.320 --> 00:32:15.880]   with BERT, sort of one of the very common NLP systems
[00:32:15.880 --> 00:32:17.720]   with some of the CNNs,
[00:32:17.720 --> 00:32:20.800]   but we're also showing that some of the classes of models
[00:32:20.800 --> 00:32:23.760]   that are more efficient sort of fundamentally,
[00:32:23.760 --> 00:32:26.000]   so efficient nets, even in the name,
[00:32:26.000 --> 00:32:29.040]   but don't run particularly well on a TPU architecture
[00:32:29.040 --> 00:32:30.000]   or on a GPU architecture,
[00:32:30.000 --> 00:32:33.440]   because they break up the sort of the structures
[00:32:33.440 --> 00:32:34.280]   that you work with,
[00:32:34.280 --> 00:32:36.520]   that they're finer grained in the group dimension
[00:32:36.520 --> 00:32:40.080]   than the sort of standard CNN architectures are.
[00:32:40.080 --> 00:32:41.520]   Those work really well on our hardware.
[00:32:41.520 --> 00:32:45.160]   We have a significantly better advantage,
[00:32:45.160 --> 00:32:48.000]   greater advantage with those kinds of architectures
[00:32:48.000 --> 00:32:50.840]   than we do with the standards and sort of CNNs.
[00:32:50.840 --> 00:32:52.160]   And that's really,
[00:32:52.160 --> 00:32:54.800]   we are sort of pretty good at both of them.
[00:32:54.800 --> 00:32:56.440]   Just everyone else is really bad
[00:32:56.440 --> 00:32:58.640]   at the kind of more efficient architectures.
[00:32:58.640 --> 00:33:01.520]   And that's the same kind of thing with these fast models
[00:33:01.520 --> 00:33:03.200]   is that fundamentally our architecture
[00:33:03.200 --> 00:33:06.600]   is designed to be massively parallel, very fine-grained.
[00:33:06.600 --> 00:33:08.440]   And so you can map these kind of fast problems
[00:33:08.440 --> 00:33:10.200]   onto it very efficiently.
[00:33:10.200 --> 00:33:12.600]   And other architectures kind of work.
[00:33:12.600 --> 00:33:15.240]   They were designed to be kind of very big block,
[00:33:15.240 --> 00:33:16.480]   bulk structured,
[00:33:16.560 --> 00:33:19.240]   and they're trying to bolt some capabilities onto it,
[00:33:19.240 --> 00:33:21.400]   but it's just fundamentally architecturally
[00:33:21.400 --> 00:33:24.080]   a bit more limited in their capabilities.
[00:33:24.080 --> 00:33:24.920]   So.
[00:33:24.920 --> 00:33:25.920]   - Also, can you explain to me
[00:33:25.920 --> 00:33:28.680]   like why it works better on, for example, BERT?
[00:33:28.680 --> 00:33:30.480]   Like I don't think of BERT as a,
[00:33:30.480 --> 00:33:31.480]   I mean, BERT's an embedding, right?
[00:33:31.480 --> 00:33:32.960]   And those are sort of,
[00:33:32.960 --> 00:33:34.040]   they're not really sparse, right?
[00:33:34.040 --> 00:33:36.320]   They're like dense, aren't they?
[00:33:36.320 --> 00:33:39.440]   And so what's going on that it's faster?
[00:33:39.440 --> 00:33:43.000]   - BERT as a model is, I mean, it has an embedding
[00:33:43.000 --> 00:33:44.720]   and then it's got sort of a,
[00:33:44.720 --> 00:33:46.800]   quite a deep stack of transformer layers.
[00:33:46.800 --> 00:33:48.760]   And so there it's just,
[00:33:48.760 --> 00:33:52.320]   we are very efficient at doing dense linear algebra.
[00:33:52.320 --> 00:33:55.360]   So we can beat the dense systems
[00:33:55.360 --> 00:33:56.720]   at doing dense linear algebra.
[00:33:56.720 --> 00:33:57.560]   - Wait, but why?
[00:33:57.560 --> 00:33:58.880]   Can you explain that to me?
[00:33:58.880 --> 00:34:00.600]   Like what are you doing?
[00:34:00.600 --> 00:34:01.520]   - So fundamentally, well,
[00:34:01.520 --> 00:34:03.200]   A, it was designed from scratch
[00:34:03.200 --> 00:34:05.000]   to target this kind of workload
[00:34:05.000 --> 00:34:08.680]   and we store parameters and activations sort of locally,
[00:34:08.680 --> 00:34:10.440]   actually within the physical chip itself.
[00:34:10.440 --> 00:34:12.960]   So one of the unique things about the IPU is
[00:34:12.960 --> 00:34:14.760]   it's a massively parallel architecture
[00:34:14.760 --> 00:34:17.800]   has about a thousand IPU cores per IPU,
[00:34:17.800 --> 00:34:19.120]   but each of those cores also embeds
[00:34:19.120 --> 00:34:20.440]   a very significant amount of memory.
[00:34:20.440 --> 00:34:23.440]   So we have about 900 megabytes of memory on each IPU
[00:34:23.440 --> 00:34:25.800]   and then we sort of gang multiple IPUs together
[00:34:25.800 --> 00:34:28.000]   into larger systems.
[00:34:28.000 --> 00:34:29.600]   - So these are like registers, I guess?
[00:34:29.600 --> 00:34:30.560]   So you have like giant registers.
[00:34:30.560 --> 00:34:33.440]   - Well, it's not really registers.
[00:34:33.440 --> 00:34:36.560]   It's just kind of a very fast local work from scratch.
[00:34:36.560 --> 00:34:38.640]   You might think about it like an L1 cache,
[00:34:38.640 --> 00:34:39.600]   but it's not a cache
[00:34:39.600 --> 00:34:41.160]   'cause it doesn't really cache anything.
[00:34:41.160 --> 00:34:43.600]   So it's the memory that we work with.
[00:34:43.600 --> 00:34:44.760]   - So I feel like what you're describing though
[00:34:44.760 --> 00:34:49.200]   is like in my dumb, like my ignorant brain,
[00:34:49.200 --> 00:34:50.600]   that's sort of like how I would sort of describe
[00:34:50.600 --> 00:34:52.240]   what a GPU is doing, right?
[00:34:52.240 --> 00:34:53.480]   So what's the difference here?
[00:34:53.480 --> 00:34:55.560]   It's like a more extreme version of that or?
[00:34:55.560 --> 00:34:59.600]   - Well, so a GPU has a primary memory system is HBM.
[00:34:59.600 --> 00:35:01.720]   And so it's external to the chip.
[00:35:01.720 --> 00:35:04.120]   I mean, it's packaged in a kind of a 3D package,
[00:35:04.120 --> 00:35:06.880]   but they are literally, they are stacks of memory
[00:35:06.880 --> 00:35:10.400]   that are glued onto a silicon wafer next to the chip.
[00:35:10.400 --> 00:35:14.240]   And so it's not in the main silicon entity right next to it.
[00:35:14.240 --> 00:35:17.040]   You have to go five millimeters
[00:35:17.040 --> 00:35:18.280]   through another silicon wafer
[00:35:18.280 --> 00:35:20.240]   and back up into a stack of memory.
[00:35:20.240 --> 00:35:25.120]   And that five millimeters means that they can only get,
[00:35:25.120 --> 00:35:28.360]   only, about a terabyte a second of memory bandwidth
[00:35:28.360 --> 00:35:29.920]   out of their memory system, something like that.
[00:35:29.920 --> 00:35:33.080]   Maybe it's a one and a half on some of the A100s I think.
[00:35:33.080 --> 00:35:37.960]   So whereas we get about 50 terabytes a second
[00:35:37.960 --> 00:35:39.760]   in and out of our memory systems
[00:35:39.760 --> 00:35:42.560]   on one IPU and actually from a power perspective,
[00:35:42.560 --> 00:35:43.960]   we probably get about two IPUs
[00:35:43.960 --> 00:35:45.840]   for one of theirs, maybe a bit more.
[00:35:45.840 --> 00:35:48.640]   So the amount of memory bandwidth we can actually deliver
[00:35:48.640 --> 00:35:50.280]   is sort of an order of magnitude,
[00:35:50.280 --> 00:35:52.840]   two orders of magnitude bigger in these systems.
[00:35:52.840 --> 00:35:55.400]   And that makes it really good at dense linear algebra
[00:35:55.400 --> 00:35:57.560]   because we can move data back and forth.
[00:35:57.560 --> 00:35:58.960]   Actually, I mean, dense linear algebra
[00:35:58.960 --> 00:36:01.760]   is a bit more limited by the core computational unit
[00:36:01.760 --> 00:36:02.960]   than the memory system.
[00:36:02.960 --> 00:36:05.680]   But a lot of our advantage comes with systems
[00:36:05.680 --> 00:36:08.560]   that are not quite as dense
[00:36:08.560 --> 00:36:10.720]   as the fewer dense linear algebra systems
[00:36:10.720 --> 00:36:13.040]   or the bits that kind of go around it.
[00:36:13.040 --> 00:36:14.360]   And so sparse systems,
[00:36:14.360 --> 00:36:16.760]   some of these other kinds of flavors are,
[00:36:16.760 --> 00:36:18.840]   that's where we really, really step out.
[00:36:18.840 --> 00:36:20.360]   So we're better on BERT,
[00:36:20.360 --> 00:36:23.360]   but we're a lot better in some of these other points.
[00:36:23.360 --> 00:36:27.320]   So it's, I mean, I said this to an American
[00:36:27.320 --> 00:36:28.880]   who didn't really understand me when I said,
[00:36:28.880 --> 00:36:30.960]   it was kind of like jam today and jam tomorrow.
[00:36:30.960 --> 00:36:33.040]   So we're really good today.
[00:36:33.040 --> 00:36:36.080]   And then you also get some great things to come tomorrow.
[00:36:36.080 --> 00:36:38.160]   Well, when we start to actually be able to exploit
[00:36:38.160 --> 00:36:40.720]   these new kinds of applications.
[00:36:40.720 --> 00:36:43.280]   - And is there any trade-offs to your approach
[00:36:43.280 --> 00:36:44.160]   or some different way?
[00:36:44.160 --> 00:36:47.000]   I mean, I'm assuming like with TPUs,
[00:36:47.000 --> 00:36:50.520]   Google was imagining a fairly similar workload, right?
[00:36:50.520 --> 00:36:53.200]   I mean, this was like machine learning inspired.
[00:36:53.200 --> 00:36:54.960]   So is there some fundamental decisions
[00:36:54.960 --> 00:36:56.520]   that you made differently here?
[00:36:56.520 --> 00:36:57.800]   And is there any kind of trade-offs
[00:36:57.800 --> 00:37:00.880]   where your chip might be harder to use
[00:37:00.880 --> 00:37:02.960]   or worse in some scenarios?
[00:37:02.960 --> 00:37:04.840]   - So the interesting thing about the TPU
[00:37:04.840 --> 00:37:07.680]   is actually sort of from a genesis and idea,
[00:37:07.680 --> 00:37:08.880]   the architectures kind of came up
[00:37:08.880 --> 00:37:10.720]   about the same kind of time.
[00:37:10.720 --> 00:37:14.040]   And the TPU went very, very big
[00:37:14.040 --> 00:37:15.320]   from a function unit perspective.
[00:37:15.320 --> 00:37:17.680]   They said, we're going to do really big function units.
[00:37:17.680 --> 00:37:20.920]   And that makes life really easy for the compiler developer.
[00:37:20.920 --> 00:37:23.840]   It makes life from a software perspective,
[00:37:23.840 --> 00:37:26.240]   it makes it a lot easier to target,
[00:37:26.240 --> 00:37:29.000]   but it means they really struggle with anything
[00:37:29.000 --> 00:37:31.360]   that's not a big, big matmul
[00:37:31.360 --> 00:37:34.320]   because their only big function unit is a very big matmul.
[00:37:34.320 --> 00:37:36.200]   Whereas we've got a lot more flexibility
[00:37:36.200 --> 00:37:38.240]   with being able to handle smaller,
[00:37:38.240 --> 00:37:39.680]   more finer grained workloads.
[00:37:39.680 --> 00:37:42.440]   So they're sort of inspired,
[00:37:42.440 --> 00:37:44.200]   we want to target machine learning,
[00:37:44.200 --> 00:37:46.280]   but the observation that they took was,
[00:37:46.280 --> 00:37:47.600]   okay, well, that means we need to be really good
[00:37:47.600 --> 00:37:48.440]   at big matmul.
[00:37:48.440 --> 00:37:49.560]   And the observation we took was,
[00:37:49.560 --> 00:37:51.600]   okay, well, that means we need to be good
[00:37:51.600 --> 00:37:52.440]   at dense linear algebra,
[00:37:52.440 --> 00:37:54.480]   but we also want to have all of this other flexibility.
[00:37:54.480 --> 00:37:57.160]   And so I would say if there's a downside
[00:37:57.160 --> 00:37:58.400]   of our architecture,
[00:37:58.400 --> 00:38:01.000]   it's that it makes the work of our compiler
[00:38:01.000 --> 00:38:03.520]   and library team quite a lot harder.
[00:38:03.520 --> 00:38:07.160]   So they've had to work to build the library
[00:38:07.160 --> 00:38:09.040]   and the software ecosystem to allow us
[00:38:09.040 --> 00:38:11.320]   to attach directly into the frameworks
[00:38:11.320 --> 00:38:13.440]   and to provide the kind of lowering
[00:38:13.440 --> 00:38:15.920]   from a large scale application workflow.
[00:38:15.920 --> 00:38:18.880]   So we write in Pytorch, we write in TensorFlow,
[00:38:18.880 --> 00:38:22.560]   to take that and translate that into something
[00:38:22.560 --> 00:38:25.000]   that maps onto our massively parallel architecture.
[00:38:25.000 --> 00:38:26.480]   And so I wouldn't,
[00:38:26.480 --> 00:38:29.080]   there's not a massive downside from a user perspective,
[00:38:29.080 --> 00:38:32.400]   it's more of a downside for our team.
[00:38:32.400 --> 00:38:34.480]   And I think it's taken our team a little bit longer
[00:38:34.480 --> 00:38:36.120]   to get that stack up and running.
[00:38:36.120 --> 00:38:39.560]   But what we do see quite interestingly with this stack
[00:38:39.560 --> 00:38:42.120]   is that we get kind of very predictable performance
[00:38:42.120 --> 00:38:44.040]   across different architectures,
[00:38:44.040 --> 00:38:46.280]   across different frameworks,
[00:38:46.280 --> 00:38:48.840]   and actually between inference and training.
[00:38:48.840 --> 00:38:51.200]   And so, whereas for some architectures,
[00:38:51.200 --> 00:38:52.760]   you might have to say,
[00:38:52.760 --> 00:38:55.560]   go through a dedicated inference backend
[00:38:55.560 --> 00:38:57.000]   to get great performance.
[00:38:57.000 --> 00:38:59.000]   And for us, we just take TensorFlow,
[00:38:59.000 --> 00:39:01.160]   we take Pytorch, and we just compile,
[00:39:01.160 --> 00:39:02.840]   sort of run from the framework,
[00:39:02.840 --> 00:39:05.600]   and we get absolute tip-top performance straight out of it.
[00:39:05.600 --> 00:39:08.080]   We put all of the work into the frontend framework
[00:39:08.080 --> 00:39:10.600]   and trying to make it as fast as possible.
[00:39:10.600 --> 00:39:11.440]   - Yeah, I guess it's funny,
[00:39:11.440 --> 00:39:13.120]   there's this thing that always makes me feel
[00:39:13.120 --> 00:39:16.960]   like I wasted my computer science education or something,
[00:39:16.960 --> 00:39:19.000]   because I use typically Nvidia chips,
[00:39:19.000 --> 00:39:21.760]   and so I upgrade the Kudanen library,
[00:39:21.760 --> 00:39:22.600]   which I think is kind of similar
[00:39:22.600 --> 00:39:24.240]   to what you're talking about.
[00:39:24.240 --> 00:39:26.040]   And I mean, I feel like sometimes it'll give me
[00:39:26.040 --> 00:39:29.400]   like a 30% speed increase.
[00:39:29.400 --> 00:39:31.680]   And I just feel like this deep mystery
[00:39:31.680 --> 00:39:33.080]   of what happened, right?
[00:39:33.080 --> 00:39:34.800]   It's like the hardware is the same,
[00:39:34.800 --> 00:39:37.440]   conceptually it seems like a fairly simple problem.
[00:39:37.440 --> 00:39:40.760]   Like how could you get such a massive increase
[00:39:40.760 --> 00:39:43.000]   with a smarter compiler?
[00:39:43.000 --> 00:39:45.400]   I guess that's kind of what some of the stuff you work on.
[00:39:45.400 --> 00:39:48.680]   Can you sort of talk about why
[00:39:48.680 --> 00:39:50.440]   this kind of conceptually simple thing
[00:39:50.440 --> 00:39:52.160]   is so complicated to get right,
[00:39:52.160 --> 00:39:56.320]   and why we can kind of continuously improve our compilers
[00:39:56.320 --> 00:39:58.080]   to make these things run faster,
[00:39:58.080 --> 00:39:59.800]   if compiler is even the right word here,
[00:39:59.800 --> 00:40:03.480]   the translation from a network to like a hardware.
[00:40:03.480 --> 00:40:05.400]   >> Yeah, I mean, I think compiler is the right word,
[00:40:05.400 --> 00:40:06.960]   and our kind of stack is probably
[00:40:06.960 --> 00:40:10.000]   about three compilers stacked, or maybe more than that.
[00:40:10.000 --> 00:40:12.800]   So I think the challenge is that these are,
[00:40:12.800 --> 00:40:16.480]   ooh, now if I was a computer scientist,
[00:40:16.480 --> 00:40:19.480]   so I think that these kind of compiler transformations
[00:40:19.480 --> 00:40:22.480]   are an NP-hard problem, I think, but I might be wrong.
[00:40:22.480 --> 00:40:27.240]   But I think that's why, is that actually solving
[00:40:27.240 --> 00:40:30.520]   these kind of systems is quite difficult.
[00:40:30.520 --> 00:40:34.320]   And so the compilers are typically developed
[00:40:34.320 --> 00:40:37.360]   to be quite general, or ideally you want a compiler
[00:40:37.360 --> 00:40:39.680]   that is, you can feed it anything,
[00:40:39.680 --> 00:40:41.040]   and it will give you something that works.
[00:40:41.040 --> 00:40:43.360]   And then, but it won't give you something
[00:40:43.360 --> 00:40:45.280]   that's 100% optimal in every domain,
[00:40:45.280 --> 00:40:48.520]   because that's a very, very tough problem to solve.
[00:40:48.520 --> 00:40:51.240]   And so as you find new sort of applications
[00:40:51.240 --> 00:40:53.520]   and architectures, then you might put a bit of work
[00:40:53.520 --> 00:40:55.480]   into trying to optimize the performance of those.
[00:40:55.480 --> 00:40:56.880]   And so sometimes what you're seeing
[00:40:56.880 --> 00:40:59.720]   is that the software engineers will have found,
[00:40:59.720 --> 00:41:02.160]   or come up with a different way of laying out
[00:41:02.160 --> 00:41:03.720]   the data sets, or a different,
[00:41:03.720 --> 00:41:05.600]   sometimes these might be fundamental
[00:41:05.600 --> 00:41:07.000]   sort of architectural innovations
[00:41:07.000 --> 00:41:09.520]   in that they change the behavior of systems.
[00:41:09.520 --> 00:41:13.320]   So that I think is what you're observing here,
[00:41:13.320 --> 00:41:16.840]   is that the GPUs have a very different execution model,
[00:41:16.840 --> 00:41:18.840]   and so sometimes when they're fusing
[00:41:18.840 --> 00:41:20.320]   and doing some kind of transformations,
[00:41:20.320 --> 00:41:22.160]   they can actually, that helps them
[00:41:22.160 --> 00:41:23.280]   in some particular areas.
[00:41:23.280 --> 00:41:28.000]   So there, and there, well, I don't really know
[00:41:28.000 --> 00:41:30.240]   too much about the development details
[00:41:30.240 --> 00:41:33.240]   of those kinds of platforms, but for us,
[00:41:33.240 --> 00:41:35.040]   I mean, one of the things I think we've observed
[00:41:35.040 --> 00:41:37.680]   is that A, I think we've still got quite a lot of headroom.
[00:41:37.680 --> 00:41:39.520]   So one of the other things that I'm excited about
[00:41:39.520 --> 00:41:42.840]   is that we are quite young in our development process
[00:41:42.840 --> 00:41:45.440]   of the libraries and the software.
[00:41:45.440 --> 00:41:48.880]   And I think we've got quite a lot of performance headroom.
[00:41:48.880 --> 00:41:50.800]   So there are some numbers that I've done
[00:41:50.800 --> 00:41:51.960]   on the back of the envelope,
[00:41:51.960 --> 00:41:53.720]   and I know how fast the chip can go.
[00:41:53.720 --> 00:41:55.920]   The chip can go at 250 teraflops,
[00:41:55.920 --> 00:41:57.600]   and it can get very close to that
[00:41:57.600 --> 00:41:59.440]   sort of sustaining linear algebra.
[00:41:59.440 --> 00:42:01.000]   And I know that some things I put through it
[00:42:01.000 --> 00:42:02.280]   don't go that fast.
[00:42:02.280 --> 00:42:04.560]   And so, and they probably should go faster
[00:42:04.560 --> 00:42:05.400]   than they're going at the moment.
[00:42:05.400 --> 00:42:07.360]   So that gives me quite a lot of hope, actually,
[00:42:07.360 --> 00:42:09.560]   that even the things that we're talking about at the moment
[00:42:09.560 --> 00:42:10.920]   have quite a lot of potential.
[00:42:10.920 --> 00:42:13.080]   And that's really the compiler we have
[00:42:13.080 --> 00:42:14.920]   is doing a pretty good job,
[00:42:14.920 --> 00:42:16.920]   but it's not doing a perfect job.
[00:42:16.920 --> 00:42:19.640]   And if we go and make it better,
[00:42:19.640 --> 00:42:22.400]   it will give us a better set of performance.
[00:42:22.400 --> 00:42:23.600]   And so, I mean, that's work.
[00:42:23.600 --> 00:42:26.920]   And actually, some of the people that are doing this work
[00:42:26.920 --> 00:42:30.680]   are, I mean, exceptionally capable engineers.
[00:42:30.680 --> 00:42:32.800]   And so it's just a case of giving them enough time
[00:42:32.800 --> 00:42:35.400]   and space to do some of this optimization.
[00:42:35.400 --> 00:42:40.200]   - And so, are your chips commercially available?
[00:42:40.200 --> 00:42:43.680]   Could I buy one and try it out?
[00:42:43.680 --> 00:42:44.520]   - Yes, absolutely.
[00:42:44.520 --> 00:42:47.640]   And actually, we're just about to launch
[00:42:47.640 --> 00:42:49.640]   the second generation of our processors.
[00:42:49.640 --> 00:42:51.800]   So we actually launched the first generation
[00:42:51.800 --> 00:42:53.880]   a year ago, I believe.
[00:42:53.880 --> 00:42:55.480]   And they've been adopted and deployed
[00:42:55.480 --> 00:42:57.040]   into Microsoft Azure.
[00:42:57.040 --> 00:42:58.680]   And so we're really excited, actually,
[00:42:58.680 --> 00:43:00.560]   about the second generation of our product
[00:43:00.560 --> 00:43:03.920]   that's being, will be, when we announced this,
[00:43:03.920 --> 00:43:06.960]   I think a month or two ago, is coming very soon.
[00:43:06.960 --> 00:43:09.440]   And is, so, I mean, the interesting thing
[00:43:09.440 --> 00:43:11.280]   is we've slightly changed the form factor
[00:43:11.280 --> 00:43:12.120]   that we're deploying these.
[00:43:12.120 --> 00:43:13.560]   We used to build sort of things that looked a bit
[00:43:13.560 --> 00:43:15.960]   like a GPU, a PCIe card.
[00:43:15.960 --> 00:43:18.240]   We've actually moved to a kind of a,
[00:43:18.240 --> 00:43:19.840]   a slightly more integrated form factor
[00:43:19.840 --> 00:43:21.800]   that has four of our IPUs in,
[00:43:21.800 --> 00:43:24.120]   looks a bit more like kind of a one-use pizza box
[00:43:24.120 --> 00:43:25.560]   and sort of server.
[00:43:25.560 --> 00:43:27.360]   And it's designed explicitly to scale.
[00:43:27.360 --> 00:43:29.920]   And so we've moved from kind of thinking about systems
[00:43:29.920 --> 00:43:32.960]   that are sort of server-based with a host processor
[00:43:32.960 --> 00:43:36.200]   and a set of sort of accelerator cards
[00:43:36.200 --> 00:43:38.320]   to a system that's designed to be able to kind of
[00:43:38.320 --> 00:43:42.880]   just wrap multiple of these IPU machines together
[00:43:42.880 --> 00:43:44.480]   and cable them with an interconnect.
[00:43:44.480 --> 00:43:46.760]   You have hosts from both across the network.
[00:43:46.760 --> 00:43:50.400]   And so disaggregate that host from IPU processing,
[00:43:50.400 --> 00:43:51.920]   but also scale IPUs.
[00:43:51.920 --> 00:43:56.320]   We can go from sort of one to 64 out to thousands of IPUs
[00:43:56.320 --> 00:43:57.600]   in a very tight integration.
[00:43:57.600 --> 00:44:00.120]   So yeah, we're really excited about this.
[00:44:00.120 --> 00:44:03.120]   And actually the performance, the scalability,
[00:44:03.120 --> 00:44:05.840]   all of the kind of aspects of this as a technology
[00:44:05.840 --> 00:44:07.440]   are really interesting.
[00:44:07.440 --> 00:44:11.440]   And we're talking about some classes of models,
[00:44:11.440 --> 00:44:13.520]   but sort of ResNet, we've talked about
[00:44:13.520 --> 00:44:14.560]   some of these kind of CNNs.
[00:44:14.560 --> 00:44:18.080]   Actually, these are all, I mean, but fairly big today,
[00:44:18.080 --> 00:44:19.600]   sort of a couple of hundred million parameters,
[00:44:19.600 --> 00:44:22.280]   but it's nowhere near the really, really big models
[00:44:22.280 --> 00:44:23.240]   that people are working with.
[00:44:23.240 --> 00:44:25.280]   So I think that's some of the things
[00:44:25.280 --> 00:44:28.600]   that we're really interested in is being able to drive
[00:44:28.600 --> 00:44:30.480]   the scale of these kinds of training systems,
[00:44:30.480 --> 00:44:32.520]   but also try and do it more efficiently.
[00:44:32.520 --> 00:44:36.200]   And so it gives people the tools to train large systems
[00:44:36.200 --> 00:44:38.280]   or train systems to high levels of accuracy
[00:44:38.280 --> 00:44:41.440]   without needing to go all the way into that
[00:44:41.440 --> 00:44:43.400]   completely dense domain.
[00:44:43.400 --> 00:44:44.920]   - Do you worry about some of the things
[00:44:44.920 --> 00:44:46.720]   that have kind of been in the zeitgeist lately
[00:44:46.720 --> 00:44:48.560]   around models getting bigger and bigger?
[00:44:48.560 --> 00:44:50.960]   Like only the biggest companies having access
[00:44:50.960 --> 00:44:53.720]   to be able to train them or like carbon footprint.
[00:44:53.720 --> 00:44:54.560]   Like, is that a real effect?
[00:44:54.560 --> 00:44:56.000]   I imagine it might actually help you,
[00:44:56.000 --> 00:44:57.720]   but maybe bad for society.
[00:44:57.720 --> 00:45:02.560]   - So the societal sort of impacts of access
[00:45:02.560 --> 00:45:05.440]   to this technology are a fascinating topic.
[00:45:05.440 --> 00:45:08.600]   I'm probably not one for this,
[00:45:08.600 --> 00:45:10.320]   because I suspect we could spend another hour
[00:45:10.320 --> 00:45:11.240]   on that alone.
[00:45:11.240 --> 00:45:13.680]   We're really focused around trying to make
[00:45:13.680 --> 00:45:17.120]   this kind of technology available
[00:45:17.120 --> 00:45:18.440]   to as many people as possible
[00:45:18.440 --> 00:45:19.960]   and also as efficient as possible.
[00:45:19.960 --> 00:45:23.280]   And so I think the way that we'll lower the bar
[00:45:23.280 --> 00:45:24.480]   for access to this kind of thing
[00:45:24.480 --> 00:45:26.600]   is by enabling people to run models
[00:45:26.600 --> 00:45:28.520]   that are more efficient and enabling them
[00:45:28.520 --> 00:45:31.200]   to work with architectures that don't require
[00:45:31.200 --> 00:45:33.440]   a billion dollars of compute to train the model.
[00:45:33.440 --> 00:45:34.720]   I mean, the big challenge around that
[00:45:34.720 --> 00:45:36.560]   is always going to be access to the data.
[00:45:36.560 --> 00:45:38.600]   Because I mean, the one thing we,
[00:45:38.600 --> 00:45:39.800]   I'm a compute person.
[00:45:39.800 --> 00:45:41.160]   I think about the compute.
[00:45:41.160 --> 00:45:43.080]   We also, to a certain extent,
[00:45:43.080 --> 00:45:46.760]   have to think about the data and access to that.
[00:45:46.760 --> 00:45:50.320]   And really that's the bit that seems to be favoring
[00:45:50.320 --> 00:45:52.000]   some of the very large organizations today,
[00:45:52.000 --> 00:45:53.400]   is just that they have the ability
[00:45:53.400 --> 00:45:55.240]   to pull together the training sets
[00:45:55.240 --> 00:45:57.440]   that most people don't have access to.
[00:45:57.440 --> 00:45:59.720]   So there are two sides to the kind of,
[00:45:59.720 --> 00:46:01.640]   the access to this kind of technology story
[00:46:01.640 --> 00:46:02.560]   that I think are-
[00:46:02.560 --> 00:46:03.480]   - What about energy issues?
[00:46:03.480 --> 00:46:05.080]   Do you think that over time,
[00:46:05.080 --> 00:46:06.560]   these kinds of chips will become
[00:46:06.560 --> 00:46:09.920]   a significant user of energy?
[00:46:09.920 --> 00:46:14.920]   - I'm not convinced compared to the rest of the fleet
[00:46:14.920 --> 00:46:20.200]   of kind of web service infrastructure in the world,
[00:46:20.200 --> 00:46:23.560]   that the ML is ever going to get to the scale
[00:46:23.560 --> 00:46:26.400]   where it's more expensive than they are.
[00:46:26.400 --> 00:46:27.240]   So I mean, I think-
[00:46:27.240 --> 00:46:30.720]   - Didn't Google say that some huge fraction
[00:46:30.720 --> 00:46:33.720]   of their compute centers was doing inference?
[00:46:34.720 --> 00:46:37.400]   - Oh, if they have, I've missed it.
[00:46:37.400 --> 00:46:40.800]   So that would be an interesting observation.
[00:46:40.800 --> 00:46:43.360]   So, I mean, it's not going to be zero.
[00:46:43.360 --> 00:46:46.560]   And so the question I think is whether,
[00:46:46.560 --> 00:46:49.400]   how much of a percentage of that it is,
[00:46:49.400 --> 00:46:51.680]   and also how much of it is going to be
[00:46:51.680 --> 00:46:53.280]   sort of training versus inference.
[00:46:53.280 --> 00:46:55.400]   I mean, I guess if they're doing,
[00:46:55.400 --> 00:46:59.440]   if they're driving their search backend via inference,
[00:46:59.440 --> 00:47:02.120]   and if they're driving all of the backends,
[00:47:02.120 --> 00:47:04.720]   sort of Google Photos and YouTube
[00:47:04.720 --> 00:47:06.120]   and all of those kinds of things.
[00:47:06.120 --> 00:47:07.480]   - And certainly they are, right?
[00:47:07.480 --> 00:47:08.320]   I think-
[00:47:08.320 --> 00:47:09.600]   - Well, yes, I guess.
[00:47:09.600 --> 00:47:11.320]   So you sort of follow down that,
[00:47:11.320 --> 00:47:14.160]   you know, maybe it is, but it's, yeah.
[00:47:14.160 --> 00:47:15.680]   So I think, yeah, you could be right.
[00:47:15.680 --> 00:47:18.600]   The inference workload could look quite large.
[00:47:18.600 --> 00:47:20.280]   But again, I think that's probably an area
[00:47:20.280 --> 00:47:23.600]   where you would be looking to deploy dedicated chips.
[00:47:23.600 --> 00:47:25.360]   And this is why people build dedicated chips,
[00:47:25.360 --> 00:47:26.640]   is because they're more efficient
[00:47:26.640 --> 00:47:28.120]   than the general purpose chips.
[00:47:28.120 --> 00:47:31.000]   And so the whole idea of trying to do this
[00:47:31.000 --> 00:47:34.000]   is to make something that is more cost effective.
[00:47:34.000 --> 00:47:39.000]   So it costs less in terms of dollars per model trained,
[00:47:39.000 --> 00:47:42.600]   or dollars per inference served to your customer.
[00:47:42.600 --> 00:47:44.200]   And part of that's the power cost,
[00:47:44.200 --> 00:47:45.560]   part of that's the procurement cost
[00:47:45.560 --> 00:47:46.400]   with these kinds of things.
[00:47:46.400 --> 00:47:48.560]   And so, so I think that kind of comes into the fact
[00:47:48.560 --> 00:47:50.040]   that that's why we build these kinds of
[00:47:50.040 --> 00:47:51.320]   special purpose architectures,
[00:47:51.320 --> 00:47:53.480]   or at least specialized architectures.
[00:47:53.480 --> 00:47:55.960]   And the other comment is with the end,
[00:47:55.960 --> 00:47:57.640]   slowing down of Moore's Law,
[00:47:57.680 --> 00:48:01.160]   it's a very significant plateau in the rate of improvement
[00:48:01.160 --> 00:48:03.800]   or the shrink and also the energy efficiency.
[00:48:03.800 --> 00:48:06.560]   We can no longer kind of rely on things just getting better.
[00:48:06.560 --> 00:48:08.360]   Sort of every two or three years,
[00:48:08.360 --> 00:48:11.520]   we'll get another 50% sort of 2X energy efficiency,
[00:48:11.520 --> 00:48:13.640]   whatever the scaling is.
[00:48:13.640 --> 00:48:14.960]   That's kind of really slowing down.
[00:48:14.960 --> 00:48:17.440]   And so the specialization of the processes
[00:48:17.440 --> 00:48:19.120]   has been driven by that.
[00:48:19.120 --> 00:48:22.240]   So we need an architecture that is more memory efficient.
[00:48:22.240 --> 00:48:26.760]   And if you go back to the kind of fundamental processor,
[00:48:26.760 --> 00:48:29.240]   we don't move data very far.
[00:48:29.240 --> 00:48:31.960]   So the whole architecture is geared around data
[00:48:31.960 --> 00:48:33.960]   staying local for the processing entities.
[00:48:33.960 --> 00:48:36.720]   And the physics of moving data
[00:48:36.720 --> 00:48:38.760]   is one of the things that really drive power consumption.
[00:48:38.760 --> 00:48:41.520]   So there's kind of doing the actual operation.
[00:48:41.520 --> 00:48:44.200]   So driving the computational units.
[00:48:44.200 --> 00:48:45.600]   And then there's moving data
[00:48:45.600 --> 00:48:47.840]   to and from your memory subsystems.
[00:48:47.840 --> 00:48:49.640]   So if your memory is very close,
[00:48:49.640 --> 00:48:51.680]   the cost of moving data there is a lot lower,
[00:48:51.680 --> 00:48:53.960]   energy costs, compared to if it's off chip,
[00:48:53.960 --> 00:48:55.760]   the cost tends to be a lot higher.
[00:48:55.760 --> 00:48:58.160]   And this kind of goes into the power consumption
[00:48:58.160 --> 00:49:01.200]   of the device, where are you spending your power?
[00:49:01.200 --> 00:49:05.000]   And so that's kind of one of the premises actually of the IPU
[00:49:05.000 --> 00:49:07.520]   is fundamentally more efficient,
[00:49:07.520 --> 00:49:09.480]   higher floating point operations,
[00:49:09.480 --> 00:49:12.880]   lots of energy input, because we don't move data as far.
[00:49:12.880 --> 00:49:15.880]   We try and keep everything as local as possible for as long.
[00:49:15.880 --> 00:49:20.560]   - I guess one more question on chips, just the timing.
[00:49:20.560 --> 00:49:22.520]   Apple recently came out with a new M1
[00:49:22.520 --> 00:49:24.120]   that a lot of folks are talking about
[00:49:24.120 --> 00:49:26.440]   that included some ML focused stuff.
[00:49:26.440 --> 00:49:28.560]   Do you have any opinion on that?
[00:49:28.560 --> 00:49:31.520]   - It's, well, a really interesting bit of tech.
[00:49:31.520 --> 00:49:33.280]   So, and they showed some quite interesting
[00:49:33.280 --> 00:49:35.440]   kind of overall sort of performance improvements.
[00:49:35.440 --> 00:49:37.840]   I mean, I think this is an example of specialization
[00:49:37.840 --> 00:49:40.240]   going out into all of these kinds of systems.
[00:49:40.240 --> 00:49:42.440]   And I think it's also an example of the spread
[00:49:42.440 --> 00:49:44.800]   of machine learning and the workload
[00:49:44.800 --> 00:49:46.440]   out into all of these kinds of systems.
[00:49:46.440 --> 00:49:50.120]   So I'm not sure in the context of Graphcore
[00:49:50.120 --> 00:49:52.160]   and building sort of data center scale,
[00:49:52.160 --> 00:49:53.480]   training and inference systems,
[00:49:53.480 --> 00:49:56.840]   probably not something that is particularly relevant
[00:49:56.840 --> 00:49:58.000]   for us in terms of the marketplace,
[00:49:58.000 --> 00:49:59.680]   but it is interesting to see.
[00:49:59.680 --> 00:50:01.680]   I mean, we've seen this with mobile phones as well,
[00:50:01.680 --> 00:50:04.480]   with sort of dedicated inference chips being embedded into.
[00:50:04.480 --> 00:50:07.080]   I mean, I think all of the ones that I've got kicking around
[00:50:07.080 --> 00:50:09.680]   have one of these things in somewhere
[00:50:09.680 --> 00:50:12.240]   that they're using for photos and other kinds of things.
[00:50:12.240 --> 00:50:13.920]   So I think that's just the kind of,
[00:50:13.920 --> 00:50:18.000]   you'd almost expect it because every kind of modern
[00:50:18.000 --> 00:50:21.960]   consumer-facing workload has some kind of ML embedded in.
[00:50:21.960 --> 00:50:24.560]   I would guess that most of them do.
[00:50:24.560 --> 00:50:25.400]   - Well, thanks so much, man.
[00:50:25.400 --> 00:50:26.400]   This has been super fun.
[00:50:26.400 --> 00:50:28.120]   I feel like even if it wasn't being recorded,
[00:50:28.120 --> 00:50:29.440]   I've learned a lot.
[00:50:29.440 --> 00:50:30.280]   I love it.
[00:50:30.280 --> 00:50:31.960]   So we always end with two questions.
[00:50:31.960 --> 00:50:33.440]   I mean, I'd love to ask you these.
[00:50:33.440 --> 00:50:35.360]   So the first is pretty open-ended.
[00:50:35.360 --> 00:50:36.320]   Well, they're both open-ended,
[00:50:36.320 --> 00:50:38.360]   but the first one is also open-ended.
[00:50:38.360 --> 00:50:39.200]   So the question is,
[00:50:39.200 --> 00:50:42.080]   what is kind of one underrated aspect of machine learning
[00:50:42.080 --> 00:50:43.960]   that you think people should pay more attention to
[00:50:43.960 --> 00:50:44.920]   than they do?
[00:50:44.920 --> 00:50:47.360]   - So machine learning is a bit of a sort of a chicken
[00:50:47.360 --> 00:50:50.720]   and an egg in that because it's built around
[00:50:50.720 --> 00:50:53.200]   processing very large volumes of data
[00:50:53.200 --> 00:50:55.160]   that requires quite a lot of compute,
[00:50:55.160 --> 00:50:57.760]   the kind of bar to actually,
[00:50:57.760 --> 00:51:00.480]   to get to a kind of a state of the art solution
[00:51:00.480 --> 00:51:01.320]   is quite high,
[00:51:01.320 --> 00:51:03.200]   just in terms of the amount of work you have to do
[00:51:03.200 --> 00:51:04.640]   from a computational perspective.
[00:51:04.640 --> 00:51:07.120]   And so you have to have any kind of data processing
[00:51:07.120 --> 00:51:08.400]   that has to be quite efficient
[00:51:08.400 --> 00:51:11.520]   and be able to run it at teraflops,
[00:51:11.520 --> 00:51:14.000]   tens of teraflops to be able to do through that.
[00:51:14.000 --> 00:51:16.200]   And so either something that's much more
[00:51:16.200 --> 00:51:19.600]   sort of data efficient in the way it learns potentially,
[00:51:19.600 --> 00:51:23.360]   or something that we can find new computational architectures
[00:51:23.360 --> 00:51:25.080]   to give us the efficiency on new classes of models.
[00:51:25.080 --> 00:51:26.000]   I think those are the things
[00:51:26.000 --> 00:51:27.480]   that might be really interesting.
[00:51:27.480 --> 00:51:28.880]   - Cool. I have to say, it's funny.
[00:51:28.880 --> 00:51:30.800]   We've had a bunch of computational chemists
[00:51:30.800 --> 00:51:31.960]   talk to us on the show
[00:51:31.960 --> 00:51:33.440]   and also just sort of customer interviews,
[00:51:33.440 --> 00:51:36.320]   and they're all talking about graph-based networks.
[00:51:36.320 --> 00:51:37.680]   It seems like that might be an area
[00:51:37.680 --> 00:51:39.160]   where there's a lot of interest.
[00:51:39.160 --> 00:51:42.040]   - One of the ones that we've been working on,
[00:51:42.040 --> 00:51:45.480]   and I'm not sure when we're going to be able to publish it,
[00:51:45.480 --> 00:51:47.760]   but it is actually a graph-based neural network
[00:51:47.760 --> 00:51:49.760]   using the Spectral Library in TensorFlow,
[00:51:49.760 --> 00:51:51.800]   which is, and it's a very small example.
[00:51:51.800 --> 00:51:54.080]   It's not anything sort of fancy or groundbreaking,
[00:51:54.080 --> 00:51:55.520]   but it's just an example, I think,
[00:51:55.520 --> 00:51:57.520]   of doing a molecular binding prediction
[00:51:57.520 --> 00:51:59.160]   using that kind of approach.
[00:51:59.160 --> 00:52:01.360]   - Cool.
[00:52:01.360 --> 00:52:03.360]   Well, the final question we always ask
[00:52:03.360 --> 00:52:04.920]   is kind of what's the biggest challenge
[00:52:04.920 --> 00:52:07.720]   of making machine learning models work in the real world?
[00:52:07.720 --> 00:52:09.760]   But I'm kind of tempted to modify it for you.
[00:52:09.760 --> 00:52:11.360]   I'm wondering, what's the biggest challenge
[00:52:11.360 --> 00:52:14.120]   of taking a new piece of hardware to market?
[00:52:14.120 --> 00:52:16.800]   It seems like there must be challenges everywhere, but-
[00:52:16.800 --> 00:52:17.640]   - Yeah.
[00:52:17.640 --> 00:52:18.480]   - What's the biggest surprising challenges?
[00:52:18.480 --> 00:52:20.640]   - I would like to answer the first one as well.
[00:52:20.640 --> 00:52:22.160]   - Oh, please, yeah, answer both.
[00:52:22.160 --> 00:52:24.640]   - One of the things that we've done quite a lot of,
[00:52:24.640 --> 00:52:28.520]   I mean, so we've talked a lot about performance,
[00:52:28.520 --> 00:52:30.400]   sort of how fast does it go?
[00:52:30.400 --> 00:52:32.920]   And actually, performance is a beautifully simple thing,
[00:52:32.920 --> 00:52:34.120]   because it's very easy to measure.
[00:52:34.120 --> 00:52:35.400]   What's the images a second?
[00:52:35.400 --> 00:52:36.680]   What's the sequences a second?
[00:52:36.680 --> 00:52:37.720]   How fast does it go?
[00:52:37.720 --> 00:52:40.720]   But the other bit of that is actually,
[00:52:40.720 --> 00:52:43.080]   you don't just care about how fast it goes.
[00:52:43.080 --> 00:52:45.680]   You care about it giving you the right answer as well.
[00:52:45.680 --> 00:52:48.480]   And so you care about your system converging.
[00:52:48.480 --> 00:52:51.600]   One of the things that we've been really interested
[00:52:51.600 --> 00:52:52.920]   in exploring, actually, part of the reason
[00:52:52.920 --> 00:52:55.200]   that we're working with Weights & Biases
[00:52:55.200 --> 00:52:57.320]   is as part of these kind of building
[00:52:57.320 --> 00:52:59.160]   very large convergent systems,
[00:52:59.160 --> 00:53:02.680]   sort of leveraging and doing all of those experiments.
[00:53:02.680 --> 00:53:05.960]   So finding the right kind of batch size
[00:53:05.960 --> 00:53:07.960]   that gives you the optimal performance
[00:53:07.960 --> 00:53:10.200]   whilst not impacting your kind of convergent scheme.
[00:53:10.200 --> 00:53:13.160]   So, and that's one thing that we've been working with.
[00:53:13.160 --> 00:53:14.920]   We've had quite a lot of fun, I think,
[00:53:14.920 --> 00:53:16.880]   with the numerical behavior in some of these systems,
[00:53:16.880 --> 00:53:19.920]   which particularly, so we talk about low precision,
[00:53:19.920 --> 00:53:21.960]   good, goes much faster.
[00:53:21.960 --> 00:53:25.440]   Also dangerous, because you need to manage the precision
[00:53:25.440 --> 00:53:26.840]   a little bit more actively than you might do
[00:53:26.840 --> 00:53:28.280]   in some other kinds of systems.
[00:53:28.280 --> 00:53:31.160]   So building a system that is both computationally
[00:53:31.160 --> 00:53:32.440]   gives you great performance
[00:53:32.440 --> 00:53:33.800]   and also gives you the right answer.
[00:53:33.800 --> 00:53:35.920]   I think that's kind of one of the things
[00:53:35.920 --> 00:53:38.480]   that we found sort of interesting
[00:53:38.480 --> 00:53:40.040]   as we kind of bring these systems up.
[00:53:40.040 --> 00:53:42.920]   And particularly, I would say that the first generations
[00:53:42.920 --> 00:53:45.960]   of our systems, we had some really interesting
[00:53:45.960 --> 00:53:49.280]   sort of convergent schemes running very, very low batch sizes
[00:53:49.280 --> 00:53:52.160]   showing actually extremely rapid convergence,
[00:53:52.160 --> 00:53:53.840]   but even on some big models.
[00:53:53.840 --> 00:53:55.880]   And they were really good.
[00:53:55.880 --> 00:53:57.360]   The one thing that we observed today,
[00:53:57.360 --> 00:53:58.640]   looking at our large scale systems
[00:53:58.640 --> 00:54:00.560]   is that they wouldn't scale.
[00:54:00.560 --> 00:54:02.800]   They didn't have enough batch size to be able to scale
[00:54:02.800 --> 00:54:04.000]   such a very large system.
[00:54:04.000 --> 00:54:06.240]   And so, and we're actually kind of reworking
[00:54:06.240 --> 00:54:07.920]   some of the systems we work with
[00:54:07.920 --> 00:54:10.040]   to support much larger batch sizes.
[00:54:10.040 --> 00:54:13.000]   So looking at optimizers, we will be using SGD
[00:54:13.000 --> 00:54:16.320]   or SGDM quite a lot, SGD momentum quite a lot.
[00:54:16.320 --> 00:54:18.200]   We're looking at sort of LAN, very large scale
[00:54:18.200 --> 00:54:20.720]   sort of batch optimizers that have been used
[00:54:20.720 --> 00:54:22.880]   by sort of Google and NVIDIA as well
[00:54:22.880 --> 00:54:24.320]   for their large scale systems.
[00:54:24.320 --> 00:54:26.600]   So yeah, so that's certainly been something
[00:54:26.600 --> 00:54:28.280]   that's been a whole bunch of fun.
[00:54:28.280 --> 00:54:30.520]   And I would say it's been very challenging.
[00:54:30.520 --> 00:54:32.400]   I mean, the number of hours of compute time
[00:54:32.400 --> 00:54:34.320]   that we have been sort of spending,
[00:54:34.320 --> 00:54:36.960]   sort of developing these kinds of systems
[00:54:36.960 --> 00:54:39.560]   and sort of finding the bugs in the models sometimes
[00:54:39.560 --> 00:54:42.480]   where, oh, we've got the layers wrong
[00:54:42.480 --> 00:54:45.880]   or there's something that's just not quite laid out correctly
[00:54:45.880 --> 00:54:47.800]   and that's impacting the convergence of these systems
[00:54:47.800 --> 00:54:48.800]   and we need to go and find that.
[00:54:48.800 --> 00:54:50.080]   So there are those kinds of things.
[00:54:50.080 --> 00:54:51.680]   In terms of actually building,
[00:54:51.680 --> 00:54:54.400]   sort of bringing the kind of new hardware to market,
[00:54:54.400 --> 00:54:57.160]   I mean, that has been a tremendous journey
[00:54:57.160 --> 00:55:00.000]   and it goes all the way from completely new architecture,
[00:55:00.000 --> 00:55:02.080]   massive amounts of memory on chip.
[00:55:02.080 --> 00:55:04.160]   How do you, at the fundamental silicon level,
[00:55:04.160 --> 00:55:06.640]   test that system and make sure you've got,
[00:55:06.640 --> 00:55:08.200]   that your processor actually works?
[00:55:08.200 --> 00:55:10.160]   So that was an interesting problem
[00:55:10.160 --> 00:55:12.160]   that some of our team had to tackle
[00:55:12.160 --> 00:55:14.480]   and we very successfully worked through.
[00:55:14.480 --> 00:55:16.360]   How do you take one of those systems
[00:55:16.360 --> 00:55:19.480]   and integrate it together into a cluster of 16 IPUs,
[00:55:19.480 --> 00:55:22.800]   a cluster of 64 IPUs, a cluster of 1,000 IPUs?
[00:55:22.800 --> 00:55:24.240]   How do you make that kind of system
[00:55:24.240 --> 00:55:25.480]   work at that kind of scale?
[00:55:25.480 --> 00:55:28.880]   How do you take all of the various applications
[00:55:28.880 --> 00:55:30.800]   and map them down to the frameworks?
[00:55:30.800 --> 00:55:33.000]   How do you support multiple different frameworks efficiently?
[00:55:33.000 --> 00:55:37.520]   I mean, there's been lots of fun across all of these spaces.
[00:55:37.520 --> 00:55:39.960]   So one of the things I would observe
[00:55:39.960 --> 00:55:44.040]   is sort of building these very large scale training systems
[00:55:44.040 --> 00:55:46.840]   is one of the big challenges.
[00:55:46.840 --> 00:55:48.800]   I mean, it's one of those kind of really big,
[00:55:48.800 --> 00:55:50.520]   it's a bit like building the old super computers.
[00:55:50.520 --> 00:55:54.680]   It's the grand challenge problems of our time, potentially.
[00:55:54.680 --> 00:55:57.600]   So it's quite interesting to go and try and do that
[00:55:57.600 --> 00:55:59.600]   from scratch with a completely new set of architectures.
[00:55:59.600 --> 00:56:02.800]   And actually, I mean, one of the fantastic things
[00:56:02.800 --> 00:56:05.120]   about Graphcore is how quickly we can move
[00:56:05.120 --> 00:56:06.760]   through some of these processes.
[00:56:06.760 --> 00:56:10.640]   Yeah, it's, there have been a lot of challenges
[00:56:10.640 --> 00:56:11.680]   through that phase.
[00:56:11.680 --> 00:56:14.040]   I would say we've met most of them with great success,
[00:56:14.040 --> 00:56:14.880]   which is quite nice.
[00:56:14.880 --> 00:56:18.600]   And we're at the point where we can now bring this all
[00:56:18.600 --> 00:56:20.960]   to the world, which is very exciting.
[00:56:20.960 --> 00:56:21.800]   - That's so exciting.
[00:56:21.800 --> 00:56:23.240]   It seems like such a fun job.
[00:56:23.240 --> 00:56:25.440]   And congratulations on the latest benchmark.
[00:56:25.440 --> 00:56:27.960]   We'll definitely put a link to that in the show notes.
[00:56:27.960 --> 00:56:28.800]   - Yes, thanks for that.
[00:56:28.800 --> 00:56:30.960]   I mean, it's been a lot of work
[00:56:30.960 --> 00:56:32.760]   from quite a large team of people.
[00:56:32.760 --> 00:56:35.640]   So, and actually very little from me.
[00:56:35.640 --> 00:56:38.720]   So the hardware and the software team at Graphcore
[00:56:38.720 --> 00:56:40.960]   have been beavering away for a long period of time
[00:56:40.960 --> 00:56:43.960]   and they have all done a really great job.
[00:56:43.960 --> 00:56:45.680]   - Awesome, thanks for your time.
[00:56:45.680 --> 00:56:47.840]   - Excellent, thanks very much, Lucas.
[00:56:47.840 --> 00:56:49.960]   - Thanks for listening to another episode
[00:56:49.960 --> 00:56:51.520]   of Gradient Dissent.
[00:56:51.520 --> 00:56:53.320]   Doing these interviews are a lot of fun
[00:56:53.320 --> 00:56:55.720]   and it's especially fun for me when I can actually hear
[00:56:55.720 --> 00:56:58.040]   from the people that are listening to these episodes.
[00:56:58.040 --> 00:57:00.760]   So if you wouldn't mind leaving a comment
[00:57:00.760 --> 00:57:03.480]   and telling me what you think or starting a conversation,
[00:57:03.480 --> 00:57:05.960]   that would make me inspired to do more of these episodes.
[00:57:05.960 --> 00:57:08.720]   And also if you wouldn't mind liking and subscribing,
[00:57:08.720 --> 00:57:10.040]   I'd appreciate that a lot.

