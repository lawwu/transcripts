
[00:00:00.000 --> 00:00:14.000]   Welcome to Module 2. In this module, we want to understand how large language models work.
[00:00:14.000 --> 00:00:19.640]   But first, let's check out some use cases that LLMs enable. LLMs can be used to generate
[00:00:19.640 --> 00:00:27.280]   text, like marketing copy or emails. They can answer questions, translate documents,
[00:00:27.280 --> 00:00:33.720]   determine the sentiment of a text. LLMs can summarize long documents. They can act as
[00:00:33.720 --> 00:00:38.920]   personal assistants or chatbots. We can use them to query tabular data, interact with
[00:00:38.920 --> 00:00:48.000]   API, or even evaluate other language models. But what happens behind the scenes? Understanding
[00:00:48.000 --> 00:00:54.200]   LLM architecture isn't necessary for building applications. It's like driving a car. You
[00:00:54.200 --> 00:00:59.400]   don't need to know how the engine works to drive. Still, some technical details can be
[00:00:59.400 --> 00:01:07.880]   helpful. Looking at GPT-4 technical report, we can read that GPT-4, one of the most known
[00:01:07.880 --> 00:01:16.480]   LLMs, is a transformer-based model pre-trained to predict the next token in a document. We
[00:01:16.480 --> 00:01:22.520]   won't dive and try to understand the transformer architecture that's not necessary for building
[00:01:22.520 --> 00:01:28.000]   LLM applications, but we want to focus on the second part of the statement, which is
[00:01:28.000 --> 00:01:34.480]   predicting the next token in a document. So here's how this works. We start with some
[00:01:34.480 --> 00:01:39.920]   input text. In our case, "Weights and Biases is." Then we tokenize the text. We need to
[00:01:39.920 --> 00:01:45.640]   split it into tokens that are represented by numbers that will fit into the black box,
[00:01:45.640 --> 00:01:51.760]   which is the LLM. Then, as an output of the LLM, we have a distribution of probabilities
[00:01:51.760 --> 00:01:58.440]   over the entire vocabulary, all of the tokens that we have available for our model. And
[00:01:58.440 --> 00:02:04.400]   each of these tokens comes with a probability that it comes as a next token in the sequence.
[00:02:04.400 --> 00:02:11.120]   And based on those probabilities, we pick, we sample one of the tokens to follow, to
[00:02:11.120 --> 00:02:15.920]   continue with the sequence. In this case, we select the token "V" because it has a high
[00:02:15.920 --> 00:02:22.880]   output probability. Then we append this token to our input sequence and we repeat the process.
[00:02:22.880 --> 00:02:28.680]   We tokenize it, we fit it into the LLM, and again, we get a distribution of probabilities
[00:02:28.680 --> 00:02:35.520]   across our vocabulary, all of the tokens. And again, we pick a token with high probability.
[00:02:35.520 --> 00:02:42.880]   In this case, let's pick "Machine." And finally, we again repeat this whole process and we
[00:02:42.880 --> 00:02:49.480]   sample the token "Learning." And if we continue with this process, we can predict, we can
[00:02:49.480 --> 00:02:56.400]   sample the text "Weights and Biases is the machine learning platform." Companies like
[00:02:56.400 --> 00:03:03.320]   OpenAI, Cohere, Mosaic, or Meta have already trained models for us. And we use them behind
[00:03:03.320 --> 00:03:08.480]   APIs, which means we do not need to train these models to use them in our applications.
[00:03:08.480 --> 00:03:13.800]   However, knowing how they were trained can provide useful insights.
[00:03:13.800 --> 00:03:19.800]   There are two main steps in training LLMs. The first is pre-training, where the model
[00:03:19.800 --> 00:03:26.160]   learns from a massive dataset with sources like the entire internet, such as Common Crawl
[00:03:26.160 --> 00:03:34.240]   C4, GitHub, Wikipedia, Books, Archive, which are academic papers, and Stack Exchange, which
[00:03:34.240 --> 00:03:40.960]   is a set of questions and answers. This pre-training dataset has been published by Meta, that trained
[00:03:40.960 --> 00:03:46.640]   LLAMA model. We don't know exactly the pre-training dataset used for training GPT-4, but we can
[00:03:46.640 --> 00:03:53.600]   imagine it must have been something similar. In this case, in pre-training, a model that
[00:03:53.600 --> 00:03:59.280]   has gone through this phase is pretty good in predicting texts such as found in this
[00:03:59.280 --> 00:04:07.720]   dataset on the internet, on GitHub, on Wikipedia, and so on. But this may not be enough. We
[00:04:07.720 --> 00:04:13.320]   actually want this model to follow our instructions, to respond to our questions. And this is where
[00:04:13.320 --> 00:04:20.440]   the second step, which is supervised instruction tuning, can be helpful. In this step, the
[00:04:20.440 --> 00:04:27.040]   model is further trained with expert-generated question-answer pairs. And this helps align
[00:04:27.040 --> 00:04:36.680]   the model with user expectations and follow instructions. Some LLMs, like GPT-4, undergo
[00:04:36.680 --> 00:04:42.000]   an additional phase, reinforcement learning from human feedback. Here, the model is trained
[00:04:42.000 --> 00:04:49.360]   to optimize for higher quality answers preferred by human judges. Understanding these training
[00:04:49.360 --> 00:04:55.360]   phases can be helpful. It can give us intuitions, for example, how to formulate a prompt in
[00:04:55.360 --> 00:05:01.960]   order to get the expected answer, the expected output from the model. In the next video,
[00:05:01.960 --> 00:05:06.120]   we'll experiment with these concepts in Jupyter Notebook with code.
[00:05:06.120 --> 00:05:09.480]   [MUSIC PLAYING]
[00:05:09.840 --> 00:05:12.840]   [MUSIC ENDS]
[00:05:12.840 --> 00:05:15.260]   (soft music)

