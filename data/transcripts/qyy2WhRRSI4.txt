
[00:00:00.000 --> 00:00:05.000]   Awesome. I will double check and make sure I'm live.
[00:00:05.000 --> 00:00:09.240]   So if you're watching live or if you're watching the recap,
[00:00:09.240 --> 00:00:10.740]   there'll be a 30 second delay.
[00:00:10.740 --> 00:00:12.720]   Sorry about that.
[00:00:12.720 --> 00:00:28.360]   I can hear an echo.
[00:00:28.360 --> 00:00:33.360]   Awesome. And we have, oh wow.
[00:00:33.360 --> 00:00:35.000]   About 20 people already.
[00:00:35.000 --> 00:00:36.160]   Welcome everybody.
[00:00:36.160 --> 00:00:41.160]   Thanks for joining us on a changed or is it change schedule
[00:00:41.160 --> 00:00:42.840]   or change of schedule?
[00:00:42.840 --> 00:00:45.680]   Either ways, thanks for joining us on a different time.
[00:00:45.680 --> 00:00:48.880]   I've been trying to wake up early,
[00:00:48.880 --> 00:00:50.520]   which means I have to go to bed early.
[00:00:50.520 --> 00:00:55.520]   So I moved this a few hours before our usual schedule time.
[00:00:55.960 --> 00:00:58.440]   Thank you for making the time and joining us live.
[00:00:58.440 --> 00:00:59.840]   We'll be reading about the,
[00:00:59.840 --> 00:01:01.960]   I'm sharing the wrong slide.
[00:01:01.960 --> 00:01:04.680]   So let me move one slide up.
[00:01:04.680 --> 00:01:06.620]   I promise that wasn't intentional.
[00:01:06.620 --> 00:01:11.080]   We'll be reading about the NFNet high performance,
[00:01:11.080 --> 00:01:14.040]   large scale image recognition without normalization.
[00:01:14.040 --> 00:01:15.500]   That's a mouthful paper.
[00:01:15.500 --> 00:01:19.840]   And we'll be going through an overall understanding
[00:01:19.840 --> 00:01:22.520]   of what's happening inside of this paper
[00:01:22.520 --> 00:01:25.140]   and why should it interest you.
[00:01:25.980 --> 00:01:29.420]   The reason why I'm covering this paper is twofold.
[00:01:29.420 --> 00:01:34.060]   I had recently interviewed Gil,
[00:01:34.060 --> 00:01:39.060]   who is one of the top ranked current Kaggle masters.
[00:01:39.060 --> 00:01:41.580]   He's almost a Kaggle grandmaster.
[00:01:41.580 --> 00:01:43.340]   And he had shared in his solution
[00:01:43.340 --> 00:01:44.900]   to the Bird Cliff competition
[00:01:44.900 --> 00:01:48.020]   that they found this quite useful.
[00:01:48.020 --> 00:01:51.260]   So I was quite curious about reading this paper myself.
[00:01:51.260 --> 00:01:54.860]   And for my own selfish reasons,
[00:01:54.860 --> 00:01:56.700]   I decided to pick this paper up
[00:01:56.700 --> 00:01:58.100]   and that's why we'll be covering this.
[00:01:58.100 --> 00:02:03.100]   Let me put that link in the YouTube chat.
[00:02:03.100 --> 00:02:09.340]   If you go to YouTube and just search for ctds.show,
[00:02:09.340 --> 00:02:10.900]   you will be able to find this interview
[00:02:10.900 --> 00:02:12.400]   on the top of the list.
[00:02:12.400 --> 00:02:15.940]   So that's the origin story behind this paper.
[00:02:15.940 --> 00:02:20.820]   Since more SOTA papers have come out since,
[00:02:20.820 --> 00:02:23.540]   I will not be taking an in-depth look
[00:02:23.540 --> 00:02:28.420]   into how higher accuracy is it for this paper,
[00:02:28.420 --> 00:02:29.580]   which is how we've been doing
[00:02:29.580 --> 00:02:31.780]   these paper reading groups in the past.
[00:02:31.780 --> 00:02:34.260]   If you're new here, welcome again.
[00:02:34.260 --> 00:02:37.060]   We'll be looking at what this paper produces
[00:02:37.060 --> 00:02:40.660]   and how are coordinates
[00:02:40.660 --> 00:02:42.740]   without normalization performing better.
[00:02:42.740 --> 00:02:47.300]   As a homework, I'll probably also livestream myself
[00:02:47.300 --> 00:02:49.380]   trying to apply this to audio data
[00:02:49.380 --> 00:02:52.820]   because the interview I mentioned had audio data in it
[00:02:52.820 --> 00:02:55.940]   and apparently these sets of neural networks
[00:02:55.940 --> 00:02:56.780]   performed really well.
[00:02:56.780 --> 00:02:58.580]   So I'm also curious to try that out.
[00:02:58.580 --> 00:03:01.020]   Awesome.
[00:03:01.020 --> 00:03:04.100]   Let me continue in my slide once I find the right tab.
[00:03:04.100 --> 00:03:05.020]   And if you're new here,
[00:03:05.020 --> 00:03:07.220]   there've been a bunch of paper reading groups in the past.
[00:03:07.220 --> 00:03:09.260]   You can find the recordings there.
[00:03:09.260 --> 00:03:10.820]   How these start off,
[00:03:10.820 --> 00:03:12.940]   I have had a common request
[00:03:12.940 --> 00:03:16.340]   to please also give a summary upfront.
[00:03:16.340 --> 00:03:18.540]   So let me do that first.
[00:03:18.540 --> 00:03:20.580]   As you can probably judge from the title,
[00:03:22.380 --> 00:03:23.220]   we're trying to assess
[00:03:23.220 --> 00:03:25.100]   how can convolutional neural networks,
[00:03:25.100 --> 00:03:30.100]   so not a transformer, just a coordinate for this case,
[00:03:30.100 --> 00:03:35.340]   performs without normalization.
[00:03:35.340 --> 00:03:38.220]   Inside of the paper, the authors
[00:03:38.220 --> 00:03:42.740]   take a look at batch normalization.
[00:03:42.740 --> 00:03:43.940]   That's a mouthful.
[00:03:43.940 --> 00:03:45.500]   Batch normalization.
[00:03:45.500 --> 00:03:46.900]   I got it the second time.
[00:03:46.900 --> 00:03:49.740]   And they try to, first of all, appreciate it.
[00:03:49.740 --> 00:03:50.620]   Why does it work?
[00:03:50.620 --> 00:03:53.020]   Why was it used so well?
[00:03:53.020 --> 00:03:57.220]   And then they try to pull it out of the equation
[00:03:57.220 --> 00:03:59.100]   and see how can they create a network
[00:03:59.100 --> 00:04:01.740]   that works better without it.
[00:04:01.740 --> 00:04:05.420]   In the process, they compare their paper
[00:04:05.420 --> 00:04:08.460]   against efficient nets and they outperform efficient nets.
[00:04:08.460 --> 00:04:11.460]   So that's the benchmark that they're trying to beat in here.
[00:04:11.460 --> 00:04:14.860]   And they achieve this by a few tricks
[00:04:14.860 --> 00:04:19.860]   that we'll be covering in this one hour live stream.
[00:04:20.860 --> 00:04:24.820]   So they account for the removal of batch normalization.
[00:04:24.820 --> 00:04:28.380]   They introduce something known as adaptive gradient clipping.
[00:04:28.380 --> 00:04:33.140]   And the last one is an incorrect point I had left in here.
[00:04:33.140 --> 00:04:38.100]   Here's an overall view of the architecture.
[00:04:38.100 --> 00:04:39.260]   Again, this is a summary.
[00:04:39.260 --> 00:04:41.020]   We'll go in depth into all of this.
[00:04:41.020 --> 00:04:44.820]   On the left, I think it's a,
[00:04:44.820 --> 00:04:48.300]   so between both of these,
[00:04:48.300 --> 00:04:51.660]   these are two different layers that you can expect.
[00:04:51.660 --> 00:04:53.660]   One has average pooling included in it
[00:04:53.660 --> 00:04:54.940]   and the other one does not.
[00:04:54.940 --> 00:04:57.100]   As you can see, this is quite similar
[00:04:57.100 --> 00:05:01.340]   to what you'd expect inside of a ResNet architecture
[00:05:01.340 --> 00:05:03.980]   minus the batch norm layer.
[00:05:03.980 --> 00:05:09.020]   So I'll let you look at this before we move back
[00:05:09.020 --> 00:05:11.460]   and start reading or start going through in depth.
[00:05:11.460 --> 00:05:16.020]   Beta and alpha don't have to make sense to you.
[00:05:16.020 --> 00:05:18.580]   It's again a new, let's say,
[00:05:18.580 --> 00:05:20.980]   parameter that has been introduced here.
[00:05:20.980 --> 00:05:22.940]   It's again to account for
[00:05:22.940 --> 00:05:25.700]   when you remove batch normalization, what does happen?
[00:05:25.700 --> 00:05:29.380]   And if you pay close attention,
[00:05:29.380 --> 00:05:32.700]   everything else would more or less look the same here.
[00:05:32.700 --> 00:05:37.580]   So this is the architectural block that you can expect.
[00:05:37.580 --> 00:05:45.380]   Awesome, so that was a very quick TikTok timed summary
[00:05:46.100 --> 00:05:48.180]   I promise I'm not starting a TikTok.
[00:05:48.180 --> 00:05:50.140]   I just wanted to give the summary upfront
[00:05:50.140 --> 00:05:53.100]   and we'll start the reading group with one announcement.
[00:05:53.100 --> 00:05:55.340]   As you know, at Weights & Biases,
[00:05:55.340 --> 00:05:57.580]   we really care about the community a lot
[00:05:57.580 --> 00:06:00.660]   and let me share this link in the chart.
[00:06:00.660 --> 00:06:12.340]   We're hosting a blogathon and this is solely for you all,
[00:06:13.420 --> 00:06:15.980]   to the people listening or watching the recap.
[00:06:15.980 --> 00:06:20.420]   This is going to run from 1st of June to 14th of June.
[00:06:20.420 --> 00:06:24.060]   So you still have about, I'd say more than a week
[00:06:24.060 --> 00:06:26.180]   to get your submissions in.
[00:06:26.180 --> 00:06:27.100]   What is this?
[00:06:27.100 --> 00:06:30.260]   We want you to get started in your blogging journey
[00:06:30.260 --> 00:06:32.500]   or to push you further in your journey.
[00:06:32.500 --> 00:06:35.260]   And for that purpose, we are also willing to give you
[00:06:35.260 --> 00:06:38.620]   our money in the form of CUDA power and merch.
[00:06:38.620 --> 00:06:43.220]   So if you want to earn some compute credits
[00:06:43.220 --> 00:06:47.300]   or WNB merchandise, which I think both are equally cool,
[00:06:47.300 --> 00:06:50.060]   you're welcome to send in your submissions,
[00:06:50.060 --> 00:06:53.220]   take a look at this link and you can understand
[00:06:53.220 --> 00:06:54.380]   how you can get started.
[00:06:54.380 --> 00:06:58.060]   It's open to any topic of interest to you.
[00:06:58.060 --> 00:07:01.140]   You can potentially also do the suggested homework
[00:07:01.140 --> 00:07:02.260]   from this live stream.
[00:07:02.260 --> 00:07:05.660]   I always suggest a few ideas at the end of the live stream
[00:07:05.660 --> 00:07:07.180]   that you can get started with.
[00:07:08.180 --> 00:07:11.180]   And those will be also great submissions in my mind.
[00:07:11.180 --> 00:07:13.940]   I might be a bit biased and you might get better scores.
[00:07:13.940 --> 00:07:14.780]   We never know.
[00:07:14.780 --> 00:07:21.100]   Or you can write on any topic of your interest.
[00:07:21.100 --> 00:07:23.500]   There's no rules really.
[00:07:23.500 --> 00:07:26.380]   Just please be mindful that you don't plagiarize
[00:07:26.380 --> 00:07:31.380]   and you don't, I think that's the only rule.
[00:07:31.380 --> 00:07:36.020]   Please don't copy paste someone else's work.
[00:07:36.020 --> 00:07:39.780]   Have fun and send in any blogs you might have.
[00:07:39.780 --> 00:07:42.220]   In return, we will definitely be happy
[00:07:42.220 --> 00:07:43.500]   to share your work further.
[00:07:43.500 --> 00:07:48.500]   Or if it's quite good and if you end up enjoying it,
[00:07:48.500 --> 00:07:52.780]   we'll also be sending some prizes to winners.
[00:07:52.780 --> 00:07:55.460]   So if you want our money, please,
[00:07:55.460 --> 00:07:58.340]   this is a good chance to get started with writing.
[00:07:58.340 --> 00:08:04.780]   So here's the agenda for today.
[00:08:04.780 --> 00:08:06.140]   And now I'll be slowing things down
[00:08:06.140 --> 00:08:11.140]   and not rushing through the entire overview in one second.
[00:08:11.140 --> 00:08:14.340]   We'll be looking at the prerequisites
[00:08:14.340 --> 00:08:16.580]   because there are a few in this paper.
[00:08:16.580 --> 00:08:18.900]   I'll be giving the overview of the paper first
[00:08:18.900 --> 00:08:22.660]   and then we'll go into thorough reading of the paper
[00:08:22.660 --> 00:08:24.500]   followed by an overview of the architecture.
[00:08:24.500 --> 00:08:26.340]   And one really cool thing that the authors
[00:08:26.340 --> 00:08:29.060]   have included in this paper,
[00:08:29.060 --> 00:08:30.860]   which as I watched Yannick's video
[00:08:30.860 --> 00:08:31.940]   just before this live stream,
[00:08:31.940 --> 00:08:34.660]   he also said the authors have shared
[00:08:34.660 --> 00:08:36.460]   what didn't work really well for them
[00:08:36.460 --> 00:08:39.500]   on experiments that had harmful effects.
[00:08:39.500 --> 00:08:42.100]   It's just one page, but it was a really cool read.
[00:08:42.100 --> 00:08:43.860]   And we'll also go through that.
[00:08:43.860 --> 00:08:45.220]   After that, you'll get some ideas
[00:08:45.220 --> 00:08:47.220]   on what you can potentially write about.
[00:08:47.220 --> 00:08:51.420]   So I want to mention another incredible resource
[00:08:51.420 --> 00:08:52.380]   that I had used today.
[00:08:52.380 --> 00:08:55.380]   And the reason it's not linked here is
[00:08:55.380 --> 00:08:59.380]   it's easier if I just show you how to find it.
[00:08:59.380 --> 00:09:01.340]   So I'll just go to YouTube
[00:09:01.340 --> 00:09:03.260]   and search for Yannick Culture NFNet.
[00:09:03.260 --> 00:09:07.940]   And I highly encourage everyone to check this video out
[00:09:07.940 --> 00:09:10.620]   before watching this or after watching this.
[00:09:10.620 --> 00:09:13.060]   I always read the paper by myself.
[00:09:13.060 --> 00:09:17.820]   And after that, I end up watching Yannick's videos.
[00:09:17.820 --> 00:09:20.020]   He's an absolutely incredible...
[00:09:20.020 --> 00:09:23.900]   I'm trying to find the right word.
[00:09:23.900 --> 00:09:25.260]   He's not just a content creator
[00:09:25.260 --> 00:09:30.260]   because it's really quite depth-filled content.
[00:09:30.260 --> 00:09:33.540]   So Yannick's video is also an absolute incredible resource
[00:09:33.540 --> 00:09:35.300]   that I would recommend everyone to check out.
[00:09:35.300 --> 00:09:38.060]   I have learned a lot from that video
[00:09:38.060 --> 00:09:39.900]   and I'll probably be using a lot of things
[00:09:39.900 --> 00:09:41.060]   that he taught in that.
[00:09:41.060 --> 00:09:45.060]   You can also check out the official repository
[00:09:45.060 --> 00:09:47.700]   for the paper's implementation.
[00:09:47.700 --> 00:09:48.580]   It's in JAX.
[00:09:48.580 --> 00:09:49.700]   And if you're new to JAX,
[00:09:49.700 --> 00:09:52.420]   we have been doing a JAX series that you all can learn from.
[00:09:52.420 --> 00:09:57.420]   So most of that I can promote to you all
[00:09:57.420 --> 00:10:00.460]   to watch or learn the better.
[00:10:00.460 --> 00:10:05.980]   And I would also mention the interview
[00:10:05.980 --> 00:10:09.540]   where I first learned of this network.
[00:10:09.540 --> 00:10:14.220]   So now let's start with the overview of the paper.
[00:10:14.220 --> 00:10:16.740]   The authors address that there are some issues
[00:10:16.740 --> 00:10:18.140]   with batch normalization.
[00:10:19.180 --> 00:10:23.340]   And these include, there's a slight memory overhead.
[00:10:23.340 --> 00:10:26.220]   These usually cause a shift
[00:10:26.220 --> 00:10:28.860]   when you're working with train and test.
[00:10:28.860 --> 00:10:34.020]   So they first mentioned why batch normalization
[00:10:34.020 --> 00:10:35.340]   is really helpful.
[00:10:35.340 --> 00:10:38.900]   Does anyone want to take a job at this?
[00:10:38.900 --> 00:10:42.620]   Why do we use batch normalization?
[00:10:42.620 --> 00:10:46.120]   (computer mouse clicking)
[00:10:46.120 --> 00:10:52.820]   I'll give everyone a few seconds to answer.
[00:10:52.820 --> 00:10:54.820]   If not, I'll continue myself.
[00:10:54.820 --> 00:10:59.220]   So the question is, why do we use batch normalization?
[00:10:59.220 --> 00:11:02.720]   (computer mouse clicking)
[00:11:02.720 --> 00:11:28.640]  , okay.
[00:11:28.640 --> 00:11:31.360]   Before I embarrass myself further, I'll give the answer.
[00:11:31.360 --> 00:11:34.440]   It's to stabilize the training with really deep networks.
[00:11:34.440 --> 00:11:40.840]   When you batch normal, it's a sort of a chicken egg situation
[00:11:40.840 --> 00:11:46.060]   but to create a very deep networks,
[00:11:46.060 --> 00:11:48.080]   you need to add normalization.
[00:11:48.080 --> 00:11:50.520]   For some reason that the original paper
[00:11:50.520 --> 00:11:53.200]   didn't cite correctly or didn't share correctly,
[00:11:53.200 --> 00:11:54.520]   it was corrected afterwards.
[00:11:54.520 --> 00:11:56.960]   Batch normalization works really well.
[00:11:56.960 --> 00:11:59.760]   Inside of a very deep neural network,
[00:11:59.760 --> 00:12:03.360]   batch normalization helps create
[00:12:03.360 --> 00:12:06.920]   a properly normalized distribution.
[00:12:06.920 --> 00:12:08.560]   Yannick had drawn this in his video,
[00:12:08.560 --> 00:12:12.200]   so I'll copy his style and let me
[00:12:12.200 --> 00:12:16.040]   share my screen again and draw this somewhere.
[00:12:16.040 --> 00:12:17.520]   My Microsoft OneNote.
[00:12:17.520 --> 00:12:24.440]   Share screen, select OneNote,
[00:12:24.440 --> 00:12:26.320]   zoom in somewhere, pick up my pen.
[00:12:26.960 --> 00:12:30.460]   (computer mouse clicking)
[00:12:30.460 --> 00:12:33.960]   So inside of a network, you can assume
[00:12:33.960 --> 00:12:36.560]   that whatever parameters are being passed
[00:12:36.560 --> 00:12:38.040]   eventually get distorted.
[00:12:38.040 --> 00:12:42.440]   And ideally you want these normalized.
[00:12:42.440 --> 00:12:47.280]   So with batch norm, you will make these nice
[00:12:47.280 --> 00:12:52.280]   and normalized around the origin of the axis,
[00:12:52.280 --> 00:12:54.760]   which is what normalization is supposed to do.
[00:12:54.760 --> 00:12:56.480]   Again, I learned this from Yannick's video,
[00:12:56.480 --> 00:12:59.480]   so I'm directly sharing what he said in the video.
[00:12:59.480 --> 00:13:02.280]   Please do check that video out and his channel as well.
[00:13:02.280 --> 00:13:10.360]   After that, so that's what batch normalization does.
[00:13:10.360 --> 00:13:12.360]   And when you start removing it,
[00:13:12.360 --> 00:13:14.040]   you have to of course account for the fact
[00:13:14.040 --> 00:13:16.800]   that nothing else is taking care of this.
[00:13:16.800 --> 00:13:20.080]   Another concept that we absolutely need to know
[00:13:20.080 --> 00:13:22.120]   before we start reading the paper
[00:13:23.200 --> 00:13:25.000]   is what is gradient clipping?
[00:13:25.000 --> 00:13:27.200]   So hopefully I can get one answer this time
[00:13:27.200 --> 00:13:30.960]   if anyone wants to take a job, what is gradient clipping?
[00:13:49.480 --> 00:13:51.960]   I missed Nikhil Nair's answer
[00:13:51.960 --> 00:13:55.600]   from for why do we use batch normalization?
[00:13:55.600 --> 00:13:57.160]   He says to speed up training.
[00:13:57.160 --> 00:14:02.720]   It's mostly to stabilize it as well.
[00:14:02.720 --> 00:14:05.800]   In some ways, if you remove batch normalization,
[00:14:05.800 --> 00:14:09.260]   you could speed up your training,
[00:14:09.260 --> 00:14:11.860]   but you'll also not be able to train the deep model.
[00:14:19.380 --> 00:14:22.320]   Awesome, Nikhil has another answer, thanks Nikhil.
[00:14:22.320 --> 00:14:27.160]   And I hope everyone else is as interactive as Nikhil.
[00:14:27.160 --> 00:14:28.640]   For my sakes.
[00:14:28.640 --> 00:14:31.280]   Guys, I'm just in a room speaking to a screen
[00:14:31.280 --> 00:14:34.360]   and this doesn't feel as interactive, please.
[00:14:34.360 --> 00:14:35.480]   Help me out a bit.
[00:14:35.480 --> 00:14:38.520]   Nikhil says it's to curb vanishing gradient problem
[00:14:38.520 --> 00:14:40.040]   or exploding gradient.
[00:14:40.040 --> 00:14:42.560]   Awesome, so it's the latter, Nikhil.
[00:14:42.560 --> 00:14:47.040]   It's just to curb the exploding gradient problem,
[00:14:47.040 --> 00:14:47.960]   as I understand.
[00:14:47.960 --> 00:14:51.480]   Let me share my screen and again, elaborate on this.
[00:14:51.480 --> 00:14:55.780]   So as you go about training a model,
[00:14:55.780 --> 00:14:58.140]   there are two problems like Nikhil pointed out.
[00:14:58.140 --> 00:15:02.820]   One of them is, let me write this somewhere.
[00:15:02.820 --> 00:15:04.420]   It's called vanishing gradients.
[00:15:04.420 --> 00:15:08.320]   I'm so happy of the fact that my writing,
[00:15:08.320 --> 00:15:10.080]   handwriting has improved a bit now.
[00:15:10.080 --> 00:15:14.820]   Vanishing gradient happens when your gradients
[00:15:15.780 --> 00:15:17.100]   start to approach zero.
[00:15:17.100 --> 00:15:21.260]   They start to disappear inside the layers.
[00:15:21.260 --> 00:15:24.440]   This leads to a problem where everything
[00:15:24.440 --> 00:15:28.500]   eventually becomes zero and your model doesn't get trained,
[00:15:28.500 --> 00:15:31.940]   right, because everything inside the model is just now zeros.
[00:15:31.940 --> 00:15:34.940]   The other problem is when you have gradients
[00:15:34.940 --> 00:15:38.380]   and as you're updating these with backprop,
[00:15:38.380 --> 00:15:42.440]   let's say you get a really huge update,
[00:15:42.440 --> 00:15:45.040]   which causes these to go towards infinity.
[00:15:45.880 --> 00:15:48.840]   This is known as exploding gradients
[00:15:48.840 --> 00:15:51.460]   because they're quite literally exploding.
[00:15:51.460 --> 00:15:55.120]   So to deal with this,
[00:15:55.120 --> 00:16:00.780]   we do something known as gradient clipping,
[00:16:00.780 --> 00:16:05.240]   which involves, let's say,
[00:16:05.240 --> 00:16:10.160]   your previous gradient was 0.1
[00:16:10.160 --> 00:16:13.480]   and the next one somehow goes to,
[00:16:13.480 --> 00:16:17.240]   well, let's say it's 0.01 and somehow your model tells you
[00:16:17.240 --> 00:16:18.720]   the next one should be 10.
[00:16:18.720 --> 00:16:20.920]   Okay, you go there and you say,
[00:16:20.920 --> 00:16:25.920]   maybe you should take it easy and you clip this at 0.1.
[00:16:25.920 --> 00:16:29.200]   So if it suggests a value
[00:16:29.200 --> 00:16:36.080]   larger than this, the value just gets clipped at that.
[00:16:36.080 --> 00:16:39.200]   So you can imagine the graph to be somewhat
[00:16:39.200 --> 00:16:40.300]   like a Rayleigh graph.
[00:16:40.500 --> 00:16:44.000]   (computer mouse clicking)
[00:16:44.000 --> 00:16:49.500]   Sorry, not like a Rayleigh, it's more of a constant.
[00:16:49.500 --> 00:16:52.340]   So if the value goes to greater than value,
[00:16:52.340 --> 00:16:53.840]   which you want to clip this at,
[00:16:53.840 --> 00:16:56.540]   the gradient gets set to this value.
[00:16:56.540 --> 00:16:58.020]   So that's gradient clipping.
[00:16:58.020 --> 00:17:03.660]   Let me see if there's anything else I want to mention.
[00:17:03.660 --> 00:17:09.260]   And yep, there's one small question that I have.
[00:17:09.260 --> 00:17:12.180]   Has anyone else heard of NF-ResNet?
[00:17:12.180 --> 00:17:15.540]   And I'm just curious if you have any vague idea
[00:17:15.540 --> 00:17:18.140]   about the difference between NF-ResNet and NFNet.
[00:17:18.140 --> 00:17:21.940]   Are they the same thing or are they different?
[00:17:21.940 --> 00:17:25.440]   (computer mouse clicking)
[00:17:51.460 --> 00:17:54.060]   I didn't get an answer, so I'll continue by myself.
[00:17:54.060 --> 00:17:57.300]   They are different, they're not the same.
[00:17:57.300 --> 00:18:00.620]   And the NF-ResNet paper, as I understand,
[00:18:00.620 --> 00:18:02.260]   came out before NFNet.
[00:18:02.260 --> 00:18:03.580]   We won't be covering that today.
[00:18:03.580 --> 00:18:05.300]   You're welcome to check that out if you want.
[00:18:05.300 --> 00:18:07.300]   I wanted to get the point across that,
[00:18:07.300 --> 00:18:09.380]   no, these are not the same.
[00:18:09.380 --> 00:18:11.180]   These are different papers and models
[00:18:11.180 --> 00:18:14.180]   that came out different times.
[00:18:14.180 --> 00:18:19.580]   With that, we are ready to go into reading the paper.
[00:18:19.580 --> 00:18:21.820]   So three things, again, that we need to know.
[00:18:21.820 --> 00:18:24.460]   What is batch norm, what is skip connections,
[00:18:24.460 --> 00:18:25.940]   and what is gradient clipping?
[00:18:25.940 --> 00:18:30.100]   I'll just wait to check if there are any questions
[00:18:30.100 --> 00:18:30.940]   around this.
[00:18:30.940 --> 00:18:32.700]   I'll, again, encourage everyone to write them
[00:18:32.700 --> 00:18:33.820]   in the chat anywhere.
[00:18:33.820 --> 00:18:38.500]   So I'll wait for 10 seconds before moving on.
[00:18:38.500 --> 00:18:42.220]   If anyone has any questions, now is a good time.
[00:18:42.220 --> 00:18:45.800]   (audience member coughing)
[00:18:45.800 --> 00:19:08.700]   Awesome, I don't see any questions,
[00:19:08.700 --> 00:19:10.620]   so I'll continue as promised.
[00:19:10.620 --> 00:19:12.100]   Let me share back.
[00:19:12.100 --> 00:19:19.220]   Not this, sorry.
[00:19:19.220 --> 00:19:22.060]   I want to share one note so that I can annotate it.
[00:19:22.060 --> 00:19:26.500]   I got the right one this time.
[00:19:26.500 --> 00:19:28.940]   Awesome, so now we'll be going through the paper.
[00:19:28.940 --> 00:19:32.420]   Not word by word.
[00:19:32.420 --> 00:19:34.060]   I usually, here's how I do this.
[00:19:34.060 --> 00:19:37.060]   I read the paper ahead of time, if you're new here,
[00:19:37.060 --> 00:19:39.940]   and I just highlight the parts that I think are interesting
[00:19:39.940 --> 00:19:41.060]   and worth your time.
[00:19:41.060 --> 00:19:44.540]   So inside a 30-minute summary,
[00:19:44.540 --> 00:19:46.340]   things you should absolutely know.
[00:19:46.340 --> 00:19:48.780]   Those are the bits I cover here.
[00:19:48.780 --> 00:19:54.700]   So let's start with the graph on the right.
[00:19:54.700 --> 00:20:03.020]   They are claiming about the fact that our NFNet F1 model,
[00:20:03.020 --> 00:20:09.620]   which would be this one, achieves comparable accuracy
[00:20:10.020 --> 00:20:11.540]   to efficient at B7,
[00:20:11.540 --> 00:20:20.020]   while being 8.7 times faster to train.
[00:20:20.020 --> 00:20:25.980]   So image at top one accuracy is on the y-axis.
[00:20:25.980 --> 00:20:30.500]   As you can see, it's slightly larger, I would say, no?
[00:20:30.500 --> 00:20:36.140]   It looks like it's slightly larger.
[00:20:36.140 --> 00:20:41.140]   And as you can see, the training latency,
[00:20:41.140 --> 00:20:44.500]   so per seconds per step on TPUv3,
[00:20:44.500 --> 00:20:48.100]   with an incredible batch size of 32,
[00:20:48.100 --> 00:20:53.740]   is much smaller.
[00:20:53.740 --> 00:20:58.340]   So the model iterates through this in about 0.2 seconds,
[00:20:58.340 --> 00:21:03.340]   I'd like to say, and here it's about 1.4 times,
[00:21:03.340 --> 00:21:04.740]   1.4 seconds.
[00:21:05.740 --> 00:21:09.460]   And then they say our NFNet F5 model
[00:21:09.460 --> 00:21:14.460]   has similar latency to BNet, sorry, B7,
[00:21:14.460 --> 00:21:19.100]   but has a sort of top one accuracy.
[00:21:19.100 --> 00:21:21.980]   So they're really comparing their model
[00:21:21.980 --> 00:21:24.380]   against EfficientNet, which again,
[00:21:24.380 --> 00:21:26.420]   if you're getting confused,
[00:21:26.420 --> 00:21:30.300]   the blue line are the EfficientNet family of models.
[00:21:30.300 --> 00:21:35.180]   Remember, whenever authors work on a convolutional network,
[00:21:35.180 --> 00:21:38.020]   they really try to scale their models up.
[00:21:38.020 --> 00:21:39.220]   How do they do that?
[00:21:39.220 --> 00:21:41.460]   Let me scroll down a bit and see if there's
[00:21:41.460 --> 00:21:48.220]   a table highlighting this, which there is.
[00:21:48.220 --> 00:21:55.780]   So for this case, this table shows the family depth
[00:21:55.780 --> 00:21:59.660]   drop rates and input resolution.
[00:21:59.660 --> 00:22:02.620]   As you can see, as you go deeper in the model,
[00:22:02.620 --> 00:22:07.380]   the number of layers increases, the depth increases.
[00:22:07.380 --> 00:22:10.660]   So the authors always have a tiny version of the model,
[00:22:10.660 --> 00:22:12.620]   a big version of the model,
[00:22:12.620 --> 00:22:15.500]   and nowadays there's an Excel version of the model too.
[00:22:15.500 --> 00:22:21.580]   Those are the ones being compared here.
[00:22:21.580 --> 00:22:25.140]   And that's why you have EfficientNet B2, blah, blah, blah,
[00:22:25.140 --> 00:22:28.500]   B7, same for this F0 up to F5.
[00:22:29.500 --> 00:22:34.500]   So that's what is an outcome of this.
[00:22:34.500 --> 00:22:38.380]   Now they start by talking about batch normalization
[00:22:38.380 --> 00:22:39.780]   and why is it important.
[00:22:39.780 --> 00:22:47.260]   Without normalization, they say these models do not match
[00:22:47.260 --> 00:22:51.740]   the test accuracies of best batch normalized networks.
[00:22:51.740 --> 00:22:53.860]   And they're talking about ResNets.
[00:22:53.860 --> 00:22:56.340]   In this work, they develop something known
[00:22:56.340 --> 00:23:01.140]   as an adaptive gradient clipping,
[00:23:01.140 --> 00:23:04.500]   which takes care of these instabilities
[00:23:04.500 --> 00:23:08.260]   and design a significantly improved class
[00:23:08.260 --> 00:23:11.540]   of normalizer-free ResNets.
[00:23:11.540 --> 00:23:13.860]   And the normalizer that they're getting rid of
[00:23:13.860 --> 00:23:16.300]   is batch normalization.
[00:23:16.300 --> 00:23:21.580]   And then they again say it's much faster and is so-so.
[00:23:21.580 --> 00:23:24.500]   After this, they talk about why was batch normalization
[00:23:24.500 --> 00:23:28.340]   introduced and how is it key to all
[00:23:28.340 --> 00:23:32.260]   of the models for a few years.
[00:23:32.260 --> 00:23:35.500]   And after that, they talk about-- let me switch colors
[00:23:35.500 --> 00:23:36.660]   here.
[00:23:36.660 --> 00:23:40.060]   I think this should be in red.
[00:23:40.060 --> 00:23:43.060]   I can't select red, so let's do it with pink.
[00:23:43.060 --> 00:23:44.700]   After that, they talk about three
[00:23:44.700 --> 00:23:48.220]   significant practical disadvantages
[00:23:48.220 --> 00:23:49.780]   of batch normalization.
[00:23:49.820 --> 00:23:52.500]   So for some reason, they really want
[00:23:52.500 --> 00:23:56.220]   to be critical about batch normalization,
[00:23:56.220 --> 00:23:59.780]   and that's what they will be removing later on.
[00:23:59.780 --> 00:24:03.340]   It is surprisingly expensive, computationally primitive,
[00:24:03.340 --> 00:24:05.060]   and there's a memory overhead when you're
[00:24:05.060 --> 00:24:06.900]   trying batch normalization.
[00:24:06.900 --> 00:24:10.300]   As a homework, you could train or create a really vanilla
[00:24:10.300 --> 00:24:11.620]   model.
[00:24:11.620 --> 00:24:12.780]   Just run it on your CPU.
[00:24:12.780 --> 00:24:16.980]   You could also write about this.
[00:24:16.980 --> 00:24:19.140]   So just run it on your CPU.
[00:24:19.140 --> 00:24:22.740]   Add a few layers of batch normalization.
[00:24:22.740 --> 00:24:23.820]   Benchmark it.
[00:24:23.820 --> 00:24:26.260]   Benchmark it without the BN layer.
[00:24:26.260 --> 00:24:27.700]   Accuracy doesn't matter.
[00:24:27.700 --> 00:24:30.420]   It's just to prove the fact that there's a memory overhead,
[00:24:30.420 --> 00:24:31.820]   and you can see for yourself.
[00:24:31.820 --> 00:24:33.660]   You'll observe that.
[00:24:33.660 --> 00:24:41.660]   I saw Nikhil ask a question about the nomenclature
[00:24:41.660 --> 00:24:42.660]   of F1 to F5.
[00:24:42.660 --> 00:24:44.180]   I think I've already answered that.
[00:24:44.180 --> 00:24:45.460]   Please let me know if I didn't.
[00:24:45.980 --> 00:24:48.940]   [INAUDIBLE]
[00:24:48.940 --> 00:24:56.100]   And this increases the time required
[00:24:56.100 --> 00:24:59.300]   to evaluate the gradient in some networks.
[00:24:59.300 --> 00:25:03.300]   Second-- so that was just the first disadvantage--
[00:25:03.300 --> 00:25:06.980]   it introduces a discrepancy between the behavior
[00:25:06.980 --> 00:25:11.460]   of model during training and at inference,
[00:25:11.460 --> 00:25:13.420]   since there are some hyperparameters that
[00:25:13.420 --> 00:25:16.260]   need to be tuned.
[00:25:16.260 --> 00:25:18.980]   Third, it breaks the independence
[00:25:18.980 --> 00:25:23.580]   between training examples in the mini-batch.
[00:25:23.580 --> 00:25:26.100]   And then they talk about how this actually negatively
[00:25:26.100 --> 00:25:28.340]   affects the model.
[00:25:28.340 --> 00:25:32.780]   And Yannick actually explains how this could cause leakage.
[00:25:32.780 --> 00:25:35.340]   I'll leave you to watch that video.
[00:25:35.340 --> 00:25:40.060]   But if you don't set up your validation splits correctly,
[00:25:40.060 --> 00:25:41.860]   you could have a data leakage issue,
[00:25:41.860 --> 00:25:43.820]   and you could have very accurate models that
[00:25:43.820 --> 00:25:47.660]   could be potentially dangerous.
[00:25:47.660 --> 00:25:52.660]   Nikhil is saying, thanks for explaining the nomenclature.
[00:25:52.660 --> 00:25:54.500]   You're welcome, Nikhil.
[00:25:54.500 --> 00:25:55.820]   And I missed another question.
[00:25:55.820 --> 00:25:58.700]   Does NF-ResNet have residual blocks?
[00:25:58.700 --> 00:25:59.540]   I believe so, yes.
[00:25:59.540 --> 00:26:08.260]   So let me jump ahead to what I need to cover next.
[00:26:08.260 --> 00:26:11.660]   [AUDIO OUT]
[00:26:11.660 --> 00:26:22.460]   No, I have nothing in the left column.
[00:26:22.460 --> 00:26:24.620]   Here, they summarize their main contributions.
[00:26:24.620 --> 00:26:26.200]   Again, I've already spoken about this,
[00:26:26.200 --> 00:26:29.980]   but I'll keep repeating these as they come up.
[00:26:29.980 --> 00:26:34.100]   They propose adaptive gradient clipping,
[00:26:34.100 --> 00:26:37.700]   which clips gradient based on unit-wise ratio
[00:26:37.700 --> 00:26:40.340]   of gradient norms to parameter norms.
[00:26:40.340 --> 00:26:44.780]   I didn't understand any bit of this sentence.
[00:26:44.780 --> 00:26:48.780]   And if you don't, I said that to help make you feel better.
[00:26:48.780 --> 00:26:50.740]   If you do, let me know.
[00:26:50.740 --> 00:26:52.620]   But I had to actually watch Yannick's video
[00:26:52.620 --> 00:26:56.860]   and just read the equation a few times to understand this.
[00:26:56.860 --> 00:27:02.420]   Then the design of family of NF-ResNets
[00:27:02.420 --> 00:27:06.100]   called NF-Nets, conveniently.
[00:27:06.100 --> 00:27:11.460]   And then they talk about, again, how faster these are to train.
[00:27:11.460 --> 00:27:18.300]   And that the fact that they are more accurate.
[00:27:18.300 --> 00:27:20.620]   Another homework, you could compare these
[00:27:20.620 --> 00:27:23.540]   against some other model that is faster
[00:27:23.540 --> 00:27:27.940]   to train for your data set and see how it fares.
[00:27:27.940 --> 00:27:31.820]   Again, I keep giving these ideas as experiments to have fun with.
[00:27:31.820 --> 00:27:34.860]   If you have your own ideas, you're welcome to run these.
[00:27:34.860 --> 00:27:37.140]   I just leave them out there so that you can potentially
[00:27:37.140 --> 00:27:39.420]   try something.
[00:27:39.420 --> 00:27:43.100]   The third contribution that the authors have.
[00:27:43.100 --> 00:27:46.860]   NF-Nets achieve substantially higher validation accuracies
[00:27:46.860 --> 00:27:51.300]   than batch normalization when fine-tuning on ImageNet
[00:27:51.300 --> 00:27:53.500]   after pre-training on a large private data
[00:27:53.500 --> 00:27:56.380]   set of 300 million labeled images.
[00:27:56.380 --> 00:28:01.020]   So the authors have a secretive data set
[00:28:01.020 --> 00:28:04.100]   that has about 300 million labeled images.
[00:28:04.100 --> 00:28:10.460]   And that helps them even further their model accuracy.
[00:28:10.460 --> 00:28:16.940]   So after this, they help you understand
[00:28:16.940 --> 00:28:18.860]   what batch normalization is.
[00:28:18.860 --> 00:28:22.900]   And I'll just be reading the bold headlines here
[00:28:22.900 --> 00:28:26.500]   and giving a bit of fluff.
[00:28:26.500 --> 00:28:28.460]   So batch normalization downscales
[00:28:28.460 --> 00:28:29.340]   the residual branch.
[00:28:30.060 --> 00:28:32.060]   I'm trying to see if I should highlight anything.
[00:28:32.060 --> 00:28:34.260]   I think this explains itself.
[00:28:34.260 --> 00:28:36.460]   And this eliminates a mean shift.
[00:28:36.460 --> 00:28:38.820]   What do they mean by this?
[00:28:38.820 --> 00:28:45.020]   So when you are using, let's say, ReLU from here,
[00:28:45.020 --> 00:28:52.100]   since ReLU has a graph like so, and ReLU basically
[00:28:52.100 --> 00:28:55.380]   means a graph like this, you can see
[00:28:55.380 --> 00:28:58.140]   that ReLU is 0.
[00:28:58.140 --> 00:29:04.420]   And ReLU basically means if x is greater than 0, x is equal to x.
[00:29:04.420 --> 00:29:07.860]   Otherwise, 0.
[00:29:07.860 --> 00:29:18.300]   Since this has this nature of being slightly positive
[00:29:18.300 --> 00:29:24.380]   or always positive, this causes a mean shift in the parameters
[00:29:24.380 --> 00:29:31.540]   and results in activations that have a non-zero mean.
[00:29:31.540 --> 00:29:35.860]   So batch normalization takes care of this.
[00:29:35.860 --> 00:29:38.900]   It is a regularizer.
[00:29:38.900 --> 00:29:41.740]   And as I explained earlier, inside of the layers,
[00:29:41.740 --> 00:29:44.660]   it causes a fix in the distribution.
[00:29:47.740 --> 00:29:55.340]   And thereby, it actually allows a large batch training.
[00:29:55.340 --> 00:29:59.420]   In theory, this happens because it smoothens the loss landscape.
[00:29:59.420 --> 00:30:02.460]   And this allows you to have a higher learning rate.
[00:30:02.460 --> 00:30:08.340]   Even because of all of these, they
[00:30:08.340 --> 00:30:10.980]   really want to remove batch normalization because
[00:30:10.980 --> 00:30:13.300]   of the first three critical issues
[00:30:13.300 --> 00:30:16.380]   that they had raised about it.
[00:30:16.380 --> 00:30:18.100]   Just remember that.
[00:30:18.100 --> 00:30:21.740]   So the reason they talk about what batch normalization does
[00:30:21.740 --> 00:30:23.820]   is because they want to account for those changes
[00:30:23.820 --> 00:30:25.080]   and actually take care of this.
[00:30:25.080 --> 00:30:36.620]   So the first thing that they do, they
[00:30:36.620 --> 00:30:42.660]   employ a residual block of the form this,
[00:30:42.660 --> 00:30:46.380]   which I had to read a few times to understand.
[00:30:46.380 --> 00:30:52.420]   So let's take a look at it again from the architectural image.
[00:30:52.420 --> 00:30:53.740]   Because I found that was easier.
[00:30:53.740 --> 00:30:56.740]   I should have just scrolled to the 22nd page
[00:30:56.740 --> 00:30:59.660]   or somewhere like that to understand it.
[00:30:59.660 --> 00:31:02.220]   And I wouldn't have been so confused.
[00:31:02.220 --> 00:31:02.700]   Here we go.
[00:31:02.700 --> 00:31:09.940]   So the flow of activation or data inside the model
[00:31:09.940 --> 00:31:12.140]   happens like so.
[00:31:12.140 --> 00:31:14.580]   Beta comes before the block.
[00:31:14.580 --> 00:31:17.780]   Alpha is the value that accounts for what's
[00:31:17.780 --> 00:31:19.300]   coming out of the block.
[00:31:19.300 --> 00:31:23.500]   And then this goes over to a skip connection,
[00:31:23.500 --> 00:31:25.100]   like so on the right.
[00:31:25.100 --> 00:31:29.460]   And on the left, all of this gets accounted for
[00:31:29.460 --> 00:31:31.660]   and moves on to the next layer.
[00:31:31.660 --> 00:31:38.980]   So now if I scroll back, here, F denotes the current layer.
[00:31:38.980 --> 00:31:43.420]   So whatever is happening inside of the convolutional block.
[00:31:43.420 --> 00:31:51.420]   I plus 1 is the next layer.
[00:31:51.420 --> 00:31:57.220]   Hi is actually the inputs to the ith layer.
[00:31:57.220 --> 00:32:00.540]   And alpha and beta are, let's say, just parameters
[00:32:00.540 --> 00:32:05.060]   or scalars that help you adjust this.
[00:32:08.420 --> 00:32:11.980]   And then they give a bit of more definition
[00:32:11.980 --> 00:32:16.300]   around how this works and how it helps with the mean shift.
[00:32:16.300 --> 00:32:18.860]   I'm going to skip that, because if you
[00:32:18.860 --> 00:32:20.940]   want to get an overview of the architecture,
[00:32:20.940 --> 00:32:22.100]   I think this is good enough.
[00:32:22.100 --> 00:32:27.940]   I'm just checking for questions.
[00:32:27.940 --> 00:32:31.380]   I don't see any, so I'll keep going.
[00:32:31.380 --> 00:32:36.580]   Next thing they talk about is adaptive gradient clipping
[00:32:36.580 --> 00:32:38.900]   for making the batch sizes larger.
[00:32:38.900 --> 00:32:46.100]   So I've already talked about gradient clipping.
[00:32:46.100 --> 00:32:52.180]   And let me explain this via the graph.
[00:32:52.180 --> 00:32:55.540]   So you really want larger batch sizes to train a model faster.
[00:32:55.540 --> 00:33:04.340]   And sorry, I'll be looking towards the right now,
[00:33:04.340 --> 00:33:06.460]   because I've lost power and my monitor does not
[00:33:06.460 --> 00:33:10.700]   have backup since I gave preference to my deep learning
[00:33:10.700 --> 00:33:12.180]   rig and not the monitor.
[00:33:12.180 --> 00:33:14.940]   I have my priorities, and I'm sorry they're like this.
[00:33:14.940 --> 00:33:18.340]   I'll be looking here and not here.
[00:33:18.340 --> 00:33:25.540]   So if you take a look at the graph on the right,
[00:33:25.540 --> 00:33:34.660]   you can see that with about a batch of 2048,
[00:33:34.660 --> 00:33:44.100]   the NF ResNet plateaus with performance.
[00:33:44.100 --> 00:33:45.420]   Am I looking at the right graph?
[00:33:45.420 --> 00:33:46.140]   Yes, I think so.
[00:33:46.140 --> 00:33:53.140]   But when they introduce a clipping threshold--
[00:33:53.140 --> 00:33:56.100]   yeah, I was worried I'm not explaining this correctly.
[00:33:56.100 --> 00:33:59.100]   Introducing the clipping threshold actually fixes this.
[00:33:59.100 --> 00:34:01.660]   Sorry about that.
[00:34:01.660 --> 00:34:03.980]   So when they do that, as you can see,
[00:34:03.980 --> 00:34:09.340]   it allows them to set the batch sizes to even larger values.
[00:34:09.340 --> 00:34:11.260]   So with that, for a ResNet-50, they
[00:34:11.260 --> 00:34:13.940]   are able to set larger batch sizes
[00:34:13.940 --> 00:34:18.700]   and keep the accuracy going further.
[00:34:18.700 --> 00:34:21.940]   So inside of adaptive gradient clipping,
[00:34:21.940 --> 00:34:24.580]   you can think of this as something
[00:34:24.580 --> 00:34:29.460]   that's being calculated unit-wise
[00:34:29.460 --> 00:34:31.260]   throughout the model.
[00:34:31.260 --> 00:34:38.140]   And it helps increasing the batch size.
[00:34:38.140 --> 00:34:44.540]   So usually in gradient clipping, as I was pointing out earlier--
[00:34:44.540 --> 00:34:47.980]   and now let me look at the camera--
[00:34:47.980 --> 00:34:50.060]   with gradient clipping, you usually
[00:34:50.060 --> 00:34:52.460]   clip based on a constant.
[00:34:52.460 --> 00:34:56.580]   Here, it's being changed throughout the training
[00:34:56.580 --> 00:34:58.460]   of the model.
[00:34:58.460 --> 00:35:02.940]   And that helps you use much larger batch sizes,
[00:35:02.940 --> 00:35:05.900]   like I'd shown in the graph.
[00:35:05.900 --> 00:35:08.580]   After that, they do a bunch of ablations
[00:35:08.580 --> 00:35:12.020]   around why AGC is helpful, how that works.
[00:35:12.020 --> 00:35:13.980]   I'm going to skip those, because I just
[00:35:13.980 --> 00:35:16.460]   wanted to give a brief overview of this.
[00:35:16.740 --> 00:35:20.220]   [AUDIO OUT]
[00:35:20.220 --> 00:35:33.300]   I'm just skimming to see if I want to highlight anything.
[00:35:33.300 --> 00:35:38.460]   One critical feedback that Janik had here
[00:35:38.460 --> 00:35:42.020]   is the authors are using dropout.
[00:35:42.020 --> 00:35:45.860]   And dropout is somewhat similar to batch norm,
[00:35:45.860 --> 00:35:56.220]   in the sense that it changes the training and test.
[00:35:56.220 --> 00:35:59.980]   It changes how training and testing is being performed,
[00:35:59.980 --> 00:36:02.740]   since you perform dropout while you're training your model.
[00:36:02.740 --> 00:36:08.540]   So Janik's feedback-- again, I'm just quoting his video--
[00:36:08.540 --> 00:36:12.620]   was the authors claim that all of these approaches we just
[00:36:12.620 --> 00:36:19.740]   looked at, AGC and the other things, help the model.
[00:36:19.740 --> 00:36:22.380]   But we can't really pinpoint it to one fact,
[00:36:22.380 --> 00:36:26.140]   because we are also tweaking other things here.
[00:36:26.140 --> 00:36:29.260]   So he wanted to introduce just one thing.
[00:36:29.260 --> 00:36:32.900]   One paper that actually does compare things individually
[00:36:32.900 --> 00:36:34.460]   is the Cornecs paper.
[00:36:34.460 --> 00:36:36.820]   I had actually covered that in our reading group,
[00:36:36.820 --> 00:36:38.740]   so you can check out the recording.
[00:36:38.740 --> 00:36:40.380]   Should be in the same playlist.
[00:36:40.380 --> 00:36:42.580]   But that's one of the papers where the author actually
[00:36:42.580 --> 00:36:44.420]   credit each individual contribution
[00:36:44.420 --> 00:36:46.740]   to different things that they add.
[00:36:46.740 --> 00:36:59.820]   Again, I'll point out two things.
[00:36:59.820 --> 00:37:01.940]   Before this, they were talking about how
[00:37:01.940 --> 00:37:06.580]   they made training perform better on TPUs
[00:37:06.580 --> 00:37:08.980]   by changing a few things.
[00:37:08.980 --> 00:37:10.820]   I think you can read through those.
[00:37:10.820 --> 00:37:12.900]   I think worth highlighting is they
[00:37:12.900 --> 00:37:15.460]   changed the depth scaling pattern for ResNets.
[00:37:15.460 --> 00:37:29.660]   And after that-- sorry, I'm still scrolling through.
[00:37:29.660 --> 00:37:36.860]   Give me one second.
[00:37:36.860 --> 00:37:37.980]   I have this pop-up coming.
[00:37:37.980 --> 00:37:48.860]   [AUDIO OUT]
[00:37:48.860 --> 00:37:49.580]   Sorry about that.
[00:37:49.580 --> 00:37:50.620]   Let me reshare my screen.
[00:37:50.620 --> 00:37:53.980]   [AUDIO OUT]
[00:37:54.700 --> 00:37:57.740]   [AUDIO OUT]
[00:37:57.740 --> 00:38:00.740]   [AUDIO OUT]
[00:38:00.740 --> 00:38:03.740]   [AUDIO OUT]
[00:38:03.740 --> 00:38:31.740]   [AUDIO OUT]
[00:38:31.740 --> 00:38:33.100]   I'm really sorry about that.
[00:38:33.100 --> 00:38:38.300]   I had to disconnect my monitor since that pop-up
[00:38:38.300 --> 00:38:39.020]   wasn't going away.
[00:38:39.020 --> 00:38:40.740]   Sorry about that.
[00:38:40.740 --> 00:38:41.860]   And I have to join again.
[00:38:41.860 --> 00:38:48.220]   [AUDIO OUT]
[00:38:48.220 --> 00:38:49.180]   Reshare my screen.
[00:38:49.180 --> 00:38:56.500]   [AUDIO OUT]
[00:38:56.500 --> 00:38:58.540]   And make sure it's--
[00:38:58.540 --> 00:38:59.900]   sorry about that again.
[00:38:59.900 --> 00:39:02.380]   I can now zoom out since I'm back on my monitor.
[00:39:02.380 --> 00:39:05.900]   Sorry about that, everyone.
[00:39:05.900 --> 00:39:08.620]   So they made two changes to the model backbone.
[00:39:08.620 --> 00:39:12.460]   They note that the default depth for ResNets
[00:39:12.460 --> 00:39:14.820]   involves non-uniformly increasing
[00:39:14.820 --> 00:39:15.780]   the number of layers.
[00:39:15.780 --> 00:39:23.860]   And they discover that this strategy is suboptimal.
[00:39:23.860 --> 00:39:33.020]   [AUDIO OUT]
[00:39:33.020 --> 00:39:35.900]   They explored several choices for the backbone
[00:39:35.900 --> 00:39:39.220]   for the smallest model.
[00:39:39.220 --> 00:39:41.140]   And they basically settled on this layer
[00:39:41.140 --> 00:39:44.220]   after experimenting a bit.
[00:39:44.220 --> 00:39:48.100]   They also reconsidered the default width pattern,
[00:39:48.100 --> 00:39:51.340]   where the first stage has 256 channels.
[00:39:51.340 --> 00:40:01.100]   They changed it to like so.
[00:40:01.100 --> 00:40:03.460]   And this is designed to increase capacity
[00:40:03.460 --> 00:40:09.900]   in the third stage of, again, the model blocks,
[00:40:09.900 --> 00:40:13.060]   while slightly reducing capacity in the fourth stage.
[00:40:17.660 --> 00:40:19.780]   Final change that they include is
[00:40:19.780 --> 00:40:23.820]   to establish a scaling strategy to produce model variants
[00:40:23.820 --> 00:40:25.180]   at different compute budgets.
[00:40:25.180 --> 00:40:34.100]   I think that's all I wanted to highlight here.
[00:40:34.100 --> 00:40:35.100]   Let's continue further.
[00:40:35.100 --> 00:40:39.940]   I won't go into much depth here.
[00:40:39.940 --> 00:40:48.940]   One thing I want to point out here is F0,
[00:40:48.940 --> 00:40:52.780]   compared against all of these, is still quite larger.
[00:40:52.780 --> 00:40:55.500]   And it still performs--
[00:40:55.500 --> 00:40:56.980]   or the training time is much faster.
[00:40:56.980 --> 00:41:09.340]   I think this should be shorter for this one, no?
[00:41:09.340 --> 00:41:16.540]   So it appears it's not shorter compared to B0.
[00:41:16.540 --> 00:41:17.580]   Interesting.
[00:41:17.580 --> 00:41:22.340]   I'm getting confused myself now.
[00:41:22.340 --> 00:41:24.980]   The accuracy is definitely higher, which is good.
[00:41:24.980 --> 00:41:28.180]   But the training time is slightly more.
[00:41:28.180 --> 00:41:37.580]   Yep, that is consistent throughout.
[00:41:37.580 --> 00:41:40.860]   I assume the throughput was higher in the first graph.
[00:41:40.860 --> 00:41:42.460]   Sorry, I'm getting confused myself.
[00:41:42.460 --> 00:41:46.300]   I should-- I'll come back and take a look at this
[00:41:46.300 --> 00:41:50.540]   and probably mention it in the comments.
[00:41:50.540 --> 00:41:52.060]   I don't want to spend more time here.
[00:41:52.060 --> 00:41:55.300]   But from this graph, here's what I understand.
[00:41:55.300 --> 00:41:58.220]   And if anyone in the live chat can correct me,
[00:41:58.220 --> 00:42:03.620]   NFNet is taking slightly longer than EfficientNets to train.
[00:42:03.620 --> 00:42:07.820]   Still more accurate, and it has more parameters.
[00:42:07.820 --> 00:42:13.260]   But I'm confused if I'm reading this in the correct way.
[00:42:13.260 --> 00:42:15.140]   I think I am, but I just want to double check
[00:42:15.140 --> 00:42:16.260]   if anyone wants to comment.
[00:42:16.260 --> 00:42:24.700]   Awesome.
[00:42:24.700 --> 00:42:25.700]   Let's continue further.
[00:42:29.980 --> 00:42:32.260]   Myself, I don't want to hear an echo.
[00:42:32.260 --> 00:42:39.100]   Sorry about that.
[00:42:39.100 --> 00:42:41.540]   I'm a bit all over the place today,
[00:42:41.540 --> 00:42:43.740]   since my monitor isn't behaving nicely.
[00:42:43.740 --> 00:42:49.580]   After that, they also share transfer top-1 accuracy.
[00:42:49.580 --> 00:42:53.260]   I'll skip this, because again, this is quite straightforward.
[00:42:53.260 --> 00:42:54.700]   You can read through all of these.
[00:42:57.300 --> 00:43:00.500]   To give you the summary, they evaluate transfer learning.
[00:43:00.500 --> 00:43:03.020]   And they don't give much detail here.
[00:43:03.020 --> 00:43:04.460]   But they just give you an overview
[00:43:04.460 --> 00:43:07.300]   of how this performs against a large data
[00:43:07.300 --> 00:43:10.180]   set of 300 million labeled images.
[00:43:10.180 --> 00:43:12.300]   What happens when they pre-train on this and transfer
[00:43:12.300 --> 00:43:13.100]   to ImageNet?
[00:43:13.100 --> 00:43:20.140]   So to conclude, this is the first time
[00:43:20.140 --> 00:43:24.020]   image recognition models, without normalization,
[00:43:24.020 --> 00:43:29.660]   can not only match accuracies of best batch normalized models,
[00:43:29.660 --> 00:43:33.300]   but also substantially exceed them
[00:43:33.300 --> 00:43:34.620]   while being faster to train.
[00:43:34.620 --> 00:43:39.860]   I'm a bit confused about this claim,
[00:43:39.860 --> 00:43:42.620]   since the training time here appears to be slightly larger.
[00:43:42.620 --> 00:43:44.580]   But I assume they converge faster, maybe.
[00:43:44.580 --> 00:43:47.820]   That's why that's the case.
[00:43:47.820 --> 00:43:50.580]   Sorry for adding to some confusion.
[00:43:51.580 --> 00:43:54.060]   OK.
[00:43:54.060 --> 00:43:56.540]   They introduce adaptive gradient clipping.
[00:43:56.540 --> 00:43:59.820]   I'll come back to my previous confusion and answer them.
[00:43:59.820 --> 00:44:02.900]   I don't want to waste everyone's time while I struggle with that.
[00:44:02.900 --> 00:44:05.860]   They also introduce adaptive gradient clipping,
[00:44:05.860 --> 00:44:07.940]   which helps stabilize large batch training.
[00:44:07.940 --> 00:44:14.380]   And they show that normalizer-free models
[00:44:14.380 --> 00:44:16.820]   are better suited to fine-tuning after pre-training
[00:44:16.820 --> 00:44:20.300]   on very large-scale data sets.
[00:44:20.300 --> 00:44:22.460]   After this, I want to highlight a few more things here.
[00:44:22.460 --> 00:44:32.660]   No, I don't want to highlight anything from here.
[00:44:32.660 --> 00:44:34.300]   One thing that I found quite interesting
[00:44:34.300 --> 00:44:46.740]   was NFNet 4+, F4+ and F4 take about 3.7k and 1.86k TPU V3
[00:44:46.740 --> 00:44:47.380]   core days.
[00:44:47.380 --> 00:44:55.460]   And this is, again, for the extra data
[00:44:55.460 --> 00:44:57.180]   for large-scale pre-training.
[00:44:57.180 --> 00:45:00.700]   But I found this quite fascinating
[00:45:00.700 --> 00:45:04.180]   that they're training on such a large data set
[00:45:04.180 --> 00:45:07.180]   with so much time being accounted for.
[00:45:07.180 --> 00:45:10.740]   And it was trained on 32 devices with a batch size
[00:45:10.740 --> 00:45:13.180]   of 32 per device.
[00:45:13.180 --> 00:45:14.620]   And then they go into depth of how
[00:45:14.620 --> 00:45:17.260]   they're syncing the gradients, all those things.
[00:45:17.260 --> 00:45:19.140]   And they also talk about how it takes
[00:45:19.140 --> 00:45:23.060]   to train these on a V100 GPU.
[00:45:23.060 --> 00:45:25.540]   As I understand, this paper came out in 2021.
[00:45:25.540 --> 00:45:28.700]   So A100 was the fastest at that time.
[00:45:28.700 --> 00:45:31.580]   But still, V100 is what's most commonly used.
[00:45:31.580 --> 00:45:35.420]   So they did benchmark on that, which is good to see.
[00:45:35.420 --> 00:45:37.620]   After this, they talk about image augmentation,
[00:45:37.620 --> 00:45:39.340]   which I'll again skip.
[00:45:39.340 --> 00:45:42.740]   They talk about SAM, which is Sharpness-Aware Minimization.
[00:45:42.740 --> 00:45:44.660]   Again, you can read through this if you want.
[00:45:44.660 --> 00:45:48.420]   I'm just giving you the meaty bits.
[00:45:48.420 --> 00:45:52.700]   One cool thing that I want to point out after this
[00:45:52.700 --> 00:45:55.740]   is not the model details.
[00:45:55.740 --> 00:46:02.700]   It is not the model overview, not the AGC downsides.
[00:46:02.700 --> 00:46:04.100]   It's the negative results.
[00:46:04.100 --> 00:46:05.480]   So this was a really cool section.
[00:46:05.480 --> 00:46:07.180]   Even Yannick pointed this out, that they
[00:46:07.180 --> 00:46:10.140]   talk about negative results.
[00:46:10.140 --> 00:46:12.620]   And they say about what all strategies
[00:46:12.620 --> 00:46:16.460]   that they developed with, did they experiment with.
[00:46:16.460 --> 00:46:18.300]   And they did try out the placement
[00:46:18.300 --> 00:46:20.500]   of squeeze and excitement layers and more.
[00:46:20.500 --> 00:46:23.500]   And they talk about what all was helpful,
[00:46:23.500 --> 00:46:26.980]   what all didn't work effectively here.
[00:46:26.980 --> 00:46:28.820]   Again, it's quite straightforward to read.
[00:46:28.820 --> 00:46:32.140]   But I just wanted to point it out, because most of us
[00:46:32.140 --> 00:46:35.900]   stop reading after the citations start to show up.
[00:46:35.900 --> 00:46:38.340]   Do scroll to the bottom and check out the negative results
[00:46:38.340 --> 00:46:41.020]   when you get to it.
[00:46:41.020 --> 00:46:41.500]   Awesome.
[00:46:41.500 --> 00:46:46.260]   So that was mostly what I wanted to cover in this overview.
[00:46:46.260 --> 00:46:50.180]   I'll leave the last few minutes for any questions.
[00:46:50.180 --> 00:46:54.100]   And in the meantime, I'll continue with my slide deck.
[00:46:54.100 --> 00:46:55.900]   If you have any questions, please, now
[00:46:55.900 --> 00:46:57.140]   is the best time to ask them.
[00:46:57.140 --> 00:47:01.820]   In the meantime, I'll give everyone
[00:47:01.820 --> 00:47:05.300]   some suggested homework to, again, try for a blog-a-thon.
[00:47:05.300 --> 00:47:08.020]   And as a reminder, please feel free to tag
[00:47:08.020 --> 00:47:15.660]   me or tag Weights and Biases whenever you share any of this
[00:47:15.660 --> 00:47:18.780]   or any blog, for that matter, that you write.
[00:47:18.780 --> 00:47:22.260]   I would rather encourage you to share it and participate
[00:47:22.260 --> 00:47:25.220]   in the blog-a-thon.
[00:47:25.220 --> 00:47:27.300]   Best case, we feature your blog.
[00:47:27.300 --> 00:47:29.700]   It reaches a few thousand people, a few hundred thousand
[00:47:29.700 --> 00:47:31.220]   people, potentially.
[00:47:31.220 --> 00:47:32.860]   And we send you some compute credits.
[00:47:32.860 --> 00:47:37.020]   So one of the homeworks I have is
[00:47:37.020 --> 00:47:39.100]   to try this model on audio data set.
[00:47:39.100 --> 00:47:44.300]   Again, I might cover this myself on Chaitanya Data Science
[00:47:44.300 --> 00:47:47.300]   sometime next week if I get the chance to livestream.
[00:47:47.300 --> 00:47:53.740]   Or I'll probably do a code walkthrough maybe.
[00:47:53.740 --> 00:47:57.180]   You could write about--
[00:47:57.180 --> 00:47:58.740]   I'll do it depending on my bandwidth,
[00:47:58.740 --> 00:48:00.420]   but I'm not sure right now.
[00:48:00.420 --> 00:48:02.700]   You could do a code walkthrough of either the JAXS
[00:48:02.700 --> 00:48:04.660]   implementation or PyTorch code.
[00:48:04.660 --> 00:48:06.460]   PyTorch code is available in TEM,
[00:48:06.460 --> 00:48:08.780]   which is PyTorch image models.
[00:48:08.780 --> 00:48:11.620]   If you're not familiar, it's an absolutely incredible framework
[00:48:11.620 --> 00:48:14.340]   that everyone pretty much uses in the PyTorch world.
[00:48:14.340 --> 00:48:17.420]   If you don't, just look up PyTorch image models.
[00:48:17.420 --> 00:48:20.460]   And this has almost the entirety of computer vision models
[00:48:20.460 --> 00:48:25.980]   that you probably ever need in PyTorch.
[00:48:25.980 --> 00:48:30.020]   I would encourage you to compare NFNet versus EfficientNet
[00:48:30.020 --> 00:48:34.700]   versus an FRSNet on a problem that you're working on.
[00:48:34.700 --> 00:48:40.500]   And then again, consider writing about it in the blog patron.
[00:48:40.500 --> 00:48:42.980]   With that, let me stop sharing my screen
[00:48:42.980 --> 00:48:45.820]   and check if there are any questions.
[00:48:45.820 --> 00:48:50.580]   I'll wait a few minutes--
[00:48:50.580 --> 00:48:52.740]   a few seconds, sorry--
[00:48:52.740 --> 00:48:53.660]   before wrapping up.
[00:48:53.660 --> 00:48:57.100]   [SIDE CONVERSATION]
[00:48:57.100 --> 00:49:15.860]   I believe there are no further questions.
[00:49:15.860 --> 00:49:19.380]   So that means we can wrap up here.
[00:49:19.380 --> 00:49:21.460]   Thanks, everyone, for joining.
[00:49:21.460 --> 00:49:24.300]   Again, consider writing about this.
[00:49:24.300 --> 00:49:26.740]   Consider joining the future reading groups.
[00:49:26.740 --> 00:49:29.380]   And if you have any questions, please
[00:49:29.380 --> 00:49:31.980]   feel free to leave a comment on the video
[00:49:31.980 --> 00:49:33.340]   that you are watching right now.
[00:49:33.340 --> 00:49:37.500]   I will get back and answer the question that I had about,
[00:49:37.500 --> 00:49:40.660]   why is the training time larger and it still trains faster
[00:49:40.660 --> 00:49:41.580]   than the other models?
[00:49:41.580 --> 00:49:45.820]   I assume it's because these converge faster.
[00:49:45.820 --> 00:49:49.420]   But I will double check and answer that.
[00:49:49.420 --> 00:49:50.920]   Thank you so much again for joining.
[00:49:50.920 --> 00:49:53.180]   These happen every month.
[00:49:53.180 --> 00:49:55.460]   And in between, you might also want
[00:49:55.460 --> 00:50:00.100]   to join any other community workshop that we are hosting.
[00:50:00.100 --> 00:50:03.500]   Right now, we are hosting the blogathon.
[00:50:03.500 --> 00:50:08.020]   We also have a few other ideas planned down the pipeline.
[00:50:08.020 --> 00:50:09.700]   So keep an eye out for that.
[00:50:09.700 --> 00:50:10.780]   Thanks again for joining.
[00:50:10.780 --> 00:50:16.300]   And I'll see you in one of our workshops or livestreams.
[00:50:16.300 --> 00:50:18.360]   you

