
[00:00:00.000 --> 00:00:02.880]   The following is a conversation with Risto Michelinan,
[00:00:02.880 --> 00:00:06.000]   a computer scientist at University of Texas at Austin
[00:00:06.000 --> 00:00:07.880]   and Associate Vice President
[00:00:07.880 --> 00:00:11.480]   of Evolutionary Artificial Intelligence at Cognizant.
[00:00:11.480 --> 00:00:14.440]   He specializes in evolutionary computation,
[00:00:14.440 --> 00:00:17.640]   but also many other topics in artificial intelligence,
[00:00:17.640 --> 00:00:19.920]   cognitive science, and neuroscience.
[00:00:19.920 --> 00:00:21.920]   Quick mention of our sponsors,
[00:00:21.920 --> 00:00:26.600]   Jordan Harbinger Show, Grammarly, Belcampo, and Indeed.
[00:00:26.600 --> 00:00:30.600]   Check them out in the description to support this podcast.
[00:00:30.600 --> 00:00:34.160]   As a side note, let me say that nature-inspired algorithms
[00:00:34.160 --> 00:00:36.840]   from ant colony optimization to genetic algorithms
[00:00:36.840 --> 00:00:39.600]   to cellular automata to neural networks
[00:00:39.600 --> 00:00:41.920]   have always captivated my imagination,
[00:00:41.920 --> 00:00:43.960]   not only for their surprising power
[00:00:43.960 --> 00:00:45.600]   in the face of long odds,
[00:00:45.600 --> 00:00:47.800]   but because they always opened up doors
[00:00:47.800 --> 00:00:50.720]   to new ways of thinking about computation.
[00:00:50.720 --> 00:00:54.200]   It does seem that in the long arc of computing history,
[00:00:54.200 --> 00:00:57.600]   running toward biology, not running away from it,
[00:00:57.600 --> 00:01:00.440]   is what leads to long-term progress.
[00:01:00.440 --> 00:01:03.240]   This is the Lex Friedman Podcast,
[00:01:03.240 --> 00:01:06.800]   and here is my conversation with Risto Michelinan.
[00:01:06.800 --> 00:01:10.240]   If we ran the earth experiment,
[00:01:10.240 --> 00:01:12.560]   this fun little experiment we're on,
[00:01:12.560 --> 00:01:15.280]   over and over and over and over a million times
[00:01:15.280 --> 00:01:19.240]   and watch the evolution of life as it pans out,
[00:01:19.240 --> 00:01:22.000]   how much variation in the outcomes of that evolution
[00:01:22.000 --> 00:01:23.240]   do you think we would see?
[00:01:23.240 --> 00:01:27.440]   Now, we should say that you are a computer scientist.
[00:01:27.440 --> 00:01:29.440]   - That's actually not such a bad question
[00:01:29.440 --> 00:01:30.440]   for computer scientists,
[00:01:30.440 --> 00:01:34.080]   because we are building simulations of these things,
[00:01:34.080 --> 00:01:36.240]   and we are simulating evolution,
[00:01:36.240 --> 00:01:38.520]   and that's a difficult question to answer in biology,
[00:01:38.520 --> 00:01:40.760]   but we can build a computational model
[00:01:40.760 --> 00:01:42.200]   and run it a million times
[00:01:42.200 --> 00:01:43.600]   and actually answer that question,
[00:01:43.600 --> 00:01:47.040]   how much variation do we see when we simulate it?
[00:01:47.040 --> 00:01:50.640]   And that's a little bit beyond what we can do today,
[00:01:50.640 --> 00:01:54.160]   but I think that we will see some regularities,
[00:01:54.160 --> 00:01:56.560]   and it took evolution also a really long time
[00:01:56.560 --> 00:01:57.760]   to get started,
[00:01:57.760 --> 00:02:02.240]   and then things accelerated really fast towards the end.
[00:02:02.240 --> 00:02:04.280]   But there are things that need to be discovered,
[00:02:04.280 --> 00:02:06.480]   and they probably will be over and over again,
[00:02:06.480 --> 00:02:11.200]   like manipulation of objects, opposable thumbs,
[00:02:11.200 --> 00:02:16.040]   and also some way to communicate,
[00:02:16.040 --> 00:02:18.280]   maybe orally, like, why will you have speech?
[00:02:18.280 --> 00:02:20.200]   It might be some other kind of sounds.
[00:02:20.800 --> 00:02:24.080]   And decision-making, but also vision.
[00:02:24.080 --> 00:02:26.200]   Eye has evolved many times,
[00:02:26.200 --> 00:02:28.160]   various vision systems have evolved.
[00:02:28.160 --> 00:02:30.760]   So we would see those kinds of solutions,
[00:02:30.760 --> 00:02:32.880]   I believe, emerge over and over again.
[00:02:32.880 --> 00:02:34.240]   They may look a little different,
[00:02:34.240 --> 00:02:36.280]   but they get the job done.
[00:02:36.280 --> 00:02:37.520]   The really interesting question is,
[00:02:37.520 --> 00:02:39.000]   would we have primates?
[00:02:39.000 --> 00:02:40.800]   Would we have humans,
[00:02:40.800 --> 00:02:43.280]   or something that resembles humans?
[00:02:43.280 --> 00:02:47.000]   And would that be an apex of evolution after a while?
[00:02:47.000 --> 00:02:48.440]   We don't know where we're going from here,
[00:02:48.440 --> 00:02:51.000]   but we certainly see a lot of tool use
[00:02:51.000 --> 00:02:54.000]   and building, constructing our environment.
[00:02:54.000 --> 00:02:56.320]   So I think that we will get that.
[00:02:56.320 --> 00:02:59.680]   We get some evolution producing some agents
[00:02:59.680 --> 00:03:02.480]   that can do that, manipulate the environment and build.
[00:03:02.480 --> 00:03:04.080]   - What do you think is special about humans?
[00:03:04.080 --> 00:03:06.040]   Like, if you were running the simulation
[00:03:06.040 --> 00:03:08.680]   and you observe humans emerge,
[00:03:08.680 --> 00:03:10.440]   like these tool makers, they start a fire,
[00:03:10.440 --> 00:03:11.720]   and all that stuff, start running around,
[00:03:11.720 --> 00:03:13.560]   building buildings, and then running for president,
[00:03:13.560 --> 00:03:15.560]   and all those kinds of things.
[00:03:15.560 --> 00:03:19.160]   What would be, how would you detect that?
[00:03:19.160 --> 00:03:20.360]   'Cause you're like really busy
[00:03:20.360 --> 00:03:23.160]   as the creator of this evolutionary system,
[00:03:23.160 --> 00:03:25.680]   so you don't have much time to observe,
[00:03:25.680 --> 00:03:28.920]   like detect if any cool stuff came up, right?
[00:03:28.920 --> 00:03:31.240]   How would you detect humans?
[00:03:31.240 --> 00:03:33.280]   - Well, you are running the simulation,
[00:03:33.280 --> 00:03:37.480]   so you also put in visualization
[00:03:37.480 --> 00:03:39.680]   and measurement techniques there.
[00:03:39.680 --> 00:03:43.240]   So if you are looking for certain things,
[00:03:43.240 --> 00:03:46.400]   like communication, you'll have detectors
[00:03:46.400 --> 00:03:48.000]   to find out whether that's happening,
[00:03:48.000 --> 00:03:50.120]   even if it's a lot simulation.
[00:03:50.120 --> 00:03:53.520]   And I think that that's what we would do.
[00:03:53.520 --> 00:03:56.360]   We know roughly what we want,
[00:03:56.360 --> 00:04:01.200]   intelligent agents that communicate, cooperate, manipulate,
[00:04:01.200 --> 00:04:03.160]   and we would build detections
[00:04:03.160 --> 00:04:05.560]   and visualizations of those processes.
[00:04:05.560 --> 00:04:08.040]   Yeah, and there's a lot of,
[00:04:08.040 --> 00:04:09.520]   we'd have to run it many times,
[00:04:09.520 --> 00:04:11.920]   and we have plenty of time to figure out
[00:04:11.920 --> 00:04:13.520]   how we detect the interesting things.
[00:04:13.520 --> 00:04:16.680]   But also, I think we do have to run it many times
[00:04:16.680 --> 00:04:21.160]   because we don't quite know what shape those will take,
[00:04:21.160 --> 00:04:23.880]   and our detectors may not be perfect for them
[00:04:23.880 --> 00:04:24.720]   to begin with.
[00:04:24.720 --> 00:04:25.760]   - Well, that seems really difficult
[00:04:25.760 --> 00:04:28.680]   to build a detector of intelligent
[00:04:28.680 --> 00:04:32.800]   or intelligent communication.
[00:04:32.800 --> 00:04:37.160]   If we take an alien perspective, observing Earth,
[00:04:37.160 --> 00:04:40.280]   are you sure that they would be able to detect humans
[00:04:40.280 --> 00:04:41.360]   as the special thing?
[00:04:41.360 --> 00:04:43.800]   Wouldn't they be already curious about other things?
[00:04:43.800 --> 00:04:47.080]   There's way more insects by body mass, I think,
[00:04:47.080 --> 00:04:50.880]   than humans by far, and colonies.
[00:04:50.880 --> 00:04:53.880]   Obviously, dolphins is the most intelligent creature
[00:04:53.880 --> 00:04:55.240]   on Earth, we all know this.
[00:04:55.240 --> 00:04:58.400]   So, it could be the dolphins that they detect.
[00:04:58.400 --> 00:05:00.840]   It could be the rockets that we seem to be launching.
[00:05:00.840 --> 00:05:03.800]   That could be the intelligent creature they detect.
[00:05:03.800 --> 00:05:06.680]   It could be some other trees.
[00:05:06.680 --> 00:05:07.960]   Trees have been here a long time.
[00:05:07.960 --> 00:05:10.560]   I just learned that sharks have been here
[00:05:10.560 --> 00:05:13.240]   400 million years, and that's longer
[00:05:13.240 --> 00:05:15.000]   than trees have been here.
[00:05:15.000 --> 00:05:17.400]   So, maybe it's the sharks, they go by age.
[00:05:17.400 --> 00:05:18.960]   Like, there's a persistent thing.
[00:05:18.960 --> 00:05:20.800]   Like, if you survive long enough,
[00:05:20.800 --> 00:05:22.320]   especially through the mass extinctions,
[00:05:22.320 --> 00:05:25.360]   that could be the thing your detector is detecting.
[00:05:25.360 --> 00:05:27.840]   Humans have been here a very short time,
[00:05:27.840 --> 00:05:30.640]   and we're just creating a lot of pollution,
[00:05:30.640 --> 00:05:31.920]   but so is the other creatures.
[00:05:31.920 --> 00:05:33.320]   So, I don't know.
[00:05:33.320 --> 00:05:35.680]   Do you think you would be able to detect humans?
[00:05:35.680 --> 00:05:37.680]   Like, how would you go about detecting,
[00:05:37.680 --> 00:05:39.120]   in the computational sense,
[00:05:39.120 --> 00:05:40.920]   maybe we can leave humans behind,
[00:05:40.920 --> 00:05:44.600]   in the computational sense, detect interesting things?
[00:05:44.600 --> 00:05:48.760]   Do you basically have to have a strict objective function
[00:05:48.760 --> 00:05:51.800]   by which you measure the performance of a system,
[00:05:51.800 --> 00:05:55.400]   or can you find curiosities and interesting things?
[00:05:55.400 --> 00:05:59.480]   - Yeah, well, I think the first measurement
[00:05:59.480 --> 00:06:02.240]   would be to detect how much of an effect
[00:06:02.240 --> 00:06:03.600]   you can have in your environment.
[00:06:03.600 --> 00:06:06.880]   So, if you look around, we have cities,
[00:06:06.880 --> 00:06:08.800]   and that is constructed environments,
[00:06:08.800 --> 00:06:11.960]   and that's where a lot of people live, most people live.
[00:06:11.960 --> 00:06:15.120]   So, that would be a good sign of intelligence,
[00:06:15.120 --> 00:06:17.920]   that you don't just live in an environment,
[00:06:17.920 --> 00:06:20.080]   but you construct it to your liking.
[00:06:20.080 --> 00:06:21.880]   And that's something pretty unique.
[00:06:21.880 --> 00:06:24.240]   I mean, certainly birds build nests and all,
[00:06:24.240 --> 00:06:25.520]   but they don't build quite cities.
[00:06:25.520 --> 00:06:29.080]   Termites build mounds and hives and things like that,
[00:06:29.080 --> 00:06:32.120]   but the complexity of the human construction cities,
[00:06:32.120 --> 00:06:34.960]   I think, would stand out, even to an external observer.
[00:06:34.960 --> 00:06:36.920]   - Of course, that's what a human would say.
[00:06:36.920 --> 00:06:38.280]   (Luke laughs)
[00:06:38.280 --> 00:06:41.000]   - Yeah, and you can certainly say that sharks
[00:06:41.000 --> 00:06:43.280]   are really smart because they've been around so long,
[00:06:43.280 --> 00:06:45.080]   and they haven't destroyed their environment,
[00:06:45.080 --> 00:06:46.640]   which humans are about to do,
[00:06:46.640 --> 00:06:48.920]   which is not a very smart thing.
[00:06:48.920 --> 00:06:52.080]   But we'll get over it, I believe.
[00:06:52.080 --> 00:06:55.320]   And we can get over it by doing some construction
[00:06:55.320 --> 00:06:58.160]   that actually is benign, and maybe even enhances
[00:06:58.160 --> 00:07:02.520]   resilience of nature.
[00:07:02.520 --> 00:07:04.480]   - So, you mentioned the simulation
[00:07:04.480 --> 00:07:07.160]   that we run over and over might start,
[00:07:07.160 --> 00:07:09.000]   it's a slow start.
[00:07:09.000 --> 00:07:12.640]   So, do you think how unlikely, first of all,
[00:07:12.640 --> 00:07:14.240]   I don't know if you think about this kind of stuff,
[00:07:14.240 --> 00:07:18.240]   but how unlikely is step number zero,
[00:07:18.240 --> 00:07:23.040]   which is the springing up, like the origin of life on Earth?
[00:07:23.040 --> 00:07:28.040]   And second, how unlikely is anything interesting
[00:07:28.040 --> 00:07:30.560]   happening beyond that?
[00:07:30.560 --> 00:07:35.560]   Sort of like the start that creates all the rich complexity
[00:07:35.560 --> 00:07:36.760]   that we see on Earth today?
[00:07:36.760 --> 00:07:38.640]   - Yeah, there are people who are working
[00:07:38.640 --> 00:07:42.320]   on exactly that problem from primordial soup,
[00:07:42.320 --> 00:07:45.880]   how do you actually get self-replicating molecules?
[00:07:45.880 --> 00:07:48.800]   And they are very close.
[00:07:48.800 --> 00:07:51.960]   With a little bit of help, you can make that happen.
[00:07:51.960 --> 00:07:55.740]   So, of course, we know what we want,
[00:07:55.740 --> 00:07:57.200]   so they can set up the conditions
[00:07:57.200 --> 00:07:59.840]   and try out conditions that are conducive to that.
[00:07:59.840 --> 00:08:04.160]   For evolution to discover that, it took a long time.
[00:08:04.160 --> 00:08:07.720]   For us to recreate it probably won't take that long.
[00:08:07.720 --> 00:08:09.880]   And the next steps from there,
[00:08:09.880 --> 00:08:12.920]   I think also with some hand-holding,
[00:08:12.920 --> 00:08:14.520]   I think we can make that happen.
[00:08:14.520 --> 00:08:18.600]   But with evolution, what was really fascinating
[00:08:18.600 --> 00:08:22.680]   was eventually the runaway evolution of the brain
[00:08:22.680 --> 00:08:24.480]   that created humans and created,
[00:08:24.480 --> 00:08:27.280]   well, also other higher animals.
[00:08:27.280 --> 00:08:29.760]   That was something that happened really fast.
[00:08:29.760 --> 00:08:32.440]   And that's a big question.
[00:08:32.440 --> 00:08:33.760]   Is that something replicable?
[00:08:33.760 --> 00:08:35.800]   Is that something that can happen?
[00:08:35.800 --> 00:08:39.200]   And if it happens, does it go in the same direction?
[00:08:39.200 --> 00:08:40.760]   That is a big question to ask.
[00:08:40.760 --> 00:08:43.000]   Even in computational terms,
[00:08:43.000 --> 00:08:47.360]   I think that it's relatively possible to come up here,
[00:08:47.360 --> 00:08:49.840]   create an experiment where we look at the primordial soup
[00:08:49.840 --> 00:08:51.280]   and the first couple of steps
[00:08:51.280 --> 00:08:53.480]   of multicellular organisms even.
[00:08:53.480 --> 00:08:55.760]   But to get something as complex as the brain,
[00:08:55.760 --> 00:08:59.680]   we don't quite know the conditions for that
[00:08:59.680 --> 00:09:01.420]   and how to even get started
[00:09:01.420 --> 00:09:02.480]   and whether we can get this kind of
[00:09:02.480 --> 00:09:03.960]   runaway evolution happening.
[00:09:03.960 --> 00:09:09.120]   - From a detector perspective,
[00:09:09.120 --> 00:09:10.800]   if we're observing this evolution,
[00:09:10.800 --> 00:09:12.360]   what do you think is the brain?
[00:09:12.360 --> 00:09:15.960]   What do you think is the, let's say, what is intelligence?
[00:09:15.960 --> 00:09:18.360]   So in terms of the thing that makes humans special,
[00:09:18.360 --> 00:09:20.080]   we seem to be able to reason,
[00:09:20.080 --> 00:09:23.520]   we seem to be able to communicate,
[00:09:23.520 --> 00:09:26.000]   but the core of that is this something
[00:09:26.000 --> 00:09:29.600]   in the broad category we might call intelligence.
[00:09:29.600 --> 00:09:33.000]   So if you put your computer scientist hat on,
[00:09:33.000 --> 00:09:37.560]   is there favorite ways you like to think about
[00:09:37.560 --> 00:09:39.780]   that question of what is intelligence?
[00:09:39.780 --> 00:09:46.320]   - Well, my goal is to create agents that are intelligent.
[00:09:46.320 --> 00:09:49.520]   - Not to define what.
[00:09:49.520 --> 00:09:50.600]   (laughing)
[00:09:50.600 --> 00:09:52.720]   - And that is a way of defining it.
[00:09:52.720 --> 00:09:57.720]   And that means that it's some kind of an object
[00:09:57.720 --> 00:10:02.720]   or a program that has limited sensory
[00:10:02.720 --> 00:10:08.000]   and effective capabilities interacting with the world,
[00:10:08.000 --> 00:10:11.720]   and then also a mechanism for making decisions.
[00:10:11.720 --> 00:10:15.860]   So with limited abilities like that, can it survive?
[00:10:15.860 --> 00:10:18.800]   Survival is the simplest goal,
[00:10:18.800 --> 00:10:20.500]   but you could also give it other goals.
[00:10:20.500 --> 00:10:21.400]   Can it multiply?
[00:10:21.400 --> 00:10:24.440]   Can it solve problems that you give it?
[00:10:24.440 --> 00:10:27.220]   And that is quite a bit less than human intelligence.
[00:10:27.220 --> 00:10:29.720]   There are, animals would be intelligent, of course,
[00:10:29.720 --> 00:10:31.080]   with that definition.
[00:10:31.080 --> 00:10:34.360]   And you might have even some other forms of life.
[00:10:34.360 --> 00:10:37.840]   Even, so what, so intelligence in that sense
[00:10:37.840 --> 00:10:42.840]   is a survival skill given resources that you have
[00:10:42.840 --> 00:10:46.080]   and using your resources so that you will stay around.
[00:10:46.080 --> 00:10:52.880]   - Do you think death, mortality is fundamental to an agent?
[00:10:52.880 --> 00:10:55.040]   So like there's, I don't know if you're familiar,
[00:10:55.040 --> 00:10:56.880]   there's a philosopher named Ernest Becker
[00:10:56.880 --> 00:11:01.220]   who wrote "The Denial of Death" and his whole idea.
[00:11:01.220 --> 00:11:04.020]   And there's folks, psychologists, cognitive scientists
[00:11:04.020 --> 00:11:06.600]   that work on terror management theory.
[00:11:06.600 --> 00:11:10.020]   And they think that one of the special things about humans
[00:11:10.020 --> 00:11:13.940]   is that we're able to sort of foresee our death, right?
[00:11:13.940 --> 00:11:16.620]   We can realize not just as animals do,
[00:11:16.620 --> 00:11:19.420]   sort of constantly fear in an instinctual sense,
[00:11:19.420 --> 00:11:21.600]   respond to all the dangers that are out there,
[00:11:21.600 --> 00:11:25.200]   but like understand that this ride ends eventually.
[00:11:25.200 --> 00:11:28.520]   And that in itself is the most,
[00:11:28.520 --> 00:11:31.160]   is the force behind all of the creative efforts
[00:11:31.160 --> 00:11:32.600]   of human nature.
[00:11:32.600 --> 00:11:33.680]   That's the philosophy.
[00:11:33.680 --> 00:11:35.280]   - I think that makes sense, a lot of sense.
[00:11:35.280 --> 00:11:38.680]   I mean, animals probably don't think of death the same way,
[00:11:38.680 --> 00:11:40.680]   but humans know that your time is limited
[00:11:40.680 --> 00:11:42.080]   and you wanna make it count.
[00:11:42.080 --> 00:11:45.020]   And you can make it count in many different ways,
[00:11:45.020 --> 00:11:47.780]   but I think that has a lot to do with creativity
[00:11:47.780 --> 00:11:50.100]   and the need for humans to do something
[00:11:50.100 --> 00:11:51.760]   beyond just surviving.
[00:11:52.680 --> 00:11:55.560]   And now going from that simple definition
[00:11:55.560 --> 00:11:57.400]   to something that's the next level,
[00:11:57.400 --> 00:12:00.200]   I think that that could be the second decision,
[00:12:00.200 --> 00:12:01.520]   the second level of definition
[00:12:01.520 --> 00:12:04.320]   that intelligence means something
[00:12:04.320 --> 00:12:06.240]   and you do something that stays behind you
[00:12:06.240 --> 00:12:10.080]   that's more than your existence.
[00:12:10.080 --> 00:12:13.280]   Something you create something that is useful for others,
[00:12:13.280 --> 00:12:16.200]   is useful in the future, not just for yourself.
[00:12:16.200 --> 00:12:18.880]   And I think that's a nice definition of intelligence
[00:12:18.880 --> 00:12:20.840]   in a next level.
[00:12:20.840 --> 00:12:23.480]   And it's also nice 'cause it doesn't require
[00:12:23.480 --> 00:12:25.240]   that they are humans or biological.
[00:12:25.240 --> 00:12:28.240]   They could be artificial agents that are intelligence.
[00:12:28.240 --> 00:12:30.360]   They could achieve those kind of goals.
[00:12:30.360 --> 00:12:35.360]   - So particular agent, the ripple effects of their existence
[00:12:35.360 --> 00:12:38.560]   on the entirety of the system is significant.
[00:12:38.560 --> 00:12:41.800]   So like they leave a trace where there's like a,
[00:12:41.800 --> 00:12:43.920]   yeah, like ripple effects.
[00:12:43.920 --> 00:12:46.080]   But see, then you go back to the butterfly
[00:12:46.080 --> 00:12:47.400]   with the flap of a wing,
[00:12:47.400 --> 00:12:50.840]   and then you can trace a lot of like nuclear wars
[00:12:50.840 --> 00:12:52.720]   and all the conflicts of human history
[00:12:52.720 --> 00:12:54.560]   somehow connected to that one butterfly
[00:12:54.560 --> 00:12:56.280]   that created all the chaos.
[00:12:56.280 --> 00:12:57.960]   So maybe that's not,
[00:12:57.960 --> 00:13:02.160]   maybe that's a very poetic way to think.
[00:13:02.160 --> 00:13:04.520]   That's something we humans in a human centric way
[00:13:04.520 --> 00:13:09.080]   wanna hope we have this impact.
[00:13:09.080 --> 00:13:12.200]   Like that is the secondary effect of our intelligence.
[00:13:12.200 --> 00:13:14.600]   We've had that long lasting impact on the world,
[00:13:14.600 --> 00:13:19.600]   but maybe the entirety of physics in the universe
[00:13:19.600 --> 00:13:22.720]   has a very long lasting effect.
[00:13:22.720 --> 00:13:25.640]   - Sure, but you can also think of it,
[00:13:25.640 --> 00:13:28.200]   what if like the wonderful life,
[00:13:28.200 --> 00:13:30.000]   what if you're not here?
[00:13:30.000 --> 00:13:31.640]   Will somebody else do this?
[00:13:31.640 --> 00:13:34.600]   Is it something that you actually contributed
[00:13:34.600 --> 00:13:37.480]   because you had something unique to contribute?
[00:13:37.480 --> 00:13:39.480]   That's a pretty high bar though.
[00:13:39.480 --> 00:13:40.680]   - Uniqueness.
[00:13:40.720 --> 00:13:44.600]   Yeah, so you have to be Mozart or something
[00:13:44.600 --> 00:13:46.360]   to actually reach that level.
[00:13:46.360 --> 00:13:48.080]   Nobody would have developed that,
[00:13:48.080 --> 00:13:50.840]   but other people might have solved this equation
[00:13:50.840 --> 00:13:53.240]   if you didn't do it.
[00:13:53.240 --> 00:13:56.240]   But also within limited scope.
[00:13:56.240 --> 00:14:00.440]   I mean, during your lifetime or next year,
[00:14:00.440 --> 00:14:02.840]   you could contribute something that unique
[00:14:02.840 --> 00:14:04.560]   that other people did not see.
[00:14:04.560 --> 00:14:09.560]   And then that could change the way things move forward
[00:14:09.800 --> 00:14:11.400]   for a while.
[00:14:11.400 --> 00:14:14.080]   So I don't think we have to be Mozart
[00:14:14.080 --> 00:14:15.400]   to be called intelligence,
[00:14:15.400 --> 00:14:18.160]   but we have this local effect that is changing.
[00:14:18.160 --> 00:14:20.200]   If you weren't there, that would not have happened.
[00:14:20.200 --> 00:14:21.560]   And it's a positive effect, of course,
[00:14:21.560 --> 00:14:23.280]   you want it to be a positive effect.
[00:14:23.280 --> 00:14:26.080]   - Do you think it's possible to engineer in
[00:14:26.080 --> 00:14:29.800]   to computational agents, a fear of mortality?
[00:14:29.800 --> 00:14:35.640]   Like, does that make any sense?
[00:14:35.640 --> 00:14:38.280]   So there's a very trivial thing where it's like,
[00:14:38.280 --> 00:14:39.720]   you could just code in a parameter,
[00:14:39.720 --> 00:14:41.360]   which is how long the life ends,
[00:14:41.360 --> 00:14:45.480]   but more of a fear of mortality,
[00:14:45.480 --> 00:14:48.960]   like awareness of the way that things end
[00:14:48.960 --> 00:14:53.960]   and somehow encoding a complex representation of that fear,
[00:14:53.960 --> 00:14:57.000]   which is like, maybe as it gets closer,
[00:14:57.000 --> 00:14:58.880]   you become more terrified.
[00:14:58.880 --> 00:15:01.640]   I mean, there seems to be something really profound
[00:15:01.640 --> 00:15:04.880]   about this fear that's not currently encodable
[00:15:04.880 --> 00:15:07.400]   in a trivial way into our programs.
[00:15:08.240 --> 00:15:11.880]   - Well, I think you're referring to the emotion of fear,
[00:15:11.880 --> 00:15:13.600]   something, 'cause we have cognitively,
[00:15:13.600 --> 00:15:16.360]   we know that we have limited lifespan
[00:15:16.360 --> 00:15:18.080]   and most of us cope with it by just,
[00:15:18.080 --> 00:15:19.680]   hey, that's what the world is like,
[00:15:19.680 --> 00:15:20.600]   and I make the most of it.
[00:15:20.600 --> 00:15:25.600]   But sometimes you can have like a fear that's not healthy,
[00:15:25.600 --> 00:15:29.320]   that paralyzes you, that you can't do anything.
[00:15:29.320 --> 00:15:32.000]   And somewhere in between there,
[00:15:32.000 --> 00:15:36.160]   not caring at all and getting paralyzed because of fear
[00:15:36.160 --> 00:15:37.280]   is a normal response,
[00:15:37.280 --> 00:15:41.440]   which is a little bit more than just logic and it's emotion.
[00:15:41.440 --> 00:15:43.680]   So now the question is what good are emotions?
[00:15:43.680 --> 00:15:46.160]   I mean, they are quite complex
[00:15:46.160 --> 00:15:48.480]   and there are multiple dimensions of emotions
[00:15:48.480 --> 00:15:52.560]   and they probably do serve as a survival function,
[00:15:52.560 --> 00:15:55.840]   heightened focus, for instance.
[00:15:55.840 --> 00:15:59.680]   And fear of death might be a really good emotion
[00:15:59.680 --> 00:16:02.640]   when you are in danger, that you recognize it.
[00:16:02.640 --> 00:16:06.360]   Even if it's not logically necessarily easy to derive
[00:16:06.360 --> 00:16:10.400]   and you don't have time for that logical deduction,
[00:16:10.400 --> 00:16:12.720]   you may be able to recognize the situation is dangerous
[00:16:12.720 --> 00:16:16.240]   and this fear kicks in and you all of a sudden perceive
[00:16:16.240 --> 00:16:18.440]   the facts that are important for that.
[00:16:18.440 --> 00:16:20.640]   And I think that's generally is the role of emotions.
[00:16:20.640 --> 00:16:24.520]   It allows you to focus what's relevant for your situation.
[00:16:24.520 --> 00:16:27.800]   And maybe if fear of death plays the same kind of role,
[00:16:27.800 --> 00:16:30.600]   but if it consumes you and it's something that you think
[00:16:30.600 --> 00:16:32.080]   in normal life when you don't have to,
[00:16:32.080 --> 00:16:34.440]   then it's not healthy and then it's not productive.
[00:16:34.440 --> 00:16:36.640]   - Yeah, but it's fascinating to think
[00:16:36.640 --> 00:16:41.640]   how to incorporate emotion into a computational agent.
[00:16:41.640 --> 00:16:44.280]   It almost seems like a silly statement to make,
[00:16:44.280 --> 00:16:47.720]   but it perhaps seems silly
[00:16:47.720 --> 00:16:49.800]   because we have such a poor understanding
[00:16:49.800 --> 00:16:53.560]   of the mechanism of emotion, of fear, of...
[00:16:53.560 --> 00:16:58.520]   I think at the core of it is another word
[00:16:58.520 --> 00:17:01.160]   that we know nothing about, but say a lot,
[00:17:01.160 --> 00:17:02.440]   which is consciousness.
[00:17:02.440 --> 00:17:08.600]   Do you ever in your work or like maybe on a coffee break,
[00:17:08.600 --> 00:17:11.640]   think about what the heck is this thing consciousness
[00:17:11.640 --> 00:17:15.000]   and is it at all useful in our thinking about AI systems?
[00:17:15.000 --> 00:17:17.400]   - Yes, it is an important question.
[00:17:17.400 --> 00:17:23.160]   You can build representations and functions,
[00:17:23.160 --> 00:17:26.760]   I think into these agents that act like emotions
[00:17:26.760 --> 00:17:28.680]   and consciousness perhaps.
[00:17:28.680 --> 00:17:31.960]   So I mentioned emotions being something
[00:17:31.960 --> 00:17:34.240]   that allow you to focus and pay attention,
[00:17:34.240 --> 00:17:35.400]   filter out what's important.
[00:17:35.400 --> 00:17:38.320]   Yeah, you can have that kind of a filter mechanism
[00:17:38.320 --> 00:17:40.360]   and it puts you in a different state.
[00:17:40.360 --> 00:17:42.120]   Your computation is in a different state.
[00:17:42.120 --> 00:17:43.600]   Certain things don't really get through
[00:17:43.600 --> 00:17:45.120]   and others are heightened.
[00:17:45.120 --> 00:17:48.480]   Now you label that box emotion.
[00:17:48.480 --> 00:17:49.880]   I don't know if that means it's an emotion,
[00:17:49.880 --> 00:17:54.280]   but it acts very much like we understand what emotions are.
[00:17:54.280 --> 00:17:56.920]   And we actually did some work like that,
[00:17:56.920 --> 00:18:01.920]   modeling hyenas who were trying to steal a kill from lions,
[00:18:01.920 --> 00:18:03.520]   which happens in Africa.
[00:18:03.520 --> 00:18:05.960]   I mean, hyenas are quite intelligent,
[00:18:05.960 --> 00:18:08.280]   but not really intelligent.
[00:18:08.280 --> 00:18:13.160]   And they have this behavior that's more complex
[00:18:13.160 --> 00:18:14.040]   than anything else they do.
[00:18:14.040 --> 00:18:17.680]   They can band together if there's about 30 of them or so,
[00:18:17.680 --> 00:18:20.040]   they can coordinate their effort
[00:18:20.040 --> 00:18:22.520]   so that they push the lions away from a kill.
[00:18:22.520 --> 00:18:24.040]   Even though the lions are so strong
[00:18:24.040 --> 00:18:28.440]   that they could kill a hyena by striking with a paw.
[00:18:28.440 --> 00:18:31.640]   But when they work together and precisely time this attack,
[00:18:31.640 --> 00:18:34.080]   the lions will leave and they get the kill.
[00:18:34.080 --> 00:18:39.080]   And probably there are some states like emotions
[00:18:39.080 --> 00:18:40.840]   that the hyenas go through.
[00:18:40.840 --> 00:18:43.640]   The first day they call for reinforcements.
[00:18:43.640 --> 00:18:45.640]   They really want that kill, but there's not enough of them.
[00:18:45.640 --> 00:18:48.480]   So they vocalize and there's more people,
[00:18:48.480 --> 00:18:50.680]   more hyenas that come around.
[00:18:50.680 --> 00:18:52.280]   And then they have two emotions.
[00:18:52.280 --> 00:18:54.360]   They're very afraid of the lion.
[00:18:54.360 --> 00:18:55.600]   So they want to stay away,
[00:18:55.600 --> 00:18:59.840]   but they also have a strong affiliation between each other.
[00:18:59.840 --> 00:19:02.160]   And then this is the balance of the two emotions.
[00:19:02.160 --> 00:19:04.880]   And also, yes, they also want the kill.
[00:19:04.880 --> 00:19:07.360]   So it's both repelled and attractive.
[00:19:07.360 --> 00:19:10.600]   But then this affiliation eventually is so strong
[00:19:10.600 --> 00:19:12.280]   that when they move, they move together,
[00:19:12.280 --> 00:19:15.400]   they act as a unit and they can perform that function.
[00:19:15.400 --> 00:19:18.440]   So there's an interesting behavior
[00:19:18.440 --> 00:19:21.400]   that seems to depend on these emotions strongly
[00:19:21.400 --> 00:19:24.280]   that makes it possible, coordinate actions.
[00:19:24.280 --> 00:19:28.880]   - And I think a critical aspect of that,
[00:19:28.880 --> 00:19:30.600]   the way you're describing is emotion there
[00:19:30.600 --> 00:19:34.320]   is a mechanism of social communication,
[00:19:34.320 --> 00:19:35.960]   of a social interaction.
[00:19:35.960 --> 00:19:40.520]   Maybe humans won't even be that intelligent
[00:19:40.520 --> 00:19:42.440]   or most things we think of as intelligent
[00:19:42.440 --> 00:19:43.500]   wouldn't be that intelligent
[00:19:43.500 --> 00:19:47.040]   without the social component of interaction.
[00:19:47.040 --> 00:19:50.240]   Maybe much of our intelligence is essentially
[00:19:50.240 --> 00:19:52.880]   in an outgrowth of social interaction.
[00:19:52.880 --> 00:19:55.720]   And maybe for the creation of intelligent agents,
[00:19:55.720 --> 00:19:59.000]   we have to be creating fundamentally social systems.
[00:19:59.000 --> 00:20:01.160]   - Yes, I strongly believe that's true.
[00:20:01.160 --> 00:20:05.520]   And yes, the communication is multifaceted.
[00:20:05.520 --> 00:20:08.120]   I mean, they vocalize and call for friends,
[00:20:08.120 --> 00:20:11.200]   but they also rub against each other and they push
[00:20:11.200 --> 00:20:14.280]   and they do all kinds of gestures and so on.
[00:20:14.280 --> 00:20:15.760]   So they don't act alone.
[00:20:15.760 --> 00:20:18.400]   And I don't think people act alone very much either,
[00:20:18.400 --> 00:20:21.120]   at least normal most of the time.
[00:20:21.120 --> 00:20:25.040]   And social systems are so strong for humans
[00:20:25.040 --> 00:20:26.800]   that I think we build everything
[00:20:26.800 --> 00:20:28.320]   on top of these kinds of structures.
[00:20:28.320 --> 00:20:30.880]   And one interesting theory around that,
[00:20:30.880 --> 00:20:32.520]   because this theory, for instance, for language,
[00:20:32.520 --> 00:20:36.200]   but language origins is that, where did language come from?
[00:20:36.200 --> 00:20:41.200]   And it's a plausible theory that first came social systems
[00:20:41.200 --> 00:20:44.180]   that you have different roles in a society.
[00:20:44.180 --> 00:20:47.380]   And then those roles are exchangeable,
[00:20:47.380 --> 00:20:49.960]   that I scratch your back, you scratch my back,
[00:20:49.960 --> 00:20:51.520]   we can exchange roles.
[00:20:51.520 --> 00:20:53.480]   And once you have the brain structures
[00:20:53.480 --> 00:20:55.760]   that allow you to understand actions in terms of roles
[00:20:55.760 --> 00:20:59.080]   that can be changed, that's the basis for language,
[00:20:59.080 --> 00:20:59.960]   for grammar.
[00:20:59.960 --> 00:21:02.520]   And now you can start using symbols to refer
[00:21:02.520 --> 00:21:04.800]   to objects in the world,
[00:21:04.800 --> 00:21:06.720]   and you have this flexible structure.
[00:21:06.720 --> 00:21:10.960]   So there's a social structure that's fundamental
[00:21:10.960 --> 00:21:12.420]   for language to develop.
[00:21:12.420 --> 00:21:13.880]   Now, again, then you have language,
[00:21:13.880 --> 00:21:17.160]   you can refer to things that are not here right now,
[00:21:17.160 --> 00:21:20.880]   and that allows you to then build all the good stuff
[00:21:20.880 --> 00:21:22.580]   about planning, for instance,
[00:21:22.580 --> 00:21:24.600]   and building things and so on.
[00:21:24.600 --> 00:21:28.240]   So yeah, I think that very strongly humans are social,
[00:21:28.240 --> 00:21:32.960]   and that gives us ability to structure the world.
[00:21:32.960 --> 00:21:35.480]   But also as a society, we can do so much more,
[00:21:35.480 --> 00:21:37.960]   'cause one person does not have to do everything.
[00:21:37.960 --> 00:21:39.760]   You can have different roles
[00:21:39.760 --> 00:21:41.680]   and together achieve a lot more.
[00:21:41.680 --> 00:21:43.140]   And that's also something we see
[00:21:43.140 --> 00:21:44.800]   in computational simulations today.
[00:21:44.800 --> 00:21:46.320]   I mean, we have multi-agent systems
[00:21:46.320 --> 00:21:47.760]   that can perform tasks.
[00:21:47.760 --> 00:21:50.000]   This fascinating demonstration,
[00:21:50.000 --> 00:21:51.880]   Marco Dorigo, I think it was,
[00:21:51.880 --> 00:21:54.000]   these robots, little robots that had to navigate
[00:21:54.000 --> 00:21:54.840]   through an environment,
[00:21:54.840 --> 00:21:57.660]   and there were things that are dangerous,
[00:21:57.660 --> 00:22:01.160]   like maybe a big chasm or some kind of groove,
[00:22:01.160 --> 00:22:03.600]   a hole, and they could not get across it.
[00:22:03.600 --> 00:22:06.480]   But if they grab each other with their gripper,
[00:22:06.480 --> 00:22:09.000]   they formed a robot that was much longer,
[00:22:09.000 --> 00:22:12.360]   like a team, and this way they could get across that.
[00:22:12.360 --> 00:22:14.240]   So this is a great example
[00:22:14.240 --> 00:22:16.680]   of how together we can achieve things
[00:22:16.680 --> 00:22:18.920]   we couldn't otherwise, like the hyenas.
[00:22:18.920 --> 00:22:21.440]   Alone they couldn't, but as a team they could.
[00:22:21.440 --> 00:22:23.200]   And I think humans do that all the time.
[00:22:23.200 --> 00:22:24.840]   We're really good at that.
[00:22:24.840 --> 00:22:28.000]   - Yeah, and the way you described the system of hyenas,
[00:22:28.000 --> 00:22:29.720]   it almost sounds algorithmic.
[00:22:29.720 --> 00:22:32.840]   Like the problem with humans is they're so complex,
[00:22:32.840 --> 00:22:35.080]   it's hard to think of them as algorithms.
[00:22:35.080 --> 00:22:39.040]   But with hyenas, it's simple enough
[00:22:39.040 --> 00:22:40.520]   to where it feels like,
[00:22:41.860 --> 00:22:43.260]   at least hopeful, that it's possible
[00:22:43.260 --> 00:22:48.260]   to create computational systems that mimic that.
[00:22:48.260 --> 00:22:52.020]   - Yeah, that's exactly why we looked at that.
[00:22:52.020 --> 00:22:53.180]   - As opposed to humans.
[00:22:53.180 --> 00:22:55.260]   - Like I said, they are intelligent,
[00:22:55.260 --> 00:22:59.420]   but they are not quite as intelligent as, say, baboons,
[00:22:59.420 --> 00:23:02.140]   which would learn a lot and would be much more flexible.
[00:23:02.140 --> 00:23:05.700]   The hyenas are relatively rigid in what they can do.
[00:23:05.700 --> 00:23:08.100]   And therefore, you could look at this behavior,
[00:23:08.100 --> 00:23:11.540]   like this is a breakthrough in evolution about to happen,
[00:23:11.540 --> 00:23:12.860]   that they've discovered something
[00:23:12.860 --> 00:23:17.560]   about social structures, communication, about cooperation,
[00:23:17.560 --> 00:23:20.580]   and it might then spill over to other things too
[00:23:20.580 --> 00:23:22.660]   in thousands of years in the future.
[00:23:22.660 --> 00:23:24.960]   - Yeah, I think the problem with baboons and humans
[00:23:24.960 --> 00:23:27.860]   is probably too much is going on inside the head,
[00:23:27.860 --> 00:23:29.180]   where we won't be able to measure it
[00:23:29.180 --> 00:23:30.340]   if we're observing the system.
[00:23:30.340 --> 00:23:34.260]   With hyenas, it's probably easier to observe
[00:23:34.260 --> 00:23:35.500]   the actual decision-making
[00:23:35.500 --> 00:23:38.660]   and the various motivations that are involved.
[00:23:38.660 --> 00:23:40.060]   - Yeah, they are visible.
[00:23:40.060 --> 00:23:45.060]   - And we can even quantify possibly their emotional state
[00:23:45.060 --> 00:23:48.180]   because they leave droppings behind.
[00:23:48.180 --> 00:23:49.580]   And there are chemicals there
[00:23:49.580 --> 00:23:52.980]   that can be associated with neurotransmitters.
[00:23:52.980 --> 00:23:55.660]   And we can separate what emotions they might have
[00:23:55.660 --> 00:23:58.980]   experienced in the last 24 hours.
[00:23:58.980 --> 00:24:04.240]   - What to you is the most beautiful, speaking of hyenas,
[00:24:04.240 --> 00:24:08.740]   what to you is the most beautiful nature-inspired algorithm
[00:24:08.740 --> 00:24:10.460]   in your work that you've come across?
[00:24:10.460 --> 00:24:13.780]   Something maybe earlier on in your work or maybe today?
[00:24:13.780 --> 00:24:17.140]   - I think evolutionary computation
[00:24:17.140 --> 00:24:19.860]   is the most amazing method.
[00:24:19.860 --> 00:24:24.380]   So what fascinates me most is that, with computers,
[00:24:24.380 --> 00:24:27.660]   is that you can get more out than you put in.
[00:24:27.660 --> 00:24:29.900]   I mean, you can write a piece of code
[00:24:29.900 --> 00:24:32.540]   and your machine does what you told it.
[00:24:32.540 --> 00:24:35.380]   I mean, this happened to me in my freshman year.
[00:24:35.380 --> 00:24:37.780]   It did something very simple and I was just amazed.
[00:24:37.780 --> 00:24:40.300]   I was blown away that it would get the number
[00:24:40.300 --> 00:24:42.220]   and it would compute the result
[00:24:42.220 --> 00:24:44.080]   and I didn't have to do it myself.
[00:24:44.080 --> 00:24:45.180]   Very simple.
[00:24:45.180 --> 00:24:47.540]   But if you push that a little further,
[00:24:47.540 --> 00:24:49.580]   you can have machines that learn
[00:24:49.580 --> 00:24:51.580]   and they might learn patterns.
[00:24:51.580 --> 00:24:54.040]   And already, say, deep learning neural networks,
[00:24:54.040 --> 00:24:59.020]   they can learn to recognize objects, sounds, patterns
[00:24:59.020 --> 00:25:00.620]   that humans have trouble with.
[00:25:00.620 --> 00:25:02.540]   And sometimes they do it better than humans.
[00:25:02.540 --> 00:25:04.300]   And that's so fascinating.
[00:25:04.300 --> 00:25:06.180]   And now if you take that one more step,
[00:25:06.180 --> 00:25:08.220]   you get something like evolutionary algorithms
[00:25:08.220 --> 00:25:10.540]   that discover things, they create things.
[00:25:10.540 --> 00:25:13.500]   They come up with solutions that you did not think of.
[00:25:13.500 --> 00:25:15.180]   And that just blows me away.
[00:25:15.180 --> 00:25:18.680]   It's so great that we can build systems, algorithms,
[00:25:18.680 --> 00:25:21.560]   that can be, in some sense, smarter than we are,
[00:25:21.560 --> 00:25:24.940]   that they can discover solutions that we might miss.
[00:25:24.940 --> 00:25:26.680]   A lot of times it is because we have, as humans,
[00:25:26.680 --> 00:25:27.900]   we have certain biases.
[00:25:27.900 --> 00:25:30.060]   We expect the solutions to be a certain way.
[00:25:30.060 --> 00:25:32.300]   And you don't put those biases into the algorithm
[00:25:32.300 --> 00:25:34.140]   so they are more free to explore.
[00:25:34.140 --> 00:25:37.820]   And evolution is just absolutely fantastic explorer.
[00:25:37.820 --> 00:25:40.420]   And that's what really is fascinating.
[00:25:40.420 --> 00:25:43.860]   - Yeah, I think I get made fun of a bit
[00:25:43.860 --> 00:25:45.920]   'cause I currently don't have any kids.
[00:25:45.920 --> 00:25:47.700]   But you mentioned programs.
[00:25:47.700 --> 00:25:50.740]   I mean, do you have kids?
[00:25:50.740 --> 00:25:51.580]   - Yeah.
[00:25:51.580 --> 00:25:52.660]   - So maybe you could speak to this.
[00:25:52.660 --> 00:25:55.740]   But there's a magic to the creative process.
[00:25:55.740 --> 00:25:59.800]   With Spot, the Boston Dynamics Spot,
[00:25:59.800 --> 00:26:02.440]   but really any robot that I've ever worked on,
[00:26:02.440 --> 00:26:04.540]   it just feels like the similar kind of joy,
[00:26:04.540 --> 00:26:06.600]   I imagine, I would have as a father.
[00:26:06.600 --> 00:26:08.420]   Not the same, perhaps, level,
[00:26:08.420 --> 00:26:10.220]   but the same kind of wonderment.
[00:26:10.220 --> 00:26:12.980]   Like, there's exactly this, which is like,
[00:26:12.980 --> 00:26:16.500]   you know what you had to do initially
[00:26:16.500 --> 00:26:19.540]   to get this thing going.
[00:26:19.540 --> 00:26:21.700]   Let's speak on the computer science side,
[00:26:21.700 --> 00:26:23.860]   like what the program looks like.
[00:26:23.860 --> 00:26:27.900]   But something about it doing more
[00:26:27.900 --> 00:26:30.900]   than what the program was written on paper,
[00:26:30.900 --> 00:26:34.700]   is like, that somehow connects to the magic
[00:26:34.700 --> 00:26:36.140]   of this entire universe.
[00:26:36.140 --> 00:26:39.220]   Like, that's like, I feel like I found God.
[00:26:39.220 --> 00:26:41.120]   Every time I like, it's like,
[00:26:41.120 --> 00:26:45.700]   'cause you've really created something that's living.
[00:26:45.700 --> 00:26:46.540]   - Yeah.
[00:26:46.540 --> 00:26:47.380]   - Even if it's a simple program.
[00:26:47.380 --> 00:26:48.780]   - It has a life of its own, has the intelligence of its own.
[00:26:48.780 --> 00:26:51.060]   It's beyond what you actually thought.
[00:26:51.060 --> 00:26:51.900]   - Yeah.
[00:26:51.900 --> 00:26:53.460]   - And that is, I think it's exactly, Spot on,
[00:26:53.460 --> 00:26:55.500]   that's exactly what it's about.
[00:26:55.500 --> 00:26:57.860]   You created something and has an ability
[00:26:57.860 --> 00:27:00.940]   to live its life and do good things.
[00:27:00.940 --> 00:27:03.260]   And you just gave it a starting point.
[00:27:03.260 --> 00:27:04.420]   So in that sense, I think it's,
[00:27:04.420 --> 00:27:06.500]   that may be part of the joy, actually.
[00:27:06.500 --> 00:27:11.020]   - But you mentioned creativity in this context,
[00:27:11.020 --> 00:27:14.160]   especially in the context of evolutionary computation.
[00:27:14.160 --> 00:27:17.420]   So, you know, we don't often think of algorithms
[00:27:17.420 --> 00:27:20.320]   as creative, so how do you think about creativity?
[00:27:20.320 --> 00:27:25.020]   - Yeah, algorithms absolutely can be creative.
[00:27:25.020 --> 00:27:28.380]   They can come up with solutions that you don't think about.
[00:27:28.380 --> 00:27:29.820]   I mean, creativity can be defined.
[00:27:29.820 --> 00:27:32.740]   A couple of requirements have to, has to be new.
[00:27:32.740 --> 00:27:35.580]   It has to be useful and it has to be surprising.
[00:27:35.580 --> 00:27:38.020]   And those certainly are true with, say,
[00:27:38.020 --> 00:27:41.580]   evolutionary computation, discovering solutions.
[00:27:41.580 --> 00:27:44.340]   So maybe an example, for instance,
[00:27:44.340 --> 00:27:47.500]   we did this collaboration with MIT Media Lab,
[00:27:47.500 --> 00:27:52.500]   Caleb Harvest Lab, where they had a hydroponic food computer
[00:27:53.820 --> 00:27:55.900]   they called it, environment that was completely
[00:27:55.900 --> 00:27:59.580]   computer controlled, nutrients, water, light, temperature,
[00:27:59.580 --> 00:28:00.940]   everything's controlled.
[00:28:00.940 --> 00:28:05.580]   Now, what do you do if you can't control everything?
[00:28:05.580 --> 00:28:07.420]   Farmers know a lot about how to do,
[00:28:07.420 --> 00:28:10.340]   how to make plants grow in their own patch of land.
[00:28:10.340 --> 00:28:13.140]   But if you can control everything, it's too much.
[00:28:13.140 --> 00:28:14.660]   And it turns out that we don't actually
[00:28:14.660 --> 00:28:16.100]   know very much about it.
[00:28:16.100 --> 00:28:20.380]   So we built a system, evolutionary optimization system,
[00:28:20.380 --> 00:28:23.740]   together with a surrogate model of how plants grow.
[00:28:23.740 --> 00:28:28.740]   And let this system explore recipes on its own.
[00:28:28.740 --> 00:28:32.260]   And initially we were focusing on light,
[00:28:32.260 --> 00:28:37.060]   how strong, what wavelengths, how long the light was on.
[00:28:37.060 --> 00:28:38.740]   And we put some boundaries,
[00:28:38.740 --> 00:28:40.340]   which we thought were reasonable.
[00:28:40.340 --> 00:28:44.580]   For instance, that there was at least six hours of darkness
[00:28:44.580 --> 00:28:47.380]   like night, because that's what we have in the world.
[00:28:47.380 --> 00:28:51.700]   And very quickly, the system evolution pushed
[00:28:51.700 --> 00:28:54.340]   all the recipes to that limit.
[00:28:54.340 --> 00:28:56.100]   We were trying to grow basil,
[00:28:56.100 --> 00:29:00.260]   and we had initially had some 200, 300 recipes,
[00:29:00.260 --> 00:29:02.380]   exploration as well as known recipes.
[00:29:02.380 --> 00:29:04.300]   But now we are going beyond that.
[00:29:04.300 --> 00:29:06.700]   And everything was like pushed at that limit.
[00:29:06.700 --> 00:29:08.540]   So we look at it and say, well, you know,
[00:29:08.540 --> 00:29:09.540]   we can easily just change it.
[00:29:09.540 --> 00:29:10.980]   Let's have it your way.
[00:29:10.980 --> 00:29:13.700]   And it turns out the system discovered
[00:29:13.700 --> 00:29:15.660]   that basil does not need to sleep.
[00:29:15.660 --> 00:29:19.700]   24 hours, lights on, and it will thrive.
[00:29:19.700 --> 00:29:21.700]   It will be bigger, it will be tastier.
[00:29:21.700 --> 00:29:24.700]   And this was a big surprise, not just to us,
[00:29:24.700 --> 00:29:28.940]   but also the biologists in the team that anticipated
[00:29:28.940 --> 00:29:32.380]   that there's some constraints that are in the world.
[00:29:32.380 --> 00:29:34.540]   For a reason, it turns out that evolution
[00:29:34.540 --> 00:29:36.180]   did not have the same bias.
[00:29:36.180 --> 00:29:38.980]   And therefore it discovered something that was creative.
[00:29:38.980 --> 00:29:41.540]   It was surprising, it was useful, and it was new.
[00:29:41.540 --> 00:29:42.900]   - That's fascinating to think about,
[00:29:42.900 --> 00:29:45.580]   like the things we think that are fundamental
[00:29:45.580 --> 00:29:48.420]   to living systems on Earth today,
[00:29:48.420 --> 00:29:49.940]   whether they're actually fundamental
[00:29:49.940 --> 00:29:53.940]   or they somehow fit the constraints of the system,
[00:29:53.940 --> 00:29:56.700]   and all we'll have to do is just remove the constraints.
[00:29:56.700 --> 00:30:00.580]   Do you ever think about, I don't know how much you know
[00:30:00.580 --> 00:30:03.540]   about brain-computer interfaces and Neuralink.
[00:30:03.540 --> 00:30:08.540]   The idea there is, you know, our brains are very limited.
[00:30:08.540 --> 00:30:11.740]   And if we just allow, we plug in,
[00:30:11.740 --> 00:30:13.980]   we provide a mechanism for a computer
[00:30:13.980 --> 00:30:17.140]   to speak with the brain, so you're thereby expanding
[00:30:17.140 --> 00:30:19.500]   the computational power of the brain,
[00:30:19.500 --> 00:30:22.940]   the possibilities there, sort of from a very high level
[00:30:22.940 --> 00:30:27.020]   philosophical perspective, is limitless.
[00:30:27.020 --> 00:30:30.700]   But I wonder how limitless it is.
[00:30:30.700 --> 00:30:33.460]   Are the constraints we have like features
[00:30:33.460 --> 00:30:36.020]   that are fundamental to our intelligence?
[00:30:36.020 --> 00:30:38.420]   Or is this just like this weird constraint
[00:30:38.420 --> 00:30:42.700]   in terms of our brain size and skull and lifespan
[00:30:42.700 --> 00:30:46.260]   and the senses, it's just the weird little
[00:30:46.260 --> 00:30:50.020]   like a quirk of evolution, and if we just open that up,
[00:30:50.020 --> 00:30:51.540]   like add much more senses,
[00:30:51.540 --> 00:30:53.740]   add much more computational power,
[00:30:53.740 --> 00:30:57.860]   the intelligence will expand exponentially.
[00:30:57.860 --> 00:31:02.860]   Do you have a sense about constraints,
[00:31:02.860 --> 00:31:05.420]   the relationship of evolution and computation
[00:31:05.420 --> 00:31:07.320]   to the constraints of the environment?
[00:31:07.320 --> 00:31:12.420]   - Well, at first I'd like to comment on that,
[00:31:12.420 --> 00:31:15.820]   like changing the inputs to human brain.
[00:31:15.820 --> 00:31:17.300]   - Yes, that would be great. - And flexibility
[00:31:17.300 --> 00:31:20.740]   of the brain, I think there's a lot of that.
[00:31:20.740 --> 00:31:22.380]   There are experiments that are done in animals,
[00:31:22.380 --> 00:31:25.740]   like migangas are, but they might be switching
[00:31:25.740 --> 00:31:29.220]   the auditory and visual information
[00:31:29.220 --> 00:31:31.500]   and going to the wrong part of the cortex,
[00:31:31.500 --> 00:31:34.180]   and the animal was still able to hear
[00:31:34.180 --> 00:31:36.540]   and perceive the visual environment.
[00:31:36.540 --> 00:31:41.180]   And there are kids that are born with severe disorders,
[00:31:41.180 --> 00:31:43.980]   and sometimes they have to remove half of the brain,
[00:31:43.980 --> 00:31:46.180]   like one half, and they still grow up,
[00:31:46.180 --> 00:31:48.380]   they have the functions migrate to the other parts.
[00:31:48.380 --> 00:31:50.420]   There's a lot of flexibility like that.
[00:31:50.420 --> 00:31:55.020]   So I think it's quite possible to hook up the brain
[00:31:55.020 --> 00:31:57.660]   with different kinds of sensors, for instance,
[00:31:57.660 --> 00:32:00.340]   and something that we don't even quite understand
[00:32:00.340 --> 00:32:02.580]   or have today, and different kinds of wavelengths
[00:32:02.580 --> 00:32:05.700]   or whatever they are, and then the brain can learn
[00:32:05.700 --> 00:32:07.380]   to make sense of it.
[00:32:07.380 --> 00:32:10.020]   And that, I think, is this good hope
[00:32:10.020 --> 00:32:12.760]   that these prosthetic devices, for instance, work,
[00:32:12.760 --> 00:32:15.740]   not because we make them so good and so easy to use,
[00:32:15.740 --> 00:32:17.860]   but the brain adapts to them and can learn
[00:32:17.860 --> 00:32:19.160]   to take advantage of them.
[00:32:19.160 --> 00:32:23.460]   And so in that sense, if there's a trouble, a problem,
[00:32:23.460 --> 00:32:26.220]   I think the brain can be used to correct it.
[00:32:26.220 --> 00:32:29.140]   Now, going beyond what we have today, can you get smarter?
[00:32:29.140 --> 00:32:31.600]   That's really much harder to do.
[00:32:31.600 --> 00:32:35.540]   Giving the brain more input probably might overwhelm it.
[00:32:35.540 --> 00:32:38.660]   It would have to learn to filter it and focus
[00:32:39.740 --> 00:32:43.340]   in order to use the information effectively.
[00:32:43.340 --> 00:32:47.160]   And augmenting intelligence with some kind
[00:32:47.160 --> 00:32:51.560]   of external devices like that might be difficult, I think.
[00:32:51.560 --> 00:32:55.680]   But replacing what's lost, I think, is quite possible.
[00:32:55.680 --> 00:32:59.360]   - Right, so our intuition allows us to sort of imagine
[00:32:59.360 --> 00:33:01.360]   that we can replace what's been lost,
[00:33:01.360 --> 00:33:03.480]   but expansion beyond what we have.
[00:33:03.480 --> 00:33:05.360]   I mean, we're already one of the most,
[00:33:05.360 --> 00:33:07.800]   if not the most intelligent things on this earth, right?
[00:33:07.800 --> 00:33:12.800]   So it's hard to imagine if the brain can hold up
[00:33:12.800 --> 00:33:15.500]   with an order of magnitude greater set
[00:33:15.500 --> 00:33:18.060]   of information thrown at it,
[00:33:18.060 --> 00:33:20.740]   if it can reason through that.
[00:33:20.740 --> 00:33:22.580]   Part of me, this is the Russian thing, I think,
[00:33:22.580 --> 00:33:25.420]   is I tend to think that the limitations
[00:33:25.420 --> 00:33:30.420]   is where the superpower is, that immortality
[00:33:30.420 --> 00:33:36.440]   and huge increase in bandwidth of information
[00:33:37.440 --> 00:33:40.200]   by connecting computers with the brain
[00:33:40.200 --> 00:33:42.680]   is not going to produce greater intelligence.
[00:33:42.680 --> 00:33:44.320]   It might produce lesser intelligence.
[00:33:44.320 --> 00:33:46.080]   So I don't know, there's something
[00:33:46.080 --> 00:33:51.080]   about the scarcity being essential
[00:33:51.080 --> 00:33:56.080]   to fitness or performance, but that could be just
[00:33:56.080 --> 00:33:59.000]   'cause we're so limited.
[00:33:59.000 --> 00:34:00.680]   - No, exactly, you make do with what you have.
[00:34:00.680 --> 00:34:04.280]   But you don't have to pipe it directly to the brain.
[00:34:04.280 --> 00:34:07.560]   I mean, we already have devices like phones
[00:34:07.560 --> 00:34:10.160]   where we can look up information at any point.
[00:34:10.160 --> 00:34:12.320]   And that can make us more productive.
[00:34:12.320 --> 00:34:14.000]   You don't have to argue about, I don't know,
[00:34:14.000 --> 00:34:16.400]   what happened in that baseball game or whatever it is,
[00:34:16.400 --> 00:34:17.680]   because you can look it up right away.
[00:34:17.680 --> 00:34:22.040]   And I think in that sense, we can learn to utilize tools.
[00:34:22.040 --> 00:34:25.240]   And that's what we have been doing for a long, long time.
[00:34:25.240 --> 00:34:29.000]   So, and we are already, the brain is already drinking
[00:34:29.000 --> 00:34:32.440]   from the fire hose, like vision.
[00:34:32.440 --> 00:34:34.520]   There's way more information in vision
[00:34:34.520 --> 00:34:35.680]   than we actually process.
[00:34:35.680 --> 00:34:38.960]   So brain's already good at identifying what matters.
[00:34:38.960 --> 00:34:42.880]   And that, we can switch that from vision
[00:34:42.880 --> 00:34:45.040]   to some other wavelength or some other kind of modality.
[00:34:45.040 --> 00:34:47.120]   But I think that the same processing principles
[00:34:47.120 --> 00:34:49.040]   probably still apply.
[00:34:49.040 --> 00:34:53.760]   But also, indeed, this ability to have information
[00:34:53.760 --> 00:34:56.000]   more accessible and more relevant, I think,
[00:34:56.000 --> 00:34:57.760]   can enhance what we do.
[00:34:57.760 --> 00:35:00.960]   I mean, kids today at school, they learn about DNA.
[00:35:00.960 --> 00:35:02.640]   I mean, things that were discovered
[00:35:02.640 --> 00:35:04.640]   just a couple of years ago,
[00:35:04.640 --> 00:35:06.480]   and it's already common knowledge,
[00:35:06.480 --> 00:35:07.600]   and we are building on it.
[00:35:07.600 --> 00:35:12.600]   And we don't see a problem where there's too much information
[00:35:12.600 --> 00:35:15.160]   that we can't absorb and learn.
[00:35:15.160 --> 00:35:17.560]   Maybe people become a little bit more narrow
[00:35:17.560 --> 00:35:20.920]   in what they know, they are in one field.
[00:35:20.920 --> 00:35:23.760]   But this information that we have accumulated,
[00:35:23.760 --> 00:35:26.120]   it is passed on, and people are picking up on it,
[00:35:26.120 --> 00:35:27.600]   and they are building on it.
[00:35:27.600 --> 00:35:31.040]   So it's not like we have reached the point of saturation.
[00:35:31.040 --> 00:35:34.520]   We have still this process that allows us to be selective
[00:35:34.520 --> 00:35:37.600]   and decide what's interesting, I think still works,
[00:35:37.600 --> 00:35:40.120]   even with the more information we have today.
[00:35:40.120 --> 00:35:42.200]   - Yeah, it's fascinating to think about
[00:35:42.200 --> 00:35:45.360]   like Wikipedia becoming a sensor,
[00:35:45.360 --> 00:35:49.080]   like so the fire hose of information from Wikipedia.
[00:35:49.080 --> 00:35:51.800]   So it's like you integrate it directly into the brain
[00:35:51.800 --> 00:35:54.240]   to where you're thinking, like you're observing the world
[00:35:54.240 --> 00:35:57.840]   with all of Wikipedia directly piping into your brain.
[00:35:57.840 --> 00:35:59.920]   So like when I see a light,
[00:35:59.920 --> 00:36:03.640]   I immediately have like the history of who invented
[00:36:03.640 --> 00:36:07.560]   electricity, like integrated very quickly into.
[00:36:07.560 --> 00:36:09.840]   So just the way you think about the world
[00:36:09.840 --> 00:36:11.240]   might be very interesting
[00:36:11.240 --> 00:36:13.280]   if you can integrate that kind of information.
[00:36:13.280 --> 00:36:15.640]   What are your thoughts, if I could ask,
[00:36:15.640 --> 00:36:20.360]   on the early steps on the Neuralink side,
[00:36:20.360 --> 00:36:21.520]   I don't know if you got a chance to see,
[00:36:21.520 --> 00:36:24.720]   but there's a monkey playing pong
[00:36:24.720 --> 00:36:25.960]   - Mm, yeah.
[00:36:25.960 --> 00:36:27.800]   - through the brain computer interface.
[00:36:27.800 --> 00:36:30.680]   And the dream there is sort of,
[00:36:30.680 --> 00:36:33.560]   you're already replacing the thumbs essentially
[00:36:33.560 --> 00:36:35.920]   that you would use to play a video game.
[00:36:35.920 --> 00:36:38.960]   The dream is to be able to increase further
[00:36:38.960 --> 00:36:43.440]   the interface by which you interact with the computer.
[00:36:43.440 --> 00:36:44.640]   Are you impressed by this?
[00:36:44.640 --> 00:36:46.480]   Are you worried about this?
[00:36:46.480 --> 00:36:48.000]   What are your thoughts as a human?
[00:36:48.000 --> 00:36:48.880]   - I think it's wonderful.
[00:36:48.880 --> 00:36:51.600]   I think it's great that we could do something like that.
[00:36:51.600 --> 00:36:55.080]   I mean, you can, there are devices that read your EEG,
[00:36:55.080 --> 00:37:00.080]   for instance, and humans can learn to control things
[00:37:00.080 --> 00:37:02.760]   using just their thoughts in that sense.
[00:37:02.760 --> 00:37:04.920]   And I don't think it's that different.
[00:37:04.920 --> 00:37:06.720]   I mean, those signals would go to limbs,
[00:37:06.720 --> 00:37:08.320]   they would go to thumbs.
[00:37:08.320 --> 00:37:11.200]   Now the same signals go through a sensor
[00:37:11.200 --> 00:37:12.920]   to some computing system.
[00:37:12.920 --> 00:37:17.520]   It still probably has to be built on human terms,
[00:37:17.520 --> 00:37:20.000]   not to overwhelm them, but utilize what's there
[00:37:20.000 --> 00:37:23.760]   and sense the right kind of patterns
[00:37:23.760 --> 00:37:24.840]   that are easy to generate.
[00:37:24.840 --> 00:37:27.760]   But, oh, that I think is really quite possible
[00:37:27.760 --> 00:37:30.720]   and wonderful and could be very much more efficient.
[00:37:30.720 --> 00:37:34.160]   - Is there, so you mentioned surprising
[00:37:34.160 --> 00:37:37.080]   being a characteristic of creativity.
[00:37:37.080 --> 00:37:39.800]   Is there something, you already mentioned a few examples,
[00:37:39.800 --> 00:37:41.920]   but is there something that jumps out at you
[00:37:41.920 --> 00:37:44.560]   as was particularly surprising
[00:37:44.600 --> 00:37:48.680]   from the various evolutionary computation systems
[00:37:48.680 --> 00:37:50.880]   you've worked on, the solutions that were
[00:37:50.880 --> 00:37:55.320]   come up along the way, not necessarily the final solutions,
[00:37:55.320 --> 00:37:58.720]   but maybe things that were even discarded.
[00:37:58.720 --> 00:38:00.760]   Is there something that just jumps to mind?
[00:38:00.760 --> 00:38:02.200]   - It happens all the time.
[00:38:02.200 --> 00:38:05.680]   I mean, evolution is so creative,
[00:38:05.680 --> 00:38:09.280]   so good at discovering solutions you don't anticipate.
[00:38:09.280 --> 00:38:12.720]   A lot of times they are taking advantage of something
[00:38:12.720 --> 00:38:13.840]   that you didn't think was there,
[00:38:13.840 --> 00:38:16.000]   like a bug in the software, for instance.
[00:38:16.000 --> 00:38:17.640]   A lot of, there's a great paper,
[00:38:17.640 --> 00:38:19.160]   the community put it together,
[00:38:19.160 --> 00:38:22.960]   about surprising anecdotes about evolutionary computation.
[00:38:22.960 --> 00:38:25.640]   A lot of them are indeed, in some software environment,
[00:38:25.640 --> 00:38:28.120]   there was a loophole or a bug,
[00:38:28.120 --> 00:38:30.600]   and the system utilizes that.
[00:38:30.600 --> 00:38:31.960]   - By the way, for people who want to read it,
[00:38:31.960 --> 00:38:33.120]   it's kind of fun to read.
[00:38:33.120 --> 00:38:36.040]   It's called "The Surprising Creativity of Digital Evolution,
[00:38:36.040 --> 00:38:39.240]   "A Collection of Anecdotes from the Evolutionary Computation
[00:38:39.240 --> 00:38:41.600]   "and Artificial Life Research Communities."
[00:38:41.600 --> 00:38:43.200]   And there's just a bunch of stories
[00:38:43.200 --> 00:38:45.880]   from all the seminal figures in this community.
[00:38:45.880 --> 00:38:48.560]   You have a story in there that relates to you,
[00:38:48.560 --> 00:38:51.040]   at least, on the tic-tac-toe memory bomb.
[00:38:51.040 --> 00:38:54.800]   So can you, I guess, describe that situation,
[00:38:54.800 --> 00:38:55.720]   if you think that's--
[00:38:55.720 --> 00:38:59.680]   - Yeah, that's a quite a bit smaller scale
[00:38:59.680 --> 00:39:03.080]   than our basic doesn't need to sleep surprise,
[00:39:03.080 --> 00:39:06.680]   but it was actually done by students in my class,
[00:39:06.680 --> 00:39:09.480]   in a neural nets evolutionary computation class.
[00:39:09.480 --> 00:39:10.680]   There was an assignment.
[00:39:11.760 --> 00:39:13.920]   It was perhaps a final project
[00:39:13.920 --> 00:39:17.720]   where people built game-playing AI.
[00:39:17.720 --> 00:39:18.840]   It was an AI class.
[00:39:18.840 --> 00:39:23.080]   And it was for tic-tac-toe or five in a row
[00:39:23.080 --> 00:39:24.600]   in a large board.
[00:39:24.600 --> 00:39:28.200]   And this one team evolved a neural network
[00:39:28.200 --> 00:39:29.960]   to make these moves.
[00:39:29.960 --> 00:39:32.760]   And they set it up, the evolution.
[00:39:32.760 --> 00:39:35.280]   They didn't really know what would come out,
[00:39:35.280 --> 00:39:37.040]   but it turned out that they did really well.
[00:39:37.040 --> 00:39:38.920]   Evolution actually won the tournament.
[00:39:38.920 --> 00:39:40.560]   And most of the time when it won,
[00:39:40.560 --> 00:39:43.480]   it won because the other teams crashed.
[00:39:43.480 --> 00:39:45.760]   And then when we look at it, like what was going on,
[00:39:45.760 --> 00:39:48.240]   was that evolution discovered that if it makes a move
[00:39:48.240 --> 00:39:49.960]   that's really, really far away,
[00:39:49.960 --> 00:39:53.440]   like millions of squares away,
[00:39:53.440 --> 00:39:57.800]   the other teams, the other programs just expanded memory
[00:39:57.800 --> 00:39:59.160]   in order to take that into account
[00:39:59.160 --> 00:40:01.200]   until they ran out of memory and crashed.
[00:40:01.200 --> 00:40:03.200]   And then you win a tournament
[00:40:03.200 --> 00:40:05.720]   by crashing all your opponents.
[00:40:05.720 --> 00:40:08.920]   - I think that's quite a profound example,
[00:40:08.920 --> 00:40:13.200]   which probably applies to most games
[00:40:13.200 --> 00:40:16.920]   from even a game theoretic perspective,
[00:40:16.920 --> 00:40:18.280]   that sometimes to win,
[00:40:18.280 --> 00:40:22.720]   you don't have to be better within the rules of the game.
[00:40:22.720 --> 00:40:27.720]   You have to come up with ways to break your opponent's brain
[00:40:27.720 --> 00:40:31.360]   if it's a human, like not through violence,
[00:40:31.360 --> 00:40:34.680]   but through some hack where the brain just is not,
[00:40:36.440 --> 00:40:39.360]   you're basically, how would you put it?
[00:40:39.360 --> 00:40:43.080]   You're going outside the constraints
[00:40:43.080 --> 00:40:45.080]   of where the brain is able to function.
[00:40:45.080 --> 00:40:46.560]   - Expectations of your opponent.
[00:40:46.560 --> 00:40:49.560]   I mean, this was even Kasparov pointed that out
[00:40:49.560 --> 00:40:51.760]   that when Deep Blue was playing against Kasparov,
[00:40:51.760 --> 00:40:55.440]   that it was not playing the same way as Kasparov expected.
[00:40:55.440 --> 00:40:59.720]   And this has to do with not having the same biases.
[00:40:59.720 --> 00:41:05.800]   And that's really one of the strengths of the AI approach.
[00:41:06.320 --> 00:41:08.120]   - Can you at a high level say,
[00:41:08.120 --> 00:41:10.400]   what are the basic mechanisms
[00:41:10.400 --> 00:41:12.800]   of evolutionary computation algorithms
[00:41:12.800 --> 00:41:15.800]   that use something that could be called
[00:41:15.800 --> 00:41:17.680]   an evolutionary approach?
[00:41:17.680 --> 00:41:18.880]   Like how does it work?
[00:41:18.880 --> 00:41:21.720]   What are the connections to the,
[00:41:21.720 --> 00:41:24.840]   what are the echoes of the connection to his biological?
[00:41:24.840 --> 00:41:27.120]   - A lot of these algorithms really do take motivation
[00:41:27.120 --> 00:41:29.600]   from biology, but they are carry catches.
[00:41:29.600 --> 00:41:31.320]   You try to essentialize it
[00:41:31.320 --> 00:41:33.600]   and take the elements that you believe matter.
[00:41:33.600 --> 00:41:35.920]   So in evolutionary computation,
[00:41:35.920 --> 00:41:38.080]   it is the creation of variation
[00:41:38.080 --> 00:41:40.720]   and then the selection upon that.
[00:41:40.720 --> 00:41:41.880]   So the creation of variation,
[00:41:41.880 --> 00:41:43.120]   you have to have some mechanism
[00:41:43.120 --> 00:41:44.760]   that allow you to create new individuals
[00:41:44.760 --> 00:41:47.120]   that are very different from what you already have.
[00:41:47.120 --> 00:41:49.040]   That's the creativity part.
[00:41:49.040 --> 00:41:50.760]   And then you have to have some way of measuring
[00:41:50.760 --> 00:41:55.600]   how well they are doing and using that measure to select
[00:41:55.600 --> 00:41:58.160]   who goes to the next generation and you continue.
[00:41:58.160 --> 00:42:00.720]   - So first you have to have some kind
[00:42:00.720 --> 00:42:03.160]   of digital representation of an individual
[00:42:03.160 --> 00:42:04.520]   that can be then modified.
[00:42:04.520 --> 00:42:08.760]   So I guess humans in biological systems have DNA
[00:42:08.760 --> 00:42:09.720]   and all those kinds of things.
[00:42:09.720 --> 00:42:12.160]   And so you have to have similar kind of encodings
[00:42:12.160 --> 00:42:13.400]   in a computer program.
[00:42:13.400 --> 00:42:15.040]   - Yes, and that is a big question.
[00:42:15.040 --> 00:42:16.960]   How do you encode these individuals?
[00:42:16.960 --> 00:42:19.560]   So there's a genotype, which is that encoding
[00:42:19.560 --> 00:42:21.400]   and then a decoding mechanism,
[00:42:21.400 --> 00:42:23.040]   which gives you the phenotype,
[00:42:23.040 --> 00:42:26.400]   which is the actual individual that then performs the task
[00:42:26.400 --> 00:42:31.280]   and in an environment can be evaluated how good it is.
[00:42:31.280 --> 00:42:33.120]   So even that mapping is a big question
[00:42:33.120 --> 00:42:34.920]   and how do you do it?
[00:42:34.920 --> 00:42:37.320]   But typically the representations are either
[00:42:37.320 --> 00:42:39.760]   they are strings of numbers or they are some kind of trees.
[00:42:39.760 --> 00:42:41.720]   Those are something that we know very well
[00:42:41.720 --> 00:42:43.560]   in computer science and we try to do that.
[00:42:43.560 --> 00:42:48.000]   But they, and DNA in some sense is also a sequence
[00:42:48.000 --> 00:42:49.480]   and a string.
[00:42:49.480 --> 00:42:52.000]   So it's not that far from it,
[00:42:52.000 --> 00:42:54.840]   but DNA also has many other aspects
[00:42:54.840 --> 00:42:56.720]   that we don't take into account necessarily
[00:42:56.720 --> 00:43:01.120]   like there's folding and interactions that are other
[00:43:01.120 --> 00:43:03.560]   than just the sequence itself.
[00:43:03.560 --> 00:43:06.000]   And lots of that is not yet captured
[00:43:06.000 --> 00:43:09.000]   and we don't know whether they are really crucial.
[00:43:09.000 --> 00:43:12.600]   Evolution, biological evolution has produced
[00:43:12.600 --> 00:43:16.000]   wonderful things, but if you look at them,
[00:43:16.000 --> 00:43:18.560]   it's not necessarily the case that every piece
[00:43:18.560 --> 00:43:20.880]   is irreplaceable and essential.
[00:43:20.880 --> 00:43:23.680]   There's a lot of baggage 'cause you have to construct it
[00:43:23.680 --> 00:43:25.360]   and it has to go through various stages
[00:43:25.360 --> 00:43:29.360]   and we still have appendix and we have tailbones
[00:43:29.360 --> 00:43:31.360]   and things like that that are not really that useful.
[00:43:31.360 --> 00:43:34.320]   If you try to explain them now, it would make no sense,
[00:43:34.320 --> 00:43:35.200]   it would be very hard.
[00:43:35.200 --> 00:43:38.200]   But if you think of us as productive evolution,
[00:43:38.200 --> 00:43:39.240]   you can see where they came from.
[00:43:39.240 --> 00:43:42.120]   They were useful at one point perhaps and no longer are,
[00:43:42.120 --> 00:43:43.400]   but they're still there.
[00:43:43.400 --> 00:43:47.080]   So that process is complex
[00:43:47.080 --> 00:43:50.800]   and your representation should support it.
[00:43:50.800 --> 00:43:55.800]   And that is quite difficult if we are limited
[00:43:56.280 --> 00:44:01.280]   with strings or trees and then we are pretty much limited
[00:44:01.280 --> 00:44:03.680]   what can be constructed.
[00:44:03.680 --> 00:44:05.600]   And one thing that we are still missing
[00:44:05.600 --> 00:44:07.520]   in evolutionary computation in particular
[00:44:07.520 --> 00:44:11.400]   is what we saw in biology, major transitions.
[00:44:11.400 --> 00:44:14.520]   So that you go from, for instance, single cell
[00:44:14.520 --> 00:44:17.160]   to multicell organisms and eventually societies.
[00:44:17.160 --> 00:44:19.560]   There are transitions of level of selection
[00:44:19.560 --> 00:44:22.080]   and level of what a unit is.
[00:44:22.080 --> 00:44:24.200]   And that's something we haven't captured
[00:44:24.200 --> 00:44:26.000]   in evolutionary computation yet.
[00:44:26.000 --> 00:44:28.640]   - Does that require a dramatic expansion
[00:44:28.640 --> 00:44:29.960]   of the representation?
[00:44:29.960 --> 00:44:31.600]   Is that what that is?
[00:44:31.600 --> 00:44:34.400]   - Most likely it does, but it's quite,
[00:44:34.400 --> 00:44:36.840]   we don't even understand it in biology very well
[00:44:36.840 --> 00:44:37.680]   where it's coming from.
[00:44:37.680 --> 00:44:40.480]   So it would be really good to look at major transitions
[00:44:40.480 --> 00:44:43.280]   in biology, try to characterize them a little bit more
[00:44:43.280 --> 00:44:45.360]   in detail, what the processes are.
[00:44:45.360 --> 00:44:48.600]   How does a, so like a unit, a cell
[00:44:48.600 --> 00:44:51.480]   is no longer evaluated alone, it's evaluated
[00:44:51.480 --> 00:44:54.720]   as part of a community, a multicell organism.
[00:44:54.720 --> 00:44:57.320]   Even though it could reproduce, now it can't alone.
[00:44:57.320 --> 00:44:59.360]   It has to have this environment.
[00:44:59.360 --> 00:45:03.400]   So there's a push to another level, at least the selection.
[00:45:03.400 --> 00:45:04.760]   - And how do you make that jump to the next level?
[00:45:04.760 --> 00:45:06.080]   - Yes, how do you make the jump?
[00:45:06.080 --> 00:45:07.240]   - As part of the algorithm.
[00:45:07.240 --> 00:45:08.160]   - Yeah, yeah.
[00:45:08.160 --> 00:45:12.080]   So we haven't really seen that in computation yet.
[00:45:12.080 --> 00:45:13.880]   And there are certainly attempts
[00:45:13.880 --> 00:45:15.800]   to have open-ended evolution.
[00:45:15.800 --> 00:45:18.400]   Things that could add more complexity
[00:45:18.400 --> 00:45:20.840]   and start selecting at a higher level,
[00:45:20.840 --> 00:45:24.680]   but it is still not quite the same
[00:45:24.680 --> 00:45:27.080]   as going from single to multi to society,
[00:45:27.080 --> 00:45:29.000]   for instance, in biology.
[00:45:29.000 --> 00:45:31.720]   - So there essentially would be,
[00:45:31.720 --> 00:45:33.440]   as opposed to having one agent,
[00:45:33.440 --> 00:45:36.240]   those agent all of a sudden spontaneously decide
[00:45:36.240 --> 00:45:41.000]   to then be together, and then your entire system
[00:45:41.000 --> 00:45:43.600]   would then be treating them as one agent.
[00:45:43.600 --> 00:45:44.720]   - Something like that.
[00:45:44.720 --> 00:45:46.320]   - Some kind of weird merger.
[00:45:46.320 --> 00:45:49.200]   But also, so you mentioned, I think you mentioned selection.
[00:45:49.200 --> 00:45:51.080]   So basically there's an agent,
[00:45:51.080 --> 00:45:54.240]   and they don't get to live on if they don't do well.
[00:45:54.240 --> 00:45:55.320]   So there's some kind of measure
[00:45:55.320 --> 00:45:57.320]   of what doing well is and isn't.
[00:45:57.320 --> 00:46:01.920]   And does mutation come into play at all
[00:46:01.920 --> 00:46:04.200]   in the process, and what role does it serve?
[00:46:04.200 --> 00:46:06.600]   - Yeah, so, and again, back to what
[00:46:06.600 --> 00:46:08.640]   the computational mechanisms of evolution computation are.
[00:46:08.640 --> 00:46:12.720]   So the way to create variation,
[00:46:12.720 --> 00:46:15.120]   you can take multiple individuals, two usually,
[00:46:15.120 --> 00:46:17.200]   but you could do more.
[00:46:17.200 --> 00:46:20.880]   And you exchange the parts of the representation.
[00:46:20.880 --> 00:46:22.720]   You do some kind of recombination,
[00:46:22.720 --> 00:46:25.000]   could be crossover, for instance.
[00:46:25.000 --> 00:46:28.920]   In biology, you do have DNA strings
[00:46:28.920 --> 00:46:32.120]   that are cut and put together again.
[00:46:32.120 --> 00:46:34.320]   We could do something like that.
[00:46:34.320 --> 00:46:36.480]   And it seems to be that in biology,
[00:46:36.480 --> 00:46:39.520]   the crossover is really the workhorse
[00:46:39.520 --> 00:46:42.160]   in biological evolution.
[00:46:42.160 --> 00:46:47.080]   In computation, we tend to rely more on mutation.
[00:46:47.080 --> 00:46:50.120]   And that is making random changes
[00:46:50.120 --> 00:46:51.320]   into parts of the chromosome.
[00:46:51.320 --> 00:46:53.200]   You could try to be intelligent
[00:46:53.200 --> 00:46:55.800]   and target certain areas of it,
[00:46:55.800 --> 00:47:00.800]   and make the mutations also follow some principle.
[00:47:00.800 --> 00:47:03.400]   Like you collect statistics of performance
[00:47:03.400 --> 00:47:05.600]   and correlations, and try to make mutations
[00:47:05.600 --> 00:47:07.960]   you believe are going to be helpful.
[00:47:07.960 --> 00:47:10.440]   That's where evolution computation has moved
[00:47:10.440 --> 00:47:12.160]   in the last 20 years.
[00:47:12.160 --> 00:47:14.000]   I mean, evolution computation has been around for 50 years,
[00:47:14.000 --> 00:47:16.160]   but a lot of the recent--
[00:47:16.160 --> 00:47:17.680]   - Success comes from mutation.
[00:47:17.680 --> 00:47:20.240]   - Comes from using statistics.
[00:47:20.240 --> 00:47:22.120]   It's like the rest of machine learning,
[00:47:22.120 --> 00:47:23.040]   based on statistics.
[00:47:23.040 --> 00:47:26.160]   We use similar tools to guide evolutionary computation.
[00:47:26.160 --> 00:47:28.760]   And in that sense, it has diverged a bit
[00:47:28.760 --> 00:47:30.680]   from biological evolution.
[00:47:30.680 --> 00:47:32.240]   And that's one of the things I think
[00:47:32.240 --> 00:47:34.200]   we could look at again,
[00:47:34.200 --> 00:47:38.520]   having a weaker selection, more crossover,
[00:47:38.520 --> 00:47:40.840]   large populations, more time,
[00:47:40.840 --> 00:47:42.840]   and maybe a different kind of creativity
[00:47:42.840 --> 00:47:43.960]   would come out of it.
[00:47:43.960 --> 00:47:47.000]   We are very impatient in evolutionary computation today.
[00:47:47.000 --> 00:47:49.600]   We want answers right now, right quickly.
[00:47:49.600 --> 00:47:52.120]   And if somebody doesn't perform, kill it.
[00:47:52.120 --> 00:47:56.480]   And biological evolution doesn't work quite that way.
[00:47:56.480 --> 00:47:57.840]   - It's more patient.
[00:47:57.840 --> 00:48:00.040]   - Yes, much more patient.
[00:48:00.040 --> 00:48:03.680]   - So I guess we need to add some kind of mating,
[00:48:03.680 --> 00:48:05.960]   some kind of dating mechanisms,
[00:48:05.960 --> 00:48:07.400]   like marriage maybe in there,
[00:48:07.400 --> 00:48:12.400]   so into our algorithms to improve the combination,
[00:48:13.680 --> 00:48:16.000]   as opposed to all mutation doing all of the work.
[00:48:16.000 --> 00:48:18.920]   - Yeah, and many ways of being successful.
[00:48:18.920 --> 00:48:21.600]   Usually in evolutionary computation, we have one goal,
[00:48:21.600 --> 00:48:25.920]   play this game really well compared to others.
[00:48:25.920 --> 00:48:28.680]   But in biology, there are many ways of being successful.
[00:48:28.680 --> 00:48:32.240]   You can build niches, you can be stronger, faster,
[00:48:32.240 --> 00:48:36.800]   larger, or smarter, or eat this or eat that.
[00:48:36.800 --> 00:48:40.600]   So there are many ways to solve the same problem of survival.
[00:48:40.600 --> 00:48:43.120]   And that then breeds creativity.
[00:48:43.840 --> 00:48:46.760]   And it allows more exploration.
[00:48:46.760 --> 00:48:48.720]   And eventually you get solutions
[00:48:48.720 --> 00:48:51.160]   that are perhaps more creative,
[00:48:51.160 --> 00:48:54.120]   rather than trying to go from initial population directly,
[00:48:54.120 --> 00:48:57.400]   or more or less directly to your maximum fitness,
[00:48:57.400 --> 00:49:00.840]   which you measure as just one metric.
[00:49:00.840 --> 00:49:03.840]   - So in a broad sense,
[00:49:03.840 --> 00:49:06.360]   before we talk about neuroevolution,
[00:49:06.360 --> 00:49:11.200]   do you see evolutionary computation
[00:49:11.200 --> 00:49:14.160]   as more effective than deep learning in certain contexts?
[00:49:14.160 --> 00:49:16.640]   Machine learning, broadly speaking.
[00:49:16.640 --> 00:49:18.680]   Maybe even supervised machine learning.
[00:49:18.680 --> 00:49:21.000]   I don't know if you want to draw any kind of lines
[00:49:21.000 --> 00:49:23.760]   and distinctions and borders where they rub up
[00:49:23.760 --> 00:49:25.400]   against each other kind of thing,
[00:49:25.400 --> 00:49:27.000]   or one is more effective than the other
[00:49:27.000 --> 00:49:28.440]   in the current state of things.
[00:49:28.440 --> 00:49:30.240]   - Yes, of course, they are very different
[00:49:30.240 --> 00:49:32.280]   and they address different kinds of problems.
[00:49:32.280 --> 00:49:36.720]   And the deep learning has been really successful
[00:49:36.720 --> 00:49:38.720]   in domains where we have a lot of data.
[00:49:39.800 --> 00:49:42.440]   And that means not just data about situations,
[00:49:42.440 --> 00:49:45.120]   but also what the right answers were.
[00:49:45.120 --> 00:49:47.840]   So labeled examples, or there might be predictions,
[00:49:47.840 --> 00:49:48.840]   might be weather prediction
[00:49:48.840 --> 00:49:51.720]   where the data itself becomes labeled.
[00:49:51.720 --> 00:49:53.160]   What happened, what the weather was today,
[00:49:53.160 --> 00:49:55.480]   and what it will be tomorrow.
[00:49:55.480 --> 00:49:58.360]   So they are very effective,
[00:49:58.360 --> 00:50:01.400]   deep learning methods on that kind of tasks.
[00:50:01.400 --> 00:50:03.400]   But there are other kinds of tasks
[00:50:03.400 --> 00:50:06.360]   where we don't really know what the right answer is.
[00:50:06.360 --> 00:50:07.520]   Game playing, for instance,
[00:50:07.520 --> 00:50:12.520]   but many robotics tasks and actions in the world,
[00:50:12.520 --> 00:50:17.680]   decision-making, and actual practical applications
[00:50:17.680 --> 00:50:19.440]   like treatments and healthcare,
[00:50:19.440 --> 00:50:21.360]   or investment in stock market.
[00:50:21.360 --> 00:50:22.680]   Many tasks are like that.
[00:50:22.680 --> 00:50:24.840]   We don't know and we'll never know
[00:50:24.840 --> 00:50:26.640]   what the optimal answers were.
[00:50:26.640 --> 00:50:28.600]   And there you need different kinds of approach.
[00:50:28.600 --> 00:50:30.840]   Reinforcement learning is one of those.
[00:50:30.840 --> 00:50:33.760]   Reinforcement learning comes from biology as well.
[00:50:33.760 --> 00:50:35.400]   Agents learn during their lifetime.
[00:50:35.400 --> 00:50:37.560]   They buries and sometimes they get sick
[00:50:37.560 --> 00:50:40.280]   and then they don't and get stronger.
[00:50:40.280 --> 00:50:42.280]   And then that's how you learn.
[00:50:42.280 --> 00:50:46.040]   And evolution is also a mechanism like that,
[00:50:46.040 --> 00:50:48.920]   but a different timescale because you have a population.
[00:50:48.920 --> 00:50:50.840]   Not an individual during his lifetime,
[00:50:50.840 --> 00:50:55.200]   but an entire population as a whole can discover what works.
[00:50:55.200 --> 00:50:58.960]   And there you can afford individuals that don't work out.
[00:50:58.960 --> 00:51:02.080]   They learn, everybody dies and you have a next generation
[00:51:02.080 --> 00:51:04.120]   and it will be better than the previous one.
[00:51:04.120 --> 00:51:07.640]   So that's the big difference between these methods.
[00:51:07.640 --> 00:51:09.840]   They apply to different kinds of problems.
[00:51:09.840 --> 00:51:15.120]   And in particular, there's often a comparison
[00:51:15.120 --> 00:51:16.640]   that's kind of interesting and important
[00:51:16.640 --> 00:51:20.080]   between reinforcement learning and evolutionary computation.
[00:51:20.080 --> 00:51:23.400]   And initially, reinforcement learning
[00:51:23.400 --> 00:51:25.960]   was about individual learning during their lifetime.
[00:51:25.960 --> 00:51:28.160]   And evolution is more engineering.
[00:51:28.160 --> 00:51:29.720]   You don't care about the lifetime.
[00:51:29.720 --> 00:51:32.600]   You don't care about all the individuals that are tested.
[00:51:32.600 --> 00:51:34.520]   You only care about the final result.
[00:51:34.520 --> 00:51:39.080]   The last one, the best candidate that evolution produced.
[00:51:39.080 --> 00:51:40.520]   And that sense, they also apply
[00:51:40.520 --> 00:51:42.520]   to different kinds of problems.
[00:51:42.520 --> 00:51:46.160]   And another boundary starting to blur a bit.
[00:51:46.160 --> 00:51:48.680]   You can use evolution as an online method
[00:51:48.680 --> 00:51:51.520]   and reinforcement learning to create engineering solutions,
[00:51:51.520 --> 00:51:55.280]   but that's still roughly the distinction.
[00:51:55.280 --> 00:52:00.280]   And from the point of view, what algorithm you wanna use,
[00:52:00.280 --> 00:52:02.280]   if you have something where there is a cost
[00:52:02.280 --> 00:52:06.080]   for every trial, reinforcement learning might be your choice.
[00:52:06.080 --> 00:52:07.760]   Now, if you have a domain
[00:52:07.760 --> 00:52:10.240]   where you can use a surrogate perhaps,
[00:52:10.240 --> 00:52:13.600]   so you don't have much of a cost for trial,
[00:52:13.600 --> 00:52:16.480]   and you want to have surprises,
[00:52:16.480 --> 00:52:18.640]   you want to explore more broadly,
[00:52:18.640 --> 00:52:23.360]   then this population-based method is perhaps a better choice
[00:52:23.360 --> 00:52:26.960]   because you can try things out that you wouldn't afford
[00:52:26.960 --> 00:52:28.560]   when you're doing reinforcement learning.
[00:52:28.560 --> 00:52:31.680]   - There's very few things as entertaining
[00:52:31.680 --> 00:52:33.760]   as watching either evolution computation
[00:52:33.760 --> 00:52:36.600]   or reinforcement learning teaching a simulated robot
[00:52:36.600 --> 00:52:37.440]   to walk.
[00:52:37.440 --> 00:52:42.400]   Maybe there's a higher level question
[00:52:42.400 --> 00:52:43.640]   that could be asked here,
[00:52:43.640 --> 00:52:47.560]   but do you find this whole space of applications
[00:52:47.560 --> 00:52:51.720]   in the robotics interesting for evolution computation?
[00:52:51.720 --> 00:52:53.520]   - Yeah, yeah, very much.
[00:52:53.520 --> 00:52:56.480]   And indeed, there are fascinating videos of that.
[00:52:56.480 --> 00:52:58.360]   And that's actually one of the examples
[00:52:58.360 --> 00:53:00.560]   where you can contrast the difference.
[00:53:00.560 --> 00:53:03.200]   - Between reinforcement learning and evolution.
[00:53:03.200 --> 00:53:06.320]   - Yes, so if you have a reinforcement learning agent,
[00:53:06.320 --> 00:53:08.000]   it tries to be conservative
[00:53:08.000 --> 00:53:11.880]   because it wants to walk as long as possible and be stable.
[00:53:11.880 --> 00:53:13.720]   But if you have evolutionary computation,
[00:53:13.720 --> 00:53:17.280]   it can afford these agents that go haywire.
[00:53:17.280 --> 00:53:19.200]   They fall flat on their face,
[00:53:19.200 --> 00:53:21.640]   and they take a step, and then they jump,
[00:53:21.640 --> 00:53:23.240]   and then again fall flat.
[00:53:23.240 --> 00:53:25.280]   And eventually what comes out of that
[00:53:25.280 --> 00:53:28.280]   is something like a falling that's controlled.
[00:53:28.280 --> 00:53:30.440]   You take another step, another step,
[00:53:30.440 --> 00:53:32.320]   and you no longer fall.
[00:53:32.320 --> 00:53:34.200]   Instead, you run, you go fast.
[00:53:34.200 --> 00:53:36.200]   So that's a way of discovering
[00:53:36.200 --> 00:53:38.400]   something that's hard to discover step by step,
[00:53:38.400 --> 00:53:40.480]   incrementally, because you can afford
[00:53:40.480 --> 00:53:43.680]   these evolutionist dead ends,
[00:53:43.680 --> 00:53:45.520]   although they are not entirely dead ends
[00:53:45.520 --> 00:53:47.760]   in the sense that they can serve as stepping stones.
[00:53:47.760 --> 00:53:49.880]   When you take two of those, put them together,
[00:53:49.880 --> 00:53:52.440]   you get something that works even better.
[00:53:52.440 --> 00:53:55.920]   And that is a great example of this kind of discovery.
[00:53:55.920 --> 00:53:58.160]   - Yeah, learning to walk is fascinating.
[00:53:58.160 --> 00:53:59.920]   I talk quite a bit to Russ Tedrake
[00:53:59.920 --> 00:54:03.120]   'cause at MIT, there's a community of folks
[00:54:03.120 --> 00:54:06.600]   who just, roboticists, who love the elegance
[00:54:06.600 --> 00:54:09.760]   and beauty of movement.
[00:54:09.760 --> 00:54:14.760]   And walking, bipedal robotics, is beautiful,
[00:54:14.760 --> 00:54:19.440]   but also exceptionally dangerous
[00:54:19.440 --> 00:54:22.400]   in the sense that you're constantly falling,
[00:54:22.400 --> 00:54:25.360]   essentially, if you want to do elegant movement.
[00:54:25.360 --> 00:54:28.400]   And the discovery of that is,
[00:54:28.920 --> 00:54:31.080]   (sighs)
[00:54:31.080 --> 00:54:33.800]   I mean, it's such a good example
[00:54:33.800 --> 00:54:37.520]   of that the discovery of a good solution
[00:54:37.520 --> 00:54:39.760]   sometimes requires a leap of faith and patience
[00:54:39.760 --> 00:54:41.520]   and all those kinds of things.
[00:54:41.520 --> 00:54:43.600]   I wonder what other spaces where you had
[00:54:43.600 --> 00:54:45.400]   to discover those kinds of things in.
[00:54:45.400 --> 00:54:47.520]   - Yeah, yeah.
[00:54:47.520 --> 00:54:51.320]   Another interesting direction is learning
[00:54:51.320 --> 00:54:56.360]   for virtual creatures, learning to walk.
[00:54:56.360 --> 00:54:59.920]   We did a study in simulation, obviously,
[00:54:59.920 --> 00:55:02.640]   that you create those creatures,
[00:55:02.640 --> 00:55:05.240]   not just their controller, but also their body.
[00:55:05.240 --> 00:55:07.880]   So you have cylinders, you have muscles,
[00:55:07.880 --> 00:55:11.160]   you have joints and sensors,
[00:55:11.160 --> 00:55:14.080]   and you're creating creatures that look quite different.
[00:55:14.080 --> 00:55:15.440]   Some of them have multiple legs,
[00:55:15.440 --> 00:55:17.640]   some of them have no legs at all.
[00:55:17.640 --> 00:55:20.080]   And then the goal was to get them to move,
[00:55:20.080 --> 00:55:21.960]   to walk, to run.
[00:55:21.960 --> 00:55:25.440]   And what was interesting is that when you evolve
[00:55:25.440 --> 00:55:28.400]   the controller together with the body,
[00:55:28.400 --> 00:55:30.480]   you get movements that look natural
[00:55:30.480 --> 00:55:33.600]   because they're optimized for that physical setup.
[00:55:33.600 --> 00:55:36.160]   And these creatures, you start believing them,
[00:55:36.160 --> 00:55:38.080]   that they're alive because they walk in a way
[00:55:38.080 --> 00:55:39.560]   that you would expect somebody
[00:55:39.560 --> 00:55:41.920]   with that kind of a setup to walk.
[00:55:41.920 --> 00:55:45.760]   - Yeah, there's something subjective also about that.
[00:55:45.760 --> 00:55:47.120]   I've been thinking a lot about that,
[00:55:47.120 --> 00:55:52.120]   especially in the human-robot interaction context.
[00:55:53.800 --> 00:55:57.400]   I mentioned Spot, the Boston Dynamics robot.
[00:55:57.400 --> 00:56:01.420]   There is something about human-robot communication.
[00:56:01.420 --> 00:56:03.380]   Let's say, let's put it in another context,
[00:56:03.380 --> 00:56:07.940]   something about human and dog context,
[00:56:07.940 --> 00:56:10.120]   like a living dog,
[00:56:10.120 --> 00:56:13.360]   where there's a dance of communication.
[00:56:13.360 --> 00:56:15.520]   First of all, the eyes, you both look at the same thing
[00:56:15.520 --> 00:56:18.080]   and dogs communicate with their eyes as well.
[00:56:18.080 --> 00:56:23.080]   Like if you and a dog want to deal with a person,
[00:56:23.200 --> 00:56:24.560]   deal with a particular object,
[00:56:24.560 --> 00:56:26.200]   you will look at the person,
[00:56:26.200 --> 00:56:28.080]   the dog will look at you and then look at the object
[00:56:28.080 --> 00:56:30.320]   and look back at you, all those kinds of things.
[00:56:30.320 --> 00:56:33.240]   But there's also just the elegance of movement.
[00:56:33.240 --> 00:56:35.840]   I mean, there's the, of course, the tail
[00:56:35.840 --> 00:56:38.080]   and all those kinds of mechanisms of communication.
[00:56:38.080 --> 00:56:41.880]   It all seems natural and often joyful.
[00:56:41.880 --> 00:56:44.360]   And for robots to communicate that
[00:56:44.360 --> 00:56:47.240]   is really difficult how to figure that out
[00:56:47.240 --> 00:56:50.800]   because it almost seems impossible to hard-code in.
[00:56:50.800 --> 00:56:53.800]   You can hard-code it for a demo purpose,
[00:56:53.800 --> 00:56:58.120]   something like that, but it's essentially choreographed.
[00:56:58.120 --> 00:57:00.300]   Like if you watch some of the Boston Dynamics videos
[00:57:00.300 --> 00:57:01.760]   where they're dancing,
[00:57:01.760 --> 00:57:05.640]   all of that is choreographed by human beings.
[00:57:05.640 --> 00:57:09.380]   But to learn how to, with your movement,
[00:57:09.380 --> 00:57:14.380]   demonstrate a naturalness, an elegance, that's fascinating.
[00:57:14.380 --> 00:57:15.720]   Of course, in the physical space,
[00:57:15.720 --> 00:57:16.840]   that's very difficult to do,
[00:57:16.840 --> 00:57:20.100]   to learn the kind of scale that you're referring to,
[00:57:20.100 --> 00:57:23.080]   but the hope is that you could do that in simulation
[00:57:23.080 --> 00:57:25.360]   and then transfer it into the physical space
[00:57:25.360 --> 00:57:28.680]   if you're able to model the robots efficiently, naturally.
[00:57:28.680 --> 00:57:31.680]   - Yeah, and sometimes I think that it requires
[00:57:31.680 --> 00:57:35.000]   a theory of mind on the side of the robot
[00:57:35.000 --> 00:57:38.920]   that they understand what you're doing
[00:57:38.920 --> 00:57:41.440]   because they themselves are doing something similar.
[00:57:41.440 --> 00:57:44.360]   And that's a big question too.
[00:57:44.360 --> 00:57:47.440]   We talked about intelligence in general
[00:57:47.440 --> 00:57:50.040]   and the social aspect of intelligence
[00:57:50.040 --> 00:57:52.040]   and I think that's what is required,
[00:57:52.040 --> 00:57:53.840]   that we humans understand other humans
[00:57:53.840 --> 00:57:57.040]   because we assume that they are similar to us.
[00:57:57.040 --> 00:57:59.120]   We have one simulation we did a while ago,
[00:57:59.120 --> 00:58:01.440]   Ken Stanley did that.
[00:58:01.440 --> 00:58:06.440]   Two robots that were competing, simulation, like I said,
[00:58:06.440 --> 00:58:09.320]   they were foraging for food to gain energy.
[00:58:09.320 --> 00:58:10.680]   And then when they were really strong,
[00:58:10.680 --> 00:58:12.680]   they would bounce into the other robot
[00:58:12.680 --> 00:58:14.860]   and win if they were stronger.
[00:58:14.860 --> 00:58:17.320]   And we watched evolution discover
[00:58:17.320 --> 00:58:18.920]   more and more complex behaviors.
[00:58:18.920 --> 00:58:21.040]   They first went to the nearest food
[00:58:21.040 --> 00:58:24.320]   and then they started to plot a trajectory
[00:58:24.320 --> 00:58:28.440]   so they get more, but then they started to pay attention
[00:58:28.440 --> 00:58:30.320]   what the other robot was doing.
[00:58:30.320 --> 00:58:32.720]   And in the end, there was a behavior
[00:58:32.720 --> 00:58:35.840]   where one of the robots, the more sophisticated one,
[00:58:35.840 --> 00:58:40.200]   sensed where the food pieces were
[00:58:40.200 --> 00:58:42.080]   and identified that the other robot
[00:58:42.080 --> 00:58:46.000]   was close to two of a very far distance
[00:58:46.000 --> 00:58:48.720]   and there was one more food nearby.
[00:58:48.720 --> 00:58:53.400]   So it faked, now I'm using anthropomorphized terms,
[00:58:53.400 --> 00:58:55.900]   but it made a move towards those other pieces
[00:58:55.900 --> 00:58:59.080]   in order for the other robot to actually go and get them.
[00:58:59.080 --> 00:59:02.400]   Because it knew that the last remaining piece of food
[00:59:02.400 --> 00:59:04.960]   was close and the other robot would have to travel
[00:59:04.960 --> 00:59:06.960]   a long way, lose its energy,
[00:59:06.960 --> 00:59:10.440]   and then lose the whole competition.
[00:59:10.440 --> 00:59:12.200]   So there was like an emergence
[00:59:12.200 --> 00:59:13.640]   of something like a theory of mind,
[00:59:13.640 --> 00:59:15.560]   knowing what the other robot would do
[00:59:16.640 --> 00:59:19.520]   to guide it towards bad behavior in order to win.
[00:59:19.520 --> 00:59:21.800]   So we can get things like that happen
[00:59:21.800 --> 00:59:23.040]   in simulation as well.
[00:59:23.040 --> 00:59:25.360]   - But that's a complete natural emergence
[00:59:25.360 --> 00:59:26.180]   of a theory of mind.
[00:59:26.180 --> 00:59:29.480]   But I feel like if you add a little bit
[00:59:29.480 --> 00:59:34.480]   of a place for a theory of mind to emerge easier
[00:59:34.480 --> 00:59:37.240]   then you can go really far.
[00:59:37.240 --> 00:59:39.760]   I mean, some of these things with evolution,
[00:59:39.760 --> 00:59:44.000]   you add a little bit of design in there,
[00:59:44.000 --> 00:59:45.560]   it'll really help.
[00:59:45.560 --> 00:59:50.560]   And I tend to think that a very simple theory of mind
[00:59:50.560 --> 00:59:54.880]   will go a really long way for cooperation between agents
[00:59:54.880 --> 00:59:57.520]   and certainly for human-robot interaction.
[00:59:57.520 --> 00:59:59.760]   Like it doesn't have to be super complicated.
[00:59:59.760 --> 01:00:03.520]   I've gotten a chance in the autonomous vehicle space
[01:00:03.520 --> 01:00:07.040]   to watch vehicles interact with pedestrians
[01:00:07.040 --> 01:00:09.920]   or pedestrians interacting with vehicles in general.
[01:00:09.920 --> 01:00:13.000]   I mean, you would think that there's a very complicated
[01:00:13.000 --> 01:00:14.520]   theory of mind thing going on,
[01:00:14.520 --> 01:00:17.000]   but I have a sense, it's not well understood yet,
[01:00:17.000 --> 01:00:19.480]   but I have a sense it's pretty dumb.
[01:00:19.480 --> 01:00:21.060]   Like it's pretty simple.
[01:00:21.060 --> 01:00:25.560]   There's a social contract there where between humans,
[01:00:25.560 --> 01:00:28.160]   a human driver and a human crossing the road
[01:00:28.160 --> 01:00:32.000]   where the human crossing the road trusts
[01:00:32.000 --> 01:00:34.600]   that the human in the car is not going to murder them.
[01:00:34.600 --> 01:00:36.000]   And there's something about,
[01:00:36.000 --> 01:00:38.220]   again, back to that mortality thing,
[01:00:38.220 --> 01:00:43.220]   there's some dance of ethics and morality
[01:00:44.000 --> 01:00:47.560]   that's built in that you're mapping your own morality
[01:00:47.560 --> 01:00:50.040]   onto the person in the car.
[01:00:50.040 --> 01:00:54.080]   And even if they're driving at a speed where you think
[01:00:54.080 --> 01:00:56.200]   if they don't stop, they're going to kill you,
[01:00:56.200 --> 01:00:58.140]   you trust that if you step in front of them,
[01:00:58.140 --> 01:00:59.480]   they're going to hit the brakes.
[01:00:59.480 --> 01:01:02.180]   And there's that weird dance that we do
[01:01:02.180 --> 01:01:04.680]   that I think is a pretty simple model,
[01:01:04.680 --> 01:01:06.840]   but of course it's very difficult
[01:01:06.840 --> 01:01:08.520]   to introspect what it is.
[01:01:08.520 --> 01:01:12.040]   And autonomous robots in the human-robot interaction context
[01:01:12.040 --> 01:01:13.800]   have to build that.
[01:01:13.800 --> 01:01:17.320]   Current robots are much less than what you're describing.
[01:01:17.320 --> 01:01:19.360]   They're currently just afraid of everything.
[01:01:19.360 --> 01:01:24.080]   They're not the kind that fall and discover how to run.
[01:01:24.080 --> 01:01:26.800]   They're more like, please don't touch anything,
[01:01:26.800 --> 01:01:29.000]   don't hurt anything, stay as far away
[01:01:29.000 --> 01:01:30.200]   from humans as possible.
[01:01:30.200 --> 01:01:34.120]   Treat humans as ballistic objects that you can't,
[01:01:34.120 --> 01:01:38.760]   that you do with a large spatial envelope,
[01:01:38.760 --> 01:01:40.800]   make sure you do not collide with.
[01:01:40.800 --> 01:01:43.440]   - That's how like you mentioned Elon Musk
[01:01:43.440 --> 01:01:45.360]   thinks about autonomous vehicles.
[01:01:45.360 --> 01:01:47.680]   I tend to think autonomous vehicles
[01:01:47.680 --> 01:01:50.640]   need to have a beautiful dance between human and machine,
[01:01:50.640 --> 01:01:53.320]   where it's not just the collision avoidance problem,
[01:01:53.320 --> 01:01:55.920]   but a weird dance.
[01:01:55.920 --> 01:02:00.000]   - Yeah, I think these systems need to be able to predict
[01:02:00.000 --> 01:02:02.320]   what will happen, what the other agent is going to do,
[01:02:02.320 --> 01:02:06.440]   and then have a structure of what the goals are
[01:02:06.440 --> 01:02:08.440]   and whether those predictions actually meet the goals.
[01:02:08.440 --> 01:02:10.880]   And you can go probably pretty far
[01:02:10.880 --> 01:02:13.600]   with that relatively simple setup already.
[01:02:13.600 --> 01:02:15.080]   But to call it a theory of mind,
[01:02:15.080 --> 01:02:16.200]   I don't think you need to.
[01:02:16.200 --> 01:02:19.280]   I mean, it doesn't matter whether the pedestrian has a mind,
[01:02:19.280 --> 01:02:21.840]   it's an object and we can predict what we will do.
[01:02:21.840 --> 01:02:23.720]   And then we can predict what the states will be
[01:02:23.720 --> 01:02:26.160]   in the future and whether they are desirable states.
[01:02:26.160 --> 01:02:27.960]   Stay away from those that are undesirable
[01:02:27.960 --> 01:02:29.720]   and go towards those that are desirable.
[01:02:29.720 --> 01:02:34.520]   So it's a relatively simple, functional approach to that.
[01:02:34.520 --> 01:02:37.040]   Where do we really need the theory of mind?
[01:02:37.920 --> 01:02:40.960]   - Maybe when you start interacting
[01:02:40.960 --> 01:02:44.160]   and you're trying to get the other agent to do something
[01:02:44.160 --> 01:02:46.480]   and jointly, so that you can jointly,
[01:02:46.480 --> 01:02:48.400]   collaboratively achieve something,
[01:02:48.400 --> 01:02:50.560]   then it becomes more complex.
[01:02:50.560 --> 01:02:51.880]   - Well, I mean, even with the pedestrians,
[01:02:51.880 --> 01:02:54.800]   you have to have a sense of where their attention,
[01:02:54.800 --> 01:02:57.840]   actual attention in terms of their gaze is,
[01:02:57.840 --> 01:03:00.760]   but also like, there's this vision science people
[01:03:00.760 --> 01:03:01.600]   talk about this all the time.
[01:03:01.600 --> 01:03:02.800]   Just because I'm looking at it
[01:03:02.800 --> 01:03:04.680]   doesn't mean I'm paying attention to it.
[01:03:04.680 --> 01:03:07.400]   So figuring out what is the person looking at,
[01:03:07.400 --> 01:03:09.840]   what is the sensory information they've taken in?
[01:03:09.840 --> 01:03:12.500]   And the theory of mind piece comes in is,
[01:03:12.500 --> 01:03:16.480]   what are they actually attending to cognitively?
[01:03:16.480 --> 01:03:19.000]   And also, what are they thinking about?
[01:03:19.000 --> 01:03:21.200]   Like, what is the computation they're performing?
[01:03:21.200 --> 01:03:24.340]   And you have probably maybe a few options,
[01:03:24.340 --> 01:03:28.260]   for the pedestrian crossing.
[01:03:28.260 --> 01:03:30.180]   It doesn't have to be, it's like a variable
[01:03:30.180 --> 01:03:31.800]   with a few discrete states,
[01:03:31.800 --> 01:03:33.280]   but you have to have a good estimation
[01:03:33.280 --> 01:03:35.480]   of which of the states that brain is in
[01:03:35.480 --> 01:03:36.600]   for the pedestrian case.
[01:03:36.600 --> 01:03:39.240]   And the same is for attending with a robot.
[01:03:39.240 --> 01:03:41.980]   If you're collaborating to pick up an object,
[01:03:41.980 --> 01:03:44.700]   you have to figure out, is the human,
[01:03:44.700 --> 01:03:47.600]   like, there's a few discrete states
[01:03:47.600 --> 01:03:48.520]   that the human could be in,
[01:03:48.520 --> 01:03:52.200]   and you have to predict that by observing the human.
[01:03:52.200 --> 01:03:53.960]   And that seems like a machine learning problem
[01:03:53.960 --> 01:03:58.960]   to figure out what's the human up to.
[01:03:58.960 --> 01:04:02.160]   It's not as simple as sort of planning,
[01:04:02.160 --> 01:04:03.920]   just because they move their arm
[01:04:03.920 --> 01:04:06.840]   means the arm will continue moving in this direction.
[01:04:06.840 --> 01:04:08.560]   You have to really have a model
[01:04:08.560 --> 01:04:09.880]   of what they're thinking about,
[01:04:09.880 --> 01:04:12.560]   and what's the motivation behind the movement of the arm.
[01:04:12.560 --> 01:04:13.880]   - Here we are talking about
[01:04:13.880 --> 01:04:16.560]   relatively simple physical actions,
[01:04:16.560 --> 01:04:19.280]   but you can take that to higher levels also,
[01:04:19.280 --> 01:04:21.760]   like to predict what the people are going to do,
[01:04:21.760 --> 01:04:26.080]   you need to know what their goals are,
[01:04:26.080 --> 01:04:28.000]   what are they trying to, are they exercising?
[01:04:28.000 --> 01:04:29.440]   Are they just trying to get somewhere?
[01:04:29.440 --> 01:04:30.880]   But even higher level, I mean,
[01:04:30.880 --> 01:04:33.900]   you are predicting what people will do in their career.
[01:04:33.900 --> 01:04:35.120]   What their life themes are.
[01:04:35.120 --> 01:04:37.840]   Do they want to be famous, rich, or do good?
[01:04:37.840 --> 01:04:40.560]   And that takes a lot more information,
[01:04:40.560 --> 01:04:43.360]   but it allows you to then predict their actions,
[01:04:43.360 --> 01:04:44.800]   what choices they might make.
[01:04:44.800 --> 01:04:48.600]   - So how does evolution and computation
[01:04:48.600 --> 01:04:50.800]   apply to the world of neural networks?
[01:04:50.800 --> 01:04:52.500]   'Cause I've seen quite a bit of work
[01:04:52.500 --> 01:04:55.520]   from you and others in the world of neuroevolution.
[01:04:55.520 --> 01:04:58.600]   So maybe first, can you say, what is this field?
[01:04:58.600 --> 01:05:00.840]   - Yeah, neuroevolution is a combination
[01:05:01.140 --> 01:05:04.320]   of neural networks and evolutionary computation
[01:05:04.320 --> 01:05:05.480]   in many different forms,
[01:05:05.480 --> 01:05:10.480]   but the early versions were simply using evolution
[01:05:10.480 --> 01:05:13.960]   as a way to construct a neural network
[01:05:13.960 --> 01:05:17.240]   instead of say, stochastic gradient descent
[01:05:17.240 --> 01:05:18.380]   or back propagation.
[01:05:18.380 --> 01:05:22.460]   Because evolution can evolve these parameters,
[01:05:22.460 --> 01:05:23.980]   weight values in a neural network,
[01:05:23.980 --> 01:05:27.180]   just like any other string of numbers, you can do that.
[01:05:27.180 --> 01:05:30.760]   And that's useful because some cases you don't have
[01:05:30.760 --> 01:05:34.800]   those targets that you need to back propagate from.
[01:05:34.800 --> 01:05:36.900]   And it might be an agent that's running a maze
[01:05:36.900 --> 01:05:39.800]   or a robot playing a game or something.
[01:05:39.800 --> 01:05:42.080]   You don't, again, you don't know what the right answer is,
[01:05:42.080 --> 01:05:43.080]   you don't have backprop,
[01:05:43.080 --> 01:05:45.860]   but this way you can still evolve a neural net.
[01:05:45.860 --> 01:05:49.000]   And neural networks are really good at these tasks
[01:05:49.000 --> 01:05:51.400]   because they recognize patterns
[01:05:51.400 --> 01:05:55.280]   and they generalize, interpolate between known situations.
[01:05:55.280 --> 01:05:57.720]   So you want to have a neural network in such a task,
[01:05:57.720 --> 01:06:00.480]   even if you don't have the supervised targets.
[01:06:00.480 --> 01:06:02.560]   So that's a reason and that's a solution.
[01:06:02.560 --> 01:06:04.200]   And also more recently now,
[01:06:04.200 --> 01:06:06.920]   when we have all this deep learning literature,
[01:06:06.920 --> 01:06:08.880]   it turns out that we can use evolution
[01:06:08.880 --> 01:06:12.480]   to optimize many aspects of those designs.
[01:06:12.480 --> 01:06:16.400]   The deep learning architectures have become so complex
[01:06:16.400 --> 01:06:18.760]   that there's little hope for us little humans
[01:06:18.760 --> 01:06:20.160]   to understand their complexity
[01:06:20.160 --> 01:06:22.880]   and what actually makes a good design.
[01:06:22.880 --> 01:06:25.840]   And now we can use evolution to give that design for you.
[01:06:25.840 --> 01:06:29.800]   And it might mean optimizing hyperparameters,
[01:06:29.800 --> 01:06:31.840]   like the depth of layers and so on,
[01:06:31.840 --> 01:06:34.680]   or the topology of the network,
[01:06:34.680 --> 01:06:36.560]   how many layers, how they're connected,
[01:06:36.560 --> 01:06:39.000]   but also other aspects like what activation functions
[01:06:39.000 --> 01:06:42.040]   you use where in the network during the learning process,
[01:06:42.040 --> 01:06:43.800]   or what loss function you use,
[01:06:43.800 --> 01:06:46.520]   you could generate that.
[01:06:46.520 --> 01:06:47.680]   Even data augmentation,
[01:06:47.680 --> 01:06:50.040]   all the different aspects of the design
[01:06:50.040 --> 01:06:53.840]   of deep learning experiments could be optimized that way.
[01:06:53.840 --> 01:06:57.000]   So that's an interaction between two mechanisms.
[01:06:57.000 --> 01:07:00.880]   But there's also, when we get more into cognitive science
[01:07:00.880 --> 01:07:02.640]   and the topics that we've been talking about,
[01:07:02.640 --> 01:07:06.200]   you could have learning mechanisms at two level timescales.
[01:07:06.200 --> 01:07:08.000]   So you do have an evolution
[01:07:08.000 --> 01:07:10.680]   that gives you baby neural networks
[01:07:10.680 --> 01:07:12.960]   that then learn during their lifetime.
[01:07:12.960 --> 01:07:15.960]   And you have this interaction of two timescales.
[01:07:15.960 --> 01:07:19.400]   And I think that can potentially be really powerful.
[01:07:19.400 --> 01:07:23.520]   Now in biology, we are not born with all our faculties.
[01:07:23.520 --> 01:07:25.480]   We have to learn, we have a developmental period.
[01:07:25.480 --> 01:07:27.360]   In humans, it's really long.
[01:07:27.360 --> 01:07:29.400]   And most animals have something.
[01:07:29.400 --> 01:07:32.200]   And probably the reason is that evolution,
[01:07:32.200 --> 01:07:34.840]   a DNA is not detailed enough
[01:07:34.840 --> 01:07:36.760]   or plentiful enough to describe them.
[01:07:36.760 --> 01:07:38.880]   We can't describe how to set the brain up.
[01:07:38.880 --> 01:07:44.920]   But we can, evolution can decide on a starting point
[01:07:44.920 --> 01:07:47.160]   and then have a learning algorithm
[01:07:47.160 --> 01:07:50.000]   that will construct the final product.
[01:07:50.000 --> 01:07:53.320]   And this interaction of intelligent,
[01:07:54.200 --> 01:07:57.720]   well, evolution that has produced a good starting point
[01:07:57.720 --> 01:08:00.800]   for the specific purpose of learning from it
[01:08:00.800 --> 01:08:03.240]   with the interaction of, with the environment.
[01:08:03.240 --> 01:08:04.720]   That can be a really powerful mechanism
[01:08:04.720 --> 01:08:08.040]   for constructing brains and constructing behaviors.
[01:08:08.040 --> 01:08:10.040]   - I like how you walk back from intelligence.
[01:08:10.040 --> 01:08:12.400]   So optimize starting point, maybe.
[01:08:12.400 --> 01:08:18.520]   Okay, there's a lot of fascinating things to ask here.
[01:08:18.520 --> 01:08:22.080]   And this is basically this dance between neural networks
[01:08:22.080 --> 01:08:23.880]   and evolution and computation.
[01:08:23.880 --> 01:08:26.240]   Could go into the category of automated machine learning
[01:08:26.240 --> 01:08:28.840]   to where you're optimizing,
[01:08:28.840 --> 01:08:31.040]   whether it's hyperparameters of the topology
[01:08:31.040 --> 01:08:33.560]   or hyperparameters taken broadly.
[01:08:33.560 --> 01:08:36.400]   But the topology thing is really interesting.
[01:08:36.400 --> 01:08:40.240]   I mean, that's not really done that effectively
[01:08:40.240 --> 01:08:41.920]   or throughout the history of machine learning
[01:08:41.920 --> 01:08:43.280]   has not been done.
[01:08:43.280 --> 01:08:45.020]   Usually there's a fixed architecture.
[01:08:45.020 --> 01:08:47.280]   Maybe there's a few components you're playing with.
[01:08:47.280 --> 01:08:50.120]   But to grow a neural network, essentially,
[01:08:50.120 --> 01:08:51.720]   the way you grow in that organism
[01:08:51.720 --> 01:08:52.960]   is really fascinating space.
[01:08:52.960 --> 01:08:57.960]   How hard is it, do you think, to grow a neural network?
[01:08:57.960 --> 01:09:00.880]   And maybe what kind of neural networks
[01:09:00.880 --> 01:09:04.680]   are more amenable to this kind of idea than others?
[01:09:04.680 --> 01:09:06.960]   I've seen quite a bit of work on recurrent neural networks.
[01:09:06.960 --> 01:09:10.920]   Is there some architectures that are friendlier than others?
[01:09:10.920 --> 01:09:15.280]   And is this just a fun, small scale set of experiments
[01:09:15.280 --> 01:09:18.760]   or do you have hope that we can be able to grow
[01:09:18.760 --> 01:09:20.280]   powerful neural networks?
[01:09:20.280 --> 01:09:21.760]   - I think we can.
[01:09:21.760 --> 01:09:24.840]   And most of the work up to now
[01:09:24.840 --> 01:09:27.080]   is taking architectures that already exist,
[01:09:27.080 --> 01:09:28.760]   that humans have designed,
[01:09:28.760 --> 01:09:30.880]   and try to optimize them further.
[01:09:30.880 --> 01:09:32.840]   And you can totally do that.
[01:09:32.840 --> 01:09:34.280]   A few years ago, we did an experiment.
[01:09:34.280 --> 01:09:38.480]   We took a winner of the image captioning competition
[01:09:38.480 --> 01:09:42.620]   and the architecture, and just broke it into pieces
[01:09:42.620 --> 01:09:45.480]   and took the pieces, and that was our search base.
[01:09:45.480 --> 01:09:46.680]   See if you can do better.
[01:09:46.680 --> 01:09:49.280]   And we indeed could, 15% better performance
[01:09:49.280 --> 01:09:52.760]   by just searching around the network design
[01:09:52.760 --> 01:09:54.000]   that humans had come up with,
[01:09:54.000 --> 01:09:55.880]   Oreo vinyls and others.
[01:09:55.880 --> 01:10:00.840]   But that's starting from a point that humans have produced.
[01:10:00.840 --> 01:10:03.480]   But we could do something more general.
[01:10:03.480 --> 01:10:05.840]   It doesn't have to be that kind of network.
[01:10:05.840 --> 01:10:08.840]   The hard part is, there are a couple of challenges.
[01:10:08.840 --> 01:10:10.760]   One of them is to define the search base.
[01:10:10.760 --> 01:10:14.680]   What are your elements and how you put them together?
[01:10:14.680 --> 01:10:18.960]   And the space is just really, really big.
[01:10:18.960 --> 01:10:21.040]   So you have to somehow constrain it
[01:10:21.040 --> 01:10:23.360]   and have some hunch of what will work,
[01:10:23.360 --> 01:10:25.840]   because otherwise everything is possible.
[01:10:25.840 --> 01:10:28.560]   And another challenge is that in order to evaluate
[01:10:28.560 --> 01:10:32.280]   how good your design is, you have to train it.
[01:10:32.280 --> 01:10:35.000]   I mean, you have to actually try it out.
[01:10:35.000 --> 01:10:37.320]   And that's currently very expensive, right?
[01:10:37.320 --> 01:10:40.400]   I mean, deep learning networks may take days to train.
[01:10:40.400 --> 01:10:42.280]   Well, imagine you having a population of 100
[01:10:42.280 --> 01:10:44.680]   and have to run it for 100 generations.
[01:10:44.680 --> 01:10:48.080]   It's not yet quite feasible computationally.
[01:10:48.080 --> 01:10:51.640]   It will be, but also there's a large carbon footprint
[01:10:51.640 --> 01:10:52.480]   and all that.
[01:10:52.480 --> 01:10:54.360]   I mean, we are using a lot of computation for doing it.
[01:10:54.360 --> 01:10:57.560]   So intelligent methods and intelligent,
[01:10:57.560 --> 01:11:01.680]   I mean, we have to do some science in order to figure out
[01:11:01.680 --> 01:11:05.320]   what the right representations are and right operators are,
[01:11:05.320 --> 01:11:07.360]   and how do we evaluate them
[01:11:07.360 --> 01:11:09.240]   without having to fully train them?
[01:11:09.240 --> 01:11:11.440]   And that is where the current research is
[01:11:11.440 --> 01:11:13.720]   and we're making progress on all those fronts.
[01:11:13.720 --> 01:11:17.920]   So yes, there are certain architectures
[01:11:17.920 --> 01:11:20.960]   that are more amenable to that approach.
[01:11:20.960 --> 01:11:23.600]   But also I think we can create our own architecture
[01:11:23.600 --> 01:11:26.280]   and all representations that are even better at that.
[01:11:26.280 --> 01:11:28.840]   - And do you think it's possible to do like
[01:11:28.840 --> 01:11:31.640]   a tiny baby network that grows into something
[01:11:31.640 --> 01:11:32.760]   that can do state-of-the-art
[01:11:32.760 --> 01:11:35.440]   and like even the simple data set like MNIST,
[01:11:35.440 --> 01:11:39.960]   and just like it just grows into a gigantic monster
[01:11:39.960 --> 01:11:42.520]   that's the world's greatest handwriting recognition system?
[01:11:42.520 --> 01:11:44.400]   - Yeah, there are approaches like that.
[01:11:44.400 --> 01:11:46.040]   Esteban Real and Cochlear, for instance,
[01:11:46.040 --> 01:11:48.560]   that worked on evolving a smaller network
[01:11:48.560 --> 01:11:52.000]   and then systematically expanding it to a larger one.
[01:11:52.000 --> 01:11:55.040]   Your elements are already there and scaling it up
[01:11:55.040 --> 01:11:56.600]   will just give you more power.
[01:11:56.600 --> 01:11:59.400]   So again, evolution gives you that starting point
[01:11:59.400 --> 01:12:02.880]   and then there's a mechanism that gives you the final result
[01:12:02.880 --> 01:12:04.600]   and a very powerful approach.
[01:12:04.600 --> 01:12:11.000]   But you could also simulate the actual growth process.
[01:12:11.000 --> 01:12:15.360]   And like I said before, evolving a starting point
[01:12:15.360 --> 01:12:18.440]   and then evolving or training the network.
[01:12:18.440 --> 01:12:21.960]   There's not that much work that's been done on that yet.
[01:12:21.960 --> 01:12:24.680]   We need some kind of a simulation environment
[01:12:24.680 --> 01:12:27.440]   so that interactions at will,
[01:12:27.440 --> 01:12:29.520]   the supervised environment doesn't really,
[01:12:29.520 --> 01:12:33.080]   it's not as easily usable here.
[01:12:33.080 --> 01:12:35.560]   - Sorry, the interaction between neural networks?
[01:12:35.560 --> 01:12:37.320]   - Yeah, the neural networks that you're creating,
[01:12:37.320 --> 01:12:42.120]   interacting the world and learning from these sequences
[01:12:42.120 --> 01:12:44.760]   of interactions, perhaps communication with others.
[01:12:44.760 --> 01:12:46.920]   (laughing)
[01:12:46.920 --> 01:12:47.760]   - That's awesome.
[01:12:47.760 --> 01:12:48.920]   - We would like to get there,
[01:12:48.920 --> 01:12:51.640]   but just the task of simulating something
[01:12:51.640 --> 01:12:53.280]   at that level is very hard.
[01:12:53.280 --> 01:12:54.120]   - It's very difficult.
[01:12:54.120 --> 01:12:55.400]   I love the idea.
[01:12:55.400 --> 01:12:58.760]   I mean, one of the powerful things about evolution on Earth
[01:12:58.760 --> 01:13:01.320]   is the predators and prey emerged.
[01:13:01.320 --> 01:13:03.560]   And like, there's just like,
[01:13:03.560 --> 01:13:05.360]   there's bigger fish and smaller fish
[01:13:05.360 --> 01:13:07.080]   and it's fascinating to think
[01:13:07.080 --> 01:13:08.360]   that you could have neural networks
[01:13:08.360 --> 01:13:09.480]   competing against each other
[01:13:09.480 --> 01:13:12.280]   and one neural network being able to destroy another one.
[01:13:12.280 --> 01:13:14.280]   There's like wars of neural networks
[01:13:14.280 --> 01:13:16.560]   competing to solve the MNIST problem.
[01:13:16.560 --> 01:13:17.400]   I don't know.
[01:13:17.400 --> 01:13:18.240]   - Yeah, yeah.
[01:13:18.240 --> 01:13:19.280]   Oh, totally, yeah, yeah, yeah.
[01:13:19.280 --> 01:13:22.720]   And we actually simulated also that prey
[01:13:22.720 --> 01:13:24.960]   and it was interesting what happened there.
[01:13:24.960 --> 01:13:26.920]   But I mean, Rajak Pallan did this
[01:13:26.920 --> 01:13:29.600]   and Kay Holcomb was a zoologist.
[01:13:29.600 --> 01:13:31.080]   So we had, again,
[01:13:31.080 --> 01:13:37.480]   we had simulated hyenas and simulated zebras.
[01:13:37.480 --> 01:13:38.320]   - Nice.
[01:13:38.320 --> 01:13:40.480]   - And initially, you know,
[01:13:40.480 --> 01:13:42.880]   the hyenas just tried to hunt them.
[01:13:42.880 --> 01:13:45.400]   And when they actually stumbled upon the zebra,
[01:13:45.400 --> 01:13:47.760]   they ate it and were happy.
[01:13:47.760 --> 01:13:51.800]   And then the zebras learned to escape
[01:13:51.800 --> 01:13:54.360]   and the hyenas learned to team up.
[01:13:54.360 --> 01:13:55.720]   And actually two of them approached
[01:13:55.720 --> 01:13:56.960]   in different directions.
[01:13:56.960 --> 01:13:59.040]   And now the zebras, their next step,
[01:13:59.040 --> 01:14:01.880]   they generated a behavior
[01:14:01.880 --> 01:14:03.920]   where they split in different directions,
[01:14:03.920 --> 01:14:06.360]   just like actually gazelles do
[01:14:07.360 --> 01:14:08.400]   when they are being hunted.
[01:14:08.400 --> 01:14:09.600]   They confuse the predator
[01:14:09.600 --> 01:14:10.920]   by going in different directions.
[01:14:10.920 --> 01:14:11.880]   That emerged.
[01:14:11.880 --> 01:14:14.360]   And then more hyenas joined
[01:14:14.360 --> 01:14:15.880]   and kind of circled them.
[01:14:15.880 --> 01:14:18.840]   And then when they circled them,
[01:14:18.840 --> 01:14:21.040]   they could actually herd the zebras together
[01:14:21.040 --> 01:14:23.520]   and eat multiple zebras.
[01:14:23.520 --> 01:14:26.560]   So there was like an arms race
[01:14:26.560 --> 01:14:28.360]   of predators and prey.
[01:14:28.360 --> 01:14:31.000]   And they gradually developed more complex behaviors,
[01:14:31.000 --> 01:14:33.880]   some of which we actually do see in nature.
[01:14:33.880 --> 01:14:36.840]   And this kind of co-evolution,
[01:14:36.840 --> 01:14:38.080]   that's competitive co-evolution,
[01:14:38.080 --> 01:14:39.600]   it's a fascinating topic
[01:14:39.600 --> 01:14:42.960]   because there's a promise or possibility
[01:14:42.960 --> 01:14:45.600]   that you will discover something new
[01:14:45.600 --> 01:14:46.480]   that you don't already know.
[01:14:46.480 --> 01:14:48.160]   You didn't build it in.
[01:14:48.160 --> 01:14:50.760]   It came from this arms race.
[01:14:50.760 --> 01:14:52.560]   It's hard to keep the arms race going.
[01:14:52.560 --> 01:14:55.400]   It's hard to have rich enough simulation
[01:14:55.400 --> 01:14:58.320]   that supports all of these complex behaviors.
[01:14:58.320 --> 01:15:00.080]   But at least for several steps,
[01:15:00.080 --> 01:15:03.680]   we've already seen it in this predator-prey scenario.
[01:15:03.680 --> 01:15:06.360]   - First of all, it's fascinating to think about this context
[01:15:06.360 --> 01:15:09.640]   in terms of evolving architectures.
[01:15:09.640 --> 01:15:12.800]   So I've studied Tesla Autopilot for a long time.
[01:15:12.800 --> 01:15:16.680]   It's one particular implementation
[01:15:16.680 --> 01:15:18.920]   of an AI system that's operating in the real world.
[01:15:18.920 --> 01:15:21.000]   I find it fascinating because of the scale
[01:15:21.000 --> 01:15:23.400]   at which it's used out in the real world.
[01:15:23.400 --> 01:15:26.240]   And I'm not sure if you're familiar with that system much,
[01:15:26.240 --> 01:15:28.600]   but Andrej Karpathy leads that team
[01:15:28.600 --> 01:15:30.120]   on the machine learning side.
[01:15:30.120 --> 01:15:34.880]   And there's a multi-task network, multi-headed network
[01:15:34.880 --> 01:15:38.920]   where there's a core, but it's trained on particular tasks
[01:15:38.920 --> 01:15:40.280]   and there's a bunch of different heads
[01:15:40.280 --> 01:15:41.760]   that are trained on that.
[01:15:41.760 --> 01:15:46.240]   Is there some lessons from evolutionary computation
[01:15:46.240 --> 01:15:48.360]   or neuroevolution that could be applied
[01:15:48.360 --> 01:15:50.960]   to this kind of multi-headed beast
[01:15:50.960 --> 01:15:52.440]   that's operating in the real world?
[01:15:52.440 --> 01:15:55.680]   - Yes, it's a very good problem for neuroevolution.
[01:15:55.680 --> 01:15:59.480]   And the reason is that when you have multiple tasks,
[01:16:00.320 --> 01:16:01.600]   they support each other.
[01:16:01.600 --> 01:16:07.480]   So let's say you're learning to classify X-ray images
[01:16:07.480 --> 01:16:09.120]   to different pathologies.
[01:16:09.120 --> 01:16:13.440]   So you have one task is to classify this disease
[01:16:13.440 --> 01:16:15.520]   and another one, this disease, another one, this one.
[01:16:15.520 --> 01:16:18.040]   And when you're learning from one disease,
[01:16:18.040 --> 01:16:21.280]   that forces certain kinds of internal representations
[01:16:21.280 --> 01:16:24.480]   and embeddings, and they can serve
[01:16:24.480 --> 01:16:27.240]   as a helpful starting point for the other tasks.
[01:16:27.240 --> 01:16:30.560]   So you are combining the wisdom of multiple tasks
[01:16:30.560 --> 01:16:32.000]   into these representations.
[01:16:32.000 --> 01:16:33.920]   And it turns out that you can do better
[01:16:33.920 --> 01:16:36.120]   in each of these tasks when you are learning
[01:16:36.120 --> 01:16:39.440]   simultaneously other tasks than you would by one task alone.
[01:16:39.440 --> 01:16:41.320]   - Which is a fascinating idea in itself, yeah.
[01:16:41.320 --> 01:16:43.440]   - Yes, and people do that all the time.
[01:16:43.440 --> 01:16:45.640]   I mean, you use knowledge of domains that you know
[01:16:45.640 --> 01:16:49.320]   in new domains, and certainly neural networks can do that.
[01:16:49.320 --> 01:16:51.880]   Where neuroevolution comes in is that
[01:16:51.880 --> 01:16:54.760]   what's the best way to combine these tasks?
[01:16:54.760 --> 01:16:57.760]   Now there's architectural design that allow you to decide
[01:16:57.760 --> 01:17:01.000]   where and how the embeddings,
[01:17:01.000 --> 01:17:02.920]   the internal representations are combined
[01:17:02.920 --> 01:17:05.560]   and how much you combine them.
[01:17:05.560 --> 01:17:07.600]   And there's quite a bit of research on that.
[01:17:07.600 --> 01:17:11.640]   And my team, Elliot Mayerson's worked on that in particular,
[01:17:11.640 --> 01:17:14.440]   like what is a good internal representation
[01:17:14.440 --> 01:17:16.720]   that supports multiple tasks?
[01:17:16.720 --> 01:17:20.160]   And we're getting to understand how that's constructed
[01:17:20.160 --> 01:17:23.680]   and what's in it so that it is in a space
[01:17:23.680 --> 01:17:26.800]   that supports multiple different heads, like you said.
[01:17:26.800 --> 01:17:31.360]   And that, I think, is fundamentally
[01:17:31.360 --> 01:17:33.960]   how biological intelligence works as well.
[01:17:33.960 --> 01:17:37.600]   You don't build a representation just for one task.
[01:17:37.600 --> 01:17:39.680]   You try to build something that's general,
[01:17:39.680 --> 01:17:42.320]   not only so that you can do better in one task
[01:17:42.320 --> 01:17:44.640]   or multiple tasks, but also future tasks
[01:17:44.640 --> 01:17:45.920]   and future challenges.
[01:17:45.920 --> 01:17:49.760]   So you learn the structure of the world,
[01:17:50.200 --> 01:17:54.040]   and that helps you in all kinds of future challenges.
[01:17:54.040 --> 01:17:56.120]   - And so you're trying to design a representation
[01:17:56.120 --> 01:17:58.440]   that will support an arbitrary set of tasks
[01:17:58.440 --> 01:18:01.040]   in a particular sort of class of problem.
[01:18:01.040 --> 01:18:03.120]   - Yeah, and also it turns out,
[01:18:03.120 --> 01:18:06.000]   and that's, again, a surprise that Elliot found,
[01:18:06.000 --> 01:18:09.800]   was that those tasks don't have to be very related.
[01:18:09.800 --> 01:18:14.120]   You can learn to do better vision by learning language
[01:18:14.120 --> 01:18:17.120]   or better language by learning about DNA structure.
[01:18:17.920 --> 01:18:20.040]   - No, somehow the world-- - What?
[01:18:20.040 --> 01:18:21.160]   (laughing)
[01:18:21.160 --> 01:18:22.680]   Yeah. - It rhymes.
[01:18:22.680 --> 01:18:28.240]   - The world rhymes, even if it's very disparate fields.
[01:18:28.240 --> 01:18:31.440]   I mean, on that small topic, let me ask you,
[01:18:31.440 --> 01:18:34.340]   'cause you've also, on the competition neuroscience side,
[01:18:34.340 --> 01:18:38.240]   you worked on both language and vision.
[01:18:38.240 --> 01:18:44.440]   What's the connection between the two?
[01:18:44.440 --> 01:18:46.840]   What's more, maybe there's a bunch of ways to ask this,
[01:18:46.840 --> 01:18:48.600]   but what's more difficult to build
[01:18:48.600 --> 01:18:50.600]   from an engineering perspective
[01:18:50.600 --> 01:18:52.320]   and evolutionary perspective,
[01:18:52.320 --> 01:18:56.080]   the human language system or the human vision system,
[01:18:56.080 --> 01:19:00.600]   or the equivalent of, in the AI space, language and vision,
[01:19:00.600 --> 01:19:03.640]   or is it the best, is the multitask idea
[01:19:03.640 --> 01:19:04.680]   that you're speaking to,
[01:19:04.680 --> 01:19:07.440]   that they need to be deeply integrated?
[01:19:07.440 --> 01:19:09.980]   - Yeah, absolutely the latter.
[01:19:09.980 --> 01:19:11.640]   Learning both at the same time,
[01:19:11.640 --> 01:19:15.200]   I think, is a fascinating direction in the future.
[01:19:15.200 --> 01:19:17.480]   So you have datasets where there's visual component
[01:19:17.480 --> 01:19:20.040]   as well as verbal descriptions, for instance,
[01:19:20.040 --> 01:19:22.760]   and that way you can learn a deeper representation,
[01:19:22.760 --> 01:19:25.160]   a more useful representation for both.
[01:19:25.160 --> 01:19:27.160]   But it's still an interesting question of
[01:19:27.160 --> 01:19:29.480]   which one is easier.
[01:19:29.480 --> 01:19:33.200]   I mean, recognizing objects or even understanding sentences,
[01:19:33.200 --> 01:19:35.800]   that's relatively possible,
[01:19:35.800 --> 01:19:37.860]   but where it becomes, where the challenges are
[01:19:37.860 --> 01:19:39.820]   is to understand the world.
[01:19:39.820 --> 01:19:42.320]   Like the visual world, the 3D,
[01:19:42.560 --> 01:19:43.640]   what are the objects doing
[01:19:43.640 --> 01:19:46.800]   and predicting what will happen, the relationships.
[01:19:46.800 --> 01:19:48.240]   That's what makes vision difficult.
[01:19:48.240 --> 01:19:51.560]   And language, obviously, it's what is being said,
[01:19:51.560 --> 01:19:52.760]   what the meaning is.
[01:19:52.760 --> 01:19:57.340]   And the meaning doesn't stop at who did what to whom.
[01:19:57.340 --> 01:19:59.780]   There are goals and plans and themes,
[01:19:59.780 --> 01:20:01.760]   and eventually you have to understand
[01:20:01.760 --> 01:20:04.720]   the entire human society and history
[01:20:04.720 --> 01:20:08.200]   in order to understand a sentence very much fully.
[01:20:08.200 --> 01:20:09.480]   There are plenty of examples
[01:20:09.480 --> 01:20:10.880]   of those kinds of short sentences
[01:20:10.880 --> 01:20:14.320]   when you bring in all the world knowledge to understand it.
[01:20:14.320 --> 01:20:15.920]   And that's the big challenge.
[01:20:15.920 --> 01:20:17.360]   Now, we are far from that,
[01:20:17.360 --> 01:20:20.640]   but even just bringing in the visual world
[01:20:20.640 --> 01:20:22.240]   together with the sentence
[01:20:22.240 --> 01:20:25.640]   will give you already a lot deeper understanding
[01:20:25.640 --> 01:20:26.880]   of what's happening.
[01:20:26.880 --> 01:20:29.760]   And I think that that's where we're going very soon.
[01:20:29.760 --> 01:20:33.000]   I mean, we've had ImageNet for a long time,
[01:20:33.000 --> 01:20:35.960]   and now we have all these text collections,
[01:20:35.960 --> 01:20:38.280]   but having both together
[01:20:38.280 --> 01:20:41.600]   and then learning a semantic understanding
[01:20:41.600 --> 01:20:42.800]   of what is happening,
[01:20:42.800 --> 01:20:45.400]   I think that will be the next step in the next few years.
[01:20:45.400 --> 01:20:46.360]   - Yeah, you're starting to see that
[01:20:46.360 --> 01:20:48.000]   with all the work with Transformers,
[01:20:48.000 --> 01:20:51.920]   was the AI community started to dip their toe
[01:20:51.920 --> 01:20:56.920]   into this idea of having language models
[01:20:56.920 --> 01:21:01.360]   that are now doing stuff with images, with vision,
[01:21:01.360 --> 01:21:03.920]   and then connecting the two.
[01:21:03.920 --> 01:21:05.920]   I mean, right now it's like these little explorations,
[01:21:05.920 --> 01:21:07.800]   we're literally dipping the toe in.
[01:21:07.800 --> 01:21:11.800]   But maybe at some point we'll just dive into the pool
[01:21:11.800 --> 01:21:13.880]   and it'll just be all seen as the same thing.
[01:21:13.880 --> 01:21:16.880]   I do still wonder what's more fundamental,
[01:21:16.880 --> 01:21:17.960]   whether vision is,
[01:21:17.960 --> 01:21:23.320]   whether we don't think about vision correctly.
[01:21:23.320 --> 01:21:24.720]   Maybe the fact, 'cause we're humans
[01:21:24.720 --> 01:21:26.720]   and we see things as beautiful and so on,
[01:21:26.720 --> 01:21:31.040]   and because we have cameras that take in pixels
[01:21:31.040 --> 01:21:33.280]   as a 2D image,
[01:21:33.280 --> 01:21:37.720]   that we don't sufficiently think about vision as language.
[01:21:38.720 --> 01:21:41.640]   Maybe Chomsky is right all along,
[01:21:41.640 --> 01:21:43.760]   that vision is fundamental to,
[01:21:43.760 --> 01:21:46.760]   sorry, that language is fundamental to everything,
[01:21:46.760 --> 01:21:49.280]   to even cognition, to even consciousness.
[01:21:49.280 --> 01:21:51.400]   The base layer is all language,
[01:21:51.400 --> 01:21:53.040]   not necessarily like English,
[01:21:53.040 --> 01:21:57.200]   but some weird abstract representation,
[01:21:57.200 --> 01:21:59.360]   the linguistic representation.
[01:21:59.360 --> 01:22:02.600]   - Yeah, well, earlier we talked about the social structures
[01:22:02.600 --> 01:22:05.400]   and that may be what's underlying the language,
[01:22:05.400 --> 01:22:06.720]   and that's the more fundamental part,
[01:22:06.720 --> 01:22:08.720]   and then language has been added on top of that.
[01:22:08.720 --> 01:22:11.160]   - Language emerges from the social interaction.
[01:22:11.160 --> 01:22:13.040]   - Yeah, that's a very good guess.
[01:22:13.040 --> 01:22:15.280]   We are visual animals, though.
[01:22:15.280 --> 01:22:17.560]   A lot of the brain is dedicated to vision,
[01:22:17.560 --> 01:22:22.560]   and also when we think about various abstract concepts,
[01:22:22.560 --> 01:22:27.760]   we usually reduce that to vision and images.
[01:22:27.760 --> 01:22:29.760]   And that's, you know, we go to a whiteboard,
[01:22:29.760 --> 01:22:33.160]   you draw pictures of very abstract concepts.
[01:22:33.160 --> 01:22:35.880]   So we tend to resort to that quite a bit,
[01:22:35.880 --> 01:22:37.520]   and that's a fundamental representation.
[01:22:37.520 --> 01:22:41.800]   It's probably possible that it predated language even.
[01:22:41.800 --> 01:22:43.960]   I mean, animals, a lot of, they don't talk,
[01:22:43.960 --> 01:22:45.840]   but they certainly do have vision.
[01:22:45.840 --> 01:22:51.720]   And language is interesting development from mastication,
[01:22:51.720 --> 01:22:55.320]   from eating, you develop an organ
[01:22:55.320 --> 01:22:57.880]   that actually can produce sound to manipulate them.
[01:22:57.880 --> 01:22:59.840]   Maybe that was an accident,
[01:22:59.840 --> 01:23:01.520]   maybe that was something that was available
[01:23:01.520 --> 01:23:05.680]   and then allowed us to do the communication,
[01:23:05.680 --> 01:23:07.480]   or maybe it was gestures.
[01:23:07.480 --> 01:23:10.680]   Sign language could have been the original proto-language.
[01:23:10.680 --> 01:23:13.960]   We don't quite know, but the language is more fundamental
[01:23:13.960 --> 01:23:17.360]   than the medium in which it's communicated.
[01:23:17.360 --> 01:23:19.960]   And I think that it comes from those representations.
[01:23:19.960 --> 01:23:26.600]   Now, in current world, they are so strongly integrated,
[01:23:26.600 --> 01:23:28.840]   it's really hard to say which one is fundamental.
[01:23:28.840 --> 01:23:32.840]   You look at the brain structures and even visual cortex,
[01:23:32.840 --> 01:23:35.200]   which is supposed to be very much just vision.
[01:23:35.200 --> 01:23:38.040]   Well, if you are thinking of semantic concepts,
[01:23:38.040 --> 01:23:41.560]   if you're thinking of language, visual cortex lights up.
[01:23:41.560 --> 01:23:44.560]   It's still useful, even for language computations.
[01:23:44.560 --> 01:23:47.200]   So there are common structures underlying them.
[01:23:47.200 --> 01:23:49.040]   So utilize what you need.
[01:23:49.040 --> 01:23:51.560]   And when you are understanding a scene,
[01:23:51.560 --> 01:23:53.200]   you're understanding relationships,
[01:23:53.200 --> 01:23:55.440]   well, that's not so far from understanding relationships
[01:23:55.440 --> 01:23:56.880]   between words and concepts.
[01:23:56.880 --> 01:23:59.160]   So I think that that's how they are integrated.
[01:23:59.160 --> 01:24:02.400]   - Yeah, and there's dreams, and once we close our eyes,
[01:24:02.400 --> 01:24:04.480]   there's still a world in there somehow operating
[01:24:04.480 --> 01:24:07.720]   and somehow possibly the visual system
[01:24:07.720 --> 01:24:09.920]   somehow integrate into all of it.
[01:24:09.920 --> 01:24:12.960]   I tend to enjoy thinking about aliens
[01:24:12.960 --> 01:24:17.400]   and thinking about the sad thing to me
[01:24:17.400 --> 01:24:21.080]   about extraterrestrial intelligent life,
[01:24:21.080 --> 01:24:24.840]   that if it visited us here on earth,
[01:24:24.840 --> 01:24:29.120]   or if we came on Mars or maybe in other solar system,
[01:24:29.120 --> 01:24:30.960]   another galaxy one day,
[01:24:30.960 --> 01:24:34.920]   that us humans would not be able to detect it
[01:24:34.920 --> 01:24:37.100]   or communicate with it or appreciate,
[01:24:37.100 --> 01:24:38.760]   like it'd be right in front of our nose
[01:24:38.760 --> 01:24:43.380]   and we're too self-obsessed to see it.
[01:24:43.380 --> 01:24:48.380]   Not self-obsessed, but our tools,
[01:24:48.380 --> 01:24:52.560]   our frameworks of thinking would not detect it
[01:24:52.560 --> 01:24:55.120]   as a good movie "Arrival" and so on,
[01:24:55.120 --> 01:24:56.760]   where Stephen Wolfram and his son,
[01:24:56.760 --> 01:24:59.360]   I think were part of developing this alien language
[01:24:59.360 --> 01:25:01.560]   of how aliens would communicate with humans.
[01:25:01.560 --> 01:25:02.960]   Do you ever think about that kind of stuff
[01:25:02.960 --> 01:25:06.840]   where if humans and aliens would be able
[01:25:06.840 --> 01:25:08.800]   to communicate with each other,
[01:25:08.800 --> 01:25:11.440]   like if we met each other at some,
[01:25:11.440 --> 01:25:12.520]   okay, we could do SETI,
[01:25:12.520 --> 01:25:16.000]   which is communicating from across a very big distance,
[01:25:16.000 --> 01:25:17.600]   but also just us, you know,
[01:25:17.600 --> 01:25:21.320]   if you did a podcast with an alien,
[01:25:21.320 --> 01:25:25.440]   do you think we'd be able to find a common language
[01:25:25.440 --> 01:25:28.440]   and a common methodology of communication?
[01:25:28.440 --> 01:25:30.880]   I think from a computational perspective,
[01:25:30.880 --> 01:25:32.440]   the way to ask that is you have
[01:25:32.440 --> 01:25:34.320]   very fundamentally different creatures,
[01:25:34.320 --> 01:25:35.440]   agents that are created,
[01:25:35.440 --> 01:25:38.480]   would they be able to find a common language?
[01:25:38.480 --> 01:25:41.000]   - Yes, I do think about that.
[01:25:41.000 --> 01:25:43.000]   I mean, I think a lot of people who are in computing,
[01:25:43.000 --> 01:25:45.360]   they, and AI in particular,
[01:25:45.360 --> 01:25:47.320]   they got into it because they were fascinated
[01:25:47.320 --> 01:25:50.740]   with science fiction and all of these options.
[01:25:50.740 --> 01:25:54.040]   I mean, Star Trek generated all kinds of devices
[01:25:54.040 --> 01:25:56.560]   that we have now, they envisioned it first,
[01:25:56.560 --> 01:26:00.700]   and it's a great motivator to think about things like that.
[01:26:00.700 --> 01:26:06.880]   And I, so one, and again, being a computational scientist
[01:26:06.880 --> 01:26:11.280]   and trying to build intelligent agents,
[01:26:11.280 --> 01:26:14.520]   what I would like to do is have a simulation
[01:26:14.520 --> 01:26:18.400]   where the agents actually evolve communication,
[01:26:18.400 --> 01:26:19.960]   not just communication, we've done that,
[01:26:19.960 --> 01:26:21.280]   people have done that many times,
[01:26:21.280 --> 01:26:23.840]   that they communicate, they signal and so on,
[01:26:23.840 --> 01:26:26.080]   but actually develop a language,
[01:26:26.080 --> 01:26:27.200]   and language means grammar,
[01:26:27.200 --> 01:26:28.880]   it means all this social structures
[01:26:28.880 --> 01:26:31.860]   and on top of that, grammatical structures.
[01:26:31.860 --> 01:26:36.000]   And we do it under various conditions
[01:26:36.000 --> 01:26:37.780]   and actually try to identify what conditions
[01:26:37.780 --> 01:26:40.880]   are necessary for it to come out.
[01:26:40.880 --> 01:26:44.240]   And then we can start asking that kind of questions.
[01:26:44.240 --> 01:26:46.280]   Are those languages that emerge
[01:26:46.280 --> 01:26:48.880]   in those different simulated environments,
[01:26:48.880 --> 01:26:50.800]   are they understandable to us?
[01:26:50.800 --> 01:26:53.560]   Can we somehow make a translation?
[01:26:53.560 --> 01:26:55.920]   We can make it a concrete question.
[01:26:55.920 --> 01:26:59.000]   - So machine translation of evolved languages,
[01:26:59.000 --> 01:27:02.040]   and so like languages that evolve come up with,
[01:27:02.040 --> 01:27:04.960]   can we translate, like I have a Google Translate
[01:27:04.960 --> 01:27:07.200]   for the evolved languages.
[01:27:07.200 --> 01:27:09.800]   - Yes, and if we do that enough,
[01:27:09.800 --> 01:27:14.120]   we have perhaps an idea what an alien language
[01:27:14.120 --> 01:27:17.200]   might be like, the space of where those languages can be.
[01:27:17.200 --> 01:27:20.000]   'Cause we can set up their environment differently.
[01:27:20.000 --> 01:27:22.040]   It doesn't need to be gravity.
[01:27:22.040 --> 01:27:24.880]   You can have all kinds of, societies can be different,
[01:27:24.880 --> 01:27:26.320]   they may have no predators,
[01:27:26.320 --> 01:27:28.520]   they may have all, everybody's a predator,
[01:27:28.520 --> 01:27:30.120]   all kinds of situations.
[01:27:30.120 --> 01:27:32.880]   And then see what the space possibly is
[01:27:32.880 --> 01:27:35.920]   where those languages are and what the difficulties are.
[01:27:35.920 --> 01:27:37.680]   That'd be really good actually to do that
[01:27:37.680 --> 01:27:39.520]   before the aliens come here.
[01:27:39.520 --> 01:27:40.920]   - Yes, it's good practice.
[01:27:40.920 --> 01:27:44.140]   On the similar connection,
[01:27:44.140 --> 01:27:48.240]   you can think of AI systems as aliens.
[01:27:48.240 --> 01:27:51.920]   Is there a ways to evolve a communication scheme for,
[01:27:51.920 --> 01:27:55.080]   there's a field you can call it like explainable AI,
[01:27:55.080 --> 01:27:59.000]   for AI systems to be able to communicate.
[01:27:59.000 --> 01:28:01.640]   So you evolve a bunch of agents,
[01:28:01.640 --> 01:28:05.440]   but for some of them to be able to talk to you also.
[01:28:05.440 --> 01:28:08.520]   So to evolve a way for agents to be able to communicate
[01:28:08.520 --> 01:28:11.080]   about their world to us humans.
[01:28:11.080 --> 01:28:13.440]   Do you think that there's possible mechanisms
[01:28:13.440 --> 01:28:14.760]   for doing that?
[01:28:14.760 --> 01:28:16.280]   - We can certainly try.
[01:28:16.280 --> 01:28:20.600]   And if it's an evolution competition system,
[01:28:20.600 --> 01:28:22.640]   for instance, you reward those solutions
[01:28:22.640 --> 01:28:24.160]   that are actually functional,
[01:28:24.160 --> 01:28:25.640]   that that communication makes sense,
[01:28:25.640 --> 01:28:29.500]   it allows us to together again, achieve common goals.
[01:28:29.500 --> 01:28:30.940]   I think that's possible.
[01:28:30.940 --> 01:28:35.160]   But even from that paper that you mentioned,
[01:28:35.160 --> 01:28:37.880]   the anecdotes, it's quite likely also
[01:28:37.880 --> 01:28:42.880]   that the agents learn to lie and fake
[01:28:42.880 --> 01:28:45.360]   and do all kinds of things like that.
[01:28:45.360 --> 01:28:47.760]   I mean, we see that in even very low level,
[01:28:47.760 --> 01:28:51.800]   like bacterial evolution, they are cheaters.
[01:28:51.800 --> 01:28:53.920]   And who's to say that what they say
[01:28:53.920 --> 01:28:55.400]   is actually what they think.
[01:28:55.400 --> 01:28:59.400]   But that's one thing that there would have to be
[01:28:59.400 --> 01:29:01.920]   some common goal so that we can evaluate
[01:29:01.920 --> 01:29:04.220]   whether that communication is at least useful.
[01:29:04.220 --> 01:29:09.040]   They may be saying things just to make us feel good
[01:29:09.040 --> 01:29:10.680]   or get us to do what we want,
[01:29:10.680 --> 01:29:12.440]   but it would not turn them off or something.
[01:29:12.440 --> 01:29:15.160]   But so we would have to understand
[01:29:15.160 --> 01:29:16.760]   their internal representations much better
[01:29:16.760 --> 01:29:20.120]   to really make sure that that translation is critical.
[01:29:20.120 --> 01:29:21.400]   But it can be useful.
[01:29:21.400 --> 01:29:24.000]   And I think it's possible to do that.
[01:29:24.000 --> 01:29:27.680]   There are examples where visualizations
[01:29:27.680 --> 01:29:30.000]   are automatically created
[01:29:30.000 --> 01:29:33.600]   so that we can look into the system
[01:29:33.600 --> 01:29:35.880]   and that language is not that far from it.
[01:29:35.880 --> 01:29:37.440]   I mean, it is a way of communicating
[01:29:37.440 --> 01:29:41.480]   and logging what you're doing in some interpretable way.
[01:29:41.480 --> 01:29:45.480]   I think a fascinating topic, yeah, to do that.
[01:29:46.600 --> 01:29:47.720]   - You're making me realize
[01:29:47.720 --> 01:29:51.080]   that it's a good scientific question
[01:29:51.080 --> 01:29:54.440]   whether lying is an effective mechanism
[01:29:54.440 --> 01:29:57.320]   for integrating yourself and succeeding in a social network
[01:29:57.320 --> 01:30:00.360]   in a world that is social.
[01:30:00.360 --> 01:30:04.540]   I tend to believe that honesty and love
[01:30:04.540 --> 01:30:09.540]   are evolutionary advantages in an environment
[01:30:09.540 --> 01:30:12.620]   where there's a network of intelligent agents.
[01:30:12.620 --> 01:30:14.840]   But it's also very possible that dishonesty
[01:30:14.840 --> 01:30:19.840]   and manipulation and even violence,
[01:30:19.840 --> 01:30:23.100]   all those kinds of things might be more beneficial.
[01:30:23.100 --> 01:30:25.900]   That's the old open question about good versus evil.
[01:30:25.900 --> 01:30:29.240]   But I tend to, I mean, I don't know if it's a hopeful,
[01:30:29.240 --> 01:30:34.240]   maybe I'm delusional, but it feels like karma is a thing,
[01:30:34.240 --> 01:30:39.560]   which is like long-term the agents
[01:30:39.560 --> 01:30:41.000]   that are just kind to others,
[01:30:41.000 --> 01:30:43.780]   sometimes for no reason, will do better.
[01:30:43.780 --> 01:30:47.560]   In a society that's not highly constrained on resources.
[01:30:47.560 --> 01:30:50.520]   So like people start getting weird and evil
[01:30:50.520 --> 01:30:51.880]   towards each other and bad
[01:30:51.880 --> 01:30:54.200]   when the resources are very low
[01:30:54.200 --> 01:30:56.960]   relative to the needs of the populace,
[01:30:56.960 --> 01:30:58.320]   especially at the basic level,
[01:30:58.320 --> 01:31:02.640]   like survival, shelter, food, all those kinds of things.
[01:31:02.640 --> 01:31:06.200]   But I tend to believe
[01:31:06.200 --> 01:31:09.200]   that once you have those things established,
[01:31:09.200 --> 01:31:12.680]   then, well, not to believe, I guess I hope
[01:31:12.680 --> 01:31:14.900]   that AI systems will be honest.
[01:31:14.900 --> 01:31:19.900]   But it's scary to think about the Turing test.
[01:31:19.900 --> 01:31:23.940]   AI systems that will eventually pass the Turing test
[01:31:23.940 --> 01:31:26.740]   will be ones that are exceptionally good at lying.
[01:31:26.740 --> 01:31:28.380]   That's a terrifying concept.
[01:31:28.380 --> 01:31:31.260]   I mean, I don't know.
[01:31:31.260 --> 01:31:34.220]   First of all, so from somebody who studied language
[01:31:34.220 --> 01:31:37.860]   and obviously are not just a world expert in AI,
[01:31:37.860 --> 01:31:41.540]   but somebody who dreams about the future of the field,
[01:31:41.540 --> 01:31:45.640]   do you hope, do you think there'll be human level
[01:31:45.640 --> 01:31:48.700]   or superhuman level intelligences in the future
[01:31:48.700 --> 01:31:51.240]   that we eventually build?
[01:31:51.240 --> 01:31:56.180]   - Well, I definitely hope that we can get there.
[01:31:56.180 --> 01:31:59.240]   One, I think, important perspective
[01:31:59.240 --> 01:32:02.240]   is that we are building AI to help us.
[01:32:02.240 --> 01:32:04.640]   That it is a tool like cars
[01:32:04.640 --> 01:32:09.340]   or language or communication.
[01:32:10.280 --> 01:32:12.760]   AI will help us be more productive.
[01:32:12.760 --> 01:32:17.560]   And that is always a condition.
[01:32:17.560 --> 01:32:20.340]   It's not something that we build and let run
[01:32:20.340 --> 01:32:22.500]   and it becomes an entity of its own
[01:32:22.500 --> 01:32:23.840]   that doesn't care about us.
[01:32:23.840 --> 01:32:27.360]   Now, of course, really far into the future,
[01:32:27.360 --> 01:32:28.780]   maybe that might be possible,
[01:32:28.780 --> 01:32:32.200]   but not in the foreseeable future when we are building it.
[01:32:32.200 --> 01:32:34.360]   And therefore we are always in a position
[01:32:34.360 --> 01:32:37.780]   of limiting what it can or cannot do.
[01:32:38.840 --> 01:32:43.840]   And your point about lying is very interesting.
[01:32:43.840 --> 01:32:49.320]   Even in these hyena societies, for instance,
[01:32:49.320 --> 01:32:52.640]   when a number of these hyenas band together
[01:32:52.640 --> 01:32:56.240]   and they take a risk and steal the kill,
[01:32:56.240 --> 01:32:58.560]   there are always hyenas that hang back
[01:32:58.560 --> 01:33:02.040]   and don't participate in that risky behavior,
[01:33:02.040 --> 01:33:06.960]   but they walk in later and join the party after the kill.
[01:33:06.960 --> 01:33:10.000]   And there are even some that may be ineffective
[01:33:10.000 --> 01:33:12.880]   and cause others to have harm.
[01:33:12.880 --> 01:33:15.440]   So, and like I said, even bacteria cheat.
[01:33:15.440 --> 01:33:17.320]   And we see in biology,
[01:33:17.320 --> 01:33:20.520]   there's always some element on opportunity.
[01:33:20.520 --> 01:33:22.160]   If you have a, I think that is this,
[01:33:22.160 --> 01:33:24.160]   because if you have a society,
[01:33:24.160 --> 01:33:26.040]   in order for society to be effective,
[01:33:26.040 --> 01:33:27.560]   you have to have this cooperation
[01:33:27.560 --> 01:33:29.020]   and you have to have trust.
[01:33:29.020 --> 01:33:32.100]   And if you have enough of agents
[01:33:32.100 --> 01:33:33.960]   who are able to trust each other,
[01:33:33.960 --> 01:33:36.560]   you can achieve a lot more.
[01:33:36.560 --> 01:33:37.480]   But if you have trust,
[01:33:37.480 --> 01:33:40.600]   you also have opportunity for cheaters and liars.
[01:33:40.600 --> 01:33:43.600]   And I don't think that's ever gonna go away.
[01:33:43.600 --> 01:33:45.200]   There will be hopefully a minority
[01:33:45.200 --> 01:33:46.640]   so that they don't get in the way.
[01:33:46.640 --> 01:33:48.720]   And we studied in these hyena simulations,
[01:33:48.720 --> 01:33:50.480]   like what the proportion needs to be
[01:33:50.480 --> 01:33:52.640]   before it's no longer functional.
[01:33:52.640 --> 01:33:55.040]   And you can point out that you can tolerate
[01:33:55.040 --> 01:33:57.240]   a few cheaters and a few liars,
[01:33:57.240 --> 01:33:59.640]   and the society can still function.
[01:33:59.640 --> 01:34:02.280]   And that's probably going to happen
[01:34:02.280 --> 01:34:05.360]   when we build these systems that autonomously learn
[01:34:06.360 --> 01:34:09.320]   that the really successful ones are honest
[01:34:09.320 --> 01:34:12.040]   because that's the best way of getting things done.
[01:34:12.040 --> 01:34:16.000]   But there probably are also intelligent agents
[01:34:16.000 --> 01:34:18.040]   that find that they can achieve their goals
[01:34:18.040 --> 01:34:20.960]   by bending the rules of cheating.
[01:34:20.960 --> 01:34:22.920]   - So there could be a huge benefit to,
[01:34:22.920 --> 01:34:25.720]   as opposed to having fixed AI systems,
[01:34:25.720 --> 01:34:30.080]   say we build an AGI system and deploying millions of them,
[01:34:30.080 --> 01:34:32.500]   it'd be that are exactly the same.
[01:34:32.500 --> 01:34:35.320]   There might be a huge benefit to
[01:34:36.320 --> 01:34:37.840]   introducing sort of from like
[01:34:37.840 --> 01:34:41.280]   an evolution competition perspective, a lot of variation.
[01:34:41.280 --> 01:34:46.280]   Sort of like diversity in all its forms is beneficial
[01:34:46.280 --> 01:34:49.920]   even if some people are assholes or some robots are assholes.
[01:34:49.920 --> 01:34:51.920]   So like it's beneficial to have that
[01:34:51.920 --> 01:34:56.720]   because you can't always a priori know
[01:34:56.720 --> 01:34:58.440]   what's good, what's bad.
[01:34:58.440 --> 01:35:01.360]   But that's a fascinating--
[01:35:01.360 --> 01:35:02.240]   - Absolutely.
[01:35:02.240 --> 01:35:04.320]   Diversity is the bread and butter.
[01:35:04.320 --> 01:35:05.760]   I mean, if you're running a competition,
[01:35:05.760 --> 01:35:08.040]   you see diversity is the one fundamental thing
[01:35:08.040 --> 01:35:09.040]   you have to have.
[01:35:09.040 --> 01:35:12.640]   And absolutely, also, it's not always good diversity.
[01:35:12.640 --> 01:35:14.920]   It may be something that can be destructive.
[01:35:14.920 --> 01:35:16.320]   We had in this hyena simulations,
[01:35:16.320 --> 01:35:19.160]   we have hyenas that just are suicidal.
[01:35:19.160 --> 01:35:20.520]   They just run and get killed.
[01:35:20.520 --> 01:35:22.760]   But they form the basis of those
[01:35:22.760 --> 01:35:24.400]   who actually are really fast,
[01:35:24.400 --> 01:35:26.040]   but stop before they get killed
[01:35:26.040 --> 01:35:28.360]   and eventually turn into this mob.
[01:35:28.360 --> 01:35:30.000]   So there might be something useful there
[01:35:30.000 --> 01:35:32.160]   if it's recombined with something else.
[01:35:32.160 --> 01:35:34.960]   So I think that as long as we can tolerate some of that,
[01:35:34.960 --> 01:35:36.840]   it may turn into something better.
[01:35:36.840 --> 01:35:38.480]   You may change the rules
[01:35:38.480 --> 01:35:40.640]   because it's so much more efficient to do something
[01:35:40.640 --> 01:35:43.280]   that was actually against the rules before.
[01:35:43.280 --> 01:35:46.480]   And we've seen society change over time
[01:35:46.480 --> 01:35:47.760]   quite a bit along those lines.
[01:35:47.760 --> 01:35:49.920]   That there were rules in society
[01:35:49.920 --> 01:35:52.160]   that we don't believe are fair anymore,
[01:35:52.160 --> 01:35:57.160]   even though they were considered proper behavior before.
[01:35:57.160 --> 01:35:58.760]   So things are changing.
[01:35:58.760 --> 01:36:00.000]   And I think that in that sense,
[01:36:00.000 --> 01:36:03.960]   I think it's a good idea to be able to tolerate some of that,
[01:36:03.960 --> 01:36:05.080]   some of that cheating,
[01:36:05.080 --> 01:36:07.480]   because eventually we might turn into something better.
[01:36:07.480 --> 01:36:10.040]   - So yeah, I think this is a message to the trolls
[01:36:10.040 --> 01:36:11.400]   and the assholes of the internet
[01:36:11.400 --> 01:36:13.480]   that you too have a beautiful purpose
[01:36:13.480 --> 01:36:15.640]   in this human ecosystem.
[01:36:15.640 --> 01:36:17.360]   So I appreciate you very much.
[01:36:17.360 --> 01:36:18.200]   - In moderate quantities.
[01:36:18.200 --> 01:36:20.360]   - In moderate quantities.
[01:36:20.360 --> 01:36:23.120]   So there's a whole field of artificial life.
[01:36:23.120 --> 01:36:24.880]   I don't know if you're connected to this field,
[01:36:24.880 --> 01:36:26.600]   if you pay attention.
[01:36:26.600 --> 01:36:28.680]   Do you think about this kind of thing?
[01:36:29.560 --> 01:36:32.240]   Is there an impressive demonstration to you
[01:36:32.240 --> 01:36:33.160]   of artificial life?
[01:36:33.160 --> 01:36:35.320]   Do you think of the agency you work with
[01:36:35.320 --> 01:36:40.320]   in the evolutionary computation perspective as life?
[01:36:40.320 --> 01:36:43.600]   And where do you think this is headed?
[01:36:43.600 --> 01:36:45.080]   Like, is there interesting systems
[01:36:45.080 --> 01:36:47.040]   that we'll be creating more and more
[01:36:47.040 --> 01:36:50.760]   that make us redefine, maybe rethink
[01:36:50.760 --> 01:36:52.440]   about the nature of life?
[01:36:52.440 --> 01:36:57.000]   - Different levels of definition and goals there.
[01:36:57.000 --> 01:36:59.760]   I mean, at some level artificial life
[01:36:59.760 --> 01:37:02.360]   can be considered multi-agent systems
[01:37:02.360 --> 01:37:05.080]   that build a society that again, achieves a goal.
[01:37:05.080 --> 01:37:07.000]   And it might be robots that go into a building
[01:37:07.000 --> 01:37:10.360]   and clean it up or after an earthquake or something.
[01:37:10.360 --> 01:37:12.880]   You can think of that as an artificial life problem
[01:37:12.880 --> 01:37:14.600]   in some sense.
[01:37:14.600 --> 01:37:16.880]   Or you can really think of it, artificial life,
[01:37:16.880 --> 01:37:19.120]   as a simulation of life
[01:37:19.120 --> 01:37:22.520]   and a tool to understand what life is
[01:37:22.520 --> 01:37:24.640]   and how life evolved on earth.
[01:37:24.640 --> 01:37:26.840]   And like I said, in artificial life conference,
[01:37:26.840 --> 01:37:29.760]   there are branches of that conference sessions
[01:37:29.760 --> 01:37:33.480]   of people who really worry about molecular designs
[01:37:33.480 --> 01:37:34.960]   and the start of life.
[01:37:34.960 --> 01:37:36.760]   Like I said, primordial soup
[01:37:36.760 --> 01:37:39.720]   where eventually you get something self-replicating
[01:37:39.720 --> 01:37:41.960]   and they're really trying to build that.
[01:37:41.960 --> 01:37:44.840]   So it's a whole range of topics.
[01:37:44.840 --> 01:37:50.840]   And I think that artificial life is a great tool
[01:37:50.840 --> 01:37:53.000]   to understand life.
[01:37:53.000 --> 01:37:55.320]   And there are questions like sustainability,
[01:37:56.440 --> 01:37:59.280]   species, we're losing species.
[01:37:59.280 --> 01:38:00.880]   How bad is it?
[01:38:00.880 --> 01:38:02.560]   Is it natural?
[01:38:02.560 --> 01:38:04.000]   Is there a tipping point?
[01:38:04.000 --> 01:38:06.480]   And where are we going?
[01:38:06.480 --> 01:38:08.080]   I mean, like the hyena evolution,
[01:38:08.080 --> 01:38:11.360]   we may have understood that there's a pivotal point
[01:38:11.360 --> 01:38:12.200]   in their evolution.
[01:38:12.200 --> 01:38:14.880]   They discovered cooperation and coordination.
[01:38:14.880 --> 01:38:18.680]   Artificial life simulations can identify that
[01:38:18.680 --> 01:38:21.320]   and maybe encourage things like that.
[01:38:22.920 --> 01:38:27.920]   And also societies can be seen as a form of life itself.
[01:38:27.920 --> 01:38:29.840]   I mean, we're not talking about biological evolution,
[01:38:29.840 --> 01:38:31.920]   we have all evolution of societies.
[01:38:31.920 --> 01:38:36.520]   Maybe some of the same phenomena emerge in that domain
[01:38:36.520 --> 01:38:39.440]   and having artificial life simulations
[01:38:39.440 --> 01:38:42.560]   and understanding could help us build better societies.
[01:38:42.560 --> 01:38:45.800]   - Yeah, and thinking from a meme perspective
[01:38:45.800 --> 01:38:49.560]   from Richard Dawkins,
[01:38:50.880 --> 01:38:54.080]   that maybe the organisms, ideas of the organisms,
[01:38:54.080 --> 01:38:56.680]   not the humans in these societies,
[01:38:56.680 --> 01:39:01.920]   it's almost like reframing what is exactly evolving.
[01:39:01.920 --> 01:39:02.920]   Maybe the interesting,
[01:39:02.920 --> 01:39:04.560]   the humans aren't the interesting thing
[01:39:04.560 --> 01:39:07.360]   as the contents of our minds is the interesting thing.
[01:39:07.360 --> 01:39:09.240]   And that's what's multiplying.
[01:39:09.240 --> 01:39:10.880]   And that's actually multiplying and evolving
[01:39:10.880 --> 01:39:13.040]   in a much faster timescale.
[01:39:13.040 --> 01:39:16.240]   And that maybe has more power on the trajectory
[01:39:16.240 --> 01:39:19.560]   of life on earth than does biological evolution.
[01:39:19.560 --> 01:39:21.000]   - Yes. - Is evolution of these ideas.
[01:39:21.000 --> 01:39:23.880]   - Yes, and it's fascinating, like I said before,
[01:39:23.880 --> 01:39:27.560]   that we can keep up somehow biologically.
[01:39:27.560 --> 01:39:30.120]   We evolve to a point where we can keep up
[01:39:30.120 --> 01:39:35.120]   with this meme evolution, literature, internet.
[01:39:35.120 --> 01:39:39.040]   We understand DNA and we understand fundamental particles.
[01:39:39.040 --> 01:39:41.400]   We didn't start that way a thousand years ago
[01:39:41.400 --> 01:39:43.360]   and we haven't evolved biologically very much,
[01:39:43.360 --> 01:39:47.040]   but somehow our minds are able to extend.
[01:39:47.040 --> 01:39:51.280]   And therefore AI can be seen also as one such step
[01:39:51.280 --> 01:39:53.440]   that we created and it's our tool.
[01:39:53.440 --> 01:39:56.360]   And it's part of that meme evolution that we created,
[01:39:56.360 --> 01:39:59.640]   even if our biological evolution does not progress as fast.
[01:39:59.640 --> 01:40:03.720]   - And us humans might only be able to understand so much.
[01:40:03.720 --> 01:40:05.800]   We're keeping up so far,
[01:40:05.800 --> 01:40:07.360]   or we think we're keeping up so far,
[01:40:07.360 --> 01:40:09.520]   but we might need AI systems to understand.
[01:40:09.520 --> 01:40:13.800]   Maybe like the physics of the universe is operating,
[01:40:13.800 --> 01:40:14.760]   look at string theory,
[01:40:14.760 --> 01:40:17.440]   maybe it's operating in much higher dimensions.
[01:40:17.440 --> 01:40:21.240]   Maybe we're totally, because of our cognitive limitations,
[01:40:21.240 --> 01:40:25.720]   are not able to truly internalize the way this world works.
[01:40:25.720 --> 01:40:28.920]   And so we're running up against the limitation
[01:40:28.920 --> 01:40:30.920]   of our own minds and we have to create
[01:40:30.920 --> 01:40:34.520]   these next level organisms like AI systems
[01:40:34.520 --> 01:40:36.280]   that would be able to understand much deeper,
[01:40:36.280 --> 01:40:38.480]   like really understand what it means to live
[01:40:38.480 --> 01:40:41.240]   in a multidimensional world
[01:40:41.240 --> 01:40:42.640]   that's outside of the four dimensions,
[01:40:42.640 --> 01:40:45.400]   the three of space and one of time.
[01:40:45.400 --> 01:40:48.200]   - Translation, and generally we can deal with the world,
[01:40:48.200 --> 01:40:49.680]   even if you don't understand all the details,
[01:40:49.680 --> 01:40:52.120]   we can use computers, even though we don't,
[01:40:52.120 --> 01:40:55.040]   most of us don't know all the structures underneath
[01:40:55.040 --> 01:40:55.880]   or drive a car.
[01:40:55.880 --> 01:40:57.280]   I mean, there are many components,
[01:40:57.280 --> 01:40:59.880]   especially new cars that you don't quite fully know,
[01:40:59.880 --> 01:41:02.720]   but you have the interface, you have an abstraction of it
[01:41:02.720 --> 01:41:05.120]   that allows you to operate it and utilize it.
[01:41:05.120 --> 01:41:08.200]   And I think that that's perfectly adequate
[01:41:08.200 --> 01:41:09.240]   and we can build on it.
[01:41:09.240 --> 01:41:12.160]   And AI can play a similar role.
[01:41:12.160 --> 01:41:18.120]   - I have to ask about beautiful artificial life systems
[01:41:18.120 --> 01:41:22.640]   or evolution computation systems, cellular automata to me.
[01:41:22.640 --> 01:41:26.200]   Like I remember it was a game changer for me early on
[01:41:26.200 --> 01:41:28.800]   in life when I saw Conway's Game of Life
[01:41:28.800 --> 01:41:31.400]   who recently passed away, unfortunately.
[01:41:31.400 --> 01:41:38.640]   It's beautiful how much complexity can emerge
[01:41:38.640 --> 01:41:40.000]   from such simple rules.
[01:41:40.000 --> 01:41:44.440]   I just don't, somehow that simplicity
[01:41:44.440 --> 01:41:48.320]   is such a powerful illustration and also humbling
[01:41:48.320 --> 01:41:50.880]   because it feels like I personally, from my perspective,
[01:41:50.880 --> 01:41:54.920]   understand almost nothing about this world
[01:41:54.920 --> 01:41:58.400]   because like my intuition fails completely
[01:41:58.400 --> 01:42:01.280]   how complexity can emerge from such simplicity.
[01:42:01.280 --> 01:42:02.640]   Like my intuition fails, I think,
[01:42:02.640 --> 01:42:04.140]   is the biggest problem I have.
[01:42:04.140 --> 01:42:08.520]   Do you find systems like that beautiful?
[01:42:08.520 --> 01:42:11.400]   Is there, do you think about cellular automata?
[01:42:11.400 --> 01:42:14.060]   Because cellular automata don't really have,
[01:42:14.060 --> 01:42:17.200]   and many other artificial life systems
[01:42:17.200 --> 01:42:18.920]   don't necessarily have an objective.
[01:42:18.920 --> 01:42:21.640]   Maybe that's a wrong way to say it.
[01:42:21.640 --> 01:42:26.640]   It's almost like it's just evolving and creating.
[01:42:26.640 --> 01:42:29.760]   And there's not even a good definition
[01:42:29.760 --> 01:42:33.080]   of what it means to create something complex
[01:42:33.080 --> 01:42:34.600]   and interesting and surprising,
[01:42:34.600 --> 01:42:36.100]   all those words that you said.
[01:42:38.000 --> 01:42:41.080]   Is there some of those systems that you find beautiful?
[01:42:41.080 --> 01:42:41.920]   - Yeah, yeah.
[01:42:41.920 --> 01:42:44.460]   And similarly, evolution does not have a goal.
[01:42:44.460 --> 01:42:49.520]   It is responding to the current situation
[01:42:49.520 --> 01:42:52.720]   and survival then creates more complexity
[01:42:52.720 --> 01:42:56.080]   and therefore we have something that we perceive as progress
[01:42:56.080 --> 01:43:00.680]   but that's not what evolution is inherently set to do.
[01:43:00.680 --> 01:43:03.280]   And yeah, that's really fascinating
[01:43:03.800 --> 01:43:08.800]   how a simple set of rules or simple mappings can,
[01:43:08.800 --> 01:43:14.520]   how from such simple mappings, complexity can emerge.
[01:43:14.520 --> 01:43:17.720]   So it's a question of emergence and self-organization.
[01:43:17.720 --> 01:43:21.500]   And the game of life is one of the simplest ones
[01:43:21.500 --> 01:43:25.680]   and very visual and therefore it drives home the point
[01:43:25.680 --> 01:43:29.120]   that it's possible that nonlinear interactions
[01:43:31.240 --> 01:43:34.720]   and this kinds of complexity can emerge from them.
[01:43:34.720 --> 01:43:37.920]   And biology and evolution is along the same lines.
[01:43:37.920 --> 01:43:40.080]   We have simple representations.
[01:43:40.080 --> 01:43:43.160]   DNA, if you really think of it, it's not that complex.
[01:43:43.160 --> 01:43:45.400]   It's a long sequence of them.
[01:43:45.400 --> 01:43:46.240]   There's lots of them
[01:43:46.240 --> 01:43:48.200]   but it's a very simple representation.
[01:43:48.200 --> 01:43:49.880]   And similar with evolutionary computation,
[01:43:49.880 --> 01:43:52.640]   whatever string or tree representation we have
[01:43:52.640 --> 01:43:57.560]   and the operations, the amount of code that's required
[01:43:57.560 --> 01:44:00.520]   to manipulate those, it's really, really little.
[01:44:00.520 --> 01:44:02.440]   And of course, game of life, even less.
[01:44:02.440 --> 01:44:06.200]   So how complexity emerges from such simple principles,
[01:44:06.200 --> 01:44:08.280]   that's absolutely fascinating.
[01:44:08.280 --> 01:44:11.440]   The challenge is to be able to control it
[01:44:11.440 --> 01:44:15.520]   and guide it and direct it so that it becomes useful.
[01:44:15.520 --> 01:44:17.920]   And like game of life is fascinating to look at
[01:44:17.920 --> 01:44:21.200]   and evolution, all the forms that come out is fascinating
[01:44:21.200 --> 01:44:24.040]   but can we actually make it useful for us?
[01:44:24.040 --> 01:44:27.040]   - And efficient because if you actually think about
[01:44:27.040 --> 01:44:30.320]   each of the cells in the game of life as a living organism,
[01:44:30.320 --> 01:44:32.600]   there's a lot of death that has to happen
[01:44:32.600 --> 01:44:34.360]   to create anything interesting.
[01:44:34.360 --> 01:44:36.480]   And so I guess the question is for us humans
[01:44:36.480 --> 01:44:38.920]   that are mortal and then life ends quickly,
[01:44:38.920 --> 01:44:43.920]   we wanna kinda hurry up and make sure we take evolution,
[01:44:43.920 --> 01:44:47.400]   the trajectory that is a little bit more efficient
[01:44:47.400 --> 01:44:49.360]   than the alternatives.
[01:44:49.360 --> 01:44:51.240]   - And that touches upon something we talked about earlier
[01:44:51.240 --> 01:44:54.600]   that evolutionary computation is very impatient.
[01:44:54.600 --> 01:44:57.160]   We have a goal, we want it right away
[01:44:57.160 --> 01:45:01.080]   versus biology has a lot of time and deep time
[01:45:01.080 --> 01:45:04.520]   and weak pressure and large populations.
[01:45:04.520 --> 01:45:08.960]   One great example of this is the novelty search.
[01:45:08.960 --> 01:45:12.440]   So evolutionary computation where you don't actually
[01:45:12.440 --> 01:45:16.520]   specify a fitness goal, something that is your actual thing
[01:45:16.520 --> 01:45:19.640]   that you want, but you just reward solutions
[01:45:19.640 --> 01:45:22.520]   that are different from what you've seen before.
[01:45:22.520 --> 01:45:23.800]   Nothing else.
[01:45:23.800 --> 01:45:25.160]   And you know what?
[01:45:25.160 --> 01:45:27.400]   You actually discover things that are interesting
[01:45:27.400 --> 01:45:28.520]   and useful that way.
[01:45:28.520 --> 01:45:31.120]   Ken Stanley and Joel Lehman did this one study
[01:45:31.120 --> 01:45:33.600]   where they actually tried to evolve
[01:45:33.600 --> 01:45:35.320]   walking behavior on robots.
[01:45:35.320 --> 01:45:36.600]   And that's actually, we talked about earlier
[01:45:36.600 --> 01:45:39.680]   where your robot actually failed in all kinds of ways
[01:45:39.680 --> 01:45:41.040]   and eventually discovered something
[01:45:41.040 --> 01:45:43.920]   that was a very efficient walk.
[01:45:43.920 --> 01:45:47.720]   And it was because they rewarded things
[01:45:47.720 --> 01:45:50.720]   that were different that you were able to discover something
[01:45:50.720 --> 01:45:53.000]   and I think that this is crucial
[01:45:53.000 --> 01:45:55.120]   because in order to be really different
[01:45:55.120 --> 01:45:56.600]   from what you already have,
[01:45:56.600 --> 01:45:59.080]   you have to utilize what is there in a domain
[01:45:59.080 --> 01:46:00.760]   to create something really different.
[01:46:00.760 --> 01:46:05.760]   So you have encoded the fundamentals of your world
[01:46:05.760 --> 01:46:08.120]   and then you make changes to those fundamentals
[01:46:08.120 --> 01:46:09.720]   you get further away.
[01:46:09.720 --> 01:46:11.520]   So that's probably what's happening
[01:46:11.520 --> 01:46:14.280]   in these systems of emergence,
[01:46:14.280 --> 01:46:17.360]   that the fundamentals are there.
[01:46:17.360 --> 01:46:18.960]   And when you follow those fundamentals,
[01:46:18.960 --> 01:46:21.080]   you get into points and some of those
[01:46:21.080 --> 01:46:22.880]   are actually interesting and useful.
[01:46:23.200 --> 01:46:25.160]   Even in that robotic walker simulation,
[01:46:25.160 --> 01:46:28.120]   there was a large set of garbage,
[01:46:28.120 --> 01:46:31.600]   but among them, there were some of these gems.
[01:46:31.600 --> 01:46:33.160]   And then those are the ones that somehow
[01:46:33.160 --> 01:46:36.600]   you have to outside recognize and make useful.
[01:46:36.600 --> 01:46:38.640]   But this kind of productive systems,
[01:46:38.640 --> 01:46:41.560]   if you code them the right kind of principles,
[01:46:41.560 --> 01:46:45.640]   I think that they encode the structure of the domain,
[01:46:45.640 --> 01:46:48.480]   then you will get to these solutions and you discover it.
[01:46:48.480 --> 01:46:52.760]   - It feels like that might also be a good way to live life.
[01:46:52.760 --> 01:46:57.760]   So let me ask, do you have advice for young people today
[01:46:57.760 --> 01:47:01.520]   about how to live life or how to succeed in their career
[01:47:01.520 --> 01:47:04.680]   or forget career, just succeed in life?
[01:47:04.680 --> 01:47:08.800]   From an evolutionary computation perspective.
[01:47:08.800 --> 01:47:11.560]   - Yes, yes, definitely.
[01:47:11.560 --> 01:47:15.880]   Explore, diversity, exploration.
[01:47:15.880 --> 01:47:19.840]   And individuals take classes in music,
[01:47:19.840 --> 01:47:23.800]   history, philosophy, math, engineering.
[01:47:23.800 --> 01:47:27.480]   See connections between them.
[01:47:27.480 --> 01:47:30.120]   Travel, learn a language.
[01:47:30.120 --> 01:47:32.160]   I mean, all this diversity is fascinating
[01:47:32.160 --> 01:47:35.480]   and we have it at our fingertips today.
[01:47:35.480 --> 01:47:37.840]   It's possible, you have to make a bit of an effort
[01:47:37.840 --> 01:47:42.000]   'cause it's not easy, but the rewards are wonderful.
[01:47:42.000 --> 01:47:43.840]   - Yeah, there's something interesting
[01:47:43.840 --> 01:47:47.400]   about an objective function of new experiences.
[01:47:47.400 --> 01:47:49.400]   So try to figure out, I mean,
[01:47:49.400 --> 01:47:55.560]   what is the maximally new experience I could have today?
[01:47:55.560 --> 01:47:59.360]   And that sort of, that novelty, optimizing for novelty
[01:47:59.360 --> 01:48:01.800]   for some period of time might be a very interesting way
[01:48:01.800 --> 01:48:06.320]   to sort of maximally expand the sets of experiences
[01:48:06.320 --> 01:48:10.360]   you had and then ground from that perspective,
[01:48:10.360 --> 01:48:14.520]   like what will be the most fulfilling trajectory
[01:48:14.520 --> 01:48:15.360]   through life?
[01:48:15.360 --> 01:48:19.200]   And of course, the flip side of that is where I come from.
[01:48:19.200 --> 01:48:21.000]   Again, maybe Russian, I don't know.
[01:48:21.000 --> 01:48:26.000]   But the choice has a detrimental effect, I think,
[01:48:26.000 --> 01:48:31.080]   at least from my mind,
[01:48:31.080 --> 01:48:35.280]   where scarcity has an empowering effect.
[01:48:35.280 --> 01:48:40.280]   So if I have very little of something
[01:48:40.280 --> 01:48:45.000]   and only one of that something, I will appreciate it deeply
[01:48:45.000 --> 01:48:48.640]   until I came to Texas recently
[01:48:48.640 --> 01:48:51.560]   and I've been pigging out on delicious, incredible meat.
[01:48:51.560 --> 01:48:53.840]   I've been fasting a lot, so I need to do that again.
[01:48:53.840 --> 01:48:56.320]   But when you fast for a few days,
[01:48:56.320 --> 01:49:00.760]   that the first taste of a food is incredible.
[01:49:00.760 --> 01:49:05.760]   So the downside of exploration is that somehow,
[01:49:05.760 --> 01:49:11.080]   maybe you can correct me,
[01:49:11.080 --> 01:49:14.240]   but somehow you don't get to experience deeply
[01:49:15.120 --> 01:49:17.520]   any one of the particular moments.
[01:49:17.520 --> 01:49:19.760]   But that could be a psychology thing.
[01:49:19.760 --> 01:49:23.680]   That could be just a very human, peculiar flaw.
[01:49:23.680 --> 01:49:26.720]   - Yeah, I didn't mean that you superficially explore.
[01:49:26.720 --> 01:49:28.360]   I mean, you can-- - Explore deeply.
[01:49:28.360 --> 01:49:31.080]   - Yeah, so you don't have to explore 100 things,
[01:49:31.080 --> 01:49:34.080]   but maybe a few topics where you can take
[01:49:34.080 --> 01:49:39.080]   a deep enough dive that you gain an understanding.
[01:49:39.080 --> 01:49:42.640]   Yourself have to decide at some point
[01:49:42.640 --> 01:49:44.440]   that this is deep enough.
[01:49:44.440 --> 01:49:49.280]   And I've obtained what I can from this topic
[01:49:49.280 --> 01:49:51.400]   and now it's time to move on.
[01:49:51.400 --> 01:49:54.040]   And that might take years.
[01:49:54.040 --> 01:49:56.280]   People sometimes switch careers
[01:49:56.280 --> 01:49:59.160]   and they may stay on some career for a decade
[01:49:59.160 --> 01:50:00.520]   and switch to another one.
[01:50:00.520 --> 01:50:01.840]   You can do it.
[01:50:01.840 --> 01:50:04.680]   You're not pretty determined to stay where you are.
[01:50:04.680 --> 01:50:09.120]   But in order to achieve something,
[01:50:09.120 --> 01:50:11.760]   10,000 hours makes, you need 10,000 hours
[01:50:11.760 --> 01:50:13.640]   to become an expert on something.
[01:50:13.640 --> 01:50:15.360]   So you don't have to become an expert,
[01:50:15.360 --> 01:50:17.200]   but to even develop an understanding
[01:50:17.200 --> 01:50:19.360]   and gain the experience that you can use later,
[01:50:19.360 --> 01:50:21.960]   you probably have to spend, like I said, it's not easy.
[01:50:21.960 --> 01:50:24.440]   You gotta spend some effort on it.
[01:50:24.440 --> 01:50:28.160]   Now, also at some point then when you have this diversity
[01:50:28.160 --> 01:50:30.360]   and you have these experiences, exploration,
[01:50:30.360 --> 01:50:33.720]   you may want to, you may find something
[01:50:33.720 --> 01:50:35.920]   that you can't stay away from.
[01:50:35.920 --> 01:50:38.720]   Like for, as it was computers, it was AI,
[01:50:38.720 --> 01:50:42.000]   it was, that you, I just have to do it.
[01:50:42.000 --> 01:50:45.240]   And I, and then it will take decades maybe
[01:50:45.240 --> 01:50:47.800]   and you are pursuing it because you figured out
[01:50:47.800 --> 01:50:49.320]   that this is really exciting
[01:50:49.320 --> 01:50:51.280]   and you can bring in your experiences.
[01:50:51.280 --> 01:50:52.760]   And there's nothing wrong with that either,
[01:50:52.760 --> 01:50:55.840]   but you asked what's the advice for young people.
[01:50:55.840 --> 01:50:57.520]   That's the exploration part.
[01:50:57.520 --> 01:51:00.120]   And then beyond that, after that exploration,
[01:51:00.120 --> 01:51:03.200]   you actually can focus and build a career.
[01:51:03.200 --> 01:51:05.800]   And even there you can switch multiple times,
[01:51:05.800 --> 01:51:09.120]   but I think that diversity exploration is fundamental
[01:51:09.120 --> 01:51:13.320]   to having a successful career as is concentration
[01:51:13.320 --> 01:51:15.520]   and spending an effort where it matters.
[01:51:15.520 --> 01:51:18.960]   And, but you are in better position to make the choice
[01:51:18.960 --> 01:51:20.400]   when you have done your homework.
[01:51:20.400 --> 01:51:21.240]   - Explored.
[01:51:21.240 --> 01:51:24.920]   So exploration precedes commitment, but both are beautiful.
[01:51:24.920 --> 01:51:29.440]   So again, from an evolutionary computation perspective,
[01:51:29.440 --> 01:51:32.440]   we'll look at all the agents that had to die
[01:51:32.440 --> 01:51:35.780]   in order to come up with different solutions in simulation.
[01:51:35.780 --> 01:51:40.280]   What do you think from that individual agent's perspective
[01:51:40.280 --> 01:51:41.880]   is the meaning of it all?
[01:51:41.880 --> 01:51:43.880]   So far as humans, you're just one agent
[01:51:43.880 --> 01:51:47.600]   who's going to be dead, unfortunately, one day too soon.
[01:51:47.600 --> 01:51:53.760]   What do you think is the why of why that agent came to be
[01:51:53.760 --> 01:51:57.560]   and eventually will be no more?
[01:51:57.560 --> 01:52:00.100]   Is there a meaning to it all?
[01:52:00.100 --> 01:52:02.480]   - Yeah, in evolution, there is meaning.
[01:52:02.480 --> 01:52:05.640]   Everything is a potential direction.
[01:52:05.640 --> 01:52:07.820]   Everything is a potential stepping stone.
[01:52:07.820 --> 01:52:11.400]   Not all of them are gonna work out.
[01:52:11.400 --> 01:52:16.400]   Some of them are foundations for further improvement.
[01:52:16.400 --> 01:52:20.240]   And even those that are perhaps going to die out
[01:52:20.240 --> 01:52:24.680]   where potential energy is potential solutions.
[01:52:24.680 --> 01:52:28.720]   In biology, we see a lot of species die off naturally
[01:52:28.720 --> 01:52:29.880]   and like the dinosaurs.
[01:52:29.880 --> 01:52:31.880]   I mean, they were really good solution for a while,
[01:52:31.880 --> 01:52:36.020]   but then it didn't turn out to be not such a good solution
[01:52:36.020 --> 01:52:37.800]   in the longterm.
[01:52:37.800 --> 01:52:39.440]   When there's an environmental change,
[01:52:39.440 --> 01:52:40.660]   you have to have diversity.
[01:52:40.660 --> 01:52:42.680]   Some other solutions become better.
[01:52:42.680 --> 01:52:45.040]   Doesn't mean that there was an attempt.
[01:52:45.040 --> 01:52:47.560]   It didn't quite work out or last,
[01:52:47.560 --> 01:52:49.360]   but there are still dinosaurs and mamas,
[01:52:49.360 --> 01:52:51.240]   at least their relatives,
[01:52:51.240 --> 01:52:54.280]   and they may one day again be useful.
[01:52:54.280 --> 01:52:55.560]   Who knows?
[01:52:55.560 --> 01:52:57.200]   So from an individual's perspective,
[01:52:57.200 --> 01:52:59.100]   you've got to think of a bigger picture
[01:52:59.100 --> 01:53:04.100]   that it is a huge engine that is innovative.
[01:53:04.100 --> 01:53:06.760]   And these elements are all part of it,
[01:53:06.760 --> 01:53:09.360]   potential innovations on their own
[01:53:09.360 --> 01:53:12.360]   and also as raw material perhaps
[01:53:12.360 --> 01:53:16.400]   or stepping stones for other things that could come after.
[01:53:16.400 --> 01:53:18.760]   - But it still feels from an individual perspective
[01:53:18.760 --> 01:53:21.080]   that I matter a lot.
[01:53:21.080 --> 01:53:24.520]   But even if I'm just a little cog in the giant machine,
[01:53:24.520 --> 01:53:28.200]   is that just a silly human notion
[01:53:28.200 --> 01:53:29.980]   in an individualistic society
[01:53:29.980 --> 01:53:31.540]   and they should let go of that?
[01:53:31.540 --> 01:53:35.800]   Do you find beauty in being part of the giant machine?
[01:53:35.800 --> 01:53:39.060]   - Yeah, I think it's meaningful.
[01:53:39.060 --> 01:53:41.580]   I think it adds purpose to your life,
[01:53:41.580 --> 01:53:43.600]   that you are part of something bigger.
[01:53:43.600 --> 01:53:50.420]   - That said, do you ponder your individual agent's mortality?
[01:53:50.420 --> 01:53:53.780]   Do you think about death?
[01:53:53.780 --> 01:53:54.780]   Do you fear death?
[01:53:56.760 --> 01:54:00.740]   - Well, certainly more now than when I was a youngster
[01:54:00.740 --> 01:54:05.740]   and did skydiving and paragliding and all these things.
[01:54:05.740 --> 01:54:06.880]   - You've become wiser.
[01:54:06.880 --> 01:54:13.980]   - There is a reason for this life arc
[01:54:13.980 --> 01:54:17.220]   that younger folks are more fearless in many ways.
[01:54:17.220 --> 01:54:18.860]   It's part of the exploration.
[01:54:18.860 --> 01:54:22.220]   They are the individuals who think,
[01:54:22.220 --> 01:54:24.900]   hmm, I wonder what's over those mountains
[01:54:24.900 --> 01:54:27.100]   or what if I go really far in that ocean?
[01:54:27.100 --> 01:54:28.040]   What would I find?
[01:54:28.040 --> 01:54:32.240]   I mean, older folks don't necessarily think that way,
[01:54:32.240 --> 01:54:34.900]   but younger do and it's kind of counterintuitive.
[01:54:34.900 --> 01:54:39.200]   So yeah, but logically it's like,
[01:54:39.200 --> 01:54:40.160]   you have a limited amount of time,
[01:54:40.160 --> 01:54:42.480]   what can you do with it that matters?
[01:54:42.480 --> 01:54:45.400]   So you try to, you have done your exploration,
[01:54:45.400 --> 01:54:48.200]   you committed to a certain direction
[01:54:48.200 --> 01:54:50.440]   and you become an expert perhaps in it.
[01:54:50.440 --> 01:54:52.600]   What can I do that matters
[01:54:52.600 --> 01:54:55.640]   with the limited resources that I have?
[01:54:55.640 --> 01:54:59.840]   That's how I think a lot of people, myself included,
[01:54:59.840 --> 01:55:02.520]   start thinking later on in their career.
[01:55:02.520 --> 01:55:05.700]   - And like you said, leave a bit of a trace
[01:55:05.700 --> 01:55:08.620]   and a bit of an impact even after the agent is gone.
[01:55:08.620 --> 01:55:10.240]   - Yeah, that's the goal.
[01:55:10.240 --> 01:55:13.720]   - Well, this was a fascinating conversation.
[01:55:13.720 --> 01:55:16.000]   I don't think there's a better way to end it.
[01:55:16.000 --> 01:55:17.120]   Thank you so much.
[01:55:17.120 --> 01:55:19.520]   So first of all, I'm very inspired
[01:55:19.520 --> 01:55:23.040]   of how vibrant the community at UT Austin and Austin is.
[01:55:23.040 --> 01:55:25.640]   It's really exciting for me to see it.
[01:55:25.640 --> 01:55:30.040]   And this whole field seems like profound philosophically,
[01:55:30.040 --> 01:55:31.400]   but also the path forward
[01:55:31.400 --> 01:55:33.400]   for the artificial intelligence community.
[01:55:33.400 --> 01:55:35.480]   So thank you so much for explaining
[01:55:35.480 --> 01:55:36.920]   so many cool things to me today
[01:55:36.920 --> 01:55:39.280]   and for wasting all of your valuable time with me.
[01:55:39.280 --> 01:55:40.520]   - Oh, it was a pleasure.
[01:55:40.520 --> 01:55:42.920]   Thanks, Lex. - I appreciate it.
[01:55:42.920 --> 01:55:44.560]   Thanks for listening to this conversation
[01:55:44.560 --> 01:55:46.040]   with Vristo McAlinan
[01:55:46.040 --> 01:55:48.760]   and thank you to the Jordan and Harbinger Show,
[01:55:48.760 --> 01:55:52.080]   Grammarly, Belcampo, and Indeed.
[01:55:52.080 --> 01:55:55.640]   Check them out in the description to support this podcast.
[01:55:55.640 --> 01:55:59.400]   And now let me leave you with some words from Carl Sagan.
[01:55:59.400 --> 01:56:01.800]   "Extinction is the rule.
[01:56:01.800 --> 01:56:05.000]   Survival is the exception."
[01:56:05.000 --> 01:56:06.120]   Thank you for listening.
[01:56:06.120 --> 01:56:07.960]   I hope to see you next time.
[01:56:07.960 --> 01:56:10.560]   (upbeat music)
[01:56:10.560 --> 01:56:13.160]   (upbeat music)
[01:56:13.160 --> 01:56:23.160]   [BLANK_AUDIO]

