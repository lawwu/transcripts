
[00:00:00.000 --> 00:00:18.900]   All right. Hey, everyone. My name is Aparna, one of the founders of Arise. We do LLM evals
[00:00:18.900 --> 00:00:25.520]   and observability. I wanted to do a session going really deep on this stuff because you
[00:00:25.520 --> 00:00:30.320]   guys all hear LLM as a judge, and you're probably like, yeah, yeah, yeah, but how does it actually
[00:00:30.320 --> 00:00:35.140]   work in the real world? Well, we work with some of the top companies in the space who
[00:00:35.140 --> 00:00:41.900]   are all deploying LLM applications, and we've seen a lot go well and not go well in the real
[00:00:41.900 --> 00:00:47.840]   world. And so even though you've probably seen this tweet from Greg a bunch, evals are
[00:00:47.840 --> 00:00:51.700]   all you need, you're probably like, what does that actually mean when you're putting it in
[00:00:51.700 --> 00:00:55.600]   the real world? And I'm going to demystify a little bit of that and talk about some real
[00:00:55.600 --> 00:01:02.300]   examples today. So first off, there's a distinction between types of evals that we should just
[00:01:02.300 --> 00:01:07.620]   clarify. First is there's model evals. If you're on Hugging Face, you're looking at the OpenLLM
[00:01:07.620 --> 00:01:13.820]   leaderboard, and you're like, okay, LLM, 3B, whatever, 7B is better than this because of
[00:01:13.820 --> 00:01:18.700]   some MMLU metric. Well, they're actually stacking and ranking different models against each other.
[00:01:18.700 --> 00:01:24.520]   And these are really helpful when you see things like the needle in the haystack test to understand
[00:01:24.520 --> 00:01:29.620]   which model to actually use. But for most of you in the room who are probably building the
[00:01:29.620 --> 00:01:35.440]   applications, you probably care more about task evals. And what I mean by that is, is the LLM
[00:01:35.440 --> 00:01:41.160]   application actually working? And how do you define evals that actually help you figure that
[00:01:41.160 --> 00:01:46.800]   out? So let's talk about how task evals work in the real world. This is probably a review to most of
[00:01:46.800 --> 00:01:51.660]   you. Most of the industry is converging around a couple different options, LLM as a judge, user
[00:01:51.660 --> 00:01:57.320]   feedback, heuristic-based approaches. Just as an overview for folks who don't know what LLM as a
[00:01:57.320 --> 00:02:04.500]   judge is, it's basically when you're using AI to evaluate AI. I take inputs from, I take the output of
[00:02:04.500 --> 00:02:09.400]   my response, I might take the context that it was given, pass it into an eval prompt template,
[00:02:09.400 --> 00:02:14.260]   and then I can actually have LLM as a judge come back with an evaluation of how it works.
[00:02:14.260 --> 00:02:19.120]   Let's talk about how this works in a simple application. So this is a really common one
[00:02:19.120 --> 00:02:24.180]   we're seeing in the ecosystem. It's a chat to purchase type of application. E-commerce
[00:02:24.180 --> 00:02:30.900]   applications use this a lot. So the way it begins is the customer asks some kind of question,
[00:02:30.900 --> 00:02:37.400]   hey, blah, blah, blah, I'm looking for a new Kindle. And there's first this component, call it like a
[00:02:37.400 --> 00:02:42.880]   router, that actually is deciding what the customer intent is. A lot of folks actually use function
[00:02:42.880 --> 00:02:48.880]   calling for this. And so there's a function call that happens, it determines what the path to send a user
[00:02:48.880 --> 00:02:56.000]   down is. And then there's the actual workflow, execution branch of what to happen. This is a really, really
[00:02:56.000 --> 00:03:03.760]   simple one. In the real world, the applications get a lot more complex. There's a real, I see probably a couple
[00:03:03.760 --> 00:03:11.920]   of these, you know, every week, where basically, the user intent gets decided by an LLM call. And then it has to
[00:03:11.920 --> 00:03:17.040]   get the user intent correct. This is what they all care about. Because otherwise, it sends users down the
[00:03:17.040 --> 00:03:23.080]   wrong path. It can send it down if users asking you a question about, hey, recommend me some product to go
[00:03:23.080 --> 00:03:28.140]   buy, but then it sends me down something related to customer support. Well, the issue actually you want
[00:03:28.140 --> 00:03:35.140]   to catch is, did it get the function call of determining that user intent correctly? And so when you look at an
[00:03:35.140 --> 00:03:41.040]   application like this, where there's a router, and routers are, you know, probably the most common agentic type of
[00:03:41.040 --> 00:03:47.100]   workflows we're actually seeing in production today. There's a router call, then there's LLM calls,
[00:03:47.100 --> 00:03:52.980]   then there's application calls, and then there's maybe even calls to traditional ML models doing in the
[00:03:52.980 --> 00:03:57.240]   middle of where you're actually calling out to search. How many of you guys have an application that looks
[00:03:57.240 --> 00:04:21.240]   like this? Or have seen app, you know, have built one internally? Okay, awesome, awesome, awesome. This is, this is kind of where we're seeing a lot of, we're seeing a lot of applications being built. It's not just a simple API call, and here's a response. It's actually built on top of levels. And so as your applications get more complex,
[00:04:21.240 --> 00:04:28.300]   your evals are going to get more complex. And there's levels to this is kind of the theme you'll hear
[00:04:28.300 --> 00:04:33.300]   today. If you're evaluating something like this, you want evals at different levels of
[00:04:33.300 --> 00:04:39.960]   your application. There's an eval, most importantly, at the router level to help you figure out
[00:04:39.960 --> 00:04:46.160]   what's the path that it should go down, did it go down the right execution branch, and
[00:04:46.160 --> 00:04:51.460]   then within each execution branch there's often component level evals that are being
[00:04:51.460 --> 00:04:56.980]   done. I'm going to actually give you guys a demo so we can show your real application
[00:04:56.980 --> 00:05:01.220]   and we can show you where something goes wrong, but just to set some context, you will actually
[00:05:01.220 --> 00:05:08.400]   dive really deep today into this router eval in applications. The key thing, I think, to
[00:05:08.400 --> 00:05:14.280]   take away is you'll see questions like this from the demo we're going to give, but users
[00:05:14.280 --> 00:05:19.300]   ask questions in the applications, and typically you want to figure out, well, did it go down
[00:05:19.300 --> 00:05:24.020]   the right function call? So in this case, did it go down something like the user asked about
[00:05:24.020 --> 00:05:29.400]   details of a product? Did it go down the product details function call? And then there's another
[00:05:29.400 --> 00:05:34.960]   kind of implicit one that's often done, which is, did we extract the right parameters for
[00:05:34.960 --> 00:05:38.380]   the function call? And do we give it the right parameters? Because if you don't give it the
[00:05:38.380 --> 00:05:42.560]   right parameters, then it doesn't matter if you pick the right function call, it's still
[00:05:42.560 --> 00:05:46.320]   not going to get it right. So these are kind of the two ones I'm going to actually walk
[00:05:46.320 --> 00:05:58.400]   through and show you guys what it looks like. I'm going to actually show you guys Phoenix
[00:05:58.400 --> 00:06:02.940]   today. Phoenix is our open source product. You guys are welcome to try it out and download
[00:06:02.940 --> 00:06:09.320]   it. This is actually Phoenix live for my application that I was talking about. And right now what you're
[00:06:09.320 --> 00:06:16.700]   actually looking at is a trace of the application, very simplified trace of the application. This is
[00:06:16.700 --> 00:06:25.700]   what a user asked me. Could you tell me if there is any current promotions for Samsung, whatever phone?
[00:06:25.700 --> 00:06:31.320]   And then this is actually the output that was responded from the application. Within this, and you can go and you can look through kind of all the different applications here, all the different kind of questions that users are asking,
[00:06:31.320 --> 00:06:38.700]   And you can actually see what kind of questions that users are asking here. Each one of these you'll actually see a full stack trace. What's most important here, and you kind of see the one I clicked on is one where it actually says it got the function
[00:06:38.700 --> 00:06:46.080]   call wrong. So I'm going to actually go dive into that one and we can go look at it. You can go look at it and see it says it got the function
[00:06:46.080 --> 00:06:53.080]   call wrong. It says the user is actually asking about current promotions for this phone. The generated function call is for a product search, which may not specifically address promotions. A more appropriate function call might be the one that directly querred the phone.
[00:06:53.080 --> 00:07:18.080]   So I'm going to actually go dive into that one and we can go look at it. You can go look at it and see it says it got the function call wrong, it says the user is actually asking about current promotions for this phone. The generated function call is for a product search which may not specifically address promotions, a more appropriate function call might be the one that directly queries promotions or discounts.
[00:07:18.080 --> 00:07:35.080]   We actually do have within the application a function call that's available for promos and discounts, might be easier to see that one in the slides, but it didn't actually call that one, it actually called the one that's specifically about product search instead.
[00:07:35.080 --> 00:07:45.080]   And so it called the wrong function call and this is actually one where the rest of the entire execution branch is going to be off because it got the first call wrong.
[00:07:45.080 --> 00:07:52.080]   So this is why it's really important to, if I had to zoom back out to just what do you care about in an application like this?
[00:07:52.080 --> 00:07:59.080]   You care about first your traces because you want to see what the heck's happening, where is it going down the flow.
[00:07:59.080 --> 00:08:12.080]   You care about evals, you care about evals knowing well something like where did it get in the application it wrong and then you also care about, we'll go into this explanations of the evaluation.
[00:08:12.080 --> 00:08:22.080]   So that when it gets it wrong, you get a view of actually where did it go wrong, what to go fix and what should I actually go do to iterate and improve the application.
[00:08:22.080 --> 00:08:28.080]   Traces, you want to evaluate it and then you want to use it to actually iterate on your application.
[00:08:28.080 --> 00:08:31.080]   That's kind of the loop that people do as they're building these.
[00:08:31.080 --> 00:08:36.080]   I can take this example, I can go add it to my data set for example.
[00:08:36.080 --> 00:08:45.080]   And then I can say, all right, every single time I get something wrong like this, I'm going to go build up my data set and then use these to now eventually run experiments.
[00:08:45.080 --> 00:08:54.080]   And run experiments where I can track and improve and this is one where I modified the prompt and I can run these experiments and then continuously iterate to make sure.
[00:08:54.080 --> 00:09:01.080]   Maybe the function description wasn't right, maybe the call from the LLM wasn't right.
[00:09:01.080 --> 00:09:10.080]   And so there's all sorts of things you can actually do to improve, but it really helps when you have evals at different levels of the application to be able to --
[00:09:10.080 --> 00:09:19.080]   evals at different levels of the application so that you know where to go focus and where to actually go improve.
[00:09:19.080 --> 00:09:25.080]   So with that, I'm going to actually jump to just some of the best practices we've seen from the ground.
[00:09:25.080 --> 00:09:31.080]   So you saw an example of basically a router-based application function calling evals.
[00:09:31.080 --> 00:09:34.080]   There's different types of levels that we see to applications.
[00:09:34.080 --> 00:09:40.080]   How many of you guys have a chatbot with multiple back and forth sessions basically in there?
[00:09:40.080 --> 00:09:48.080]   Well, typically you want evals at different levels of that, at a session level, often at a trace level, often at a span level.
[00:09:48.080 --> 00:09:55.080]   So getting the stuff to actually work in the real world isn't just single eval and we're good.
[00:09:55.080 --> 00:10:03.080]   It's often single eval, help me understand an explanation, let me drill down to where exactly, what component.
[00:10:03.080 --> 00:10:06.080]   And so these levels really help you do that.
[00:10:06.080 --> 00:10:10.080]   And what we see folks do is they actually start to do this in iterative phases.
[00:10:10.080 --> 00:10:14.080]   They first start off benchmarking the evals when they're building.
[00:10:14.080 --> 00:10:21.080]   And then as they're actually building the application and they're building each of the different components,
[00:10:21.080 --> 00:10:26.080]   they're developing those eval templates iteratively along the application.
[00:10:26.080 --> 00:10:30.080]   And then as they move into production, they can actually go monitor it, run it as jobs,
[00:10:30.080 --> 00:10:35.080]   but you're doing this as an iterative process as you're kind of building.
[00:10:35.080 --> 00:10:40.080]   If there's one thing you take away from my talk today, I hope it's actually this slide.
[00:10:40.080 --> 00:10:52.080]   Evals with explanations are by far what we see real people deploying applications, finding the most useful in production.
[00:10:52.080 --> 00:10:57.080]   A single incorrect, not incorrect is just really hard to know what to go fix.
[00:10:57.080 --> 00:11:02.080]   But when you have something like an explanation like we were looking at, it makes it easier for teams to go,
[00:11:02.080 --> 00:11:05.080]   okay, here's what I go fix, here's what I go dig into.
[00:11:05.080 --> 00:11:11.080]   And so run your evals with explanations if you can.
[00:11:11.080 --> 00:11:14.080]   There's different types of ways you can generate these evals actually.
[00:11:14.080 --> 00:11:19.080]   There's, if any of you guys are familiar with like, you know, in ML, there's like regression type of models,
[00:11:19.080 --> 00:11:21.080]   classification types of models, et cetera.
[00:11:21.080 --> 00:11:23.080]   Well, there's different types of evals too.
[00:11:23.080 --> 00:11:31.080]   There's numeric score outputs, there's categorical outputs, multi-outputs, multi-class.
[00:11:31.080 --> 00:11:33.080]   Can I actually, maybe this is a fun question.
[00:11:33.080 --> 00:11:38.080]   How many of you guys use numerical outputs as your LLM evals?
[00:11:38.080 --> 00:11:39.080]   Okay.
[00:11:39.080 --> 00:11:40.080]   Okay.
[00:11:40.080 --> 00:11:41.080]   A few brave folks.
[00:11:41.080 --> 00:11:43.080]   How many of you guys use categorical evals?
[00:11:43.080 --> 00:11:44.080]   Okay.
[00:11:44.080 --> 00:11:45.080]   Both.
[00:11:45.080 --> 00:11:46.080]   Okay.
[00:11:46.080 --> 00:11:47.080]   Nice.
[00:11:47.080 --> 00:11:52.080]   I'm going to actually share, we did a ton of research around this and we've been sharing about this,
[00:11:52.080 --> 00:12:01.080]   but if you are using numerical outputs today, I highly recommend you actually don't only rely on them.
[00:12:01.080 --> 00:12:05.080]   Here's a little research we shared and I'll share some results of this.
[00:12:05.080 --> 00:12:11.080]   But numeric scores, just for people who, you know, need a refresher on it,
[00:12:11.080 --> 00:12:16.080]   is you basically have the output of your LLM as a judge be a single number.
[00:12:16.080 --> 00:12:18.080]   And this is a simple example.
[00:12:18.080 --> 00:12:24.080]   I have a document, one where we've corrupted the document with a lot of spelling errors,
[00:12:24.080 --> 00:12:27.080]   and one where we've corrupted the document with very little spelling errors.
[00:12:27.080 --> 00:12:31.080]   So one of them, the corruption's like 80%, the other one's the corruption's like 11%.
[00:12:31.080 --> 00:12:39.080]   And we asked the LLM as a judge, hey, can you evaluate and tell us how bad of the spelling errors are actually in this document?
[00:12:39.080 --> 00:12:44.080]   And for both of them, it actually gave an eval score of 10 on it.
[00:12:44.080 --> 00:12:49.080]   And we actually noticed this was really consistent across all the foundational models.
[00:12:49.080 --> 00:12:53.080]   I think Mistral actually did pretty good compared to some of the rest.
[00:12:53.080 --> 00:13:01.080]   But across all the foundational models, it was actually pretty binary in how it did the scores.
[00:13:01.080 --> 00:13:06.080]   It was either a 0 or it was either a 1 or it was a 10.
[00:13:06.080 --> 00:13:11.080]   But it was never like this linear range of scores that you'd want it to expect.
[00:13:11.080 --> 00:13:16.080]   So as you increase the density of corruption, you actually get an increase in the number of scores.
[00:13:16.080 --> 00:13:20.080]   It was pretty binary, which kind of just indicated that if you're using numeric scores,
[00:13:20.080 --> 00:13:26.080]   it might not be the right way to evaluate because you're not going to catch the granularity.
[00:13:26.080 --> 00:13:29.080]   You know, an 80% doesn't actually mean anything.
[00:13:29.080 --> 00:13:33.080]   It's not going to really mean anything different than a 10% evaluation.
[00:13:33.080 --> 00:13:42.080]   So just a little best practices from the ground that we've been seeing as we've been running evals with customers.
[00:13:42.080 --> 00:13:45.080]   In the last like four minutes here, I'll share a couple more.
[00:13:45.080 --> 00:13:52.080]   This is slightly more model evals-related research for folks to kind of see the latest on that front.
[00:13:52.080 --> 00:13:57.080]   For folks who have been following the needle in a haystack test,
[00:13:57.080 --> 00:14:03.080]   this was a really popular one trending on Twitter recently.
[00:14:03.080 --> 00:14:07.080]   Needle in a haystack test was basically we put a needle in a haystack.
[00:14:07.080 --> 00:14:11.080]   We hid a fact in some context window.
[00:14:11.080 --> 00:14:15.080]   And the context window size can change.
[00:14:15.080 --> 00:14:19.080]   But the key thing we were also trying to figure out is,
[00:14:19.080 --> 00:14:22.080]   does placement in the context window matter?
[00:14:22.080 --> 00:14:27.080]   So this is an example where the fact was placed within the first 5% of the context window.
[00:14:27.080 --> 00:14:33.080]   This is an example where the context was placed kind of lower down in the context window, 90%.
[00:14:33.080 --> 00:14:35.080]   And the reason to do this type of research is,
[00:14:35.080 --> 00:14:38.080]   well, if you're using RAG, which I'm sure many of you guys in this room are,
[00:14:38.080 --> 00:14:43.080]   well, does it matter if what you put in the context window,
[00:14:43.080 --> 00:14:46.080]   that's the most important part, is actually lower in the document,
[00:14:46.080 --> 00:14:50.080]   does that actually impact the final output of the LLM that's given?
[00:14:50.080 --> 00:14:52.080]   And it turns out it actually does.
[00:14:52.080 --> 00:14:56.080]   So we did a lot of pressure testing against a number of foundational models.
[00:14:56.080 --> 00:15:00.080]   We do have the latest from the Opus model.
[00:15:00.080 --> 00:15:02.080]   I just don't have it in this deck right now.
[00:15:02.080 --> 00:15:07.080]   But this is actually results from Anthropic Cloud 2.1 versus GPT-4.
[00:15:07.080 --> 00:15:10.080]   The -- sorry, it's hard to read.
[00:15:10.080 --> 00:15:15.080]   But the x-axis on the bottom is basically the context window size.
[00:15:15.080 --> 00:15:19.080]   And then the y-axis is basically the depth in the document.
[00:15:19.080 --> 00:15:27.080]   And I mean, GPT-4 was for sure better in being able to retrieve the fact.
[00:15:27.080 --> 00:15:31.080]   But we consistently noticed actually that if you put the fact,
[00:15:31.080 --> 00:15:34.080]   especially as you increase the context window,
[00:15:34.080 --> 00:15:37.080]   if you put the fact earlier in the context window,
[00:15:37.080 --> 00:15:43.080]   it actually had a really hard time almost remembering or retrieving to pull that document.
[00:15:43.080 --> 00:15:49.080]   And so we repeatedly, as we ran this, saw this kind of --
[00:15:49.080 --> 00:15:52.080]   you know, red's where it gets it wrong, green's where it gets it right.
[00:15:52.080 --> 00:15:56.080]   But it consistently has this, like, red block earlier in the context window.
[00:15:56.080 --> 00:15:59.080]   So for folks who are actually using RAG,
[00:15:59.080 --> 00:16:02.080]   depending on how much information you're putting in the document,
[00:16:02.080 --> 00:16:09.080]   it's important to just balance where you place it in the document as well.
[00:16:09.080 --> 00:16:14.080]   Another couple research results, we also tested not just retrieval,
[00:16:14.080 --> 00:16:17.080]   but also retrieval with generation.
[00:16:17.080 --> 00:16:19.080]   What do I mean by generation?
[00:16:19.080 --> 00:16:23.080]   Well, after you did the retrieval, you can do things like generation on --
[00:16:23.080 --> 00:16:26.080]   it's kind of like the G in RAG.
[00:16:26.080 --> 00:16:29.080]   You actually generate a response after that.
[00:16:29.080 --> 00:16:32.080]   So some of the common types of generations were things like,
[00:16:32.080 --> 00:16:38.080]   from this financial document, round the numbers or map the dates or concatenate the strings.
[00:16:38.080 --> 00:16:41.080]   So these are all common types of generation tasks.
[00:16:41.080 --> 00:16:47.080]   And again, we stack ranked two different models against each other.
[00:16:47.080 --> 00:16:52.080]   This one's actually super interesting because GPT-4, which, you know,
[00:16:52.080 --> 00:16:57.080]   at that point, state-of-the-art, did worse than Anthropic 2.1,
[00:16:57.080 --> 00:16:59.080]   almost four times was worse.
[00:16:59.080 --> 00:17:06.080]   And we were really confused at why it was really great at retrieval,
[00:17:06.080 --> 00:17:08.080]   but it wasn't great at generation.
[00:17:08.080 --> 00:17:13.080]   And we kept going back and trying to understand, like, why this is over so many results,
[00:17:13.080 --> 00:17:15.080]   and talk to the team.
[00:17:15.080 --> 00:17:22.080]   And basically, we modified something in the prompt that made it so much better.
[00:17:22.080 --> 00:17:26.080]   We asked it to please explain yourself and then answer the question.
[00:17:26.080 --> 00:17:31.080]   If any of you guys have noticed, but Anthropic's models are slightly wordier.
[00:17:31.080 --> 00:17:36.080]   And actually, in this scenario, it was more of a feature versus a bug.
[00:17:36.080 --> 00:17:40.080]   But because it was wordier, it kept kind of asking itself to --
[00:17:40.080 --> 00:17:42.080]   it, like, thought through the process.
[00:17:42.080 --> 00:17:46.080]   And it thought through the process and then answered the question correctly
[00:17:46.080 --> 00:17:50.080]   and did the generation at the end as opposed to GPT-4.
[00:17:50.080 --> 00:17:55.080]   But when we asked GPT-4 to actually explain itself and then answer the question,
[00:17:55.080 --> 00:17:59.080]   it was able to get a pretty remarkable jump in performance on generation.
[00:17:59.080 --> 00:18:06.080]   So hopefully this was helpful to give you guys a view of just, like, different type of task
[00:18:06.080 --> 00:18:07.080]   and model evals.
[00:18:07.080 --> 00:18:12.080]   If you want to hear more about this, we're actually hosting an event, Arise Observe,
[00:18:12.080 --> 00:18:13.080]   on July 11th.
[00:18:13.080 --> 00:18:15.080]   This is my code for a free ticket.
[00:18:15.080 --> 00:18:20.080]   If any of you guys want to go, there's all sorts of researchers from OpenAI, Anthropic,
[00:18:20.080 --> 00:18:26.080]   Mistral, who are all coming to share model evals as well as builders who are sharing their
[00:18:26.080 --> 00:18:27.080]   own task evals.
[00:18:27.080 --> 00:18:28.080]   So check it out.
[00:18:28.080 --> 00:18:29.080]   Thanks, everyone.
[00:18:29.080 --> 00:18:30.080]   Thank you.
[00:18:30.080 --> 00:18:30.080]   Thank you.
[00:18:30.080 --> 00:18:30.080]   Thank you.
[00:18:30.080 --> 00:18:31.080]   Thank you.
[00:18:31.080 --> 00:18:31.080]   Thank you.
[00:18:31.080 --> 00:18:32.080]   Thank you.
[00:18:32.080 --> 00:18:33.080]   Thank you.
[00:18:33.080 --> 00:18:34.080]   Thank you.
[00:18:34.080 --> 00:18:35.080]   Thank you.
[00:18:35.080 --> 00:18:36.080]   Thank you.
[00:18:36.080 --> 00:18:37.080]   Thank you.
[00:18:37.080 --> 00:18:38.080]   Thank you.
[00:18:38.080 --> 00:18:39.080]   Thank you.
[00:18:39.080 --> 00:18:40.080]   Thank you.
[00:18:40.080 --> 00:18:41.080]   Thank you.
[00:18:41.080 --> 00:18:42.080]   Thank you.
[00:18:42.080 --> 00:18:43.080]   Thank you.
[00:18:43.080 --> 00:18:44.080]   Bye.
[00:18:44.080 --> 00:18:44.080]   Bye.
[00:18:44.080 --> 00:18:45.080]   Bye.
[00:18:45.080 --> 00:18:48.880]   We'll be right back.

