
[00:00:00.000 --> 00:00:02.480]   - Do you buy the model that once you have an AGI,
[00:00:02.480 --> 00:00:04.120]   you have a system that basically speeds up
[00:00:04.120 --> 00:00:05.160]   further AI research?
[00:00:05.160 --> 00:00:06.700]   Maybe not like an overnight sense,
[00:00:06.700 --> 00:00:08.560]   but over the course of months and years,
[00:00:08.560 --> 00:00:09.440]   you have much faster progress
[00:00:09.440 --> 00:00:10.280]   than you would have by the right side?
[00:00:10.280 --> 00:00:12.880]   - I think that's potentially possible.
[00:00:12.880 --> 00:00:15.840]   I think it partly depends what we decide,
[00:00:15.840 --> 00:00:18.280]   we as society decide to use the first AGI,
[00:00:18.280 --> 00:00:22.280]   nascent AGI systems or even proto-AGI systems for.
[00:00:22.280 --> 00:00:27.160]   So even the current LLMs seem to be pretty good at coding.
[00:00:27.160 --> 00:00:29.920]   So, and we have systems like AlphaCode,
[00:00:29.920 --> 00:00:31.600]   we also got theory improving systems.
[00:00:31.600 --> 00:00:35.560]   So one could imagine combining these ideas together
[00:00:35.560 --> 00:00:37.520]   and making them a lot better.
[00:00:37.520 --> 00:00:41.080]   And then I could imagine these systems being quite good
[00:00:41.080 --> 00:00:44.680]   at designing and helping us build future versions
[00:00:44.680 --> 00:00:46.000]   of themselves.
[00:00:46.000 --> 00:00:47.800]   But we also have to think about the safety implications
[00:00:47.800 --> 00:00:48.700]   of that, of course.
[00:00:48.700 --> 00:00:49.660]   - Yeah, I'm curious what you think about that.
[00:00:49.660 --> 00:00:51.960]   So, I mean, I'm not saying this is happening this year
[00:00:51.960 --> 00:00:54.400]   or anything, but eventually you'll be developing a model
[00:00:54.400 --> 00:00:56.720]   where during the process of development, you think,
[00:00:56.720 --> 00:00:58.960]   there's some chance that once this is fully developed,
[00:00:58.960 --> 00:01:00.880]   it will be capable of like an intelligence explosion,
[00:01:00.880 --> 00:01:02.560]   like dynamic.
[00:01:02.560 --> 00:01:05.400]   What would have to be true of that model at that point
[00:01:05.400 --> 00:01:08.640]   where you're like, I've seen these specific evals,
[00:01:08.640 --> 00:01:11.400]   I've like understand it's internal thinking enough
[00:01:11.400 --> 00:01:13.320]   and like it's future thinking that I'm comfortable
[00:01:13.320 --> 00:01:14.840]   continuing development of the system.
[00:01:14.840 --> 00:01:17.960]   - Well, look, we need a lot more understanding
[00:01:17.960 --> 00:01:19.080]   of the systems than we do today
[00:01:19.080 --> 00:01:21.760]   before I would be even confident of even explaining
[00:01:21.760 --> 00:01:24.560]   to you what we would need to tick box there.
[00:01:24.560 --> 00:01:26.680]   - Do you have a sense of what the converse answer
[00:01:26.680 --> 00:01:27.520]   would be?
[00:01:27.520 --> 00:01:28.960]   So what would have to be true where tomorrow morning
[00:01:28.960 --> 00:01:30.960]   you're like, we got to stop Gemini 2 training.
[00:01:30.960 --> 00:01:32.640]   Like what would specifically-
[00:01:32.640 --> 00:01:33.720]   - Yeah, I could imagine that.
[00:01:33.720 --> 00:01:35.800]   Like, and this is where, you know,
[00:01:35.800 --> 00:01:37.560]   things like the sandbox simulations,
[00:01:37.560 --> 00:01:40.840]   I would hope we're experimenting in a safe,
[00:01:40.840 --> 00:01:42.920]   secure environment.
[00:01:42.920 --> 00:01:45.520]   And then, you know, something happens in it,
[00:01:45.520 --> 00:01:48.480]   very unexpected happens, a new unexpected capability
[00:01:48.480 --> 00:01:49.960]   or something that we didn't want, you know,
[00:01:49.960 --> 00:01:52.400]   explicitly told the system we didn't want that it did,
[00:01:52.400 --> 00:01:53.880]   but then lied about, you know,
[00:01:53.880 --> 00:01:57.000]   these are the kinds of things where one would want to
[00:01:57.000 --> 00:01:59.880]   then dig in carefully, pause,
[00:01:59.880 --> 00:02:03.440]   and then really get to the bottom of why it was doing
[00:02:03.440 --> 00:02:05.000]   those things before one continued.
[00:02:05.000 --> 00:02:07.360]   There are many, many ideas that people have
[00:02:07.360 --> 00:02:09.880]   from much more stringent eval systems.
[00:02:09.880 --> 00:02:11.760]   I think we don't have a good enough evaluations
[00:02:11.760 --> 00:02:15.560]   and benchmarks for things like, can the system deceive you?
[00:02:15.560 --> 00:02:17.040]   Can it exfiltrate its own code?
[00:02:17.040 --> 00:02:19.480]   Sort of undesirable behaviors.
[00:02:19.480 --> 00:02:22.000]   And then there's, you know,
[00:02:22.000 --> 00:02:25.880]   ideas of actually using AI, maybe narrow AIs,
[00:02:25.880 --> 00:02:27.200]   so not general learning ones,
[00:02:27.200 --> 00:02:29.840]   but systems that are specialized for a domain
[00:02:29.840 --> 00:02:33.960]   to help us as the human scientists analyze
[00:02:33.960 --> 00:02:37.240]   and summarize what the more general system is doing, right?
[00:02:37.240 --> 00:02:39.920]   So kind of narrow AI tools.
[00:02:39.920 --> 00:02:41.720]   I think that there's a lot of promise
[00:02:41.720 --> 00:02:44.920]   in creating hardened sandboxes or simulations
[00:02:44.920 --> 00:02:47.760]   so that they're hardened with cybersecurity
[00:02:47.760 --> 00:02:51.680]   arrangements around the simulation,
[00:02:51.680 --> 00:02:53.840]   both to keep the AI in,
[00:02:53.840 --> 00:02:57.320]   but also as cybersecurity to keep hackers out.
[00:02:57.320 --> 00:03:00.280]   And then you could experiment a lot more freely
[00:03:00.280 --> 00:03:01.920]   within that sandbox domain.

