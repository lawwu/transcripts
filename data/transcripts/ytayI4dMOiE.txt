
[00:00:00.000 --> 00:00:03.900]   some I'm gonna hit go live on YouTube and wait for an echo and
[00:00:03.900 --> 00:00:07.080]   meanwhile, rotate my head like a chimp bunk and cringe at that
[00:00:07.080 --> 00:00:10.720]   when I see the recording, but the steps you need to take to
[00:00:10.720 --> 00:00:12.600]   ensure things are working as they should.
[00:00:12.600 --> 00:00:27.960]   I hear the echo which means we're live. Incredible. Sorry
[00:00:27.960 --> 00:00:31.340]   for that awkward silence. I just I sometimes need to refresh
[00:00:31.340 --> 00:00:36.780]   pages. Welcome back everyone. It's quite exciting session
[00:00:36.780 --> 00:00:39.220]   again today because we're training our first CNN model.
[00:00:39.220 --> 00:00:43.660]   Last time we had trained our first neural network using
[00:00:43.660 --> 00:00:59.580]   pytorch. I'm sorry, let me take my headphones off. So someone
[00:00:59.580 --> 00:01:04.180]   had mentioned that I'm a little quiet in the recordings. And
[00:01:04.180 --> 00:01:06.820]   I'm happy to receive any feedback. So the reason I took
[00:01:06.820 --> 00:01:10.460]   off my headphones was to make sure I don't hear myself
[00:01:10.460 --> 00:01:14.140]   talking I get I get to hear myself this this mic gives me a
[00:01:14.140 --> 00:01:17.940]   feedback. So that's why I was a little quiet in the early
[00:01:17.940 --> 00:01:21.180]   recordings would happen in the future ones. Sorry about that.
[00:01:21.180 --> 00:01:26.860]   Okay, so like I was mentioning, it's an interesting
[00:01:26.860 --> 00:01:30.260]   intersection again, because we're going to look at our
[00:01:30.260 --> 00:01:33.860]   first CNN model today, we'll also learn what convolutions
[00:01:33.860 --> 00:01:37.460]   are. And I'll try to recap what we learned in the last lecture.
[00:01:37.460 --> 00:01:42.060]   As a reminder, I really look up to fast AI, I really look up to
[00:01:42.060 --> 00:01:45.300]   Jeremy Howard, and I'm trying to follow their style. So I'm
[00:01:45.300 --> 00:01:48.220]   trying to structure the book a little differently to make it
[00:01:48.220 --> 00:01:51.900]   top down in a way where you get to see the application first and
[00:01:51.900 --> 00:01:55.660]   I skip the theoretical explanation for later. Even
[00:01:55.660 --> 00:01:58.300]   though I trust the authors placed it there for a reason,
[00:01:58.300 --> 00:02:04.560]   I'm trying to make it in a different structure, just
[00:02:04.560 --> 00:02:10.180]   because I think that might be easier for things or for you as
[00:02:10.180 --> 00:02:14.100]   you learn these things. So if things are a little off from
[00:02:14.100 --> 00:02:17.460]   the book, that's why they are just as a reminder. With that,
[00:02:17.460 --> 00:02:19.060]   let me share my screen.
[00:02:31.420 --> 00:02:36.340]   And move out the zoom things off the page from the way. Minimize
[00:02:36.340 --> 00:02:43.300]   that. Put this in full screen. And I'll hit present. Awesome.
[00:02:43.300 --> 00:02:47.460]   So the title for today's session is training your first CNN using
[00:02:47.460 --> 00:02:52.660]   torch. Last week, we had first trained up first neural network.
[00:02:52.660 --> 00:02:57.000]   Again, we'll start by understanding how do we do this
[00:02:57.000 --> 00:03:00.420]   manually? How do we do this by hand? I'll recap what we learned
[00:03:00.420 --> 00:03:04.580]   in the last lecture, I find this to be quite useful. I repeat
[00:03:04.580 --> 00:03:08.620]   things over every lecture. I'm calling this a lecture, but it's
[00:03:08.620 --> 00:03:12.060]   really a reading group. And I find it useful to repeat things
[00:03:12.060 --> 00:03:16.080]   just because that helps create a stronger mental model. So things
[00:03:16.080 --> 00:03:19.300]   might be repeated, but that's for making sure that you
[00:03:19.300 --> 00:03:22.940]   remember these concepts, at least the ones that I think are
[00:03:22.940 --> 00:03:27.080]   quite impactful. So I'll remind you all again of the homework,
[00:03:27.080 --> 00:03:30.480]   which I don't think anyone did. But even if you did, I didn't
[00:03:30.480 --> 00:03:35.640]   see you post about it. I think it's it's a nice point in our
[00:03:35.640 --> 00:03:38.880]   study group where you all can start writing about this. I, I
[00:03:38.880 --> 00:03:42.480]   just strongly want to encourage that. So if there's any reason
[00:03:42.480 --> 00:03:47.140]   that's stopping you from creating a repository or writing
[00:03:47.140 --> 00:03:50.520]   a blog post about these things, please reach out to me and I'd
[00:03:50.520 --> 00:03:53.720]   be very happy to help out very happy to review your blog posts
[00:03:53.760 --> 00:03:58.360]   if if at all. The homework from last section and these were
[00:03:58.360 --> 00:04:01.000]   open ended questions, I requested you all to check out
[00:04:01.000 --> 00:04:05.320]   different loss functions and torch. Try new activation
[00:04:05.320 --> 00:04:09.000]   functions, see how they play in, play around with hyper
[00:04:09.000 --> 00:04:12.880]   parameters. So last week, we had a hidden neural network with
[00:04:12.880 --> 00:04:19.760]   512 neurons. I had suggested you could try changing that. And as
[00:04:19.760 --> 00:04:22.680]   we had observed, it was overfitting to the training
[00:04:22.680 --> 00:04:27.920]   data set. So what changing that help at all? Spoiler alerted,
[00:04:27.920 --> 00:04:30.320]   it shouldn't because it's a fully connected neural network
[00:04:30.320 --> 00:04:36.120]   and we learn by that is overfitting. And I'll again
[00:04:36.120 --> 00:04:38.960]   reiterate this, but again, you're at a point where you
[00:04:38.960 --> 00:04:42.200]   shouldn't be sticking just to the book. So for example, last
[00:04:42.200 --> 00:04:46.800]   week, you looked at CIFAR 10. This is a really good place where
[00:04:46.800 --> 00:04:50.440]   you can also take a look at CIFAR 100. Last week, we were
[00:04:50.440 --> 00:04:54.880]   trying to identify birds and airplanes from it. You could
[00:04:54.880 --> 00:04:58.440]   just pick three, five, 10 different classes from CIFAR
[00:04:58.440 --> 00:05:02.000]   100. Maybe maybe ones that are different from CIFAR 10 so that
[00:05:02.000 --> 00:05:06.360]   you don't create another CIFAR 10. Just take five classes, try
[00:05:06.360 --> 00:05:10.200]   to train a model on that. And maybe try different transforms
[00:05:10.200 --> 00:05:14.840]   inside of TorchVision. So why am I like spending five minutes
[00:05:14.840 --> 00:05:18.560]   every week speaking about this because at least for me, it's
[00:05:18.560 --> 00:05:23.600]   been really hard to spend more time on documentation and spend
[00:05:23.600 --> 00:05:28.320]   lesser time reading more about blog posts, reading more about
[00:05:28.320 --> 00:05:31.240]   theoretical stuff, reading more about papers, all of that is
[00:05:31.240 --> 00:05:34.360]   important. That's quite important. But since we're
[00:05:34.360 --> 00:05:37.560]   highlighting the important theoretical concepts every week,
[00:05:37.560 --> 00:05:40.560]   since we're already going through that, it's I think it's
[00:05:40.560 --> 00:05:43.840]   it's a good balance if you spend time doing all of these things.
[00:05:44.840 --> 00:05:49.960]   So as a reminder, I'll drop this link in both of the charts, the
[00:05:49.960 --> 00:06:05.560]   zoom and the YouTube one once I can copy it correctly. Please
[00:06:05.600 --> 00:06:18.360]   ask questions here. And let me bring it to the YouTube chat so
[00:06:18.360 --> 00:06:26.400]   that everyone who joins later can see it. Awesome. So this is
[00:06:26.400 --> 00:06:29.040]   the place for asking any questions. If you go to this
[00:06:29.040 --> 00:06:33.720]   link, it should take you to our forums. And I'll be looking here
[00:06:33.720 --> 00:06:39.560]   for all questions and answers. So and in fact, I do have a few
[00:06:39.560 --> 00:06:43.280]   questions for today. So let me jump to that and ask those here
[00:06:43.280 --> 00:06:46.080]   because I'll come back to your answers for this. So this is
[00:06:46.080 --> 00:06:48.520]   more of a live question right now not a homework. Please try
[00:06:48.520 --> 00:06:51.480]   to answer these questions. And I'll look out for answers and
[00:06:51.480 --> 00:06:59.360]   also monitor any questions that come in from you. Dhruv please
[00:06:59.360 --> 00:07:02.080]   feel free to reach out to me if you have any problems setting up
[00:07:02.080 --> 00:07:09.400]   your blog or to the to the community broadly as well. Okay.
[00:07:09.400 --> 00:07:13.720]   And you can also like each other's questions. Just as a
[00:07:13.720 --> 00:07:18.000]   reminder, please, please get me beyond Aman's likes. It's an
[00:07:18.000 --> 00:07:21.720]   internal challenge that we're trying, but I want to beat him
[00:07:21.720 --> 00:07:27.080]   on the number of likes. Okay, enough jokes. Let's talk
[00:07:27.080 --> 00:07:31.560]   business. Now. I want to remind you all of these questions that
[00:07:31.560 --> 00:07:36.280]   we went through last week. The first one is why do we choose
[00:07:36.280 --> 00:07:39.960]   to activation function that usually has a linearity around
[00:07:39.960 --> 00:07:43.960]   zero one. So by that I mean, when if you look closely at any
[00:07:43.960 --> 00:07:50.000]   activation functions graph, why is it it's output most active
[00:07:50.000 --> 00:07:55.880]   around this range roughly or minus one to one sometimes. Why
[00:07:55.880 --> 00:07:59.320]   do we provide a dimension in softmax? You can look at the
[00:07:59.320 --> 00:08:03.240]   docs that should be easy to answer. We did this in code
[00:08:03.240 --> 00:08:07.200]   somewhere last last session, I didn't answer it because I was
[00:08:07.200 --> 00:08:10.000]   hoping someone might ask no one did. So I'll again, I separately
[00:08:10.000 --> 00:08:15.720]   why what does view three comma minus one do when you just get a
[00:08:15.720 --> 00:08:21.560]   view of a tossed tensor using those parameters. And now let's
[00:08:21.560 --> 00:08:24.760]   look at steps in traditional machine learning pipeline to
[00:08:24.760 --> 00:08:29.600]   detect words because I'll use this to explain CNNs. So if
[00:08:29.600 --> 00:08:32.320]   anyone would like to take a minute to answer steps in
[00:08:32.320 --> 00:08:35.480]   traditional machine learning pipeline to detect both so by
[00:08:35.480 --> 00:08:37.920]   that time, I mean, you're not allowed to use CNNs if you've
[00:08:37.920 --> 00:08:42.440]   used open CV two or any other tool, would you like to answer
[00:08:42.440 --> 00:08:45.360]   how can you detect boards using that?
[00:08:45.360 --> 00:08:45.880]   Okay.
[00:08:46.880 --> 00:08:47.280]   Okay.
[00:08:47.280 --> 00:08:47.880]   Okay.
[00:08:47.880 --> 00:08:56.880]   [Yes, we're good to go. Thanks, everyone. See you next time.]
[00:08:56.880 --> 00:09:24.880]   [Thanks, everyone. See you next time.]
[00:09:24.880 --> 00:09:52.880]   [Thanks, everyone. See you next time.]
[00:09:52.880 --> 00:09:54.880]   I let another answer come in.
[00:09:54.880 --> 00:10:18.880]   [Thanks, everyone. See you next time.]
[00:10:18.880 --> 00:10:27.880]   Yes, that's the right answer.
[00:10:27.880 --> 00:10:36.880]   Yes, that's the right answer.
[00:10:36.880 --> 00:11:02.880]   [Yes, that's the right answer.
[00:11:02.880 --> 00:11:08.880]   Let me pop to Outlook and talk about this.
[00:11:08.880 --> 00:11:12.880]   Let me just find some free space around here.
[00:11:12.880 --> 00:11:16.880]   I ideally want to have a blank page, but I couldn't figure that out.
[00:11:16.880 --> 00:11:22.880]   So I always take notes in the ebook, which is fine because I can always
[00:11:22.880 --> 00:11:25.880]   create copies, but it sort of spoils the view a little.
[00:11:25.880 --> 00:11:32.880]   Let me grab.
[00:11:32.880 --> 00:11:37.880]   Sorry, guys, Zoom is being a little annoying today with the pop-up.
[00:11:37.880 --> 00:11:40.880]   So I'm just trying to set all of that properly.
[00:11:40.880 --> 00:11:54.880]   I think we should be good now.
[00:11:54.880 --> 00:11:55.880]   Sorry about that.
[00:11:55.880 --> 00:12:06.880]   So the first step usually and we had looked at this in the second
[00:12:06.880 --> 00:12:13.880]   meetup, the first step would be usually to extract.
[00:12:13.880 --> 00:12:15.880]   That would be the second step. I'm sorry.
[00:12:15.880 --> 00:12:22.880]   So the first step would be to apply some some form of blurring.
[00:12:22.880 --> 00:12:28.880]   Or maybe convert the image to black and white.
[00:12:28.880 --> 00:12:32.880]   So some form of pre-processing so that we can work on it.
[00:12:32.880 --> 00:12:38.880]   Later we try to extract edges, maybe sharpen them, maybe sharpen things a little.
[00:12:38.880 --> 00:12:43.880]   Let's see what could we do after that.
[00:12:43.880 --> 00:12:47.880]   So now we have nicely pre-processed images.
[00:12:47.880 --> 00:12:49.880]   The edges are nicely sharp.
[00:12:49.880 --> 00:12:54.880]   You could also play around with contrast further.
[00:12:54.880 --> 00:12:59.880]   So you could change the contrast that is quite helpful, especially for
[00:12:59.880 --> 00:13:03.880]   the older techniques, not so much for the recent techniques, older techniques being
[00:13:03.880 --> 00:13:06.880]   non-neural networks, recent being neural networks.
[00:13:06.880 --> 00:13:11.880]   So if you play around with contrast, that helps things pop.
[00:13:11.880 --> 00:13:14.880]   By pop, I mean they're quite different from their background.
[00:13:14.880 --> 00:13:19.880]   So for example, right now, if you look at my image in Zoom or on the YouTube feed,
[00:13:19.880 --> 00:13:21.880]   the background is a little blurred.
[00:13:21.880 --> 00:13:26.880]   And that's because this camera is quite focused on me and it's making the background.
[00:13:26.880 --> 00:13:29.880]   It's making me pop out of the background.
[00:13:29.880 --> 00:13:35.880]   So that's what we want to happen for the neural network or machine learning model as well.
[00:13:35.880 --> 00:13:38.880]   With neural networks, it's not as required.
[00:13:38.880 --> 00:13:41.880]   Sometimes it's helpful, not always.
[00:13:41.880 --> 00:13:44.880]   But for traditional pipelines, it's quite helpful.
[00:13:44.880 --> 00:13:51.880]   The step after that is you could apply SVM, some form of technique that could
[00:13:51.880 --> 00:13:53.880]   finally predict what's there in the image.
[00:13:53.880 --> 00:13:56.880]   So as you can see, it's quite a few steps.
[00:13:56.880 --> 00:14:03.880]   And the claim I made back then, which I didn't prove, but the claim was these first three
[00:14:03.880 --> 00:14:06.880]   steps can be replaced by neural networks.
[00:14:06.880 --> 00:14:12.880]   So step one, step two, and step three all get combined into a neural network.
[00:14:12.880 --> 00:14:14.880]   I mean, you still have to create a dataset class.
[00:14:14.880 --> 00:14:16.880]   You still have to create a data loader.
[00:14:16.880 --> 00:14:19.880]   You still have to worry about those things.
[00:14:19.880 --> 00:14:23.880]   But broadly, all of these steps are quite taken care of.
[00:14:23.880 --> 00:14:29.880]   So let's head back to Safari.
[00:14:29.880 --> 00:14:31.880]   And I'd like to point this paper out.
[00:14:31.880 --> 00:14:33.880]   It's by Zeiler and Fergus.
[00:14:33.880 --> 00:14:35.880]   I learned about this through the FastAI course.
[00:14:35.880 --> 00:14:41.880]   And they talk about visualizing and understanding CNNs.
[00:14:41.880 --> 00:14:46.880]   Yesterday during a community meetup, a few people had mentioned they're interested in checking papers out.
[00:14:46.880 --> 00:14:48.880]   This is quite readable.
[00:14:48.880 --> 00:14:51.880]   By that I mean it's not too jargon heavy.
[00:14:51.880 --> 00:14:53.880]   I can encourage you all to check this out.
[00:14:53.880 --> 00:15:06.880]   Let me post a link to this on here.
[00:15:06.880 --> 00:15:10.880]   So what the authors have done here is they've analyzed a CNN model.
[00:15:10.880 --> 00:15:12.880]   I won't mention which one.
[00:15:12.880 --> 00:15:15.880]   That's homework for you to check out.
[00:15:15.880 --> 00:15:18.880]   And they've visualized what's happening inside of the data.
[00:15:18.880 --> 00:15:21.880]   Right now I haven't explained what a CNN is.
[00:15:21.880 --> 00:15:25.880]   We're jumping directly into it and there's a reason I'll come back to it.
[00:15:25.880 --> 00:15:30.880]   And we're looking at different layers right now.
[00:15:30.880 --> 00:15:32.880]   So let's read the subtext first.
[00:15:32.880 --> 00:15:36.880]   This is visualizations of features in a fully trained model.
[00:15:36.880 --> 00:15:39.880]   So that means the model is quite accurate.
[00:15:39.880 --> 00:15:44.880]   Let's assume it at that time was one of the most accurate models on this dataset.
[00:15:44.880 --> 00:15:46.880]   ImageNet.
[00:15:46.880 --> 00:15:52.880]   Layers 2 to 5, we show the top 9 activation in a random subset of feature maps.
[00:15:52.880 --> 00:15:53.880]   What are feature maps?
[00:15:53.880 --> 00:15:55.880]   Let's ignore that.
[00:15:55.880 --> 00:16:03.880]   But the thing being visualized here is we're looking inside the model and we're trying to figure out what every layer is learning.
[00:16:03.880 --> 00:16:14.880]   So remember when I showed you these steps that every traditional machine learning model used to take and I claimed that CNN or a deep learning model could replace.
[00:16:14.880 --> 00:16:17.880]   As you can see that quite literally is the case.
[00:16:17.880 --> 00:16:23.880]   So if you look closely, the first layer is somewhat looking, learning about colors.
[00:16:23.880 --> 00:16:32.880]   Maybe the second layer is learning about, it looks like edges, maybe a few circles that look like eyes.
[00:16:32.880 --> 00:16:33.880]   Not really.
[00:16:33.880 --> 00:16:38.880]   Maybe some shapes, ellipses, brown circles.
[00:16:38.880 --> 00:16:46.880]   If we look over to layer 3, more abstractness is being added.
[00:16:46.880 --> 00:16:48.880]   So this looks like a person to me.
[00:16:48.880 --> 00:16:51.880]   I can confirm it by looking at the right.
[00:16:51.880 --> 00:16:56.880]   These things look like pronounced objects.
[00:16:56.880 --> 00:16:59.880]   So first we learned about edges, then colors.
[00:16:59.880 --> 00:17:02.880]   Now we're learning about objects.
[00:17:02.880 --> 00:17:08.880]   So we're looking inside of a neural network and we're trying to understand what every layer is learning.
[00:17:08.880 --> 00:17:15.880]   Then we go further out and as you can see now it's learning faces, all of those things.
[00:17:15.880 --> 00:17:21.880]   And towards the last end, it quite understands what's going on inside of the images.
[00:17:21.880 --> 00:17:28.880]   The key thing here is we didn't design neural networks specifically to learn all of these things.
[00:17:28.880 --> 00:17:37.880]   I mean, we did arrive at a design that works really well for this, but we didn't design it specifically for this purpose.
[00:17:37.880 --> 00:17:44.880]   So when the creator designed a model, they didn't specify to layer 5 in the model design.
[00:17:44.880 --> 00:17:50.880]   Hey, layer 5, you need to learn about people's faces in this particular region.
[00:17:50.880 --> 00:17:53.880]   Or about, what are these, keyboards?
[00:17:53.880 --> 00:17:55.880]   Keyboards in layer 5.
[00:17:55.880 --> 00:17:58.880]   Or circles in layer 3.
[00:17:58.880 --> 00:18:06.880]   So that's truly the power of deep learning and that is also how different models learn.
[00:18:06.880 --> 00:18:08.880]   So just wanted to point that out.
[00:18:08.880 --> 00:18:12.880]   And now we'll learn, or I said learn a bunch of times.
[00:18:12.880 --> 00:18:17.880]   We'll go through CNNs and try to understand how do these things happen internally.
[00:18:17.880 --> 00:18:20.880]   But this is broadly how they work.
[00:18:20.880 --> 00:18:25.880]   So you pass in an image and through some black magic, which I haven't explained.
[00:18:25.880 --> 00:18:28.880]   It's not black magic, it's actually just matrix multiplications.
[00:18:28.880 --> 00:18:34.880]   Through some format of matrix multiplication, every layer learns about the objects at a certain level of abstraction.
[00:18:34.880 --> 00:18:36.880]   I mean, what is an object really, right?
[00:18:36.880 --> 00:18:42.880]   It's a bunch of shapes, a bunch of edges, a bunch of abstractness.
[00:18:42.880 --> 00:18:49.880]   So when I look at something that has two eyes, four legs, some fur, could be a dog, right?
[00:18:49.880 --> 00:18:53.880]   So that's how the neural network is also learning, turns out.
[00:18:53.880 --> 00:18:56.880]   It sees edges, it sees shapes.
[00:18:56.880 --> 00:19:00.880]   Later it sees the complete face and then it says, oh, that's a dog.
[00:19:00.880 --> 00:19:01.880]   Yep.
[00:19:01.880 --> 00:19:04.880]   90% chance that's Bobby.
[00:19:04.880 --> 00:19:09.880]   Bobby's our dog from the earlier session, if you remember.
[00:19:09.880 --> 00:19:18.880]   So this is how CNNs work and every layer adds a bit of complexity, a bit of abstractness to whatever the previous layer had learned.
[00:19:18.880 --> 00:19:20.880]   How does it do it?
[00:19:20.880 --> 00:19:24.880]   I'll explain it not thoroughly, but I wanted to share this intuition first with this paper.
[00:19:24.880 --> 00:19:26.880]   You can read it out.
[00:19:26.880 --> 00:19:33.880]   I just wanted to highlight this section.
[00:19:33.880 --> 00:19:39.880]   So as I mentioned, the agenda for today is to again, decap from last group, from the last lecture.
[00:19:39.880 --> 00:19:41.880]   Glance through chapter seven and eight.
[00:19:41.880 --> 00:19:44.880]   So we'll be wrapping up the first part of the book.
[00:19:44.880 --> 00:19:48.880]   I'll spend more time explaining the code and what's going on inside of different parts.
[00:19:48.880 --> 00:19:51.880]   And I'll keep pausing for questions and answers.
[00:19:51.880 --> 00:19:53.880]   As a reminder, I've already shared this link.
[00:19:53.880 --> 00:19:56.880]   Let me put it in the Zoom chat again.
[00:19:56.880 --> 00:20:13.880]   All questions are welcomed in this link, just because it's really hard for me to monitor everything.
[00:20:13.880 --> 00:20:17.880]   Awesome.
[00:20:17.880 --> 00:20:21.880]   So we looked at modeling last, last.
[00:20:21.880 --> 00:20:26.880]   I want to say lecture, but let's, let's just call it in the last call.
[00:20:26.880 --> 00:20:28.880]   Here are the five basic steps of modeling.
[00:20:28.880 --> 00:20:30.880]   You need to gather the data you need to visualize.
[00:20:30.880 --> 00:20:38.880]   Start with the simplest model, split with an independent evaluation state, set, iterate until the model fits.
[00:20:38.880 --> 00:20:45.880]   Hopefully it will overfit and then you cut back.
[00:20:45.880 --> 00:20:47.880]   How do you select the activation functions?
[00:20:47.880 --> 00:20:49.880]   They should be nonlinear.
[00:20:49.880 --> 00:20:52.880]   They should allow approximating more complex function.
[00:20:52.880 --> 00:21:01.880]   They should be differentiable because remember autograd, they should have a sensitive range.
[00:21:01.880 --> 00:21:03.880]   Chapter seven highlights.
[00:21:03.880 --> 00:21:05.880]   We looked at CIFAR 10 dataset.
[00:21:05.880 --> 00:21:08.880]   We defined a torch dataset.
[00:21:08.880 --> 00:21:11.880]   We learned what transforms were from torch vision.
[00:21:11.880 --> 00:21:14.880]   We looked at two tensor, normalize.
[00:21:14.880 --> 00:21:15.880]   What does that do?
[00:21:15.880 --> 00:21:16.880]   Why is that important?
[00:21:16.880 --> 00:21:19.880]   And we understood new optimizer and loss functions.
[00:21:19.880 --> 00:21:23.880]   We also looked at N L L loss, whatever that stands for.
[00:21:23.880 --> 00:21:27.880]   You can look up the docs if you don't remember.
[00:21:27.880 --> 00:21:34.880]   But if I go back to this problem that adds mentioned of modeling, we had looked at a fully connected network.
[00:21:34.880 --> 00:21:36.880]   We gathered our data from CIFAR 10.
[00:21:36.880 --> 00:21:39.880]   We had two classes, right?
[00:21:39.880 --> 00:21:41.880]   Birds and airplanes.
[00:21:41.880 --> 00:21:44.880]   We visualize the data we plotted a red car.
[00:21:44.880 --> 00:21:48.880]   If you remember, we started with the simplest model that came to mind.
[00:21:48.880 --> 00:21:51.880]   It was a fully connected model.
[00:21:51.880 --> 00:21:53.880]   And then we iterated.
[00:21:53.880 --> 00:21:59.880]   We learned that the model doesn't generalize too well.
[00:21:59.880 --> 00:22:03.880]   So the problems with fully connected networks are they prone to overfitting?
[00:22:03.880 --> 00:22:05.880]   They're positionally dependent.
[00:22:05.880 --> 00:22:15.880]   Let me elaborate what this means.
[00:22:15.880 --> 00:22:22.880]   So if you remember what we've done last lecture, and if I switch to a different color.
[00:22:22.880 --> 00:22:30.880]   So CIFAR 10 has images in the following dimensions.
[00:22:30.880 --> 00:22:35.880]   Three, because it's RGB and the image sizes are 32 by 32.
[00:22:35.880 --> 00:22:43.880]   So what we did last time, we combined all of this into a single layer.
[00:22:43.880 --> 00:22:46.880]   So we had 1072 of these.
[00:22:46.880 --> 00:22:50.880]   Now the way we did this.
[00:22:50.880 --> 00:22:55.880]   So an image, if you look closely, an image is just a collection of numbers, right?
[00:22:55.880 --> 00:22:59.880]   Essentially, you can think of this as 32 by 32.
[00:22:59.880 --> 00:23:06.880]   But what we did, we took all of these layers and stacked them together.
[00:23:06.880 --> 00:23:11.880]   So essentially, this goes here.
[00:23:11.880 --> 00:23:14.880]   The second layer comes here.
[00:23:14.880 --> 00:23:24.880]   Now the problem with that is if you have a human in here, and as you can see,
[00:23:24.880 --> 00:23:26.880]   I have quite the exceptional drawing skills.
[00:23:26.880 --> 00:23:30.880]   I expect you all to excuse that.
[00:23:30.880 --> 00:23:38.880]   This is what you might end up with, which at best makes some sense.
[00:23:38.880 --> 00:23:44.880]   I mean, it's not necessary that things that make sense to us should help a neural network.
[00:23:44.880 --> 00:23:46.880]   Neural network can find hidden patterns.
[00:23:46.880 --> 00:23:51.880]   That's the promise of them for using neural networks in the first place.
[00:23:51.880 --> 00:23:54.880]   But this is not the most ideal scenario, right?
[00:23:54.880 --> 00:23:58.880]   So a person is defined just like so.
[00:23:58.880 --> 00:24:02.880]   If the person is a bit rotated towards the left, towards the right,
[00:24:02.880 --> 00:24:07.880]   the image is incomplete, that becomes a problem.
[00:24:07.880 --> 00:24:09.880]   And we can't account for that.
[00:24:09.880 --> 00:24:13.880]   So one of the ways to account for that would be you could look at the data,
[00:24:13.880 --> 00:24:18.880]   you could try to augment it, you could try changing the images.
[00:24:18.880 --> 00:24:22.880]   But that's not the ideal case.
[00:24:22.880 --> 00:24:24.880]   That's not the best case.
[00:24:24.880 --> 00:24:32.880]   So that's why we'd like to use or we introduce convolutions.
[00:24:32.880 --> 00:24:36.880]   I'll switch back to sharing the right screen.
[00:24:36.880 --> 00:24:39.880]   Ideally, I want to be quite fast at this.
[00:24:39.880 --> 00:24:43.880]   You all have been coming here for a few weeks, so you know I'm getting faster.
[00:24:43.880 --> 00:24:47.880]   But sorry about the little lag that happens when I switch between windows.
[00:24:47.880 --> 00:24:51.880]   So convolutions, here's the promise and how do they work?
[00:24:51.880 --> 00:24:54.880]   I'll talk about that in a second.
[00:24:54.880 --> 00:24:58.880]   But they're local operations on neighborhoods.
[00:24:58.880 --> 00:25:03.880]   So the problem earlier was we weren't accounting for what's happening
[00:25:03.880 --> 00:25:07.880]   inside of an image or what's happening in different areas of the image.
[00:25:07.880 --> 00:25:11.880]   So for example, I'm in the middle of this frame right now.
[00:25:11.880 --> 00:25:16.880]   You could see my monitor in the background, you could see my PC towards the right.
[00:25:16.880 --> 00:25:21.880]   But there's no knowledge for the model, the fully connected model
[00:25:21.880 --> 00:25:26.880]   about different spatial placements of things.
[00:25:26.880 --> 00:25:29.880]   That's one thing that we solve here.
[00:25:29.880 --> 00:25:31.880]   These are translation invariant.
[00:25:31.880 --> 00:25:35.880]   By that, I mean if I move towards the left or move towards the right,
[00:25:35.880 --> 00:25:37.880]   you don't have to worry too much.
[00:25:37.880 --> 00:25:42.880]   And this model would have a few fewer parameters
[00:25:42.880 --> 00:25:44.880]   compared to the fully connected one.
[00:25:44.880 --> 00:25:50.880]   So last week, if you remember, we had 3.7 million parameters
[00:25:50.880 --> 00:25:53.880]   for a simple model that was overfitting to two classes.
[00:25:53.880 --> 00:25:58.880]   And now if you look at any recent architecture,
[00:25:58.880 --> 00:26:01.880]   that's still a reasonable number, but not the ideal case.
[00:26:01.880 --> 00:26:06.880]   So that means some of these parameters are being wasted.
[00:26:06.880 --> 00:26:10.880]   They aren't learning what they're supposed to, or they aren't doing the best.
[00:26:10.880 --> 00:26:14.880]   So that's why we use CNNs.
[00:26:14.880 --> 00:26:20.880]   And let me again switch back to the pen and paper explanation of it.
[00:26:20.880 --> 00:26:24.880]   So to the person asking for the resources link,
[00:26:24.880 --> 00:26:31.880]   all of these are being shared in this chart.
[00:26:31.880 --> 00:26:37.880]   I always get distracted with reading the answers, so I'll spend a minute here.
[00:26:37.880 --> 00:26:40.880]   You're right about this approach.
[00:26:40.880 --> 00:26:45.880]   So I had given the challenge of not using a neural network here.
[00:26:45.880 --> 00:26:49.880]   So ideally you would have a SVM with more manual hand tuning.
[00:26:49.880 --> 00:26:52.880]   And the reason for that was I wanted to show you that CNN layers
[00:26:52.880 --> 00:27:03.880]   can learn all of these things.
[00:27:03.880 --> 00:27:05.880]   Adil's answer is correct.
[00:27:05.880 --> 00:27:11.880]   So we want to avoid the problem of vanishing and exploding gradients.
[00:27:11.880 --> 00:27:15.880]   I haven't mentioned these terms, which means Adil is an advanced user.
[00:27:15.880 --> 00:27:20.880]   But vanishing gradients means we already know what gradients are.
[00:27:20.880 --> 00:27:24.880]   Let's, or simply in a simple manner, they're just model parameters,
[00:27:24.880 --> 00:27:27.880]   things your numbers, the model is learning.
[00:27:27.880 --> 00:27:31.880]   So when they vanish, they start becoming smaller and smaller
[00:27:31.880 --> 00:27:34.880]   and they become eventually they converge to zero.
[00:27:34.880 --> 00:27:37.880]   So all of the weights eventually become zero.
[00:27:37.880 --> 00:27:41.880]   The opposite of that, which we did look at in the previous call,
[00:27:41.880 --> 00:27:45.880]   I won't say lecture to be respectful of the book.
[00:27:45.880 --> 00:27:49.880]   They might tend to infinity or they would grow so much
[00:27:49.880 --> 00:27:51.880]   that they become infinity.
[00:27:51.880 --> 00:28:03.880]   So that is known as exploding gradients.
[00:28:03.880 --> 00:28:07.880]   No, that is not the right answer for why we passed dimensions of max.
[00:28:07.880 --> 00:28:10.880]   I'll share the right answer.
[00:28:10.880 --> 00:28:13.880]   But the other ones are right as well.
[00:28:13.880 --> 00:28:14.880]   Thanks for that.
[00:28:14.880 --> 00:28:25.880]   Activation functions are linear, so it is sensitive to values in the center.
[00:28:25.880 --> 00:28:30.880]   I believe you're confusing yourself a little.
[00:28:30.880 --> 00:28:35.880]   The main reason for how activation functions having this sensitive
[00:28:35.880 --> 00:28:45.880]   area is because ideally you want them to avoid exploding or vanishing.
[00:28:45.880 --> 00:28:48.880]   As Adil had mentioned.
[00:28:48.880 --> 00:28:53.880]   Please let me know if that's not clear.
[00:28:53.880 --> 00:28:55.880]   I'd be happy to reiterate on this.
[00:28:55.880 --> 00:29:02.880]   So, all right, moving on further, I'll try to explain convolutions
[00:29:02.880 --> 00:29:06.880]   and let me scroll down to that bit of the chapter.
[00:29:06.880 --> 00:29:15.880]   It's in chapter 8, so I have to scroll quite a bit.
[00:29:15.880 --> 00:29:17.880]   This is what we had done last week.
[00:29:17.880 --> 00:29:21.880]   And as you can see, it's not the best use, right?
[00:29:21.880 --> 00:29:27.880]   So all of these pixels are being laid out linearly.
[00:29:27.880 --> 00:29:33.880]   Please excuse my circling skills, but I hope it makes sense.
[00:29:33.880 --> 00:29:40.880]   We're just losing the complete meaning of the image as we just lay it out linearly.
[00:29:40.880 --> 00:29:42.880]   And we don't want to do that.
[00:29:42.880 --> 00:29:47.880]   So that's why we look at CNNs and I've already claimed their benefits.
[00:29:47.880 --> 00:29:52.880]   Let me try to prove why those are really helpful.
[00:29:52.880 --> 00:30:03.880]   Awesome.
[00:30:03.880 --> 00:30:08.880]   So here's, and as I claimed earlier, everything in neural networks
[00:30:08.880 --> 00:30:10.880]   is pretty much matrix multiplication.
[00:30:10.880 --> 00:30:16.880]   So you have your image now, and this time we'll be creating something
[00:30:16.880 --> 00:30:20.880]   known as a kernel and we'll be performing a convolution.
[00:30:20.880 --> 00:30:25.880]   Quite a scary word and what's actually happening here is we're just
[00:30:25.880 --> 00:30:29.880]   performing matrix multiplication a bunch of times.
[00:30:29.880 --> 00:30:33.880]   So let's say in a simplified manner, please don't focus on this image.
[00:30:33.880 --> 00:30:36.880]   I'll try to explain it first myself and then I'll come back to this.
[00:30:36.880 --> 00:30:42.880]   So let's say, and I learned this through Louis Serrano's YouTube channel to credit,
[00:30:42.880 --> 00:30:44.880]   but I'll explain it in a similar fashion.
[00:30:44.880 --> 00:30:53.880]   Let's say your task as a deep learning practitioner is to predict the letter A
[00:30:53.880 --> 00:30:56.880]   in text, wherever that appears and that's through images.
[00:30:56.880 --> 00:31:00.880]   So you can't just simply read through the text.
[00:31:00.880 --> 00:31:03.880]   The marketing team has told you to use AI.
[00:31:03.880 --> 00:31:08.880]   So we're going to scan the text and try to predict through image now.
[00:31:08.880 --> 00:31:14.880]   Now there, but how do you teach a model about this?
[00:31:14.880 --> 00:31:18.880]   There are a bunch of ways you could use the fully connected network as earlier.
[00:31:18.880 --> 00:31:24.880]   The way proposed with CNNs is known as creating these kernels.
[00:31:24.880 --> 00:31:27.880]   So ideally let's not look at the weights.
[00:31:27.880 --> 00:31:28.880]   Let's not look at anything.
[00:31:28.880 --> 00:31:37.880]   I would want a kernel that can predict this and this and the shape.
[00:31:37.880 --> 00:31:38.880]   Right.
[00:31:38.880 --> 00:31:45.880]   If I put all of this together, that's an AI.
[00:31:45.880 --> 00:31:51.880]   So one, two, three, four, five.
[00:31:51.880 --> 00:32:00.880]   So we want to check if the given input image has all of these numbers and we
[00:32:00.880 --> 00:32:04.880]   create certain kernel weights or we create these numbers.
[00:32:04.880 --> 00:32:10.880]   So what I'm going to do is now I'm going to create a kernel that can
[00:32:10.880 --> 00:32:19.880]   match this pattern inside of the given image and we're going to do a dot product.
[00:32:19.880 --> 00:32:28.880]   So if this gives me a yes and towards the later end in my final output
[00:32:28.880 --> 00:32:36.880]   from this, if I get two yeses, another few yeses for these.
[00:32:36.880 --> 00:32:40.880]   So let's say all of these output to a yes.
[00:32:40.880 --> 00:32:44.880]   So yes, say I'm all of these edges and all of these shapes exist
[00:32:44.880 --> 00:32:47.880]   in this particular orientation.
[00:32:47.880 --> 00:32:52.880]   Let's say I have this particular output and then I pass this on to the model
[00:32:52.880 --> 00:32:58.880]   and the model will tell me A is present.
[00:32:58.880 --> 00:33:01.880]   Let's say with 87% probability.
[00:33:01.880 --> 00:33:03.880]   Now your marketing team is happy.
[00:33:03.880 --> 00:33:04.880]   You get a promotion.
[00:33:04.880 --> 00:33:08.880]   The company is ready for its next round.
[00:33:08.880 --> 00:33:13.880]   You've done an incredible job and you're able to detect different letters.
[00:33:13.880 --> 00:33:15.880]   I'm not trying to mock any company.
[00:33:15.880 --> 00:33:17.880]   I'm just trying to make this funny.
[00:33:17.880 --> 00:33:25.880]   So I'm not trying to make fun of anyone if anyone gets offended.
[00:33:25.880 --> 00:33:30.880]   But as you can see, the main reason for creating these kernels is
[00:33:30.880 --> 00:33:35.880]   we create this particular and these are also learned.
[00:33:35.880 --> 00:33:36.880]   So we don't create this.
[00:33:36.880 --> 00:33:41.880]   The model eventually learns this as I had shown earlier and this gets done
[00:33:41.880 --> 00:33:42.880]   automatically.
[00:33:42.880 --> 00:33:44.880]   So that's the promise of CNNs.
[00:33:44.880 --> 00:33:49.880]   But inside of every single kernel, we're trying to match one particular
[00:33:49.880 --> 00:33:50.880]   object.
[00:33:50.880 --> 00:33:56.880]   So let's say if you're trying to predict what a dog is, one particular
[00:33:56.880 --> 00:34:00.880]   kernel will learn to, you know, let's say learn about its fur.
[00:34:00.880 --> 00:34:03.880]   Other few will learn about its eyes.
[00:34:03.880 --> 00:34:07.880]   Let's say you're looking at golden retrievers.
[00:34:07.880 --> 00:34:15.880]   So one particular kernel would learn about golden fur and the output
[00:34:15.880 --> 00:34:20.880]   from all of these would give you a certain number, which won't mean
[00:34:20.880 --> 00:34:25.880]   anything to you at all, but it'll tell you the fact or tell the model
[00:34:25.880 --> 00:34:28.880]   the fact that, Hey, this is what is present inside of the model.
[00:34:28.880 --> 00:34:29.880]   Yes, it has furs.
[00:34:29.880 --> 00:34:33.880]   Yes, it has dog like eyes, but it does not have golden fur.
[00:34:33.880 --> 00:34:36.880]   In fact, it has dotted fur.
[00:34:36.880 --> 00:34:38.880]   Maybe that could be a dalmatian.
[00:34:38.880 --> 00:34:41.880]   Let's say 60% probability you get an output.
[00:34:41.880 --> 00:34:44.880]   So this is what's happening inside of the model.
[00:34:44.880 --> 00:34:46.880]   We take an image.
[00:34:46.880 --> 00:34:58.880]   I'm sorry.
[00:34:58.880 --> 00:35:03.880]   Let me share the link because I see a few questions in the zoom chart
[00:35:03.880 --> 00:35:05.880]   pop up about the correct link.
[00:35:05.880 --> 00:35:09.880]   And I'll post it in.
[00:35:09.880 --> 00:35:17.880]   So there's the link to ask questions.
[00:35:17.880 --> 00:35:19.880]   I'll again, keep an eye out on it.
[00:35:19.880 --> 00:35:26.880]   To the people asking which book is this?
[00:35:26.880 --> 00:35:28.880]   This is Pytorch with deep learning.
[00:35:28.880 --> 00:35:31.880]   Sorry, deep learning with Pytorch.
[00:35:31.880 --> 00:35:33.880]   Five weeks into it and I still can't remember the name.
[00:35:33.880 --> 00:35:36.880]   It's called deep learning with Pytorch.
[00:35:36.880 --> 00:35:38.880]   It's there on the Pytorch website as well.
[00:35:38.880 --> 00:35:45.880]   So coming back to this, we're trying to predict letters.
[00:35:45.880 --> 00:35:47.880]   And I took the example of A.
[00:35:47.880 --> 00:35:56.880]   Then I claimed that a CNN has all of these detectors.
[00:35:56.880 --> 00:36:01.880]   Which through certain black magic known as matrix multiplication
[00:36:01.880 --> 00:36:06.880]   or scalar products gives you a certain collection of numbers,
[00:36:06.880 --> 00:36:11.880]   which tells you how much of these things are inside of an image
[00:36:11.880 --> 00:36:14.880]   and somehow lead you to a probability.
[00:36:14.880 --> 00:36:23.880]   I'm sorry.
[00:36:23.880 --> 00:36:24.880]   I just sent a message to Ravi.
[00:36:24.880 --> 00:36:26.880]   I should have sent it to everyone.
[00:36:26.880 --> 00:36:28.880]   So thanks for pointing that out.
[00:36:28.880 --> 00:36:32.880]   Here's the link for everyone to ask questions.
[00:36:32.880 --> 00:36:33.880]   Sorry about that.
[00:36:33.880 --> 00:36:39.880]   So inside of CNNs, which are much better than fully connected
[00:36:39.880 --> 00:36:42.880]   networks for some reason that I haven't explained,
[00:36:42.880 --> 00:36:43.880]   this is what happens.
[00:36:43.880 --> 00:36:46.880]   You take an image, you take a bunch of kernels,
[00:36:46.880 --> 00:36:48.880]   which have particular numbers.
[00:36:48.880 --> 00:36:56.880]   So this particular kernel gets dot producted and convolved.
[00:36:56.880 --> 00:37:01.880]   And this is different from the meaning of mathematical convolutions
[00:37:01.880 --> 00:37:04.880]   as the authors explain in the sub note.
[00:37:04.880 --> 00:37:06.880]   I was just trying to point that out.
[00:37:06.880 --> 00:37:10.880]   There's a subtle difference between Pytorch's convolution
[00:37:10.880 --> 00:37:12.880]   and mathematics convolution.
[00:37:12.880 --> 00:37:15.880]   But I won't dive into that.
[00:37:15.880 --> 00:37:16.880]   So we take this kernel.
[00:37:16.880 --> 00:37:19.880]   And as you can see, this is three by three.
[00:37:19.880 --> 00:37:21.880]   Let's say our image is here.
[00:37:21.880 --> 00:37:23.880]   It's four by four.
[00:37:23.880 --> 00:37:26.880]   So four pixels by four pixels.
[00:37:26.880 --> 00:37:28.880]   This is three pixels by three pixels.
[00:37:28.880 --> 00:37:29.880]   And what do we do?
[00:37:29.880 --> 00:37:35.880]   We take this particular kernel and place it in this particular box.
[00:37:35.880 --> 00:37:38.880]   First, we get an output.
[00:37:38.880 --> 00:37:41.880]   Which is the number five.
[00:37:41.880 --> 00:37:43.880]   Actually check this for some reason.
[00:37:43.880 --> 00:37:47.880]   It is correct, but you can, you can validate that for yourself.
[00:37:47.880 --> 00:37:49.880]   Then we move it to the right.
[00:37:49.880 --> 00:37:51.880]   So we just move it by one.
[00:37:51.880 --> 00:37:56.880]   And now we take this particular, let me switch colors.
[00:37:56.880 --> 00:37:59.880]   So we move it to the right.
[00:37:59.880 --> 00:38:03.880]   And take a dot product of this particular box.
[00:38:03.880 --> 00:38:05.880]   And get an output.
[00:38:05.880 --> 00:38:08.880]   Then we move it down again.
[00:38:08.880 --> 00:38:13.880]   And take a dot product with these numbers.
[00:38:13.880 --> 00:38:18.880]   So this gives a particular output.
[00:38:18.880 --> 00:38:22.880]   That gives some information to the neural network.
[00:38:22.880 --> 00:38:25.880]   This is what convolutional neural networks do.
[00:38:25.880 --> 00:38:27.880]   And this is known as a kernel.
[00:38:27.880 --> 00:38:30.880]   These numbers are kernel weights.
[00:38:30.880 --> 00:38:37.880]   And this process of getting this output by moving this kernel across the entire input or the previous layer.
[00:38:37.880 --> 00:38:41.880]   Is known as a convolution.
[00:38:41.880 --> 00:38:45.880]   So a few things to observe here.
[00:38:45.880 --> 00:38:49.880]   I had earlier claimed in my particular image.
[00:38:49.880 --> 00:38:51.880]   You can see me in the center.
[00:38:51.880 --> 00:38:53.880]   You can see this GPU box.
[00:38:53.880 --> 00:38:55.880]   You can see my monitor back there.
[00:38:55.880 --> 00:38:59.880]   But if I move to the, this would be your left.
[00:38:59.880 --> 00:39:00.880]   No, this would be your right.
[00:39:00.880 --> 00:39:01.880]   Sorry.
[00:39:01.880 --> 00:39:04.880]   If I move to your right, I'm still saying to you, nothing has changed.
[00:39:04.880 --> 00:39:06.880]   Neural networks are quite dumb.
[00:39:06.880 --> 00:39:09.880]   This, this could become, I don't know, something else.
[00:39:09.880 --> 00:39:11.880]   I don't want to make too much fun of myself.
[00:39:11.880 --> 00:39:13.880]   And we're trying to account for this.
[00:39:13.880 --> 00:39:18.880]   So we're trying to make it known to the neural network that, hey, this is still me.
[00:39:18.880 --> 00:39:20.880]   This is still Sayem.
[00:39:20.880 --> 00:39:27.880]   The nice thing about doing this operation is we're taking into account all of these pixels.
[00:39:27.880 --> 00:39:33.880]   So as I move towards the left or right, now you have a general understanding of my surroundings.
[00:39:33.880 --> 00:39:38.880]   So if I stop sharing again and move towards your right.
[00:39:38.880 --> 00:39:45.880]   In this neighborhood of pixels that you see, I'm still Sayem.
[00:39:45.880 --> 00:39:56.880]   So if you perform a convolution in this area, the convolutions give enough context that this is the region where you can see Sayem in there.
[00:39:56.880 --> 00:40:01.880]   You can see a human in there and it's possibly Sayem because he's drinking chai.
[00:40:01.880 --> 00:40:09.880]   So this is the essence of convolutional neural networks.
[00:40:09.880 --> 00:40:12.880]   This complete process is known as convolving.
[00:40:12.880 --> 00:40:19.880]   I think the authors jump into code, but I wanted to point a few more things out.
[00:40:19.880 --> 00:40:21.880]   So you could do a lot of things.
[00:40:21.880 --> 00:40:27.880]   And generally another thing I want to point out, there are a lot of kernels.
[00:40:27.880 --> 00:40:31.880]   So, you know, let's say there could be 20 and that's a good reason, right?
[00:40:31.880 --> 00:40:42.880]   If you look at just how much it takes in my claimed method, which has no real world analogy and was stolen from Louis Serrano, an incredible content creator.
[00:40:42.880 --> 00:40:46.880]   Just detecting A required these five kernels.
[00:40:46.880 --> 00:40:55.880]   So let's say your input image has, what's a simplified way of representing it?
[00:40:55.880 --> 00:41:00.880]   I'm sorry, let me just scroll down and start fresh.
[00:41:00.880 --> 00:41:02.880]   I'm spending some time explaining this.
[00:41:02.880 --> 00:41:09.880]   I'm sorry if it's already familiar to everyone, but because I feel convolutions are quite central to a lot of things.
[00:41:09.880 --> 00:41:14.880]   So let me split this into pixels.
[00:41:14.880 --> 00:41:18.880]   Let's see.
[00:41:18.880 --> 00:41:21.880]   This could be another layer.
[00:41:21.880 --> 00:41:28.880]   This could be another layer.
[00:41:28.880 --> 00:41:38.880]   So now we'll be looking for using the different edge detection methods I claimed or using different kernels is we're looking for these particular shapes.
[00:41:38.880 --> 00:41:50.880]   So as we perform convolutions, and let's say you take a, what's a reasonable number two by two kernel note, usually even number.
[00:41:50.880 --> 00:41:52.880]   Kernels don't exist.
[00:41:52.880 --> 00:41:53.880]   We don't use them.
[00:41:53.880 --> 00:41:54.880]   Try that.
[00:41:54.880 --> 00:41:55.880]   It's quite painful.
[00:41:55.880 --> 00:41:57.880]   It's not common.
[00:41:57.880 --> 00:42:01.880]   I think the authors mentioned it as well, but for just for this explanation, I'm using this number.
[00:42:01.880 --> 00:42:15.880]   So don't remember this number outside of my explanation, but let's say we come up with a quite mathematical way of detecting the shape and then the shape.
[00:42:15.880 --> 00:42:21.880]   As you can see, it takes an insane number of these kernels to come at this number.
[00:42:21.880 --> 00:42:28.880]   So ideally what would happen in this process, we would pass our input through a set of convolutions.
[00:42:28.880 --> 00:42:30.880]   You get an output through that.
[00:42:30.880 --> 00:42:33.880]   So input goes through CNNs.
[00:42:33.880 --> 00:42:42.880]   You'll get an output and you might pass it through another few CNN layers and do a few more options, few more operations on it.
[00:42:42.880 --> 00:42:43.880]   Sorry.
[00:42:43.880 --> 00:42:47.880]   So if you look at resonates, right?
[00:42:47.880 --> 00:42:56.880]   Resonate have up to 30 years, up to 101 layers, up to 1 51 layers.
[00:42:56.880 --> 00:43:01.880]   And essentially these are all convolutions in a way there's this more to it.
[00:43:01.880 --> 00:43:09.880]   I'm simplifying it, but these are convolutions that are happening inside to every single output.
[00:43:09.880 --> 00:43:17.880]   There's more, but simplified, we're trying to detect images through this particular process.
[00:43:17.880 --> 00:43:22.880]   So this in a very simplified explanation is the crux of CNNs.
[00:43:22.880 --> 00:43:29.880]   Let me pop over to the questions and see if there's any questions.
[00:43:29.880 --> 00:43:41.880]   I really wanted to do good explanation of this, so I'd be happy to reiterate if it's not helpful to anyone.
[00:43:41.880 --> 00:43:43.880]   Sile, thanks for that link.
[00:43:43.880 --> 00:43:45.880]   I'll also find the video later.
[00:43:45.880 --> 00:43:55.880]   I just remember the video from from memory and I came up with that explanation through that.
[00:43:55.880 --> 00:44:00.880]   So Tauseef, not really, and Adil I'd like to correct you.
[00:44:00.880 --> 00:44:05.880]   You can specify kernel shape, but you can't force what a kernel will learn.
[00:44:05.880 --> 00:44:09.880]   So and this is one nice thing about deep learning.
[00:44:09.880 --> 00:44:14.880]   The observations come much after the practice has been observed.
[00:44:14.880 --> 00:44:16.880]   So the theory usually follows later.
[00:44:16.880 --> 00:44:18.880]   We also look at batch normalization.
[00:44:18.880 --> 00:44:23.880]   I think I'll skip it today, but the chapter mentions batch normalization.
[00:44:23.880 --> 00:44:28.880]   And the authors themselves had given a wrong explanation of why it works.
[00:44:28.880 --> 00:44:36.880]   So after we discovered that CNNs are quite useful in detecting images,
[00:44:36.880 --> 00:44:42.880]   Zyler and Fergus actually later on showed us, hey guys, this is what they learn inside of all of these layers.
[00:44:42.880 --> 00:44:53.880]   I'll wait for these persons to finish the question and then I'll head back to the explanation.
[00:44:53.880 --> 00:45:09.880]   Anything or any follow up questions for how convolutions work?
[00:45:09.880 --> 00:45:24.880]   I just want to make it super clear for everyone.
[00:45:24.880 --> 00:45:41.880]   Oh no, John, this is not a new question.
[00:45:41.880 --> 00:45:43.880]   It's a totally legitimate question.
[00:45:43.880 --> 00:45:49.880]   So John's question is a color picture would be a combination of RGB values.
[00:45:49.880 --> 00:45:54.880]   That's right.
[00:45:54.880 --> 00:46:00.880]   And he's not able to understand how on and off for a pixel in a certain location maps to RGB value for color.
[00:46:00.880 --> 00:46:02.880]   It's not that simple.
[00:46:02.880 --> 00:46:04.880]   That's why I claimed it to be a simple explanation.
[00:46:04.880 --> 00:46:08.880]   So let me go back and again, explain all of that stuff.
[00:46:08.880 --> 00:46:15.880]   Or actually let me just use Safari.
[00:46:15.880 --> 00:46:20.880]   So these are colored images also.
[00:46:20.880 --> 00:46:25.880]   But inside of these, as you can see, different layers learn different things.
[00:46:25.880 --> 00:46:31.880]   So earlier the convolutions or the kernels are just learning about edges.
[00:46:31.880 --> 00:46:36.880]   Those numbers I had mentioned, they're also learned parameters.
[00:46:36.880 --> 00:46:38.880]   So the model learns themselves.
[00:46:38.880 --> 00:46:40.880]   Those numbers themselves.
[00:46:40.880 --> 00:46:43.880]   We usually initialize them using quite different techniques.
[00:46:43.880 --> 00:46:46.880]   But these are learned over time.
[00:46:46.880 --> 00:46:50.880]   And it's something, it's an observation that comes afterward.
[00:46:50.880 --> 00:46:52.880]   But this is what's observed in practice.
[00:46:52.880 --> 00:47:01.880]   The earlier layers learn about shapes, objects, and towards the later end, you would have color and all of those details come in.
[00:47:01.880 --> 00:47:04.880]   I'm sure the color does factor in.
[00:47:04.880 --> 00:47:06.880]   So you can, you can try this example.
[00:47:06.880 --> 00:47:13.880]   Just create a dataset consisting of only red cars and then pass it a green car.
[00:47:13.880 --> 00:47:17.880]   And maybe try to fill in just green boats in there.
[00:47:17.880 --> 00:47:21.880]   And when you pass a green car, most probably it'll just predict it to be a boat.
[00:47:21.880 --> 00:47:32.880]   And that's because the models or the weights have also learned somewhere in these weird visualizations that just makes sense to us because we have this paper.
[00:47:32.880 --> 00:47:38.880]   The model would have learned somewhere that a green is boat and red is car.
[00:47:38.880 --> 00:47:41.880]   I hope that's helpful.
[00:47:41.880 --> 00:47:43.880]   Feel free to ask again if this doesn't help.
[00:47:43.880 --> 00:47:50.880]   I'll again take Adil's comment.
[00:47:50.880 --> 00:48:02.880]   Thank you.
[00:48:02.880 --> 00:48:03.880]   Thank you.
[00:48:03.880 --> 00:48:04.880]   He's now a deep learning expert.
[00:48:04.880 --> 00:48:07.880]   He's been in the first day classes for a long time.
[00:48:07.880 --> 00:48:08.880]   So he's right.
[00:48:08.880 --> 00:48:13.880]   As he says the shape identifiers or these kernels are learned.
[00:48:13.880 --> 00:48:16.880]   Just to reiterate, we don't define them.
[00:48:16.880 --> 00:48:20.880]   Even this particular visualization is an after realization.
[00:48:20.880 --> 00:48:33.880]   So after we learned that his CNNs work really well for ImageNet, someone went ahead and told us, hey, this is what's happening inside of the layers.
[00:48:33.880 --> 00:48:42.880]   All right, I'll come back and answer your question later on if there's anything to explain.
[00:48:42.880 --> 00:48:48.880]   Let me see if I need to reiterate anything from here.
[00:48:48.880 --> 00:48:51.880]   Chapter six, we had looked at neural network subclassing last week.
[00:48:51.880 --> 00:49:02.880]   Yes, we had looked at data sets.
[00:49:02.880 --> 00:49:04.880]   So this is where the question was coming from.
[00:49:04.880 --> 00:49:08.880]   What does image view three comma minus one do?
[00:49:08.880 --> 00:49:11.880]   And I leave it as an open question again.
[00:49:11.880 --> 00:49:17.880]   But these are the things I want you to understand because right now, that's one of my mistakes.
[00:49:17.880 --> 00:49:22.880]   We're just going through any material that I hope you all can avoid.
[00:49:22.880 --> 00:49:25.880]   This thing is nicely created and it makes sense.
[00:49:25.880 --> 00:49:36.880]   But when I'd be left without any material or when I'm tying this on a new data set, I wouldn't be able to figure out what to do with the dimensions or how to proceed with things.
[00:49:36.880 --> 00:49:40.880]   Like I couldn't figure out why are we permuting.
[00:49:40.880 --> 00:49:47.880]   And if anyone remembers here, the reason is because the input to PyTorch goes in a different format where the channels go first.
[00:49:47.880 --> 00:49:52.880]   But for printing the image, we need to again change the orientation.
[00:49:52.880 --> 00:49:55.880]   Why am I giving this so much importance?
[00:49:55.880 --> 00:49:58.880]   Because I want to take your training wheels off now.
[00:49:58.880 --> 00:50:01.880]   We're almost coming at the end of part one of the book.
[00:50:01.880 --> 00:50:05.880]   And I really want you all to try these techniques on different data sets.
[00:50:05.880 --> 00:50:07.880]   You won't get good results at all.
[00:50:07.880 --> 00:50:08.880]   I can promise that.
[00:50:08.880 --> 00:50:12.880]   But you'll start to build an intuition and I'll stack over time.
[00:50:12.880 --> 00:50:16.880]   Right now you can take a simple, fully connected network and apply it to different things.
[00:50:16.880 --> 00:50:25.880]   Not the best use of your time, but it'll help build an intuition for how do these APIs work.
[00:50:25.880 --> 00:50:27.880]   How do you use TorchVision?
[00:50:27.880 --> 00:50:32.880]   And that's what possibly you would spend your career doing in deep learning.
[00:50:32.880 --> 00:50:33.880]   I mean, there's more to it.
[00:50:33.880 --> 00:50:35.880]   There's a lot more to it.
[00:50:35.880 --> 00:50:38.880]   But hopefully it'll be in the PyTorch ecosystem.
[00:50:38.880 --> 00:50:42.880]   And this is a really good time to get in the practice of using the API.
[00:50:42.880 --> 00:50:46.880]   So that's why I'm really emphasizing on this repeatedly quite a bit.
[00:50:46.880 --> 00:50:49.880]   Another homework I'd want to point out here.
[00:50:49.880 --> 00:50:52.880]   So we created these two particular classes.
[00:50:52.880 --> 00:50:55.880]   And there's a method known as subset class.
[00:50:55.880 --> 00:50:56.880]   Let's see what does it do.
[00:50:56.880 --> 00:51:00.880]   Look up TorchTalks.
[00:51:00.880 --> 00:51:02.880]   Takes me to the latest documentation.
[00:51:02.880 --> 00:51:05.880]   Subset class.
[00:51:05.880 --> 00:51:18.880]   I think this is the one.
[00:51:18.880 --> 00:51:25.880]   So inside of Torch.utils.data.subset.
[00:51:25.880 --> 00:51:27.880]   I'm sorry, it's not subset class.
[00:51:27.880 --> 00:51:28.880]   It's data.subset.
[00:51:28.880 --> 00:51:35.880]   It takes a subset of the data at the specified indices.
[00:51:35.880 --> 00:51:40.880]   So when you download CIFAR-100, and this is again a homework for you all.
[00:51:40.880 --> 00:51:42.880]   Feel free to ignore it totally.
[00:51:42.880 --> 00:51:49.880]   But the homework is, once I'm able to find the correct notebook without
[00:51:49.880 --> 00:51:51.880]   confusing myself too much, yes, this is the one.
[00:51:51.880 --> 00:51:54.880]   So here we manually specify these indices.
[00:51:54.880 --> 00:51:57.880]   I want you all to take this knowledge.
[00:51:57.880 --> 00:51:59.880]   Download CIFAR-100 this time.
[00:51:59.880 --> 00:52:04.880]   So just place CIFAR-100 this time.
[00:52:04.880 --> 00:52:12.880]   And use subset model to select a few classes and create a model on top of it.
[00:52:12.880 --> 00:52:16.880]   It doesn't have to be any level of accurate.
[00:52:16.880 --> 00:52:20.880]   But try to make it work from end to end.
[00:52:20.880 --> 00:52:24.880]   So try to get to an input and output for all of the classes and get some
[00:52:24.880 --> 00:52:27.880]   probabilities, even if it's random, even if it's untrained.
[00:52:27.880 --> 00:52:31.880]   And then share it with us and we'll help you debug.
[00:52:31.880 --> 00:52:35.880]   We means the community will help you debug and improve that model.
[00:52:35.880 --> 00:52:45.880]   I'm just scrolling through these to see if I wanted to highlight anything else.
[00:52:45.880 --> 00:52:49.880]   And the reason I'm going back and highlighting these things, because I
[00:52:49.880 --> 00:52:53.880]   glance over a few things by intention, because I want to again repeat them
[00:52:53.880 --> 00:52:56.880]   so that they can be solidified in your understanding.
[00:52:56.880 --> 00:53:01.880]   I'm sure you all can understand all of these in a single go, but I find this
[00:53:01.880 --> 00:53:10.880]   is a little better of a better approach of just highlighting things.
[00:53:10.880 --> 00:53:13.880]   I'm again, last week I had skipped over data loaders because I'll come
[00:53:13.880 --> 00:53:21.880]   back to this later.
[00:53:21.880 --> 00:53:25.880]   I would also request you all to check out what does new model do.
[00:53:25.880 --> 00:53:29.880]   Actually, let's look at that right now because it confused me quite a bit.
[00:53:29.880 --> 00:53:31.880]   Also, this was the first time I saw it.
[00:53:31.880 --> 00:53:36.880]   So let's head back to our trustee documentation and search for new metal.
[00:53:36.880 --> 00:53:40.880]   I see there.
[00:53:40.880 --> 00:53:45.880]   New mail input should return an integer returns the total number of elements
[00:53:45.880 --> 00:53:46.880]   in the input tensor.
[00:53:46.880 --> 00:53:47.880]   That makes sense.
[00:53:47.880 --> 00:54:01.880]   So if I create a random tensor and I see a, that gives me one 20.
[00:54:01.880 --> 00:54:06.880]   I'm just trying to multiply all of these and that does give you one 20.
[00:54:06.880 --> 00:54:12.880]   All for four by four tensor, which should have 16 number of elements.
[00:54:12.880 --> 00:54:13.880]   You get 16.
[00:54:13.880 --> 00:54:29.880]   So number of elements is 16.
[00:54:29.880 --> 00:54:30.880]   All right.
[00:54:30.880 --> 00:54:33.880]   So this is a nice place to look into this.
[00:54:33.880 --> 00:54:39.880]   Convolutions exist as con one D conf two D and conf three D layers inside
[00:54:39.880 --> 00:54:44.880]   of the torch dot NN API.
[00:54:44.880 --> 00:54:49.880]   So far, we've defined a sequential model that has this fully connected layers.
[00:54:49.880 --> 00:54:54.880]   So these five, five and two layers that are created are just fully connected,
[00:54:54.880 --> 00:54:56.880]   which means all of these layers are connected.
[00:54:56.880 --> 00:55:00.880]   But as we saw with convolutions, that's not the case.
[00:55:00.880 --> 00:55:05.880]   So let's look at what do these numbers signify by again, heading over
[00:55:05.880 --> 00:55:11.880]   to the trustee documentation.
[00:55:11.880 --> 00:55:15.880]   So concluding says inside of torch dot NN, as we would expect, this is
[00:55:15.880 --> 00:55:20.880]   a neural network component applies a 2D convolution over an input signal
[00:55:20.880 --> 00:55:23.880]   composed of several input planes.
[00:55:23.880 --> 00:55:32.880]   So if I go back to your question from earlier, John, every particular
[00:55:32.880 --> 00:55:35.880]   convolution gets applied to every single plane.
[00:55:35.880 --> 00:55:40.880]   So every layer of red, green, blue, or how many channels you might have
[00:55:40.880 --> 00:55:45.880]   for different forms of input images, this would be applied to that.
[00:55:45.880 --> 00:55:48.880]   That's what the documentations tell you.
[00:55:48.880 --> 00:55:54.880]   And we pass in the input channels, the output channel and the kernel size.
[00:55:54.880 --> 00:55:58.880]   What is tried? We'll look at that in a second.
[00:55:58.880 --> 00:56:14.880]   Thanks for this link.
[00:56:14.880 --> 00:56:16.880]   That's a fantastic explanation.
[00:56:16.880 --> 00:56:25.880]   And I've typed in a few weird characters.
[00:56:25.880 --> 00:56:32.880]   That's all right.
[00:56:32.880 --> 00:56:35.880]   I'm just reading through this and trying to make sure if there's
[00:56:35.880 --> 00:56:39.880]   anything of importance to be explained here.
[00:56:39.880 --> 00:56:40.880]   You can run through this.
[00:56:40.880 --> 00:56:44.880]   I deleted this on purpose, but try to change these numbers.
[00:56:44.880 --> 00:56:47.880]   This just sharpens the image or detects edges.
[00:56:47.880 --> 00:56:54.880]   So if you take this particular weight inside a convolution layer that
[00:56:54.880 --> 00:56:58.880]   we defined that has a size of three by three, which is why we have to
[00:56:58.880 --> 00:57:04.880]   pass in nine numbers, it should just create sharp or detect sharp edges.
[00:57:04.880 --> 00:57:08.880]   Another question to you all, why are we detaching this here?
[00:57:08.880 --> 00:57:11.880]   I let you explore that and answer that for yourself.
[00:57:11.880 --> 00:57:17.880]   Now let's, let's talk about padding.
[00:57:17.880 --> 00:57:20.880]   And I would also like to cover stride here.
[00:57:20.880 --> 00:57:30.880]   So one of the things that happened earlier in this particular
[00:57:30.880 --> 00:57:36.880]   experiment or venture of us to create a detector for our marketing team
[00:57:36.880 --> 00:57:38.880]   so that we can send it over to our board.
[00:57:38.880 --> 00:57:44.880]   We were taking a dot product of everything across everything.
[00:57:44.880 --> 00:57:47.880]   And that's a lot of dot products, right?
[00:57:47.880 --> 00:57:52.880]   Because I'm trying to find my pen.
[00:57:52.880 --> 00:57:53.880]   Sorry, one second.
[00:57:53.880 --> 00:58:05.880]   So usually your phones can take much larger images than this.
[00:58:05.880 --> 00:58:08.880]   These are the image sizes you expect, right?
[00:58:08.880 --> 00:58:14.880]   Or three by two 24 by two 24.
[00:58:14.880 --> 00:58:18.880]   And that's, that's a lot of convolutions for a kernel.
[00:58:18.880 --> 00:58:22.880]   That's three by three, five by five, nine by nine.
[00:58:22.880 --> 00:58:23.880]   Those are the usual sizes.
[00:58:23.880 --> 00:58:26.880]   Now there's a way around it.
[00:58:26.880 --> 00:58:35.880]   I'm trying to find the explanation in the book.
[00:58:35.880 --> 00:58:40.880]   If they have explained it at all, I'll skip it and explain here.
[00:58:40.880 --> 00:58:54.880]   But let's say in going back to our original image here.
[00:58:54.880 --> 00:58:58.880]   Now this time the marketing team has been told to cut back resources
[00:58:58.880 --> 00:59:07.880]   and create the most optimized a detector so that we can beat the competition.
[00:59:07.880 --> 00:59:11.880]   But so far we, the engineers had come up with this algorithm that
[00:59:11.880 --> 00:59:15.880]   was taking so many matrix multiplications and that was quite CPU intensive.
[00:59:15.880 --> 00:59:19.880]   So the marketing team wants to beat the competition and use lesser
[00:59:19.880 --> 00:59:21.880]   CPU resources and do it faster.
[00:59:21.880 --> 00:59:26.880]   So the engineering team decides instead of doing this convolution
[00:59:26.880 --> 00:59:35.880]   first followed by this one here, this ride is equal to one.
[00:59:35.880 --> 00:59:40.880]   So we're moving in one pixel every single time.
[00:59:40.880 --> 00:59:45.880]   Instead of doing this, we'll skip one.
[00:59:45.880 --> 00:59:50.880]   So this image doesn't have any further places to go, but let's
[00:59:50.880 --> 00:59:51.880]   say there were a few more columns here.
[00:59:51.880 --> 01:00:00.880]   So instead of coming here as the next pixel, the kernel would just
[01:00:00.880 --> 01:00:05.880]   skip two and come to this particular layer.
[01:00:05.880 --> 01:00:10.880]   Let's try to understand why does this make sense?
[01:00:10.880 --> 01:00:13.880]   So when you're looking at my image and I'm sorry, I don't have better
[01:00:13.880 --> 01:00:18.880]   examples, so I just keep sharing my video stream every single time.
[01:00:18.880 --> 01:00:22.880]   But this particular video that you see right now, it's on YouTube.
[01:00:22.880 --> 01:00:29.880]   It should be in HD, which is one about 2000 by 1100 pixels.
[01:00:29.880 --> 01:00:35.880]   If you skip two pixels from those in any direction should be fine.
[01:00:35.880 --> 01:00:37.880]   I am still the same human.
[01:00:37.880 --> 01:00:41.880]   I'm not a pixel small, at least in this, if you're looking at stars,
[01:00:41.880 --> 01:00:42.880]   that could be a different story.
[01:00:42.880 --> 01:00:49.880]   But starting from this edge, if you skip a few, you will be fine.
[01:00:49.880 --> 01:00:52.880]   So that's why intuitively this makes sense.
[01:00:52.880 --> 01:00:58.880]   So you can safely skip a few layers without worrying too much.
[01:00:58.880 --> 01:01:02.880]   That is known as stride and you can set it.
[01:01:02.880 --> 01:01:09.880]   Now, another problem that originates here is we need to pad the image
[01:01:09.880 --> 01:01:12.880]   and that's one of the arguments you can pass.
[01:01:12.880 --> 01:01:18.880]   So when we start performing this convolution, sometimes there's not
[01:01:18.880 --> 01:01:21.880]   enough to the outside of the image.
[01:01:21.880 --> 01:01:27.880]   So if you want to start here, right, there's nothing.
[01:01:27.880 --> 01:01:31.880]   Which means we would have to start here instead, but instead, if we
[01:01:31.880 --> 01:01:36.880]   want to start right from the edge, right from this column of two to two,
[01:01:36.880 --> 01:01:42.880]   the way to do that, we can request by torch by torch, please add zeros.
[01:01:42.880 --> 01:01:50.880]   Who are image just pad black everywhere.
[01:01:50.880 --> 01:01:55.880]   So in pixel terms, zero stands for black and two 55 is white.
[01:01:55.880 --> 01:01:56.880]   Right?
[01:01:56.880 --> 01:02:01.880]   So if we pad zero zero zero across the three RGB layers, that means
[01:02:01.880 --> 01:02:02.880]   this is a black pixel.
[01:02:02.880 --> 01:02:04.880]   That's what by torch does.
[01:02:04.880 --> 01:02:11.880]   So in our image, temporarily we add these pixels so that we can perform
[01:02:11.880 --> 01:02:16.880]   convolutions along the edges and assuming there's something important
[01:02:16.880 --> 01:02:20.880]   almost never is, but assuming there is along the edge, we can also
[01:02:20.880 --> 01:02:22.880]   detect that.
[01:02:22.880 --> 01:02:25.880]   And that is not the only reason why we do this.
[01:02:25.880 --> 01:02:28.880]   It's also to be able to perform this operation along the complete
[01:02:28.880 --> 01:02:31.880]   image, but that's why we pad the image.
[01:02:31.880 --> 01:02:34.880]   Now you might be tempted to ask, Hey, Sam, this is a really stupid
[01:02:34.880 --> 01:02:35.880]   way.
[01:02:35.880 --> 01:02:37.880]   Why are you just putting black pixels?
[01:02:37.880 --> 01:02:39.880]   You're right.
[01:02:39.880 --> 01:02:41.880]   There are a few more ways of it.
[01:02:41.880 --> 01:02:45.880]   So you could do average padding, which means you could just take
[01:02:45.880 --> 01:02:47.880]   average of these values.
[01:02:47.880 --> 01:02:51.880]   I think it's called mean padding and put those values.
[01:02:51.880 --> 01:02:55.880]   So let's say what's the average of these one by four, right?
[01:02:55.880 --> 01:02:59.880]   So you could put one by four padding around these pixels.
[01:02:59.880 --> 01:03:02.880]   So one by four, one by four, one by four.
[01:03:02.880 --> 01:03:06.880]   I'm hoping you all would be used to my annoying handwriting at this
[01:03:06.880 --> 01:03:06.880]   point.
[01:03:06.880 --> 01:03:07.880]   I can't change it.
[01:03:07.880 --> 01:03:08.880]   So I apologize.
[01:03:08.880 --> 01:03:12.880]   The other one is known as reflective padding.
[01:03:12.880 --> 01:03:13.880]   It's not covered in the book.
[01:03:13.880 --> 01:03:14.880]   I'm just highlighting it.
[01:03:14.880 --> 01:03:19.880]   So what do you do in this particular layer outside?
[01:03:19.880 --> 01:03:21.880]   You just take a reflection.
[01:03:21.880 --> 01:03:24.880]   So the same exact same number 0101.
[01:03:24.880 --> 01:03:27.880]   And it's known as reflection.
[01:03:27.880 --> 01:03:31.880]   If you because if you look at the image, it'll feel like it's the
[01:03:31.880 --> 01:03:36.880]   exact same image being reflected along the edges.
[01:03:36.880 --> 01:03:40.880]   If it's like a wide enough, you can't see one pixel inside of an
[01:03:40.880 --> 01:03:44.880]   image, but if you do reflection padding, it should be visible as
[01:03:44.880 --> 01:03:47.880]   if you've placed a mirror along the edges.
[01:03:47.880 --> 01:03:51.880]   There are a few more ways of padding, but these are the two
[01:03:51.880 --> 01:03:56.880]   ones that are most useful, at least more useful than zero padding.
[01:03:56.880 --> 01:03:59.880]   So I just wanted to highlight these.
[01:03:59.880 --> 01:04:13.880]   Okay, let's head back to the question and see if there are any
[01:04:13.880 --> 01:04:24.880]   questions and otherwise I'll continue.
[01:04:24.880 --> 01:04:47.880]   Yeah, Brandon Rurer is amazing.
[01:04:47.880 --> 01:04:51.880]   Alright, continuing further on.
[01:04:51.880 --> 01:04:54.880]   So we know our fancy neural network that we have set up our
[01:04:54.880 --> 01:04:58.880]   R&D department from and now accepting the challenge from the
[01:04:58.880 --> 01:05:01.880]   marketing department of making it less CPU intensive and
[01:05:01.880 --> 01:05:03.880]   learning all of these techniques.
[01:05:03.880 --> 01:05:06.880]   Coming back to it has quite a few layers.
[01:05:06.880 --> 01:05:10.880]   Remember sequential just puts these in a sequence.
[01:05:10.880 --> 01:05:14.880]   So it literally tells PyTorch, PyTorch please, after performing
[01:05:14.880 --> 01:05:18.880]   this operation, pass them through a tanh layer, pass them through
[01:05:18.880 --> 01:05:22.880]   a max pooling layer, pass them through convolution layer again
[01:05:22.880 --> 01:05:25.880]   and pass them through a tanh layer again and just do these
[01:05:25.880 --> 01:05:28.880]   for a bunch of times.
[01:05:28.880 --> 01:05:31.880]   Something is missing here.
[01:05:31.880 --> 01:05:42.880]   My eyes tell me it's another tanh layer, but you all can verify.
[01:05:42.880 --> 01:05:50.880]   Oh, I'm sorry.
[01:05:50.880 --> 01:05:53.880]   Apparently my screen isn't showing.
[01:05:53.880 --> 01:05:57.880]   Thanks for pointing that out.
[01:05:57.880 --> 01:06:04.880]   Sorry about that.
[01:06:04.880 --> 01:06:07.880]   Let me share screens again.
[01:06:07.880 --> 01:06:12.880]   Can someone please confirm if this is visible?
[01:06:12.880 --> 01:06:19.880]   Awesome.
[01:06:19.880 --> 01:06:21.880]   Thanks for pointing that out guys.
[01:06:21.880 --> 01:06:23.880]   Let me go full screen.
[01:06:23.880 --> 01:06:27.880]   So please ignore what I just said.
[01:06:27.880 --> 01:06:29.880]   I'll just speak about it again.
[01:06:29.880 --> 01:06:36.880]   So, so far we had only seen a tanh nn.linear in here.
[01:06:36.880 --> 01:06:39.880]   But after accepting the challenge from the marketing team and
[01:06:39.880 --> 01:06:44.880]   coming up with our R&D department, we've added a few layers now.
[01:06:44.880 --> 01:06:49.880]   So inside of sequential and remember sequential just tells
[01:06:49.880 --> 01:06:53.880]   PyTorch, PyTorch please apply these steps in sequence to
[01:06:53.880 --> 01:06:55.880]   whatever input we send you.
[01:06:55.880 --> 01:06:58.880]   Just do that and our teams will be happy.
[01:06:58.880 --> 01:07:00.880]   So we send it through a con2d layer.
[01:07:00.880 --> 01:07:04.880]   Then we pass it through a tanh activation followed by a max
[01:07:04.880 --> 01:07:07.880]   pooling layer, which I haven't explained what does it do.
[01:07:07.880 --> 01:07:10.880]   We'll come to that.
[01:07:10.880 --> 01:07:14.880]   Another con2d so on and so forth so that this becomes quite
[01:07:14.880 --> 01:07:19.880]   interesting and different neural network.
[01:07:19.880 --> 01:07:23.880]   And the authors tell us why this is a much better approach.
[01:07:23.880 --> 01:07:27.880]   When you sum this, it's much, much, much smaller than 3.7
[01:07:27.880 --> 01:07:28.880]   million parameters.
[01:07:28.880 --> 01:07:31.880]   And it's much, much more accurate than our fully connected
[01:07:31.880 --> 01:07:36.880]   network, which means and through practice, at least for
[01:07:36.880 --> 01:07:39.880]   this particular quite challenging problem of detecting birds
[01:07:39.880 --> 01:07:43.880]   and airplanes, we can claim that CNNs are much better than
[01:07:43.880 --> 01:07:47.880]   that particular architecture and extending that claim.
[01:07:47.880 --> 01:07:49.880]   I would say they are much better for images.
[01:07:49.880 --> 01:07:54.880]   I'm sure all of you know all of this already, but if
[01:07:54.880 --> 01:07:57.880]   questioned on this, you can just say with lesser parameters,
[01:07:57.880 --> 01:08:00.880]   CNNs outperform fully connected networks much, much better.
[01:08:00.880 --> 01:08:03.880]   Which is why they're used for images.
[01:08:03.880 --> 01:08:05.880]   Now we use attention.
[01:08:05.880 --> 01:08:07.880]   That's a completely different discussion.
[01:08:07.880 --> 01:08:10.880]   It's not covered in this book, so I'll skip that so that I
[01:08:10.880 --> 01:08:12.880]   don't have to do that homework.
[01:08:12.880 --> 01:08:14.880]   But this is why it works really well.
[01:08:14.880 --> 01:08:18.880]   And I'll skip further to see if there are things that I would
[01:08:18.880 --> 01:08:21.880]   like to cover.
[01:08:21.880 --> 01:08:24.880]   No, I think this is a good point to cover max pooling.
[01:08:24.880 --> 01:08:26.880]   So let's do that.
[01:08:26.880 --> 01:08:29.880]   So let's look at what is Max pool 2D.
[01:08:29.880 --> 01:08:33.880]   And actually, this notebook is connected, I guess.
[01:08:33.880 --> 01:08:39.880]   Which means that this particular notebook is not connected.
[01:08:39.880 --> 01:08:47.880]   I'm just checking if my server is still connected to my
[01:08:47.880 --> 01:08:48.880]   laptop.
[01:08:48.880 --> 01:08:49.880]   Yes, it is.
[01:08:49.880 --> 01:08:58.880]   I'll just head over to the docs.
[01:08:58.880 --> 01:09:01.880]   I think it should be inside of torsion.nn.
[01:09:01.880 --> 01:09:04.880]   But I'll just say myself the embarrassment and look here.
[01:09:04.880 --> 01:09:11.880]   So this applies a 2D max pooling.
[01:09:11.880 --> 01:09:13.880]   We don't know what that is.
[01:09:13.880 --> 01:09:17.880]   Over an input signal composed of several input planes.
[01:09:17.880 --> 01:09:20.880]   So John, again, we have a few input planes.
[01:09:20.880 --> 01:09:24.880]   But as you know, after performing all of those convolutions,
[01:09:24.880 --> 01:09:26.880]   those could be RGB or maybe not.
[01:09:26.880 --> 01:09:33.880]   If padding is non-zero, then input is implicitly zero padded
[01:09:33.880 --> 01:09:35.880]   on both sides.
[01:09:35.880 --> 01:09:40.880]   Tiletion controls the spacing between kernel points.
[01:09:40.880 --> 01:09:50.880]   So we found this nice link.
[01:09:50.880 --> 01:09:52.880]   I think someone had shared this in the chat as well.
[01:09:52.880 --> 01:09:54.880]   But let's quickly look at this.
[01:09:54.880 --> 01:09:57.880]   So this is how convolutions are getting applied.
[01:09:57.880 --> 01:10:00.880]   And we're getting these outputs.
[01:10:00.880 --> 01:10:14.880]   And when we pad and do a few strides,
[01:10:14.880 --> 01:10:16.880]   we only pick different areas of interest.
[01:10:16.880 --> 01:10:18.880]   So those are shown in blue.
[01:10:18.880 --> 01:10:25.880]   I was expecting this to have max pooling explanation,
[01:10:25.880 --> 01:10:26.880]   but it does not.
[01:10:26.880 --> 01:10:30.880]   All right.
[01:10:30.880 --> 01:10:32.880]   So we know what goes in.
[01:10:32.880 --> 01:10:33.880]   We know what goes out.
[01:10:33.880 --> 01:10:36.880]   But we still don't know what does 2D max pooling do.
[01:10:36.880 --> 01:10:46.880]   So let's look at the book and see what does it do from there.
[01:10:46.880 --> 01:10:50.880]   So quite simply, now we have an output of a conv activation,
[01:10:50.880 --> 01:10:58.880]   which is an input to the next max pooling layer.
[01:10:58.880 --> 01:11:01.880]   So far we've seen that in the neural network.
[01:11:01.880 --> 01:11:07.880]   Max pooling will simply take these four numbers and return their maximum values.
[01:11:07.880 --> 01:11:10.880]   So we get five from here.
[01:11:10.880 --> 01:11:12.880]   Let's see two from here.
[01:11:12.880 --> 01:11:14.880]   Yes, we get two.
[01:11:14.880 --> 01:11:16.880]   Should be two from here again.
[01:11:16.880 --> 01:11:18.880]   Yes, it's two.
[01:11:18.880 --> 01:11:20.880]   And hopefully two again.
[01:11:20.880 --> 01:11:23.880]   So that's what max pooling does.
[01:11:23.880 --> 01:11:24.880]   Why does this work?
[01:11:24.880 --> 01:11:27.880]   We're trying to down sample the image.
[01:11:27.880 --> 01:11:30.880]   Because once we've done all of these convolutions,
[01:11:30.880 --> 01:11:35.880]   we get so many of these outputs and we'd like to reduce the size.
[01:11:35.880 --> 01:11:39.880]   So max pooling helps us down sample the image.
[01:11:39.880 --> 01:11:41.880]   There are two more methods to it.
[01:11:41.880 --> 01:11:45.880]   Let me see if those are mentioned in the book.
[01:11:45.880 --> 01:11:46.880]   I think those are mentioned in the book.
[01:11:46.880 --> 01:11:47.880]   Yes.
[01:11:47.880 --> 01:11:51.880]   So there is average pooling, which isn't used now.
[01:11:51.880 --> 01:11:58.880]   There is max pooling.
[01:11:58.880 --> 01:12:00.880]   Yes, these are the two approaches.
[01:12:00.880 --> 01:12:02.880]   I thought this was mentioned as the third one.
[01:12:02.880 --> 01:12:08.880]   But it could also perform started convolutions, which you discussed already.
[01:12:08.880 --> 01:12:14.880]   So max pooling is simply applied by calling an end dot max pool 2D.
[01:12:14.880 --> 01:12:18.880]   That takes out the maximum values from here.
[01:12:18.880 --> 01:12:21.880]   Again, why does this intuitively work with neural networks?
[01:12:21.880 --> 01:12:23.880]   This is not a lot of intuition.
[01:12:23.880 --> 01:12:26.880]   There's an explanation to why maybe these things work,
[01:12:26.880 --> 01:12:28.880]   but that's after the practice.
[01:12:28.880 --> 01:12:31.880]   So coming back to my image,
[01:12:31.880 --> 01:12:35.880]   now you've created a filter map that has been created for,
[01:12:35.880 --> 01:12:40.880]   from convolutions through this image of mine.
[01:12:40.880 --> 01:12:41.880]   In this particular neighborhood,
[01:12:41.880 --> 01:12:44.880]   a lot of the kernels would be telling you, yes,
[01:12:44.880 --> 01:12:48.880]   you can see a set of glasses here,
[01:12:48.880 --> 01:12:50.880]   which means high probability for Siam,
[01:12:50.880 --> 01:12:55.880]   because you've never seen me not wear glasses.
[01:12:55.880 --> 01:12:57.880]   And a lot of those are not useful.
[01:12:57.880 --> 01:12:58.880]   Right?
[01:12:58.880 --> 01:13:03.880]   So if 50 of these pixel values tell you, yes, glasses, yes, glasses,
[01:13:03.880 --> 01:13:05.880]   yes, glasses, yes, glasses,
[01:13:05.880 --> 01:13:09.880]   you can just take a max pooling, max pool out of those
[01:13:09.880 --> 01:13:11.880]   and throw the rest away.
[01:13:11.880 --> 01:13:16.880]   And that's helpful because it reduces the number of scalar operations,
[01:13:16.880 --> 01:13:22.880]   tensor operations, scalar multiplications that need to be done.
[01:13:22.880 --> 01:13:25.880]   And that again, takes the load away from our graphic card
[01:13:25.880 --> 01:13:33.880]   or takes the load away from our R&D team.
[01:13:33.880 --> 01:13:36.880]   So our company is into mad on us, mad at us.
[01:13:36.880 --> 01:13:43.880]   That's why we follow this approach.
[01:13:43.880 --> 01:13:45.880]   I want to point one thing out.
[01:13:45.880 --> 01:13:47.880]   Thanks for sharing this link Dhruv.
[01:13:47.880 --> 01:13:51.880]   Andrew is one of the best teachers in our community.
[01:13:51.880 --> 01:13:53.880]   But I want to point one thing out.
[01:13:53.880 --> 01:13:57.880]   It's great to find these resources.
[01:13:57.880 --> 01:13:59.880]   And I would encourage you to check out the lecture by Andrew.
[01:13:59.880 --> 01:14:03.880]   He's like, there's no comparison at all between what is being taught here
[01:14:03.880 --> 01:14:04.880]   and that lecture.
[01:14:04.880 --> 01:14:06.880]   Obviously, that is far, far better.
[01:14:06.880 --> 01:14:08.880]   I would definitely say that.
[01:14:08.880 --> 01:14:10.880]   But this is also a reading group.
[01:14:10.880 --> 01:14:13.880]   Our end goal here is to understand this book.
[01:14:13.880 --> 01:14:16.880]   Our end goal here is to understand PyTorch.
[01:14:16.880 --> 01:14:19.880]   This is not to encourage you from checking out other courses.
[01:14:19.880 --> 01:14:23.880]   But the reason I've been emphasizing taking out the documentation
[01:14:23.880 --> 01:14:27.880]   and playing around with things is because I have fallen into this loop
[01:14:27.880 --> 01:14:29.880]   of not understanding things thoroughly.
[01:14:29.880 --> 01:14:33.880]   And then finding a ton of resources because there's no lack of them.
[01:14:33.880 --> 01:14:37.880]   And still not having a good enough intuition because for me,
[01:14:37.880 --> 01:14:43.880]   I can understand the mathy bits, but not so much of the code.
[01:14:43.880 --> 01:14:45.880]   At least at that level.
[01:14:45.880 --> 01:14:49.880]   So I'd encourage you to play around with the code more.
[01:14:49.880 --> 01:14:51.880]   And let the intuition follow later on.
[01:14:51.880 --> 01:14:55.880]   So instead of focusing on the mathy explanations,
[01:14:55.880 --> 01:14:58.880]   this is part of the promise that Jeremy Howard guarantees you.
[01:14:58.880 --> 01:15:00.880]   And I'm a student of that school.
[01:15:00.880 --> 01:15:06.880]   Please learn about the bigger picture first.
[01:15:06.880 --> 01:15:07.880]   Pun intended.
[01:15:07.880 --> 01:15:09.880]   Learn about how to apply these things first.
[01:15:09.880 --> 01:15:25.880]   I think this is an older notebook, so I can close this.
[01:15:25.880 --> 01:15:29.880]   So now we apply all of these layers.
[01:15:29.880 --> 01:15:33.880]   And sorry, I reopened that notebook by mistake.
[01:15:33.880 --> 01:15:38.880]   We apply max pooling layers.
[01:15:38.880 --> 01:15:41.880]   And now our complete neural network is defined.
[01:15:41.880 --> 01:15:43.880]   We can define the forward function.
[01:15:43.880 --> 01:15:47.880]   We have a complete model that we can create using net.
[01:15:47.880 --> 01:15:50.880]   And we can further proceed.
[01:15:50.880 --> 01:15:54.880]   After this, the authors introduced the functional API.
[01:15:54.880 --> 01:15:57.880]   And they also introduced data loaders.
[01:15:57.880 --> 01:15:59.880]   I had skipped this in the last lecture,
[01:15:59.880 --> 01:16:04.880]   because I thought it would be, again, beneficial to highlight it here.
[01:16:04.880 --> 01:16:08.880]   So I'll go back and try to explain that again.
[01:16:08.880 --> 01:16:12.880]   Or explain that for the first time.
[01:16:12.880 --> 01:16:17.880]   So in this particular notebook that I think eventually covers data loaders,
[01:16:17.880 --> 01:16:21.880]   we had looked at how to use data sets, how to create CIFAR10,
[01:16:21.880 --> 01:16:23.880]   how to perform transforms on it,
[01:16:23.880 --> 01:16:26.880]   and how do we get an input and output from it.
[01:16:26.880 --> 01:16:28.880]   And try to train a model on top of it.
[01:16:28.880 --> 01:16:34.880]   This time, we call tors.utils.data.dataLoader.
[01:16:34.880 --> 01:16:40.880]   And hopefully you all can see this pop-up.
[01:16:40.880 --> 01:16:43.880]   But the docstring tells me it's a data loader,
[01:16:43.880 --> 01:16:45.880]   combines a data set and a sampler,
[01:16:45.880 --> 01:16:49.880]   provides an iterable over the given data set.
[01:16:49.880 --> 01:16:54.880]   So instead of us creating a loop going over every single element,
[01:16:54.880 --> 01:16:56.880]   we can now create a data loader.
[01:16:56.880 --> 01:16:58.880]   We pass it CIFAR2.
[01:16:58.880 --> 01:17:00.880]   What is CIFAR2?
[01:17:00.880 --> 01:17:05.880]   It's a tensor of values.
[01:17:05.880 --> 01:17:07.880]   Let's see if we can...
[01:17:07.880 --> 01:17:15.880]   I should be looking at the length.
[01:17:21.880 --> 01:17:24.880]   So it has 10,000 examples in it.
[01:17:24.880 --> 01:17:28.880]   Let me go up and look at what is CIFAR2.
[01:17:28.880 --> 01:17:34.880]   It's the data set, but I'm just trying to find the definition.
[01:17:34.880 --> 01:17:42.880]   So we extracted two images out of CIFAR10, which live in CIFAR2.
[01:17:42.880 --> 01:17:44.880]   This time, we're creating a data loader.
[01:17:44.880 --> 01:17:48.880]   As the name gives away, it just loads the data.
[01:17:48.880 --> 01:17:50.880]   Quite obvious. Genius, I am.
[01:17:50.880 --> 01:17:52.880]   Thanks for pointing that out.
[01:17:52.880 --> 01:17:58.880]   So all we need to point a data loader to is the data set.
[01:17:58.880 --> 01:18:00.880]   We tell it the batch size,
[01:18:00.880 --> 01:18:03.880]   and we request it to enable shuffling.
[01:18:03.880 --> 01:18:08.880]   Now this time, for the first time in this particular book,
[01:18:08.880 --> 01:18:13.880]   sorry, we look at batch sizes.
[01:18:13.880 --> 01:18:16.880]   And this is where things start to get more interesting.
[01:18:16.880 --> 01:18:19.880]   We don't look at GPUs in this lecture,
[01:18:19.880 --> 01:18:23.880]   but we'll look at it in the next lecture, in the next discussion.
[01:18:23.880 --> 01:18:29.880]   You can only fit so many images on your computer's memory.
[01:18:29.880 --> 01:18:32.880]   That is where you need more batch sizes.
[01:18:32.880 --> 01:18:34.880]   That is where you need bigger batch sizes.
[01:18:34.880 --> 01:18:38.880]   So we create a batch size of 64, one of the standard numbers.
[01:18:38.880 --> 01:18:41.880]   It's usually in multiples, in powers of 2.
[01:18:41.880 --> 01:18:46.880]   So 24, 8, 16, 32, 64, 128, 256, those numbers.
[01:18:48.880 --> 01:18:50.880]   And we create a train loader out of it.
[01:18:50.880 --> 01:18:53.880]   This hopefully would load the training data set.
[01:18:53.880 --> 01:18:59.880]   The claim was this helps us make our training loop shorter.
[01:18:59.880 --> 01:19:02.880]   So anything that PyTorch does, anything at all,
[01:19:02.880 --> 01:19:06.880]   is to help us reduce the boilerplate code.
[01:19:06.880 --> 01:19:10.880]   There's FastAI, there are other frameworks that again build on top further,
[01:19:10.880 --> 01:19:13.880]   that help us reduce this even more.
[01:19:13.880 --> 01:19:16.880]   But anything that we do is to reduce the code
[01:19:16.880 --> 01:19:19.880]   so that we can reduce the mistakes and reduce our efforts.
[01:19:19.880 --> 01:19:22.880]   So instead of defining a longer loop this time,
[01:19:22.880 --> 01:19:28.880]   we just iterate over the epochs, the images and labels.
[01:19:28.880 --> 01:19:33.880]   And this time we use train loader instead of
[01:19:33.880 --> 01:19:40.880]   manually calling things from our data set.
[01:19:40.880 --> 01:19:43.880]   This time we just use a data loader
[01:19:43.880 --> 01:19:46.880]   because we can iterate over it. It's iterable.
[01:19:46.880 --> 01:19:49.880]   You can look at the documentation again
[01:19:49.880 --> 01:19:51.880]   to understand what's going on here.
[01:19:51.880 --> 01:19:54.880]   And we perform the same operations as earlier.
[01:19:54.880 --> 01:19:58.880]   So that is where data loaders come into play.
[01:19:58.880 --> 01:20:02.880]   They give you an iterable from the data set
[01:20:02.880 --> 01:20:05.880]   in batch sizes of the number we have defined.
[01:20:05.880 --> 01:20:08.880]   And these are randomly picked.
[01:20:09.880 --> 01:20:27.880]   I think this is a nice part to go back to any questions.
[01:20:27.880 --> 01:20:30.880]   And wrap things up.
[01:20:30.880 --> 01:20:33.880]   Again, there are more things to cover in this particular chapter,
[01:20:33.880 --> 01:20:37.880]   but we're also at the point where it will take more than 10 minutes.
[01:20:37.880 --> 01:20:39.880]   So I'll try to wrap up.
[01:20:39.880 --> 01:20:42.880]   So I would again like to assign some homework as well today.
[01:20:42.880 --> 01:20:45.880]   You can totally ignore it. No need to do it at all.
[01:20:45.880 --> 01:20:49.880]   If you do, please, please tag me and @matesinbiases.
[01:20:49.880 --> 01:20:52.880]   If you share it on any social, we'd be happy to review
[01:20:52.880 --> 01:20:55.880]   and help share it across our community.
[01:20:55.880 --> 01:21:03.880]   Your homework is try these different subsets,
[01:21:03.880 --> 01:21:07.880]   quite literally subset and chain data set from inside of data.
[01:21:07.880 --> 01:21:10.880]   Try to work with C400 this time.
[01:21:10.880 --> 01:21:13.880]   Maybe write a blog of how you can pick three classes from C400,
[01:21:13.880 --> 01:21:17.880]   subset into them and train a CNN.
[01:21:17.880 --> 01:21:25.880]   So it could be a blog on exploring subset utility from PyTorch.
[01:21:25.880 --> 01:21:29.880]   You can try Conv1D on a time series data set.
[01:21:29.880 --> 01:21:33.880]   That could be another interesting challenge.
[01:21:33.880 --> 01:21:37.880]   The authors have claimed in the book Conv1D is used for time series data set,
[01:21:37.880 --> 01:21:41.880]   Conv2D for images, 3D for videos and X-ray images.
[01:21:41.880 --> 01:21:43.880]   They haven't proved that point.
[01:21:43.880 --> 01:21:48.880]   We do use Conv3D later in the book, but try to use Conv1D somewhere.
[01:21:48.880 --> 01:21:50.880]   Even if it doesn't work, that's totally fine.
[01:21:50.880 --> 01:21:55.880]   Just tell us what you tried and tell us how that went.
[01:21:55.880 --> 01:22:00.880]   I'll hop back to take any questions.
[01:22:00.880 --> 01:22:02.880]   If not, I'll wrap things up.
[01:22:02.880 --> 01:22:15.880]   Awesome.
[01:22:15.880 --> 01:22:18.880]   I'll take another minute because I want to highlight one thing.
[01:22:18.880 --> 01:22:28.880]   So yesterday we had our first community hangout and yesterday Ravi had shared his journey.
[01:22:28.880 --> 01:22:34.880]   Today I requested Vinayak to share his, but unfortunately he wasn't able to join today.
[01:22:34.880 --> 01:22:37.880]   He'll present his journey or tell us about his journey in the next week.
[01:22:37.880 --> 01:22:44.880]   I asked Ravi if since I was quite smartly not that proactive about recording it,
[01:22:44.880 --> 01:22:45.880]   if he wanted to share his journey.
[01:22:45.880 --> 01:22:50.880]   He's not well today, so I'll just point out his blog and tell us,
[01:22:50.880 --> 01:22:53.880]   tell all of you about the things I've learned from him.
[01:22:53.880 --> 01:22:56.880]   So Ravi has been joining all of our study groups for a long, long time.
[01:22:56.880 --> 01:23:00.880]   And I just want to point out his incredible blog post.
[01:23:00.880 --> 01:23:06.880]   So he currently is a full stack developer and I wouldn't say he's a machine learning enthusiast.
[01:23:06.880 --> 01:23:08.880]   He's quite the machine learning expert.
[01:23:08.880 --> 01:23:12.880]   He puts together these incredible notes and shares them on Twitter.
[01:23:12.880 --> 01:23:15.880]   His Twitter is Ravi Mashroo.
[01:23:15.880 --> 01:23:24.880]   And he's been sharing these incredible resources, both on his blog and with us.
[01:23:24.880 --> 01:23:27.880]   Yesterday he told us about his journey.
[01:23:27.880 --> 01:23:31.880]   I'm just highlighting it because I'm using this opportunity to share his journey since he's not well.
[01:23:31.880 --> 01:23:38.880]   But the one thing I've learned is he told me yesterday, he's just putting these out as notes for himself.
[01:23:38.880 --> 01:23:45.880]   And the one thing he told me that he learned was to not be afraid of putting these things out.
[01:23:45.880 --> 01:23:55.880]   So for him, it's about creating these notes, polishing them to a certain extent and following a rigid schedule every single week.
[01:23:55.880 --> 01:24:03.880]   And I want to really, first of all, applaud him for such a successful blog and also really thank him for all of his contributions.
[01:24:03.880 --> 01:24:10.880]   Because consistency, at least in my experience, I've run a podcast for a long time.
[01:24:10.880 --> 01:24:13.880]   I've tried to be quite consistent on it and I was able to.
[01:24:13.880 --> 01:24:18.880]   And the thing I've learned in my journey in deep learning is consistency really trumps everything.
[01:24:18.880 --> 01:24:28.880]   So the one thing all of us can learn from Ravi is as long as you're consistent and as long as you're sincere in your approach of sharing these materials,
[01:24:28.880 --> 01:24:32.880]   you'll get to a great level quite fast.
[01:24:32.880 --> 01:24:41.880]   Ravi had shared his first blog a little while ago on June 19th, right around the time when we started our Fastbook reading group.
[01:24:41.880 --> 01:24:46.880]   And now he's writing incredible posts on, you know, building movie recommenders.
[01:24:46.880 --> 01:24:48.880]   And there's also a mix of other things as well.
[01:24:48.880 --> 01:24:54.880]   I think this one shows us how to do that in Excel, if I remember correctly.
[01:24:54.880 --> 01:24:59.880]   But as you can see, if you read through this, it's quite a high quality blog post.
[01:24:59.880 --> 01:25:08.880]   I really want to applaud him on this effort and really thank him for all of his contributions and share this example with all of you.
[01:25:08.880 --> 01:25:14.880]   If you can be regular and if you can put out such things, it's quite helpful in your learning journey.
[01:25:14.880 --> 01:25:20.880]   And it's also really beneficial for all of us to learn and read and learn from.
[01:25:20.880 --> 01:25:26.880]   So thank you so much, Ravi, for all of these efforts.
[01:25:26.880 --> 01:25:33.880]   I'll take one last look if there are any questions, but I really just wanted to spend a few minutes highlighting Ravi's journey.
[01:25:33.880 --> 01:25:38.880]   And let me.
[01:25:38.880 --> 01:25:47.880]   So please find Ravi on Twitter and please check out his blog on this link.
[01:25:47.880 --> 01:25:57.880]   Thank you so much, Ravi, for all of these things.
[01:25:57.880 --> 01:25:59.880]   All right, there's no link to the PowerPoint.
[01:25:59.880 --> 01:26:01.880]   These are shamelessly stolen from the book.
[01:26:01.880 --> 01:26:07.880]   So it's not any resources that have been created.
[01:26:07.880 --> 01:26:08.880]   So I won't share the slide deck.
[01:26:08.880 --> 01:26:11.880]   I would encourage you to not even watch this recording.
[01:26:11.880 --> 01:26:14.880]   Instead, spend time on the core repository.
[01:26:14.880 --> 01:26:18.880]   There's nothing that I've explained that's not there in the book.
[01:26:18.880 --> 01:26:20.880]   So please spend time there.
[01:26:20.880 --> 01:26:22.880]   And on that note, I think we can wrap up.
[01:26:22.880 --> 01:26:23.880]   Thank you everyone for joining.
[01:26:23.880 --> 01:26:26.880]   Next week, we'll wrap up part one of the book.
[01:26:26.880 --> 01:26:28.880]   I think I might change the timing.
[01:26:28.880 --> 01:26:29.880]   So I just want to give you a heads up.
[01:26:29.880 --> 01:26:31.880]   I might shift the study group.
[01:26:31.880 --> 01:26:32.880]   I'll run a poll for that.
[01:26:32.880 --> 01:26:34.880]   If not, we'll meet again next Sunday.
[01:26:34.880 --> 01:26:43.880]   Finish the first part of the book and start on the second part where we'll build our first X-ray image reader using PyTorch.
[01:26:43.880 --> 01:26:44.880]   Thank you.
[01:26:44.880 --> 01:26:48.880]   [End of session]
[01:26:48.880 --> 01:26:51.880]   [End of session]
[01:26:51.880 --> 01:27:01.880]   [BLANK_AUDIO]

