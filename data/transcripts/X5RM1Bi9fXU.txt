
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:05.320]   - I'm Emanuel Hussain.
[00:00:05.320 --> 00:00:06.880]   I'm gonna be the host for today.
[00:00:06.880 --> 00:00:09.720]   I'm a entrepreneur in residence at Fast.ai.
[00:00:09.720 --> 00:00:14.120]   And with me, I have two of my esteemed colleagues
[00:00:14.120 --> 00:00:15.120]   in machine learning.
[00:00:15.120 --> 00:00:18.440]   Emanuel, he's currently an independent consultant
[00:00:18.440 --> 00:00:23.040]   and he was previously a staff ML engineer at Stripe.
[00:00:23.040 --> 00:00:25.360]   And he actually worked on some very interesting things
[00:00:25.360 --> 00:00:27.840]   at Stripe, like fraud models,
[00:00:27.840 --> 00:00:30.080]   which a lot of people end up using
[00:00:30.080 --> 00:00:31.280]   'cause a lot of people use Stripe.
[00:00:31.280 --> 00:00:33.080]   That was really interesting.
[00:00:33.080 --> 00:00:36.660]   And he was also head of AI at Insight Data Science.
[00:00:36.660 --> 00:00:39.980]   And that's also a big deal to me
[00:00:39.980 --> 00:00:42.340]   because a lot of the colleagues that I had
[00:00:42.340 --> 00:00:45.680]   when I first started in data science were from Insight.
[00:00:45.680 --> 00:00:49.840]   Also, you may know Emanuel from his book,
[00:00:49.840 --> 00:00:53.200]   "Machine Learning Powered Applications" from O'Reilly,
[00:00:53.200 --> 00:00:58.160]   is one of the first ML ops type books
[00:00:58.160 --> 00:01:01.680]   that I remember reading or seeing out there.
[00:01:01.680 --> 00:01:03.300]   And it's a really good book.
[00:01:03.300 --> 00:01:05.160]   I highly recommend checking it out.
[00:01:05.160 --> 00:01:08.000]   It's kind of like technology agnostic
[00:01:08.000 --> 00:01:10.660]   and kind of walks through a very end-to-end way
[00:01:10.660 --> 00:01:13.640]   of thinking about operationalizing machine learning,
[00:01:13.640 --> 00:01:15.440]   just really good.
[00:01:15.440 --> 00:01:18.780]   Also with us, we have Shreya Shankar.
[00:01:18.780 --> 00:01:21.800]   She is currently a computer scientist
[00:01:21.800 --> 00:01:23.440]   living in San Francisco.
[00:01:23.440 --> 00:01:26.700]   She is doing a PhD in databases at UC Berkeley.
[00:01:26.700 --> 00:01:30.520]   And she was, prior to this,
[00:01:30.520 --> 00:01:32.840]   she was the first ML engineer at Viaduct
[00:01:32.840 --> 00:01:35.460]   and also did research at Google Brain
[00:01:35.460 --> 00:01:38.100]   and also software engineering at Facebook.
[00:01:38.100 --> 00:01:40.160]   One thing that's really interesting about Shreya
[00:01:40.160 --> 00:01:43.520]   is her research is very,
[00:01:43.520 --> 00:01:46.040]   it touches applied machine learning
[00:01:46.040 --> 00:01:51.040]   in a ways that you don't think academics usually do,
[00:01:51.040 --> 00:01:52.520]   but she's actually very ingrained
[00:01:52.520 --> 00:01:55.880]   into what practitioners are doing
[00:01:55.880 --> 00:01:57.380]   in applied machine learning.
[00:01:57.380 --> 00:01:58.880]   She focuses on that.
[00:01:58.880 --> 00:02:03.880]   And recently she published a paper
[00:02:03.880 --> 00:02:08.040]   surveying the field of applied machine learning
[00:02:08.040 --> 00:02:09.560]   and its pain points.
[00:02:09.560 --> 00:02:10.840]   And that has been discussed.
[00:02:10.840 --> 00:02:12.680]   You've probably, there's a,
[00:02:12.680 --> 00:02:13.920]   she's given some talks on that
[00:02:13.920 --> 00:02:17.000]   and she was recently interviewed by Lucas Biewald,
[00:02:17.000 --> 00:02:20.440]   the CEO of Weights and Biases, about that paper.
[00:02:20.440 --> 00:02:22.560]   So when I first heard of LLM Ops,
[00:02:22.560 --> 00:02:24.840]   I wanted to poke my eyes out
[00:02:24.840 --> 00:02:27.040]   'cause I thought, oh, is this like another,
[00:02:27.040 --> 00:02:28.280]   I literally at first saw the word,
[00:02:28.280 --> 00:02:31.880]   okay, this seems like some kind of hype word or something.
[00:02:31.880 --> 00:02:34.680]   Emmanuel, I know you do some consulting around LLMs.
[00:02:34.680 --> 00:02:36.560]   You do a lot of consulting around LLMs.
[00:02:36.560 --> 00:02:39.520]   And Shreya, you also have been looking into LLM APIs.
[00:02:39.520 --> 00:02:44.840]   So my question for both of y'all is,
[00:02:44.840 --> 00:02:47.760]   can you share any Ops stuff you think
[00:02:47.760 --> 00:02:49.880]   around LLMs that have been useful so far
[00:02:49.880 --> 00:02:51.080]   or if there is something useful?
[00:02:51.080 --> 00:02:52.280]   And like, also, what is your sense
[00:02:52.280 --> 00:02:53.480]   of the kinds of enterprise systems
[00:02:53.480 --> 00:02:54.720]   that might be useful here?
[00:02:54.720 --> 00:02:57.560]   Or if you have looked at that at all.
[00:02:57.560 --> 00:03:00.680]   - It's a big question.
[00:03:00.680 --> 00:03:04.240]   Okay, so I have one project that I'm working on
[00:03:04.240 --> 00:03:07.120]   in which I have two, I don't know what to call them,
[00:03:07.120 --> 00:03:10.000]   client, customer, sounds really fancy,
[00:03:10.000 --> 00:03:12.400]   but like I'm not a startup, I'm a researcher.
[00:03:12.400 --> 00:03:16.200]   The two case studies that I'm working with
[00:03:16.200 --> 00:03:17.920]   that are using LLMs.
[00:03:19.600 --> 00:03:23.040]   And the target audience that I'm interested in
[00:03:23.040 --> 00:03:24.560]   is people who know how to program,
[00:03:24.560 --> 00:03:25.880]   but they're not data experts
[00:03:25.880 --> 00:03:28.000]   and they're not like ML experts.
[00:03:28.000 --> 00:03:30.000]   So they can write Python,
[00:03:30.000 --> 00:03:33.000]   but they don't know how to reason
[00:03:33.000 --> 00:03:34.880]   between training and serving pipelines.
[00:03:34.880 --> 00:03:36.920]   And they don't know what to think about,
[00:03:36.920 --> 00:03:39.840]   like updating data, what state is updating,
[00:03:39.840 --> 00:03:42.480]   how do I choose the API, blah, blah, blah,
[00:03:42.480 --> 00:03:43.920]   all sorts of things.
[00:03:43.920 --> 00:03:47.080]   But they can make HTTP requests.
[00:03:47.080 --> 00:03:49.480]   They can write Python, they can deploy Python.
[00:03:49.480 --> 00:03:52.960]   So just to give you a sense of that.
[00:03:52.960 --> 00:03:56.560]   So when it comes to LLM ops,
[00:03:56.560 --> 00:04:01.240]   I think the first big challenge that I'm seeing
[00:04:01.240 --> 00:04:03.240]   that like stuff like link chain
[00:04:03.240 --> 00:04:06.360]   is kind of solving is state management.
[00:04:06.360 --> 00:04:11.360]   I first deploy my pipeline as a developer
[00:04:13.040 --> 00:04:15.840]   and I learn new information
[00:04:15.840 --> 00:04:19.040]   that I want to inject into the prompt
[00:04:19.040 --> 00:04:20.560]   or keep in the prompt,
[00:04:20.560 --> 00:04:22.080]   or I want to fine tune on,
[00:04:22.080 --> 00:04:28.520]   or I want to, I don't know, summarize, make some action.
[00:04:28.520 --> 00:04:34.840]   And this state can be different per end user.
[00:04:34.840 --> 00:04:37.840]   So I guess one big challenge around there
[00:04:37.840 --> 00:04:39.640]   is like how do I do that state management
[00:04:39.640 --> 00:04:43.760]   as I iteratively question my LLM API?
[00:04:43.760 --> 00:04:45.560]   And there's like a lot of research these days
[00:04:45.560 --> 00:04:47.240]   and kind of chain of thought prompting
[00:04:47.240 --> 00:04:49.440]   or all these fancy terminology that is just,
[00:04:49.440 --> 00:04:51.800]   how do I reason about state management
[00:04:51.800 --> 00:04:54.720]   to finagle my way to the best user experience?
[00:04:54.720 --> 00:04:57.200]   So that's one.
[00:04:57.200 --> 00:05:00.280]   And I think the other one that I'm seeing design pattern
[00:05:00.280 --> 00:05:02.160]   that people are struggling with
[00:05:02.160 --> 00:05:07.160]   is how do I incorporate information about the world
[00:05:08.000 --> 00:05:11.440]   that the model might not have been trained on?
[00:05:11.440 --> 00:05:15.400]   So one example of this is like retrieval
[00:05:15.400 --> 00:05:17.480]   or like I have custom documentation
[00:05:17.480 --> 00:05:19.120]   that I want to make a chatbot on.
[00:05:19.120 --> 00:05:25.040]   So solutions like link chain might summarize each document,
[00:05:25.040 --> 00:05:27.400]   put it in a vector and then do some sort of
[00:05:27.400 --> 00:05:32.400]   like nearest neighbor, vector similarity search and so forth.
[00:05:32.400 --> 00:05:36.840]   But every time I call the summarization
[00:05:36.840 --> 00:05:39.040]   or call the LLM inference API, right?
[00:05:39.040 --> 00:05:40.080]   I'm paying money.
[00:05:40.080 --> 00:05:44.080]   There's a lot of, I don't know, open questions
[00:05:44.080 --> 00:05:47.640]   about like what are the best ways to solve these problems?
[00:05:47.640 --> 00:05:49.800]   - Okay, when I'm using things like link chain,
[00:05:49.800 --> 00:05:52.360]   there's a nagging feeling and maybe just,
[00:05:52.360 --> 00:05:56.680]   I'm just like a crusty developer or something stage of my life
[00:05:56.680 --> 00:05:58.840]   almost have this feeling that I guess
[00:05:58.840 --> 00:06:01.960]   doing something somewhat simple, not too simple,
[00:06:01.960 --> 00:06:05.680]   but it feels a lot of complexity for what it's doing,
[00:06:05.680 --> 00:06:07.440]   at least for a lot of things.
[00:06:07.440 --> 00:06:09.480]   And like, where does it fall for you
[00:06:09.480 --> 00:06:11.480]   in terms of trade-offs?
[00:06:11.480 --> 00:06:14.200]   - It feels complex because it is so hard
[00:06:14.200 --> 00:06:18.600]   to make an abstraction that solves everybody's LLM use case.
[00:06:18.600 --> 00:06:21.480]   But all LLM pipelines are like the model
[00:06:21.480 --> 00:06:23.120]   with a bunch of guardrails around it,
[00:06:23.120 --> 00:06:25.880]   a bunch of rules, a bunch of filters, whatever it is.
[00:06:25.880 --> 00:06:27.480]   And all of these rules and guardrails
[00:06:27.480 --> 00:06:29.560]   are different for each application.
[00:06:29.560 --> 00:06:31.560]   That is the kind, it's really frustrating,
[00:06:31.560 --> 00:06:34.280]   I think from the tool builder perspective.
[00:06:34.280 --> 00:06:37.560]   How do I build an abstraction that's simple
[00:06:37.560 --> 00:06:40.840]   and solves more than one case?
[00:06:40.840 --> 00:06:45.000]   But I feel you on the, yeah, it's so complicated.
[00:06:45.000 --> 00:06:46.840]   Why are there 10 different contexts?
[00:06:46.840 --> 00:06:50.800]   Why do I have a different integration
[00:06:50.800 --> 00:06:53.640]   with each LLM provider,
[00:06:53.640 --> 00:06:56.840]   with each vector search data, vector embedding database?
[00:06:56.840 --> 00:07:01.040]   I'm with you on the frustration here.
[00:07:02.040 --> 00:07:04.200]   - I think part of it too comes from,
[00:07:04.200 --> 00:07:07.040]   like if, you know, back when I was at Stripe
[00:07:07.040 --> 00:07:09.920]   and a lot of ML companies, you have these very,
[00:07:09.920 --> 00:07:11.720]   you manage to cast your business problem
[00:07:11.720 --> 00:07:14.960]   in a very well-defined mathematical way.
[00:07:14.960 --> 00:07:17.000]   Like eventually you manage to cast,
[00:07:17.000 --> 00:07:19.080]   you know, like fraud prevention at Stripe.
[00:07:19.080 --> 00:07:22.920]   So like, okay, we have a binary outcome, zero or one.
[00:07:22.920 --> 00:07:24.120]   We have other numbers.
[00:07:24.120 --> 00:07:26.200]   We use the numbers to predict the final number.
[00:07:26.200 --> 00:07:29.320]   It's like a very well-framed situation
[00:07:29.320 --> 00:07:30.160]   that you end up in the end.
[00:07:30.160 --> 00:07:31.840]   And so you can build a bunch of tooling around it
[00:07:31.840 --> 00:07:32.680]   'cause you're like, okay,
[00:07:32.680 --> 00:07:33.800]   well, I can make a lot of assumptions, right?
[00:07:33.800 --> 00:07:35.160]   I can make the assumption that,
[00:07:35.160 --> 00:07:37.120]   you know, you have two classes you care about.
[00:07:37.120 --> 00:07:39.480]   You're gonna care about the distribution of these classes
[00:07:39.480 --> 00:07:41.240]   in various domains.
[00:07:41.240 --> 00:07:43.760]   You wanna slice and dice the data, et cetera, et cetera.
[00:07:43.760 --> 00:07:47.200]   It was like, it feels like the output space
[00:07:47.200 --> 00:07:50.320]   and input space of LLMs is just like all of language.
[00:07:50.320 --> 00:07:52.680]   And so you have this massive beast
[00:07:52.680 --> 00:07:54.600]   that can kind of like input or output anything.
[00:07:54.600 --> 00:07:56.480]   You can't really make assumptions.
[00:07:56.480 --> 00:07:59.640]   Like even I've worked with folks
[00:07:59.640 --> 00:08:02.240]   and try to use them as something
[00:08:02.240 --> 00:08:04.400]   that's closer to classifiers,
[00:08:04.400 --> 00:08:06.080]   but because they output natural text,
[00:08:06.080 --> 00:08:08.080]   sometimes your LLM that you're using as a classifier
[00:08:08.080 --> 00:08:09.440]   is just gonna output like another token
[00:08:09.440 --> 00:08:10.400]   that you didn't expect.
[00:08:10.400 --> 00:08:13.920]   So your tooling has to kind of like handle
[00:08:13.920 --> 00:08:17.160]   just a really like a huge variety, I feel, of edge cases.
[00:08:17.160 --> 00:08:19.840]   And so it makes it hard to build like nice, clean abstractions.
[00:08:19.840 --> 00:08:23.240]   They have to be these kind of like messy air balls.
[00:08:23.240 --> 00:08:24.400]   - Manuel, can you tell us a little bit
[00:08:24.400 --> 00:08:26.040]   about your work on LLM consulting?
[00:08:26.040 --> 00:08:28.880]   Like what kind of stuff do you do?
[00:08:28.880 --> 00:08:32.400]   Like what, like does it usually involve?
[00:08:32.400 --> 00:08:34.680]   I mean, I have a feeling like it's different
[00:08:34.680 --> 00:08:37.000]   than like traditional ML projects
[00:08:37.000 --> 00:08:38.760]   that we have seen in the past.
[00:08:38.760 --> 00:08:40.720]   So that's why I think it's interesting.
[00:08:40.720 --> 00:08:44.520]   - Yeah, so maybe for context,
[00:08:44.520 --> 00:08:47.080]   what I worked on most recently before doing this
[00:08:47.080 --> 00:08:50.520]   was like very traditional ML stuff, as you point out.
[00:08:50.520 --> 00:08:53.640]   So like making new models, making ML models better,
[00:08:53.640 --> 00:08:55.840]   or working on, you know, I spent a year
[00:08:55.840 --> 00:08:57.880]   kind of leading a large ML op effort.
[00:08:57.880 --> 00:09:00.320]   But again, for kind of traditional ML, you know,
[00:09:00.320 --> 00:09:04.720]   so making model releases easier, safer, that sort of stuff.
[00:09:04.720 --> 00:09:09.720]   And now recently working with LLMs, initially I worked,
[00:09:09.720 --> 00:09:12.840]   I did a bit of work with just prompt engineering,
[00:09:12.840 --> 00:09:14.040]   which is not machine learning.
[00:09:14.040 --> 00:09:17.040]   It's just, I don't know, insanity in my opinion,
[00:09:17.040 --> 00:09:19.240]   but that's just the world we live in.
[00:09:19.240 --> 00:09:20.800]   So doing some of that,
[00:09:20.800 --> 00:09:24.040]   and then basically comparing various APIs
[00:09:24.040 --> 00:09:26.160]   to various open source models.
[00:09:26.160 --> 00:09:28.880]   I think I actually think there's like an interesting topic
[00:09:28.880 --> 00:09:30.920]   there for ops, which is like,
[00:09:30.920 --> 00:09:32.320]   you have all these open source models
[00:09:32.320 --> 00:09:35.320]   and even just running them is a pain,
[00:09:35.320 --> 00:09:38.160]   much less serving them, fine tuning them.
[00:09:38.160 --> 00:09:40.000]   And so that's been the other side of the work,
[00:09:40.000 --> 00:09:41.640]   kind of like getting these models to run,
[00:09:41.640 --> 00:09:43.480]   getting these models like be fine tuned
[00:09:43.480 --> 00:09:45.880]   and have good results for tasks.
[00:09:45.880 --> 00:09:50.880]   What often happens is a company will start with a API
[00:09:50.880 --> 00:09:55.600]   and it'll work for a while.
[00:09:55.600 --> 00:09:57.720]   And then at some point maybe they'll want to expand
[00:09:57.720 --> 00:10:00.080]   outside of that API, or like they have some,
[00:10:00.080 --> 00:10:01.040]   exactly what Trey was saying,
[00:10:01.040 --> 00:10:03.320]   they have some internal data that they want to leverage
[00:10:03.320 --> 00:10:05.760]   and they try doing the kind of like embedding vector store
[00:10:05.760 --> 00:10:07.560]   and maybe they found that it didn't work that well,
[00:10:07.560 --> 00:10:09.880]   so they want to fine tune their own model.
[00:10:09.880 --> 00:10:10.720]   So that's the sort of work,
[00:10:10.720 --> 00:10:13.320]   kind of comparing that approach and its costs
[00:10:13.320 --> 00:10:15.400]   and its trade-offs to, you know,
[00:10:15.400 --> 00:10:18.800]   having a prompt engineer,
[00:10:18.800 --> 00:10:22.000]   just do prompt engineering for two more weeks.
[00:10:22.000 --> 00:10:23.720]   I think the last thing I'll say about it,
[00:10:23.720 --> 00:10:25.600]   which I think is really fascinating in the field
[00:10:25.600 --> 00:10:27.880]   and a lot of my work is like,
[00:10:27.880 --> 00:10:29.480]   it's hard right now to know
[00:10:29.480 --> 00:10:31.320]   what will give you the best bang for your buck.
[00:10:31.320 --> 00:10:33.240]   So it's hard to know whether you should like
[00:10:33.240 --> 00:10:35.080]   spend 10 more hours prompt engineering
[00:10:35.080 --> 00:10:37.160]   or whether like you've reached the limit of the model
[00:10:37.160 --> 00:10:38.480]   and you should just get another model
[00:10:38.480 --> 00:10:41.040]   or bring your problem differently.
[00:10:41.040 --> 00:10:44.320]   And I think that that's maybe like one of the challenges too
[00:10:44.320 --> 00:10:46.400]   with building nice abstractions is that
[00:10:46.400 --> 00:10:51.000]   people don't really know what the next best step is.
[00:10:51.000 --> 00:10:51.840]   - It's interesting.
[00:10:51.840 --> 00:10:56.000]   It's familiar almost from traditional machine learning
[00:10:56.000 --> 00:10:56.880]   in some ways.
[00:10:56.880 --> 00:10:58.080]   - Yeah.
[00:10:58.080 --> 00:10:59.240]   - Okay, I want to switch gears.
[00:10:59.240 --> 00:11:01.040]   I want to definitely come back to LLMs.
[00:11:01.040 --> 00:11:03.320]   I think we could spend the entire talk talking about LLMs
[00:11:03.320 --> 00:11:06.440]   is how interesting they are and like how they shift.
[00:11:06.440 --> 00:11:08.200]   There's like a different modality of work
[00:11:08.200 --> 00:11:09.720]   and all kinds of stuff.
[00:11:09.720 --> 00:11:11.200]   I don't want to get too hung up on that.
[00:11:11.200 --> 00:11:16.200]   So Shreya, in the recent paper that we're discussing
[00:11:16.200 --> 00:11:19.320]   on operationalizing machine learning,
[00:11:19.320 --> 00:11:21.600]   you qualified the participants as,
[00:11:21.600 --> 00:11:26.000]   you know, by if they are carrying a pager,
[00:11:26.000 --> 00:11:27.600]   if they're going to get paged,
[00:11:27.600 --> 00:11:29.600]   if something goes wrong with the model,
[00:11:29.600 --> 00:11:31.640]   that was like a criteria for,
[00:11:31.640 --> 00:11:34.120]   okay, are they doing applied ML?
[00:11:34.120 --> 00:11:36.280]   And I thought that was super interesting.
[00:11:36.280 --> 00:11:40.000]   And the reason that is, is like from my experience,
[00:11:40.000 --> 00:11:42.680]   I observed like I've only seen pager duty
[00:11:42.680 --> 00:11:44.560]   kind of assigned to machine learning engineers
[00:11:44.560 --> 00:11:46.520]   maybe a third of the time.
[00:11:46.520 --> 00:11:49.480]   And I would like to see it done more often.
[00:11:49.480 --> 00:11:54.480]   But one kind of interesting sort of detail is,
[00:11:54.480 --> 00:11:57.760]   I've seen that like the monitoring systems
[00:11:57.760 --> 00:12:00.040]   that the rest of the organization uses,
[00:12:00.040 --> 00:12:03.600]   like, I don't know, Datadog, Prometheus, whatever,
[00:12:03.600 --> 00:12:07.280]   for web apps, like the ML folks have a hard time
[00:12:07.280 --> 00:12:11.400]   pushing their metrics and things
[00:12:11.400 --> 00:12:13.200]   and instrumenting them there.
[00:12:13.200 --> 00:12:15.480]   And then what ends up happening is,
[00:12:15.480 --> 00:12:18.640]   like, it just kind of gets left out, sort of.
[00:12:18.640 --> 00:12:22.480]   And then some other more loose thing
[00:12:22.480 --> 00:12:24.440]   is there like a dashboard or something
[00:12:24.440 --> 00:12:27.120]   is not really carrying a pager.
[00:12:27.120 --> 00:12:29.480]   So I'm just curious, like, again,
[00:12:29.480 --> 00:12:32.080]   this could be just 'cause like I'm just doing a lot of,
[00:12:32.080 --> 00:12:34.480]   like, associate a lot of maybe uncool stuff.
[00:12:34.480 --> 00:12:37.320]   But like, how does it work?
[00:12:37.320 --> 00:12:39.360]   Like, how does it, the people,
[00:12:39.360 --> 00:12:41.000]   you see people getting trapped in that?
[00:12:41.000 --> 00:12:44.960]   Or like, how did they kind of fit in with,
[00:12:44.960 --> 00:12:48.720]   like, let's say the rest of monitoring or, you know?
[00:12:48.720 --> 00:12:49.560]   - Yeah.
[00:12:49.560 --> 00:12:52.720]   Okay, I think that's a great question on like,
[00:12:52.720 --> 00:12:55.520]   why can't people use Datadog?
[00:12:55.520 --> 00:12:59.760]   So let's be clear, SREs get a ton of alerts also,
[00:12:59.760 --> 00:13:01.840]   but they have the ability to look at an alert
[00:13:01.840 --> 00:13:03.920]   and tell you whether it's meaningful or not.
[00:13:03.920 --> 00:13:08.200]   Even the most expert ML engineer is looking at an ML alert
[00:13:08.200 --> 00:13:11.200]   and just cannot tell you whether this feature
[00:13:11.200 --> 00:13:13.440]   drifted by this much, is this meaningful or not?
[00:13:13.440 --> 00:13:15.080]   No one knows.
[00:13:15.080 --> 00:13:17.360]   So that is like the difference, I think.
[00:13:17.360 --> 00:13:19.000]   It's not necessarily the tooling,
[00:13:19.000 --> 00:13:21.040]   it's just the fact that semantically,
[00:13:21.040 --> 00:13:25.000]   we don't understand what actually causes the ML pipeline
[00:13:25.000 --> 00:13:28.720]   to be broken enough to warrant some engineer's attention.
[00:13:28.720 --> 00:13:31.840]   So that's one thing.
[00:13:31.840 --> 00:13:34.160]   I think another thing is they might not be,
[00:13:34.160 --> 00:13:36.640]   ML engineers might not be carrying a pager,
[00:13:36.640 --> 00:13:38.520]   but they will get Slack notification
[00:13:38.520 --> 00:13:40.200]   or CloudWatch notifications
[00:13:40.200 --> 00:13:43.640]   that were always in the form of this column,
[00:13:43.640 --> 00:13:46.800]   which is feature or output prediction or whatever,
[00:13:46.800 --> 00:13:50.960]   has this deviation from this threshold,
[00:13:50.960 --> 00:13:53.240]   like X deviation from Y threshold.
[00:13:53.240 --> 00:13:55.440]   And this is just a pain.
[00:13:55.440 --> 00:13:58.840]   You're gonna get hundreds of these things.
[00:13:58.840 --> 00:14:00.680]   In my recent data validation paper,
[00:14:00.680 --> 00:14:01.800]   which I promise I will put,
[00:14:01.800 --> 00:14:06.800]   I hopefully will put online before this panel goes live,
[00:14:10.120 --> 00:14:15.120]   but we identified several causes of false positive alerts.
[00:14:15.120 --> 00:14:17.920]   One is highly correlated features.
[00:14:17.920 --> 00:14:21.080]   Data scientists love making highly correlated features,
[00:14:21.080 --> 00:14:25.000]   like probability of click after one second,
[00:14:25.000 --> 00:14:26.880]   probability of click after three seconds,
[00:14:26.880 --> 00:14:27.840]   after five seconds.
[00:14:27.840 --> 00:14:30.040]   These are by definition correlated,
[00:14:30.040 --> 00:14:31.160]   but they all exist
[00:14:31.160 --> 00:14:33.760]   because they make the model have higher performance.
[00:14:33.760 --> 00:14:37.000]   So in this case,
[00:14:37.120 --> 00:14:40.480]   how do you handle very correlated features?
[00:14:40.480 --> 00:14:42.160]   A lot of people, as Emmanuel said,
[00:14:42.160 --> 00:14:45.680]   like to monitor all the features as well as the predictions.
[00:14:45.680 --> 00:14:47.040]   If I were to tell somebody,
[00:14:47.040 --> 00:14:48.480]   if you had to monitor,
[00:14:48.480 --> 00:14:50.200]   just monitor your prediction,
[00:14:50.200 --> 00:14:53.040]   like plot histograms of your prediction column.
[00:14:53.040 --> 00:14:53.880]   That's it.
[00:14:53.880 --> 00:14:57.280]   Sliding window histogram of what you predict.
[00:14:57.280 --> 00:14:58.360]   You will identify,
[00:14:58.360 --> 00:15:00.480]   whatever anomaly you identify there
[00:15:00.480 --> 00:15:03.320]   is worth your attention.
[00:15:03.320 --> 00:15:06.240]   You won't catch all the problems,
[00:15:06.240 --> 00:15:07.680]   but whatever you catch,
[00:15:07.680 --> 00:15:09.360]   you'll catch like a real issue,
[00:15:09.360 --> 00:15:13.440]   which means like the feature problem is very hard to do.
[00:15:13.440 --> 00:15:17.120]   Another big thing is we identify in the paper
[00:15:17.120 --> 00:15:20.680]   is that people love to compare the current partition
[00:15:20.680 --> 00:15:23.720]   to an aggregation of all historical partitions,
[00:15:23.720 --> 00:15:26.640]   which does not make a lot of sense.
[00:15:26.640 --> 00:15:28.640]   Like suppose today is a holiday.
[00:15:28.640 --> 00:15:31.400]   I only want to compare it against previous holidays.
[00:15:31.400 --> 00:15:32.480]   Suppose it's the weekend.
[00:15:32.480 --> 00:15:35.520]   I only want to compare it against previous weekends.
[00:15:35.520 --> 00:15:37.200]   I'm going to get false positive alerts
[00:15:37.200 --> 00:15:39.640]   if I compare Saturday to Friday
[00:15:39.640 --> 00:15:41.600]   for a lot of like e-commerce.
[00:15:41.600 --> 00:15:44.160]   So let's just, it's going to happen.
[00:15:44.160 --> 00:15:45.720]   So like being smarter,
[00:15:45.720 --> 00:15:50.200]   kind of like how do we alert just like bare minimum
[00:15:50.200 --> 00:15:52.320]   and not just throw alerts at everything
[00:15:52.320 --> 00:15:54.440]   and submit this huge PR
[00:15:54.440 --> 00:15:57.320]   with all of these things that we're alerting.
[00:15:57.320 --> 00:15:59.520]   I think that can go a really long way.
[00:15:59.520 --> 00:16:00.360]   - Yeah.
[00:16:00.360 --> 00:16:04.560]   That gave me PTSD for experience getting paid.
[00:16:04.560 --> 00:16:05.400]   - Tell us more.
[00:16:05.400 --> 00:16:07.160]   - That's a PTSD is really interesting
[00:16:07.160 --> 00:16:09.520]   because I think it like probably highlights
[00:16:09.520 --> 00:16:11.320]   what a lot of people feel.
[00:16:11.320 --> 00:16:12.320]   It gave me PTSD.
[00:16:12.320 --> 00:16:13.600]   It gives me PTSD too,
[00:16:13.600 --> 00:16:15.840]   which is why I like to talk about it.
[00:16:15.840 --> 00:16:16.680]   It's like a therapy.
[00:16:16.680 --> 00:16:19.680]   - I think everything you said was exactly on the money.
[00:16:19.680 --> 00:16:23.720]   Like both the kind of like,
[00:16:23.720 --> 00:16:25.640]   it's actually super hard to craft these alerts.
[00:16:25.640 --> 00:16:27.680]   Like everything you said makes sense.
[00:16:27.680 --> 00:16:30.240]   And yet it's not the first thing that I think anyone thinks
[00:16:30.240 --> 00:16:31.120]   of when they're crafting the alert.
[00:16:31.120 --> 00:16:32.280]   Like, you know, we'll just look at like
[00:16:32.280 --> 00:16:33.120]   whatever the running mean.
[00:16:33.120 --> 00:16:35.280]   And if it, you know, like outside of a certain battle,
[00:16:35.280 --> 00:16:36.120]   we'll do something.
[00:16:36.120 --> 00:16:38.480]   Or even if you do like a more complex statistical measure,
[00:16:38.480 --> 00:16:39.880]   you forget that like, yeah,
[00:16:39.880 --> 00:16:42.080]   you're going to get paged every Sunday or something
[00:16:42.080 --> 00:16:43.800]   because like the traffic changes.
[00:16:43.800 --> 00:16:44.680]   And so it takes,
[00:16:44.680 --> 00:16:46.840]   I feel like you learn this because you get paid Sunday
[00:16:46.840 --> 00:16:48.840]   and then you actually do the right thing.
[00:16:48.840 --> 00:16:50.640]   The other thing is that, yeah,
[00:16:50.640 --> 00:16:51.880]   I think to Hamil's point,
[00:16:51.880 --> 00:16:54.320]   a lot of these tools make that debugging hard.
[00:16:54.320 --> 00:16:57.160]   Like I feel like I'm decent at ML
[00:16:57.160 --> 00:17:00.200]   and I can't tell you how many times I've gotten paged.
[00:17:00.200 --> 00:17:01.320]   I look at the chart and I'm like, oh yeah,
[00:17:01.320 --> 00:17:03.520]   something is going horribly wrong.
[00:17:03.520 --> 00:17:04.640]   And then I zoom out, you know,
[00:17:04.640 --> 00:17:06.160]   I was just looking at the last five minutes of the,
[00:17:06.160 --> 00:17:08.280]   I zoom out like 24 hours or a week.
[00:17:08.280 --> 00:17:10.120]   And I'm like, oh no, this is like normal traffic patterns.
[00:17:10.120 --> 00:17:11.400]   Just like, where, you know,
[00:17:11.400 --> 00:17:13.440]   in like the evidence law of our traffic
[00:17:13.440 --> 00:17:14.960]   or something of the kind.
[00:17:14.960 --> 00:17:18.760]   And I think a lot of tools aren't really built for that,
[00:17:18.760 --> 00:17:20.080]   or they don't really show you that well.
[00:17:20.080 --> 00:17:22.520]   Like a lot of like histograms are just like by default,
[00:17:22.520 --> 00:17:25.240]   the most recent end minutes without contextualizing it.
[00:17:25.240 --> 00:17:29.040]   I don't know if like there's easy ways,
[00:17:29.040 --> 00:17:31.280]   or if there's opportunities to like build specific tools
[00:17:31.280 --> 00:17:32.200]   rather than do this,
[00:17:32.200 --> 00:17:33.120]   or if it's just to your point,
[00:17:33.120 --> 00:17:35.440]   using them better and kind of thinking carefully about.
[00:17:35.440 --> 00:17:37.800]   - Were you forced to use like the traditional,
[00:17:37.800 --> 00:17:39.240]   like to also Shreya's point,
[00:17:39.240 --> 00:17:44.240]   like I have been forced or attempted,
[00:17:44.240 --> 00:17:45.520]   people have attempted to force me
[00:17:45.520 --> 00:17:47.960]   to use those traditional data dog Prometheus stuff
[00:17:47.960 --> 00:17:51.560]   to jam in my ML stuff in there.
[00:17:51.560 --> 00:17:54.160]   And which is why I have PTSD.
[00:17:54.160 --> 00:17:55.080]   Is that common?
[00:17:55.080 --> 00:17:59.120]   Like, have you been forced to jam your stuff in there too?
[00:17:59.120 --> 00:18:01.160]   - I'll maybe frame what you're saying in a different way,
[00:18:01.160 --> 00:18:02.920]   which is I think there's usually a unified stack.
[00:18:02.920 --> 00:18:04.680]   And it's just like, that's the solution used for logging,
[00:18:04.680 --> 00:18:07.720]   for log searching, for log recognition, for like whatever.
[00:18:07.720 --> 00:18:10.840]   And so, yeah, like I, but I also don't know,
[00:18:10.840 --> 00:18:11.680]   maybe I don't know the field well enough,
[00:18:11.680 --> 00:18:13.360]   but I don't know if there's like a cool ML solution
[00:18:13.360 --> 00:18:14.200]   that does this for you,
[00:18:14.200 --> 00:18:16.200]   like that you could use to replace some of these.
[00:18:16.200 --> 00:18:18.280]   It does like the ML part really well.
[00:18:18.280 --> 00:18:19.120]   - I don't know.
[00:18:19.120 --> 00:18:20.520]   Yeah, I don't know either.
[00:18:20.520 --> 00:18:21.600]   - Yeah.
[00:18:21.600 --> 00:18:22.440]   - That's a good question.
[00:18:22.440 --> 00:18:23.280]   We should find out.
[00:18:23.280 --> 00:18:26.560]   - Nothing I've seen, I don't know.
[00:18:26.560 --> 00:18:29.960]   Yeah, I've tried to use like,
[00:18:29.960 --> 00:18:32.400]   I once went on this journey of like,
[00:18:32.400 --> 00:18:35.560]   I'm going to prove to everyone that you can use Prometheus
[00:18:35.560 --> 00:18:37.960]   to monitor anything ML related.
[00:18:37.960 --> 00:18:42.320]   And like, yeah, I could,
[00:18:42.320 --> 00:18:45.320]   but it was just like shoehorning.
[00:18:45.320 --> 00:18:48.320]   I just felt like I was trying to like shove like a,
[00:18:48.320 --> 00:18:49.400]   what is that analogy?
[00:18:49.400 --> 00:18:51.200]   Like a square peg in the round hole.
[00:18:51.200 --> 00:18:55.120]   Like I like whittled it down and I made it work somehow,
[00:18:55.120 --> 00:18:57.920]   but I was doing all these like high cardinality joins
[00:18:57.920 --> 00:18:59.480]   over Prometheus QL,
[00:18:59.480 --> 00:19:02.440]   like with things that are just like you shouldn't be doing.
[00:19:02.440 --> 00:19:05.000]   And in the past,
[00:19:05.000 --> 00:19:07.600]   I think I was more software engineer oriented.
[00:19:07.600 --> 00:19:10.080]   Like you should be able to do any problem
[00:19:10.080 --> 00:19:11.360]   with a software engineering tool.
[00:19:11.360 --> 00:19:14.000]   Like ML ops should be subsumed in DevOps
[00:19:14.000 --> 00:19:18.840]   or LLM ops should be subsumed and like consumed in ML ops.
[00:19:18.840 --> 00:19:21.760]   But I feel like, you know, these things warrant,
[00:19:21.760 --> 00:19:24.040]   these are new design patterns, new things we're learning.
[00:19:24.040 --> 00:19:25.720]   They do warrant different tools.
[00:19:25.720 --> 00:19:28.520]   It's okay.
[00:19:29.520 --> 00:19:31.720]   The other thing I'll add in terms of tooling,
[00:19:31.720 --> 00:19:33.080]   'cause I was thinking how many were mentioning
[00:19:33.080 --> 00:19:35.000]   that like ML engineers should, you know,
[00:19:35.000 --> 00:19:36.960]   maybe carry pagers more often.
[00:19:36.960 --> 00:19:40.000]   I feel like I used to agree with that sentiment a lot
[00:19:40.000 --> 00:19:41.560]   and I still do to a certain extent.
[00:19:41.560 --> 00:19:43.640]   But one thing that's clear to me is that
[00:19:43.640 --> 00:19:46.760]   the resolution of issues is just so different for ML
[00:19:46.760 --> 00:19:48.160]   that you kind of want different tools, right?
[00:19:48.160 --> 00:19:52.120]   Whereas like, if you see, you know, engineers or SREs,
[00:19:52.120 --> 00:19:53.360]   like there'll be a lot of like,
[00:19:53.360 --> 00:19:55.680]   oh, what does this bit of code do?
[00:19:55.680 --> 00:19:57.320]   Let's make like a quick PR that fixes it
[00:19:57.320 --> 00:19:59.000]   or roll back this change.
[00:19:59.000 --> 00:20:00.720]   Whereas for ML, at least in my experience,
[00:20:00.720 --> 00:20:03.080]   like 99% of the time it's retrain the model,
[00:20:03.080 --> 00:20:06.360]   try a split or circuit break the model,
[00:20:06.360 --> 00:20:09.240]   like until we can get up and actually like investigate
[00:20:09.240 --> 00:20:11.400]   and just like, you know, don't run the model
[00:20:11.400 --> 00:20:13.600]   if that's something that you can do at all.
[00:20:13.600 --> 00:20:16.520]   Which feels like you're not really providing
[00:20:16.520 --> 00:20:18.360]   any of the value for being like an ML expert.
[00:20:18.360 --> 00:20:20.160]   You're doing that like, it's just like,
[00:20:20.160 --> 00:20:21.680]   you could almost just automate it, right?
[00:20:21.680 --> 00:20:24.800]   Like don't even page anyone, just do that automatically.
[00:20:24.800 --> 00:20:25.640]   And so, I don't know.
[00:20:25.640 --> 00:20:27.680]   I think there is a room for like,
[00:20:27.680 --> 00:20:31.320]   either it's tools or just a better model
[00:20:31.320 --> 00:20:33.320]   of like organizing ML software
[00:20:33.320 --> 00:20:35.920]   so that you can actually do these like quick hot fixes.
[00:20:35.920 --> 00:20:37.000]   But I just think like as a field,
[00:20:37.000 --> 00:20:38.800]   we're not really there yet.
[00:20:38.800 --> 00:20:40.880]   We just have very blunt instruments.
[00:20:40.880 --> 00:20:44.080]   - Both of you mentioned kind of DevOps
[00:20:44.080 --> 00:20:45.800]   and like Trey, you mentioned, okay,
[00:20:45.800 --> 00:20:47.320]   you went on this quest for from ETS
[00:20:47.320 --> 00:20:49.720]   and like software engineer mindset.
[00:20:49.720 --> 00:20:51.960]   One question that always comes up around that,
[00:20:51.960 --> 00:20:55.160]   when we talk about this like ML ops term
[00:20:55.160 --> 00:20:56.960]   is like people like talk about DevOps
[00:20:56.960 --> 00:20:59.760]   and how like it's inspired by DevOps or whatever.
[00:20:59.760 --> 00:21:02.840]   And then like there's a central kind of tool,
[00:21:02.840 --> 00:21:06.840]   a set of tools in DevOps, like CI/CD tool.
[00:21:06.840 --> 00:21:09.280]   And so people always ask me,
[00:21:09.280 --> 00:21:14.720]   like, is there CI, like what is CI/CD for ML?
[00:21:14.720 --> 00:21:17.600]   And I don't really, I mean, yeah,
[00:21:17.600 --> 00:21:19.320]   people always ask me that.
[00:21:19.320 --> 00:21:22.000]   The answer is very complex.
[00:21:22.000 --> 00:21:25.840]   It's not just, okay, GitHub actions on your ML.
[00:21:25.840 --> 00:21:27.840]   I mean, that's not, doesn't make sense.
[00:21:27.840 --> 00:21:34.440]   But so like, how do y'all think about this CI/CD for ML?
[00:21:34.440 --> 00:21:39.280]   Like, is there something there to like carry over
[00:21:39.280 --> 00:21:40.480]   in some ways?
[00:21:40.480 --> 00:21:42.560]   And like, by the way, I'm teaching a course on this,
[00:21:42.560 --> 00:21:45.240]   like CI/CD for ML, the teaser.
[00:21:45.240 --> 00:21:48.880]   So I don't have all the answers either.
[00:21:49.960 --> 00:21:51.760]   So I'm just curious, like what your
[00:21:51.760 --> 00:21:54.920]   kind of take on the CI/CD is.
[00:21:54.920 --> 00:21:57.640]   - So when we started the interview study paper,
[00:21:57.640 --> 00:22:01.040]   we named the project CI/CD in machine learning
[00:22:01.040 --> 00:22:04.160]   because our goal was to go talk to a bunch of people
[00:22:04.160 --> 00:22:07.000]   about their CI/CD practices.
[00:22:07.000 --> 00:22:09.160]   But really quickly in the first three interviews,
[00:22:09.160 --> 00:22:11.120]   we didn't include that in the study.
[00:22:11.120 --> 00:22:12.720]   But we did three interviews and we realized
[00:22:12.720 --> 00:22:15.720]   that already by saying CI/CD, we biased them so hard
[00:22:15.720 --> 00:22:18.720]   that there was like no point of recovery in the interview.
[00:22:18.720 --> 00:22:22.800]   So we just, that's why we dropped the CI/CD.
[00:22:22.800 --> 00:22:26.840]   The first big learning I had was that everybody
[00:22:26.840 --> 00:22:29.160]   is in their different stage in their ML journey,
[00:22:29.160 --> 00:22:32.160]   but they all may say that they do production ML.
[00:22:32.160 --> 00:22:34.200]   Some people will say they do production ML
[00:22:34.200 --> 00:22:36.920]   and their job is to generate dashboards for their CEO.
[00:22:36.920 --> 00:22:38.760]   Some people will say they do production ML
[00:22:38.760 --> 00:22:41.560]   and they work on Instagram recommendations.
[00:22:41.560 --> 00:22:44.280]   So like, at least for the interview study paper,
[00:22:44.280 --> 00:22:47.920]   we wanted to hear more about the latter and not the former,
[00:22:47.920 --> 00:22:50.360]   even though they're both valid production ML.
[00:22:50.360 --> 00:22:54.320]   So there's that.
[00:22:54.320 --> 00:22:58.080]   We found that it was easier to think about CI/CD
[00:22:58.080 --> 00:23:00.360]   when we were like talking to people
[00:23:00.360 --> 00:23:02.280]   who worked on specific applications.
[00:23:02.280 --> 00:23:05.200]   So for example, in autonomous vehicles,
[00:23:05.200 --> 00:23:09.080]   we talked to people from three, two,
[00:23:09.080 --> 00:23:11.560]   two or three different autonomous vehicle companies.
[00:23:11.560 --> 00:23:16.480]   And they have like very robust CI/CD practices
[00:23:16.480 --> 00:23:19.680]   around like data unit tests that they must pass
[00:23:19.680 --> 00:23:21.240]   before they deploy any new model.
[00:23:21.240 --> 00:23:22.520]   Like they must be passing,
[00:23:22.520 --> 00:23:25.320]   like be able to classify this pedestrian
[00:23:25.320 --> 00:23:27.440]   at this introduction or like this visibility
[00:23:27.440 --> 00:23:29.400]   or this lack of visibility or whatever.
[00:23:29.400 --> 00:23:31.880]   So they've got this like curated set
[00:23:31.880 --> 00:23:33.800]   of like adversarial examples,
[00:23:33.800 --> 00:23:36.480]   but they're consistently making sure passes.
[00:23:36.480 --> 00:23:39.080]   But their task, their domain needs that.
[00:23:39.080 --> 00:23:44.080]   So their CI/CD includes this as well as a validation set,
[00:23:44.360 --> 00:23:46.800]   as well as code review, whatever.
[00:23:46.800 --> 00:23:51.440]   There are other forms of CI/CD,
[00:23:51.440 --> 00:23:55.640]   like retail analytics companies,
[00:23:55.640 --> 00:23:58.960]   where they talked a lot about,
[00:23:58.960 --> 00:24:02.800]   when they have many, many pushes to the model.
[00:24:02.800 --> 00:24:06.720]   So in AV companies, you don't have that many new models
[00:24:06.720 --> 00:24:08.160]   or new pushes to the model.
[00:24:08.160 --> 00:24:09.840]   But when you have many pushes to the model,
[00:24:09.840 --> 00:24:13.600]   it's very easy to get diverging evaluation sets
[00:24:13.600 --> 00:24:15.320]   or evaluation notebooks.
[00:24:15.320 --> 00:24:18.360]   Like if somebody forks an evaluation notebook
[00:24:18.360 --> 00:24:20.240]   and can mix a little bit of modification to that
[00:24:20.240 --> 00:24:24.360]   and is using that to now deploy all the new versions,
[00:24:24.360 --> 00:24:28.360]   now you've got this like mess, right, to try to reconcile.
[00:24:28.360 --> 00:24:32.080]   So how do we make sure that every evaluation
[00:24:32.080 --> 00:24:34.200]   and deployment process is the exact same
[00:24:34.200 --> 00:24:35.880]   for every engineer at the company?
[00:24:35.880 --> 00:24:38.160]   I don't know if this answers your question,
[00:24:38.160 --> 00:24:40.520]   but it just, it really felt like,
[00:24:40.520 --> 00:24:42.560]   depending on the patterns,
[00:24:42.560 --> 00:24:44.280]   unlike the states of deployment,
[00:24:44.280 --> 00:24:46.800]   CI/CD just meant a different thing to everybody
[00:24:46.800 --> 00:24:49.360]   in a way that I have not seen in software.
[00:24:49.360 --> 00:24:51.920]   - Yeah, my experience is the word CI/CD
[00:24:51.920 --> 00:24:55.800]   is people don't know what the term means even.
[00:24:55.800 --> 00:24:59.680]   Like it's kind of like the word Coke for soda.
[00:24:59.680 --> 00:25:03.200]   It's like, you know, it's like just a kind of a sort of,
[00:25:03.200 --> 00:25:05.840]   oh, like it's like CircleCI or GitHub Actions,
[00:25:05.840 --> 00:25:08.480]   like that's CI/CD, but like what is,
[00:25:08.480 --> 00:25:10.880]   a lot of people don't even know what CI/CD stand,
[00:25:10.880 --> 00:25:13.880]   like the acronym even means.
[00:25:13.880 --> 00:25:17.000]   And like, it gets really confusing really fast.
[00:25:17.000 --> 00:25:20.360]   Daniel, do you want to continue your thought on CI/CD?
[00:25:20.360 --> 00:25:22.080]   - Yeah, I mean, really quickly,
[00:25:22.080 --> 00:25:25.000]   I think I basically agree with everything Treya said.
[00:25:25.000 --> 00:25:26.560]   I think there's maybe like two takeaways
[00:25:26.560 --> 00:25:29.240]   that are like really, really quick and valuable,
[00:25:29.240 --> 00:25:33.440]   which is one, automating as much of it as possible,
[00:25:33.440 --> 00:25:36.400]   because I think a lot of like ML pipelines die
[00:25:36.400 --> 00:25:37.960]   by exactly the example that you mentioned,
[00:25:37.960 --> 00:25:42.120]   which is I use a different eval set than yours.
[00:25:42.120 --> 00:25:44.120]   And so I ship a model that actually sucks
[00:25:44.120 --> 00:25:47.200]   or the way that we deploy model
[00:25:47.200 --> 00:25:48.440]   is a bunch of manual things.
[00:25:48.440 --> 00:25:49.880]   And if you deploy it slightly differently,
[00:25:49.880 --> 00:25:51.360]   something will break.
[00:25:51.360 --> 00:25:53.440]   So, automating everything,
[00:25:53.440 --> 00:25:56.400]   and then like thinking of model performance,
[00:25:56.400 --> 00:25:57.960]   that's kind of like successive checks.
[00:25:57.960 --> 00:26:02.160]   So you mentioned these specific examples,
[00:26:02.160 --> 00:26:04.640]   which I've heard called like golden sets sometimes
[00:26:04.640 --> 00:26:07.480]   of like data that your model must perform well on.
[00:26:07.480 --> 00:26:08.760]   Ahead of that, you can have a check,
[00:26:08.760 --> 00:26:10.560]   which is, does this model perform better
[00:26:10.560 --> 00:26:12.280]   than the old model?
[00:26:12.280 --> 00:26:13.680]   You know, if it does go to the next check.
[00:26:13.680 --> 00:26:15.760]   And then after that,
[00:26:15.760 --> 00:26:17.440]   usually having something like shadow mode,
[00:26:17.440 --> 00:26:18.760]   where you can like observe your model
[00:26:18.760 --> 00:26:19.680]   in the production environment
[00:26:19.680 --> 00:26:21.560]   without it actually doing anything.
[00:26:21.560 --> 00:26:23.880]   And you can verify that it's, for example,
[00:26:23.880 --> 00:26:26.400]   its distribution is not too far in terms of KL divergence,
[00:26:26.400 --> 00:26:28.240]   let's say from the prior model or something.
[00:26:28.240 --> 00:26:31.240]   And so you have this like assembly line of checks.
[00:26:31.240 --> 00:26:32.560]   And if at any point, you know,
[00:26:32.560 --> 00:26:33.800]   it kind of like fails on those,
[00:26:33.800 --> 00:26:35.800]   then you have an engineer looking at it,
[00:26:36.720 --> 00:26:38.360]   or you draw it back or something,
[00:26:38.360 --> 00:26:40.480]   something that I think is like a reasonable approach
[00:26:40.480 --> 00:26:41.320]   to CIC for ML.
[00:26:41.320 --> 00:26:43.320]   And then like each of the steps
[00:26:43.320 --> 00:26:44.360]   will probably be super different
[00:26:44.360 --> 00:26:45.520]   for each company as Shreya was saying,
[00:26:45.520 --> 00:26:47.600]   but at least like having that approach
[00:26:47.600 --> 00:26:49.600]   is pretty generalizable, I think.
[00:26:49.600 --> 00:26:52.160]   - Okay, so it looks like we're coming up on time,
[00:26:52.160 --> 00:26:54.960]   but we didn't get to talk about some things
[00:26:54.960 --> 00:26:56.760]   that kind of want to mention real quick.
[00:26:56.760 --> 00:26:59.400]   Just, so Shreya has an upcoming paper
[00:26:59.400 --> 00:27:02.040]   called "Moving Fast with Broken Data,"
[00:27:02.040 --> 00:27:05.600]   which is like basically encapsulates
[00:27:05.600 --> 00:27:07.840]   all of data science in my opinion,
[00:27:07.840 --> 00:27:08.680]   because that's what really--
[00:27:08.680 --> 00:27:09.840]   - Wow, that's generous.
[00:27:09.840 --> 00:27:11.200]   - That's what it feels like.
[00:27:11.200 --> 00:27:14.000]   So Shreya, do you want to say something quickly about that?
[00:27:14.000 --> 00:27:16.880]   Like that paper, what people should look out for?
[00:27:16.880 --> 00:27:21.600]   - Yeah, how do we automatically validate and monitor data
[00:27:21.600 --> 00:27:24.080]   without user specifying, you know, bounds
[00:27:24.080 --> 00:27:26.520]   and the metrics that they care about and everything,
[00:27:26.520 --> 00:27:29.760]   that making sure they're up to date as data changes.
[00:27:29.760 --> 00:27:31.320]   And how do we do this precisely
[00:27:31.320 --> 00:27:33.280]   with good recall of failures?
[00:27:34.160 --> 00:27:36.320]   - And there's a lot of things I wanted to get to
[00:27:36.320 --> 00:27:41.320]   in terms of, you know, Emmanuel work on LLMs,
[00:27:41.320 --> 00:27:43.080]   you know, things around like what, you know,
[00:27:43.080 --> 00:27:47.400]   how to fine tune stuff effectively,
[00:27:47.400 --> 00:27:50.520]   and, you know, like the accessibility is different models.
[00:27:50.520 --> 00:27:54.200]   Emmanuel, you do consulting as well around this.
[00:27:54.200 --> 00:27:56.520]   Where can people find you
[00:27:56.520 --> 00:27:58.760]   if they want to get in touch with you?
[00:27:58.760 --> 00:28:02.000]   - Yeah, I'd be happy to chat with anyone
[00:28:02.000 --> 00:28:04.200]   that's kind of trying to make LLMs work
[00:28:04.200 --> 00:28:06.760]   for a practical problem or trying to kind of find their way
[00:28:06.760 --> 00:28:10.280]   through fine tuning these models for useful tasks.
[00:28:10.280 --> 00:28:13.520]   So even if you just want to grab coffee and chat,
[00:28:13.520 --> 00:28:15.760]   happy to talk about that stuff, I think it's fascinating.
[00:28:15.760 --> 00:28:18.800]   Two ways to reach me, if Twitter still exists
[00:28:18.800 --> 00:28:22.320]   when you're listening to this, @MLPowered on Twitter,
[00:28:22.320 --> 00:28:26.240]   and if not, MLPowered.com is my website
[00:28:26.240 --> 00:28:30.120]   and there's like a email you can use to reach out there.
[00:28:30.120 --> 00:28:32.160]   - All right, great, and Shreya, where can people find you?
[00:28:32.160 --> 00:28:34.240]   What's the best way to sort of engage with you
[00:28:34.240 --> 00:28:36.000]   on your research or anything else?
[00:28:36.000 --> 00:28:39.880]   - Email me, shreyashankar@berkeley.edu.
[00:28:39.880 --> 00:28:44.280]   And there's like a 50% chance that I reply right now.
[00:28:44.280 --> 00:28:45.200]   I'm really sorry.
[00:28:45.200 --> 00:28:49.120]   - That's pretty high. - Hoping to increase this number.
[00:28:49.120 --> 00:28:50.360]   - That's not bad. - Maybe it's generous,
[00:28:50.360 --> 00:28:53.280]   maybe 30%, but I'm really trying, I promise.
[00:28:53.280 --> 00:28:55.860]   (upbeat music)
[00:28:55.860 --> 00:28:58.440]   (upbeat music)
[00:28:58.440 --> 00:29:01.940]   Produced by the U.S. Embassy in the Philippines

