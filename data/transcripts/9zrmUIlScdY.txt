
[00:00:00.000 --> 00:00:02.580]   I am your host, Charles Fry.
[00:00:02.580 --> 00:00:07.740]   We'll be talking about how you can use the Weights & Biases platform to
[00:00:07.740 --> 00:00:12.160]   organize hyperparameter sweeps in the PyTorch library.
[00:00:12.160 --> 00:00:14.880]   So Weights & Biases, if you look at the screen that I'm sharing,
[00:00:14.880 --> 00:00:17.080]   you'll see a list of modular tools.
[00:00:17.080 --> 00:00:19.560]   We've got experiment tracking and dataset versioning.
[00:00:19.560 --> 00:00:22.520]   We've got collaboration and reproducibility.
[00:00:22.520 --> 00:00:25.880]   And then you've, we've got what we're going to be talking about today.
[00:00:25.880 --> 00:00:28.960]   We've got our tool for model optimization, which is sweet.
[00:00:29.000 --> 00:00:33.480]   The webpage that I'm sharing here is our GitHub repo for our examples.
[00:00:33.480 --> 00:00:37.540]   This repository has just a whole ton of example scripts, example notebooks,
[00:00:37.540 --> 00:00:41.760]   and example co-labs that demonstrate how to use our tools.
[00:00:41.760 --> 00:00:45.520]   So to run this demo this morning, I'm actually just going to go in
[00:00:45.520 --> 00:00:48.600]   this page here of all of our examples.
[00:00:48.600 --> 00:00:52.040]   We've got examples for specific frameworks, specific tools.
[00:00:52.040 --> 00:00:54.840]   I'm going to go to this one for optimizing hyperparameters
[00:00:54.840 --> 00:00:56.640]   in sweeps and open it up.
[00:00:56.640 --> 00:00:58.080]   So you can do the same thing there.
[00:00:58.280 --> 00:01:01.800]   I guess first let's clarify what do I mean by hyperparameter sweeps.
[00:01:01.800 --> 00:01:05.200]   In general, when we have a machine learning model, especially deep
[00:01:05.200 --> 00:01:09.640]   learning models, there are a lot of knobs to twiddle that we aren't
[00:01:09.640 --> 00:01:11.760]   going to optimize programmatically.
[00:01:11.760 --> 00:01:15.520]   We aren't going to use gradient descent to determine the values of those knobs.
[00:01:15.520 --> 00:01:18.200]   The things that we do use gradient descent to determine the values
[00:01:18.200 --> 00:01:22.360]   of would be the literal weights and biases, say of our neural network model.
[00:01:22.360 --> 00:01:25.800]   Things we don't tend to use gradient descent to set are things like our
[00:01:25.800 --> 00:01:29.280]   learning rate or the amount of weight decay, the amount of dropouts.
[00:01:29.280 --> 00:01:32.200]   Some of these are just prohibitively expensive to train.
[00:01:32.200 --> 00:01:34.400]   Others are fundamentally discrete.
[00:01:34.400 --> 00:01:37.520]   And so not something that we can train with gradient based methods.
[00:01:37.520 --> 00:01:41.640]   The alternative to setting them programmatically is what one is
[00:01:41.640 --> 00:01:43.360]   to set them directly ourselves.
[00:01:43.360 --> 00:01:46.360]   So to just say, Oh, a good value for all of these parameters.
[00:01:46.360 --> 00:01:49.280]   I guess you can see a list of parameters on the bottom right side of the screen,
[00:01:49.280 --> 00:01:51.800]   like num layers, batch size, learning rate.
[00:01:51.800 --> 00:01:53.520]   I just know good values for these parameters.
[00:01:53.520 --> 00:01:54.200]   I can set them.
[00:01:54.280 --> 00:01:57.560]   But when you're working on a new problem, it's often unclear what the right
[00:01:57.560 --> 00:01:59.280]   values for those parameters might be.
[00:01:59.280 --> 00:02:04.400]   And so the alternative to setting them programmatically or knowing what they
[00:02:04.400 --> 00:02:08.080]   should be a priori is to figure it out yourself.
[00:02:08.080 --> 00:02:11.160]   And the only way to figure it out that people have found is to try a bunch of
[00:02:11.160 --> 00:02:14.520]   them and to sweep through a bunch of the values.
[00:02:14.520 --> 00:02:16.600]   So that is what a hyper parameter sweep is.
[00:02:16.600 --> 00:02:20.720]   Each of the dots on this chart right here represents a single run on the
[00:02:20.720 --> 00:02:25.880]   Y axis is how well that run did higher being worse, and then over time, we try
[00:02:25.880 --> 00:02:28.760]   a whole bunch of different values of the parameters and we can see that the best
[00:02:28.760 --> 00:02:32.040]   value that we've seen is getting better over time.
[00:02:32.040 --> 00:02:36.120]   So what I'm going to show you how to do today is how to set up and execute a
[00:02:36.120 --> 00:02:40.040]   hyper parameter sweep in weights and biases and log it to our web platform
[00:02:40.040 --> 00:02:44.320]   and end up with super cool, really useful charts like these that not only are
[00:02:44.320 --> 00:02:48.360]   these just the static charts, these images here are already pretty useful
[00:02:48.400 --> 00:02:51.680]   relative to what I at least can put together with MatPotLib in just a few
[00:02:51.680 --> 00:02:55.640]   minutes, but they're also interactive and do things that are extremely
[00:02:55.640 --> 00:02:57.440]   difficult to do in raw Python.
[00:02:57.440 --> 00:03:00.320]   And we'll do all of this for you for academic and personal projects.
[00:03:00.320 --> 00:03:05.240]   The product is free and subject only to reasonable limits, like no uploading
[00:03:05.240 --> 00:03:09.840]   10 terabytes of data that actually represents say the Lord of the Rings
[00:03:09.840 --> 00:03:12.000]   trilogy in 4k that you can't get away with.
[00:03:12.000 --> 00:03:12.480]   All right.
[00:03:12.480 --> 00:03:16.680]   So to jump in, how does a hyper parameter sweep weights and biases work?
[00:03:16.880 --> 00:03:19.560]   There's basically three steps define the sweep.
[00:03:19.560 --> 00:03:22.040]   What's the metric that we're trying to get down?
[00:03:22.040 --> 00:03:26.280]   What's the program that we're going to execute in order to run our neural
[00:03:26.280 --> 00:03:28.800]   network training or our machine learning training, what are all
[00:03:28.800 --> 00:03:30.360]   these parameters that we're using?
[00:03:30.360 --> 00:03:33.240]   So that definition step, that's the sort of biggest one.
[00:03:33.240 --> 00:03:36.000]   And then the steps two and three are then just a single line of code.
[00:03:36.000 --> 00:03:37.040]   We initialize a sweep.
[00:03:37.040 --> 00:03:40.240]   We say, Hey, I'm ready to run a sweep that has this definition.
[00:03:40.240 --> 00:03:42.240]   And then we start running agents.
[00:03:42.240 --> 00:03:44.880]   We say, all right, now let's start doing this sweep.
[00:03:44.880 --> 00:03:45.360]   All right.
[00:03:45.360 --> 00:03:46.680]   I want this computer to run a sweep.
[00:03:46.680 --> 00:03:49.520]   I want this other computer to also join this same sweep.
[00:03:49.520 --> 00:03:51.360]   And then you're ready to go.
[00:03:51.360 --> 00:03:54.680]   And you just need to sit and wait for those CPUs and GPUs to churn those
[00:03:54.680 --> 00:03:56.720]   floats and get your final values.
[00:03:56.720 --> 00:03:58.600]   We will begin at the beginning.
[00:03:58.600 --> 00:04:01.000]   This is starting from essentially scratch.
[00:04:01.000 --> 00:04:04.960]   You'll need to, once you've got your Python environment, your deep learning
[00:04:04.960 --> 00:04:11.280]   environment set up on your computer, you got to get the WNB library installed.
[00:04:11.280 --> 00:04:12.960]   It looks like I already installed this.
[00:04:12.960 --> 00:04:17.720]   You might see a little pip install going on here in your version of this cell.
[00:04:17.720 --> 00:04:20.480]   And then at the bottom there's WNB.login.
[00:04:20.480 --> 00:04:24.400]   We know who you are and so that we can organize your projects and make sure
[00:04:24.400 --> 00:04:26.520]   no one else logs things to your projects.
[00:04:26.520 --> 00:04:29.480]   So if you don't have a Waits and Biases account, you'll get a
[00:04:29.480 --> 00:04:32.200]   link to create an account there.
[00:04:32.200 --> 00:04:35.800]   And so just to give us your email password and under a minute later,
[00:04:35.800 --> 00:04:37.400]   you'll be logged in and ready to go.
[00:04:37.400 --> 00:04:42.000]   So moving on, let's talk the three steps for defining the sweep.
[00:04:42.080 --> 00:04:46.200]   Fundamentally, a sweep is a combination of a strategy for trying out a bunch
[00:04:46.200 --> 00:04:49.800]   of hyper parameter values and code that evaluates them, those are the sort
[00:04:49.800 --> 00:04:51.720]   of two pieces that make up a sweep.
[00:04:51.720 --> 00:04:55.680]   And that strategy could be as simple as trying every option, or it could be
[00:04:55.680 --> 00:04:59.360]   something really complicated and optimal, like combining Bayesian
[00:04:59.360 --> 00:05:01.240]   approaches with early stopping.
[00:05:01.240 --> 00:05:04.080]   Whatever that strategy is, you just need to define it in
[00:05:04.080 --> 00:05:05.560]   the form of a configuration.
[00:05:05.560 --> 00:05:08.400]   And this is the most complex part of the sweep.
[00:05:08.400 --> 00:05:11.520]   It's still fairly simple relative to writing all this stuff your own.
[00:05:11.640 --> 00:05:14.360]   We're going to go through that in great detail to make sure it's
[00:05:14.360 --> 00:05:16.000]   clear how this configuration works.
[00:05:16.000 --> 00:05:19.440]   So when you're setting up sweep in a notebook like this one, then that
[00:05:19.440 --> 00:05:21.640]   config objects is a nested dictionary.
[00:05:21.640 --> 00:05:24.680]   But when you're running a sweep via the command line, the config
[00:05:24.680 --> 00:05:26.240]   object will be a YAML file.
[00:05:26.240 --> 00:05:30.320]   The core idea though, is that it's a collection, a nested data structure.
[00:05:30.320 --> 00:05:34.760]   The keys are strings and they say, okay, this is the thing whose value I want to
[00:05:34.760 --> 00:05:38.600]   set, and then the values in that dictionary then set what are each
[00:05:38.600 --> 00:05:39.680]   of these configuration settings.
[00:05:39.680 --> 00:05:42.400]   This is a pretty standard way of structuring a configuration.
[00:05:42.400 --> 00:05:45.560]   A lot of people who are writing code like this for themselves use this sort of
[00:05:45.560 --> 00:05:46.920]   nested dictionary structure like this.
[00:05:46.920 --> 00:05:51.000]   At least I did that when I was setting up the experiments that were part of my thesis.
[00:05:51.000 --> 00:05:53.440]   If I had done them as a weights bias sweep, I probably would have saved
[00:05:53.440 --> 00:05:55.680]   myself like two, three weeks of engineering time of building
[00:05:55.680 --> 00:05:56.880]   that sort of stuff for myself.
[00:05:56.880 --> 00:05:58.800]   So what goes in that nested dictionary?
[00:05:58.800 --> 00:06:01.760]   The first thing you need to do is you need to pick the method of your sweep,
[00:06:01.760 --> 00:06:04.400]   the strategy for choosing new parameter values.
[00:06:04.400 --> 00:06:09.320]   We have three search methods, grid search, random search, and Bayesian search.
[00:06:09.360 --> 00:06:13.200]   Grid search lays down a grid over the set of all possible hyper parameters
[00:06:13.200 --> 00:06:14.960]   and checks out all those values.
[00:06:14.960 --> 00:06:17.080]   Literally every possible value.
[00:06:17.080 --> 00:06:19.960]   It's very effective, but it can be computationally very costly.
[00:06:19.960 --> 00:06:24.040]   The other sort of dumb and easy strategy for sweeps is random search.
[00:06:24.040 --> 00:06:27.400]   Just pick a random value for every hyper parameter.
[00:06:27.400 --> 00:06:30.600]   And though that sounds very dumb, it's not how we pick the
[00:06:30.600 --> 00:06:31.960]   weights for a neural network.
[00:06:31.960 --> 00:06:33.280]   We do something much smarter.
[00:06:33.280 --> 00:06:36.240]   Random search is actually extremely effective and it scales
[00:06:36.240 --> 00:06:37.560]   very well to large problems.
[00:06:37.720 --> 00:06:41.280]   The other alternative and one that a lot of people like is Bayesian search, which
[00:06:41.280 --> 00:06:43.360]   essentially tries to create a probabilistic model.
[00:06:43.360 --> 00:06:43.680]   Okay.
[00:06:43.680 --> 00:06:47.360]   If I, I've tried hyper parameter values over here and they were all really bad.
[00:06:47.360 --> 00:06:50.120]   So I'm actually going to try my hyper parameter values over there.
[00:06:50.120 --> 00:06:52.800]   Those are probably going to be better than the ones over here.
[00:06:52.800 --> 00:06:56.400]   So intuitively it seems like this should work really well, but our intuition
[00:06:56.400 --> 00:06:58.680]   works best in low dimensional spaces.
[00:06:58.680 --> 00:07:01.840]   When you have a ton of hyper parameters, Bayesian search scales poorly.
[00:07:01.840 --> 00:07:04.720]   So you have to be pretty careful about using Bayesian search.
[00:07:04.800 --> 00:07:09.160]   Often you'll end up expending a bunch of effort and not getting anything better
[00:07:09.160 --> 00:07:11.600]   than you would have gotten by just using a random search.
[00:07:11.600 --> 00:07:15.080]   So we'll stick with random for this one, though you're welcome to try and
[00:07:15.080 --> 00:07:18.320]   adjust this code to use the Bayesian search instead, if you're interested in that.
[00:07:18.320 --> 00:07:22.240]   For those Bayesian sweeps, you need to tell us what the metric is
[00:07:22.240 --> 00:07:24.160]   that you are trying to improve.
[00:07:24.160 --> 00:07:28.200]   And you need to tell us whether your goal is to make this metric low or metric high.
[00:07:28.200 --> 00:07:30.240]   Loss is a metric that we want to minimize.
[00:07:30.240 --> 00:07:33.440]   Accuracy would be a metric that we'd have the goal of maximizing.
[00:07:33.480 --> 00:07:36.560]   You only need this for Bayesian sweeps, but I like to include it in all of my
[00:07:36.560 --> 00:07:39.520]   sweeps because it's just good documentation.
[00:07:39.520 --> 00:07:42.600]   It says, okay, what is the metric for this experiment?
[00:07:42.600 --> 00:07:44.640]   And what do I want out of that?
[00:07:44.640 --> 00:07:47.960]   Just in case six months later, you come back to some variable name like
[00:07:47.960 --> 00:07:50.760]   Valgy batch, and you don't really know.
[00:07:50.760 --> 00:07:54.160]   Whether that was supposed to be high or low in six months or a year down the line.
[00:07:54.160 --> 00:07:58.240]   A question in the Q and a, is there a rule of thumb to use one hyper
[00:07:58.240 --> 00:08:00.520]   parameter optimization over the other?
[00:08:00.640 --> 00:08:05.720]   I would say the first thing I almost always start with random search and
[00:08:05.720 --> 00:08:08.840]   then random search, we usually reveal that there are some values
[00:08:08.840 --> 00:08:10.080]   that are just not very good.
[00:08:10.080 --> 00:08:14.040]   And I'll use that to shrink down the space of possible hyper parameters.
[00:08:14.040 --> 00:08:17.320]   I'm looking over until it's small enough that I can do a grid search that works
[00:08:17.320 --> 00:08:19.880]   well, where I can just check every possibility that's left.
[00:08:19.880 --> 00:08:23.600]   I can check all of them to find the best one, but I think most people tend to
[00:08:23.600 --> 00:08:27.200]   pick either random search or Bayesian search and learn some sort of tricks
[00:08:27.200 --> 00:08:30.240]   of the trade to make one of those really good and then specialize in it.
[00:08:30.480 --> 00:08:33.920]   I think that's something that's generally true about architectural
[00:08:33.920 --> 00:08:34.960]   choices in deep learning.
[00:08:34.960 --> 00:08:38.600]   The most important thing is that, you know, how to work with the tools that
[00:08:38.600 --> 00:08:41.760]   you're using, not that you're using the absolute best tool, whether it's
[00:08:41.760 --> 00:08:45.600]   nonlinearities, data augmentation strategies, whatever, just pick some
[00:08:45.600 --> 00:08:47.600]   things to learn how to use really well.
[00:08:47.600 --> 00:08:49.120]   And you'll be able to get good performance.
[00:08:49.120 --> 00:08:51.080]   Doesn't matter which things you pick.
[00:08:51.080 --> 00:08:52.240]   A great question.
[00:08:52.240 --> 00:08:55.680]   Now we're at the meat of the sweep definition, which is to
[00:08:55.680 --> 00:08:57.520]   name those hyper parameters.
[00:08:57.600 --> 00:09:00.760]   What are those parameters and what are the legal values for them?
[00:09:00.760 --> 00:09:05.400]   What are the possible choices that I can make to set a value of the fully
[00:09:05.400 --> 00:09:08.640]   connected layer size, the size of the hidden layer of the network or the
[00:09:08.640 --> 00:09:10.280]   amount of dropout that I'm going to apply?
[00:09:10.280 --> 00:09:13.640]   The simplest way to do that is just list all the legal values.
[00:09:13.640 --> 00:09:16.040]   And that makes a ton of sense with some of our hyper parameters.
[00:09:16.040 --> 00:09:19.360]   Like maybe I'm comparing the possible choices of optimizer.
[00:09:19.360 --> 00:09:21.760]   And there's really only a finite number of options.
[00:09:21.760 --> 00:09:23.680]   And most people just pick between two of them.
[00:09:23.680 --> 00:09:27.240]   So for even for a really broad hyper parameter optimization, I would really
[00:09:27.240 --> 00:09:31.160]   only need two values for choice of optimizer, Adam and SGD.
[00:09:31.160 --> 00:09:34.960]   And similarly, there's a lot of things where even though in principle, there's
[00:09:34.960 --> 00:09:38.520]   an infinite number of choices, like I can have a hidden layer that has
[00:09:38.520 --> 00:09:40.960]   between one and infinity nodes in it.
[00:09:40.960 --> 00:09:46.080]   There's usually a pretty small range in which you want to look and checking
[00:09:46.080 --> 00:09:50.520]   every value, 128 nodes in the hidden layer, 129 nodes in the hidden layer,
[00:09:50.520 --> 00:09:52.160]   130 nodes in the hidden layer.
[00:09:52.160 --> 00:09:53.240]   It's just too much.
[00:09:53.240 --> 00:09:54.800]   So you want to specify down.
[00:09:55.000 --> 00:09:57.440]   Maybe these are the values that are probably good.
[00:09:57.440 --> 00:09:59.120]   They're nicely spaced from each other.
[00:09:59.120 --> 00:10:00.560]   So you can explore a little bit.
[00:10:00.560 --> 00:10:04.520]   You don't need as much flexibility necessarily as every natural number
[00:10:04.520 --> 00:10:07.760]   could be a possible hidden layer size, even though that's in principle true.
[00:10:07.760 --> 00:10:10.280]   As I said, the sweep config is a nested dictionary.
[00:10:10.280 --> 00:10:15.080]   So the parameters go in as a dictionary under the key parameters
[00:10:15.080 --> 00:10:16.800]   in that overall sweep config.
[00:10:16.800 --> 00:10:19.760]   But there's also sometimes where we have hyper parameters that we do have
[00:10:19.760 --> 00:10:22.920]   the ability to set in our code, but that we don't want to change.
[00:10:23.080 --> 00:10:26.560]   So the example here is that I'm just going to run my training for one epoch
[00:10:26.560 --> 00:10:31.520]   to see how things do in their first epoch of training, and I don't necessarily want
[00:10:31.520 --> 00:10:34.520]   to try and optimize this hyper parameter, but I do want to set it.
[00:10:34.520 --> 00:10:38.080]   So just setting the, a single legal value for hyper parameter means that
[00:10:38.080 --> 00:10:39.680]   we just only ever try one value.
[00:10:39.680 --> 00:10:42.600]   And this is also good, you know, as little as possible, relying on things
[00:10:42.600 --> 00:10:46.880]   like defaults, magic numbers, things like that in our code and writing down, okay.
[00:10:46.880 --> 00:10:51.000]   This is the number of epochs that went into this code at this time.
[00:10:51.120 --> 00:10:53.400]   This is the number of epochs that were run and that's all
[00:10:53.400 --> 00:10:54.520]   logged to weights and biases.
[00:10:54.520 --> 00:10:59.120]   Much better for reproducibility than say changing the default value on your
[00:10:59.120 --> 00:11:03.120]   script, as I have done sometimes when I was crunched for a deadline and I really
[00:11:03.120 --> 00:11:04.720]   needed to change some experiments around.
[00:11:04.720 --> 00:11:07.080]   For grid search, actually, that's all you're ever going to need.
[00:11:07.080 --> 00:11:08.520]   And grid search is a great choice.
[00:11:08.520 --> 00:11:12.080]   It's actually one of the more popular choices for our users who use our sweep
[00:11:12.080 --> 00:11:16.320]   tool, but for a random search, all the values of a parameter are equally
[00:11:16.320 --> 00:11:18.200]   likely to be chosen on a given run.
[00:11:18.200 --> 00:11:20.120]   And sometimes, you know, that just won't do.
[00:11:20.240 --> 00:11:23.920]   Maybe we want a normally distributed values for a parameter.
[00:11:23.920 --> 00:11:27.680]   We actually do want to try out every value between zero and 0.1.
[00:11:27.680 --> 00:11:30.520]   Any value could be a good choice.
[00:11:30.520 --> 00:11:34.160]   And we don't want to a priori say, Oh, it has to be one of these specific values.
[00:11:34.160 --> 00:11:35.080]   Ooh, a question.
[00:11:35.080 --> 00:11:37.480]   Are there cases when grid is not an ideal choice?
[00:11:37.480 --> 00:11:38.120]   Yes.
[00:11:38.120 --> 00:11:39.360]   Grid is not a good choice.
[00:11:39.360 --> 00:11:42.720]   If you've got a lot of hyper parameters with a lot of potential values.
[00:11:42.720 --> 00:11:45.600]   If I've got 10 parameters that I'm optimizing over and they've
[00:11:45.600 --> 00:11:47.080]   each got two possible choices.
[00:11:47.280 --> 00:11:50.600]   That's two to the 10 or 1024 options.
[00:11:50.600 --> 00:11:54.600]   A grid search will take 1024 steps to try out all of those.
[00:11:54.600 --> 00:11:56.680]   We'll do them in a very specific order.
[00:11:56.680 --> 00:12:00.720]   There's a decent chance that you might not see any good values of
[00:12:00.720 --> 00:12:02.120]   the parameters until the very end.
[00:12:02.120 --> 00:12:06.200]   If you do a random search on those exact same values of the parameters,
[00:12:06.200 --> 00:12:10.440]   you're less likely to spend a whole bunch of time only working on
[00:12:10.440 --> 00:12:12.480]   bad values of the hyper parameters.
[00:12:12.480 --> 00:12:15.040]   So random can be a better choice than grid.
[00:12:15.280 --> 00:12:17.680]   If you have a lot of values, it converges a little bit faster.
[00:12:17.680 --> 00:12:18.680]   A question.
[00:12:18.680 --> 00:12:21.640]   Do you have guidelines on picking hyper parameter ranges and
[00:12:21.640 --> 00:12:23.480]   distributions for random search?
[00:12:23.480 --> 00:12:28.160]   So I'll answer this one, but what I want to first do is just on the front of how
[00:12:28.160 --> 00:12:32.440]   do you set up these distributions for sweeps when you're doing these random
[00:12:32.440 --> 00:12:35.640]   searches, but you need to be able to do is you need to specify the probability
[00:12:35.640 --> 00:12:40.320]   distribution for the values that are coming out rather than just saying
[00:12:40.320 --> 00:12:42.720]   it's uniform over these specific.
[00:12:42.960 --> 00:12:43.560]   Values.
[00:12:43.560 --> 00:12:47.360]   And the way that's done is again, we're going to nest another dictionary in
[00:12:47.360 --> 00:12:48.960]   there, it's dictionaries all the way down.
[00:12:48.960 --> 00:12:53.120]   And the way it works is the name of the parameter is your key.
[00:12:53.120 --> 00:12:55.800]   The value there is a dictionary that describes the
[00:12:55.800 --> 00:12:56.960]   distribution you want to use.
[00:12:56.960 --> 00:12:59.240]   If you check out our docs, you'll be able to see all the
[00:12:59.240 --> 00:13:00.800]   distributions that we make available.
[00:13:00.800 --> 00:13:03.520]   In the end uniform ends up being a really common choice.
[00:13:03.520 --> 00:13:07.040]   It's just now, instead of being uniform over a fixed set of values, it's over
[00:13:07.040 --> 00:13:11.400]   an infinite set of values, whether that's numbers between zero and 0.1.
[00:13:11.480 --> 00:13:16.000]   Or a normal distribution, which goes over any possible floating point number.
[00:13:16.000 --> 00:13:17.860]   We have a lot of sort of flexibility here.
[00:13:17.860 --> 00:13:20.360]   We've got log distributed things, which are very common
[00:13:20.360 --> 00:13:21.760]   in hyperparameter searches.
[00:13:21.760 --> 00:13:25.620]   I want to try every order of magnitude and I want to try those uniformly, but
[00:13:25.620 --> 00:13:30.560]   I don't necessarily want to try uniformly across every value between one and 1000.
[00:13:30.560 --> 00:13:34.520]   I want to try something like one, 10, a hundred, a thousand, something like that.
[00:13:34.520 --> 00:13:36.100]   So we've got these log versions.
[00:13:36.100 --> 00:13:40.160]   We've got quantized versions that you can get say integers out of these
[00:13:40.160 --> 00:13:44.120]   distributions, details all on our docs there.
[00:13:44.120 --> 00:13:47.920]   So to return to, do you have guidelines on picking hyperparameter ranges
[00:13:47.920 --> 00:13:49.440]   and distributions for random search?
[00:13:49.440 --> 00:13:54.200]   What I would say is the biggest mistake I see people doing is they use uniform
[00:13:54.200 --> 00:13:57.960]   distributions instead of log uniform distributions, log uniform distributions
[00:13:57.960 --> 00:14:01.960]   like this one have evenly distributed logarithms, which is a mathy way of
[00:14:01.960 --> 00:14:05.840]   saying that they try out orders of magnitude with equal probability.
[00:14:05.840 --> 00:14:07.960]   That is very commonly what you want to do.
[00:14:07.960 --> 00:14:10.240]   It's definitely the case for batch size and learning rate.
[00:14:10.240 --> 00:14:13.720]   I think that's probably also the case for things like hidden layer size.
[00:14:13.720 --> 00:14:17.100]   So I would, if you're not using log transform distributions, I would
[00:14:17.100 --> 00:14:19.080]   use log transformed distributions.
[00:14:19.080 --> 00:14:22.760]   And then the other tip is you're probably going to run more
[00:14:22.760 --> 00:14:24.040]   than one hyperparameter sweep.
[00:14:24.040 --> 00:14:27.080]   So start off broad and then narrow down only once you've
[00:14:27.080 --> 00:14:28.880]   started to try out values.
[00:14:28.880 --> 00:14:29.940]   Great question.
[00:14:29.940 --> 00:14:31.200]   Okay.
[00:14:31.200 --> 00:14:35.880]   We've added in these values that are random over these broad ranges.
[00:14:36.280 --> 00:14:39.520]   And then when we're finished, we've got our nice nested dictionary
[00:14:39.520 --> 00:14:41.000]   here, that's our sweep config.
[00:14:41.000 --> 00:14:45.040]   And you can see if you've worked with YAML files, you'll see the similarity
[00:14:45.040 --> 00:14:49.160]   is maybe you can see it here that we've got keys and we've got values.
[00:14:49.160 --> 00:14:50.560]   Some of those are dictionaries.
[00:14:50.560 --> 00:14:52.000]   Some of those are things like lists.
[00:14:52.000 --> 00:14:54.640]   We also also have some things like early termination, but I wanted
[00:14:54.640 --> 00:14:55.840]   to keep this relatively simple.
[00:14:55.840 --> 00:15:00.420]   If you want to know more, check out our docs or check out this big
[00:15:00.420 --> 00:15:04.040]   collection of examples, these examples in the YAML format, we've got a whole
[00:15:04.040 --> 00:15:05.960]   bunch of them at that link there.
[00:15:05.960 --> 00:15:06.260]   All right.
[00:15:06.260 --> 00:15:08.760]   Now it is time to initialize the sweep.
[00:15:08.760 --> 00:15:11.000]   So what we did in that first step was we defined our sweep.
[00:15:11.000 --> 00:15:13.080]   We said, okay, here's how I want to try things.
[00:15:13.080 --> 00:15:15.080]   And here are the things that I want to try.
[00:15:15.080 --> 00:15:17.000]   Now it's time to get started.
[00:15:17.000 --> 00:15:22.600]   So the basic way that this is set up is that there is a sweep controller
[00:15:22.600 --> 00:15:24.440]   that runs on our machines.
[00:15:24.440 --> 00:15:28.360]   And then on your machines, the work is executed that actually trains
[00:15:28.360 --> 00:15:29.880]   your machine learning algorithms.
[00:15:29.880 --> 00:15:33.440]   This is a division of labor between deciding what to do next
[00:15:33.440 --> 00:15:35.000]   and actually doing it right.
[00:15:35.000 --> 00:15:39.080]   This is we're splitting those two parts of the sweep from each other.
[00:15:39.080 --> 00:15:42.140]   You can set this up differently if you want to locally control your sweeps.
[00:15:42.140 --> 00:15:45.800]   That is an option that we provide, but the sort of the typical sweep is we do
[00:15:45.800 --> 00:15:48.880]   that sort of central decision-making according to your configuration
[00:15:48.880 --> 00:15:49.960]   about what to try next.
[00:15:49.960 --> 00:15:54.520]   And then we just are ready to send that to any machine that asks, okay,
[00:15:54.520 --> 00:15:56.540]   what's the next step in this sweep?
[00:15:56.540 --> 00:16:00.000]   And this makes it really easy to scale up sweeps by just getting a whole
[00:16:00.000 --> 00:16:03.360]   bunch of machines and on those separate machines, maybe those are things
[00:16:03.360 --> 00:16:06.200]   you're spinning up in the cloud with something like Google Kubernetes
[00:16:06.200 --> 00:16:08.240]   engine or AWS, something like that.
[00:16:08.240 --> 00:16:12.000]   Or maybe it's your laptop and your desktop and your friend's laptop.
[00:16:12.000 --> 00:16:15.640]   Uh, your roommates out of town for the weekend and you can run jobs on their
[00:16:15.640 --> 00:16:19.440]   machine so long as it's got the code, then it can participate in the sweep.
[00:16:19.440 --> 00:16:21.840]   And we handle all this communication stuff for you.
[00:16:21.840 --> 00:16:25.680]   So that's one of the trickiest parts about setting up a really parallel
[00:16:25.680 --> 00:16:27.440]   hyper parameter optimization yourself.
[00:16:27.440 --> 00:16:29.680]   We're taking that, taking on that burden for you.
[00:16:29.740 --> 00:16:33.260]   And so the only thing that you need to do is to run this WNB dot
[00:16:33.260 --> 00:16:35.300]   sweep command with that config.
[00:16:35.300 --> 00:16:38.980]   There's some things that you can set here, configuring this sweep,
[00:16:38.980 --> 00:16:41.180]   giving us a username or an entity.
[00:16:41.180 --> 00:16:44.700]   So maybe you're working on a team, setting a project name, but fundamentally
[00:16:44.700 --> 00:16:47.420]   you just need to pass this thing, this sweep config, and it will
[00:16:47.420 --> 00:16:50.040]   start up a sweep on our website.
[00:16:50.040 --> 00:16:53.080]   So let me actually click this sweep URL.
[00:16:53.080 --> 00:16:57.340]   So this here is now the webpage for the sweep that we're about to run.
[00:16:57.340 --> 00:16:59.580]   It's got some information about this.
[00:16:59.580 --> 00:17:02.580]   Sort of the config is already showing up here.
[00:17:02.580 --> 00:17:04.980]   That config that we just built is now here.
[00:17:04.980 --> 00:17:08.980]   This guy is now sitting here waiting for somebody to ask, Hey, I
[00:17:08.980 --> 00:17:10.620]   want to run part of this sweep.
[00:17:10.620 --> 00:17:13.540]   What are the hyper parameter values I should use?
[00:17:13.540 --> 00:17:16.380]   That is step three, running the sweep agent.
[00:17:16.380 --> 00:17:19.640]   That is how you start querying that controller for
[00:17:19.640 --> 00:17:21.420]   hyper parameter values to use.
[00:17:21.420 --> 00:17:24.420]   In order to run a sweep agent, we have to define the actual training process.
[00:17:24.420 --> 00:17:26.620]   Up to this point, we've just been talking about the
[00:17:26.620 --> 00:17:28.300]   hyper parameter optimization.
[00:17:28.360 --> 00:17:30.860]   We haven't really talked about like what those hyper parameters
[00:17:30.860 --> 00:17:31.740]   actually get used for, right?
[00:17:31.740 --> 00:17:33.660]   They're, they're going to be used to train a neural network.
[00:17:33.660 --> 00:17:35.320]   We have to write the code for that.
[00:17:35.320 --> 00:17:37.420]   So this section here first does that.
[00:17:37.420 --> 00:17:39.000]   Sort of basic PyTorch pipeline.
[00:17:39.000 --> 00:17:41.700]   If you want to hear a little bit more about how weights and biases
[00:17:41.700 --> 00:17:43.220]   is instrumented in PyTorch.
[00:17:43.220 --> 00:17:45.220]   There's a Colab link there.
[00:17:45.220 --> 00:17:46.860]   It's just three, four lines.
[00:17:46.860 --> 00:17:51.060]   Just these three pieces of our library here, init, config, and log.
[00:17:51.060 --> 00:17:53.620]   But most of this, there's a lot of code there, but that's pretty
[00:17:53.620 --> 00:17:55.700]   much just boilerplate PyTorch.
[00:17:55.700 --> 00:17:58.000]   So now we're ready to start sweeping.
[00:17:58.100 --> 00:17:59.780]   The sweep controller is sitting there waiting.
[00:17:59.780 --> 00:18:02.780]   It's now time for somebody to talk to that sweep controller and say,
[00:18:02.780 --> 00:18:06.300]   Hey, give me a collection of hyper parameters and I will go and check
[00:18:06.300 --> 00:18:09.380]   how those did, and I'll tell you what the results were.
[00:18:09.380 --> 00:18:11.480]   And that's this wanb agent command.
[00:18:11.480 --> 00:18:13.900]   So actually I got a question that's relevant here.
[00:18:13.900 --> 00:18:16.360]   Is there a way to set the maximum number of runs of a sweep
[00:18:16.360 --> 00:18:17.260]   or do they run forever?
[00:18:17.260 --> 00:18:20.500]   So with a random sweep, there's no stopping criterion, right?
[00:18:20.500 --> 00:18:22.900]   With a grid sweep, there is a stopping criterion.
[00:18:22.900 --> 00:18:24.700]   Once you've tried everything you can stop.
[00:18:25.660 --> 00:18:28.220]   But with a random search, there's no way to stop.
[00:18:28.220 --> 00:18:31.820]   So you actually have to say, okay, this is a maximum number
[00:18:31.820 --> 00:18:33.140]   of times that I want to run.
[00:18:33.140 --> 00:18:35.660]   That is part of a wanb agent command.
[00:18:35.660 --> 00:18:38.500]   So you can say this agent will run this many times.
[00:18:38.500 --> 00:18:40.340]   This agent will run this many times.
[00:18:40.340 --> 00:18:41.980]   So right here, I've set it to five.
[00:18:41.980 --> 00:18:43.380]   This is a random search.
[00:18:43.380 --> 00:18:46.380]   It could have run infinitely, but we're going to run just five.
[00:18:46.380 --> 00:18:50.060]   Let me make sure all the cells before have been run.
[00:18:50.060 --> 00:18:52.260]   And then we can execute this guy.
[00:18:52.260 --> 00:18:53.380]   So this is starting to run.
[00:18:53.460 --> 00:18:57.700]   The agent got that config up there at the top of the screen, highlighted right
[00:18:57.700 --> 00:19:00.740]   there from the controller, from online.
[00:19:00.740 --> 00:19:02.340]   Agent says, all right, cool.
[00:19:02.340 --> 00:19:03.340]   Time to start training.
[00:19:03.340 --> 00:19:06.660]   So the training starts, we download the MNIST dataset because this is our
[00:19:06.660 --> 00:19:08.580]   first training run, and then we do our run.
[00:19:08.580 --> 00:19:12.020]   Now the results of the run comes out and these are quick runs.
[00:19:12.020 --> 00:19:13.080]   The training is already done.
[00:19:13.080 --> 00:19:14.180]   One epoch is already done.
[00:19:14.180 --> 00:19:18.300]   And we print some of the quick summary of that to the command line.
[00:19:18.300 --> 00:19:20.180]   The details of it are on our web app.
[00:19:20.180 --> 00:19:21.300]   We'll see that in just a second.
[00:19:21.340 --> 00:19:23.940]   And then it's time for another run to start.
[00:19:23.940 --> 00:19:26.940]   So then the agent says, Hey controller, you got another run for me?
[00:19:26.940 --> 00:19:28.980]   Controller says, yes, actually I do.
[00:19:28.980 --> 00:19:36.300]   Try these try batch size 163 dropout 0.5 FC layer size 512 learning rate 0.078.
[00:19:36.300 --> 00:19:36.580]   Yada.
[00:19:36.580 --> 00:19:40.060]   So it gets a new config from the controller and then runs again.
[00:19:40.060 --> 00:19:41.140]   It finishes.
[00:19:41.140 --> 00:19:44.180]   And this process repeats until we hit that count number.
[00:19:44.180 --> 00:19:46.420]   In this case, if it were a grid search, it would be until
[00:19:46.420 --> 00:19:47.740]   we'd tried all possibilities.
[00:19:48.020 --> 00:19:50.780]   In any case, this repeats until we hit a stopping point and the
[00:19:50.780 --> 00:19:54.220]   controller says, Hey agent, I'm actually, I don't have anything more for you.
[00:19:54.220 --> 00:19:54.820]   I'm done.
[00:19:54.820 --> 00:19:55.420]   That's it.
[00:19:55.420 --> 00:19:56.580]   And the agent stops.
[00:19:56.580 --> 00:19:59.020]   So it's now run five different.
[00:19:59.020 --> 00:20:02.540]   It's run five different hyper parameter settings, and that's all been logged.
[00:20:02.540 --> 00:20:04.580]   This is what the sweet page then looks like.
[00:20:04.580 --> 00:20:06.840]   Let's focus on these charts here.
[00:20:06.840 --> 00:20:08.260]   And let me zoom out a bit.
[00:20:08.260 --> 00:20:11.580]   We've got the loss over time for each executed run.
[00:20:11.580 --> 00:20:14.180]   What was the loss at the end of training?
[00:20:14.180 --> 00:20:16.220]   We've got this parameter important spot.
[00:20:16.220 --> 00:20:18.700]   So this, we train a random forest.
[00:20:18.700 --> 00:20:21.980]   And we do a quick linear fit to determine which of your
[00:20:21.980 --> 00:20:24.180]   parameters were most important.
[00:20:24.180 --> 00:20:26.580]   And then we've also got this nice parallel coordinates plot.
[00:20:26.580 --> 00:20:30.620]   So actually instead of going in greater detail about these, these plots are most
[00:20:30.620 --> 00:20:34.500]   useful once you've really got a nice long sweep going right when you've got lots
[00:20:34.500 --> 00:20:37.060]   and lots of runs that you can compare.
[00:20:37.060 --> 00:20:41.500]   So I'm going to pull a Martha Stewart move and say, I've got this sweep that I put
[00:20:41.500 --> 00:20:44.060]   in the oven a couple hours ago and pull that out.
[00:20:44.220 --> 00:20:48.300]   So this is the exact same code, but it's 100 runs instead of five.
[00:20:48.300 --> 00:20:50.980]   And it's got those same plots here, but now they've got a
[00:20:50.980 --> 00:20:52.660]   little bit more meat to them.
[00:20:52.660 --> 00:20:57.660]   Again, with each run, we log a loss and we keep track of that over time.
[00:20:57.660 --> 00:20:59.900]   So you can see how did this sweep go?
[00:20:59.900 --> 00:21:04.020]   One fun thing I've actually noticed a lot about sweeps is you can see that the,
[00:21:04.020 --> 00:21:07.900]   the returns are usually pretty small for hyper parameter optimization.
[00:21:07.900 --> 00:21:11.580]   That basically in your first couple of runs, you'll see one that's pretty good.
[00:21:11.780 --> 00:21:15.060]   And then it will only get better slowly after that point.
[00:21:15.060 --> 00:21:18.540]   So it's often not that important to try more than like 10 or
[00:21:18.540 --> 00:21:20.300]   20 runs rather than a hundred.
[00:21:20.300 --> 00:21:23.580]   You'll usually find that you've gotten something close to optimal.
[00:21:23.580 --> 00:21:26.520]   This is why I recommend random search that just empirically
[00:21:26.520 --> 00:21:28.300]   you see plots like this a lot.
[00:21:28.300 --> 00:21:32.500]   So then now our parameter importance and correlation plots here are, are
[00:21:32.500 --> 00:21:33.700]   maybe a little bit more informative.
[00:21:33.700 --> 00:21:36.820]   It says, okay, the learning rate is really important to final performance.
[00:21:36.820 --> 00:21:38.100]   Maybe that's not so surprising.
[00:21:38.100 --> 00:21:39.300]   We're only in a single epoch.
[00:21:39.340 --> 00:21:41.580]   The higher your learning rate, the more you'll probably be able
[00:21:41.580 --> 00:21:43.420]   to learn within that one epoch.
[00:21:43.420 --> 00:21:48.900]   It looks like the atom optimizer has a positive correlation
[00:21:48.900 --> 00:21:50.500]   with the value of the loss.
[00:21:50.500 --> 00:21:54.460]   So that means, oh, it looks like actually I think I am reading
[00:21:54.460 --> 00:21:55.200]   this the other way around.
[00:21:55.200 --> 00:21:58.700]   So the correlation is positive between the learning rate and the loss.
[00:21:58.700 --> 00:22:02.020]   And that means that when the learning rate goes up, the loss actually goes up.
[00:22:02.020 --> 00:22:04.500]   So that suggests maybe there's some instability in training.
[00:22:04.500 --> 00:22:09.020]   And it looks like if our optimizer is Adam, then the loss goes up.
[00:22:09.060 --> 00:22:12.540]   So you can see that it's nice to combine actually these parameter importance
[00:22:12.540 --> 00:22:14.820]   plots with these parallel coordinates plots.
[00:22:14.820 --> 00:22:18.140]   So let me walk you through what this parallel coordinates plot is.
[00:22:18.140 --> 00:22:20.620]   This guy says, all right, here are all of your runs.
[00:22:20.620 --> 00:22:22.140]   Each run is a single line here.
[00:22:22.140 --> 00:22:24.100]   You can see which run it was.
[00:22:24.100 --> 00:22:27.940]   It's Pius sweep and sweepy sweep and all the various randomly
[00:22:27.940 --> 00:22:29.780]   named runs in this sweep.
[00:22:29.780 --> 00:22:30.940]   Each one is a line.
[00:22:30.940 --> 00:22:33.820]   They're colored according to your metric.
[00:22:33.820 --> 00:22:35.380]   So here it was loss.
[00:22:35.420 --> 00:22:39.700]   And then they pass through on each of these vertical bars here, they pass
[00:22:39.700 --> 00:22:41.300]   through the value of the hyper parameter.
[00:22:41.300 --> 00:22:45.340]   So this is for all the ones that had a fully connected layer size of 512.
[00:22:45.340 --> 00:22:46.540]   They all pass through this point.
[00:22:46.540 --> 00:22:50.100]   The ones that had SGD is the optimizer all pass through this point.
[00:22:50.100 --> 00:22:53.380]   Then the learning rates, those were random and uniform.
[00:22:53.380 --> 00:22:54.940]   So they're all over the place here.
[00:22:54.940 --> 00:22:59.140]   So how you read this parallel coordinates plot is basically you look for patterns.
[00:22:59.140 --> 00:23:00.980]   Where do you see splotches of color?
[00:23:01.260 --> 00:23:06.060]   And I see a splotch of bright yellow color indicating a high loss first
[00:23:06.060 --> 00:23:11.780]   here with the fully connected layer size being up at 512, the bigger networks.
[00:23:11.780 --> 00:23:13.420]   We're actually struggling a little bit more.
[00:23:13.420 --> 00:23:15.740]   It seems like we might have some optimization issues.
[00:23:15.740 --> 00:23:20.740]   And then you can also see, it looks like they are primarily
[00:23:20.740 --> 00:23:22.700]   ones that had the Adam optimizer.
[00:23:22.700 --> 00:23:27.260]   So you can see these, all these brightly colored, these purple and light purple
[00:23:27.260 --> 00:23:30.820]   and yellow lines are all passing through the Adam point here.
[00:23:30.940 --> 00:23:34.220]   We can also, these charts are interactive, so we can do things like filter down
[00:23:34.220 --> 00:23:38.100]   to just these couple of runs, and we can see them really clearly on top of the
[00:23:38.100 --> 00:23:41.500]   background of all the other runs in just transparent gray, and that can be
[00:23:41.500 --> 00:23:42.980]   useful for picking out these patterns.
[00:23:42.980 --> 00:23:45.660]   You'll notice that it also changes all these other plots here.
[00:23:45.660 --> 00:23:49.740]   We can now see the losses of those, their loss on every batch.
[00:23:49.740 --> 00:23:53.300]   So you can see that there was this big zoom in the loss, like jumped really
[00:23:53.300 --> 00:23:55.300]   high and then they never recovered.
[00:23:55.300 --> 00:23:59.540]   The interactive nature of these charts is really great for debugging and also
[00:23:59.540 --> 00:24:01.620]   learning from your experiments really quickly.
[00:24:01.620 --> 00:24:06.620]   So I love using these charts in the workspace and in the reports to explore
[00:24:06.620 --> 00:24:10.540]   the results from hyperparameter sweeps, especially a hyperparameter sweep like
[00:24:10.540 --> 00:24:12.900]   this produces a really rich dataset.
[00:24:12.900 --> 00:24:17.140]   And without an equally rich visualization scheme, you're going to really struggle
[00:24:17.140 --> 00:24:20.660]   to get everything out of that, that you could, there's still plenty of other
[00:24:20.660 --> 00:24:22.060]   stuff to talk about with sweeps.
[00:24:22.060 --> 00:24:23.820]   We talked about all these things.
[00:24:23.820 --> 00:24:28.380]   There's some links here for a couple of different examples that you can try.
[00:24:28.380 --> 00:24:29.580]   If you want to play around with them.
[00:24:29.580 --> 00:24:33.140]   It also has examples that can let you try out some of the more advanced
[00:24:33.140 --> 00:24:37.620]   features like Bayesian hyperband for combining Bayesian search with early
[00:24:37.620 --> 00:24:42.380]   stopping, a hyperopt also early stopping, but with random search, these features
[00:24:42.380 --> 00:24:46.500]   that can really soup up your hyperparameter sweeps and get really good configurations
[00:24:46.500 --> 00:24:48.140]   with really small amounts of compute.
[00:24:48.140 --> 00:24:51.380]   So I recommend you check those out and start getting your hands dirty with
[00:24:51.380 --> 00:24:55.900]   the WaitSpicy sweep tool to select and optimize your hyperparameters.
[00:24:57.580 --> 00:24:58.740]   Hey friends, Charles here.
[00:24:58.740 --> 00:25:00.180]   Thanks for watching my video.
[00:25:00.180 --> 00:25:01.900]   If you enjoyed it, give it a like.
[00:25:01.900 --> 00:25:06.620]   If you want more WaitSpicy's tutorial and demo content, subscribe to our channel.
[00:25:06.620 --> 00:25:10.780]   And if you've got any questions, comments, ideas for future videos, leave a comment
[00:25:10.780 --> 00:25:12.220]   below, we'd love to hear from you.
[00:25:12.220 --> 00:25:24.020]   Bye.

