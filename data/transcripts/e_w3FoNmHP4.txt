
[00:00:00.000 --> 00:00:06.000]   And so now with just one click, we can turn this table into a plot that we can visually explore.
[00:00:06.000 --> 00:00:07.000]   Let's check it out.
[00:00:07.000 --> 00:00:14.000]   Hey, what is up everybody?
[00:00:14.000 --> 00:00:16.000]   Ivan from Weights & Biases here.
[00:00:16.000 --> 00:00:22.000]   And in this video, we'll be analyzing the sentiment of stock market news headlines as being positive, negative or neutral
[00:00:22.000 --> 00:00:29.000]   by inferencing a hugging face model on a large dataset that contains stock market news headlines.
[00:00:29.000 --> 00:00:34.000]   And we'll also be using WNB tables as a way of interactively exploring tabular data
[00:00:34.000 --> 00:00:39.000]   and suddenly checking our models and making sure that they perform properly.
[00:00:39.000 --> 00:00:48.000]   WNB or Weights & Biases is a machine learning tools platform and tables is one of the tools at our disposal.
[00:00:48.000 --> 00:00:56.000]   Tables allow us to log images, audio and text data and then interactively explore them right in our browser.
[00:00:56.000 --> 00:01:03.000]   Tables are useful for exploring datasets or as we'll be doing in this video, visualizing model predictions.
[00:01:03.000 --> 00:01:07.000]   So now let's really quickly talk about what is FinBert.
[00:01:07.000 --> 00:01:14.000]   So FinBert is essentially a version of Google's famous BERT model, which was fine-tuned on financial data.
[00:01:14.000 --> 00:01:18.000]   What we're trying to do is to analyze sentiment of stock market news headlines
[00:01:18.000 --> 00:01:23.000]   and those involve a lot of jargon that's very domain specific
[00:01:23.000 --> 00:01:29.000]   and just normal English language models, they do not necessarily generalize well to the financial domain.
[00:01:29.000 --> 00:01:34.000]   And so that's why we're going to be using FinBert, which was trained on financial data
[00:01:34.000 --> 00:01:38.000]   and which can do a lot better of a job with analyzing that sentiment.
[00:01:38.000 --> 00:01:43.000]   So right now we're looking at a Google call up notebook, which is used to load the data,
[00:01:43.000 --> 00:01:46.000]   which in our case, we're using a dataset that the model was not trained on.
[00:01:46.000 --> 00:01:51.000]   We're using a Kaggle dataset, which is called Daily Financial News for 6000+ Stock,
[00:01:51.000 --> 00:01:56.000]   which was scraped from the internet and contains a lot of the stock market news headlines.
[00:01:56.000 --> 00:01:58.000]   And that is what we're interested in.
[00:01:58.000 --> 00:02:03.000]   Then this call up notebook inferences the Hugging Face FinBert model on that data
[00:02:03.000 --> 00:02:09.000]   and logs it to Weights and Biases and logs the model prediction says WNB tables.
[00:02:09.000 --> 00:02:13.000]   You can log a WNB table by formatting a pandas data frame
[00:02:13.000 --> 00:02:17.000]   and then just simply logging it with one line of code.
[00:02:17.000 --> 00:02:21.000]   And if you're looking to log and explore some data using WNB tables,
[00:02:21.000 --> 00:02:24.000]   you may also find documentation useful to help you get started.
[00:02:24.000 --> 00:02:27.000]   And I will leave a link to that in the video description.
[00:02:27.000 --> 00:02:32.000]   So when the model inference has finished and you've logged the data to Weights and Biases,
[00:02:32.000 --> 00:02:36.000]   you can click on this link to open the run page.
[00:02:36.000 --> 00:02:44.000]   So the way WNB works is that we can have a project and we can log multiple runs to it.
[00:02:44.000 --> 00:02:49.000]   So you can think of this as us being able to log model predictions for different models
[00:02:49.000 --> 00:02:54.000]   or for different data sets and then being able to compare all of them in a single project page
[00:02:54.000 --> 00:02:56.000]   in a single dashboard like this one.
[00:02:56.000 --> 00:03:02.000]   In our case, we're interested in this specific set of model predictions visualized as a WNB table.
[00:03:02.000 --> 00:03:06.000]   So let's first take a look. Here we have our headlines.
[00:03:06.000 --> 00:03:10.000]   Here we have the stock to which the headlines is attributed to.
[00:03:10.000 --> 00:03:15.000]   And we have the sentiment analysis predictions as being positive, negative or neutral.
[00:03:15.000 --> 00:03:20.000]   First thing we can do is sort the predictions in ascending or descending orders
[00:03:20.000 --> 00:03:24.000]   and try to find, say, the most negative headlines.
[00:03:24.000 --> 00:03:31.000]   So out of communications fell 18 percent, US traffic rail down 6 percent, fall 7.9 percent.
[00:03:31.000 --> 00:03:36.000]   Let's try also sorting by the positive headlines.
[00:03:36.000 --> 00:03:44.000]   Here we can see ratings improved, IBD ratings upgrade improved, credit continue, revenues increase.
[00:03:44.000 --> 00:03:48.000]   Here we can see the word improve appear in the two most positive headlines.
[00:03:48.000 --> 00:03:55.000]   So let's use WNB tables to check if the word improve always biases the model to output positive predictions.
[00:03:55.000 --> 00:03:59.000]   All right. So the first thing we can do is to remove the run name column,
[00:03:59.000 --> 00:04:04.000]   because right now we're dealing with a table that only has the predictions from one run.
[00:04:04.000 --> 00:04:08.000]   So this column is essentially the same for all of the headlines.
[00:04:08.000 --> 00:04:15.000]   So I'll just remove it. Now I want to create a new column that will check if a given headline contains the word improve.
[00:04:15.000 --> 00:04:19.000]   Here's how we can do it. So click insert one. Right.
[00:04:19.000 --> 00:04:22.000]   There'll be a new column. Then I click column settings.
[00:04:22.000 --> 00:04:33.000]   And here I say row headline that headline that lower contains improve.
[00:04:33.000 --> 00:04:36.000]   And now we can group by this column.
[00:04:36.000 --> 00:04:45.000]   So now we can see that the model predictions were grouped by whether the headline which was input into the model contains the word improve or not.
[00:04:45.000 --> 00:04:54.000]   We can see those headlines here, but I prefer a way of looking at them by also inserting a new column, going into its settings,
[00:04:54.000 --> 00:05:04.000]   selecting the page size of one and being able to see a given headline and the model predictions for that specific headline.
[00:05:04.000 --> 00:05:12.000]   We can probably just remove this one. And we can probably also now remove the stacks column because we can see the stack for a given headline here.
[00:05:12.000 --> 00:05:20.000]   So now we can have our headlines. As you can see, we have 20 of them containing the word improve and around all the other ones not containing that word.
[00:05:20.000 --> 00:05:27.000]   Now we can look at the value distributions for the positive, negative and neutral predictions for the specific group of headlines.
[00:05:27.000 --> 00:05:34.000]   Now here we can see that 16 of the 20 headlines that contain the word improve are highly, highly positive.
[00:05:34.000 --> 00:05:44.000]   So it kind of leads us to suspect that the word improve does correlate, at least correlated this point with the model analyzing the sentiment as being positive.
[00:05:44.000 --> 00:05:54.000]   But to be sure, we can go here and say column settings and we can average the different classes.
[00:05:54.000 --> 00:05:59.000]   And as you can see now, what we suspected by looking at the value distributions turned out to be true,
[00:05:59.000 --> 00:06:09.000]   which is that the headlines that contain the word improve are on average 88 percent positive and only 3 percent negative and 7 percent neutral.
[00:06:09.000 --> 00:06:17.000]   So there seems to be a very strong correlation where the headline contains the word improve that it's going to be classified as positive.
[00:06:17.000 --> 00:06:21.000]   But just looking at the numbers might not tell us the full story.
[00:06:21.000 --> 00:06:27.000]   And that's why we can make a plot which will contain the headlines and their predictions.
[00:06:27.000 --> 00:06:37.000]   So first thing I'll do is I'll remove the averages here.
[00:06:37.000 --> 00:06:42.000]   And ungroup the entries by whether they contain the word improve.
[00:06:42.000 --> 00:06:48.000]   So what we got to do now is to click on the gear icon and click plot table query.
[00:06:48.000 --> 00:07:02.000]   And so here we can see an interactive dashboard which contains all of our headlines and where they fall in the distribution of being positive or negative, according to, of course, the thin bird model predictions.
[00:07:02.000 --> 00:07:06.000]   As you can see right now, the X dimension is the positive column.
[00:07:06.000 --> 00:07:10.000]   The Y dimension is the negative column and the label is our headlines.
[00:07:10.000 --> 00:07:12.000]   So as you can see, we've got a lot of headlines.
[00:07:12.000 --> 00:07:16.000]   So we've got a lot of different points on this plot.
[00:07:16.000 --> 00:07:18.000]   And this plot is really customizable.
[00:07:18.000 --> 00:07:28.000]   And so what we're interested in finding out is looking at where do those examples that contain the word improve fall in comparison to all the other examples.
[00:07:28.000 --> 00:07:38.000]   So what we can do is we can instead of the label being a headline, we can make the label be headlines which contain the word improve will be our label.
[00:07:38.000 --> 00:07:52.000]   And for tall tab, aka for the text that shows up when we hover with our mouse over a specific point like here, we'll set it to just, you know, be headlines so that we could then hover over a point and read what headline it's actually about.
[00:07:52.000 --> 00:07:55.000]   And now we can click apply.
[00:07:55.000 --> 00:07:57.000]   And voila.
[00:07:57.000 --> 00:08:01.000]   Now we can see that all of the yellow points are the one that contain the word improve.
[00:08:01.000 --> 00:08:10.000]   And as we can see, it's true that they all land like most of the yellow points, they land in the area of being really, really positive.
[00:08:10.000 --> 00:08:12.000]   So now let's look at the actual examples.
[00:08:12.000 --> 00:08:28.000]   If we zoom in here, we can see that this one is shows improved relative price strength, improves Parkinson's symptoms, improved picture, improved operational performance, neurogenetic, improved technical strength.
[00:08:28.000 --> 00:08:37.000]   So now we can see that the headline that our model classified as being positive, which contain the word improve, are actually pretty positive if we read them ourselves.
[00:08:37.000 --> 00:08:43.000]   And we're not able to see any sort of bias that our model could have towards the word improve.
[00:08:43.000 --> 00:08:59.000]   So it's ending up being a bit of a sanity check in this regard. But who knows, maybe one of these days me or you will be able to catch some inconsistencies and be able to diagnose many sort of problems or bugs which may arise in the pipeline of training ML models.
[00:08:59.000 --> 00:09:04.000]   And so now let's look at the examples with the word improve that are classified to not be that positive.
[00:09:04.000 --> 00:09:11.000]   So, for instance, this guy here, Baldwin and hardware, living bread, bread and the hardware and home improvement division of.
[00:09:11.000 --> 00:09:19.000]   So as you can see here, the word improve appears more in the way of describing an industry and is not saying specifically that something has improved.
[00:09:19.000 --> 00:09:24.000]   So here the model also performs properly. And now let's look at this point.
[00:09:24.000 --> 00:09:31.000]   Also, Humana misses Q4 earnings expectations, but improves year over year analyst block.
[00:09:31.000 --> 00:09:37.000]   So as you can see here, it misses Q4 earnings expectations, but it improves the year over year.
[00:09:37.000 --> 00:09:49.000]   And yeah, that seems fair to me looking at it. So it's not so it's not as positive as, say, this example with the word improve, because it also misses some earnings expectations.
[00:09:49.000 --> 00:09:54.000]   So as you can see, the model is classifying the examples with the word improve properly.
[00:09:54.000 --> 00:10:00.000]   And now we can just have the peace of mind that the model actually works as expected.
[00:10:00.000 --> 00:10:07.000]   So that's it for this video, and I hope that it at least piqued your interest to explore more about what WMB tables can do.
[00:10:07.000 --> 00:10:15.000]   Links to the code that is used to generate this table will be in the video description and link to the table docs will also be in the video description.
[00:10:15.000 --> 00:10:17.000]   So thank you for watching this video.
[00:10:17.000 --> 00:10:24.000]   Smash that like button to let us know that you enjoyed it and consider subscribing to our channel to see more tutorials, interviews and talks.
[00:10:24.000 --> 00:10:29.000]   And thank you for watching this video. I really hope that you enjoyed it and found it useful.

