
[00:00:00.000 --> 00:00:09.000]   All right, we are live.
[00:00:09.000 --> 00:00:11.560]   Hello, everyone.
[00:00:11.560 --> 00:00:20.240]   I am Charles Fry, and I am your host today in this W&B live stream.
[00:00:20.240 --> 00:00:23.240]   Go ahead and say hello in the chat.
[00:00:23.240 --> 00:00:26.940]   Let me know where you are coming from.
[00:00:26.940 --> 00:00:29.020]   Maybe that's where you're coming from in the world.
[00:00:29.020 --> 00:00:32.320]   I think we're going to have some folks worldwide here at this.
[00:00:32.320 --> 00:00:38.400]   I'm coming to you from Oakland, California, where it is 8 in the morning.
[00:00:38.400 --> 00:00:45.840]   And also maybe let me know how you heard about this live stream and maybe what brought you
[00:00:45.840 --> 00:00:48.140]   here.
[00:00:48.140 --> 00:00:55.520]   So just pop that into the chat while I tell you a little bit about what this live stream
[00:00:55.520 --> 00:00:58.960]   is about, what our goal is here.
[00:00:58.960 --> 00:01:04.120]   So what we're going to be doing is we're going to be working through one of my favorite and
[00:01:04.120 --> 00:01:11.600]   one of a lot of people's favorite PyTorch tutorials called "What is Torch.NN, Really?"
[00:01:11.600 --> 00:01:18.440]   by Jeremy Howard, who is the sort of lead creator at Fast.ai.
[00:01:18.440 --> 00:01:27.360]   And the idea in this tutorial is that we are -- we're going to start with just absolutely
[00:01:27.360 --> 00:01:37.040]   raw PyTorch, no special features, just the very basic sort of stuff that PyTorch offers.
[00:01:37.040 --> 00:01:43.300]   The -- just sort of some math, basically.
[00:01:43.300 --> 00:01:48.360]   That's the core of PyTorch, fast matrix math with derivatives.
[00:01:48.360 --> 00:01:52.080]   And then we're going to add more and more features from the rest of PyTorch from this
[00:01:52.080 --> 00:02:00.040]   Torch.NN module, the neural networks module of PyTorch, until we've got a nice well-written
[00:02:00.040 --> 00:02:06.040]   neural network sort of set of training code.
[00:02:06.040 --> 00:02:09.320]   I'm starting to see some things come in to the chat.
[00:02:09.320 --> 00:02:11.900]   Some folks saying hello from China.
[00:02:11.900 --> 00:02:13.760]   Some compliments to Oakland.
[00:02:13.760 --> 00:02:14.760]   It is a beautiful city.
[00:02:14.760 --> 00:02:18.040]   I love Oakland.
[00:02:18.040 --> 00:02:20.020]   And oh, coming in from Italy.
[00:02:20.020 --> 00:02:21.020]   That's nice.
[00:02:21.020 --> 00:02:22.020]   Yeah.
[00:02:22.020 --> 00:02:23.020]   So...
[00:02:23.020 --> 00:02:32.200]   So, I took basically these -- what is Torch.NN, really, this tutorial, and just modified it
[00:02:32.200 --> 00:02:36.760]   a little bit for use with this Jupyter Notebook presentation tool that I'm going to be using.
[00:02:36.760 --> 00:02:46.960]   So, what you can see over there is this -- some Jupyter Notebook slides that I'll be going
[00:02:46.960 --> 00:02:52.600]   back and forth here, like, basically as a slide deck.
[00:02:52.600 --> 00:02:56.160]   But underneath it is a Jupyter Notebook.
[00:02:56.160 --> 00:03:02.680]   So, just to show you what that looks like, just to be, you know, say nothing up my sleeves,
[00:03:02.680 --> 00:03:05.000]   there's the Jupyter Notebook that backs this.
[00:03:05.000 --> 00:03:09.120]   You can see it's just a regular Jupyter Notebook.
[00:03:09.120 --> 00:03:13.360]   And I am...
[00:03:13.360 --> 00:03:18.240]   All I've done is add this little rise extension that allows you to make these cool presentations
[00:03:18.240 --> 00:03:19.240]   like this.
[00:03:19.240 --> 00:03:23.400]   So, you can follow along with this in a live collab.
[00:03:23.400 --> 00:03:25.780]   You won't get the presentation style.
[00:03:25.780 --> 00:03:29.200]   For that, you have to use raw Jupyter Notebooks.
[00:03:29.200 --> 00:03:33.360]   Oh, somebody else saying hello from Dusseldorf in Germany.
[00:03:33.360 --> 00:03:34.360]   Nice.
[00:03:34.360 --> 00:03:35.360]   Hey there, Vincent.
[00:03:35.360 --> 00:03:38.720]   Oh, and Anmal saying hello from India.
[00:03:38.720 --> 00:03:41.360]   Hi there, Anmal.
[00:03:41.360 --> 00:03:42.360]   All right.
[00:03:42.360 --> 00:03:44.240]   So, let's dive in.
[00:03:44.240 --> 00:03:49.240]   Now that I've given you a little bit of background.
[00:03:49.240 --> 00:03:54.280]   So, the core...
[00:03:54.280 --> 00:03:57.800]   I copied a lot of the text out of this tutorial here.
[00:03:57.800 --> 00:04:01.400]   So, the words here are pretty much all Jeremy Howard's.
[00:04:01.400 --> 00:04:05.360]   I'll just be writing a little bit of commentary as I read them.
[00:04:05.360 --> 00:04:08.760]   So, PyTorch has these really, really nice modules and classes.
[00:04:08.760 --> 00:04:13.440]   MainTorch.nn, Torch.optim, Dataset and Dataloader.
[00:04:13.440 --> 00:04:16.520]   And they're easy to use.
[00:04:16.520 --> 00:04:20.060]   Almost too easy to use, you know?
[00:04:20.060 --> 00:04:23.760]   Because people will use them without actually understanding what exactly they're doing.
[00:04:23.760 --> 00:04:25.880]   So, they don't know how to customize them.
[00:04:25.880 --> 00:04:29.720]   So, the idea of this tutorial, the conceit of this tutorial is to use the most basic
[00:04:29.720 --> 00:04:36.120]   PyTorch tensor functionality of matrix math and derivatives and then build up from there
[00:04:36.120 --> 00:04:42.680]   to incrementally in this really nice sort of step by step way, adding one feature from
[00:04:42.680 --> 00:04:50.400]   these higher level components of PyTorch until we've got basically much cleaner code at the
[00:04:50.400 --> 00:04:53.160]   end that does...
[00:04:53.160 --> 00:04:57.800]   That's either simpler or more flexible.
[00:04:57.800 --> 00:05:05.520]   So there's kind of this presumption that you're a little bit familiar with the basics of tensor
[00:05:05.520 --> 00:05:07.640]   operations.
[00:05:07.640 --> 00:05:11.840]   Or at least that you've tried something like NumPy or MATLAB.
[00:05:11.840 --> 00:05:18.520]   PyTorch stuff is very, very similar in terms of just the tensor operations.
[00:05:18.520 --> 00:05:20.320]   So there's a presumption that you're a little bit familiar with that.
[00:05:20.320 --> 00:05:24.200]   I think some folks are coming here from our PyTorch reading group that we organized on
[00:05:24.200 --> 00:05:25.360]   this channel.
[00:05:25.360 --> 00:05:29.640]   Or possibly coming from our FastAI, Fastbook reading group.
[00:05:29.640 --> 00:05:31.160]   So you should be...
[00:05:31.160 --> 00:05:34.640]   You'll probably be loosely familiar with some of these ideas and we're gonna really explicate
[00:05:34.640 --> 00:05:40.560]   how those ideas get used to make a neural network here.
[00:05:40.560 --> 00:05:46.480]   So before we get started doing stuff with a model, we gotta have our data set up.
[00:05:46.480 --> 00:05:54.080]   So we're gonna use this classic handwritten data set called MNIST, black and white images
[00:05:54.080 --> 00:05:57.000]   of hand-drawn digits between zero and nine.
[00:05:57.000 --> 00:06:01.120]   And we're gonna try and be as explicit as possible about all the pieces that we're using
[00:06:01.120 --> 00:06:02.920]   in this tutorial.
[00:06:02.920 --> 00:06:04.920]   So down to the brass tacks.
[00:06:04.920 --> 00:06:12.440]   When we're dealing with data, we've got to use all the...
[00:06:12.440 --> 00:06:16.360]   We've got to use lots of pieces of the Python standard library and other things for handling
[00:06:16.360 --> 00:06:18.320]   files and dealing with files.
[00:06:18.320 --> 00:06:21.560]   So we're gonna be using pathlib for dealing with paths.
[00:06:21.560 --> 00:06:26.080]   It's a nice way to abstract paths away from just plain old strings.
[00:06:26.080 --> 00:06:31.480]   It's part of the Python 3 standard library, starting with, I think, 3.5 Python?
[00:06:31.480 --> 00:06:32.480]   Correct me if I'm wrong in the chat.
[00:06:32.480 --> 00:06:35.200]   If you know which version they added paths to.
[00:06:35.200 --> 00:06:38.240]   And then we'll use the requests library to download the data set.
[00:06:38.240 --> 00:06:46.520]   Requests is the preferred way, I think, by most people to use to download, to, like,
[00:06:46.520 --> 00:06:49.720]   pull stuff from the internet programmatically with Python.
[00:06:49.720 --> 00:06:52.840]   For at least relatively simple things.
[00:06:52.840 --> 00:06:54.520]   So yeah.
[00:06:54.520 --> 00:06:57.760]   So this requests...
[00:06:57.760 --> 00:06:59.120]   We'll start with path.
[00:06:59.120 --> 00:07:02.120]   We're building our path where our data is gonna go here.
[00:07:02.120 --> 00:07:03.840]   Relatively simple.
[00:07:03.840 --> 00:07:06.760]   And then we want to grab a URL from...
[00:07:06.760 --> 00:07:09.240]   Or we want to grab a file from a URL.
[00:07:09.240 --> 00:07:13.440]   So we're pulling it with requests and writing it to the disk.
[00:07:13.440 --> 00:07:15.800]   So nothing neural network-y yet.
[00:07:15.800 --> 00:07:21.000]   But what I do like about this tutorial is just how pedantic it is about getting every
[00:07:21.000 --> 00:07:24.760]   single piece very explicit.
[00:07:24.760 --> 00:07:26.280]   What are we doing here?
[00:07:26.280 --> 00:07:28.040]   Why are we doing it?
[00:07:28.040 --> 00:07:32.240]   So in that spirit, we've got a data set that's a NumPy array.
[00:07:32.240 --> 00:07:33.840]   It was pickled.
[00:07:33.840 --> 00:07:38.320]   Pickling is how you save Python objects to disk.
[00:07:38.320 --> 00:07:40.440]   The pickle library.
[00:07:40.440 --> 00:07:47.160]   And so it takes these things that you have in your Python, like, memory, and it saves
[00:07:47.160 --> 00:07:48.160]   them.
[00:07:48.160 --> 00:07:51.200]   It's kind of how you would pickle a vegetable.
[00:07:51.200 --> 00:07:53.120]   So it lasts a little bit longer.
[00:07:53.120 --> 00:07:57.120]   This is also the basic idea behind how, say, a Torch model is saved.
[00:07:57.120 --> 00:08:02.080]   It's also, I think, in the end, based on pickling.
[00:08:02.080 --> 00:08:06.160]   So we gotta unpickle our data, is what it's called.
[00:08:06.160 --> 00:08:09.320]   You can't unpickle a vegetable, but you can unpickle data.
[00:08:09.320 --> 00:08:13.040]   So we gotta unpickle our data here.
[00:08:13.040 --> 00:08:14.040]   And get it loaded.
[00:08:14.040 --> 00:08:16.760]   So we've got a training set and a validation set.
[00:08:16.760 --> 00:08:19.800]   We'll talk a little bit more about what a validation set means, if you haven't seen
[00:08:19.800 --> 00:08:21.000]   that before.
[00:08:21.000 --> 00:08:25.320]   But importantly, we've got our inputs and our targets.
[00:08:25.320 --> 00:08:33.520]   Our inputs are 28 by 28 images stored as vectors.
[00:08:33.520 --> 00:08:39.240]   So stored as things of shape 28 by 28.
[00:08:39.240 --> 00:08:40.480]   So here's an example.
[00:08:40.480 --> 00:08:46.080]   You can see that 28 by 28 is not a particularly big image.
[00:08:46.080 --> 00:08:47.080]   But you can see it there.
[00:08:47.080 --> 00:08:50.000]   Let me resize that image while we're here.
[00:08:50.000 --> 00:08:52.320]   No, not reshape.
[00:08:52.320 --> 00:08:54.240]   Resize here.
[00:08:54.240 --> 00:08:56.760]   Let's do 256 by 256.
[00:08:56.760 --> 00:08:57.760]   There we go.
[00:08:57.760 --> 00:08:58.760]   All right.
[00:08:58.760 --> 00:09:00.760]   Slightly bigger version of that image.
[00:09:00.760 --> 00:09:07.160]   So this is one of the benefits, actually, of doing your presentations as a Jupyter Notebook.
[00:09:07.160 --> 00:09:11.120]   You can change these sorts of things on the fly using Python.
[00:09:11.120 --> 00:09:15.880]   So if you have questions, actually, about the material, if you post them in the chat,
[00:09:15.880 --> 00:09:20.840]   I can try and resolve those and change things around with the code.
[00:09:20.840 --> 00:09:25.200]   For example, if you want to know a little bit more about this resize method, I could
[00:09:25.200 --> 00:09:26.440]   do something like that.
[00:09:26.440 --> 00:09:29.400]   Pull up the docs here.
[00:09:29.400 --> 00:09:35.920]   And now here we've got the resizing method pulled up with all the information about it
[00:09:35.920 --> 00:09:38.060]   in case we wanted to read more.
[00:09:38.060 --> 00:09:40.720]   But for now, we don't care about those details.
[00:09:40.720 --> 00:09:43.240]   Let's just show the image.
[00:09:43.240 --> 00:09:45.920]   So this is our data.
[00:09:45.920 --> 00:09:47.400]   It's handwritten digits.
[00:09:47.400 --> 00:09:53.760]   We want to train an algorithm to say this digit is a 5, another digit, maybe let's grab
[00:09:53.760 --> 00:09:55.200]   a different digit even.
[00:09:55.200 --> 00:09:56.840]   We can also do that.
[00:09:56.840 --> 00:09:59.600]   So this one, we'd want our network to say that's a 3.
[00:09:59.600 --> 00:10:00.600]   What about this one?
[00:10:00.600 --> 00:10:04.280]   We'd want our network to say this one is an 8.
[00:10:04.280 --> 00:10:09.480]   So yeah, that's the neural network problem that we are tackling here.
[00:10:09.480 --> 00:10:10.760]   So that's a classic problem.
[00:10:10.760 --> 00:10:13.240]   So I'm not spending too much time on it.
[00:10:13.240 --> 00:10:19.440]   If you want, post in the chat and I can send links to other tutorials where you can learn
[00:10:19.440 --> 00:10:22.280]   a little bit more about that classic example.
[00:10:22.280 --> 00:10:25.440]   Oh, I see there's some more stuff coming in in the chat.
[00:10:25.440 --> 00:10:26.440]   Let me look back there.
[00:10:26.440 --> 00:10:28.880]   Oh, hello from Spain.
[00:10:28.880 --> 00:10:32.080]   Victor de Vierna del Carse is coming in from Spain.
[00:10:32.080 --> 00:10:33.080]   Hey there.
[00:10:33.080 --> 00:10:36.760]   Santosh Prashant and Shwetank, hello to all of you.
[00:10:36.760 --> 00:10:38.960]   Oh, thanks for the compliment, Shwetank.
[00:10:38.960 --> 00:10:40.600]   Shwetank loves my live streams.
[00:10:40.600 --> 00:10:41.600]   That's great.
[00:10:41.600 --> 00:10:44.200]   I love your live streams, too.
[00:10:44.200 --> 00:10:49.540]   So PyTorch uses-- NumPy uses arrays.
[00:10:49.540 --> 00:10:53.720]   And maybe you're familiar with arrays if you're coming from MATLAB or if you've used NumPy
[00:10:53.720 --> 00:11:00.040]   and other parts of numerical Python-- numerical computing with Python before.
[00:11:00.040 --> 00:11:02.920]   The idea of an array is very common.
[00:11:02.920 --> 00:11:07.000]   The equivalent of an array in PyTorch is called a tensor.
[00:11:07.000 --> 00:11:13.340]   There's not much difference on a mathematical level, but inside of the computer program,
[00:11:13.340 --> 00:11:16.280]   we need to be explicit that these things are tensors.
[00:11:16.280 --> 00:11:28.040]   And what is actually going on under the hood in what an actual tensor is, is it's got extra
[00:11:28.040 --> 00:11:36.080]   things on top of just being an array with data in it that allow us to do machine learning.
[00:11:36.080 --> 00:11:38.240]   So the content looks the same.
[00:11:38.240 --> 00:11:44.960]   So we've got-- what this cell is doing here is it's printing out our training data, the
[00:11:44.960 --> 00:11:50.520]   data we're going to show our neural network for the inputs, x, and the targets, y.
[00:11:50.520 --> 00:11:56.160]   So you can see the tensor full of numbers in this component of the screen here, that
[00:11:56.160 --> 00:11:57.520]   guy.
[00:11:57.520 --> 00:11:58.520]   The first one's a 5.
[00:11:58.520 --> 00:12:00.560]   The second one's a 0.
[00:12:00.560 --> 00:12:03.960]   That last one is an 8.
[00:12:03.960 --> 00:12:10.520]   And we've also got some useful information about these tensors also being printed.
[00:12:10.520 --> 00:12:14.400]   So there's features of these and functions defined on them that you might be familiar
[00:12:14.400 --> 00:12:16.300]   with from other array libraries.
[00:12:16.300 --> 00:12:21.800]   So y_train.min, y_train.max here are telling us what the minimum and maximum of that target's
[00:12:21.800 --> 00:12:22.800]   tensor is.
[00:12:22.800 --> 00:12:24.720]   And minimum is 0, the maximum is 9.
[00:12:24.720 --> 00:12:26.640]   Those are our labels.
[00:12:26.640 --> 00:12:32.600]   And then we've also got the shape of our x_train tensor here.
[00:12:32.600 --> 00:12:35.680]   Tensors are a really important fact about tensors.
[00:12:35.680 --> 00:12:40.680]   It's maybe the most important piece of information about a tensor besides its contents is its
[00:12:40.680 --> 00:12:41.680]   shape.
[00:12:41.680 --> 00:12:44.720]   We won't be talking too much about that stuff here, about the linear algebra.
[00:12:44.720 --> 00:12:49.240]   If you want, we also have the Math for ML series that talks about linear algebra, calculus,
[00:12:49.240 --> 00:12:52.320]   and probability that really explains a lot of those ideas.
[00:12:52.320 --> 00:12:54.380]   So there's videos for that on our channel.
[00:12:54.380 --> 00:12:58.240]   And we're also adding some more videos on that coming soon in the next couple of weeks.
[00:12:58.240 --> 00:13:00.000]   So keep an eye out for that.
[00:13:00.000 --> 00:13:06.640]   Some nice videos on math and Python and machine learning.
[00:13:06.640 --> 00:13:09.640]   But yeah, but for now, we've just got these tensors.
[00:13:09.640 --> 00:13:11.520]   We're running with those.
[00:13:11.520 --> 00:13:14.160]   And this is all we need, really.
[00:13:14.160 --> 00:13:18.400]   Tensors and tensor math is all we need in order to make a neural network from scratch.
[00:13:18.400 --> 00:13:22.360]   So back in the day when I was first getting started with training neural networks almost
[00:13:22.360 --> 00:13:25.720]   10 years ago now, this is how you had to do it.
[00:13:25.720 --> 00:13:29.260]   You had to roll your own torch.nn.
[00:13:29.260 --> 00:13:33.720]   So this is a throwback for me.
[00:13:33.720 --> 00:13:38.800]   So we're going to create our model using nothing but these PyTorch tensor operations.
[00:13:38.800 --> 00:13:44.000]   And I'm not going to go in detail over what's happening inside neural networks.
[00:13:44.000 --> 00:13:48.160]   There's course.fast.ai is Jeremy Howard's course.
[00:13:48.160 --> 00:13:49.400]   He's the author of this tutorial.
[00:13:49.400 --> 00:13:51.920]   He made that course.
[00:13:51.920 --> 00:13:57.120]   We also have tutorial information about neural networks on our channel.
[00:13:57.120 --> 00:13:58.760]   You can find that as well.
[00:13:58.760 --> 00:14:01.600]   The CEO of Weights and Biases, Lucas Biewald, did this really nice course.
[00:14:01.600 --> 00:14:07.860]   It's in Keras rather than Torch, but it's a really nice course on neural networks.
[00:14:07.860 --> 00:14:14.240]   So PyTorch gives us methods to create tensors and create random tensors.
[00:14:14.240 --> 00:14:16.880]   So tensors full of random numbers or tensors full of zeros.
[00:14:16.880 --> 00:14:22.080]   And we're going to use those to create the weights and biases, no relation to the company,
[00:14:22.080 --> 00:14:28.320]   so to speak, the weights and biases, the actual numbers in our neural network for just a simple
[00:14:28.320 --> 00:14:31.080]   little linear model.
[00:14:31.080 --> 00:14:34.720]   These are just regular tensors.
[00:14:34.720 --> 00:14:37.900]   And there's just one particular special edition.
[00:14:37.900 --> 00:14:43.120]   We tell PyTorch, track this because I'm going to ask you to give me gradients for this.
[00:14:43.120 --> 00:14:46.520]   I'm going to ask you to do calculus on this tensor.
[00:14:46.520 --> 00:14:48.800]   Tell me what would happen if I changed its values.
[00:14:48.800 --> 00:14:55.840]   So we have this require grad argument that we pass when we make the tensor saying, pay
[00:14:55.840 --> 00:14:58.040]   attention to this tensor, PyTorch.
[00:14:58.040 --> 00:15:01.800]   And that will cause PyTorch to record all the operations done on the tensor.
[00:15:01.800 --> 00:15:05.860]   So PyTorch is, you know, while we're running PyTorch is in the background sort of taking
[00:15:05.860 --> 00:15:10.240]   notes and being like, okay, you added it, and then you multiplied it by 5, and then
[00:15:10.240 --> 00:15:14.040]   you applied the sine function to it or whatever.
[00:15:14.040 --> 00:15:18.260]   And then that will allow it to calculate gradients during back propagation automatically.
[00:15:18.260 --> 00:15:22.320]   So gradients and back propagation, these are ideas you might have come across in a different
[00:15:22.320 --> 00:15:29.120]   class where you've learned about neural networks.
[00:15:29.120 --> 00:15:31.960]   But these are the things that we need to do in order to make our neural networks better,
[00:15:31.960 --> 00:15:35.900]   in order to optimize them on our data.
[00:15:35.900 --> 00:15:41.800]   So for the weights, we make our weight tensor, and then we apply requires grad afterwards
[00:15:41.800 --> 00:15:45.120]   because we don't want to take the gradient with respect to the initialization.
[00:15:45.120 --> 00:15:47.040]   That would be somewhat strange.
[00:15:47.040 --> 00:15:49.240]   So we don't want to do that now.
[00:15:49.240 --> 00:15:51.640]   So that's our first two lines there.
[00:15:51.640 --> 00:15:53.280]   Weights equals torch dot rand n.
[00:15:53.280 --> 00:15:57.160]   That's a random normal matrix we're creating there.
[00:15:57.160 --> 00:15:59.900]   And then we're requiring the gradient.
[00:15:59.900 --> 00:16:01.760]   And then the biases we're just going to fill with zero.
[00:16:01.760 --> 00:16:06.760]   So we set all the entries of the bias tensor to zero.
[00:16:06.760 --> 00:16:08.540]   And let's see.
[00:16:08.540 --> 00:16:11.000]   I think I'll print the weights on the next slide.
[00:16:11.000 --> 00:16:13.240]   So I'm not going to -- or the biases, maybe.
[00:16:13.240 --> 00:16:16.440]   So I'm not going to print them just now.
[00:16:16.440 --> 00:16:19.040]   But just some notes about this.
[00:16:19.040 --> 00:16:28.400]   One is you have to pick the -- to say you're making a random matrix is -- you have to say
[00:16:28.400 --> 00:16:31.560]   what kind of random values do you want.
[00:16:31.560 --> 00:16:33.100]   Normal is a really common choice.
[00:16:33.100 --> 00:16:34.880]   Sometimes people use uniform.
[00:16:34.880 --> 00:16:36.840]   But the other really important thing is scale.
[00:16:36.840 --> 00:16:41.880]   In addition to needing to know the shapes of tensors, especially when you have a random
[00:16:41.880 --> 00:16:46.400]   tensor, the other big thing you need to pay attention to is how big are the numbers here.
[00:16:46.400 --> 00:16:51.440]   And this particular choice, where you multiply by 1 over the square root of the number of
[00:16:51.440 --> 00:16:53.760]   inputs is called Xavier initialization.
[00:16:53.760 --> 00:16:57.440]   And I believe it's the default in PyTorch.
[00:16:57.440 --> 00:16:59.920]   There are actually better initializations.
[00:16:59.920 --> 00:17:01.800]   There are different ways of doing it.
[00:17:01.800 --> 00:17:04.520]   Maybe more sophisticated, better ways of doing it.
[00:17:04.520 --> 00:17:07.480]   But this is a nice and simple one.
[00:17:07.480 --> 00:17:11.720]   And is at the very least very closely related to the default in PyTorch, if it's not exactly
[00:17:11.720 --> 00:17:14.160]   the default.
[00:17:14.160 --> 00:17:20.680]   The other little bonus piece of information here is we've got this underscore at the end
[00:17:20.680 --> 00:17:21.680]   requires grad.
[00:17:21.680 --> 00:17:26.600]   When you're doing stuff in PyTorch at the bottom level of PyTorch, you can put an underscore
[00:17:26.600 --> 00:17:30.840]   at the end of almost any function and say do this operation without moving the tensor
[00:17:30.840 --> 00:17:32.880]   in memory.
[00:17:32.880 --> 00:17:36.360]   Or without creating a new tensor.
[00:17:36.360 --> 00:17:39.560]   That perform this operation in place.
[00:17:39.560 --> 00:17:41.400]   So that can sometimes be useful.
[00:17:41.400 --> 00:17:45.600]   We want the weights to require grad, but we don't want to make a new tensor to do that.
[00:17:45.600 --> 00:17:50.040]   So that's why we use the in place version of dot requires grad.
[00:17:50.040 --> 00:17:51.800]   All right.
[00:17:51.800 --> 00:17:56.800]   So this level of detail that we're going into here is not the level of detail you normally
[00:17:56.800 --> 00:17:58.640]   have to operate when you're writing PyTorch, right?
[00:17:58.640 --> 00:18:03.480]   Because we are just doing we're pretending that PyTorch was just low level tensor operations
[00:18:03.480 --> 00:18:08.960]   and we're writing our own neural network completely from scratch.
[00:18:08.960 --> 00:18:14.080]   So just a reminder that this level of detail is not the level of detail you always need.
[00:18:14.080 --> 00:18:19.400]   So for folks coming from, say, like, fastbook who are maybe used to working at a higher
[00:18:19.400 --> 00:18:20.400]   level, don't worry.
[00:18:20.400 --> 00:18:26.280]   We'll be getting back up to the higher level by the end of this stream.
[00:18:26.280 --> 00:18:31.560]   And now we need to make our model.
[00:18:31.560 --> 00:18:35.860]   And back in the day, one of the tricky parts about this, like, this goes even actually
[00:18:35.860 --> 00:18:40.720]   before I started writing neural networks, so maybe 15 or 20 years ago, you would have
[00:18:40.720 --> 00:18:46.160]   had to write your own derivatives for your model as well as just writing your model.
[00:18:46.160 --> 00:18:49.440]   But we don't have to do that anymore.
[00:18:49.440 --> 00:18:51.560]   Because all we have to do is write a forward pass.
[00:18:51.560 --> 00:18:55.040]   We have to write how our model operates on the data.
[00:18:55.040 --> 00:18:58.840]   As data goes through, it gets transformed by the various components of our model.
[00:18:58.840 --> 00:19:04.920]   So here data comes into our model, xb, gets matrix multiplied with the weights, and then
[00:19:04.920 --> 00:19:09.600]   we add the biases to it, to the output.
[00:19:09.600 --> 00:19:16.200]   Then since we're doing a classification network, we have to toss this log softmax on the end
[00:19:16.200 --> 00:19:22.840]   to turn the outputs of our model into sort of log probabilities.
[00:19:22.840 --> 00:19:26.920]   And if you want to know more about that, again, the Math for ML series talks a lot about why
[00:19:26.920 --> 00:19:33.120]   we need these log probabilities here and talks about where these ideas come from in a classification
[00:19:33.120 --> 00:19:34.360]   network.
[00:19:34.360 --> 00:19:39.080]   But we're focusing on the mechanics right now rather than the sort of theory.
[00:19:39.080 --> 00:19:43.360]   And mechanically, the important thing here is that I'm doing all these operations here,
[00:19:43.360 --> 00:19:51.120]   x minus x, like e to the x, summed along its last dimension, take the logarithm of that,
[00:19:51.120 --> 00:19:52.120]   and then reshape.
[00:19:52.120 --> 00:19:58.200]   So I'm doing all of these operations here.
[00:19:58.200 --> 00:20:00.160]   But I only need to write them for the forward pass.
[00:20:00.160 --> 00:20:03.960]   I don't need to write anything about what the derivatives of these things are, because
[00:20:03.960 --> 00:20:11.880]   any sort of standard Python function or callable object can be used to write a model in PyTorch.
[00:20:11.880 --> 00:20:13.840]   And you'll get gradients automatically.
[00:20:13.840 --> 00:20:17.320]   Yeah, so do we have any more notes here?
[00:20:17.320 --> 00:20:22.600]   Yeah, at is our dot product or our matrix multiplication.
[00:20:22.600 --> 00:20:26.600]   And the key thing to come out of this with is PyTorch provides you a lot of pre-written
[00:20:26.600 --> 00:20:30.860]   stuff, pre-rolled components, to make it easy, just because there's lots of things people
[00:20:30.860 --> 00:20:33.160]   use over and over again.
[00:20:33.160 --> 00:20:38.800]   But even though that is the case, loss functions, activation functions, things like that, you
[00:20:38.800 --> 00:20:41.280]   can easily write your own using plain Python.
[00:20:41.280 --> 00:20:44.240]   And PyTorch will make this code pretty fast.
[00:20:44.240 --> 00:20:50.560]   So it'll write it so that it runs on the GPU or runs on the CPU in a vectorized form.
[00:20:50.560 --> 00:20:55.440]   And so you don't have to worry about those things.
[00:20:55.440 --> 00:20:56.440]   That's the other big thing.
[00:20:56.440 --> 00:21:01.480]   Besides getting derivatives for us automatically, PyTorch also makes our code run fast for us
[00:21:01.480 --> 00:21:04.520]   automatically.
[00:21:04.520 --> 00:21:10.760]   And if you want to learn a little bit more about how PyTorch makes your code run fast
[00:21:10.760 --> 00:21:15.400]   and how it gets things running on the GPU and stuff like that, I recently wrote a little
[00:21:15.400 --> 00:21:18.360]   report about that using the PyTorch profile.
[00:21:18.360 --> 00:21:22.160]   Or let me drop that link in the chat, because that's a good one.
[00:21:22.160 --> 00:21:23.880]   That's a good one to learn more about.
[00:21:23.880 --> 00:21:24.880]   PyTorch.
[00:21:24.880 --> 00:21:25.880]   OK, wanb.me/trace-report.
[00:21:25.880 --> 00:21:31.460]   OK, if that link doesn't work, let me know, and I'll fix it.
[00:21:31.460 --> 00:21:37.860]   But yeah, that talks a lot about what is the forward and backward pass and mechanically
[00:21:37.860 --> 00:21:40.360]   in terms of how they are run on the GPU.
[00:21:40.360 --> 00:21:43.200]   But we won't talk about that today.
[00:21:43.200 --> 00:21:46.800]   We've got enough just grokking the Torch library.
[00:21:46.800 --> 00:21:51.080]   All right, so we've defined a model function.
[00:21:51.080 --> 00:21:53.720]   We did def model just now.
[00:21:53.720 --> 00:21:58.760]   And we can call that on a batch of data and take a look at the outputs.
[00:21:58.760 --> 00:22:07.920]   So data, like, images, a bunch of images together is called a batch or a mini batch, depending
[00:22:07.920 --> 00:22:10.380]   on who you ask.
[00:22:10.380 --> 00:22:16.600]   And so we're pulling out from X-train, we're pulling 64 examples here, passing them through
[00:22:16.600 --> 00:22:19.240]   our model and looking at what the model's outputs are.
[00:22:19.240 --> 00:22:22.160]   So the model's outputting some numbers.
[00:22:22.160 --> 00:22:27.040]   These are log probabilities here in that tensor on the left.
[00:22:27.040 --> 00:22:35.280]   So this tensor on the left here is our predictions on the first input digit.
[00:22:35.280 --> 00:22:43.060]   You can see that there are 64 predictions and there are-- so for the 64 inputs.
[00:22:43.060 --> 00:22:44.600]   And then each one has 10 entries.
[00:22:44.600 --> 00:22:50.600]   Each entry corresponds to the logarithm of the probability the network assigns to the
[00:22:50.600 --> 00:22:52.200]   statement.
[00:22:52.200 --> 00:22:53.440]   This image is a 0.
[00:22:53.440 --> 00:22:54.720]   This image is a 1.
[00:22:54.720 --> 00:22:57.160]   All the way up to this image is a 9.
[00:22:57.160 --> 00:23:00.480]   Oh, I got a request to zoom in on the slides.
[00:23:00.480 --> 00:23:01.480]   I'll do that.
[00:23:01.480 --> 00:23:05.680]   I might have to zoom out again a little bit.
[00:23:05.680 --> 00:23:11.460]   And when I do-- just because there's a lot to fit vertically at a couple points.
[00:23:11.460 --> 00:23:15.080]   But hopefully a little bit of zoom there is helpful.
[00:23:15.080 --> 00:23:16.920]   Great.
[00:23:16.920 --> 00:23:20.720]   So there's one more note here.
[00:23:20.720 --> 00:23:23.680]   I talked a little bit about the contents and the shape of the tensor.
[00:23:23.680 --> 00:23:31.800]   Notice the preds tensor there has this grad_fn also attached to it.
[00:23:31.800 --> 00:23:38.800]   And that preds tensor has not just the values of the tensor, but this gradient function.
[00:23:38.800 --> 00:23:44.160]   And this gradient function is what PyTorch is using to record what to do when you want
[00:23:44.160 --> 00:23:46.440]   to do calculus on this tensor.
[00:23:46.440 --> 00:23:53.640]   So that grad_fn says, here's the operations that created this tensor.
[00:23:53.640 --> 00:23:57.440]   So you can then go back and find the other tensors.
[00:23:57.440 --> 00:24:00.440]   So we can actually take a look at what that looks like.
[00:24:00.440 --> 00:24:06.840]   What this computational graph that PyTorch is building for you as we go.
[00:24:06.840 --> 00:24:08.520]   So how to read this graph?
[00:24:08.520 --> 00:24:12.680]   At the bottom, we have our output in green.
[00:24:12.680 --> 00:24:14.280]   This is our preds tensor.
[00:24:14.280 --> 00:24:17.480]   And then going all the way back, we have our weights and our biases.
[00:24:17.480 --> 00:24:23.760]   The W and B, weights and biases in our model.
[00:24:23.760 --> 00:24:27.880]   And what you can see is there's a matrix multiplication here.
[00:24:27.880 --> 00:24:29.680]   Mm backward.
[00:24:29.680 --> 00:24:30.760]   And add backward.
[00:24:30.760 --> 00:24:34.120]   That's matrix multiplication of weight with input.
[00:24:34.120 --> 00:24:36.160]   And then also adding the bias.
[00:24:36.160 --> 00:24:40.240]   So that gets us to our X here.
[00:24:40.240 --> 00:24:42.320]   Or the output of our first layer.
[00:24:42.320 --> 00:24:48.640]   And then this block here is our log softmax calculation.
[00:24:48.640 --> 00:24:50.400]   Let me actually go back.
[00:24:50.400 --> 00:24:53.880]   In our log softmax calculation, we took the value X
[00:24:53.880 --> 00:24:55.360]   that goes into the log softmax.
[00:24:55.360 --> 00:24:57.280]   And we did this set of transforms on it.
[00:24:57.280 --> 00:25:00.160]   And we also subtracted its value.
[00:25:00.160 --> 00:25:03.720]   And all those operations are recorded here in this computational graph.
[00:25:03.720 --> 00:25:10.320]   And you can see that this input to log softmax, add backward 0 here,
[00:25:10.320 --> 00:25:12.880]   both passes through this graph in two ways.
[00:25:12.880 --> 00:25:16.280]   It both goes into this subtraction operation.
[00:25:16.280 --> 00:25:19.520]   And it goes through that series of X sum log.
[00:25:19.520 --> 00:25:23.640]   So you can see all the operations that we did on the tensor--
[00:25:23.640 --> 00:25:30.560]   on all the tensors in our network, our humble small little network,
[00:25:30.560 --> 00:25:34.560]   are recorded here.
[00:25:34.560 --> 00:25:37.600]   So this is a big part of what PyTorch is doing,
[00:25:37.600 --> 00:25:41.520]   is tracking this stuff for us so that we don't have to do it ourselves
[00:25:41.520 --> 00:25:43.480]   and say, these are the gradients.
[00:25:43.480 --> 00:25:47.520]   And this is the derivative of this operation,
[00:25:47.520 --> 00:25:49.680]   and all that kind of stuff.
[00:25:49.680 --> 00:25:51.680]   Calculus is automatable.
[00:25:51.680 --> 00:25:52.800]   And PyTorch automates it.
[00:25:52.800 --> 00:26:02.440]   So one last thing that we need to do--
[00:26:02.440 --> 00:26:05.120]   I think this is the absolute last thing--
[00:26:05.120 --> 00:26:07.040]   we need a loss function.
[00:26:07.040 --> 00:26:10.280]   And the loss function that people use is the negative log likelihood,
[00:26:10.280 --> 00:26:12.400]   AKA the surprise.
[00:26:12.400 --> 00:26:14.880]   And it's calculated in this particular way.
[00:26:14.880 --> 00:26:18.720]   There's a hack for calculating it with integer labels here.
[00:26:18.720 --> 00:26:20.880]   But it's based-- it's an entropy loss.
[00:26:20.880 --> 00:26:24.840]   It's like this model, it said there was a really low probability
[00:26:24.840 --> 00:26:27.800]   of this digit being a 1, but it actually was a 1.
[00:26:27.800 --> 00:26:32.080]   And so we want to punish the model the more--
[00:26:32.080 --> 00:26:39.280]   the lower probability it assigns to the correct label.
[00:26:39.280 --> 00:26:43.320]   So now not only can we run something through the network,
[00:26:43.320 --> 00:26:47.000]   but we can also calculate a loss.
[00:26:47.000 --> 00:26:48.080]   So now we're running--
[00:26:48.080 --> 00:26:51.960]   we have a batch of x's that gave us our predictions.
[00:26:51.960 --> 00:26:56.960]   Now we're calculating the loss on the combination of those predictions
[00:26:56.960 --> 00:26:57.960]   and the true label.
[00:26:57.960 --> 00:27:02.120]   So that's loss func that we defined here, that negative log likelihood.
[00:27:02.120 --> 00:27:09.800]   And we're taking the loss with our predictions and the labels.
[00:27:09.800 --> 00:27:11.880]   So this is where we're starting at.
[00:27:11.880 --> 00:27:13.960]   And it's about 2.3.
[00:27:13.960 --> 00:27:16.160]   That happens to be the log of 10.
[00:27:16.160 --> 00:27:20.960]   So this is basically the loss you would get from guessing randomly
[00:27:20.960 --> 00:27:30.360]   or always guessing the same digit every single time.
[00:27:30.360 --> 00:27:32.480]   You would get, I think, the same answer here.
[00:27:32.480 --> 00:27:40.360]   It's the-- it's one digit in nats, if you know your information theory.
[00:27:40.360 --> 00:27:46.920]   That's where this starting loss value actually comes from, this value of 2.3.
[00:27:46.920 --> 00:27:50.720]   So I'm comparing it against log 10 there.
[00:27:50.720 --> 00:27:53.280]   All right, so we're almost done.
[00:27:53.280 --> 00:27:58.080]   We also-- so I just gave you a breakdown of where that cross-entropy loss value
[00:27:58.080 --> 00:27:59.600]   came from, how to interpret it.
[00:27:59.600 --> 00:28:01.920]   But it's maybe easier to think in terms of accuracy
[00:28:01.920 --> 00:28:05.200]   if we want to know how well our model is doing.
[00:28:05.200 --> 00:28:08.040]   So accuracy, we just want to--
[00:28:08.040 --> 00:28:11.840]   there are tools for calculating this in Fast.ai, in PyTorch Lightning,
[00:28:11.840 --> 00:28:13.760]   and Torch Metrics.
[00:28:13.760 --> 00:28:19.400]   But we're doing this in pure PyTorch, so let's calculate it ourselves.
[00:28:19.400 --> 00:28:23.760]   We check whether-- from our predictions, we grab the highest probability.
[00:28:23.760 --> 00:28:24.960]   What was the high--
[00:28:24.960 --> 00:28:28.920]   to which value did our model assign the highest probability?
[00:28:28.920 --> 00:28:32.000]   Was that index-- is that index the right index?
[00:28:32.000 --> 00:28:36.760]   Is it equal to the targets?
[00:28:36.760 --> 00:28:42.440]   Then we take the average of this Boolean tensor
[00:28:42.440 --> 00:28:46.680]   here that's saying, are our predictions the same as the labels?
[00:28:47.600 --> 00:28:50.040]   All right, so then we can check our starting accuracy.
[00:28:50.040 --> 00:28:52.840]   We're actually doing a little bit better than chance here, 15%,
[00:28:52.840 --> 00:28:54.640]   rather than 10%.
[00:28:54.640 --> 00:28:59.520]   But we're not doing super great.
[00:28:59.520 --> 00:29:06.640]   You can easily get 95, 99, nearly 100% performance on MNIST.
[00:29:06.640 --> 00:29:10.280]   So we're now ready to run a training loop.
[00:29:10.280 --> 00:29:14.360]   So we needed the loss and the accuracy, and then we're ready to go.
[00:29:14.360 --> 00:29:18.920]   So we've put all the pieces in place for doing a--
[00:29:18.920 --> 00:29:22.920]   for training our neural network.
[00:29:22.920 --> 00:29:25.240]   So we're going to-- what do we need?
[00:29:25.240 --> 00:29:29.000]   We need data to give to our model to make predictions.
[00:29:29.000 --> 00:29:30.120]   We need a loss.
[00:29:30.120 --> 00:29:32.000]   So how well did our model do?
[00:29:32.000 --> 00:29:34.560]   And then we need to calculate the gradients.
[00:29:34.560 --> 00:29:36.480]   We do that with .backward.
[00:29:36.480 --> 00:29:39.320]   And then we update the parameters of the model using the gradients.
[00:29:39.320 --> 00:29:41.880]   So the rule here is the gradient descent rule.
[00:29:41.880 --> 00:29:44.160]   We take the gradients, and we basically
[00:29:44.160 --> 00:29:50.160]   subtract the gradient scaled from the current value of the tensor.
[00:29:50.160 --> 00:29:53.360]   So again, the math for machine learning
[00:29:53.360 --> 00:29:59.760]   will tell you a little bit more about where those things come from.
[00:29:59.760 --> 00:30:02.400]   So let me-- I'm going to zoom out again a little bit,
[00:30:02.400 --> 00:30:04.640]   just so we can see all of the code.
[00:30:04.640 --> 00:30:06.840]   I think I can get away with that.
[00:30:06.840 --> 00:30:09.840]   So what are we doing?
[00:30:09.840 --> 00:30:12.000]   What are we doing?
[00:30:12.000 --> 00:30:15.120]   We're going to range over our data multiple times.
[00:30:15.120 --> 00:30:18.960]   The number of times we do that is controlled by this for loop here.
[00:30:18.960 --> 00:30:22.560]   Each instance of that for loop, each iteration of it, is called an epoch.
[00:30:22.560 --> 00:30:24.560]   That's one loop over all of our data.
[00:30:24.560 --> 00:30:28.400]   And we loop over our data bit by bit, batch by batch.
[00:30:28.400 --> 00:30:30.480]   So that's the second for loop there.
[00:30:30.480 --> 00:30:36.080]   And we're pulling out the values in x and the values in y,
[00:30:36.080 --> 00:30:37.920]   passing the x values through our model,
[00:30:37.920 --> 00:30:42.000]   comparing them to the predictions with the loss,
[00:30:42.000 --> 00:30:49.560]   and then calling loss.backward to calculate those gradients.
[00:30:49.560 --> 00:30:51.840]   PyTorch is ready to calculate them.
[00:30:51.840 --> 00:30:55.840]   Like, whenever it's doing operations with tensors that require grad,
[00:30:55.840 --> 00:30:56.760]   it's tracking this.
[00:30:56.760 --> 00:30:59.080]   It's tracking everything it needs to calculate it.
[00:30:59.080 --> 00:31:00.120]   But it's lazy.
[00:31:00.120 --> 00:31:04.280]   It's waiting for the moment when you're going to ask for that gradient.
[00:31:04.280 --> 00:31:06.520]   And that's loss.backward.
[00:31:06.520 --> 00:31:10.320]   And then we want to apply those gradients.
[00:31:10.320 --> 00:31:14.360]   And that's this width block here.
[00:31:14.360 --> 00:31:16.800]   So if you've used PyTorch before to train a model,
[00:31:16.800 --> 00:31:19.320]   some of this might actually look somewhat unfamiliar to you
[00:31:19.320 --> 00:31:22.960]   because you've probably used things from Torch.nn and Torch.optim.
[00:31:22.960 --> 00:31:27.560]   And in that case, you don't have to do all this stuff by hand.
[00:31:27.560 --> 00:31:30.440]   But what we're doing here in this last width block
[00:31:30.440 --> 00:31:34.600]   is we're saying, let's update those parameters.
[00:31:34.600 --> 00:31:37.800]   Let's take the weights and subtract the gradient from them.
[00:31:37.800 --> 00:31:39.440]   That's what this minus equal does.
[00:31:39.440 --> 00:31:41.080]   It's an in-place subtraction.
[00:31:41.080 --> 00:31:45.520]   Let's take the gradient of the weights, weights.grad here,
[00:31:45.520 --> 00:31:48.080]   and subtract it from the weights.
[00:31:48.080 --> 00:31:52.120]   We scale it with this learning rate here,
[00:31:52.120 --> 00:31:55.480]   which helps us control some of the numerical properties here.
[00:31:55.480 --> 00:32:03.200]   Maybe I won't go into the physics analogy here.
[00:32:03.200 --> 00:32:06.480]   But let's just say that sometimes the gradient may be really big,
[00:32:06.480 --> 00:32:08.800]   even though the parameters are really small,
[00:32:08.800 --> 00:32:12.640]   and you don't want to make a huge change to the parameters.
[00:32:12.640 --> 00:32:16.120]   And the learning rate helps us control that.
[00:32:16.120 --> 00:32:19.240]   So that's a little magic number that you need to pick.
[00:32:19.240 --> 00:32:22.440]   And I think you can also try and set it programmatically.
[00:32:22.440 --> 00:32:24.040]   There's ways of doing that as well.
[00:32:24.040 --> 00:32:27.760]   But we're just going to set a little fixed value.
[00:32:27.760 --> 00:32:30.960]   Then the last thing that we need to do, this dot backward,
[00:32:30.960 --> 00:32:35.800]   a funny thing about it is it actually accumulates gradients
[00:32:35.800 --> 00:32:38.400]   over time, which is what allows people to do more complicated
[00:32:38.400 --> 00:32:42.200]   things with PyTorch than just this simple forward-backward
[00:32:42.200 --> 00:32:45.280]   update kind of style that neural networks use,
[00:32:45.280 --> 00:32:49.960]   or the mainstream neural networks use.
[00:32:49.960 --> 00:32:53.200]   So because of that, we have to do this last operation here.
[00:32:53.200 --> 00:32:54.840]   We take the weights.
[00:32:54.840 --> 00:32:55.960]   We grab their gradient.
[00:32:55.960 --> 00:32:57.200]   And we set it to 0.
[00:32:57.200 --> 00:32:58.740]   So we zero it out.
[00:32:58.740 --> 00:33:02.720]   We get rid of the values that have accumulated into it.
[00:33:02.720 --> 00:33:04.120]   And we do that in place.
[00:33:04.120 --> 00:33:05.480]   That's what this underscore means.
[00:33:05.480 --> 00:33:07.740]   Just like this minus equals here means
[00:33:07.740 --> 00:33:09.440]   we do a subtraction in place, here,
[00:33:09.440 --> 00:33:11.760]   this underscore in PyTorch means we
[00:33:11.760 --> 00:33:15.960]   do this replacement of the values in place
[00:33:15.960 --> 00:33:17.800]   without making a new tensor.
[00:33:17.800 --> 00:33:20.080]   So we set the values to 0, which means we can then
[00:33:20.080 --> 00:33:24.920]   start this loop over again, sort of like reset ourselves
[00:33:24.920 --> 00:33:28.040]   and do this over again.
[00:33:28.040 --> 00:33:31.040]   You'll also-- this parameter update thing
[00:33:31.040 --> 00:33:32.480]   is inside the width block.
[00:33:32.480 --> 00:33:34.940]   The width block of torch.nograd says,
[00:33:34.940 --> 00:33:37.840]   don't track these operations for gradients.
[00:33:37.840 --> 00:33:39.380]   You would only need to do that if you
[00:33:39.380 --> 00:33:41.520]   were trying to do gradient descent on your gradient
[00:33:41.520 --> 00:33:43.000]   descent, if you were trying to take
[00:33:43.000 --> 00:33:45.440]   the gradient of your optimization process, which
[00:33:45.440 --> 00:33:51.400]   is possible but tricky, that sort of meta approach, which
[00:33:51.400 --> 00:33:55.560]   you could use to, say, optimize your learning rate over time.
[00:33:55.560 --> 00:33:59.840]   But we won't be doing that, so we put it inside a nograd.
[00:33:59.840 --> 00:34:02.840]   Right, so most of these notes here from Jeremy's tutorial
[00:34:02.840 --> 00:34:05.640]   are things we've already talked about.
[00:34:05.640 --> 00:34:07.480]   Yeah, lost it backward adds the gradients
[00:34:07.480 --> 00:34:10.640]   to whatever is already stored rather than replacing them.
[00:34:10.640 --> 00:34:12.800]   And yeah, there's a little bit more detail.
[00:34:12.800 --> 00:34:15.960]   If you have the link to the Colab that's
[00:34:15.960 --> 00:34:18.160]   floating by my face there, you can
[00:34:18.160 --> 00:34:22.480]   get these links to things like more information
[00:34:22.480 --> 00:34:23.680]   about PyTorch autograd.
[00:34:24.680 --> 00:34:27.800]   All right, so we've done it.
[00:34:27.800 --> 00:34:31.200]   We've done our from scratch implementation here.
[00:34:31.200 --> 00:34:35.600]   So actually, let me go ahead and update my to-do list here.
[00:34:35.600 --> 00:34:36.480]   Boom.
[00:34:36.480 --> 00:34:42.720]   We have completed our from scratch neural network,
[00:34:42.720 --> 00:34:47.600]   and now we're going to move on to refactoring.
[00:34:47.600 --> 00:34:49.280]   Once, of course, we check to make sure
[00:34:49.280 --> 00:34:51.600]   that our code actually worked, how
[00:34:51.600 --> 00:34:54.120]   do you check whether machine learning code actually worked?
[00:34:54.120 --> 00:34:55.560]   That's a really tough question.
[00:34:55.560 --> 00:34:57.600]   But the biggest-- the sort of starting thing
[00:34:57.600 --> 00:34:59.360]   is does the loss go down?
[00:34:59.360 --> 00:35:02.240]   Does the metric that I care about go up?
[00:35:02.240 --> 00:35:03.800]   So the loss indeed goes down.
[00:35:03.800 --> 00:35:06.320]   If I compare the starting loss to the final loss,
[00:35:06.320 --> 00:35:13.600]   we started off at about one digit, 2.3 cross entropy,
[00:35:13.600 --> 00:35:16.080]   and now we're much closer to zero cross entropy.
[00:35:16.080 --> 00:35:17.920]   That's what you want to see.
[00:35:17.920 --> 00:35:25.400]   And then our accuracy went from about 10% on this batch
[00:35:25.400 --> 00:35:27.200]   to 100%.
[00:35:27.200 --> 00:35:31.320]   So our loss has decreased, and our accuracy has increased.
[00:35:31.320 --> 00:35:35.560]   So it's a good kind of correctness check, sanity
[00:35:35.560 --> 00:35:38.880]   check, if you will, to make sure these things happen
[00:35:38.880 --> 00:35:41.840]   in the neural network code that you write,
[00:35:41.840 --> 00:35:43.680]   in the machine learning code that you write.
[00:35:43.680 --> 00:35:47.240]   These are sort of like the way you would unit test
[00:35:47.240 --> 00:35:51.560]   your machine learning code.
[00:35:51.560 --> 00:35:54.640]   All right, so now it's time to start our refactoring.
[00:35:54.640 --> 00:35:59.080]   So we built-- we're not really for a long time, actually.
[00:35:59.080 --> 00:36:01.200]   We're not going to add any new features to what's
[00:36:01.200 --> 00:36:03.200]   happening in our code.
[00:36:03.200 --> 00:36:05.560]   For a good chunk of the rest of this stream,
[00:36:05.560 --> 00:36:09.640]   we're just going to be doing some refactoring here,
[00:36:09.640 --> 00:36:13.480]   which is-- refactoring is right when you replace code
[00:36:13.480 --> 00:36:16.360]   with different code that does the same thing.
[00:36:16.360 --> 00:36:18.320]   So the behavior of the program doesn't change,
[00:36:18.320 --> 00:36:20.120]   but the way it's written changes.
[00:36:20.120 --> 00:36:23.120]   And that can make it easier to make it do other stuff,
[00:36:23.120 --> 00:36:25.480]   or it can make it easier for humans to read it.
[00:36:25.480 --> 00:36:27.200]   And we're going to do both of those things
[00:36:27.200 --> 00:36:30.240]   as we do our refactor here.
[00:36:30.240 --> 00:36:33.280]   So this is how we're going to learn what PyTorch.nn really
[00:36:33.280 --> 00:36:35.280]   is, because we're going to see how
[00:36:35.280 --> 00:36:37.760]   it takes the code that we wrote in raw PyTorch
[00:36:37.760 --> 00:36:40.960]   and makes it better.
[00:36:40.960 --> 00:36:43.920]   So our goal here at each step should
[00:36:43.920 --> 00:36:47.080]   be to make our code shorter, more understandable,
[00:36:47.080 --> 00:36:50.880]   and/or more flexible at each step.
[00:36:50.880 --> 00:36:54.920]   So the first and easiest step is to just get rid
[00:36:54.920 --> 00:36:57.840]   of some of those handwritten activation and loss functions
[00:36:57.840 --> 00:37:01.640]   with things from torch.nn.functional,
[00:37:01.640 --> 00:37:05.680]   which is the component of PyTorch that
[00:37:05.680 --> 00:37:08.240]   mostly is full of functions.
[00:37:08.240 --> 00:37:11.040]   So if you look here, I've printed out
[00:37:11.040 --> 00:37:15.280]   the elements of torch.nn.functional here.
[00:37:15.280 --> 00:37:19.480]   So these are all of the things inside torch.nn.functional.
[00:37:19.480 --> 00:37:22.760]   You can see that most of them have lowercase letters
[00:37:22.760 --> 00:37:26.240]   at the front, which is sort of Python standard style
[00:37:26.240 --> 00:37:28.760]   to indicate that something is a function.
[00:37:28.760 --> 00:37:33.160]   And in fact, let me do a little bit of a wild thing here.
[00:37:33.160 --> 00:37:39.560]   Let's check what these things are.
[00:37:39.560 --> 00:37:42.000]   Are these things all functions?
[00:37:42.000 --> 00:37:45.880]   Are these things all callables here in F?
[00:37:45.880 --> 00:37:48.840]   And so that's what I'm doing with this little for loop here.
[00:37:48.840 --> 00:37:50.920]   I'm looping through everything in torch.nn.functional
[00:37:50.920 --> 00:37:53.240]   and checking whether it's something that you can call,
[00:37:53.240 --> 00:37:54.960]   whether it is a function.
[00:37:54.960 --> 00:37:57.840]   And the answer is, for almost all of them, yes.
[00:37:57.840 --> 00:38:01.280]   So these elements of torch.nn.functional,
[00:38:01.280 --> 00:38:07.280]   aliased usually as just F, are functions primarily.
[00:38:07.280 --> 00:38:12.120]   So functions in the pure sense, functions with no state.
[00:38:12.120 --> 00:38:15.480]   So the other parts of the library mainly contain classes,
[00:38:15.480 --> 00:38:17.520]   contain definitions for objects, whereas these
[00:38:17.520 --> 00:38:19.600]   are definitions for functions.
[00:38:19.600 --> 00:38:22.440]   So you also-- there are a bunch of nice functions
[00:38:22.440 --> 00:38:24.900]   that we need in neural networks, like pooling functions,
[00:38:24.900 --> 00:38:26.680]   in addition to loss and activations.
[00:38:26.680 --> 00:38:28.520]   There's also functions for doing things
[00:38:28.520 --> 00:38:32.080]   like convolutions in linear operations.
[00:38:32.080 --> 00:38:34.720]   But it's better to use other parts of the library
[00:38:34.720 --> 00:38:36.800]   for those layers of the neural network,
[00:38:36.800 --> 00:38:42.080]   as we'll see, I think, in the next section.
[00:38:42.080 --> 00:38:45.560]   So we can drop both the negative log likelihood loss
[00:38:45.560 --> 00:38:49.760]   and the log softmax activation, because PyTorch puts those
[00:38:49.760 --> 00:38:51.920]   together into a single function, cross entropy
[00:38:51.920 --> 00:38:53.720]   that combines the two.
[00:38:53.720 --> 00:38:56.120]   This makes the operation more numerically stable,
[00:38:56.120 --> 00:39:01.440]   less prone to error with really extreme values.
[00:39:01.440 --> 00:39:03.320]   So we can-- that means we actually
[00:39:03.320 --> 00:39:06.560]   can just drop the activation function from our model
[00:39:06.560 --> 00:39:09.080]   and just write loss func is F dot cross entropy.
[00:39:09.080 --> 00:39:12.080]   And now our model is just this linear component here.
[00:39:12.080 --> 00:39:17.160]   Take the input, matrix, multiply the weights, add the biases.
[00:39:17.160 --> 00:39:21.080]   Let's check and see if we still get the same loss
[00:39:21.080 --> 00:39:22.200]   and accuracies before.
[00:39:22.200 --> 00:39:25.780]   Yep, we still get that low loss and that accuracy
[00:39:25.780 --> 00:39:28.480]   of 100% on this particular batch.
[00:39:28.480 --> 00:39:29.560]   So we've refactored.
[00:39:29.560 --> 00:39:31.520]   We've changed the way the code is written,
[00:39:31.520 --> 00:39:34.560]   but we have not changed what it does.
[00:39:34.560 --> 00:39:35.840]   And this is a simplification.
[00:39:35.840 --> 00:39:37.840]   We've pulled the log softmax out of the model.
[00:39:37.840 --> 00:39:40.120]   We no longer have our own implementation
[00:39:40.120 --> 00:39:45.800]   of that log softmax with that x sum log.
[00:39:45.800 --> 00:39:48.920]   So that's good.
[00:39:48.920 --> 00:39:50.280]   We've simplified our model.
[00:39:50.280 --> 00:39:52.040]   And we take advantage of the fact
[00:39:52.040 --> 00:39:53.800]   that the operations that we're doing
[00:39:53.800 --> 00:39:58.180]   are common operations in neural networks,
[00:39:58.180 --> 00:40:00.980]   and so they're in PyTorch already.
[00:40:00.980 --> 00:40:03.460]   And somebody's written a really nice implementation of them
[00:40:03.460 --> 00:40:07.820]   that, say, doesn't break on really extreme values.
[00:40:07.820 --> 00:40:11.820]   So we can take advantage of the hard work of the folks
[00:40:11.820 --> 00:40:18.140]   at PyTorch and in the community that contributes to PyTorch
[00:40:18.140 --> 00:40:21.380]   that they've made these high-quality implementations
[00:40:21.380 --> 00:40:23.500]   of all the things we need for our neural networks.
[00:40:23.500 --> 00:40:25.160]   So that one was pretty straightforward.
[00:40:25.160 --> 00:40:27.440]   All right, let's update our to-do list.
[00:40:27.440 --> 00:40:28.300]   Boom.
[00:40:28.300 --> 00:40:34.200]   And we now move on to adding some modules.
[00:40:34.200 --> 00:40:37.540]   So let's refactor using nn.module.
[00:40:37.540 --> 00:40:41.080]   So this is a slightly bigger refactor than the last one.
[00:40:41.080 --> 00:40:44.000]   nn.module and nn.parameter are the things
[00:40:44.000 --> 00:40:46.280]   we're going to use to clean up our training loop.
[00:40:46.280 --> 00:40:50.620]   What these do is they kind of wrap our tensors
[00:40:50.620 --> 00:40:55.180]   and track tensors that are going to be used not as the inputs
[00:40:55.180 --> 00:40:57.580]   or the things being passed around inside a neural network,
[00:40:57.580 --> 00:41:00.220]   but are used as the parameters of a neural network.
[00:41:00.220 --> 00:41:02.260]   That's what nn.parameter does.
[00:41:02.260 --> 00:41:04.300]   Gives us convenience functions for those.
[00:41:04.300 --> 00:41:07.900]   And the modules will sort of wrap up this idea of,
[00:41:07.900 --> 00:41:09.500]   I have things that I want to apply
[00:41:09.500 --> 00:41:13.060]   to inputs that maybe have lots of pieces or components
[00:41:13.060 --> 00:41:14.180]   to them.
[00:41:14.180 --> 00:41:17.900]   So these are inside Torch.nn.
[00:41:17.900 --> 00:41:23.060]   nn.module is a class that we subclass
[00:41:23.060 --> 00:41:25.380]   when we make new components for our neural network.
[00:41:25.380 --> 00:41:28.300]   So an nn.module is this generic class
[00:41:28.300 --> 00:41:31.460]   that has a bunch of attributes and methods
[00:41:31.460 --> 00:41:34.820]   that we'll be using either explicitly or under the hood.
[00:41:34.820 --> 00:41:37.700]   I'll try and point out the under the hood cases when we go by them.
[00:41:37.700 --> 00:41:40.660]   So it has a convenience function for pulling out
[00:41:40.660 --> 00:41:43.660]   all the tensors that require grad that are attached to it,
[00:41:43.660 --> 00:41:45.100]   dot parameters.
[00:41:45.100 --> 00:41:46.980]   It is a convenience function for zeroing
[00:41:46.980 --> 00:41:49.220]   the gradients of all of those parameters all at once.
[00:41:49.220 --> 00:41:53.100]   So .0 grad, so we don't have to do it ourselves.
[00:41:53.100 --> 00:41:58.020]   So this general nn.module is sort of the way that in--
[00:41:58.020 --> 00:42:02.580]   I like to think of it as a way to have functions that also
[00:42:02.580 --> 00:42:04.020]   have state.
[00:42:04.020 --> 00:42:05.700]   So normally, when you write a function,
[00:42:05.700 --> 00:42:07.420]   it doesn't really have state to it.
[00:42:07.420 --> 00:42:09.300]   It doesn't accumulate information over time.
[00:42:09.300 --> 00:42:14.380]   It just has its source code, and it operates on inputs.
[00:42:14.380 --> 00:42:17.580]   But with a neural network, our neural network
[00:42:17.580 --> 00:42:19.220]   is a function in some sense.
[00:42:19.220 --> 00:42:21.300]   It takes in inputs and returns outputs,
[00:42:21.300 --> 00:42:23.020]   but it also needs to keep track of state.
[00:42:23.020 --> 00:42:26.020]   It needs to know what its parameters values are.
[00:42:26.020 --> 00:42:27.700]   And that's what nn.module does.
[00:42:27.700 --> 00:42:31.740]   It tracks those parameters, that state.
[00:42:31.740 --> 00:42:33.140]   So we want to have something that's
[00:42:33.140 --> 00:42:36.540]   kind of a hybrid of a function and an object,
[00:42:36.540 --> 00:42:40.180]   and that's what nn.module is, stateful functions.
[00:42:40.180 --> 00:42:42.460]   So here is our example here.
[00:42:42.460 --> 00:42:46.500]   Our MNIST logistic model, which has weights and biases.
[00:42:46.500 --> 00:42:53.180]   These are nn.parameters, so tensors that require grad.
[00:42:53.180 --> 00:42:57.340]   And we assign them at the top level of our module.
[00:42:57.340 --> 00:42:58.780]   And then the module knows--
[00:42:58.780 --> 00:43:02.220]   the convenient thing here is that any attribute
[00:43:02.220 --> 00:43:04.780]   of the module that's a parameter will
[00:43:04.780 --> 00:43:08.740]   be treated specially by PyTorch for a couple of things,
[00:43:08.740 --> 00:43:12.180]   like that .parameters method and that .0grad method as well,
[00:43:12.180 --> 00:43:14.380]   I believe.
[00:43:14.380 --> 00:43:17.900]   So if we call--
[00:43:17.900 --> 00:43:19.660]   we can define our module here.
[00:43:19.660 --> 00:43:21.860]   Now, instead of-- we used to define a function.
[00:43:21.860 --> 00:43:26.140]   Now we define a method called forward.
[00:43:26.140 --> 00:43:28.020]   Oh, yeah, one quick note.
[00:43:28.020 --> 00:43:31.340]   nn.module, PyTorch-specific concept.
[00:43:31.340 --> 00:43:33.820]   We'll use-- it's a very important class in PyTorch,
[00:43:33.820 --> 00:43:36.380]   maybe the most important after tensor.
[00:43:36.380 --> 00:43:38.660]   So the most important class in Torch.nn
[00:43:38.660 --> 00:43:41.500]   is definitely nn.module.
[00:43:41.500 --> 00:43:45.340]   This is different, totally distinct from a Python module,
[00:43:45.340 --> 00:43:48.540]   lowercase m, a file of Python code that could be imported.
[00:43:48.540 --> 00:43:53.300]   These are different things, same word, unfortunately.
[00:43:53.300 --> 00:43:56.220]   So now we're using an object instead of using a function.
[00:43:56.220 --> 00:43:58.020]   So we've got to instantiate our model.
[00:43:58.020 --> 00:44:01.340]   So mnist logistic was that class that we just defined.
[00:44:01.340 --> 00:44:04.220]   We instantiate it.
[00:44:04.220 --> 00:44:08.300]   But now we can still calculate the loss in the exact same way.
[00:44:08.300 --> 00:44:12.540]   nn.module objects are used as if they are functions.
[00:44:12.540 --> 00:44:17.900]   So I literally just parentheses around input still.
[00:44:17.900 --> 00:44:20.260]   So it looks just like I'm calling a function.
[00:44:20.260 --> 00:44:28.020]   But in this case, I'm using this module object.
[00:44:28.020 --> 00:44:29.220]   All right, one second here.
[00:44:29.220 --> 00:44:32.180]   I think I'm having a little bit of a video glitch.
[00:44:32.180 --> 00:44:32.980]   One second.
[00:44:32.980 --> 00:44:34.180]   Boop, boop.
[00:44:34.180 --> 00:44:43.500]   All right, let me know if you see anything strange going on
[00:44:43.500 --> 00:44:46.820]   with the video there.
[00:44:46.820 --> 00:44:51.220]   I'm going to keep going for a little bit just to see.
[00:44:51.220 --> 00:44:55.900]   But the upshot is we apply inputs to our--
[00:44:55.900 --> 00:44:59.940]   we give our model inputs just as though it were a function.
[00:44:59.940 --> 00:45:04.020]   But it's actually an object.
[00:45:04.020 --> 00:45:07.700]   Under the hood, PyTorch is calling that forward method
[00:45:07.700 --> 00:45:12.140]   whenever I give an input to my model.
[00:45:12.140 --> 00:45:14.660]   So we're getting-- this is now the exact same thing
[00:45:14.660 --> 00:45:16.060]   that we had before.
[00:45:16.060 --> 00:45:19.100]   But now we have this object, this class,
[00:45:19.100 --> 00:45:22.100]   that can track our state for us.
[00:45:22.100 --> 00:45:28.620]   So the big improvement is to this component of our training
[00:45:28.620 --> 00:45:33.700]   loop to the part where we applied the gradients.
[00:45:33.700 --> 00:45:37.460]   We had to manually update the values for each parameter
[00:45:37.460 --> 00:45:41.180]   by name, manually zero them out with code
[00:45:41.180 --> 00:45:43.340]   that looked like this here.
[00:45:43.340 --> 00:45:47.300]   And now we can take advantage of the fact
[00:45:47.300 --> 00:45:54.060]   that the module knows about the parameters attached to it,
[00:45:54.060 --> 00:45:57.060]   and can do things like loop over them
[00:45:57.060 --> 00:45:59.340]   and zero out their gradients.
[00:45:59.340 --> 00:46:02.540]   So we still have to do it in a withTorch.noGrad block.
[00:46:02.540 --> 00:46:05.980]   But now we're looping over the parameters with a for loop.
[00:46:05.980 --> 00:46:10.380]   And instead of just writing out all the parameters, which
[00:46:10.380 --> 00:46:13.100]   would be a real pain if we had a more complicated model,
[00:46:13.100 --> 00:46:15.300]   and we just zero the gradients of everything
[00:46:15.300 --> 00:46:18.860]   at once with .zeroGrad.
[00:46:18.860 --> 00:46:26.900]   So let's take a look at how that changes our code.
[00:46:26.900 --> 00:46:31.740]   So the fit function--
[00:46:31.740 --> 00:46:34.180]   we're now taking that loop that we wrote previously
[00:46:34.180 --> 00:46:38.140]   and turning it into a function now, this fit function here.
[00:46:38.140 --> 00:46:43.180]   We're looping over the data in our inner for loop.
[00:46:43.180 --> 00:46:45.460]   We do that multiple times in our outer for loop.
[00:46:45.460 --> 00:46:47.660]   The big change here is now that our model
[00:46:47.660 --> 00:46:54.620]   is a Torch module instead of just some functions using
[00:46:54.620 --> 00:47:00.020]   Torch, we have this nice, clean, much cleaner application
[00:47:00.020 --> 00:47:03.060]   of gradient descent down here at the bottom,
[00:47:03.060 --> 00:47:07.380]   with no need to mention that we just have weights and biases,
[00:47:07.380 --> 00:47:11.020]   and to know the names of those tensors.
[00:47:11.020 --> 00:47:12.620]   So nice and clean.
[00:47:12.620 --> 00:47:15.100]   The fit function runs all as well.
[00:47:15.100 --> 00:47:16.900]   We can check that we didn't break anything.
[00:47:16.900 --> 00:47:19.100]   Again, our loss goes down.
[00:47:19.100 --> 00:47:20.300]   Good news.
[00:47:20.300 --> 00:47:24.180]   We haven't broken anything with our refactor.
[00:47:24.180 --> 00:47:24.820]   All right.
[00:47:24.820 --> 00:47:28.420]   So that takes us through--
[00:47:28.420 --> 00:47:31.900]   oh, no, we have a little bit more refactoring using modules.
[00:47:31.900 --> 00:47:37.980]   So the Torch.nn.modules, there are
[00:47:37.980 --> 00:47:41.820]   lots of examples of them that are very, very common modules.
[00:47:41.820 --> 00:47:47.220]   So one, we manually defined our linear layer
[00:47:47.220 --> 00:47:50.260]   of our neural network, our logistic regression.
[00:47:50.260 --> 00:47:53.060]   We took the inputs, matrix multiplied them,
[00:47:53.060 --> 00:47:59.700]   and added some values with xb at self.weights plus self.biases.
[00:47:59.700 --> 00:48:01.380]   But PyTorch, actually, that's something
[00:48:01.380 --> 00:48:03.260]   people do all the time in neural networks.
[00:48:03.260 --> 00:48:05.060]   And so this is built into PyTorch
[00:48:05.060 --> 00:48:10.140]   so that you are using the same code, the same implementation
[00:48:10.140 --> 00:48:12.620]   as everybody else, and can take advantage of improvements
[00:48:12.620 --> 00:48:14.500]   as they come along.
[00:48:14.500 --> 00:48:19.420]   So nn.linear here, we can read the docs for this a little bit
[00:48:19.420 --> 00:48:21.420]   if we want to learn a little bit more about it.
[00:48:21.420 --> 00:48:23.580]   This is doing that linear transformation
[00:48:23.580 --> 00:48:27.620]   to incoming data, y equals xa transpose plus b,
[00:48:27.620 --> 00:48:31.900]   just like that at self.weights plus self.biases.
[00:48:31.900 --> 00:48:37.980]   So there's lots and lots of these predefined layers
[00:48:37.980 --> 00:48:39.380]   that can really simplify our code
[00:48:39.380 --> 00:48:41.180]   and can make it faster because they maybe
[00:48:41.180 --> 00:48:44.140]   know little details about how to call things in PyTorch
[00:48:44.140 --> 00:48:46.380]   to make it run better.
[00:48:46.380 --> 00:48:49.060]   So-- boop-- that should be--
[00:48:49.060 --> 00:48:50.540]   yeah, need to zoom out a little bit
[00:48:50.540 --> 00:48:52.340]   to be able to see it all here.
[00:48:52.340 --> 00:48:55.260]   But now our MNIST logistic is even simpler.
[00:48:55.260 --> 00:48:57.980]   We just define that it has a linear layer.
[00:48:57.980 --> 00:49:00.460]   So we instantiate it, just like we instantiated the weights
[00:49:00.460 --> 00:49:03.460]   and biases when we initialized our network.
[00:49:03.460 --> 00:49:06.100]   We need to instantiate this nn.linear module
[00:49:06.100 --> 00:49:09.260]   when we create our network.
[00:49:09.260 --> 00:49:13.460]   But then, because it's a module, it has its own forward method.
[00:49:13.460 --> 00:49:16.100]   So we just call the module on its inputs.
[00:49:16.100 --> 00:49:17.700]   So there is this kind of cool thing
[00:49:17.700 --> 00:49:22.100]   where the components of a torch.nn.module
[00:49:22.100 --> 00:49:24.840]   are additional torch.nn.modules.
[00:49:24.840 --> 00:49:27.620]   It's kind of like a recursive data structure.
[00:49:27.620 --> 00:49:32.140]   Modules contain modules, just like lists can contain lists.
[00:49:32.140 --> 00:49:36.300]   And an array or a tensor is kind of like a list of lists.
[00:49:36.300 --> 00:49:41.020]   And so models are actually modules composed of modules.
[00:49:41.020 --> 00:49:44.220]   So that's a cool feature about the design of PyTorch
[00:49:44.220 --> 00:49:47.740]   that's much nicer than I think most people wouldn't
[00:49:47.740 --> 00:49:50.580]   think to design it that way if they were building
[00:49:50.580 --> 00:49:52.980]   their own neural network library.
[00:49:52.980 --> 00:49:55.100]   So that's a neat little feature.
[00:49:55.100 --> 00:49:57.660]   And so we have our linear layer here
[00:49:57.660 --> 00:50:02.220]   that is a module attached to our overall model.
[00:50:02.220 --> 00:50:04.940]   So we can instantiate our model and calculate the loss
[00:50:04.940 --> 00:50:06.540]   just the same way we did before.
[00:50:06.540 --> 00:50:07.540]   Nothing has changed here.
[00:50:07.540 --> 00:50:09.580]   Again, we're just refactoring.
[00:50:09.580 --> 00:50:13.580]   Aren't really changing the major components of the behavior.
[00:50:13.580 --> 00:50:19.100]   We're just changing what is sort of under the hood
[00:50:19.100 --> 00:50:21.020]   to make it cleaner and easier.
[00:50:21.020 --> 00:50:23.980]   And we can even still call the same fit method as before.
[00:50:23.980 --> 00:50:28.220]   Because all that the fit method knew was that model was a module.
[00:50:28.220 --> 00:50:31.060]   So it had dot parameters and had dot zero grad.
[00:50:31.060 --> 00:50:33.980]   That fit function that we wrote will work with this model
[00:50:33.980 --> 00:50:35.380]   and make its loss go down.
[00:50:35.380 --> 00:50:38.860]   As you can see here, the loss goes down from two and change
[00:50:38.860 --> 00:50:41.500]   down to close to zero.
[00:50:41.500 --> 00:50:46.420]   And it's able to do that because we're just
[00:50:46.420 --> 00:50:48.660]   using the features of a module.
[00:50:48.660 --> 00:50:53.580]   We aren't using the special fact that the one we wrote at first
[00:50:53.580 --> 00:51:00.180]   had weights and biases as special as attributes
[00:51:00.180 --> 00:51:01.900]   at the top level.
[00:51:01.900 --> 00:51:04.620]   That's what we used when we were writing our own loops by hand.
[00:51:04.620 --> 00:51:10.220]   So torch.nn allows us to write more general and flexible code.
[00:51:10.220 --> 00:51:12.860]   So we also want to--
[00:51:12.860 --> 00:51:15.980]   so that covers our refactoring using modules.
[00:51:15.980 --> 00:51:23.100]   So let's put another one on the board in the old to-do list.
[00:51:23.100 --> 00:51:27.780]   All right.
[00:51:27.780 --> 00:51:29.020]   Yes.
[00:51:29.020 --> 00:51:31.180]   Oh, is that to-do list not updating?
[00:51:31.180 --> 00:51:32.460]   There we go.
[00:51:32.460 --> 00:51:34.340]   All right.
[00:51:34.340 --> 00:51:42.380]   So we're now going to refactor using the torch.optim library
[00:51:42.380 --> 00:51:44.700]   or submodule of PyTorch.
[00:51:44.700 --> 00:51:47.380]   Optim is going to handle the optimization,
[00:51:47.380 --> 00:51:50.780]   the updating of our parameters to make them better.
[00:51:50.780 --> 00:51:56.740]   So this torch.optim has a bunch of optimization algorithms.
[00:51:56.740 --> 00:52:00.220]   These update parameters, instead of us doing it manually,
[00:52:00.220 --> 00:52:03.540]   we had this manually coded step with torch.nograd
[00:52:03.540 --> 00:52:04.860]   do all this stuff.
[00:52:04.860 --> 00:52:08.700]   Instead, we-- and we happen to use stochastic gradient descent,
[00:52:08.700 --> 00:52:10.540]   which is very simple, easy to write.
[00:52:10.540 --> 00:52:12.660]   A lot of other optimizers that work very well
[00:52:12.660 --> 00:52:14.740]   with neural networks are not so easy to write.
[00:52:14.740 --> 00:52:18.340]   Easy to write incorrectly, you might say.
[00:52:18.340 --> 00:52:20.220]   And so we don't want--
[00:52:20.220 --> 00:52:22.500]   so we really don't want to be writing that ourselves.
[00:52:22.500 --> 00:52:25.580]   I wrote a bunch of optimization algorithms myself,
[00:52:25.580 --> 00:52:27.980]   a bunch of second-order optimization algorithms
[00:52:27.980 --> 00:52:31.860]   in grad school, and boy, did I write those incorrectly
[00:52:31.860 --> 00:52:33.060]   like 10 times.
[00:52:33.060 --> 00:52:35.220]   I had to come up with pretty sophisticated testing
[00:52:35.220 --> 00:52:38.140]   to make sure I was doing things right.
[00:52:38.140 --> 00:52:41.260]   And you don't want that to be happening
[00:52:41.260 --> 00:52:42.860]   when what you really want to be doing
[00:52:42.860 --> 00:52:44.740]   is trying out cool stuff with neural networks
[00:52:44.740 --> 00:52:47.660]   or building a cool ML application.
[00:52:47.660 --> 00:52:49.380]   So we want to use those optimizers
[00:52:49.380 --> 00:52:53.140]   so they both give us better correctness guarantees
[00:52:53.140 --> 00:52:54.860]   and they clean up our code.
[00:52:54.860 --> 00:52:59.140]   That for loop becomes opt.step, and the zero grad
[00:52:59.140 --> 00:53:01.540]   stays the same, opt.zerograd.
[00:53:01.540 --> 00:53:04.140]   But you no longer-- if you had multiple models, for example,
[00:53:04.140 --> 00:53:06.100]   the optimizer would take care of that.
[00:53:06.100 --> 00:53:10.900]   A little bit of zoomed out here.
[00:53:10.900 --> 00:53:13.860]   This one ended up being a very long slide.
[00:53:13.860 --> 00:53:16.460]   Yeah, opt.zerograd resets the gradient to zero.
[00:53:16.460 --> 00:53:18.420]   Yeah, and we need to call it before calculating
[00:53:18.420 --> 00:53:19.960]   the gradient for the next mini-batch,
[00:53:19.960 --> 00:53:22.340]   just like we needed to do zerograd before doing
[00:53:22.340 --> 00:53:23.780]   the next mini-batch each time.
[00:53:23.780 --> 00:53:30.740]   So at this point, we can also refactor.
[00:53:30.740 --> 00:53:32.540]   We've made it easier to make our model.
[00:53:32.540 --> 00:53:33.940]   We've made it easier--
[00:53:33.940 --> 00:53:36.220]   we've now got this optimizer for optimizing it.
[00:53:36.220 --> 00:53:38.900]   So let's make a nice little cute function to build this.
[00:53:38.900 --> 00:53:43.500]   ABD is asking in the chat, will we
[00:53:43.500 --> 00:53:47.780]   be going through the stochastic weight averaging optimizer?
[00:53:47.780 --> 00:53:48.740]   I wasn't planning on it.
[00:53:48.740 --> 00:53:51.940]   Maybe if we have time at the end, we could try that out.
[00:53:51.940 --> 00:53:53.780]   You can swap out the--
[00:53:53.780 --> 00:53:57.260]   a nice thing about this is you can swap out the optimizer
[00:53:57.260 --> 00:54:01.420]   pretty easily by just changing this optim.whatever here.
[00:54:01.420 --> 00:54:03.540]   The optimizers are all these different classes.
[00:54:03.540 --> 00:54:08.300]   So we could change that to optim.somethingelse.
[00:54:08.300 --> 00:54:13.220]   I guess optim.addadelta, addagrad, addem.
[00:54:13.220 --> 00:54:15.340]   We could change that to something else if we wanted.
[00:54:15.340 --> 00:54:18.700]   I'm going to stick with SGD.
[00:54:18.700 --> 00:54:22.540]   But at the level that we're operating right now,
[00:54:22.540 --> 00:54:27.540]   I'm trying to stay away from too much of the actual machine
[00:54:27.540 --> 00:54:30.260]   learning ideas and theory here and trying
[00:54:30.260 --> 00:54:33.020]   to focus on the mechanics of the code
[00:54:33.020 --> 00:54:38.720]   because these are two separate things, mechanically what's
[00:54:38.720 --> 00:54:42.340]   happening versus why are we doing this,
[00:54:42.340 --> 00:54:45.620]   why does this make models better, et cetera, et cetera.
[00:54:45.620 --> 00:54:51.860]   This stream is really about understanding
[00:54:51.860 --> 00:54:56.340]   the mechanics of Torch.nn and Torch in general
[00:54:56.340 --> 00:55:04.580]   rather than understanding what's going on inside of our models.
[00:55:04.580 --> 00:55:07.580]   All right, so we define this little getModel function,
[00:55:07.580 --> 00:55:10.500]   which will allow us to just create our MNIST logistic
[00:55:10.500 --> 00:55:14.340]   model and an optimizer for it as we go.
[00:55:14.340 --> 00:55:16.740]   And it makes our lives a little bit easier.
[00:55:16.740 --> 00:55:20.660]   And now we are using that optimizer
[00:55:20.660 --> 00:55:22.580]   in our backwards--
[00:55:22.580 --> 00:55:25.180]   after our backwards pass to update the parameters
[00:55:25.180 --> 00:55:26.460]   down here at the bottom.
[00:55:26.460 --> 00:55:32.780]   And we can see that even though we've changed our setup,
[00:55:32.780 --> 00:55:35.580]   we're still getting pretty much the exact same behavior.
[00:55:35.580 --> 00:55:40.500]   Our model starts off with a loss of 2.3,
[00:55:40.500 --> 00:55:45.660]   random chance performance, and then improves drastically
[00:55:45.660 --> 00:55:47.980]   when we run our training code there.
[00:55:47.980 --> 00:55:52.180]   So we've changed these things.
[00:55:52.180 --> 00:55:54.940]   We've added Optin, and we've added an optimizer,
[00:55:54.940 --> 00:55:58.660]   but we have not changed the actual behavior.
[00:55:58.660 --> 00:56:01.780]   So our code keeps getting cleaner and cleaner and more
[00:56:01.780 --> 00:56:06.020]   and more reusable as we add more high-level Torch features
[00:56:06.020 --> 00:56:09.820]   from Torch.nn and Torch.optin, but the behavior
[00:56:09.820 --> 00:56:11.780]   isn't changing.
[00:56:11.780 --> 00:56:17.740]   All right, so that's Optin done.
[00:56:17.740 --> 00:56:24.140]   And so now let's move on to adding data loaders.
[00:56:24.140 --> 00:56:30.220]   So we've mostly been focusing on refactoring the model code more
[00:56:30.220 --> 00:56:32.580]   than the data code.
[00:56:32.580 --> 00:56:35.180]   Playing with model code, I find, is a lot more fun
[00:56:35.180 --> 00:56:36.860]   than playing with the data code.
[00:56:36.860 --> 00:56:38.660]   But we need to have both.
[00:56:38.660 --> 00:56:41.580]   We need to do both here.
[00:56:41.580 --> 00:56:43.260]   So we're going to use data sets.
[00:56:43.260 --> 00:56:46.420]   There's this abstract data set class in PyTorch.
[00:56:46.420 --> 00:56:49.460]   A data set is anything that has a len function
[00:56:49.460 --> 00:56:52.540]   and a getItem function can be a data set.
[00:56:52.540 --> 00:56:55.420]   So len is what happens when you call--
[00:56:55.420 --> 00:57:00.500]   this dunder len method is what gets called when you call len
[00:57:00.500 --> 00:57:03.340]   on something.
[00:57:03.340 --> 00:57:06.140]   So if I call len string, I'm calling--
[00:57:06.140 --> 00:57:06.700]   let's see.
[00:57:06.700 --> 00:57:07.380]   Can I do this?
[00:57:07.380 --> 00:57:10.620]   Yeah, len of some string.
[00:57:10.620 --> 00:57:13.860]   Really what's happening is I'm taking that string
[00:57:13.860 --> 00:57:18.580]   and I'm calling its dunder len method.
[00:57:18.580 --> 00:57:21.380]   And these two things are the same.
[00:57:21.380 --> 00:57:25.140]   This is Python magic, dunder magic, as they call it,
[00:57:25.140 --> 00:57:27.860]   that makes it more easier to use.
[00:57:27.860 --> 00:57:30.140]   But you can hook into this with your own classes
[00:57:30.140 --> 00:57:33.700]   if you define a dunder len and a dunder getItem.
[00:57:33.700 --> 00:57:37.060]   And we will go into a ton of detail
[00:57:37.060 --> 00:57:39.940]   about writing things with data sets.
[00:57:39.940 --> 00:57:44.260]   Data engineering is a whole field of its own.
[00:57:44.260 --> 00:57:47.140]   But there's a nice tutorial on the PyTorch website
[00:57:47.140 --> 00:57:49.580]   for creating custom data sets.
[00:57:49.580 --> 00:57:52.020]   We're going to use a pretty simple data set here,
[00:57:52.020 --> 00:57:54.260]   which is just a data set that wraps tensors.
[00:57:54.260 --> 00:57:56.060]   You just send it-- you give this thing
[00:57:56.060 --> 00:57:59.580]   tensors that fit in memory, and it makes a data set out
[00:57:59.580 --> 00:58:01.300]   of them.
[00:58:01.300 --> 00:58:02.780]   So why do we have--
[00:58:02.780 --> 00:58:05.700]   oh, the other dunder method is getItem.
[00:58:05.700 --> 00:58:10.340]   That's what gets called whenever you index into something
[00:58:10.340 --> 00:58:12.740]   in Python, so using those square brackets.
[00:58:12.740 --> 00:58:16.980]   And so by defining a length and a way of indexing,
[00:58:16.980 --> 00:58:20.260]   this also gives us a way to iterate, index, slice
[00:58:20.260 --> 00:58:23.420]   along our data sets.
[00:58:23.420 --> 00:58:25.100]   And for a tensor data set, that's
[00:58:25.100 --> 00:58:27.020]   along the first index of the tensor,
[00:58:27.020 --> 00:58:30.380]   the batch index of the tensor.
[00:58:30.380 --> 00:58:32.220]   So this is going to make it easier for us
[00:58:32.220 --> 00:58:35.660]   to access those x and y train values.
[00:58:35.660 --> 00:58:38.540]   So a tensor data set can have multiple tensors associated
[00:58:38.540 --> 00:58:39.100]   with it.
[00:58:39.100 --> 00:58:41.620]   And when you iterate over it, when you slice through it,
[00:58:41.620 --> 00:58:42.980]   you get both at once.
[00:58:42.980 --> 00:58:46.100]   So previously, we pulled an x batch and a y batch
[00:58:46.100 --> 00:58:48.860]   separately from x train and y train.
[00:58:48.860 --> 00:58:51.960]   And now with this refactor using this data set,
[00:58:51.960 --> 00:58:53.580]   we can pull them at the same time.
[00:58:53.580 --> 00:58:57.160]   So this saved us one line, so not a huge savings
[00:58:57.160 --> 00:58:58.460]   in terms of lines.
[00:58:58.460 --> 00:59:02.300]   But it's made things a little bit simpler
[00:59:02.300 --> 00:59:08.220]   to use a data set instead of just those raw tensors.
[00:59:08.220 --> 00:59:12.060]   So where this would really help is if your data set doesn't
[00:59:12.060 --> 00:59:14.900]   fit in memory and you're having to load stuff from disk,
[00:59:14.900 --> 00:59:17.620]   you keep that logic out of your training loop
[00:59:17.620 --> 00:59:22.020]   and inside your definition of your data set.
[00:59:22.020 --> 00:59:25.380]   And then you don't have to have all this file reading
[00:59:25.380 --> 00:59:29.340]   and writing code or preprocessing code mixed
[00:59:29.340 --> 00:59:30.340]   in with your training.
[00:59:30.340 --> 00:59:34.180]   It would be in the definition of your data set class,
[00:59:34.180 --> 00:59:36.540]   which is nice.
[00:59:36.540 --> 00:59:39.580]   So now let's refactor using the last--
[00:59:39.580 --> 00:59:44.340]   I believe this is the last piece of Torch.nn
[00:59:44.340 --> 00:59:47.060]   that we're really going to use.
[00:59:47.060 --> 00:59:48.860]   Here are the main pieces.
[00:59:48.860 --> 00:59:51.460]   The last bit is this data loader.
[00:59:51.460 --> 00:59:54.460]   And a data loader abstracts the process
[00:59:54.460 --> 00:59:59.620]   of actually turning a data set into batches.
[00:59:59.620 --> 01:00:03.180]   So data loader is responsible for managing batches.
[01:00:03.180 --> 01:00:05.860]   It may seem a little bit strange that we
[01:00:05.860 --> 01:00:08.140]   have a separate class in order to do this
[01:00:08.140 --> 01:00:09.700]   with our simple example.
[01:00:09.700 --> 01:00:13.240]   But data loaders really-- so for us, really, all it's going to do
[01:00:13.240 --> 01:00:16.520]   is turn this complicated indexing thing
[01:00:16.520 --> 01:00:19.660]   we're doing to pull out our batches into just a for loop.
[01:00:19.660 --> 01:00:21.100]   So that already looks kind of nice.
[01:00:21.100 --> 01:00:22.140]   It's kind of clean.
[01:00:22.140 --> 01:00:24.180]   But really, data loaders allow you
[01:00:24.180 --> 01:00:26.780]   to be much, much faster with loading your data
[01:00:26.780 --> 01:00:28.500]   with multiprocessing.
[01:00:28.500 --> 01:00:33.140]   And when you start to do more complicated kinds of batching,
[01:00:33.140 --> 01:00:36.100]   you really need these data loaders to do it.
[01:00:36.100 --> 01:00:39.220]   But we won't see any of the advanced data loader features
[01:00:39.220 --> 01:00:41.300]   here in this tutorial.
[01:00:41.300 --> 01:00:42.980]   So for now, all it's doing for us
[01:00:42.980 --> 01:00:46.540]   is making it easier to iterate over batches.
[01:00:46.540 --> 01:00:48.820]   So we used to iterate over batches manually
[01:00:48.820 --> 01:00:50.700]   by controlling an index.
[01:00:50.700 --> 01:00:53.020]   So we had an index number.
[01:00:53.020 --> 01:00:54.660]   We were multiplying it by the batch size
[01:00:54.660 --> 01:00:57.500]   to jump through our tensor at a given sort of stride
[01:00:57.500 --> 01:01:00.100]   and pull out entries.
[01:01:00.100 --> 01:01:06.060]   The data loader-- instead, we say, let's loop over this data
[01:01:06.060 --> 01:01:06.540]   loader.
[01:01:06.540 --> 01:01:09.500]   So a data loader defines an iteration procedure
[01:01:09.500 --> 01:01:10.700]   that pulls out the batches.
[01:01:10.700 --> 01:01:14.660]   So instead, we have for xb, yb in train data loader.
[01:01:14.660 --> 01:01:20.260]   That's how we pull out the data, rather than
[01:01:20.260 --> 01:01:23.620]   having this kind of looping structure, which is really nice
[01:01:23.620 --> 01:01:26.500]   to sort of separation of concerns.
[01:01:26.500 --> 01:01:29.380]   The data loader is concerned about how to build batches.
[01:01:29.380 --> 01:01:31.980]   And the training code just cares that it can get batches
[01:01:31.980 --> 01:01:34.860]   from the data loader.
[01:01:34.860 --> 01:01:43.660]   And if we run this code, we once again get that the loss goes
[01:01:43.660 --> 01:01:44.020]   down.
[01:01:44.020 --> 01:01:48.020]   When we run it, it gets down closer to zero.
[01:01:48.020 --> 01:01:51.900]   But now, our code's gotten much, much shorter.
[01:01:51.900 --> 01:01:55.340]   We've lost, I think, five or six lines of code,
[01:01:55.340 --> 01:02:00.420]   which is a lot for something that's only seven lines of code
[01:02:00.420 --> 01:02:03.220]   that started only at 12 lines of code.
[01:02:03.220 --> 01:02:06.660]   And not only that, but we've also made--
[01:02:06.660 --> 01:02:08.300]   we're not just playing code golf here.
[01:02:08.300 --> 01:02:13.900]   We're also making our code more reusable, more extensible.
[01:02:13.900 --> 01:02:16.340]   So that allows us-- we've now gone through everything
[01:02:16.340 --> 01:02:19.460]   that we need, since we've made it to the end with this data
[01:02:19.460 --> 01:02:22.020]   loader here, to answer the question,
[01:02:22.020 --> 01:02:25.340]   what is Torch.nn, really?
[01:02:25.340 --> 01:02:30.460]   That is the title of this tutorial.
[01:02:30.460 --> 01:02:33.260]   So what are the components of Torch.nn
[01:02:33.260 --> 01:02:38.500]   that really matter for building our neural networks?
[01:02:38.500 --> 01:02:44.860]   We've got modules, which create these callable things that
[01:02:44.860 --> 01:02:47.540]   behave like a function, but also contain state.
[01:02:47.540 --> 01:02:49.900]   So they take in inputs and give outputs,
[01:02:49.900 --> 01:02:53.020]   but they also have information that persists over time,
[01:02:53.020 --> 01:02:55.140]   like neural net layer weights.
[01:02:55.140 --> 01:02:58.580]   Its main responsibility is to track the parameters
[01:02:58.580 --> 01:03:00.700]   that it contains.
[01:03:00.700 --> 01:03:03.380]   A parameter, by the way, is a wrapper for a tensor
[01:03:03.380 --> 01:03:06.140]   that tells the module that it has something
[01:03:06.140 --> 01:03:09.060]   that needs to be updated during backprop, weights, and biases,
[01:03:09.060 --> 01:03:11.740]   parameters in general.
[01:03:11.740 --> 01:03:18.100]   So modules and parameters are layers of our neural network,
[01:03:18.100 --> 01:03:19.420]   are pieces of our neural network,
[01:03:19.420 --> 01:03:24.980]   and the tunable bits of those pieces.
[01:03:24.980 --> 01:03:30.100]   Then inside Torch.nn, there is this module, lowercase m,
[01:03:30.100 --> 01:03:33.300]   so a Python module, that contains
[01:03:33.300 --> 01:03:35.140]   all the non-stateful operations, stuff
[01:03:35.140 --> 01:03:37.260]   that don't need to be modules, that don't need
[01:03:37.260 --> 01:03:40.660]   to track information over time.
[01:03:40.660 --> 01:03:42.300]   So this is non-stateful operations,
[01:03:42.300 --> 01:03:44.260]   or things like losses, activations.
[01:03:44.260 --> 01:03:49.780]   Matrix multiplication is defined in Torch.nn.functional.
[01:03:49.780 --> 01:03:54.580]   So this is all the components of PyTorch without state.
[01:03:54.580 --> 01:03:57.140]   It's kind of like a tiny subcomponent of Torch
[01:03:57.140 --> 01:04:00.220]   that's a functional programming language,
[01:04:00.220 --> 01:04:02.660]   if you've ever played around with a functional programming
[01:04:02.660 --> 01:04:04.940]   language.
[01:04:04.940 --> 01:04:07.980]   And then we've also got Torch.optim in there.
[01:04:07.980 --> 01:04:11.660]   Torch.optim has our optimizers, like SGD,
[01:04:11.660 --> 01:04:13.500]   that update the weights of parameters
[01:04:13.500 --> 01:04:14.900]   during the backwards step.
[01:04:14.900 --> 01:04:20.540]   So a Torch.optim knows about parameters
[01:04:20.540 --> 01:04:23.420]   and knows that they need updating.
[01:04:23.420 --> 01:04:25.700]   Let me zoom out here so we've got enough space for all
[01:04:25.700 --> 01:04:27.820]   the pieces of Torch.nn here.
[01:04:27.820 --> 01:04:30.900]   The last two bits, everything up to this point,
[01:04:30.900 --> 01:04:34.300]   has pretty much focused just on models,
[01:04:34.300 --> 01:04:36.940]   on the way our data gets transformed.
[01:04:36.940 --> 01:04:40.940]   The last two pieces are the actual data itself.
[01:04:40.940 --> 01:04:44.380]   We have an abstract interface for our data
[01:04:44.380 --> 01:04:47.380]   based off of knowing how long the data set is,
[01:04:47.380 --> 01:04:49.060]   how many entries does it have, and how
[01:04:49.060 --> 01:04:50.620]   do I get a particular entry.
[01:04:50.620 --> 01:04:52.180]   That's len and getItem.
[01:04:52.180 --> 01:04:53.180]   Give us those.
[01:04:53.180 --> 01:04:56.260]   And then data loaders take data sets
[01:04:56.260 --> 01:04:59.300]   and turn them into iterators over batches.
[01:04:59.300 --> 01:05:01.220]   So data loaders handle batching.
[01:05:01.220 --> 01:05:06.740]   Data sets handle getting the data off the disk.
[01:05:06.740 --> 01:05:09.300]   So this is what Torch.nn is.
[01:05:09.300 --> 01:05:11.820]   You could do all this stuff yourself.
[01:05:11.820 --> 01:05:15.220]   Baseline Torch-- basically, all the pieces of Torch.nn
[01:05:15.220 --> 01:05:17.700]   are implemented in terms of the rest of Torch.
[01:05:17.700 --> 01:05:25.100]   So the core components of Torch that are fast, accelerated,
[01:05:25.100 --> 01:05:28.420]   matrix math with derivatives, that's
[01:05:28.420 --> 01:05:30.740]   what the core functionality of Torch is.
[01:05:30.740 --> 01:05:33.460]   And then Torch.nn wraps all that stuff up
[01:05:33.460 --> 01:05:36.820]   into these nice little pieces here-- module parameter,
[01:05:36.820 --> 01:05:39.940]   functional components, optimizers, data sets,
[01:05:39.940 --> 01:05:41.060]   and data loaders--
[01:05:41.060 --> 01:05:44.060]   and basically does all the things
[01:05:44.060 --> 01:05:46.500]   that you would normally do in order
[01:05:46.500 --> 01:05:49.180]   to use the basic pieces of Torch in order
[01:05:49.180 --> 01:05:50.260]   to build a neural network.
[01:05:50.260 --> 01:05:52.740]   That's what Torch.nn really is.
[01:05:52.740 --> 01:06:00.180]   So I've got my beloved ta-da emoji here
[01:06:00.180 --> 01:06:03.780]   because we have reached the end of this--
[01:06:03.780 --> 01:06:06.860]   we've answered our question of what is Torch.nn, really.
[01:06:06.860 --> 01:06:11.900]   So maybe it's worth celebrating for a brief moment.
[01:06:11.900 --> 01:06:16.020]   So these pieces have made our training loop dramatically
[01:06:16.020 --> 01:06:18.780]   smaller and easier to understand.
[01:06:18.780 --> 01:06:23.100]   And so we're not done yet because we
[01:06:23.100 --> 01:06:27.620]   wrote a very bare-bones version of neural network training.
[01:06:27.620 --> 01:06:32.900]   And now what we need to do now is
[01:06:32.900 --> 01:06:37.340]   add a bunch of features that are necessary to create
[01:06:37.340 --> 01:06:39.500]   effective models in practice.
[01:06:39.500 --> 01:06:45.140]   Oh, yeah, Sebastian says in the chat, Torch.nn
[01:06:45.140 --> 01:06:47.220]   is kind of like Scikit-learn, but it's best
[01:06:47.220 --> 01:06:48.540]   specialized to neural networks.
[01:06:48.540 --> 01:06:50.260]   Yeah, yeah, Scikit-learn is kind of built
[01:06:50.260 --> 01:06:54.620]   on top of NumPy and SciPy and sort of creates
[01:06:54.620 --> 01:07:00.100]   a bunch of machine learning algorithms,
[01:07:00.100 --> 01:07:03.140]   the high-level algorithms out of those pieces.
[01:07:03.140 --> 01:07:04.660]   Torch.nn does the same thing.
[01:07:04.660 --> 01:07:06.660]   It's just that PyTorch is one library
[01:07:06.660 --> 01:07:08.940]   instead of being split into Scikit-learn and SciPy
[01:07:08.940 --> 01:07:09.440]   and NumPy.
[01:07:09.440 --> 01:07:18.300]   All right, so now let's start to make our neural network--
[01:07:18.300 --> 01:07:22.300]   we're now going to start adding features rather than just
[01:07:22.300 --> 01:07:22.980]   refactoring.
[01:07:22.980 --> 01:07:26.580]   We're not just making our neural network code
[01:07:26.580 --> 01:07:28.020]   cleaner or more extensible.
[01:07:28.020 --> 01:07:30.540]   We're making it better now.
[01:07:30.540 --> 01:07:32.040]   So the first thing we're going to do
[01:07:32.040 --> 01:07:33.500]   is we're going to add logging.
[01:07:33.500 --> 01:07:35.900]   So we're going to use Weights and Biases.
[01:07:35.900 --> 01:07:38.580]   Weights and Biases is the company
[01:07:38.580 --> 01:07:39.740]   that runs this channel.
[01:07:39.740 --> 01:07:43.860]   It's the developer toolkit for machine learning.
[01:07:43.860 --> 01:07:47.100]   I like to think of it as the multiplayer machine learning
[01:07:47.100 --> 01:07:49.420]   developer toolkit because what it does
[01:07:49.420 --> 01:07:52.420]   is it allows you to reproduce results.
[01:07:52.420 --> 01:07:54.960]   So multiplayer in terms of playing with yourself
[01:07:54.960 --> 01:07:56.860]   in the future, but really also to make
[01:07:56.860 --> 01:08:00.300]   these interactive dashboards, register models and data sets,
[01:08:00.300 --> 01:08:08.860]   share them, and create blog posts or other ways
[01:08:08.860 --> 01:08:10.660]   of sharing information about your machine
[01:08:10.660 --> 01:08:11.780]   learning with other people.
[01:08:11.780 --> 01:08:13.860]   Right now, all we've done is use print statements
[01:08:13.860 --> 01:08:15.540]   to talk about what our models are doing.
[01:08:15.540 --> 01:08:19.140]   Good luck convincing somebody in a meeting
[01:08:19.140 --> 01:08:22.900]   or in a talk of something just using print statements.
[01:08:22.900 --> 01:08:25.500]   We need more than that in order to communicate our machine
[01:08:25.500 --> 01:08:26.220]   learning results.
[01:08:26.220 --> 01:08:29.140]   And that's a big part of what Weights and Biases provides.
[01:08:29.140 --> 01:08:35.140]   So there's a Python library, an open source Python library
[01:08:35.140 --> 01:08:36.140]   on GitHub.
[01:08:36.140 --> 01:08:38.420]   That's the Weights and Biases client
[01:08:38.420 --> 01:08:42.580]   that allows you to track stuff that
[01:08:42.580 --> 01:08:46.800]   happens in your neural network training
[01:08:46.800 --> 01:08:51.620]   and save it to W and B, so to a web service.
[01:08:51.620 --> 01:08:54.260]   So there's two different-- or there's actually three ways
[01:08:54.260 --> 01:08:58.900]   that you can pronounce W and B. So W and B
[01:08:58.900 --> 01:09:02.540]   is the acronym for our name, Weights and Biases,
[01:09:02.540 --> 01:09:06.580]   named after the components of a neural network layer.
[01:09:06.580 --> 01:09:08.740]   So that's one way to pronounce it.
[01:09:08.740 --> 01:09:11.100]   And so that's connected to our name.
[01:09:11.100 --> 01:09:14.180]   Or you could call it WANDB, which
[01:09:14.180 --> 01:09:20.900]   is because it is magical and will really cast a spell
[01:09:20.900 --> 01:09:23.540]   over your neural networks.
[01:09:23.540 --> 01:09:26.060]   Or you can call it WANDB because it's
[01:09:26.060 --> 01:09:28.860]   like a database for information about the experiments
[01:09:28.860 --> 01:09:33.860]   and the machine learning runs that you have done.
[01:09:33.860 --> 01:09:36.760]   And for this, we only need to do two things in order
[01:09:36.760 --> 01:09:39.800]   to get the sort of basic stuff going with Weights and Biases.
[01:09:39.800 --> 01:09:42.740]   We run WANDB.init to say, all right,
[01:09:42.740 --> 01:09:47.340]   we're about to do some neural network training.
[01:09:47.340 --> 01:09:50.380]   We give this project a name, TorchNN.
[01:09:50.380 --> 01:09:53.700]   And so this says, hey, W and B, I'm about to track something.
[01:09:53.700 --> 01:09:56.460]   I'm about to do some machine learning.
[01:09:56.460 --> 01:09:58.300]   You can also track any Python code you want.
[01:09:58.300 --> 01:09:59.340]   I have used it to track--
[01:09:59.340 --> 01:10:01.420]   I was playing around with the Fibonacci sequence.
[01:10:01.420 --> 01:10:03.340]   And I tracked that in Weights and Biases.
[01:10:03.340 --> 01:10:04.780]   So you can do that, too.
[01:10:04.780 --> 01:10:07.540]   Most people use it for machine learning, though.
[01:10:07.540 --> 01:10:10.660]   And all we need to do is add to that block of our training
[01:10:10.660 --> 01:10:13.500]   code, we add WANDB.log here.
[01:10:13.500 --> 01:10:16.460]   And we pass in a dictionary of stuff that we want to log.
[01:10:16.460 --> 01:10:18.340]   For now, we're just going to track
[01:10:18.340 --> 01:10:20.260]   what epoch was this batch in and what
[01:10:20.260 --> 01:10:22.060]   was the loss on that batch.
[01:10:22.060 --> 01:10:25.140]   So relatively simple information to track here.
[01:10:25.140 --> 01:10:28.460]   And you can see, when you do that,
[01:10:28.460 --> 01:10:31.260]   when you run WANDB.init, you get a link
[01:10:31.260 --> 01:10:33.420]   to a dashboard where you can see the information
[01:10:33.420 --> 01:10:36.460]   that you're logging, though the information is also
[01:10:36.460 --> 01:10:38.540]   saved locally.
[01:10:38.540 --> 01:10:41.420]   So you can see that I've got this--
[01:10:41.420 --> 01:10:42.900]   you can actually tell now that I'm
[01:10:42.900 --> 01:10:46.380]   running this on Windows here.
[01:10:46.380 --> 01:10:51.980]   And so we can view-- most information logged to WANDB,
[01:10:51.980 --> 01:10:54.020]   we can view it actually directly inside Jupyter.
[01:10:54.020 --> 01:11:00.980]   There's no need to necessarily leave Jupyter in order
[01:11:00.980 --> 01:11:05.700]   to look at the stuff logged to Weights and Biases.
[01:11:05.700 --> 01:11:08.140]   So here's-- we logged the loss and the epoch.
[01:11:08.140 --> 01:11:09.980]   So previously, we just looked at the loss
[01:11:09.980 --> 01:11:12.020]   at the beginning of training and saw it was high.
[01:11:12.020 --> 01:11:14.980]   And then at the end of training, we saw it was low.
[01:11:14.980 --> 01:11:18.380]   But we calculated the loss over and over and over again
[01:11:18.380 --> 01:11:19.300]   during training.
[01:11:19.300 --> 01:11:22.700]   And that information was just lost during training.
[01:11:22.700 --> 01:11:26.180]   So we lost all of that detailed information
[01:11:26.180 --> 01:11:28.340]   by adding that WANDB.log here.
[01:11:28.340 --> 01:11:32.540]   This is not-- this is saved, so we didn't lose it.
[01:11:32.540 --> 01:11:34.860]   It's not only saved in that--
[01:11:34.860 --> 01:11:36.820]   like, it's available locally, but it's also
[01:11:36.820 --> 01:11:38.700]   available online.
[01:11:38.700 --> 01:11:44.980]   So that link that was produced would go to a dashboard version
[01:11:44.980 --> 01:11:46.820]   of this online.
[01:11:46.820 --> 01:11:48.740]   But I'm just looking at that same information
[01:11:48.740 --> 01:11:50.060]   inside Jupyter here.
[01:11:50.060 --> 01:11:52.340]   And we have a plotting library.
[01:11:52.340 --> 01:11:56.220]   So we could do things like change the y-axis and the x-axis
[01:11:56.220 --> 01:11:56.900]   here.
[01:11:56.900 --> 01:12:00.340]   So maybe I want to change the y-axis there a little bit,
[01:12:00.340 --> 01:12:02.780]   apply a little bit of smoothing, because there's some noise
[01:12:02.780 --> 01:12:03.280]   here.
[01:12:03.280 --> 01:12:05.940]   And I don't necessarily care so much about--
[01:12:05.940 --> 01:12:10.260]   maybe I want just the running average here or some Gaussian
[01:12:10.260 --> 01:12:14.500]   smoothing so that I get a nice, smooth loss curve.
[01:12:14.500 --> 01:12:17.780]   So I can see how is the loss moving in aggregate.
[01:12:17.780 --> 01:12:20.500]   There's lots of other stuff you can do with WNB charts.
[01:12:20.500 --> 01:12:23.740]   But the most important thing that I want to emphasize here
[01:12:23.740 --> 01:12:26.260]   is that you can share them with other people.
[01:12:26.260 --> 01:12:29.300]   So I'm going to add this guy to a report,
[01:12:29.300 --> 01:12:33.580]   to a little-- to a sort of combo dashboard blog post,
[01:12:33.580 --> 01:12:37.860]   and say, yeah, the loss is going down.
[01:12:37.860 --> 01:12:41.100]   So there's-- now what I've got here is not just a chart,
[01:12:41.100 --> 01:12:43.420]   but some information.
[01:12:43.420 --> 01:12:53.180]   So this chart was logged during the torch.nn livestream.
[01:12:53.180 --> 01:12:54.540]   Great.
[01:12:54.540 --> 01:12:56.580]   We're really happy that our loss is going down.
[01:12:56.580 --> 01:12:58.540]   So let's put that there.
[01:12:58.540 --> 01:13:02.140]   And in addition to having just the ability to write stuff,
[01:13:02.140 --> 01:13:05.220]   add these kind of WNB charts here,
[01:13:05.220 --> 01:13:06.860]   there's lots of other stuff you can do.
[01:13:06.860 --> 01:13:12.380]   So for example, let me go ahead and grab this here.
[01:13:12.380 --> 01:13:12.880]   Boom.
[01:13:12.880 --> 01:13:17.100]   That's a link to the livestream.
[01:13:17.100 --> 01:13:19.260]   Ooh, hold on.
[01:13:19.260 --> 01:13:20.380]   So you can add links.
[01:13:20.380 --> 01:13:21.580]   That's nice.
[01:13:21.580 --> 01:13:22.700]   That's cute.
[01:13:22.700 --> 01:13:25.460]   But if you grab a full YouTube link--
[01:13:25.460 --> 01:13:29.700]   I guess not a YouTube.be link, but the full YouTube link here,
[01:13:29.700 --> 01:13:32.260]   not a short link, but the actual link--
[01:13:32.260 --> 01:13:36.820]   paste that in, and boom, a link to the recording of the time
[01:13:36.820 --> 01:13:37.940]   that I ran this run.
[01:13:37.940 --> 01:13:46.260]   So here's a recording of the logging of this run and more.
[01:13:46.260 --> 01:13:51.620]   So that's now inside of this report that I've made.
[01:13:51.620 --> 01:13:55.300]   So my colleagues can come along and see what exactly
[01:13:55.300 --> 01:13:56.900]   was I doing when I logged this.
[01:13:56.900 --> 01:13:59.060]   There's lots of other stuff.
[01:13:59.060 --> 01:14:03.020]   Let me do one last thing here in this report.
[01:14:03.020 --> 01:14:05.580]   What else can we log?
[01:14:05.580 --> 01:14:08.100]   We track a lot of stuff besides the stuff
[01:14:08.100 --> 01:14:12.660]   that you explicitly ask us to save, so for your convenience.
[01:14:12.660 --> 01:14:16.740]   One fun one that I noticed when I was playing around with this
[01:14:16.740 --> 01:14:25.340]   is the actual notebook and the actual executions
[01:14:25.340 --> 01:14:27.900]   that were associated with this guy.
[01:14:27.900 --> 01:14:30.260]   Let's try looking at this notebook.
[01:14:30.260 --> 01:14:31.660]   Oh, that one didn't work.
[01:14:31.660 --> 01:14:32.540]   Shucks.
[01:14:32.540 --> 01:14:35.940]   All right, well, that's what happens in live demos.
[01:14:35.940 --> 01:14:38.260]   Not everything works every single time.
[01:14:38.260 --> 01:14:42.020]   But you can add lots of different things in here.
[01:14:42.020 --> 01:14:44.500]   Let's just directly add the code.
[01:14:44.500 --> 01:14:47.420]   I didn't save the code this time.
[01:14:47.420 --> 01:14:50.500]   You can add different things in here, other things that--
[01:14:50.500 --> 01:14:52.900]   if I had logged other stuff, there'd be more stuff to log.
[01:14:52.900 --> 01:14:53.900]   So actually, let's go on.
[01:14:53.900 --> 01:14:57.420]   And instead of spending more time on this run
[01:14:57.420 --> 01:14:59.580]   that I didn't log much on, let's go and look
[01:14:59.580 --> 01:15:00.460]   at some other things.
[01:15:00.460 --> 01:15:02.860]   Oh, by the way, since I've saved that report,
[01:15:02.860 --> 01:15:04.620]   I can now share that with other folks.
[01:15:04.620 --> 01:15:06.420]   So let me just drop that in the chat there.
[01:15:06.420 --> 01:15:11.420]   You can go and see that report that I just made and look at it.
[01:15:11.420 --> 01:15:13.340]   So this allows you to share information
[01:15:13.340 --> 01:15:15.500]   with other folks who you're working with,
[01:15:15.500 --> 01:15:17.660]   whether that's other folks you're working with via
[01:15:17.660 --> 01:15:20.580]   the internet, or your colleagues,
[01:15:20.580 --> 01:15:22.900]   or if you want to write a blog post that you
[01:15:22.900 --> 01:15:24.220]   want to share with people.
[01:15:24.220 --> 01:15:25.900]   You can do that all in WNB.
[01:15:25.900 --> 01:15:32.580]   So what were we doing here?
[01:15:32.580 --> 01:15:34.740]   Just to remind, we started a run.
[01:15:34.740 --> 01:15:35.820]   We did some stuff.
[01:15:35.820 --> 01:15:37.580]   We logged it to WNB.
[01:15:37.580 --> 01:15:42.220]   The last thing we need to do is close that run out.
[01:15:42.220 --> 01:15:47.420]   So say WNB, I'm done with what I was doing before.
[01:15:47.420 --> 01:15:51.100]   I'm no longer doing any machine learning on this run.
[01:15:51.100 --> 01:15:52.780]   So it's closed out.
[01:15:52.780 --> 01:15:55.660]   And let me just quickly show you,
[01:15:55.660 --> 01:15:58.460]   there is this general dashboard here.
[01:15:58.460 --> 01:16:00.380]   Let me also drop that link in the chat,
[01:16:00.380 --> 01:16:03.620]   if anybody wants to check that one out,
[01:16:03.620 --> 01:16:06.260]   that's got a bunch of information about our run.
[01:16:06.260 --> 01:16:10.580]   So it does have that code here that I can view.
[01:16:10.580 --> 01:16:16.660]   So here is that Jupyter code, saved to Weights and Biases.
[01:16:16.660 --> 01:16:21.420]   So if you're coming along and reproducing somebody else's
[01:16:21.420 --> 01:16:24.180]   work, you don't need to ask them to send you the Jupyter
[01:16:24.180 --> 01:16:25.420]   notebook that they ran.
[01:16:25.420 --> 01:16:27.940]   You can just find it.
[01:16:27.940 --> 01:16:30.460]   Lots of other metadata and other information
[01:16:30.460 --> 01:16:35.420]   is logged here, system metrics, all kinds of things like that,
[01:16:35.420 --> 01:16:37.140]   without me having to explicitly say
[01:16:37.140 --> 01:16:38.420]   I want to log all those things.
[01:16:38.420 --> 01:16:40.760]   And that's the nice thing about using a logging framework
[01:16:40.760 --> 01:16:42.500]   instead of doing it yourself.
[01:16:42.500 --> 01:16:45.140]   The logging framework will handle a bunch of stuff
[01:16:45.140 --> 01:16:49.060]   you wouldn't think to log, but you may need three or six
[01:16:49.060 --> 01:16:52.260]   months down the line.
[01:16:52.260 --> 01:16:55.220]   Then the last thing, just since we're doing stuff in PyTorch,
[01:16:55.220 --> 01:16:57.720]   I wanted to show you a little special PyTorch tool we have,
[01:16:57.720 --> 01:17:04.060]   WANB.watch, that tracks the values of gradients
[01:17:04.060 --> 01:17:11.260]   and parameters and stores them as histograms on WNB.
[01:17:11.260 --> 01:17:14.420]   So let's take a look at what this dashboard looks like.
[01:17:14.420 --> 01:17:17.300]   So now in addition to the loss and the epoch values
[01:17:17.300 --> 01:17:21.980]   that we log directly, we've also got the biases and the weights.
[01:17:21.980 --> 01:17:24.060]   They're actual values stored as histograms,
[01:17:24.060 --> 01:17:25.220]   so we can see the weights.
[01:17:25.220 --> 01:17:27.420]   Actually, a lot of the weights are very close to zero.
[01:17:27.420 --> 01:17:28.300]   That's interesting.
[01:17:28.300 --> 01:17:31.260]   We could probably compress this network.
[01:17:31.260 --> 01:17:33.060]   Oh, no, those are the gradients.
[01:17:33.060 --> 01:17:34.900]   Well, the weights are also close to zero.
[01:17:34.900 --> 01:17:37.740]   That's this set of charts at the bottom
[01:17:37.740 --> 01:17:40.300]   here are the parameter values themselves,
[01:17:40.300 --> 01:17:45.540]   and then the gradient values are in this other panel here.
[01:17:45.540 --> 01:17:48.300]   So I got that just by calling WANB.watch.
[01:17:48.300 --> 01:17:49.800]   So that information is tracked, so you
[01:17:49.800 --> 01:17:51.960]   can watch for things like gradients getting too big,
[01:17:51.960 --> 01:17:53.980]   gradients getting too small, look
[01:17:53.980 --> 01:17:59.060]   for interesting patterns in your gradients or in your weights.
[01:17:59.060 --> 01:18:04.340]   So you can learn more about that on our YouTube channel.
[01:18:04.340 --> 01:18:07.140]   I've got some videos specifically
[01:18:07.140 --> 01:18:08.620]   on integration with PyTorch.
[01:18:08.620 --> 01:18:12.180]   That's what this video here is.
[01:18:12.180 --> 01:18:14.380]   But then there's also--
[01:18:14.380 --> 01:18:18.140]   oh, yeah, I forgot the share dialog on YouTube
[01:18:18.140 --> 01:18:21.580]   does not like the version of Windows I'm currently running,
[01:18:21.580 --> 01:18:24.260]   so I won't be able to share that directly.
[01:18:24.260 --> 01:18:26.540]   But you can find this video on our channel,
[01:18:26.540 --> 01:18:29.060]   and we have this playlist of a bunch of videos
[01:18:29.060 --> 01:18:30.680]   of how to integrate weights and biases
[01:18:30.680 --> 01:18:33.020]   and use weights and biases tools.
[01:18:33.020 --> 01:18:37.980]   So you can learn more by checking that out.
[01:18:37.980 --> 01:18:43.420]   And oh, yeah, just by the way, what am I doing here?
[01:18:43.420 --> 01:18:46.300]   I'm using the IPython.display library,
[01:18:46.300 --> 01:18:49.700]   which is a hidden gem, I would say, in IPython and Jupyter,
[01:18:49.700 --> 01:18:51.620]   which allows you to just render and show
[01:18:51.620 --> 01:18:53.700]   lots and lots of different things.
[01:18:53.700 --> 01:18:57.060]   So people are used to printing stuff and showing stuff
[01:18:57.060 --> 01:19:00.100]   in Jupyter Notebooks, but they mostly print things like arrays
[01:19:00.100 --> 01:19:01.900]   or lists or strings.
[01:19:01.900 --> 01:19:04.380]   But you can actually display--
[01:19:04.380 --> 01:19:06.460]   there's a special YouTube video class here,
[01:19:06.460 --> 01:19:09.260]   which pulls videos from YouTube.
[01:19:09.260 --> 01:19:11.460]   So there's a lot of stuff in there,
[01:19:11.460 --> 01:19:13.300]   and there's a lot of things that kind of hook
[01:19:13.300 --> 01:19:15.740]   into the rich display features of Jupyter Notebooks,
[01:19:15.740 --> 01:19:20.300]   including those weights and biases runs I was showing you.
[01:19:20.300 --> 01:19:25.740]   All right, so we've added logging to our setup.
[01:19:25.740 --> 01:19:28.420]   So now let's keep going and talk about some of the other things
[01:19:28.420 --> 01:19:30.340]   that we might want to add.
[01:19:30.340 --> 01:19:32.580]   We'll do a little bit more-- we'll add some validation
[01:19:32.580 --> 01:19:35.340]   and do a little bit more refactoring here.
[01:19:35.340 --> 01:19:39.660]   So in the first section, we were just
[01:19:39.660 --> 01:19:42.540]   trying to just get a reasonable training loop set up
[01:19:42.540 --> 01:19:44.860]   for use on our training data.
[01:19:44.860 --> 01:19:46.740]   And we only had one type of data,
[01:19:46.740 --> 01:19:49.020]   and that was just to sort of make things simpler
[01:19:49.020 --> 01:19:50.740]   while we were writing everything out.
[01:19:50.740 --> 01:19:53.620]   While we were writing everything explicitly,
[01:19:53.620 --> 01:19:56.580]   we really only wanted to have one data set.
[01:19:56.580 --> 01:19:59.500]   It would have been a pain to loop over multiple data sets.
[01:19:59.500 --> 01:20:02.780]   But you really always should have a validation data set,
[01:20:02.780 --> 01:20:04.820]   data that your model has not seen,
[01:20:04.820 --> 01:20:06.820]   that's not been used to update its parameters,
[01:20:06.820 --> 01:20:09.580]   that you can use to check to see whether your model does well
[01:20:09.580 --> 01:20:11.700]   on data it hasn't seen before.
[01:20:11.700 --> 01:20:14.180]   This is what's going to happen in production.
[01:20:14.180 --> 01:20:16.700]   You aren't just going to feed in the network data it's already
[01:20:16.700 --> 01:20:17.340]   seen before.
[01:20:17.340 --> 01:20:20.420]   You're going to feed in new data coming in from your users
[01:20:20.420 --> 01:20:22.620]   or coming in from a sensor stream.
[01:20:22.620 --> 01:20:24.420]   And so this validation set sort of
[01:20:24.420 --> 01:20:27.060]   mimics that by having data that is not
[01:20:27.060 --> 01:20:30.060]   used to update the parameters.
[01:20:30.060 --> 01:20:33.700]   So it would have been really hard to add validation data
[01:20:33.700 --> 01:20:36.260]   when we were writing everything raw in PyTorch.
[01:20:36.260 --> 01:20:38.500]   But now that we've got this two-liner that's
[01:20:38.500 --> 01:20:41.620]   like build a tensor data set, turn it into a data loader,
[01:20:41.620 --> 01:20:42.980]   now it's really easy to do that.
[01:20:42.980 --> 01:20:46.420]   We just grab that validation data, the input and output
[01:20:46.420 --> 01:20:48.860]   validation data, and make a different data set and data
[01:20:48.860 --> 01:20:50.620]   loader here.
[01:20:50.620 --> 01:20:53.140]   So there's a couple of notes here on the code.
[01:20:53.140 --> 01:20:57.340]   One is that shuffling training data is really important.
[01:20:57.340 --> 01:21:02.020]   This is a feature of the various stochastic optimizers
[01:21:02.020 --> 01:21:03.220]   that we use.
[01:21:03.220 --> 01:21:05.220]   If the batches are too similar to each other,
[01:21:05.220 --> 01:21:06.720]   like if the first batch is all ones,
[01:21:06.720 --> 01:21:10.100]   second batch is all twos, third batch is all threes,
[01:21:10.100 --> 01:21:14.620]   then that can actually cause really gnarly misbehavior
[01:21:14.620 --> 01:21:16.180]   of your optimizer.
[01:21:16.180 --> 01:21:18.540]   So you definitely want to shuffle the training data.
[01:21:18.540 --> 01:21:20.380]   With validation data, we aren't using
[01:21:20.380 --> 01:21:22.780]   it to update the parameters, so it doesn't matter
[01:21:22.780 --> 01:21:25.380]   whether the gradients are correlated from batch to batch.
[01:21:25.380 --> 01:21:27.300]   We don't calculate those gradients,
[01:21:27.300 --> 01:21:28.740]   so we don't want to shuffle it.
[01:21:28.740 --> 01:21:33.940]   We'll also use a batch size for the validation set
[01:21:33.940 --> 01:21:36.100]   twice as large as the one for the training set.
[01:21:36.100 --> 01:21:39.020]   So we use BS batch size for the training data loader.
[01:21:39.020 --> 01:21:42.180]   We double it for the validation data,
[01:21:42.180 --> 01:21:44.620]   because when you do back propagation,
[01:21:44.620 --> 01:21:46.940]   calculating the gradients takes up memory.
[01:21:46.940 --> 01:21:49.780]   We needed to calculate and store those gradients in order
[01:21:49.780 --> 01:21:52.820]   to opt and then update the parameter values
[01:21:52.820 --> 01:21:55.940]   on the basis of those gradients.
[01:21:55.940 --> 01:21:59.440]   That takes roughly the same amount of memory
[01:21:59.440 --> 01:22:04.460]   to do back propagation as it did to do the forward operation.
[01:22:04.460 --> 01:22:11.620]   So you end up with twice as much stuff stored in your GPU RAM.
[01:22:11.620 --> 01:22:16.860]   And so the forward and backwards pass
[01:22:16.860 --> 01:22:19.580]   takes twice as much memory as just a forward pass.
[01:22:19.580 --> 01:22:21.780]   And so we use--
[01:22:21.780 --> 01:22:24.100]   in the validation data, where we just do a forward pass,
[01:22:24.100 --> 01:22:26.100]   we don't do back propagation, we can
[01:22:26.100 --> 01:22:28.820]   have a batch size twice as large.
[01:22:28.820 --> 01:22:31.240]   And that'll help us compute the loss more quickly,
[01:22:31.240 --> 01:22:33.780]   because we're basically better able to take advantage
[01:22:33.780 --> 01:22:35.700]   of the parallel computations that
[01:22:35.700 --> 01:22:40.300]   are going on with vectorized CPU code or GPU code.
[01:22:40.300 --> 01:22:42.260]   So we'll calculate and log the validation loss
[01:22:42.260 --> 01:22:44.180]   at the end of each epoch.
[01:22:44.180 --> 01:22:45.860]   Just a little note here.
[01:22:45.860 --> 01:22:48.700]   When you're training your model, you
[01:22:48.700 --> 01:22:50.620]   want to call this .train method.
[01:22:50.620 --> 01:22:52.540]   When you are just evaluating your model,
[01:22:52.540 --> 01:22:55.620]   you want to call this .eval method.
[01:22:55.620 --> 01:23:01.500]   .train sets all the components of your model
[01:23:01.500 --> 01:23:03.540]   so that it tells all of them, hey, it's
[01:23:03.540 --> 01:23:05.180]   about to be time for training.
[01:23:05.180 --> 01:23:07.420]   And some pieces of the model really care about that.
[01:23:07.420 --> 01:23:10.100]   We aren't using any of them, but batch norm and dropout
[01:23:10.100 --> 01:23:13.980]   are two very prominent examples that behave differently
[01:23:13.980 --> 01:23:15.540]   in training and inference.
[01:23:15.540 --> 01:23:17.900]   Batch norm collects statistics during training
[01:23:17.900 --> 01:23:21.020]   and then fixes them during inference or evaluation.
[01:23:21.020 --> 01:23:24.060]   Dropout randomly gets rid of neurons during training,
[01:23:24.060 --> 01:23:28.260]   but then does not do that during evaluation.
[01:23:28.260 --> 01:23:30.420]   So this is another utility of having something
[01:23:30.420 --> 01:23:32.900]   like torch.nn.
[01:23:32.900 --> 01:23:38.500]   These tricky bits about context switching
[01:23:38.500 --> 01:23:43.620]   are handled by torch.nn with this simple model.train,
[01:23:43.620 --> 01:23:44.740]   model.eval.
[01:23:44.740 --> 01:23:47.260]   But make sure you don't forget to do that when you
[01:23:47.260 --> 01:23:50.860]   are writing your PyTorch code.
[01:23:50.860 --> 01:23:53.500]   And now we're also logging that validation loss.
[01:23:53.500 --> 01:23:57.940]   So we compute the validation loss on all of the things
[01:23:57.940 --> 01:24:02.740]   in our data loader and log that.
[01:24:02.740 --> 01:24:03.580]   Let's take a look.
[01:24:03.580 --> 01:24:07.500]   So now we have the validation loss
[01:24:07.500 --> 01:24:08.780]   along with the training loss.
[01:24:08.780 --> 01:24:11.820]   Actually, I kind of want to see those next to each other,
[01:24:11.820 --> 01:24:13.860]   how the training loss and validation loss compare
[01:24:13.860 --> 01:24:16.060]   to each other.
[01:24:16.060 --> 01:24:20.940]   And let's make that nice, thick line.
[01:24:20.940 --> 01:24:22.620]   Yeah, so you can see the validation loss
[01:24:22.620 --> 01:24:24.500]   is a little bit lower than the training loss.
[01:24:24.500 --> 01:24:25.300]   That's good to see.
[01:24:25.300 --> 01:24:27.700]   We aren't overfitting with our model yet.
[01:24:27.700 --> 01:24:31.180]   So that's what this chart with our train loss and validation
[01:24:31.180 --> 01:24:31.980]   loss shows.
[01:24:31.980 --> 01:24:36.580]   So it's good to see.
[01:24:36.580 --> 01:24:40.260]   You'd hate to see overfitting.
[01:24:40.260 --> 01:24:44.340]   And now we have a little bit more refactoring to do.
[01:24:44.340 --> 01:24:48.420]   We have all that stuff that we're doing to fit our model.
[01:24:48.420 --> 01:24:53.460]   That's pretty generic, like calculating training loss,
[01:24:53.460 --> 01:24:55.940]   calculating validation loss, updating parameters.
[01:24:55.940 --> 01:24:58.340]   All that stuff is generic enough that you can probably
[01:24:58.340 --> 01:25:04.660]   reuse it across projects, across models and data sets.
[01:25:04.660 --> 01:25:09.420]   And we'll also-- we calculate loss on batches,
[01:25:09.420 --> 01:25:11.460]   both for the training set and the validation set.
[01:25:11.460 --> 01:25:15.940]   So we'll make this loss batch function to make that the same.
[01:25:15.940 --> 01:25:21.820]   And we'll use that if this loss batch function gets
[01:25:21.820 --> 01:25:23.500]   an optimizer, then it does backprop.
[01:25:23.500 --> 01:25:25.100]   If not, it doesn't do backprop.
[01:25:25.100 --> 01:25:27.660]   So that's a cute little way of splitting it up.
[01:25:27.660 --> 01:25:32.980]   If you use a high-level library, like PyTorch Lightning or Fast
[01:25:32.980 --> 01:25:37.420]   AI, then this is closer to the level
[01:25:37.420 --> 01:25:39.540]   where you're actually going to operate at when you're
[01:25:39.540 --> 01:25:42.140]   writing stuff than the level we've been operating up
[01:25:42.140 --> 01:25:44.260]   to this point.
[01:25:44.260 --> 01:25:47.820]   So also, if you've used Keras, TensorFlow Keras,
[01:25:47.820 --> 01:25:52.660]   you also operate at the level of calling a .fit on your models
[01:25:52.660 --> 01:25:56.300]   rather than iterating over gradients yourself
[01:25:56.300 --> 01:25:58.500]   or zeroing gradients yourself.
[01:25:58.500 --> 01:26:02.660]   Right.
[01:26:02.660 --> 01:26:03.300]   OK.
[01:26:03.300 --> 01:26:08.940]   So we have that loss batch function to do that.
[01:26:08.940 --> 01:26:10.660]   And we wrap all this stuff up in fit.
[01:26:10.660 --> 01:26:12.100]   Fit needs to know how many epochs
[01:26:12.100 --> 01:26:14.420]   to train a model with which loss function,
[01:26:14.420 --> 01:26:18.140]   applying what optimizer to that loss function
[01:26:18.140 --> 01:26:19.380]   to calculate--
[01:26:19.380 --> 01:26:22.060]   to take gradients and turn them into parameter updates
[01:26:22.060 --> 01:26:23.220]   for a model.
[01:26:23.220 --> 01:26:26.100]   It needs to know the training data loader and the validation
[01:26:26.100 --> 01:26:28.500]   data loader so it can loop over them.
[01:26:28.500 --> 01:26:30.420]   And then lastly, I've added these.
[01:26:30.420 --> 01:26:32.700]   So I've added to the existing code
[01:26:32.700 --> 01:26:34.500]   that Jeremy Howard wrote for this tutorial,
[01:26:34.500 --> 01:26:37.900]   I've added these two little extra hooks, onTrainStep
[01:26:37.900 --> 01:26:42.540]   and onValEpochEnd, to do our logging for us.
[01:26:42.540 --> 01:26:50.620]   So if you pass in a function for onTrainStep or onValEpochEnd,
[01:26:50.620 --> 01:26:53.540]   then it gets called with the training loss
[01:26:53.540 --> 01:26:56.100]   and the validation loss.
[01:26:56.100 --> 01:26:59.740]   But otherwise, you can just pass in none and nothing happens.
[01:26:59.740 --> 01:27:04.740]   So this allows us to turn on and off logging with our model.
[01:27:04.740 --> 01:27:07.300]   And this sort of structure here is
[01:27:07.300 --> 01:27:10.460]   kind of a simple version of a hook and callback system.
[01:27:10.460 --> 01:27:13.900]   So places in code, like a function like .fit,
[01:27:13.900 --> 01:27:16.220]   where you can provide a function and that function
[01:27:16.220 --> 01:27:17.180]   will get called.
[01:27:17.180 --> 01:27:19.860]   So a more sophisticated version of this hook and callback system
[01:27:19.860 --> 01:27:23.580]   is how logging code and a bunch of other bonus features
[01:27:23.580 --> 01:27:25.700]   and things like FastAI and PyTorch Lightning
[01:27:25.700 --> 01:27:27.460]   are implemented.
[01:27:27.460 --> 01:27:31.900]   So learning rate scheduling, changing around
[01:27:31.900 --> 01:27:34.300]   the behavior of gradients, all this kind of stuff, hooks
[01:27:34.300 --> 01:27:36.220]   and callbacks are how these are implemented.
[01:27:36.220 --> 01:27:39.820]   So this is a very simple version of it for our very simple model.
[01:27:39.820 --> 01:27:45.860]   But this is a demonstration of the principle here.
[01:27:45.860 --> 01:27:51.140]   So we'll see what the callback for logging
[01:27:51.140 --> 01:27:53.340]   looks like in a second.
[01:27:53.340 --> 01:27:55.460]   We've also got, in addition to fit,
[01:27:55.460 --> 01:27:59.660]   we also want to wrap our data loader building stuff.
[01:27:59.660 --> 01:28:03.220]   This wraps-- this sort of abstracts
[01:28:03.220 --> 01:28:06.340]   the idea of building a data loader from a data set,
[01:28:06.340 --> 01:28:09.020]   doubling the batch size, and shuffling the training data
[01:28:09.020 --> 01:28:10.180]   but not the validation data.
[01:28:10.180 --> 01:28:12.220]   That's something you're going to do all the time.
[01:28:12.220 --> 01:28:14.100]   So we can do that.
[01:28:14.100 --> 01:28:17.820]   For our logging, we're going to have four--
[01:28:17.820 --> 01:28:18.940]   we're going to refactor it.
[01:28:18.940 --> 01:28:21.060]   Just a dictionary of information is going to come in.
[01:28:21.060 --> 01:28:22.660]   We'll save that to weights and biases.
[01:28:22.660 --> 01:28:26.140]   That's our fitting operation.
[01:28:26.140 --> 01:28:28.220]   And then we'll wrap our fit function
[01:28:28.220 --> 01:28:34.980]   with fit and log that just does weights and biases stuff,
[01:28:34.980 --> 01:28:39.780]   initializes a run, watches the parameters and the gradients,
[01:28:39.780 --> 01:28:44.260]   and then uses that log dict function
[01:28:44.260 --> 01:28:49.300]   to log the information on the end of each training batch
[01:28:49.300 --> 01:28:52.780]   and at the end of a validation epoch.
[01:28:52.780 --> 01:28:55.340]   So now our whole process of painting data loaders,
[01:28:55.340 --> 01:28:59.820]   fitting and then making a model, and then fitting a model
[01:28:59.820 --> 01:29:04.260]   and tracking what happened in it can be run
[01:29:04.260 --> 01:29:06.100]   in just three lines of code.
[01:29:06.100 --> 01:29:07.500]   So that's great.
[01:29:07.500 --> 01:29:09.660]   It's still very flexible.
[01:29:09.660 --> 01:29:11.260]   We can change around what our model is.
[01:29:11.260 --> 01:29:13.300]   We can change around what our data is.
[01:29:13.300 --> 01:29:17.300]   But this high-level stuff has all
[01:29:17.300 --> 01:29:20.580]   been made relatively simple.
[01:29:20.580 --> 01:29:23.060]   And so this is not built into Torch.nn.
[01:29:23.060 --> 01:29:24.940]   We're moving beyond Torch.nn now.
[01:29:24.940 --> 01:29:26.400]   But this is the kind of stuff that's
[01:29:26.400 --> 01:29:29.540]   built into those higher-level libraries.
[01:29:29.540 --> 01:29:35.220]   So HuggingFace, FastAI, PyTorch Lightning, PyTorch Ignite,
[01:29:35.220 --> 01:29:37.460]   any of these other higher-level frameworks
[01:29:37.460 --> 01:29:42.620]   for doing PyTorch training.
[01:29:42.620 --> 01:29:45.300]   And yeah, let's just check and see that everything
[01:29:45.300 --> 01:29:49.460]   logged to W and B. Yeah, I also tracked the batch size just
[01:29:49.460 --> 01:29:53.180]   for fun, since there was more information to track.
[01:29:53.180 --> 01:29:54.420]   Anything else interesting?
[01:29:54.420 --> 01:29:57.500]   Oh, yeah, the training loss.
[01:29:57.500 --> 01:29:59.780]   Oh, right, I put the training loss and validation loss
[01:29:59.780 --> 01:30:02.100]   together on one chart here.
[01:30:02.100 --> 01:30:04.700]   Oh, some fun other stuff we track.
[01:30:04.700 --> 01:30:09.260]   We track the graph of your model if you do wandb.watch.
[01:30:09.260 --> 01:30:10.500]   So this one's pretty simple.
[01:30:10.500 --> 01:30:11.700]   We just had that one module.
[01:30:11.700 --> 01:30:15.780]   We'll see a little bit of a more complicated example later.
[01:30:15.780 --> 01:30:17.580]   Oh, this one's fun.
[01:30:17.580 --> 01:30:20.220]   If you're using Jupyter, we track the session history.
[01:30:20.220 --> 01:30:22.460]   So we track every cell that you ran.
[01:30:22.460 --> 01:30:24.300]   So remember, if you were watching
[01:30:24.300 --> 01:30:26.380]   at the beginning of the stream, I had some images.
[01:30:26.380 --> 01:30:27.380]   I resized them.
[01:30:27.380 --> 01:30:29.320]   I ran the cell a couple of different times.
[01:30:29.320 --> 01:30:32.140]   You can see that's tracked so that if somebody
[01:30:32.140 --> 01:30:34.020]   is coming along and reproducing this run,
[01:30:34.020 --> 01:30:38.260]   they can see, oh, you did something weird 10 cells above
[01:30:38.260 --> 01:30:40.020]   when you ran the cell 10 times.
[01:30:40.020 --> 01:30:43.060]   That's why we're not getting the same answers when we both run
[01:30:43.060 --> 01:30:47.980]   this notebook, because you didn't run it beginning to end.
[01:30:47.980 --> 01:30:51.180]   You also did some maybe little naughty stuff
[01:30:51.180 --> 01:30:53.020]   with your Jupyter.
[01:30:53.020 --> 01:30:56.620]   You ran the same cell multiple times.
[01:30:56.620 --> 01:30:59.100]   So we track that session history for you as well.
[01:30:59.100 --> 01:31:03.140]   That's a neat little feature.
[01:31:03.140 --> 01:31:07.300]   But yeah, so all that and just once you've abstracted it,
[01:31:07.300 --> 01:31:09.380]   it can be done in three lines of code.
[01:31:09.380 --> 01:31:13.420]   If you're using FastAI, Hugging Face, or PyTorch Lightning,
[01:31:13.420 --> 01:31:14.860]   WNB is integrated into them.
[01:31:14.860 --> 01:31:19.060]   So not only you get all this nice high-level neural network
[01:31:19.060 --> 01:31:22.140]   behavior in terms of training and validation data
[01:31:22.140 --> 01:31:25.340]   loaders and optimization, but you also
[01:31:25.340 --> 01:31:30.780]   will get that logging in the same few lines of code.
[01:31:30.780 --> 01:31:33.100]   And these same basic three lines of code
[01:31:33.100 --> 01:31:35.220]   can train a wide variety of models.
[01:31:35.220 --> 01:31:39.940]   And we're going to now switch to doing a CNN on a GPU,
[01:31:39.940 --> 01:31:42.380]   on a graphics processing unit.
[01:31:42.380 --> 01:31:45.420]   So that's what we're going to close out here today doing.
[01:31:45.420 --> 01:31:48.220]   We're going to do a--
[01:31:48.220 --> 01:31:52.220]   we're going to switch over, rewrite our model
[01:31:52.220 --> 01:31:54.460]   as a convolutional neural network,
[01:31:54.460 --> 01:31:57.100]   so a neural network designed for working with images.
[01:31:57.100 --> 01:31:59.820]   And then we're going to run it on a GPU.
[01:31:59.820 --> 01:32:03.020]   So we're going to use multiple layers now instead of just one
[01:32:03.020 --> 01:32:04.340]   layer.
[01:32:04.340 --> 01:32:06.740]   And we'll be able to use them because they don't assume
[01:32:06.740 --> 01:32:07.820]   anything about the model.
[01:32:07.820 --> 01:32:09.900]   Because they've successfully abstracted away
[01:32:09.900 --> 01:32:13.220]   the details of the model from the details of training,
[01:32:13.220 --> 01:32:14.980]   we'll be able to run exactly those functions
[01:32:14.980 --> 01:32:17.780]   with no modifications.
[01:32:17.780 --> 01:32:21.180]   So the difference with this model from our last one
[01:32:21.180 --> 01:32:23.780]   is we're going to use conv2d instead of--
[01:32:23.780 --> 01:32:27.380]   so nn.conv2d instead of nn.linear.
[01:32:27.380 --> 01:32:28.980]   So I won't go into details about how
[01:32:28.980 --> 01:32:30.420]   to build a convolutional network.
[01:32:30.420 --> 01:32:37.420]   That's stuff that's covered in our courses on deep learning.
[01:32:37.420 --> 01:32:39.620]   But we still need to do the same thing.
[01:32:39.620 --> 01:32:43.380]   We initialize our module and create its sub-modules.
[01:32:43.380 --> 01:32:45.380]   So we initialize those convolutional layers.
[01:32:45.380 --> 01:32:46.620]   We've got three of them.
[01:32:46.620 --> 01:32:49.580]   They have parameters that we choose.
[01:32:49.580 --> 01:32:51.660]   These are based off of pretty typical parameters.
[01:32:51.660 --> 01:32:53.740]   You can look on the internet to find
[01:32:53.740 --> 01:32:56.200]   other examples of convolutional networks and the parameters
[01:32:56.200 --> 01:32:56.940]   that they choose.
[01:32:56.940 --> 01:32:58.700]   In general, you kind of want to stick to stuff
[01:32:58.700 --> 01:33:01.580]   people have done in the past and only slowly sort of expand
[01:33:01.580 --> 01:33:03.140]   outward from there.
[01:33:03.140 --> 01:33:06.460]   So copying and pasting code is perfectly fine.
[01:33:06.460 --> 01:33:15.420]   And then we've got our forward method here
[01:33:15.420 --> 01:33:18.500]   that takes the inputs and passes them through those layers.
[01:33:18.500 --> 01:33:20.260]   We've got some functional components here.
[01:33:20.260 --> 01:33:22.140]   We're using torsion.nn.functional,
[01:33:22.140 --> 01:33:25.380]   that capital F there, to apply activation functions,
[01:33:25.380 --> 01:33:29.540]   and then do, at the end, to get our final outputs,
[01:33:29.540 --> 01:33:31.340]   we're going to use average pooling
[01:33:31.340 --> 01:33:35.020]   to take the output of our last convolutional layer
[01:33:35.020 --> 01:33:39.780]   and turn it into a single value for each class.
[01:33:39.780 --> 01:33:44.140]   And that will be our final values
[01:33:44.140 --> 01:33:46.340]   that go into that log softmax.
[01:33:46.340 --> 01:33:48.460]   So that's that last average pool 2D layer,
[01:33:48.460 --> 01:33:51.420]   also coming from torsion.nn.functional.
[01:33:51.420 --> 01:33:53.740]   I see a question from Rahul in the YouTube chat.
[01:33:53.740 --> 01:33:55.700]   I'm assuming I would need to provide some other permissions
[01:33:55.700 --> 01:33:57.940]   to log the notebook history other than just installing 1B.
[01:33:57.940 --> 01:33:58.620]   Is that correct?
[01:33:58.620 --> 01:33:59.540]   Yeah, absolutely.
[01:33:59.540 --> 01:34:02.100]   So I was showing you that the session history was
[01:34:02.100 --> 01:34:05.020]   logged to Weights and Biases.
[01:34:05.020 --> 01:34:06.740]   Source code can be intellectual property.
[01:34:06.740 --> 01:34:09.260]   We don't want to steal your property
[01:34:09.260 --> 01:34:11.140]   or accidentally share it with the whole world
[01:34:11.140 --> 01:34:12.460]   if you didn't mean to.
[01:34:12.460 --> 01:34:16.260]   So that's a setting you have to enable on your WNB profile
[01:34:16.260 --> 01:34:18.500]   to say, yeah, I want to save code.
[01:34:18.500 --> 01:34:23.820]   So that's definitely something to--
[01:34:23.820 --> 01:34:25.540]   yeah, don't worry.
[01:34:25.540 --> 01:34:27.940]   We won't be sharing your information.
[01:34:27.940 --> 01:34:33.300]   Yeah, so-- oh, yeah, by the way, the logging stuff here
[01:34:33.300 --> 01:34:34.820]   does require a WNB account.
[01:34:34.820 --> 01:34:38.740]   If you were to run the notebook, the one whose link is floating
[01:34:38.740 --> 01:34:42.180]   down by my face here, you'll need
[01:34:42.180 --> 01:34:44.700]   to create a Weights and Biases account to do that logging.
[01:34:44.700 --> 01:34:47.580]   The accounts are free if you're doing--
[01:34:47.580 --> 01:34:52.380]   so long as you're not doing work in a team, in industry.
[01:34:52.380 --> 01:34:54.820]   So if you're in a profit setting,
[01:34:54.820 --> 01:34:57.100]   then just like with GitHub, there's
[01:34:57.100 --> 01:35:00.060]   paid features of Weights and Biases that you would want.
[01:35:00.060 --> 01:35:02.740]   But if you're just working on your own,
[01:35:02.740 --> 01:35:04.540]   or if you're trying this out, or if you're
[01:35:04.540 --> 01:35:08.180]   an academic or a nonprofit, then there's
[01:35:08.180 --> 01:35:12.780]   free or very low-cost versions of WNB available.
[01:35:12.780 --> 01:35:14.900]   But we give you 100 gigabytes of free storage
[01:35:14.900 --> 01:35:19.860]   of things like models and data sets,
[01:35:19.860 --> 01:35:24.020]   and then a pretty much unbounded amount of storage
[01:35:24.020 --> 01:35:27.540]   for metrics and things that you might log.
[01:35:27.540 --> 01:35:33.620]   All right, so let's see.
[01:35:33.620 --> 01:35:35.940]   Getting back in and talking about our convolutional neural
[01:35:35.940 --> 01:35:36.820]   network here.
[01:35:36.820 --> 01:35:39.580]   Yeah, we've got three layers of convolutions.
[01:35:39.580 --> 01:35:43.580]   We've got a reshape operation.
[01:35:43.580 --> 01:35:45.260]   We've got view.
[01:35:45.260 --> 01:35:49.620]   So it's the PyTorch version of NumPy's reshape.
[01:35:49.620 --> 01:35:54.780]   But yeah, we've got nonlinear activations and linear layers,
[01:35:54.780 --> 01:35:57.060]   where those linear layers are convolutional.
[01:35:57.060 --> 01:36:00.980]   And we've got average pooling at the end to give us--
[01:36:00.980 --> 01:36:03.300]   this is also called global average pooling, by the way,
[01:36:03.300 --> 01:36:06.780]   giving us one number for each class.
[01:36:06.780 --> 01:36:09.300]   Also, one little additional piece,
[01:36:09.300 --> 01:36:11.020]   because this CNN is a little bit harder
[01:36:11.020 --> 01:36:15.140]   to train than the linear network we were working with before,
[01:36:15.140 --> 01:36:18.660]   we add this momentum variation of stochastic gradient descent,
[01:36:18.660 --> 01:36:20.180]   which is just a keyword argument.
[01:36:20.180 --> 01:36:21.780]   It's not even a different optimizer.
[01:36:21.780 --> 01:36:25.540]   It's just a keyword argument, stochastic gradient descent.
[01:36:25.540 --> 01:36:28.740]   You'll notice this one takes a lot longer to run.
[01:36:28.740 --> 01:36:32.580]   It takes like 10 seconds instead of under a second.
[01:36:32.580 --> 01:36:33.860]   Yeah, run time.
[01:36:33.860 --> 01:36:36.260]   Yeah, we can see the run time was about 10 seconds there
[01:36:36.260 --> 01:36:39.180]   in our run summary at the bottom.
[01:36:39.180 --> 01:36:42.100]   And yeah, so training and validation loss,
[01:36:42.100 --> 01:36:44.020]   both going down still.
[01:36:44.020 --> 01:36:45.980]   We can see the gradients and parameter values
[01:36:45.980 --> 01:36:49.660]   for all those convolutions and their weights and biases.
[01:36:49.660 --> 01:36:50.500]   That's interesting.
[01:36:50.500 --> 01:36:52.820]   It looks like there's some different bias
[01:36:52.820 --> 01:36:55.380]   values for different components of the convolutions.
[01:36:55.380 --> 01:36:57.060]   That might be fun to dig into.
[01:36:57.060 --> 01:36:59.100]   One bias is behaving very differently
[01:36:59.100 --> 01:36:59.940]   than all the others.
[01:36:59.940 --> 01:37:04.900]   Oh, yeah, that model looks a little bit more interesting now.
[01:37:04.900 --> 01:37:08.100]   We've got those convolutional layers in the graph.
[01:37:08.100 --> 01:37:11.780]   So this is something logged thanks to wanb.watch.
[01:37:11.780 --> 01:37:14.860]   We've got this information about output shapes, parameter
[01:37:14.860 --> 01:37:18.740]   counts, and the structure of our graph.
[01:37:18.740 --> 01:37:26.740]   All right, a little bit more refactoring still,
[01:37:26.740 --> 01:37:32.780]   which is-- so you may have noticed what we did is we
[01:37:32.780 --> 01:37:35.100]   chained a bunch of operations together here.
[01:37:35.100 --> 01:37:37.260]   So in our forward method, we're just
[01:37:37.260 --> 01:37:39.460]   composing a bunch of operations one after another.
[01:37:39.460 --> 01:37:43.020]   There's no ifs to branch things.
[01:37:43.020 --> 01:37:44.300]   There's no for loops.
[01:37:44.300 --> 01:37:47.660]   There's not really this complicated control flow,
[01:37:47.660 --> 01:37:51.340]   nor are inputs being copied and sent to multiple places.
[01:37:51.340 --> 01:37:55.700]   It's just the input of one operation--
[01:37:55.700 --> 01:37:57.140]   sorry, the output of one operation
[01:37:57.140 --> 01:37:58.940]   becomes the input of the next operation.
[01:37:58.940 --> 01:38:02.780]   So xb is sent through a convolution,
[01:38:02.780 --> 01:38:05.660]   then through a rectified linear, then through a convolution,
[01:38:05.660 --> 01:38:07.900]   then through a rectified linear unit.
[01:38:07.900 --> 01:38:13.060]   So we're doing this all very sequentially.
[01:38:13.060 --> 01:38:15.540]   And PyTorch provides a convenience class,
[01:38:15.540 --> 01:38:19.380]   nn.sequential, that will simplify your code
[01:38:19.380 --> 01:38:22.940]   and can actually also make it faster.
[01:38:22.940 --> 01:38:25.460]   Sequential object runs the modules contained within it
[01:38:25.460 --> 01:38:28.360]   in a sequential manner to simplify
[01:38:28.360 --> 01:38:30.900]   the writing of our neural network
[01:38:30.900 --> 01:38:36.660]   in order to fully make everything a sequential.
[01:38:36.660 --> 01:38:39.220]   We'll take that .view.
[01:38:39.220 --> 01:38:42.140]   That's the only part that doesn't have a layer to it.
[01:38:42.140 --> 01:38:44.140]   Everything else that we were doing-- our reluse,
[01:38:44.140 --> 01:38:46.140]   our average pools, our comms-- all of those
[01:38:46.140 --> 01:38:47.900]   have Torch.nn.modules.
[01:38:47.900 --> 01:38:50.780]   But we can write our own module, and that's what this does.
[01:38:50.780 --> 01:38:54.180]   This lambda layer here takes in a function
[01:38:54.180 --> 01:38:57.380]   and will apply that function to its inputs.
[01:38:57.380 --> 01:38:59.340]   So this is a new nn.module.
[01:38:59.340 --> 01:39:02.180]   We're subclassing nn.module here to make
[01:39:02.180 --> 01:39:03.700]   a new layer called lambda.
[01:39:03.700 --> 01:39:09.280]   So sequential requires you to define everything
[01:39:09.280 --> 01:39:10.940]   as a bunch of modules.
[01:39:10.940 --> 01:39:15.460]   It's a module that contains effectively a list of modules.
[01:39:15.460 --> 01:39:17.900]   So we have to turn that first operation
[01:39:17.900 --> 01:39:21.660]   that we did in our convolutional network,
[01:39:21.660 --> 01:39:23.420]   just going back a little bit here.
[01:39:23.420 --> 01:39:25.500]   We have to turn all of these things
[01:39:25.500 --> 01:39:28.820]   into layers in order to run it with sequential.
[01:39:28.820 --> 01:39:35.100]   And that's exactly what our lambda layer
[01:39:35.100 --> 01:39:37.300]   will allow us to do.
[01:39:37.300 --> 01:39:40.620]   It will allow us to turn that last bit of the view
[01:39:40.620 --> 01:39:42.740]   into a sequential.
[01:39:42.740 --> 01:39:45.180]   So in order to make a model with sequential,
[01:39:45.180 --> 01:39:50.100]   we just pass in basically a list of the modules
[01:39:50.100 --> 01:39:51.500]   that we want to go into our model.
[01:39:51.500 --> 01:39:55.660]   So first, we reshape with that lambda layer.
[01:39:55.660 --> 01:39:58.800]   Then we apply convolutions and then rectified linear units
[01:39:58.800 --> 01:40:01.380]   convolutions, then rectified linear layer.
[01:40:01.380 --> 01:40:03.460]   Convolution, then rectified linear layer,
[01:40:03.460 --> 01:40:05.700]   closing out with the average pool 2D,
[01:40:05.700 --> 01:40:08.980]   and another lambda layer to do that last little bit
[01:40:08.980 --> 01:40:10.860]   of reshaping at the end.
[01:40:10.860 --> 01:40:13.860]   But now we've made our model with this simple nn.sequential
[01:40:13.860 --> 01:40:14.380]   here.
[01:40:14.380 --> 01:40:18.620]   And that's much cleaner than the way
[01:40:18.620 --> 01:40:22.020]   we were doing it before, where we had to define the forward
[01:40:22.020 --> 01:40:27.740]   pass, where we had to do a bunch of extra work.
[01:40:27.740 --> 01:40:30.680]   But if we're doing a relatively simple kind of neural network
[01:40:30.680 --> 01:40:34.320]   that just sort of feeds everything forward
[01:40:34.320 --> 01:40:37.640]   with no splitting, no branching, no control flow,
[01:40:37.640 --> 01:40:40.640]   then we can use nn.sequential.
[01:40:40.640 --> 01:40:42.200]   Let's take a look.
[01:40:42.200 --> 01:40:43.880]   The one thing I would want to say
[01:40:43.880 --> 01:40:47.900]   is that also this gives us a nicer look for our graph.
[01:40:47.900 --> 01:40:52.680]   The graph that we track with wandb.watch here
[01:40:52.680 --> 01:40:54.960]   only pays attention to the modules.
[01:40:54.960 --> 01:40:56.840]   And so by turning everything into a module,
[01:40:56.840 --> 01:40:58.620]   we now have a much clearer explication
[01:40:58.620 --> 01:41:03.160]   of what our model was that's stored in this nice, clean way
[01:41:03.160 --> 01:41:09.800]   along with our run data, and along with our charts,
[01:41:09.800 --> 01:41:14.560]   along with our metadata, and our source code,
[01:41:14.560 --> 01:41:16.200]   and all these other nice things.
[01:41:16.200 --> 01:41:18.880]   Oh, you can even see information about my system hardware.
[01:41:18.880 --> 01:41:21.320]   I got a 1080 Ti GPU, which we'll be
[01:41:21.320 --> 01:41:22.680]   making use of in just a minute.
[01:41:22.680 --> 01:41:29.840]   Oh, yeah.
[01:41:29.840 --> 01:41:34.080]   This little refactoring here that we need to do--
[01:41:34.080 --> 01:41:36.720]   we wrote a CNN, but we wrote it in a way
[01:41:36.720 --> 01:41:40.560]   that assumed we were getting 28 by 28 length vectors
[01:41:40.560 --> 01:41:44.080]   and had a CNN grid size at the end of 4 by 4
[01:41:44.080 --> 01:41:47.320]   when we built that average pooling.
[01:41:47.320 --> 01:41:49.360]   So let's just get rid of those two assumptions
[01:41:49.360 --> 01:41:52.360]   so that the model works with any 2D single channel image.
[01:41:52.360 --> 01:41:55.000]   So get rid of the lambda layer by moving the data
[01:41:55.000 --> 01:41:56.880]   preprocessing into a generator.
[01:41:56.880 --> 01:41:58.720]   So we'd have to write a new preprocessor
[01:41:58.720 --> 01:41:59.640]   for different data.
[01:41:59.640 --> 01:42:09.840]   But now it's at least separated out from the actual model code.
[01:42:09.840 --> 01:42:13.360]   Our model, our CNN model, now doesn't necessarily
[01:42:13.360 --> 01:42:17.800]   know that it's being fed images from MNIST in particular.
[01:42:17.800 --> 01:42:19.400]   Our data loader does.
[01:42:19.400 --> 01:42:21.540]   We're writing a wrapped version of our data loader
[01:42:21.540 --> 01:42:23.160]   that does the preprocessing for us.
[01:42:23.160 --> 01:42:24.880]   But this sort of thing about what's
[01:42:24.880 --> 01:42:26.940]   the shape of the inputs, that's properly
[01:42:26.940 --> 01:42:29.360]   considered that's the domain of the data loader,
[01:42:29.360 --> 01:42:32.720]   not the domain of the model.
[01:42:32.720 --> 01:42:35.880]   And that in order to be able to change
[01:42:35.880 --> 01:42:39.760]   the sizes of our input data but still get
[01:42:39.760 --> 01:42:41.240]   that the final output of our network
[01:42:41.240 --> 01:42:45.240]   is one number for each class, we replace the average pool
[01:42:45.240 --> 01:42:48.480]   2D with adaptive average pooling 2D, which defines
[01:42:48.480 --> 01:42:50.440]   the size of output tensors.
[01:42:50.440 --> 01:42:53.240]   And we say we want an output tensor that's one by one,
[01:42:53.240 --> 01:42:57.120]   rather than saying our input tensor is
[01:42:57.120 --> 01:42:59.600]   going to be four by four, which is basically what we did
[01:42:59.600 --> 01:43:02.000]   with nn.averagepool2D.
[01:43:02.000 --> 01:43:04.420]   So this allows our model to work with any size input, which
[01:43:04.420 --> 01:43:04.920]   is great.
[01:43:04.920 --> 01:43:07.840]   This is an important feature of convolutional neural networks.
[01:43:07.840 --> 01:43:11.400]   And it's easy to write them in a way that's easy to write
[01:43:11.400 --> 01:43:13.240]   but then makes it so that they can't handle
[01:43:13.240 --> 01:43:15.560]   different sizes of data.
[01:43:15.560 --> 01:43:17.240]   So this is the cleaner way of writing
[01:43:17.240 --> 01:43:19.820]   that convolutional network so that it works with different size
[01:43:19.820 --> 01:43:22.880]   inputs so that if I deploy a web app and it's operating
[01:43:22.880 --> 01:43:24.520]   on images, I don't want all my users
[01:43:24.520 --> 01:43:28.600]   to have to upload the exact 256 by 256 images.
[01:43:28.600 --> 01:43:33.880]   That's not good experience for somebody using my app.
[01:43:33.880 --> 01:43:35.520]   All right, so let's try it out.
[01:43:35.520 --> 01:43:38.720]   We've built this convolutional network.
[01:43:38.720 --> 01:43:43.040]   We've wrapped the data loader, made things nice and simple.
[01:43:43.040 --> 01:43:44.780]   We aren't actually checking to see that it
[01:43:44.780 --> 01:43:46.240]   works on data of different sizes.
[01:43:46.240 --> 01:43:47.600]   That would maybe be good.
[01:43:47.600 --> 01:43:49.860]   It's always good to test your code,
[01:43:49.860 --> 01:43:53.020]   do unit testing and integration testing.
[01:43:53.020 --> 01:43:55.060]   But we won't be doing that here.
[01:43:55.060 --> 01:43:57.080]   That's a really important component
[01:43:57.080 --> 01:43:59.100]   of designing good neural network code that's
[01:43:59.100 --> 01:44:02.140]   not covered in this tutorial.
[01:44:02.140 --> 01:44:03.800]   But yeah, maybe I should do something
[01:44:03.800 --> 01:44:05.780]   about that on this channel soon.
[01:44:05.780 --> 01:44:10.020]   Let me know in the chat if you think that's a good idea.
[01:44:10.020 --> 01:44:13.620]   So the last thing we'll do here is
[01:44:13.620 --> 01:44:17.660]   accelerate our code with a graphics processing unit,
[01:44:17.660 --> 01:44:19.100]   with a graphics card.
[01:44:19.100 --> 01:44:23.060]   So CUDA-capable GPUs, these are NVIDIA GPUs.
[01:44:23.060 --> 01:44:24.860]   Pretty much exclusively, it's kind of hard
[01:44:24.860 --> 01:44:28.060]   to run GPU-accelerated code for neural networks
[01:44:28.060 --> 01:44:30.860]   on anything other than an NVIDIA GPU.
[01:44:30.860 --> 01:44:32.340]   So these can be hard to come by.
[01:44:32.340 --> 01:44:34.100]   They can be really expensive.
[01:44:34.100 --> 01:44:37.100]   You're fighting with cryptocurrency miners
[01:44:37.100 --> 01:44:40.740]   and other folks to buy the GPUs.
[01:44:40.740 --> 01:44:42.860]   But you can also rent them for about $0.50 an hour
[01:44:42.860 --> 01:44:46.060]   from a cloud provider or use something like Google Colab
[01:44:46.060 --> 01:44:50.100]   to get them for free for limited periods of time.
[01:44:50.100 --> 01:44:52.620]   Or Kaggle kernels or paper space gradient,
[01:44:52.620 --> 01:44:54.860]   these are all places where you can find
[01:44:54.860 --> 01:44:57.300]   GPUs to accelerate your code.
[01:44:57.300 --> 01:45:00.580]   GPUs do matrix math really, really fast.
[01:45:00.580 --> 01:45:04.260]   And PyTorch is designed to make it
[01:45:04.260 --> 01:45:06.640]   so you can write this high-level Python the way we've
[01:45:06.640 --> 01:45:07.900]   been writing it.
[01:45:07.900 --> 01:45:09.740]   But then it gets compiled or turned
[01:45:09.740 --> 01:45:16.180]   into this fast GPU code if you have a GPU available.
[01:45:16.180 --> 01:45:20.700]   So you can check whether the GPU is available and working.
[01:45:20.700 --> 01:45:24.860]   So my GPU is available, ready to go.
[01:45:24.860 --> 01:45:27.980]   In Torch, this is like a core idea in Torch
[01:45:27.980 --> 01:45:31.140]   is that there are devices that are describing a model
[01:45:31.140 --> 01:45:31.980]   or building a model.
[01:45:31.980 --> 01:45:34.940]   And then there's possibly separate devices
[01:45:34.940 --> 01:45:37.300]   where the models actually run, where the actual tensor
[01:45:37.300 --> 01:45:39.300]   operations occur.
[01:45:39.300 --> 01:45:42.340]   And so these are Torch.devices.
[01:45:42.340 --> 01:45:44.100]   And the key thing that you have to do
[01:45:44.100 --> 01:45:46.500]   if you're writing PyTorch code as opposed
[01:45:46.500 --> 01:45:49.060]   to code in a higher-level library built on top of it
[01:45:49.060 --> 01:45:54.260]   is move models and tensors onto the appropriate device.
[01:45:54.260 --> 01:45:57.940]   So onto the GPU if you're using a GPU, onto a TPU
[01:45:57.940 --> 01:46:00.180]   if you're using that.
[01:46:00.180 --> 01:46:01.740]   This is something that you have to do
[01:46:01.740 --> 01:46:04.860]   if you're using one of these neural network accelerating
[01:46:04.860 --> 01:46:06.580]   chips.
[01:46:06.580 --> 01:46:14.340]   So for moving data tensors to an accelerator like a GPU or a TPU,
[01:46:14.340 --> 01:46:17.500]   that's kind of the responsibility of the data
[01:46:17.500 --> 01:46:21.220]   loader, in my opinion, rather than the model.
[01:46:21.220 --> 01:46:23.740]   The model shouldn't necessarily be the one keeping
[01:46:23.740 --> 01:46:27.300]   track of where the data is.
[01:46:27.300 --> 01:46:30.060]   The model does need to know where it is.
[01:46:30.060 --> 01:46:34.300]   It needs to know that it's on the GPU.
[01:46:34.300 --> 01:46:39.300]   And so the-- oops, I got an extra copy of the optimizer
[01:46:39.300 --> 01:46:39.860]   here.
[01:46:39.860 --> 01:46:42.860]   So let's drop that.
[01:46:42.860 --> 01:46:45.500]   So when we make our model and then move it
[01:46:45.500 --> 01:46:50.780]   to that device, to the GPU, just like we move our data tensors
[01:46:50.780 --> 01:46:57.820]   to the device when we load them, model that to device
[01:46:57.820 --> 01:47:03.540]   here to make sure that the model knows that it's on a GPU
[01:47:03.540 --> 01:47:06.140]   and should expect to operate on tensors that are on the GPU.
[01:47:06.140 --> 01:47:08.060]   If you forget to do either one of those things,
[01:47:08.060 --> 01:47:11.020]   you'll get a weird error, a kind of gnarly error message that's
[01:47:11.020 --> 01:47:11.940]   like, I can't--
[01:47:11.940 --> 01:47:14.380]   I don't know what to do if you give me one tensor on the CPU
[01:47:14.380 --> 01:47:15.260]   and one on the GPU.
[01:47:15.260 --> 01:47:18.100]   What am I supposed to do with that?
[01:47:18.100 --> 01:47:20.460]   So you have to track these things.
[01:47:20.460 --> 01:47:22.140]   If you're using a higher level framework,
[01:47:22.140 --> 01:47:25.140]   FastAI, PyTorch Lightning, Hugging Face, whatever,
[01:47:25.140 --> 01:47:28.580]   these things will be handled for you, which is very, very nice,
[01:47:28.580 --> 01:47:31.780]   especially when you're using multiple GPUs, multiple TPUs,
[01:47:31.780 --> 01:47:34.300]   and things get really wild.
[01:47:34.300 --> 01:47:37.780]   You may find that the code runs faster now.
[01:47:37.780 --> 01:47:41.620]   So in my experience, it hasn't necessarily
[01:47:41.620 --> 01:47:44.540]   run faster on my GPU.
[01:47:44.540 --> 01:47:46.100]   I think that's a little bit-- it has
[01:47:46.100 --> 01:47:48.140]   to do with the model would maybe need to be bigger.
[01:47:48.140 --> 01:47:50.140]   The batch sizes would need to be bigger.
[01:47:50.140 --> 01:47:53.340]   Yeah, this ran in 7 seconds instead of 10 seconds.
[01:47:53.340 --> 01:48:00.340]   Yeah, maybe running with bigger batch sizes for more epochs
[01:48:00.340 --> 01:48:03.900]   might be-- we might see a bigger runtime difference
[01:48:03.900 --> 01:48:06.940]   for this GPU-accelerated code.
[01:48:06.940 --> 01:48:09.100]   It could be challenging to write code that actually
[01:48:09.100 --> 01:48:11.100]   makes full use of the GPU.
[01:48:11.100 --> 01:48:13.180]   So actually, let me show you an example of that,
[01:48:13.180 --> 01:48:14.580]   wanb.me/traceworkspace.
[01:48:14.580 --> 01:48:24.980]   Real quick here, let me show you an example of some runs in--
[01:48:24.980 --> 01:48:26.700]   what an actual PyTorch--
[01:48:26.700 --> 01:48:29.180]   what the actual code that gets executed
[01:48:29.180 --> 01:48:32.300]   when you're running PyTorch, what that looks like using
[01:48:32.300 --> 01:48:34.780]   this little trace viewer here.
[01:48:34.780 --> 01:48:36.940]   Takes a second to load because it's
[01:48:36.940 --> 01:48:38.980]   got a lot of information in it.
[01:48:38.980 --> 01:48:40.340]   Great.
[01:48:40.340 --> 01:48:46.180]   So taking a look at this guy, the important thing--
[01:48:46.180 --> 01:48:48.700]   let me just collapse a bunch of this other stuff here.
[01:48:48.700 --> 01:48:51.660]   This is the stuff that's happening on the GPU here,
[01:48:51.660 --> 01:48:53.060]   this middle row.
[01:48:53.060 --> 01:48:54.820]   And you can see these colored blocks
[01:48:54.820 --> 01:48:57.860]   are operations that are happening on the GPU.
[01:48:57.860 --> 01:48:59.620]   And you can see there's lots of gray here.
[01:48:59.620 --> 01:49:02.260]   There's lots of times when the GPU is not doing anything.
[01:49:02.260 --> 01:49:07.660]   It's waiting on the Python code that's executing, for example.
[01:49:07.660 --> 01:49:09.940]   It's waiting on stuff happening on the CPU,
[01:49:09.940 --> 01:49:12.340]   not stuff that's happening on the GPU.
[01:49:12.340 --> 01:49:13.980]   And that can be particularly bad.
[01:49:13.980 --> 01:49:14.580]   Let me see.
[01:49:14.580 --> 01:49:20.860]   I think I have a nice report that directly shows these.
[01:49:20.860 --> 01:49:22.700]   We need to increase data loader efficiency.
[01:49:22.700 --> 01:49:25.740]   Yes, here's a good report, a little quick dashboard
[01:49:25.740 --> 01:49:30.620]   that I put together in W&B that shows that profiling
[01:49:30.620 --> 01:49:35.500]   information and shows an example of some accidentally badly
[01:49:35.500 --> 01:49:37.380]   written PyTorch code that doesn't really
[01:49:37.380 --> 01:49:39.940]   make good use of the GPU.
[01:49:39.940 --> 01:49:42.740]   So that's now loading for me.
[01:49:42.740 --> 01:49:43.420]   Great.
[01:49:43.420 --> 01:49:45.500]   Pop this guy open.
[01:49:45.500 --> 01:49:49.180]   And you can see-- actually, let's collapse those CPU
[01:49:49.180 --> 01:49:50.780]   traces.
[01:49:50.780 --> 01:49:54.620]   And you can see here this GPU stream here,
[01:49:54.620 --> 01:49:56.740]   which shows me everything that happened on the GPU.
[01:49:56.740 --> 01:49:59.060]   You can see it's almost entirely gray.
[01:49:59.060 --> 01:50:01.860]   There's almost nothing happening on the GPU most of the time.
[01:50:01.860 --> 01:50:04.700]   And it turns out, if you really look into this,
[01:50:04.700 --> 01:50:07.820]   you can see that it's because the data loader is not
[01:50:07.820 --> 01:50:09.180]   written right.
[01:50:09.180 --> 01:50:15.540]   And so you can read more about how to read those traces
[01:50:15.540 --> 01:50:19.140]   and how to write fast PyTorch code in the report
[01:50:19.140 --> 01:50:21.740]   that I just shared in the chat.
[01:50:21.740 --> 01:50:23.940]   Oh, yeah, Prashanth loves the profiler integration.
[01:50:23.940 --> 01:50:24.860]   I love it, too.
[01:50:24.860 --> 01:50:26.580]   I think it's awesome.
[01:50:26.580 --> 01:50:27.780]   All right.
[01:50:27.780 --> 01:50:31.300]   So it's almost 10 o'clock, which is
[01:50:31.300 --> 01:50:33.140]   when I plan to stop this stream.
[01:50:33.140 --> 01:50:35.540]   And conveniently enough, if I look,
[01:50:35.540 --> 01:50:37.220]   we are at the end of my slides.
[01:50:37.220 --> 01:50:40.260]   Yeah, I don't have anything more here.
[01:50:40.260 --> 01:50:43.100]   Yeah, let me just--
[01:50:43.100 --> 01:50:44.740]   yeah, look through here.
[01:50:44.740 --> 01:50:46.020]   There we go.
[01:50:46.020 --> 01:50:46.860]   Yeah.
[01:50:46.860 --> 01:50:49.660]   All right, using our GPU was the last sort of idea
[01:50:49.660 --> 01:50:51.180]   that I wanted to cover here.
[01:50:51.180 --> 01:50:56.500]   So let me actually turn off my screen share then.
[01:50:56.500 --> 01:51:00.460]   And maybe-- oh, no.
[01:51:00.460 --> 01:51:02.420]   I'm having some view--
[01:51:02.420 --> 01:51:04.880]   I don't know if anybody else in the audience does streaming,
[01:51:04.880 --> 01:51:08.340]   but I'm having a little bit of OBS troubles here.
[01:51:08.340 --> 01:51:15.460]   My stream does not look in OBS like it looks on YouTube.
[01:51:15.460 --> 01:51:19.940]   So I cannot move my face and resize it the way I'd like to
[01:51:19.940 --> 01:51:21.860]   as I say goodbye here.
[01:51:21.860 --> 01:51:24.460]   So the show must go on, I guess.
[01:51:24.460 --> 01:51:27.500]   So I'll just say we do a lot of these kinds
[01:51:27.500 --> 01:51:31.700]   of educational experiences here on the WNB YouTube channel.
[01:51:31.700 --> 01:51:35.380]   We've got a podcast that's sort of at a higher level, where
[01:51:35.380 --> 01:51:40.220]   our CEO interviews folks from the world of ML
[01:51:40.220 --> 01:51:43.380]   from the industry and sort of gets their inside takes on how
[01:51:43.380 --> 01:51:47.020]   to do ML well and learns--
[01:51:47.020 --> 01:51:49.260]   it's a very informative, really fun podcast,
[01:51:49.260 --> 01:51:50.860]   Gradient Descent.
[01:51:50.860 --> 01:51:52.100]   That's on our YouTube channel.
[01:51:52.100 --> 01:51:55.540]   And then I put up educational videos, do live streams.
[01:51:55.540 --> 01:51:57.420]   We've got a PyTorch reading group
[01:51:57.420 --> 01:51:59.220]   going through the official PyTorch book.
[01:51:59.220 --> 01:52:01.960]   We've got another reading group going through the Fast AI Fast
[01:52:01.960 --> 01:52:03.020]   book.
[01:52:03.020 --> 01:52:05.580]   So if you're not part of those initiatives,
[01:52:05.580 --> 01:52:07.740]   check out our YouTube channel for more information
[01:52:07.740 --> 01:52:10.220]   about how to join those.
[01:52:10.220 --> 01:52:14.420]   So with that, I'm going to close out here.
[01:52:14.420 --> 01:52:19.140]   And I will hopefully see you all around in the WNB community.
[01:52:19.140 --> 01:52:22.460]   We've got blog posts.
[01:52:22.460 --> 01:52:23.740]   We've got a Slack forum.
[01:52:23.740 --> 01:52:26.140]   We've got lots of places.
[01:52:26.140 --> 01:52:29.500]   And we'll be doing more of these live streams coming up.
[01:52:29.500 --> 01:52:32.020]   So yeah, I'm starting to see some thanks coming in
[01:52:32.020 --> 01:52:34.980]   in the chat from Wael and Rahul.
[01:52:34.980 --> 01:52:38.180]   Thanks so much for sticking through this session,
[01:52:38.180 --> 01:52:40.220]   learning a little bit more about PyTorch.
[01:52:40.220 --> 01:52:43.740]   And yeah, I look forward to seeing you all elsewhere
[01:52:43.740 --> 01:52:46.540]   in the world of machine learning.
[01:52:46.540 --> 01:52:50.820]   So until then, take care and happy learning.
[01:52:50.820 --> 01:52:55.820]   .

