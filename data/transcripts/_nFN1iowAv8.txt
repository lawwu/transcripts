
[00:00:00.000 --> 00:00:08.720]   weights and biases salon. So while folks are coming in, I thought I'd ask a little poll
[00:00:08.720 --> 00:00:15.440]   question as we've done at a few of these previous salons. So today was the final official announcement
[00:00:15.440 --> 00:00:22.720]   of the new NVIDIA line of GPUs, the 3000 series. And there's a lot of excitement by folks in the
[00:00:22.720 --> 00:00:29.440]   deep learning community about these, including Jeremy Howard of Fast.ai. So he was tweeting
[00:00:29.440 --> 00:00:34.880]   that pretty much everyone doing deep learning that can afford a 3090 is going to be buying at least
[00:00:34.880 --> 00:00:44.160]   one. So the stats of these new GPUs are right there next to my face. And so the question I
[00:00:44.160 --> 00:00:52.960]   wanted to ask was, are you going to be picking up one of this new line of NVIDIA GPUs? So you
[00:00:52.960 --> 00:01:04.240]   can answer this question by going to itempool.com/wanb/live. And yeah, just let us know,
[00:01:04.240 --> 00:01:11.440]   are you thinking about picking up the RTX 3090? That's the beefy one. It's got 24 gigabytes of
[00:01:11.440 --> 00:01:18.800]   RAM. But that one costs $1,500. So maybe you're one more for the a little bit more economical
[00:01:19.360 --> 00:01:24.560]   $700 RTX 3080. That's the one that I have my eyes on for my home machine.
[00:01:24.560 --> 00:01:29.600]   Oh, and it looks like our other panelist has joined us. Hey there, Andrew.
[00:01:29.600 --> 00:01:38.880]   So he'll be going second. Well, who we have coming up first is Dylan Payton of the Betka Lab at
[00:01:38.880 --> 00:01:44.640]   Tubingen. So folks are still coming in on YouTube and on Zoom. So I'll give them a second.
[00:01:47.040 --> 00:01:53.600]   Just to remind you, if you want to participate in this little poll here about the new NVIDIA
[00:01:53.600 --> 00:02:00.640]   GPUs, head to the URL that's above my head there, itempool.com/wanb/live and answer.
[00:02:00.640 --> 00:02:09.920]   So that's also been dropped into the chat, the Zoom chat, if you want to see it. And I'll make
[00:02:09.920 --> 00:02:14.640]   sure that Kayla drops that also into our YouTube chat. So we'll come back to that, and we'll see
[00:02:14.640 --> 00:02:27.680]   what the results are in between the two talks. So with that out of the way, let's go ahead and
[00:02:27.680 --> 00:02:33.200]   get started with the actual talks. So Dylan is joining us from Germany. It's very, very late
[00:02:33.200 --> 00:02:39.200]   there. So thanks a lot, Dylan, for being willing to stay up late to share the work that you've
[00:02:39.200 --> 00:02:48.000]   been doing with everybody in the salon community. Dylan is a good friend of mine because we worked
[00:02:48.000 --> 00:02:53.440]   together at the Redwood Center for Theoretical Neuroscience at Berkeley. We did our PhDs there.
[00:02:53.440 --> 00:02:59.440]   Dylan was in Vigorous Science. I was in Neuroscience. Can I make it any more obvious,
[00:02:59.440 --> 00:03:05.360]   to quote Avril Lavigne? So Dylan's been doing some really great work with the Betka Lab,
[00:03:05.360 --> 00:03:11.920]   and I'm really excited to see what he has in store for us. So Dylan, go ahead and take it away.
[00:03:11.920 --> 00:03:18.160]   Okay, cool. All right, you can see that okay?
[00:03:18.160 --> 00:03:22.800]   Yep. All right, yeah. So thanks a lot,
[00:03:22.800 --> 00:03:28.640]   Charles, for having me. I'm excited to talk about this work. It's a new approach to
[00:03:28.640 --> 00:03:33.840]   unsupervised representation learning with specific application to disentanglement.
[00:03:34.400 --> 00:03:38.880]   I worked on it with these people named on the slide, and like Charles said,
[00:03:38.880 --> 00:03:43.680]   in Matias Betka's lab at the University of TÃ¼bingen. Okay, so let's get started.
[00:03:43.680 --> 00:03:47.840]   I'm just going to, just as a quick overview, I'm going to spend some time talking about
[00:03:47.840 --> 00:03:52.880]   disentanglement. What is it? Why is it important? Where does it come from? What has been done so far?
[00:03:52.880 --> 00:03:57.440]   And then I'll go into our approach for disentanglement, which includes introducing
[00:03:57.440 --> 00:04:02.320]   new datasets intended to move the field towards more natural benchmarks.
[00:04:03.920 --> 00:04:08.000]   So as Charles said, my background is in vision science, so I like to think about the task of
[00:04:08.000 --> 00:04:14.320]   disentanglement in the context of the human visual system. Generally, our task is to disentangle the
[00:04:14.320 --> 00:04:19.440]   original sources from the mixed signal that comes to our eye, and this is an extremely difficult
[00:04:19.440 --> 00:04:25.360]   task. One example of why is that our eyes have a 2D array of receptors, and the world is in 3D.
[00:04:25.360 --> 00:04:30.080]   So for any image that would be projected under our eye, there's an infinite number of scenarios
[00:04:31.120 --> 00:04:37.280]   that could lead to that image. And this is also going to be true for disentangling from
[00:04:37.280 --> 00:04:44.560]   digital images or videos. And so in disentanglement, we have what we call sources. Some example of
[00:04:44.560 --> 00:04:48.400]   these sources are highlighted in red. They're pretty intuitive. They're basically just like
[00:04:48.400 --> 00:04:55.600]   the high-level descriptors that one would use to explain a scene. And so for one observation that
[00:04:55.600 --> 00:05:00.080]   we can use to our advantage in the natural world is that certain properties of the world are
[00:05:00.080 --> 00:05:04.160]   generally persistent while others change. For example, if the tiger moves, it's still a tiger.
[00:05:04.160 --> 00:05:09.200]   Or another example is as the day progresses, the lighting changes, the visual signal is going to
[00:05:09.200 --> 00:05:14.240]   change a lot, but the tree is still a tree. And so you might notice that the two examples I just
[00:05:14.240 --> 00:05:17.680]   gave involve things that change over time, and that's going to become important later
[00:05:17.680 --> 00:05:23.200]   when I explain our model. So ultimately, I'm interested in how the brain figures this out.
[00:05:23.200 --> 00:05:26.880]   None of the work that I'm going to present today is actually a proposal for specific computation
[00:05:26.880 --> 00:05:32.080]   in the brain. We do provide a partial solution to this problem that exploits the statistics of
[00:05:32.080 --> 00:05:37.440]   the world. And we demonstrate via an engineered approach that it's at least plausible that the
[00:05:37.440 --> 00:05:42.480]   brain could exploit such statistics as well. So as with any problem in science, the first thing
[00:05:42.480 --> 00:05:47.920]   we want to do is simplify it as much as possible and define it in more concrete terms. So that's
[00:05:47.920 --> 00:05:53.920]   what I'm going to do here. On this slide in black, I have the steps that we'd be taking that are
[00:05:53.920 --> 00:05:58.240]   associated with disentanglement. In red, I have examples of what those steps might be. And in blue,
[00:05:58.240 --> 00:06:02.720]   I've assigned some variables to make it easier to talk about later. So first, let's talk about the
[00:06:02.720 --> 00:06:09.520]   generator or the world. We have our sources, which I'm calling S. And so in the case of, for example,
[00:06:09.520 --> 00:06:13.840]   fake generated data, like in video games, we know exactly what those sources are. We have exact
[00:06:13.840 --> 00:06:18.720]   information for everything we need to make an image, like lighting and position, pose, identity.
[00:06:19.680 --> 00:06:25.440]   For real world data, natural images and videos, we don't know what all the sources are. We have
[00:06:25.440 --> 00:06:30.320]   to only guess. And these sources are mixed with the ground truth generative model. So for video
[00:06:30.320 --> 00:06:34.160]   games, it would be a graphics engine. And for natural signals, it's just like physical processes,
[00:06:34.160 --> 00:06:39.120]   like bouncing around, interacting with things in the world. And this mixture is a complicated
[00:06:39.120 --> 00:06:44.080]   process, produces these entangled signals, which we could think of as images. And then our goal
[00:06:44.080 --> 00:06:49.280]   is to try to disentangle that. So we want to learn something about this world process.
[00:06:50.240 --> 00:06:55.520]   One common next step is to use an encoder. So we want to recover sources from the mixed data
[00:06:55.520 --> 00:07:02.000]   and produce a disentangled code, which we'll call latent code. Many approaches also try to learn
[00:07:02.000 --> 00:07:07.440]   to generate new samples from the latent code. And so they learn to approximate the original
[00:07:07.440 --> 00:07:14.960]   ground truth generator and generate data samples. So in our model, what we're going to learn or
[00:07:15.200 --> 00:07:20.880]   things we could learn are the functions f and g, which could be neural networks or other algorithms.
[00:07:20.880 --> 00:07:25.360]   In our case, they're going to be neural networks. And there's a few criteria that we might use to
[00:07:25.360 --> 00:07:30.640]   check to see if f and g are correct. One is, does my latent code match my original sources? So if
[00:07:30.640 --> 00:07:36.000]   we have access to the original sources, we can just check that directly. Does my encoder match
[00:07:36.000 --> 00:07:41.360]   the inverse of the ground truth generator? That's at g star negative 1 means I'm looking at the
[00:07:41.360 --> 00:07:46.400]   inverse of that generator. Have I been able to undo that mixing process? Or does my generator
[00:07:46.400 --> 00:07:50.480]   match the ground truth generator? And an easy way to check that is just, does my generated data look
[00:07:50.480 --> 00:07:55.120]   like the original ground truth data? And so if I generate a data set x, does it look like the data
[00:07:55.120 --> 00:08:01.360]   set x star? Another common way to check that the latent code is sensible is to just purposefully
[00:08:01.360 --> 00:08:07.280]   vary the latent code z and look at what happens in the generated images. So for example, if z is a
[00:08:07.280 --> 00:08:12.000]   vector of 10 numbers and I want to vary just one of those numbers and I see that in my image,
[00:08:12.000 --> 00:08:17.440]   the size of an object changes without anything else changing, then I can assign that specific
[00:08:17.440 --> 00:08:25.120]   number in z a size label. So there's a lot of different approaches to disentanglement. And
[00:08:25.120 --> 00:08:31.360]   these different approaches use or focus on different portions of this task. So for example,
[00:08:31.360 --> 00:08:36.400]   ICA, independent component analysis, is a really common one that dates back to the '80s.
[00:08:37.200 --> 00:08:42.320]   And they only focus on the demixing part, the disentangling part. So the upsides of this type
[00:08:42.320 --> 00:08:48.720]   of algorithm is it's easier than considering the whole problem, both in terms of the mathematical
[00:08:48.720 --> 00:08:54.240]   description and the applications. It really narrows down the true problem of disentanglement
[00:08:54.240 --> 00:08:58.960]   to the question of, can we undo the process that happens in the real world? And another thing is,
[00:08:58.960 --> 00:09:03.280]   it was proven a long time ago that it can produce identifiable solutions, which I'm going to describe
[00:09:03.280 --> 00:09:08.960]   more in a few slides. However, some downsides are, one, we don't have a good way of verifying
[00:09:08.960 --> 00:09:13.360]   that we've preserved all the information. So we don't really have a good way of checking that
[00:09:13.360 --> 00:09:18.480]   z has all the information in x, whereas if we were regenerating data samples, then we would
[00:09:18.480 --> 00:09:23.200]   know for sure. And we also don't have a good way of generating new data, which has a lot of its
[00:09:23.200 --> 00:09:30.160]   own interesting applications. So ICA has been around for a very long time. And so there's a
[00:09:30.160 --> 00:09:37.200]   bunch of places where ICA gets used in industry. So some classic applications of ICA are the cocktail
[00:09:37.200 --> 00:09:41.040]   party problems. This is where you have a cocktail party, a bunch of people are talking, you have a
[00:09:41.040 --> 00:09:46.400]   couple of microphones in the room, and you're recording all the conversations. And so the
[00:09:46.400 --> 00:09:50.560]   conversations are getting mixed. And then the goal is to de-mix that conversation or disentangle it
[00:09:50.560 --> 00:09:56.800]   so that the output is individual persons, what they're saying. Another example of ICA is estimating
[00:09:56.800 --> 00:10:01.040]   the underlying structure of natural images. So people have shown that you can take a natural
[00:10:01.040 --> 00:10:07.440]   image as input, and the output is estimates of the core atomic elements that could be used to
[00:10:07.440 --> 00:10:13.040]   construct those natural images, visual elements. I don't mean like literal atomic elements.
[00:10:13.040 --> 00:10:19.200]   So another interesting application that I thought would be relevant with the recent,
[00:10:19.200 --> 00:10:23.680]   or topical with the recent news on Neuralink is to look at brain data. So this is not the same
[00:10:23.680 --> 00:10:28.080]   type of data as Neuralink, but still interesting nonetheless. And the reason why this is
[00:10:28.080 --> 00:10:35.440]   interesting is because it allows me to, or it has relation to identifiability. So the task here is
[00:10:35.440 --> 00:10:40.240]   we have a EEG array. So it's just like a thing that goes in your head and it records brain
[00:10:40.240 --> 00:10:45.280]   activity. And the brain activity is coming from neurons. Neurons are producing electrical signals,
[00:10:45.280 --> 00:10:48.720]   but they're getting mixed up. They're going through brain data, through your skull, et cetera.
[00:10:49.600 --> 00:10:55.920]   And so if we train a disentangling model on ICA, the goal is to get out relevant signals that are
[00:10:55.920 --> 00:11:02.400]   relevant to cognition or action or whatever. And so identifiability is important in this case.
[00:11:02.400 --> 00:11:07.520]   What identifiability means is that we can guarantee mathematically that the solution it
[00:11:07.520 --> 00:11:12.160]   finds is the correct solution. And there's different amounts of identifiability from an
[00:11:12.160 --> 00:11:19.200]   exact match to a match up to permutations or up to some linear operation. It was shown a long time
[00:11:19.200 --> 00:11:26.400]   ago that ICA can do identifiable disentanglement when the process is linear, but it's actually
[00:11:26.400 --> 00:11:30.880]   impossible to do it when the process is nonlinear for the standard ICA algorithm.
[00:11:30.880 --> 00:11:39.120]   And so there have been solutions proposed for quite a while for nonlinear ICA, but only in
[00:11:39.120 --> 00:11:43.280]   the last five years or so have they really started making advancements to realistic data.
[00:11:43.280 --> 00:11:48.080]   And these solutions assume that you have some additional information that we need
[00:11:48.640 --> 00:11:53.920]   that we didn't need with regular ICA. So with linear ICA, we can solve the problem just with
[00:11:53.920 --> 00:11:58.640]   random samples of the data, but with nonlinear ICA, we need more information like a label
[00:11:58.640 --> 00:12:03.120]   or a time step or some guarantee that a certain number of sources are only changing
[00:12:03.120 --> 00:12:10.400]   from one time point to another. So another alternative method for doing disentanglement
[00:12:10.400 --> 00:12:15.520]   is with generative adversarial networks or GANs. And so these kind of look at the second half of
[00:12:15.520 --> 00:12:20.960]   the problem only. They just generate data from random samples of a latent code and then check
[00:12:20.960 --> 00:12:25.920]   that that data looks realistic. So in practice, these are the most successful in terms of large
[00:12:25.920 --> 00:12:31.440]   scale datasets. They can work on high definition images, much higher scale datasets than ICA,
[00:12:31.440 --> 00:12:36.000]   but they're difficult to construct a good metric for assessing disentanglement because you've
[00:12:36.000 --> 00:12:41.200]   completely disregarded the original ground truth sources in the whole framework. And so there's
[00:12:41.200 --> 00:12:46.720]   no notion of identifiability and therefore the model itself is less interpretable. And another
[00:12:46.720 --> 00:12:51.040]   important difference is that we can't actually apply it to the same tasks that we could apply
[00:12:51.040 --> 00:12:58.960]   ICA to. I can't give you data and have you pass it through your encoder and give me sources.
[00:12:58.960 --> 00:13:07.040]   So the third approach is the one that we use, which is to use auto encoders. So the auto
[00:13:07.040 --> 00:13:12.000]   encoder approach uses all the steps above and the upsides is in theory all of the same upsides as
[00:13:12.000 --> 00:13:18.080]   the other two. However, generally it works in practice on larger scale data than ICA,
[00:13:18.080 --> 00:13:24.320]   but smaller scale data than GANs. And until very recently, there were not any approaches to prove
[00:13:24.320 --> 00:13:31.440]   identifiability for auto encoder based models. But some recent work has shown that you can
[00:13:31.440 --> 00:13:36.400]   produce an unidentifiable disentanglement code. I don't have time to go into that now,
[00:13:36.400 --> 00:13:41.600]   but I'd encourage people to check out our paper or the references on the screen to look into those.
[00:13:41.600 --> 00:13:45.600]   So our approach is using an auto encoder. So I'm going to spend the rest of my time
[00:13:45.600 --> 00:13:51.760]   focusing on that. Okay. So that's kind of the background. Now I'm going to go into
[00:13:51.760 --> 00:13:57.280]   our approach, which we'll start with looking at benchmarks that are used for this task.
[00:13:57.280 --> 00:14:05.280]   So disentanglement task is best done when you have a metric that you can use to assess the
[00:14:05.280 --> 00:14:10.160]   performance. And so a big advancement was made recently in this with the disentanglement library.
[00:14:10.160 --> 00:14:16.320]   And so this library, they use a collected a bunch of different generative graphics engines.
[00:14:16.320 --> 00:14:23.680]   And so that means we know the exact parameters used to generate images. And
[00:14:23.680 --> 00:14:30.800]   it allows us to be able to verify the ground truth generating sources with what comes out
[00:14:30.800 --> 00:14:37.440]   of per model. So as I alluded to before, it's been known for like 20 years or so that it's
[00:14:37.440 --> 00:14:40.880]   not possible to perform identifiable disentanglement when you're only getting
[00:14:40.880 --> 00:14:47.280]   independent image samples. And so what we have done is achieved this identifiable disentanglement
[00:14:47.280 --> 00:14:53.040]   by extending these datasets into the time domain, and then using time information to
[00:14:53.040 --> 00:14:57.840]   constrain our model using the statistics that we found to constrain our model.
[00:14:59.360 --> 00:15:04.160]   So when extending it to the time domain, we asked the question of what time statistics do we want
[00:15:04.160 --> 00:15:09.360]   to use? And we decided to try to use real data. So the first dataset that we use is YouTube.
[00:15:09.360 --> 00:15:17.120]   So we pulled videos straight from YouTube. And then this other group uses a pre-training image
[00:15:17.120 --> 00:15:23.360]   segmentation algorithm to extract these binary masks. And so we take the masks and we record
[00:15:23.360 --> 00:15:29.120]   the measure, the scale and position data of the masks. And then we construct our own sprite
[00:15:29.120 --> 00:15:35.040]   dataset using the measurements that we have. And we call that natural sprites. So natural sprites
[00:15:35.040 --> 00:15:40.160]   are images generating using the sprite world graphics engine. So they're simple, well-controlled
[00:15:40.160 --> 00:15:45.760]   objects. We know all the parameters like the shape and orientation and everything. But we've added
[00:15:45.760 --> 00:15:52.320]   time component to the made them videos. And these statistics of this time match exactly the
[00:15:52.320 --> 00:15:58.880]   statistics from these YouTube videos. So this is a step, the first step towards running this problem
[00:15:58.880 --> 00:16:08.960]   on realistic videos. As a next step, we used the kitty dataset. So this is masks extracted directly
[00:16:08.960 --> 00:16:15.440]   from self-driving car cameras. So we have these dataset of videos from self-driving cars. And the
[00:16:15.440 --> 00:16:21.360]   cars also have a LIDAR sensor. It's a laser depth sensor, which makes it really good at detecting
[00:16:21.360 --> 00:16:25.600]   objects in the world. If a person's in front of a car, they're going to be at a certain depth and
[00:16:25.600 --> 00:16:29.120]   everything else is going to be at a different depth. So it's easy to get a very fine grained mask.
[00:16:29.120 --> 00:16:35.040]   And so we, again, another group uses this information to extract these masks. We pulled
[00:16:35.040 --> 00:16:41.520]   out the human category and converted it into this dataset. And again, we measure the position and
[00:16:41.520 --> 00:16:48.640]   area of these ground truth vectors. So this is, again, a further step closer towards real video.
[00:16:49.200 --> 00:16:53.840]   The downside of this compared to natural sprites is that we've, with natural sprites, we have all
[00:16:53.840 --> 00:16:59.040]   of the parameters for generating the video. We know exactly what parameters went into the
[00:16:59.040 --> 00:17:05.840]   generating model. Whereas here, we only have the position and area. We're just limited by what we
[00:17:05.840 --> 00:17:15.120]   can measure. So now that we have these datasets, we wanted to build a model that exploits the
[00:17:15.120 --> 00:17:19.920]   statistics of this data or is constrained by the statistics of this data. So we measured those
[00:17:19.920 --> 00:17:25.840]   statistics. So these are looking at the time varying statistics. This is looking at transitions
[00:17:25.840 --> 00:17:31.600]   of these measurements from one frame to the next. And we found that the distributions are all sparse.
[00:17:31.600 --> 00:17:37.520]   So what that means is that they're peaked at zero and they have very heavy tails. So that means sharp
[00:17:37.520 --> 00:17:43.040]   changes could occur in some latent sources, but most of the other sources would remain unchanged
[00:17:43.040 --> 00:17:48.960]   between adjacent time points. So now when I'm saying sources, I mean properties like position,
[00:17:48.960 --> 00:17:54.320]   identity, and step size. So as an example, if you imagine you're sitting at a traffic light and
[00:17:54.320 --> 00:17:59.440]   someone's going across the crosswalk, that person's x position might change quite a bit in time,
[00:17:59.440 --> 00:18:02.960]   but their size and identity and shape are relatively constant.
[00:18:02.960 --> 00:18:11.680]   Using this idea, we decided to constrain our model by imposing a prior that adhered to this
[00:18:12.640 --> 00:18:20.080]   this statistic that we see. So again, we have an auto encoder model. We get images
[00:18:20.080 --> 00:18:25.120]   x. I'm showing my pointer here. And then it goes through an encoder to get our latent code z.
[00:18:25.120 --> 00:18:31.200]   And what we do is we establish a prior on our model to impose this regularity that we saw
[00:18:31.200 --> 00:18:38.320]   in the natural data. And so this is represented here by this heat map. So the darker red indicates
[00:18:38.320 --> 00:18:42.800]   a higher probability. So for the first time step, we don't really know anything about it. So we have
[00:18:42.800 --> 00:18:48.560]   a very general prior that we place in the model. And that's signified here. And the next time step,
[00:18:48.560 --> 00:18:54.000]   this is the encoding at that first time step. So now we actually have seen the object and we know
[00:18:54.000 --> 00:18:57.840]   where it is, which we marked by the x. And we have some probability distribution around it.
[00:18:57.840 --> 00:19:06.240]   We, given that information, we can enforce another prior of the time step t given time step t minus
[00:19:06.240 --> 00:19:10.800]   1. And this is the one that we asked to be sparse. So we, again, say that we only want a few of the
[00:19:10.800 --> 00:19:16.000]   latents to change. We only expect a few of the latents to change. And now when we encode the
[00:19:16.000 --> 00:19:21.280]   next time step, because we have imposed this prior, we have a high likelihood of doing a good
[00:19:21.280 --> 00:19:26.640]   job at encoding the next time step. And I'm not going to go into it very much. But it turns out
[00:19:26.640 --> 00:19:32.240]   that having this sparse prior, because it is shaped the way it is, it allows us to prove
[00:19:32.240 --> 00:19:41.600]   identifiability with a very simple proof. OK, so we applied this to both the data sets
[00:19:41.600 --> 00:19:46.640]   in the disentanglement loop as well as our new data sets. And we found that we did a lot better
[00:19:46.640 --> 00:19:50.480]   than previous approaches. This is a really big plot. There's a lot going on. I'm just going to
[00:19:50.480 --> 00:19:55.840]   highlight the important points, most notably the red circle around our model, which does the best.
[00:19:55.840 --> 00:20:01.600]   And the way you can interpret these green boxes here is this is looking at the correlation between
[00:20:01.600 --> 00:20:06.640]   the latent components in z and the ground truth factors. So each column is a different ground
[00:20:06.640 --> 00:20:12.720]   truth factor. So ideally, if it was a perfect solution, you would just see a dark green row
[00:20:12.720 --> 00:20:17.840]   diagonal of 100, indicating that every ground truth factor is perfectly correlated with one
[00:20:17.840 --> 00:20:23.440]   of the latents. And another way to look at it is to just look at the values themselves. So here,
[00:20:23.440 --> 00:20:28.960]   what we're doing is we're changing those latent values. And we're looking at how z-- or sorry,
[00:20:28.960 --> 00:20:32.720]   we're changing the ground truth values. And we're looking at how z changes with respect to that.
[00:20:32.720 --> 00:20:37.120]   And so again, a perfect solution would be a diagonal line. Or in the case of shape,
[00:20:37.120 --> 00:20:40.000]   where there's discrete categories, we would just have separated points.
[00:20:40.000 --> 00:20:44.240]   And you can see that a solution like this, for example,
[00:20:44.240 --> 00:20:48.880]   it'd be very difficult to decode what the x position was given a value of z.
[00:20:54.320 --> 00:21:00.800]   So the summary of the advantages to our approach, it's parsimonious. So our mathematical setup is
[00:21:00.800 --> 00:21:05.040]   simple. It can be applied to a lot of different data types, including videos with natural
[00:21:05.040 --> 00:21:11.440]   transitions. It's identifiable. So we prove that our model is more identifiable than previous
[00:21:11.440 --> 00:21:17.760]   approaches. By more, I mean we have fewer constraints. It's applicable to a broader
[00:21:20.720 --> 00:21:27.920]   amount of data sets. And it's identifiable up to permutations as opposed to up to something like
[00:21:27.920 --> 00:21:33.520]   a linear transformation, which previous approaches were. And then we also just empirically show
[00:21:33.520 --> 00:21:39.120]   improved disentanglement on these more constrained data sets. Ultimately, our goal is to move towards
[00:21:39.120 --> 00:21:44.960]   natural data. But we believe that we've offered these data sets as a way to help push the field
[00:21:44.960 --> 00:21:51.680]   in that direction. Yeah, and that's what I got. So this is the links that you can go to
[00:21:51.680 --> 00:22:00.160]   to check out more. Yeah, thanks a lot. Open to questions. Great. So if folks have questions,
[00:22:00.160 --> 00:22:06.480]   you can put them in the Zoom chat or the YouTube chat, and we'll pose them to Dylan. Also,
[00:22:06.480 --> 00:22:12.800]   Andrew, if you have questions, it'd be great to hear any questions that you have. I'll kick us
[00:22:12.800 --> 00:22:19.120]   off with a couple of questions. So the first question is, what do you think is needed to take
[00:22:19.120 --> 00:22:25.520]   these nonlinear disentanglement approaches and be able to apply them to full high definition
[00:22:25.520 --> 00:22:32.640]   natural video? Is it just calculation? Is it just computation, rather? Or are there big algorithmic
[00:22:32.640 --> 00:22:40.480]   advances that need to be made? Yeah, that's a tough question. So it's not really clear what--
[00:22:40.480 --> 00:22:46.400]   so autoencoders by themselves are capable of encoding and decoding high resolution images.
[00:22:46.400 --> 00:22:52.720]   When you stick them in this variational framework, so this probabilistic framework, where you're
[00:22:52.720 --> 00:22:57.760]   not just asking it to encode an image and then decode that image, but you're also asking for
[00:22:57.760 --> 00:23:02.640]   the encoding to have these certain properties, it seems like the scalability kind of falls off.
[00:23:03.920 --> 00:23:10.480]   And so this might come down to just the network's ability to approximate this probabilistic
[00:23:10.480 --> 00:23:15.600]   distribution, which is what it's trying to do with variational inference. And so one thing that
[00:23:15.600 --> 00:23:22.640]   we're working on now with continuing this is to modify the architecture to an architecture that
[00:23:22.640 --> 00:23:27.280]   can handle-- like a recurrent architecture that can handle more complicated computation
[00:23:28.000 --> 00:23:33.680]   without having to have an extremely deep network. And so we're hoping that we can
[00:23:33.680 --> 00:23:37.840]   go farther with fewer layers to try to scale up the model.
[00:23:37.840 --> 00:23:47.360]   Interesting. So Joshua Clancy from YouTube asks, how did you implement those priors? So how are
[00:23:47.360 --> 00:23:54.080]   those priors enforced on the autoencoder? Sure, yeah. So it ends up just being a training loss,
[00:23:54.080 --> 00:24:00.400]   but it comes from a probabilistic description. So the variational autoencoder basically specifies
[00:24:00.400 --> 00:24:12.480]   that the encoder itself solves a lower bound on likelihood of the representation given the images.
[00:24:12.480 --> 00:24:21.120]   And so we end up with a loss function that is a term that asks that information is preserved as
[00:24:21.120 --> 00:24:26.560]   much as possible. So that's basically looking at the difference between x star and x. And then a
[00:24:26.560 --> 00:24:33.200]   second term that says that we want the latent space to look as much as possible like random
[00:24:33.200 --> 00:24:39.040]   draws from these two probability distributions where-- let's see if we can go back to the slide.
[00:24:39.040 --> 00:24:44.640]   So the one probability distribution is just a standard Gaussian prior. So this is the standard
[00:24:44.640 --> 00:24:49.920]   prior that's used in all VAEs. And then we added a second probability distribution. Given the first
[00:24:49.920 --> 00:24:57.040]   time point, we encode the first image. We use that image to condition our second prior. And
[00:24:57.040 --> 00:25:03.600]   then when we feed in the second image, we have a loss term for the second image using the second
[00:25:03.600 --> 00:25:08.000]   prior. So it ends up just being the same KL divergence-- or similar KL divergence term that
[00:25:08.000 --> 00:25:12.960]   you get in the standard VAE framework. Interesting. While we're on this slide,
[00:25:12.960 --> 00:25:18.160]   I think I just wanted to check my understanding of your identifiability proof. The essence of it is
[00:25:18.160 --> 00:25:23.200]   basically that if you look at that prior t given t minus 1, you can see that it's axis aligned,
[00:25:23.200 --> 00:25:30.400]   right? That you see those two sharp lines oriented up and horizontally and vertically.
[00:25:30.400 --> 00:25:35.040]   And that's what gives you identifiability that the axes are really important as opposed to that
[00:25:35.040 --> 00:25:41.040]   Gaussian prior where it's isotropic. And so there's no special directions of any kind.
[00:25:41.040 --> 00:25:45.600]   Yeah, exactly. So the prior that you're looking at here for t minus 1, that's just a standard
[00:25:45.600 --> 00:25:50.720]   Gaussian prior, which is used for beta VAEs and other VAE approaches to disentanglement.
[00:25:50.720 --> 00:25:54.960]   And the problem with this prior is it's rotationally symmetric. So what that means
[00:25:54.960 --> 00:26:00.800]   is in my latent space, I can multiply my z variable by a rotation matrix that can rotate
[00:26:00.800 --> 00:26:06.480]   at any arbitrary amount. And it would still be equally valid under our loss function.
[00:26:06.480 --> 00:26:12.080]   And so there's no way of knowing, given the loss, whether we actually found the right solution or
[00:26:12.080 --> 00:26:17.280]   not, because the ground truth has one solution. And our latent has infinite solutions depending
[00:26:17.280 --> 00:26:22.560]   on what rotation you do. And that's actually the core argument for ICA not being identifiable.
[00:26:22.560 --> 00:26:29.200]   And so yeah, the shape of our prior, the fact that we chose a sparse prior, it has these axis
[00:26:29.200 --> 00:26:34.960]   aligned probability densities, marginals. That means that when you rotate it, it's not the same
[00:26:34.960 --> 00:26:40.800]   anymore. And so if it minimizes the loss, if it finds a solution, it is the right solution.
[00:26:40.800 --> 00:26:48.000]   And I should say that with this kind of stuff, identifiability is proven in this mathematical
[00:26:48.000 --> 00:26:51.600]   sense. And of course, we have to relax a lot of those constraints when we apply the model in
[00:26:51.600 --> 00:26:56.480]   practice. And so it was important for us to do a lot of empirical tests at the same time. We show
[00:26:56.480 --> 00:27:02.720]   identifiability, but we also want to make sure that when we relax the constraints that we needed
[00:27:02.720 --> 00:27:07.600]   to impose to show identifiability to actually run it on real data, we still see a good job
[00:27:07.600 --> 00:27:13.120]   at disentanglement. So when you look at the results, like no, it's not perfect, but it's
[00:27:13.120 --> 00:27:18.640]   doing a lot better than everyone else. And in theory, given the right conditions, it should
[00:27:18.640 --> 00:27:24.240]   do perfectly. And we do test it on toy data, like very toy data, where it exactly matches
[00:27:24.240 --> 00:27:33.280]   all the conditions and then it perfectly disentangles it. Cool. Yeah, this is interesting
[00:27:33.280 --> 00:27:38.800]   work. And I'm looking forward to see as you continue closer towards nonlinear disentanglement
[00:27:38.800 --> 00:27:44.240]   in natural data. And hopefully folks take up these data sets that you put out there,
[00:27:44.240 --> 00:27:52.080]   that kitty mask data set, and improve on this and make some steps towards better disentangling
[00:27:52.080 --> 00:27:58.480]   natural videos. Yeah. So a lot of the-- Yash and Lucas and David, they did a lot of work
[00:27:58.480 --> 00:28:04.240]   on the GitHub repository. So if you're interested in trying this out, that same repo is where you
[00:28:04.240 --> 00:28:10.560]   go to get the data and also where you go to test out our model. And it integrates pretty well with
[00:28:10.560 --> 00:28:18.560]   the disentanglement library already and the metrics that they use. Great. Well, so that's
[00:28:18.560 --> 00:28:28.080]   all the time we have for Dylan's talk. Thanks for coming. And so while we-- looks like Andrew's
[00:28:28.080 --> 00:28:34.960]   getting ready. While that's happening, I want to reveal the results of our poll. So we asked
[00:28:34.960 --> 00:28:43.280]   folks whether they were going to pick up one of the new line of NVIDIA GPUs. And let's see what
[00:28:43.280 --> 00:28:54.480]   folks said. And it looks like we've got two people who aren't going to pick up anything,
[00:28:54.480 --> 00:28:59.120]   two people who are going to pick up the 3090, and two people who are going to pick up the 3080.
[00:28:59.120 --> 00:29:06.720]   So interesting. Looks like people-- the 3070 there, the cheapest one, is I think $499. Really,
[00:29:06.720 --> 00:29:11.360]   you know, very affordable. It looks like folks wanted something a little beefier. The 3090 there
[00:29:11.360 --> 00:29:19.040]   with the 24 gigabytes of RAM, really good for running your tiny GPTs or what have you.
[00:29:20.960 --> 00:29:27.200]   Great. Well, thanks, everybody, for participating in the poll. And let's turn it over to Andrew.
[00:29:27.200 --> 00:29:33.440]   So I'm really excited. Hi, everybody. Now you can see I'm just sitting here in my backyard.
[00:29:33.440 --> 00:29:39.040]   So this is a real scene, not a fake one. Still enjoying the good weather here. I'm just outside
[00:29:39.040 --> 00:29:44.480]   of Portland, Oregon. Actually, I'm going to ask you, Charles, I could go either two directions
[00:29:44.480 --> 00:29:50.960]   on presentations. I was going to do my sort of generalized presentation on what people are doing
[00:29:50.960 --> 00:29:58.720]   in production here in 2020. But since you were all talking about autoencoders and that, I have a
[00:29:58.720 --> 00:30:05.680]   20, 25-minute-long training presentation, how to write autoencoders in TensorFlow.
[00:30:05.680 --> 00:30:09.200]   What do you think would be more appropriate for our audience?
[00:30:11.280 --> 00:30:16.240]   Yeah, I think that the first thing that you mentioned is probably the best one,
[00:30:16.240 --> 00:30:22.000]   just a sort of general overview. I'm really excited about your deep learning design patterns
[00:30:22.000 --> 00:30:26.960]   book. That's what I've been telling people about when I've been promoting the salon. So it'd be
[00:30:26.960 --> 00:30:31.840]   great to see some of those ideas and thoughts. Okay. All right. So let me, I'm going to share
[00:30:31.840 --> 00:30:43.360]   my screen here. Give me a moment. Chrome tab. There we go. Okay. People should be able to see
[00:30:43.360 --> 00:30:54.000]   my screen. Yep. So I'll go ahead and fall presentation mode. Okay. For those who aren't
[00:30:54.000 --> 00:31:00.880]   familiar with me, I'm Andrew Ferilich. I work at Google Cloud AI. I'm in developer relations.
[00:31:00.880 --> 00:31:06.240]   That means I'm primarily external facing. So you'll see me at events like this.
[00:31:06.240 --> 00:31:14.480]   Sometimes I'm at universities as an adjunct professor. Other times I'm with some of our
[00:31:14.480 --> 00:31:22.560]   very large enterprise clients, helping their ML teams get over production problems.
[00:31:22.560 --> 00:31:30.160]   But because I'm external facing and I'm out there a lot, I've had the opportunity over the last few
[00:31:30.160 --> 00:31:38.800]   years really to see firsthand what's been transitioning in production at the very largest
[00:31:38.800 --> 00:31:46.880]   places. Okay. So I'm going to start here with a little timeline. Okay. So-
[00:31:46.880 --> 00:31:52.080]   Just a quick note. The bottom of your slides, I think are slightly cut off. I think if you just-
[00:31:52.080 --> 00:31:57.040]   Yeah. I hear on some presentations that happens. So I'm going to go out of full mode and just go
[00:31:57.040 --> 00:32:01.120]   like this. Okay. That way nothing gets cut off. Right?
[00:32:01.120 --> 00:32:02.880]   Great. Yeah.
[00:32:02.880 --> 00:32:12.400]   Okay. All right. So when I started in production, it was early 2017. And the marketing data back
[00:32:12.400 --> 00:32:21.120]   then said there was about 10,000 data scientists. Okay. And AI and ML was just coming on everybody's
[00:32:21.120 --> 00:32:27.760]   radar. And most of the people involved in the big companies, you had decision makers,
[00:32:27.760 --> 00:32:35.680]   engineering staff, but they really lacked having really data science or MLE people.
[00:32:35.680 --> 00:32:42.800]   So most of that workforce was being done as an ML consultant. Okay. By 2018,
[00:32:42.800 --> 00:32:49.120]   a lot of these companies were moving away from planning and discovery to exploration and
[00:32:49.120 --> 00:32:55.440]   prototyping. And so now you started to see senior engineering management involved. Typically by now,
[00:32:55.440 --> 00:33:01.280]   at least one senior data scientist has been hired as a lead and they're starting to fill in the team.
[00:33:01.280 --> 00:33:08.080]   You know, with data engineers, I started to notice that they tended to pull people from the DB,
[00:33:08.080 --> 00:33:14.160]   the database team. Okay. And again, a strong dependence on ML consultants.
[00:33:14.720 --> 00:33:23.280]   Where we really saw the change was in 2019. By now, you know, AI/ML is a fundamental aspect
[00:33:23.280 --> 00:33:30.880]   of their business. You know, the CTO is involved. You now have really rounded out the data science
[00:33:30.880 --> 00:33:36.560]   team, hiring, you know, junior data scientists to support the senior data scientists. You have
[00:33:36.560 --> 00:33:43.120]   machine learning engineers, data engineers, you're starting to bring in ML ops. And we also saw that
[00:33:43.120 --> 00:33:52.240]   they started using third party services like ML API services and turnkey solutions were emerging
[00:33:52.240 --> 00:33:59.760]   at the time from integrated system vendors. By 2020, you know, which is the current, or at least
[00:33:59.760 --> 00:34:08.000]   before the pandemic hit, you know, really at the enterprise level, the entire C-suite is involved.
[00:34:09.040 --> 00:34:15.440]   And you just see, you know, again, a continued growth in the number of people hired into these
[00:34:15.440 --> 00:34:22.480]   positions. And you also see a lot more use of what we call now managed services. And those like,
[00:34:22.480 --> 00:34:28.240]   come not just, you know, obviously Google provides managed services, but there are a lot of very
[00:34:28.240 --> 00:34:34.880]   large companies that do. And then really a big growth in turnkey solutions. And also the labor
[00:34:34.880 --> 00:34:42.640]   pool has changed a lot. We've gone from 10,000 data scientists three years ago to now 250,000
[00:34:42.640 --> 00:34:49.840]   data scientists worldwide and 2 million people who are characterized as ML practitioners. So it's
[00:34:49.840 --> 00:34:57.360]   been quite an explosion in the workforce. So we'll just quickly go over sort of the transition
[00:34:58.080 --> 00:35:07.520]   in the technology. Okay. So again, when I started in 2017, I'm going to our clients and we're talking
[00:35:07.520 --> 00:35:14.160]   plant, they're talking planning and discovery. They were still doing what I would call business
[00:35:14.160 --> 00:35:20.400]   intelligence, sort of that classical form of machine learning. They weren't doing deep learning.
[00:35:20.400 --> 00:35:29.520]   The tools they tended to be using were psychic learn and the NLTK toolkit. Typically they were
[00:35:29.520 --> 00:35:35.440]   doing cart analysis and the background of many of the people involved was a statistics background.
[00:35:35.440 --> 00:35:43.680]   2018 is really when we started to see a big change, starting to move towards deep learning,
[00:35:43.680 --> 00:35:50.080]   starting to see bringing in TensorFlow and PyTorch, start using things like transfer learning. Again,
[00:35:50.480 --> 00:35:57.440]   starting to use third party services like ML APIs, managed training services. And then the
[00:35:57.440 --> 00:36:03.200]   backgrounds really started to expand of the people involved went from stats and now people who are
[00:36:03.200 --> 00:36:11.680]   computer science and data backgrounds. 2019 continued to expand. We started to see more and
[00:36:11.680 --> 00:36:17.760]   more reinforcement learning coming in, more and more different ways of doing automatic learning,
[00:36:19.280 --> 00:36:26.640]   built-in algorithms, hyperparameter search. And we started to see the use of a lesser expensive
[00:36:26.640 --> 00:36:31.920]   version of network architecture search called macro architecture search to sort of find
[00:36:31.920 --> 00:36:39.360]   new architectures to use for training a model. And again, bringing in managed end-to-end services,
[00:36:39.360 --> 00:36:45.920]   turnkey services, and the backgrounds really started to expand and include operational people.
[00:36:47.040 --> 00:36:54.400]   What we saw or predicted in 2020 as companies move into full production is really building on
[00:36:54.400 --> 00:37:00.400]   the data validation side. And on the other end, we're really starting to see an explosion,
[00:37:00.400 --> 00:37:12.560]   what we call automatic learning. Let me just click this way. So here in 2020, what we see as far as
[00:37:12.560 --> 00:37:18.720]   roles at the enterprise level, they tend to be split across two organizations. One of them we
[00:37:18.720 --> 00:37:24.960]   typically refer to as the innovation center and the other one is production. We see our research
[00:37:24.960 --> 00:37:30.800]   data scientists in the innovation center, our applied data scientists, the more senior ones
[00:37:30.800 --> 00:37:36.240]   tend to straddle the two. Some of them are in innovation, some of them are production.
[00:37:36.240 --> 00:37:42.160]   We also see that with the machine learning engineers. But then on the production side,
[00:37:42.160 --> 00:37:47.200]   we additionally see the machine learning operations, we call them ML ops, we see our
[00:37:47.200 --> 00:37:55.280]   data engineers, and we're also seeing a new type of software engineer that we refer to as an AI
[00:37:55.280 --> 00:38:02.480]   application engineer. For the most part, they're doing their job the same way they always have,
[00:38:02.480 --> 00:38:09.920]   you know, with the same practices and processes. But the packages, the libraries they're using now
[00:38:09.920 --> 00:38:15.520]   are AI libraries. And the other thing they have to get used to is historically,
[00:38:15.520 --> 00:38:24.560]   you know, algorithms always gave a, you could say a single answer, two plus two was always four.
[00:38:24.560 --> 00:38:30.240]   And now they have to learn how do I deal with an algorithm whose output is really a probability
[00:38:30.240 --> 00:38:37.760]   distribution. And that's also put into question how QA fits in. Again, QA has always been built
[00:38:37.760 --> 00:38:44.080]   on this discrete world, you know, that there's every set of inputs has a certain exact output,
[00:38:44.080 --> 00:38:50.240]   and you want to get, say, 100% of your tests correct. Well, what's the correct output? Okay.
[00:38:50.240 --> 00:38:57.680]   And so there's still uncertainty exactly how QA works in a world where our applications now work
[00:38:57.680 --> 00:39:08.960]   on probability distributions. So this is just kind of showing at the end of 2019, what was a
[00:39:08.960 --> 00:39:16.320]   typical production flow. Okay. So again, you would have some kind of data warehousing here,
[00:39:16.320 --> 00:39:24.400]   I represent as a data repo, your data engineers have designed some system to distribute data
[00:39:24.400 --> 00:39:31.040]   on a regular or you could say recurrent basis for model training at a scale. We're not training one
[00:39:31.040 --> 00:39:37.600]   model anymore, or the organization may be training vast number of different models. But even within
[00:39:37.600 --> 00:39:45.600]   that one model, they will train multiple instances to find out which one will produce the best
[00:39:45.600 --> 00:39:52.480]   result. And even within an instance, we have things called warm up training, where we do numerical
[00:39:52.480 --> 00:39:58.080]   stabilization in the model, and we might have sort of short training, multiple short trainings
[00:39:58.080 --> 00:40:05.200]   to get each one of these ready for full training. And so there's just massive number of model
[00:40:05.200 --> 00:40:11.920]   training. So once you got an instance of a model train, there's usually some kind of validation
[00:40:11.920 --> 00:40:19.280]   check. Typically, the organization now has a repository for their train models, where they can
[00:40:19.280 --> 00:40:25.520]   do, you know, tracking the same way as source control, but now it's for models. So they're able
[00:40:25.520 --> 00:40:32.000]   to go to the repository in their internal evaluation, answer the question, is this instance
[00:40:32.000 --> 00:40:38.400]   of the model better than the previous one? And if not, I just repeat this process. But if it is,
[00:40:38.400 --> 00:40:43.760]   I'm going to go ahead and version control it and put it into the repo. Then finally, we're going
[00:40:43.760 --> 00:40:50.320]   to deploy it. But even at that point, we really don't know that the model is actually going to be
[00:40:50.320 --> 00:40:57.200]   better when we put it in the wild. We made that determination initially based on our value
[00:40:57.200 --> 00:41:04.080]   validation data. So we end up deploying it. And they typically do some form of A/B testing. That
[00:41:04.080 --> 00:41:09.360]   is, they have the original version out there, the new version. Some people see their original,
[00:41:09.360 --> 00:41:15.680]   some people see the new version. And they have some metric where they can measure how well it's
[00:41:15.680 --> 00:41:23.040]   performing. And based on that, they're going to get insights, which leads to more data labeling,
[00:41:23.040 --> 00:41:28.720]   more data is added to the data warehouse, and then the cycle is just repeated, and it goes on
[00:41:28.720 --> 00:41:35.680]   continuous. So, you know, a couple of years ago, when I'd be talking to people in production at
[00:41:35.680 --> 00:41:43.120]   these companies, you know, how often they retrain a model, typically I heard 30 days. You know,
[00:41:43.120 --> 00:41:50.640]   by the end of 2019, you know, a normal retraining cycle was, you know, every week,
[00:41:50.640 --> 00:41:55.840]   seven days. And then in other cases, people are retraining literally every single day.
[00:41:55.840 --> 00:42:00.720]   I'll pause for a moment if somebody has a question.
[00:42:03.920 --> 00:42:10.000]   Yeah, I actually have a couple ones. One I'd like to know, actually, from that 2019 production flow,
[00:42:10.000 --> 00:42:17.920]   what are some of the biggest gaps you've seen between validation performance and performance
[00:42:17.920 --> 00:42:23.040]   in that A/B testing setting where you actually have real world data and real world--
[00:42:23.040 --> 00:42:28.320]   Yeah, yeah. You know, the biggest problems, of course, we have are falling into two categories,
[00:42:29.200 --> 00:42:36.400]   serving SKU and data drift, okay? And again, since you're all big talking about distributions,
[00:42:36.400 --> 00:42:39.520]   you can actually re-explain those as distributions, but we won't.
[00:42:39.520 --> 00:42:46.720]   I came back, right, and had a distribution theories. Okay. So, for our audience, if you
[00:42:46.720 --> 00:42:53.120]   don't know what serving SKU is, so your training data, let's say it's classes, a classification,
[00:42:53.120 --> 00:42:59.840]   you have 10 classes, and you have a certain number per class percentage, okay? And you train
[00:42:59.840 --> 00:43:08.320]   it in proportion to that. You keep it balanced, okay? And let's say 10 classes and nine of them
[00:43:08.320 --> 00:43:16.080]   are highly accurate, 98%. And then there's a 10th class, it's not so good. It's more like 70%.
[00:43:16.640 --> 00:43:26.320]   But when you report the average across all 10, it looks really high, 94%, and you're really happy.
[00:43:26.320 --> 00:43:30.400]   And then you put it out there in the real world, and you're only getting 70%, you know? And you're
[00:43:30.400 --> 00:43:36.400]   asking, "What's going on?" Well, it turns out almost everything it sees is from that 10th class,
[00:43:36.400 --> 00:43:42.640]   okay? And that's called a serving SKU. The distribution of what-- it's not that what it
[00:43:42.640 --> 00:43:48.160]   sees is different than what it's trained on, but the frequency, the distribution and the frequency,
[00:43:48.160 --> 00:43:54.400]   okay? So, we call that serving SKU. That's one problem we hit. Another one we hit is data drift.
[00:43:54.400 --> 00:44:00.800]   You just see examples we never trained on. They're from a different-- they may be from the same
[00:44:00.800 --> 00:44:07.440]   population, but they're from a different distribution, okay? So, you have to look at it.
[00:44:07.440 --> 00:44:15.360]   No matter what I'm training on, I really don't have every possible example out there. So,
[00:44:15.360 --> 00:44:23.040]   let's say you're trying to train a model to predict the shoe size of every male in North
[00:44:23.040 --> 00:44:31.600]   America based on some features. So, the population would be if you had every exact adult male in that
[00:44:31.600 --> 00:44:37.840]   number, but you don't. So, you got some subpopulation, okay? And the question is,
[00:44:37.840 --> 00:44:45.120]   how represented is that subpopulation? That's your statistic, okay, of the overall population.
[00:44:45.120 --> 00:44:51.280]   In general, it's not. And so, in data drift, your model is seeing a subpopulation of the same
[00:44:51.280 --> 00:44:57.280]   population, but it's a different subpopulation. And that's one of the big things we try to find
[00:44:57.280 --> 00:45:04.160]   in A/B testing. And then that gives us insight into two things, labeling more data, getting more
[00:45:04.160 --> 00:45:12.960]   data. It's more representative, but also making validation slices that represent not just the
[00:45:12.960 --> 00:45:18.080]   training data, but the serving skew and the data drift. Hopefully, I didn't take too long
[00:45:18.080 --> 00:45:24.000]   answering that question. Those are great, great answers. And yeah, it's interesting to see that
[00:45:24.000 --> 00:45:28.880]   those problems are sort of classical statistics in a lot of ways. Yeah, absolutely classical
[00:45:28.880 --> 00:45:36.880]   statistics. Also, one more question before you move on from Jan Chang on Zoom. How would
[00:45:36.880 --> 00:45:41.840]   architecture search fit into that production flow? Is that part of it or is that something separate?
[00:45:41.840 --> 00:45:52.320]   Yeah, I see that as something separate. Obviously, the architecture search produced that model that's
[00:45:52.320 --> 00:45:59.680]   being trained in that production flow diagram. And my presentation is six parts. I don't know how far
[00:45:59.680 --> 00:46:03.680]   we'll go. I'll have you cut me off when we're done. But each one's independent. But one of
[00:46:03.680 --> 00:46:09.280]   them actually talks about the trends and using those types of techniques in production.
[00:46:09.280 --> 00:46:14.080]   Gotcha. Well, I want to hear as much as possible, so I'm going to let you keep going.
[00:46:14.080 --> 00:46:22.960]   We'll continue on. Okay. So another big thing that really changed in 2019 is, I call this not
[00:46:22.960 --> 00:46:39.520]   just a model anymore. So up until 2017, 2018, typically, we trained individual models. And
[00:46:39.520 --> 00:46:45.600]   even if an application used multiple models, they were still separate models. You tended to have some
[00:46:45.600 --> 00:46:53.840]   back-end application on a server, like written in Java or C#. And your models tended to be deployed
[00:46:53.840 --> 00:47:01.040]   like as a microservice or an HTTP endpoint. And your back-end application would make calls out
[00:47:01.040 --> 00:47:10.320]   to them. Well, we don't really do it that way. Nowadays, we actually build applications that
[00:47:10.320 --> 00:47:17.440]   are a composition or what we call an amalgamation of the models, where the models are interconnected
[00:47:17.440 --> 00:47:23.680]   to themselves. And the ultimate goal is that collection, that amalgamation of the models
[00:47:23.680 --> 00:47:30.960]   becomes the entire application. The whole idea of a back-end application, making these calls out to
[00:47:30.960 --> 00:47:38.320]   these models go away. And what we're finding is these models, as we move towards amalgamations,
[00:47:38.320 --> 00:47:46.320]   are becoming more multitask. They're doing more self or meta learning. They're sharing
[00:47:46.320 --> 00:47:52.080]   common layers, and they're connecting between each other via learned embeddings.
[00:47:54.240 --> 00:48:02.160]   So I'm going to step you how we get there. I'm a computer vision person, so I always put
[00:48:02.160 --> 00:48:08.800]   everything in the context of computer vision. That's my expertise. So today, when we look at
[00:48:08.800 --> 00:48:17.840]   models, there are standard design patterns. And in the convolutional neural network world,
[00:48:17.840 --> 00:48:23.840]   we look at them as being broken into three major components, a stem, a learner, and a task.
[00:48:24.560 --> 00:48:31.040]   So the stem is really where the image data is coming in, and we do this first course level
[00:48:31.040 --> 00:48:38.400]   generation of feature maps. It's sort of prepping the system. And one of the important thing is,
[00:48:38.400 --> 00:48:47.200]   is depending on the size of the image, it wants to, it's going to do some dimensionality reduction,
[00:48:47.200 --> 00:48:53.600]   but it wants to do it in a way that matches what our expectations are when we get to the end of
[00:48:53.600 --> 00:48:59.120]   the learner. Because at the end of the learner is what Dillon referred to as the latent space.
[00:48:59.120 --> 00:49:07.280]   We want it to be a certain dimensionality. And so that's the job of the stem. The learner is what we
[00:49:07.280 --> 00:49:14.320]   do, what we say now does representational learning. And what we mean by that is we want to learn
[00:49:14.320 --> 00:49:20.640]   the essential features of the data, which is really kind of different than the past where we
[00:49:20.640 --> 00:49:27.680]   said we wanted to learn the data or the data set. Well, that approach really led to what we call
[00:49:27.680 --> 00:49:34.960]   memorization, okay, where you really just end up memorizing the data. And what we really want to
[00:49:34.960 --> 00:49:40.800]   do is, quote, not learn the data, but learn what are the essential features that make up the data.
[00:49:40.800 --> 00:49:46.320]   And that's the only thing we want in the latent space. So again, that's not really very different
[00:49:46.320 --> 00:49:52.720]   than Dillon. Sometimes we do pre-training on these models using an encoder portion of an auto
[00:49:52.720 --> 00:50:02.240]   encoder or a GAN or synthetic data just to force the model to sort of initially sort of call,
[00:50:02.240 --> 00:50:08.000]   you know, set itself on learning only essential features. And so that's the learner. And then
[00:50:08.000 --> 00:50:13.920]   finally, the task. Now that I got this latent space, what is the task I want to learn? It could
[00:50:13.920 --> 00:50:20.800]   be something simple as classification. It could be multitask as an object detection. You're learning
[00:50:20.800 --> 00:50:26.320]   two classes, two tasks. You're learning a bounding box, and you're learning to classify
[00:50:26.320 --> 00:50:37.040]   what's inside that bounding box. So yeah, I brought up object detection. Okay. So this is
[00:50:37.040 --> 00:50:44.880]   just to remind people that today models are multi, many models nowadays in production are
[00:50:44.880 --> 00:50:52.320]   multitask. I consider object detection sort of the granddaddy of them all. Okay. So you start your
[00:50:52.320 --> 00:50:58.880]   STEM convolutional group, and guess those original feature maps ready at the right size, right
[00:50:58.880 --> 00:51:04.320]   dimensionality. It's got that coarse level feature extraction. Then you're going to have this
[00:51:04.320 --> 00:51:10.320]   convolutional body or net, and that's your reuse because you're going to reuse it for multiple
[00:51:10.320 --> 00:51:15.920]   purposes. Okay. Sometimes they refer to the output of that as the shared feature maps.
[00:51:15.920 --> 00:51:24.240]   And so there's various things you want to do here. So in an RPN is you want to propose regions within
[00:51:24.240 --> 00:51:30.880]   the image that might be an object. So these are like your candidate bounding boxes. And then you
[00:51:30.880 --> 00:51:36.320]   want to pull them together, ones that are overlapping and so forth to sort of a final,
[00:51:36.320 --> 00:51:41.840]   smaller final set of candidates. And then you're going to classify each one of those. Is this a
[00:51:41.840 --> 00:51:48.400]   bounding box, a foreground or a background? And if it's a foreground, then you want to classify
[00:51:48.400 --> 00:51:55.920]   what's in it. And on top of that, you want to then fine tune the bounding box around it. So
[00:51:55.920 --> 00:52:04.000]   here you have a classifier and a regressor. Okay. So let's kind of move to full scale model
[00:52:04.000 --> 00:52:10.720]   amalgamation. So one of the areas of enterprise customers I've worked with for Google is in sports
[00:52:10.720 --> 00:52:17.760]   broadcasting. Okay. And this is the kind of amalgamation that we help them with. So obviously
[00:52:17.760 --> 00:52:24.800]   it's sports and they have some live video. And so that's going into a shared convolutional neural
[00:52:24.800 --> 00:52:31.440]   network and you got shared layers and that shared layers, the outputs from them go into an object
[00:52:31.440 --> 00:52:40.160]   detection model. Okay. So that gives us a chance to say what's in that frame. So you have everything
[00:52:40.160 --> 00:52:48.720]   from, you know, detecting a person, detecting a bat, detecting a ball, detecting audience in the
[00:52:48.720 --> 00:52:55.120]   stadium and so forth. So there's all your object detection. But these are also embeddings. These
[00:52:55.120 --> 00:53:02.240]   are those little latent spaces. Okay. And what we can do is we can use their location in the
[00:53:02.240 --> 00:53:08.400]   original image to crop them out of the latent space. So it's not the entire latent space,
[00:53:08.400 --> 00:53:14.240]   just a little crop outs out of them. We call them object level embeddings. We can take those object
[00:53:14.240 --> 00:53:20.320]   level embeddings and identify the ones that are classified as a person. Past that embedding in
[00:53:20.320 --> 00:53:26.160]   the model is not trained on the original image. It is trained on the object embedding as the output
[00:53:26.160 --> 00:53:34.880]   of this to recognize the players. Okay. And if it's a player, then to take that information,
[00:53:34.880 --> 00:53:41.760]   those same embeddings to another model trained on those beddings to do pose estimation. So it might
[00:53:41.760 --> 00:53:48.640]   say, you know, realize the player is standing in a batting position. So now look at all the things I
[00:53:48.640 --> 00:53:54.880]   have. I have the information about who, what player it is. I know his pose. I have all these
[00:53:54.880 --> 00:54:00.880]   object embeddings. I put them together. We now have a dense embedding and we can put that
[00:54:00.880 --> 00:54:08.880]   into another model trained on that output to predict the action. For example, you know, again,
[00:54:08.880 --> 00:54:17.440]   context of baseball player at the mound ready to bat. Okay. Because we can predict that action,
[00:54:17.440 --> 00:54:24.400]   we can take that predicted action into an image captioning model, produce the closed caption text.
[00:54:24.400 --> 00:54:31.280]   Today, broadcasters send their, you know, the sports games all over the world. So you can have
[00:54:31.280 --> 00:54:40.240]   automatic translation into the particular market. Okay. And then for those who are, say, visual or
[00:54:40.240 --> 00:54:45.520]   maybe it's radio or something, they're able to then take that and go text to speech.
[00:54:45.520 --> 00:54:51.280]   The important thing here is this whole process of models, there's no back end application.
[00:54:51.920 --> 00:55:01.920]   This entire amalgamation of models is the application. Okay. Just got three more slides
[00:55:01.920 --> 00:55:10.240]   on this section and the last questions. Another area we work on, I call it model fusion because
[00:55:10.240 --> 00:55:17.920]   it has some similarities to the autonomous world with sensor fusion. Okay. So we work with utility
[00:55:17.920 --> 00:55:22.720]   companies. Okay. And of course they got transmission, the big giant transmission lines.
[00:55:22.720 --> 00:55:30.080]   Historically, a long time ago for maintenance purposes, they actually would fly periodically
[00:55:30.080 --> 00:55:36.320]   a helicopter out, you know, into the mountains where all these big transmission towers are.
[00:55:36.320 --> 00:55:44.560]   There'd be a three person crew. You'd have the pilot. You have some guy with binoculars looking
[00:55:44.560 --> 00:55:49.920]   out, staring at these poles, trying to spot defects or problems. And then a third person
[00:55:49.920 --> 00:55:57.440]   taking those. Highly expensive. Okay. Well, eventually they trained drones to, in a sense,
[00:55:57.440 --> 00:56:02.960]   do the flying, but you had two people in back. Otherwise, you know, visually observing. Again,
[00:56:02.960 --> 00:56:07.840]   it's before you had a model, but you got rid of the high cost of the helicopter and the pilot.
[00:56:07.840 --> 00:56:14.160]   Then eventually, of course, with deep learning, they were able to train object detection models.
[00:56:14.560 --> 00:56:21.600]   Okay. That could classify, you know, what it saw there for defects or for maintenance. And this was
[00:56:21.600 --> 00:56:30.000]   a dramatic reduction in cost and is fairly standard throughout the whole industry. But the thing was,
[00:56:30.000 --> 00:56:36.560]   is that there's an older technology on every one of these transmission poles. Okay. When there's
[00:56:36.560 --> 00:56:45.760]   a problem, a break or something's just not right. Okay. It changes the impedance in the line. And so
[00:56:45.760 --> 00:56:54.160]   there's these impedance sensors, this old IOT technology that is continuously transmitting
[00:56:54.160 --> 00:57:03.200]   information back to the utility company. The problem is, is reliability of these sensors,
[00:57:03.200 --> 00:57:10.640]   because that impedance value changes in it. What it means can be caused by, can be affected by
[00:57:10.640 --> 00:57:19.280]   things simply by climate and other things. Okay. So you can't use like one set of numbers and say,
[00:57:19.280 --> 00:57:24.720]   this number always means that. But it turns out that since they're flying these,
[00:57:24.720 --> 00:57:32.560]   and these are now already trained and automatically classifying, the output of these guys
[00:57:32.560 --> 00:57:39.760]   now become the labels for this old school IOT technology. And they're able to train
[00:57:39.760 --> 00:57:47.440]   an anomaly detection model with high accuracy, with these low cost technology using this
[00:57:47.440 --> 00:57:55.600]   drone who's been otherwise machine learned to be the labeler for it, further reducing the cost.
[00:57:58.640 --> 00:58:05.920]   So another area I've worked with several large internet companies whose business has something
[00:58:05.920 --> 00:58:14.720]   to do with homes, anything from selling them, renting them to renovating them. Okay. And we
[00:58:14.720 --> 00:58:23.680]   have one basic pipeline amalgamation that works for all of them. Okay. And the basic premise here
[00:58:24.400 --> 00:58:30.800]   is they'll have a website and you as a user, you're going to upload pictures of your home.
[00:58:30.800 --> 00:58:37.680]   Okay. So the first step is, is to have a model, a simple binary classifier model that determines,
[00:58:37.680 --> 00:58:42.800]   is that picture an interior or an exterior? And for the moment, I'm going to skip what's
[00:58:42.800 --> 00:58:47.120]   inside here. That's in the next slide, but they're practically a mirror of each other.
[00:58:47.920 --> 00:58:54.560]   But the output of whatever this amalgamation is, that output, that embedding goes into three
[00:58:54.560 --> 00:59:00.960]   backend models. Okay. One's going to classify, is a multi-class classifier, and it's going to
[00:59:00.960 --> 00:59:07.920]   classify the market appeal per demographic. Another model take that same information,
[00:59:07.920 --> 00:59:14.880]   the same latent space or embedding, be a regressor and come up with a valuation, such as how much
[00:59:14.880 --> 00:59:21.840]   the house would sell for or how much it should rent for. And then another regressor for renovation,
[00:59:21.840 --> 00:59:26.880]   like in repairs. So let's kind of look at the interior of what's inside of this, that makes
[00:59:26.880 --> 00:59:33.040]   the amalgamation. Okay. So let's go back here. Coming out of the binary classifier, isn't just
[00:59:33.040 --> 00:59:40.880]   a yes or no interior or exterior. Remember I say, these are multi-output models that share
[00:59:40.880 --> 00:59:46.480]   from different layers. So you have an embedding. So the embedding coming out of this is now the
[00:59:46.480 --> 00:59:52.720]   input to that interior. So the interior is not seeing the original image anymore. It is seeing
[00:59:52.720 --> 00:59:59.680]   that embedding. This is typically two tier. Okay. So first we want to do a course level object
[00:59:59.680 --> 01:00:05.120]   detection of what type of room it is, particularly if we're inside the house. Is it a living room?
[01:00:05.120 --> 01:00:12.640]   Is it a dining room? Is it a kitchen? And then for that room, another object detection on a
[01:00:12.640 --> 01:00:18.560]   detail level, what items in the room. So if you're talking about a kitchen, you might be talking
[01:00:18.560 --> 01:00:25.200]   about the stove, the refrigerator, or the table in the bathroom, you might be talking about the
[01:00:25.200 --> 01:00:32.240]   shower or the toilet, et cetera. Well, those are our anemones. Okay. And again, it's object
[01:00:32.240 --> 01:00:39.280]   detection. It knows the location in the embedding where it is. It can crop it out, which spatially
[01:00:39.280 --> 01:00:45.040]   maps back to the original image. So we've got all these tiny crop embeddings that are your anemones.
[01:00:45.040 --> 01:00:51.200]   At the same time, we take that same information from that embedding and train a multiclass
[01:00:51.200 --> 01:00:59.440]   classifier on a per room basis to classify the overall condition. Okay. Now you take that
[01:00:59.440 --> 01:01:05.840]   condition or first you take these embeddings and then on us from them, just like we did here at a
[01:01:05.840 --> 01:01:13.120]   course level, at a fine level, we have three models. One is classifying the condition on
[01:01:13.120 --> 01:01:18.960]   individual anemone, another one, the market appeal, another one, the valuation. Collectively,
[01:01:18.960 --> 01:01:26.320]   all these anemones come out, their exit here along with the overall condition of the room
[01:01:26.320 --> 01:01:34.080]   for one big GANs embedding. And that's what came out here that did that final aggregation.
[01:01:34.080 --> 01:01:39.760]   So that's an amalgamation. So I'll pause for a moment for anybody to ask a question.
[01:01:39.760 --> 01:01:47.520]   Yeah. So it's about six o'clock, which is usually when we wrap up. So I think we should, I would
[01:01:47.520 --> 01:01:50.880]   love to actually ask you a couple of questions before we go, but I don't think we'll be able
[01:01:50.880 --> 01:01:54.960]   to hear any more of your presentation. Okay. That's fine. Yeah. Go for some questions.
[01:01:55.600 --> 01:02:00.400]   Yeah. So Gary Kuvich on Zoom asked a really great question. Are there any papers that you can share
[01:02:00.400 --> 01:02:06.480]   with us on this concept of amalgamations and maybe architectures and training styles that are best
[01:02:06.480 --> 01:02:13.600]   for this style of machine learning? Amalgamations isn't really something that
[01:02:13.600 --> 01:02:20.560]   people in research really, the direction they would go, they're thinking theoretical.
[01:02:21.680 --> 01:02:29.600]   So amalgamations are more of, it's just a practice that has evolved with people in production who
[01:02:29.600 --> 01:02:36.480]   have to work with vast number of moving parts. Okay. And trying to move away, you know, trying
[01:02:36.480 --> 01:02:42.000]   to make it more efficient. And the way to make it more efficient is you got to move away from that
[01:02:42.000 --> 01:02:45.920]   back end application on a server. You just somehow have to get rid of that.
[01:02:48.400 --> 01:02:53.920]   Yeah, that makes sense. And I was going to say that in my time in academia, this wasn't really
[01:02:53.920 --> 01:02:57.760]   on people's radar. I mean, the idea of like transfer learning is definitely something
[01:02:57.760 --> 01:03:02.480]   people talk about, but it feels a little different than this like amalgamation idea
[01:03:02.480 --> 01:03:06.880]   that you're talking about. Yeah. In my deep learning book,
[01:03:06.880 --> 01:03:13.280]   I definitely have a significant sections on amalgamation. It's one of my, everybody at Google
[01:03:13.280 --> 01:03:19.200]   cloud is an expert at, you know, one or more things, you know, they're all great people. And
[01:03:19.200 --> 01:03:25.200]   one of the things I'm in, I've made myself an expert at is how at the enterprise level companies
[01:03:25.200 --> 01:03:30.960]   are doing amalgamations. Great. I'll make sure to share your,
[01:03:30.960 --> 01:03:38.160]   a link to your book, the early access version of your book on Manning with the attendees on Zoom
[01:03:38.160 --> 01:03:44.000]   and YouTube. So they can check that out. Yeah. And yeah. And I'm not sure if you have my YouTube
[01:03:44.000 --> 01:03:51.120]   channel for like the first dozen chapters of the book, I have a YouTube presentation.
[01:03:51.120 --> 01:03:54.640]   It's like, you know, the cliff notes for the chapter.
[01:03:54.640 --> 01:04:00.640]   Yeah. I'll make sure that gets, that gets shared. And also if you have a link to the slides,
[01:04:00.640 --> 01:04:03.200]   if you could send that to me, I can send that out to our attendees.
[01:04:03.200 --> 01:04:09.840]   I can definitely do that, Charles. Great. Okay. I haven't seen any other
[01:04:09.840 --> 01:04:16.960]   questions besides wanting to get access come in. I had one question that I want to make sure that we,
[01:04:16.960 --> 01:04:23.200]   that I don't get it, that I don't miss my chance to ask, which is in some of your earliest slides,
[01:04:23.200 --> 01:04:30.320]   you mentioned the, in 2017, there were 10,000 data scientists and now there's 250,000 of them.
[01:04:30.320 --> 01:04:34.800]   So what do you think, you know, what do you think is the 25X growth that's going to happen between
[01:04:34.800 --> 01:04:39.840]   2020 and 2023? Do you think that there's an opportunity for that kind of explosive growth
[01:04:39.840 --> 01:04:43.360]   elsewhere in machine learning and data science? Or do you think it's somewhere else?
[01:04:43.360 --> 01:04:50.080]   I think we're going to, as far as the high end definition of a data scientist,
[01:04:50.080 --> 01:04:57.280]   I think that's pretty much going to plateau out. Okay. I think the advancement of these
[01:04:57.280 --> 01:05:05.520]   frameworks and tools, a lot can be accomplished, you know, with more of a machine learning engineer.
[01:05:05.520 --> 01:05:17.760]   Okay. Okay. But I do see as we gradually teach programmers, what I call how to program the graph,
[01:05:17.760 --> 01:05:22.320]   it's a different way of thinking. Once they learn to program a graph, they're a machine learning
[01:05:22.320 --> 01:05:31.280]   engineer. Okay. My belief is just a matter of a few years where effectively all application
[01:05:31.280 --> 01:05:35.920]   programmers will be programming the graph. Somewhere on the graph, they'll be programming.
[01:05:35.920 --> 01:05:44.800]   We see numbers and marketing data that currently there are about 25 million programmers in the
[01:05:44.800 --> 01:05:48.960]   world. So given that it should be about a tenfold growth.
[01:05:50.960 --> 01:05:52.560]   Interesting. Okay. That's a...
[01:05:52.560 --> 01:05:58.560]   But again, that's just my opinionated view of the world that everybody's going to be programming
[01:05:58.560 --> 01:06:08.480]   somewhere on the graph. And amalgamation is really a graph of graphs with subgraph communicating
[01:06:08.480 --> 01:06:11.760]   with each other. Right. Right. So you're not back
[01:06:11.760 --> 01:06:15.920]   propagating through that entire amalgamation, you're just doing it through individual ones,
[01:06:15.920 --> 01:06:19.200]   but it still is a graph of interconnected nodes.
[01:06:19.200 --> 01:06:26.400]   Graphs. That's an interesting perspective. And I certainly would love it if every programmer learned
[01:06:26.400 --> 01:06:31.360]   to do a little bit of machine learning. It would mean definitely job security for people who teach
[01:06:31.360 --> 01:06:36.400]   machine learning. Yeah. That's kind of one of the things I do.
[01:06:36.400 --> 01:06:42.320]   Great. Yeah. Well, it was a pleasure to have you on the salon. Thanks a lot for coming by.
[01:06:42.320 --> 01:06:48.320]   Thanks to everybody also for tuning in. I'm going to just do a quick promo before we go for
[01:06:48.320 --> 01:06:56.160]   everything else. So our panelists can head out if they'd like. But yeah, let me pop up.
[01:06:56.160 --> 01:07:05.840]   So one is in two weeks, we're going to have Robert Nishihara of Ray and AnyScale, who will be talking
[01:07:05.840 --> 01:07:12.160]   about using AnyScale and Ray and Raytune as tools to perform machine learning at any scale,
[01:07:12.160 --> 01:07:18.080]   whether that's an individual project all the way up to 10,000 GPUs. So that'll be in two weeks
[01:07:18.080 --> 01:07:25.440]   on September 15th. Also, you can catch the lawns that you missed on our YouTube channel. Just
[01:07:25.440 --> 01:07:30.720]   search for Weights and Biases on YouTube. We've also got other videos up there, including our
[01:07:30.720 --> 01:07:36.080]   podcast Gradient to Descent, which has a really great video with Jeremy Howard talking about the
[01:07:36.080 --> 01:07:42.640]   future of machine learning and Julia. And also, I think, some stuff about changing diapers. So real
[01:07:44.320 --> 01:07:52.160]   Renaissance, man, and a real interesting podcast. And then also, besides those kinds of events,
[01:07:52.160 --> 01:07:56.800]   we also have a Slack forum, the Weights and Biases forum. You can sign up for it at
[01:07:56.800 --> 01:08:05.760]   bit.ly/wb-slack. And we just had the CEO of Kaggle on to answer a bunch of questions last Friday. We
[01:08:05.760 --> 01:08:10.560]   tend to do those almost every Friday. We have a new AMA. We had somebody from the PyTorch
[01:08:10.560 --> 01:08:15.840]   development team on. And it's really a great chance to get in-depth answers to questions
[01:08:15.840 --> 01:08:20.720]   that you have. So there's other things in our Slack, lots of great conversations happening
[01:08:20.720 --> 01:08:29.920]   all the time. So you should join us. Come through. And whether you join us or not,
[01:08:29.920 --> 01:08:34.960]   I will hopefully see you in two weeks at our next salon. So thanks a lot to everybody,
[01:08:34.960 --> 01:08:44.880]   especially to Dylan and to Andrew. And everyone, take care.

