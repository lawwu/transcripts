
[00:00:00.000 --> 00:00:02.580]   (upbeat music)
[00:00:02.580 --> 00:00:07.280]   - I'm very happy to be here.
[00:00:07.280 --> 00:00:08.900]   Thank you for the invitation.
[00:00:08.900 --> 00:00:11.620]   So I'm gonna be talking about synthetic data in 2024.
[00:00:11.620 --> 00:00:14.620]   And then I'm gonna be talking about small on-device models.
[00:00:14.620 --> 00:00:17.760]   So I think the most interesting thing
[00:00:17.760 --> 00:00:19.960]   about synthetic data this year is that like,
[00:00:19.960 --> 00:00:21.400]   now we have it everywhere
[00:00:21.400 --> 00:00:24.000]   in the large language models pipeline.
[00:00:24.000 --> 00:00:26.860]   I think initially synthetic data was mainly used
[00:00:26.860 --> 00:00:28.300]   just for post-training,
[00:00:28.300 --> 00:00:29.980]   because naturally that's the part
[00:00:29.980 --> 00:00:33.220]   where we needed human annotators to show the models
[00:00:33.220 --> 00:00:35.060]   how they should answer instructions,
[00:00:35.060 --> 00:00:38.360]   how they should be helpful and not toxic.
[00:00:38.360 --> 00:00:41.060]   And when we had LLMs that were really performant,
[00:00:41.060 --> 00:00:43.900]   we replaced the not human annotators
[00:00:43.900 --> 00:00:45.700]   just with the synthesic data.
[00:00:45.700 --> 00:00:46.820]   And then after that,
[00:00:46.820 --> 00:00:49.500]   we realized that we don't really have good benchmarks
[00:00:49.500 --> 00:00:52.780]   to measure if models follow instructions well,
[00:00:52.780 --> 00:00:55.700]   if they are creative enough, or if they are chatty enough.
[00:00:55.700 --> 00:00:59.380]   So we also started using LLMs as judges.
[00:00:59.380 --> 00:01:02.740]   And I think this year and towards the end of last year,
[00:01:02.740 --> 00:01:05.380]   we also went to the pre-training parts
[00:01:05.380 --> 00:01:08.740]   and we started generating synthetic data for pre-training
[00:01:08.740 --> 00:01:11.100]   to kind of replace some parts of the web.
[00:01:11.100 --> 00:01:12.660]   And the motivation behind that
[00:01:12.660 --> 00:01:15.360]   is that you have a lot of control over synthetic data.
[00:01:15.360 --> 00:01:16.740]   You can control your prompt
[00:01:16.740 --> 00:01:19.500]   and basically also the kind of data that you generate.
[00:01:19.500 --> 00:01:21.780]   So instead of just trying to filter the web,
[00:01:21.780 --> 00:01:23.860]   you could try to get the LLM to generate
[00:01:23.860 --> 00:01:26.300]   what you think the best web pages could look like
[00:01:26.300 --> 00:01:28.100]   and then train your models on that.
[00:01:28.100 --> 00:01:30.580]   So this is how we went from not having synthetic data
[00:01:30.580 --> 00:01:33.220]   at all in the LLM pipeline to having this everywhere.
[00:01:33.220 --> 00:01:36.540]   And so the cool thing is like today,
[00:01:36.540 --> 00:01:40.580]   you can train an LLM with like an entirely synthetic pipeline.
[00:01:40.580 --> 00:01:42.820]   For example, you can use our Cosmopedia datasets
[00:01:42.820 --> 00:01:45.960]   and you can train a 1B model on like 150 billion tokens
[00:01:45.960 --> 00:01:48.140]   that are 100% synthetic.
[00:01:48.140 --> 00:01:49.860]   And those are also of good quality.
[00:01:49.860 --> 00:01:51.500]   And then you can instruction tune the model
[00:01:51.500 --> 00:01:53.460]   on a synthetic SFT dataset.
[00:01:53.460 --> 00:01:55.900]   You can also do DPO on a synthetic dataset.
[00:01:55.900 --> 00:01:57.660]   And then to evaluate if the model is good,
[00:01:57.660 --> 00:02:00.760]   you can use a benchmark that uses LLMs as a judge,
[00:02:00.760 --> 00:02:03.420]   for example, MTBench or AlpacaEval.
[00:02:03.420 --> 00:02:05.200]   So I think this is like a really mind blowing
[00:02:05.200 --> 00:02:06.520]   because like just a few years ago,
[00:02:06.520 --> 00:02:08.860]   we wouldn't think this is possible.
[00:02:08.860 --> 00:02:11.380]   And I think there's a lot of concerns about model collapse
[00:02:11.380 --> 00:02:13.020]   and I'm gonna talk about that later,
[00:02:13.020 --> 00:02:15.900]   but we'll see that like if we use synthetic data properly
[00:02:15.900 --> 00:02:18.840]   and we curate it carefully, that shouldn't happen.
[00:02:18.840 --> 00:02:23.300]   And the reason synthetic data is very popular right now
[00:02:23.300 --> 00:02:25.660]   is that we have really strong models,
[00:02:25.660 --> 00:02:27.860]   both open and closed.
[00:02:27.860 --> 00:02:29.500]   It is really cheap and fast to use
[00:02:29.500 --> 00:02:31.420]   compared to human annotations,
[00:02:31.420 --> 00:02:33.800]   which cost a lot and take a lot of time.
[00:02:33.800 --> 00:02:36.100]   And also for open models right now,
[00:02:36.100 --> 00:02:38.260]   we have some really good inference frameworks.
[00:02:38.260 --> 00:02:39.660]   So if you have enough GPUs,
[00:02:39.660 --> 00:02:41.740]   it's really easy to spawn these GPUs
[00:02:41.740 --> 00:02:44.300]   and generate like a lot of synthetic data.
[00:02:44.300 --> 00:02:47.320]   Some examples are VLM, TGI and TensorRT.
[00:02:47.320 --> 00:02:52.500]   Now let's talk about the elephant in the room,
[00:02:52.500 --> 00:02:53.660]   model collapse.
[00:02:53.660 --> 00:02:54.500]   Is this the end?
[00:02:54.500 --> 00:02:56.540]   If you look at the media and all of like,
[00:02:56.540 --> 00:02:58.660]   for example, some papers in nature,
[00:02:58.660 --> 00:03:01.740]   it's really scary because there's a lot of synthetic data
[00:03:01.740 --> 00:03:02.860]   out there in the web
[00:03:02.860 --> 00:03:04.240]   and naturally we train on the web.
[00:03:04.240 --> 00:03:06.860]   So we're gonna be training a lot of synthetic data.
[00:03:06.860 --> 00:03:08.700]   And if model collapse is gonna happen,
[00:03:08.700 --> 00:03:11.100]   we should really try to take that seriously.
[00:03:11.100 --> 00:03:14.860]   And the other issue is that, as I said,
[00:03:14.860 --> 00:03:17.020]   we think a lot of people think the web is polluted
[00:03:17.020 --> 00:03:19.140]   because there's a lot of synthetic data.
[00:03:19.140 --> 00:03:21.620]   And for example, when we're building fine web datasets,
[00:03:21.620 --> 00:03:23.180]   here, Guillermo and Hinek,
[00:03:23.180 --> 00:03:25.060]   we're interested in like how much synthetic data
[00:03:25.060 --> 00:03:26.580]   is there in the web?
[00:03:26.580 --> 00:03:29.860]   So there isn't really a method to properly measure
[00:03:29.860 --> 00:03:31.180]   the amount of synthetic data
[00:03:31.180 --> 00:03:33.820]   or to save a webpage synthetic or not.
[00:03:33.820 --> 00:03:36.260]   But one thing we can do is to try to look for like
[00:03:36.260 --> 00:03:37.900]   proxy words, for example,
[00:03:37.900 --> 00:03:40.500]   expressions like as a large language model
[00:03:40.500 --> 00:03:41.900]   or words like Delve,
[00:03:41.900 --> 00:03:44.500]   that we know are actually generated by ChatGPT.
[00:03:44.500 --> 00:03:47.340]   We could try to measure the amount of these words
[00:03:47.340 --> 00:03:50.040]   in our dataset and compare them to the previous years.
[00:03:50.040 --> 00:03:52.940]   For example, here, we measured like these words ratio
[00:03:52.940 --> 00:03:54.820]   in different dumps of Common Crawl.
[00:03:54.820 --> 00:03:57.380]   And we can see that like the ratio really increased
[00:03:57.380 --> 00:03:58.980]   after ChatGPT's release.
[00:03:58.980 --> 00:04:03.420]   So if we were to say that synthetic data amount didn't change
[00:04:03.420 --> 00:04:05.500]   you would expect this ratio to stay constant,
[00:04:05.500 --> 00:04:06.980]   which is not the case.
[00:04:06.980 --> 00:04:09.560]   So there's a lot of synthetic data probably on the web,
[00:04:09.560 --> 00:04:12.040]   but does this really make models worse?
[00:04:12.040 --> 00:04:14.180]   So what we did is we trained different models
[00:04:14.180 --> 00:04:15.580]   on these different dumps,
[00:04:15.580 --> 00:04:18.220]   and we then computed their performance
[00:04:18.220 --> 00:04:20.040]   on popular like NLP benchmarks,
[00:04:20.040 --> 00:04:22.320]   and then we computed the aggregated score.
[00:04:22.320 --> 00:04:24.480]   And surprisingly, you can see that the latest dumps
[00:04:24.480 --> 00:04:27.200]   are actually even better than the dumps that are before.
[00:04:27.200 --> 00:04:29.040]   So if there's some synthetic data there,
[00:04:29.040 --> 00:04:31.860]   at least it did not make the models worse.
[00:04:31.860 --> 00:04:34.440]   Yeah, which is really encouraging.
[00:04:34.440 --> 00:04:36.720]   So personally, I wouldn't say the web is positive
[00:04:36.720 --> 00:04:37.960]   with synthetic data.
[00:04:37.960 --> 00:04:40.180]   Maybe it's even making it more rich.
[00:04:40.180 --> 00:04:45.240]   And the issue with like model collapse is that,
[00:04:45.240 --> 00:04:46.440]   for example, those studies,
[00:04:46.440 --> 00:04:48.320]   they were done at like a small scale,
[00:04:48.320 --> 00:04:50.760]   and you would ask the model to complete, for example,
[00:04:50.760 --> 00:04:51.920]   a Wikipedia paragraph,
[00:04:51.920 --> 00:04:54.240]   and then you would train it on these new generations,
[00:04:54.240 --> 00:04:56.080]   and you would do that iteratively.
[00:04:56.080 --> 00:04:57.280]   I think if you do that approach,
[00:04:57.280 --> 00:05:00.000]   it's normal to observe this kind of behavior,
[00:05:00.000 --> 00:05:01.160]   because the quality is gonna be worse
[00:05:01.160 --> 00:05:02.400]   because the model is already small.
[00:05:02.400 --> 00:05:04.480]   And then if you train it just on these generations,
[00:05:04.480 --> 00:05:06.400]   you shouldn't expect it to become better.
[00:05:06.400 --> 00:05:07.640]   But what we're really doing here
[00:05:07.640 --> 00:05:09.560]   is that we take a model that is very large,
[00:05:09.560 --> 00:05:11.200]   and we try to distill its knowledge
[00:05:11.200 --> 00:05:12.880]   into a model that is smaller.
[00:05:12.880 --> 00:05:13.920]   And in this way,
[00:05:13.920 --> 00:05:16.160]   you can expect to get like better performance
[00:05:16.160 --> 00:05:18.040]   for your small model.
[00:05:18.040 --> 00:05:20.800]   And using synthetic data for pre-training
[00:05:20.800 --> 00:05:22.400]   has become really popular
[00:05:22.400 --> 00:05:25.600]   after the textbooks are all you need papers,
[00:05:25.600 --> 00:05:29.160]   where Microsoft basically trained a series of small models
[00:05:29.160 --> 00:05:33.600]   on textbooks that were using a large LLM.
[00:05:33.600 --> 00:05:35.040]   And then they found that these models
[00:05:35.040 --> 00:05:38.320]   were actually better than models that are much larger.
[00:05:38.320 --> 00:05:39.600]   So this was really interesting.
[00:05:39.600 --> 00:05:41.640]   It was like a first of its time,
[00:05:41.640 --> 00:05:44.200]   but it was also met with a lot of skepticism,
[00:05:44.200 --> 00:05:45.560]   which is a good thing in research,
[00:05:45.560 --> 00:05:48.240]   it pushes you to question things.
[00:05:48.240 --> 00:05:50.920]   Because the dataset that they trained on was not public.
[00:05:50.920 --> 00:05:52.720]   So people were not really sure
[00:05:52.720 --> 00:05:54.560]   if these models are really good,
[00:05:54.560 --> 00:05:57.120]   or maybe there's just some data contamination.
[00:05:57.120 --> 00:05:58.640]   So it was really hard to check
[00:05:58.640 --> 00:06:01.560]   if you just have the weights of the models.
[00:06:01.560 --> 00:06:03.760]   And as Hugging Face, because we're like open source,
[00:06:03.760 --> 00:06:05.760]   we tried to reproduce what they did.
[00:06:05.760 --> 00:06:07.880]   So this is our Cosmopedia dataset.
[00:06:07.880 --> 00:06:09.840]   We basically tried to follow a similar approach
[00:06:09.840 --> 00:06:11.400]   to what they documented in the paper.
[00:06:11.400 --> 00:06:14.120]   And we created a synthetic dataset of textbooks
[00:06:14.120 --> 00:06:15.760]   and blog posts and stories
[00:06:15.760 --> 00:06:18.200]   that had almost 30 billion tokens.
[00:06:18.200 --> 00:06:20.760]   And we trained some models on that.
[00:06:20.760 --> 00:06:23.720]   And we found that the key ingredient
[00:06:23.720 --> 00:06:26.000]   to getting a good dataset that is synthetic
[00:06:26.000 --> 00:06:28.880]   is trying as much as possible to keep it diverse.
[00:06:28.880 --> 00:06:31.160]   Because if you just throw the same prompts as your model,
[00:06:31.160 --> 00:06:34.080]   like generate a textbook about linear algebra,
[00:06:34.080 --> 00:06:35.720]   and even if you change the temperature,
[00:06:35.720 --> 00:06:37.080]   the textbooks are gonna look alike.
[00:06:37.080 --> 00:06:40.680]   So there's no way you could scale to millions of samples.
[00:06:40.680 --> 00:06:43.800]   And the way you do that is by creating prompts
[00:06:43.800 --> 00:06:46.680]   that have some seeds that make them diverse.
[00:06:46.680 --> 00:06:48.560]   In our case, the prompt,
[00:06:48.560 --> 00:06:50.880]   we would ask the model to generate a textbook,
[00:06:50.880 --> 00:06:54.160]   but make it related to an extract from a webpage.
[00:06:54.160 --> 00:06:57.440]   And also we try to frame it to stay within topic.
[00:06:57.440 --> 00:07:00.080]   For example, here, we put like an extract
[00:07:00.080 --> 00:07:02.200]   about cardiovascular bioimaging,
[00:07:02.200 --> 00:07:04.640]   and then we ask the model to generate a textbook
[00:07:04.640 --> 00:07:08.240]   related to medicine that is also related to this webpage.
[00:07:08.240 --> 00:07:09.600]   And this is a really nice approach
[00:07:09.600 --> 00:07:11.960]   because there's so many webpages out there.
[00:07:11.960 --> 00:07:14.760]   So you can be sure that your generation
[00:07:14.760 --> 00:07:19.760]   is not gonna be diverse when you change the seed example.
[00:07:19.760 --> 00:07:21.200]   One thing that's challenging with this
[00:07:21.200 --> 00:07:23.000]   is that you want the seed samples
[00:07:23.000 --> 00:07:25.320]   to be related to your topics.
[00:07:25.320 --> 00:07:27.720]   So we use like a search tool
[00:07:27.720 --> 00:07:30.000]   to try to go all of fine web datasets
[00:07:30.000 --> 00:07:32.320]   and find the pages that are related to the topics
[00:07:32.320 --> 00:07:33.360]   we're interested in.
[00:07:33.360 --> 00:07:36.120]   And then we also do a lot of experiments
[00:07:36.120 --> 00:07:39.320]   with the type of generations we want the model to generate.
[00:07:39.320 --> 00:07:41.160]   For example, we ask it for textbooks
[00:07:41.160 --> 00:07:43.840]   for middle school students or a textbook for a college.
[00:07:43.840 --> 00:07:45.880]   And we found that like some generation styles
[00:07:45.880 --> 00:07:47.600]   help on some specific benchmarks
[00:07:47.600 --> 00:07:49.760]   while others help on other benchmarks.
[00:07:49.760 --> 00:07:52.640]   For example, college textbooks are really good for MMLU,
[00:07:52.640 --> 00:07:54.720]   while middle school textbooks are good for benchmarks
[00:07:54.720 --> 00:07:56.840]   like OpenBook UA and Pico.
[00:07:56.840 --> 00:08:01.600]   This is like a sample from like our search tool.
[00:08:01.600 --> 00:08:04.080]   For example, you have a top category, which is a topic,
[00:08:04.080 --> 00:08:05.520]   and then you have some subtopics,
[00:08:05.520 --> 00:08:06.920]   and then you have the topic hits,
[00:08:06.920 --> 00:08:09.200]   which are basically the webpages in fine web
[00:08:09.200 --> 00:08:10.760]   that's belong to these topics.
[00:08:10.760 --> 00:08:14.640]   And here you can see the comparison between Cosmopedia.
[00:08:14.640 --> 00:08:18.640]   We had two versions, V1 and V2 in blue and red,
[00:08:18.640 --> 00:08:20.640]   and you can see the comparison to fine web.
[00:08:20.640 --> 00:08:22.880]   And as you can see throughout the training,
[00:08:22.880 --> 00:08:25.840]   training on Cosmopedia was consistently better.
[00:08:25.840 --> 00:08:27.160]   So we managed to get a dataset
[00:08:27.160 --> 00:08:29.800]   that was actually good to train these models on.
[00:08:29.800 --> 00:08:31.840]   It's of course so much smaller than fine web,
[00:08:31.840 --> 00:08:33.600]   it's only 30 billion tokens,
[00:08:33.600 --> 00:08:36.200]   but that's the scale that's Microsoft datasets was.
[00:08:36.200 --> 00:08:39.240]   So we kind of managed to reproduce a bit what they did,
[00:08:39.240 --> 00:08:41.880]   and the dataset is public, so everyone can go there,
[00:08:41.880 --> 00:08:43.880]   check if everything is all right.
[00:08:43.880 --> 00:08:49.840]   And this is the recent paper from NVIDIA, Nemotron CC.
[00:08:49.840 --> 00:08:51.920]   They took things a bit further
[00:08:51.920 --> 00:08:54.120]   and they generated not a few billion tokens,
[00:08:54.120 --> 00:08:57.960]   but 1.9 trillion tokens, which is huge.
[00:08:57.960 --> 00:09:00.040]   And we can see later how they did that.
[00:09:00.040 --> 00:09:01.920]   It's more of like rephrasing the web.
[00:09:01.920 --> 00:09:04.640]   So we can see today that there's like
[00:09:04.640 --> 00:09:07.680]   some really huge synthetic datasets out there,
[00:09:07.680 --> 00:09:08.520]   and they're public,
[00:09:08.520 --> 00:09:11.080]   so like you can try to filter them even further
[00:09:11.080 --> 00:09:13.480]   if you wanna get like more high quality corpses.
[00:09:13.480 --> 00:09:18.040]   So for this rephrasing the web,
[00:09:18.040 --> 00:09:22.080]   this approach was suggested in this paper by Pratyush,
[00:09:22.080 --> 00:09:23.600]   where basically in this paper,
[00:09:23.600 --> 00:09:27.160]   they take some samples from C4 datasets,
[00:09:27.160 --> 00:09:28.720]   and then they use an LLM
[00:09:28.720 --> 00:09:31.880]   to rewrite these samples into a better format.
[00:09:31.880 --> 00:09:34.840]   For example, they ask an LLM to rewrite the sample
[00:09:34.840 --> 00:09:38.760]   into a Wikipedia passage or into a Q&A page.
[00:09:38.760 --> 00:09:41.160]   And the interesting thing in this approach
[00:09:41.160 --> 00:09:43.800]   is that you can use a model that is small
[00:09:43.800 --> 00:09:46.520]   because rewriting doesn't require knowledge,
[00:09:46.520 --> 00:09:49.240]   it's just rewriting a page into a different style.
[00:09:49.240 --> 00:09:51.960]   So the model doesn't need to have like knowledge
[00:09:51.960 --> 00:09:54.320]   that is like extensive of what is rewriting,
[00:09:54.320 --> 00:09:57.120]   compared to just asking a model to generate a new textbook
[00:09:57.120 --> 00:09:59.480]   and not giving it like ground truth.
[00:09:59.480 --> 00:10:02.240]   So here they rewrite some samples from C4
[00:10:02.240 --> 00:10:04.080]   into Q&A, into Wikipedia,
[00:10:04.080 --> 00:10:06.440]   and they find that doing this works better
[00:10:06.440 --> 00:10:07.960]   than training just on C4.
[00:10:07.960 --> 00:10:13.880]   And so what they did in Nemotron CC is a similar approach.
[00:10:13.880 --> 00:10:18.320]   They rewrite some pages from Common Crawl for two reasons.
[00:10:18.320 --> 00:10:22.400]   One is to like improve pages that are low quality.
[00:10:22.400 --> 00:10:25.120]   So they rewrite them into, for example, Wikipedia page,
[00:10:25.120 --> 00:10:26.400]   so they look better.
[00:10:26.440 --> 00:10:29.680]   And another reason is to create more diverse datasets.
[00:10:29.680 --> 00:10:33.000]   So they have a dataset that they already heavily filtered,
[00:10:33.000 --> 00:10:35.720]   and then they take these pages that are already high quality
[00:10:35.720 --> 00:10:37.840]   and they ask the model to rewrite them
[00:10:37.840 --> 00:10:41.360]   in Q&A format into like open-ended questions
[00:10:41.360 --> 00:10:42.960]   or like multi-choice questions.
[00:10:42.960 --> 00:10:45.920]   So this way they can reuse the same page multiple times
[00:10:45.920 --> 00:10:48.520]   without fearing like having multiple duplicates
[00:10:48.520 --> 00:10:50.160]   because it's the same information,
[00:10:50.160 --> 00:10:52.440]   but it's gonna be written differently.
[00:10:52.440 --> 00:10:54.480]   So I think that's also a really interesting approach
[00:10:54.480 --> 00:10:57.200]   for like generating synthetic data
[00:10:57.200 --> 00:10:59.840]   just by rephrasing the pages that you already have.
[00:10:59.840 --> 00:11:04.000]   There's also this approach called Prox
[00:11:04.000 --> 00:11:06.880]   where they try to start from a webpage
[00:11:06.880 --> 00:11:08.440]   and then they generate a program
[00:11:08.440 --> 00:11:10.160]   which finds how to write that page
[00:11:10.160 --> 00:11:12.160]   to make it better and less noisy.
[00:11:12.160 --> 00:11:13.280]   For example, here you can see
[00:11:13.280 --> 00:11:16.200]   that there's some leftover metadata in the webpage
[00:11:16.200 --> 00:11:17.800]   and you don't necessarily want to keep that
[00:11:17.800 --> 00:11:19.240]   for training your model.
[00:11:19.240 --> 00:11:22.600]   So they train a model that can generate programs
[00:11:22.600 --> 00:11:25.880]   that can like normalize and remove lines that are extra.
[00:11:25.880 --> 00:11:27.680]   So I think this approach is also interesting,
[00:11:27.680 --> 00:11:29.240]   but it's maybe less scalable
[00:11:29.240 --> 00:11:31.480]   than the approaches that I presented before.
[00:11:31.480 --> 00:11:36.080]   So that was it for like rephrasing
[00:11:36.080 --> 00:11:37.920]   and generating new textbooks.
[00:11:37.920 --> 00:11:40.360]   Another approach that I think is really good
[00:11:40.360 --> 00:11:41.600]   and becoming really popular
[00:11:41.600 --> 00:11:44.200]   for using synthetic data for pre-training
[00:11:44.200 --> 00:11:47.200]   is basically building better classifiers
[00:11:47.200 --> 00:11:48.960]   for filtering the web.
[00:11:48.960 --> 00:11:50.920]   For example, here we released a dataset
[00:11:50.920 --> 00:11:53.720]   called FindWebEdu and the way we built it
[00:11:53.720 --> 00:11:57.080]   is by taking Llama3 and asking it to rate
[00:11:57.080 --> 00:12:00.920]   the educational content of webpages from zero to five.
[00:12:00.920 --> 00:12:03.720]   So for example, if a page is like a really good textbook
[00:12:03.720 --> 00:12:05.880]   that could be useful in a school setting,
[00:12:05.880 --> 00:12:07.320]   it would get a really high score.
[00:12:07.320 --> 00:12:10.040]   And if a page is just like an advertisement
[00:12:10.040 --> 00:12:13.320]   or promotional material, it would get a lower score.
[00:12:13.320 --> 00:12:16.240]   And then after that, we take these synthetic annotations
[00:12:16.240 --> 00:12:18.240]   and we train a classifier on them.
[00:12:18.240 --> 00:12:20.880]   It's a classifier like a BERT model.
[00:12:20.880 --> 00:12:23.640]   And then we run this classifier on all of FindWeb,
[00:12:23.640 --> 00:12:25.920]   which is a 15 trillion tokens datasets.
[00:12:25.920 --> 00:12:28.360]   And then we only keep the pages that have like a score
[00:12:28.360 --> 00:12:29.600]   that's higher than three.
[00:12:29.600 --> 00:12:31.000]   So for example, in our case,
[00:12:31.000 --> 00:12:34.880]   we went from 15 trillion tokens to just 1.5 trillion tokens.
[00:12:34.880 --> 00:12:37.160]   Those are really highly educational.
[00:12:37.160 --> 00:12:40.280]   And as you can see here, FindWebEdu outperforms
[00:12:40.280 --> 00:12:44.120]   all the other public web datasets by a larger margin
[00:12:44.120 --> 00:12:45.520]   on a couple of benchmarks.
[00:12:45.520 --> 00:12:47.840]   Here I show the aggregated score.
[00:12:47.840 --> 00:12:50.040]   And you can see that this approach is really effective
[00:12:50.040 --> 00:12:53.240]   for filtering web datasets to get like better corpuses
[00:12:53.240 --> 00:12:54.880]   for training your LLMs.
[00:12:54.880 --> 00:13:00.400]   Others also try to do this approach.
[00:13:00.400 --> 00:13:03.240]   There's, for example, the DCLM datasets,
[00:13:03.240 --> 00:13:05.000]   where they also train the classifier,
[00:13:05.000 --> 00:13:07.320]   but not to detect educational content.
[00:13:07.320 --> 00:13:10.160]   Instead, they trained it on OpenHermes dataset,
[00:13:10.160 --> 00:13:12.440]   which is a dataset for instruction tuning.
[00:13:12.440 --> 00:13:15.920]   And also they explain like IM5 subreddits.
[00:13:15.920 --> 00:13:20.000]   And then they also get really high quality datasets,
[00:13:20.000 --> 00:13:22.800]   which is like a very information dense
[00:13:22.800 --> 00:13:25.000]   and can help you train some really good LLMs.
[00:13:25.000 --> 00:13:27.960]   And then Nemotron and Common Crawl,
[00:13:27.960 --> 00:13:29.520]   they also did this approach,
[00:13:29.520 --> 00:13:31.440]   but instead of using one classifier,
[00:13:31.440 --> 00:13:33.920]   they used an ensemble of classifiers.
[00:13:33.920 --> 00:13:36.200]   So they use, for example, the DCLM classifier
[00:13:36.200 --> 00:13:38.160]   and also classifiers like the ones we used
[00:13:38.160 --> 00:13:39.520]   in FindWebEducational.
[00:13:39.520 --> 00:13:41.400]   And then they combine these scores
[00:13:41.400 --> 00:13:43.160]   into with an ensemble method
[00:13:43.160 --> 00:13:46.000]   to only retain the best high quality pages.
[00:13:46.000 --> 00:13:48.400]   And they get a dataset that works even better
[00:13:48.400 --> 00:13:49.840]   than the ones we developed.
[00:13:49.840 --> 00:13:54.360]   So that was it for like synthetic data for pre-training.
[00:13:54.360 --> 00:13:56.640]   Now we can go back to post-training.
[00:13:56.640 --> 00:13:58.080]   I think there's a lot of interesting
[00:13:58.080 --> 00:13:59.960]   post-training datasets out there.
[00:13:59.960 --> 00:14:01.760]   One that was released recently,
[00:14:01.760 --> 00:14:03.840]   the Agent Instruct by Microsoft,
[00:14:03.840 --> 00:14:07.080]   where they basically try to target some specific skills
[00:14:07.080 --> 00:14:10.080]   and improve the performance of models on them.
[00:14:10.080 --> 00:14:11.800]   For example, here you can see code,
[00:14:11.800 --> 00:14:14.160]   brain teasers, open domain QA,
[00:14:14.160 --> 00:14:17.120]   and they managed to get a dataset that outperforms
[00:14:17.120 --> 00:14:19.120]   this with fine-tuning Mistral 7B on it.
[00:14:19.120 --> 00:14:21.960]   It outperforms the original instruct model
[00:14:21.960 --> 00:14:23.840]   that was released by Mistral.
[00:14:23.840 --> 00:14:28.840]   And as I said, to get good synthetic data,
[00:14:28.840 --> 00:14:30.400]   you really have to have a framework
[00:14:30.400 --> 00:14:33.000]   to make sure that your data is diverse.
[00:14:33.000 --> 00:14:34.240]   So for example, for them,
[00:14:34.240 --> 00:14:36.200]   they always see the generations
[00:14:36.200 --> 00:14:39.040]   on either source code or raw text documents.
[00:14:39.040 --> 00:14:40.880]   And then they rewrite them to make sure
[00:14:40.880 --> 00:14:43.080]   they're easier to generate instructions from.
[00:14:43.080 --> 00:14:44.080]   And then they use that
[00:14:44.080 --> 00:14:46.600]   for their like instruction data generation.
[00:14:47.600 --> 00:14:50.560]   There's also the Tool3 SFT mixture,
[00:14:50.560 --> 00:14:53.360]   which was released recently by Allen AI.
[00:14:53.360 --> 00:14:54.760]   It's also really good quality
[00:14:54.760 --> 00:14:57.280]   and it covers a wide range of tasks.
[00:14:57.280 --> 00:15:00.240]   And the way they make sure that this dataset is diverse
[00:15:00.240 --> 00:15:04.120]   is by using personas from the Persona Hub datasets,
[00:15:04.120 --> 00:15:05.800]   which is basically a dataset of like,
[00:15:05.800 --> 00:15:07.560]   I think over a million personas.
[00:15:07.560 --> 00:15:09.800]   And for example, in the Tool3 mixture
[00:15:09.800 --> 00:15:11.440]   to generate like a new code snippet,
[00:15:11.440 --> 00:15:13.200]   they would give like the model persona,
[00:15:13.200 --> 00:15:15.640]   for example, a machine learning researcher
[00:15:15.640 --> 00:15:17.240]   interested in neural networks,
[00:15:17.240 --> 00:15:19.920]   and then ask it to generate like a coding problem.
[00:15:19.920 --> 00:15:22.480]   This way you make sure that your dataset is really diverse,
[00:15:22.480 --> 00:15:24.480]   and then you can further filter the datasets,
[00:15:24.480 --> 00:15:26.400]   for example, using the reward models.
[00:15:26.400 --> 00:15:30.720]   We also released a dataset called Smalltalk,
[00:15:30.720 --> 00:15:33.520]   and we also tried to cover the wide range of tasks.
[00:15:33.520 --> 00:15:35.280]   And as you can see here, for example,
[00:15:35.280 --> 00:15:37.880]   when fine-tuning Mistral 7B on the dataset,
[00:15:37.880 --> 00:15:40.920]   we also outperformed the original Mistral instruct
[00:15:40.920 --> 00:15:42.600]   on a number of benchmarks,
[00:15:42.600 --> 00:15:44.120]   notably on mathematics
[00:15:44.120 --> 00:15:46.240]   and instruction following with IF-EVL.
[00:15:46.240 --> 00:15:50.280]   Another paper that's really interesting
[00:15:50.280 --> 00:15:52.000]   I wanted to mention is this one
[00:15:52.000 --> 00:15:55.600]   called the Multilingual Data Arbitrage by Cohere.
[00:15:55.600 --> 00:15:58.200]   And basically they want to generate a dataset
[00:15:58.200 --> 00:16:00.360]   for post-training that is multilingual,
[00:16:00.360 --> 00:16:01.960]   and they have a really interesting problem.
[00:16:01.960 --> 00:16:04.120]   It's the fact that there isn't like one model
[00:16:04.120 --> 00:16:06.840]   that's really good at all the languages they wanted.
[00:16:06.840 --> 00:16:09.240]   So what they do is that like they use
[00:16:09.240 --> 00:16:12.080]   not just one teacher model, but multiple teachers,
[00:16:12.080 --> 00:16:13.600]   and then they have a router,
[00:16:13.600 --> 00:16:15.760]   which basically sends the prompts they have
[00:16:15.760 --> 00:16:16.840]   to all these models.
[00:16:16.840 --> 00:16:18.360]   And then they get the completions,
[00:16:18.360 --> 00:16:19.520]   and they have a reward model
[00:16:19.520 --> 00:16:21.240]   that trace all these generations
[00:16:21.240 --> 00:16:23.000]   and only keeps the best one.
[00:16:23.000 --> 00:16:24.920]   And this is like arbitrage and finance.
[00:16:24.920 --> 00:16:27.480]   So what I think was interesting in this,
[00:16:27.480 --> 00:16:28.800]   it shows that like synthetic data,
[00:16:28.800 --> 00:16:31.040]   it doesn't have to come from a single model.
[00:16:31.040 --> 00:16:33.080]   And because we have so many good models now,
[00:16:33.080 --> 00:16:34.760]   you could like pull these models together
[00:16:34.760 --> 00:16:37.440]   and get like a dataset that's of a really high quality,
[00:16:37.440 --> 00:16:40.320]   and that's diverse, and that covers all your needs.
[00:16:43.520 --> 00:16:46.760]   I was supposed to put a meme there, but lack of time.
[00:16:46.760 --> 00:16:52.640]   Yeah, so that was it for like synthetic data.
[00:16:52.640 --> 00:16:55.000]   And now we can go to see what's happening
[00:16:55.000 --> 00:16:57.840]   in the small models field in 2024.
[00:16:57.840 --> 00:17:01.000]   I don't know if you know,
[00:17:01.000 --> 00:17:03.480]   but like now we have some really good small models.
[00:17:03.480 --> 00:17:08.480]   For example, Lama 3.2 1b, it matches Lama 2.13b from,
[00:17:08.480 --> 00:17:11.680]   that was released last year on the LMSS arena,
[00:17:11.680 --> 00:17:14.160]   which is basically the default go-to leaderboard
[00:17:14.160 --> 00:17:17.440]   for evaluating models using human evaluation.
[00:17:17.440 --> 00:17:18.600]   And as you can see here,
[00:17:18.600 --> 00:17:20.600]   the scores of the models are really close.
[00:17:20.600 --> 00:17:22.640]   So I think we've made like a huge leap forward
[00:17:22.640 --> 00:17:24.160]   in terms of small models.
[00:17:24.160 --> 00:17:28.120]   Of course, that's just one data point, but there's more.
[00:17:28.120 --> 00:17:30.600]   For example, if you look at this chart
[00:17:30.600 --> 00:17:32.960]   from the Quent 2.5 blog posts,
[00:17:32.960 --> 00:17:35.640]   it shows that today we have some really good models
[00:17:35.640 --> 00:17:39.160]   that are only like 3 billion parameters and 4 billion,
[00:17:39.160 --> 00:17:41.600]   the score really high on MMLU,
[00:17:41.600 --> 00:17:45.040]   which is a really popular benchmark for evaluating models.
[00:17:45.040 --> 00:17:47.360]   And you can see here that the blue dots
[00:17:47.360 --> 00:17:52.160]   have more than 65 on MMLU and the gray ones have less.
[00:17:52.160 --> 00:17:55.000]   And for example, Lama 33b had less.
[00:17:55.000 --> 00:17:59.480]   So now we have a 3b model that outperforms a 33b model
[00:17:59.480 --> 00:18:02.840]   that was released earlier on MMLU benchmark.
[00:18:02.840 --> 00:18:05.760]   So I think now people are starting to realize
[00:18:05.760 --> 00:18:08.840]   that like we shouldn't just scale and scale models,
[00:18:08.840 --> 00:18:11.360]   but we should try to make them more efficient.
[00:18:11.360 --> 00:18:14.760]   I don't know if you knew,
[00:18:14.760 --> 00:18:18.480]   but you can also chat with a 3b+ model on your iPhone.
[00:18:18.480 --> 00:18:21.120]   For example, here, this is an app called PocketPal,
[00:18:21.120 --> 00:18:24.080]   where you can go and select a model from Hugging Face.
[00:18:24.080 --> 00:18:25.480]   It has a large choice.
[00:18:25.480 --> 00:18:28.840]   For example, here, we loaded the PHY 3.5,
[00:18:28.840 --> 00:18:32.400]   which is 3.8 billion parameters on this iPhone,
[00:18:32.400 --> 00:18:33.840]   and we can chat with it.
[00:18:33.840 --> 00:18:37.600]   And you can see that even the latency is also acceptable.
[00:18:37.600 --> 00:18:40.240]   For example, here, I asked it to give me a joke
[00:18:40.240 --> 00:18:43.000]   about NeurIPS, so let's see what it has to say.
[00:18:43.000 --> 00:18:49.480]   Okay, why did the neural network attend NeurIPS?
[00:18:49.480 --> 00:18:52.320]   Because it heard there would be a lot of layers and fun,
[00:18:52.320 --> 00:18:54.760]   and it wanted to train its sense of humor.
[00:18:54.760 --> 00:18:57.400]   So not very funny, but at least it can run on device.
[00:18:57.400 --> 00:19:02.160]   Yeah, so I think now we have good small models,
[00:19:02.160 --> 00:19:04.600]   but we also have like good frameworks and tools
[00:19:04.600 --> 00:19:06.240]   to use these small models.
[00:19:06.240 --> 00:19:09.320]   So I think we're really close to having like really on-edge
[00:19:09.320 --> 00:19:12.440]   and on-device models that are really good.
[00:19:12.440 --> 00:19:15.160]   And I think for a while, we've had this narrative
[00:19:15.160 --> 00:19:18.280]   that just training larger models is better.
[00:19:18.280 --> 00:19:22.040]   Of course, this is supported by science scaling laws.
[00:19:22.040 --> 00:19:23.440]   As you can see here, for example,
[00:19:23.440 --> 00:19:26.080]   when we scale the model size, the loss is lower,
[00:19:26.080 --> 00:19:28.240]   and obviously you get a better model.
[00:19:28.240 --> 00:19:31.000]   But, and we can see this, for example,
[00:19:31.000 --> 00:19:32.760]   in the GPT family of models,
[00:19:32.760 --> 00:19:34.920]   how we went from just 100 million parameters
[00:19:34.920 --> 00:19:37.000]   to more than a trillion parameters.
[00:19:37.000 --> 00:19:39.720]   And of course, we all observed the performance improvement
[00:19:39.720 --> 00:19:42.160]   when using the latest model.
[00:19:42.160 --> 00:19:43.760]   But one thing that we shouldn't forget
[00:19:43.760 --> 00:19:45.360]   is that when we scale the model,
[00:19:45.360 --> 00:19:48.120]   we also scale the inference costs and time.
[00:19:48.120 --> 00:19:51.480]   And so the largest models are gonna cost so much more.
[00:19:51.480 --> 00:19:56.320]   So I think now, instead of just building larger models,
[00:19:56.320 --> 00:19:59.120]   we should be focusing on building more efficient models.
[00:19:59.120 --> 00:20:01.680]   It's no longer a race for the largest models,
[00:20:01.680 --> 00:20:04.040]   since these models are really expensive to run,
[00:20:04.040 --> 00:20:07.240]   and they require a really good infrastructure to do that,
[00:20:07.240 --> 00:20:10.560]   and they cannot run on, for example, consumer hardware.
[00:20:10.560 --> 00:20:12.920]   And when you try to build more efficient models
[00:20:12.920 --> 00:20:14.960]   that match larger models,
[00:20:14.960 --> 00:20:16.760]   that's when you can really unlock
[00:20:16.760 --> 00:20:18.960]   some really interesting on-device use cases.
[00:20:18.960 --> 00:20:21.920]   And I think a trend that we're noticing now
[00:20:21.920 --> 00:20:24.840]   is the trend of training smaller models longer.
[00:20:24.840 --> 00:20:28.280]   For example, if you compare how long Lama was trained
[00:20:28.280 --> 00:20:29.720]   compared to Lama 3,
[00:20:29.720 --> 00:20:33.280]   there is a huge increase in the pre-training length.
[00:20:33.280 --> 00:20:35.320]   Lama was trained on 1 trillion tokens,
[00:20:35.320 --> 00:20:38.600]   but Lama 3 A to B was trained on 15 trillion tokens.
[00:20:38.600 --> 00:20:41.920]   So Meta managed to get a model that's the same size,
[00:20:41.920 --> 00:20:43.760]   but it performs so much better
[00:20:43.760 --> 00:20:47.960]   by choosing to spend the sacrifice during training,
[00:20:47.960 --> 00:20:49.960]   because as we know, training is a one-time cost,
[00:20:49.960 --> 00:20:52.080]   but inference is something that's ongoing.
[00:20:52.080 --> 00:20:58.840]   If we wanna see what are the small models reads in 2024,
[00:20:58.840 --> 00:21:02.080]   I think this mobile LLM paper by Meta is interesting.
[00:21:02.080 --> 00:21:04.840]   They try to study different models
[00:21:04.840 --> 00:21:07.760]   that have less than 1 billion parameters
[00:21:07.760 --> 00:21:10.000]   and find which architecture makes most sense
[00:21:10.000 --> 00:21:11.120]   for these models.
[00:21:11.120 --> 00:21:13.040]   For example, they find that depth
[00:21:13.040 --> 00:21:15.040]   is more important than width.
[00:21:15.040 --> 00:21:16.600]   So it's more important to have models
[00:21:16.600 --> 00:21:19.920]   that have more layers than just making them more wide.
[00:21:19.920 --> 00:21:22.400]   They also find that GQA helps,
[00:21:22.400 --> 00:21:24.400]   that tying the embedding helps.
[00:21:24.400 --> 00:21:26.120]   So I think it's a nice study overall
[00:21:26.120 --> 00:21:29.040]   for models that are just a few hundred million parameters.
[00:21:30.240 --> 00:21:32.880]   There's also the Apple Intelligence Tech Report,
[00:21:32.880 --> 00:21:34.520]   which is interesting.
[00:21:34.520 --> 00:21:36.760]   So for Apple Intelligence, they had two models,
[00:21:36.760 --> 00:21:40.600]   one that was on server and another model that was on device.
[00:21:40.600 --> 00:21:42.800]   It had 3 billion parameters.
[00:21:42.800 --> 00:21:44.160]   And I think the interesting part
[00:21:44.160 --> 00:21:45.400]   is that they trained this model
[00:21:45.400 --> 00:21:47.800]   using pruning and then distillation.
[00:21:47.800 --> 00:21:49.200]   And for example, they have this table
[00:21:49.200 --> 00:21:52.080]   where they show that using pruning and distillation
[00:21:52.080 --> 00:21:54.640]   works much better than training from scratch.
[00:21:54.640 --> 00:21:56.360]   And they also have some interesting insights
[00:21:56.360 --> 00:21:59.560]   about how they specialize their models on specific tasks.
[00:21:59.560 --> 00:22:02.040]   Like for example, summarization and rewriting.
[00:22:02.040 --> 00:22:07.000]   There's also this paper by NVIDIA
[00:22:07.000 --> 00:22:08.480]   that was released recently.
[00:22:08.480 --> 00:22:10.840]   I think you've already had a talk about hybrid models.
[00:22:10.840 --> 00:22:12.720]   That was all interesting.
[00:22:12.720 --> 00:22:16.040]   And this model, they used a hybrid architecture
[00:22:16.040 --> 00:22:18.800]   between state space models and transformers.
[00:22:18.800 --> 00:22:20.600]   And they managed to train a 1B model
[00:22:20.600 --> 00:22:22.040]   that's really performant
[00:22:22.040 --> 00:22:24.440]   without needing to train it on a lot of tokens.
[00:22:24.440 --> 00:22:28.200]   And regarding our work,
[00:22:28.200 --> 00:22:30.800]   we just recently released Small M2.
[00:22:30.800 --> 00:22:32.840]   So it's a series of three models,
[00:22:32.840 --> 00:22:35.920]   which are the best in class in each model size.
[00:22:35.920 --> 00:22:40.080]   For example, our 1.7B model outperforms Lama 1B
[00:22:40.080 --> 00:22:42.160]   and also 0.2.5.
[00:22:42.160 --> 00:22:44.520]   And how we managed to train this model
[00:22:44.520 --> 00:22:46.240]   is that we spent a lot of time
[00:22:46.240 --> 00:22:48.600]   trying to curate the pre-training datasets.
[00:22:48.600 --> 00:22:49.840]   We did a lot of ablations,
[00:22:49.840 --> 00:22:52.240]   trying to find which datasets are good
[00:22:52.240 --> 00:22:53.760]   and also how to mix them.
[00:22:53.760 --> 00:22:56.400]   We also created some new math and code datasets
[00:22:56.400 --> 00:22:57.880]   that we're releasing soon.
[00:22:57.880 --> 00:22:59.560]   But you basically really spent a lot of time
[00:22:59.560 --> 00:23:01.160]   trying to find what's the best mixture
[00:23:01.160 --> 00:23:03.000]   that you can train these models on.
[00:23:03.000 --> 00:23:05.440]   And then we spent some time trying to like,
[00:23:05.440 --> 00:23:07.560]   we also trained these models for very long.
[00:23:07.560 --> 00:23:09.440]   For example, Small M1 was trained
[00:23:09.440 --> 00:23:11.240]   only on 1 trillion tokens,
[00:23:11.240 --> 00:23:13.840]   but this model is trained on 11 trillion tokens.
[00:23:13.840 --> 00:23:15.800]   And we saw that the performance kept improving.
[00:23:15.800 --> 00:23:18.120]   The models didn't really plateau mid-training,
[00:23:18.120 --> 00:23:19.400]   which I think is really interesting.
[00:23:19.400 --> 00:23:22.520]   It shows that you can train such small models for very long
[00:23:22.520 --> 00:23:24.320]   and keep getting performance gains.
[00:23:26.480 --> 00:23:29.320]   What's interesting about Small M2 is that it's fully open.
[00:23:29.320 --> 00:23:32.040]   We also released the pre-training code base,
[00:23:32.040 --> 00:23:33.840]   the fine-tuning code and datasets
[00:23:33.840 --> 00:23:36.280]   and also evaluation in this repository.
[00:23:36.280 --> 00:23:41.440]   Also, there's really interesting small models for text,
[00:23:41.440 --> 00:23:42.720]   but also for vision.
[00:23:42.720 --> 00:23:44.600]   For example, here you can see Small VLM,
[00:23:44.600 --> 00:23:46.680]   which is a 2B model that's really efficient.
[00:23:46.680 --> 00:23:48.360]   It doesn't consume a lot of RAM
[00:23:48.360 --> 00:23:50.480]   and it also has a good performance.
[00:23:50.480 --> 00:23:55.040]   There's also Moondream 0.5B, which was released recently.
[00:23:55.040 --> 00:23:57.240]   It's like the smallest vision language model.
[00:23:57.240 --> 00:23:59.840]   And as you can see, there isn't a big trade-off
[00:23:59.840 --> 00:24:01.560]   compared to Moondream 2B.
[00:24:01.560 --> 00:24:05.840]   So now I showed you that we have
[00:24:05.840 --> 00:24:07.280]   some really good small models.
[00:24:07.280 --> 00:24:09.080]   We also have the tools to use them,
[00:24:09.080 --> 00:24:11.920]   but why should you consider using small models and when?
[00:24:11.920 --> 00:24:15.840]   I think small models are really interesting
[00:24:15.840 --> 00:24:18.240]   because of the on-device feature.
[00:24:18.240 --> 00:24:20.760]   Because these models are small and they can run fast,
[00:24:20.760 --> 00:24:22.880]   you can basically run them on your laptop,
[00:24:22.880 --> 00:24:24.720]   but also on your mobile phone.
[00:24:24.720 --> 00:24:27.200]   And this means that your dataset stays locally.
[00:24:27.200 --> 00:24:30.240]   You don't have to send your queries to third parties.
[00:24:30.240 --> 00:24:32.200]   And this really enhances privacy.
[00:24:32.200 --> 00:24:33.040]   That was, for example,
[00:24:33.040 --> 00:24:35.760]   one of the big selling points for Apple Intelligence.
[00:24:35.760 --> 00:24:39.920]   Also, right now we really have so many frameworks
[00:24:39.920 --> 00:24:41.520]   to do on-device inference.
[00:24:41.520 --> 00:24:45.480]   For example, there's MLX, MLC, LLAMA, CPP, Transformers.js.
[00:24:45.480 --> 00:24:46.800]   So we have a lot of options
[00:24:46.800 --> 00:24:48.840]   and each of them have great features.
[00:24:48.840 --> 00:24:51.200]   So you have so many options for doing that.
[00:24:52.800 --> 00:24:54.920]   Small models are also really powerful
[00:24:54.920 --> 00:24:56.720]   if you choose to specialize them.
[00:24:56.720 --> 00:24:59.360]   For example, here there's a startup called NuMind,
[00:24:59.360 --> 00:25:00.320]   which took small LLAM,
[00:25:00.320 --> 00:25:03.440]   and then they fine-tuned this on text extraction datasets.
[00:25:03.440 --> 00:25:04.960]   And they managed to get a model
[00:25:04.960 --> 00:25:07.880]   that's not very far from models that are much larger.
[00:25:07.880 --> 00:25:10.000]   So I think text extraction is like one use case
[00:25:10.000 --> 00:25:12.240]   where small models can be really performant
[00:25:12.240 --> 00:25:13.720]   and it makes sense to use them
[00:25:13.720 --> 00:25:15.640]   instead of just using larger models.
[00:25:15.640 --> 00:25:19.560]   You can also chat with these models in browser.
[00:25:19.560 --> 00:25:21.120]   For example, here you can go there,
[00:25:21.120 --> 00:25:23.560]   you can load the model, you can even turn off your internet
[00:25:23.560 --> 00:25:26.240]   and just start chatting with the model locally.
[00:25:26.240 --> 00:25:29.480]   Speaking of text extraction,
[00:25:29.480 --> 00:25:31.040]   if you don't want to fine-tune the models,
[00:25:31.040 --> 00:25:34.440]   there's really good method of structure generation.
[00:25:34.440 --> 00:25:35.960]   We can basically force the models
[00:25:35.960 --> 00:25:38.520]   to follow a JSON schema that you defined.
[00:25:38.520 --> 00:25:40.680]   For example, here we try to force the model
[00:25:40.680 --> 00:25:44.840]   to follow a schema for extracting key information
[00:25:44.840 --> 00:25:46.160]   from GitHub issues.
[00:25:46.160 --> 00:25:48.000]   So we can input free text,
[00:25:48.000 --> 00:25:50.680]   which is a complaint about a GitHub repository,
[00:25:50.680 --> 00:25:52.040]   something not working.
[00:25:52.040 --> 00:25:53.080]   And then you can run it there
[00:25:53.080 --> 00:25:55.400]   and the model can extract anything that is relevant
[00:25:55.400 --> 00:25:57.240]   for your GitHub issue creation.
[00:25:57.240 --> 00:25:58.640]   For example, the priority.
[00:25:58.640 --> 00:26:00.400]   For example, here priority is high,
[00:26:00.400 --> 00:26:01.880]   the type of the issue, bug,
[00:26:01.880 --> 00:26:03.840]   and then a title and the estimation
[00:26:03.840 --> 00:26:05.680]   of how long this will take to fix.
[00:26:05.680 --> 00:26:08.000]   And you can just like do this in the browser.
[00:26:08.000 --> 00:26:11.120]   You can transform your text into a GitHub issue
[00:26:11.120 --> 00:26:12.680]   that's properly formatted.
[00:26:12.680 --> 00:26:19.000]   So what's next for synthetic data and small models?
[00:26:19.000 --> 00:26:21.520]   I think that domain specific synthetic data
[00:26:21.520 --> 00:26:23.600]   is gonna be, it's already important,
[00:26:23.600 --> 00:26:25.600]   it's gonna be even more important.
[00:26:25.600 --> 00:26:28.720]   For example, generating synthetic data for math.
[00:26:28.720 --> 00:26:31.120]   I think this really would help improve
[00:26:31.120 --> 00:26:33.080]   the reasoning of a lot of models.
[00:26:33.080 --> 00:26:34.320]   And a lot of people are doing it,
[00:26:34.320 --> 00:26:36.080]   for example, Quint 2.5 math,
[00:26:36.080 --> 00:26:38.400]   everyone's trying to reproduce a one.
[00:26:38.400 --> 00:26:40.280]   And so I think for synthetic data,
[00:26:40.280 --> 00:26:42.120]   trying to specialize it on some domains
[00:26:42.120 --> 00:26:43.920]   is gonna be really important.
[00:26:43.920 --> 00:26:45.280]   And then for small models,
[00:26:45.280 --> 00:26:47.800]   I think specializing them through fine-tuning,
[00:26:47.800 --> 00:26:49.840]   it's also gonna be really important.
[00:26:49.840 --> 00:26:51.160]   'Cause I think a lot of companies
[00:26:51.160 --> 00:26:53.240]   are just trying to use these large models
[00:26:53.240 --> 00:26:54.640]   because they are better.
[00:26:54.640 --> 00:26:55.560]   But on some tasks,
[00:26:55.560 --> 00:26:57.640]   I think you can already get decent performance
[00:26:57.640 --> 00:26:58.480]   with small models.
[00:26:58.480 --> 00:27:01.760]   So you don't need to pay like a cost that's much larger
[00:27:01.760 --> 00:27:05.800]   just to make your model better at your task by a few percent.
[00:27:05.800 --> 00:27:07.400]   And this is not just for text.
[00:27:07.400 --> 00:27:09.800]   And I think it also applies for other modalities
[00:27:09.800 --> 00:27:11.800]   like vision and audio.
[00:27:11.800 --> 00:27:13.320]   And I think you should also watch out
[00:27:13.320 --> 00:27:15.720]   for on-device frameworks and applications.
[00:27:15.720 --> 00:27:17.000]   For example, like the app I showed,
[00:27:17.000 --> 00:27:19.240]   Pokestpal, Olama, all these frameworks
[00:27:19.240 --> 00:27:20.760]   are becoming really popular.
[00:27:20.760 --> 00:27:22.120]   And I'm pretty sure that we're gonna get
[00:27:22.120 --> 00:27:24.160]   like more of them in 2025.
[00:27:24.160 --> 00:27:26.680]   And users really like that.
[00:27:26.680 --> 00:27:31.280]   Maybe for other, I should also say a hot take.
[00:27:31.280 --> 00:27:32.520]   I think that like in AI,
[00:27:32.520 --> 00:27:35.120]   we started like with fine-tuning, for example,
[00:27:35.120 --> 00:27:38.280]   trying to make BERT work on some specific use cases
[00:27:38.280 --> 00:27:40.080]   and really struggling to do that.
[00:27:40.080 --> 00:27:41.960]   And then we had some models that are much larger.
[00:27:41.960 --> 00:27:44.760]   So we just switched to like prompt engineering
[00:27:44.760 --> 00:27:46.760]   to get the models to solve our tasks.
[00:27:46.760 --> 00:27:48.360]   I think we're going back to fine-tuning
[00:27:48.360 --> 00:27:50.360]   where we realized these models are really costly.
[00:27:50.360 --> 00:27:51.880]   It's better to use just a small model.
[00:27:51.880 --> 00:27:53.360]   We'll try to specialize it.
[00:27:53.360 --> 00:27:54.880]   So I think it's a little bit of a cycle
[00:27:54.880 --> 00:27:57.200]   and we're gonna start to see like more fine-tuning
[00:27:57.200 --> 00:27:59.920]   and less of just like prompt engineering the models.
[00:27:59.920 --> 00:28:01.960]   So that was my talk.
[00:28:01.960 --> 00:28:02.960]   Thank you for following.
[00:28:02.960 --> 00:28:05.600]   And if you have any questions, we can take them now.
[00:28:05.600 --> 00:28:07.120]   (audience applauding)

