
[00:00:00.000 --> 00:00:05.800]   I find personally a lot of impact in being downstream of these problems.
[00:00:05.800 --> 00:00:07.700]   If I'm going to make messes, I have to clean them up.
[00:00:07.700 --> 00:00:11.880]   So in some sense, my policy work is an attempt to make sure that as I'm on the bleeding edge
[00:00:11.880 --> 00:00:15.640]   of creating this technology, I'm also providing that same insight to policymakers so they
[00:00:15.640 --> 00:00:17.960]   can adapt to this as quickly as possible.
[00:00:17.960 --> 00:00:22.320]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:22.320 --> 00:00:24.600]   and I'm your host, Lukas Biewald.
[00:00:24.600 --> 00:00:29.880]   Jonathan Frankel is chief scientist at MosaicML and a soon to be Harvard professor.
[00:00:29.880 --> 00:00:34.640]   He wrote an exceptional paper, Lottery Ticket Hypothesis, about how neural networks learn
[00:00:34.640 --> 00:00:36.560]   and how you can prune them.
[00:00:36.560 --> 00:00:40.360]   He's also taught policy at Georgetown University Law Center.
[00:00:40.360 --> 00:00:44.320]   This is a super interesting conversation and I hope you enjoy it as much as I did.
[00:00:44.320 --> 00:00:49.520]   All right, why don't we start by hearing about your journey to what you're doing now.
[00:00:49.520 --> 00:00:51.800]   I think you've had an interesting background and career.
[00:00:51.800 --> 00:00:53.440]   Probably great to start there.
[00:00:53.440 --> 00:00:55.040]   Yeah, it's been a winding road.
[00:00:55.040 --> 00:00:59.600]   If you go and look at my CV, you'll be a little bit confused, I think, about some of
[00:00:59.600 --> 00:01:02.080]   the things that have happened and how they came to be.
[00:01:02.080 --> 00:01:07.320]   The high level is I'm a computer scientist trained from the beginning, from undergraduate
[00:01:07.320 --> 00:01:08.600]   all the way to the present.
[00:01:08.600 --> 00:01:12.520]   I'm actually defending my dissertation this Friday, so I can't quite say I have three
[00:01:12.520 --> 00:01:15.840]   degrees yet, but very, very close to it.
[00:01:15.840 --> 00:01:18.080]   But it's a bit of an odd trajectory.
[00:01:18.080 --> 00:01:22.000]   As an undergraduate, I did some research on programming language theory, which is what
[00:01:22.000 --> 00:01:23.680]   I got my master's degree in.
[00:01:23.680 --> 00:01:26.880]   Then I went and spent a year teaching at a law school and doing technology policy work
[00:01:26.880 --> 00:01:27.880]   in DC.
[00:01:27.880 --> 00:01:33.160]   Then I came back to MIT and wrote a paper on cryptography and somehow stumbled my way
[00:01:33.160 --> 00:01:36.560]   into machine learning somewhere along the way, having never taken a class on the topic
[00:01:36.560 --> 00:01:38.360]   prior to grad school.
[00:01:38.360 --> 00:01:42.240]   I think that journey, there are really two big takeaways, I think, if you want to understand
[00:01:42.240 --> 00:01:44.920]   me better and kind of get to know how I think about the world.
[00:01:44.920 --> 00:01:48.440]   One is that as you start connecting the dots on the topics that I've worked on and what
[00:01:48.440 --> 00:01:53.240]   I've been good at, there are all the messy, hard to measure problems.
[00:01:53.240 --> 00:01:56.440]   I don't like to work on clean things where you get a nice proof and call it a day.
[00:01:56.440 --> 00:02:01.320]   I love working on the messy things that intrinsically don't have answers, security and privacy,
[00:02:01.320 --> 00:02:07.280]   law and policy, and now deep learning where there's no nice, neat proof that's going to
[00:02:07.280 --> 00:02:08.960]   wrap everything up and tell us all the answers.
[00:02:08.960 --> 00:02:10.200]   It's going to be intrinsically messy.
[00:02:10.200 --> 00:02:12.960]   We're dealing with complex problems in real world data.
[00:02:12.960 --> 00:02:17.240]   What we do at Mosaic is to try to exploit that messiness and find a path through it
[00:02:17.240 --> 00:02:19.960]   in order to deliver more efficiency to people.
[00:02:19.960 --> 00:02:23.600]   If you really want to connect the dots, that's really how you'd put the pieces together as
[00:02:23.600 --> 00:02:25.120]   far as I understand it.
[00:02:25.120 --> 00:02:26.120]   Awesome.
[00:02:26.120 --> 00:02:31.840]   Well, I'd love to start with, this is probably bad from a podcast marketing perspective,
[00:02:31.840 --> 00:02:35.480]   but I want to start with the thing that I'm most interested to ask you about, which is
[00:02:35.480 --> 00:02:42.920]   you did a really well-known paper on pruning neural networks, lottery ticket hypothesis,
[00:02:42.920 --> 00:02:45.280]   I believe, or what was the title of it?
[00:02:45.280 --> 00:02:46.280]   That's the one.
[00:02:46.280 --> 00:02:47.480]   That's notoriety at this point.
[00:02:47.640 --> 00:02:53.400]   I guess before we get into it, if you could describe the thesis or the key results of
[00:02:53.400 --> 00:02:55.480]   the paper, and then I have a few questions for you.
[00:02:55.480 --> 00:02:56.480]   Yeah.
[00:02:56.480 --> 00:02:58.880]   Speaking of describing a thesis, in my other tab right now, I've got Overleaf open with
[00:02:58.880 --> 00:02:59.880]   that thesis.
[00:02:59.880 --> 00:03:05.280]   But the really simple statement, not the 200-page version, because I'm sure nobody wants to
[00:03:05.280 --> 00:03:06.280]   hear that.
[00:03:06.280 --> 00:03:08.240]   If you want to, it'll be an archive.
[00:03:08.240 --> 00:03:14.640]   But the really simple version is that the neural networks we train, ask yourself, why
[00:03:14.640 --> 00:03:17.760]   do we train them in the particular way that we do, with that learning rate, with this
[00:03:17.760 --> 00:03:21.600]   recipe, with this optimizer, with this kind of normalization?
[00:03:21.600 --> 00:03:25.880]   The answer is usually, well, because someone else did it and they got it to work.
[00:03:25.880 --> 00:03:28.000]   Usually when it comes to ResNet, Kai Ming did it that way.
[00:03:28.000 --> 00:03:31.440]   When it comes to a transformer, Ashish Vaswani did it that way.
[00:03:31.440 --> 00:03:33.560]   And so that's the reason why we train it that way.
[00:03:33.560 --> 00:03:38.520]   And in many ways, the story of my career in machine learning is questioning those choices.
[00:03:38.520 --> 00:03:41.760]   And in the lottery ticket work, I questioned one very specific choice.
[00:03:41.760 --> 00:03:43.200]   Why do we use all these weights?
[00:03:43.200 --> 00:03:44.380]   These networks are really big.
[00:03:44.380 --> 00:03:47.660]   We know they're so-called over-parameterized, but why?
[00:03:47.660 --> 00:03:50.760]   And I read at that time in my career, all these papers on neural network pruning, this
[00:03:50.760 --> 00:03:55.160]   topic where you train the network and then delete connections that seem unnecessary.
[00:03:55.160 --> 00:03:59.280]   And you end up with a much smaller network that, as far as we can tell, performs about
[00:03:59.280 --> 00:04:01.560]   the same as the original network we started with.
[00:04:01.560 --> 00:04:03.360]   Why did we need those weights to begin with then?
[00:04:03.360 --> 00:04:07.400]   Is there something intrinsically harder about learning than there is then representing what
[00:04:07.400 --> 00:04:08.400]   you've learned?
[00:04:08.400 --> 00:04:12.360]   Is it easier to know the rules of calculus than it is to learn and process them for the
[00:04:12.360 --> 00:04:13.360]   first time?
[00:04:13.360 --> 00:04:16.320]   Maybe our networks have to be big early on and can get small later as they get kind of
[00:04:16.320 --> 00:04:19.160]   smarter and have more compact representation.
[00:04:19.160 --> 00:04:22.840]   That was what one of my professors at MIT told me when I asked him, why can't we train
[00:04:22.840 --> 00:04:24.600]   smaller networks?
[00:04:24.600 --> 00:04:28.920]   And the lottery ticket ideas are one way that I found to make it possible to train smaller
[00:04:28.920 --> 00:04:30.080]   networks.
[00:04:30.080 --> 00:04:33.520]   And the trick is that any weight you were going to delete at the end of training, you
[00:04:33.520 --> 00:04:34.840]   never really need it.
[00:04:34.840 --> 00:04:40.560]   You could have gotten rid of it at the beginning or nearly at the beginning, but with one catch.
[00:04:40.560 --> 00:04:43.480]   When we create a neural network, we kind of set each connection to a random value at the
[00:04:43.480 --> 00:04:44.480]   beginning.
[00:04:44.480 --> 00:04:45.920]   We have to initialize it to something.
[00:04:45.920 --> 00:04:46.920]   We don't know what yet.
[00:04:46.920 --> 00:04:50.200]   And the whole point of optimization is to get those weights to good values.
[00:04:50.200 --> 00:04:55.160]   But it turns out those random values aren't so random, or rather, the specific sample
[00:04:55.160 --> 00:04:58.360]   we get from that random distribution is really important.
[00:04:58.360 --> 00:05:03.240]   And each weight in this smaller, sparse, pruned network needs to be set to the right value
[00:05:03.240 --> 00:05:04.440]   for it to be able to learn.
[00:05:04.440 --> 00:05:08.720]   And what I found is actually the values that those weights are randomly assigned actually
[00:05:08.720 --> 00:05:11.600]   are really important for making those particular weights important.
[00:05:11.600 --> 00:05:13.640]   This sub-network won the lottery.
[00:05:13.640 --> 00:05:15.840]   It happened to get a set of weights that allowed it to train well.
[00:05:15.840 --> 00:05:19.160]   If you sample a new set of initializations for it, it does really badly.
[00:05:19.160 --> 00:05:23.320]   And this initialization sensitivity is something that we don't typically see when we train
[00:05:23.320 --> 00:05:25.480]   traditional neural networks that aren't pruned.
[00:05:25.480 --> 00:05:29.120]   Ironically, it's something we now see all the time with these foundation models.
[00:05:29.120 --> 00:05:31.360]   The whole point of a foundation model is a good initialization.
[00:05:31.360 --> 00:05:34.080]   So I think these ideas have come back around again in a neat way.
[00:05:34.080 --> 00:05:41.280]   So are you saying that really the point of having many more weights than you need is
[00:05:41.280 --> 00:05:45.120]   just that some of them randomly get assigned good initialization values?
[00:05:45.120 --> 00:05:46.120]   Is that what you're saying?
[00:05:46.120 --> 00:05:47.240]   It's a possibility.
[00:05:47.240 --> 00:05:50.680]   And the only reason I'll say it's a possibility is because I'm an empiricist.
[00:05:50.680 --> 00:05:54.360]   If I'm going to make a claim, I need to have an experiment to evaluate it and try to falsify
[00:05:54.360 --> 00:05:55.360]   it.
[00:05:55.360 --> 00:05:57.000]   And it's hard to figure out how to falsify that claim.
[00:05:57.000 --> 00:06:00.680]   If I wanted to do that, I'd really have to try every possible sub-network to see how
[00:06:00.680 --> 00:06:04.640]   many lucky ones there are and whether there are other sub-networks that got lucky that
[00:06:04.640 --> 00:06:07.880]   just didn't happen to emerge or whether this one was the one and only.
[00:06:07.880 --> 00:06:10.640]   That's my conjecture, but testing it is very difficult.
[00:06:10.640 --> 00:06:14.680]   That perhaps by a certain point in training, the network is optimizing in a lower dimensional
[00:06:14.680 --> 00:06:17.160]   subspace that some of the weights just become unnecessary.
[00:06:17.160 --> 00:06:21.600]   And so learning actually can take place pretty successfully without those extraneous weights,
[00:06:21.600 --> 00:06:25.160]   or at least whatever subspace it's in could be axis aligned in such a way that you could
[00:06:25.160 --> 00:06:27.360]   prune a bunch of weights from the network.
[00:06:27.360 --> 00:06:30.320]   Now, again, testing that is exceedingly difficult.
[00:06:30.320 --> 00:06:31.760]   If you have a way to do that, please let me know.
[00:06:31.760 --> 00:06:34.120]   I'd love to have another dissertation chapter.
[00:06:34.120 --> 00:06:37.840]   But I think that is the high level conjecture that in the original paper, we called it the
[00:06:37.840 --> 00:06:38.840]   lottery ticket hypothesis.
[00:06:38.840 --> 00:06:41.920]   There's a hypothesis and a conjecture, and that is the statement of the conjecture.
[00:06:41.920 --> 00:06:46.840]   And I guess one way to check it is just to go back to what they were originally set to
[00:06:46.840 --> 00:06:51.260]   and see that it has the same quality of performance, right?
[00:06:51.260 --> 00:06:55.360]   So when you go back to what they originally set to and it has the same quality of performance,
[00:06:55.360 --> 00:06:58.400]   that at least indicates that that sub-network is sufficient, but it doesn't necessarily
[00:06:58.400 --> 00:07:01.320]   mean that sub-network is actually important to the training when the whole network is
[00:07:01.320 --> 00:07:02.320]   there.
[00:07:02.320 --> 00:07:05.740]   It could be that we've found our way into some sufficient sub-network that was actually
[00:07:05.740 --> 00:07:07.920]   doing nothing in the context of the whole network.
[00:07:07.920 --> 00:07:11.880]   So being able to say what that sub-network is doing as part of a whole is a little bit
[00:07:11.880 --> 00:07:12.880]   more difficult.
[00:07:12.880 --> 00:07:15.280]   It's entirely possible that the optimization picture looks completely different when you
[00:07:15.280 --> 00:07:19.560]   have the dense network that isn't pruned and the sparse network that is pruned.
[00:07:19.560 --> 00:07:22.640]   And we do know that that optimization behavior is pretty different.
[00:07:22.640 --> 00:07:23.640]   Pretty different.
[00:07:23.640 --> 00:07:29.520]   So you can't just reset the weights to what they were when you started training and then
[00:07:29.520 --> 00:07:32.840]   remove all other weights and get the same performance then, I guess?
[00:07:32.840 --> 00:07:34.080]   Oh, you definitely can.
[00:07:34.080 --> 00:07:35.080]   Oh, you can?
[00:07:35.080 --> 00:07:36.080]   Yeah.
[00:07:36.080 --> 00:07:39.960]   That's kind of the crux of the lottery ticket experiment is removing all the weights except
[00:07:39.960 --> 00:07:42.840]   those that you kept from pruning and then just setting them back to their initializations
[00:07:42.840 --> 00:07:44.240]   and training them again.
[00:07:44.240 --> 00:07:45.600]   That does work quite well.
[00:07:45.600 --> 00:07:49.640]   But the question is, what purpose was that sub-network serving within the context of
[00:07:49.640 --> 00:07:51.220]   the dense network?
[00:07:51.220 --> 00:07:52.320]   That sub-network is good.
[00:07:52.320 --> 00:07:55.360]   It's able to learn on its own, but that doesn't necessarily mean it was useful in any way
[00:07:55.360 --> 00:07:56.960]   for the dense network.
[00:07:56.960 --> 00:07:59.560]   It's entirely possible that there are two completely different dynamics going on when
[00:07:59.560 --> 00:08:02.280]   you have the whole network versus the sub-network.
[00:08:02.280 --> 00:08:04.360]   And I can't say for certain.
[00:08:04.360 --> 00:08:08.000]   That gets into tricky empirical scientific questions that we don't really have an experiment
[00:08:08.000 --> 00:08:09.560]   for right now.
[00:08:09.560 --> 00:08:16.520]   But you observed that the performance of the sub-network is similar to the entire network,
[00:08:16.520 --> 00:08:17.520]   right?
[00:08:17.520 --> 00:08:18.520]   The dense network.
[00:08:18.520 --> 00:08:19.520]   Definitely.
[00:08:19.520 --> 00:08:20.520]   Definitely.
[00:08:20.520 --> 00:08:21.520]   I'm not quite sure what you're saying there.
[00:08:21.520 --> 00:08:27.560]   I mean, the sub-network seems like it's responsible for all the performance then of the entire
[00:08:27.560 --> 00:08:28.560]   network, right?
[00:08:28.560 --> 00:08:31.120]   I think it's a necessity versus sufficient distinction.
[00:08:31.120 --> 00:08:34.720]   The sub-network is certainly sufficient to get good performance, but it's unclear whether
[00:08:34.720 --> 00:08:36.000]   it's actually necessary.
[00:08:36.000 --> 00:08:39.320]   And one way to actually test this is to take that sub-network and try...
[00:08:39.320 --> 00:08:40.320]   I'll pose you.
[00:08:40.320 --> 00:08:41.480]   I love these thought experiments.
[00:08:41.480 --> 00:08:45.760]   Take that sub-network and instead of keeping only the sub-network, actually delete only
[00:08:45.760 --> 00:08:47.760]   the sub-network and keep all the other weights.
[00:08:47.760 --> 00:08:50.880]   So you've got a sub-network that's like one-tenth of the size of your original network and you've
[00:08:50.880 --> 00:08:51.880]   just wiped it out.
[00:08:51.880 --> 00:08:54.560]   What do you think is going to happen to that dense network when you try to train it, except
[00:08:54.560 --> 00:08:56.760]   missing with that hole in the middle where the sub-network should be?
[00:08:56.760 --> 00:08:57.760]   Is it going to do really badly?
[00:08:57.760 --> 00:09:01.520]   I mean, I guess it's an empirical question, but I would sort of imagine that it would
[00:09:01.520 --> 00:09:04.400]   find another 10% to lean on.
[00:09:04.400 --> 00:09:05.920]   Is that right?
[00:09:05.920 --> 00:09:06.920]   That's exactly right.
[00:09:06.920 --> 00:09:13.440]   And so the claim that it's necessarily leaning on that 10%, I think it's something that we
[00:09:13.440 --> 00:09:16.200]   can conjecture about, but it's something we can't say for certain because we don't have
[00:09:16.200 --> 00:09:17.320]   evidence to back it up.
[00:09:17.320 --> 00:09:20.860]   Because if we were to delete that, it'll lean on a different 10% if the leaning is even
[00:09:20.860 --> 00:09:21.860]   happening.
[00:09:21.860 --> 00:09:24.520]   We could make that claim, but we need some hard evidence to show that it's even leaning
[00:09:24.520 --> 00:09:26.000]   on that 10% to begin with.
[00:09:26.000 --> 00:09:29.720]   That 10% happened to have the highest magnitude weights at the end of training, but even magnitudes
[00:09:29.720 --> 00:09:33.400]   of weights doesn't necessarily confer importance to that.
[00:09:33.400 --> 00:09:36.320]   It's hard to say which weights are actually important and which weights aren't for the
[00:09:36.320 --> 00:09:37.320]   function.
[00:09:37.320 --> 00:09:40.680]   And using magnitude as a heuristic is a very bad one.
[00:09:40.680 --> 00:09:42.840]   At least there are all sorts of fancier ones in the literature.
[00:09:42.840 --> 00:09:46.340]   They don't tend to work that much better, but people would argue that magnitudes are
[00:09:46.340 --> 00:09:47.340]   a very naive thing to do.
[00:09:47.340 --> 00:09:48.680]   Yeah, I guess that makes sense.
[00:09:48.680 --> 00:09:53.080]   Do you think that high rates of dropout cause more of the network to get used?
[00:09:53.080 --> 00:09:59.060]   I would imagine that if there's a lot of dropout happening, it might force the network to use
[00:09:59.060 --> 00:10:03.540]   more of the weights available, at least to have a redundant mechanism now.
[00:10:03.540 --> 00:10:04.540]   Maybe.
[00:10:04.540 --> 00:10:06.020]   There are a couple of complications there.
[00:10:06.020 --> 00:10:09.420]   One is that dropout typically works in the neurons rather than the weights.
[00:10:09.420 --> 00:10:12.820]   And so it may end up having a very different effect potentially.
[00:10:12.820 --> 00:10:16.340]   There does tend to be a huge difference between pruning weights and pruning neurons in terms
[00:10:16.340 --> 00:10:18.820]   of how well you do and how much of the network you can prune.
[00:10:18.820 --> 00:10:23.160]   The network seems to like having extra representation capacity, that is extra neurons, but each
[00:10:23.160 --> 00:10:25.300]   neuron seems to not use that many different inputs.
[00:10:25.300 --> 00:10:30.260]   Hence why you can prune individual weights much more easily than pruning entire neurons,
[00:10:30.260 --> 00:10:33.100]   even if you're pruning in effect the same number of weights.
[00:10:33.100 --> 00:10:38.680]   So the other piece here is that we have this intuition for what dropout might be doing.
[00:10:38.680 --> 00:10:41.100]   We don't necessarily have evidence to back up that that's what's happening.
[00:10:41.100 --> 00:10:45.180]   The original dropout paper makes all sorts of claims that I would consider pretty outlandish
[00:10:45.180 --> 00:10:47.980]   and unsupported by any empirical evidence.
[00:10:47.980 --> 00:10:50.740]   And I like to only say what I have evidence to show.
[00:10:50.740 --> 00:10:54.580]   So it's hard to say that there's, again, necessarily a relationship there unless we
[00:10:54.580 --> 00:10:59.020]   can come up with an experiment to test whether dropout is somehow making the network more
[00:10:59.020 --> 00:11:01.060]   robust to pruning or something along those lines.
[00:11:01.060 --> 00:11:04.020]   Well, that does seem like an empirical question, right?
[00:11:04.020 --> 00:11:07.540]   If dropout makes to pruning.
[00:11:07.540 --> 00:11:09.660]   Is there sort of an inflection point in the pruning?
[00:11:09.660 --> 00:11:14.260]   Like do you have a sense of like, hey, you can prune up to X percent before there's problems
[00:11:14.260 --> 00:11:18.340]   that sort of generally holds across networks or across ranges of data or anything like
[00:11:18.340 --> 00:11:19.340]   that?
[00:11:19.340 --> 00:11:21.500]   Not that holds in general, unfortunately.
[00:11:21.500 --> 00:11:24.780]   And one nice way to test this is actually even for the same network in the same training
[00:11:24.780 --> 00:11:29.540]   regime, you can play with the difficulty of the data in ways that make it harder to prune
[00:11:29.540 --> 00:11:30.540]   or easier to prune.
[00:11:30.540 --> 00:11:33.940]   So, you know, as an example, training a network on just a standard image task, you can prune
[00:11:33.940 --> 00:11:34.940]   some percent of the network.
[00:11:34.940 --> 00:11:38.780]   And, you know, let's say for a ResNet 20 on CIFAR 10, something that anyone listening
[00:11:38.780 --> 00:11:41.860]   to this can probably train in a few minutes on a GPU at this point.
[00:11:41.860 --> 00:11:45.460]   That's about 90 percent of the network that can be pruned or 85 percent, somewhere in
[00:11:45.460 --> 00:11:47.940]   there before accuracy completely starts to drop off.
[00:11:47.940 --> 00:11:50.780]   If you prune all the weights, obviously things don't go very well and there's some inflection
[00:11:50.780 --> 00:11:51.780]   point there.
[00:11:51.780 --> 00:11:55.500]   If you were to try doing this on an adversarially robust training task, which demands more of
[00:11:55.500 --> 00:11:59.940]   the network and is a little bit more capacity intensive, one would imagine, you aren't able
[00:11:59.940 --> 00:12:02.780]   to prune as much typically before accuracy starts to drop off.
[00:12:02.780 --> 00:12:06.580]   You know, anything, the task, the way that you optimize the network, the final performance,
[00:12:06.580 --> 00:12:09.060]   they can all affect your ability to prune and how much you can prune.
[00:12:09.060 --> 00:12:10.820]   I wish there were a nice general rule of thumb.
[00:12:10.820 --> 00:12:15.780]   The answer is usually somewhere between 2x and 10x, you know, compression via pruning.
[00:12:15.780 --> 00:12:20.260]   Although, you know, in some crazy cases, if people set it up right, you can prune 100x
[00:12:20.260 --> 00:12:22.120]   or prune down to 100x smaller.
[00:12:22.120 --> 00:12:25.060]   Usually people set that up to make their pruning methods look really good in the literature,
[00:12:25.060 --> 00:12:27.540]   even if, you know, at the end of the day, those are toy examples that are just meant
[00:12:27.540 --> 00:12:31.340]   to get gaudy looking numbers as opposed to, you know, really being scientific.
[00:12:31.340 --> 00:12:33.660]   Is it even consistent across training runs?
[00:12:33.660 --> 00:12:38.260]   Like, do the pruning performance stay the same on the same dataset with just, say, different
[00:12:38.260 --> 00:12:39.520]   random initializations?
[00:12:39.520 --> 00:12:42.940]   It does tend to be pretty consistent across random initializations and random seeds.
[00:12:42.940 --> 00:12:46.480]   But then again, we've chosen, you know, in some sense, we've evolved the way that we
[00:12:46.480 --> 00:12:49.740]   train these networks to be consistent across initializations and seeds.
[00:12:49.740 --> 00:12:53.460]   You know, we've spent 20 or 30 years trying to do exactly that.
[00:12:53.460 --> 00:12:56.460]   And you know, to the point I mentioned earlier about how these sparse networks are very picky
[00:12:56.460 --> 00:13:00.140]   about their initializations, I imagine that if we had made it the goal 30 years ago to
[00:13:00.140 --> 00:13:03.800]   have networks whose subnetworks aren't picky about initialization, we might have a completely
[00:13:03.800 --> 00:13:06.460]   different architecture and completely different optimizers.
[00:13:06.460 --> 00:13:10.240]   So you know, we have to remember that, you know, 30 years of grad student descent has
[00:13:10.240 --> 00:13:13.920]   landed us on these particular networks with good properties that, you know, in this case
[00:13:13.920 --> 00:13:14.920]   we're exploding.
[00:13:14.920 --> 00:13:18.800]   And so I guess there's, you know, there's tons of different properties of a network
[00:13:18.800 --> 00:13:21.200]   that you could examine.
[00:13:21.200 --> 00:13:24.800]   Like, is there like a practical application of pruning that gets you excited about it?
[00:13:24.800 --> 00:13:28.440]   Or what even caused you to look into this?
[00:13:28.440 --> 00:13:31.400]   I like it, honestly, for the scientific application.
[00:13:31.400 --> 00:13:34.720]   I'm really excited about the idea that we can understand how these neural networks learn
[00:13:34.720 --> 00:13:36.520]   more effectively.
[00:13:36.520 --> 00:13:41.160]   You know, right now, or at least when I started doing my research back in 2018, one thing
[00:13:41.160 --> 00:13:44.160]   that really struck me was just how utterly unscientific the literature is.
[00:13:44.160 --> 00:13:48.560]   It's just littered with all these claims about flat minima and about the noise of stochastic
[00:13:48.560 --> 00:13:53.160]   gradient descent, about what dropout does, about internal covariate shift, most famously
[00:13:53.160 --> 00:13:56.320]   with batch norm, just, you know, a term that they completely made up in the paper and never
[00:13:56.320 --> 00:14:00.920]   actually tested to see whether the effect was real before they proposed their, you know,
[00:14:00.920 --> 00:14:02.400]   supposed remedy.
[00:14:02.400 --> 00:14:05.480]   That was just how the science was back at that time.
[00:14:05.480 --> 00:14:10.780]   And I feel like I sound old when I say this, the science has gotten a lot better, but now
[00:14:10.780 --> 00:14:13.720]   those sorts of claims don't generally get into the literature without some evidence
[00:14:13.720 --> 00:14:15.160]   supporting them.
[00:14:15.160 --> 00:14:19.560]   And I like to hope that, you know, the lottery ticket work was part of that trend, that,
[00:14:19.560 --> 00:14:22.720]   you know, we do want to get a better scientific empirical understanding.
[00:14:22.720 --> 00:14:26.040]   And it's not enough just to, you know, say things and not try to support them with facts
[00:14:26.040 --> 00:14:31.120]   the way that a lot of the older so-called great papers, you know, from around 2014 to
[00:14:31.120 --> 00:14:32.120]   2019 do.
[00:14:32.120 --> 00:14:35.840]   But I mean, the other piece was, obviously, I was very jealous of the labs at MIT that
[00:14:35.840 --> 00:14:36.840]   had GPUs.
[00:14:36.840 --> 00:14:38.440]   My lab did not.
[00:14:38.440 --> 00:14:40.000]   And I thought, you know, that's not fair.
[00:14:40.000 --> 00:14:41.000]   Can't we make this more efficient?
[00:14:41.000 --> 00:14:42.400]   Can't we get rid of some of those weights?
[00:14:42.400 --> 00:14:44.960]   Won't that reduce the cost of training?
[00:14:44.960 --> 00:14:50.240]   Unfortunately, you know, doing unstructured sparse pruning is generally, it's very difficult
[00:14:50.240 --> 00:14:52.920]   to accelerate that because, you know, it's an irregular pattern.
[00:14:52.920 --> 00:14:54.840]   The hardware is not designed for it.
[00:14:54.840 --> 00:14:58.480]   There are certain specialized chips that can do unstructured sparsity pretty well, but,
[00:14:58.480 --> 00:15:01.840]   you know, they're not widely accessible and the sparsity isn't generally applicable right
[00:15:01.840 --> 00:15:02.840]   now.
[00:15:02.840 --> 00:15:05.880]   You know, for those listening who are working on those chips, feel free to let me know if
[00:15:05.880 --> 00:15:08.960]   I'm wrong, but that's certainly been my experience so far.
[00:15:08.960 --> 00:15:12.000]   So you know, I would say this was a bit of a swing and a miss on that front.
[00:15:12.000 --> 00:15:14.240]   It was certainly effective on the scientific front.
[00:15:14.240 --> 00:15:17.160]   We've got all sorts of cool ideas that have come out of the lottery ticket work.
[00:15:17.160 --> 00:15:20.920]   But I think for me, Mosaic is really kind of, you know, the second at bat in some sense.
[00:15:20.920 --> 00:15:24.320]   It's an attempt to ask the same question, you know, how do these networks really learn
[00:15:24.320 --> 00:15:28.140]   empirically and is everything we're doing necessary or are recipes actually good?
[00:15:28.140 --> 00:15:29.900]   Are there better ways to train them out there?
[00:15:29.900 --> 00:15:32.940]   This time without the sparsity, which is hard to take advantage of, and instead with an
[00:15:32.940 --> 00:15:37.360]   eye toward anything that will actually speed things up and actually produce cost savings,
[00:15:37.360 --> 00:15:39.280]   you know, immediately today on real hardware.
[00:15:39.280 --> 00:15:44.040]   And so I guess, you know, it's funny, I was thinking maybe this is a good segue into Mosaic,
[00:15:44.040 --> 00:15:49.240]   but like when I think about, you know, transformers and attention, that is another case like Dropout
[00:15:49.240 --> 00:15:55.360]   where we have these sort of like evocative words like attention that one wonders, you
[00:15:55.360 --> 00:16:00.680]   know, how real the sort of hand wavy explanations are, but we still, I think, you know, kind
[00:16:00.680 --> 00:16:01.680]   of generally use them.
[00:16:01.680 --> 00:16:06.160]   I'm curious if you have thoughts on how much transformers is sort of just a product of
[00:16:06.160 --> 00:16:09.520]   somebody doing something that kind of worked well and everyone's sort of copying the details
[00:16:09.520 --> 00:16:12.640]   of it or, you know, some kind of fundamental insight.
[00:16:12.640 --> 00:16:18.040]   Like, do you think if you ran back history a hundred times, you would get transformers?
[00:16:18.040 --> 00:16:21.600]   Like what parts of transformers do you think you would get in every case?
[00:16:21.600 --> 00:16:26.160]   And what's just sort of the one off of like the sort of random path that we took to this
[00:16:26.160 --> 00:16:27.160]   architecture?
[00:16:27.160 --> 00:16:28.160]   That's a great question.
[00:16:28.160 --> 00:16:30.600]   And it's hard to say.
[00:16:30.600 --> 00:16:34.320]   I do think there is something pretty fundamental to what we call self-attention.
[00:16:34.320 --> 00:16:37.240]   I don't know what it is that's so fundamental to it.
[00:16:37.240 --> 00:16:39.440]   That's very tough to say.
[00:16:39.440 --> 00:16:40.920]   It does seem to work quite well.
[00:16:40.920 --> 00:16:44.720]   And we've had plenty of attempts to replace it that have, you know, had varying degrees
[00:16:44.720 --> 00:16:46.840]   of success, but still nothing has supplanted it.
[00:16:46.840 --> 00:16:50.680]   And given that the attention is so effective and also so cheap relative to the massive
[00:16:50.680 --> 00:16:55.000]   feed-forward layers we use in, you know, our giant models, you know, the really, you
[00:16:55.000 --> 00:16:57.880]   know, 10 billion plus parameter models today.
[00:16:57.880 --> 00:16:59.480]   There's no reason not to use it if it's effective.
[00:16:59.480 --> 00:17:03.360]   It's not really asking to be replaced in some sense, the way the batch norm is asking to
[00:17:03.360 --> 00:17:04.360]   be replaced.
[00:17:04.360 --> 00:17:06.440]   If anyone has a batch norm replacement, please let me know.
[00:17:06.440 --> 00:17:08.280]   I want to get rid of it very badly.
[00:17:08.280 --> 00:17:10.200]   I think it's such a simple architecture.
[00:17:10.200 --> 00:17:11.520]   I think we would have arrived there eventually.
[00:17:11.520 --> 00:17:15.640]   Like at the end of the day, the self-attention is really the most powerful new component.
[00:17:15.640 --> 00:17:17.440]   Otherwise it's just a feed-forward network.
[00:17:17.440 --> 00:17:20.040]   And the self-attention was already kind of bouncing around the literature in various
[00:17:20.040 --> 00:17:24.200]   ways and the folks who wrote the paper really put the pieces together exceedingly nicely.
[00:17:24.200 --> 00:17:26.960]   You know, these, these good inductive biases are hard to come by.
[00:17:26.960 --> 00:17:32.680]   I have a bet going with Sasha Rush right now that transformers will still be the go-to
[00:17:32.680 --> 00:17:36.240]   way to train NLP models in another five years or so.
[00:17:36.240 --> 00:17:41.000]   And that bet is placed because convolutions have lasted a really long time in vision.
[00:17:41.000 --> 00:17:45.000]   The vision transformer is still something that I almost never get asked for at Mosaic.
[00:17:45.000 --> 00:17:46.480]   It's an academic curiosity by and large.
[00:17:46.480 --> 00:17:48.920]   The ResNet is the real workhorse still.
[00:17:48.920 --> 00:17:54.080]   So, you know, convolutional networks are, they've stood the test of time.
[00:17:54.080 --> 00:17:55.120]   Recurrent networks stood the test of time.
[00:17:55.120 --> 00:17:56.760]   I think transformers will as well.
[00:17:56.760 --> 00:18:00.080]   These little inductive bias insights are pretty hard to come by.
[00:18:00.080 --> 00:18:03.500]   But when you take a step back, they're relatively simple tricks at the end of the day.
[00:18:03.500 --> 00:18:08.360]   And I guess what, what then matters for, for speeding up training at this point and kind
[00:18:08.360 --> 00:18:11.880]   of what are the things you're working on at, at, at Mosaic around that?
[00:18:11.880 --> 00:18:13.840]   Everything matters.
[00:18:13.840 --> 00:18:18.080]   I wish I could tell you that when we get a 7x speed up on a model, like we did with ResNet
[00:18:18.080 --> 00:18:22.840]   50 on ImageNet or a 3x or 4x speed up on BERT pre-training, which will announce probably
[00:18:22.840 --> 00:18:26.520]   by the time this podcast is out, it will be announced.
[00:18:26.520 --> 00:18:29.760]   Or you know, the speed ups we'll have coming on GPT-3 that I don't know yet, but I'm sure
[00:18:29.760 --> 00:18:32.760]   will be out by the time this podcast is out.
[00:18:32.760 --> 00:18:36.540]   I wish I could tell you here's the one neat trick you need to do to get that speed up.
[00:18:36.540 --> 00:18:41.400]   The answer is the ResNet 50 recipe, that 7x is 17 different interventions into training,
[00:18:41.400 --> 00:18:45.920]   affecting everything from data augmentation, the inductive bias of the network, the optimizer,
[00:18:45.920 --> 00:18:48.520]   the regularization, the loss function.
[00:18:48.520 --> 00:18:51.480]   It's basically anything and everything that there is in the network, even shaping how
[00:18:51.480 --> 00:18:54.320]   things go over the course of training.
[00:18:54.320 --> 00:18:58.360]   I wish it were one thing, but you know, as with all good systems optimization, it's 5%
[00:18:58.360 --> 00:18:59.360]   here, 5% there.
[00:18:59.360 --> 00:19:03.280]   And once you stack enough of that up, you get to something really impressive sounding
[00:19:03.280 --> 00:19:04.280]   like 7x.
[00:19:04.280 --> 00:19:09.480]   But I guess the challenge, you know, with like maybe all neural net research, right,
[00:19:09.480 --> 00:19:14.960]   is like each experiment is kind of expensive and these things don't typically, in my experience,
[00:19:14.960 --> 00:19:17.920]   sort of add up linearly.
[00:19:17.920 --> 00:19:22.140]   How do you even kind of know what's contributing to your speed ups?
[00:19:22.140 --> 00:19:24.120]   This is why we have a research team.
[00:19:24.120 --> 00:19:28.960]   This is the hard part of our jobs, is trying to piece together what may work together with
[00:19:28.960 --> 00:19:29.960]   what.
[00:19:29.960 --> 00:19:32.840]   You know, people often ask me, is the secret sauce of Mosaic speed up methods?
[00:19:32.840 --> 00:19:33.840]   And the answer is no.
[00:19:33.840 --> 00:19:36.640]   You know, we put that out open source for free.
[00:19:36.640 --> 00:19:40.200]   The secret sauce of Mosaic is the research team that has developed the methodologies
[00:19:40.200 --> 00:19:42.600]   and the intuitions and the ways of thinking about the problem.
[00:19:42.600 --> 00:19:45.160]   It's an emerging science, this kind of science of composition.
[00:19:45.160 --> 00:19:48.320]   It doesn't necessarily, we don't necessarily have a good recipe.
[00:19:48.320 --> 00:19:50.880]   I wish there were some automated system that would do it for us so I could tell all the
[00:19:50.880 --> 00:19:52.800]   researchers to go to the beach.
[00:19:52.800 --> 00:19:55.880]   But a lot of it really comes down to some principles we're developing, like the early
[00:19:55.880 --> 00:19:59.120]   part of training, nothing that important or interesting tends to get learned.
[00:19:59.120 --> 00:20:03.580]   So, you know, we can generally get away with, say, decreasing the resolution of images or
[00:20:03.580 --> 00:20:08.600]   truncating sentences or, you know, playing with the masking ratio for a BERT or something
[00:20:08.600 --> 00:20:09.840]   like that.
[00:20:09.840 --> 00:20:13.000]   Principles like that you really only have a certain budget of regularization for a given
[00:20:13.000 --> 00:20:14.120]   training run length.
[00:20:14.120 --> 00:20:17.280]   And so you need to use that wisely on things that won't slow down training.
[00:20:17.280 --> 00:20:20.600]   And there are regularization methods that are, you know, no effect and some that are
[00:20:20.600 --> 00:20:22.160]   actually pretty meaningful slowdowns.
[00:20:22.160 --> 00:20:24.000]   And you have to choose wisely on that front.
[00:20:24.000 --> 00:20:27.920]   Some ideas around balancing which parts of training you, you know, if you make back prop
[00:20:27.920 --> 00:20:30.400]   faster, you've got to make forward prop faster as well.
[00:20:30.400 --> 00:20:34.080]   Otherwise you start hitting diminishing returns on anything else that makes back prop faster.
[00:20:34.080 --> 00:20:36.120]   But there is a lot of art to this as well.
[00:20:36.120 --> 00:20:40.760]   How do you get it such that it's good enough for this model, but not so overly specific
[00:20:40.760 --> 00:20:44.520]   to one dataset that it won't work if somebody comes and has a new dataset they want to try
[00:20:44.520 --> 00:20:45.520]   on this model.
[00:20:45.520 --> 00:20:48.840]   There, there's some kind of balance between, you know, how specific and fast the recipe
[00:20:48.840 --> 00:20:51.760]   is and how general and perhaps slower the recipe is.
[00:20:51.760 --> 00:20:54.520]   And again, these are all kind of, you know, in some sense, subjective trade-offs that
[00:20:54.520 --> 00:20:55.520]   we have to make.
[00:20:55.520 --> 00:20:56.920]   It is a little artisanal at this point.
[00:20:56.920 --> 00:20:58.600]   Do you think it'll stay artisanal?
[00:20:58.600 --> 00:21:00.880]   Does that seem like a stable place?
[00:21:00.880 --> 00:21:03.520]   I think it'll stay pretty artisanal.
[00:21:03.520 --> 00:21:04.880]   In some ways that's good for business.
[00:21:04.880 --> 00:21:08.840]   Like if it's not artisanal enough, my research team needs to find other stuff to work on,
[00:21:08.840 --> 00:21:12.880]   but it is artisanal in so far as every model is different.
[00:21:12.880 --> 00:21:13.880]   The way that we train it is different.
[00:21:13.880 --> 00:21:16.960]   And each of these interventions is different and has a weird effect.
[00:21:16.960 --> 00:21:21.000]   And you, and the way I think of lottery ticket as being just one of these interventions among
[00:21:21.000 --> 00:21:23.360]   dozens that we've tried at Mosaic.
[00:21:23.360 --> 00:21:26.440]   And so the way that I've spent five years getting to know lottery ticket and all the
[00:21:26.440 --> 00:21:30.080]   ins and outs of how sparse networks behave, we kind of have to get to know each of these
[00:21:30.080 --> 00:21:33.080]   interventions and how it behaves and what effects it has.
[00:21:33.080 --> 00:21:34.800]   And that's a long journey.
[00:21:34.800 --> 00:21:38.880]   And then seeing that applied to a new model, sometimes our ideas translate over and sometimes
[00:21:38.880 --> 00:21:39.880]   they don't.
[00:21:39.880 --> 00:21:44.600]   And understanding why or why not, that's new knowledge we can use to build on and try to
[00:21:44.600 --> 00:21:45.800]   understand these methods better.
[00:21:45.800 --> 00:21:48.000]   But they do almost feel like friends in a lot of ways.
[00:21:48.000 --> 00:21:52.800]   And they have complicated personalities and understanding how they work together is, it's
[00:21:52.800 --> 00:21:57.160]   tricky, frustrating at times and makes our researchers want to pull their hair out.
[00:21:57.160 --> 00:22:02.640]   But I think it's really, there is some intuition and some high level rules of thumb that we
[00:22:02.640 --> 00:22:05.800]   start to use, but I don't think we'll be automating this anytime soon.
[00:22:05.800 --> 00:22:09.840]   It's like network architecture search, or hyperparameter search, but even more difficult
[00:22:09.840 --> 00:22:13.320]   because now we're adding different changes to training whose effects are difficult to
[00:22:13.320 --> 00:22:16.040]   predict until you've really trained the model to the end.
[00:22:16.040 --> 00:22:19.560]   And so I guess before we go too far down this path, we should probably talk about Mosaic
[00:22:19.560 --> 00:22:20.560]   ML.
[00:22:20.560 --> 00:22:22.040]   I mean, what's the story behind it?
[00:22:22.040 --> 00:22:24.480]   What do you guys offer to the world?
[00:22:24.480 --> 00:22:28.040]   The way that, I'll try out a new way of describing it and you can let me know if this is any
[00:22:28.040 --> 00:22:29.040]   good.
[00:22:29.040 --> 00:22:30.040]   I kind of, I like to try these things.
[00:22:30.040 --> 00:22:31.040]   Love it.
[00:22:31.040 --> 00:22:32.040]   Yeah.
[00:22:32.040 --> 00:22:35.400]   So, you know, in the hardware world, we have these foundries.
[00:22:35.400 --> 00:22:39.600]   We have a company like TSMC, Taiwan Semiconductor Manufacturing Company.
[00:22:39.600 --> 00:22:43.560]   They're, you know, all sorts of geopolitical interesting right now because they are the
[00:22:43.560 --> 00:22:46.920]   best place to train, to get your semiconductors made.
[00:22:46.920 --> 00:22:51.200]   They have the most advanced process technology, the smallest transistors, which means, you
[00:22:51.200 --> 00:22:53.900]   know, the best power efficiency and the best performance of anyone.
[00:22:53.900 --> 00:22:55.640]   And there are a few other companies in the world that do this.
[00:22:55.640 --> 00:22:59.880]   There's Samsung, you know, there's Global Foundries, which used to be part of AMD.
[00:22:59.880 --> 00:23:01.920]   Intel has its own internal foundries.
[00:23:01.920 --> 00:23:07.680]   But at the end of the day, if you're Nvidia, AMD, Apple, anybody, you go to one of these
[00:23:07.680 --> 00:23:11.580]   foundries and you say, hey, there's a chip I'd like to get made.
[00:23:11.580 --> 00:23:15.760]   They give you, you know, TSMC gives you some high level APIs or some high level, you know,
[00:23:15.760 --> 00:23:18.040]   abstractions you can use to design your chip for them.
[00:23:18.040 --> 00:23:20.640]   Then you go and you hand it to them and say, print me the chip.
[00:23:20.640 --> 00:23:25.680]   And they're not experts in chip making, you know, you know, TSMC or in designing chips,
[00:23:25.680 --> 00:23:28.960]   TSMC doesn't make its own CPU.
[00:23:28.960 --> 00:23:32.480]   They're really, really good at taking your designs and bringing them to fruition.
[00:23:32.480 --> 00:23:34.000]   They have all the latest technology.
[00:23:34.000 --> 00:23:35.680]   They have the most efficient stuff.
[00:23:35.680 --> 00:23:37.800]   They know how to improve yields.
[00:23:37.800 --> 00:23:39.680]   And I think of us at Mosaic as being TSMC.
[00:23:39.680 --> 00:23:40.680]   Interesting.
[00:23:40.680 --> 00:23:47.360]   So can you describe like what, talk about one customer and what their specification
[00:23:47.360 --> 00:23:48.360]   looked like?
[00:23:48.360 --> 00:23:49.360]   Definitely.
[00:23:49.360 --> 00:23:51.320]   So we're not in the business, we're not making a GPT-3 clone.
[00:23:51.320 --> 00:23:53.360]   You know, we're not training our own language model for us.
[00:23:53.360 --> 00:23:56.940]   We're never going to put out an API and say, this is the Mosaic GPT, come here instead of
[00:23:56.940 --> 00:23:58.600]   OpenAI or something like that.
[00:23:58.600 --> 00:24:01.800]   We're happy to take all comers and say, come print your model.
[00:24:01.800 --> 00:24:03.960]   We have the latest technology for doing it efficiently.
[00:24:03.960 --> 00:24:06.760]   We know how to improve yield as much as possible.
[00:24:06.760 --> 00:24:09.800]   So we know how to kind of make these training runs go well the first time.
[00:24:09.800 --> 00:24:14.300]   I can stretch this metaphor quite far and we'll see how far it goes.
[00:24:14.300 --> 00:24:19.640]   But we've worked with several customers now who want to train large language models that
[00:24:19.640 --> 00:24:23.400]   have specific properties or where they have some specific data set they want to use, or
[00:24:23.400 --> 00:24:26.480]   for compliance reasons, they can't use GPT-3 internally.
[00:24:26.480 --> 00:24:29.480]   You can imagine lots of enterprise customers are pretty concerned about what might be in
[00:24:29.480 --> 00:24:32.240]   that model and the fact that the training data isn't public.
[00:24:32.240 --> 00:24:33.740]   And everybody has lots of data.
[00:24:33.740 --> 00:24:36.120]   That's been one thing that struck me.
[00:24:36.120 --> 00:24:40.400]   Companies don't realize they have lots of data, but everyone has tons of unlabeled data.
[00:24:40.400 --> 00:24:44.060]   Someone came to me and they were using some pre-trained models on ImageNet to do some
[00:24:44.060 --> 00:24:45.060]   vision stuff.
[00:24:45.060 --> 00:24:46.720]   And we asked, do you have any unlabeled data?
[00:24:46.720 --> 00:24:48.240]   And they were like, yeah, we've got a little.
[00:24:48.240 --> 00:24:49.240]   We said, how much?
[00:24:49.240 --> 00:24:50.240]   Nine petabytes.
[00:24:50.240 --> 00:24:54.080]   And I was like, and you're pre-training on ImageNet?
[00:24:54.080 --> 00:24:55.080]   Are you kidding me?
[00:24:55.440 --> 00:24:57.800]   Someone else, they were using the BERT pre-trained on Wikitext.
[00:24:57.800 --> 00:25:00.920]   And we said to them, do you have any unlabeled data in your domain?
[00:25:00.920 --> 00:25:01.920]   And they were like, yeah, a little.
[00:25:01.920 --> 00:25:02.920]   We're like, how much?
[00:25:02.920 --> 00:25:07.920]   300 billion tokens, enough to train an exceedingly large language model.
[00:25:07.920 --> 00:25:10.480]   It's like, why are you using Wikitext?
[00:25:10.480 --> 00:25:17.080]   So I think the data is out there and people want these large language models.
[00:25:17.080 --> 00:25:21.120]   Often they want them with some specific properties or with something tweaked about the model.
[00:25:21.120 --> 00:25:24.840]   That's what we're here for, to give you a great process technology and the ability to
[00:25:24.840 --> 00:25:26.080]   customize to what you'd like.
[00:25:26.080 --> 00:25:28.200]   So I like this foundry metaphor quite a bit.
[00:25:28.200 --> 00:25:32.160]   And I think it distinguishes very well what we do and what we don't do.
[00:25:32.160 --> 00:25:34.800]   We're not here to put the chips in computers and run them.
[00:25:34.800 --> 00:25:36.800]   We're not here to do inference.
[00:25:36.800 --> 00:25:38.920]   We're not here to design your model for you.
[00:25:38.920 --> 00:25:40.520]   We're not a consulting company.
[00:25:40.520 --> 00:25:45.240]   We're here to help you build the best darn model you want for the lowest amount of money
[00:25:45.240 --> 00:25:47.920]   and to get something that works really efficiently.
[00:25:47.920 --> 00:25:53.040]   But I guess when you say that you won't build the model for me, it kind of sounds like you're
[00:25:53.040 --> 00:25:54.440]   building the model for me.
[00:25:54.440 --> 00:25:57.840]   I guess I should specify what building the model means.
[00:25:57.840 --> 00:26:02.280]   I'm not going to tell you which model you should use.
[00:26:02.280 --> 00:26:06.240]   I'm not going to tell you, you know, you should use a ResNet for this and you should use a
[00:26:06.240 --> 00:26:07.240]   GPT for that.
[00:26:07.240 --> 00:26:11.360]   I'm not going to tell you, here's how to set up your optimization problem on this.
[00:26:11.360 --> 00:26:14.160]   I'm not going to go through and help you curate your data or things like that.
[00:26:14.160 --> 00:26:17.640]   Really my focus is on you're ready to train.
[00:26:17.640 --> 00:26:21.480]   Let's train the best darn model we can and let's get it right the first time.
[00:26:21.480 --> 00:26:25.200]   So I'm not here to solve your machine learning problem or to set up your machine learning
[00:26:25.200 --> 00:26:26.200]   problem for you.
[00:26:26.200 --> 00:26:30.000]   I'm here to help you train the model that you'll eventually use in production once you
[00:26:30.000 --> 00:26:34.600]   figure out how you want to solve that problem, strategically how you want to go about it.
[00:26:34.600 --> 00:26:35.600]   Interesting.
[00:26:35.600 --> 00:26:40.000]   And I guess one challenge, putting myself in your shoes, might be that it's kind of
[00:26:40.000 --> 00:26:46.480]   hard to know, at least in my experience, a priori, how well training is going to go.
[00:26:46.480 --> 00:26:51.100]   How do you work that out with the prospective customer?
[00:26:51.100 --> 00:26:52.560]   It's tricky.
[00:26:52.560 --> 00:26:55.920]   I think we're learning how to hold people's hands through that process a little bit in
[00:26:55.920 --> 00:26:59.160]   the same way that I'm sure TSMC does not say, here are our tools.
[00:26:59.160 --> 00:27:02.040]   Let us know when you want to print a billion of these chips.
[00:27:02.040 --> 00:27:06.280]   You want to go through and you do sampling and you simulate the chips before you build
[00:27:06.280 --> 00:27:07.280]   it.
[00:27:07.280 --> 00:27:08.280]   And we have a similar process.
[00:27:08.280 --> 00:27:11.360]   Before you train that 100 billion parameter model, we should probably train a billion
[00:27:11.360 --> 00:27:14.440]   parameter one and make sure that things work end to end and you get a result that looks
[00:27:14.440 --> 00:27:15.680]   reasonable.
[00:27:15.680 --> 00:27:17.720]   Did you use the right tokenizer?
[00:27:17.720 --> 00:27:21.060]   Are you getting results that if we train a one billion, then a three billion, are the
[00:27:21.060 --> 00:27:23.520]   results getting better?
[00:27:23.520 --> 00:27:25.060]   Is your data quality high enough?
[00:27:25.060 --> 00:27:29.100]   Can we go back through and make sure that all the inputs are looking good?
[00:27:29.100 --> 00:27:32.260]   And we've seen everything that could possibly go wrong at this point.
[00:27:32.260 --> 00:27:34.700]   And there's so much that goes wrong when you train these models.
[00:27:34.700 --> 00:27:38.620]   I know the folks at Facebook were really kind and put out their logbook for all the stuff
[00:27:38.620 --> 00:27:43.140]   that happened when they were training the OPT model, the 175 billion one.
[00:27:43.140 --> 00:27:44.460]   Everything you can imagine went wrong.
[00:27:44.460 --> 00:27:46.560]   Hardware dies mid-training run.
[00:27:46.560 --> 00:27:47.560]   You get loss spikes.
[00:27:47.560 --> 00:27:50.340]   The resumption times are really long if you're not careful.
[00:27:50.340 --> 00:27:53.260]   These multi-terabyte checkpoints you have to load.
[00:27:53.260 --> 00:27:56.460]   Getting the data loader spun back to the point you are in training could take a very long
[00:27:56.460 --> 00:27:58.460]   time.
[00:27:58.460 --> 00:27:59.860]   And just some weird stuff happens.
[00:27:59.860 --> 00:28:05.660]   In some cases, they're just like, on a training run this long, you'll get memory errors.
[00:28:05.660 --> 00:28:10.340]   That one in a billion cosmic ray striking your memory will happen.
[00:28:10.340 --> 00:28:14.620]   You get things like, oops, I used a different tokenizer for training as I did for evaluation.
[00:28:14.620 --> 00:28:18.740]   And so all my evaluation results look really bad, even though my model is really great.
[00:28:18.740 --> 00:28:21.260]   So we try to get all that stuff ironed out at the small scale.
[00:28:21.260 --> 00:28:26.700]   And then we'll work with you to go bigger and bigger and bigger until we're ready to
[00:28:26.700 --> 00:28:30.260]   build the giant chip, as it were, and really actually train the model.
[00:28:30.260 --> 00:28:34.300]   And I guess in my experience, people typically don't just want to train a model one time.
[00:28:34.300 --> 00:28:37.140]   They want to continuously update it forever.
[00:28:37.140 --> 00:28:40.940]   Do you then take over that process for a customer?
[00:28:40.940 --> 00:28:42.460]   How does that work?
[00:28:42.460 --> 00:28:44.060]   We're certainly in the loop on that front.
[00:28:44.060 --> 00:28:49.900]   We have APIs and a Python software development kit where if you want to do data intake and
[00:28:49.900 --> 00:28:55.860]   just schedule retraining to take place, you can just do that on our platform.
[00:28:55.860 --> 00:28:58.660]   We've got the platform and it's very easy to program around it for simple automation
[00:28:58.660 --> 00:28:59.660]   like that.
[00:28:59.660 --> 00:29:00.660]   And we have tools to help you do that.
[00:29:00.660 --> 00:29:02.180]   And yeah, I think you're right.
[00:29:02.180 --> 00:29:04.500]   A lot of people say to us, aren't you going to have a bad business?
[00:29:04.500 --> 00:29:08.100]   Like, customer's going to come to you once, train that model, and then leave.
[00:29:08.100 --> 00:29:11.960]   And I think the metaphor our CEO, Naveen, likes to use is, if you're building a piece
[00:29:11.960 --> 00:29:15.540]   of software and you get to version 1.0, do you fire all your engineers and say, "Oh,
[00:29:15.540 --> 00:29:16.540]   I'm done.
[00:29:16.540 --> 00:29:17.540]   Software's done."
[00:29:17.540 --> 00:29:19.300]   No, of course, you've got more features you want to build.
[00:29:19.300 --> 00:29:20.300]   You've got things you want to update.
[00:29:20.300 --> 00:29:21.820]   Nothing is ever really done.
[00:29:21.820 --> 00:29:23.260]   And I think it's good for our business.
[00:29:23.260 --> 00:29:26.820]   But also, once you've done that big training run, we're developing ways to make sure that
[00:29:26.820 --> 00:29:31.220]   your second training run is much cheaper based on taking advantage of aspects of your big
[00:29:31.220 --> 00:29:32.220]   training run.
[00:29:32.220 --> 00:29:36.340]   And that's a place where we're investing pretty deeply in the technology so that each incremental
[00:29:36.340 --> 00:29:38.460]   run should be cheaper and cheaper than the last one.
[00:29:38.460 --> 00:29:40.580]   I always think of it like a frequent flyer program.
[00:29:40.580 --> 00:29:43.220]   The more you train, the more you save in some sense.
[00:29:43.220 --> 00:29:46.660]   But there's a lot of really interesting science behind how to do that without having your
[00:29:46.660 --> 00:29:50.440]   first model determine how all of your other models are going to go, because your data
[00:29:50.440 --> 00:29:52.940]   may change a lot.
[00:29:52.940 --> 00:29:56.780]   And I guess, how do you think about engaging with the research community?
[00:29:56.780 --> 00:30:01.140]   I mean, obviously, you're still publishing papers, but you also talked about your secret
[00:30:01.140 --> 00:30:02.140]   sauce.
[00:30:02.140 --> 00:30:07.660]   Is there a bright line in your mind about what you publish and what you keep to yourself?
[00:30:07.660 --> 00:30:08.660]   Definitely.
[00:30:08.740 --> 00:30:11.140]   I mean, the first thing I'll say is we don't publish.
[00:30:11.140 --> 00:30:13.220]   That is one line I did draw for the team early on.
[00:30:13.220 --> 00:30:14.220]   We're not Google-brained.
[00:30:14.220 --> 00:30:16.380]   We're not here to be an open-ended research team.
[00:30:16.380 --> 00:30:17.940]   We have a job to do and customers to serve.
[00:30:17.940 --> 00:30:19.740]   We do science in service of that.
[00:30:19.740 --> 00:30:23.700]   But for anyone here who's looking for an interesting job, don't come here if you want to write
[00:30:23.700 --> 00:30:25.220]   papers or do open-ended research.
[00:30:25.220 --> 00:30:26.420]   That's not what we do.
[00:30:26.420 --> 00:30:29.740]   We do like to share our results, and we'll talk about everything.
[00:30:29.740 --> 00:30:32.000]   We have to talk about our speed-up methods.
[00:30:32.000 --> 00:30:34.660]   If we don't talk about them, imagine if you came to me and said, "Hey, I want to train
[00:30:34.660 --> 00:30:35.660]   this model."
[00:30:35.660 --> 00:30:37.460]   And I said to you, "Well, we're not going to train that model.
[00:30:37.460 --> 00:30:40.060]   We'll train something slightly different, but I won't tell you what.
[00:30:40.060 --> 00:30:41.060]   That's a secret."
[00:30:41.060 --> 00:30:42.180]   You wouldn't really trust me.
[00:30:42.180 --> 00:30:44.660]   We do have to be open about that algorithmically.
[00:30:44.660 --> 00:30:47.500]   So the secret sauce is really, I think, a couple things.
[00:30:47.500 --> 00:30:50.980]   One is the expertise we built as a team to be able to really attack these models and
[00:30:50.980 --> 00:30:51.980]   speed them up.
[00:30:51.980 --> 00:30:54.980]   The secret sauce is, in some sense, experience and wisdom and kind of the culture and the
[00:30:54.980 --> 00:30:57.700]   scientific practices we have on the team.
[00:30:57.700 --> 00:31:02.420]   The way that we make money is that we put all our speed-ups out there, but our cloud
[00:31:02.420 --> 00:31:05.860]   platform, our orchestration software, the tools that make it really easy to train these
[00:31:05.860 --> 00:31:10.420]   giant models, the managed version of this, that you have to pay for.
[00:31:10.420 --> 00:31:13.540]   And that's really, when you're training a large language model, good luck doing it without
[00:31:13.540 --> 00:31:14.540]   this.
[00:31:14.540 --> 00:31:17.540]   You're going to have to stay up 24/7 and watch the loss for the spike and then figure out
[00:31:17.540 --> 00:31:18.540]   how to roll back and restart.
[00:31:18.540 --> 00:31:21.460]   And a lot of those tools are part of our paid offering.
[00:31:21.460 --> 00:31:24.340]   So you do publish then your algorithms.
[00:31:24.340 --> 00:31:26.460]   Am I understanding that right?
[00:31:26.460 --> 00:31:27.460]   Oh, sorry.
[00:31:27.460 --> 00:31:30.140]   Let me clarify the word "publish" here, because I think we're using it differently.
[00:31:30.140 --> 00:31:33.540]   We don't submit papers to conferences for publication in that way, but we certainly
[00:31:33.540 --> 00:31:37.860]   do share openly what our algorithms are, what our recipes are, and that's all available
[00:31:37.860 --> 00:31:40.660]   in blog posts and in our open source composer library.
[00:31:40.660 --> 00:31:43.420]   So that is freely available for anyone to see and use.
[00:31:43.420 --> 00:31:47.380]   But I guess publish in the academic sense, honestly, it just takes too long.
[00:31:47.380 --> 00:31:50.740]   And we can disseminate our results without having to go through peer review and all that
[00:31:50.740 --> 00:31:51.740]   good stuff.
[00:31:51.740 --> 00:31:52.740]   I see.
[00:31:52.740 --> 00:31:53.740]   I see.
[00:31:53.740 --> 00:31:58.500]   Another topic I wanted to make sure I hit with you is, it seems like you're a bit of
[00:31:58.500 --> 00:32:04.260]   a skeptic of this current approach leading to AGI.
[00:32:04.260 --> 00:32:07.340]   And you seem maybe quite sure about that point of view.
[00:32:07.340 --> 00:32:12.460]   I wonder if you wanted to say more how you came to that or if that's a fair characterization
[00:32:12.460 --> 00:32:15.020]   of your perspective.
[00:32:15.020 --> 00:32:20.420]   I think it's a very fair characterization of my perspective.
[00:32:20.420 --> 00:32:23.460]   First of all, getting a good definition for AGI is pretty tough.
[00:32:23.460 --> 00:32:25.860]   Either it's everything or nothing.
[00:32:25.860 --> 00:32:28.340]   It's something pie in the sky that we'll never really reach.
[00:32:28.340 --> 00:32:30.740]   It's through human intelligence.
[00:32:30.740 --> 00:32:35.060]   In which case, trying to get that out of a feedforward neural network seems like a -- I
[00:32:35.060 --> 00:32:40.420]   think I've heard the metaphor building the ladder to the moon a lot lately.
[00:32:40.420 --> 00:32:41.420]   That's not how we're going to get there.
[00:32:41.420 --> 00:32:43.500]   It's going to take something fundamentally different.
[00:32:43.500 --> 00:32:50.300]   If AGI is kind of -- it's possible GPT-Chad is AGI if you have a very narrow definition
[00:32:50.300 --> 00:32:51.300]   of what AGI is.
[00:32:51.300 --> 00:32:52.880]   And I've heard some people arguing that.
[00:32:52.880 --> 00:32:56.580]   So if you want to go -- I think AGI is really an all or nothing term.
[00:32:56.580 --> 00:33:01.460]   And I'm more of the -- more people talk about it as the all sort of definition.
[00:33:01.460 --> 00:33:06.620]   This is really truly general human-like intelligence and the ability to learn and adapt in an environment.
[00:33:06.620 --> 00:33:10.940]   In which case, a feedforward neural network is not going to get us there.
[00:33:10.940 --> 00:33:13.660]   This is fundamentally not the right technology for that.
[00:33:13.660 --> 00:33:18.920]   I really think AGI is being used pretty cynically by a lot of people in the field as a way to
[00:33:18.920 --> 00:33:20.660]   get people to give them money.
[00:33:20.660 --> 00:33:23.140]   Either get people to give them money because they claim they're going to make something
[00:33:23.140 --> 00:33:26.740]   happen or get people to give them money because they claim they're going to study something
[00:33:26.740 --> 00:33:28.900]   catastrophic that would happen.
[00:33:28.900 --> 00:33:34.140]   But either way, I take it as a cynical kind of -- in pursuit of resources, in pursuit
[00:33:34.140 --> 00:33:37.540]   of power and money, not something that people mean very seriously.
[00:33:37.540 --> 00:33:41.420]   At least other than the extent to which they're misleading others.
[00:33:41.420 --> 00:33:43.700]   Interesting.
[00:33:43.700 --> 00:33:50.660]   What are some things that -- some sort of reasoning tasks maybe that you think a feedforward
[00:33:50.660 --> 00:33:54.900]   network surely wouldn't be able to do?
[00:33:54.900 --> 00:33:59.180]   I mean, the average feedforward network today, even GPT-Chad, is probably just looking at
[00:33:59.180 --> 00:34:01.900]   a very long context.
[00:34:01.900 --> 00:34:06.100]   And so, if that context is essentially our memory space and our state space, and the
[00:34:06.100 --> 00:34:10.380]   model is able to just write back to that context and reuse it for future token prediction,
[00:34:10.380 --> 00:34:14.660]   that's a pretty raw way of giving the model the ability to interact with itself and interact
[00:34:14.660 --> 00:34:16.380]   with an environment.
[00:34:16.380 --> 00:34:19.660]   So it's hard to point to a task.
[00:34:19.660 --> 00:34:21.240]   People like to quiz me on this.
[00:34:21.240 --> 00:34:23.700]   What is the SAT score at which you'd consider this to be AGI?
[00:34:23.700 --> 00:34:28.020]   I had someone really, really badger me about that when I gave a talk recently, when I expressed
[00:34:28.020 --> 00:34:29.020]   skepticism of AGI.
[00:34:29.020 --> 00:34:32.740]   So it's hard to pin on a task to say, "The model can't do this right now, and if it does
[00:34:32.740 --> 00:34:33.740]   that, that's AGI."
[00:34:33.740 --> 00:34:34.740]   Look at the Turing test.
[00:34:34.740 --> 00:34:37.860]   I mean, we've been passing the Turing test for 40 or 50 years.
[00:34:37.860 --> 00:34:42.380]   And that's been a pretty awful test of whether something like ELISA had AGI.
[00:34:42.380 --> 00:34:49.340]   So I don't like to point to one task and say, "This is a thing that something must do in
[00:34:49.340 --> 00:34:51.420]   order to be AGI."
[00:34:51.420 --> 00:34:56.580]   But I don't think the setup of a feed-forward network where we're just adding tokens to
[00:34:56.580 --> 00:35:00.620]   a context and hoping that it's able to take advantage of all these tokens isn't in any
[00:35:00.620 --> 00:35:02.620]   way going to lead to some kind of general intelligence.
[00:35:03.340 --> 00:35:08.980]   I mean, I guess, honestly, I don't think I have a super strong point of view.
[00:35:08.980 --> 00:35:15.780]   But you seem very empirical, and it seems like a very strong claim to say, "Surely this
[00:35:15.780 --> 00:35:18.100]   approach can't do this."
[00:35:18.100 --> 00:35:22.820]   And I guess maybe you're saying that it's just so poorly defined that that's not a meaningless
[00:35:22.820 --> 00:35:23.820]   claim.
[00:35:23.820 --> 00:35:26.300]   But I guess it's sort of...
[00:35:26.300 --> 00:35:31.220]   I think it's a meaningless claim, but I also think that for many definitions of AGI, I
[00:35:31.220 --> 00:35:33.620]   don't think feed-forward neural networks are really going to be able to pull it off.
[00:35:33.620 --> 00:35:37.540]   Yeah, so I guess I'm just trying to get at that of what are some things maybe...
[00:35:37.540 --> 00:35:39.700]   This is not a gotcha question.
[00:35:39.700 --> 00:35:43.660]   It's just for what are the kinds of things that you think feed-forward networks will
[00:35:43.660 --> 00:35:46.740]   never be able to do?
[00:35:46.740 --> 00:35:51.780]   I mean, right now we're watching GPT chat really struggle with long context lengths,
[00:35:51.780 --> 00:35:55.580]   where someone goes back and forth with it for enough iterations that it starts cycling,
[00:35:55.580 --> 00:35:58.980]   or it clearly loses track of what was happening earlier on.
[00:35:58.980 --> 00:36:00.980]   We still don't even know how to solve that basic problem.
[00:36:00.980 --> 00:36:02.780]   That's a problem we're going to have to overcome.
[00:36:02.780 --> 00:36:07.340]   We can really do large scale, just handling large amounts of information, being able to
[00:36:07.340 --> 00:36:11.500]   somehow reason about it hierarchically or something like that.
[00:36:11.500 --> 00:36:13.740]   We're still nowhere close to that.
[00:36:13.740 --> 00:36:19.860]   And I think now that people are finding some of the soft spots of GPT chat, we're seeing
[00:36:19.860 --> 00:36:20.980]   that happen in real time.
[00:36:20.980 --> 00:36:23.300]   That's a basic problem we're going to have to overcome.
[00:36:23.300 --> 00:36:27.380]   These models still attend to the tokens that are closest to the current token.
[00:36:27.380 --> 00:36:30.180]   They don't really attend to that far.
[00:36:30.180 --> 00:36:32.980]   And these things end up in reasoning in loops because of that.
[00:36:32.980 --> 00:36:35.860]   But if we want things to reason, this seems like a pretty inefficient way to get something
[00:36:35.860 --> 00:36:37.660]   to reason in and of itself.
[00:36:37.660 --> 00:36:41.980]   So I'm pretty skeptical that just taking the same things and making them bigger will solve
[00:36:41.980 --> 00:36:42.980]   any of these problems.
[00:36:42.980 --> 00:36:46.220]   And those are basic problems we're going to have to overcome before we get there.
[00:36:46.220 --> 00:36:52.020]   You're unusual that you have, I think, maybe a really strong interest in policy.
[00:36:52.020 --> 00:36:58.020]   Maybe could you tell us a little about that and what you think is important at this moment?
[00:36:58.020 --> 00:37:01.940]   I guess it's December 2022.
[00:37:01.940 --> 00:37:04.700]   What should we be...
[00:37:04.700 --> 00:37:07.740]   What kinds of things are you advocating for?
[00:37:07.740 --> 00:37:09.420]   So I'm curious.
[00:37:09.420 --> 00:37:12.420]   I'm going to turn the question on you for a moment.
[00:37:12.420 --> 00:37:15.020]   How would you define policy?
[00:37:15.020 --> 00:37:18.420]   I love to do this to people because you always get interesting answers.
[00:37:18.420 --> 00:37:22.340]   How would I define policy?
[00:37:22.340 --> 00:37:30.900]   I guess my first thought is government regulation of what companies can and can't do.
[00:37:30.900 --> 00:37:38.820]   And then I think there's another thread of what maybe outside of regulation, what companies
[00:37:38.820 --> 00:37:46.100]   should do to make sure that the work they do has a positive impact on the world.
[00:37:46.100 --> 00:37:47.940]   So what am I missing?
[00:37:48.460 --> 00:37:55.180]   Your first one here, regulation, I would consider that law, but not policy.
[00:37:55.180 --> 00:37:57.380]   So that's an instantiation of policy.
[00:37:57.380 --> 00:38:00.540]   But I think the big distinction here is this question, the second point you got to, what
[00:38:00.540 --> 00:38:01.700]   should we do?
[00:38:01.700 --> 00:38:03.460]   What is kind of the ought?
[00:38:03.460 --> 00:38:06.540]   Or what should we be accomplishing?
[00:38:06.540 --> 00:38:08.020]   What do we want the world to look like?
[00:38:08.020 --> 00:38:11.340]   And in some sense, the rest is implementation details.
[00:38:11.340 --> 00:38:13.660]   That's when you get to law concreteness.
[00:38:13.660 --> 00:38:16.820]   But even policy at a high level can be simple questions of what should we do or what direction
[00:38:16.820 --> 00:38:18.500]   do we want the world to move in?
[00:38:18.500 --> 00:38:22.900]   And so from a policy perspective, I don't see policy as necessarily advocacy.
[00:38:22.900 --> 00:38:23.940]   Advocacy is one thing you can do.
[00:38:23.940 --> 00:38:26.060]   You can advocate for what we should do.
[00:38:26.060 --> 00:38:30.420]   But the other is simply providing consultation to the people who do make policy, to the parliamentarians
[00:38:30.420 --> 00:38:34.820]   from around the world or what have you, the people who are setting policy and trying to
[00:38:34.820 --> 00:38:37.940]   figure out what direction they want their countries or they want the world to move in.
[00:38:37.940 --> 00:38:40.780]   That tends to be the role that I take.
[00:38:40.780 --> 00:38:44.860]   I tend to be a technical expert that gets called in to help provide context to policymakers
[00:38:44.860 --> 00:38:48.540]   on topics, in this case related to machine learning, but in the past it was privacy or
[00:38:48.540 --> 00:38:49.540]   security.
[00:38:49.540 --> 00:38:52.940]   So I spent a year at Georgetown Law kind of as the technologist in residence, helping
[00:38:52.940 --> 00:38:57.100]   them to make their decisions better on what kinds of policies they recommended or what
[00:38:57.100 --> 00:39:01.820]   kinds of research they did or how they understood what their findings were on various topics.
[00:39:01.820 --> 00:39:04.500]   Specifically in that case, police use of facial recognition in the US.
[00:39:04.500 --> 00:39:08.440]   We did a big study showing that I think at that time, one third of all American adults
[00:39:08.440 --> 00:39:11.440]   were in a police facial recognition database.
[00:39:11.440 --> 00:39:13.020]   At that time, that was earth shattering news.
[00:39:13.020 --> 00:39:16.580]   Today, I think we all understand we're probably in clear view and a bunch of other things
[00:39:16.580 --> 00:39:20.220]   we've given into a surveillance state in a way that we hadn't before.
[00:39:20.220 --> 00:39:24.500]   Today I spend a lot of time with an organization called the OECD, which is kind of a UN style
[00:39:24.500 --> 00:39:29.980]   organization that does economic policy for kind of mostly the democratic capitalist countries
[00:39:29.980 --> 00:39:34.500]   and helps to do research and help them do things like think about national AI capacity
[00:39:34.500 --> 00:39:35.700]   and how they should be setting that.
[00:39:35.700 --> 00:39:40.500]   So it's less about advocacy, but I think the important distinction here I'd make is I'm
[00:39:40.500 --> 00:39:42.380]   one input into this process.
[00:39:42.380 --> 00:39:46.100]   I'm a source of consultation and a source of expertise and a source of detailed knowledge
[00:39:46.100 --> 00:39:47.740]   about how AI does and doesn't work.
[00:39:47.740 --> 00:39:48.900]   I can provide feedback.
[00:39:48.900 --> 00:39:52.740]   I can make recommendations about when someone has a policy goal, what the right implementation
[00:39:52.740 --> 00:39:53.740]   would look like.
[00:39:53.740 --> 00:39:57.620]   I do see a lot of us in computer science kind of expressing the hubris that we should be
[00:39:57.620 --> 00:40:00.860]   the policy makers or we should set the final policy.
[00:40:00.860 --> 00:40:02.340]   We're not just one input into the process.
[00:40:02.340 --> 00:40:05.300]   We know better than the lawyers who've been thinking about questions of say fairness and
[00:40:05.300 --> 00:40:09.180]   bias for decades, centuries, however long that is.
[00:40:09.180 --> 00:40:13.060]   A lot of our questions around alignment or safety or things like that, did we ever realize
[00:40:13.060 --> 00:40:17.500]   there are regulatory agencies that have been dealing with say automobile safety for a very
[00:40:17.500 --> 00:40:21.140]   long time and probably have some good ideas about how to structure constraints on what
[00:40:21.140 --> 00:40:23.320]   we would think of as a safe car.
[00:40:23.320 --> 00:40:26.760]   In computer science, we tend to have the hubris to think that we can reinvent the wheel better
[00:40:26.760 --> 00:40:27.760]   than other people.
[00:40:27.760 --> 00:40:28.760]   We like to disrupt things.
[00:40:28.760 --> 00:40:31.500]   In the case of a lot of these topics, I think we're leading ourselves wrong and perhaps
[00:40:31.500 --> 00:40:37.200]   the right way is to engage with people who have built up expertise specifically in taking
[00:40:37.200 --> 00:40:42.300]   on these kinds of ambiguous questions that don't have clear answers.
[00:40:42.300 --> 00:40:45.860]   We should be consultants, but we're not the only input into that process.
[00:40:45.860 --> 00:40:48.700]   We should trust people who have legitimately studied this, not people who have made up
[00:40:48.700 --> 00:40:52.140]   new definitions of fairness because they thought it was interesting.
[00:40:52.140 --> 00:40:57.420]   Maybe I'll ask a question in a different way.
[00:40:57.420 --> 00:41:09.780]   Are there, I guess, you have front row seats to the explosion of use cases around language
[00:41:09.780 --> 00:41:12.320]   and vision models.
[00:41:12.320 --> 00:41:15.720]   What kind of concerns you the most about where things are headed?
[00:41:15.720 --> 00:41:19.560]   I think we're getting to a place, this is not a novel concern, but I think we are getting
[00:41:19.560 --> 00:41:22.840]   to a place where I think you're seeing this even with all the things I've seen on Twitter
[00:41:22.840 --> 00:41:24.000]   with GPT chat.
[00:41:24.000 --> 00:41:26.760]   These models are very confident even where they're full of crap.
[00:41:26.760 --> 00:41:31.360]   These models sound very convincing even when they're speaking complete nonsense.
[00:41:31.360 --> 00:41:34.800]   We don't have a way to tell the difference right now.
[00:41:34.800 --> 00:41:40.900]   We've seen this danger many times in the past in other forms that information from a source
[00:41:40.900 --> 00:41:47.260]   that seems reliable or feels reliable doesn't necessarily have to be true.
[00:41:47.260 --> 00:41:52.880]   The ability, the line between what does and doesn't feel true, let me put that a different
[00:41:52.880 --> 00:41:53.880]   way.
[00:41:53.880 --> 00:41:58.240]   But it's becoming a lot of our training as people about how to tell the difference between
[00:41:58.240 --> 00:42:02.840]   what is and isn't true and what should and shouldn't be trusted is being exploited by
[00:42:02.840 --> 00:42:06.000]   some of these models in order to convince us that things are true that aren't or make
[00:42:06.000 --> 00:42:07.760]   things seem real that aren't.
[00:42:07.760 --> 00:42:11.960]   We're not mentally prepared for a model that sounds really confident and speaks really
[00:42:11.960 --> 00:42:16.600]   intelligently, but it's just BSing because it's a language model that was trained on
[00:42:16.600 --> 00:42:18.800]   Wikipedia and Reddit.
[00:42:18.800 --> 00:42:22.320]   Or pictures coming out of something like a diffusion model that really seem real but
[00:42:22.320 --> 00:42:23.320]   aren't.
[00:42:23.320 --> 00:42:27.240]   And the world is moving much faster than our cognitive biases are.
[00:42:27.240 --> 00:42:32.600]   I think we'll adapt in the same way that people adapted to yellow journalism back at the turn
[00:42:32.600 --> 00:42:37.360]   of the 1800s to the 1900s.
[00:42:37.360 --> 00:42:41.080]   We've adapted, I think, reasonably well to fake news as people were now pretty skeptical
[00:42:41.080 --> 00:42:46.680]   of what we read online, even if it looks like it comes from a publication of some kind.
[00:42:46.680 --> 00:42:51.760]   And we'll adapt here, but things are moving so fast, it's hard for anyone to keep up.
[00:42:51.760 --> 00:42:53.960]   And it's hard to really...
[00:42:53.960 --> 00:42:55.720]   I don't know, I don't think we're ready.
[00:42:55.720 --> 00:42:59.720]   I don't think our cognitive biases are quite ready for the onslaught that came last year,
[00:42:59.720 --> 00:43:03.840]   let alone the one that's coming this year, let alone the one that will come next year.
[00:43:03.840 --> 00:43:14.360]   I guess as someone building a foundry for making lots of these models, are there things
[00:43:14.360 --> 00:43:19.720]   that you feel obligated to do on your side to help with these issues?
[00:43:19.720 --> 00:43:28.800]   Or is it really just a training of the consumers of these models to maybe not trust confidence,
[00:43:28.800 --> 00:43:33.200]   which might be a useful thing for people to do anyway?
[00:43:33.200 --> 00:43:34.880]   No, we're definitely obligated.
[00:43:34.880 --> 00:43:38.000]   And there are a lot of different ways of addressing that.
[00:43:38.000 --> 00:43:41.720]   I find personally a lot of impact in being downstream of these problems.
[00:43:41.720 --> 00:43:44.540]   If I'm going to make messes, I have to clean them up.
[00:43:44.540 --> 00:43:49.120]   So in some sense, my policy work is an attempt to make sure that as I'm on the bleeding edge
[00:43:49.120 --> 00:43:53.080]   of creating this technology, I'm also providing that same insight to policy makers so they
[00:43:53.080 --> 00:43:55.600]   can adapt to this as quickly as possible.
[00:43:55.600 --> 00:44:00.740]   And to make sure that we're asking the right things of people as these models change.
[00:44:00.740 --> 00:44:04.320]   Part of it is also that we need to be responsible about who we work with.
[00:44:04.320 --> 00:44:07.920]   There are some companies that at the end of the day, we may choose not to work with.
[00:44:07.920 --> 00:44:13.300]   There's some organizations we may choose not to work with if we don't think they're mature
[00:44:13.300 --> 00:44:15.520]   enough to handle this technology properly.
[00:44:15.520 --> 00:44:20.800]   That means partially that we need to move further down the chain, not just to how do
[00:44:20.800 --> 00:44:26.160]   you build this model, but trying to think of the right metaphor for in the chip world.
[00:44:26.160 --> 00:44:30.120]   But it's probably something like helping a company pen test their processor to make sure
[00:44:30.120 --> 00:44:34.080]   that I'm not going to tell you how to build it, but I do want to provide you with a toolkit
[00:44:34.080 --> 00:44:37.960]   to make sure that you built it such that it's robust to X, Y, and Z, such that you don't
[00:44:37.960 --> 00:44:40.200]   have timing channel attacks.
[00:44:40.200 --> 00:44:45.320]   So we may need to move further down the chain and help people evaluate their models effectively.
[00:44:45.320 --> 00:44:48.280]   It is something though that a lot of fantastic organizations are out there already working
[00:44:48.280 --> 00:44:51.600]   on and far be it from us to reinvent the wheel.
[00:44:51.600 --> 00:44:56.600]   Part of it is having great partners like Weights and Biases, who we work with extensively to
[00:44:56.600 --> 00:44:59.440]   make sure that we can offer customers a full solution.
[00:44:59.440 --> 00:45:02.660]   No one company is going to solve all their problems, but there are fantastic companies
[00:45:02.660 --> 00:45:08.120]   out there looking at questions of bias who are probably going to adapt as these models
[00:45:08.120 --> 00:45:12.560]   fool us or get around our cognitive biases in increasingly sophisticated ways.
[00:45:12.560 --> 00:45:15.920]   I'm going to want to show up to a customer saying, "Hi, we're Mosaic.
[00:45:15.920 --> 00:45:20.320]   We train models, but here's our preferred partner who we work with closely, who's an
[00:45:20.320 --> 00:45:23.880]   expert in how to help you evaluate and test this model before you put it out in the real
[00:45:23.880 --> 00:45:26.640]   world and we highly recommend you work with them.
[00:45:26.640 --> 00:45:27.640]   Here's our partner."
[00:45:27.640 --> 00:45:30.720]   In the same way that today we say, "Here's our preferred partner for experiment tracking.
[00:45:30.720 --> 00:45:32.640]   We highly recommend you work with them because they're great."
[00:45:32.640 --> 00:45:34.000]   We don't want to solve everyone's problems.
[00:45:34.000 --> 00:45:37.760]   It's about putting all these pieces together into one great solution.
[00:45:37.760 --> 00:45:41.000]   But at some point, if we get big enough, we'll certainly convene an advisory board of some
[00:45:41.000 --> 00:45:42.000]   kind.
[00:45:42.000 --> 00:45:45.200]   That's something I've seen work reasonably well from my perspective as a policy person
[00:45:45.200 --> 00:45:46.200]   in that world.
[00:45:46.200 --> 00:45:48.280]   There are certainly a lot of friends who, "You know who you are.
[00:45:48.280 --> 00:45:52.080]   I'll be calling on you for a favor to help us make good decisions."
[00:45:52.080 --> 00:45:55.800]   Someone on the outside who has the trust of the community and has my trust to help us
[00:45:55.800 --> 00:45:57.680]   make good decisions on that front.
[00:45:57.680 --> 00:46:03.840]   Someone did liken what we do, one of my friends, to building cyber weapons for people.
[00:46:03.840 --> 00:46:06.640]   These models can certainly be used in that way.
[00:46:06.640 --> 00:46:11.000]   We do have responsibility to help make sure that these models are being used carefully
[00:46:11.000 --> 00:46:13.960]   and keep an eye on what our customers want to do with them.
[00:46:13.960 --> 00:46:23.200]   I guess you have really front row seats into applications of these models.
[00:46:23.200 --> 00:46:30.000]   What's your perspective on what new use cases are opening up with this technology?
[00:46:30.000 --> 00:46:31.400]   Everything.
[00:46:31.400 --> 00:46:37.520]   I will give you a worse answer than just browsing your Twitter feed right now, to some extent.
[00:46:37.520 --> 00:46:42.800]   I'm watching my research group back at MIT try to make GPT chat write programs.
[00:46:42.800 --> 00:46:45.720]   I guess I won't scoop them because this podcast won't be out for a little while.
[00:46:45.720 --> 00:46:48.240]   But they just have a Slack channel where they're playing around with this and looking at the
[00:46:48.240 --> 00:46:50.400]   strengths and weaknesses of this model.
[00:46:50.400 --> 00:46:51.400]   It's really impressive.
[00:46:51.400 --> 00:46:56.120]   Even if it's deeply flawed, it's so impressive that we've gotten to the point where the flaws
[00:46:56.120 --> 00:46:59.920]   we can point out are things like, "Well, it doesn't seem to remember facts that well."
[00:46:59.920 --> 00:47:02.200]   That's a huge win over where we were a couple of years ago.
[00:47:02.200 --> 00:47:06.720]   We need to celebrate these wins even if these things aren't perfect.
[00:47:06.720 --> 00:47:10.240]   For folks who are using diffusion models for all sorts of really creative things, I don't
[00:47:10.240 --> 00:47:12.640]   want to scoop them either.
[00:47:12.640 --> 00:47:17.480]   But things that go beyond artistic purposes or things that go toward using it to generate
[00:47:17.480 --> 00:47:21.560]   new products or new ideas or new design.
[00:47:21.560 --> 00:47:22.560]   Really the...
[00:47:22.560 --> 00:47:24.520]   I'm trying to think of the right...
[00:47:24.520 --> 00:47:25.960]   Really I'm not the domain expert here.
[00:47:25.960 --> 00:47:30.800]   I think the beautiful thing about this is the barriers to entry are low enough that
[00:47:30.800 --> 00:47:33.920]   any domain expert can see how this tool can help solve their problems.
[00:47:33.920 --> 00:47:37.920]   I had someone reach out recently about using this for some sports-related purposes that
[00:47:37.920 --> 00:47:38.920]   I thought were really cool.
[00:47:38.920 --> 00:47:42.280]   I wouldn't have thought of that, but this person happens to work in the sports industry
[00:47:42.280 --> 00:47:46.600]   and had an idea for how to use a large language model for that.
[00:47:46.600 --> 00:47:49.200]   It may not be quite the right thing or it may take some more machine learning work,
[00:47:49.200 --> 00:47:52.440]   but it's at a point where this is mainstream enough that I'm not the one to tell you the
[00:47:52.440 --> 00:47:53.440]   cool applications.
[00:47:53.440 --> 00:47:56.760]   Go look at the world and all the brilliant creative professionals out there and the people
[00:47:56.760 --> 00:47:58.560]   who are in their own industries trying to solve problems.
[00:47:58.560 --> 00:48:00.720]   They're the ones who should do it.
[00:48:00.720 --> 00:48:03.640]   In service of not trying to have too much hubris, in service of trying to be humble
[00:48:03.640 --> 00:48:06.560]   in the way I'm around policy, I'm the foundry.
[00:48:06.560 --> 00:48:09.440]   TSMC is not the one thinking of innovative hardware solutions.
[00:48:09.440 --> 00:48:12.520]   They're not Cerebris building a wafer scale engine.
[00:48:12.520 --> 00:48:16.760]   They're not Graphcore thinking of a whole new way to organize a chip or anything like
[00:48:16.760 --> 00:48:17.760]   that.
[00:48:17.760 --> 00:48:19.200]   They're just saying, "Oh, that was brilliant.
[00:48:19.200 --> 00:48:20.280]   Cool use of our technology.
[00:48:20.280 --> 00:48:21.280]   Let's help you make it."
[00:48:21.280 --> 00:48:28.080]   But I guess if I'm coming to Mosaic, I'm probably doing more than just fine-tuning an open-source
[00:48:28.080 --> 00:48:29.080]   model out there.
[00:48:29.080 --> 00:48:32.920]   So I must have something that I really, really care about.
[00:48:32.920 --> 00:48:36.320]   So I'd love to just get...
[00:48:36.320 --> 00:48:39.920]   Who comes to Mosaic to do this and why are they doing it?
[00:48:39.920 --> 00:48:46.000]   I understand why you might not want to use GPT-3, which has to be hosted in the cloud
[00:48:46.000 --> 00:48:47.520]   and you can't hold the model.
[00:48:47.520 --> 00:48:56.120]   But there are open language models out there right now.
[00:48:56.120 --> 00:49:02.280]   And what causes a company to undertake this very big expense of building one of their
[00:49:02.280 --> 00:49:04.760]   own foundation models?
[00:49:04.760 --> 00:49:06.040]   Data.
[00:49:06.040 --> 00:49:10.000]   It's specifically data in one word.
[00:49:10.000 --> 00:49:15.520]   And by that, I mean that your data is your identity.
[00:49:15.520 --> 00:49:19.340]   That means both from a personal perspective, but also from a company perspective.
[00:49:19.340 --> 00:49:21.840]   Every company is sitting on so much unlabeled data.
[00:49:21.840 --> 00:49:25.500]   We now have incredible methods to leverage that unlabeled data between images and text
[00:49:25.500 --> 00:49:30.360]   and probably soon combining the two and combining all sorts of other modalities.
[00:49:30.360 --> 00:49:34.520]   That data is your identity and would you rather use the same identity as everybody else?
[00:49:34.520 --> 00:49:35.640]   Would you rather use your identity?
[00:49:35.640 --> 00:49:39.600]   Would you rather use Reddit, which is probably a pretty large part of what's GPT-3?
[00:49:39.600 --> 00:49:41.480]   Would you rather use your data and your identity?
[00:49:41.480 --> 00:49:44.080]   And I think that's what people are really coming for.
[00:49:44.080 --> 00:49:45.080]   So that makes sense.
[00:49:45.080 --> 00:49:49.360]   But then what are people doing downstream of it that's so important that they're willing
[00:49:49.360 --> 00:49:52.120]   to make this huge investment?
[00:49:52.120 --> 00:49:56.760]   Anything and everything from actually using this for customer service scenarios to using
[00:49:56.760 --> 00:50:02.160]   this to do interesting open-ended classification tasks or the kinds of few shot or zero shot
[00:50:02.160 --> 00:50:07.680]   tasks, being able to prompt in a domain specific way, anything and everything.
[00:50:07.680 --> 00:50:12.020]   All the standard applications that you see of a GPT model as applied to whatever their
[00:50:12.020 --> 00:50:13.920]   scenario is inside their company.
[00:50:13.920 --> 00:50:18.660]   It's the same applications, but with the benefit of having your data imbued into this model.
[00:50:18.660 --> 00:50:21.960]   One thing that we always think about or one paradigm that I'm thinking about a lot these
[00:50:21.960 --> 00:50:27.600]   days is the idea that these large language models are really databases.
[00:50:27.600 --> 00:50:29.960]   They know things.
[00:50:29.960 --> 00:50:35.600]   You can query the GPT chat and get all sorts of really interesting facts out of it.
[00:50:35.600 --> 00:50:36.760]   A lot of those facts aren't quite right.
[00:50:36.760 --> 00:50:39.840]   There was a beautiful thread earlier today about asking what the fastest marine mammal
[00:50:39.840 --> 00:50:42.280]   is and it said a falcon.
[00:50:42.280 --> 00:50:45.280]   But it has knowledge and it has facts.
[00:50:45.280 --> 00:50:47.640]   There's a great bit of work from Ophir Press.
[00:50:47.640 --> 00:50:49.720]   I'm a researcher at the University of Washington.
[00:50:49.720 --> 00:50:51.480]   He's a PhD student right now.
[00:50:51.480 --> 00:50:52.680]   He's on the job market, by the way.
[00:50:52.680 --> 00:50:56.040]   I have to say that for anybody I know who's on the job market.
[00:50:56.040 --> 00:51:00.040]   Doing this thing he calls self-ask, where he gets the model to reason through a task
[00:51:00.040 --> 00:51:03.000]   by asking repeated follow-up questions.
[00:51:03.000 --> 00:51:06.240]   And then he did this really cool thing where he Googled each of those follow-up questions
[00:51:06.240 --> 00:51:11.240]   and took whatever answer Google gave in its knowledge box and gave that back to the model.
[00:51:11.240 --> 00:51:15.200]   So having these models now interact with databases or having these models be the databases themselves
[00:51:15.200 --> 00:51:20.440]   and probably some combination between the two, I think of every relational database
[00:51:20.440 --> 00:51:23.680]   out there as an exact version of a database.
[00:51:23.680 --> 00:51:26.000]   You have a schema, you have a way of querying data.
[00:51:26.000 --> 00:51:28.280]   These large language models are in some sense soft databases.
[00:51:28.280 --> 00:51:30.320]   You can ask them natural language questions.
[00:51:30.320 --> 00:51:34.800]   They can find relationships between data that aren't expressed in a relational database
[00:51:34.800 --> 00:51:39.160]   but might be expressed if you give it enough data and teach it what text looks like or
[00:51:39.160 --> 00:51:40.960]   what language looks like.
[00:51:40.960 --> 00:51:44.920]   And you can query these things like databases or even connect them to databases as well.
[00:51:44.920 --> 00:51:48.520]   So I think that's really, for me, I see that as an emerging application area.
[00:51:48.520 --> 00:51:53.200]   As thinking of these not as models but as these soft databases that give you the ability
[00:51:53.200 --> 00:51:56.840]   to make connections that you could never do if you had an exact relational database.
[00:51:56.840 --> 00:51:59.320]   So kind of fuzzy databases or approximate databases.
[00:51:59.320 --> 00:52:04.680]   I'm sure someone will coin a much cleverer term than that, but that's how I think of
[00:52:04.680 --> 00:52:05.680]   them today.
[00:52:05.680 --> 00:52:10.240]   And when you think of it from that perspective, do you want your database to be whatever a
[00:52:10.240 --> 00:52:11.600]   web crawl open AI used?
[00:52:11.600 --> 00:52:13.960]   Or do you want your database to be your data?
[00:52:13.960 --> 00:52:15.880]   The answer is probably a combination of both.
[00:52:15.880 --> 00:52:17.560]   - Probably both, I was going to say, yeah.
[00:52:17.560 --> 00:52:19.420]   - But you certainly want your data in there.
[00:52:19.420 --> 00:52:22.240]   And if you have a lot of data, whether you're pre-training from scratch or starting from
[00:52:22.240 --> 00:52:27.120]   OPT or some other pre-trained starting point, you still need to imbue this model with your
[00:52:27.120 --> 00:52:28.400]   data.
[00:52:28.400 --> 00:52:34.000]   - How many companies do you think will try to build these large models from scratch?
[00:52:34.000 --> 00:52:37.360]   - Hundreds, at least, possibly thousands.
[00:52:37.360 --> 00:52:40.920]   At least our business so far seems to reflect that possibility.
[00:52:40.920 --> 00:52:43.840]   Business is really good for training large language models.
[00:52:43.840 --> 00:52:48.560]   We do feel a little bit like TSMC right now in that getting capacity on TSMC is really
[00:52:48.560 --> 00:52:52.280]   hard and you've got to be an Apple-sized customer to be able to book that capacity a large way
[00:52:52.280 --> 00:52:53.280]   in advance.
[00:52:53.280 --> 00:52:56.280]   And I feel that way right now in terms of how we're booking our large language model
[00:52:56.280 --> 00:52:57.280]   training at the moment.
[00:52:57.280 --> 00:53:01.640]   My team will certainly, if you manage to find them right now, they will tell you that as
[00:53:01.640 --> 00:53:02.640]   well.
[00:53:02.640 --> 00:53:03.640]   But the answer is everybody.
[00:53:03.640 --> 00:53:05.280]   Everybody is sitting on so much data.
[00:53:05.280 --> 00:53:07.840]   Many companies are going to need lots of these things.
[00:53:07.840 --> 00:53:10.440]   And lots of companies are going to need at least one of these things.
[00:53:10.440 --> 00:53:13.840]   We're seeing this all the way down to relatively small companies that want to do some fine
[00:53:13.840 --> 00:53:17.860]   tuning of these models because they have some business-specific data that is really important
[00:53:17.860 --> 00:53:20.160]   for them to be able to use these models effectively.
[00:53:20.160 --> 00:53:26.720]   So I genuinely think, what is it, GPT Chat got up to a million users after a week of
[00:53:26.720 --> 00:53:27.720]   use.
[00:53:27.720 --> 00:53:30.220]   I think that should tell you something about the number of people who have found interesting
[00:53:30.220 --> 00:53:34.000]   use cases or at least very curious about where this technology might fit in.
[00:53:34.000 --> 00:53:39.080]   And if that's a company, they have a lot of data that they can use to customize this model
[00:53:39.080 --> 00:53:40.080]   for them.
[00:53:40.080 --> 00:53:43.200]   And really, at the end of the day, if we've learned one thing from these large language
[00:53:43.200 --> 00:53:44.200]   models, it's all about the data.
[00:53:44.200 --> 00:53:45.440]   The models aren't very interesting.
[00:53:45.440 --> 00:53:47.040]   They're transformers, yeah.
[00:53:47.040 --> 00:53:50.720]   It's a cool technology, but transformers are pretty simple compared to an LSTM or something
[00:53:50.720 --> 00:53:52.400]   like that.
[00:53:52.400 --> 00:53:54.880]   It's not even about the way we train it, other than the fact that the way we train it is
[00:53:54.880 --> 00:53:57.600]   really expensive right now and Mosaic needs to get that cheaper.
[00:53:57.600 --> 00:53:59.600]   The data is where the magic happens.
[00:53:59.600 --> 00:54:02.360]   The data is what gives this model its superpowers.
[00:54:02.360 --> 00:54:05.600]   And that's a place where everybody's going to want to customize.
[00:54:05.600 --> 00:54:09.400]   It's interesting you say that as someone that does the model building.
[00:54:09.400 --> 00:54:15.280]   Do you offer any kind of services like active learning or ways to improve the data if you
[00:54:15.280 --> 00:54:18.520]   feel that the data is the most important input into the model?
[00:54:18.520 --> 00:54:21.760]   How do you engage with the data?
[00:54:21.760 --> 00:54:26.000]   Right now, we have a lot of excellent partners who are fantastic at this.
[00:54:26.000 --> 00:54:28.200]   And we're a startup, we need to stay focused.
[00:54:28.200 --> 00:54:32.600]   And our focus is on making it cheap enough that you can even contemplate doing this.
[00:54:32.600 --> 00:54:35.080]   The data is a problem you only think about when you can actually train the model.
[00:54:35.080 --> 00:54:40.200]   And if I told you the model was $10 million to train, well, in many cases, you don't care
[00:54:40.200 --> 00:54:41.520]   about data quality at that point.
[00:54:41.520 --> 00:54:43.520]   You just know that you're not going to be able to afford to train it.
[00:54:43.520 --> 00:54:48.520]   If I tell you that model is $100,000 to train, then there's another conversation to be had.
[00:54:48.520 --> 00:54:51.960]   And I think for the first time, we're even having that conversation in a way that I don't
[00:54:51.960 --> 00:54:55.640]   think we could have prior to a lot of the efficiency work that's happened at MosaicML
[00:54:55.640 --> 00:54:57.120]   and elsewhere.
[00:54:57.120 --> 00:55:00.040]   So now that we're in this place, it's something we're definitely thinking about.
[00:55:00.040 --> 00:55:03.040]   And it's something that we're building tools to provide.
[00:55:03.040 --> 00:55:05.960]   But it's something that there are a lot of fantastic companies out there and a lot of
[00:55:05.960 --> 00:55:10.720]   fantastic partners that are experts in data and far be it from us to walk in and think
[00:55:10.720 --> 00:55:11.920]   we know better than them.
[00:55:11.920 --> 00:55:13.320]   Can you tell me some of your favorites?
[00:55:13.320 --> 00:55:15.400]   I mean, I'm curious.
[00:55:15.400 --> 00:55:20.800]   I'm neutral in this because I used to do a data collection company, but I haven't in
[00:55:20.800 --> 00:55:21.800]   years.
[00:55:21.800 --> 00:55:22.800]   Do you have a-
[00:55:22.800 --> 00:55:25.440]   I don't want to play favorites or in public or anything like that.
[00:55:25.440 --> 00:55:30.040]   There are a lot of great folks out there and we've worked with a lot of different folks
[00:55:30.040 --> 00:55:32.160]   in the past and we're probably going to work with a lot of folks in the future.
[00:55:32.160 --> 00:55:36.440]   So I don't want to play favorites here, but it's a really competitive space, which means
[00:55:36.440 --> 00:55:40.720]   there are a lot of really smart people working hard to do better at data curation and data
[00:55:40.720 --> 00:55:41.720]   labeling.
[00:55:41.720 --> 00:55:45.880]   And I would trust them over me right now, for sure.
[00:55:45.880 --> 00:55:48.800]   I haven't been doing this for years the way that many of these companies have.
[00:55:48.800 --> 00:55:50.720]   And you should go to the experts.
[00:55:50.720 --> 00:55:54.360]   We're really, really good at training and we do that really well.
[00:55:54.360 --> 00:55:59.440]   And depending on what kinds of data someone's working on and what kinds of customers or
[00:55:59.440 --> 00:56:03.000]   what kinds of companies are looking to work with, we can point them in a number of different
[00:56:03.000 --> 00:56:04.000]   directions.
[00:56:04.000 --> 00:56:06.960]   But it's fun being a startup.
[00:56:06.960 --> 00:56:08.840]   It's like being in academia in some sense.
[00:56:08.840 --> 00:56:10.280]   Nobody's an expert in everything.
[00:56:10.280 --> 00:56:13.640]   And if you want to accomplish anything significant, you have to collaborate.
[00:56:13.640 --> 00:56:17.640]   And I like to look at the world as being full of awesome collaborators who we can work with.
[00:56:17.640 --> 00:56:18.640]   All right.
[00:56:18.640 --> 00:56:21.840]   Well, we always finish with two questions and I want to make sure I have time to get
[00:56:21.840 --> 00:56:22.840]   them in.
[00:56:22.840 --> 00:56:27.640]   And one of them is something that I'm sure you'll have a good answer for, which is kind
[00:56:27.640 --> 00:56:32.000]   of outside of what you're doing now, what's another area of research that you wish you
[00:56:32.000 --> 00:56:34.000]   had time to look into?
[00:56:34.000 --> 00:56:39.680]   What do you think is an under-appreciated research topic in machine learning?
[00:56:39.680 --> 00:56:41.000]   Oh, man.
[00:56:41.000 --> 00:56:43.320]   There are so many.
[00:56:43.320 --> 00:56:46.800]   I would tell everyone who's working on adversarial examples or federated learning or anything
[00:56:46.800 --> 00:56:53.600]   else that's kind of academic and not very exciting to go work on any one of these instead.
[00:56:53.600 --> 00:56:56.120]   For me, I am really excited about the data questions.
[00:56:56.120 --> 00:57:00.080]   I think understanding how much data quality really matters, understanding how much these
[00:57:00.080 --> 00:57:04.040]   things like reinforcement learning with human feedback or just instruction fine tuning actually
[00:57:04.040 --> 00:57:05.040]   matter.
[00:57:05.040 --> 00:57:06.040]   Are these red herrings?
[00:57:06.040 --> 00:57:07.720]   You know, opening, I said they were important.
[00:57:07.720 --> 00:57:08.720]   Why should we believe them?
[00:57:08.720 --> 00:57:13.080]   Why should we believe anyone until we've reproduced this and seen whether it's actually valuable?
[00:57:13.080 --> 00:57:17.160]   There's a great opportunity for us to, you know, for those of us in academia and, you
[00:57:17.160 --> 00:57:20.840]   know, in my other life, I will be an academic next fall again.
[00:57:20.840 --> 00:57:24.640]   And technically, I guess I still am for another three days until I defend.
[00:57:24.640 --> 00:57:28.680]   For those of us in the academic world, these are fantastic questions to ask.
[00:57:28.680 --> 00:57:33.080]   I don't think we should take for granted that someone said, you know, someone like opening,
[00:57:33.080 --> 00:57:34.440]   I said, this is what they do.
[00:57:34.440 --> 00:57:37.600]   We're talked about it and therefore it must be what they're doing.
[00:57:37.600 --> 00:57:39.320]   These are questions we can scientifically approach.
[00:57:39.320 --> 00:57:43.400]   And I think the beautiful thing about this is they're not that expensive to take on in
[00:57:43.400 --> 00:57:46.240]   a lot of ways, especially the fine tuning questions.
[00:57:46.240 --> 00:57:47.720]   Those just involve fine tuning a model.
[00:57:47.720 --> 00:57:49.320]   They don't involve training something from scratch.
[00:57:49.320 --> 00:57:52.360]   The data questions are a little trickier, but even at small scales, we should be able
[00:57:52.360 --> 00:57:55.680]   to see effects that should give us a sense for what may happen at large scales.
[00:57:55.680 --> 00:57:58.200]   And so for me, this seems to be the key leverage point right now.
[00:57:58.200 --> 00:58:02.600]   I mean, the other questions that always get me excited are the questions of how these
[00:58:02.600 --> 00:58:04.660]   models really learn.
[00:58:04.660 --> 00:58:08.400]   This is a wildly complex process where we're taking tons of data, tons of parameters and
[00:58:08.400 --> 00:58:11.560]   tons of compute and throwing it together in a stew and mixing it.
[00:58:11.560 --> 00:58:13.760]   And sometimes good things seem to come out.
[00:58:13.760 --> 00:58:17.240]   But what are kind of the chemical processes actually happening there?
[00:58:17.240 --> 00:58:19.200]   How does learning take place?
[00:58:19.200 --> 00:58:21.040]   What does the network learn over time and how does it learn?
[00:58:21.040 --> 00:58:22.640]   What are those dynamics?
[00:58:22.640 --> 00:58:26.000]   And that always gets back to the application is the mosaic question of how do we speed
[00:58:26.000 --> 00:58:27.240]   it up?
[00:58:27.240 --> 00:58:30.800]   But the mere act of understanding how learning happens is itself a fascinating scientific
[00:58:30.800 --> 00:58:31.800]   question.
[00:58:31.800 --> 00:58:35.280]   And if for those looking for PhD programs, and I don't think it's still too late to apply
[00:58:35.280 --> 00:58:39.960]   to Harvard this fall, you better really start writing and get your letters like yesterday.
[00:58:39.960 --> 00:58:42.680]   But if you want to come work with me on a PhD at some point, those are the questions
[00:58:42.680 --> 00:58:44.160]   I'm most excited about academically.
[00:58:44.160 --> 00:58:48.320]   Because I just find the fact that you can stir all this stuff together and get something
[00:58:48.320 --> 00:58:51.840]   like GPT-3 out to be endlessly mind blowing.
[00:58:51.840 --> 00:58:55.440]   I totally agree.
[00:58:55.440 --> 00:58:57.360]   Totally different direction, I guess.
[00:58:57.360 --> 00:59:04.000]   When you look at taking these models and actually getting them running in the real world, both
[00:59:04.000 --> 00:59:08.160]   sort of inside of Mosaic and then I guess that's your customers where you hand off the
[00:59:08.160 --> 00:59:09.160]   model.
[00:59:09.160 --> 00:59:11.400]   I'm curious about where you see the unexpected bottlenecks.
[00:59:11.400 --> 00:59:18.200]   What are the actual hard parts about getting a big model, doing something useful for someone
[00:59:18.200 --> 00:59:19.320]   in the real world?
[00:59:19.320 --> 00:59:21.160]   It's always the stupid stuff.
[00:59:21.160 --> 00:59:24.920]   Just like anyone who's ever done software engineering knows, it's always the semicolon
[00:59:24.920 --> 00:59:25.920]   you forgot.
[00:59:25.920 --> 00:59:28.600]   It's never that your algorithm has the wrong time complexity.
[00:59:28.600 --> 00:59:32.120]   It's always that you mixed up two variable names or that you accidentally overwrote a
[00:59:32.120 --> 00:59:35.040]   variable somewhere in your for loop and didn't realize it.
[00:59:35.040 --> 00:59:36.800]   We see that all the time.
[00:59:36.800 --> 00:59:40.920]   Like the example I gave of using a different tokenizer for training than for evaluation
[00:59:40.920 --> 00:59:42.920]   and thinking that your model is not training at all.
[00:59:42.920 --> 00:59:44.560]   It's those kinds of mistakes.
[00:59:44.560 --> 00:59:48.600]   But there are so many different places where this can happen because these are such complex
[00:59:48.600 --> 00:59:54.120]   systems that are so nuanced that it's those dumb mistakes that kneecap you.
[00:59:54.120 --> 00:59:58.080]   Did you know, and I didn't know this, that on an A100 or a V100, if you change the memory
[00:59:58.080 --> 01:00:01.680]   format of a ResNet to channels last, this doesn't change anything about the model.
[01:00:01.680 --> 01:00:05.240]   It's a one model dot channels last or something like that.
[01:00:05.240 --> 01:00:06.800]   You get a 30% speed up.
[01:00:06.800 --> 01:00:07.800]   Did you know?
[01:00:07.800 --> 01:00:08.800]   I sure didn't know.
[01:00:08.800 --> 01:00:12.000]   Did PhD student me really wish he knew that?
[01:00:12.000 --> 01:00:13.000]   Yeah.
[01:00:13.000 --> 01:00:16.000]   It would have saved me a huge amount of time and a lot of money for my advisor and I would
[01:00:16.000 --> 01:00:18.000]   have done a lot more science.
[01:00:18.000 --> 01:00:21.760]   Who knew it was written down in a beta PyTorch documentation page somewhere.
[01:00:21.760 --> 01:00:24.320]   A few people in the know knew about it.
[01:00:24.320 --> 01:00:27.720]   We wrote it down and put it out very publicly and we try to do that with everything so people
[01:00:27.720 --> 01:00:28.720]   know where to find it.
[01:00:28.720 --> 01:00:32.800]   But it's the little things that always kill you, not the big stuff.
[01:00:32.800 --> 01:00:36.460]   At the end of the day, we can figure out how to get the stuff running, but it's the little
[01:00:36.460 --> 01:00:38.040]   things that just drag you down.
[01:00:38.040 --> 01:00:40.080]   It's death by a thousand cuts a lot of the time.
[01:00:40.080 --> 01:00:43.940]   I think the other killers are always efficiency.
[01:00:43.940 --> 01:00:46.220]   When you're training at this scale, if it's not efficient, it's going to take the life
[01:00:46.220 --> 01:00:47.220]   of the universe.
[01:00:47.220 --> 01:00:49.480]   That's kind of the mosaic problem in some sense.
[01:00:49.480 --> 01:00:53.120]   The other is that there's a difference between getting it running and getting it running
[01:00:53.120 --> 01:00:55.800]   smoothly and reliably and consistently.
[01:00:55.800 --> 01:00:58.040]   The difference between having something jury rigged.
[01:00:58.040 --> 01:00:59.560]   I'll give you a quick anecdote here.
[01:00:59.560 --> 01:01:02.360]   Whether this makes the cut or not, we'll see.
[01:01:02.360 --> 01:01:05.200]   Here's how I trained my models back in my PhD.
[01:01:05.200 --> 01:01:10.480]   Google was kind enough to give me some free TPU capacity, but there wasn't a job scheduler
[01:01:10.480 --> 01:01:11.480]   associated with this.
[01:01:11.480 --> 01:01:16.200]   You had to just manually spin up a TPU, SSH in, start your job, SSH out, let it go, check
[01:01:16.200 --> 01:01:17.200]   in at the TPU.
[01:01:17.200 --> 01:01:20.200]   Sometimes it froze up or died or my code crashed.
[01:01:20.200 --> 01:01:23.560]   So I wrote a little orchestration platform on top of it.
[01:01:23.560 --> 01:01:27.240]   I wrote a Google spreadsheet and used the Google spreadsheet API so that whenever I
[01:01:27.240 --> 01:01:31.360]   typed a line into the spreadsheet with an experiment, my little demon process I had
[01:01:31.360 --> 01:01:34.300]   would read the experiment off, check to see if it was already running.
[01:01:34.300 --> 01:01:38.400]   If not, it would assign it to TPU, kick it off, run it, and update the spreadsheet with
[01:01:38.400 --> 01:01:41.800]   its status, change the color of the line and everything.
[01:01:41.800 --> 01:01:42.800]   And I used this to train...
[01:01:42.800 --> 01:01:43.800]   Nice.
[01:01:43.800 --> 01:01:47.240]   I used this to collect over the course of my PhD almost a million checkpoints.
[01:01:47.240 --> 01:01:50.960]   I just deleted those checkpoints because they were taking up God knows how much space.
[01:01:50.960 --> 01:01:53.380]   And it was over a million files deleted.
[01:01:53.380 --> 01:01:55.560]   This was the jankiest thing in the world.
[01:01:55.560 --> 01:01:56.620]   It worked.
[01:01:56.620 --> 01:01:59.480]   To my advisor's chagrin, it worked.
[01:01:59.480 --> 01:02:02.160]   It was terrible and unscalable and unsustainable.
[01:02:02.160 --> 01:02:05.740]   And if I'd had to do something even bigger if I were running a business and this were
[01:02:05.740 --> 01:02:08.620]   what we're holding things together, God help me.
[01:02:08.620 --> 01:02:10.640]   It was good enough for a PhD student.
[01:02:10.640 --> 01:02:12.680]   And a lot of us settle for that quality of solution.
[01:02:12.680 --> 01:02:14.400]   A lot of us say we're okay with that.
[01:02:14.400 --> 01:02:19.280]   In the Weights and Biases context, a lot of us say, "I'm okay with TensorBoard, even though
[01:02:19.280 --> 01:02:25.040]   it requires 128 CPU cores and gigabytes or terabytes of memory, and it still crashes
[01:02:25.040 --> 01:02:27.560]   all the time and doesn't really do anything right."
[01:02:27.560 --> 01:02:32.240]   And in some sense, there's nicer stuff out there.
[01:02:32.240 --> 01:02:34.200]   We know how to build better tools.
[01:02:34.200 --> 01:02:37.280]   But I think many of us are...
[01:02:37.280 --> 01:02:41.880]   We've been in academia for so long that we feel like we don't deserve better.
[01:02:41.880 --> 01:02:43.280]   Why should I have access to the nice tools?
[01:02:43.280 --> 01:02:44.280]   I'm an academic.
[01:02:44.280 --> 01:02:45.280]   I've been in academia.
[01:02:45.280 --> 01:02:48.520]   My research team says this to me all the time.
[01:02:48.520 --> 01:02:50.400]   Our engineers will say, "Why didn't you report this bug?
[01:02:50.400 --> 01:02:53.280]   Why did you just do this horrific thing to work around it?"
[01:02:53.280 --> 01:02:56.680]   And after we go through enough rounds, it typically comes down to, "Well, I didn't want
[01:02:56.680 --> 01:03:00.400]   to disrupt you, and I didn't feel like I deserved to have this bug fixed."
[01:03:00.400 --> 01:03:06.240]   We've accepted this self-flagellation as part of being an academic or being a researcher.
[01:03:06.240 --> 01:03:09.660]   But to some extent, I think it's that.
[01:03:09.660 --> 01:03:13.560]   There are clean, good solutions that just work out there for a lot of these problems.
[01:03:13.560 --> 01:03:17.080]   And if they're not, someone's building them right now.
[01:03:17.080 --> 01:03:22.380]   And that janky solution you knitted together using Slurm, you shouldn't use that.
[01:03:22.380 --> 01:03:26.940]   That TensorBoard-based solution that crashes half the time and gets the lines mixed up
[01:03:26.940 --> 01:03:29.800]   and has these weird spikes and gaps and everything in the data.
[01:03:29.800 --> 01:03:32.220]   There's something better out there than that.
[01:03:32.220 --> 01:03:35.260]   And I think we should be willing to use it.
[01:03:35.260 --> 01:03:39.580]   We also have this philosophy in computer science of, "I shouldn't have to pay for things."
[01:03:39.580 --> 01:03:44.380]   My poor friend who was using LibreOffice, when they could have just paid a relatively
[01:03:44.380 --> 01:03:48.440]   small amount of money to Microsoft or Google and gotten a product that really genuinely
[01:03:48.440 --> 01:03:49.440]   did work.
[01:03:49.440 --> 01:03:51.660]   But instead, they really wanted to do that.
[01:03:51.660 --> 01:03:55.180]   I had a professor I worked with who really wanted to use his Linux laptop, even though
[01:03:55.180 --> 01:03:58.580]   it almost never connected to the Wi-Fi and barely ever printed.
[01:03:58.580 --> 01:04:01.780]   And I think we look at things like Slurm or TensorBoard and we think, "I see those as
[01:04:01.780 --> 01:04:03.420]   kind of the LibreOffice type solution."
[01:04:03.420 --> 01:04:07.340]   It will kind of sometimes get the job done, but also it's not that expensive to get the
[01:04:07.340 --> 01:04:09.100]   real thing, especially if you're an academic.
[01:04:09.100 --> 01:04:10.340]   There's academic pricing for all this stuff.
[01:04:10.340 --> 01:04:15.020]   And if you're a company, the cost of making your engineer's time, the cost of your engineer's
[01:04:15.020 --> 01:04:19.020]   time absolutely dwarfs the amount of money you'll pay to any one of these companies out
[01:04:19.020 --> 01:04:20.500]   there to get a product that works.
[01:04:20.500 --> 01:04:25.100]   So I do think there's something tied up in this, "Did I homebrew something crappy because
[01:04:25.100 --> 01:04:26.860]   I didn't know any better?
[01:04:26.860 --> 01:04:29.580]   Did I deserve better?
[01:04:29.580 --> 01:04:32.860]   Do I have this intrinsic trained interest to not pay for things?"
[01:04:32.860 --> 01:04:37.140]   Because that's, I don't know, we have an open source ethos in computer science.
[01:04:37.140 --> 01:04:39.660]   But all of those things I see dragging people down a lot.
[01:04:39.660 --> 01:04:43.260]   They certainly, to some extent, the tools didn't exist when I needed them to, but now
[01:04:43.260 --> 01:04:44.900]   there are fantastic tools out there.
[01:04:44.900 --> 01:04:49.980]   And if someone were to have the same infrastructure I had or the same tools that I had, or the
[01:04:49.980 --> 01:04:53.220]   same capabilities I had, the same hardware, the TPUs, they should not have done it the
[01:04:53.220 --> 01:04:55.300]   way that I did it in this day and age.
[01:04:55.300 --> 01:04:58.820]   There are tools out there that will help you to use that infrastructure more effectively.
[01:04:58.820 --> 01:05:01.780]   So we should be focusing on the problem here.
[01:05:01.780 --> 01:05:05.020]   And I think I've strayed way away from your question and rambled a lot, and we'll see
[01:05:05.020 --> 01:05:07.140]   how this gets cut up when all is said and done.
[01:05:07.140 --> 01:05:10.780]   But I think to make a very long story short, it's not as hard as it used to be.
[01:05:10.780 --> 01:05:14.420]   There really are tools that will help you avoid making mistakes and you should just
[01:05:14.420 --> 01:05:15.420]   use them.
[01:05:15.420 --> 01:05:20.260]   Well, there's a role here for you as head of marketing.
[01:05:20.260 --> 01:05:21.260]   Thanks, Jonathan.
[01:05:21.260 --> 01:05:24.260]   It's great to talk.
[01:05:24.260 --> 01:05:31.940]   And as a dedicated user of your product, we're willing to pay to not have to suffer and it's
[01:05:31.940 --> 01:05:32.940]   worth it for us.
[01:05:32.940 --> 01:05:33.940]   Nice.
[01:05:33.940 --> 01:05:34.940]   Thank you.
[01:05:34.940 --> 01:05:35.940]   We appreciate it.
[01:05:35.940 --> 01:05:40.420]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:05:40.420 --> 01:05:45.140]   to the show notes in the description where you can find links to all the papers that
[01:05:45.140 --> 01:05:49.420]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:05:49.420 --> 01:05:50.420]   produce.
[01:05:50.420 --> 01:05:50.700]   So check it out.
[01:05:50.700 --> 01:05:53.280]   (upbeat music)
[01:05:53.280 --> 01:05:55.340]   you

