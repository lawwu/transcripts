
[00:00:00.000 --> 00:00:05.960]   By basically talking to a bunch of folks, building stuff with language models and noticing
[00:00:05.960 --> 00:00:09.000]   that there was a lot of common abstractions in terms of what they were doing, in terms
[00:00:09.000 --> 00:00:12.560]   of constructing prompts, in terms of calling out to language models, in terms of calling
[00:00:12.560 --> 00:00:15.260]   out to embeddings, in terms of calling out to vector stores.
[00:00:15.260 --> 00:00:19.240]   So the idea is to make it really easy to have these building blocks, swap them out for each
[00:00:19.240 --> 00:00:22.760]   other, assemble them, and then have some pre-run templates to get started with.
[00:00:22.760 --> 00:00:27.120]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:27.120 --> 00:00:29.120]   and I'm your host, Lukas Biewald.
[00:00:29.120 --> 00:00:36.280]   Harrison Chase is the CEO and co-founder of LangChain, which is one of the fastest growing
[00:00:36.280 --> 00:00:41.640]   open source projects of all time, and something almost everyone uses these days when they
[00:00:41.640 --> 00:00:44.840]   build applications on top of large language models.
[00:00:44.840 --> 00:00:50.760]   We recently did an integration with LangChain in our product, WMB Prompts, and so we spent
[00:00:50.760 --> 00:00:54.380]   a lot of time working with Harrison and have walked away super impressed.
[00:00:54.380 --> 00:00:58.240]   This is an interview where we go super deep into the technical details, and I hope you
[00:00:58.240 --> 00:00:59.240]   enjoy it.
[00:00:59.240 --> 00:01:04.320]   So Harrison, I think for most of our audience, you probably don't need an introduction, but
[00:01:04.320 --> 00:01:09.680]   we probably should start with a description of what LangChain is before we delve into
[00:01:09.680 --> 00:01:10.680]   the details.
[00:01:10.680 --> 00:01:11.680]   Yeah, absolutely.
[00:01:11.680 --> 00:01:16.960]   So LangChain is a framework for building applications powered by large language models.
[00:01:16.960 --> 00:01:21.680]   So we've got Python and JavaScript packages.
[00:01:21.680 --> 00:01:28.040]   They provide a lot of the components, so the individual modules with some standard interfaces
[00:01:28.040 --> 00:01:32.600]   and standard abstractions around what each module and component is.
[00:01:32.600 --> 00:01:35.800]   And then we also have a bunch of chains, which are basically ways of stringing together those
[00:01:35.800 --> 00:01:41.040]   components to achieve particular basically end-to-end applications.
[00:01:41.040 --> 00:01:43.960]   So an example of an end-to-end application would be like question answering over your
[00:01:43.960 --> 00:01:48.320]   documents or something, which combines a bunch of different components from a language model
[00:01:48.320 --> 00:01:53.560]   to a vector store to embeddings to prompts and strings them together in a particular
[00:01:53.560 --> 00:01:54.560]   way.
[00:01:54.560 --> 00:01:56.840]   And yeah, so that's basically what LangChain is.
[00:01:56.840 --> 00:02:00.960]   I guess, could you go into a little more detail about what it does, like maybe just using
[00:02:00.960 --> 00:02:03.560]   that example that you just gave?
[00:02:03.560 --> 00:02:04.560]   Yeah.
[00:02:04.560 --> 00:02:10.400]   So LangChain started by basically talking to a bunch of folks building stuff with language
[00:02:10.400 --> 00:02:13.400]   models and noticing that there was a lot of common abstractions in terms of what they
[00:02:13.400 --> 00:02:15.440]   were doing.
[00:02:15.440 --> 00:02:19.760]   And so there were a lot of common abstractions around what they were doing in terms of constructing
[00:02:19.760 --> 00:02:24.040]   prompts, in terms of calling out to language models, in terms of calling out to embeddings,
[00:02:24.040 --> 00:02:26.320]   in terms of calling out to vector stores.
[00:02:26.320 --> 00:02:31.520]   So for the example that I kind of just gave of document question answering, the motivation
[00:02:31.520 --> 00:02:35.080]   for doing this is that you might, you want to have a chat experience over your documents.
[00:02:35.080 --> 00:02:41.240]   So chat GPT, but it knows specific contents of your documents or of particular documents
[00:02:41.240 --> 00:02:42.960]   that it wasn't trained on.
[00:02:42.960 --> 00:02:46.840]   And so what that general process looks like is you'll get the user question that comes
[00:02:46.840 --> 00:02:49.200]   in based on that user question.
[00:02:49.200 --> 00:02:51.080]   You'll then look up relevant documents.
[00:02:51.080 --> 00:02:56.020]   And so this is done in a generic retriever interface, but by far the most common way
[00:02:56.020 --> 00:02:59.080]   to do this is to do some type of vector lookup.
[00:02:59.080 --> 00:03:01.960]   So you'll create an embedding with an embedding model.
[00:03:01.960 --> 00:03:06.600]   You'll then look up based on that embedding, other embeddings that are similar in the vector
[00:03:06.600 --> 00:03:08.120]   space.
[00:03:08.120 --> 00:03:11.240]   And those are of the documents that you ingest in the ingest step.
[00:03:11.240 --> 00:03:13.720]   So these are all your documents that you initially pre-processed.
[00:03:13.720 --> 00:03:15.360]   You create, you chunk them up into chunks.
[00:03:15.360 --> 00:03:17.040]   You create embedding for each chunk.
[00:03:17.040 --> 00:03:18.680]   You store them in a vector store.
[00:03:18.680 --> 00:03:21.560]   Now you've got this question that comes in, you look up similar vectors, you get those
[00:03:21.560 --> 00:03:23.000]   documents back.
[00:03:23.000 --> 00:03:26.320]   And then basically you construct your prompt in a particular way where you say, "Hey, this
[00:03:26.320 --> 00:03:28.080]   is the user question.
[00:03:28.080 --> 00:03:32.600]   I want to answer it according to the information in these documents."
[00:03:32.600 --> 00:03:36.440]   And you pass it the four documents that it retrieves.
[00:03:36.440 --> 00:03:39.540]   And then you pass that to the language model and then you get back a response.
[00:03:39.540 --> 00:03:45.320]   So there's this flow of going from question to embedding to documents that you get back
[00:03:45.320 --> 00:03:50.120]   to constructed prompts to calling the language model to getting a response.
[00:03:50.120 --> 00:03:53.640]   And sometimes that response can also have some structure to it, which you might parse
[00:03:53.640 --> 00:03:54.640]   on the other end.
[00:03:54.640 --> 00:03:57.280]   So it might have an answer and then some sources.
[00:03:57.280 --> 00:04:02.600]   And so all of these components are individual modules in link chain that you can string
[00:04:02.600 --> 00:04:04.120]   together and swap out really easily.
[00:04:04.120 --> 00:04:06.280]   So you can swap out the embedding model that you use.
[00:04:06.280 --> 00:04:08.200]   You can swap out the language model that you use.
[00:04:08.200 --> 00:04:12.120]   You can swap out the vector store that you use.
[00:04:12.120 --> 00:04:18.080]   And yeah, so the idea is to make it really easy to have these building blocks, swap them
[00:04:18.080 --> 00:04:21.320]   out for each other, assemble them, and then have some pre-run templates to get started
[00:04:21.320 --> 00:04:22.320]   with.
[00:04:22.320 --> 00:04:26.720]   But so I guess it's important to say though, that you don't actually make any of that underlying
[00:04:26.720 --> 00:04:27.720]   stuff.
[00:04:27.720 --> 00:04:31.720]   Like you're not making a vector store and you're not making an LLM.
[00:04:31.720 --> 00:04:35.240]   You're kind of like this abstraction layer on top of it.
[00:04:35.240 --> 00:04:36.240]   Yeah.
[00:04:36.240 --> 00:04:39.640]   So I would say for like half the stuff that we have, we don't make.
[00:04:39.640 --> 00:04:42.440]   So we don't make the LLMs, we don't make the embedding models, we don't make the vector
[00:04:42.440 --> 00:04:44.040]   stores.
[00:04:44.040 --> 00:04:47.200]   For the other half of the stuff, and these are kind of like newer, maybe like smaller
[00:04:47.200 --> 00:04:51.520]   things that are starting to arise out of working with language models, we actually do have
[00:04:51.520 --> 00:04:52.520]   in-house.
[00:04:52.520 --> 00:04:55.720]   So we have a bunch of different ways of splitting text, because now this is really important
[00:04:55.720 --> 00:04:57.640]   because of the context limits.
[00:04:57.640 --> 00:04:59.480]   And there just wasn't a good library for this.
[00:04:59.480 --> 00:05:02.040]   So we have like 10 different methods for doing this.
[00:05:02.040 --> 00:05:06.040]   We have a bunch of document loaders to load documents from other sources.
[00:05:06.040 --> 00:05:09.440]   We have a bunch of stuff related to prompts, because again, prompts weren't really around
[00:05:09.440 --> 00:05:10.440]   until recently.
[00:05:10.440 --> 00:05:14.680]   So we have a lot of different prompt templates, and we have a lot of output parsers as well,
[00:05:14.680 --> 00:05:18.920]   which parse the output of language models into a structured format.
[00:05:18.920 --> 00:05:23.160]   So yeah, it kind of depends on the component, but some of them we just wrap and provide
[00:05:23.160 --> 00:05:24.160]   a standard interface.
[00:05:24.160 --> 00:05:27.160]   And then others, we actually do have implementations of ourselves.
[00:05:27.160 --> 00:05:32.920]   It's interesting though, like you've built a library that's become one of the most popular
[00:05:32.920 --> 00:05:36.800]   libraries of all time very quickly.
[00:05:36.800 --> 00:05:43.040]   Do you have a sense of what sort of drives the popularity of LangChain?
[00:05:43.040 --> 00:05:49.040]   I mean, I think there's definitely a big tailwind of everyone's just excited to be building
[00:05:49.040 --> 00:05:52.480]   with language models, and there's real value that they can unlock.
[00:05:52.480 --> 00:05:59.280]   And I do think there is a gap, especially when LangChain started, between the underlying
[00:05:59.280 --> 00:06:03.200]   APIs for the language models and then the end user experiences.
[00:06:03.200 --> 00:06:07.920]   And I think LangChain aimed to kind of like make going from the API to the end user experience
[00:06:07.920 --> 00:06:09.840]   as easy as possible.
[00:06:09.840 --> 00:06:15.580]   And so I think, yeah, honestly, I think I'd attribute it to one, just like really good
[00:06:15.580 --> 00:06:18.280]   now models that empower a lot of these things.
[00:06:18.280 --> 00:06:23.100]   And there's just a lot of interest and useful applications that you can create with them.
[00:06:23.100 --> 00:06:26.380]   And then two, I think a lot of the goal has always been to make creating these applications
[00:06:26.380 --> 00:06:27.380]   as easy as possible.
[00:06:27.380 --> 00:06:30.680]   And that's kind of what we've been always optimizing for.
[00:06:30.680 --> 00:06:36.260]   And so hopefully people kind of like are drawn to LangChain because it does provide a way
[00:06:36.260 --> 00:06:41.860]   of quickly getting started and building these more complex applications.
[00:06:41.860 --> 00:06:44.620]   Like if you're just going to make a simple call to a language model, you honestly don't
[00:06:44.620 --> 00:06:46.180]   need LangChain for a lot of things.
[00:06:46.180 --> 00:06:49.180]   It's when you start getting into some of the more complex applications that we provide
[00:06:49.180 --> 00:06:53.700]   some templating and scaffolding for that I think it really shines.
[00:06:53.700 --> 00:06:57.620]   So you think the main thing is kind of the chaining of multiple things together?
[00:06:57.620 --> 00:06:58.620]   Is that fair?
[00:06:58.620 --> 00:06:59.620]   Yeah.
[00:06:59.620 --> 00:07:09.020]   I think that's, I think chaining is definitely, chaining is definitely one of the main things.
[00:07:09.020 --> 00:07:16.140]   I think there's a lot of the utils we also have are around, like I would just say like
[00:07:16.140 --> 00:07:18.100]   more complex things.
[00:07:18.100 --> 00:07:22.580]   So like, you know, even if you're making a single call to the language model at the end
[00:07:22.580 --> 00:07:25.700]   of the day, like in the document question answering thing, you're doing a lot of work
[00:07:25.700 --> 00:07:29.300]   to carefully construct that prompt basically.
[00:07:29.300 --> 00:07:33.620]   And so I think like for a lot of the more complex prompt construction things, we have
[00:07:33.620 --> 00:07:37.780]   utils that are handy there.
[00:07:37.780 --> 00:07:42.820]   And there is another aspect, which I think is, there is another aspect, which I think
[00:07:42.820 --> 00:07:47.060]   will be more useful over time, which is basically the ability to have a standard interface for
[00:07:47.060 --> 00:07:49.060]   these models and swap them out.
[00:07:49.060 --> 00:07:52.500]   The reason I say it will be more useful over time is because to be honest at the moment,
[00:07:52.500 --> 00:07:54.580]   everyone's using open AI.
[00:07:54.580 --> 00:07:57.500]   But I think like as other models become more and more viable, and I think we're starting
[00:07:57.500 --> 00:08:02.060]   to see that with some of the anthropic models, having the standard interface to easily swap
[00:08:02.060 --> 00:08:05.820]   them in and out, I think that will actually be a pretty big value add as well.
[00:08:05.820 --> 00:08:08.700]   What's the complexity that arises in the prompt construction?
[00:08:08.700 --> 00:08:11.780]   Like could you give me a little bit of a flavor of that?
[00:08:11.780 --> 00:08:18.220]   I think there's, so there's a few different things that generally go into like a prompt
[00:08:18.220 --> 00:08:19.780]   construction bit.
[00:08:19.780 --> 00:08:21.820]   There's a base kind of like instruction set.
[00:08:21.820 --> 00:08:25.140]   And so this could be like the system message that you have with like chat GPT.
[00:08:25.140 --> 00:08:26.660]   That's a good way of thinking about it.
[00:08:26.660 --> 00:08:32.980]   But even with like the normal LLMs, you still have some kind of like base instruction set.
[00:08:32.980 --> 00:08:36.500]   And that's generally fixed by the application developer.
[00:08:36.500 --> 00:08:38.740]   Then you probably have like a user input.
[00:08:38.740 --> 00:08:42.180]   And so this is like the real time input of what they type into the chat box or something
[00:08:42.180 --> 00:08:44.340]   like that.
[00:08:44.340 --> 00:08:50.580]   And then you have a bunch of data that you gather based on the user input or metadata
[00:08:50.580 --> 00:08:53.820]   associated with the user input to also put in the prompt.
[00:08:53.820 --> 00:08:59.500]   And so an example of this is like the relevant documents that you might want to do question
[00:08:59.500 --> 00:09:00.500]   answering over.
[00:09:00.500 --> 00:09:05.860]   And so that's a really simple example of getting other data that's based on the user input
[00:09:05.860 --> 00:09:07.900]   and putting it in the prompt.
[00:09:07.900 --> 00:09:13.100]   I think other examples as you start to get more and more complex are when you start to
[00:09:13.100 --> 00:09:21.940]   do things like personalization for specific users, you could basically look up attributes
[00:09:21.940 --> 00:09:24.860]   about that user and put them into the prompt as well.
[00:09:24.860 --> 00:09:29.780]   And so it's not based on the user input, but it's about the user and it's not in the system
[00:09:29.780 --> 00:09:30.780]   message.
[00:09:30.780 --> 00:09:35.060]   And so then, yeah, I think that's a good example of adding on top of kind of like the retrieve
[00:09:35.060 --> 00:09:36.060]   documents.
[00:09:36.060 --> 00:09:43.260]   And then another kind of like another source of information that you often want to put
[00:09:43.260 --> 00:09:45.620]   in prompts is basically previous interactions.
[00:09:45.620 --> 00:09:50.700]   And these can be previous interactions between the user and the AI.
[00:09:50.700 --> 00:09:53.780]   And so this is like a conversation you want to be able to ask follow up questions.
[00:09:53.780 --> 00:09:57.940]   You want to be able to ask questions about things that were answered previously.
[00:09:57.940 --> 00:10:04.020]   But this can also be previous interactions between the AI and another system like a tool.
[00:10:04.020 --> 00:10:08.300]   So when it uses a search engine, it should know like, hey, previously I typed this into
[00:10:08.300 --> 00:10:09.980]   the search engine.
[00:10:09.980 --> 00:10:13.460]   I got back a response and that's a previous step.
[00:10:13.460 --> 00:10:17.940]   And so, yeah, I think you pretty quickly start to, again, for simple applications where you
[00:10:17.940 --> 00:10:23.180]   maybe just like use only the user input, it's pretty easy to call out to the language model
[00:10:23.180 --> 00:10:24.180]   by itself.
[00:10:24.180 --> 00:10:27.940]   But when you start doing this more kind of like complex kind of like prompt construction,
[00:10:27.940 --> 00:10:31.100]   I think that's where some of like the prompt templates and prompt helpers.
[00:10:31.100 --> 00:10:32.540]   Oh, there's another thing as well.
[00:10:32.540 --> 00:10:36.940]   I see there's so many that I keep on forgetting all of them, but like a few short examples,
[00:10:36.940 --> 00:10:41.540]   basically examples where you tell the language model like this is an input.
[00:10:41.540 --> 00:10:43.700]   This is what the correct output is.
[00:10:43.700 --> 00:10:44.700]   This is another input.
[00:10:44.700 --> 00:10:46.180]   This is another correct output.
[00:10:46.180 --> 00:10:52.580]   And the language model uses that to basically guide what it does the next time around.
[00:10:52.580 --> 00:10:55.260]   And so we have a lot of utils for kind of like working with all those different pieces
[00:10:55.260 --> 00:10:58.300]   and then constructing them in kind of like a standard way.
[00:10:58.300 --> 00:10:59.300]   All right.
[00:10:59.300 --> 00:11:00.300]   Well, thank you.
[00:11:00.300 --> 00:11:01.300]   Thank you.
[00:11:01.300 --> 00:11:03.160]   So that definitely grounds this discussion.
[00:11:03.160 --> 00:11:08.220]   But before we get too far into it, I really wanted to hear the story of Langchain.
[00:11:08.220 --> 00:11:12.420]   I mean, you've had this kind of massive success.
[00:11:12.420 --> 00:11:14.620]   Like what were you thinking when you built it?
[00:11:14.620 --> 00:11:18.740]   Like what were you trying to do and like what changed as the product started to take off?
[00:11:18.740 --> 00:11:23.420]   So Langchain started, I think I was looking this up the other day, the first release or
[00:11:23.420 --> 00:11:26.620]   the first tweet, because everything's a tweet these days.
[00:11:26.620 --> 00:11:30.860]   The first tweet was I think October 24th or 25th of 2002.
[00:11:30.860 --> 00:11:36.380]   So about seven months ago now, eight months, something like that.
[00:11:36.380 --> 00:11:42.260]   And the context for that was I knew I was leaving my current job.
[00:11:42.260 --> 00:11:43.580]   I was exploring in the space.
[00:11:43.580 --> 00:11:47.420]   I wasn't sure at all what I wanted to do next.
[00:11:47.420 --> 00:11:50.500]   At this point, language models were starting to become a thing.
[00:11:50.500 --> 00:11:54.660]   I went to a bunch of meetups, chatted with a bunch of folks who were building things
[00:11:54.660 --> 00:11:56.820]   and saw what I thought some common abstractions were.
[00:11:56.820 --> 00:12:00.700]   And so I put them in a Python package, not intending to start a company around it or
[00:12:00.700 --> 00:12:04.780]   anything like that, basically using it as a way to further explore the space and basically
[00:12:04.780 --> 00:12:08.060]   get to know what like interesting things could be.
[00:12:08.060 --> 00:12:11.580]   And it turned out that this package itself was pretty interesting to people.
[00:12:11.580 --> 00:12:14.500]   And so got some positive feedback on that.
[00:12:14.500 --> 00:12:20.500]   I was also having a lot of fun just working with it and so spent some more time on it.
[00:12:20.500 --> 00:12:24.060]   And then I would say after like a month or so, I realized that there was probably like
[00:12:24.060 --> 00:12:30.660]   a decent kind of like opportunity here to build something really cool and really awesome.
[00:12:30.660 --> 00:12:39.540]   And so by the time I ended up leaving my job, which was in like mid January at that point,
[00:12:39.540 --> 00:12:42.740]   my plan was always to like spend a few months figuring out what I wanted to do next.
[00:12:42.740 --> 00:12:45.740]   But I knew it was worth working on this full time.
[00:12:45.740 --> 00:12:51.860]   So I teamed up with Ankush, my co-founder, who I'd worked with previously.
[00:12:51.860 --> 00:12:55.820]   And I think we incorporated in like late January, early February.
[00:12:55.820 --> 00:12:58.660]   And that's kind of like when it became a real company.
[00:12:58.660 --> 00:13:05.100]   And were you working on like building applications at the time or how did you even kind of, how
[00:13:05.100 --> 00:13:08.460]   did you even decide to build these specific abstractions besides the meetups that you're
[00:13:08.460 --> 00:13:09.460]   going to?
[00:13:09.460 --> 00:13:13.860]   Yeah, so my, so my background is in kind of like ML and MLOps.
[00:13:13.860 --> 00:13:19.820]   So I worked at Kencho, a fintech company, doing some time series and NLP stuff for a
[00:13:19.820 --> 00:13:21.740]   few years right out of college.
[00:13:21.740 --> 00:13:25.460]   And then for the past three and a half years, I was working at Robust Intelligence, which
[00:13:25.460 --> 00:13:28.060]   was an MLOps company.
[00:13:28.060 --> 00:13:32.300]   And so I did some application stuff at Kencho and then I did more kind of like MLOps and
[00:13:32.300 --> 00:13:35.240]   tooling stuff at Robust.
[00:13:35.240 --> 00:13:38.620]   And I knew I liked kind of like, yeah, I like the tooling side of things.
[00:13:38.620 --> 00:13:39.980]   I like the MLOps side of things.
[00:13:39.980 --> 00:13:45.780]   I think I just enjoy building tools to help other people do machine learning in a more
[00:13:45.780 --> 00:13:47.180]   easy way or something like that.
[00:13:47.180 --> 00:13:49.500]   So, so I knew I always liked that.
[00:13:49.500 --> 00:13:54.340]   And then there was a hackathon at Robust where I built an application as well that informed
[00:13:54.340 --> 00:13:58.460]   some of, or that, that I think that was the first, I think I built that before Linkgrain
[00:13:58.460 --> 00:13:59.460]   and used it.
[00:13:59.460 --> 00:14:02.940]   Yeah, that, that was, that was one of the things that like piqued my curiosity in the
[00:14:02.940 --> 00:14:07.400]   space and, and I actually built a question answering bot over our Notion documentation.
[00:14:07.400 --> 00:14:11.780]   So very related to a lot of the stuff that we've been talking about.
[00:14:11.780 --> 00:14:16.220]   Were there any tools that inspired Langchain?
[00:14:16.220 --> 00:14:19.740]   I mean, this is a question, like I've been, I've been trying to think of like parallels
[00:14:19.740 --> 00:14:23.860]   for, for Langchain a lot recently as we try to figure out like what exactly the company
[00:14:23.860 --> 00:14:24.860]   is and stuff like that.
[00:14:24.860 --> 00:14:29.300]   Honestly, I don't know if there's any like amazing parallels.
[00:14:29.300 --> 00:14:34.660]   Like I think there's certainly kind of like some, there's certainly some kind of like
[00:14:34.660 --> 00:14:39.860]   parallels to like some type of like PyTorch or TensorFlow or Keras type thing where you've
[00:14:39.860 --> 00:14:45.420]   got this, this abstraction layer on top of the underlying tensors.
[00:14:45.420 --> 00:14:49.940]   But I don't think those are perfect because I think a lot of Langchains about like connectivity
[00:14:49.940 --> 00:14:54.220]   and connecting different things and then, and less so than like complex kind of like
[00:14:54.220 --> 00:14:56.980]   graph computations or anything like that.
[00:14:56.980 --> 00:15:02.100]   I guess I'm not necessarily, you know, demanding like a parallel in terms of like where it
[00:15:02.100 --> 00:15:03.500]   sits in the stack or something.
[00:15:03.500 --> 00:15:08.100]   But I'm curious, like maybe just like what other tools did you like using that might
[00:15:08.100 --> 00:15:13.100]   have kind of influenced the design of the docs or the API or the structure?
[00:15:13.100 --> 00:15:15.180]   Is there, is there anything that comes to mind like that?
[00:15:15.180 --> 00:15:16.700]   That's a good question.
[00:15:16.700 --> 00:15:22.880]   There, there, I mean, I think a lot of the design of, I mean, honestly, I think a lot
[00:15:22.880 --> 00:15:27.260]   of the design was really strongly influenced by my time at Kensho.
[00:15:27.260 --> 00:15:29.860]   Like I think Kensho had amazing engineers.
[00:15:29.860 --> 00:15:36.820]   And I think a lot of the things in terms of, yeah, design principles and, and, and, and
[00:15:36.820 --> 00:15:39.220]   what to document and how to think about things.
[00:15:39.220 --> 00:15:42.300]   And I don't think, I don't think Linkgrain lives up to the same standards that Kensho
[00:15:42.300 --> 00:15:43.300]   had internally, to be honest.
[00:15:43.300 --> 00:15:47.100]   But I think a lot of it comes, comes mostly from that.
[00:15:47.100 --> 00:15:51.420]   I think, yeah, I mean, and another thing, like one of the reasons I started at open
[00:15:51.420 --> 00:15:53.940]   source was to work with people in the open again.
[00:15:53.940 --> 00:15:56.540]   And a lot of those initial people were actually from Kensho.
[00:15:56.540 --> 00:16:00.140]   So if you look at the initial contributor log, there's a lot of ex-Kenshons there.
[00:16:00.140 --> 00:16:04.540]   And yeah, I mean, I was definitely really kind of like influenced by a lot of the engineering
[00:16:04.540 --> 00:16:05.540]   culture there.
[00:16:05.540 --> 00:16:10.780]   And, and a lot of their, I know a lot of the learnings I took from them.
[00:16:10.780 --> 00:16:12.300]   So that's definitely kind of like the main source.
[00:16:12.300 --> 00:16:16.140]   Can you give me an example of, of one of the learnings or best practices that you, you
[00:16:16.140 --> 00:16:17.140]   brought with you?
[00:16:17.140 --> 00:16:18.140]   Yeah.
[00:16:18.140 --> 00:16:25.140]   I mean, I still remember one of the, basically in this was, this was from a tech talk, I
[00:16:25.140 --> 00:16:29.860]   think that someone gave at Kensho, but he was a very good friend of some of the engineers
[00:16:29.860 --> 00:16:30.860]   there.
[00:16:30.860 --> 00:16:34.740]   And it was basically around just like inputs and outputs should kind of be like as simple
[00:16:34.740 --> 00:16:35.740]   as possible.
[00:16:35.740 --> 00:16:39.940]   And, and this is like a really small thing, but like, you know, if you have like a list
[00:16:39.940 --> 00:16:43.420]   of like a thousand or if you have, if you have like 10 different arguments, they can
[00:16:43.420 --> 00:16:44.620]   sometimes like conflict each other.
[00:16:44.620 --> 00:16:48.100]   There can be kind of like conflicting information and then you have to do like validations inside.
[00:16:48.100 --> 00:16:51.820]   And it's like, Oh, like if this is this, and then this can't be this, and then this other
[00:16:51.820 --> 00:16:54.020]   thing can't be this.
[00:16:54.020 --> 00:16:59.380]   And so I think a lot of the interfaces and, and, you know, there's, there's definitely,
[00:16:59.380 --> 00:17:02.780]   you know, I haven't adhered to this super stringently because of the pace of growth,
[00:17:02.780 --> 00:17:04.660]   but a lot of them try to be as simple as possible.
[00:17:04.660 --> 00:17:10.500]   So I think like an example of this is the, the retriever interface that we have is kind
[00:17:10.500 --> 00:17:14.700]   of just the, it's an in, the input's a single string and the output is a list of documents
[00:17:14.700 --> 00:17:15.700]   and that's really it.
[00:17:15.700 --> 00:17:20.060]   And then, and, and so like, you know, the, the other, other subclasses can kind of like
[00:17:20.060 --> 00:17:24.500]   configure them however they want, but the standard interface is really dead simple.
[00:17:24.500 --> 00:17:29.340]   It's made it incredibly easy to add all different types of retrievers from, from vector stores
[00:17:29.340 --> 00:17:35.300]   to like hosted retriever services because it is kind of like so simple and that.
[00:17:35.300 --> 00:17:40.500]   And so I think like, yeah, just simplicity is kind of like one big thing.
[00:17:40.500 --> 00:17:45.060]   Yeah, that's, that's probably like the main thing that jumps to mind.
[00:17:45.060 --> 00:17:46.380]   What about community building?
[00:17:46.380 --> 00:17:49.060]   Like you seem to have a real knack for it.
[00:17:49.060 --> 00:17:53.660]   Like, I mean, how do you think about that both in terms of the open source contributors
[00:17:53.660 --> 00:17:58.020]   and the sort of broader ecosystem of integrations that you're creating?
[00:17:58.020 --> 00:17:59.460]   Like I see you all over the place.
[00:17:59.460 --> 00:18:03.060]   So it seems like it is something that you're appear to be prioritizing.
[00:18:03.060 --> 00:18:04.060]   Yeah.
[00:18:04.060 --> 00:18:11.740]   I mean, I think, you know, it also just started like, or it, like I don't have a super strategic
[00:18:11.740 --> 00:18:12.740]   view on it, to be honest.
[00:18:12.740 --> 00:18:15.980]   Like, I think it started as an open source project.
[00:18:15.980 --> 00:18:20.060]   And part of the reason for making it open source was to work with a bunch of other folks.
[00:18:20.060 --> 00:18:24.820]   And so like, yeah, I absolutely prioritize like trying to get people like my friends
[00:18:24.820 --> 00:18:28.660]   to contribute and trying to encourage and random people on the internet to contribute.
[00:18:28.660 --> 00:18:31.780]   And that absolutely was again, cause I was just doing it for, for fun.
[00:18:31.780 --> 00:18:34.260]   It wasn't really like, I didn't know I was going to start a company.
[00:18:34.260 --> 00:18:35.260]   It wasn't strategic at all.
[00:18:35.260 --> 00:18:37.940]   It was very much like, I just want to work on this.
[00:18:37.940 --> 00:18:39.820]   There's this, there's a lot to do.
[00:18:39.820 --> 00:18:44.060]   I guess that's another part is I think there's so much to do in this space because it is
[00:18:44.060 --> 00:18:49.220]   so new and moving so fast that like, you know, trying to do everything yourself would be
[00:18:49.220 --> 00:18:50.220]   crazy.
[00:18:50.220 --> 00:18:54.540]   And so we, we, we love to partner with people all over and all these different areas have
[00:18:54.540 --> 00:18:58.340]   tons of different kinds of like integrations and players with them.
[00:18:58.340 --> 00:19:03.900]   And yeah, I think, you know, someone, someone said to me recently, like radical inclusivity,
[00:19:03.900 --> 00:19:06.620]   like, I don't know, we just partner with like basically everyone, cause I think there is
[00:19:06.620 --> 00:19:09.940]   so much to do in this space and we can't do it all.
[00:19:09.940 --> 00:19:13.660]   You know, in terms of community, like I think there was definitely inspired a lot by, by
[00:19:13.660 --> 00:19:14.660]   Huggy Face and Clem.
[00:19:14.660 --> 00:19:20.220]   I think he's done in the, in the company there has done a fantastic job at that.
[00:19:20.220 --> 00:19:24.780]   And I think that's probably like the, the, the gold standard in terms of community building,
[00:19:24.780 --> 00:19:29.820]   at least in the ML space where, which is where I come from.
[00:19:29.820 --> 00:19:31.380]   And, but yeah, I don't know.
[00:19:31.380 --> 00:19:34.220]   I, I, you know, I'd never done open source before.
[00:19:34.220 --> 00:19:39.140]   I can't claim to have a super kind of like strategic thought about it.
[00:19:39.140 --> 00:19:44.980]   Really that my kind of thought there was just like build, build useful things and be nice
[00:19:44.980 --> 00:19:45.980]   about it.
[00:19:45.980 --> 00:19:49.100]   And that'll kind of, you know, cause again, I was just doing it for kind of like fun.
[00:19:49.100 --> 00:19:50.420]   And so that's kind of how it started.
[00:19:50.420 --> 00:19:52.260]   And yeah, I don't know.
[00:19:52.260 --> 00:19:56.260]   I love, I love going to events and I love chatting with folks who are kind of like building
[00:19:56.260 --> 00:19:59.420]   with it, building similar things, just hearing how people are thinking about it.
[00:19:59.420 --> 00:20:01.340]   It's I got a lot of energy from that as well.
[00:20:01.340 --> 00:20:02.980]   So it's yeah, just kind of fun.
[00:20:02.980 --> 00:20:07.060]   I would imagine one of the challenges that happens as the project gets bigger and more
[00:20:07.060 --> 00:20:12.460]   people get involved is keeping sort of simple, consistent interfaces and consistency gets
[00:20:12.460 --> 00:20:13.460]   really hard.
[00:20:13.460 --> 00:20:14.460]   Like, how do you think about that?
[00:20:14.460 --> 00:20:17.780]   Like, how do you even stay on top of all the pull requests that come in?
[00:20:17.780 --> 00:20:18.780]   Yeah.
[00:20:18.780 --> 00:20:23.820]   I mean, you know, that, that's it, it's hard that that's, that's, that's part of the reason
[00:20:23.820 --> 00:20:28.260]   why we turned it into a company and raise money as well was to bring on people to help
[00:20:28.260 --> 00:20:29.900]   out with that.
[00:20:29.900 --> 00:20:35.780]   I think like the and, and honestly the answer is still, we don't do a good enough job as
[00:20:35.780 --> 00:20:40.500]   we should be as I think as we speak, there's over like 200 open PRs and over a thousand
[00:20:40.500 --> 00:20:41.500]   issues.
[00:20:41.500 --> 00:20:45.620]   And so there absolutely is kind of like a big backlog of things to go through.
[00:20:45.620 --> 00:20:50.380]   I think the way that we've started thinking about it is there are certain areas where
[00:20:50.380 --> 00:20:55.300]   we know we, we, we need and want a lot of community contributions and help.
[00:20:55.300 --> 00:20:59.280]   And these can be, these are mostly around things where there's a long tail of things
[00:20:59.280 --> 00:21:05.100]   like document loaders, vector source to some extent, LLM integrations to some extent, prompts
[00:21:05.100 --> 00:21:06.700]   is a very good example.
[00:21:06.700 --> 00:21:10.220]   Basically things where there's a lot of different implementations and we don't have the best
[00:21:10.220 --> 00:21:11.220]   ideas for everything.
[00:21:11.220 --> 00:21:15.220]   And we want to make it really easy for people to contribute those.
[00:21:15.220 --> 00:21:19.580]   So we try to spend most of our energy on like setting up kind of like what the right abstractions
[00:21:19.580 --> 00:21:23.900]   there are, making sure they don't get, get changed to kind of setting up contributor
[00:21:23.900 --> 00:21:26.100]   guidelines for those specific areas.
[00:21:26.100 --> 00:21:30.740]   And then, and then really yeah, working with the community to add a bunch of different
[00:21:30.740 --> 00:21:31.740]   implementations.
[00:21:31.740 --> 00:21:36.220]   So, so more on kind of like the framework and framing rather than specific implementation
[00:21:36.220 --> 00:21:37.220]   of things.
[00:21:37.220 --> 00:21:45.260]   How has your experience changed as you've kind of gone from like a small project with
[00:21:45.260 --> 00:21:50.700]   like very little attention to, you know, getting lots of, you know, VC funding and, and kind
[00:21:50.700 --> 00:21:52.220]   of really being in the spotlight?
[00:21:52.220 --> 00:21:56.380]   I don't actually know if it's changed that much, to be honest.
[00:21:56.380 --> 00:22:03.260]   I think when we were, you know, I think when we were raising money, a lot of our initial
[00:22:03.260 --> 00:22:07.580]   kind of like, a lot of our pitchers, basically it's still extremely early in the space.
[00:22:07.580 --> 00:22:10.180]   It's still moving like extremely fast.
[00:22:10.180 --> 00:22:14.540]   We know there's this like massive need for, for some framework for building these things
[00:22:14.540 --> 00:22:17.620]   that that's kind of like what we want to want to want to focus on.
[00:22:17.620 --> 00:22:19.100]   And we've continued to do that.
[00:22:19.100 --> 00:22:22.940]   Like basically everything we're working on is, is open source and either the Python or
[00:22:22.940 --> 00:22:26.540]   JavaScript kind of like frameworks.
[00:22:26.540 --> 00:22:32.640]   I think there's so for like in internally, I don't know if it's kind of like changed
[00:22:32.640 --> 00:22:35.460]   our mindset too much.
[00:22:35.460 --> 00:22:43.480]   I also think like, you know, I don't think like, I think we have had kind of like a good
[00:22:43.480 --> 00:22:49.620]   amount of success so far, but I don't think that internally we're I don't know.
[00:22:49.620 --> 00:22:53.620]   I think we're still kind of like, we're 0.0 point something for a reason, is I'll just
[00:22:53.620 --> 00:22:56.780]   say that like, we're very cognizant that things are moving really fast.
[00:22:56.780 --> 00:23:00.420]   We're not kind of like getting, we hope we're not getting too set in the abstractions that
[00:23:00.420 --> 00:23:01.420]   we have.
[00:23:01.420 --> 00:23:05.500]   We want to like really keep up to date with some of the research papers that are that
[00:23:05.500 --> 00:23:09.980]   are going on and continue to add those in kind of like as rapidly as possible and really
[00:23:09.980 --> 00:23:15.780]   still have a lot of what people liked about linkchain from the start.
[00:23:15.780 --> 00:23:24.300]   And so, yeah, like, you know, I think I don't know if there's that much that has has changed.
[00:23:24.300 --> 00:23:29.900]   Do you have any particular real world examples of applications built on linkchain that you're
[00:23:29.900 --> 00:23:31.820]   particularly proud of?
[00:23:31.820 --> 00:23:38.980]   Yeah, I think the so by far the most common ones are kind of like the ones that combine.
[00:23:38.980 --> 00:23:44.660]   So I'd say there's two categories of applications that linkchain is really good at enabling.
[00:23:44.660 --> 00:23:50.140]   One is ones that combine that basically personalize kind of like LLMs to your data.
[00:23:50.140 --> 00:23:52.180]   And this can be adding as contextual information.
[00:23:52.180 --> 00:23:55.700]   So this can be like chat over your documents, chat over your SQL database, chat over your
[00:23:55.700 --> 00:23:56.700]   CSVs.
[00:23:56.700 --> 00:24:00.700]   It can also be personalized in the sense of like adding in few shot examples.
[00:24:00.700 --> 00:24:06.380]   So I think there are there are a lot of I mean, there are a lot of examples of chat
[00:24:06.380 --> 00:24:07.380]   over your documents.
[00:24:07.380 --> 00:24:11.060]   By now, I think they're out there out in Twitter all the time.
[00:24:11.060 --> 00:24:15.420]   I think like, you know, if I if I had to pick some of those that that I like best, I mean,
[00:24:15.420 --> 00:24:20.300]   I think there was a really good example by a student at Williams College that did one
[00:24:20.300 --> 00:24:26.940]   where it was it was it was kind of like using information around like rare kind of like
[00:24:26.940 --> 00:24:32.940]   diseases basically that chat GPT just didn't know about or didn't have kind of like the
[00:24:32.940 --> 00:24:33.940]   depth of knowledge.
[00:24:33.940 --> 00:24:37.860]   And so it was grounding it in more kind of like specific focused literature on that.
[00:24:37.860 --> 00:24:43.140]   And I think that's a really good example of, you know, I think obviously, like question
[00:24:43.140 --> 00:24:45.380]   answering over your notion is very practical and applied.
[00:24:45.380 --> 00:24:48.300]   But I think this was a really cool example of something that has a lot of really like
[00:24:48.300 --> 00:24:50.860]   good kind of like societal benefits and stuff like that.
[00:24:50.860 --> 00:24:51.860]   So I like that.
[00:24:51.860 --> 00:24:52.860]   That's really cool.
[00:24:52.860 --> 00:24:53.860]   Do you have a link to that?
[00:24:53.860 --> 00:24:54.860]   Or how can we find that?
[00:24:54.860 --> 00:24:55.860]   We should put that?
[00:24:55.860 --> 00:24:56.860]   Yeah, I can.
[00:24:56.860 --> 00:24:57.860]   We actually did a guest blog post by them on the link chain blog.
[00:24:57.860 --> 00:24:58.860]   Oh, cool.
[00:24:58.860 --> 00:25:01.860]   I can find a link and it's linked to their original stuff there.
[00:25:01.860 --> 00:25:02.860]   Yeah.
[00:25:02.860 --> 00:25:06.620]   And and then the other type of applications I think link chain is really good at enabling
[00:25:06.620 --> 00:25:08.980]   their kind of like agentic applications.
[00:25:08.980 --> 00:25:12.300]   So basically things where you use the language model as a reasoning engine and it kind of
[00:25:12.300 --> 00:25:15.700]   like decides what to do.
[00:25:15.700 --> 00:25:19.780]   And and these kind of like involve tool usage.
[00:25:19.780 --> 00:25:26.460]   And they also involve they also involve they can involve some of like the contextual stuff
[00:25:26.460 --> 00:25:27.460]   as well.
[00:25:27.460 --> 00:25:33.140]   And and it also involves a little bit of like memory and agentic memory and kind of like
[00:25:33.140 --> 00:25:34.140]   remembering things.
[00:25:34.820 --> 00:25:38.700]   I mean, so one of the more creative applications this doesn't this isn't super heavy on the
[00:25:38.700 --> 00:25:43.900]   tool usage, but in terms of like deciding what to do next, there was a Dungeons and
[00:25:43.900 --> 00:25:49.540]   Dragons kind of like implementation where someone basically used it as a dungeon master.
[00:25:49.540 --> 00:25:52.420]   And then, you know, the humans would kind of like, you know, say what they wanted to
[00:25:52.420 --> 00:25:53.420]   do.
[00:25:53.420 --> 00:25:55.700]   And then the dungeon master would kind of like make up what happened and it would remember
[00:25:55.700 --> 00:25:57.420]   the game state and remember different things.
[00:25:57.420 --> 00:26:02.340]   And so I thought that was kind of like really, really creative.
[00:26:02.340 --> 00:26:05.900]   In terms of ones that are really heavy on tool use, because that, again, is like one
[00:26:05.900 --> 00:26:12.620]   of the main kind of like value things that that language provides.
[00:26:12.620 --> 00:26:17.340]   I mean, there's an example today of I mean, if we're being honest, I think a lot of these
[00:26:17.340 --> 00:26:19.900]   applications haven't really made it into production yet.
[00:26:19.900 --> 00:26:23.020]   And I think this is this is one of the big things that we're focusing on as well.
[00:26:23.020 --> 00:26:28.300]   So there are some cooler demos, but I don't know if I've seen and I were working with
[00:26:28.300 --> 00:26:33.380]   a bunch of companies that are close to getting them in production, or some that have them
[00:26:33.380 --> 00:26:37.100]   in production, but are just like still in stealth, basically.
[00:26:37.100 --> 00:26:39.700]   So I don't know if there are any like super public ones.
[00:26:39.700 --> 00:26:44.060]   I wanted to ask you about, there was kind of like a popular post on Hacker News that
[00:26:44.060 --> 00:26:49.540]   you might have seen where it was like, I could make Lang chain and like 100 lines of code,
[00:26:49.540 --> 00:26:54.860]   which I felt like followed a rich tradition of amazing stuff like, you know, Dropbox and
[00:26:54.860 --> 00:26:58.220]   segment, you know, getting made fun of for being like, too simple, like, oh, I could
[00:26:58.220 --> 00:27:02.740]   do this in like three lines of X, like, I almost feel like that made me think, oh, like
[00:27:02.740 --> 00:27:08.700]   chance more likely to be successful than I previously thought based on my pattern recognition.
[00:27:08.700 --> 00:27:10.740]   But you know, I think there was like, kind of two notable things.
[00:27:10.740 --> 00:27:14.620]   I mean, one was like, interesting that, you know, the person felt like they could replicate
[00:27:14.620 --> 00:27:18.100]   a lot of the text so simply, and I'm curious what you thought.
[00:27:18.100 --> 00:27:22.020]   But then it seemed like a lot of the comments on the Hacker News article were like, hey,
[00:27:22.020 --> 00:27:26.620]   we're using Lang chain now, you know, but we plan to not use it like when we go into
[00:27:26.620 --> 00:27:28.060]   production.
[00:27:28.060 --> 00:27:31.740]   And I'm curious if that like, I'm curious your reaction to that, like if it like bothered
[00:27:31.740 --> 00:27:35.020]   you or you think they might keep it in production or that you think there's like, you know,
[00:27:35.020 --> 00:27:39.580]   more features that you would, that you'd want to add, or maybe Lang chain is more for an
[00:27:39.580 --> 00:27:43.980]   experimental mode, not unlike, you know, weights and biases, at least in the beginning.
[00:27:43.980 --> 00:27:46.140]   Yeah, what do you, what do you think?
[00:27:46.140 --> 00:27:47.500]   Yeah, I think so.
[00:27:47.500 --> 00:27:53.260]   First one, like that the post, I mean, I think it is, you know, the, I think, if I'm remembering
[00:27:53.260 --> 00:27:57.420]   correctly, the core idea that it kind of like replicated was this React style prompting
[00:27:57.420 --> 00:28:06.780]   loop that came out in a paper in, by Xinyu in, I think it was like September or something
[00:28:06.780 --> 00:28:09.220]   like that of last year.
[00:28:09.220 --> 00:28:14.780]   And I think the, and I think there's a few things there.
[00:28:14.780 --> 00:28:21.180]   Like one, like I think the, it is as with many things kind of like in this field now,
[00:28:21.180 --> 00:28:24.700]   like it's, it is a really simple idea that empowers a lot of cool use cases.
[00:28:24.700 --> 00:28:27.980]   And so, you know, I don't like, yeah, you can do it in a hundred lines of code.
[00:28:27.980 --> 00:28:28.980]   That's really cool.
[00:28:28.980 --> 00:28:29.980]   I don't know.
[00:28:29.980 --> 00:28:33.180]   And I think the other thing is like, you know, obviously that's not all kind of like Lang
[00:28:33.180 --> 00:28:37.380]   chain does both in terms of like the functionality and other things.
[00:28:37.380 --> 00:28:44.180]   And I think that actually speaks to some of the value that we do provide in terms of kind
[00:28:44.180 --> 00:28:47.660]   of like, yeah, you can run it in a hundred lines of code.
[00:28:47.660 --> 00:28:50.180]   I don't think you can put something in production with a hundred lines of code.
[00:28:50.180 --> 00:28:52.340]   And I think there's a lot that still needs to go in production.
[00:28:52.340 --> 00:28:57.660]   And I think, and I think we're looking to kind of like provide that, that, that, that
[00:28:57.660 --> 00:28:58.660]   stuff.
[00:28:58.660 --> 00:29:01.980]   And so that actually goes to the second point as well, which is, I think it probably depends
[00:29:01.980 --> 00:29:05.660]   really heavily on the type of application that you're putting in production.
[00:29:05.660 --> 00:29:11.940]   So again, for like some of the simple applications where there's not a lot of kind of like complex
[00:29:11.940 --> 00:29:16.980]   prompt construction going on, or there's not a lot of chaining I think it's completely
[00:29:16.980 --> 00:29:19.420]   valid to not use Lang chain at all.
[00:29:19.420 --> 00:29:22.420]   And it's, I think it's definitely the case that maybe Lang chain provides some inspiration
[00:29:22.420 --> 00:29:25.100]   for getting started, but then it's not needed after that.
[00:29:25.100 --> 00:29:28.500]   For the more complex applications though, where there is kind of like either complex
[00:29:28.500 --> 00:29:34.220]   prompt construction going on or some of like the iterative stuff back and forth.
[00:29:34.220 --> 00:29:38.540]   I think there, I think getting those types of applications in production especially the
[00:29:38.540 --> 00:29:42.660]   more agentic stuff is, is more complicated than people think.
[00:29:42.660 --> 00:29:47.540]   Like I think it's pretty easy to make a Twitter demo and get some functionality there with,
[00:29:47.540 --> 00:29:49.580]   with or without Lang chain.
[00:29:49.580 --> 00:29:52.300]   But I think it is really difficult to get this in production and that that's actually
[00:29:52.300 --> 00:29:56.980]   like very much where we are focused and we are helping people kind of put things in production
[00:29:56.980 --> 00:29:57.980]   around that.
[00:29:57.980 --> 00:30:02.260]   So I think it is just a little bit of I don't know, I think there's a lot of nuance there
[00:30:02.260 --> 00:30:04.280]   basically and that's kind of like my take on it.
[00:30:04.280 --> 00:30:11.380]   When you think about the full API of Lang chain today, which abstractions do you feel
[00:30:11.380 --> 00:30:15.640]   like happy with and you feel are stable and like which ones do you feel like you want
[00:30:15.640 --> 00:30:18.240]   to keep working on and need to change?
[00:30:18.240 --> 00:30:24.040]   I think some of the abstractions around, I think the abstractions that are decent are
[00:30:24.040 --> 00:30:28.840]   the ones around like some of the, I think prompts are good.
[00:30:28.840 --> 00:30:33.000]   And then I think the document ingestion retrieval stuff is good.
[00:30:33.000 --> 00:30:35.960]   So we recently made a change about a month ago to make the retriever interface like really
[00:30:35.960 --> 00:30:37.200]   simple as I talked about before.
[00:30:37.200 --> 00:30:42.280]   And I think we got a bunch of really positive feedback from people that were doing retrieval
[00:30:42.280 --> 00:30:46.080]   based things and they found it really easy to plug into and really easy to use.
[00:30:46.080 --> 00:30:48.920]   So I think the retriever abstraction is pretty good there.
[00:30:48.920 --> 00:30:54.720]   I think prompts are also something where some of the simpler ones, prompts are maybe a mixed
[00:30:54.720 --> 00:30:55.720]   bag.
[00:30:55.720 --> 00:30:58.360]   I think we need to do some more stuff around like few shot prompting and make those good.
[00:30:58.360 --> 00:31:02.960]   But I think we actually have some solid abstractions in terms of the prompt classes, example selectors,
[00:31:02.960 --> 00:31:06.640]   output parsers, those types of basic functionalities there.
[00:31:06.640 --> 00:31:13.240]   I think the ones that I'm excited about kind of like working on are memory and agents.
[00:31:13.240 --> 00:31:18.840]   So I think agents, I think we have a pretty good abstraction now for like the React style
[00:31:18.840 --> 00:31:21.960]   thing.
[00:31:21.960 --> 00:31:25.560]   But I think there are a lot of new interesting techniques that are coming out as well.
[00:31:25.560 --> 00:31:28.400]   So there's actually one that I want to work on today, which is kind of like a plan and
[00:31:28.400 --> 00:31:31.680]   then solve style agent.
[00:31:31.680 --> 00:31:36.080]   And so I think there'll be pretty kind of like consistent, I think there'll be a lot
[00:31:36.080 --> 00:31:37.920]   of new innovation here basically.
[00:31:37.920 --> 00:31:41.880]   And so it's less than I'm not happy with our abstractions, but I'm just, I'm looking forward
[00:31:41.880 --> 00:31:44.480]   to adding to them basically.
[00:31:44.480 --> 00:31:46.760]   For memory, I'm not happy at all.
[00:31:46.760 --> 00:31:48.840]   Or that's an exaggeration.
[00:31:48.840 --> 00:31:51.520]   For memory, there's like things that I know we should probably change.
[00:31:51.520 --> 00:31:56.840]   And I think memory is another one where we're still trying to figure out kind of like what
[00:31:56.840 --> 00:32:01.160]   exactly does it mean for a chain or an agent to have memory and there's different types
[00:32:01.160 --> 00:32:02.160]   of memory.
[00:32:02.160 --> 00:32:03.760]   There's like chat history memory.
[00:32:03.760 --> 00:32:06.400]   There's like long-term memory.
[00:32:06.400 --> 00:32:10.320]   There's now like this really cool idea of like reflection, which you can do not only
[00:32:10.320 --> 00:32:18.240]   on, not only on kind of like steps or outputs of an agent, but on past observations, like
[00:32:18.240 --> 00:32:21.720]   update state almost.
[00:32:21.720 --> 00:32:24.320]   And so like, how does that tie into memory?
[00:32:24.320 --> 00:32:29.680]   And then there's also memory of not just like AI to human interactions, but AI to tool interactions
[00:32:29.680 --> 00:32:30.680]   and stuff like that.
[00:32:30.680 --> 00:32:37.880]   And so I think memory is one where I think there'll be a lot of progression.
[00:32:37.880 --> 00:32:43.120]   I guess one thing I'm curious about is evaluation, right?
[00:32:43.120 --> 00:32:47.920]   I mean, I know Lecture has an evaluation module, but it does seem like maybe the biggest unsolved
[00:32:47.920 --> 00:32:50.800]   problem that we hear at Weights & Biases.
[00:32:50.800 --> 00:32:56.360]   And even we hear things like, I was talking with the CEO of Repl.it, Amjad, and he mentioned
[00:32:56.360 --> 00:33:01.000]   that their LLM that they put into production, they do testing by vibes only, right?
[00:33:01.000 --> 00:33:04.880]   So basically what that means is they just kind of try it and see if it feels better
[00:33:04.880 --> 00:33:07.320]   or worse than the last version.
[00:33:07.320 --> 00:33:12.240]   And actually I see that all over the place and it just seems sort of like dying for improvement.
[00:33:12.240 --> 00:33:15.120]   Do you have thoughts on that and ways that you want to help there?
[00:33:15.120 --> 00:33:16.120]   Yeah.
[00:33:16.120 --> 00:33:21.200]   I mean, I think one of the, so I think there's a few things.
[00:33:21.200 --> 00:33:27.400]   One is like the way of, as silly as it sounds, like the vibes thing isn't crazy at all.
[00:33:27.400 --> 00:33:30.440]   And I've heard that from like multiple people where you kind of, and I think it's basically
[00:33:30.440 --> 00:33:36.320]   you kind of look at it and you gain an intuition for what's good and what's bad and where it
[00:33:36.320 --> 00:33:38.720]   might be messing up.
[00:33:38.720 --> 00:33:43.240]   And so I think things that help visualize what's going on under the hood, and I know
[00:33:43.240 --> 00:33:50.280]   Weights & Biases has done a bunch in this space recently, are really helpful for that.
[00:33:50.280 --> 00:33:56.280]   The other thing I'll say is, I am optimistic that we can do better than that.
[00:33:56.280 --> 00:34:00.280]   I think that's a perfectly fine start to be clear, but I think like the next step starts
[00:34:00.280 --> 00:34:04.360]   to become like, yeah, can you automate some of these vibes basically?
[00:34:04.360 --> 00:34:08.160]   And I think the thing that I'm most excited about there is basically using language models
[00:34:08.160 --> 00:34:12.280]   themselves to assess these vibes.
[00:34:12.280 --> 00:34:15.200]   I really like that saying, so I'm going to start using that a lot.
[00:34:15.200 --> 00:34:20.360]   But the idea is basically have a language model look at the output or have it look at
[00:34:20.360 --> 00:34:25.120]   the trajectory of the agent or something and start giving it a score or something like
[00:34:25.120 --> 00:34:26.120]   that.
[00:34:26.120 --> 00:34:29.160]   So I think, yeah, there's kind of like two, yeah, those are kind of like two different
[00:34:29.160 --> 00:34:30.160]   things that I'm thinking about.
[00:34:30.160 --> 00:34:34.080]   Like, how can we make it more evident what's going on so humans can look at it and understand
[00:34:34.080 --> 00:34:35.080]   it more easily.
[00:34:35.080 --> 00:34:38.960]   And that is a type of evaluation.
[00:34:38.960 --> 00:34:42.000]   And then the other one is trying to automate some of that with language models.
[00:34:42.000 --> 00:34:46.800]   If I can flip this around on you now, kind of like, yeah, what are you guys seeing at
[00:34:46.800 --> 00:34:48.800]   weights and biases and how are you thinking about that?
[00:34:48.800 --> 00:34:51.000]   Well, I think I agree with your assessment.
[00:34:51.000 --> 00:34:54.440]   I mean, I think there's no substitute for looking at individual examples.
[00:34:54.440 --> 00:34:59.880]   I mean, that takes me back to my very first job where I was doing search relevance and
[00:34:59.880 --> 00:35:04.760]   a lot of times I think people were overly looking at the statistics on like what looks
[00:35:04.760 --> 00:35:08.880]   better or worse based on a held out test set.
[00:35:08.880 --> 00:35:12.920]   And not enough people are looking at, okay, what is this thing actually doing in specific
[00:35:12.920 --> 00:35:14.320]   cases?
[00:35:14.320 --> 00:35:18.320]   But then at the same time, you had this effect when I was working in search that the CEO's
[00:35:18.320 --> 00:35:22.440]   daughter would have a homework assignment where she got a bad search result and then
[00:35:22.440 --> 00:35:24.920]   the CEO would be freaking out about it.
[00:35:24.920 --> 00:35:32.080]   And it would kind of cause, it would make it really hard to do kind of systematic improvements
[00:35:32.080 --> 00:35:38.800]   when you're just sort of looking at these specific examples maybe too closely.
[00:35:38.800 --> 00:35:45.280]   So I feel like the first thing that we wanted to do with you guys was make the obvious thing
[00:35:45.280 --> 00:35:48.760]   that people are doing today, the testing by vibes as easy as possible.
[00:35:48.760 --> 00:35:56.120]   And that was like a big design element of the integration that we did with you.
[00:35:56.120 --> 00:36:01.400]   But then, I mean, I've been wondering what you're seeing as well.
[00:36:01.400 --> 00:36:07.000]   We saw OpenAI evals, which seemed like the main thing that that's doing is just like
[00:36:07.000 --> 00:36:12.480]   you're saying, like asking the same language model to look at the result and say, does
[00:36:12.480 --> 00:36:15.960]   it seem like a good result or a bad result?
[00:36:15.960 --> 00:36:19.080]   And it does seem like risky.
[00:36:19.080 --> 00:36:21.760]   Like if you did that with a human, there'd certainly be blind spots.
[00:36:21.760 --> 00:36:27.840]   If you asked me to grade this interview myself, I could imagine that being effective or I
[00:36:27.840 --> 00:36:31.160]   could imagine that going a bit haywire.
[00:36:31.160 --> 00:36:36.760]   But I'm kind of curious what sort of the best practices you've seen there just specifically
[00:36:36.760 --> 00:36:42.840]   if I, you know, we actually have a document search engine that we felt that had weights
[00:36:42.840 --> 00:36:43.840]   and biases.
[00:36:43.840 --> 00:36:49.880]   Like, would your first recommendation to get a score of how well it's doing be to ask
[00:36:49.880 --> 00:36:56.360]   say GPT-4, does this look like a good answer to the question is posed?
[00:36:56.360 --> 00:36:57.360]   Honestly, probably.
[00:36:57.360 --> 00:36:58.360]   Yeah.
[00:36:58.360 --> 00:37:02.560]   I mean, it depends on what type of score you want to get, but I do think that's a promising
[00:37:02.560 --> 00:37:03.560]   way forward.
[00:37:03.560 --> 00:37:06.120]   I think some other good literature in this space is like the anthropic stuff.
[00:37:06.120 --> 00:37:09.840]   So Anthropic put out a lot of good stuff around this related to like constitutional AI.
[00:37:09.840 --> 00:37:15.240]   And I think like notably like one thing that they did there is there's, they're like, they're
[00:37:15.240 --> 00:37:19.680]   and I think they did some like quantitative studies showing that, you know, like some
[00:37:19.680 --> 00:37:23.360]   of the scoring did better than humans or at level with humans when looking at things.
[00:37:23.360 --> 00:37:26.440]   And so they did a bunch of amazing benchmarking there.
[00:37:26.440 --> 00:37:30.120]   I think what they did is it's more complex than just a single language model call, though.
[00:37:30.120 --> 00:37:35.120]   I think they had some type of like, or at least like they had this whole, because another
[00:37:35.120 --> 00:37:39.120]   thing is like auto generation of some of this data as well.
[00:37:39.120 --> 00:37:43.000]   So like when you're talking about like, you know, your document bought over weights and
[00:37:43.000 --> 00:37:49.080]   biases docs, like when you want to grade it on whether the question was like right or
[00:37:49.080 --> 00:37:53.140]   not, like it's helpful to have like ground truth answers to grade it against.
[00:37:53.140 --> 00:37:56.920]   And so I think this also gets back to like what type of score you're talking about.
[00:37:56.920 --> 00:38:02.760]   But I think like one thing for that is it's like generating kind of like question answer
[00:38:02.760 --> 00:38:05.120]   pairs, then you have a ground truth answer.
[00:38:05.120 --> 00:38:07.760]   And then that makes it a lot easier to ask the bot.
[00:38:07.760 --> 00:38:12.320]   Like, you know, it's a lot easier to ask the bot, like, does this answer look like the
[00:38:12.320 --> 00:38:13.320]   expected answer?
[00:38:13.320 --> 00:38:15.200]   Then like, does this answer look correct?
[00:38:15.200 --> 00:38:19.160]   Because then that's, I'm less optimistic about the latter.
[00:38:19.160 --> 00:38:20.160]   Former is pretty good.
[00:38:20.160 --> 00:38:23.800]   And I think the interesting thing about the anthropic paper was that their pipeline for
[00:38:23.800 --> 00:38:27.480]   coming up with those questions and maybe the eval as well, I'm not 100% sure, but their
[00:38:27.480 --> 00:38:30.520]   pipeline for coming up at the very least was like very complex.
[00:38:30.520 --> 00:38:33.520]   So we have a simple example of this in link chain where you can generate kind of like
[00:38:33.520 --> 00:38:36.960]   questions, but it's just a single pass of the language model.
[00:38:36.960 --> 00:38:40.640]   Their pipeline had like many, many steps where they would generate questions, then refine
[00:38:40.640 --> 00:38:42.900]   them, then filter some out.
[00:38:42.900 --> 00:38:48.360]   And so I think there's a lot of really interesting work to be done there.
[00:38:48.360 --> 00:38:52.720]   I guess that's a good segue into the next question of my list, actually, which are,
[00:38:52.720 --> 00:38:59.400]   do you have a sense of the trade-offs between using LLMs right off the shelf versus fine
[00:38:59.400 --> 00:39:01.360]   tuning LLMs in terms of performance?
[00:39:01.360 --> 00:39:06.120]   Like what would you recommend as best practice to someone in different situations?
[00:39:06.120 --> 00:39:14.160]   I think so for a long time, long time being like a few months, I basically recommend,
[00:39:14.160 --> 00:39:21.160]   like I was, I think even now get started with like open AI or anthropic or one of the other
[00:39:21.160 --> 00:39:24.320]   models that are off the shelf.
[00:39:24.320 --> 00:39:27.200]   And for a while I said, you know, don't even think about kind of like fine tuning for a
[00:39:27.200 --> 00:39:28.200]   very long time.
[00:39:28.200 --> 00:39:32.080]   Like you can do, you can do in context learning with like few shot examples and that's pretty
[00:39:32.080 --> 00:39:33.080]   good.
[00:39:33.080 --> 00:39:36.600]   You can also do prompt engineering to kind of like improve it.
[00:39:36.600 --> 00:39:39.600]   I think we are at an interesting point right now where there are a lot of like good open
[00:39:39.600 --> 00:39:43.840]   source models with permissive licenses and they're starting to be more examples of how
[00:39:43.840 --> 00:39:45.480]   you can fine tune them.
[00:39:45.480 --> 00:39:50.000]   This is something that we want to look at more internally.
[00:39:50.000 --> 00:39:56.280]   And so I'm not sure like a hundred percent how close it is, but I think we're, I'm more
[00:39:56.280 --> 00:39:59.960]   optimistic now that you can like start to fine tune things.
[00:39:59.960 --> 00:40:03.460]   My take would still be like, you know, get started with an open source model.
[00:40:03.460 --> 00:40:07.400]   If you can't do it with GPT-4, it's probably pretty unlikely unless it's like a really
[00:40:07.400 --> 00:40:12.220]   narrow kind of like hyper specified problem that fine tuning will help.
[00:40:12.220 --> 00:40:16.960]   So get started with GPT-4, see how far that can take you.
[00:40:16.960 --> 00:40:21.600]   Build up kind of like, build up this intuition for what like good examples look like, build
[00:40:21.600 --> 00:40:25.180]   up some data sets and then maybe you can start to fine tune after that.
[00:40:25.180 --> 00:40:28.680]   And if you were going to look to a model to fine tune, where would you start?
[00:40:28.680 --> 00:40:33.080]   Like what would be your most basic model to start with to fine tuning?
[00:40:33.080 --> 00:40:36.220]   Honestly, this is something where we need to do more exploration.
[00:40:36.220 --> 00:40:39.700]   I think there's been a lot of like, like I think Mosaic came out with a model like last
[00:40:39.700 --> 00:40:42.320]   week that seems like really, really promising.
[00:40:42.320 --> 00:40:43.320]   Haven't tried to fine tune it.
[00:40:43.320 --> 00:40:47.160]   So, so don't know how, how, how that would go.
[00:40:47.160 --> 00:40:50.480]   You know, there, there, there are a lot of style models, which a lot of people have kind
[00:40:50.480 --> 00:40:53.240]   of like showed to fine tune, but there's some licensing issues there.
[00:40:53.240 --> 00:40:55.860]   So there's some considerations there.
[00:40:55.860 --> 00:41:00.560]   I know, I think Eugene Yan on Twitter just put out a cool list or cool repo of like all
[00:41:00.560 --> 00:41:03.180]   open source models with Prometheus licenses.
[00:41:03.180 --> 00:41:05.520]   So I mean, I can tell you our plan for what we're going to do.
[00:41:05.520 --> 00:41:08.480]   We're going to like, you know, we're going to, we're going to pick an example where we
[00:41:08.480 --> 00:41:09.960]   think fine tuning might be useful.
[00:41:09.960 --> 00:41:14.240]   And so like probably like narrow application, maybe starting with like question answering
[00:41:14.240 --> 00:41:16.200]   or something.
[00:41:16.200 --> 00:41:21.120]   And then yeah, probably look at, probably look at Eugene's GitHub, see which model seems
[00:41:21.120 --> 00:41:26.320]   like the easiest to fine tune and do some experiments with, with there.
[00:41:26.320 --> 00:41:28.560]   Would you ever include fine tuning as part of Langchain?
[00:41:28.560 --> 00:41:30.000]   I think potentially, yeah.
[00:41:30.000 --> 00:41:35.400]   I think, I think, I think, I think, I mean, I think absolutely.
[00:41:35.400 --> 00:41:36.400]   Yeah.
[00:41:36.400 --> 00:41:39.160]   I think there's, you know, so the standard interface we have around language models makes
[00:41:39.160 --> 00:41:45.360]   it really easy to substitute out you know, GPT-4 for some other language model.
[00:41:45.360 --> 00:41:51.400]   And so I think, and then I think a lot of like the goal of Langchain is to make it as
[00:41:51.400 --> 00:41:55.740]   easy as possible to develop LLM applications and a big blocker for people that we are hearing
[00:41:55.740 --> 00:41:59.920]   is kind of like cost latency of models.
[00:41:59.920 --> 00:42:02.580]   And I think fine tuning does solve those problems.
[00:42:02.580 --> 00:42:05.500]   It's like maybe not the easiest thing to do right now.
[00:42:05.500 --> 00:42:09.040]   And so I think that's where if, if the technology is there, that's maybe something that we could
[00:42:09.040 --> 00:42:14.960]   help with because I think it's pretty consistent with kind of like our goal and mission.
[00:42:14.960 --> 00:42:17.640]   So yeah, I think it, I think it fits in nicely.
[00:42:17.640 --> 00:42:21.120]   Well consider integrating with Weights and Biases if you, if you do that, that's kind
[00:42:21.120 --> 00:42:23.120]   of our bread and butter.
[00:42:23.120 --> 00:42:24.120]   Yeah.
[00:42:24.120 --> 00:42:26.800]   Well, can I ask a follow up about that as well?
[00:42:26.800 --> 00:42:31.480]   So I think there's some like, so obviously I've used Weights and Biases a lot for, for
[00:42:31.480 --> 00:42:33.760]   training models from, from kind of like scratch.
[00:42:33.760 --> 00:42:37.840]   And now there's this new type of like, kind of like training of models, which is prompt
[00:42:37.840 --> 00:42:41.440]   engineering and like adding in a few shot examples and, and, and stuff like that.
[00:42:41.440 --> 00:42:44.920]   And so how are, how are you guys kind of like thinking of the, like, what do you see as
[00:42:44.920 --> 00:42:45.920]   parallels?
[00:42:45.920 --> 00:42:48.160]   What do you see as differences?
[00:42:48.160 --> 00:42:51.640]   And obviously like there's, we've done the integration to help kind of like just shed
[00:42:51.640 --> 00:42:53.360]   light on what's going on under the hood.
[00:42:53.360 --> 00:42:57.280]   But yeah, like, you know, are there, yeah.
[00:42:57.280 --> 00:43:01.920]   Like if there's, yeah, if there's like prompt tuning that goes on or something like that,
[00:43:01.920 --> 00:43:04.280]   would Weights and Biases fit in there nicely?
[00:43:04.280 --> 00:43:07.240]   Like what are you thinking of as, as parallels or new thing?
[00:43:07.240 --> 00:43:08.240]   Yeah.
[00:43:08.240 --> 00:43:11.280]   I mean, I've been thinking about this a lot actually, as you can imagine.
[00:43:11.280 --> 00:43:15.440]   I think that, I think that there are some, there's some real similarities.
[00:43:15.440 --> 00:43:21.520]   And I think one big similarity is that unlike traditional software development, I think
[00:43:21.520 --> 00:43:29.240]   that both ML engineering and prompt engineering involves a ton of experimentation.
[00:43:29.240 --> 00:43:33.360]   And that experimentation, you know, typically happens in a notebook, but I think Weights
[00:43:33.360 --> 00:43:40.040]   and Biases has a role to play as ironically more like a well-kept lab notebook, like not
[00:43:40.040 --> 00:43:41.600]   like a Jupyter notebook, right?
[00:43:41.600 --> 00:43:44.760]   Where you're actually, the problem with Jupyter notebooks is you actually lose all the stuff
[00:43:44.760 --> 00:43:45.760]   that you do.
[00:43:45.760 --> 00:43:46.760]   They're kind of ephemeral.
[00:43:46.760 --> 00:43:51.160]   And I think what Weights and Biases is good at is keeping track of all the stuff, you
[00:43:51.160 --> 00:43:52.160]   know, that you did.
[00:43:52.160 --> 00:43:57.880]   So I think there's like an experiment tracking role to play, a major role to play for, you
[00:43:57.880 --> 00:44:01.160]   know, prompt engineering, which is why we're actually so excited about the integration
[00:44:01.160 --> 00:44:02.160]   that we've done with you.
[00:44:02.160 --> 00:44:04.720]   You've mentioned a few times, we'll definitely put in the show notes, is one of the things
[00:44:04.720 --> 00:44:08.400]   that like prompted this whole interview, which I'm excited about, you know, regardless,
[00:44:08.400 --> 00:44:11.520]   but we are super excited about that integration.
[00:44:11.520 --> 00:44:13.800]   But I think there are some really big differences.
[00:44:13.800 --> 00:44:21.200]   And I think the biggest difference for us is that more people can do prompt engineering
[00:44:21.200 --> 00:44:22.200]   more easily.
[00:44:22.200 --> 00:44:25.640]   I think it's more accessible than ML engineering.
[00:44:25.640 --> 00:44:27.520]   So the audience for us is a little bit different.
[00:44:27.520 --> 00:44:31.040]   Like, you know, we're really used to putting all kinds of charts and graphs, and then our
[00:44:31.040 --> 00:44:35.080]   users are like, these aren't sophisticated enough charts and graphs that we do, you know,
[00:44:35.080 --> 00:44:37.560]   plotly integration, and then they want more exotic stuff.
[00:44:37.560 --> 00:44:41.720]   So we do like a Vega integration to give people like, you know, complete access to anything
[00:44:41.720 --> 00:44:42.720]   that they would ever want.
[00:44:42.720 --> 00:44:47.960]   And then, you know, we just kind of can't believe the amount of essentially data exploration
[00:44:47.960 --> 00:44:54.120]   and visualization that people do on top of their results.
[00:44:54.120 --> 00:45:00.280]   I feel like when the natural medium is text, it's a little bit of a different experience,
[00:45:00.280 --> 00:45:01.280]   right?
[00:45:01.280 --> 00:45:05.720]   We do a lot more reading and probably a lot more testing by vibes, honestly, than sort
[00:45:05.720 --> 00:45:10.640]   of like quantitative analysis over, you know, millions of records.
[00:45:10.640 --> 00:45:14.520]   I think ideally, in every case, people should be doing both.
[00:45:14.520 --> 00:45:20.600]   But that's caused us to try to make our text interfaces a lot more easy to use.
[00:45:20.600 --> 00:45:26.960]   And also, you know, we have this notion of experiment in Weights and Biases, which is
[00:45:26.960 --> 00:45:29.080]   like kind of an individual run.
[00:45:29.080 --> 00:45:33.280]   And I feel like the prompt engineering experience is like a little bit more organic, right?
[00:45:33.280 --> 00:45:36.880]   Where you can actually kind of try lots of different prompts and like tweak them.
[00:45:36.880 --> 00:45:42.600]   And so it's a little bit of a different workflow.
[00:45:42.600 --> 00:45:44.240]   And you know, the chaining is interesting too.
[00:45:44.240 --> 00:45:49.040]   I mean, that actually you do do in machine learning, but I think you do it even more,
[00:45:49.040 --> 00:45:50.040]   you know, with prompts.
[00:45:50.040 --> 00:45:54.840]   And I think there's an analogy to hyperparameter search as well, probably, but it's a little
[00:45:54.840 --> 00:45:55.840]   bit different.
[00:45:55.840 --> 00:46:01.280]   But I think at the end of the day, you know, the role that I think we should play is the
[00:46:01.280 --> 00:46:06.520]   same where it's like all about the sort of like logging and tracking and reproducibility,
[00:46:06.520 --> 00:46:10.840]   you know, and helping people get these things reliable enough that they can, you know, run
[00:46:10.840 --> 00:46:12.280]   them in production.
[00:46:12.280 --> 00:46:19.920]   And honestly, I don't think we ourselves know everything that we have to do to kind of get
[00:46:19.920 --> 00:46:20.920]   to that point.
[00:46:20.920 --> 00:46:24.920]   Like, we're still iterating with our users like crazy and, you know, talking to people
[00:46:24.920 --> 00:46:25.920]   like you.
[00:46:25.920 --> 00:46:26.920]   Yeah.
[00:46:26.920 --> 00:46:30.720]   I mean, I don't think anyone knows to some extent, like that's what makes it so exciting.
[00:46:30.720 --> 00:46:36.480]   Like what are you kind of like when you say, I think experiments are really interesting
[00:46:36.480 --> 00:46:40.120]   because like two of the things I mentioned earlier, like, you know, cost and latency.
[00:46:40.120 --> 00:46:42.520]   And so I think you definitely track latency.
[00:46:42.520 --> 00:46:43.960]   I saw that in the integration.
[00:46:43.960 --> 00:46:45.200]   Do you track cost as well?
[00:46:45.200 --> 00:46:48.480]   And like, what other things are you seeing yourselves kind of like track?
[00:46:48.480 --> 00:46:53.000]   Because as we've talked about, it's hard to track vibes at the current state.
[00:46:53.000 --> 00:46:54.560]   Well, actually, I think so.
[00:46:54.560 --> 00:46:58.880]   We don't track costs right now, but we probably will by the time this recording comes out.
[00:46:58.880 --> 00:47:01.440]   And that's important, obviously.
[00:47:01.440 --> 00:47:07.880]   But I think more important than cost or latency is actually helping people understand the
[00:47:07.880 --> 00:47:09.480]   vibes.
[00:47:09.480 --> 00:47:14.600]   And so I actually think having a really easy interface to seeing what the inputs and outputs
[00:47:14.600 --> 00:47:20.720]   are and also the intermediate inputs and outputs, I think that's actually the most core requirement
[00:47:20.720 --> 00:47:25.600]   because you want to make it really simple for people to like spin through, you know,
[00:47:25.600 --> 00:47:30.480]   lots of examples and get a feel for how well things are working.
[00:47:30.480 --> 00:47:33.800]   And I think also explore text.
[00:47:33.800 --> 00:47:39.160]   So finding like examples where certain things happened, I think is also important, which
[00:47:39.160 --> 00:47:43.280]   is why we're putting a lot of thinking into like, how do you actually like search the
[00:47:43.280 --> 00:47:47.440]   giant table of all the prompts that you've built?
[00:47:47.440 --> 00:47:52.080]   Because at some scale, I think just gets so unwieldy and you might find these areas where
[00:47:52.080 --> 00:47:53.080]   stuff is happening.
[00:47:53.080 --> 00:47:56.680]   And I guess like another dumb thing is just like error messages, right?
[00:47:56.680 --> 00:48:01.400]   Like I mean, none of these like backend systems are perfectly reliable.
[00:48:01.400 --> 00:48:03.360]   The error messages are pretty cryptic.
[00:48:03.360 --> 00:48:06.440]   And I feel like if you run a thousand prompts through it, you're like guaranteed to get
[00:48:06.440 --> 00:48:08.600]   a couple of weird errors.
[00:48:08.600 --> 00:48:11.160]   And you know, we want to help people find those.
[00:48:11.160 --> 00:48:14.920]   That may go away over time as these things get more robust, but it seems like kind of
[00:48:14.920 --> 00:48:16.480]   an acute issue right now.
[00:48:16.480 --> 00:48:17.480]   Yeah.
[00:48:17.480 --> 00:48:23.160]   I mean, I think the, I like what you said about like the finding, searching things like,
[00:48:23.160 --> 00:48:28.320]   like I like, so, so at Robust, we were in the Mops space and there's a lot of like monitoring
[00:48:28.320 --> 00:48:30.120]   companies that kind of do similar things.
[00:48:30.120 --> 00:48:34.720]   I've seen a lot of cool kind of like embedding based products that basically, you know, create
[00:48:34.720 --> 00:48:36.320]   embeddings for prompts and then do cluster.
[00:48:36.320 --> 00:48:41.040]   I think Arise had a really good one that does embeddings, plots them out, you can then explore
[00:48:41.040 --> 00:48:42.040]   different things.
[00:48:42.040 --> 00:48:46.120]   And I think, I mean, I think that's useful for both like finding issues and debugging,
[00:48:46.120 --> 00:48:50.320]   but then also just getting a sense of what users are doing, which is, I mean, actually,
[00:48:50.320 --> 00:48:51.320]   yeah.
[00:48:51.320 --> 00:48:55.840]   Like, I mean, I think there's some interesting things where like, that's like useful, not
[00:48:55.840 --> 00:49:01.400]   just for kind of like the person who's debugging it, but also to like under like product insights
[00:49:01.400 --> 00:49:02.400]   almost.
[00:49:02.400 --> 00:49:07.720]   Have you thought about trying to like bridge that gap and having weights and biases if
[00:49:07.720 --> 00:49:12.760]   you are looking for like searching different things, be like a, yeah, like for, for a product
[00:49:12.760 --> 00:49:15.920]   person who's trying to understand how people are using their, their thing.
[00:49:15.920 --> 00:49:17.960]   Oh, for getting product insights.
[00:49:17.960 --> 00:49:18.960]   Yeah.
[00:49:18.960 --> 00:49:25.040]   That's certainly an off-label use of weights and biases that we see a significant number
[00:49:25.040 --> 00:49:26.640]   of people do.
[00:49:26.640 --> 00:49:35.320]   I do think it's important to have like a real shared sense of like what the ideal user profile
[00:49:35.320 --> 00:49:36.920]   is and what they're doing.
[00:49:36.920 --> 00:49:43.720]   So, you know, we don't build for that use case today, although ML does blur into product
[00:49:43.720 --> 00:49:44.720]   quite a bit.
[00:49:44.720 --> 00:49:48.600]   And so many of the conversations that I've had in this podcast have ended up being around,
[00:49:48.600 --> 00:49:50.160]   you know, more kind of product stuff.
[00:49:50.160 --> 00:49:55.640]   And I think there are these unique product challenges of making like unreliable ML systems
[00:49:55.640 --> 00:49:59.880]   or unreliable, you know, prompt systems into reliable products that people like.
[00:49:59.880 --> 00:50:04.520]   And that I think is more of a product management challenge than a technology challenge.
[00:50:04.520 --> 00:50:09.520]   So, you know, I guess as I say it, I'm not sure there's quite a bright line, but, you
[00:50:09.520 --> 00:50:13.400]   know, we're not trying to compete with like, you know, like a mixed panel or something
[00:50:13.400 --> 00:50:14.400]   like that at this point.
[00:50:14.400 --> 00:50:15.400]   Yeah.
[00:50:15.400 --> 00:50:18.720]   And I mean, I'm just like thinking about this now, but like, you know, in the old world
[00:50:18.720 --> 00:50:22.520]   of machine learning where the inputs were like numbers and features and stuff like that,
[00:50:22.520 --> 00:50:24.360]   like that'd be really hard to interpret, right.
[00:50:24.360 --> 00:50:26.080]   For the everyday person.
[00:50:26.080 --> 00:50:29.480]   But now the input's like text and it's like, anyone can do that.
[00:50:29.480 --> 00:50:30.480]   And it's based on vibes.
[00:50:30.480 --> 00:50:31.480]   And so maybe there is overlap.
[00:50:31.480 --> 00:50:32.480]   I don't know.
[00:50:32.480 --> 00:50:33.480]   Well, let me ask you this.
[00:50:33.480 --> 00:50:38.040]   We have another question on our list that I was curious to get your thoughts on.
[00:50:38.040 --> 00:50:41.920]   I look on product hunt, you know, every couple of days and I always see some kind of like
[00:50:41.920 --> 00:50:45.840]   no code prompt engineering type of thing.
[00:50:45.840 --> 00:50:51.160]   Do you have ambitions to serve an audience that can't code at all?
[00:50:51.160 --> 00:50:52.600]   At the moment, no.
[00:50:52.600 --> 00:50:57.120]   I think there are a lot of great projects flow wise and Langtrace are two built on top
[00:50:57.120 --> 00:51:01.360]   of Langchain that are pretty good at that.
[00:51:01.360 --> 00:51:06.240]   And I think the reason is like, I think for a lot of the more complex applications, it's
[00:51:06.240 --> 00:51:08.240]   still really hard to get them into production.
[00:51:08.240 --> 00:51:11.280]   Like we talk about agents, they're not widely in production yet.
[00:51:11.280 --> 00:51:16.280]   They're like maybe in like, you know, a few of kind of like the, you know, super early
[00:51:16.280 --> 00:51:20.360]   startups that we're working with or have them in production, but like they're not widely
[00:51:20.360 --> 00:51:21.360]   in production.
[00:51:21.360 --> 00:51:23.000]   I think it's just really hard.
[00:51:23.000 --> 00:51:30.120]   And so I think like Intel, we can do that with, and I think like, I think you and all
[00:51:30.120 --> 00:51:35.040]   the ones that are like highly technical teams that are doing it through code, not through
[00:51:35.040 --> 00:51:38.120]   some, not through, through UI.
[00:51:38.120 --> 00:51:42.600]   And so I, to me, that's the most interesting thing to kind of like push on and go after.
[00:51:42.600 --> 00:51:43.600]   And that's the direction we're going.
[00:51:43.600 --> 00:51:47.280]   I think there are like super valuable kind of like, you know, I think there's some, some
[00:51:47.280 --> 00:51:51.320]   great stuff being done by, by those two companies I just mentioned, as well as, you know, I
[00:51:51.320 --> 00:51:54.800]   think like Mishpa is doing a lot of great stuff with bubble and enabling like a whole
[00:51:54.800 --> 00:51:57.960]   new set of people to build on top of Langchain.
[00:51:57.960 --> 00:52:01.640]   I think that's something where we're very happy to work with the community on and kind
[00:52:01.640 --> 00:52:04.960]   of like have them kind of like push the boundary on.
[00:52:04.960 --> 00:52:08.840]   I think our skillset is just a bit more suited for like helping put some of these more complex
[00:52:08.840 --> 00:52:09.840]   use cases in production.
[00:52:09.840 --> 00:52:11.400]   And that's really what we're pushing.
[00:52:11.400 --> 00:52:16.840]   So you also like to focus on a single ideal user profile.
[00:52:16.840 --> 00:52:23.560]   We try to, I mean, one of the issues, there's so much going on, so it's tough too, but yeah.
[00:52:23.560 --> 00:52:27.920]   Do you, do you think that prompt engineering is a real job?
[00:52:27.920 --> 00:52:30.280]   I'll say I've heard a lot about prompt engineering.
[00:52:30.280 --> 00:52:33.360]   I've never met anyone that described themselves as a prompt engineer.
[00:52:33.360 --> 00:52:38.520]   I have seen tons of people that are doing prompt engineering, but it sort of seems like
[00:52:38.520 --> 00:52:41.440]   they're not only doing prompt engineering.
[00:52:41.440 --> 00:52:44.120]   I'm curious what you're seeing on your side.
[00:52:44.120 --> 00:52:45.120]   Yeah.
[00:52:45.120 --> 00:52:49.320]   I mean, I think like I think it also depends on like what exactly you mean by prompt engineering.
[00:52:49.320 --> 00:52:52.160]   And I think there's like maybe like two separate things.
[00:52:52.160 --> 00:52:54.920]   Like one is like, if we go back to like, so, so, okay.
[00:52:54.920 --> 00:52:58.520]   So one term that I'm trying to popularize, I haven't started officially trying to popularize
[00:52:58.520 --> 00:53:03.680]   it, but that I've been saying in conversations is like prompt construction.
[00:53:03.680 --> 00:53:05.440]   Because I think prompt construction is really important.
[00:53:05.440 --> 00:53:08.440]   What I specifically mean by prompt construction is like pulling in the relevant pieces of
[00:53:08.440 --> 00:53:12.720]   information, whether it be prior chats or like reference documents or something like
[00:53:12.720 --> 00:53:13.720]   that.
[00:53:13.720 --> 00:53:17.120]   And I think this is so important because a lot of the applications that people are doing
[00:53:17.120 --> 00:53:22.880]   are asking language models to produce generations grounded, grounded in some of the context
[00:53:22.880 --> 00:53:23.880]   that's in the prompt.
[00:53:23.880 --> 00:53:27.920]   And so if you don't construct the prompt with the relevant information, then like, obviously
[00:53:27.920 --> 00:53:28.920]   it's not going to get the right answer.
[00:53:28.920 --> 00:53:31.560]   Like one of the biggest mistakes for question answering systems.
[00:53:31.560 --> 00:53:35.960]   And this goes back to kind of like some of the evaluation stuff isn't the biggest mistake
[00:53:35.960 --> 00:53:38.400]   usually is that it doesn't retrieve the right information.
[00:53:38.400 --> 00:53:39.400]   Like retrieval is super important.
[00:53:39.400 --> 00:53:43.600]   And if it doesn't retrieve the right information or if it retrieves partial information, obviously
[00:53:43.600 --> 00:53:44.960]   it's going to get it wrong.
[00:53:44.960 --> 00:53:46.760]   So I think prompt construction is really important.
[00:53:46.760 --> 00:53:51.280]   And I think that will always be work done on prompt construction in terms of like thinking
[00:53:51.280 --> 00:53:55.520]   about what should go into a prompt, engineering some of the systems to put the stuff in the
[00:53:55.520 --> 00:53:58.000]   prompt in a reliable and quick manner.
[00:53:58.000 --> 00:54:02.480]   Then there's the other type of prompt engineering, which is like, I think, and I think this is,
[00:54:02.480 --> 00:54:07.480]   this was much more common in the early days where like, you know, you're like, you're
[00:54:07.480 --> 00:54:09.880]   playing around with like the verbiage of the instructions.
[00:54:09.880 --> 00:54:10.880]   I mean, okay.
[00:54:10.880 --> 00:54:14.200]   So even in instructions, you still have to be like pretty clear about what you want it
[00:54:14.200 --> 00:54:15.200]   to do.
[00:54:15.200 --> 00:54:19.360]   And I think that will converge to basically how I would explain to a human to do a task
[00:54:19.360 --> 00:54:20.360]   or something like that.
[00:54:20.360 --> 00:54:23.220]   You still have to be pretty clear, but then there's like weird stuff where you're like
[00:54:23.220 --> 00:54:28.400]   adding extra spaces or like, you know, messing around with some of like the punctuation or,
[00:54:28.400 --> 00:54:32.880]   or I mean, I think this is way more evident in some of the image prompting where you like
[00:54:32.880 --> 00:54:37.360]   add all these like random characters or random sequences and it messes it up.
[00:54:37.360 --> 00:54:40.960]   I don't think that'll be around, but I think like everything around like constructing the
[00:54:40.960 --> 00:54:44.480]   prompt and like being good about instructing the prompt.
[00:54:44.480 --> 00:54:48.440]   And this might be like more, you know, writing skills than engineering skills.
[00:54:48.440 --> 00:54:52.920]   But I think those are, I think those are here to stay as long as we have prompts.
[00:54:52.920 --> 00:54:53.920]   That's actually funny.
[00:54:53.920 --> 00:55:01.560]   My first, my first company was called CrowdFlower and it did essentially, it was a system for
[00:55:01.560 --> 00:55:05.340]   like, you know, collecting training data, which is mainly like, you know, making these
[00:55:05.340 --> 00:55:10.140]   tasks for, you know, crowdsourced humans to work on.
[00:55:10.140 --> 00:55:14.260]   And I actually think it's a, has a lot of similarities to prompt engineering where you
[00:55:14.260 --> 00:55:18.300]   have to like, you know, pull in all the relevant information and be like clear about what you
[00:55:18.300 --> 00:55:19.300]   want.
[00:55:19.300 --> 00:55:23.400]   And it's surprising how hard that is for people to do, especially when they don't exactly
[00:55:23.400 --> 00:55:25.400]   know what the input data set is, right.
[00:55:25.400 --> 00:55:29.200]   There's always like 10% of cases where you're like pulling in data that's really different
[00:55:29.200 --> 00:55:32.760]   than what you thought it was and where your instructions weren't as clear as you thought
[00:55:32.760 --> 00:55:33.760]   they were.
[00:55:33.760 --> 00:55:40.220]   So I totally agree with you that this kind of yeah, prompt construction is absolutely
[00:55:40.220 --> 00:55:41.220]   here to stay.
[00:55:41.220 --> 00:55:42.220]   Yeah.
[00:55:42.220 --> 00:55:45.100]   And I mean, I think like as, as, you know, as we think about like how to make it clear
[00:55:45.100 --> 00:55:46.520]   what's going on, right.
[00:55:46.520 --> 00:55:52.160]   Like, I don't know, like how, how do we as human, like if, if I, if I give someone unclear
[00:55:52.160 --> 00:55:57.140]   instructions, like how do I debug if it's unclear instructions or something like that?
[00:55:57.140 --> 00:56:02.920]   And this is like, I don't have anything specific in mind, but like, you know, I think like
[00:56:02.920 --> 00:56:06.920]   if we do think that language models are going to the point where it's, you know, you, you
[00:56:06.920 --> 00:56:11.480]   naturally or language is this, is the, is the universal interface you, you tell them
[00:56:11.480 --> 00:56:15.520]   what to do and they like, how, how do we debug like our human conversations?
[00:56:15.520 --> 00:56:19.720]   And are there parallels to that, that we can use there for like prompt construction?
[00:56:19.720 --> 00:56:20.720]   Yeah.
[00:56:20.720 --> 00:56:27.760]   You mentioned feeling like Anthropic was becoming like an interesting competitor to, to open
[00:56:27.760 --> 00:56:28.760]   AI.
[00:56:28.760 --> 00:56:34.440]   I was sort of thinking from your perspective as essentially this kind of layer on top of
[00:56:34.440 --> 00:56:37.840]   all these things, I realize there's a lot of other stuff that you do, you know, kind
[00:56:37.840 --> 00:56:42.880]   of a world that's like a monopsony with, you know, only open AI creating large models would
[00:56:42.880 --> 00:56:47.680]   be kind of worrying for the, the work that you do.
[00:56:47.680 --> 00:56:51.560]   I love the work OpenAI does, I admire it, but you know, it also is kind of worrying
[00:56:51.560 --> 00:56:55.600]   for us here at Weights and Biases, but you know, I'm curious like what you're, how you're
[00:56:55.600 --> 00:56:56.600]   seeing the space change.
[00:56:56.600 --> 00:57:00.480]   Do you have any predictions on, you know, how, how you, things might, how you think
[00:57:00.480 --> 00:57:03.160]   things might unfold in terms of LLM providers?
[00:57:03.160 --> 00:57:08.840]   Do you think there'll be one or thousands or, or, or where you think that the world goes?
[00:57:08.840 --> 00:57:13.800]   I think, I think there'll probably be like multiple good ones.
[00:57:13.800 --> 00:57:19.680]   And then I think, and I think those will always be like a step up above the open source.
[00:57:19.680 --> 00:57:25.400]   But then I think there'll be a really, you know, wide and vibrant open source community.
[00:57:25.400 --> 00:57:31.840]   And I think, I think basically, so I think that'll happen.
[00:57:31.840 --> 00:57:35.080]   And then I think it becomes like, what's the Delta between the models?
[00:57:35.080 --> 00:57:37.280]   What are the applications that live in the Delta?
[00:57:37.280 --> 00:57:40.360]   And then like, if you start to fine tune, like how easy is it to take an open source
[00:57:40.360 --> 00:57:44.640]   model and fine tune it so that on a specific task, it gets up to par with, with the open
[00:57:44.640 --> 00:57:48.760]   source or with the private models.
[00:57:48.760 --> 00:57:52.120]   And I don't know, I mean, I've gotten, I've gotten more bullish on open source in the
[00:57:52.120 --> 00:57:55.360]   past like week or two, I can tell you that I still don't have, you know, I still think
[00:57:55.360 --> 00:58:02.840]   the lag behind the private models for, for, you know, a very good amount of time.
[00:58:02.840 --> 00:58:06.640]   But I do think there'll be like, you know, I'm optimistic that there will be kind of
[00:58:06.640 --> 00:58:11.840]   like multiple, multiple kind of like really good private models.
[00:58:11.840 --> 00:58:16.200]   And yeah, I mean, it's crazy impressive what OpenAI has done, right?
[00:58:16.200 --> 00:58:23.120]   Like they're, they're, they're, they're far ahead in this field.
[00:58:23.120 --> 00:58:26.320]   And they were even, yeah, they're far ahead in this field.
[00:58:26.320 --> 00:58:30.200]   And they're continuing to push out features at like an incredible pace.
[00:58:30.200 --> 00:58:31.920]   And so it's really, really impressive.
[00:58:31.920 --> 00:58:36.200]   I do think they'll probably be like, yeah, you know, over the next like year or so, one
[00:58:36.200 --> 00:58:42.120]   or two other companies, maybe more that get to that level.
[00:58:42.120 --> 00:58:44.320]   And yeah, but I don't know.
[00:58:44.320 --> 00:58:45.680]   Do you have any takes on this?
[00:58:45.680 --> 00:58:47.800]   I think this is something that everyone's asking themselves.
[00:58:47.800 --> 00:58:49.520]   So I'd be curious for your take.
[00:58:49.520 --> 00:58:52.720]   Yeah, I'm really not sure.
[00:58:52.720 --> 00:58:58.320]   It does seem like there's so many smart people working on this right now.
[00:58:58.320 --> 00:59:02.840]   And so many people that seem to have access to really significant amounts of funding that
[00:59:02.840 --> 00:59:07.760]   I would have thought we would have seen, you know, more impressive competition by now,
[00:59:07.760 --> 00:59:12.280]   I guess we sort of have seen things that are, you know, can compete with like, you know,
[00:59:12.280 --> 00:59:15.080]   OpenAI Circa like a year or two ago.
[00:59:15.080 --> 00:59:23.320]   I've been impressed how much of a gap they've been able to maintain lately.
[00:59:23.320 --> 00:59:27.320]   So I mean, I guess I'm more like, like, let's like, you know, wait and see what happens.
[00:59:27.320 --> 00:59:31.400]   I could see arguments for, for either side of that.
[00:59:31.400 --> 00:59:37.840]   In a way, I think it's also kind of like, you know, technical questions decide this
[00:59:37.840 --> 00:59:38.840]   too, right?
[00:59:38.840 --> 00:59:44.240]   Like, how much does it really matter to, you know, to have orders of magnitude more infrastructure,
[00:59:44.240 --> 00:59:45.240]   right?
[00:59:45.240 --> 00:59:48.280]   Like, I mean, I think at the end of the day, that's going to be expensive.
[00:59:48.280 --> 00:59:51.680]   And if, you know, always staying orders of magnitude ahead of everybody else in terms
[00:59:51.680 --> 00:59:56.400]   of like, compute infrastructure gives you like a big gap, then I do think that's a world
[00:59:56.400 --> 01:00:00.040]   that like kind of wants to be, you know, more of a small number of players.
[01:00:00.040 --> 01:00:03.600]   But you know, if, if we run out of data or something like that, I mean, Moore's law is
[01:00:03.600 --> 01:00:07.240]   a powerful force and it changes things like quite a bit.
[01:00:07.240 --> 01:00:11.160]   So I think it's also interesting because I think like a lot of this stuff that OpenAI
[01:00:11.160 --> 01:00:14.840]   is putting out recently is like not just the models, right?
[01:00:14.840 --> 01:00:15.840]   It's almost more agents.
[01:00:15.840 --> 01:00:16.840]   They've got the coding thing.
[01:00:16.840 --> 01:00:18.520]   They've got the web browsing thing.
[01:00:18.520 --> 01:00:20.160]   They've got plugins.
[01:00:20.160 --> 01:00:26.720]   Like these are, these are using, I think probably like specifically trained models, but the
[01:00:26.720 --> 01:00:29.720]   end thing that they're putting out isn't just the model.
[01:00:29.720 --> 01:00:33.480]   And so I also wonder if we'll start to see like, you know, Google or Anthropic start
[01:00:33.480 --> 01:00:34.480]   to do that.
[01:00:34.480 --> 01:00:38.240]   Because I think if you look at like what people are wowing on, on Twitter these days, it's
[01:00:38.240 --> 01:00:42.560]   not like, it's not the OpenAI playground or chat completion playground anymore.
[01:00:42.560 --> 01:00:46.080]   It's like the code generation thing or the, or the web browser thing.
[01:00:46.080 --> 01:00:52.160]   And so I think they're starting to like push their advantage, not only in like the model
[01:00:52.160 --> 01:00:56.960]   space, but in kind of like the hooking it up to tools and then using it as an agent.
[01:00:56.960 --> 01:01:01.720]   So yeah, curious to see how, so I guess there's basically like two separate things.
[01:01:01.720 --> 01:01:05.840]   Like can, can they keep on having the best underlying model and can they keep on pushing
[01:01:05.840 --> 01:01:10.040]   their, or can they start pushing their advantage even more in kind of like this, this agent
[01:01:10.040 --> 01:01:11.040]   like tool.
[01:01:11.040 --> 01:01:12.040]   Totally.
[01:01:12.040 --> 01:01:16.800]   Well, you know, we always end with two questions and I want to slightly modify them for you.
[01:01:16.800 --> 01:01:20.400]   Normally ask what's an underrated aspect of machine learning that you think people should
[01:01:20.400 --> 01:01:22.040]   may pay more attention to.
[01:01:22.040 --> 01:01:24.960]   But I feel like in this case, I want to confine that to like the LLM space.
[01:01:24.960 --> 01:01:28.680]   Like, do you think there's like areas of work, I mean, there's so much happening on Twitter
[01:01:28.680 --> 01:01:33.520]   and I don't even know what's underrated or overrated in terms of attention, but I guess
[01:01:33.520 --> 01:01:38.840]   if you had more time to explore are there like particular like LLM techniques or like
[01:01:38.840 --> 01:01:43.080]   aspects of using LLMs that you would want to investigate yourself?
[01:01:43.080 --> 01:01:44.080]   Yeah.
[01:01:44.080 --> 01:01:47.960]   I think one really under explored thing is like user level personalization.
[01:01:47.960 --> 01:01:51.400]   I think you look at like a lot of the apps and they're, they're combining that with like
[01:01:51.400 --> 01:01:55.000]   general sources of data, but they're not really starting to personalize it to the user.
[01:01:55.000 --> 01:01:59.680]   And I think this could look like, you know adjusting the prompt over time through some
[01:01:59.680 --> 01:02:01.560]   reflection step on a user level.
[01:02:01.560 --> 01:02:05.960]   I think this could, we just released kind of like a blog post today on how you can use
[01:02:05.960 --> 01:02:10.360]   feature stories in addition to prompts to bring them to in this like prompt construction
[01:02:10.360 --> 01:02:13.480]   way to bring in kind of like user level information.
[01:02:13.480 --> 01:02:18.600]   And so I think like user level personalization with LLMs is probably one of the larger, and
[01:02:18.600 --> 01:02:22.020]   I get why it's under explored because you kind of have to make these applications generally
[01:02:22.020 --> 01:02:25.640]   useful and then worry about personalization, but I'm really excited for that.
[01:02:25.640 --> 01:02:26.640]   Okay.
[01:02:26.640 --> 01:02:30.920]   And final question is when these things go into production, what ends up being the hardest
[01:02:30.920 --> 01:02:38.160]   part of actually taking like a working demo using LangChain and turning that into a product
[01:02:38.160 --> 01:02:39.840]   that people can actually use?
[01:02:39.840 --> 01:02:43.880]   I mean on the main thing is still getting reliability good enough, I think for the,
[01:02:43.880 --> 01:02:46.200]   for the like complex use cases.
[01:02:46.200 --> 01:02:48.640]   I think it's getting, getting reliability good enough.
[01:02:48.640 --> 01:02:52.160]   In demos it's fine if it works 10% of the time.
[01:02:52.160 --> 01:02:55.080]   In production it's usually not.
[01:02:55.080 --> 01:02:58.280]   And then in the really important production use cases, it's definitely not.
[01:02:58.280 --> 01:03:03.280]   So I think it's, you know, I think it's, I think the challenge is still getting into
[01:03:03.280 --> 01:03:04.280]   production rather.
[01:03:04.280 --> 01:03:05.280]   Yeah.
[01:03:05.280 --> 01:03:06.280]   Fair enough.
[01:03:06.280 --> 01:03:07.280]   Harrison, thanks so much for your time.
[01:03:07.280 --> 01:03:08.280]   Thank you, Lucas.
[01:03:08.280 --> 01:03:09.280]   Thanks for having me.
[01:03:09.280 --> 01:03:13.120]   If you're enjoying these interviews and you want to learn more, please click on the link
[01:03:13.120 --> 01:03:17.840]   to the show notes in the description where you can find links to all the papers that
[01:03:17.840 --> 01:03:22.240]   are mentioned, supplemental material, and a transcription that we work really hard to
[01:03:22.240 --> 01:03:23.240]   produce.
[01:03:23.240 --> 01:03:23.240]   So check it out.
[01:03:23.240 --> 01:03:26.600]   [MUSIC PLAYING]
[01:03:26.600 --> 01:03:28.740]   [MUSIC PLAYING]

