
[00:00:00.000 --> 00:00:03.040]   what is the timescale on which you think alignment is solvable?
[00:00:03.040 --> 00:00:05.920]   I think this is a really difficult question because I actually think often
[00:00:05.920 --> 00:00:08.520]   people are thinking about kind of alignment in the wrong way.
[00:00:08.520 --> 00:00:13.360]   I think there's a general feeling that it's like models are misaligned or like
[00:00:13.360 --> 00:00:17.560]   there's like an alignment problem to solve, kind of like the Riemann hypothesis
[00:00:17.560 --> 00:00:20.200]   or something like someday we'll crack the Riemann hypothesis.
[00:00:20.200 --> 00:00:25.640]   I don't quite think it's like that, not in a way that's worse or better.
[00:00:25.640 --> 00:00:28.600]   It might be just as bad or just as unpredictable.
[00:00:28.800 --> 00:00:32.160]   I think this frame of like, man, we haven't cracked the problem yet.
[00:00:32.160 --> 00:00:35.600]   We haven't solved the Riemann hypothesis isn't quite right.
[00:00:35.600 --> 00:00:38.880]   I think of it more as.
[00:00:38.880 --> 00:00:42.320]   Already with today's systems, we are not very good at controlling them,
[00:00:42.320 --> 00:00:45.840]   and the consequences of that could be could be could be very bad.
[00:00:45.840 --> 00:00:51.680]   We just need to get more ways of like increasing the likelihood
[00:00:51.680 --> 00:00:54.640]   that are that are that, you know, that we can control our models
[00:00:54.640 --> 00:00:56.360]   and understand what's going on in them.
[00:00:56.360 --> 00:01:00.400]   And like we have some of them so far, they aren't that good yet.
[00:01:00.400 --> 00:01:05.120]   But, you know, I don't think of this as binary of like works and not works.
[00:01:05.120 --> 00:01:06.720]   We're going to develop more.
[00:01:06.720 --> 00:01:09.200]   And I do think that over over the next two to three years,
[00:01:09.200 --> 00:01:13.120]   we're going to start eating that probability mass of ways things can go wrong.
[00:01:13.120 --> 00:01:16.040]   When I think of like, you know, why am I why am I scared?
[00:01:16.040 --> 00:01:18.080]   Few things I think of.
[00:01:18.080 --> 00:01:21.960]   One is look like I think the thing that's really hard to argue with
[00:01:21.960 --> 00:01:24.040]   is like there will be powerful models.
[00:01:24.040 --> 00:01:25.400]   They will be agentic.
[00:01:25.400 --> 00:01:26.920]   We're getting towards them.
[00:01:26.920 --> 00:01:31.640]   If such a model wanted to wreak havoc and destroy humanity or whatever,
[00:01:31.640 --> 00:01:34.560]   I think we have basically no ability to stop it.
[00:01:34.560 --> 00:01:38.600]   Like that's that's I think just just if that's not true at some point,
[00:01:38.600 --> 00:01:41.240]   it'll it will reach the point where it's true as we scale the models.
[00:01:41.240 --> 00:01:44.040]   So that definitely seems the case.
[00:01:44.040 --> 00:01:48.720]   And I think a second thing that seems the case is that we seem to be bad
[00:01:48.720 --> 00:01:51.560]   at controlling the models, not in any particular way,
[00:01:51.840 --> 00:01:53.880]   but just their statistical systems.
[00:01:53.880 --> 00:01:57.280]   And you can ask a million things and they can say a million things in reply.
[00:01:57.280 --> 00:02:00.680]   And, you know, you might not have thought of a millionth of one thing
[00:02:00.680 --> 00:02:02.360]   that does something crazy.
[00:02:02.360 --> 00:02:05.240]   Or when you train them, you train them in this very abstract way.
[00:02:05.240 --> 00:02:06.680]   And you might not understand
[00:02:06.680 --> 00:02:10.280]   all the consequences of of what they do in response to that.
[00:02:10.280 --> 00:02:13.880]   I mean, I think the best example we've seen of that is like being
[00:02:13.880 --> 00:02:17.280]   and being in Sydney, right, where it's like, I don't know how they train that model.
[00:02:17.280 --> 00:02:20.400]   I don't know what they did to make it do all this weird stuff like,
[00:02:20.400 --> 00:02:23.920]   you know, threaten threaten people and, you know, have this kind of weird,
[00:02:23.920 --> 00:02:25.360]   obsessive personality.
[00:02:25.360 --> 00:02:28.760]   But but what it shows is that we can get something very different
[00:02:28.760 --> 00:02:30.960]   from and maybe opposite to what we intended.
[00:02:30.960 --> 00:02:34.480]   And so I actually think facts, number one, in fact, number two
[00:02:34.480 --> 00:02:36.960]   are like enough to be really worried.
[00:02:36.960 --> 00:02:41.120]   Like you don't need all this detailed stuff about, you know,
[00:02:41.120 --> 00:02:45.440]   convergent instrumental goals or, you know, analogies to evolution.
[00:02:45.440 --> 00:02:48.160]   Like actually one and two for me are pretty motivated.
[00:02:48.160 --> 00:02:50.240]   I'm like, OK, this thing's going to be powerful.
[00:02:50.240 --> 00:02:51.640]   It could destroy us.
[00:02:51.640 --> 00:02:56.600]   And like all the ones built so far, like, you know, are at pretty decent risk
[00:02:56.600 --> 00:02:58.600]   of doing some random shit we don't understand.
[00:02:58.600 --> 00:03:03.520]   If we take our current understanding and move that to two very powerful models,
[00:03:03.520 --> 00:03:06.920]   you might just be in this world where it's like, OK, you make something
[00:03:06.920 --> 00:03:09.160]   and depending on the details, maybe it's totally fine.
[00:03:09.160 --> 00:03:13.240]   You know, not really alignment by default, but but just kind of like
[00:03:13.240 --> 00:03:14.800]   it depends on a lot of the details.
[00:03:14.800 --> 00:03:18.000]   And like if you if you're very careful about all those details
[00:03:18.000 --> 00:03:19.720]   and you know what you're doing, you're getting it right.
[00:03:19.720 --> 00:03:23.680]   But we have a high susceptibility to you mess something up in a way
[00:03:23.680 --> 00:03:26.480]   that you didn't really understand was connected to.
[00:03:26.480 --> 00:03:29.480]   Actually, instead of making all the humans happy, it wants to,
[00:03:29.480 --> 00:03:30.680]   you know, turn them into pumpkins.
[00:03:30.680 --> 00:03:32.640]   Yeah, I just some weird shit, right?
[00:03:32.640 --> 00:03:36.040]   Because the models are so powerful, you know, they're like these kind of giants
[00:03:36.040 --> 00:03:37.880]   that are, you know, they're they're like, you know, they're
[00:03:37.880 --> 00:03:39.000]   standing in the landscape.
[00:03:39.000 --> 00:03:40.800]   And if they start to move their arms around randomly,
[00:03:40.800 --> 00:03:42.280]   they could just break everything.
[00:03:42.280 --> 00:03:43.920]   I don't think we're aligned by default.
[00:03:43.920 --> 00:03:47.520]   I don't think we're doomed by default and have some problem we need to solve.
[00:03:47.800 --> 00:03:49.720]   It has some kind of different character.
[00:03:49.720 --> 00:03:54.520]   Now, what I do think is that hopefully within a timescale of two to three years,
[00:03:54.520 --> 00:03:58.640]   we get better at diagnosing when the models are good and when they're bad.
[00:03:58.640 --> 00:04:04.200]   We get better at increasing our repertoire of methods to train the model
[00:04:04.200 --> 00:04:07.560]   that they're less likely to do bad things and more likely to do good things
[00:04:07.560 --> 00:04:11.000]   in a way that isn't just relevant to the current models, but scales.
[00:04:11.000 --> 00:04:14.680]   And we can help develop that with interpretability as the test set.
[00:04:14.840 --> 00:04:17.840]   You know, it's almost like right now, if I try and, you know,
[00:04:17.840 --> 00:04:20.480]   juggle five balls or something, I can juggle three balls, right?
[00:04:20.480 --> 00:04:23.640]   I actually can. But but I can't juggle five balls at all, right?
[00:04:23.640 --> 00:04:25.440]   You have to practice a lot to do that.
[00:04:25.440 --> 00:04:27.120]   If I were to do that, I would mostly try.
[00:04:27.120 --> 00:04:29.080]   I would I would almost certainly drop them.
[00:04:29.080 --> 00:04:32.640]   And then just just over time, you just get better at the task of controlling
[00:04:32.640 --> 00:04:36.040]   the balls. People love to kind of propose these broad plans and say,
[00:04:36.040 --> 00:04:37.240]   like, oh, this is the way we should do it.
[00:04:37.240 --> 00:04:38.400]   This is the way we should do it.
[00:04:38.400 --> 00:04:41.520]   I think the honest fact is that we're figuring this out as we go along.
[00:04:41.800 --> 00:04:44.800]   The way we discover new things and understand the structure
[00:04:44.800 --> 00:04:48.240]   of what's going to work and what's what's not is by playing around with things.
[00:04:48.240 --> 00:04:51.200]   Not that we should just kind of blindly say, oh, this worked here.
[00:04:51.200 --> 00:04:52.560]   And so we'll work there.
[00:04:52.560 --> 00:04:55.880]   But you you really you really start to understand the patterns.
[00:04:55.880 --> 00:05:00.160]   I think right now there's just there's there's way there's way too many assumptions.
[00:05:00.160 --> 00:05:02.920]   There's way too much overconfidence about how all this is going to go.
[00:05:02.920 --> 00:05:07.440]   I have a substantial probability mass on this all goes wrong.
[00:05:07.440 --> 00:05:09.280]   It's a complete disaster,
[00:05:09.280 --> 00:05:12.080]   but in a completely different way than anyone had anticipated.

