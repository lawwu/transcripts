
[00:00:00.000 --> 00:00:04.840]   Hi, and welcome to this video on implementing transformer models
[00:00:04.840 --> 00:00:06.760]   with TensorFlow.
[00:00:06.760 --> 00:00:10.960]   So we're going to go through six steps, which
[00:00:10.960 --> 00:00:14.680]   are downloading and preprocessing data,
[00:00:14.680 --> 00:00:17.520]   initializing the HuggingFace tokenizer model--
[00:00:17.520 --> 00:00:22.440]   and by HuggingFace, I mean the Transformers framework.
[00:00:22.440 --> 00:00:27.880]   Then we encode input data to get input ID and attention tensors.
[00:00:27.880 --> 00:00:30.880]   Then we build the full model architecture.
[00:00:30.880 --> 00:00:35.760]   So that is our input layers, which go into BERT,
[00:00:35.760 --> 00:00:38.960]   and then the output layers post-BERT.
[00:00:38.960 --> 00:00:42.160]   Then it's back to the normal TensorFlow process
[00:00:42.160 --> 00:00:45.360]   where we set up our optimizer, metrics, and loss,
[00:00:45.360 --> 00:00:47.440]   and then we begin training.
[00:00:47.440 --> 00:00:51.200]   And we will cover each one of these steps in this video.
[00:00:51.200 --> 00:00:54.640]   So first, we need to actually get our data.
[00:00:54.640 --> 00:00:59.680]   So we're going to use the IMDB movie review data set,
[00:00:59.680 --> 00:01:01.840]   or it may actually be Rotten Tomatoes.
[00:01:01.840 --> 00:01:03.840]   I'm not sure.
[00:01:03.840 --> 00:01:07.440]   Now, this data set provides us with sentiment ratings
[00:01:07.440 --> 00:01:11.640]   from 0, which is terrible, up to 4, which is amazing.
[00:01:11.640 --> 00:01:13.760]   You can get the data set from Kaggle,
[00:01:13.760 --> 00:01:16.160]   or we can just download it using the Kaggle API, which
[00:01:16.160 --> 00:01:18.160]   is what we're going to do here.
[00:01:18.160 --> 00:01:21.160]   Now, if you haven't used Kaggle API before, that's fine.
[00:01:21.160 --> 00:01:26.440]   All you need to do is install Kaggle,
[00:01:26.440 --> 00:01:31.320]   and then you need to head over to the Kaggle website,
[00:01:31.320 --> 00:01:37.120]   go to your account page, scroll down to, I think,
[00:01:37.120 --> 00:01:48.120]   it's API integration, download kaggle.json,
[00:01:48.120 --> 00:01:51.160]   and then you need to place kaggle.json
[00:01:51.160 --> 00:01:53.960]   in the correct Kaggle folder, which will have been created
[00:01:53.960 --> 00:01:55.520]   when you did the pip install.
[00:01:55.520 --> 00:01:58.360]   Now, if you're not sure where that is, all you need to do
[00:01:58.360 --> 00:02:03.840]   is import Kaggle, like that.
[00:02:03.840 --> 00:02:07.400]   And when you execute this, a OS error will appear,
[00:02:07.400 --> 00:02:10.880]   and it will say you need kaggle.json, which you don't
[00:02:10.880 --> 00:02:12.920]   have, you need to put it in this folder.
[00:02:12.920 --> 00:02:15.720]   You just go ahead and put kaggle.json in that folder,
[00:02:15.720 --> 00:02:16.960]   and then you are ready.
[00:02:16.960 --> 00:02:27.240]   Now, we just need to initialize our API and authenticate it.
[00:02:27.240 --> 00:02:35.520]   And now, we can use the competition download file
[00:02:35.520 --> 00:02:38.040]   method to download our data.
[00:02:39.040 --> 00:02:42.640]   And we are going to import the data into this directory.
[00:02:42.640 --> 00:02:50.680]   Now, let's refresh up here, and we can see it.
[00:02:50.680 --> 00:02:52.560]   Now, obviously, it's a zip file, so we
[00:02:52.560 --> 00:02:54.560]   need to quickly unzip that.
[00:02:54.560 --> 00:02:57.160]   We can also do this in Python, or you can do it manually.
[00:02:57.160 --> 00:02:59.720]   But for now, we're just going to do it this way.
[00:02:59.720 --> 00:03:03.880]   So we're going to import the data into this directory.
[00:03:03.880 --> 00:03:05.560]   We need to quickly unzip that.
[00:03:05.560 --> 00:03:09.160]   We can also do this in Python, or you can do it manually.
[00:03:09.160 --> 00:03:10.800]   We will just do it manually.
[00:03:10.800 --> 00:03:12.400]   It's easier-- well, quicker, at least.
[00:03:12.400 --> 00:03:32.280]   And there we go.
[00:03:32.280 --> 00:03:33.560]   We now have our data here.
[00:03:33.560 --> 00:03:37.600]   It's a tab-delimited file.
[00:03:37.600 --> 00:03:39.880]   So if we open it, you can see here,
[00:03:39.880 --> 00:03:43.280]   we're delimiting by the tab character.
[00:03:43.280 --> 00:03:46.160]   And we can see we have our phrase and our sentiment,
[00:03:46.160 --> 00:03:50.440]   which are the two that we care most about.
[00:03:50.440 --> 00:03:52.880]   Now, we'll use pandas to read data.
[00:04:01.280 --> 00:04:06.040]   And because it's a tab-delimited file, we use read CSV.
[00:04:06.040 --> 00:04:10.960]   And then we just set the separator to tab.
[00:04:10.960 --> 00:04:15.960]   Now, this data set has a full sentence,
[00:04:15.960 --> 00:04:20.760]   which we can see with the phrase ID is 1 and sentence ID is 1.
[00:04:20.760 --> 00:04:23.120]   That is our full sentence or full phrase.
[00:04:23.120 --> 00:04:27.680]   And then we have lots of parts of that same phrase cut down
[00:04:27.680 --> 00:04:32.520]   into different pieces and given a sentiment value.
[00:04:32.520 --> 00:04:34.600]   Now, I mean, you can use this.
[00:04:34.600 --> 00:04:38.040]   But I'm going to avoid it because I'm just
[00:04:38.040 --> 00:04:41.040]   going to be using the training data for both the training
[00:04:41.040 --> 00:04:42.840]   set and the validation set.
[00:04:42.840 --> 00:04:46.920]   And I don't want to pollute the validation set
[00:04:46.920 --> 00:04:49.240]   with very similar phrases that we
[00:04:49.240 --> 00:04:51.760]   have used in the training data.
[00:04:51.760 --> 00:04:54.840]   So we're just going to drop duplicates and keep
[00:04:54.840 --> 00:04:58.240]   the first element of every unique sentence ID.
[00:04:58.240 --> 00:05:09.920]   And here, you can see that we have now
[00:05:09.920 --> 00:05:14.680]   removed those duplicates or segments.
[00:05:14.680 --> 00:05:21.000]   With the segments removed, we only have 8,500.
[00:05:21.000 --> 00:05:24.760]   Now, we need to move on to encoding our data.
[00:05:24.760 --> 00:05:28.840]   So for that, we are going to be using the transformers
[00:05:28.840 --> 00:05:31.760]   framework, which we will also be using for the transformer
[00:05:31.760 --> 00:05:32.800]   itself.
[00:05:32.800 --> 00:05:38.000]   And this works by providing a tokenizer and the model
[00:05:38.000 --> 00:05:40.800]   itself for each transformer.
[00:05:40.800 --> 00:05:42.600]   So we're going to be using BERT.
[00:05:42.600 --> 00:05:45.640]   And that means that we are going to import or initialize
[00:05:45.640 --> 00:05:49.520]   a BERT model and also the BERT tokenizer, which
[00:05:49.520 --> 00:05:52.080]   is already pre-built.
[00:05:52.080 --> 00:05:54.640]   Now, before we do this encoding, we
[00:05:54.640 --> 00:05:59.000]   need to figure out how long we want each sequence to be,
[00:05:59.000 --> 00:06:02.600]   because the encoding method also acts as our padding
[00:06:02.600 --> 00:06:05.200]   or truncation method.
[00:06:05.200 --> 00:06:08.160]   So to do that, we will get the sequence length
[00:06:08.160 --> 00:06:12.680]   in words of each sentence and plot that out and just
[00:06:12.680 --> 00:06:16.240]   go by eye and say, OK, around here,
[00:06:16.240 --> 00:06:19.000]   won't cut off too much data.
[00:06:19.000 --> 00:06:21.440]   So first, we need to get the sequence length
[00:06:21.440 --> 00:06:23.360]   of every sentence in here.
[00:06:23.360 --> 00:06:30.320]   Now, what we're going to do here is get the length
[00:06:30.320 --> 00:06:34.320]   of each sentence split.
[00:06:34.320 --> 00:06:38.680]   Now, split will, by default, split by spaces.
[00:06:38.680 --> 00:06:41.520]   So we will get a list of words here.
[00:06:41.520 --> 00:06:43.280]   Now, we need to actually visualize this.
[00:06:43.280 --> 00:06:45.680]   So we will use matplotlib and seaborn
[00:06:45.680 --> 00:06:48.480]   just because they're super easy and quick to use.
[00:06:48.480 --> 00:06:56.680]   And then we will also set the seaborn style just
[00:06:56.680 --> 00:06:59.400]   to make it a bit more visually appealing.
[00:06:59.400 --> 00:07:06.480]   And we will also increase the figure size
[00:07:06.480 --> 00:07:08.080]   so we can actually see what's going on.
[00:07:08.080 --> 00:07:14.720]   And then we will use a distribution plot.
[00:07:14.720 --> 00:07:20.920]   And here, we can see the distribution
[00:07:20.920 --> 00:07:26.480]   of the length of each sequence in our data set.
[00:07:26.480 --> 00:07:32.600]   Now, we could cut it off maybe around 40 or even 50.
[00:07:32.600 --> 00:07:36.280]   I think we'll go with 50 just so we get as much data in there
[00:07:36.280 --> 00:07:37.000]   as possible.
[00:07:40.800 --> 00:07:47.480]   So we'll set sequence length equal to 50.
[00:07:47.480 --> 00:07:50.600]   Now, we need to initialize our tokenizer.
[00:07:50.600 --> 00:07:55.040]   And before we do that, actually, we
[00:07:55.040 --> 00:07:58.120]   need to import it from the Transformers library.
[00:08:10.680 --> 00:08:14.320]   And we are getting our model from a pre-trained model.
[00:08:14.320 --> 00:08:19.920]   And we're using BERT, base, cased.
[00:08:19.920 --> 00:08:30.760]   Now, cased here refers to whether BERT distinguishes
[00:08:30.760 --> 00:08:34.440]   the difference between uppercase and lowercase characters.
[00:08:34.440 --> 00:08:36.800]   The alternative would be uncased.
[00:08:36.800 --> 00:08:39.000]   And this would just not distinguish
[00:08:39.000 --> 00:08:41.440]   the difference between uppercase and lowercase.
[00:08:41.440 --> 00:08:44.040]   Everything would just become lowercase.
[00:08:44.040 --> 00:08:46.520]   But when people are on the internet
[00:08:46.520 --> 00:08:49.560]   and they want to shout and seem angry,
[00:08:49.560 --> 00:08:52.240]   people put everything in capital letters.
[00:08:52.240 --> 00:08:54.720]   So BERT can probably pick up on this
[00:08:54.720 --> 00:08:58.320]   and tell that someone is being dramatic or shouting at you
[00:08:58.320 --> 00:09:00.080]   over the internet or whatever else.
[00:09:00.080 --> 00:09:02.160]   And because we are classifying sentiment here,
[00:09:02.160 --> 00:09:03.920]   it's probably quite important.
[00:09:07.200 --> 00:09:08.960]   Now that we've initialized our tokenizer,
[00:09:08.960 --> 00:09:11.760]   we can go on to the encoding.
[00:09:11.760 --> 00:09:15.520]   So we use the encode plus method, which looks like this.
[00:09:15.520 --> 00:09:31.960]   So you'll see here we've just defined or hardcoded
[00:09:31.960 --> 00:09:35.000]   a single line, which is hello world.
[00:09:35.000 --> 00:09:37.480]   We are using a max length of 50.
[00:09:37.480 --> 00:09:41.640]   We want the encoder to truncate any text
[00:09:41.640 --> 00:09:43.000]   that is longer than 50 tokens.
[00:09:43.000 --> 00:09:44.880]   Obviously, this one will not be.
[00:09:44.880 --> 00:09:48.080]   But when we are feeding all of our data through it,
[00:09:48.080 --> 00:09:49.480]   we need this in there.
[00:09:49.480 --> 00:09:53.400]   And on the other hand,
[00:09:53.400 --> 00:09:57.240]   we also want anything shorter than 50
[00:09:57.240 --> 00:10:00.280]   to be padded with pad tokens.
[00:10:00.280 --> 00:10:03.760]   In this case, we will end up with 48 of these padding tokens.
[00:10:03.760 --> 00:10:08.760]   And here we are just telling the tokenizer
[00:10:08.760 --> 00:10:13.640]   to pad up to the value that we have given
[00:10:13.640 --> 00:10:15.560]   in the max length argument.
[00:10:15.560 --> 00:10:20.760]   Now BERT comes with several special tokens.
[00:10:20.760 --> 00:10:24.200]   We have the start sequence, end of sequence,
[00:10:24.200 --> 00:10:27.200]   padding, unknown, and mask tokens.
[00:10:27.200 --> 00:10:32.200]   In order to add these in during the encoding method here,
[00:10:32.200 --> 00:10:35.760]   we need to write add special tokens true.
[00:10:35.760 --> 00:10:41.360]   And in this case, all it's gonna do
[00:10:41.360 --> 00:10:44.520]   is add the start of sequence token,
[00:10:44.520 --> 00:10:46.200]   the end of sequence token,
[00:10:46.200 --> 00:10:48.720]   and then it's gonna add all of our padding values.
[00:10:48.720 --> 00:10:52.400]   Now, there are several different outputs
[00:10:52.400 --> 00:10:55.680]   that we can get from this encode plus method.
[00:10:55.680 --> 00:10:57.400]   By default, we have input IDs
[00:10:57.400 --> 00:11:00.640]   and the return token type IDs.
[00:11:00.640 --> 00:11:03.200]   Now, the token type IDs we don't really need,
[00:11:03.200 --> 00:11:06.680]   so we can tell the tokenizer to not return those.
[00:11:06.680 --> 00:11:13.240]   But we do need input IDs,
[00:11:13.240 --> 00:11:15.800]   which is fine, we get them by default,
[00:11:15.800 --> 00:11:18.880]   and also the attention mask tensor.
[00:11:18.880 --> 00:11:20.360]   To return this, we just write return
[00:11:20.360 --> 00:11:21.720]   attention mask equals true.
[00:11:21.720 --> 00:11:28.200]   Finally, because we are working in TensorFlow,
[00:11:28.200 --> 00:11:30.480]   we need to return TensorFlow tensors.
[00:11:30.480 --> 00:11:43.800]   Okay, so here we have our outputs.
[00:11:43.800 --> 00:11:45.200]   So we get two tensors.
[00:11:45.200 --> 00:11:47.520]   One of those are the input IDs,
[00:11:47.520 --> 00:11:51.420]   and then also our attention mask here.
[00:11:51.420 --> 00:11:54.760]   So we input the sequence hello world,
[00:11:54.760 --> 00:11:57.400]   and we can see that this value here
[00:11:57.400 --> 00:12:00.920]   and input IDs is hello, and this is world.
[00:12:00.920 --> 00:12:05.320]   Now, the 101 and 102 you see here
[00:12:05.320 --> 00:12:09.600]   are the start of sequence and end of sequence tokens
[00:12:09.600 --> 00:12:12.400]   used by BERT, and the remaining zeros
[00:12:12.400 --> 00:12:14.160]   are simply the padding tokens.
[00:12:14.160 --> 00:12:16.440]   We also have the attention mask,
[00:12:16.440 --> 00:12:19.280]   and this is used to tell BERT
[00:12:19.280 --> 00:12:22.240]   what tokens to calculate attention for,
[00:12:22.240 --> 00:12:24.720]   and which tokens to just completely ignore.
[00:12:24.720 --> 00:12:28.600]   So where we have a one, that tells BERT,
[00:12:28.600 --> 00:12:30.680]   yep, pay attention to this.
[00:12:30.680 --> 00:12:34.620]   Where there's a zero, it means just ignore it.
[00:12:34.620 --> 00:12:37.320]   So we have zeros for every padding token
[00:12:37.320 --> 00:12:39.600]   because padding tokens aren't important to us,
[00:12:39.600 --> 00:12:41.480]   it's just padding.
[00:12:41.480 --> 00:12:45.520]   But then we have ones for the end of sequence
[00:12:45.520 --> 00:12:47.440]   and start of sequence tokens,
[00:12:47.440 --> 00:12:49.320]   and also hello and world,
[00:12:49.320 --> 00:12:51.520]   because they are actually important
[00:12:51.520 --> 00:12:53.600]   for BERT to pay attention to.
[00:12:53.600 --> 00:12:56.480]   Now, of course, this is just one,
[00:12:56.480 --> 00:13:01.480]   and we need to do this for every sample in our dataset.
[00:13:01.480 --> 00:13:02.880]   So we'll go ahead and do that.
[00:13:02.880 --> 00:13:06.040]   Now, we're just gonna use a simple for loop
[00:13:06.040 --> 00:13:11.040]   to add each value or sequence into a NumPy array,
[00:13:11.040 --> 00:13:13.360]   which we will initialize now.
[00:13:13.360 --> 00:13:15.700]   So first import NumPy,
[00:13:19.000 --> 00:13:21.160]   and then initialize our two arrays.
[00:13:21.160 --> 00:13:27.520]   Both of them are gonna be the same size,
[00:13:27.520 --> 00:13:30.720]   so it's gonna be the length of our data frame
[00:13:30.720 --> 00:13:36.140]   by the sequence length that we have defined, which is 50.
[00:13:36.140 --> 00:13:44.720]   And we do this twice.
[00:13:44.720 --> 00:13:47.260]   We also want one for the attention mask.
[00:13:48.220 --> 00:13:53.220]   And we can see the shape of our arrays here.
[00:13:53.220 --> 00:14:00.500]   Now, we're just gonna use a for loop to do this.
[00:14:00.500 --> 00:14:01.700]   It's only a small dataset,
[00:14:01.700 --> 00:14:05.780]   so whatever we use doesn't really matter.
[00:14:05.780 --> 00:14:09.380]   We are only processing and encoding this data one time,
[00:14:09.380 --> 00:14:12.780]   and then we will save it and then load it from memory
[00:14:12.780 --> 00:14:14.740]   when we're actually training our model.
[00:14:14.740 --> 00:14:19.740]  , and there we have our for loop.
[00:14:43.100 --> 00:14:45.700]   This will go through each sequence
[00:14:45.700 --> 00:14:48.100]   and add each one of those sequences
[00:14:48.100 --> 00:14:51.020]   into the perspective index
[00:14:51.020 --> 00:14:54.440]   of our initialized zero arrays here.
[00:14:54.440 --> 00:15:05.700]   Okay, and then here we can see our complete arrays.
[00:15:05.700 --> 00:15:09.460]   So at the top, we have our input IDs and XIDs,
[00:15:09.460 --> 00:15:11.260]   and we can see each one starts
[00:15:11.260 --> 00:15:14.340]   with our start of sequence token, 101,
[00:15:14.340 --> 00:15:15.820]   followed by a few words.
[00:15:15.820 --> 00:15:17.340]   Then there will be the end of sequence token
[00:15:17.340 --> 00:15:18.700]   somewhere in the middle there,
[00:15:18.700 --> 00:15:22.180]   and then at the end, we have our padding token.
[00:15:22.180 --> 00:15:24.580]   And then we can also see in Xmask,
[00:15:24.580 --> 00:15:26.060]   which is our attention mask,
[00:15:26.060 --> 00:15:28.700]   we have the ones to pay attention to
[00:15:28.700 --> 00:15:30.620]   and the zeros to ignore,
[00:15:30.620 --> 00:15:34.180]   which obviously correspond to the respective values
[00:15:34.180 --> 00:15:35.700]   in the XIDs array.
[00:15:35.700 --> 00:15:38.940]   Now, for our labels,
[00:15:38.940 --> 00:15:41.940]   we are actually going to one-hot encode them.
[00:15:41.940 --> 00:15:45.460]   So at the moment, let's see what we have.
[00:15:45.460 --> 00:15:55.320]   So we will get values of four, one, three, two, and zero.
[00:15:55.320 --> 00:16:00.860]   Though you can't see it here.
[00:16:00.860 --> 00:16:03.140]   Now, this is a pretty good format
[00:16:03.140 --> 00:16:05.980]   to go straight in and one-hot encode.
[00:16:05.980 --> 00:16:09.580]   So all we need to do here is create a array
[00:16:09.580 --> 00:16:11.780]   from the DataFrame column.
[00:16:11.780 --> 00:16:22.380]   And then here, like we did before,
[00:16:22.380 --> 00:16:25.460]   we are just initializing a empty zero array.
[00:16:25.460 --> 00:16:34.640]   And what we are doing here is we are taking the array size.
[00:16:34.640 --> 00:16:39.620]   So if I, okay, let's do it a little differently.
[00:16:39.620 --> 00:16:48.140]   So the array size is just the length of our DataFrame.
[00:16:48.140 --> 00:16:53.140]   And array.max is what we see maximum value within our array,
[00:16:53.140 --> 00:16:55.220]   which is the number four.
[00:16:55.220 --> 00:16:56.860]   And then we are adding one onto that,
[00:16:56.860 --> 00:16:59.840]   which is saying here that we want a zero array
[00:16:59.840 --> 00:17:04.840]   of 8,529 rows by five columns.
[00:17:04.840 --> 00:17:08.940]   Like so.
[00:17:08.940 --> 00:17:11.460]   Now, at the moment, we just have an empty zero array.
[00:17:11.460 --> 00:17:14.300]   So we just need to add ones in the indices
[00:17:14.300 --> 00:17:16.900]   where we have a value.
[00:17:16.900 --> 00:17:18.900]   And we do that very easily like this.
[00:17:18.900 --> 00:17:26.860]   So we create a range of values
[00:17:26.860 --> 00:17:31.860]   from zero to 8,528, which is the array size here.
[00:17:31.860 --> 00:17:36.660]   And then within that, we add array.
[00:17:36.660 --> 00:17:39.460]   Because in array, we have each sentiment value.
[00:17:39.460 --> 00:17:41.380]   So zero, four, three, two.
[00:17:41.380 --> 00:17:44.940]   And this will add a one in that index.
[00:17:44.940 --> 00:17:46.940]   So zero, three, four.
[00:17:46.940 --> 00:17:50.040]   And that produces our one-hot encoding.
[00:17:50.040 --> 00:17:54.420]   Like so.
[00:17:54.420 --> 00:17:57.700]   Here, our rating was one.
[00:17:57.700 --> 00:17:59.420]   Here, four.
[00:17:59.420 --> 00:18:00.260]   Here, one.
[00:18:00.260 --> 00:18:01.080]   And so on.
[00:18:01.080 --> 00:18:05.100]   Now, I said before that typically we'd save these
[00:18:05.100 --> 00:18:06.780]   before loading them in.
[00:18:06.780 --> 00:18:08.620]   So we are gonna save them.
[00:18:08.620 --> 00:18:10.940]   Obviously, we don't need to load them back in.
[00:18:10.940 --> 00:18:13.580]   But I will show you how to anyway,
[00:18:13.580 --> 00:18:17.500]   because going forwards, if you want to retrain data,
[00:18:17.500 --> 00:18:19.060]   you have to do this.
[00:18:19.060 --> 00:18:21.060]   Otherwise, you'd have to do everything all over again.
[00:18:21.060 --> 00:18:23.220]   And when you're working with bigger datasets,
[00:18:23.220 --> 00:18:25.500]   that will take quite some time.
[00:18:25.500 --> 00:18:28.900]   So first, we'll just save them.
[00:18:28.900 --> 00:18:31.060]   (silence)
[00:18:31.060 --> 00:18:54.420]   So now we've saved them,
[00:18:54.420 --> 00:18:57.140]   and we've just removed all of them from memory.
[00:18:57.140 --> 00:19:01.260]   So now we will not be able to access any of them.
[00:19:01.260 --> 00:19:05.020]   Going forwards, we are just going to load them back in,
[00:19:05.020 --> 00:19:06.940]   which is exactly the same process.
[00:19:06.940 --> 00:19:08.140]   Quickly write that out.
[00:19:08.140 --> 00:19:10.300]   (silence)
[00:19:10.300 --> 00:19:12.460]   (silence)
[00:19:12.460 --> 00:19:30.540]   And now we have our data back in.
[00:19:30.540 --> 00:19:33.060]   Like so.
[00:19:33.060 --> 00:19:37.980]   Okay, so now we need to put all of our arrays
[00:19:37.980 --> 00:19:40.340]   into a TensorFlow dataset object.
[00:19:40.340 --> 00:19:43.620]   So we'll use a dataset object
[00:19:43.620 --> 00:19:46.020]   because it makes things a lot easier.
[00:19:46.020 --> 00:19:49.220]   So we can restructure the data, shuffle,
[00:19:49.220 --> 00:19:51.940]   and batch it in just a few lines of code,
[00:19:51.940 --> 00:19:55.020]   which is a lot faster in terms of performance,
[00:19:55.020 --> 00:19:57.860]   and also a lot faster for us to write down.
[00:19:57.860 --> 00:19:59.460]   It's a much simpler code.
[00:19:59.460 --> 00:20:02.140]   So we'll import TensorFlow.
[00:20:05.660 --> 00:20:08.300]   And also, if you're using a GPU,
[00:20:08.300 --> 00:20:10.860]   you can check that it is being detected
[00:20:10.860 --> 00:20:13.020]   by your system with this.
[00:20:13.020 --> 00:20:23.820]   And there we can see I have a GPU
[00:20:23.820 --> 00:20:26.260]   being picked up by TensorFlow.
[00:20:26.260 --> 00:20:28.620]   So first, we need to restructure the data.
[00:20:28.620 --> 00:20:33.620]   TensorFlow expects our data to be input as a tuple.
[00:20:33.620 --> 00:20:36.220]   So that tuple consists of our inputs
[00:20:36.220 --> 00:20:38.820]   and our target or output labels.
[00:20:38.820 --> 00:20:41.820]   Now, because we are using BERT,
[00:20:41.820 --> 00:20:43.940]   our data structure is slightly different
[00:20:43.940 --> 00:20:45.660]   because in the input tuple,
[00:20:45.660 --> 00:20:48.380]   we actually have a dictionary,
[00:20:48.380 --> 00:20:50.780]   and that dictionary contains a key
[00:20:50.780 --> 00:20:55.780]   that is input_ids, which maps to our xids array.
[00:20:55.780 --> 00:20:58.860]   We also have another key, attention_mask,
[00:20:58.860 --> 00:21:02.100]   which maps to our xmask array.
[00:21:03.100 --> 00:21:06.100]   So first, we actually need to create our dataset object,
[00:21:06.100 --> 00:21:08.260]   which we do like so.
[00:21:08.260 --> 00:21:21.820]   And this just creates a generator
[00:21:21.820 --> 00:21:23.620]   which contains all of our data
[00:21:23.620 --> 00:21:26.460]   in the tuple-like format
[00:21:26.460 --> 00:21:30.340]   where each tuple contains one xids array,
[00:21:30.340 --> 00:21:32.980]   xmask array, and label array.
[00:21:33.900 --> 00:21:36.620]   So we can actually view one of those, like so.
[00:21:36.620 --> 00:21:43.500]   And here we can see each one of our arrays.
[00:21:43.500 --> 00:21:47.620]   So the first one, xids, xmask, and then labels.
[00:21:47.620 --> 00:21:52.660]   Now, TensorFlow expects our input data
[00:21:52.660 --> 00:21:56.220]   as a dataset to be in a tuple format.
[00:21:56.220 --> 00:22:00.380]   The zero index of that tuple needs to be our input values
[00:22:00.380 --> 00:22:05.380]   and the one index of that tuple needs to be our labels.
[00:22:05.380 --> 00:22:08.820]   Now, in our case, it's also slightly different
[00:22:08.820 --> 00:22:12.220]   because we are using two inputs.
[00:22:12.220 --> 00:22:14.420]   So within that tuple, we have our input,
[00:22:14.420 --> 00:22:18.660]   and within that input, we have a dictionary.
[00:22:18.660 --> 00:22:21.940]   And that dictionary consists of a key,
[00:22:21.940 --> 00:22:24.780]   input_ids, which maps to our xids array,
[00:22:24.780 --> 00:22:29.100]   and attention_mask, which must map to our xmask array.
[00:22:30.020 --> 00:22:31.740]   So to create this structure,
[00:22:31.740 --> 00:22:35.020]   we need to build a mapping function, like so.
[00:22:35.020 --> 00:22:44.220]   And here we return the format we need.
[00:22:44.220 --> 00:22:49.220]   So input_ids, which will map to input_ids,
[00:22:49.220 --> 00:22:54.860]   and attention_mask,
[00:22:58.380 --> 00:23:00.140]   which will map to mask.
[00:23:00.140 --> 00:23:04.060]   Then, because we are still expecting
[00:23:04.060 --> 00:23:05.900]   to use the input output tuple,
[00:23:05.900 --> 00:23:08.260]   we also need to add labels onto the end there.
[00:23:08.260 --> 00:23:13.780]   And to apply this mapping function to our dataset object,
[00:23:13.780 --> 00:23:15.820]   we use the map method.
[00:23:15.820 --> 00:23:25.980]   So now we can view a single row in this dataset.
[00:23:26.940 --> 00:23:29.100]   (silence)
[00:23:29.100 --> 00:23:34.100]   And now we can see a slightly different format.
[00:23:34.100 --> 00:23:37.660]   So we have our input dictionary here,
[00:23:37.660 --> 00:23:40.620]   where we have input_ids and attention_mask,
[00:23:40.620 --> 00:23:44.460]   and then we have our output tensor here.
[00:23:44.460 --> 00:23:48.620]   Now, I said before that this makes shuffling
[00:23:48.620 --> 00:23:51.340]   and batching our dataset very easy.
[00:23:51.340 --> 00:23:55.380]   So we can do it in a single line, like this.
[00:23:55.980 --> 00:23:58.140]   (silence)
[00:23:58.140 --> 00:24:07.220]   So here we're gonna put our samples into batches of 32,
[00:24:07.220 --> 00:24:08.780]   and then the value that I've given
[00:24:08.780 --> 00:24:10.780]   to the shuffle method here,
[00:24:10.780 --> 00:24:13.340]   essentially just needs to be very large.
[00:24:13.340 --> 00:24:14.940]   The larger your dataset,
[00:24:14.940 --> 00:24:16.700]   the larger this number needs to be.
[00:24:16.700 --> 00:24:22.300]   I take a sample of my dataset and increase the number.
[00:24:22.300 --> 00:24:24.980]   If I can see that this is not shuffling,
[00:24:24.980 --> 00:24:26.540]   the dataset properly.
[00:24:26.540 --> 00:24:32.500]   Okay, so now we have our shuffle batch dataset
[00:24:32.500 --> 00:24:33.940]   in the correct format.
[00:24:33.940 --> 00:24:36.340]   So now we just need to split it
[00:24:36.340 --> 00:24:38.980]   into our training and validation sets.
[00:24:38.980 --> 00:24:41.180]   Okay, and to do that,
[00:24:41.180 --> 00:24:45.260]   we need to get the total size of our dataset
[00:24:45.260 --> 00:24:46.500]   now that it is batched.
[00:24:46.500 --> 00:24:48.820]   So we can do that, like so.
[00:24:54.940 --> 00:24:57.860]   Now, because the dataset object is a generator,
[00:24:57.860 --> 00:25:00.700]   we can't just take the length of it directly,
[00:25:00.700 --> 00:25:02.580]   so we have to convert it into a list.
[00:25:02.580 --> 00:25:06.820]   Now, if you're working with a very large dataset,
[00:25:06.820 --> 00:25:08.700]   this is probably not the right method to use,
[00:25:08.700 --> 00:25:12.460]   and you should instead take the size of the dataset
[00:25:12.460 --> 00:25:14.140]   that you know already,
[00:25:14.140 --> 00:25:16.900]   and calculate the current length of it,
[00:25:16.900 --> 00:25:18.820]   including the batch size from that.
[00:25:18.820 --> 00:25:21.860]   But for us, this is fine,
[00:25:21.860 --> 00:25:26.860]   and we can see that dataset length is 267.
[00:25:26.860 --> 00:25:30.980]   Now, if we want a nine to 10 split
[00:25:30.980 --> 00:25:34.020]   between the training and validation set,
[00:25:34.020 --> 00:25:37.660]   we simply use a split value of 0.9.
[00:25:37.660 --> 00:25:44.420]   And then we get our training and validation sets
[00:25:44.420 --> 00:25:46.540]   as two different datasets.
[00:25:49.860 --> 00:25:53.060]   To split the dataset, we use take and skip.
[00:25:53.060 --> 00:25:54.540]   We used take earlier on,
[00:25:54.540 --> 00:25:59.380]   and what it does is simply takes the specified number of,
[00:25:59.380 --> 00:26:02.180]   in this case, batches, and nothing else.
[00:26:02.180 --> 00:26:04.060]   Skip, on the other hand, does the opposite.
[00:26:04.060 --> 00:26:06.780]   It skips a specified number of batches,
[00:26:06.780 --> 00:26:09.260]   and then takes the remainder.
[00:26:09.260 --> 00:26:12.020]   (mouse clicking)
[00:26:12.020 --> 00:26:29.580]   And then at the end here, we can delete dataset
[00:26:29.580 --> 00:26:31.020]   if space is an issue.
[00:26:31.020 --> 00:26:36.100]   Okay, so now our data is ready.
[00:26:36.100 --> 00:26:40.540]   We can go on to actually building our model architecture.
[00:26:40.540 --> 00:26:43.060]   So first, we initialize BERT,
[00:26:43.060 --> 00:26:46.660]   and to do that, we need to import tf.autoModel
[00:26:46.660 --> 00:26:48.380]   from the Transformers library.
[00:26:48.380 --> 00:26:55.700]   And we initialize BERT like so.
[00:26:55.700 --> 00:27:04.700]   So here, remember to use the same model
[00:27:04.700 --> 00:27:07.100]   that you're using to initialize your tokenizer.
[00:27:07.100 --> 00:27:13.940]   And here, we can see that we have now imported BERT.
[00:27:13.940 --> 00:27:15.460]   So we have BERT,
[00:27:15.460 --> 00:27:19.660]   but we need to build a network around BERT as well.
[00:27:19.660 --> 00:27:22.940]   First thing we need to do is define our input layers,
[00:27:22.940 --> 00:27:24.140]   and of course, there are two,
[00:27:24.140 --> 00:27:26.740]   because we have the input IDs and the attention mask.
[00:27:33.740 --> 00:27:36.700]   And the shape is simply the sequence length
[00:27:36.700 --> 00:27:37.740]   that we are using.
[00:27:37.740 --> 00:27:43.540]   And the name here is very important.
[00:27:43.540 --> 00:27:47.660]   This needs to match up to the dictionary value
[00:27:47.660 --> 00:27:51.100]   that we have defined in our dataset here.
[00:27:51.100 --> 00:27:56.060]   So input IDs, input IDs, and attention mask, okay?
[00:27:56.060 --> 00:27:57.700]   So these need to match up.
[00:27:57.700 --> 00:28:02.180]   Otherwise, TensorFlow does not know where these are going.
[00:28:02.180 --> 00:28:04.940]   (mouse clicking)
[00:28:04.940 --> 00:28:10.740]   And we do the same here,
[00:28:10.740 --> 00:28:12.940]   but we are doing this for the attention mask.
[00:28:12.940 --> 00:28:25.380]   So those are our two input layers,
[00:28:25.380 --> 00:28:28.180]   and now we need to pull the embeddings
[00:28:28.180 --> 00:28:30.140]   from our initialized BERT model.
[00:28:30.780 --> 00:28:32.980]   (mouse clicking)
[00:28:32.980 --> 00:28:35.780]   BERT consumes our two input layers,
[00:28:35.780 --> 00:28:45.940]   like so, and BERT will return two tensors to us.
[00:28:45.940 --> 00:28:48.140]   One of those is the last hidden state,
[00:28:48.140 --> 00:28:49.980]   which is what we are interested in.
[00:28:49.980 --> 00:28:53.980]   That is a 3D tensor, which provides all the information
[00:28:53.980 --> 00:28:57.660]   from the last hidden state of the BERT model.
[00:28:59.180 --> 00:29:03.300]   The second tensor that we are going to ignore
[00:29:03.300 --> 00:29:05.340]   is called the pooler output,
[00:29:05.340 --> 00:29:08.980]   and the pooler output is essentially the last hidden state
[00:29:08.980 --> 00:29:11.620]   run through a feed-forward
[00:29:11.620 --> 00:29:14.700]   or linear activation function and pooled.
[00:29:14.700 --> 00:29:18.180]   So that creates a 2D tensor,
[00:29:18.180 --> 00:29:21.740]   which can be used for classification if you want,
[00:29:21.740 --> 00:29:24.020]   but we are going to pool it ourselves,
[00:29:24.020 --> 00:29:27.020]   so we will not be using that tensor.
[00:29:27.740 --> 00:29:29.940]   (mouse clicking)
[00:29:29.940 --> 00:29:34.740]   Okay, so here you can experiment with adding LSTM layers,
[00:29:34.740 --> 00:29:37.780]   convolutional layers, or anything else,
[00:29:37.780 --> 00:29:40.380]   but for now, to keep things simple,
[00:29:40.380 --> 00:29:43.220]   we are just going to add a global max pooling layer,
[00:29:43.220 --> 00:29:48.220]   which will convert our output 3D tensor into a 2D tensor.
[00:29:48.220 --> 00:29:51.180]   Again, you could skip this,
[00:29:51.180 --> 00:29:55.220]   and you could just output the pooler output tensor,
[00:29:55.220 --> 00:29:58.820]   like this, by changing the zero to a one,
[00:29:58.820 --> 00:30:00.660]   but we are not going to be using that.
[00:30:00.660 --> 00:30:05.020]   (mouse clicking)
[00:30:05.020 --> 00:30:16.500]   Okay, so up here,
[00:30:16.500 --> 00:30:20.140]   we just need to define the input data types as well,
[00:30:20.140 --> 00:30:21.060]   which I missed.
[00:30:21.060 --> 00:30:23.820]   (mouse clicking)
[00:30:23.820 --> 00:30:28.380]   And that will remove the type error.
[00:30:28.380 --> 00:30:34.460]   Now, we need to normalize our outputs here.
[00:30:34.460 --> 00:30:38.060]   This will almost always give better results
[00:30:38.060 --> 00:30:40.020]   when we are actually training the model.
[00:30:40.020 --> 00:30:42.780]   (mouse clicking)
[00:30:42.780 --> 00:30:48.980]   And then following this,
[00:30:48.980 --> 00:30:52.620]   we will go into our Densely Connected Neural Network layers,
[00:30:52.620 --> 00:30:55.820]   which are in charge of actually figuring out
[00:30:55.820 --> 00:30:59.660]   the classification of our BERT embedding outputs.
[00:30:59.660 --> 00:31:02.420]   (mouse clicking)
[00:31:18.260 --> 00:31:21.460]   And then we want to add a dropout layer here.
[00:31:21.460 --> 00:31:26.260]   This just prevents any overfitting or too much overfitting.
[00:31:26.260 --> 00:31:31.820]   Then we add another Densely Connected Neural Network.
[00:31:31.820 --> 00:31:44.620]   And finally, we are creating our output layer,
[00:31:44.620 --> 00:31:47.100]   which is going to be a Densely Connected Neural Network
[00:31:47.100 --> 00:31:50.140]   with a Softmax activation function.
[00:31:50.140 --> 00:31:59.980]   Now, we use Softmax here because we have our three labels,
[00:31:59.980 --> 00:32:03.220]   or no, sorry, five labels in the output.
[00:32:03.220 --> 00:32:04.620]   So let me change this.
[00:32:04.620 --> 00:32:07.700]   So we have our five labels in the output
[00:32:07.700 --> 00:32:09.660]   because we have one hot encoded,
[00:32:09.660 --> 00:32:12.260]   the zero, one, two, three, and four.
[00:32:16.060 --> 00:32:19.420]   And finally, we just give it a name of outputs.
[00:32:19.420 --> 00:32:26.420]   Now, that is our model architecture,
[00:32:26.420 --> 00:32:29.740]   but we still need to tell TensorFlow
[00:32:29.740 --> 00:32:33.300]   what our input layers are and what our output layer is.
[00:32:33.300 --> 00:32:37.220]   So to do that, we define our model, like so.
[00:32:37.220 --> 00:32:43.900]   And to the inputs, we pass input IDs and math.
[00:32:44.900 --> 00:32:47.260]   And then to the outputs, we just pass Y.
[00:32:47.260 --> 00:32:53.780]   Finally, we have our model,
[00:32:53.780 --> 00:32:57.340]   so we can actually execute that
[00:32:57.340 --> 00:32:59.260]   and produce a model summary here
[00:32:59.260 --> 00:33:01.940]   so we can see what we have built.
[00:33:01.940 --> 00:33:11.500]   Okay, and here we can see our model.
[00:33:11.980 --> 00:33:13.460]   Now, if we scroll down to the bottom,
[00:33:13.460 --> 00:33:16.580]   we can see the number of parameters in our model,
[00:33:16.580 --> 00:33:19.700]   and we have quite a lot, 108 parameters.
[00:33:19.700 --> 00:33:22.500]   Almost all of them are trainable.
[00:33:22.500 --> 00:33:24.980]   Now, BERT is a very big model,
[00:33:24.980 --> 00:33:28.740]   and I wouldn't recommend training it
[00:33:28.740 --> 00:33:33.220]   unless you have a specific reason to.
[00:33:33.220 --> 00:33:37.580]   Now, for this dataset, it's definitely overkill.
[00:33:37.580 --> 00:33:40.180]   So what we can do is actually go in here
[00:33:40.180 --> 00:33:43.900]   and we can actually freeze the BERT model
[00:33:43.900 --> 00:33:49.420]   by freezing the third layer at index two
[00:33:49.420 --> 00:33:51.180]   of our model layers.
[00:33:51.180 --> 00:33:55.340]   And we simply set trainable equal to false to do that.
[00:33:55.340 --> 00:34:00.900]   So if we now look at our model summary,
[00:34:00.900 --> 00:34:03.580]   we will see that the number of parameters
[00:34:03.580 --> 00:34:05.140]   is exactly the same,
[00:34:05.140 --> 00:34:07.340]   but the number of trainable parameters
[00:34:07.340 --> 00:34:09.140]   will have reduced by a lot.
[00:34:10.140 --> 00:34:15.140]   So here, we now have 104,000 trainable parameters
[00:34:15.140 --> 00:34:19.220]   rather than 108 million trainable parameters.
[00:34:19.220 --> 00:34:23.820]   We can go ahead and put together our optimizer,
[00:34:23.820 --> 00:34:28.020]   loss and accuracy, compile our model, and begin training.
[00:34:28.020 --> 00:34:31.380]   Now, for our optimizer, we're just gonna use Adam.
[00:34:35.700 --> 00:34:40.700]   With a learning rate of 0.01.
[00:34:40.700 --> 00:34:48.020]   For the loss, because we are using one-hot encoding
[00:34:48.020 --> 00:34:49.740]   for our outputs, we are going to use
[00:34:49.740 --> 00:34:52.020]   the categorical cross-entropy loss.
[00:34:52.020 --> 00:35:00.380]   And finally, for our accuracy,
[00:35:00.380 --> 00:35:02.980]   we are also gonna use categorical accuracy
[00:35:02.980 --> 00:35:03.980]   for the same reason.
[00:35:04.980 --> 00:35:07.140]   (silence)
[00:35:07.140 --> 00:35:14.900]   And we can compile our model.
[00:35:14.900 --> 00:35:28.620]   So here, I've just missed the R.
[00:35:32.140 --> 00:35:35.060]   And now we can actually train our model.
[00:35:35.060 --> 00:35:37.980]   So just do model fit.
[00:35:37.980 --> 00:35:45.420]   We have our training data, our validation set.
[00:35:45.420 --> 00:35:53.460]   And I've found that for this model,
[00:35:53.460 --> 00:35:55.740]   we have to use a lot of epochs
[00:35:55.740 --> 00:35:57.940]   to actually get a good accuracy out of it.
[00:35:57.940 --> 00:36:02.940]   So I'm gonna train it for a total of maybe 140 epochs.
[00:36:02.940 --> 00:36:08.260]   Now, depending on your GPU, if you're using a GPU,
[00:36:08.260 --> 00:36:10.780]   this will still take quite some time.
[00:36:10.780 --> 00:36:13.660]   So if you're using a CPU, it will take a very long time.
[00:36:13.660 --> 00:36:16.580]   So maybe reduce the number of epochs
[00:36:16.580 --> 00:36:21.420]   or reduce the data set size a little further if you want.
[00:36:21.420 --> 00:36:23.940]   So I'm gonna start training this,
[00:36:23.940 --> 00:36:26.020]   and I will see you on the other side.
[00:36:26.180 --> 00:36:28.180]   (silence)
[00:36:28.180 --> 00:36:31.820]   Okay, so we have finished training.
[00:36:31.820 --> 00:36:35.820]   And you can see that the accuracy over time
[00:36:35.820 --> 00:36:37.700]   is actually quite good.
[00:36:37.700 --> 00:36:40.100]   It goes up very slowly,
[00:36:40.100 --> 00:36:42.100]   which is why using a lot of epochs
[00:36:42.100 --> 00:36:43.420]   has been quite useful here.
[00:36:43.420 --> 00:36:47.300]   But then if we take a look at the accuracy at the end,
[00:36:47.300 --> 00:36:49.740]   it's 82%, which is not bad.
[00:36:49.740 --> 00:36:51.460]   But I think more importantly is the fact
[00:36:51.460 --> 00:36:53.660]   that it was still going up very gradually.
[00:36:53.660 --> 00:36:55.860]   I think with further training,
[00:36:55.860 --> 00:36:59.340]   this could quite easily get to about 90% on this data set,
[00:36:59.340 --> 00:37:01.220]   and considering it's a very small data set,
[00:37:01.220 --> 00:37:03.060]   that's pretty good.
[00:37:03.060 --> 00:37:04.660]   Now, if we look at the validation accuracy,
[00:37:04.660 --> 00:37:07.620]   we actually get a high number by quite a bit.
[00:37:07.620 --> 00:37:12.620]   So this actually went up to 94% here, almost 95.
[00:37:12.620 --> 00:37:16.980]   And I would assume that this is because
[00:37:16.980 --> 00:37:21.540]   within the validation set, there are more easy examples,
[00:37:21.540 --> 00:37:22.660]   whereas in the training set,
[00:37:22.660 --> 00:37:26.140]   we have some more difficult examples.
[00:37:26.140 --> 00:37:28.780]   But nonetheless, these are pretty good results
[00:37:28.780 --> 00:37:30.460]   for quickly putting together a model.
[00:37:30.460 --> 00:37:31.900]   It's not a particularly big model,
[00:37:31.900 --> 00:37:34.940]   other than the BERT encoder in the middle,
[00:37:34.940 --> 00:37:39.300]   but otherwise, it's a pretty straightforward, simple model.
[00:37:39.300 --> 00:37:41.860]   So it's pretty cool that you can get these results
[00:37:41.860 --> 00:37:44.260]   on sentiment analysis in so little time.
[00:37:44.260 --> 00:37:48.940]   And going forward with other data sets with more data
[00:37:48.940 --> 00:37:51.420]   can definitely do a lot better.
[00:37:51.420 --> 00:37:53.900]   At the same time, we can also improve the model.
[00:37:53.900 --> 00:37:56.820]   We can add LSTM layers to the classifier
[00:37:56.820 --> 00:37:58.420]   or convolution neural net layers,
[00:37:58.420 --> 00:38:00.900]   or even just more densely connected
[00:38:00.900 --> 00:38:02.260]   neural network layers as well.
[00:38:02.260 --> 00:38:04.860]   So there is a lot that we can actually do with this.
[00:38:04.860 --> 00:38:07.460]   But for now, that's everything.
[00:38:07.460 --> 00:38:09.300]   So I hope you enjoyed the video.
[00:38:09.300 --> 00:38:11.140]   I hope it's been useful to you.
[00:38:11.140 --> 00:38:12.220]   If you have any questions,
[00:38:12.220 --> 00:38:14.020]   let me know in the comments below.
[00:38:14.020 --> 00:38:16.180]   But otherwise, thank you for watching,
[00:38:16.180 --> 00:38:17.620]   and I will see you next time.
[00:38:17.620 --> 00:38:27.620]   [BLANK_AUDIO]

