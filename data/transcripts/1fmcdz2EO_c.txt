
[00:00:00.000 --> 00:00:06.000]   in one or two years, we'll find that you can use them for a lot of more like involved tasks
[00:00:06.000 --> 00:00:10.880]   than they can do now. So you could, you could imagine having the models to carry out a whole
[00:00:10.880 --> 00:00:15.760]   coding project instead of maybe giving you one suggestion on how to write a function.
[00:00:15.760 --> 00:00:22.000]   So you could imagine the model, like you giving it sort of high level instructions on what to,
[00:00:22.000 --> 00:00:27.120]   what to code up and it'll go and write many files and test it, look at the output,
[00:00:27.120 --> 00:00:33.120]   iterate on that a bit. So just much more complex tasks. Moving away from sort of one-off queries,
[00:00:33.120 --> 00:00:38.640]   like using the model kind of like a search engine, a smarter search engine and more towards like
[00:00:38.640 --> 00:00:42.880]   having a whole project that I'm like doing in collaboration with the model.
[00:00:42.880 --> 00:00:49.520]   And it knows everything I've done. It's proactively like suggesting things for me to
[00:00:49.520 --> 00:00:54.480]   try or it's going and doing work in the background. And fundamentally the unlock is that it can act
[00:00:54.480 --> 00:00:59.760]   coherently for long enough to write multiple files of code or what has changed between now and then?
[00:00:59.760 --> 00:01:05.760]   Most of the training data is more like doing single steps at a time. And I would expect us to
[00:01:05.760 --> 00:01:14.480]   do more for training the models to carry out these longer projects. So I'd say any kind of training,
[00:01:14.480 --> 00:01:20.640]   any like doing RL to learn how to do these tasks, however you do it, whether it's,
[00:01:20.640 --> 00:01:26.640]   whether you're supervising the final output or supervising it, like each step, I think any kind
[00:01:26.640 --> 00:01:32.000]   of training at carrying out these long projects is going to make them a lot better. And since
[00:01:32.000 --> 00:01:40.160]   the whole area is pretty new, I'd say there's just a lot of low-hanging fruit in doing this
[00:01:40.160 --> 00:01:46.320]   kind of training. So I'd say that's one thing. Also, I would expect that as the models get
[00:01:46.320 --> 00:01:53.520]   better, they're just better at recovering from errors or they have just, they're better at
[00:01:53.520 --> 00:01:59.040]   dealing with dealing with edge cases or when things go wrong, they know how to recover from it.
[00:01:59.040 --> 00:02:06.240]   So the models will be more sample efficient. So you don't have to collect a ton of data to teach
[00:02:06.240 --> 00:02:11.920]   them how to get back on track. Just a little bit of data or, or just their like generalization from,
[00:02:13.200 --> 00:02:18.880]   from other abilities will allow them to get back on the track, on track. Whereas current models
[00:02:18.880 --> 00:02:22.240]   might just get stuck and get lost. I'm not sure I understood actually how,
[00:02:22.240 --> 00:02:28.800]   I want to understand more explicitly how the generalization helps you get back on track.
[00:02:28.800 --> 00:02:32.720]   Yeah. If you collect a diverse dataset, you're going to get a little bit of everything in it.
[00:02:32.720 --> 00:02:39.600]   And, and if you have models that generalize really well, even if there's just a couple
[00:02:39.600 --> 00:02:44.160]   examples of getting back on track, I see, or even like maybe in the pre-training,
[00:02:44.160 --> 00:02:48.560]   there's examples of getting back on track. Then like the model will be able to generalize from
[00:02:48.560 --> 00:02:57.280]   those other things it's seen to the current situation. So I think like if you have models
[00:02:57.280 --> 00:03:01.920]   that are weaker, you might be able to get them to do almost anything with enough data, but you might
[00:03:01.920 --> 00:03:08.240]   have to put a lot of effort into a particular domain or skill. Whereas for a stronger model,
[00:03:08.240 --> 00:03:11.840]   it might just do the right thing without any training data or any effort.
[00:03:11.840 --> 00:03:16.080]   Right now we have models that are on a per token basis, pretty smart. Like they might be as smart
[00:03:16.080 --> 00:03:21.040]   as humans on a per token basis, the smartest humans. And the thing that prevents them from
[00:03:21.040 --> 00:03:26.000]   being as useful as they could be is that five minutes from now, they're not going to be so
[00:03:26.000 --> 00:03:31.040]   writing your code in a way that's coherent and aligns with the broader goals you have for your
[00:03:31.040 --> 00:03:37.680]   project or something. If it's the case that once you start this long horizon RL training regime,
[00:03:37.680 --> 00:03:42.400]   it immediately unlocks your ability to be coherent for longer periods of time.
[00:03:42.400 --> 00:03:47.840]   Should we be predicting something that is human level as soon as that regime is unlocked? And
[00:03:47.840 --> 00:03:52.560]   if not, then what is remaining after you can plan for a year and execute projects that take that
[00:03:52.560 --> 00:03:56.400]   long? I wouldn't expect everything to be immediately solved by doing any training
[00:03:56.400 --> 00:04:02.160]   like this. I would think there'll be other miscellaneous deficits that the models have
[00:04:02.160 --> 00:04:09.600]   that cause them to get stuck or not make progress or make worse decisions than humans. So I wouldn't
[00:04:09.600 --> 00:04:16.800]   say I expect that this one little thing will unlock all capabilities, but yeah, it's not clear,
[00:04:16.800 --> 00:04:22.320]   but it might like some improvement in the ability to do long horizon tasks might go quite far.
[00:04:22.320 --> 00:04:26.560]   Does that imply that unless there are these other bottlenecks, which they may or may not be,
[00:04:27.280 --> 00:04:32.720]   by next year you could have models that are potentially like human level in terms of acting
[00:04:32.720 --> 00:04:37.600]   like, like you're interacting with this as a colleague and it's like, it's like as good as
[00:04:37.600 --> 00:04:40.720]   interacting with a human colleague, you can tell them to go do stuff and they go to get it done.
[00:04:40.720 --> 00:04:46.640]   What seems wrong with that picture of this is the capabilities you think might be possible?
[00:04:46.640 --> 00:04:50.160]   Yeah, it's hard to say exactly what will be the deficit. I mean, I would say that
[00:04:51.200 --> 00:04:58.800]   when you talk to the models today, they have various weaknesses besides long-term coherence
[00:04:58.800 --> 00:05:06.080]   in terms of also like really thinking hard about things or paying attention to what you ask them.
[00:05:06.080 --> 00:05:18.640]   So I would say I wouldn't expect just improving the coherence a little bit to be all it takes to
[00:05:18.640 --> 00:05:25.440]   get to AGI, but I guess I wouldn't be able to articulate exactly what the main weakness is
[00:05:25.440 --> 00:05:30.160]   that'll stop them from like being a fully functional colleague.
[00:05:30.160 --> 00:05:30.660]   Thank you.
[00:05:30.660 --> 00:05:32.720]   you
[00:05:32.720 --> 00:05:42.720]   [BLANK_AUDIO]

