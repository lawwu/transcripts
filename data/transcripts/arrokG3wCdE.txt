
[00:00:00.000 --> 00:00:02.920]   The jury found Pfizer guilty of fraud
[00:00:02.920 --> 00:00:04.600]   and racketeering violations.
[00:00:04.600 --> 00:00:06.520]   - How does Big Pharma affect your mind?
[00:00:06.520 --> 00:00:08.800]   - Everyone's allowed their own opinion.
[00:00:08.800 --> 00:00:11.920]   I don't think everyone's allowed their own scientific facts.
[00:00:11.920 --> 00:00:13.800]   - Does Pfizer play by the rules?
[00:00:13.800 --> 00:00:16.440]   - Pfizer isn't battling the FDA.
[00:00:16.440 --> 00:00:18.760]   Pfizer has joined the FDA.
[00:00:18.760 --> 00:00:24.320]   - The following is a conversation with John Abramson,
[00:00:24.320 --> 00:00:26.520]   faculty at Harvard Medical School,
[00:00:26.520 --> 00:00:29.160]   a family physician for over two decades,
[00:00:29.160 --> 00:00:32.120]   and author of the new book "Sickening,"
[00:00:32.120 --> 00:00:35.000]   about how Big Pharma broke American healthcare
[00:00:35.000 --> 00:00:37.360]   and how we can fix it.
[00:00:37.360 --> 00:00:40.200]   This conversation with John Abramson
[00:00:40.200 --> 00:00:43.680]   is a critical exploration of the pharmaceutical industry.
[00:00:43.680 --> 00:00:45.400]   I wanted to talk to John
[00:00:45.400 --> 00:00:48.160]   in order to provide a countervailing perspective
[00:00:48.160 --> 00:00:50.720]   to the one expressed in my podcast episode
[00:00:50.720 --> 00:00:55.080]   with the CEO of Pfizer, Albert Bourla.
[00:00:55.080 --> 00:00:58.920]   And here, please allow me to say a few additional words
[00:00:58.920 --> 00:01:01.880]   about this episode with the Pfizer CEO,
[00:01:01.880 --> 00:01:04.920]   and in general, about why I do these conversations
[00:01:04.920 --> 00:01:06.800]   and how I approach them.
[00:01:06.800 --> 00:01:10.520]   If this is not interesting to you, please skip ahead.
[00:01:10.520 --> 00:01:13.080]   What do I hope to do with this podcast?
[00:01:13.080 --> 00:01:15.600]   I want to understand human nature,
[00:01:15.600 --> 00:01:18.040]   the best and the worst of it.
[00:01:18.040 --> 00:01:19.840]   I want to understand how power, money,
[00:01:19.840 --> 00:01:21.840]   and fame changes people.
[00:01:21.840 --> 00:01:24.600]   I want to understand why atrocities are committed
[00:01:24.600 --> 00:01:27.240]   by crowds that believe they're doing good.
[00:01:27.240 --> 00:01:29.280]   All this, ultimately,
[00:01:29.280 --> 00:01:30.720]   because I want to understand
[00:01:30.720 --> 00:01:33.240]   how we can build a better world together,
[00:01:33.240 --> 00:01:35.400]   to find hope for the future,
[00:01:35.400 --> 00:01:38.520]   and to rediscover each time
[00:01:38.520 --> 00:01:40.880]   through the exploration of ideas
[00:01:40.880 --> 00:01:43.360]   just how beautiful this life is,
[00:01:43.360 --> 00:01:45.360]   this, our human civilization,
[00:01:45.360 --> 00:01:47.640]   in all of its full complexity,
[00:01:47.640 --> 00:01:49.280]   the forces of good and evil,
[00:01:49.280 --> 00:01:52.280]   of war and peace, of hate and love.
[00:01:52.280 --> 00:01:55.560]   I don't think I can do this with a heart and mind
[00:01:55.560 --> 00:01:57.320]   that is not open, fragile,
[00:01:57.320 --> 00:02:00.920]   and willing to empathize with all human beings,
[00:02:00.920 --> 00:02:03.400]   even those in the darkest corners of our world.
[00:02:03.400 --> 00:02:06.760]   To attack is easy.
[00:02:06.760 --> 00:02:09.520]   To understand is hard.
[00:02:09.520 --> 00:02:11.800]   And I choose the hard path.
[00:02:11.800 --> 00:02:13.840]   I have learned over the past few months
[00:02:13.840 --> 00:02:17.360]   that this path involves me getting more and more attacked
[00:02:17.360 --> 00:02:19.000]   from all sides.
[00:02:19.000 --> 00:02:21.600]   I will get attacked when I host people
[00:02:21.600 --> 00:02:24.480]   like Jay Bhattacharya or Francis Collins,
[00:02:24.480 --> 00:02:28.120]   Jamie Mertzl or Vincent Reconiello,
[00:02:28.120 --> 00:02:31.840]   when I stand for my friend Joe Rogan,
[00:02:31.840 --> 00:02:34.720]   when I host tech leaders like Mark Zuckerberg,
[00:02:34.720 --> 00:02:36.400]   Elon Musk, and others,
[00:02:36.400 --> 00:02:39.560]   when I eventually talk to Vladimir Putin,
[00:02:39.560 --> 00:02:41.960]   Barack Obama, and other figures
[00:02:41.960 --> 00:02:43.880]   that have turned the tides of history.
[00:02:43.880 --> 00:02:49.920]   I have and I will get called stupid, naive, weak,
[00:02:49.920 --> 00:02:53.640]   and I will take these words with respect,
[00:02:53.640 --> 00:02:57.200]   humility, and love, and I will get better.
[00:02:57.200 --> 00:03:00.680]   I will listen, think, learn, and improve.
[00:03:00.680 --> 00:03:04.480]   One thing I can promise is there's no amount of money
[00:03:04.480 --> 00:03:06.880]   or fame that can buy my opinion
[00:03:06.880 --> 00:03:09.240]   or make me go against my principles.
[00:03:09.240 --> 00:03:13.480]   There's no amount of pressure that can break my integrity.
[00:03:13.480 --> 00:03:16.120]   There's nothing in this world I need
[00:03:16.120 --> 00:03:18.520]   that I don't already have.
[00:03:18.520 --> 00:03:21.400]   Life itself is the fundamental gift.
[00:03:21.400 --> 00:03:23.440]   Everything else is just a bonus.
[00:03:24.240 --> 00:03:26.440]   That is freedom.
[00:03:26.440 --> 00:03:28.440]   That is happiness.
[00:03:28.440 --> 00:03:31.480]   If I die today, I will die a happy man.
[00:03:31.480 --> 00:03:35.720]   Now, a few comments about my approach
[00:03:35.720 --> 00:03:39.360]   and lessons learned from the Albert Bourla conversation.
[00:03:39.360 --> 00:03:41.640]   The goal was to reveal as much as I could
[00:03:41.640 --> 00:03:43.680]   about the human being before me,
[00:03:43.680 --> 00:03:46.720]   and to give him the opportunity to contemplate
[00:03:46.720 --> 00:03:49.680]   in long form the complexities of his role,
[00:03:49.680 --> 00:03:52.960]   including the tension between making money
[00:03:52.960 --> 00:03:54.400]   and helping people,
[00:03:54.400 --> 00:03:57.960]   the corruption that so often permeates human institutions,
[00:03:57.960 --> 00:04:02.160]   the crafting of narratives through advertisements, and so on.
[00:04:02.160 --> 00:04:03.840]   I only had one hour,
[00:04:03.840 --> 00:04:07.240]   and so this wasn't the time to address these issues deeply,
[00:04:07.240 --> 00:04:09.960]   but to show if Albert struggled with them
[00:04:09.960 --> 00:04:12.160]   in the privacy of his own mind,
[00:04:12.160 --> 00:04:16.040]   and if he would let down the veil of political speak
[00:04:16.040 --> 00:04:19.160]   for a time to let me connect with a man
[00:04:19.160 --> 00:04:22.320]   who decades ago chose to become a veterinarian,
[00:04:22.320 --> 00:04:26.080]   who wanted to help lessen the amount of suffering in the world.
[00:04:26.080 --> 00:04:28.160]   I had no pressure placed on me.
[00:04:28.160 --> 00:04:29.560]   There were no rules.
[00:04:29.560 --> 00:04:31.920]   The questions I was asking were all mine
[00:04:31.920 --> 00:04:33.920]   and not seen by Pfizer folks.
[00:04:33.920 --> 00:04:38.480]   I had no care whether I ever talked to another CEO again.
[00:04:38.480 --> 00:04:41.600]   None of this was part of the calculation
[00:04:41.600 --> 00:04:43.920]   in my limited brain computer.
[00:04:43.920 --> 00:04:45.720]   I didn't want to grill him
[00:04:45.720 --> 00:04:48.680]   the way politicians grill CEOs in Congress.
[00:04:48.680 --> 00:04:53.680]   I thought that this approach is easy, self-serving, dehumanizing,
[00:04:53.680 --> 00:04:56.040]   and it reveals nothing.
[00:04:56.040 --> 00:04:58.840]   I wanted to reveal the genuine intellectual struggle,
[00:04:58.840 --> 00:05:01.200]   vision, and motivation of a human being,
[00:05:01.200 --> 00:05:02.480]   and if that fails,
[00:05:02.480 --> 00:05:06.640]   I trusted the listener to draw their own conclusion and insights
[00:05:06.640 --> 00:05:07.920]   from the result,
[00:05:07.920 --> 00:05:09.880]   whether it's the words spoken,
[00:05:09.880 --> 00:05:11.880]   or the words left unspoken,
[00:05:11.880 --> 00:05:14.280]   or simply the silence.
[00:05:14.280 --> 00:05:15.640]   And that's just it.
[00:05:15.640 --> 00:05:20.120]   I fundamentally trust the intelligence of the listener.
[00:05:20.120 --> 00:05:21.720]   You.
[00:05:21.720 --> 00:05:24.680]   In fact, if I criticize the person too hard
[00:05:24.680 --> 00:05:26.640]   or celebrate the person too much,
[00:05:26.640 --> 00:05:30.640]   I feel I fail to give the listener a picture of the human being
[00:05:30.640 --> 00:05:33.760]   that is uncontaminated by my opinion
[00:05:33.760 --> 00:05:36.320]   or the opinion of the crowd.
[00:05:36.320 --> 00:05:39.120]   I trust that you have the fortitude and the courage
[00:05:39.120 --> 00:05:40.920]   to use your own mind,
[00:05:40.920 --> 00:05:44.120]   to empathize, and to think.
[00:05:44.120 --> 00:05:46.160]   Two practical lessons I took away.
[00:05:46.160 --> 00:05:48.320]   First, I will more strongly push
[00:05:48.320 --> 00:05:51.760]   for longer conversations of three, four, or more hours
[00:05:51.760 --> 00:05:53.240]   versus just one hour.
[00:05:53.240 --> 00:05:55.800]   60 minutes is too short for the guest to relax
[00:05:55.800 --> 00:05:58.080]   and to think slowly and deeply,
[00:05:58.080 --> 00:06:00.680]   and for me to ask many follow-up questions
[00:06:00.680 --> 00:06:02.760]   or follow interesting tangents.
[00:06:02.760 --> 00:06:05.680]   Ultimately, I think it's in the interest of everyone,
[00:06:05.680 --> 00:06:06.920]   including the guests,
[00:06:06.920 --> 00:06:11.400]   that we talk in true long form for many hours.
[00:06:11.400 --> 00:06:13.640]   Second, these conversations with leaders
[00:06:13.640 --> 00:06:15.960]   can be aided by further conversations
[00:06:15.960 --> 00:06:18.560]   with people who wrote books about those leaders
[00:06:18.560 --> 00:06:20.240]   or their industries,
[00:06:20.240 --> 00:06:22.240]   those that can steel man each perspective
[00:06:22.240 --> 00:06:24.960]   and attempt to give an objective analysis.
[00:06:24.960 --> 00:06:26.640]   I think of Teddy Roosevelt's speech
[00:06:26.640 --> 00:06:28.320]   about the man in the arena.
[00:06:28.320 --> 00:06:32.200]   I want to talk to both the men and women in the arena
[00:06:32.200 --> 00:06:35.920]   and the critics and the supporters in the stands.
[00:06:35.920 --> 00:06:38.680]   For the former, I lean toward wanting to understand
[00:06:38.680 --> 00:06:43.200]   one human being's struggle with the ideas.
[00:06:43.200 --> 00:06:44.960]   For the latter, I lean towards
[00:06:44.960 --> 00:06:48.200]   understanding the ideas themselves.
[00:06:48.200 --> 00:06:50.120]   That's why I wanted to have this conversation
[00:06:50.120 --> 00:06:51.600]   with John Abramson,
[00:06:51.600 --> 00:06:55.400]   who is an outspoken critic of the pharmaceutical industry.
[00:06:55.400 --> 00:06:58.640]   I hope it helps add context and depth
[00:06:58.640 --> 00:07:02.120]   to the conversation I had with the Pfizer CEO.
[00:07:02.120 --> 00:07:06.520]   In the end, I may do worse than I could have or should have.
[00:07:06.520 --> 00:07:09.920]   Always, I will listen to the criticisms without ego,
[00:07:09.920 --> 00:07:14.440]   and I promise I will work hard to improve.
[00:07:14.440 --> 00:07:19.840]   But let me say finally that cynicism is easy.
[00:07:19.840 --> 00:07:24.320]   Optimism, true optimism, is hard.
[00:07:24.320 --> 00:07:29.760]   It is the belief that we can and we will build a better world
[00:07:29.760 --> 00:07:32.520]   and that we can only do it together.
[00:07:32.520 --> 00:07:34.840]   This is the fight worth fighting.
[00:07:34.840 --> 00:07:36.320]   So here we go.
[00:07:36.320 --> 00:07:39.000]   Once more into the breach, dear friends.
[00:07:39.000 --> 00:07:41.480]   I love you all.
[00:07:41.480 --> 00:07:43.720]   This is the Lex Friedman Podcast.
[00:07:43.720 --> 00:07:47.160]   To support it, please check out our sponsors in the description.
[00:07:47.160 --> 00:07:52.200]   And now, here's my conversation with John Abramson.
[00:07:52.200 --> 00:07:55.040]   Your faculty at Harvard Medical School,
[00:07:55.040 --> 00:07:57.640]   your family physician for over two decades,
[00:07:57.640 --> 00:08:00.960]   rated one of the best family physicians in Massachusetts.
[00:08:00.960 --> 00:08:03.240]   You wrote the book "Overdosed America"
[00:08:03.240 --> 00:08:07.200]   and the new book coming out now called "Sickening,"
[00:08:07.200 --> 00:08:10.000]   about how Big Pharma broke American healthcare,
[00:08:10.000 --> 00:08:14.640]   including science and research, and how we can fix it.
[00:08:14.640 --> 00:08:18.600]   First question, what is the biggest problem with Big Pharma
[00:08:18.600 --> 00:08:21.720]   that if fixed would be the most impactful?
[00:08:21.720 --> 00:08:24.800]   So if you can snap your fingers and fix one thing,
[00:08:24.800 --> 00:08:26.640]   what would be the most impactful, you think?
[00:08:26.640 --> 00:08:34.240]   - The biggest problem is the way they determine the content,
[00:08:34.240 --> 00:08:37.480]   the accuracy, and the completeness
[00:08:37.480 --> 00:08:42.400]   of what doctors believe to be the full range of knowledge
[00:08:42.400 --> 00:08:45.880]   that they need to best take care of their patients.
[00:08:45.880 --> 00:08:51.360]   So that with the knowledge having been taken over
[00:08:51.360 --> 00:08:52.720]   by the commercial interests,
[00:08:52.720 --> 00:08:55.880]   primarily the pharmaceutical industry,
[00:08:55.880 --> 00:08:59.400]   the purpose of that knowledge is to maximize the profits
[00:08:59.400 --> 00:09:03.200]   that get returned to investors and shareholders,
[00:09:03.200 --> 00:09:07.120]   and not to optimize the health of the American people.
[00:09:07.120 --> 00:09:11.360]   So rebalancing that equation would be the most important
[00:09:11.360 --> 00:09:14.640]   thing to do to get our healthcare back
[00:09:14.640 --> 00:09:16.360]   aimed in the right direction.
[00:09:16.360 --> 00:09:20.600]   - Okay, so there's a tension between helping people
[00:09:20.600 --> 00:09:22.160]   and making money.
[00:09:22.160 --> 00:09:26.280]   So if we look at particularly the task of helping people
[00:09:26.280 --> 00:09:30.480]   in medicine, in healthcare, is it possible
[00:09:30.480 --> 00:09:35.320]   if money is the primary sort of mechanism
[00:09:35.320 --> 00:09:38.200]   by which you achieve that as a motivator,
[00:09:38.200 --> 00:09:39.720]   is it possible to get that right?
[00:09:39.720 --> 00:09:43.040]   - I think it is, Lex, but I think it is not possible
[00:09:43.040 --> 00:09:46.640]   without guardrails that maintain the integrity
[00:09:46.640 --> 00:09:48.640]   and the balance of the knowledge.
[00:09:48.640 --> 00:09:51.920]   Without those guardrails, it's like trying to play
[00:09:51.920 --> 00:09:54.800]   a professional basketball game without referees
[00:09:54.800 --> 00:09:57.520]   and having players call their own fouls.
[00:09:57.520 --> 00:09:59.840]   But the players are paid to win,
[00:09:59.840 --> 00:10:02.320]   and you can't count on them to call their own fouls.
[00:10:02.320 --> 00:10:05.000]   So we have referees who are in charge.
[00:10:05.000 --> 00:10:08.120]   We don't have those referees in American healthcare.
[00:10:08.120 --> 00:10:13.120]   That's the biggest way that American healthcare
[00:10:13.120 --> 00:10:17.440]   is distinguished from healthcare in other wealthy nations.
[00:10:17.440 --> 00:10:19.640]   - So okay, so you mentioned Milton Friedman,
[00:10:19.640 --> 00:10:24.160]   and you mentioned his book called "Capitalism and Freedom."
[00:10:24.160 --> 00:10:27.120]   He writes that there are only three legitimate functions
[00:10:27.120 --> 00:10:30.080]   of government to preserve law and order,
[00:10:30.080 --> 00:10:31.800]   to enforce private contracts,
[00:10:31.800 --> 00:10:35.760]   and to ensure that private markets work.
[00:10:35.760 --> 00:10:40.120]   You said that that was a radical idea at the time,
[00:10:40.120 --> 00:10:41.800]   but we're failing on all three.
[00:10:41.800 --> 00:10:43.680]   How are we failing?
[00:10:43.680 --> 00:10:46.680]   And also maybe the bigger picture is,
[00:10:46.680 --> 00:10:49.640]   what are the strengths and weaknesses of capitalism
[00:10:49.640 --> 00:10:51.840]   when it comes to medicine and healthcare?
[00:10:51.840 --> 00:10:53.120]   - Can we separate those out?
[00:10:53.120 --> 00:10:55.200]   'Cause those are two huge questions.
[00:10:55.200 --> 00:10:58.080]   So how we're failing on all three,
[00:10:58.080 --> 00:11:01.240]   and these are the minimal functions
[00:11:01.240 --> 00:11:06.120]   that our guru of free market capitalism said
[00:11:06.120 --> 00:11:07.600]   the government should perform.
[00:11:07.600 --> 00:11:10.100]   So this is the absolute baseline.
[00:11:10.100 --> 00:11:13.720]   On preserving law and order,
[00:11:13.720 --> 00:11:17.440]   the drug companies routinely violate the law
[00:11:17.440 --> 00:11:20.160]   in terms of their marketing,
[00:11:20.160 --> 00:11:25.160]   and in terms of their,
[00:11:25.240 --> 00:11:29.200]   presentation of the results of their trials.
[00:11:29.200 --> 00:11:32.960]   I know this because I was an expert in litigation
[00:11:32.960 --> 00:11:34.660]   for about 10 years.
[00:11:34.660 --> 00:11:40.040]   I presented some of what I learned in civil litigation
[00:11:40.040 --> 00:11:42.600]   to the FBI and the Department of Justice,
[00:11:42.600 --> 00:11:46.360]   and that case led to the biggest criminal fine
[00:11:46.360 --> 00:11:49.040]   in US history as of 2009.
[00:11:49.040 --> 00:11:54.920]   And I testified in a federal trial in 2010,
[00:11:55.760 --> 00:12:00.040]   and the jury found Pfizer guilty of fraud
[00:12:00.040 --> 00:12:02.400]   and racketeering violations.
[00:12:02.400 --> 00:12:07.400]   In terms of violating the law, it's a routine occurrence.
[00:12:07.400 --> 00:12:10.840]   The drug companies have paid $38 billion worth of fines
[00:12:10.840 --> 00:12:14.520]   from I think 1991 to 2017.
[00:12:14.520 --> 00:12:20.800]   It's never been enough to stop the misrepresentation
[00:12:20.800 --> 00:12:23.240]   of their data.
[00:12:23.240 --> 00:12:25.680]   And rarely are the fines greater
[00:12:25.680 --> 00:12:27.320]   than the profits that were made.
[00:12:27.320 --> 00:12:31.880]   See, executives have not gone to jail
[00:12:31.880 --> 00:12:36.460]   for misrepresenting data that have involved
[00:12:36.460 --> 00:12:38.480]   even tens of thousands of deaths
[00:12:38.480 --> 00:12:42.160]   in the case of Vioxx, OxyContin as well.
[00:12:42.160 --> 00:12:45.560]   And when companies plead guilty to felonies,
[00:12:45.560 --> 00:12:48.400]   which is not an unusual occurrence,
[00:12:48.400 --> 00:12:51.280]   the government usually allows the companies,
[00:12:51.280 --> 00:12:54.600]   the parent companies to allow subsidiaries
[00:12:54.600 --> 00:12:58.920]   to take the plea so that they are not one step closer
[00:12:58.920 --> 00:13:01.280]   to getting disbarred from Medicare,
[00:13:01.280 --> 00:13:03.400]   not being able to participate in Medicare.
[00:13:03.400 --> 00:13:10.000]   So in that sense, there is a mechanism
[00:13:10.000 --> 00:13:15.040]   that is appearing to impose law and order
[00:13:15.040 --> 00:13:18.180]   on drug company behavior, but it's clearly not enough.
[00:13:18.180 --> 00:13:19.520]   It's not working.
[00:13:19.520 --> 00:13:24.520]   - Can you actually speak to human nature here?
[00:13:24.520 --> 00:13:26.320]   Are people corrupt?
[00:13:26.320 --> 00:13:28.400]   Are people malevolent?
[00:13:28.400 --> 00:13:32.880]   Are people ignorant that work at the low level
[00:13:32.880 --> 00:13:36.480]   and at the high level at Pfizer, for example,
[00:13:36.480 --> 00:13:38.640]   at big pharma companies?
[00:13:38.640 --> 00:13:40.440]   How is this possible?
[00:13:40.440 --> 00:13:43.280]   So I believe, just on a small tangent,
[00:13:43.280 --> 00:13:45.240]   that most people are good.
[00:13:45.240 --> 00:13:48.920]   And I actually believe if you join big pharma,
[00:13:48.920 --> 00:13:51.280]   so a company like Pfizer,
[00:13:51.280 --> 00:13:54.920]   your life trajectory often involves dreaming
[00:13:54.920 --> 00:13:58.920]   and wanting and enjoying helping people.
[00:13:58.920 --> 00:13:59.760]   - Yes.
[00:13:59.760 --> 00:14:03.440]   - And so, and then we look at the outcomes
[00:14:03.440 --> 00:14:07.040]   that you're describing and it looks,
[00:14:07.040 --> 00:14:09.400]   and that's why the narrative takes hold,
[00:14:09.400 --> 00:14:13.180]   that Pfizer CEO Albert Bourla, who I talked to,
[00:14:13.180 --> 00:14:15.440]   is malevolent.
[00:14:15.440 --> 00:14:19.520]   The sense is like these companies are evil.
[00:14:19.520 --> 00:14:24.520]   So if the different parts, the people, are good
[00:14:24.520 --> 00:14:27.360]   and they want to do good, how are we getting these outcomes?
[00:14:27.360 --> 00:14:32.360]   - Yeah, I think it has to do with the cultural milieu
[00:14:32.360 --> 00:14:35.320]   that this is unfolding in.
[00:14:35.320 --> 00:14:40.320]   And we need to look at sociology to understand this,
[00:14:41.440 --> 00:14:46.440]   that when the cultural milieu is set up
[00:14:46.440 --> 00:14:53.680]   to maximize the returns on investment for shareholders
[00:14:53.680 --> 00:14:57.520]   and other venture capitalists and hedge funds and so forth,
[00:14:57.520 --> 00:15:00.640]   when that defines the culture
[00:15:00.640 --> 00:15:04.240]   and the higher up you are in the corporation,
[00:15:04.240 --> 00:15:09.240]   the more you're in on the game of getting rewarded
[00:15:10.080 --> 00:15:13.040]   for maximizing the profits of the investors.
[00:15:13.040 --> 00:15:14.880]   That's the culture they live in.
[00:15:14.880 --> 00:15:20.760]   And it becomes normative behavior to do things with science
[00:15:20.760 --> 00:15:26.900]   that look normal in that environment
[00:15:26.900 --> 00:15:29.980]   and are shared values within that environment
[00:15:29.980 --> 00:15:34.900]   by good people whose self-evaluation becomes modified
[00:15:34.900 --> 00:15:37.660]   by the goals that are shared by the people around them.
[00:15:39.060 --> 00:15:44.060]   And within that milieu, you have one set of standards,
[00:15:44.060 --> 00:15:48.680]   and then the rest of good American people
[00:15:48.680 --> 00:15:50.720]   have the expectation that the drug companies
[00:15:50.720 --> 00:15:55.280]   are trying to make money, but that they're playing by rules
[00:15:55.280 --> 00:15:58.480]   that aren't part of the insider milieu.
[00:15:58.480 --> 00:16:00.440]   - That's fascinating.
[00:16:00.440 --> 00:16:04.680]   The game they're playing modifies the culture
[00:16:06.280 --> 00:16:09.440]   inside the meetings, inside the rooms, day to day,
[00:16:09.440 --> 00:16:12.420]   that there's a bubble that forms.
[00:16:12.420 --> 00:16:15.660]   Like we're all in bubbles of different sizes.
[00:16:15.660 --> 00:16:18.340]   And that bubble allows you to drift
[00:16:18.340 --> 00:16:23.080]   in terms of what you see as ethical and unethical,
[00:16:23.080 --> 00:16:28.500]   because you see the game as just part of the game.
[00:16:28.500 --> 00:16:30.500]   So marketing is just part of the game.
[00:16:30.500 --> 00:16:35.280]   Paying the fines is just part of the game of science.
[00:16:36.120 --> 00:16:38.240]   - Yeah, and without guardrails,
[00:16:38.240 --> 00:16:42.200]   it becomes even more part of the game.
[00:16:42.200 --> 00:16:44.660]   You keep moving in that direction
[00:16:44.660 --> 00:16:48.180]   if you're not bumping up against guardrails.
[00:16:48.180 --> 00:16:49.900]   And I think that's how we've gotten
[00:16:49.900 --> 00:16:52.220]   to the extreme situation we're in now.
[00:16:52.220 --> 00:16:57.200]   - So like I mentioned, I spoke with Pfizer CEO,
[00:16:57.200 --> 00:17:00.980]   Albert Bourla, and I'd like to raise with you
[00:17:00.980 --> 00:17:02.880]   some of the concerns I raised with him.
[00:17:03.820 --> 00:17:06.160]   So one, you already mentioned,
[00:17:06.160 --> 00:17:08.280]   I raised the concern that Pfizer's engaged
[00:17:08.280 --> 00:17:11.220]   in aggressive advertising campaigns.
[00:17:11.220 --> 00:17:14.240]   As you can imagine, he said no.
[00:17:14.240 --> 00:17:16.540]   What do you think?
[00:17:16.540 --> 00:17:20.320]   - I think you're both right.
[00:17:20.320 --> 00:17:23.700]   I think that the, I agree with you,
[00:17:23.700 --> 00:17:26.280]   that the aggressive advertising campaigns
[00:17:26.280 --> 00:17:30.560]   do not add value to society.
[00:17:30.560 --> 00:17:34.980]   And I agree with him that they're, for the most part, legal,
[00:17:34.980 --> 00:17:36.980]   and it's the way the game is played.
[00:17:36.980 --> 00:17:38.620]   - Right, so sorry to interrupt,
[00:17:38.620 --> 00:17:42.300]   but oftentimes his responses are,
[00:17:42.300 --> 00:17:46.320]   especially now, he's been CEO
[00:17:46.320 --> 00:17:48.740]   for only like two years, three years,
[00:17:48.740 --> 00:17:50.500]   he says Pfizer was a different company,
[00:17:50.500 --> 00:17:53.840]   we've made mistakes in the past.
[00:17:53.840 --> 00:17:56.340]   We don't make mistakes anymore.
[00:17:56.340 --> 00:18:00.340]   That there's rules, and we play by the rules.
[00:18:00.340 --> 00:18:02.760]   So like, with every concern raised,
[00:18:02.760 --> 00:18:06.020]   there's very, very strict rules, as he says.
[00:18:06.020 --> 00:18:08.500]   In fact, he says sometimes way too strict,
[00:18:08.500 --> 00:18:10.180]   and we play by them.
[00:18:10.180 --> 00:18:12.180]   And so in that sense, advertisement,
[00:18:12.180 --> 00:18:14.320]   it doesn't seem like it's too aggressive,
[00:18:14.320 --> 00:18:16.020]   because it's playing by the rules.
[00:18:16.020 --> 00:18:19.800]   And relative to the other, again, it's the game,
[00:18:19.800 --> 00:18:22.140]   relative to the other companies,
[00:18:22.140 --> 00:18:23.880]   it's actually not that aggressive.
[00:18:23.880 --> 00:18:26.540]   Relative to the other big pharma companies.
[00:18:26.540 --> 00:18:27.500]   - Yes, yes.
[00:18:27.500 --> 00:18:30.300]   I hope we can quickly get back to whether or not
[00:18:30.300 --> 00:18:32.780]   they're playing by the rules, but in general.
[00:18:32.780 --> 00:18:34.380]   But let's just look at the question
[00:18:34.380 --> 00:18:36.660]   of advertising specifically.
[00:18:36.660 --> 00:18:39.620]   I think that's a good example of what it looks like
[00:18:39.620 --> 00:18:43.140]   from within that culture, and from outside that culture.
[00:18:43.140 --> 00:18:49.780]   He's saying that we follow the law on our advertising.
[00:18:49.780 --> 00:18:51.540]   We state the side effects,
[00:18:51.540 --> 00:18:53.940]   and we state the FDA approved indications,
[00:18:53.940 --> 00:18:57.700]   and we do what the law says we have to do for advertising.
[00:18:57.700 --> 00:19:01.780]   And I have not, I've not been an expert in litigation
[00:19:01.780 --> 00:19:04.980]   for a few years, and I don't know what's going on currently,
[00:19:04.980 --> 00:19:07.060]   but let's take him at his word.
[00:19:07.060 --> 00:19:08.020]   It could be true.
[00:19:08.020 --> 00:19:09.940]   It might not be, but it could be.
[00:19:09.940 --> 00:19:14.940]   But if that's true, in his world, in his culture,
[00:19:14.940 --> 00:19:17.280]   that's ethical business behavior.
[00:19:17.280 --> 00:19:22.560]   From a common sense person's point of view,
[00:19:22.560 --> 00:19:27.220]   a drug company paying highly skilled media folks
[00:19:27.220 --> 00:19:30.420]   to take the information about the drug
[00:19:30.420 --> 00:19:34.380]   and create the illusion, the emotional impact,
[00:19:34.380 --> 00:19:38.000]   and the takeaway message for viewers of advertisements
[00:19:38.000 --> 00:19:41.200]   that grossly exaggerate the benefit of the drug
[00:19:41.200 --> 00:19:45.380]   and minimize the harms, it's sociopathic behavior
[00:19:45.380 --> 00:19:49.960]   to have viewers of ads leave the ad
[00:19:49.960 --> 00:19:52.980]   with an unrealistic impression
[00:19:52.980 --> 00:19:56.220]   of the benefits and harms of the drug.
[00:19:56.220 --> 00:19:58.900]   And yet, he's playing by the rules.
[00:19:58.900 --> 00:20:01.300]   He's doing his job as CEO
[00:20:01.300 --> 00:20:04.580]   to maximize the effect of his advertising.
[00:20:04.580 --> 00:20:07.620]   And if he doesn't do it, this is a key point,
[00:20:07.620 --> 00:20:11.940]   if he doesn't do it, he'll get fired and the next guy will.
[00:20:11.940 --> 00:20:13.620]   - So the people that survive in the company,
[00:20:13.620 --> 00:20:16.700]   the people that get raises in the company
[00:20:16.700 --> 00:20:17.780]   and move up in the company
[00:20:17.780 --> 00:20:19.220]   are the ones that play by the rules,
[00:20:19.220 --> 00:20:21.540]   and that's how the game solidifies itself.
[00:20:21.540 --> 00:20:24.380]   But the game is within the bounds of the law.
[00:20:24.380 --> 00:20:27.260]   Sometimes, most of the time, not always.
[00:20:27.260 --> 00:20:29.220]   - We'll return to that question.
[00:20:29.220 --> 00:20:31.660]   I'm actually more concerned
[00:20:31.660 --> 00:20:34.340]   about the effect of advertisement
[00:20:34.340 --> 00:20:39.340]   in a kind of much larger scale
[00:20:39.340 --> 00:20:44.520]   on the people that are getting funded by the advertisement
[00:20:44.520 --> 00:20:47.500]   in self-censorship, just like more subtle,
[00:20:47.500 --> 00:20:52.500]   more passive pressure to not say anything negative.
[00:20:52.980 --> 00:20:57.980]   Because I've seen this and I've been saddened by it,
[00:20:57.980 --> 00:21:03.140]   that people sacrifice integrity in small ways
[00:21:03.140 --> 00:21:05.720]   when they're being funded by a particular company.
[00:21:05.720 --> 00:21:09.460]   They don't see themselves as doing so,
[00:21:09.460 --> 00:21:12.880]   but you could just clearly see that the space of opinions
[00:21:12.880 --> 00:21:15.220]   that they're willing to engage in
[00:21:15.220 --> 00:21:18.460]   or a space of ideas they're willing to play with
[00:21:18.460 --> 00:21:22.460]   is one that doesn't include negative,
[00:21:22.460 --> 00:21:25.580]   anything that could possibly be negative about the company.
[00:21:25.580 --> 00:21:28.900]   They just choose not to, 'cause why?
[00:21:28.900 --> 00:21:30.620]   And that's really sad to me,
[00:21:30.620 --> 00:21:33.640]   that if you give me 100 bucks,
[00:21:33.640 --> 00:21:36.340]   I'm less likely to say something negative about you.
[00:21:36.340 --> 00:21:39.900]   That makes me sad,
[00:21:39.900 --> 00:21:42.580]   because the reason I wouldn't say something negative
[00:21:42.580 --> 00:21:45.600]   about you I prefer is the pressure of friendship
[00:21:45.600 --> 00:21:48.340]   and human connection, those kinds of things.
[00:21:48.340 --> 00:21:50.700]   So I understand that.
[00:21:50.700 --> 00:21:52.140]   That's also a problem, by the way,
[00:21:52.140 --> 00:21:54.340]   so they start having dinners and shaking hands
[00:21:54.340 --> 00:21:56.260]   and, "Oh, aren't we friends?"
[00:21:56.260 --> 00:22:00.240]   But the fact that money has that effect is really sad to me.
[00:22:00.240 --> 00:22:04.120]   On the news media, on the journalists, on scientists,
[00:22:04.120 --> 00:22:06.900]   that's scary to me.
[00:22:06.900 --> 00:22:09.220]   But of course, the direct advertisement to consumers,
[00:22:09.220 --> 00:22:11.300]   like you said, is potentially a very negative effect.
[00:22:11.300 --> 00:22:13.020]   I wanted to ask if,
[00:22:13.020 --> 00:22:15.860]   what do you think is the most negative impact
[00:22:15.860 --> 00:22:17.100]   of advertisement?
[00:22:17.100 --> 00:22:20.300]   Is it that direct to consumer on television?
[00:22:20.300 --> 00:22:22.300]   Is it advertisement to doctors,
[00:22:22.300 --> 00:22:26.340]   which I'm surprised to learn I was vaguely looking at
[00:22:26.340 --> 00:22:28.860]   is more than the advertisement,
[00:22:28.860 --> 00:22:32.660]   more is spent on advertising to doctors than to consumers.
[00:22:32.660 --> 00:22:34.060]   That's really confusing to me.
[00:22:34.060 --> 00:22:35.820]   It's fascinating, actually.
[00:22:35.820 --> 00:22:38.940]   And then also, obviously, the law side of things
[00:22:38.940 --> 00:22:40.980]   is the lobbying dollars,
[00:22:40.980 --> 00:22:42.660]   which I think is less than all of those.
[00:22:42.660 --> 00:22:44.700]   But anyway, it's in the ballpark.
[00:22:44.700 --> 00:22:46.540]   What concerns you most?
[00:22:46.540 --> 00:22:49.780]   - Well, it's the whole nexus of influence.
[00:22:49.780 --> 00:22:51.100]   There's not one thing,
[00:22:51.100 --> 00:22:53.980]   and they don't invest all their,
[00:22:53.980 --> 00:22:55.580]   they don't put all their eggs in one basket.
[00:22:55.580 --> 00:23:00.580]   It's a whole surround sound program here.
[00:23:00.580 --> 00:23:04.460]   But in terms of advertisements,
[00:23:04.460 --> 00:23:06.100]   let's take the advertisement,
[00:23:06.100 --> 00:23:09.620]   trulicity is a diabetes drug,
[00:23:09.620 --> 00:23:12.540]   for type two diabetes, an injectable drug.
[00:23:12.540 --> 00:23:15.660]   And it lowers blood sugar just about as well
[00:23:15.660 --> 00:23:18.520]   as metformin does.
[00:23:18.520 --> 00:23:21.040]   Metformin costs about $4 a month.
[00:23:21.040 --> 00:23:25.780]   Trulicity costs, I think, $6,200 a year.
[00:23:25.780 --> 00:23:29.500]   So $48 a year versus 6,200.
[00:23:29.500 --> 00:23:31.600]   Trulicity has distinguished itself
[00:23:31.600 --> 00:23:35.100]   because the manufacturer did a study
[00:23:35.100 --> 00:23:37.340]   that showed that it significantly reduces
[00:23:37.340 --> 00:23:41.080]   the risk of cardiovascular disease in diabetics.
[00:23:41.080 --> 00:23:44.380]   And they got approval on the basis of that study,
[00:23:44.380 --> 00:23:48.100]   that very large study being statistically significant.
[00:23:48.100 --> 00:23:50.120]   What the, so the ad,
[00:23:50.120 --> 00:23:53.340]   the ads obviously extol the virtues of trulicity
[00:23:53.340 --> 00:23:56.820]   because it reduces the risk of heart disease and stroke.
[00:23:56.820 --> 00:23:59.340]   And that's one of the major morbidities,
[00:23:59.340 --> 00:24:01.580]   risks of type two diabetes.
[00:24:01.580 --> 00:24:03.660]   What the ad doesn't say is that you have to treat
[00:24:03.660 --> 00:24:08.100]   323 people to prevent one non-fatal event
[00:24:08.100 --> 00:24:10.300]   at a cost of $2.7 million.
[00:24:10.300 --> 00:24:13.820]   And even more importantly than that,
[00:24:13.820 --> 00:24:17.720]   what the ad doesn't say is that the evidence shows
[00:24:17.720 --> 00:24:22.020]   that engaging in an active, healthy lifestyle program
[00:24:22.020 --> 00:24:25.860]   reduces the risk of heart disease and strokes far more
[00:24:25.860 --> 00:24:27.460]   than trulicity does.
[00:24:27.460 --> 00:24:32.140]   Now, to be fair to the company, the sponsor,
[00:24:32.140 --> 00:24:37.140]   there's never been a study that compared trulicity
[00:24:37.140 --> 00:24:39.780]   to lifestyle changes.
[00:24:39.780 --> 00:24:42.700]   But that's part of the problem of our advertising.
[00:24:42.700 --> 00:24:45.340]   You would think in a rational society
[00:24:45.340 --> 00:24:50.300]   that was way out on a limb as a lone country
[00:24:50.300 --> 00:24:52.300]   besides New Zealand that allows
[00:24:52.300 --> 00:24:54.340]   direct-to-consumer advertising,
[00:24:54.340 --> 00:24:59.140]   that part of allowing direct-to-consumer advertising
[00:24:59.140 --> 00:25:03.060]   would be to mandate that the companies establish
[00:25:03.060 --> 00:25:05.940]   whether their drug is better than, say,
[00:25:05.940 --> 00:25:09.940]   healthy lifestyle adoption to prevent the problems
[00:25:09.940 --> 00:25:11.820]   that they claim to be preventing.
[00:25:11.820 --> 00:25:13.820]   But we don't require that.
[00:25:13.820 --> 00:25:17.580]   So the companies can afford to do very large studies
[00:25:17.580 --> 00:25:19.540]   so that very small differences
[00:25:19.540 --> 00:25:21.860]   become statistically significant.
[00:25:21.860 --> 00:25:23.860]   And their studies are asking the question,
[00:25:23.860 --> 00:25:25.660]   how can we sell more drug?
[00:25:25.660 --> 00:25:27.300]   They're not asking the question,
[00:25:27.300 --> 00:25:30.620]   how can we prevent cardiovascular disease
[00:25:30.620 --> 00:25:32.700]   in people with type 2 diabetes?
[00:25:32.700 --> 00:25:34.220]   And that's how we get off in this,
[00:25:34.220 --> 00:25:38.540]   we're now in the extreme arm of this distortion
[00:25:38.540 --> 00:25:43.060]   of our medical knowledge of studying how to sell more drugs
[00:25:43.060 --> 00:25:45.500]   than how to make people more healthy.
[00:25:45.500 --> 00:25:48.940]   - That's a really great thing to compare it to,
[00:25:48.940 --> 00:25:51.380]   is lifestyle changes.
[00:25:51.380 --> 00:25:53.260]   'Cause that should be the bar.
[00:25:53.260 --> 00:25:56.680]   If you do some basic diet, exercise,
[00:25:56.680 --> 00:25:58.420]   all those kinds of things,
[00:25:58.420 --> 00:26:00.260]   how does this drug compare to that?
[00:26:00.260 --> 00:26:01.420]   - Right, right.
[00:26:01.420 --> 00:26:04.100]   And that study was done, actually, in the '90s.
[00:26:04.100 --> 00:26:06.060]   It's called the Diabetes Prevention Program.
[00:26:06.060 --> 00:26:09.180]   It was federally funded by the NIH
[00:26:09.180 --> 00:26:13.260]   so that there wasn't this drug company imperative
[00:26:13.260 --> 00:26:16.820]   to just try to prove your drug was better than nothing.
[00:26:16.820 --> 00:26:19.660]   And it was a very well-designed study,
[00:26:19.660 --> 00:26:22.460]   randomized, controlled trial,
[00:26:22.460 --> 00:26:25.020]   in people who were at high risk of diabetes,
[00:26:25.020 --> 00:26:26.820]   so-called pre-diabetics.
[00:26:26.820 --> 00:26:30.020]   And they were randomized to three different groups,
[00:26:30.020 --> 00:26:33.780]   a placebo group, a group that got treated with metformin,
[00:26:33.780 --> 00:26:36.180]   and a group that got treated
[00:26:36.180 --> 00:26:38.860]   with intensive lifestyle counseling.
[00:26:38.860 --> 00:26:42.180]   So this study really tested
[00:26:42.180 --> 00:26:45.980]   whether you can get people in a randomized, controlled trial
[00:26:45.980 --> 00:26:49.260]   assigned to intensive lifestyle changes,
[00:26:49.260 --> 00:26:50.660]   whether that works.
[00:26:50.660 --> 00:26:54.640]   Now, the common wisdom amongst physicians,
[00:26:54.640 --> 00:26:56.100]   and I think in general,
[00:26:56.100 --> 00:26:57.940]   is that you can't get people to change.
[00:26:57.940 --> 00:26:59.260]   You can do whatever you want.
[00:26:59.260 --> 00:27:00.300]   You can stand on your head.
[00:27:00.300 --> 00:27:01.580]   You can beg and plead.
[00:27:01.580 --> 00:27:02.620]   People won't change.
[00:27:02.620 --> 00:27:05.060]   So give it up, and let's just move on with the drugs
[00:27:05.060 --> 00:27:06.440]   and not waste any time.
[00:27:06.440 --> 00:27:08.260]   Except this study that was published
[00:27:08.260 --> 00:27:11.020]   in the New England Journal, I think, in 2002,
[00:27:11.020 --> 00:27:12.860]   shows that's wrong,
[00:27:12.860 --> 00:27:16.180]   that the people who were in the intensive lifestyle group
[00:27:16.180 --> 00:27:18.060]   ended up losing 10 pounds,
[00:27:18.060 --> 00:27:21.220]   exercising five times a week, maintaining it,
[00:27:21.220 --> 00:27:26.220]   and reduced their risk of getting diabetes by 58%,
[00:27:26.220 --> 00:27:27.900]   compared to the metformin group,
[00:27:27.900 --> 00:27:31.940]   which reduced its risk of getting diabetes by 31%.
[00:27:31.940 --> 00:27:34.880]   So that exact study was done,
[00:27:34.880 --> 00:27:37.920]   and it showed that lifestyle intervention is the winner.
[00:27:37.920 --> 00:27:44.440]   - Who, as a small tangent, is the leader?
[00:27:44.440 --> 00:27:49.140]   Who is supposed to fight for the side of lifestyle changes?
[00:27:49.140 --> 00:27:54.140]   Where's the big pharma version of lifestyle changes?
[00:27:54.140 --> 00:27:57.240]   Who's supposed to have the big bully pulpit,
[00:27:57.240 --> 00:28:00.960]   the big money behind lifestyle changes, in your sense?
[00:28:00.960 --> 00:28:03.400]   Because that seems to be missing
[00:28:03.400 --> 00:28:06.280]   in a lot of our discussions about health policy.
[00:28:06.280 --> 00:28:08.040]   - Right, that's exactly right.
[00:28:08.040 --> 00:28:12.800]   And the answer is that we assume
[00:28:12.800 --> 00:28:15.920]   that the market has to solve all of these problems,
[00:28:15.920 --> 00:28:18.320]   and the market can't solve all of these problems.
[00:28:18.320 --> 00:28:23.240]   There needs to be some way of protecting the public interest
[00:28:23.240 --> 00:28:26.500]   for things that aren't financially driven,
[00:28:26.500 --> 00:28:28.760]   so that the overriding question has to be
[00:28:28.760 --> 00:28:31.420]   how best to improve Americans' health,
[00:28:31.420 --> 00:28:36.200]   not companies funding studies to try and prove
[00:28:36.200 --> 00:28:39.260]   that their new inexpensive drug is better
[00:28:39.260 --> 00:28:40.880]   and should be used.
[00:28:40.880 --> 00:28:45.440]   - Well, some of that is also people like yourself.
[00:28:45.440 --> 00:28:48.800]   I mean, it's funny, you spoke with Joe Rogan.
[00:28:48.800 --> 00:28:50.960]   He constantly espouses lifestyle changes.
[00:28:50.960 --> 00:28:55.960]   So some of it is almost like understanding the problems
[00:28:55.960 --> 00:28:58.160]   that big pharma is creating in society,
[00:28:58.160 --> 00:29:02.460]   and then sort of these influential voices
[00:29:02.460 --> 00:29:03.560]   speaking up against it.
[00:29:03.560 --> 00:29:08.560]   So whether they're scientists or just regular communicators.
[00:29:08.560 --> 00:29:11.300]   - Yeah, I think you gotta tip your hat to Joe
[00:29:11.300 --> 00:29:13.140]   for getting that message out.
[00:29:13.140 --> 00:29:17.340]   And he clearly believes it and does his best.
[00:29:17.340 --> 00:29:21.020]   But it's not coming out in the legitimate avenues,
[00:29:21.020 --> 00:29:26.020]   in the legitimate channels that are evidence-based medicine
[00:29:26.020 --> 00:29:31.020]   and from the sources that the docs are trained to listen to
[00:29:31.020 --> 00:29:34.320]   and modify their patient care on.
[00:29:34.320 --> 00:29:36.460]   Now, it's not 100%.
[00:29:36.460 --> 00:29:40.180]   I mean, there are articles in the big journals
[00:29:40.180 --> 00:29:42.160]   about the benefits of lifestyle,
[00:29:42.160 --> 00:29:45.800]   but they don't carry the same gravitas
[00:29:45.800 --> 00:29:48.220]   as the randomized controlled trials
[00:29:48.220 --> 00:29:50.300]   that test this drug against placebo
[00:29:50.300 --> 00:29:52.340]   or this drug against another drug.
[00:29:52.340 --> 00:29:55.660]   So the Joe Rogans of the world keep going.
[00:29:55.660 --> 00:29:57.020]   I tip my hat.
[00:29:57.020 --> 00:30:00.900]   But it's not gonna carry the day for most of the people
[00:30:00.900 --> 00:30:04.220]   until it has the legitimacy of the medical establishment.
[00:30:04.220 --> 00:30:05.940]   - Yeah, like something that the doctors
[00:30:05.940 --> 00:30:07.100]   really pay attention to.
[00:30:07.100 --> 00:30:09.100]   Well, there's an entire mechanism
[00:30:09.100 --> 00:30:11.280]   established for testing drugs.
[00:30:11.280 --> 00:30:14.380]   There's not an entire mechanism established
[00:30:14.380 --> 00:30:17.580]   in terms of scientific rigor of testing lifestyle changes.
[00:30:17.580 --> 00:30:20.460]   I mean, it's more difficult.
[00:30:20.460 --> 00:30:22.700]   I mean, everything's difficult in science,
[00:30:23.660 --> 00:30:27.100]   science that involves humans especially,
[00:30:27.100 --> 00:30:30.620]   but it's just, these studies are very expensive.
[00:30:30.620 --> 00:30:31.940]   They're difficult.
[00:30:31.940 --> 00:30:33.420]   It's difficult to find conclusions
[00:30:33.420 --> 00:30:35.460]   and to control all the variables.
[00:30:35.460 --> 00:30:37.340]   And so it's very easy to dismiss them
[00:30:37.340 --> 00:30:40.940]   unless you really do a huge study that's very well-funded.
[00:30:40.940 --> 00:30:42.820]   And so maybe the doctors just lean
[00:30:42.820 --> 00:30:45.740]   towards the simpler studies over and over,
[00:30:45.740 --> 00:30:48.020]   which is what the drug companies fund.
[00:30:48.020 --> 00:30:50.400]   They can control more variables.
[00:30:51.380 --> 00:30:53.660]   See, but the control there is sometimes
[00:30:53.660 --> 00:31:00.940]   by hiding things too, right?
[00:31:00.940 --> 00:31:03.500]   So sometimes you can just say
[00:31:03.500 --> 00:31:06.460]   that this is a well-controlled study
[00:31:06.460 --> 00:31:09.300]   by pretending there's a bunch of other stuff,
[00:31:09.300 --> 00:31:13.300]   just ignoring the stuff that could be correlated,
[00:31:13.300 --> 00:31:15.580]   it could be the real cause of the effects you're seeing,
[00:31:15.580 --> 00:31:17.380]   all that kind of stuff.
[00:31:17.380 --> 00:31:21.740]   So money can buy ignorance, I suppose, in science.
[00:31:21.740 --> 00:31:24.740]   - It buys the kind of blinders that are on,
[00:31:24.740 --> 00:31:28.060]   that don't look outside the reductionist model.
[00:31:28.060 --> 00:31:31.500]   And that's another issue is that we kind of,
[00:31:31.500 --> 00:31:34.340]   nobody says to doctors in training,
[00:31:34.340 --> 00:31:39.340]   only listen to reductionist studies and conclusions
[00:31:39.340 --> 00:31:42.100]   and methods of promoting health.
[00:31:42.100 --> 00:31:44.020]   Nobody says that explicitly,
[00:31:44.020 --> 00:31:47.660]   but the respectable science
[00:31:47.660 --> 00:31:49.780]   has to do with controlling the factors.
[00:31:49.780 --> 00:31:54.220]   And I mean, it just doesn't make sense to me.
[00:31:54.220 --> 00:31:55.460]   I'm gonna pick on Trulicity
[00:31:55.460 --> 00:31:57.280]   'cause it's such an obvious example,
[00:31:57.280 --> 00:32:01.360]   but it's not more egregious than many others.
[00:32:01.360 --> 00:32:03.740]   It doesn't make sense to me to allow a drug
[00:32:03.740 --> 00:32:07.000]   to be advertised as preventing cardiovascular disease
[00:32:07.000 --> 00:32:10.000]   when you haven't included lifestyle changes
[00:32:10.000 --> 00:32:11.700]   as an arm in the study.
[00:32:11.700 --> 00:32:14.660]   It's just so crystal clear
[00:32:14.660 --> 00:32:17.460]   that the purpose of that study is to sell Trulicity.
[00:32:17.460 --> 00:32:20.460]   It's not to prevent cardiovascular disease.
[00:32:20.460 --> 00:32:24.760]   If we were in charge, I would try to convince you
[00:32:24.760 --> 00:32:26.020]   that anywhere that study,
[00:32:26.020 --> 00:32:31.020]   the results of that study were presented to physicians,
[00:32:31.020 --> 00:32:33.580]   it would be stamped in big red letters,
[00:32:33.580 --> 00:32:37.720]   this study did not compare Trulicity to lifestyle changes.
[00:32:37.720 --> 00:32:38.980]   They need to know that.
[00:32:38.980 --> 00:32:40.660]   And the docs are kind of trained,
[00:32:40.660 --> 00:32:42.660]   these blinders get put on,
[00:32:42.660 --> 00:32:46.300]   and they're trained to kind of forget that that's not there.
[00:32:46.300 --> 00:32:48.220]   - Do you think, so first of all,
[00:32:48.220 --> 00:32:51.300]   that's a small or big change to advertisement
[00:32:51.300 --> 00:32:53.100]   that seems obvious to say,
[00:32:53.100 --> 00:32:56.500]   like in force that it should be compared
[00:32:56.500 --> 00:32:57.680]   to lifestyle changes.
[00:32:57.680 --> 00:33:02.580]   Do you think advertisements period in the United States
[00:33:02.580 --> 00:33:05.920]   for pharmaceutical drugs should be banned?
[00:33:05.920 --> 00:33:07.540]   - I think they can't be banned.
[00:33:07.540 --> 00:33:09.220]   So it doesn't matter what I think.
[00:33:09.220 --> 00:33:10.520]   (Lex laughing)
[00:33:10.520 --> 00:33:11.360]   - Okay.
[00:33:11.360 --> 00:33:13.180]   Let's say you were a dictator,
[00:33:13.180 --> 00:33:15.180]   and two, why can't they be banned?
[00:33:15.180 --> 00:33:16.740]   - Okay.
[00:33:16.740 --> 00:33:17.740]   - Answer either one.
[00:33:17.740 --> 00:33:22.820]   - I believe, I've been told by lawyers who I trust,
[00:33:22.820 --> 00:33:27.280]   that the freedom of speech in the US Constitution
[00:33:27.280 --> 00:33:29.380]   is such that you can't ban them,
[00:33:29.380 --> 00:33:33.300]   that you could ban cigarettes and alcohol,
[00:33:33.300 --> 00:33:35.660]   which have no therapeutic use,
[00:33:35.660 --> 00:33:37.660]   but drugs have a therapeutic use,
[00:33:37.680 --> 00:33:41.600]   and advertisements about them can't be banned.
[00:33:41.600 --> 00:33:43.680]   Let's assume that they can't be,
[00:33:43.680 --> 00:33:45.840]   'cause we know they won't be anyway,
[00:33:45.840 --> 00:33:49.120]   but let's assume they can't be,
[00:33:49.120 --> 00:33:51.820]   and especially our Supreme Court now
[00:33:51.820 --> 00:33:55.900]   would be unlikely to take that seriously.
[00:33:55.900 --> 00:33:57.360]   But that's not the issue.
[00:33:57.360 --> 00:34:00.240]   The issue is that if the drug companies
[00:34:00.240 --> 00:34:02.620]   wanna spend their money advertising,
[00:34:02.620 --> 00:34:06.880]   they should have to have independent analysis
[00:34:06.880 --> 00:34:10.440]   of the message that the viewers are left with
[00:34:10.440 --> 00:34:13.400]   about the drug, so that it's realistic.
[00:34:13.400 --> 00:34:15.520]   What's the chance the drug will help them?
[00:34:15.520 --> 00:34:19.000]   Well, in trulicity, it's one out of 323.
[00:34:19.000 --> 00:34:21.120]   322 people aren't gonna benefit
[00:34:21.120 --> 00:34:23.700]   from the cardiovascular risk reduction.
[00:34:23.700 --> 00:34:26.880]   What's the true cost?
[00:34:26.880 --> 00:34:30.640]   When drugs advertise that you may be able to get this
[00:34:30.640 --> 00:34:33.880]   for a $25 copay or something,
[00:34:33.880 --> 00:34:35.960]   tens of thousands of dollars a year drug,
[00:34:35.960 --> 00:34:40.100]   for a $25 copay, what an enormous disservice that is,
[00:34:40.100 --> 00:34:42.600]   to misrepresent the cost to society.
[00:34:42.600 --> 00:34:44.040]   That should not be allowed.
[00:34:44.040 --> 00:34:48.680]   So you should have to make it clear to the viewers
[00:34:48.680 --> 00:34:49.960]   how many people are gonna benefit,
[00:34:49.960 --> 00:34:51.680]   what's your chance of benefiting,
[00:34:51.680 --> 00:34:53.560]   how does it compare to lifestyle changes
[00:34:53.560 --> 00:34:55.840]   or less expensive therapies,
[00:34:55.840 --> 00:34:58.440]   what do you give up if you use a less expensive therapy
[00:34:58.440 --> 00:34:59.960]   or gain, perhaps.
[00:34:59.960 --> 00:35:01.160]   - And how much it costs.
[00:35:01.160 --> 00:35:02.280]   - How much it costs.
[00:35:02.280 --> 00:35:03.560]   Now, that can go either way,
[00:35:03.560 --> 00:35:06.640]   'cause if you say Humira costs $72,000
[00:35:06.640 --> 00:35:08.960]   and it's no more effective as a first-line drug
[00:35:08.960 --> 00:35:12.280]   than methotrexate, which costs $480,
[00:35:12.280 --> 00:35:14.960]   people might say, "I want the expensive drug
[00:35:14.960 --> 00:35:17.720]   "'cause I can get it for a $25 copay."
[00:35:17.720 --> 00:35:21.640]   So you'd have to temper that a little bit.
[00:35:21.640 --> 00:35:25.560]   - Oh, you mean people are so, they don't care.
[00:35:25.560 --> 00:35:27.640]   - They don't care, their insurance is gonna cover it
[00:35:27.640 --> 00:35:29.440]   and it's a $25 copay,
[00:35:29.440 --> 00:35:31.580]   but we could figure out how to deal with that.
[00:35:31.580 --> 00:35:35.240]   The main point is that if we assume
[00:35:35.240 --> 00:35:38.760]   that advertisements are gonna keep going, and they are,
[00:35:38.760 --> 00:35:43.760]   we could require that there be outside evaluation
[00:35:43.760 --> 00:35:48.880]   of the message that reasonable, unbiased viewers
[00:35:48.880 --> 00:35:50.960]   take away from the ads,
[00:35:50.960 --> 00:35:54.220]   and the ads would have to tell the truth about the drug.
[00:35:54.220 --> 00:36:00.720]   - And the truth should have sub-truth guardrails,
[00:36:01.100 --> 00:36:03.680]   meaning like the cost that we talked about,
[00:36:03.680 --> 00:36:07.080]   the effects compared to things that actually,
[00:36:07.080 --> 00:36:11.800]   lifestyle changes, just these details,
[00:36:11.800 --> 00:36:16.520]   very strict guardrails of what actually has to be specified.
[00:36:16.520 --> 00:36:19.360]   - And I would make it against the law
[00:36:19.360 --> 00:36:23.360]   to have family picnics or dogs catching Frisbees in the ads.
[00:36:23.360 --> 00:36:26.160]   - So, (laughs)
[00:36:26.160 --> 00:36:28.660]   you mean 95% of the ads, yes.
[00:36:30.600 --> 00:36:32.620]   I mean, there's something dark and inauthentic
[00:36:32.620 --> 00:36:34.500]   about those advertisements, but they see,
[00:36:34.500 --> 00:36:36.240]   I mean, I'm sure they're being done
[00:36:36.240 --> 00:36:38.600]   'cause they work for the target audience.
[00:36:38.600 --> 00:36:45.120]   And then the doctors too.
[00:36:45.120 --> 00:36:48.720]   Can you really buy a doctor's opinion?
[00:36:48.720 --> 00:36:51.320]   Why does it have such an effect on doctors,
[00:36:51.320 --> 00:36:53.520]   advertisement to doctors?
[00:36:53.520 --> 00:36:55.800]   Like you as a physician, again,
[00:36:55.800 --> 00:36:58.360]   like from everything I've seen, people love you.
[00:36:58.360 --> 00:36:59.280]   (laughs)
[00:36:59.280 --> 00:37:04.280]   And I've just, people should definitely look you up from,
[00:37:04.280 --> 00:37:09.280]   there's a bunch of videos of you giving talks on YouTube,
[00:37:09.280 --> 00:37:14.280]   and it's just, it's so refreshing to hear
[00:37:14.280 --> 00:37:17.440]   just the clarity of thought about health policy,
[00:37:17.440 --> 00:37:19.640]   about healthcare, just the way you think
[00:37:19.640 --> 00:37:20.640]   throughout the years.
[00:37:20.640 --> 00:37:21.480]   - Thank you.
[00:37:21.480 --> 00:37:23.120]   - So, like it's easy to think about,
[00:37:23.120 --> 00:37:25.360]   like maybe you're criticizing Big Pharma,
[00:37:25.360 --> 00:37:28.800]   that's one part of the message that you're talking about,
[00:37:28.800 --> 00:37:33.000]   but that's not, like your brilliance actually shines
[00:37:33.000 --> 00:37:35.440]   in the positive, in the solutions and how to do it.
[00:37:35.440 --> 00:37:40.440]   So as a doctor, what affects your mind?
[00:37:40.440 --> 00:37:43.120]   And how does Big Pharma affect your mind?
[00:37:43.120 --> 00:37:46.480]   - Number one, the information that comes through
[00:37:46.480 --> 00:37:50.320]   legitimate sources that doctors have been taught
[00:37:50.320 --> 00:37:52.560]   to rely on, evidence-based medicine,
[00:37:52.560 --> 00:37:55.420]   the articles in peer-reviewed journals,
[00:37:55.420 --> 00:37:57.020]   the guidelines that are issued.
[00:37:57.020 --> 00:37:59.240]   Now, those are problematic,
[00:37:59.240 --> 00:38:03.240]   because when an article is peer-reviewed
[00:38:03.240 --> 00:38:05.140]   and published in a respected journal,
[00:38:05.140 --> 00:38:10.360]   people and doctors obviously assume
[00:38:10.360 --> 00:38:15.360]   that the peer reviewers have had access to the data
[00:38:15.360 --> 00:38:18.400]   and they've independently analyzed the data,
[00:38:18.400 --> 00:38:21.920]   and they corroborate the findings in the manuscript
[00:38:21.920 --> 00:38:25.760]   that was submitted, or they give feedback to the authors
[00:38:25.760 --> 00:38:28.180]   and say, "We disagree with you on this point,
[00:38:28.180 --> 00:38:30.680]   "and would you please check our analysis,
[00:38:30.680 --> 00:38:32.460]   "and if you agree with us, make it."
[00:38:32.460 --> 00:38:35.620]   That's what they assume the peer-review process is,
[00:38:35.620 --> 00:38:36.920]   but it's not.
[00:38:36.920 --> 00:38:39.240]   The peer reviewers don't have the data.
[00:38:39.240 --> 00:38:41.840]   The peer reviewers have the manuscript
[00:38:41.840 --> 00:38:46.260]   that's been submitted by the, usually in conjunction with,
[00:38:46.260 --> 00:38:51.260]   or by the drug company that manufactures the drug.
[00:38:51.540 --> 00:38:56.540]   So peer reviewers are unable to perform the job
[00:38:56.540 --> 00:38:59.740]   that doctors think they're performing
[00:38:59.740 --> 00:39:03.300]   to vet the data to assure that it's accurate
[00:39:03.300 --> 00:39:05.100]   and reasonably complete.
[00:39:05.100 --> 00:39:06.340]   They can't do it.
[00:39:06.340 --> 00:39:09.500]   And then we have the clinical practice guidelines,
[00:39:09.500 --> 00:39:11.340]   which are increasingly more important,
[00:39:11.340 --> 00:39:15.900]   as the information, the flow of information
[00:39:15.900 --> 00:39:18.740]   keeps getting brisker and brisker,
[00:39:18.740 --> 00:39:22.020]   and docs need to get to the bottom line quickly.
[00:39:22.020 --> 00:39:25.700]   Clinical practice guidelines become much more important,
[00:39:25.700 --> 00:39:28.800]   and we assume that the authors
[00:39:28.800 --> 00:39:30.380]   of those clinical practice guidelines
[00:39:30.380 --> 00:39:32.380]   have independently analyzed the data
[00:39:32.380 --> 00:39:35.880]   from the clinical trials and make their recommendations
[00:39:35.880 --> 00:39:39.140]   that set the standards of care based on their analysis.
[00:39:39.140 --> 00:39:40.860]   That's not what happens.
[00:39:40.860 --> 00:39:44.180]   The experts who write the clinical trials
[00:39:44.180 --> 00:39:49.180]   rely almost entirely on the publications
[00:39:49.180 --> 00:39:51.940]   presenting the results of the clinical trials,
[00:39:51.940 --> 00:39:53.980]   which are peer reviewed, but the peer reviewers
[00:39:53.980 --> 00:39:56.340]   haven't had access to the data.
[00:39:56.340 --> 00:40:01.140]   So we've got a system of the highest level of evidence
[00:40:01.140 --> 00:40:03.780]   that doctors have been trained over and over again
[00:40:03.780 --> 00:40:06.420]   to rely on to practice evidence-based medicine
[00:40:06.420 --> 00:40:10.860]   to be good doctors that has not been verified.
[00:40:10.860 --> 00:40:14.300]   - Do you think that data that's coming
[00:40:14.300 --> 00:40:18.180]   from the pharma companies, do you think they're,
[00:40:18.180 --> 00:40:22.520]   what level of manipulation is going on with that data?
[00:40:22.520 --> 00:40:25.940]   Is it at the study design level?
[00:40:25.940 --> 00:40:28.140]   Is it at literally there's some data
[00:40:28.140 --> 00:40:33.140]   that you just keep off, you know, keep out of the charts,
[00:40:33.140 --> 00:40:38.580]   keep out of the aggregate analysis that you then publish?
[00:40:38.580 --> 00:40:41.380]   Or is it the worst case,
[00:40:41.380 --> 00:40:44.620]   which is just change some of the numbers?
[00:40:44.620 --> 00:40:46.160]   - It happened, all three happened.
[00:40:46.160 --> 00:40:48.540]   I can't, I don't know what the denominator is,
[00:40:48.540 --> 00:40:51.660]   but I spent about 10 years in litigation.
[00:40:51.660 --> 00:40:55.820]   And for example, in Vioxx, which was withdrawn
[00:40:55.820 --> 00:40:59.500]   from the market in 2004 in the biggest drug recall
[00:40:59.500 --> 00:41:04.500]   in American history, the problem was that it got recalled
[00:41:06.100 --> 00:41:09.060]   when a study that Merck sponsored showed
[00:41:09.060 --> 00:41:11.620]   that Vioxx doubled the risk, more than doubled the risk
[00:41:11.620 --> 00:41:15.300]   of heart attacks, strokes, and blood clots,
[00:41:15.300 --> 00:41:18.120]   serious blood clots, it got pulled then.
[00:41:18.120 --> 00:41:20.660]   But there was a study, a bigger study
[00:41:20.660 --> 00:41:22.760]   that had been published in 2000
[00:41:22.760 --> 00:41:24.660]   in the New England Journal of Medicine
[00:41:24.660 --> 00:41:28.580]   that showed that Vioxx was a better drug
[00:41:28.580 --> 00:41:33.580]   for arthritis and pain, not because it was more effective.
[00:41:34.240 --> 00:41:36.440]   It's no more effective than Aleve or Advil,
[00:41:36.440 --> 00:41:40.200]   but because it was less likely
[00:41:40.200 --> 00:41:43.200]   to cause serious GI complications,
[00:41:43.200 --> 00:41:45.100]   bleeds and perforations in the gut.
[00:41:45.100 --> 00:41:48.160]   Now, in that study that was published
[00:41:48.160 --> 00:41:51.620]   in the New England Journal that was never corrected,
[00:41:51.620 --> 00:41:55.960]   it was a little bit modified 15 months
[00:41:55.960 --> 00:41:57.540]   after the drug was taken off the market,
[00:41:57.540 --> 00:42:01.560]   but never corrected, Merck left out three heart attacks.
[00:42:02.680 --> 00:42:05.620]   And the FDA knew that Merck left out three heart attacks,
[00:42:05.620 --> 00:42:10.420]   and the FDA's analysis of the data from that study
[00:42:10.420 --> 00:42:14.720]   said that the FDA wasn't gonna do the analysis
[00:42:14.720 --> 00:42:16.940]   without the three heart attacks in it.
[00:42:16.940 --> 00:42:19.660]   And the important part of this story
[00:42:19.660 --> 00:42:23.100]   is that there were 12 authors listed on that study
[00:42:23.100 --> 00:42:26.260]   in the New England Journal, two were Merck employees,
[00:42:26.260 --> 00:42:27.780]   they knew about the three heart attacks
[00:42:27.780 --> 00:42:29.620]   that had been omitted.
[00:42:29.620 --> 00:42:34.440]   The other 10 authors, the academic authors,
[00:42:34.440 --> 00:42:37.060]   didn't know about it, they hadn't seen that data.
[00:42:37.060 --> 00:42:41.920]   So Merck just, they had an excuse,
[00:42:41.920 --> 00:42:44.240]   it's complicated and the FDA didn't accept it,
[00:42:44.240 --> 00:42:46.800]   so there's no reason to go into it.
[00:42:46.800 --> 00:42:48.800]   But Merck just left out the three heart attacks.
[00:42:48.800 --> 00:42:51.300]   And the three heart attacks, it may seem three heart attacks
[00:42:51.300 --> 00:42:54.160]   in a 10,000 person study may seem like nothing,
[00:42:54.160 --> 00:42:57.540]   except they completely changed the statistics
[00:42:57.540 --> 00:43:00.020]   so that had the three heart attacks been included,
[00:43:00.020 --> 00:43:02.600]   the only conclusion that Merck could have made
[00:43:02.600 --> 00:43:04.700]   was that Vioxx significantly increased
[00:43:04.700 --> 00:43:06.360]   the risk of heart attack.
[00:43:06.360 --> 00:43:09.600]   And they abbreviated their endpoint
[00:43:09.600 --> 00:43:12.380]   from heart attacks, strokes, and blood clots
[00:43:12.380 --> 00:43:13.820]   to just heart attacks.
[00:43:13.820 --> 00:43:17.120]   - Yeah, so those are maybe in their mind,
[00:43:17.120 --> 00:43:18.300]   they're also playing by the rules
[00:43:18.300 --> 00:43:19.820]   because of some technical excuse
[00:43:19.820 --> 00:43:22.220]   that you mentioned that was rejected.
[00:43:22.220 --> 00:43:23.980]   How can this-- - No, no, no,
[00:43:23.980 --> 00:43:26.500]   let me interrupt, no, that's not true.
[00:43:27.340 --> 00:43:32.040]   The study was completed, the blind was broken,
[00:43:32.040 --> 00:43:34.420]   meaning they looked at the data.
[00:43:34.420 --> 00:43:37.360]   In March of 2000, the article was published
[00:43:37.360 --> 00:43:40.060]   in the New England Journal in November of 2000.
[00:43:40.060 --> 00:43:45.060]   In March of 2000, there was an email by the head scientist
[00:43:45.060 --> 00:43:48.180]   that was published in the Wall Street Journal
[00:43:48.180 --> 00:43:53.820]   that said the day that the data were unblinded,
[00:43:53.820 --> 00:43:57.660]   that it's a shame that the cardiovascular events are there,
[00:43:57.660 --> 00:44:03.620]   but the drug will do well and we will do well.
[00:44:03.620 --> 00:44:10.780]   - But removing the three heart attacks,
[00:44:10.780 --> 00:44:12.300]   how does that happen?
[00:44:12.300 --> 00:44:16.780]   Who has to convince themselves?
[00:44:16.780 --> 00:44:18.540]   Is this pure malevolence?
[00:44:18.540 --> 00:44:21.400]   - You have to be the judge of that,
[00:44:21.400 --> 00:44:23.620]   but the person who was in charge
[00:44:23.620 --> 00:44:25.700]   of the Data Safety Monitoring Board
[00:44:25.700 --> 00:44:29.560]   issued a letter that said they'll stop
[00:44:29.560 --> 00:44:32.380]   counting cardiovascular events
[00:44:32.380 --> 00:44:35.100]   a month before the trial is over
[00:44:35.100 --> 00:44:37.560]   and they'll continue counting GI events.
[00:44:37.560 --> 00:44:43.300]   And that person got a contract to consult with Merck
[00:44:43.300 --> 00:44:47.540]   for $5,000 a day, I think for 12 days a year
[00:44:47.540 --> 00:44:49.540]   for one or two years,
[00:44:49.540 --> 00:44:54.540]   that was signed, that contract was signed
[00:44:54.540 --> 00:44:58.180]   within two weeks of the decision
[00:44:58.180 --> 00:45:00.420]   to stop counting heart attacks.
[00:45:00.420 --> 00:45:03.100]   - I wanna understand that man or woman.
[00:45:03.100 --> 00:45:07.300]   I wanna, I want, it's the,
[00:45:07.300 --> 00:45:09.580]   been reading a lot about Nazi Germany
[00:45:09.580 --> 00:45:12.260]   and thinking a lot about the good Germans
[00:45:12.260 --> 00:45:18.300]   because I want to understand so that we can each
[00:45:19.040 --> 00:45:22.100]   encourage each other to take the small heroic actions
[00:45:22.100 --> 00:45:23.820]   that prevents that.
[00:45:23.820 --> 00:45:28.420]   Because it feels to me removing malevolence from the table
[00:45:28.420 --> 00:45:31.260]   where it's just a pure psychopathic person,
[00:45:31.260 --> 00:45:34.880]   that there's just a momentum created by the game,
[00:45:34.880 --> 00:45:35.720]   like you mentioned.
[00:45:35.720 --> 00:45:36.680]   - Yes.
[00:45:36.680 --> 00:45:41.020]   - And so it takes reversing the momentum
[00:45:41.020 --> 00:45:44.180]   within a company, I think requires
[00:45:44.180 --> 00:45:46.880]   many small acts of heroism.
[00:45:46.880 --> 00:45:50.660]   Not gigantic, I'm going to leave and become a whistleblower
[00:45:50.660 --> 00:45:52.480]   and publish a book about it.
[00:45:52.480 --> 00:45:57.060]   But small, quiet acts of pressuring against this.
[00:45:57.060 --> 00:45:59.220]   Like, what are we doing here?
[00:45:59.220 --> 00:46:00.460]   We're trying to help people.
[00:46:00.460 --> 00:46:01.700]   Is this the right thing to do?
[00:46:01.700 --> 00:46:03.380]   Looking in the mirror constantly and asking,
[00:46:03.380 --> 00:46:05.240]   is this the right thing to do?
[00:46:05.240 --> 00:46:07.620]   I mean, that's how, that's what integrity is.
[00:46:07.620 --> 00:46:11.220]   Acknowledging the pressures you're under
[00:46:11.220 --> 00:46:13.620]   and then still be able to zoom out and think,
[00:46:13.620 --> 00:46:15.380]   what is the right thing to do here?
[00:46:16.620 --> 00:46:18.600]   But the data, hiding the data,
[00:46:18.600 --> 00:46:22.660]   makes it too easy to live in ignorance.
[00:46:22.660 --> 00:46:25.420]   So like, within those, inside those companies.
[00:46:25.420 --> 00:46:34.500]   So your idea is that the reviewers should see the data.
[00:46:34.500 --> 00:46:36.340]   That's one step.
[00:46:36.340 --> 00:46:39.740]   So to even push back on that idea is,
[00:46:39.740 --> 00:46:43.340]   I assume you mean that data remains private
[00:46:43.340 --> 00:46:47.020]   except to the peer reviewers.
[00:46:47.020 --> 00:46:49.580]   The problem, of course, as you probably know,
[00:46:49.580 --> 00:46:51.980]   is the peer review process is not perfect.
[00:46:51.980 --> 00:46:55.460]   You know, it's individuals.
[00:46:55.460 --> 00:46:58.740]   It feels like there should be a lot more eyes on the data
[00:46:58.740 --> 00:47:00.420]   than just the peer reviewers.
[00:47:00.420 --> 00:47:01.340]   - Yes.
[00:47:01.340 --> 00:47:03.500]   This is not a hard problem to solve.
[00:47:03.500 --> 00:47:06.620]   When a study is completed,
[00:47:06.620 --> 00:47:09.000]   a clinical study report is made.
[00:47:09.000 --> 00:47:12.260]   And it's usually several thousand pages.
[00:47:12.260 --> 00:47:15.900]   And what it does is it takes the raw patient data
[00:47:15.900 --> 00:47:19.660]   and it tabulates it in the ways,
[00:47:19.660 --> 00:47:22.020]   it's supposedly and usually,
[00:47:22.020 --> 00:47:25.620]   in the ways that the company has pre-specified.
[00:47:25.620 --> 00:47:28.420]   So that you then end up with a searchable,
[00:47:28.420 --> 00:47:30.720]   let's say 3,000 page document.
[00:47:30.720 --> 00:47:36.180]   As I became more experienced as an expert in litigation,
[00:47:36.180 --> 00:47:39.660]   I could go through those documents pretty quickly.
[00:47:39.660 --> 00:47:42.040]   Quickly may mean 20 hours or 40 hours,
[00:47:42.040 --> 00:47:44.340]   but it doesn't mean three months of my work.
[00:47:44.340 --> 00:47:49.120]   And see if the company's,
[00:47:49.120 --> 00:47:51.660]   if the way the company has analyzed the data
[00:47:51.660 --> 00:47:55.800]   is consistent with their statistical analysis plan
[00:47:55.800 --> 00:48:00.060]   and their pre-specified outcome measures.
[00:48:00.060 --> 00:48:01.300]   It's not hard.
[00:48:01.300 --> 00:48:02.800]   And I think you're right.
[00:48:02.800 --> 00:48:06.200]   Peer reviewers, I don't peer review clinical trials,
[00:48:06.200 --> 00:48:09.300]   but I peer review other kinds of articles.
[00:48:09.300 --> 00:48:11.540]   I have to do one on the airplane on the way home.
[00:48:11.540 --> 00:48:12.380]   And it's hard.
[00:48:12.380 --> 00:48:15.620]   I mean, we're just ordinary mortal people volunteering.
[00:48:15.620 --> 00:48:19.160]   - Unpaid, the motivation is not clear.
[00:48:19.160 --> 00:48:22.080]   - The motivation is to keep,
[00:48:22.080 --> 00:48:27.920]   to be a good citizen in the medical community
[00:48:27.920 --> 00:48:31.120]   and to be on friendly terms with the journals
[00:48:31.120 --> 00:48:33.260]   so that if you want to get published,
[00:48:33.260 --> 00:48:37.320]   there's sort of an unspoken incentive.
[00:48:37.320 --> 00:48:39.840]   - As somebody who enjoys game theory,
[00:48:39.840 --> 00:48:42.200]   I feel like that motivation is good,
[00:48:42.200 --> 00:48:43.500]   but could be a lot better.
[00:48:43.500 --> 00:48:46.540]   - Yes, you should get more recognition
[00:48:46.540 --> 00:48:50.260]   or in some way academic credit for it.
[00:48:50.260 --> 00:48:52.980]   It should go to your career advancement.
[00:48:52.980 --> 00:48:54.380]   - If it's an important paper
[00:48:54.380 --> 00:48:56.600]   and you recognize it's an important paper
[00:48:56.600 --> 00:48:58.420]   as a great peer reviewer,
[00:48:58.420 --> 00:49:01.780]   that this is not in that area where it's
[00:49:01.780 --> 00:49:05.920]   clearly a piece of crap paper
[00:49:05.920 --> 00:49:08.300]   or clearly an awesome paper
[00:49:08.300 --> 00:49:10.980]   that doesn't have controversial aspects to it
[00:49:10.980 --> 00:49:13.140]   and it's just a beautiful piece of work,
[00:49:13.140 --> 00:49:14.660]   okay, those are easy.
[00:49:14.660 --> 00:49:17.740]   And then there's like the very difficult gray area
[00:49:17.740 --> 00:49:20.220]   which may require many, many days of work
[00:49:20.220 --> 00:49:21.900]   on your part as a peer reviewer.
[00:49:21.900 --> 00:49:24.400]   So it's not just a couple hours,
[00:49:24.400 --> 00:49:27.300]   but really seriously reading.
[00:49:27.300 --> 00:49:30.700]   Like some papers can take months to really understand.
[00:49:30.700 --> 00:49:32.780]   So if you really want to struggle,
[00:49:32.780 --> 00:49:35.900]   there has to be an incentive for that struggle.
[00:49:35.900 --> 00:49:40.900]   - Yes, and billions of dollars ride on some of these studies.
[00:49:40.900 --> 00:49:43.780]   - And lies, right?
[00:49:43.780 --> 00:49:44.700]   Not to mention.
[00:49:44.700 --> 00:49:48.340]   - Right, but it would be easy to have
[00:49:48.340 --> 00:49:52.380]   full-time statisticians hired by the journals
[00:49:52.380 --> 00:49:53.860]   or shared by the journals
[00:49:53.860 --> 00:50:00.300]   who were independent of any other financial incentive
[00:50:00.300 --> 00:50:03.980]   to go over these kind of methodological issues
[00:50:03.980 --> 00:50:08.900]   and take responsibility for certifying the analyses
[00:50:08.900 --> 00:50:11.180]   that are done and then pass it on
[00:50:11.180 --> 00:50:14.100]   to the volunteer peer reviewers.
[00:50:14.100 --> 00:50:15.940]   - See, I believe even in this,
[00:50:15.940 --> 00:50:19.420]   in the sort of capitalism or even social capital,
[00:50:19.420 --> 00:50:23.580]   after watching Twitter in the time of COVID
[00:50:23.580 --> 00:50:27.420]   and just looking at people that investigate themselves,
[00:50:27.420 --> 00:50:30.060]   I believe in the citizenry.
[00:50:30.060 --> 00:50:32.460]   People, if you give them access to the data,
[00:50:32.460 --> 00:50:35.900]   like these citizen scientists arise.
[00:50:35.900 --> 00:50:38.360]   A lot of them on the, it's kind of funny.
[00:50:38.360 --> 00:50:42.220]   A lot of people are just really used to working with data.
[00:50:42.220 --> 00:50:44.620]   They don't know anything about medicine
[00:50:44.620 --> 00:50:46.820]   and they don't have actually the biases
[00:50:46.820 --> 00:50:48.900]   that a lot of doctors and medical
[00:50:48.900 --> 00:50:51.060]   and a lot of the people that read these papers,
[00:50:51.060 --> 00:50:53.260]   they'll just go raw into the data
[00:50:53.260 --> 00:50:56.140]   and look at it with, like they're bored almost,
[00:50:56.140 --> 00:50:58.340]   and they do incredible analysis.
[00:50:58.340 --> 00:51:01.060]   So there's some argument to be made
[00:51:01.060 --> 00:51:04.060]   for a lot of this data to become public.
[00:51:04.060 --> 00:51:07.120]   Like de-anonymized, no, sorry, anonymized,
[00:51:07.120 --> 00:51:11.100]   all that kind of stuff, but for a lot of it to be public,
[00:51:11.100 --> 00:51:13.340]   especially when you're talking about things
[00:51:13.340 --> 00:51:16.940]   as impactful as some of these drugs.
[00:51:16.940 --> 00:51:18.100]   - I agree 100%.
[00:51:18.100 --> 00:51:19.960]   So let's turn the micro,
[00:51:19.960 --> 00:51:22.140]   let's get a little bit more granular.
[00:51:22.140 --> 00:51:24.180]   On the peer review issue,
[00:51:24.180 --> 00:51:27.780]   we're talking about pre-publication transparencies.
[00:51:27.780 --> 00:51:29.580]   And that is critically important.
[00:51:29.580 --> 00:51:33.580]   Once a paper is published, the horses are out of the barn
[00:51:33.580 --> 00:51:34.820]   and docs are gonna read it,
[00:51:34.820 --> 00:51:36.780]   take it as evidence-based medicine.
[00:51:36.780 --> 00:51:41.000]   The economists call what then happens as stickiness,
[00:51:41.000 --> 00:51:43.340]   that the docs hold on to their beliefs.
[00:51:43.340 --> 00:51:47.300]   And my own voice inside says,
[00:51:47.300 --> 00:51:52.000]   once doctors start doing things to their patients' bodies,
[00:51:52.000 --> 00:51:53.660]   they're really not too enthusiastic
[00:51:53.660 --> 00:51:55.420]   about hearing it was wrong.
[00:51:55.420 --> 00:51:57.860]   - Yeah, that's the stickiness of human nature.
[00:51:57.860 --> 00:52:00.900]   Wow, so that bar, once it's published,
[00:52:00.900 --> 00:52:04.780]   the doctors, that's when the stickiness emerges.
[00:52:04.780 --> 00:52:05.780]   Wow, yeah. - Yeah, yeah.
[00:52:05.780 --> 00:52:08.260]   It's hard to put that toothpaste back in the tube.
[00:52:08.260 --> 00:52:11.540]   Now, that's pre-publication transparency,
[00:52:11.540 --> 00:52:13.020]   which is essential.
[00:52:13.020 --> 00:52:17.480]   And you could have, whoever saw that data pre-publication
[00:52:17.480 --> 00:52:20.000]   could sign confidentiality agreements
[00:52:20.000 --> 00:52:22.520]   so that the drug companies couldn't argue
[00:52:22.520 --> 00:52:24.700]   that we're just opening the spigots of our data
[00:52:24.700 --> 00:52:28.660]   and people can copy it and blah, all the excuses they make.
[00:52:28.660 --> 00:52:30.540]   You could argue that you didn't have to,
[00:52:30.540 --> 00:52:32.420]   but let's just let them do it.
[00:52:32.420 --> 00:52:35.180]   Let the peer reviewers sign confidentiality agreements
[00:52:35.180 --> 00:52:36.780]   and they won't leak the data.
[00:52:36.780 --> 00:52:39.880]   But then you have to go to post-publication transparency,
[00:52:39.880 --> 00:52:41.740]   which is what you were just getting at,
[00:52:41.740 --> 00:52:46.740]   to let the data free and let citizens
[00:52:46.740 --> 00:52:50.540]   and citizen scientists and other doctors
[00:52:50.540 --> 00:52:52.680]   who are interested have at it.
[00:52:53.660 --> 00:52:56.640]   Kind of like Wikipedia, have at it.
[00:52:56.640 --> 00:53:00.120]   Let it out and let people criticize each other.
[00:53:00.120 --> 00:53:03.140]   - Okay, so speaking of the data,
[00:53:03.140 --> 00:53:08.140]   the FDA asked 55 years to release Pfizer vaccine data.
[00:53:08.140 --> 00:53:12.220]   This is also something I raised with Albert Bourla.
[00:53:12.220 --> 00:53:13.900]   - What did he say?
[00:53:13.900 --> 00:53:16.860]   - There's several things I didn't like about what he said.
[00:53:16.860 --> 00:53:18.360]   So some things are expected
[00:53:18.360 --> 00:53:21.120]   and some of it is just revealing the human being,
[00:53:21.120 --> 00:53:23.020]   which is what I'm interested in doing.
[00:53:23.780 --> 00:53:28.100]   But he said he wasn't aware of the 75 and the 55.
[00:53:28.100 --> 00:53:29.860]   - I'm sorry, wait a minute.
[00:53:29.860 --> 00:53:31.100]   He wasn't aware of?
[00:53:31.100 --> 00:53:34.180]   - The how long, so here, I'll explain what he, okay.
[00:53:34.180 --> 00:53:37.380]   - Do you know that since you spoke to him,
[00:53:37.380 --> 00:53:42.140]   Pfizer has petitioned the judge to join the suit
[00:53:42.140 --> 00:53:47.140]   in behalf of the FDA's request to release that data
[00:53:47.140 --> 00:53:50.060]   over 55 or 75 years?
[00:53:50.060 --> 00:53:52.240]   Pfizer's fully aware of what's going on.
[00:53:52.240 --> 00:53:56.300]   He's aware, I'm sure he's aware in some formulation,
[00:53:56.300 --> 00:53:59.100]   the exact years he might have not been aware,
[00:53:59.100 --> 00:54:01.240]   but the point is that there is,
[00:54:01.240 --> 00:54:06.640]   that is the FDA, the relationship of Pfizer and the FDA,
[00:54:06.640 --> 00:54:11.040]   in terms of me being able to read human beings,
[00:54:11.040 --> 00:54:14.380]   was the thing he was most uncomfortable with,
[00:54:14.380 --> 00:54:17.580]   that he didn't wanna talk about the FDA.
[00:54:17.580 --> 00:54:20.120]   And that, it was clear
[00:54:20.120 --> 00:54:22.340]   that there was a relationship there
[00:54:22.340 --> 00:54:26.460]   that if the words you use may do a lot of harm,
[00:54:26.460 --> 00:54:28.560]   potentially because like you're saying,
[00:54:28.560 --> 00:54:30.140]   there might be lawsuits going on,
[00:54:30.140 --> 00:54:32.380]   there's litigation, there's legal stuff,
[00:54:32.380 --> 00:54:33.500]   all that kind of stuff.
[00:54:33.500 --> 00:54:36.640]   And then there's a lot of games being played in this space.
[00:54:36.640 --> 00:54:40.080]   So I don't know how to interpret it,
[00:54:40.080 --> 00:54:41.600]   if he's actually aware or not,
[00:54:41.600 --> 00:54:46.600]   but the deeper truth is that he's deeply uncomfortable
[00:54:49.660 --> 00:54:52.280]   bringing light to this part of the game.
[00:54:52.280 --> 00:54:56.020]   - Yes, and I'm gonna read between the lines
[00:54:56.020 --> 00:54:59.980]   and Albert Bourla certainly didn't ask me to speak for him,
[00:54:59.980 --> 00:55:02.500]   but I think, but when did you speak to him?
[00:55:02.500 --> 00:55:03.460]   How long ago?
[00:55:03.460 --> 00:55:05.820]   - Wow, time flies when you're having fun.
[00:55:05.820 --> 00:55:06.660]   Two months ago.
[00:55:06.660 --> 00:55:07.480]   - Two months ago.
[00:55:07.480 --> 00:55:12.060]   So that was just recently, it's come out,
[00:55:12.060 --> 00:55:14.520]   just in the past week, it's come out,
[00:55:14.520 --> 00:55:18.940]   that Pfizer isn't battling the FDA.
[00:55:18.940 --> 00:55:23.940]   Pfizer has joined the FDA in the opposition to the request
[00:55:23.940 --> 00:55:29.900]   to release these documents in the same amount of time
[00:55:29.900 --> 00:55:33.180]   that the FDA took to evaluate them.
[00:55:33.180 --> 00:55:34.060]   - Yeah.
[00:55:34.060 --> 00:55:39.060]   - So Pfizer is offering to help the FDA
[00:55:39.060 --> 00:55:48.740]   to petition the judge to not release these documents
[00:55:48.740 --> 00:55:51.540]   to not enforce the timeline
[00:55:51.540 --> 00:55:54.100]   that he seems to be moving towards.
[00:55:54.100 --> 00:55:55.620]   - So for people who are not familiar,
[00:55:55.620 --> 00:55:59.140]   we're talking about the Freedom of Information Act request
[00:55:59.140 --> 00:56:04.140]   to release the Pfizer vaccine data, study data,
[00:56:04.140 --> 00:56:07.180]   to release as much of the data as possible,
[00:56:07.180 --> 00:56:08.940]   like the raw data, the details,
[00:56:08.940 --> 00:56:12.460]   or actually not even the raw data, it's data.
[00:56:12.460 --> 00:56:14.620]   Doesn't matter, there's details to it.
[00:56:14.620 --> 00:56:17.980]   And I think the response from the FDA
[00:56:17.980 --> 00:56:21.440]   is that, yes, of course,
[00:56:21.440 --> 00:56:28.780]   but we can only publish some X number of pages a day.
[00:56:28.780 --> 00:56:31.020]   - 500 pages.
[00:56:31.020 --> 00:56:32.700]   - 500 pages of data.
[00:56:32.700 --> 00:56:33.980]   - It's not a day, though.
[00:56:33.980 --> 00:56:34.820]   It's a--
[00:56:34.820 --> 00:56:35.640]   - Whatever.
[00:56:35.640 --> 00:56:36.480]   - A week, I think.
[00:56:36.480 --> 00:56:38.580]   - The point is, whatever they're able to publish
[00:56:38.580 --> 00:56:39.420]   is ridiculous.
[00:56:39.420 --> 00:56:44.420]   It's like, my printer can only print three pages a day
[00:56:45.500 --> 00:56:47.980]   and we cannot afford a second printer.
[00:56:47.980 --> 00:56:52.300]   So it's some kind of bureaucratic language for,
[00:56:52.300 --> 00:56:53.960]   there's a process to this.
[00:56:53.960 --> 00:56:58.060]   And now you're saying that Pfizer is obviously
[00:56:58.060 --> 00:57:03.520]   more engaged in helping this kind of bureaucratic process
[00:57:03.520 --> 00:57:08.520]   prosper in its full absurdity, Kafka-esque absurdity.
[00:57:08.520 --> 00:57:11.860]   So what is this?
[00:57:11.860 --> 00:57:13.780]   This really bothered people.
[00:57:13.780 --> 00:57:14.620]   This really--
[00:57:14.620 --> 00:57:15.700]   - This is really troublesome.
[00:57:15.700 --> 00:57:19.660]   And just to put it in just plain English terms,
[00:57:19.660 --> 00:57:23.500]   Pfizer's making the case that it can't,
[00:57:23.500 --> 00:57:27.420]   the FDA and Pfizer together are making the case
[00:57:27.420 --> 00:57:29.780]   that they can't go through the documents.
[00:57:29.780 --> 00:57:33.660]   It's gonna take them some number of hundredfold,
[00:57:33.660 --> 00:57:37.100]   hundreds of folds more time to go through the documents
[00:57:37.100 --> 00:57:39.820]   than the FDA required to go through the documents
[00:57:39.820 --> 00:57:42.260]   to approve the vaccines,
[00:57:42.260 --> 00:57:44.900]   to give the vaccines full FDA approval.
[00:57:44.900 --> 00:57:48.900]   And the FDA's argument, talk about Kafka-esque,
[00:57:48.900 --> 00:57:51.300]   is that to do it more rapidly
[00:57:51.300 --> 00:57:53.020]   would cost them $3 million.
[00:57:53.020 --> 00:57:59.820]   $3 million equals one hour of vaccine sales over two years.
[00:57:59.820 --> 00:58:04.300]   One hour of sales.
[00:58:04.300 --> 00:58:05.860]   And they can't come up with the money.
[00:58:05.860 --> 00:58:07.980]   And now Pfizer has joined the suit
[00:58:07.980 --> 00:58:10.220]   to help the FDA fight off this judge,
[00:58:10.220 --> 00:58:12.860]   this mean judge who thinks they ought to release the data.
[00:58:12.860 --> 00:58:14.980]   But evidently Pfizer isn't offering
[00:58:14.980 --> 00:58:17.260]   to come up with the $3 million either.
[00:58:17.260 --> 00:58:19.140]   So bought for $3 million, I mean,
[00:58:19.140 --> 00:58:24.140]   maybe the FDA should do a GoFundMe campaign.
[00:58:24.140 --> 00:58:27.580]   - Well, obviously the money thing,
[00:58:27.580 --> 00:58:31.300]   I'm sure if Elon Musk comes along and says,
[00:58:31.300 --> 00:58:35.460]   "I'll give you 100 million, publish it now,"
[00:58:35.460 --> 00:58:37.860]   I think they'll come up with another.
[00:58:37.860 --> 00:58:41.700]   So, I mean, it's clear that there's cautiousness.
[00:58:41.700 --> 00:58:47.100]   I don't know the source of it from the FDA.
[00:58:47.100 --> 00:58:49.660]   - There's only one explanation that I can think of,
[00:58:49.660 --> 00:58:53.020]   which is that the FDA and Pfizer
[00:58:53.020 --> 00:58:54.560]   don't wanna release the data.
[00:58:54.560 --> 00:58:59.380]   They don't wanna release the three or 500,000 pages
[00:58:59.380 --> 00:59:02.040]   of documents.
[00:59:02.040 --> 00:59:05.380]   And I don't know what's in there.
[00:59:05.380 --> 00:59:08.140]   I wanna say one thing very clearly.
[00:59:08.140 --> 00:59:10.120]   I am not an anti-vaxxer.
[00:59:10.120 --> 00:59:11.940]   I believe the vaccines work.
[00:59:11.940 --> 00:59:15.220]   I believe everybody should get vaccinated.
[00:59:15.220 --> 00:59:17.620]   The evidence is clear that if you're vaccinated,
[00:59:17.620 --> 00:59:20.900]   you reduce your risk of dying of COVID by 20 fold.
[00:59:20.900 --> 00:59:23.460]   And we've got new sub-variants coming along.
[00:59:23.460 --> 00:59:26.620]   And I just wanna be very clear about this.
[00:59:26.620 --> 00:59:31.580]   That said, there's something I would give you
[00:59:31.580 --> 00:59:35.100]   10 to one odds on a bet that there's something in that data
[00:59:35.100 --> 00:59:40.100]   that is gonna be embarrassing to either FDA or Pfizer
[00:59:40.100 --> 00:59:41.300]   or both.
[00:59:41.300 --> 00:59:42.120]   - So, there's two options.
[00:59:42.120 --> 00:59:43.740]   I agree with you 100%.
[00:59:43.740 --> 00:59:46.720]   One is they know of embarrassing things.
[00:59:46.720 --> 00:59:48.200]   That's option one.
[00:59:48.200 --> 00:59:51.780]   And option two, they haven't invested enough
[00:59:51.780 --> 00:59:54.620]   to truly understand the data.
[00:59:54.620 --> 00:59:56.460]   I mean, it's a lot of data.
[00:59:56.460 --> 00:59:59.140]   That they have a sense there might be something
[00:59:59.140 --> 01:00:00.020]   embarrassing in there.
[01:00:00.020 --> 01:00:02.100]   And if we release it,
[01:00:02.100 --> 01:00:04.420]   surely the world will discover the embarrassing.
[01:00:04.420 --> 01:00:08.880]   And to do a sort of, to steel man their argument,
[01:00:08.880 --> 01:00:11.700]   they'll take the small, the press,
[01:00:11.700 --> 01:00:14.420]   the people will take the small embarrassing things
[01:00:14.420 --> 01:00:16.420]   and blow them up into big things.
[01:00:16.420 --> 01:00:20.260]   - Yes, and support the anti-vax campaign.
[01:00:20.260 --> 01:00:22.700]   I think that's all possible.
[01:00:22.700 --> 01:00:27.060]   Nonetheless, the data are about the original clinical trial.
[01:00:27.060 --> 01:00:32.180]   And the emergency use authorization
[01:00:32.180 --> 01:00:34.700]   was based on the first few months
[01:00:34.700 --> 01:00:36.220]   of the data from that trial.
[01:00:36.220 --> 01:00:37.820]   And it was a two year trial.
[01:00:37.820 --> 01:00:40.180]   The rest of that data has not been opened up.
[01:00:40.180 --> 01:00:43.500]   And there was not an advisory committee meeting
[01:00:43.500 --> 01:00:44.980]   to look at that data
[01:00:44.980 --> 01:00:47.400]   when the FDA granted full authorization.
[01:00:47.400 --> 01:00:49.260]   Again, I am pro-vaccine.
[01:00:49.260 --> 01:00:52.580]   I am not making an anti-vax argument here.
[01:00:52.580 --> 01:00:56.100]   But I suspect that there's something pretty serious
[01:00:56.100 --> 01:00:57.420]   in that data.
[01:00:57.420 --> 01:01:00.060]   And the reason why I'm not an anti-vaxxer,
[01:01:01.000 --> 01:01:03.400]   having not been able to see the data
[01:01:03.400 --> 01:01:06.060]   that the FDA and Pfizer seem to willing,
[01:01:06.060 --> 01:01:09.980]   not just to put effort into preventing the release of,
[01:01:09.980 --> 01:01:13.100]   but seem to have quite a bit of energy into preventing,
[01:01:13.100 --> 01:01:16.500]   invest quite a bit of energy in not releasing that data.
[01:01:16.500 --> 01:01:18.400]   The reason why that doesn't tip me over
[01:01:18.400 --> 01:01:20.220]   into the anti-vaxxer side
[01:01:20.220 --> 01:01:22.400]   is because that's clinical trial data,
[01:01:22.400 --> 01:01:23.680]   early clinical trial data
[01:01:23.680 --> 01:01:25.800]   that involved several thousand people.
[01:01:25.800 --> 01:01:28.920]   We now have millions of data points
[01:01:28.920 --> 01:01:31.080]   from people who have had the vaccine.
[01:01:31.080 --> 01:01:32.960]   This is real world data
[01:01:32.960 --> 01:01:35.760]   showing the efficacy of the vaccines.
[01:01:35.760 --> 01:01:38.120]   And so far, knock on wood,
[01:01:38.120 --> 01:01:41.200]   there aren't side effects
[01:01:41.200 --> 01:01:45.120]   that overcome the benefits of vaccine.
[01:01:45.120 --> 01:01:46.480]   So I'm with you.
[01:01:46.480 --> 01:01:51.360]   I'm now, I guess, three shots of the vaccine.
[01:01:51.360 --> 01:01:55.760]   But there's a lot of people that are kind of saying,
[01:01:55.760 --> 01:01:58.960]   well, even the data on the real world use,
[01:01:58.960 --> 01:02:00.360]   large scale data,
[01:02:00.360 --> 01:02:05.640]   is messy.
[01:02:05.640 --> 01:02:06.800]   The way it's being reported,
[01:02:06.800 --> 01:02:08.760]   the way it's being interpreted.
[01:02:08.760 --> 01:02:11.520]   Well, one thing is clear to me
[01:02:11.520 --> 01:02:13.840]   that it is being politicized.
[01:02:13.840 --> 01:02:16.120]   I mean, if you just look objectively,
[01:02:16.120 --> 01:02:21.740]   don't have to go to, at the shallow surface level,
[01:02:21.740 --> 01:02:24.120]   it seems like there's two groups
[01:02:25.180 --> 01:02:29.000]   that, I can't even put a term to it
[01:02:29.000 --> 01:02:32.160]   because it's not really pro-vaccine versus anti-vaccine
[01:02:32.160 --> 01:02:37.160]   'cause it's pro-vaccine, triple mask,
[01:02:37.160 --> 01:02:41.160]   Democrat, liberal,
[01:02:41.160 --> 01:02:44.680]   and then anti-mandate, whatever those groups are.
[01:02:44.680 --> 01:02:46.520]   I can't quite, 'cause they're changing.
[01:02:46.520 --> 01:02:50.360]   Anti-mask, but not really, but kind of.
[01:02:50.360 --> 01:02:53.240]   So those two groups that feel political in nature,
[01:02:53.240 --> 01:02:54.880]   not scientific in nature,
[01:02:54.880 --> 01:02:56.840]   it's, they're bickering,
[01:02:56.840 --> 01:03:01.200]   and then it's clear that this data is being interpreted
[01:03:01.200 --> 01:03:03.120]   by the different groups differently.
[01:03:03.120 --> 01:03:07.440]   It's very difficult for me as a human being
[01:03:07.440 --> 01:03:11.160]   to understand where the truth lies,
[01:03:11.160 --> 01:03:14.040]   especially given how much money's flying around
[01:03:14.040 --> 01:03:15.360]   on all sides.
[01:03:15.360 --> 01:03:19.360]   So the anti-vaxxers can make a lot of money too.
[01:03:19.360 --> 01:03:20.200]   Let's not forget this.
[01:03:20.200 --> 01:03:22.500]   From the individual perspective,
[01:03:22.500 --> 01:03:25.360]   you can become famous being an anti-vaxxer.
[01:03:25.360 --> 01:03:28.060]   And so there's a lot of incentives on all sides here.
[01:03:28.060 --> 01:03:33.320]   And there's real human emotion and fear
[01:03:33.320 --> 01:03:35.260]   and also credibility.
[01:03:35.260 --> 01:03:41.120]   Scientists don't wanna ruin their reputation
[01:03:41.120 --> 01:03:43.120]   if they speak out in whatever,
[01:03:43.120 --> 01:03:45.960]   like speak their opinion or
[01:03:45.960 --> 01:03:49.560]   they look at some slice of the data
[01:03:49.560 --> 01:03:51.320]   and begin to interpret it in some kind of way.
[01:03:51.320 --> 01:03:53.100]   They're very, it's clear that fear
[01:03:53.100 --> 01:03:54.960]   is dominating the discourse here,
[01:03:54.960 --> 01:03:57.020]   especially in the scientific community.
[01:03:57.020 --> 01:03:59.340]   So I don't know what to make of that.
[01:03:59.340 --> 01:04:05.640]   And the only happy people here is Pfizer.
[01:04:05.640 --> 01:04:08.660]   It's just plowing all ahead.
[01:04:08.660 --> 01:04:13.240]   I mean, with every single variant,
[01:04:13.240 --> 01:04:17.760]   there's very, I would say,
[01:04:17.760 --> 01:04:21.900]   outside of arguably a very flawed system,
[01:04:21.900 --> 01:04:23.960]   there's a lot of incredible scientific
[01:04:23.960 --> 01:04:25.760]   and engineering work being done
[01:04:25.760 --> 01:04:29.160]   in constantly developing new antiviral drugs,
[01:04:29.160 --> 01:04:33.360]   new vaccines to deal with the variants.
[01:04:33.360 --> 01:04:37.520]   So they're happily being a capitalist machine.
[01:04:37.520 --> 01:04:42.520]   And it's very difficult to know what to do with that.
[01:04:42.520 --> 01:04:46.580]   - And let's just put this in perspective for folks.
[01:04:46.580 --> 01:04:48.480]   The best-selling drug in the world
[01:04:48.480 --> 01:04:51.400]   has been Humira for a number of years.
[01:04:51.400 --> 01:04:55.520]   It's approved for the treatment of rheumatoid arthritis
[01:04:55.520 --> 01:04:57.760]   and eight other indications.
[01:04:57.760 --> 01:05:02.160]   And it's sold about $20 billion globally
[01:05:02.160 --> 01:05:03.840]   over the past few years.
[01:05:03.840 --> 01:05:07.160]   It leveled out, it peaked at that level.
[01:05:07.160 --> 01:05:12.160]   Pfizer expects to sell $65 billion of vaccine
[01:05:12.160 --> 01:05:16.280]   in the first two years of the pandemic.
[01:05:16.280 --> 01:05:19.960]   So this is by far the biggest selling
[01:05:19.960 --> 01:05:22.700]   and most profitable drug that's ever come along.
[01:05:22.700 --> 01:05:26.980]   - Can I ask you a difficult question here?
[01:05:26.980 --> 01:05:31.520]   In the fog that we're operating in here,
[01:05:31.520 --> 01:05:38.660]   on the Pfizer-BioNTech vaccine,
[01:05:38.660 --> 01:05:43.480]   what was done well and what was done badly
[01:05:43.480 --> 01:05:45.840]   that you can see now?
[01:05:45.840 --> 01:05:50.080]   It seems like we'll know more decades from now.
[01:05:50.080 --> 01:05:51.360]   - Yes.
[01:05:51.360 --> 01:05:53.620]   - But now in the fog of today,
[01:05:53.620 --> 01:05:58.880]   with the $65 billion flying around,
[01:05:58.880 --> 01:06:03.040]   where do you land?
[01:06:03.040 --> 01:06:07.000]   - So we're gonna get to what I think
[01:06:07.000 --> 01:06:08.480]   is one of the key problems
[01:06:08.480 --> 01:06:10.960]   with the pharmaceutical industry model
[01:06:10.960 --> 01:06:15.040]   in the United States about being profit-driven.
[01:06:16.040 --> 01:06:21.040]   So in 2016, the NIH did the key infrastructure work
[01:06:21.040 --> 01:06:25.220]   to make mRNA vaccines.
[01:06:25.220 --> 01:06:29.320]   That gets left out of the discussion a lot.
[01:06:29.320 --> 01:06:32.600]   And Pfizer-BioNTech actually paid royalties
[01:06:32.600 --> 01:06:36.000]   voluntarily to the NIH.
[01:06:36.000 --> 01:06:36.880]   I don't know how much it was.
[01:06:36.880 --> 01:06:38.520]   I don't think it was a whole lot of money,
[01:06:38.520 --> 01:06:41.200]   but I think they wanted to avoid the litigation
[01:06:41.200 --> 01:06:43.200]   that Moderna got itself into
[01:06:43.200 --> 01:06:46.840]   by just taking that 2016 knowledge
[01:06:46.840 --> 01:06:50.120]   and having that be the foundation of their product.
[01:06:50.120 --> 01:06:54.640]   So Pfizer took that and they did their R&D.
[01:06:54.640 --> 01:06:59.160]   They paid for their R&D, having received that technology.
[01:06:59.160 --> 01:07:03.800]   And when they got the genetic code from China
[01:07:03.800 --> 01:07:08.800]   about the virus, they very quickly made a vaccine
[01:07:08.800 --> 01:07:10.920]   and the vaccine works.
[01:07:10.920 --> 01:07:13.840]   And President Trump, to his credit,
[01:07:13.840 --> 01:07:16.080]   launched Operation Warp Speed
[01:07:16.080 --> 01:07:18.160]   and just threw money at the problem.
[01:07:18.160 --> 01:07:22.360]   They just said, "We spent five times more per person
[01:07:22.360 --> 01:07:24.140]   "than the EU early on.
[01:07:24.140 --> 01:07:26.800]   "Just pay them whatever they want.
[01:07:26.800 --> 01:07:28.400]   "Let's just get this going."
[01:07:28.400 --> 01:07:32.400]   And Americans were vaccinated more quickly.
[01:07:32.400 --> 01:07:34.200]   We paid a lot of money.
[01:07:34.200 --> 01:07:37.160]   The one mistake that I think the federal government made
[01:07:37.160 --> 01:07:40.640]   was they were paying these guaranteed fortunes
[01:07:40.640 --> 01:07:45.360]   and they didn't require that the companies participate
[01:07:45.360 --> 01:07:49.080]   in a program to do global vaccinations.
[01:07:49.080 --> 01:07:53.520]   So the companies, doing their business model,
[01:07:53.520 --> 01:07:55.520]   distributed the vaccines
[01:07:55.520 --> 01:07:57.280]   where they would make the most money.
[01:07:57.280 --> 01:07:59.100]   And obviously, they would make the most money
[01:07:59.100 --> 01:08:00.040]   in the first world.
[01:08:00.040 --> 01:08:04.360]   And almost, I think, 85% of the vaccines early on
[01:08:04.360 --> 01:08:05.960]   went to the first world.
[01:08:05.960 --> 01:08:10.560]   And very, very few vaccinations went to the third world.
[01:08:10.560 --> 01:08:15.560]   So what happened is there was such a low vaccination rate.
[01:08:15.560 --> 01:08:21.480]   In May of 2021, there was an all-hands-on-deck cry for help
[01:08:21.480 --> 01:08:25.180]   from the World Trade Organization,
[01:08:25.180 --> 01:08:31.540]   the World Health Organization, the IMF, and the World Bank,
[01:08:31.540 --> 01:08:34.560]   made a plea for $50 billion
[01:08:35.520 --> 01:08:39.080]   so that we could get to 40% vaccination rate
[01:08:39.080 --> 01:08:42.320]   in the third world by the end of 2021.
[01:08:42.320 --> 01:08:46.560]   And it was unrequited.
[01:08:46.560 --> 01:08:47.520]   Nobody answered.
[01:08:47.520 --> 01:08:53.880]   And now Africa has about a 8.9% vaccination rate.
[01:08:53.880 --> 01:08:57.120]   India's coming up, but it's been very low.
[01:08:57.120 --> 01:09:00.040]   The problem with all this is,
[01:09:00.040 --> 01:09:03.840]   I believe those mRNA vaccines are excellent vaccines.
[01:09:04.720 --> 01:09:07.960]   But if we leave the third world unvaccinated,
[01:09:07.960 --> 01:09:12.780]   we're gonna have a constant supply of variants of COVID
[01:09:12.780 --> 01:09:15.800]   that are gonna come back into the United States
[01:09:15.800 --> 01:09:20.760]   and harm Americans exactly like Delta and Omicron have.
[01:09:20.760 --> 01:09:22.980]   So we've made a great drug.
[01:09:22.980 --> 01:09:25.720]   It reduces the risk of mortality
[01:09:25.720 --> 01:09:28.400]   in Americans who get it by a lot.
[01:09:28.400 --> 01:09:31.060]   But we're not doing what we need to do
[01:09:31.060 --> 01:09:33.300]   to protect Americans from Omicron.
[01:09:33.300 --> 01:09:34.760]   You don't have to be an idealist
[01:09:34.760 --> 01:09:36.960]   and worry about global vaccine equity.
[01:09:36.960 --> 01:09:41.360]   If you're just ordinary selfish people like most of us are,
[01:09:41.360 --> 01:09:43.620]   and you're worried about the health of Americans,
[01:09:43.620 --> 01:09:47.280]   you would ensure global vaccine distribution.
[01:09:47.280 --> 01:09:49.120]   Let me just make one more point.
[01:09:49.120 --> 01:09:51.760]   That $50 billion that was requested
[01:09:51.760 --> 01:09:55.280]   by the four organizations back in May of 2021,
[01:09:55.280 --> 01:09:59.320]   32 billionaires made $50 billion
[01:09:59.320 --> 01:10:01.440]   from the vaccines at that point,
[01:10:01.440 --> 01:10:03.960]   took it into their private wealth.
[01:10:03.960 --> 01:10:05.060]   So what had been taken,
[01:10:05.060 --> 01:10:06.260]   this enormous amounts of money
[01:10:06.260 --> 01:10:08.380]   that had been taken into private wealth
[01:10:08.380 --> 01:10:11.620]   was enough to do what those organizations said
[01:10:11.620 --> 01:10:14.780]   needed to be done to prevent the sub-variants
[01:10:14.780 --> 01:10:16.660]   from coming back and doing what they're doing.
[01:10:16.660 --> 01:10:19.100]   - So the money was there, but how does the motivation,
[01:10:19.100 --> 01:10:22.580]   the money-driven motivation of big pharma lead to that,
[01:10:22.580 --> 01:10:28.500]   that kind of allocation of vaccines?
[01:10:28.500 --> 01:10:29.860]   - Because they can make-
[01:10:29.860 --> 01:10:31.380]   - More money in the United States.
[01:10:31.380 --> 01:10:33.100]   - Yeah, they're gonna distribute their vaccines
[01:10:33.100 --> 01:10:34.600]   where they can make the most money.
[01:10:34.600 --> 01:10:35.440]   - Right.
[01:10:35.440 --> 01:10:39.960]   Is there a malevolent aspect to this where,
[01:10:39.960 --> 01:10:44.560]   boy, I don't like saying this,
[01:10:44.560 --> 01:10:49.560]   but that they don't see it as a huge problem
[01:10:49.560 --> 01:10:53.200]   that variants will come back to the United States?
[01:10:53.200 --> 01:10:56.760]   - I think it's the issue we were talking about earlier on,
[01:10:56.760 --> 01:10:58.560]   where they're in a different culture
[01:10:58.560 --> 01:11:02.500]   and their culture is that their moral obligation,
[01:11:02.500 --> 01:11:04.600]   as Milton Friedman would say,
[01:11:04.600 --> 01:11:06.280]   is to maximize the profits
[01:11:06.280 --> 01:11:07.800]   that they return to shareholders.
[01:11:07.800 --> 01:11:10.600]   - And don't think about the bigger picture.
[01:11:10.600 --> 01:11:11.560]   - The collateral damage,
[01:11:11.560 --> 01:11:12.760]   don't think about the collateral damage.
[01:11:12.760 --> 01:11:16.800]   - And also kind of believe, convince yourself
[01:11:16.800 --> 01:11:20.160]   that if we give into this capitalist machine
[01:11:20.160 --> 01:11:23.160]   in this very narrow sense of capitalism,
[01:11:23.160 --> 01:11:25.920]   that in the end, they'll do the most good.
[01:11:25.920 --> 01:11:30.480]   This kind of belief that if we just maximize profits,
[01:11:30.480 --> 01:11:32.640]   we'll do the most good.
[01:11:32.640 --> 01:11:36.800]   - Yeah, that's an orthodoxy of several decades ago,
[01:11:36.800 --> 01:11:40.200]   and I don't think people can really say that in good faith.
[01:11:40.200 --> 01:11:43.720]   When you're talking about vaccinating the third world
[01:11:43.720 --> 01:11:44.920]   so we don't get hurt,
[01:11:44.920 --> 01:11:47.280]   it's a little bit hard to make the argument
[01:11:47.280 --> 01:11:48.500]   that the world's a better place
[01:11:48.500 --> 01:11:51.080]   because the profits of the investors went up.
[01:11:51.080 --> 01:11:52.520]   - Yeah, but at the same time,
[01:11:54.800 --> 01:11:58.080]   I think that's a belief you can hold.
[01:11:58.080 --> 01:12:00.220]   I mean, I've interacted with a bunch of folks
[01:12:00.220 --> 01:12:02.720]   that kind of, it's the,
[01:12:02.720 --> 01:12:05.520]   I don't wanna mischaracterize Ayn Rand, okay?
[01:12:05.520 --> 01:12:07.380]   I respect a lot of people,
[01:12:07.380 --> 01:12:10.120]   but there's a belief that can take hold.
[01:12:10.120 --> 01:12:13.960]   If I just focus on this particular maximization,
[01:12:13.960 --> 01:12:16.080]   it will do the most good for the world.
[01:12:16.080 --> 01:12:19.240]   The problem is when you choose what to maximize
[01:12:19.240 --> 01:12:20.740]   and you put blinders on,
[01:12:20.740 --> 01:12:24.760]   it's too easy to start making gigantic mistakes.
[01:12:24.760 --> 01:12:28.160]   That have a big negative impact on society.
[01:12:28.160 --> 01:12:30.720]   So it really matters what you're maximizing.
[01:12:30.720 --> 01:12:33.720]   - Right, and if we had a true democracy
[01:12:33.720 --> 01:12:35.300]   and everybody had one vote,
[01:12:35.300 --> 01:12:39.600]   everybody got decent information and had one vote,
[01:12:39.600 --> 01:12:44.080]   Ayn Rand's position would get some votes, but not many.
[01:12:44.080 --> 01:12:47.840]   And it would be way outvoted by the common people.
[01:12:47.840 --> 01:12:53.880]   - Let me ask you about this very difficult topic
[01:12:53.880 --> 01:12:58.880]   talking to Mark Zuckerberg of Metta,
[01:12:58.880 --> 01:13:03.040]   the topic of censorship.
[01:13:03.040 --> 01:13:04.780]   I don't know if you've heard,
[01:13:04.780 --> 01:13:08.960]   but there's a guy named Robert Malone and Peter McCullough
[01:13:08.960 --> 01:13:10.880]   that were removed from many platforms
[01:13:10.880 --> 01:13:14.200]   for speaking about the COVID vaccine as being risky.
[01:13:14.200 --> 01:13:16.800]   They were both on Joe Rogan's program.
[01:13:16.800 --> 01:13:22.460]   What do you think about censorship in this space?
[01:13:23.600 --> 01:13:28.600]   In this difficult space where so much is controlled by,
[01:13:28.600 --> 01:13:31.560]   not controlled, but influenced by advertisements
[01:13:31.560 --> 01:13:32.540]   from big pharma.
[01:13:32.540 --> 01:13:38.120]   And science can even be influenced by big pharma.
[01:13:38.120 --> 01:13:41.280]   Where do you lean on this?
[01:13:41.280 --> 01:13:46.280]   Should we allow, should we lean towards freedom
[01:13:46.280 --> 01:13:50.120]   and just allow all the voices,
[01:13:50.120 --> 01:13:54.560]   even those that go against the scientific consensus?
[01:13:54.560 --> 01:13:59.560]   Is that one way to fight the science
[01:13:59.560 --> 01:14:01.760]   that is funded by big pharma?
[01:14:01.760 --> 01:14:05.360]   Or is that do more harm than good,
[01:14:05.360 --> 01:14:08.480]   having too many voices that are contending here?
[01:14:08.480 --> 01:14:10.640]   Should the ultimate battle be fought
[01:14:10.640 --> 01:14:15.160]   in the space of scientific publications?
[01:14:15.160 --> 01:14:19.360]   - And particularly in the era of COVID,
[01:14:19.360 --> 01:14:22.600]   where there are large public health ramifications
[01:14:22.600 --> 01:14:27.440]   to this public discourse, the ante is way up.
[01:14:27.440 --> 01:14:30.020]   So I don't have a simple answer to that.
[01:14:30.020 --> 01:14:34.800]   I think everyone's allowed their own opinion.
[01:14:34.800 --> 01:14:38.680]   I don't think everyone's allowed their own scientific facts.
[01:14:38.680 --> 01:14:42.240]   And how we develop a mechanism
[01:14:42.240 --> 01:14:45.360]   that's other than an open internet
[01:14:45.360 --> 01:14:48.140]   where whoever is shouting the loudest
[01:14:48.140 --> 01:14:52.880]   gets the most clicks and the rage creates value
[01:14:52.880 --> 01:14:54.240]   on the internet.
[01:14:54.240 --> 01:14:58.200]   I think that's not a good mechanism for working this out.
[01:14:58.200 --> 01:14:59.760]   And I don't think we have one.
[01:14:59.760 --> 01:15:01.800]   I don't have a solution to this.
[01:15:01.800 --> 01:15:05.300]   I mean, ideally, if we had a philosopher king,
[01:15:05.300 --> 01:15:08.720]   we could have a panel of people
[01:15:08.720 --> 01:15:12.760]   who were not conflicted by rigid opinions
[01:15:13.880 --> 01:15:18.780]   decide on what the boundaries of public discourse might be.
[01:15:18.780 --> 01:15:21.740]   I don't think it should be fully open.
[01:15:21.740 --> 01:15:24.380]   I don't think people who are making,
[01:15:24.380 --> 01:15:28.300]   who are committed to an anti-vaccine position
[01:15:28.300 --> 01:15:31.000]   and will tailor their interpretation
[01:15:31.000 --> 01:15:34.900]   of complex scientific data to support their opinion,
[01:15:34.900 --> 01:15:36.780]   I think that can be harmful.
[01:15:36.780 --> 01:15:39.260]   Constraining their speech can be harmful as well.
[01:15:39.260 --> 01:15:42.180]   So I don't have an answer here, but yeah.
[01:15:42.180 --> 01:15:45.760]   - I tend to believe that it's more dangerous
[01:15:45.760 --> 01:15:49.320]   to censor anti-vax messages.
[01:15:49.320 --> 01:15:53.360]   The way to defeat anti-vax messages
[01:15:53.360 --> 01:15:56.440]   is by being great communicators,
[01:15:56.440 --> 01:15:58.320]   by being great scientific communicators.
[01:15:58.320 --> 01:16:03.320]   So it's not that we need to censor the things we don't like.
[01:16:03.320 --> 01:16:08.240]   We need to be better at communicating the things we do like
[01:16:08.240 --> 01:16:10.760]   or the things that we do believe represent
[01:16:10.760 --> 01:16:13.920]   the deep scientific truth.
[01:16:13.920 --> 01:16:18.400]   Because I think if you censor,
[01:16:18.400 --> 01:16:20.600]   you get worse at doing science
[01:16:20.600 --> 01:16:24.860]   and you give the wrong people power.
[01:16:24.860 --> 01:16:30.980]   So I tend to believe that you should give power
[01:16:30.980 --> 01:16:33.400]   to the individual scientists
[01:16:33.400 --> 01:16:35.720]   and also give them the responsibility
[01:16:35.720 --> 01:16:38.880]   of being better educators, communicators,
[01:16:38.880 --> 01:16:41.680]   expressors of scientific ideas,
[01:16:41.680 --> 01:16:43.480]   put pressure on them to release data,
[01:16:43.480 --> 01:16:46.800]   to release that data in a way that's easily consumable,
[01:16:46.800 --> 01:16:49.200]   not just like very difficult to understand,
[01:16:49.200 --> 01:16:50.760]   but in a way that it can be understood
[01:16:50.760 --> 01:16:52.500]   by a large number of people.
[01:16:52.500 --> 01:16:57.160]   So the battle should be fought in the open space of ideas
[01:16:57.160 --> 01:17:02.160]   versus in the quiet space of journals.
[01:17:02.160 --> 01:17:05.960]   I think we no longer have that comfort,
[01:17:05.960 --> 01:17:08.280]   especially at the highest of stakes.
[01:17:08.280 --> 01:17:11.600]   So this kind of idea that a couple of peer reviewers
[01:17:11.600 --> 01:17:14.280]   decide the fate of billions
[01:17:14.280 --> 01:17:18.920]   doesn't seem to be sustainable,
[01:17:18.920 --> 01:17:23.720]   especially given a very real observation now
[01:17:23.720 --> 01:17:29.600]   that the reason Robert Malone has a large following
[01:17:29.600 --> 01:17:32.980]   is there's a deep distrust of institutions,
[01:17:32.980 --> 01:17:37.720]   deep distrust of scientists, of science as an institution,
[01:17:37.720 --> 01:17:41.360]   of power centers, of companies, of everything,
[01:17:41.360 --> 01:17:43.940]   and perhaps rightfully so.
[01:17:43.940 --> 01:17:45.500]   But the way to defend against that
[01:17:45.500 --> 01:17:49.740]   is not for the powerful to build a bigger wall,
[01:17:49.740 --> 01:17:51.720]   it's for the powerful to be authentic
[01:17:51.720 --> 01:17:55.760]   and maybe a lot of them to get fired
[01:17:55.760 --> 01:17:58.900]   and for new minds, for new fresh scientists,
[01:17:58.900 --> 01:18:01.800]   ones who are more authentic, more real,
[01:18:01.800 --> 01:18:03.960]   better communicators to step up.
[01:18:03.960 --> 01:18:06.480]   So I fear censorship
[01:18:06.480 --> 01:18:11.480]   because it feels like censorship is an even harder job
[01:18:11.480 --> 01:18:16.720]   to do it well than being good communicators.
[01:18:16.720 --> 01:18:19.200]   And it seems like it's always the C students
[01:18:19.200 --> 01:18:21.360]   that end up doing the censorship.
[01:18:21.360 --> 01:18:25.080]   That it's like, it's always the incompetent people
[01:18:25.080 --> 01:18:28.800]   and not just the incompetent, but the biggest whiners.
[01:18:28.800 --> 01:18:33.000]   So like what happens is the people
[01:18:33.000 --> 01:18:36.520]   that get the most emotional and the most outraged
[01:18:36.520 --> 01:18:38.220]   will drive the censorship.
[01:18:38.220 --> 01:18:42.560]   And it doesn't seem like reason drives the censorship.
[01:18:42.560 --> 01:18:44.840]   That's just objectively observing
[01:18:44.840 --> 01:18:47.960]   how censorship seems to work in this current.
[01:18:47.960 --> 01:18:50.560]   So there's so many forms of censorship.
[01:18:50.560 --> 01:18:51.960]   You know, you look at the Soviet Union
[01:18:51.960 --> 01:18:54.040]   with the propaganda or Nazi Germany,
[01:18:54.040 --> 01:18:55.760]   it's a very different level of censorship.
[01:18:55.760 --> 01:18:59.120]   People tend to conflate all of these things together.
[01:18:59.120 --> 01:19:01.640]   You know, social media trying desperately
[01:19:01.640 --> 01:19:05.920]   to have trillions or hundreds of billions
[01:19:05.920 --> 01:19:09.360]   of exchanges a day and like try to make sure
[01:19:09.360 --> 01:19:12.880]   that their platform has some semblance
[01:19:12.880 --> 01:19:16.360]   of like, quote, healthy conversations.
[01:19:16.360 --> 01:19:18.560]   Like people just don't go insane.
[01:19:18.560 --> 01:19:20.840]   They actually like using the platform
[01:19:20.840 --> 01:19:23.400]   and they censor based on that.
[01:19:23.400 --> 01:19:24.920]   That's a different level of censorship.
[01:19:24.920 --> 01:19:28.040]   But even there, you can really run afoul
[01:19:28.040 --> 01:19:32.440]   of the people that get, the whiny C students
[01:19:32.440 --> 01:19:34.880]   controlling too much of the censorship.
[01:19:34.880 --> 01:19:39.480]   I believe that you should actually put the responsibility
[01:19:39.480 --> 01:19:42.480]   on the self-proclaimed holders of truth,
[01:19:42.480 --> 01:19:45.620]   aka scientists, at being better communicators.
[01:19:45.620 --> 01:19:47.600]   - I agree with that.
[01:19:47.600 --> 01:19:51.440]   I'm not advocating for any kind of censorship,
[01:19:51.440 --> 01:19:55.600]   but Marshall McLuhan was very influential
[01:19:55.600 --> 01:19:57.200]   when I was in college.
[01:19:57.200 --> 01:20:02.200]   And his, that meme, the medium is the message.
[01:20:02.200 --> 01:20:04.840]   It's a little bit hard to understand
[01:20:04.840 --> 01:20:06.840]   when you're comparing radio to TV
[01:20:06.840 --> 01:20:09.920]   and saying radio is hotter or TV is hotter or something.
[01:20:09.920 --> 01:20:12.520]   But we now have the medium is the message
[01:20:12.520 --> 01:20:14.240]   in a way that we've never seen,
[01:20:14.240 --> 01:20:16.240]   we've never imagined before,
[01:20:16.240 --> 01:20:20.920]   where rage and anger and polarization
[01:20:22.820 --> 01:20:27.820]   are what drives the traffic on the internet.
[01:20:27.820 --> 01:20:33.220]   And we don't, it's a question of building the commons.
[01:20:33.220 --> 01:20:36.060]   Ideally, I don't know how to get there,
[01:20:36.060 --> 01:20:38.460]   so I'm not pretending to have a solution.
[01:20:38.460 --> 01:20:42.300]   But the commons of discourse about this particular issue
[01:20:42.300 --> 01:20:47.220]   about vaccines has been largely destroyed by the edges,
[01:20:47.220 --> 01:20:50.100]   by the drug companies and the advocates on the one side
[01:20:50.100 --> 01:20:54.740]   and the people who just criticize and think
[01:20:54.740 --> 01:20:57.780]   that even though the data are flawed,
[01:20:57.780 --> 01:21:01.060]   that there's no way vaccines can be beneficial.
[01:21:01.060 --> 01:21:04.140]   And to have those people screaming at each other
[01:21:04.140 --> 01:21:07.260]   does nothing to improve the health
[01:21:07.260 --> 01:21:10.740]   of the 95% of the people in the middle
[01:21:10.740 --> 01:21:15.740]   who want to know what the rational way to go forward is
[01:21:15.740 --> 01:21:18.620]   and protect their families from COVID
[01:21:18.620 --> 01:21:21.500]   and live a good life and be able to participate
[01:21:21.500 --> 01:21:22.620]   in the economy.
[01:21:22.620 --> 01:21:25.260]   And that's the problem.
[01:21:25.260 --> 01:21:26.460]   I don't have a solution.
[01:21:26.460 --> 01:21:29.580]   - Well, there's a difficult problem for Spotify and YouTube.
[01:21:29.580 --> 01:21:30.420]   I don't know if you heard,
[01:21:30.420 --> 01:21:33.420]   this is a thing that Joe Rogan is currently going through
[01:21:33.420 --> 01:21:36.740]   as a platform, whether to censor the conversation
[01:21:36.740 --> 01:21:39.180]   that, for example, Joe's having.
[01:21:39.180 --> 01:21:40.140]   So I don't know if you heard,
[01:21:40.140 --> 01:21:43.940]   but Neil Young and other musicians have kind of spoke out
[01:21:43.940 --> 01:21:45.820]   and saying they're going to leave the platform
[01:21:45.820 --> 01:21:49.780]   because Joe Rogan is allowed to be on this platform
[01:21:49.780 --> 01:21:51.380]   having these kinds of conversations
[01:21:51.380 --> 01:21:53.020]   with the likes of Robert Malone.
[01:21:53.020 --> 01:21:57.820]   And it's clear to me that Spotify and YouTube
[01:21:57.820 --> 01:21:59.860]   are being significantly influenced
[01:21:59.860 --> 01:22:03.420]   by these extreme voices, I can mention on each side.
[01:22:03.420 --> 01:22:05.820]   And it's also clear to me that Facebook is the same
[01:22:05.820 --> 01:22:07.700]   and that was going back and forth.
[01:22:07.700 --> 01:22:10.340]   In fact, that's why Facebook has been oscillating
[01:22:10.340 --> 01:22:12.660]   on the censorship is like one group gets louder
[01:22:12.660 --> 01:22:14.980]   than the other, depending on whether
[01:22:14.980 --> 01:22:16.260]   it's an election year.
[01:22:16.260 --> 01:22:21.220]   There's several things to say here.
[01:22:21.220 --> 01:22:24.620]   So one, it does seem, I think you put it really well,
[01:22:24.620 --> 01:22:27.620]   it would be amazing if these platforms could find mechanisms
[01:22:27.620 --> 01:22:32.060]   to listen to the center, to the big center
[01:22:32.060 --> 01:22:35.460]   that's actually going to be affected by the results
[01:22:35.460 --> 01:22:39.360]   of our pursuit of scientific truth, right?
[01:22:39.360 --> 01:22:42.140]   And listen to those voices.
[01:22:42.140 --> 01:22:45.820]   I also believe that most people are intelligent enough
[01:22:45.820 --> 01:22:49.380]   to process information and to make up their own minds.
[01:22:49.380 --> 01:22:51.720]   Like they're not in terms of,
[01:22:51.720 --> 01:22:55.260]   it's complicated, of course,
[01:22:55.260 --> 01:22:57.140]   'cause we've just been talking about advertisement
[01:22:57.140 --> 01:22:58.920]   and how people can be influenced.
[01:22:58.920 --> 01:23:03.920]   But I feel like if you have raw long form podcasts
[01:23:03.920 --> 01:23:08.460]   or programs where people express their mind
[01:23:08.460 --> 01:23:12.420]   and express their argument in full,
[01:23:12.420 --> 01:23:15.540]   I think people can hear it to make up their own mind.
[01:23:15.540 --> 01:23:17.780]   And if those arguments have a platform
[01:23:17.780 --> 01:23:20.020]   on which they can live, then other people
[01:23:20.020 --> 01:23:23.820]   could provide better arguments if they disagree with it.
[01:23:23.820 --> 01:23:26.780]   And now we as human beings, as rational,
[01:23:26.780 --> 01:23:29.140]   as intelligent human beings can look at both
[01:23:29.140 --> 01:23:30.580]   and make up our own minds.
[01:23:30.580 --> 01:23:33.100]   And that's where social media can be very good
[01:23:33.100 --> 01:23:35.940]   at like this collective intelligence.
[01:23:35.940 --> 01:23:39.180]   We together listen to all of these voices
[01:23:39.180 --> 01:23:40.660]   and make up our own mind.
[01:23:40.660 --> 01:23:42.860]   Humble ourselves actually often.
[01:23:42.860 --> 01:23:46.700]   You know, you think you know, like you're an expert,
[01:23:46.700 --> 01:23:48.620]   say you have a PhD in a certain thing,
[01:23:48.620 --> 01:23:50.940]   so there's this confidence that comes with that.
[01:23:50.940 --> 01:23:54.340]   And the collective intelligence, uncensored,
[01:23:54.340 --> 01:23:56.900]   allows you to humble yourself eventually.
[01:23:56.900 --> 01:24:01.180]   Like as you discovery, all it takes is a few times,
[01:24:01.180 --> 01:24:05.060]   you know, looking back five years later,
[01:24:05.060 --> 01:24:07.260]   realizing I was wrong.
[01:24:07.260 --> 01:24:09.060]   And that's really healthy for a scientist,
[01:24:09.060 --> 01:24:11.020]   that's really healthy for anybody to go through.
[01:24:11.020 --> 01:24:13.900]   And only through having that open discourse
[01:24:13.900 --> 01:24:15.940]   can you really have that.
[01:24:15.940 --> 01:24:20.940]   That said, Spotify also, just like Pfizer is a company,
[01:24:20.940 --> 01:24:26.740]   which is why this podcast,
[01:24:26.740 --> 01:24:29.200]   I don't know if you know what RSS feeds are,
[01:24:29.200 --> 01:24:31.520]   but podcasts can't be censored.
[01:24:31.520 --> 01:24:33.260]   So Joe's in the unfortunate position,
[01:24:33.260 --> 01:24:35.340]   he only lives on Spotify.
[01:24:35.340 --> 01:24:37.900]   So Spotify has been actually very good
[01:24:37.900 --> 01:24:40.660]   at saying we're staying out of it for now.
[01:24:40.660 --> 01:24:44.780]   But RSS, this is pirate radio.
[01:24:44.780 --> 01:24:47.040]   Nobody can censor, it's the internet.
[01:24:47.040 --> 01:24:51.860]   So financially, in terms of platforms,
[01:24:51.860 --> 01:24:53.620]   this cannot be censored,
[01:24:53.620 --> 01:24:56.740]   which is why podcasts are really beautiful.
[01:24:56.740 --> 01:25:00.180]   And so if Spotify or YouTube
[01:25:00.180 --> 01:25:04.240]   wants to be the host of podcasts,
[01:25:04.240 --> 01:25:09.240]   I think where they flourish is free expression,
[01:25:09.240 --> 01:25:12.860]   no matter how crazy.
[01:25:12.860 --> 01:25:16.540]   - Yes, but I do wanna push back a little bit
[01:25:16.540 --> 01:25:18.380]   on what you're saying.
[01:25:18.380 --> 01:25:23.080]   So I have anti-fax friends who I love.
[01:25:23.080 --> 01:25:26.120]   I mean, they're dear, cherished friends.
[01:25:26.120 --> 01:25:27.620]   And they'll send me stuff.
[01:25:28.660 --> 01:25:33.180]   And it'll take me an hour to go through what they sent
[01:25:33.180 --> 01:25:36.580]   to see if it is credible.
[01:25:36.580 --> 01:25:40.520]   And usually it's not.
[01:25:40.520 --> 01:25:42.820]   It's not a random sample of the anti-vax argument.
[01:25:42.820 --> 01:25:46.760]   I'm not saying I can disprove the anti-vax argument,
[01:25:46.760 --> 01:25:49.140]   but I am saying that it's almost like
[01:25:49.140 --> 01:25:54.140]   we were talking about how medical science, clinical trials,
[01:25:54.140 --> 01:25:56.740]   the presentation of clinical trials to physicians
[01:25:56.740 --> 01:25:57.940]   could be improved.
[01:25:57.940 --> 01:26:00.500]   And the first thing we came up with
[01:26:00.500 --> 01:26:04.380]   is to have pre-publication transparency
[01:26:04.380 --> 01:26:06.180]   in the peer review process.
[01:26:06.180 --> 01:26:08.660]   So bad information, biased information,
[01:26:08.660 --> 01:26:11.720]   doesn't get out as if it's legitimate
[01:26:11.720 --> 01:26:15.280]   and you can't put it back, recapture it once it gets out.
[01:26:15.280 --> 01:26:18.380]   I think there's an element of that
[01:26:18.380 --> 01:26:21.940]   in the arguments that are going on about vaccines.
[01:26:21.940 --> 01:26:23.180]   And they're on both sides,
[01:26:23.180 --> 01:26:25.780]   but I think the anti-vax side
[01:26:25.780 --> 01:26:30.180]   puts out more units of information
[01:26:30.180 --> 01:26:33.460]   claiming to show that the vaccines don't work.
[01:26:33.460 --> 01:26:36.500]   And I guess in an ideal situation,
[01:26:36.500 --> 01:26:41.100]   there would be real-time fact-checking by independent people,
[01:26:41.100 --> 01:26:43.780]   not to censor it, but to just say
[01:26:43.780 --> 01:26:45.860]   that study was set up to do this
[01:26:45.860 --> 01:26:47.940]   and this is what the conclusions were.
[01:26:47.940 --> 01:26:52.420]   So the way it was stated is on one side of this argument.
[01:26:52.420 --> 01:26:53.700]   - But that's what I'm arguing.
[01:26:53.700 --> 01:26:55.060]   I agree with you.
[01:26:55.060 --> 01:26:58.500]   What I'm arguing is that this big network of humans
[01:26:58.500 --> 01:27:00.940]   that we have that is the collective intelligence
[01:27:00.940 --> 01:27:03.980]   can do that real-time if you allow it to,
[01:27:03.980 --> 01:27:05.780]   if you encourage people to do it.
[01:27:05.780 --> 01:27:07.860]   And the scientists, as opposed to,
[01:27:07.860 --> 01:27:10.380]   listen, I interact with a lot of colleagues,
[01:27:10.380 --> 01:27:12.520]   a lot of friends that are scientists,
[01:27:12.520 --> 01:27:14.060]   they roll their eyes.
[01:27:14.060 --> 01:27:16.500]   Their response is like, ugh.
[01:27:16.500 --> 01:27:18.820]   Like they don't want to interact with this.
[01:27:18.820 --> 01:27:22.900]   But that's just not the right response.
[01:27:22.900 --> 01:27:26.420]   When a huge number of people believe this,
[01:27:26.420 --> 01:27:30.020]   it is your job as communicators to defend your ideas.
[01:27:30.020 --> 01:27:33.140]   It is no longer the case that you go to a conference
[01:27:33.140 --> 01:27:36.420]   and defend your ideas to two other nerds
[01:27:36.420 --> 01:27:38.580]   that have been working on the same problem forever.
[01:27:38.580 --> 01:27:40.300]   I mean, sure, you can do that,
[01:27:40.300 --> 01:27:44.060]   but then you're rejecting the responsibility
[01:27:44.060 --> 01:27:48.060]   you have explicitly or implicitly accepted
[01:27:48.060 --> 01:27:49.820]   when you go into this field,
[01:27:49.820 --> 01:27:52.620]   that you will defend the ideas of truth
[01:27:52.620 --> 01:27:54.180]   and the way to defend them
[01:27:54.180 --> 01:27:56.820]   is in the open battlefield of ideas
[01:27:56.820 --> 01:27:59.460]   and to become a better communicator.
[01:27:59.460 --> 01:28:01.020]   And I believe that when you have a large,
[01:28:01.020 --> 01:28:03.780]   you said you invested one or two hours in this particular,
[01:28:03.780 --> 01:28:07.820]   but that's little ants interacting at scale,
[01:28:07.820 --> 01:28:12.060]   I think that allows us to progress towards truth,
[01:28:12.060 --> 01:28:14.580]   at least, you know, at least I hope so.
[01:28:14.580 --> 01:28:15.900]   - I think you're an optimist.
[01:28:15.900 --> 01:28:18.060]   I want to work with you a little bit on this.
[01:28:18.060 --> 01:28:18.940]   (Lex laughing)
[01:28:18.940 --> 01:28:22.500]   Let's say a person like Joe Rogan,
[01:28:22.500 --> 01:28:26.100]   who by the way, had me on his podcast and let me-
[01:28:26.100 --> 01:28:27.260]   - It was an amazing conversation.
[01:28:27.260 --> 01:28:28.100]   I really enjoyed it.
[01:28:28.100 --> 01:28:29.060]   - Well, thank you.
[01:28:29.060 --> 01:28:30.020]   I did too.
[01:28:30.020 --> 01:28:31.540]   And I didn't know Joe.
[01:28:31.540 --> 01:28:32.900]   I didn't know much about his podcast.
[01:28:32.900 --> 01:28:34.940]   - He pushed back on Joe a bunch, which is great.
[01:28:34.940 --> 01:28:35.780]   (Lex laughing)
[01:28:35.780 --> 01:28:36.620]   - And he was- - I love it.
[01:28:36.620 --> 01:28:38.460]   - He was a gentleman and we had it out.
[01:28:38.460 --> 01:28:40.620]   In fact, he put one clip,
[01:28:40.620 --> 01:28:43.140]   at one point he said something that was a little bit wrong
[01:28:43.140 --> 01:28:44.300]   and I corrected him.
[01:28:44.300 --> 01:28:46.980]   And he had the guy who- - Jamie.
[01:28:46.980 --> 01:28:48.940]   - Jamie, he had Jamie check it
[01:28:48.940 --> 01:28:51.220]   and was very forthright in saying,
[01:28:51.220 --> 01:28:53.580]   yeah, you know, John got it right here.
[01:28:53.580 --> 01:28:54.860]   We got to modify this.
[01:28:54.860 --> 01:28:55.700]   In any event.
[01:28:55.700 --> 01:28:56.540]   (Lex laughing)
[01:28:56.540 --> 01:28:57.380]   In any event. - You got him.
[01:28:57.380 --> 01:28:58.220]   (Lex laughing)
[01:28:58.220 --> 01:29:00.020]   - Well, I wasn't trying to get him.
[01:29:00.020 --> 01:29:01.620]   I was just trying to- - No, no, no, no.
[01:29:01.620 --> 01:29:03.380]   Totally, it was a beautiful exchange.
[01:29:03.380 --> 01:29:04.880]   There was so much respect in the room,
[01:29:04.880 --> 01:29:05.940]   pushing back and forth.
[01:29:05.940 --> 01:29:07.020]   It was great. - Yeah.
[01:29:07.020 --> 01:29:08.980]   So I respect him.
[01:29:08.980 --> 01:29:13.180]   And I think when he has somebody on
[01:29:13.180 --> 01:29:16.720]   who's a dyed in the wool anti-vaxxer,
[01:29:16.720 --> 01:29:21.680]   the question is how can you balance,
[01:29:21.680 --> 01:29:24.460]   if it needs balance, in real time?
[01:29:24.460 --> 01:29:26.300]   I'm not talking about afterwards.
[01:29:26.300 --> 01:29:27.720]   I'm talking in real time.
[01:29:27.720 --> 01:29:30.820]   Maybe you record, well, he does record it, obviously,
[01:29:30.820 --> 01:29:33.760]   but maybe when there's a statement made
[01:29:33.760 --> 01:29:38.040]   that is made as if it's fact-based,
[01:29:38.040 --> 01:29:43.040]   maybe that statement should be checked by some folks
[01:29:43.680 --> 01:29:48.140]   who, imaginary folks who are trustworthy.
[01:29:48.140 --> 01:29:51.680]   And in real time, as that discussion
[01:29:51.680 --> 01:29:54.240]   is being played on the podcast,
[01:29:54.240 --> 01:29:59.120]   to show what independent experts say about that claim.
[01:29:59.120 --> 01:30:00.240]   - That's a really interesting idea.
[01:30:00.240 --> 01:30:01.640]   By the way, for some reason,
[01:30:01.640 --> 01:30:04.000]   this idea popped into my head now is,
[01:30:04.000 --> 01:30:05.640]   I think real time is very difficult.
[01:30:05.640 --> 01:30:07.320]   And it's not difficult,
[01:30:07.320 --> 01:30:09.400]   but it kind of ruins the conversation
[01:30:09.400 --> 01:30:10.840]   'cause you want the idea to breathe.
[01:30:10.840 --> 01:30:11.840]   - Yeah.
[01:30:11.840 --> 01:30:15.200]   I think what's very possible is before it's published,
[01:30:15.200 --> 01:30:17.020]   it's the pre-publication,
[01:30:17.020 --> 01:30:20.400]   before it's published, you let a bunch of people review it
[01:30:20.400 --> 01:30:25.280]   and they can add their voices in post before it's published.
[01:30:25.280 --> 01:30:27.180]   They can add arguments,
[01:30:27.180 --> 01:30:31.520]   arguments against certain parts.
[01:30:31.520 --> 01:30:32.880]   That's very interesting to sort of,
[01:30:32.880 --> 01:30:37.320]   as one podcast, publish addendums.
[01:30:37.320 --> 01:30:39.880]   Publish the peer review together with the publication.
[01:30:39.880 --> 01:30:40.720]   - Yes.
[01:30:40.720 --> 01:30:41.920]   - That's very interesting.
[01:30:41.920 --> 01:30:44.160]   I might actually do that.
[01:30:44.160 --> 01:30:45.280]   That's really interesting.
[01:30:45.280 --> 01:30:47.160]   'Cause I've been doing more debates
[01:30:47.160 --> 01:30:51.740]   where you at the same time have multiple people,
[01:30:51.740 --> 01:30:54.800]   which has a different dynamic because both people,
[01:30:54.800 --> 01:30:58.840]   I mean, it's really nice to have the time to pause
[01:30:58.840 --> 01:31:02.080]   just by yourself to fact check,
[01:31:02.080 --> 01:31:04.160]   to look at the study that was mentioned,
[01:31:04.160 --> 01:31:05.640]   to understand what's going on.
[01:31:05.640 --> 01:31:09.520]   So the peer review process, to have a little bit of time.
[01:31:09.520 --> 01:31:10.440]   That's really interesting.
[01:31:10.440 --> 01:31:14.400]   I actually would, I'd like to try that.
[01:31:14.400 --> 01:31:17.760]   To agree with you on some point in terms of anti-vax,
[01:31:17.760 --> 01:31:20.680]   I've been fascinated by listening to arguments
[01:31:20.680 --> 01:31:23.840]   from this community of folks that's been quite large
[01:31:23.840 --> 01:31:25.360]   called the Flat Earthers,
[01:31:25.360 --> 01:31:28.080]   the people that believe the Earth is flat.
[01:31:28.080 --> 01:31:30.920]   And I don't know if you've ever listened to them
[01:31:30.920 --> 01:31:33.760]   or read their arguments,
[01:31:33.760 --> 01:31:36.160]   but it's fascinating how consistent
[01:31:36.160 --> 01:31:37.800]   and convincing it all sounds
[01:31:37.800 --> 01:31:39.600]   when you just kind of take it in.
[01:31:39.600 --> 01:31:43.720]   Just like, just take it in like listening normally.
[01:31:43.720 --> 01:31:45.340]   It's all very logical.
[01:31:45.340 --> 01:31:49.920]   Like if you don't think very, well, no.
[01:31:49.920 --> 01:31:55.560]   So the thing is, the reality is
[01:31:55.560 --> 01:31:57.280]   at the very basic human level
[01:31:57.280 --> 01:32:00.600]   with our limited cognitive capabilities,
[01:32:00.600 --> 01:32:03.680]   the Earth is pretty flat when you go outside
[01:32:03.680 --> 01:32:04.880]   and you look, it's flat.
[01:32:04.880 --> 01:32:08.040]   So like when you use common sense reasoning,
[01:32:08.040 --> 01:32:09.960]   it's very easy to play to that
[01:32:09.960 --> 01:32:12.080]   to convince you that the Earth is flat.
[01:32:12.080 --> 01:32:13.640]   Plus there's powerful organizations
[01:32:13.640 --> 01:32:16.260]   that want to manipulate you and so on.
[01:32:16.260 --> 01:32:20.920]   But then there's the whole progress of science
[01:32:20.920 --> 01:32:22.600]   and physics of the past,
[01:32:22.600 --> 01:32:26.120]   but that's difficult to integrate into your thought process.
[01:32:26.120 --> 01:32:29.640]   So it's very true that people should listen
[01:32:29.640 --> 01:32:33.400]   to Flat Earthers because it was very revealing to me
[01:32:33.400 --> 01:32:35.120]   how easily it is,
[01:32:35.120 --> 01:32:37.920]   how easy it is to be convinced of basically anything
[01:32:37.920 --> 01:32:42.440]   by charismatic arguments.
[01:32:42.440 --> 01:32:44.280]   - Right, and if we're arguing
[01:32:44.280 --> 01:32:46.920]   about whether the Earth is flat or not,
[01:32:46.920 --> 01:32:48.760]   as long as we're not navigating airplanes
[01:32:48.760 --> 01:32:49.920]   and doing other kinds of things,
[01:32:49.920 --> 01:32:53.800]   trying to get satellites to do transmission,
[01:32:53.800 --> 01:32:56.200]   it's not that important, what I believe.
[01:32:56.200 --> 01:32:59.480]   But if we're arguing about how we approach
[01:32:59.480 --> 01:33:02.400]   the worst public health crisis in,
[01:33:02.400 --> 01:33:03.320]   I don't know how long,
[01:33:03.320 --> 01:33:06.360]   I think we're getting worse than the Spanish flu now.
[01:33:06.360 --> 01:33:07.800]   I don't know what the total global deaths
[01:33:07.800 --> 01:33:08.680]   with Spanish flu were,
[01:33:08.680 --> 01:33:10.120]   but in the United States,
[01:33:10.120 --> 01:33:12.440]   we certainly have more deaths than we had from Spanish flu.
[01:33:12.440 --> 01:33:14.720]   - Plus the economic pain and suffering.
[01:33:14.720 --> 01:33:19.680]   - Yes, yes, and the damage to the kids, school and so forth.
[01:33:19.680 --> 01:33:23.060]   We got a problem and it's not going away, unfortunately.
[01:33:23.060 --> 01:33:25.000]   So when we get a problem like that,
[01:33:25.000 --> 01:33:28.520]   it's not just an interesting bar room conversation
[01:33:28.520 --> 01:33:30.720]   about whether the Earth is flat.
[01:33:30.720 --> 01:33:32.580]   There are millions of lives involved.
[01:33:33.580 --> 01:33:36.460]   - Let me ask you yet another question,
[01:33:36.460 --> 01:33:40.260]   an issue I raised with Pfizer CEO Albert Bourla.
[01:33:40.260 --> 01:33:45.340]   It's the question of revolving doors,
[01:33:45.340 --> 01:33:47.420]   that there seems to be a revolving door
[01:33:47.420 --> 01:33:51.060]   between Pfizer, FDA and CDC.
[01:33:51.060 --> 01:33:53.220]   People that have worked at the FDA
[01:33:53.220 --> 01:33:56.420]   now work at Pfizer and vice versa,
[01:33:56.420 --> 01:33:58.620]   including the CDC and so on.
[01:33:58.620 --> 01:34:01.700]   What do you think about that?
[01:34:01.700 --> 01:34:04.060]   - So first of all, his response once again is,
[01:34:04.060 --> 01:34:06.380]   there's rules, there's very strict rules
[01:34:06.380 --> 01:34:07.460]   and we follow them.
[01:34:07.460 --> 01:34:10.260]   Do you think that's a problem?
[01:34:10.260 --> 01:34:11.980]   - Hoo-ha.
[01:34:11.980 --> 01:34:16.220]   - And also maybe this is a good time to talk about
[01:34:16.220 --> 01:34:18.220]   this Pfizer play by the rules.
[01:34:18.220 --> 01:34:20.340]   - One at a time.
[01:34:20.340 --> 01:34:21.180]   - One at a time.
[01:34:21.180 --> 01:34:22.700]   - Okay, and this isn't even about Pfizer,
[01:34:22.700 --> 01:34:24.300]   but it's an answer to the question.
[01:34:24.300 --> 01:34:25.140]   - Yes.
[01:34:25.140 --> 01:34:27.500]   - So there's this drug, Adjahilm,
[01:34:27.500 --> 01:34:31.340]   that was approved by the FDA maybe six months ago.
[01:34:31.340 --> 01:34:34.940]   It's a drug to prevent the progression
[01:34:34.940 --> 01:34:37.160]   of low-grade Alzheimer's disease.
[01:34:37.160 --> 01:34:43.340]   The target for drug development for Alzheimer's disease
[01:34:43.340 --> 01:34:47.020]   has been the amyloid, reducing the amyloid plaques
[01:34:47.020 --> 01:34:50.300]   in the brain, which correlate with the progression
[01:34:50.300 --> 01:34:52.140]   of Alzheimer's.
[01:34:52.140 --> 01:34:57.140]   And Biogen showed that its drug, Adjahilm,
[01:34:57.140 --> 01:35:01.020]   reduces amyloid plaques in the brain.
[01:35:01.020 --> 01:35:03.060]   They did two clinical trials
[01:35:03.060 --> 01:35:05.620]   to determine the clinical efficacy,
[01:35:05.620 --> 01:35:07.900]   and they found that neither trial
[01:35:07.900 --> 01:35:09.980]   showed a meaningful benefit.
[01:35:09.980 --> 01:35:14.340]   And in those two trials, 33% more people
[01:35:14.340 --> 01:35:18.080]   in the Adjahilm group developed symptomatic brain swelling
[01:35:18.080 --> 01:35:20.820]   and bleeding than people in the placebo group.
[01:35:20.820 --> 01:35:25.940]   There was an advisory committee convened
[01:35:25.940 --> 01:35:30.460]   to debate and determine how they felt
[01:35:30.460 --> 01:35:34.260]   about the approvability of Adjahilm, given those facts.
[01:35:34.260 --> 01:35:37.140]   And those facts aren't in dispute.
[01:35:37.140 --> 01:35:41.580]   They're in Biogen slides, as well as FDA documents.
[01:35:41.580 --> 01:35:46.580]   The advisory committee voted 10 against approval
[01:35:46.580 --> 01:35:49.940]   and one abstain.
[01:35:49.940 --> 01:35:52.660]   So that's essentially universal,
[01:35:52.660 --> 01:35:56.260]   unanimous vote against approving Adjahilm.
[01:35:56.260 --> 01:35:58.280]   Now, the advisory committees have been
[01:35:58.280 --> 01:36:03.280]   pretty much cleansed of financial conflicts of interest.
[01:36:03.280 --> 01:36:08.280]   So this advisory committee votes 10 no, one abstention,
[01:36:08.280 --> 01:36:13.180]   and the FDA overrules the unanimous opinion
[01:36:13.180 --> 01:36:16.180]   of its advisory committee and approves the drug.
[01:36:16.180 --> 01:36:21.360]   Three of the members of the advisory committee resign.
[01:36:21.360 --> 01:36:22.320]   They say, "We're not gonna be part,
[01:36:22.320 --> 01:36:24.700]   "if the FDA's not gonna listen to a unanimous vote
[01:36:24.700 --> 01:36:26.620]   "against approving this drug,
[01:36:26.620 --> 01:36:30.520]   "which shows more harm than benefit, undisputed.
[01:36:30.520 --> 01:36:33.820]   "We're not gonna participate in this."
[01:36:33.820 --> 01:36:36.680]   And the argument against approval
[01:36:36.680 --> 01:36:40.440]   is that the surrogate endpoint, the reduction of amyloid,
[01:36:40.440 --> 01:36:43.340]   the progression of amyloid plaques,
[01:36:43.340 --> 01:36:48.020]   is known by the FDA not to be a valid clinical indicator.
[01:36:48.020 --> 01:36:49.140]   It doesn't correlate.
[01:36:49.140 --> 01:36:52.000]   27 studies have shown it doesn't correlate
[01:36:52.000 --> 01:36:53.340]   with clinical progression.
[01:36:53.340 --> 01:36:55.720]   Interrupting the amyloid plaques doesn't mean
[01:36:55.720 --> 01:37:00.260]   that your Alzheimer's doesn't get worse.
[01:37:00.260 --> 01:37:06.920]   So it seems like it's a slam dunk and the FDA made a mistake
[01:37:06.920 --> 01:37:09.360]   and they should do whatever they do
[01:37:09.360 --> 01:37:12.040]   to protect their bureaucratic reputation.
[01:37:12.040 --> 01:37:15.280]   So the head of the Bureau of the FDA,
[01:37:15.280 --> 01:37:17.320]   the Center for Drug Evaluation and Research
[01:37:17.320 --> 01:37:21.880]   that approves new drugs, who had spent 16 years
[01:37:21.880 --> 01:37:25.280]   as an executive in the pharmaceutical industry,
[01:37:25.280 --> 01:37:28.000]   issued a statement and said,
[01:37:28.000 --> 01:37:30.900]   "What we should do in this situation
[01:37:30.900 --> 01:37:35.900]   "is to loosen the prohibition of financial ties of interest
[01:37:35.900 --> 01:37:38.520]   "with the drug companies
[01:37:38.520 --> 01:37:41.340]   "so we get less emotional responses."
[01:37:41.340 --> 01:37:46.140]   Said this, it's in print.
[01:37:46.140 --> 01:37:51.520]   - People are just too emotional about this.
[01:37:51.520 --> 01:37:52.920]   - People were just too emotional.
[01:37:52.920 --> 01:37:55.040]   The 10 people who voted against it
[01:37:55.040 --> 01:37:58.440]   and the no people who voted for it, it's all too emotional.
[01:37:58.440 --> 01:38:00.000]   So this gets back,
[01:38:00.000 --> 01:38:02.560]   this is a long answer to your short question.
[01:38:02.560 --> 01:38:04.880]   I think this is a wonderful window
[01:38:04.880 --> 01:38:07.120]   into the thinking of the FDA
[01:38:07.120 --> 01:38:11.120]   that financial conflicts of interest don't matter
[01:38:11.120 --> 01:38:13.240]   in a situation when I think it's obvious
[01:38:13.240 --> 01:38:14.960]   that they would matter.
[01:38:14.960 --> 01:38:18.000]   - But there's not a direct financial conflict of interest.
[01:38:18.000 --> 01:38:21.160]   It's kind of, like it's not,
[01:38:21.160 --> 01:38:26.080]   like Albert said, there's rules.
[01:38:26.080 --> 01:38:27.160]   I mean, you're not allowed
[01:38:27.160 --> 01:38:29.720]   to have direct financial conflicts of interest.
[01:38:29.720 --> 01:38:32.240]   It's indirect.
[01:38:32.240 --> 01:38:34.520]   - Right, but what I'm saying is,
[01:38:34.520 --> 01:38:36.540]   I'm not denying what he said is true,
[01:38:36.540 --> 01:38:41.960]   but the FDA, a high official in the FDA
[01:38:41.960 --> 01:38:45.440]   is saying that we need to allow conflicts of interest
[01:38:45.440 --> 01:38:47.220]   in our advisory committee meetings.
[01:38:47.220 --> 01:38:49.280]   - Wow.
[01:38:49.280 --> 01:38:53.320]   - And that, she wants to change the rules.
[01:38:53.320 --> 01:38:54.160]   - Right.
[01:38:54.160 --> 01:38:58.040]   - So Albert Borla would still be playing by the rules,
[01:38:58.040 --> 01:39:03.040]   but it just shows how one-sided the thinking here is.
[01:39:03.040 --> 01:39:05.280]   - But you think that's influenced by the fact
[01:39:05.280 --> 01:39:07.360]   that there were pharmaceutical executives
[01:39:07.360 --> 01:39:09.920]   working at the FDA and vice versa.
[01:39:09.920 --> 01:39:11.760]   - And they think that's a great idea.
[01:39:11.760 --> 01:39:14.520]   - Who gets to fix this?
[01:39:14.520 --> 01:39:16.480]   Do you think it should be just banned?
[01:39:16.480 --> 01:39:17.320]   Like if you worked--
[01:39:17.320 --> 01:39:19.040]   - I don't know, two separate questions.
[01:39:19.040 --> 01:39:22.280]   One is, should the officials at the FDA
[01:39:22.280 --> 01:39:24.800]   come from pharma and vice versa?
[01:39:24.800 --> 01:39:25.640]   - Yes.
[01:39:25.640 --> 01:39:26.460]   - That's one question.
[01:39:26.460 --> 01:39:27.400]   And the other question is,
[01:39:27.400 --> 01:39:29.400]   should advisory committee members be allowed
[01:39:29.400 --> 01:39:31.680]   to have financial conflicts of interest?
[01:39:31.680 --> 01:39:33.120]   - Yes.
[01:39:33.120 --> 01:39:38.120]   - I think, in my opinion, and people might say I'm biased,
[01:39:38.120 --> 01:39:40.320]   I think advisory committee people
[01:39:40.320 --> 01:39:42.080]   should not have conflicts of interest.
[01:39:42.080 --> 01:39:44.880]   I think their only interest ought to be the public interest.
[01:39:44.880 --> 01:39:49.240]   And that was true from my understanding of the situation.
[01:39:49.240 --> 01:39:51.280]   It's the afterward in my book,
[01:39:51.280 --> 01:39:54.200]   I spent some time studying it about Adjahilm.
[01:39:54.200 --> 01:39:55.640]   I think it's a slam dunk
[01:39:55.640 --> 01:39:57.680]   that there ought to be no conflicts of interest.
[01:39:57.680 --> 01:39:59.760]   Now, the head of CDER,
[01:39:59.760 --> 01:40:01.400]   Center for Drug Evaluation Research,
[01:40:01.400 --> 01:40:04.660]   thinks that that's gonna give you a biased result
[01:40:04.660 --> 01:40:07.380]   because we don't have company influence.
[01:40:07.380 --> 01:40:12.380]   And that, I think, shows how biased their thinking is,
[01:40:14.360 --> 01:40:17.340]   that not having company influence is a bias.
[01:40:17.340 --> 01:40:21.200]   - Let me try to load that in.
[01:40:21.200 --> 01:40:23.320]   I'm trying to empathize with the belief
[01:40:23.320 --> 01:40:26.760]   that companies should have a voice at the table.
[01:40:26.760 --> 01:40:30.440]   I mean, yeah, it's part of the game.
[01:40:30.440 --> 01:40:31.400]   They've convinced themselves
[01:40:31.400 --> 01:40:33.200]   that this is how it should be played.
[01:40:33.200 --> 01:40:36.320]   - But they have a voice at the table.
[01:40:36.320 --> 01:40:37.760]   They've designed the studies.
[01:40:37.760 --> 01:40:39.360]   - Right, that's their voice.
[01:40:39.360 --> 01:40:40.960]   That's the whole point. - They've analyzed the data.
[01:40:40.960 --> 01:40:43.080]   I mean, what bigger voice do you deserve?
[01:40:43.080 --> 01:40:47.080]   - But I do also think, on the more challenging question,
[01:40:47.080 --> 01:40:50.200]   I do think that there should be a ban.
[01:40:50.200 --> 01:40:53.640]   If you work at a pharmaceutical company,
[01:40:53.640 --> 01:40:57.640]   you should not be allowed to work at any regulatory agency.
[01:40:57.640 --> 01:41:01.520]   - Yes.
[01:41:01.520 --> 01:41:03.960]   - You should not, I mean, that, going back and forth,
[01:41:03.960 --> 01:41:06.960]   it just, even if it's 30 years later.
[01:41:06.960 --> 01:41:11.020]   - I agree, and I have another nomination for a ban.
[01:41:11.020 --> 01:41:13.760]   We're in this crazy situation where Medicare
[01:41:13.760 --> 01:41:16.440]   is not allowed to negotiate the price of drugs
[01:41:16.440 --> 01:41:17.780]   with the drug companies.
[01:41:17.780 --> 01:41:20.820]   So the drug companies get a patent on a new drug.
[01:41:20.820 --> 01:41:22.440]   Unlike every other developed country,
[01:41:22.440 --> 01:41:24.000]   they can charge whatever they want.
[01:41:24.000 --> 01:41:27.800]   So they have a monopoly on a utility
[01:41:27.800 --> 01:41:29.560]   'cause no one else can make the drug.
[01:41:29.560 --> 01:41:32.000]   Charge whatever they want, and Medicare has to pay for it.
[01:41:32.000 --> 01:41:35.640]   And you say, how did we get in this crazy situation?
[01:41:35.640 --> 01:41:39.600]   So how we got here is that in 2003,
[01:41:39.600 --> 01:41:42.020]   when Medicare Part D was passed,
[01:41:42.020 --> 01:41:45.680]   Billy Towson was head of the Ways and Means Committee
[01:41:45.680 --> 01:41:48.960]   in the House, played a key role in ushering this through
[01:41:48.960 --> 01:41:52.440]   with the non-negotiation clause of it.
[01:41:52.440 --> 01:41:53.960]   And after it was passed,
[01:41:53.960 --> 01:41:57.480]   Billy Towson did not finish out his term in Congress.
[01:41:57.480 --> 01:42:00.400]   He went to pharma for a $2 million a year job.
[01:42:00.400 --> 01:42:05.180]   - This is, this is incredible.
[01:42:05.180 --> 01:42:08.080]   - You might think that a ban on that would be a good idea.
[01:42:09.480 --> 01:42:11.000]   - I spoke with Francis Collins,
[01:42:11.000 --> 01:42:13.720]   head of the NIH on this podcast.
[01:42:13.720 --> 01:42:18.720]   He and NIH have a lot of power over funding in science.
[01:42:18.720 --> 01:42:23.520]   What are they doing right?
[01:42:23.520 --> 01:42:24.960]   What are they doing wrong?
[01:42:24.960 --> 01:42:28.760]   In this interplay with big pharma,
[01:42:28.760 --> 01:42:30.420]   how connected are they?
[01:42:30.420 --> 01:42:33.760]   Again, returning to the question,
[01:42:33.760 --> 01:42:35.480]   what are they doing right?
[01:42:35.480 --> 01:42:37.680]   What are they doing wrong, in your view?
[01:42:37.680 --> 01:42:41.160]   - So my knowledge of the NIH is not as granular
[01:42:41.160 --> 01:42:43.240]   as my knowledge of pharma.
[01:42:43.240 --> 01:42:47.520]   That said, in broad brushstrokes,
[01:42:47.520 --> 01:42:51.200]   the NIH is doing the infrastructure work
[01:42:51.200 --> 01:42:53.400]   for all drug development.
[01:42:53.400 --> 01:42:56.680]   I think they've participated in 100% of the drugs
[01:42:56.680 --> 01:42:58.880]   that have been approved by the FDA
[01:42:58.880 --> 01:43:00.560]   over the past 10 years or so.
[01:43:00.560 --> 01:43:03.040]   They've done infrastructure work.
[01:43:03.080 --> 01:43:08.080]   And what they do is not work on particular drugs,
[01:43:08.080 --> 01:43:12.320]   but they develop work on drug targets,
[01:43:12.320 --> 01:43:16.880]   on targets in the human body that can be affected by drugs
[01:43:16.880 --> 01:43:21.520]   and might be beneficial to turn on or off.
[01:43:21.520 --> 01:43:23.320]   And then the drug companies can,
[01:43:23.320 --> 01:43:26.360]   when they find a target that is mutable
[01:43:26.360 --> 01:43:29.360]   and potentially beneficial,
[01:43:29.360 --> 01:43:32.040]   then the drug companies can take the research
[01:43:32.040 --> 01:43:34.640]   and choose to invest in the development of the drugs,
[01:43:34.640 --> 01:43:35.660]   specific drug.
[01:43:35.660 --> 01:43:38.360]   That's our model.
[01:43:38.360 --> 01:43:43.360]   Now, 96% of the research that's done in clinical trials
[01:43:43.360 --> 01:43:47.280]   in the United States is about drugs and devices.
[01:43:47.280 --> 01:43:49.960]   And only a fraction of the 4% that's left over
[01:43:49.960 --> 01:43:51.760]   is about preventive medicine
[01:43:51.760 --> 01:43:54.480]   and how to make Americans healthier.
[01:43:54.480 --> 01:43:58.520]   I think, again, from the satellite view,
[01:43:58.520 --> 01:44:03.520]   the NIH is investing more in science
[01:44:03.520 --> 01:44:07.280]   that can lead to commercial development
[01:44:07.280 --> 01:44:10.160]   rather than, as you said at the beginning of the podcast,
[01:44:10.160 --> 01:44:13.480]   there's no big fitness and lifestyle industry
[01:44:13.480 --> 01:44:15.960]   that can counter pharma.
[01:44:15.960 --> 01:44:19.700]   So I think at the NIH level, that countering can be done.
[01:44:19.700 --> 01:44:22.520]   And the Diabetes Prevention Program study
[01:44:22.520 --> 01:44:23.640]   that we talked about before,
[01:44:23.640 --> 01:44:26.480]   where lifestyle was part of a randomized trial
[01:44:26.480 --> 01:44:28.880]   and was shown to be more effective than metformin
[01:44:28.880 --> 01:44:30.960]   at preventing the development of diabetes,
[01:44:30.960 --> 01:44:34.440]   that is absolute proof positive
[01:44:34.440 --> 01:44:36.120]   that investing in that kind of science
[01:44:36.120 --> 01:44:37.880]   can produce good results.
[01:44:37.880 --> 01:44:42.880]   So I think that we're aimed at drug development
[01:44:42.880 --> 01:44:44.840]   and what we ought to be aimed at
[01:44:44.840 --> 01:44:47.680]   is an epidemiological approach
[01:44:47.680 --> 01:44:49.880]   to improving the health of all Americans.
[01:44:49.880 --> 01:44:54.080]   We rank 68th in the world in healthy life expectancy.
[01:44:55.040 --> 01:44:58.040]   Despite spending an extra trillion and a half dollars a year.
[01:44:58.040 --> 01:45:03.960]   And I believe strongly that the reason why we've gotten
[01:45:03.960 --> 01:45:09.200]   in this crazy position is because the knowledge
[01:45:09.200 --> 01:45:12.440]   that we're producing is about new drugs and devices
[01:45:12.440 --> 01:45:15.720]   and it's not about improving population health.
[01:45:15.720 --> 01:45:19.640]   In this problem, the NIH is the perfect institution
[01:45:19.640 --> 01:45:23.120]   to play a role in rebalancing our research agenda.
[01:45:23.120 --> 01:45:24.920]   - And some of that is on the leadership side
[01:45:24.920 --> 01:45:27.880]   with Francis Collins and Anthony Fauci,
[01:45:27.880 --> 01:45:32.440]   not just speaking about basically everything
[01:45:32.440 --> 01:45:34.720]   that just leads to drug development, vaccine development,
[01:45:34.720 --> 01:45:36.720]   but also speaking about healthy lifestyles
[01:45:36.720 --> 01:45:40.760]   and speaking about health, not just sickness.
[01:45:40.760 --> 01:45:41.920]   - Yes, and investing.
[01:45:41.920 --> 01:45:43.160]   - Investing. - Investing in health.
[01:45:43.160 --> 01:45:45.300]   - I mean, it's like,
[01:45:45.300 --> 01:45:48.960]   one fee is the other.
[01:45:48.960 --> 01:45:51.160]   One, you have to communicate to the public
[01:45:51.160 --> 01:45:53.960]   the importance of investing in health
[01:45:53.960 --> 01:45:57.800]   and that leads to you getting props for investing in health
[01:45:57.800 --> 01:45:59.520]   and then you can invest in health more and more
[01:45:59.520 --> 01:46:01.640]   and that communicates, I mean,
[01:46:01.640 --> 01:46:05.080]   everything that Anthony Fauci says or Francis Collins says
[01:46:05.080 --> 01:46:07.200]   has an impact on scientists.
[01:46:07.200 --> 01:46:12.080]   I mean, it sets the priorities.
[01:46:12.080 --> 01:46:16.080]   I don't think they, it's the sad thing about leaders,
[01:46:16.080 --> 01:46:22.080]   forgive me for saying the word, but mediocre leaders,
[01:46:22.080 --> 01:46:25.980]   is they don't see themselves as part of a game.
[01:46:25.980 --> 01:46:29.920]   They don't see the momentum.
[01:46:29.920 --> 01:46:31.160]   It's like a fish in the water.
[01:46:31.160 --> 01:46:32.920]   They don't see the water.
[01:46:32.920 --> 01:46:36.080]   Great leaders stand up and reverse the direction
[01:46:36.080 --> 01:46:37.120]   of how things are going
[01:46:37.120 --> 01:46:39.920]   and I actually put a lot of responsibility,
[01:46:39.920 --> 01:46:43.520]   some people say too much, but whatever.
[01:46:43.520 --> 01:46:46.440]   I think leaders carry the responsibility.
[01:46:46.440 --> 01:46:48.800]   I put a lot of responsibility on Anthony Fauci
[01:46:48.800 --> 01:46:51.360]   and Francis Collins for not actually speaking
[01:46:51.360 --> 01:46:55.880]   a lot more about health, not, and bigger,
[01:46:55.880 --> 01:47:00.880]   inspiring people in the power
[01:47:00.880 --> 01:47:05.540]   and the trustworthiness of science.
[01:47:05.540 --> 01:47:11.720]   You know, that's on the shoulders of Anthony Fauci.
[01:47:11.720 --> 01:47:15.800]   - I'm gonna abstain from that 'cause I'm not expert enough.
[01:47:15.800 --> 01:47:17.780]   - Neither am I, but I'm opinionated.
[01:47:17.780 --> 01:47:21.080]   - I am too, but not on camera.
[01:47:21.080 --> 01:47:22.520]   - Yes. (laughs)
[01:47:22.520 --> 01:47:27.200]   - No, but seriously, the problem is pretty simple,
[01:47:27.200 --> 01:47:31.400]   that we're investing 96% of our funding
[01:47:31.400 --> 01:47:33.520]   of clinical research in drugs and devices
[01:47:33.520 --> 01:47:36.840]   and 80% of our health is determined
[01:47:36.840 --> 01:47:38.120]   by how we live our lives.
[01:47:38.120 --> 01:47:39.180]   - Yes.
[01:47:39.180 --> 01:47:41.720]   - And this is ridiculous.
[01:47:41.720 --> 01:47:45.640]   The United States is going further and further
[01:47:45.640 --> 01:47:49.780]   behind the other wealthy countries in terms of our health.
[01:47:49.780 --> 01:47:53.600]   We ranked 38th in healthy life expectancy in 2000,
[01:47:53.600 --> 01:47:57.040]   and now we're spending a trillion and a half dollars extra,
[01:47:57.040 --> 01:47:58.360]   and we rank 68th.
[01:47:58.360 --> 01:47:59.520]   We've gone down.
[01:47:59.520 --> 01:48:02.400]   - You have this excellent, there's a few charts
[01:48:02.400 --> 01:48:06.440]   that I'll overlay that tell the story
[01:48:06.440 --> 01:48:09.720]   in really powerful ways.
[01:48:09.720 --> 01:48:13.600]   So one is the healthcare spending as percentage of GDP
[01:48:13.600 --> 01:48:17.840]   that on the x-axis is years and the y-axis is percentage,
[01:48:17.840 --> 01:48:20.820]   and the United States as compared to other countries
[01:48:20.820 --> 01:48:25.820]   on average has been much larger and growing.
[01:48:25.820 --> 01:48:30.540]   - Right, we are now spending 7% more of our GDP,
[01:48:30.540 --> 01:48:35.220]   17.7% versus 10.7% on healthcare.
[01:48:35.220 --> 01:48:38.820]   7%, and I think GDP is the fairest way
[01:48:38.820 --> 01:48:40.700]   to compare healthcare spending.
[01:48:40.700 --> 01:48:43.460]   Per person in dollars, we're spending even,
[01:48:43.460 --> 01:48:45.540]   the difference is even greater,
[01:48:45.540 --> 01:48:48.200]   but other costs vary with GDP,
[01:48:48.200 --> 01:48:50.800]   so let's stick with the conservative way to do it.
[01:48:50.800 --> 01:48:55.800]   - 17.7 or 18% of GDP,
[01:48:55.800 --> 01:49:00.740]   18% of GDP spent on healthcare,
[01:49:00.740 --> 01:49:04.800]   7% higher than the comparable country average.
[01:49:04.800 --> 01:49:05.640]   - Right.
[01:49:05.640 --> 01:49:09.960]   - 17.7% versus 10.7, 7% higher.
[01:49:09.960 --> 01:49:14.960]   - Right, and 7% of $23 trillion GDP
[01:49:15.140 --> 01:49:19.040]   is more than $1.5 trillion a year in excess.
[01:49:19.040 --> 01:49:20.520]   - And then you have another chart
[01:49:20.520 --> 01:49:23.440]   that shows healthcare system performance
[01:49:23.440 --> 01:49:24.840]   compared to spending,
[01:49:24.840 --> 01:49:29.800]   and there's a point cloud of different countries,
[01:49:29.800 --> 01:49:33.160]   the x-axis being healthcare spending
[01:49:33.160 --> 01:49:36.380]   as a percentage of GDP, which we just talked about,
[01:49:36.380 --> 01:49:40.920]   that US is 7% higher than the average,
[01:49:40.920 --> 01:49:44.520]   and then on the y-axis is performance.
[01:49:44.520 --> 01:49:48.280]   So x-axis spending, y-axis performance,
[01:49:48.280 --> 01:49:49.700]   and there's a point cloud,
[01:49:49.700 --> 01:49:52.400]   we'll overlay this if you're watching on YouTube,
[01:49:52.400 --> 01:49:57.000]   of a bunch of countries that have high performance
[01:49:57.000 --> 01:50:00.080]   for what they're spending,
[01:50:00.080 --> 01:50:04.640]   and then US is all alone
[01:50:04.640 --> 01:50:07.500]   on the right bottom side of the chart
[01:50:07.500 --> 01:50:10.760]   where it's low performance and high spending.
[01:50:10.760 --> 01:50:11.600]   - Correct.
[01:50:12.880 --> 01:50:17.880]   - So this is a system that is abiding by spending
[01:50:17.880 --> 01:50:21.160]   that is directed by the most profitable ways
[01:50:21.160 --> 01:50:22.480]   to deliver healthcare.
[01:50:22.480 --> 01:50:25.040]   - So you put that in the hands of big pharma,
[01:50:25.040 --> 01:50:27.320]   is you maximize for profit,
[01:50:27.320 --> 01:50:31.620]   you're going to decrease performance and increase spending.
[01:50:31.620 --> 01:50:34.320]   - Yes, but I wanna qualify that
[01:50:34.320 --> 01:50:36.320]   and say it's not all big pharma's fault.
[01:50:36.320 --> 01:50:39.320]   They're not responsible for all the problems
[01:50:39.320 --> 01:50:41.200]   in our healthcare system.
[01:50:41.200 --> 01:50:43.200]   They're not responsible for the administrative costs,
[01:50:43.200 --> 01:50:47.400]   for example, but they are the largest component
[01:50:47.400 --> 01:50:51.320]   of our rising healthcare costs,
[01:50:51.320 --> 01:50:54.160]   and it has to do with this knowledge issue.
[01:50:54.160 --> 01:50:56.780]   Controlling the knowledge that doctors have
[01:50:56.780 --> 01:51:01.240]   makes it so that doctors can live with this situation,
[01:51:01.240 --> 01:51:04.880]   believing that it's optimal, when it's a wreck.
[01:51:04.880 --> 01:51:06.160]   - Yeah.
[01:51:06.160 --> 01:51:08.680]   Let me ask you the big, so as a physician,
[01:51:10.160 --> 01:51:13.680]   so everything you've seen, we've talked about 80%
[01:51:13.680 --> 01:51:15.960]   of the impact on health is lifestyle.
[01:51:15.960 --> 01:51:20.220]   How do we live longer?
[01:51:20.220 --> 01:51:22.720]   What advice would you give to general people?
[01:51:22.720 --> 01:51:27.720]   What space of ideas result in living longer
[01:51:27.720 --> 01:51:30.760]   and higher quality lives?
[01:51:30.760 --> 01:51:33.600]   - Right, this is a very simple question to answer.
[01:51:33.600 --> 01:51:37.840]   Exercise for at least a half hour,
[01:51:37.840 --> 01:51:39.400]   at least five times a week.
[01:51:40.400 --> 01:51:42.400]   Number one.
[01:51:42.400 --> 01:51:44.200]   Number two, don't smoke.
[01:51:44.200 --> 01:51:49.320]   Number three, maintain a reasonably healthy body weight.
[01:51:49.320 --> 01:51:53.760]   Some people argue that being lower than a BMI of 25
[01:51:53.760 --> 01:51:54.840]   is healthy.
[01:51:54.840 --> 01:51:59.280]   I think that may be true, but I think getting above 30
[01:51:59.280 --> 01:52:01.940]   is unhealthy, and that ought to be.
[01:52:01.940 --> 01:52:06.940]   Now, that's largely impacted by socioeconomic status,
[01:52:07.640 --> 01:52:09.960]   and we don't want to blame the victims here.
[01:52:09.960 --> 01:52:12.640]   So we got to understand that when we talk about
[01:52:12.640 --> 01:52:17.080]   all of these things, not cigarettes, but exercise
[01:52:17.080 --> 01:52:21.340]   and a good diet and maintaining a healthy body weight,
[01:52:21.340 --> 01:52:26.160]   we have to include in doing those things
[01:52:26.160 --> 01:52:32.040]   the impediments to people of lower socioeconomic status
[01:52:32.040 --> 01:52:34.400]   being able to make those changes.
[01:52:34.400 --> 01:52:38.160]   We've got to understand that personal responsibility
[01:52:38.160 --> 01:52:42.280]   accounts for some of this, but also social circumstances
[01:52:42.280 --> 01:52:44.000]   accounts for some of it.
[01:52:44.000 --> 01:52:47.000]   And back to your fishbowl analogy,
[01:52:47.000 --> 01:52:50.040]   if you're swimming in a fishbowl,
[01:52:50.040 --> 01:52:52.200]   if you live in a fish tank that's not being properly
[01:52:52.200 --> 01:52:55.720]   maintained, the approach wouldn't be to treat
[01:52:55.720 --> 01:53:00.720]   individual sick fish, it would be to fix your fish tank
[01:53:01.560 --> 01:53:04.720]   to get the bacteria out of it and whatever bad stuff
[01:53:04.720 --> 01:53:08.440]   is in there and make your fish tank healthier.
[01:53:08.440 --> 01:53:12.820]   Well, we invest far less than the other wealthy countries do.
[01:53:12.820 --> 01:53:13.660]   We're flipped.
[01:53:13.660 --> 01:53:17.700]   We have the mirror image in the spending on social
[01:53:17.700 --> 01:53:20.840]   determinants of health and medical determinants of health.
[01:53:20.840 --> 01:53:22.920]   We have exactly the wrong order.
[01:53:22.920 --> 01:53:26.640]   And not only does that choke off social determinants
[01:53:26.640 --> 01:53:28.320]   of health, which are very important,
[01:53:28.320 --> 01:53:32.880]   but actually just the ratio, even if you were spending,
[01:53:32.880 --> 01:53:37.120]   if we raise the social spending and raise our medical
[01:53:37.120 --> 01:53:40.360]   spending in proportion, it's the ratio of social spending
[01:53:40.360 --> 01:53:42.920]   to medical spending that's the problem.
[01:53:42.920 --> 01:53:44.560]   So, and why do we do that?
[01:53:44.560 --> 01:53:47.400]   Well, the answer is perfectly obvious that the way
[01:53:47.400 --> 01:53:51.840]   to transfer money from working Americans to investors
[01:53:51.840 --> 01:53:53.660]   is through the biomedical model,
[01:53:53.660 --> 01:53:57.720]   not through the social health model.
[01:53:57.720 --> 01:54:02.720]   And that's the problem for, and I'd like to discuss this
[01:54:02.720 --> 01:54:06.320]   because the market isn't gonna get us
[01:54:06.320 --> 01:54:08.020]   to a reasonable allocation.
[01:54:08.020 --> 01:54:10.800]   All the other wealthy countries that are so much healthier
[01:54:10.800 --> 01:54:14.000]   than we are and spending so much less than we are
[01:54:14.000 --> 01:54:17.080]   have some form of government intervention
[01:54:17.080 --> 01:54:20.440]   in the quality of the health data that's available
[01:54:20.440 --> 01:54:25.440]   in the budgeting of health and social factors.
[01:54:25.920 --> 01:54:28.040]   And we don't, we're kind of the wild west
[01:54:28.040 --> 01:54:31.040]   and we let the market determine those allocations.
[01:54:31.040 --> 01:54:36.040]   And it's an awful failure, it's a horrendous failure.
[01:54:36.040 --> 01:54:40.800]   - So one argument against government, or sorry,
[01:54:40.800 --> 01:54:43.720]   an alternative to the government intervention
[01:54:43.720 --> 01:54:49.680]   is the market can work better if the citizenry
[01:54:49.680 --> 01:54:51.660]   has better information.
[01:54:51.660 --> 01:54:53.620]   So one argument is that, you know,
[01:54:54.180 --> 01:54:58.700]   communicators like podcasts and so on,
[01:54:58.700 --> 01:55:01.100]   but other channels of communication
[01:55:01.100 --> 01:55:03.860]   will be the way to fight big pharma.
[01:55:03.860 --> 01:55:07.580]   Your book is the way to, so by providing information.
[01:55:07.580 --> 01:55:10.360]   The alternative to the government intervention
[01:55:10.360 --> 01:55:12.780]   on every aspect of this, including communication
[01:55:12.780 --> 01:55:15.300]   with the doctors is to provide them other information
[01:55:15.300 --> 01:55:18.620]   and not allow the market to provide that information
[01:55:18.620 --> 01:55:23.620]   by basically making it exciting to buy books,
[01:55:23.620 --> 01:55:28.140]   to make better and better communicators on Twitter,
[01:55:28.140 --> 01:55:31.860]   through books, through op-eds, through podcasts,
[01:55:31.860 --> 01:55:33.980]   through so on, so basically,
[01:55:33.980 --> 01:55:38.340]   'cause there's a lot of incentive to communicate
[01:55:38.340 --> 01:55:40.420]   against the messages of big pharma.
[01:55:40.420 --> 01:55:43.620]   There is incentive because people want to understand
[01:55:43.620 --> 01:55:45.740]   what's good for their lives and they're willing to listen
[01:55:45.740 --> 01:55:48.700]   to charismatic people that are able to clearly explain
[01:55:48.700 --> 01:55:50.940]   what is good for them.
[01:55:50.940 --> 01:55:54.000]   - And they do, and more than 80% of people
[01:55:54.000 --> 01:55:56.660]   think that drugs cost too much and the drug industry
[01:55:56.660 --> 01:55:58.780]   is too interested in profits.
[01:55:58.780 --> 01:56:02.340]   - But they still get influenced.
[01:56:02.340 --> 01:56:05.260]   - They can't, you can't get the vote through Congress.
[01:56:05.260 --> 01:56:06.860]   - Yeah. - You know,
[01:56:06.860 --> 01:56:08.740]   Democrats and Republicans alike
[01:56:08.740 --> 01:56:10.260]   are taking money from Congress,
[01:56:10.260 --> 01:56:13.940]   and somehow it just doesn't work out
[01:56:13.940 --> 01:56:17.180]   that these even small changes,
[01:56:17.180 --> 01:56:21.780]   I mean, the pared down part of Medicare,
[01:56:21.780 --> 01:56:26.780]   the plan for increasing Medicare negotiation drug costs
[01:56:26.780 --> 01:56:31.180]   in Build Back Better, it's literally gonna reduce
[01:56:31.180 --> 01:56:36.020]   the number of new drugs that are beneficial,
[01:56:36.020 --> 01:56:39.480]   uniquely beneficial, by about one new drug
[01:56:39.480 --> 01:56:42.120]   or two new drugs over 30 years.
[01:56:43.860 --> 01:56:48.420]   It will have virtually an indecipherable impact.
[01:56:48.420 --> 01:56:53.420]   And yet pharma is talking about the impact on innovation.
[01:56:53.420 --> 01:56:55.900]   And if you vote for this,
[01:56:55.900 --> 01:56:58.260]   if you let your congressman vote for this,
[01:56:58.260 --> 01:57:03.260]   you're gonna severely slow down drug innovation
[01:57:03.260 --> 01:57:07.040]   and that's gonna affect the quality of your life.
[01:57:09.540 --> 01:57:14.540]   - Let me ask you about over-medication
[01:57:14.540 --> 01:57:19.700]   that we've been talking about from different angles,
[01:57:19.700 --> 01:57:22.660]   but one difficult question for me,
[01:57:22.660 --> 01:57:25.500]   I'll just, I'll pick one of the difficult topics,
[01:57:25.500 --> 01:57:26.940]   depression.
[01:57:26.940 --> 01:57:31.940]   So depression is a serious, painful condition
[01:57:31.940 --> 01:57:36.280]   that leads to a lot of people suffering in the world.
[01:57:37.220 --> 01:57:40.500]   And yet it is likely they were over-prescribing
[01:57:40.500 --> 01:57:42.340]   antidepressants.
[01:57:42.340 --> 01:57:47.020]   So as a doctor, as a patient, as a healthcare system,
[01:57:47.020 --> 01:57:50.300]   as a society, what do we do with that fact
[01:57:50.300 --> 01:57:53.000]   that people suffer?
[01:57:53.000 --> 01:57:56.540]   There's a lot of people suffering from depression
[01:57:56.540 --> 01:58:00.500]   and there's also people suffering from over-prescribing
[01:58:00.500 --> 01:58:01.940]   of antidepressants.
[01:58:01.940 --> 01:58:02.840]   - Right.
[01:58:02.840 --> 01:58:06.820]   So a paper in the New England Journal by Eric Turner
[01:58:06.820 --> 01:58:10.780]   showed that the data, if you put all the data together
[01:58:10.780 --> 01:58:15.620]   from antidepressants, you find out that antidepressants
[01:58:15.620 --> 01:58:19.340]   are not effective for people who are depressed
[01:58:19.340 --> 01:58:21.060]   but don't have a major depression.
[01:58:21.060 --> 01:58:25.300]   Major depression is a serious problem.
[01:58:25.300 --> 01:58:27.260]   People can't function normally,
[01:58:27.260 --> 01:58:31.020]   they have a hard time getting out,
[01:58:31.020 --> 01:58:34.220]   performing their normal social roles.
[01:58:35.780 --> 01:58:39.460]   But what's happened is that the publicity,
[01:58:39.460 --> 01:58:43.020]   I mean, Prozac Nation was a good example
[01:58:43.020 --> 01:58:45.980]   of making the argument that why should people
[01:58:45.980 --> 01:58:48.180]   settle for normal happiness when they can have
[01:58:48.180 --> 01:58:49.780]   better than normal happiness?
[01:58:49.780 --> 01:58:52.460]   And if you're not having normal happiness,
[01:58:52.460 --> 01:58:53.540]   you should take a drug.
[01:58:53.540 --> 01:58:58.540]   Well, that concept that serotonin metabolism
[01:58:58.540 --> 01:59:03.520]   is the root cause of depression
[01:59:03.520 --> 01:59:05.500]   is really a destructive one.
[01:59:05.500 --> 01:59:08.740]   We have drugs that change serotonin metabolism,
[01:59:08.740 --> 01:59:12.140]   but we don't know if that's why antidepressants
[01:59:12.140 --> 01:59:14.420]   work on major depression.
[01:59:14.420 --> 01:59:16.020]   And they certainly don't work on everybody
[01:59:16.020 --> 01:59:16.860]   with major depression.
[01:59:16.860 --> 01:59:18.500]   I forget what the number needed to treat is.
[01:59:18.500 --> 01:59:20.820]   I think it's around four.
[01:59:20.820 --> 01:59:23.580]   One out of four people have significant improvement.
[01:59:23.580 --> 01:59:28.180]   But the people without major depression don't get better.
[01:59:28.180 --> 01:59:30.900]   And the vast majority of these drugs are used
[01:59:30.900 --> 01:59:33.700]   for people without major depression.
[01:59:33.700 --> 01:59:37.260]   So what's happened is that the feelings
[01:59:37.260 --> 01:59:42.020]   of life satisfaction, of happiness and not sadness
[01:59:42.020 --> 01:59:43.940]   have been medicalized.
[01:59:43.940 --> 01:59:47.860]   The normal range of feelings have been medicalized.
[01:59:47.860 --> 01:59:51.020]   And that's not to say that they shouldn't be attended to,
[01:59:51.020 --> 01:59:54.340]   but the evidence shows that attending to them
[01:59:54.340 --> 01:59:57.020]   by giving somebody a medicine doesn't help
[01:59:57.020 --> 01:59:59.660]   except that they feel like somebody cares about them
[01:59:59.660 --> 02:00:01.420]   and believes that they're suffering.
[02:00:01.420 --> 02:00:05.380]   But there are problems in living that give rise
[02:00:05.380 --> 02:00:08.860]   to much of this symptomatology of less
[02:00:08.860 --> 02:00:10.100]   than major depression.
[02:00:10.100 --> 02:00:13.340]   And let's call it what it is and figure out a way
[02:00:13.340 --> 02:00:16.720]   to help people in visual therapy, group therapy.
[02:00:16.720 --> 02:00:19.880]   Maybe lifestyle modification would work.
[02:00:19.880 --> 02:00:20.880]   We got to try that.
[02:00:20.880 --> 02:00:24.780]   But let's call it what it is instead of saying,
[02:00:24.780 --> 02:00:29.780]   oh, you're in this vast basket of people who are depressed,
[02:00:29.860 --> 02:00:31.540]   so we'll give you an antidepressant,
[02:00:31.540 --> 02:00:34.220]   even though the evidence shows that people
[02:00:34.220 --> 02:00:36.620]   who are suffering from your level of depression
[02:00:36.620 --> 02:00:38.220]   don't get better.
[02:00:38.220 --> 02:00:42.540]   - And that's a consequence of not focusing
[02:00:42.540 --> 02:00:46.060]   on preventative medicine, the lifestyle changes,
[02:00:46.060 --> 02:00:47.140]   all that kind of stuff.
[02:00:47.140 --> 02:00:49.660]   - Well, yes, but it's really a consequence
[02:00:49.660 --> 02:00:53.060]   of the drug companies creating the impression
[02:00:53.060 --> 02:00:54.720]   that if you're sad, take a pill.
[02:00:56.660 --> 02:01:01.140]   - If you're non-major depression,
[02:01:01.140 --> 02:01:03.460]   how do you overcome depression?
[02:01:03.460 --> 02:01:06.700]   - Well, you have to talk about what the problem is.
[02:01:06.700 --> 02:01:09.900]   - So talk therapy, lifestyle changes.
[02:01:09.900 --> 02:01:12.300]   - Well, no, I'm not jumping to that.
[02:01:12.300 --> 02:01:16.140]   I'm saying that you ought to, A,
[02:01:16.140 --> 02:01:19.440]   the way you feel must be respected.
[02:01:19.440 --> 02:01:21.180]   - Yeah, acknowledge that you're suffering.
[02:01:21.180 --> 02:01:22.700]   - Acknowledge that you're suffering
[02:01:22.700 --> 02:01:24.740]   and deal with healthcare providers
[02:01:24.740 --> 02:01:27.260]   who acknowledge that you're suffering.
[02:01:27.260 --> 02:01:30.220]   So let's take that first step.
[02:01:30.220 --> 02:01:32.260]   - Big first step also.
[02:01:32.260 --> 02:01:33.580]   - Big first step, yeah.
[02:01:33.580 --> 02:01:36.260]   Family docs are pretty good at that.
[02:01:36.260 --> 02:01:39.880]   That's kind of the arena that caused me
[02:01:39.880 --> 02:01:41.920]   to go into family medicine,
[02:01:41.920 --> 02:01:44.300]   the subjective experience of the patient.
[02:01:44.300 --> 02:01:48.100]   Okay, so you're a person who is not getting
[02:01:48.100 --> 02:01:49.660]   the enjoyment out of their life
[02:01:49.660 --> 02:01:52.080]   that they feel they ought to be getting.
[02:01:52.080 --> 02:01:54.660]   Now let's figure out why.
[02:01:54.660 --> 02:01:57.300]   And whether that means some time with a social worker,
[02:01:57.300 --> 02:01:59.060]   some time with a psychiatrist,
[02:01:59.060 --> 02:02:01.320]   some time with a psychiatric nurse,
[02:02:01.320 --> 02:02:04.100]   I'm not sure how you'd best do that,
[02:02:04.100 --> 02:02:05.780]   most effectively and efficiently,
[02:02:05.780 --> 02:02:07.500]   but that's what you need to do.
[02:02:07.500 --> 02:02:11.680]   And it may be that there's a marital problem
[02:02:11.680 --> 02:02:13.620]   and there's something going on
[02:02:13.620 --> 02:02:18.580]   and one of the spouses can't find satisfaction
[02:02:18.580 --> 02:02:21.500]   in the life they have to live within the relationship.
[02:02:21.500 --> 02:02:24.620]   Maybe there's a past history of trauma or abuse
[02:02:24.620 --> 02:02:28.800]   that somebody is projecting onto their current situation.
[02:02:28.800 --> 02:02:31.080]   Maybe there's socioeconomic circumstances
[02:02:31.080 --> 02:02:34.740]   where they can't find a job that gives them self-respect
[02:02:34.740 --> 02:02:35.980]   and enough money to live.
[02:02:35.980 --> 02:02:39.700]   An infinite range of things,
[02:02:39.700 --> 02:02:42.080]   but let's figure out, make a diagnosis first.
[02:02:42.080 --> 02:02:45.460]   The diagnosis isn't that the person feels sadder
[02:02:45.460 --> 02:02:48.620]   than they want to feel.
[02:02:48.620 --> 02:02:51.980]   The diagnosis is why does the person feel sadder
[02:02:51.980 --> 02:02:53.140]   than they want to feel?
[02:02:53.140 --> 02:02:57.780]   - You mentioned this is what made you want to get
[02:02:57.780 --> 02:02:59.240]   into family medicine.
[02:02:59.240 --> 02:03:03.100]   As a doctor, what do you think about the saying,
[02:03:03.100 --> 02:03:05.380]   save one life, save the world?
[02:03:05.380 --> 02:03:13.780]   This was always moving to me about doctors
[02:03:13.780 --> 02:03:16.740]   'cause you have this human in front of you
[02:03:17.700 --> 02:03:20.920]   and your time is worth money.
[02:03:20.920 --> 02:03:27.300]   What you prescribe and your efforts after the visit
[02:03:27.300 --> 02:03:31.860]   are worth money and it seems like the task of the doctor
[02:03:31.860 --> 02:03:34.140]   is to not think about any of that.
[02:03:34.140 --> 02:03:42.020]   Not the task, but it seems like a great doctor,
[02:03:42.020 --> 02:03:45.100]   despite all that, just forgets it all
[02:03:45.100 --> 02:03:47.060]   and just cares about the one human
[02:03:47.060 --> 02:03:51.780]   and somehow that feels like the love and effort you put
[02:03:51.780 --> 02:03:53.900]   into helping one person is the thing
[02:03:53.900 --> 02:03:55.460]   that will save the world.
[02:03:55.460 --> 02:03:58.420]   It's not like some economic argument
[02:03:58.420 --> 02:04:03.420]   or some political argument or financial argument.
[02:04:03.420 --> 02:04:09.540]   It's a very human drive that ultimately is behind
[02:04:09.540 --> 02:04:13.020]   all of this that will do good for the world.
[02:04:13.020 --> 02:04:15.620]   - Yes, I think that's true.
[02:04:15.620 --> 02:04:19.660]   And at the same time, I think it's equally true
[02:04:19.660 --> 02:04:23.660]   that all physicians need to have a sense of responsibility
[02:04:23.660 --> 02:04:28.660]   about how the common resources are allocated
[02:04:28.660 --> 02:04:34.140]   to serve the whole population's interest best.
[02:04:34.140 --> 02:04:36.380]   That's a tension that you have as a physician.
[02:04:36.380 --> 02:04:38.460]   Let's take the extreme example.
[02:04:38.460 --> 02:04:41.420]   Let's say you had a patient in front of you
[02:04:41.420 --> 02:04:46.340]   who if you gave a $1, $10 billion pill to,
[02:04:46.340 --> 02:04:47.660]   you would save their life.
[02:04:47.660 --> 02:04:52.380]   I would just be tortured by that as a physician
[02:04:52.380 --> 02:04:56.100]   because I know that $10 billion spent properly
[02:04:56.100 --> 02:05:00.260]   in an epidemiologically guided way
[02:05:00.260 --> 02:05:03.580]   is gonna save a whole lot more lives than one life.
[02:05:03.580 --> 02:05:06.380]   - So it's also your responsibility as a physician
[02:05:06.380 --> 02:05:08.240]   to walk away from that patient.
[02:05:08.240 --> 02:05:10.540]   - I wouldn't say that.
[02:05:10.540 --> 02:05:12.060]   - I think it's your responsibility--
[02:05:12.060 --> 02:05:12.900]   - To be tortured by the choice.
[02:05:12.900 --> 02:05:14.020]   - To be tortured by it.
[02:05:14.020 --> 02:05:15.160]   That's exactly right.
[02:05:15.160 --> 02:05:18.440]   - The human condition.
[02:05:18.440 --> 02:05:24.700]   That's a tough job, but yeah, yeah,
[02:05:24.700 --> 02:05:27.220]   to maintain your humanity through it all.
[02:05:27.220 --> 02:05:30.260]   - Yeah, but you've been asking at different points
[02:05:30.260 --> 02:05:35.260]   in this conversation, why are doctors so complacent
[02:05:35.260 --> 02:05:38.860]   about the tremendous amount of money we're spending?
[02:05:38.860 --> 02:05:41.460]   Why do they accept knowledge from different sources
[02:05:41.460 --> 02:05:44.340]   that may not pan out when they really know the truth?
[02:05:44.340 --> 02:05:48.380]   And the answer is that they're trying to do their best
[02:05:48.380 --> 02:05:49.500]   for their patients.
[02:05:49.500 --> 02:05:55.500]   And it's the same kind of torture
[02:05:55.500 --> 02:05:59.880]   to figure out what the hell is going on with the data.
[02:05:59.880 --> 02:06:03.460]   And that's a sort of future project.
[02:06:03.460 --> 02:06:06.160]   And maybe people will read my book
[02:06:06.160 --> 02:06:08.140]   and maybe they'll get a little more excited about it,
[02:06:08.140 --> 02:06:10.180]   become more legitimate in practice.
[02:06:10.180 --> 02:06:13.660]   I would feel like my life was worthwhile if that happened.
[02:06:13.660 --> 02:06:17.180]   But at the same time, they've got to do something
[02:06:17.180 --> 02:06:18.780]   with the patient in front of them.
[02:06:18.780 --> 02:06:21.140]   They've got to make a decision.
[02:06:21.140 --> 02:06:24.860]   And they probably, there are not many weirdos like me
[02:06:24.860 --> 02:06:27.300]   who invest their life in figuring out
[02:06:27.300 --> 02:06:28.340]   what's behind the data.
[02:06:28.340 --> 02:06:29.820]   They're trying to get through the day
[02:06:29.820 --> 02:06:31.660]   and do the right thing for their patient.
[02:06:31.660 --> 02:06:35.020]   So they're tortured by that decision too.
[02:06:35.020 --> 02:06:38.460]   - And so if you're not careful,
[02:06:38.460 --> 02:06:43.460]   Big Pharma can manipulate that drive
[02:06:43.460 --> 02:06:44.980]   to try to help the patient,
[02:06:44.980 --> 02:06:49.740]   that humanity of dealing with the uncertainty of it all.
[02:06:49.740 --> 02:06:51.980]   Like what is the best thing to do?
[02:06:51.980 --> 02:06:53.800]   Big Pharma can step in and use money
[02:06:53.800 --> 02:06:55.540]   to manipulate that humanity.
[02:06:55.540 --> 02:06:57.540]   - Yeah, I would state it quite differently.
[02:06:57.540 --> 02:07:00.980]   It's sort of an opt out rather than an opt in.
[02:07:00.980 --> 02:07:02.860]   Big Pharma will do that.
[02:07:02.860 --> 02:07:04.460]   And you need to opt out of it.
[02:07:05.460 --> 02:07:08.020]   (inhales)
[02:07:08.020 --> 02:07:11.340]   - What advice would you give to a young person today
[02:07:11.340 --> 02:07:13.180]   in high school or college,
[02:07:13.180 --> 02:07:17.020]   stepping into this complicated world
[02:07:17.020 --> 02:07:22.020]   full of advertisements, of big powerful institutions,
[02:07:22.020 --> 02:07:24.900]   of big rich companies,
[02:07:24.900 --> 02:07:27.100]   how to have a positive impact in the world,
[02:07:27.100 --> 02:07:29.460]   how to live a life they can be proud of?
[02:07:29.460 --> 02:07:33.300]   - I would say, should that person
[02:07:34.300 --> 02:07:38.140]   who has only good motives go into medicine?
[02:07:38.140 --> 02:07:39.780]   They have an inclination to go into medicine
[02:07:39.780 --> 02:07:42.100]   and they've asked me what I think about that
[02:07:42.100 --> 02:07:45.460]   given what I know about the undermining
[02:07:45.460 --> 02:07:47.700]   of American healthcare at this point.
[02:07:47.700 --> 02:07:51.500]   And my answer is, if you got the calling, you should do it.
[02:07:51.500 --> 02:07:54.360]   You should do it because nobody's gonna do it
[02:07:54.360 --> 02:07:55.200]   better than you.
[02:07:55.200 --> 02:07:57.860]   And if you don't have the calling
[02:07:57.860 --> 02:08:01.080]   and you're in it for the money,
[02:08:01.080 --> 02:08:03.300]   you're not gonna be proud of yourself.
[02:08:03.300 --> 02:08:07.740]   - How do you prevent yourself from doing,
[02:08:07.740 --> 02:08:12.740]   from letting the system change you over years and years?
[02:08:12.740 --> 02:08:17.980]   Like letting the game of pharmaceutical influence
[02:08:17.980 --> 02:08:19.820]   affect you?
[02:08:19.820 --> 02:08:22.640]   - It's a very hard question
[02:08:22.640 --> 02:08:27.640]   because the sociologic norms are to be affected
[02:08:28.080 --> 02:08:33.000]   and to trust the sources of information
[02:08:33.000 --> 02:08:36.380]   that are largely controlled by the drug industry.
[02:08:36.380 --> 02:08:38.220]   And that's why I wrote "Sickening"
[02:08:38.220 --> 02:08:43.220]   is to try and help those people in the medical profession
[02:08:43.220 --> 02:08:50.620]   to understand that what's going on right now looks normal,
[02:08:50.620 --> 02:08:52.380]   but it's not.
[02:08:52.380 --> 02:08:55.460]   The health of Americans is going downhill.
[02:08:55.460 --> 02:08:57.060]   Our society is getting ruined
[02:08:57.060 --> 02:09:01.020]   by the money that's getting pulled out of other
[02:09:01.020 --> 02:09:06.660]   socially beneficial uses to pay for healthcare
[02:09:06.660 --> 02:09:08.560]   that is not helping us.
[02:09:08.560 --> 02:09:14.600]   - So fundamentally, the thing that is normal,
[02:09:14.600 --> 02:09:19.120]   not question the normal, don't,
[02:09:19.120 --> 02:09:23.700]   if you conform, conform hesitantly.
[02:09:23.700 --> 02:09:26.140]   - Well, you have to conform.
[02:09:26.140 --> 02:09:28.960]   You can't become a doctor without conforming.
[02:09:28.960 --> 02:09:32.340]   I just made it through.
[02:09:32.340 --> 02:09:35.100]   (both laughing)
[02:09:35.100 --> 02:09:38.680]   But there aren't many and it's hard work,
[02:09:38.680 --> 02:09:42.460]   but you have to conform.
[02:09:42.460 --> 02:09:44.980]   And even with my colleagues in my own practice,
[02:09:44.980 --> 02:09:48.740]   I couldn't convince them that some of the beliefs they had
[02:09:48.740 --> 02:09:51.500]   about how best to practice weren't accurate.
[02:09:51.500 --> 02:09:55.500]   There's one scene, a younger physician
[02:09:55.500 --> 02:09:57.940]   had prescribed hormone replacement therapy,
[02:09:57.940 --> 02:10:00.420]   this is back in 2000, 2001,
[02:10:00.420 --> 02:10:02.660]   had prescribed hormone replacement therapy
[02:10:02.660 --> 02:10:04.340]   for one of my patients
[02:10:04.340 --> 02:10:07.140]   who happened to be a really good personal friend.
[02:10:07.140 --> 02:10:12.140]   And I saw that patient covering for my colleague
[02:10:12.140 --> 02:10:15.660]   at one point and I saw that her hormone replacement therapy
[02:10:15.660 --> 02:10:17.260]   had been renewed.
[02:10:17.260 --> 02:10:19.500]   And I said, "Are you having hot flashes or any problem?"
[02:10:19.500 --> 02:10:23.180]   "No, no, no, no, but Dr. So-and-so
[02:10:23.180 --> 02:10:25.140]   said it's better for my health."
[02:10:25.140 --> 02:10:26.700]   And I said, "No, it's not.
[02:10:26.700 --> 02:10:28.440]   The research is showing that it's not.
[02:10:28.440 --> 02:10:31.300]   It's harmful for your health and I think you should stop it."
[02:10:31.300 --> 02:10:35.820]   So my colleague approached me when she saw the chart
[02:10:35.820 --> 02:10:38.060]   and said, "Wait a minute, that's my patient,
[02:10:38.060 --> 02:10:40.520]   maybe your friend, but it's my patient."
[02:10:40.520 --> 02:10:45.520]   And I went to a conference from my alma mater,
[02:10:45.520 --> 02:10:49.060]   medical school, and they said that healthy people
[02:10:49.060 --> 02:10:51.220]   should be given hormone replacement.
[02:10:51.220 --> 02:10:53.920]   And I said, "There's gotta be drug companies
[02:10:53.920 --> 02:10:55.260]   involved in this."
[02:10:55.260 --> 02:10:57.700]   And she said, "No, no, no, it was at my university.
[02:10:57.700 --> 02:10:59.900]   It was not a drug company thing.
[02:10:59.900 --> 02:11:02.320]   We didn't go to a Caribbean island."
[02:11:02.320 --> 02:11:03.860]   I said, "Do you have the syllabus?"
[02:11:03.860 --> 02:11:05.100]   She said, "Yeah."
[02:11:05.100 --> 02:11:07.640]   And she went and got the syllabus and sure enough,
[02:11:07.640 --> 02:11:09.480]   it was sponsored by a drug company.
[02:11:09.480 --> 02:11:11.300]   - They're everywhere.
[02:11:11.300 --> 02:11:12.140]   - They're everywhere.
[02:11:12.140 --> 02:11:16.300]   And it's back to Kuhn that groups of experts
[02:11:16.300 --> 02:11:19.780]   share unspoken assumptions.
[02:11:19.780 --> 02:11:22.780]   And in order to be included in that group of experts,
[02:11:22.780 --> 02:11:25.140]   you have to share those unspoken assumptions.
[02:11:25.140 --> 02:11:27.860]   And what I'm hoping to do with my book, "Sickening,"
[02:11:27.860 --> 02:11:31.820]   and being here, having this wonderful conversation with you,
[02:11:31.820 --> 02:11:36.180]   is to create an alternative to this normal
[02:11:36.180 --> 02:11:41.180]   that people can pursue and practice better medicine
[02:11:45.020 --> 02:11:47.180]   and also prevent burnout.
[02:11:47.180 --> 02:11:49.320]   I mean, about half the doctors complain
[02:11:49.320 --> 02:11:51.220]   that they're burned out and they've had it.
[02:11:51.220 --> 02:11:54.180]   And I think that this is a subject,
[02:11:54.180 --> 02:11:57.540]   I don't have data on this, this is just my opinion,
[02:11:57.540 --> 02:11:59.860]   but I think that a lot of that burnout
[02:11:59.860 --> 02:12:04.340]   is so-called moral injury from practicing in a way
[02:12:04.340 --> 02:12:08.380]   that the docs know isn't working.
[02:12:08.380 --> 02:12:10.940]   - It's not actually providing an alternative
[02:12:10.940 --> 02:12:12.980]   to the normal, it's expanding the normal,
[02:12:12.980 --> 02:12:14.900]   it's shifting the normal, just like with Kuhn.
[02:12:14.900 --> 02:12:17.000]   I mean, you're basically looking for,
[02:12:17.000 --> 02:12:23.540]   to shift the way medicine is done to the original,
[02:12:23.540 --> 02:12:27.100]   I mean, to the intent that it represents,
[02:12:27.100 --> 02:12:30.580]   that the ideal of medicine, of healthcare.
[02:12:30.580 --> 02:12:33.500]   - Yeah, in Kuhnian terms, to have a revolution.
[02:12:33.500 --> 02:12:36.820]   And that revolution would be to practice medicine
[02:12:36.820 --> 02:12:41.700]   in a way that will be epidemiologically most effective,
[02:12:41.700 --> 02:12:45.300]   not most profitable for the people who are providing you
[02:12:45.300 --> 02:12:47.340]   with what's called knowledge.
[02:12:47.340 --> 02:12:51.360]   - You helped a lot of people as a doctor, as an educator,
[02:12:51.360 --> 02:12:56.500]   live better lives, live longer,
[02:12:56.500 --> 02:12:59.140]   but you yourself are a mortal being.
[02:12:59.140 --> 02:13:02.220]   Do you think about your own mortality?
[02:13:02.220 --> 02:13:03.580]   Do you think about your death?
[02:13:03.580 --> 02:13:04.780]   Are you afraid of death?
[02:13:04.780 --> 02:13:07.600]   - I'm not, I've faced it.
[02:13:10.580 --> 02:13:11.860]   I've been close.
[02:13:11.860 --> 02:13:12.740]   - With yourself?
[02:13:12.740 --> 02:13:14.940]   - Yeah, yeah.
[02:13:14.940 --> 02:13:16.180]   - How do you think about it?
[02:13:16.180 --> 02:13:20.060]   What wisdom do you gain from having come close to death,
[02:13:20.060 --> 02:13:21.820]   the fact that the whole thing ends?
[02:13:21.820 --> 02:13:24.040]   - It's liberating.
[02:13:24.040 --> 02:13:26.580]   It's very liberating.
[02:13:26.580 --> 02:13:30.860]   I mean, I'm serious, I was close, and not too long ago.
[02:13:30.860 --> 02:13:39.020]   And it was a sense of, you know,
[02:13:39.540 --> 02:13:44.540]   this may be the way it ends, and I've done my best.
[02:13:44.540 --> 02:13:47.040]   It's not been perfect.
[02:13:47.040 --> 02:13:51.220]   And if it ends here, it ends here.
[02:13:51.220 --> 02:13:54.460]   The people around me are trying to do their best.
[02:13:54.460 --> 02:13:57.900]   And in fact, I got pulled out of it,
[02:13:57.900 --> 02:14:01.500]   but it didn't look like I was gonna get pulled out of it.
[02:14:01.500 --> 02:14:06.180]   - Are you ultimately grateful for the ride,
[02:14:06.180 --> 02:14:07.420]   even though it ends?
[02:14:07.420 --> 02:14:12.420]   - Well, it's a little, I think so.
[02:14:12.420 --> 02:14:15.660]   If I know, you know, you can't take the ride
[02:14:15.660 --> 02:14:17.580]   if you know it's gonna end well.
[02:14:17.580 --> 02:14:18.660]   (both laughing)
[02:14:18.660 --> 02:14:20.980]   It's not the real ride, it's just a ride.
[02:14:20.980 --> 02:14:25.180]   But I, having gone through the whole thing,
[02:14:25.180 --> 02:14:30.180]   I definitely freed me of a sense of anxiety about death.
[02:14:30.180 --> 02:14:35.500]   And it said to me, "Do your best every day,
[02:14:35.500 --> 02:14:37.340]   "'cause it's gonna end sometime."
[02:14:37.340 --> 02:14:40.780]   - I apologize for the ridiculously big question,
[02:14:40.780 --> 02:14:45.700]   but what do you think is the meaning of life,
[02:14:45.700 --> 02:14:47.560]   of our human existence?
[02:14:47.560 --> 02:14:54.620]   - I think it's to care about something
[02:14:54.620 --> 02:14:55.860]   and do your best with it.
[02:14:55.860 --> 02:15:01.540]   Whether it's being a doctor and trying to make sure
[02:15:01.540 --> 02:15:04.540]   that the greatest number of people get the best healthcare,
[02:15:05.340 --> 02:15:08.140]   or it's a gardener who wants to have
[02:15:08.140 --> 02:15:10.420]   the most beautiful plants, or it's a grandparent
[02:15:10.420 --> 02:15:12.780]   who wants to have a good relationship
[02:15:12.780 --> 02:15:13.700]   with their grandchildren.
[02:15:13.700 --> 02:15:18.700]   But whatever it is that gives you a sense of meaning,
[02:15:18.700 --> 02:15:21.860]   as long as it doesn't hurt other people,
[02:15:21.860 --> 02:15:24.980]   to really commit yourself to it.
[02:15:24.980 --> 02:15:27.460]   That commitment, being in that commitment,
[02:15:27.460 --> 02:15:29.860]   for me, is the meaning of life.
[02:15:29.860 --> 02:15:32.980]   - Put your whole heart and soul into the thing.
[02:15:32.980 --> 02:15:33.820]   - Yep.
[02:15:34.500 --> 02:15:37.620]   - What is it, the Bukowski poem, "Go All the Way."
[02:15:37.620 --> 02:15:41.460]   John, you're an incredible human being,
[02:15:41.460 --> 02:15:42.460]   incredible educator.
[02:15:42.460 --> 02:15:45.020]   Like I said, I recommend people listen to your lectures.
[02:15:45.020 --> 02:15:48.380]   It's so refreshing to see that clarity of thought
[02:15:48.380 --> 02:15:49.300]   and brilliance.
[02:15:49.300 --> 02:15:51.860]   And obviously, your criticism of Big Pharma,
[02:15:51.860 --> 02:15:56.420]   or your illumination of the mechanisms of Big Pharma
[02:15:56.420 --> 02:15:58.380]   is really important at this time.
[02:15:58.380 --> 02:16:02.260]   So I really hope people read your book,
[02:16:02.260 --> 02:16:03.900]   "Sickening," that's out today,
[02:16:03.900 --> 02:16:05.740]   or depending on when this comes out.
[02:16:05.740 --> 02:16:11.100]   Thank you so much for spending your extremely valuable time
[02:16:11.100 --> 02:16:12.260]   with me today.
[02:16:12.260 --> 02:16:13.100]   It was amazing.
[02:16:13.100 --> 02:16:15.500]   - Well, Lex, I wanna back to you.
[02:16:15.500 --> 02:16:18.700]   Thanks for engaging in this conversation,
[02:16:18.700 --> 02:16:21.420]   for creating the space to have it,
[02:16:21.420 --> 02:16:25.140]   and creating a listenership that is interested
[02:16:25.140 --> 02:16:27.420]   in understanding serious ideas.
[02:16:27.420 --> 02:16:29.500]   And I really appreciate the conversation.
[02:16:29.500 --> 02:16:30.980]   - And I should mention that offline,
[02:16:30.980 --> 02:16:34.060]   you told me you listened to the Gilbert Strang episode.
[02:16:34.060 --> 02:16:35.980]   So for anyone who don't know Gilbert Strang,
[02:16:35.980 --> 02:16:39.020]   another epic human being that you should check out.
[02:16:39.020 --> 02:16:41.300]   If you don't know anything about mathematics
[02:16:41.300 --> 02:16:43.260]   or linear algebra, go look him up.
[02:16:43.260 --> 02:16:47.300]   He's one of the great mathematics educators of all time.
[02:16:47.300 --> 02:16:49.060]   Of all the people you mentioned to me,
[02:16:49.060 --> 02:16:50.820]   I appreciate that you mentioned him,
[02:16:50.820 --> 02:16:54.100]   'cause he is a rockstar of mathematics.
[02:16:54.100 --> 02:16:55.380]   John, thank you so much for talking to me.
[02:16:55.380 --> 02:16:56.220]   This was awesome.
[02:16:56.220 --> 02:16:57.900]   - Great, thank you.
[02:16:57.900 --> 02:16:59.340]   - Thanks for listening to this conversation
[02:16:59.340 --> 02:17:00.780]   with John Abramson.
[02:17:00.780 --> 02:17:02.140]   To support this podcast,
[02:17:02.140 --> 02:17:04.980]   please check out our sponsors in the description.
[02:17:04.980 --> 02:17:08.800]   And now, let me leave you some words from Marcus Aurelius.
[02:17:08.800 --> 02:17:14.740]   Waste no time arguing about what a good man should be.
[02:17:14.740 --> 02:17:15.620]   Be one.
[02:17:15.620 --> 02:17:19.860]   Thank you for listening, and hope to see you next time.
[02:17:19.860 --> 02:17:22.440]   (upbeat music)
[02:17:22.440 --> 02:17:25.020]   (upbeat music)
[02:17:25.020 --> 02:17:35.020]   [BLANK_AUDIO]

