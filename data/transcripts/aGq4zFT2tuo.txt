
[00:00:00.000 --> 00:00:07.000]   If you take those big models, you're running into the problem like you need already compute
[00:00:07.000 --> 00:00:11.700]   power, you need infrastructure, you need ML ops, you needed a whole department to actually
[00:00:11.700 --> 00:00:13.560]   make use of those models.
[00:00:13.560 --> 00:00:15.340]   Not many people have that, right?
[00:00:15.340 --> 00:00:18.500]   Especially those companies that it's most useful for.
[00:00:18.500 --> 00:00:22.880]   You're listening to Gradient Dissent, a show about machine learning in the real world,
[00:00:22.880 --> 00:00:25.080]   and I'm your host, Lukas Biewald.
[00:00:25.080 --> 00:00:28.060]   Today I'm talking to Johannes Aderbach.
[00:00:28.060 --> 00:00:32.380]   He was originally a quantum physicist and then went into machine learning, and he's
[00:00:32.380 --> 00:00:37.320]   currently VP of machine learning at Merantix Momentum.
[00:00:37.320 --> 00:00:42.360]   Merantix is a really interesting company that develops all sorts of machine learning applications
[00:00:42.360 --> 00:00:47.380]   for customers, small and large, and then really deploys them into these real world systems
[00:00:47.380 --> 00:00:48.380]   and hands them off.
[00:00:48.380 --> 00:00:53.600]   So we really get into real world applications of ML and factories and other places, the
[00:00:53.600 --> 00:00:58.260]   tooling required to make machine learning work, and also how to do smooth handoffs so
[00:00:58.260 --> 00:01:01.460]   that customers are actually successful in that transition.
[00:01:01.460 --> 00:01:04.580]   This is a really interesting conversation and I hope you enjoy it.
[00:01:04.580 --> 00:01:09.220]   My first question for you is looking at your resume, you're one in the long line of people
[00:01:09.220 --> 00:01:13.780]   that kind of moved from physics into machine learning.
[00:01:13.780 --> 00:01:18.580]   I'd love to hear what that journey was like, studying quantum physics.
[00:01:18.580 --> 00:01:22.860]   I think you worked on a little bit of quantum engineering or quantum computing, right?
[00:01:22.860 --> 00:01:25.600]   And then now you do machine learning.
[00:01:25.600 --> 00:01:26.600]   How did that happen?
[00:01:26.600 --> 00:01:29.540]   That's a great question.
[00:01:29.540 --> 00:01:34.920]   I think initially I was super excited about physics because physics, I just saw as something
[00:01:34.920 --> 00:01:35.920]   to understand the world.
[00:01:35.920 --> 00:01:41.220]   And I'm really excited about understanding how things work and taking things apart to
[00:01:41.220 --> 00:01:42.620]   put it back together.
[00:01:42.620 --> 00:01:45.860]   That's always drawn me to physics rather than engineering.
[00:01:45.860 --> 00:01:49.140]   And I was on track to just do a career in physics.
[00:01:49.140 --> 00:01:51.660]   And then AlexNet came out and the ImageNet challenge happened.
[00:01:51.660 --> 00:01:55.380]   And I'm like, holy crap, there is something really cool happening.
[00:01:55.380 --> 00:01:59.700]   And it's always funny to tell people, I did my PhD before ImageNet was a thing because
[00:01:59.700 --> 00:02:03.100]   that makes me really old, but it's kind of an exciting time.
[00:02:03.100 --> 00:02:07.740]   And so when I heard that, I was like, well, I want to reconsider my career as a physicist
[00:02:07.740 --> 00:02:09.420]   anyway at that point.
[00:02:09.420 --> 00:02:14.980]   And looked into what this AlexNet was about and the ImageNet challenge and discovered
[00:02:14.980 --> 00:02:19.780]   this whole field of data science and big data that was starting off at that time.
[00:02:19.780 --> 00:02:23.340]   And that's a very natural transition for a physicist because we're good at statistics,
[00:02:23.340 --> 00:02:26.020]   we're good at modeling, we like math.
[00:02:26.020 --> 00:02:28.900]   And then I fell in love with this big data, data science.
[00:02:28.900 --> 00:02:35.940]   And since then I've been continuously driving at understanding the language of data and
[00:02:35.940 --> 00:02:38.860]   ML is just like an expression of that language.
[00:02:38.860 --> 00:02:42.500]   And that's why I fell in love with it and now I'm here.
[00:02:42.500 --> 00:02:47.620]   And you did do some work in quantum computing, is that right?
[00:02:47.620 --> 00:02:52.980]   Do you think that quantum computing has anything to apply to ML or do you think ML has anything
[00:02:52.980 --> 00:02:55.140]   to apply to quantum computing?
[00:02:55.140 --> 00:02:56.140]   How do you think about that?
[00:02:56.140 --> 00:02:58.340]   That's a great question.
[00:02:58.340 --> 00:03:02.980]   I think it actually, it's mutually beneficial and I see there will be a convergence of those
[00:03:02.980 --> 00:03:06.500]   two fields in the near future.
[00:03:06.500 --> 00:03:09.380]   There's four different quadrants that we can talk about.
[00:03:09.380 --> 00:03:14.980]   We have classical and quantum in terms of engineering and in terms of data.
[00:03:14.980 --> 00:03:18.580]   So you have quantum data and classical data and you have quantum algorithms and classical
[00:03:18.580 --> 00:03:19.580]   algorithms.
[00:03:19.580 --> 00:03:23.220]   And so you can actually start to think in those four quadrants.
[00:03:23.220 --> 00:03:28.980]   And I think that right now we see that a lot of effort is being put into using quantum
[00:03:28.980 --> 00:03:31.660]   algorithms to classical data.
[00:03:31.660 --> 00:03:34.540]   And that I think is actually potentially the wrong way to think about it.
[00:03:34.540 --> 00:03:40.420]   We should always think about like quantum algorithms for quantum data and maybe classical
[00:03:40.420 --> 00:03:41.940]   algorithms for classical data.
[00:03:41.940 --> 00:03:44.340]   And these cross fields are a little bit more complicated to solve.
[00:03:44.340 --> 00:03:48.660]   And there I think is like cross fertilization is going to be happening.
[00:03:48.660 --> 00:03:51.860]   What is quantum data?
[00:03:51.860 --> 00:03:55.860]   Quantum data is essentially data that comes out of quantum states.
[00:03:55.860 --> 00:04:01.020]   I don't know how deep you are into quantum computing, but typically in quantum computing,
[00:04:01.020 --> 00:04:06.540]   we don't talk about definite outcomes in a way, but we're describing systems by way functions,
[00:04:06.540 --> 00:04:12.300]   which are naively speaking the square root of probabilities, quote unquote.
[00:04:12.300 --> 00:04:13.820]   Don't take this too serious.
[00:04:13.820 --> 00:04:19.660]   And so what you get with this is essentially expressions through quantum data, which has
[00:04:19.660 --> 00:04:21.700]   a face and an amplitude.
[00:04:21.700 --> 00:04:24.780]   If you start measuring this, you get a lot of complex numbers, you get various different
[00:04:24.780 --> 00:04:27.220]   types of phenomena.
[00:04:27.220 --> 00:04:31.860]   And those data typically take an exponential time to read out into classical states.
[00:04:31.860 --> 00:04:34.940]   When you have a quantum state and you want to completely express that quantum state in
[00:04:34.940 --> 00:04:39.620]   classical data, you get an exponential overhead in storage.
[00:04:39.620 --> 00:04:43.300]   But what's the situation in the real world where I would have quantum data?
[00:04:43.300 --> 00:04:49.560]   I can imagine how these quantum computers produce that, but when would I be collecting
[00:04:49.560 --> 00:04:51.540]   quantum data?
[00:04:51.540 --> 00:04:53.220]   When you actually deal with quantum systems, right?
[00:04:53.220 --> 00:04:58.260]   If you want to start to understand molecules, for example, very deep interactions of molecular
[00:04:58.260 --> 00:05:02.740]   properties, they are ruled by quantum rules.
[00:05:02.740 --> 00:05:08.300]   And so if you want to simulate molecules, you rather want to do it in quantum space
[00:05:08.300 --> 00:05:10.060]   than in classical space.
[00:05:10.060 --> 00:05:12.060]   That's really the way to go.
[00:05:12.060 --> 00:05:17.460]   So that's why modern or early stage today's quantum computers are more simulators of other
[00:05:17.460 --> 00:05:18.460]   quantum systems.
[00:05:18.460 --> 00:05:22.900]   So you use these computers to simulate quantum systems that you want to study on a very,
[00:05:22.900 --> 00:05:24.540]   very controlled fashions.
[00:05:24.540 --> 00:05:27.100]   And then you deal with quantum data at that point.
[00:05:27.100 --> 00:05:30.860]   And are we actually able to simulate quantum systems in a useful way?
[00:05:30.860 --> 00:05:36.780]   Because I have experience with classical mechanics systems and the simulations seem to break
[00:05:36.780 --> 00:05:37.780]   down very quickly.
[00:05:37.780 --> 00:05:43.820]   So I can only imagine that the quantum simulations are much harder and probably harder to make
[00:05:43.820 --> 00:05:44.820]   accurate.
[00:05:44.820 --> 00:05:47.260]   We are getting really good results.
[00:05:47.260 --> 00:05:50.580]   And a lot of quantum experimental physics is essentially doing that.
[00:05:50.580 --> 00:05:55.300]   We have toy models that we use in order to validate our mathematical theories.
[00:05:55.300 --> 00:06:03.300]   A good example is a field that I worked in back in the past, which is quantum optics,
[00:06:03.300 --> 00:06:07.600]   where we have a lot of laser fields and single atoms.
[00:06:07.600 --> 00:06:13.020]   And we start to put them together in a certain fashion in these laser fields so that we can
[00:06:13.020 --> 00:06:17.020]   simulate materials that we really have hard time understanding.
[00:06:17.020 --> 00:06:19.500]   Like for example, high temperature superconductivity.
[00:06:19.500 --> 00:06:24.260]   We have certain types of mathematical models, statistical models that we think about how
[00:06:24.260 --> 00:06:27.100]   these things can come across or can come about.
[00:06:27.100 --> 00:06:31.580]   And then in order to study the effects of these models, we use a very clean system that
[00:06:31.580 --> 00:06:36.300]   we have a high level of control for and try to simulate those mathematical models and
[00:06:36.300 --> 00:06:41.380]   see if those models then give rise to these phenomena that we see, for example, in these
[00:06:41.380 --> 00:06:44.120]   materials that have high temperature superconductivity.
[00:06:44.120 --> 00:06:48.300]   So we use a much simpler system to simulate a much more complex system in order to probe
[00:06:48.300 --> 00:06:52.060]   our understanding of the physical laws here in this case.
[00:06:52.060 --> 00:06:54.500]   Is there applications of ML to that?
[00:06:54.500 --> 00:06:59.140]   I feel like we've talked on this show to some chemists in different fields and they've been
[00:06:59.140 --> 00:07:03.700]   sort of using ML maybe to approximate these kinds of interactions.
[00:07:03.700 --> 00:07:06.060]   Is that an interesting field to you?
[00:07:06.060 --> 00:07:12.060]   I think that's an interesting field to me, but actually I think I'm much more excited
[00:07:12.060 --> 00:07:16.200]   about a completely different avenue of applying ML to quantum systems.
[00:07:16.200 --> 00:07:20.220]   If you think about building a quantum computer, you have a lot of different qubits.
[00:07:20.220 --> 00:07:21.380]   These are like the atomic units.
[00:07:21.380 --> 00:07:26.660]   You have bits in a computer, a classical computer, you have qubits in a quantum computer.
[00:07:26.660 --> 00:07:32.740]   To use and address these qubits, we have to very, very meticulously control those qubits
[00:07:32.740 --> 00:07:36.500]   in order to really make them do what we want because you cannot just flip a switch from
[00:07:36.500 --> 00:07:39.660]   zero to one, but you have to control everything between zero and one.
[00:07:39.660 --> 00:07:44.180]   It's a very, very analogous computer, like an analog computer to a certain extent.
[00:07:44.180 --> 00:07:48.980]   And in order to control these kind of systems, I think here is where ML comes into play because
[00:07:48.980 --> 00:07:54.420]   you can use, for example, reinforcement learning techniques to do optimal control of these
[00:07:54.420 --> 00:07:59.180]   quantum gates in order to facilitate those two qubit interactions or three qubit interactions
[00:07:59.180 --> 00:08:01.700]   in order to get a high fidelity quantum computer.
[00:08:01.700 --> 00:08:06.620]   And I think that might be one of the early applications of ML to quantum systems and
[00:08:06.620 --> 00:08:07.620]   to quantum computers.
[00:08:07.620 --> 00:08:12.820]   And my firm belief is that we probably need machine learning techniques, modern machine
[00:08:12.820 --> 00:08:17.140]   learning techniques in order to scale quantum computers to the sizes that they are actually
[00:08:17.140 --> 00:08:18.140]   useful.
[00:08:18.140 --> 00:08:19.140]   Interesting.
[00:08:19.140 --> 00:08:23.580]   I feel like I've met a number of people in machine learning that feel like they're refugees
[00:08:23.580 --> 00:08:24.580]   from quantum computing.
[00:08:24.580 --> 00:08:30.260]   They felt like it didn't really have a path to real world applications and moved into
[00:08:30.260 --> 00:08:31.260]   machine learning.
[00:08:31.260 --> 00:08:35.300]   When I saw your resume, I wondered if you were one of those people, but it sounds like
[00:08:35.300 --> 00:08:38.740]   you're pretty optimistic about the future of quantum computing.
[00:08:38.740 --> 00:08:39.740]   Yeah.
[00:08:39.740 --> 00:08:43.860]   I think that the question is on which timescale, right?
[00:08:43.860 --> 00:08:47.860]   Quantum computer is still very nascent and I feel that quantum computing will go through
[00:08:47.860 --> 00:08:52.140]   the same kind of winters that machine learning went through a while.
[00:08:52.140 --> 00:08:56.180]   When this will happen, I don't know, but we will see these kinds of winters coming out.
[00:08:56.180 --> 00:09:00.740]   I, in my lifetime, want to see some more impact on a shorter term timescale.
[00:09:00.740 --> 00:09:03.420]   And I think that machine learning is the right path for that.
[00:09:03.420 --> 00:09:05.700]   And I actually don't think that I shut the door.
[00:09:05.700 --> 00:09:10.100]   At some point I want to do a bit of quantum computing again, but maybe take my ML knowledge
[00:09:10.100 --> 00:09:16.020]   to quantum systems in order to facilitate some better approaches to do that.
[00:09:16.020 --> 00:09:21.180]   But right now, quantum computing is very much at the hardware level and I'm a software guy.
[00:09:21.180 --> 00:09:22.180]   Cool.
[00:09:22.180 --> 00:09:24.180]   Well, tell me about your work at Merantix.
[00:09:24.180 --> 00:09:28.940]   Maybe we can start with what Merantix is and what you work on there.
[00:09:28.940 --> 00:09:29.940]   Yeah, sure.
[00:09:29.940 --> 00:09:33.900]   So Merantix is a super cool construct, actually.
[00:09:33.900 --> 00:09:36.860]   We have like two separate units where we have Merantix Momentum.
[00:09:36.860 --> 00:09:40.540]   We have Merantix Studio, which is the overarching company.
[00:09:40.540 --> 00:09:47.740]   Merantix Studio is actually a venture studio that focuses on deep tech in Berlin.
[00:09:47.740 --> 00:09:53.340]   The idea here is that we have like pre-vetted industry cases where we then look for what
[00:09:53.340 --> 00:09:57.900]   we call entrepreneurs in residence that want to work on certain critical domains that we
[00:09:57.900 --> 00:10:05.980]   deem necessary in order to bring AI into broad adoption outside of just B2C businesses.
[00:10:05.980 --> 00:10:12.100]   And the venture studio looks at those different use cases, then starts to seat an entrepreneur
[00:10:12.100 --> 00:10:17.900]   in residence, lets them have like six months to a year of vetting the use case and then
[00:10:17.900 --> 00:10:19.700]   build up their venture.
[00:10:19.700 --> 00:10:23.340]   So Merantix Momentum is one of these special ventures because we are actually not an independent
[00:10:23.340 --> 00:10:24.340]   venture.
[00:10:24.340 --> 00:10:27.660]   We are 100% subsidiary of Merantix Studio.
[00:10:27.660 --> 00:10:32.860]   And we are focusing on these use cases where it's not big enough to actually build a venture
[00:10:32.860 --> 00:10:41.380]   by itself, but actually still need help for certain domains where we try to focus on use
[00:10:41.380 --> 00:10:48.740]   cases of clients that have actual problems to see how can we actually apply ML techniques
[00:10:48.740 --> 00:10:52.700]   and ML deployment techniques and ML ops to help those customers in need.
[00:10:52.700 --> 00:10:57.420]   Classic example are, for example, visual quality control manufacturers.
[00:10:57.420 --> 00:11:05.340]   They have no IT stack, they have no IT system, but they have very hard visual quality control
[00:11:05.340 --> 00:11:06.340]   problems.
[00:11:06.340 --> 00:11:11.880]   So building a vision classifier based on a convolutional network just offers itself.
[00:11:11.880 --> 00:11:14.860]   We built that for them, make sure that it's actually scalable and then also help them
[00:11:14.860 --> 00:11:17.780]   put it into production close to the sensors.
[00:11:17.780 --> 00:11:22.140]   You can't build an own venture around it, but Merantix Momentum can actually do it.
[00:11:22.140 --> 00:11:23.140]   And that's what we're here for.
[00:11:23.140 --> 00:11:24.140]   And so within that ecosystem...
[00:11:24.140 --> 00:11:27.500]   Well, I guess, why do you think you can't build a venture around that?
[00:11:27.500 --> 00:11:31.620]   I mean, it seems like that'd be pretty useful to a lot of people.
[00:11:31.620 --> 00:11:36.940]   I think the question is how quick do you gain significant market cap?
[00:11:36.940 --> 00:11:40.180]   I think eventually you can build a venture around this, but I think the adoption is not
[00:11:40.180 --> 00:11:42.700]   big enough yet in order to build your own venture around it.
[00:11:42.700 --> 00:11:48.220]   And in a way, Merantix Momentum is the venture that can actually do that because we are in
[00:11:48.220 --> 00:11:53.220]   that sense, we are a professional services department where we go in and say, "Hey, you
[00:11:53.220 --> 00:11:54.220]   have a problem.
[00:11:54.220 --> 00:11:56.580]   You want to have a one-off machine learning model.
[00:11:56.580 --> 00:11:58.820]   We can help you get there."
[00:11:58.820 --> 00:11:59.820]   And that's what we're doing.
[00:11:59.820 --> 00:12:03.580]   And so that's kind of the venture around that, but you wouldn't build a venture to just go
[00:12:03.580 --> 00:12:08.140]   out and do visual quality control for company X, Y, or Z.
[00:12:08.140 --> 00:12:09.140]   So how does it work?
[00:12:09.140 --> 00:12:13.620]   I mean, I would think that doing this kind of thing for customers would be very hard
[00:12:13.620 --> 00:12:14.700]   to scope, right?
[00:12:14.700 --> 00:12:17.500]   Because I feel like one of the challenges of machine learning is you don't really know
[00:12:17.500 --> 00:12:21.600]   in advance how well a particular application is going to work.
[00:12:21.600 --> 00:12:26.420]   And then downstream from that, it'd probably be hard for customers to estimate how well
[00:12:26.420 --> 00:12:30.260]   different levels of quality of the model would really impact their business.
[00:12:30.260 --> 00:12:34.460]   So how do you make sure that a company is going to be happy at the end of one of these
[00:12:34.460 --> 00:12:39.220]   engagements, or do you just view it as sort of an experiment?
[00:12:39.220 --> 00:12:40.860]   That's a really great question.
[00:12:40.860 --> 00:12:42.860]   And I think that we are getting some tractions on that.
[00:12:42.860 --> 00:12:48.300]   So the key here is to work early with customers to understand their needs.
[00:12:48.300 --> 00:12:52.840]   We really have very intense engagements before we start our work to make sure, is the use
[00:12:52.840 --> 00:12:55.740]   case actually solvable?
[00:12:55.740 --> 00:12:57.060]   How big is the challenge?
[00:12:57.060 --> 00:12:59.180]   What kind of data challenges do we meet?
[00:12:59.180 --> 00:13:02.540]   Which kind of approaches would we actually take and really take the customer on a journey
[00:13:02.540 --> 00:13:05.740]   before we really say, "Now we start engaging."
[00:13:05.740 --> 00:13:10.420]   And the way that we approach this is like a staged approach where we have more individual
[00:13:10.420 --> 00:13:15.700]   workshops, which we call the AI Hub, which is a pre-study to an actual work engagement
[00:13:15.700 --> 00:13:17.140]   implementation engagement.
[00:13:17.140 --> 00:13:21.620]   So that the customer understands what can be achieved with which data, with which kind
[00:13:21.620 --> 00:13:22.620]   of effort.
[00:13:22.620 --> 00:13:24.300]   And then we start the implementation work.
[00:13:24.300 --> 00:13:28.180]   And then when implementation work comes, of course, it's a professional services, there's
[00:13:28.180 --> 00:13:33.660]   always a little bit of security and risk, but we already mitigated the risk significantly.
[00:13:33.660 --> 00:13:36.340]   And often it comes out that some problems are not solvable.
[00:13:36.340 --> 00:13:40.900]   And then we go to a different type of model, which I'm actually working at.
[00:13:40.900 --> 00:13:41.900]   What type of model is that?
[00:13:41.900 --> 00:13:42.900]   You want that unsolvable problems?
[00:13:42.900 --> 00:13:43.900]   Is that what I just heard you say?
[00:13:43.900 --> 00:13:51.100]   No, no, not unsolvable problems, but problems that you cannot just do in a client engagement.
[00:13:51.100 --> 00:13:55.040]   There's a different funding strategy that also exists in the US to a certain extent,
[00:13:55.040 --> 00:14:00.140]   but much more so in Germany and Europe, which is publicly funded research projects.
[00:14:00.140 --> 00:14:03.700]   The German state or the federal government is interested in solving certain types of
[00:14:03.700 --> 00:14:05.880]   problems that are industry spanning.
[00:14:05.880 --> 00:14:09.740]   But they're too hard for just a single company to just work on it because you have to bring
[00:14:09.740 --> 00:14:12.640]   many, many different domain experts together.
[00:14:12.640 --> 00:14:17.340]   And so they fund consortial research, which is typically like four to 10 partners, but
[00:14:17.340 --> 00:14:22.620]   you have application partners that bring their challenges, problems, and data sets with them.
[00:14:22.620 --> 00:14:28.000]   Then you have academic partners that bring in academic state of the art research facilities.
[00:14:28.000 --> 00:14:32.140]   And then you also have professional service company like us who really understand deployment
[00:14:32.140 --> 00:14:36.820]   models, deep tech industry applications, how do you make machine learning models robust
[00:14:36.820 --> 00:14:37.820]   in that.
[00:14:37.820 --> 00:14:43.980]   And you engage in translational transfer research to use the academic results to apply to industry
[00:14:43.980 --> 00:14:44.980]   problems.
[00:14:44.980 --> 00:14:48.420]   Once you solve that, then you have enough data to actually then bring it to a client
[00:14:48.420 --> 00:14:51.300]   engagement in a B2B relationship.
[00:14:51.300 --> 00:14:54.300]   Can you talk about some of the things you're working on specifically?
[00:14:54.300 --> 00:14:55.300]   Specifically?
[00:14:55.300 --> 00:15:01.460]   Yeah, we have a bunch of research projects that are going on with big manufacturers and
[00:15:01.460 --> 00:15:03.020]   automotives in Germany.
[00:15:03.020 --> 00:15:09.660]   We just are about to finish a project on self-driving cars, autonomous vehicles.
[00:15:09.660 --> 00:15:11.740]   Very classic use case for Germany, I would say.
[00:15:11.740 --> 00:15:18.660]   And here the idea really is that car manufacturers do not really understand all the details that
[00:15:18.660 --> 00:15:25.500]   are involved in building a, for example, segmentation map for optical flow application, but they
[00:15:25.500 --> 00:15:31.140]   are very, very good in understanding functional safety regards.
[00:15:31.140 --> 00:15:36.180]   And so really bringing those two domains together of saying, we need self-driving cars, autonomous
[00:15:36.180 --> 00:15:40.180]   vehicles, but we don't know how to build the segmentation models.
[00:15:40.180 --> 00:15:41.380]   We need this domain expertise.
[00:15:41.380 --> 00:15:45.060]   And we said, we know how to build those segmentation models, but we don't know actually what are
[00:15:45.060 --> 00:15:47.940]   the safety critical features and how do we bring those together?
[00:15:47.940 --> 00:15:50.820]   And that was a research project that we worked at.
[00:15:50.820 --> 00:15:51.820]   That's cool.
[00:15:51.820 --> 00:15:56.380]   So you're doing segmentation on vision basically from vehicles?
[00:15:56.380 --> 00:15:57.380]   Yep.
[00:15:57.380 --> 00:15:59.540]   So there's computer vision is one of them.
[00:15:59.540 --> 00:16:05.240]   We were investigating synthetic data sets where you have essentially a rendered data
[00:16:05.240 --> 00:16:08.940]   set in order to pre-train those models.
[00:16:08.940 --> 00:16:12.740]   Optical flow detection, bounding box detection, person detection.
[00:16:12.740 --> 00:16:16.340]   These are some classic models.
[00:16:16.340 --> 00:16:21.740]   We also have other research projects that are much more going into optimization problems
[00:16:21.740 --> 00:16:26.500]   where you need to understand how manufacturing pipelines actually look like.
[00:16:26.500 --> 00:16:27.500]   Cool example.
[00:16:27.500 --> 00:16:33.740]   I unfortunately cannot name the company name, but imagine you have a critical element for
[00:16:33.740 --> 00:16:36.140]   building a car seat.
[00:16:36.140 --> 00:16:37.620]   There's metal bars.
[00:16:37.620 --> 00:16:42.900]   These metal bars, they are funnily enough going through 50 different manufacturing steps.
[00:16:42.900 --> 00:16:45.780]   It sounds crazy, but it's actually true.
[00:16:45.780 --> 00:16:51.020]   Those 50 manufacturing steps are distributed over 10 different factories of five different
[00:16:51.020 --> 00:16:52.020]   just-in-time partners.
[00:16:52.020 --> 00:16:53.020]   Wow.
[00:16:53.020 --> 00:16:55.300]   Can you give me some examples of what these steps might be?
[00:16:55.300 --> 00:16:58.980]   It's hard to picture 50 steps in a metal bar.
[00:16:58.980 --> 00:17:05.100]   It's like the raw metal forming to the raw rot, then the first processing to bring it
[00:17:05.100 --> 00:17:10.500]   to the right rot, then you do a chroming of the rot, then you start the first bending
[00:17:10.500 --> 00:17:14.820]   iteration, then you re-chrome, re-finish, do the second bending, do the next step, and
[00:17:14.820 --> 00:17:16.300]   so on until it's in the right shape.
[00:17:16.300 --> 00:17:17.300]   Wow, amazing.
[00:17:17.300 --> 00:17:18.700]   There's a lot of these steps.
[00:17:18.700 --> 00:17:20.540]   I didn't know about that either.
[00:17:20.540 --> 00:17:23.220]   It's pretty crazy.
[00:17:23.220 --> 00:17:29.020]   What happens now is that in your manufacturing process, a mistake happens at step number
[00:17:29.020 --> 00:17:30.380]   10.
[00:17:30.380 --> 00:17:36.300]   You don't notice that mistake until step number 15 when your metal bar is a little bit outside
[00:17:36.300 --> 00:17:37.300]   of specifications.
[00:17:37.300 --> 00:17:42.100]   Typically, what happens is that now you take this whole batch and you put it to scrap metal
[00:17:42.100 --> 00:17:43.580]   and start from scratch.
[00:17:43.580 --> 00:17:50.420]   However, the challenge now is can you do something in step number 20, maybe, that you can bring
[00:17:50.420 --> 00:17:55.820]   that rot back into specifications so that at process step 30, 40, 50, it fits again
[00:17:55.820 --> 00:17:56.820]   back into specifications.
[00:17:56.820 --> 00:18:03.740]   Now, you can imagine this is a very high-dimensional optimization problem with a very sparse reward
[00:18:03.740 --> 00:18:07.380]   signal, so classic optimization problem.
[00:18:07.380 --> 00:18:09.220]   That's the kind of research project that we're working at.
[00:18:09.220 --> 00:18:14.820]   Now, it's a question like what kind of techniques in the field of ML can we use and transfer
[00:18:14.820 --> 00:18:19.380]   to those kind of problems, and what kind of data do we actually need for that?
[00:18:19.380 --> 00:18:20.660]   What would be the choice here?
[00:18:20.660 --> 00:18:26.940]   What would you do differently at, say, step 20 that might make it useful in the end?
[00:18:26.940 --> 00:18:29.060]   We have to find the kind of levers, right?
[00:18:29.060 --> 00:18:32.740]   And there is different types of process that maybe you don't heat it up as much, or you
[00:18:32.740 --> 00:18:36.540]   over bend it a little bit into one direction and re-bend in the other direction.
[00:18:36.540 --> 00:18:40.060]   Maybe you do a refinishing at some point.
[00:18:40.060 --> 00:18:41.900]   These are all the levels that we have.
[00:18:41.900 --> 00:18:44.580]   We have to explore what is the actual problem.
[00:18:44.580 --> 00:18:49.140]   And here, you start to see the devil's in the details.
[00:18:49.140 --> 00:18:51.180]   What are actually the effects that matter?
[00:18:51.180 --> 00:18:52.940]   It's a causal inference problem.
[00:18:52.940 --> 00:18:54.660]   It's a Bayesian learning problem.
[00:18:54.660 --> 00:18:56.460]   We don't know yet because we just started this project.
[00:18:56.460 --> 00:19:01.140]   So I wish I knew the answer, but then I would have already published something on that.
[00:19:01.140 --> 00:19:07.780]   Bas, you're just working on a totally wide range of machine learning applications in
[00:19:07.780 --> 00:19:08.780]   the real world.
[00:19:08.780 --> 00:19:09.940]   That's right.
[00:19:09.940 --> 00:19:14.500]   You must be building a really interesting set of tools to make this possible.
[00:19:14.500 --> 00:19:18.820]   Can you talk about the stuff that you're building that works across all of these different
[00:19:18.820 --> 00:19:19.820]   applications?
[00:19:19.820 --> 00:19:25.460]   Yeah, no, that's a super question because I think that's one of the things that we do
[00:19:25.460 --> 00:19:27.860]   extremely well and we have a lot of fun doing that.
[00:19:27.860 --> 00:19:32.180]   Maybe let's start a little bit back because one of the challenges that we have, of course,
[00:19:32.180 --> 00:19:38.380]   being in Europe, lots of companies have very, very little trust in cloud deployments here.
[00:19:38.380 --> 00:19:42.300]   So you have to start with the customer and saying like, what happens here?
[00:19:42.300 --> 00:19:45.900]   And one of the things that people are super afraid of is vendor lock-in.
[00:19:45.900 --> 00:19:48.940]   So we have to build a tool stack that really is cloud agnostic.
[00:19:48.940 --> 00:19:55.300]   So we can deploy it on-prem, we can do it on GCP, AWS, Azure, you name it, whatever
[00:19:55.300 --> 00:19:56.300]   it is.
[00:19:56.300 --> 00:19:59.420]   So that's the first pre-order that we need to understand how to build a stack that's
[00:19:59.420 --> 00:20:02.340]   completely agnostic of the underlying cloud.
[00:20:02.340 --> 00:20:06.340]   And so in order to do that, we start, of course, building stuff on Terraform and Kubernetes.
[00:20:06.340 --> 00:20:11.020]   So we do extensive use of those systems to automate a lot of deployment tasks.
[00:20:11.020 --> 00:20:13.180]   So infrastructure is code.
[00:20:13.180 --> 00:20:18.620]   Now, once you start going into all of these files, you're getting fairly quickly lost
[00:20:18.620 --> 00:20:23.860]   in them because these configuration files start to become very, very complicated.
[00:20:23.860 --> 00:20:27.820]   So we started to build tools to automate how we actually write deployment files.
[00:20:27.820 --> 00:20:32.300]   So we have an internal tool, which we also funnily enough call DevTool, that essentially
[00:20:32.300 --> 00:20:38.740]   is nothing else than building very specifically pre-programmed template files in order to
[00:20:38.740 --> 00:20:41.940]   spin up complete deployments automatically.
[00:20:41.940 --> 00:20:48.260]   And so we are completely independent of the actual underlying cloud because we can just
[00:20:48.260 --> 00:20:51.060]   spin up the templates of a full deployment cluster.
[00:20:51.060 --> 00:20:56.340]   And on top of that, we can then start using all kinds of other tools that we need in these
[00:20:56.340 --> 00:20:57.340]   clusters what we deploy.
[00:20:57.340 --> 00:21:00.420]   We are typically heavily reliant on Dockers.
[00:21:00.420 --> 00:21:07.500]   So we build a Docker file that we can then deploy on a pod that we command using Kubernetes
[00:21:07.500 --> 00:21:08.500]   or Terraform.
[00:21:08.500 --> 00:21:12.820]   For the deployments, then we use Selden, we use a flight pipeline to automate complete
[00:21:12.820 --> 00:21:13.820]   learning pipelines.
[00:21:13.820 --> 00:21:18.380]   CI/CD in that loop is done with flight.
[00:21:18.380 --> 00:21:21.860]   And right now we still have cloud built, but we're already thinking about how to get that
[00:21:21.860 --> 00:21:23.100]   out of the loop.
[00:21:23.100 --> 00:21:26.820]   So we're trying to be really, really cloud agnostic and build a whole stack ecosystem
[00:21:26.820 --> 00:21:29.300]   on this model and ML tools.
[00:21:29.300 --> 00:21:36.060]   And does this stack that you're deploying into a customer's production environment,
[00:21:36.060 --> 00:21:44.180]   does this include training or is it just for running a model for the customer?
[00:21:44.180 --> 00:21:47.260]   So that really depends on what a customer actually wants.
[00:21:47.260 --> 00:21:51.140]   We are right now, we are targeting towards ML ops level two.
[00:21:51.140 --> 00:21:52.900]   I think that's what Google calls it.
[00:21:52.900 --> 00:21:54.220]   We're not quite there yet.
[00:21:54.220 --> 00:21:59.020]   But so right now we still have a split between manually triggering a retraining that we do
[00:21:59.020 --> 00:22:04.260]   internally using our stack in the cloud or on their on-premise system.
[00:22:04.260 --> 00:22:08.580]   And then also having a separate manual step to actually deploy it into production.
[00:22:08.580 --> 00:22:09.580]   And we're doing both of them.
[00:22:09.580 --> 00:22:13.540]   Like we can actually do it, the deployment step and the retraining step using all of
[00:22:13.540 --> 00:22:14.540]   our infrastructure.
[00:22:14.540 --> 00:22:19.620]   And the target really doesn't matter because we build a cloud agnostic, we can, for example,
[00:22:19.620 --> 00:22:24.420]   do a retraining on our internal cloud, which we mostly use GCP right now for us.
[00:22:24.420 --> 00:22:28.660]   But if the customer wants to have the model in their production stack, we train it on
[00:22:28.660 --> 00:22:31.420]   our cloud and then move it to their production stack on-prem.
[00:22:31.420 --> 00:22:35.620]   And I guess, what have you learned building these tools?
[00:22:35.620 --> 00:22:38.220]   I mean, it sounds like you're making the stuff, you're deploying it.
[00:22:38.220 --> 00:22:41.020]   There's many people trying to build these things.
[00:22:41.020 --> 00:22:45.980]   What have been the kind of lessons actually when these things get deployed into customers'
[00:22:45.980 --> 00:22:46.980]   systems?
[00:22:46.980 --> 00:22:51.860]   That it's really, really hard still to do.
[00:22:51.860 --> 00:22:52.860]   Why is it hard?
[00:22:52.860 --> 00:22:53.860]   Because it's conceptually, it's simple.
[00:22:53.860 --> 00:22:57.100]   Like what actually really makes it hard?
[00:22:57.100 --> 00:23:01.140]   It's actually not that hard if customers are okay with using cloud deployments.
[00:23:01.140 --> 00:23:06.740]   I think what makes it hard is if they're using on-prem in their own stack, because then suddenly
[00:23:06.740 --> 00:23:13.060]   the tools are not yet at that point where you can just abstract away every kind of sysadmin.
[00:23:13.060 --> 00:23:17.260]   You're always having this touch point between how is the hardware actually managed and how
[00:23:17.260 --> 00:23:18.260]   can you deploy it.
[00:23:18.260 --> 00:23:24.300]   As soon as you have a Kubernetes cluster installed on premise, you're probably fine again.
[00:23:24.300 --> 00:23:27.520]   But until you get there, you cannot abstract that system away.
[00:23:27.520 --> 00:23:31.260]   And then you're also getting these realities of the business that you sometimes have to
[00:23:31.260 --> 00:23:36.220]   deal with IoT devices and then deploying stuff onto IoT.
[00:23:36.220 --> 00:23:37.220]   That's really not there yet.
[00:23:37.220 --> 00:23:40.980]   And I think the tools are falling short on that end, but I think that's just a matter
[00:23:40.980 --> 00:23:45.620]   of time until we have more tools that are ready for IoT deployments.
[00:23:45.620 --> 00:23:48.700]   How do you think about monitoring the system in production?
[00:23:48.700 --> 00:23:52.220]   I'd imagine these things could be somewhat mission critical, but I noticed you didn't
[00:23:52.220 --> 00:23:54.500]   really mention production monitoring.
[00:23:54.500 --> 00:23:57.100]   How do you think about that?
[00:23:57.100 --> 00:23:58.680]   I think it's very important and we do it.
[00:23:58.680 --> 00:24:04.060]   We are not necessarily deploying extremely mission critical systems right now.
[00:24:04.060 --> 00:24:07.300]   So that's what we haven't done yet.
[00:24:07.300 --> 00:24:09.220]   I think we're getting there soon.
[00:24:09.220 --> 00:24:13.740]   But right now it's mostly just like measuring uptime and making sure that the stack doesn't
[00:24:13.740 --> 00:24:15.260]   fall on the load.
[00:24:15.260 --> 00:24:21.300]   So it's just like the standard production monitoring that is just Rafaana load testing,
[00:24:21.300 --> 00:24:24.920]   throughput measurements and these kinds of things, not necessarily decision making and
[00:24:24.920 --> 00:24:26.460]   auditing trails in that regard.
[00:24:26.460 --> 00:24:30.620]   So it's more like a standard site reliability monitoring in that sense.
[00:24:30.620 --> 00:24:34.540]   That can be automated fairly easily using Rafaana or any other monitoring tool that
[00:24:34.540 --> 00:24:35.540]   you like.
[00:24:35.540 --> 00:24:36.540]   Got it.
[00:24:36.540 --> 00:24:37.540]   Got it.
[00:24:37.540 --> 00:24:43.740]   I thought you might want to talk about some of the tools that you've developed like Squirrel
[00:24:43.740 --> 00:24:45.540]   and Parrot and Chameleon.
[00:24:45.540 --> 00:24:47.940]   Can you describe what these are?
[00:24:47.940 --> 00:24:50.220]   Yeah, that's really cool.
[00:24:50.220 --> 00:24:54.780]   My personal favorite right now is Squirrel just because we're just about to launch it
[00:24:54.780 --> 00:24:58.980]   and then release it out into the world, which is super fascinating.
[00:24:58.980 --> 00:25:03.980]   The goal here is that if you take a look into the ecosystem, we have very, very good at
[00:25:03.980 --> 00:25:08.640]   building ML models for training on single GPUs.
[00:25:08.640 --> 00:25:14.020]   But as soon as anybody encountered for the first time trying to deal with multiple GPUs,
[00:25:14.020 --> 00:25:15.980]   you're getting into big problems.
[00:25:15.980 --> 00:25:20.740]   And many frameworks have come across that are actually helping you to distribute a model,
[00:25:20.740 --> 00:25:24.500]   but nobody has really thought about how do you distribute the data.
[00:25:24.500 --> 00:25:25.820]   And there are not many frameworks out there.
[00:25:25.820 --> 00:25:29.940]   There's a few things that we have looked at that are trying to solve that and the ecosystem
[00:25:29.940 --> 00:25:37.420]   is getting bigger, but we are now decided we want to go into a place where we can really
[00:25:37.420 --> 00:25:40.540]   make data loading on distributed systems as easy as possible.
[00:25:40.540 --> 00:25:46.180]   It doesn't need to be only for deep learning, but it can be for a lot of different things.
[00:25:46.180 --> 00:25:50.220]   And on top of that, also build in potential access control levels.
[00:25:50.220 --> 00:25:53.380]   You want to pull that one from this bucket, the next one from that bucket, the third one
[00:25:53.380 --> 00:25:56.420]   from this bucket, and make sure that you mix and match this very well.
[00:25:56.420 --> 00:26:00.780]   That's what Skrll is really about, to make data access and data storage and data writing
[00:26:00.780 --> 00:26:05.700]   super, super simple, as simple as you can do it by just abstracting away the file system.
[00:26:05.700 --> 00:26:09.940]   It can be on a cloud, it can be on local, it can just be pulled from the internet, and
[00:26:09.940 --> 00:26:12.140]   it should be easy to integrate in any kind of framework.
[00:26:12.140 --> 00:26:14.460]   And that's really what we're doing here.
[00:26:14.460 --> 00:26:17.180]   And your plan is to make this open source?
[00:26:17.180 --> 00:26:19.460]   The idea is to make this open source, exactly.
[00:26:19.460 --> 00:26:20.820]   Cool, cool.
[00:26:20.820 --> 00:26:24.700]   And I guess, do you have a preference of other open source tooling?
[00:26:24.700 --> 00:26:29.340]   Do you guys kind of standardize on your ML framework and things like that?
[00:26:29.340 --> 00:26:34.540]   What's sort of your set of tools that you would typically like to use?
[00:26:34.540 --> 00:26:36.740]   We of course also standardizing as much as we can.
[00:26:36.740 --> 00:26:40.060]   You can imagine having many, many customers, you want to have standardized tools.
[00:26:40.060 --> 00:26:43.300]   So our standard framework is PyTorch.
[00:26:43.300 --> 00:26:45.980]   That's what we're doing internally for training these models.
[00:26:45.980 --> 00:26:48.860]   We're also betting a lot of PyTorch Lightning as an easy framework.
[00:26:48.860 --> 00:26:53.740]   We're also using Hydra that's developed by Facebook as an interface and an entry point
[00:26:53.740 --> 00:26:56.620]   into those systems.
[00:26:56.620 --> 00:26:59.140]   Why did you pick PyTorch Lightning?
[00:26:59.140 --> 00:27:01.020]   What did you like about that?
[00:27:01.020 --> 00:27:08.540]   I think the idea here is that it really abstracts away much of what ML training frameworks have
[00:27:08.540 --> 00:27:09.540]   to do.
[00:27:09.540 --> 00:27:12.740]   You're writing a data loader, you're having an optimizer, you're having a training loop,
[00:27:12.740 --> 00:27:14.580]   and you have a logger.
[00:27:14.580 --> 00:27:20.820]   And typically when you just look at typical GitHub repositories, everybody writes for
[00:27:20.820 --> 00:27:25.300]   a batch in data loader, do all of these kinds of things.
[00:27:25.300 --> 00:27:26.740]   It's a very repetitive code.
[00:27:26.740 --> 00:27:30.540]   Just abstract this away, use some software engineering so it's robust, and then you can
[00:27:30.540 --> 00:27:32.580]   go with that.
[00:27:32.580 --> 00:27:35.540]   It's especially important if you're doing production models, where you just have to
[00:27:35.540 --> 00:27:39.740]   retrain and you need to be stable on that.
[00:27:39.740 --> 00:27:44.140]   Software maintenance is, I think, one of those things that is not really in the academic
[00:27:44.140 --> 00:27:48.500]   community, which is a surprise to me, because the field that is coming out of the engineering
[00:27:48.500 --> 00:27:52.740]   should value good code qualities a little bit more, I feel.
[00:27:52.740 --> 00:27:54.420]   So we have to do it ourselves.
[00:27:54.420 --> 00:27:59.260]   Use tools that make maintenance and debugging of machine learning models easier.
[00:27:59.260 --> 00:28:02.180]   Frameworks are the way to go for that, because you don't want to build it yourself if the
[00:28:02.180 --> 00:28:05.340]   community can help you maintain the systems.
[00:28:05.340 --> 00:28:07.700]   Do you also use PyTorch to run the models in production?
[00:28:07.700 --> 00:28:12.900]   I know some people will change the format or do something to the model before it's deployed.
[00:28:12.900 --> 00:28:18.020]   Do you just load up the model as serialized from PyTorch or do you do anything special
[00:28:18.020 --> 00:28:19.020]   there?
[00:28:19.020 --> 00:28:24.060]   No, we typically deserialize it from PyTorch directly, because right now our modus is to
[00:28:24.060 --> 00:28:26.180]   ship Dockers around the world.
[00:28:26.180 --> 00:28:31.180]   I think eventually we probably, for certain applications, need to go into a more standardized
[00:28:31.180 --> 00:28:34.740]   framework like ONNX or something like that.
[00:28:34.740 --> 00:28:42.020]   That will change the game potentially, but right now we are still using the binary Docker.
[00:28:42.020 --> 00:28:45.740]   Where do you see gaps in the tooling right now as someone that likes to make and sell
[00:28:45.740 --> 00:28:48.060]   ML tools?
[00:28:48.060 --> 00:28:54.340]   What parts of the stack feel mature and what parts feel broken?
[00:28:54.340 --> 00:29:00.300]   What feels broken to me is that you have to plug many systems into many systems.
[00:29:00.300 --> 00:29:04.940]   That feels a little bit sad because that makes it really hard sometimes to stay abreast of
[00:29:04.940 --> 00:29:05.940]   the edge.
[00:29:05.940 --> 00:29:08.300]   I don't think that there's anything lacking in the community right now.
[00:29:08.300 --> 00:29:13.260]   I more feel like the problem is that too many people are building too many tools instead
[00:29:13.260 --> 00:29:17.660]   of just coming together and take one tool and bring it to the next level.
[00:29:17.660 --> 00:29:22.540]   The thing that happens is that people try to be different from others instead of making
[00:29:22.540 --> 00:29:26.340]   one tool that solves a lot of problems.
[00:29:26.340 --> 00:29:30.540]   A counter example where this worked really well is in the data science world.
[00:29:30.540 --> 00:29:35.740]   You just need two or three libraries in the data science world, which is Scikit-learn,
[00:29:35.740 --> 00:29:38.420]   Xamppi and Pandas, and you're set.
[00:29:38.420 --> 00:29:42.340]   If you're going into ML Ops domain, I don't know how many tools out there.
[00:29:42.340 --> 00:29:44.420]   You probably know better than me.
[00:29:44.420 --> 00:29:45.420]   I wonder sometimes why.
[00:29:45.420 --> 00:29:46.420]   Yeah, that's fair.
[00:29:46.420 --> 00:29:51.940]   I mean, I definitely think there's always a moment where there's an explosion of ideas
[00:29:51.940 --> 00:29:54.740]   and tools and then things start to standardize for sure.
[00:29:54.740 --> 00:29:58.180]   I think we're still at that explosion stage.
[00:29:58.180 --> 00:30:01.820]   That's what makes it interesting to be in this world right now.
[00:30:01.820 --> 00:30:02.820]   I agree.
[00:30:02.820 --> 00:30:04.580]   I think that there's a lot of distractions we haven't figured out.
[00:30:04.580 --> 00:30:06.740]   For example, deployment to IoT.
[00:30:06.740 --> 00:30:12.020]   What I'm super curious about that I haven't seen much development until recently is how
[00:30:12.020 --> 00:30:15.500]   do you deploy models in heterogeneous environments?
[00:30:15.500 --> 00:30:17.420]   How do you train on heterogeneous environments?
[00:30:17.420 --> 00:30:20.600]   I think there's still a lot of ML tooling that needs to get better.
[00:30:20.600 --> 00:30:24.380]   Not everybody has a huge data center of homogeneous hardware.
[00:30:24.380 --> 00:30:29.260]   So how do we deploy models or train models on heterogeneous hardware?
[00:30:29.260 --> 00:30:33.700]   I guess another question I have is how do you hand off these models to a customer?
[00:30:33.700 --> 00:30:40.480]   You say you give them a docker, but if they want to keep iterating on a model once they've
[00:30:40.480 --> 00:30:43.980]   taken it from you, are they able to do that?
[00:30:43.980 --> 00:30:44.980]   How do you think about that?
[00:30:44.980 --> 00:30:50.260]   Because it does feel like machine learning projects are never really complete, if you
[00:30:50.260 --> 00:30:51.260]   know what I mean.
[00:30:51.260 --> 00:30:54.340]   Yeah, no, I understand what you're saying.
[00:30:54.340 --> 00:30:55.340]   It depends on the customer.
[00:30:55.340 --> 00:30:58.440]   I don't think that there's a one rule fits all.
[00:30:58.440 --> 00:31:02.420]   Some customers just come back and say, "Hey, we need retraining or we need a fresh up.
[00:31:02.420 --> 00:31:05.740]   Can you do that for us?" because they don't have an IT department.
[00:31:05.740 --> 00:31:08.580]   Some people want to jumpstart their IT department.
[00:31:08.580 --> 00:31:12.380]   They say, "Okay, we know machine learning is the future.
[00:31:12.380 --> 00:31:17.980]   We don't have an IT department yet, but maybe we engage with you and you help us to jumpstart
[00:31:17.980 --> 00:31:18.980]   the engine."
[00:31:18.980 --> 00:31:21.380]   And then they start continuing in that code.
[00:31:21.380 --> 00:31:26.740]   It's always, of course, a conversation because it's also tricky for us to say, "Hey, we're
[00:31:26.740 --> 00:31:27.740]   offering our expertise.
[00:31:27.740 --> 00:31:32.700]   We put in a lot of sweat, tears, and blood," and then you take it to the next level.
[00:31:32.700 --> 00:31:33.700]   That's always sad as well.
[00:31:33.700 --> 00:31:36.900]   So it's always a tricky conversation, but we are happy to help people.
[00:31:36.900 --> 00:31:42.660]   And I ultimately think that everyone benefits if the community just grows.
[00:31:42.660 --> 00:31:47.240]   I guess another question I wanted to ask you about is you've written a few thought pieces
[00:31:47.240 --> 00:31:48.240]   on AI.
[00:31:48.240 --> 00:31:52.000]   I don't know if you have a favorite, but I think one interesting one is you're writing
[00:31:52.000 --> 00:31:55.880]   on the impact of NLP models on the real world.
[00:31:55.880 --> 00:32:03.760]   If you could summarize for people who haven't read it, my perspective is that in a way,
[00:32:03.760 --> 00:32:08.500]   the NLP field seems to be doing a whole bunch of very amazing things.
[00:32:08.500 --> 00:32:13.180]   And I know people argue about, is this real intelligence or not, or how much does it really
[00:32:13.180 --> 00:32:14.180]   matter?
[00:32:14.180 --> 00:32:19.760]   But I guess from my perspective as a technologist and enthusiast, I can't believe how good
[00:32:19.760 --> 00:32:22.440]   text generation has got in some sense.
[00:32:22.440 --> 00:32:30.820]   And yet I think the impact to me is smaller than I would have imagined from how impressive
[00:32:30.820 --> 00:32:31.820]   the demos look.
[00:32:31.820 --> 00:32:34.020]   I don't know how you feel about that.
[00:32:34.020 --> 00:32:35.920]   No, I see your point.
[00:32:35.920 --> 00:32:40.380]   And I think that is exactly the reason why I like working where I am, because it's right
[00:32:40.380 --> 00:32:43.500]   in the middle of driving the adoption of modern AI techniques.
[00:32:43.500 --> 00:32:47.820]   I think the reason why you feel the impact is not as big as it could have been or should
[00:32:47.820 --> 00:32:53.700]   have been, is that it's really, really hard to bring technology like that to people who
[00:32:53.700 --> 00:32:56.340]   are not technologists like us.
[00:32:56.340 --> 00:32:57.940]   And that's really the challenge here.
[00:32:57.940 --> 00:33:00.260]   You have to bridge that gap.
[00:33:00.260 --> 00:33:04.580]   And there is this early adopter gap, and that needs to be bridged.
[00:33:04.580 --> 00:33:05.900]   And we are not there yet.
[00:33:05.900 --> 00:33:06.900]   I'm also with you.
[00:33:06.900 --> 00:33:11.380]   I don't really want to get into this philosophical debate.
[00:33:11.380 --> 00:33:12.600]   Is it intelligent?
[00:33:12.600 --> 00:33:15.980]   Is it conscious or whatever it is?
[00:33:15.980 --> 00:33:17.140]   It's useful technology.
[00:33:17.140 --> 00:33:21.780]   Let's bring it to the people and have them have a better life with it.
[00:33:21.780 --> 00:33:22.980]   Let's solve some problems with that.
[00:33:22.980 --> 00:33:25.260]   And that's maybe like the philosophical side.
[00:33:25.260 --> 00:33:30.460]   The practical side is if you take those big models, you're running into the problem like
[00:33:30.460 --> 00:33:35.740]   you need already compute power, you need infrastructure, you need ML ops, you needed a whole department
[00:33:35.740 --> 00:33:38.060]   to actually make use of those models.
[00:33:38.060 --> 00:33:39.840]   Not many people have that, right?
[00:33:39.840 --> 00:33:42.460]   Especially those companies that it's most useful for.
[00:33:42.460 --> 00:33:46.160]   Take for example, news outlets or media outlets.
[00:33:46.160 --> 00:33:50.060]   They are completely focused on a very different problem.
[00:33:50.060 --> 00:33:56.740]   They don't have technologies that just take a GPT-2 or even a GPT-3 size model to put
[00:33:56.740 --> 00:33:59.860]   into production and then figure out the use cases.
[00:33:59.860 --> 00:34:02.460]   That's just not how the economics of these companies work.
[00:34:02.460 --> 00:34:04.660]   And so bringing it to those people is really hard.
[00:34:04.660 --> 00:34:07.420]   And I think it's the reason why we don't see that impact yet.
[00:34:07.420 --> 00:34:10.940]   And it's going to come, but it's still going to take a few years.
[00:34:10.940 --> 00:34:16.140]   What do you think are the next things that we're going to notice just as consumers from
[00:34:16.140 --> 00:34:20.700]   the impact of these more powerful NLP models?
[00:34:20.700 --> 00:34:24.880]   I do think that a lot of stuff that will come is improvements in search.
[00:34:24.880 --> 00:34:32.160]   I think that the sickness that we get from similarity clustering is significant and we
[00:34:32.160 --> 00:34:35.500]   just need to figure out how to adopt that into real world, right?
[00:34:35.500 --> 00:34:40.100]   If you just run GPT-3 size models, the search is slow.
[00:34:40.100 --> 00:34:42.140]   So we just need to do some improvements on that.
[00:34:42.140 --> 00:34:45.260]   But I do think that we see a re-ranking on that front.
[00:34:45.260 --> 00:34:50.120]   I also think that a lot of automation will happen for automated text generation.
[00:34:50.120 --> 00:34:51.120]   And that's a positive thing.
[00:34:51.120 --> 00:34:53.220]   I don't know how much time you spend on emails.
[00:34:53.220 --> 00:34:55.780]   I certainly do a lot and you probably do too.
[00:34:55.780 --> 00:34:58.540]   And it would be nice to just automate some of that stuff away.
[00:34:58.540 --> 00:35:03.740]   I also talked to several customers in Germany that have this funky problem where they're
[00:35:03.740 --> 00:35:06.320]   in a logistics space.
[00:35:06.320 --> 00:35:13.620]   And logistics is a very old school domain where you get very free form order forms.
[00:35:13.620 --> 00:35:17.980]   And there are a modest of people that just do nothing else than taking those emails that
[00:35:17.980 --> 00:35:22.860]   are just free floating written and turn them into structured text by just manually copy
[00:35:22.860 --> 00:35:24.700]   pasting into a structured field.
[00:35:24.700 --> 00:35:25.700]   Sounds easy.
[00:35:25.700 --> 00:35:26.700]   It's not.
[00:35:26.700 --> 00:35:28.780]   It's a very, very hard NLP task.
[00:35:28.780 --> 00:35:33.060]   Once we bring these big models into that realm, I think there will be a lot of automation
[00:35:33.060 --> 00:35:34.300]   for the better.
[00:35:34.300 --> 00:35:36.580]   And so I do think there's a lot of potential.
[00:35:36.580 --> 00:35:39.340]   I'm very excited about the future of those models.
[00:35:39.340 --> 00:35:40.540]   Cool.
[00:35:40.540 --> 00:35:45.020]   You also wrote an article on AI and regulation I wanted to touch on.
[00:35:45.020 --> 00:35:47.780]   I'm curious your perspective on regulation.
[00:35:47.780 --> 00:35:53.060]   I mean, obviously it's coming, but I'd be interested to know what you think about it,
[00:35:53.060 --> 00:35:56.420]   what good regulation would look like.
[00:35:56.420 --> 00:35:57.420]   If I only knew, right?
[00:35:57.420 --> 00:35:59.860]   That's a good discussion.
[00:35:59.860 --> 00:36:04.780]   I think being in Europe, one of the things that I needed to learn is that how can you
[00:36:04.780 --> 00:36:11.640]   use regulation in order to build value systems of a society into your AI deployments?
[00:36:11.640 --> 00:36:12.780]   And that can be a good thing.
[00:36:12.780 --> 00:36:19.780]   I think the regulation needs to address the realities of AI as being an experimental technology
[00:36:19.780 --> 00:36:24.820]   that we need to deal with these uncertainties, but also make sure that we are not opening
[00:36:24.820 --> 00:36:32.300]   the door for extreme abuses and give people and consumers the right to protest.
[00:36:32.300 --> 00:36:33.660]   How to exactly build those regulations?
[00:36:33.660 --> 00:36:34.660]   I don't know.
[00:36:34.660 --> 00:36:40.420]   I think that what I appreciate about the regulatory frameworks that we have in the EU is that
[00:36:40.420 --> 00:36:44.460]   we are more willing to iterate on regulations, which is good.
[00:36:44.460 --> 00:36:47.460]   We make a draft, we see how it's being in practice.
[00:36:47.460 --> 00:36:49.380]   Some things work, some things don't work.
[00:36:49.380 --> 00:36:50.700]   We try to adjust.
[00:36:50.700 --> 00:36:53.100]   Classic example, GDPR and the cookie banners.
[00:36:53.100 --> 00:36:57.100]   I don't know how many cookies you have to click away.
[00:36:57.100 --> 00:37:00.940]   It's really annoying and people got it and now we're trying to figure out how to build
[00:37:00.940 --> 00:37:03.300]   a regulation that we don't have to do this anymore.
[00:37:03.300 --> 00:37:05.860]   But it takes time and I think it's a process.
[00:37:05.860 --> 00:37:11.260]   I think as a technologist, you're actually building software for humans.
[00:37:11.260 --> 00:37:12.820]   You don't build technology for your own sake.
[00:37:12.820 --> 00:37:17.100]   You're building in order to make something better, to do something better, to make somebody's
[00:37:17.100 --> 00:37:18.100]   life better.
[00:37:18.100 --> 00:37:25.380]   But I guess specifically, what's a regulation that you would like to see happen?
[00:37:25.380 --> 00:37:32.780]   What I would like to see happen is to allow for ML models to have a sandbox environment
[00:37:32.780 --> 00:37:37.900]   where you can say, I can do tests on real world scenarios where I can collect data in
[00:37:37.900 --> 00:37:41.140]   the real world in a given risk frame.
[00:37:41.140 --> 00:37:44.020]   And then you can get risk certifications that are going up.
[00:37:44.020 --> 00:37:48.500]   I said, okay, I did my first test that was an exposure of, I don't know, a million dollar
[00:37:48.500 --> 00:37:49.500]   in risk.
[00:37:49.500 --> 00:37:51.500]   Just arbitrary number.
[00:37:51.500 --> 00:37:53.540]   Don't take them for fixed prices.
[00:37:53.540 --> 00:37:55.900]   A certifier says, okay, that's great.
[00:37:55.900 --> 00:37:57.740]   Now we can go to the next iteration phase.
[00:37:57.740 --> 00:38:02.140]   And then you build up this risk where you can say a certifier is willing to back you
[00:38:02.140 --> 00:38:04.900]   up on insurance for a given risk factor.
[00:38:04.900 --> 00:38:09.740]   Because only then can you actually use these experimental technologies to go out into the
[00:38:09.740 --> 00:38:10.740]   real world.
[00:38:10.740 --> 00:38:13.540]   Because right now, hands are often bound, right?
[00:38:13.540 --> 00:38:19.660]   Like by data privacy issue, by copyright issues, by security concerns.
[00:38:19.660 --> 00:38:24.980]   And so the regulatory uncertainties around that, especially for a startup that builds
[00:38:24.980 --> 00:38:26.380]   ML is really, really high.
[00:38:26.380 --> 00:38:30.980]   And so I would like to see having protected environments where you are allowed to test
[00:38:30.980 --> 00:38:32.620]   things within a certain box.
[00:38:32.620 --> 00:38:38.100]   And I think that would be a good regulation because the consumer can slowly gather trust
[00:38:38.100 --> 00:38:40.340]   and can see what it can do in the real world.
[00:38:40.340 --> 00:38:45.940]   You start to see curiosity and you have it under control to a certain extent because
[00:38:45.940 --> 00:38:51.260]   you know if the company does something wrong, it's going to get penalized and that's bad
[00:38:51.260 --> 00:38:52.260]   for the company.
[00:38:52.260 --> 00:38:57.340]   So I think that would be a good regulation I would like to see in this form or another.
[00:38:57.340 --> 00:39:02.780]   I saw you also wrote on ML and environmental impact and that's something I care about a
[00:39:02.780 --> 00:39:05.300]   lot and have looked at.
[00:39:05.300 --> 00:39:06.300]   What's your thoughts there?
[00:39:06.300 --> 00:39:10.700]   I mean, do you feel like people should be finding ways to train models with less compute?
[00:39:10.700 --> 00:39:16.940]   I mean, how do you reconcile the fact that you're also doing model training in your day-to-day
[00:39:16.940 --> 00:39:17.940]   job?
[00:39:17.940 --> 00:39:20.940]   It's a complicated question.
[00:39:21.940 --> 00:39:27.740]   On the one hand, big models and ML models are really powerful and important.
[00:39:27.740 --> 00:39:32.500]   On the other hand, you need to make sure that you're not burning up the planet with them.
[00:39:32.500 --> 00:39:37.700]   So my stance on this is let's reuse those models as much as you can.
[00:39:37.700 --> 00:39:39.220]   Fine tuning, zero short learning.
[00:39:39.220 --> 00:39:43.540]   Once you train them and really invest it in that money, let's make sure that this cost,
[00:39:43.540 --> 00:39:46.540]   this carbon footprint and the monetary stuff amortizes.
[00:39:46.540 --> 00:39:48.980]   And that's what we're currently seeing.
[00:39:48.980 --> 00:39:53.020]   There's a lot of interest in training these big models, pre-training them because they
[00:39:53.020 --> 00:39:54.580]   fine tune very well.
[00:39:54.580 --> 00:39:58.140]   I just feel like there's too many people who want to just build them from scratch and not
[00:39:58.140 --> 00:40:00.980]   figure out what can we do with the existing ones.
[00:40:00.980 --> 00:40:04.740]   And I hope to see a change a little bit in that.
[00:40:04.740 --> 00:40:05.740]   That's my take on it.
[00:40:05.740 --> 00:40:10.580]   It's not just like shun it, but also let's be conscious about it.
[00:40:10.580 --> 00:40:13.740]   Makes sense.
[00:40:13.740 --> 00:40:15.740]   We always end with two questions.
[00:40:15.740 --> 00:40:20.780]   And the second to last question that we always end with is what's a topic in machine learning
[00:40:20.780 --> 00:40:22.500]   that you think is understudied?
[00:40:22.500 --> 00:40:28.500]   Or what's something that if you had more time, you would love to look into more deeply?
[00:40:28.500 --> 00:40:33.380]   If I had more time, I would probably pull out my physicist's head again and try to understand
[00:40:33.380 --> 00:40:37.100]   a lot of the optimization problems within machine learning.
[00:40:37.100 --> 00:40:43.140]   There's a whole field that is just ripe for discovery, which is the combination of loss
[00:40:43.140 --> 00:40:48.980]   landscapes and optimization problems in deep learning models and a connection to statistical
[00:40:48.980 --> 00:40:49.980]   physics.
[00:40:49.980 --> 00:40:53.620]   And I think that is a really, really valuable lesson.
[00:40:53.620 --> 00:40:57.780]   It can actually help statistical physicists to understand certain things better, but also
[00:40:57.780 --> 00:41:01.300]   statistical physics can probably help the ML community to understand much better what's
[00:41:01.300 --> 00:41:02.500]   actually happening under the hood.
[00:41:02.500 --> 00:41:05.780]   And I would love to contribute to this much more, but that's very far away from what I
[00:41:05.780 --> 00:41:08.780]   would do every day.
[00:41:08.780 --> 00:41:13.460]   I've seen papers on this topic and I always find them impenetrable because I think I don't
[00:41:13.460 --> 00:41:18.740]   have the background in physics that people are assuming.
[00:41:18.740 --> 00:41:23.620]   Can you describe a little bit of what this says to someone like me who maybe knows some
[00:41:23.620 --> 00:41:27.420]   of the math and is interested, but doesn't quite follow?
[00:41:27.420 --> 00:41:34.180]   Is there an interesting result that you could point to from this analogy?
[00:41:34.180 --> 00:41:37.340]   Physicists typically think in terms of what we call a phase diagram.
[00:41:37.340 --> 00:41:39.860]   The phase diagram is the different states of water.
[00:41:39.860 --> 00:41:42.820]   You have vapor, water, and ice.
[00:41:42.820 --> 00:41:47.380]   Similar effects happen in all kinds of other physical materials.
[00:41:47.380 --> 00:41:52.220]   And one of the funny things that you can see is that these kind of phase transitions are
[00:41:52.220 --> 00:41:56.580]   different where you go from one phase to another phase, like from liquid to vapor.
[00:41:56.580 --> 00:42:01.260]   These kinds of transitions also happen in optimization landscapes of machine learning
[00:42:01.260 --> 00:42:02.260]   problems.
[00:42:02.260 --> 00:42:06.600]   For example, when you tune the number of parameters in a model, you go from the model not being
[00:42:06.600 --> 00:42:12.500]   able to optimize at all to the model just suddenly optimizing perfectly.
[00:42:12.500 --> 00:42:19.020]   And people describe this as a spin glass to jamming transition, very technical term.
[00:42:19.020 --> 00:42:24.180]   But it essentially means from being like an almost quasi-frozen state to something that
[00:42:24.180 --> 00:42:26.460]   is just very, very, very viscous.
[00:42:26.460 --> 00:42:28.100]   And it's a very different physical properties.
[00:42:28.100 --> 00:42:30.500]   And you can see those in machine learning models.
[00:42:30.500 --> 00:42:36.720]   So these are the early indications that you can use these kind of method and tools that
[00:42:36.720 --> 00:42:41.840]   we developed in statistical physics to understand the dynamics that happen in machine learning
[00:42:41.840 --> 00:42:42.840]   models.
[00:42:42.840 --> 00:42:47.080]   And ultimately, I think this will help us also train these models much better and at
[00:42:47.080 --> 00:42:48.080]   much cheaper cost.
[00:42:48.080 --> 00:42:49.080]   Cool.
[00:42:49.080 --> 00:42:54.620]   Well, on a much more practical note, when you think about all the models that you've
[00:42:54.620 --> 00:42:59.040]   trained and put into production, what's the hardest piece of that at this moment?
[00:42:59.040 --> 00:43:04.960]   What is the biggest challenge from a customer wanting a model to do a particular thing to
[00:43:04.960 --> 00:43:09.720]   that thing deployed and working inside of their infrastructure?
[00:43:09.720 --> 00:43:15.200]   I think actually getting the high quality data is really hard because that's where the
[00:43:15.200 --> 00:43:19.680]   customer comes in and you need to actually pick them up at that point and tell them it's
[00:43:19.680 --> 00:43:24.600]   not just data in and model out, but you need high quality data.
[00:43:24.600 --> 00:43:33.600]   We did a project for a semantic segmentations of very, very fine detailed mistakes on huge
[00:43:33.600 --> 00:43:34.600]   metal surfaces.
[00:43:34.600 --> 00:43:36.460]   These are tiny scratches.
[00:43:36.460 --> 00:43:41.040]   You have maybe five or six pixels on a thousand by thousand pixel image.
[00:43:41.040 --> 00:43:43.120]   And you need to find a loss function for that.
[00:43:43.120 --> 00:43:46.960]   And now these images are recorded from various different angles and labeled by different
[00:43:46.960 --> 00:43:47.960]   people.
[00:43:47.960 --> 00:43:48.960]   So on some images, there's a scratch.
[00:43:48.960 --> 00:43:49.960]   On some images, there's not.
[00:43:49.960 --> 00:43:53.600]   Same piece of metal, but you see the scratch and you don't see the scratch.
[00:43:53.600 --> 00:43:58.560]   So helping people understand how to label data, how to bring the data into a quality
[00:43:58.560 --> 00:44:02.440]   that the model can actually pick something up is really the complicated part.
[00:44:02.440 --> 00:44:04.400]   I think that's an understudied problem.
[00:44:04.400 --> 00:44:06.120]   How did you actually get the data labeled in this case?
[00:44:06.120 --> 00:44:10.880]   I do have some experience with data labeling.
[00:44:10.880 --> 00:44:15.860]   Essentially having an armada of people that use the labeling tool and teach them what
[00:44:15.860 --> 00:44:18.620]   to label for and get a huge feedback loop.
[00:44:18.620 --> 00:44:22.160]   Did you build a custom tool for this to find the scratches?
[00:44:22.160 --> 00:44:24.400]   Yeah, we used open source software.
[00:44:24.400 --> 00:44:29.200]   I don't know actually which piece we used and then just adjusted it for that use case
[00:44:29.200 --> 00:44:31.000]   in order to make this quick and fast.
[00:44:31.000 --> 00:44:32.000]   Awesome.
[00:44:32.000 --> 00:44:33.000]   Well, thank you so much.
[00:44:33.000 --> 00:44:35.200]   This is really fun and so many different insights.
[00:44:35.200 --> 00:44:36.200]   I love it.
[00:44:36.200 --> 00:44:37.200]   Thank you.
[00:44:37.200 --> 00:44:38.200]   Yeah, thank you.
[00:44:38.200 --> 00:44:42.520]   If you're enjoying these interviews and you want to learn more, please click on the link
[00:44:42.520 --> 00:44:47.240]   to the show notes in the description where you can find links to all the papers that
[00:44:47.240 --> 00:44:51.640]   are mentioned, supplemental material, and a transcription that we work really hard to
[00:44:51.640 --> 00:44:53.220]   so check it out.

