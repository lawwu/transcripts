
[00:00:00.000 --> 00:00:07.000]   So audio is a huge field and it's actually arguably the field that really started the interest in deep learning.
[00:00:07.000 --> 00:00:14.000]   So we are just going to scratch the very, very surface of audio in this video.
[00:00:14.000 --> 00:00:22.000]   And what I really want to show you is that we can take the exact same techniques that we applied to text and image classification and apply it to audio.
[00:00:22.000 --> 00:00:24.000]   Now it's not totally obvious how you would do that, right?
[00:00:24.000 --> 00:00:30.000]   I mean, like audio comes in a really different format than like an image or text, right?
[00:00:30.000 --> 00:00:36.000]   I mean, basically, typically we represent it as kind of like a wave or maybe two waves if you have stereo sound.
[00:00:36.000 --> 00:00:40.000]   So how do we actually get it in a format where we can process it and what do we do with it?
[00:00:40.000 --> 00:00:46.000]   You know, audio files tend to be big and it tends to be just complicated to ingest them and handle them.
[00:00:46.000 --> 00:00:50.000]   So I'm going to do a very, very small classification example.
[00:00:50.000 --> 00:00:54.000]   The idea is we want to classify people saying different specific words.
[00:00:54.000 --> 00:00:58.000]   And we're going to see how well we can do that with some really simple Keras techniques.
[00:00:58.000 --> 00:01:04.000]   So here's the task. We want to classify sounds and the sounds are people speaking.
[00:01:04.000 --> 00:01:06.000]   And we're going to classify them into what the person is saying.
[00:01:06.000 --> 00:01:12.000]   So I found online wave files of various people saying the words bed, happy, and cat.
[00:01:12.000 --> 00:01:16.000]   And actually there were a lot more sets of wave files there.
[00:01:16.000 --> 00:01:22.000]   You can follow a link we'll put in the comments to download more if you want to classify different words.
[00:01:22.000 --> 00:01:28.000]   And what we're going to do is we're going to take those wave files, do some transformations on it,
[00:01:28.000 --> 00:01:34.000]   and then run various types of neural nets to see how well they classify this data.
[00:01:34.000 --> 00:01:41.000]   So, you know, first of all, we do the standard sort of importing libraries like Keras and actually a pre-processed library
[00:01:41.000 --> 00:01:45.000]   that I mostly copied from another audio processing Git project.
[00:01:45.000 --> 00:01:49.000]   And that does things like transform the wave files into spectrograms.
[00:01:49.000 --> 00:01:54.000]   So the next thing that's going to happen is we set the number of buckets in our spectrogram
[00:01:54.000 --> 00:01:58.000]   and we set the length of time that we want to operate over.
[00:01:58.000 --> 00:02:03.000]   And then we use a function from this pre-processing library to transform these wave files
[00:02:03.000 --> 00:02:06.000]   into something that looks more like a sonic spectrogram.
[00:02:06.000 --> 00:02:10.000]   Now, you may not have seen a spectrogram before. You can find lots of apps that do this.
[00:02:10.000 --> 00:02:17.000]   In a spectrogram, the x-axis is time typically, and the y-axis is the frequency of sound.
[00:02:17.000 --> 00:02:21.000]   And then the darkness is the amount of energy at that frequency.
[00:02:21.000 --> 00:02:26.000]   So in music or in science, you typically get these spectrograms that have sort of even intervals
[00:02:26.000 --> 00:02:28.000]   or logarithmic intervals between the frequencies.
[00:02:28.000 --> 00:02:35.000]   But actually when you're processing speech, there's a slightly different transformation that people typically do called MFCC.
[00:02:35.000 --> 00:02:40.000]   And so that's the one that I do here, but you can just roughly think of it as buckets of frequencies
[00:02:40.000 --> 00:02:42.000]   and kind of buckets of time.
[00:02:42.000 --> 00:02:47.000]   So we do that transformation, and then we actually load the training and test set
[00:02:47.000 --> 00:02:51.000]   into the familiar X-train, X-test, Y-train, Y-test values.
[00:02:51.000 --> 00:02:58.000]   This is just like previous videos. X-train was often a set of images in previous videos.
[00:02:58.000 --> 00:03:02.000]   In this video, it's going to be sets of audio spectrograms, essentially,
[00:03:02.000 --> 00:03:05.000]   and X-test is going to be validation data for that.
[00:03:05.000 --> 00:03:09.000]   Y-train is going to be the labels, so 0 corresponds to bed, 1 corresponds to happy,
[00:03:09.000 --> 00:03:11.000]   and 2 corresponds to cat.
[00:03:11.000 --> 00:03:14.000]   And Y-test is the same but corresponding to the test data.
[00:03:14.000 --> 00:03:18.000]   Then we're going to actually reshape our data a little bit.
[00:03:18.000 --> 00:03:21.000]   We're going to add a channel element, and this is because typically with audio,
[00:03:21.000 --> 00:03:23.000]   you're going to have a left channel and a right channel.
[00:03:23.000 --> 00:03:28.000]   Now in this case, we've actually removed the channels, so there really is only one channel.
[00:03:28.000 --> 00:03:32.000]   But this might make the code a little more generalizable to typical audio files
[00:03:32.000 --> 00:03:35.000]   that you'll see out there in the wild.
[00:03:35.000 --> 00:03:38.000]   And then before we do anything else, I think it's nice to take a look
[00:03:38.000 --> 00:03:42.000]   at the data that we're dealing with with the imshow commands.
[00:03:42.000 --> 00:03:45.000]   Now that works super well when we're dealing with images.
[00:03:45.000 --> 00:03:48.000]   You can actually look at the image and see, oh, that's a number 4,
[00:03:48.000 --> 00:03:50.000]   or oh, that's a picture of my friend's face.
[00:03:50.000 --> 00:03:53.000]   With audio spectrograms, it's a little less clear what's going on,
[00:03:53.000 --> 00:03:55.000]   but it's kind of nice to look at anyway.
[00:03:55.000 --> 00:03:59.000]   We could look at the hundredth value of X-train, and we could see that
[00:03:59.000 --> 00:04:04.000]   it seems like it starts off a little quieter and maybe gets a little bit louder.
[00:04:04.000 --> 00:04:06.000]   It's a little hard to interpret.
[00:04:06.000 --> 00:04:15.000]   We can also print out the corresponding Y-train label and see what that was,
[00:04:15.000 --> 00:04:20.000]   and it looks to me like it must be the zeroth label, and that would be "bed."
[00:04:20.000 --> 00:04:27.000]   So this is some kind of distorted spectrogram with somebody saying "bed."
[00:04:27.000 --> 00:04:30.000]   One more thing before we get started, we have to transform Y-train
[00:04:30.000 --> 00:04:32.000]   and Y-test into one-hot versions of those.
[00:04:32.000 --> 00:04:35.000]   So we talk about this a lot in previous videos, and you can find it there,
[00:04:35.000 --> 00:04:40.000]   but essentially going from this single number to a vector of numbers
[00:04:40.000 --> 00:04:43.000]   where the 1 corresponds to the label that we want.
[00:04:43.000 --> 00:04:47.000]   And then, as usual, we're going to start with the simplest possible model,
[00:04:47.000 --> 00:04:49.000]   and in that case, it's a perceptron.
[00:04:49.000 --> 00:04:53.000]   So as usual, we're going to first call flatten to remove all the structure
[00:04:53.000 --> 00:04:56.000]   of our data, so the buckets and the length and the channels.
[00:04:56.000 --> 00:04:58.000]   We're going to flatten all that out into a single vector.
[00:04:58.000 --> 00:05:01.000]   And then we're going to call a dense layer on that,
[00:05:01.000 --> 00:05:04.000]   and that's going to be a fully connected layer with, in this case,
[00:05:04.000 --> 00:05:08.000]   three different outputs, one corresponding to each word that we're trying to classify
[00:05:08.000 --> 00:05:12.000]   and the typical softmax activation function we use when we're trying to do
[00:05:12.000 --> 00:05:14.000]   multi-class classification.
[00:05:14.000 --> 00:05:18.000]   We're going to use categorical cross-entropy as usual and the atom optimizer,
[00:05:18.000 --> 00:05:23.000]   and we're also going to report on accuracy in this case.
[00:05:23.000 --> 00:05:26.000]   All right, so let's fit that model.
[00:05:26.000 --> 00:05:30.000]   And you can see that in this case, because the data set's reasonably small,
[00:05:30.000 --> 00:05:39.000]   the model runs quite fast.
[00:05:39.000 --> 00:05:44.000]   But you can actually see that this very simple linear model gets us around
[00:05:44.000 --> 00:05:49.000]   approximately 80% accuracy on the validation data, which is not bad.
[00:05:49.000 --> 00:05:51.000]   Okay, so now here's the really cool thing.
[00:05:51.000 --> 00:05:55.000]   Because we have our data in such a standard format, we can actually pull
[00:05:55.000 --> 00:05:58.000]   from all the different types of models that we've built in earlier videos
[00:05:58.000 --> 00:06:00.000]   to make this model better.
[00:06:00.000 --> 00:06:03.000]   So the first thing that we can try, and this is something that people really do,
[00:06:03.000 --> 00:06:05.000]   we can apply a convolutional network to this.
[00:06:05.000 --> 00:06:09.000]   Now you might argue that maybe we should use a 1D convolution more like text,
[00:06:09.000 --> 00:06:13.000]   and you can try that, right, because maybe you could think of each frequency
[00:06:13.000 --> 00:06:16.000]   as a separate channel.
[00:06:16.000 --> 00:06:20.000]   But because actually the channels do have meaning, or the frequencies do have
[00:06:20.000 --> 00:06:23.000]   meaning, like two frequencies close to each other actually are kind of
[00:06:23.000 --> 00:06:28.000]   semantically close, I think a 2D convolution is a reasonable thing also to try.
[00:06:28.000 --> 00:06:29.000]   So let's start with that.
[00:06:29.000 --> 00:06:33.000]   And you can find in my ML Class Videos directory, you can actually find examples
[00:06:33.000 --> 00:06:35.000]   of all these different classifiers.
[00:06:35.000 --> 00:06:40.000]   So let's actually just go into CNN.py and see what happens when we paste in
[00:06:40.000 --> 00:06:46.000]   a standard kind of one-level convolutional neural network.
[00:06:46.000 --> 00:06:53.000]   So we can just copy this model code right into our notebook here.
[00:06:53.000 --> 00:07:08.000]   Now we just have to change the input shape to be buckets, len, and channels.
[00:07:08.000 --> 00:07:18.000]   And we can just set this to be a 3 by 3 convolution.
[00:07:18.000 --> 00:07:23.000]   Set the dense layer size to 128.
[00:07:23.000 --> 00:07:29.000]   We can compile the model in the same way.
[00:07:29.000 --> 00:07:31.000]   And then we can fit the model in the exact same way.
[00:07:31.000 --> 00:07:37.000]   And again, because it's such a small number of samples, it learns very fast.
[00:07:37.000 --> 00:07:43.000]   Let's take a look in the app.
[00:07:43.000 --> 00:07:46.000]   And actually this model is very, very good.
[00:07:46.000 --> 00:07:54.000]   So this model gets over 90% accuracy, 93%, 94% accuracy on our test data
[00:07:54.000 --> 00:07:56.000]   just right off the bat, which is really cool.
[00:07:56.000 --> 00:07:58.000]   We've actually taken the machinery that we've learned in different domains
[00:07:58.000 --> 00:08:00.000]   and applied it to this to a different domain.
[00:08:00.000 --> 00:08:04.000]   And the same intuition that we had that convolutions might work better
[00:08:04.000 --> 00:08:06.000]   actually turns out to be the case.
[00:08:06.000 --> 00:08:10.000]   And you might think, well, if one convolution works well,
[00:08:10.000 --> 00:08:12.000]   what about two convolutions?
[00:08:12.000 --> 00:08:16.000]   So we can take this same thing that we did before
[00:08:16.000 --> 00:08:25.000]   and take a convolution and a pooling and then a second convolution and a pooling.
[00:08:25.000 --> 00:08:32.000]   Build this model here, compile it.
[00:08:32.000 --> 00:08:36.000]   And actually we could go into a project.
[00:08:36.000 --> 00:08:41.000]   We could call this one Perceptron.
[00:08:41.000 --> 00:08:46.000]   We could call this guy one convolution.
[00:08:46.000 --> 00:09:10.000]   Call this guy two convolutions.
[00:09:10.000 --> 00:09:14.000]   So you can see here that our two convolution model is actually slightly better
[00:09:14.000 --> 00:09:17.000]   than our one convolution model, which is awesome.
[00:09:17.000 --> 00:09:21.000]   It's like maybe a 94% accuracy versus a 93% accuracy.
[00:09:21.000 --> 00:09:26.000]   But another thing that's pretty glaring, which is that this is the test accuracy,
[00:09:26.000 --> 00:09:29.000]   and on the training data, both the one convolution and the two convolution model
[00:09:29.000 --> 00:09:31.000]   have 100% accuracy.
[00:09:31.000 --> 00:09:34.000]   So it seems like we have an issue with overfitting.
[00:09:34.000 --> 00:09:39.000]   And again, we can apply all the intuitions that we learned on text and image data
[00:09:39.000 --> 00:09:41.000]   to this problem.
[00:09:41.000 --> 00:09:43.000]   So the clear thing to do when you see this, the first thing to try,
[00:09:43.000 --> 00:09:45.000]   is to add some dropout.
[00:09:45.000 --> 00:09:48.000]   So let's put a little bit of dropout in our model.
[00:09:48.000 --> 00:09:51.000]   We can put it in the same place that we did before.
[00:09:51.000 --> 00:09:56.000]   So we can say model add dropout.
[00:09:56.000 --> 00:10:04.000]   Maybe drop out a quarter of the stuff and drop out a quarter of the stuff again.
[00:10:04.000 --> 00:10:20.000]   Compile the model and run fit.
[00:10:20.000 --> 00:10:25.000]   And you can see that the two convolutions with dropout is actually learning slower
[00:10:25.000 --> 00:10:31.000]   on the training data, but it actually kind of continues to improve.
[00:10:31.000 --> 00:10:33.000]   And the same thing happens on the test data.
[00:10:33.000 --> 00:10:37.000]   It starts off a little bit worse, but as it runs over time,
[00:10:37.000 --> 00:10:39.000]   it gets better and better and better.
[00:10:39.000 --> 00:10:43.000]   So this dropout actually allows the model to fit the data even a little bit better
[00:10:43.000 --> 00:10:45.000]   than without the dropout.
[00:10:45.000 --> 00:10:47.000]   So all the things that we expect, all the theory and intuitions that we've learned
[00:10:47.000 --> 00:10:51.000]   so far, they apply to audio equally as well as images.
[00:10:51.000 --> 00:10:53.000]   Which I just think is super cool.
[00:10:53.000 --> 00:10:57.000]   Maybe we'll let that run a little bit.
[00:10:57.000 --> 00:11:00.000]   Then there's one more thing that you can try, which we did on text,
[00:11:00.000 --> 00:11:04.000]   which is we could take LSTMs or GRUs and apply it to audio.
[00:11:04.000 --> 00:11:08.000]   And this might make sense, especially if we had variable length audio files
[00:11:08.000 --> 00:11:10.000]   or much longer audio files, this might make sense.
[00:11:10.000 --> 00:11:14.000]   I think actually CNNs probably make a little more sense for these tiny files
[00:11:14.000 --> 00:11:15.000]   where they run well.
[00:11:15.000 --> 00:11:19.000]   But let's take a peek and see how they do so we can copy the code
[00:11:19.000 --> 00:11:31.000]   from our LSTM video.
[00:11:31.000 --> 00:11:59.000]   [ Video playing ]
[00:11:59.000 --> 00:12:08.000]   And so when we copy the code in, you see that actually we get an error
[00:12:08.000 --> 00:12:10.000]   and it's a shape error.
[00:12:10.000 --> 00:12:12.000]   And it's because LSTM expects a two-dimensional input,
[00:12:12.000 --> 00:12:15.000]   not a three-dimensional input.
[00:12:15.000 --> 00:12:16.000]   So you get this scary error message.
[00:12:16.000 --> 00:12:20.000]   In this case, though, remember we actually added the channel variable later.
[00:12:20.000 --> 00:12:25.000]   So we could do a more complicated reshape, but I think the simplest thing to do
[00:12:25.000 --> 00:12:37.000]   is just undo the reshaping that we did before.
[00:12:37.000 --> 00:12:46.000]   And then we can try the LSTM.
[00:12:46.000 --> 00:12:50.000]   Now, the LSTM's performance is significantly worse than the Convolution's,
[00:12:50.000 --> 00:12:52.000]   but that might be because we had a small LSTM.
[00:12:52.000 --> 00:12:56.000]   It also could be the fact that our data is actually not very long,
[00:12:56.000 --> 00:13:00.000]   and I think LSTMs would matter more as the data gets much longer.
[00:13:00.000 --> 00:13:03.000]   So we could spend some time really doing hyperparameter tuning
[00:13:03.000 --> 00:13:06.000]   and maybe get this LSTM to the same accuracy as CNNs.
[00:13:06.000 --> 00:13:09.000]   But I'll just say for these kind of short audio files,
[00:13:09.000 --> 00:13:13.000]   I think CNNs are going to be faster, train faster and run faster,
[00:13:13.000 --> 00:13:15.000]   and probably the better choice.
[00:13:15.000 --> 00:13:18.000]   But if we were classifying really long conversations,
[00:13:18.000 --> 00:13:21.000]   that's where LSTMs might really shine.
[00:13:21.000 --> 00:13:24.000]   I guess my biggest point here, and we can go deeper in subsequent videos
[00:13:24.000 --> 00:13:27.000]   on all types of audio processing, but the big point that I want to make
[00:13:27.000 --> 00:13:31.000]   is actually the stuff that you're learning is really transferable across domains.
[00:13:31.000 --> 00:13:35.000]   I mean, domain expertise has a huge role to play here,
[00:13:35.000 --> 00:13:40.000]   but this stuff with CNNs is surprisingly transferable in many different areas,
[00:13:40.000 --> 00:13:42.000]   and I think that's just super exciting.
[00:13:42.000 --> 00:13:44.000]   So we'll do some more videos on audio.
[00:13:44.000 --> 00:13:47.000]   We'll also do some more videos on more complicated architectures.
[00:13:47.000 --> 00:13:49.000]   Can't wait to do them.
[00:13:49.000 --> 00:13:52.000]   [no audio]
[00:13:52.000 --> 00:13:55.000]   [no audio]
[00:13:55.000 --> 00:13:58.000]   [no audio]
[00:13:58.000 --> 00:14:03.640]   (SILENCE)

