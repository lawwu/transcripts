
[00:00:00.000 --> 00:00:07.640]   Today, I'm going to talk to you about vision transformers,
[00:00:07.640 --> 00:00:09.440]   since this is all about transformers,
[00:00:09.440 --> 00:00:12.200]   specifically their application
[00:00:12.200 --> 00:00:14.400]   for visual representation learning.
[00:00:14.400 --> 00:00:16.360]   But before we jump into transformers,
[00:00:16.360 --> 00:00:18.080]   I'm going to spend like 10 or 15 minutes
[00:00:18.080 --> 00:00:20.560]   giving you a lot of context on all of this,
[00:00:20.560 --> 00:00:24.320]   and specifically also on the vision part of things,
[00:00:24.320 --> 00:00:27.200]   because I think the majority of what you have seen
[00:00:27.200 --> 00:00:28.960]   and will see will be about language.
[00:00:29.960 --> 00:00:32.280]   All right, so let's get started.
[00:00:32.280 --> 00:00:34.400]   My goal and that of my close collaborators
[00:00:34.400 --> 00:00:37.000]   is to find general visual representation,
[00:00:37.000 --> 00:00:40.720]   and you're going to soon see what that means and why,
[00:00:40.720 --> 00:00:42.640]   or what can we do if we imagine
[00:00:42.640 --> 00:00:45.040]   we have a general visual representation.
[00:00:45.040 --> 00:00:46.680]   The hope is that with this,
[00:00:46.680 --> 00:00:49.640]   we can kickstart all kinds of tasks
[00:00:49.640 --> 00:00:51.280]   that require visual input.
[00:00:51.280 --> 00:00:52.960]   That means most tasks that you do
[00:00:52.960 --> 00:00:55.120]   when you have your eyes open, basically,
[00:00:56.680 --> 00:01:00.240]   because if you have a good understanding of what you see,
[00:01:00.240 --> 00:01:03.040]   then you can much quicker understand what's going on
[00:01:03.040 --> 00:01:04.640]   and what you should do.
[00:01:04.640 --> 00:01:09.720]   And eventually, I have now a little kid since a year,
[00:01:09.720 --> 00:01:13.320]   and so I really want that when he's grown up,
[00:01:13.320 --> 00:01:16.080]   that there is like some kind of robot.
[00:01:16.080 --> 00:01:18.080]   It doesn't need to be nice and pretty like in movies,
[00:01:18.080 --> 00:01:21.640]   just maybe an arm or whatever, that my kid could teach,
[00:01:21.640 --> 00:01:23.520]   or my parents who cannot program
[00:01:23.520 --> 00:01:25.600]   can teach to do some boring task
[00:01:25.600 --> 00:01:27.440]   that they really don't want to do.
[00:01:27.440 --> 00:01:29.440]   And I believe one component of this
[00:01:29.440 --> 00:01:31.800]   is a good visual representation
[00:01:31.800 --> 00:01:33.880]   that generalizes to understanding
[00:01:33.880 --> 00:01:36.000]   the world visually everywhere.
[00:01:36.000 --> 00:01:38.160]   It's not all that's required, but it's one part,
[00:01:38.160 --> 00:01:40.000]   and the part that I'm trying to push.
[00:01:40.000 --> 00:01:42.520]   So this is for context and motivation
[00:01:42.520 --> 00:01:45.080]   on working on general visual representation,
[00:01:45.080 --> 00:01:47.920]   and one good example of a general visual representation
[00:01:47.920 --> 00:01:49.920]   is the humans,
[00:01:49.920 --> 00:01:51.680]   and I'm going to show you what I mean by that.
[00:01:51.680 --> 00:01:55.680]   So here is a task that I give you.
[00:01:55.680 --> 00:01:58.120]   There is three classes, class A, B, and C,
[00:01:58.120 --> 00:02:00.560]   and I give you five images of each class, okay?
[00:02:00.560 --> 00:02:04.040]   And here I give you a new image,
[00:02:04.040 --> 00:02:09.320]   and I'm sure that by now you all know which class it is.
[00:02:09.320 --> 00:02:11.760]   I'm not going to ask because I don't actually see you.
[00:02:11.760 --> 00:02:14.560]   If I was in the room, I would do the raised hands,
[00:02:14.560 --> 00:02:17.160]   but I'm sure you know it's class A now.
[00:02:17.160 --> 00:02:19.280]   Okay, this is fine. We have seen millions of flowers
[00:02:19.280 --> 00:02:20.840]   in our lives, hopefully,
[00:02:21.600 --> 00:02:23.280]   but there is other kinds of pictures,
[00:02:23.280 --> 00:02:25.360]   like this satellite images
[00:02:25.360 --> 00:02:26.960]   that you don't see much in your life.
[00:02:26.960 --> 00:02:29.520]   Some people may have never seen it sometimes,
[00:02:29.520 --> 00:02:32.840]   like when you fly or maybe on TV or in the Internet or so,
[00:02:32.840 --> 00:02:35.680]   but it's rather rare, but still, same story.
[00:02:35.680 --> 00:02:39.760]   Three classes, class A, B, C, five images of each,
[00:02:39.760 --> 00:02:41.720]   and I show you a new image.
[00:02:41.720 --> 00:02:44.720]   This might be a little bit less trivial than the flower,
[00:02:44.720 --> 00:02:48.520]   but I think I've spent enough time talking that by now,
[00:02:48.520 --> 00:02:51.160]   most of you should know that this is class B.
[00:02:51.160 --> 00:02:53.720]   Shows a, what is it, basketball court, right?
[00:02:53.720 --> 00:02:57.760]   All right, now even more abstract.
[00:02:57.760 --> 00:03:00.200]   You don't see this in real life, all right,
[00:03:00.200 --> 00:03:03.160]   but still, I give you images of class A and B.
[00:03:03.160 --> 00:03:05.320]   I have just two to make it a bit easier here
[00:03:05.320 --> 00:03:08.120]   because you need to use your brain a little bit more,
[00:03:08.120 --> 00:03:10.680]   and I show you this new image,
[00:03:10.680 --> 00:03:14.400]   and now I should do a little bit of small talk
[00:03:14.400 --> 00:03:15.960]   to let you think,
[00:03:15.960 --> 00:03:19.120]   like you see that there is like spheres, boxes, and whatnot,
[00:03:19.120 --> 00:03:21.760]   and by now, I hope that most of you know
[00:03:21.760 --> 00:03:24.320]   that this is class A. Why?
[00:03:24.320 --> 00:03:26.880]   Because there is three objects in class A,
[00:03:26.880 --> 00:03:29.640]   and class B is always, what is it, five objects,
[00:03:29.640 --> 00:03:33.080]   no matter what they are, what they look like.
[00:03:33.080 --> 00:03:37.680]   Okay, I think by now, you more or less understand
[00:03:37.680 --> 00:03:40.160]   what I mean when I mean a good visual representation,
[00:03:40.160 --> 00:03:42.400]   general visual representation, right?
[00:03:42.400 --> 00:03:46.640]   Some, I don't know how to call it,
[00:03:46.640 --> 00:03:51.360]   in your brain, in your eyes such that you can quickly see
[00:03:51.360 --> 00:03:53.760]   something new and understand what's going on
[00:03:53.760 --> 00:03:57.480]   with just a few examples, and then generalize from that,
[00:03:57.480 --> 00:04:00.320]   right, and that's the goal.
[00:04:00.320 --> 00:04:02.680]   Then the next step, if you have the goal,
[00:04:02.680 --> 00:04:04.640]   how do we measure progress towards it?
[00:04:04.640 --> 00:04:07.680]   And this is a paper we did a few years ago
[00:04:07.680 --> 00:04:08.800]   with my collaborators,
[00:04:08.800 --> 00:04:10.920]   which we call the Visual Task Adaptation Benchmark.
[00:04:10.920 --> 00:04:13.120]   It's kind of formalization of the little game
[00:04:13.120 --> 00:04:16.280]   that we just played, so it's a benchmark,
[00:04:16.880 --> 00:04:20.280]   and there is some component that you,
[00:04:20.280 --> 00:04:22.440]   or anybody who participates in the benchmark does,
[00:04:22.440 --> 00:04:24.800]   which is creating a model with some data.
[00:04:24.800 --> 00:04:28.160]   We don't really care what data, what model, how, what not.
[00:04:28.160 --> 00:04:30.440]   Just you come with a model.
[00:04:30.440 --> 00:04:35.280]   Then we come with this landscape of all possible visual tasks
[00:04:35.280 --> 00:04:38.360]   that kind of make sense, which is a vague statement,
[00:04:38.360 --> 00:04:41.160]   and we sample some tasks from that,
[00:04:41.160 --> 00:04:44.840]   and this is kind of the task that you have just seen.
[00:04:44.840 --> 00:04:49.160]   They were actually taken out of this Task Adaptation Benchmark,
[00:04:49.160 --> 00:04:53.000]   and we have, for a first step, made 19 such tasks
[00:04:53.000 --> 00:04:56.320]   where we try to cover broad types of visual tasks,
[00:04:56.320 --> 00:04:59.320]   not just classes of natural images
[00:04:59.320 --> 00:05:01.200]   like these dogs and cats things,
[00:05:01.200 --> 00:05:04.480]   but also of very specialized images like satellite image,
[00:05:04.480 --> 00:05:07.360]   also non-classification tasks that involve counting,
[00:05:07.360 --> 00:05:09.360]   like the one I showed you before, right,
[00:05:09.360 --> 00:05:13.200]   but that can be expressed in this simple classification API,
[00:05:13.200 --> 00:05:15.840]   but that logically requires some more thinking.
[00:05:15.840 --> 00:05:19.760]   Some things like distance, we have something with cars
[00:05:19.760 --> 00:05:22.560]   and with distance of the closest car and things like that.
[00:05:22.560 --> 00:05:27.000]   It should cover a broad range of variation,
[00:05:27.000 --> 00:05:32.000]   and then with the model that you came to this benchmark,
[00:05:32.000 --> 00:05:35.760]   you can do some adaptation step on each of the datasets,
[00:05:35.760 --> 00:05:37.560]   one after another or at the same time.
[00:05:37.560 --> 00:05:38.920]   It doesn't really matter,
[00:05:38.920 --> 00:05:40.720]   but then you should have, as a result,
[00:05:40.720 --> 00:05:44.040]   a model of this dataset, which is very small.
[00:05:44.040 --> 00:05:46.960]   It just has seen a few examples for each class
[00:05:46.960 --> 00:05:48.560]   that then performs well there,
[00:05:48.560 --> 00:05:50.880]   and then we just take the average score
[00:05:50.880 --> 00:05:52.040]   across all of these tasks,
[00:05:52.040 --> 00:05:54.040]   and this is what we call the VTAP task,
[00:05:54.040 --> 00:05:56.120]   and this is how, for now,
[00:05:56.120 --> 00:06:01.040]   we judge how good of a general visual representation
[00:06:01.040 --> 00:06:03.320]   does your model and adaptation algorithm have,
[00:06:03.320 --> 00:06:08.560]   and now just for some nomenclature, this preparation,
[00:06:08.560 --> 00:06:10.720]   we have words that we often use pre-training.
[00:06:10.720 --> 00:06:12.560]   Sometimes we call it the upstream,
[00:06:12.560 --> 00:06:15.640]   like upstream data, upstream training, something,
[00:06:15.640 --> 00:06:18.680]   so I may use this word interchangeably with pre-training,
[00:06:18.680 --> 00:06:20.200]   and then there is the second part,
[00:06:20.200 --> 00:06:21.920]   which we usually call transfer,
[00:06:21.920 --> 00:06:23.720]   and then sometimes we say downstream,
[00:06:23.720 --> 00:06:29.440]   and the adaptation, in principle, it's whatever you want,
[00:06:29.440 --> 00:06:32.200]   but for our work, we almost always just use very simple,
[00:06:32.200 --> 00:06:34.680]   fine-tuning without any bits and whistles
[00:06:34.680 --> 00:06:36.720]   because it's simple and works well.
[00:06:36.720 --> 00:06:39.600]   In general, we try to do things as simple as possible.
[00:06:39.600 --> 00:06:42.720]   It still works well, and so sometimes I even just say,
[00:06:42.720 --> 00:06:44.280]   like, fine-tuning when fine-tuning.
[00:06:44.280 --> 00:06:47.120]   That means moving from this pre-training to the transfer.
[00:06:47.120 --> 00:06:52.440]   All right, so so far for the settings, so far so good?
[00:06:52.440 --> 00:06:58.600]   Good. Then the question is, how do we get there,
[00:06:58.600 --> 00:07:00.640]   and we spend a lot of time thinking about this
[00:07:00.640 --> 00:07:02.240]   and trying different things,
[00:07:02.240 --> 00:07:04.600]   and this is also roughly the outline
[00:07:04.600 --> 00:07:07.520]   of all that I have available to talk about,
[00:07:07.520 --> 00:07:10.120]   which doesn't mean we're going to cover everything,
[00:07:10.120 --> 00:07:13.640]   so I'm not going to go, like, through the outline exactly,
[00:07:13.640 --> 00:07:15.160]   but you will see this again and again,
[00:07:15.160 --> 00:07:17.040]   and as you see, vision transformer,
[00:07:17.040 --> 00:07:19.080]   field transformer only comes a little bit later.
[00:07:19.080 --> 00:07:20.680]   There's some stuff before that,
[00:07:20.680 --> 00:07:23.640]   so this one, just really quickly
[00:07:23.640 --> 00:07:25.280]   because it doesn't matter for this course,
[00:07:25.280 --> 00:07:28.440]   is that we spend some time trying self-supervised pre-training
[00:07:28.440 --> 00:07:30.200]   which is very popular in language,
[00:07:30.200 --> 00:07:32.760]   and in vision only recently has become popular,
[00:07:32.760 --> 00:07:35.480]   and it doesn't work that way.
[00:07:35.480 --> 00:07:38.080]   You don't need to understand these bars,
[00:07:38.080 --> 00:07:40.000]   but basically higher is better,
[00:07:40.000 --> 00:07:42.880]   and here, just look at the blue ones.
[00:07:42.880 --> 00:07:47.000]   That's the VTAP score for this few-shot VTAP,
[00:07:47.000 --> 00:07:49.680]   and self-supervised learning performs like this bar.
[00:07:49.680 --> 00:07:52.520]   We tried multiple methods and multiple models and so on.
[00:07:52.520 --> 00:07:53.960]   It was a proper good benchmark,
[00:07:53.960 --> 00:07:55.560]   but it was a couple years ago.
[00:07:55.560 --> 00:08:00.080]   Then we moved on to semi-supervised training,
[00:08:00.080 --> 00:08:03.440]   so a few labeled examples and a ton of unlabeled examples.
[00:08:03.440 --> 00:08:05.000]   That's this next blue bar.
[00:08:05.000 --> 00:08:06.400]   Did you actually see the mouse cursor?
[00:08:06.400 --> 00:08:08.000]   Sorry.
[00:08:08.000 --> 00:08:11.680]   - We don't see the mouse cursor.
[00:08:11.680 --> 00:08:16.240]   - Maybe I need to do some laser --
[00:08:16.240 --> 00:08:17.640]   - Oh, we can see it. We can see it.
[00:08:17.640 --> 00:08:19.960]   - Yeah. - Oh, okay.
[00:08:19.960 --> 00:08:22.520]   - Yeah, so then semi-supervised is that blue bar
[00:08:22.520 --> 00:08:24.880]   which is a lot higher than this other blue bar,
[00:08:24.880 --> 00:08:26.400]   so what this means to us
[00:08:26.400 --> 00:08:28.280]   is that by adding a few labeled examples,
[00:08:28.280 --> 00:08:31.040]   we're able to get much better
[00:08:31.040 --> 00:08:33.240]   or much more general visual representation.
[00:08:33.240 --> 00:08:36.240]   Then I'm not going to spend more time on this
[00:08:36.240 --> 00:08:38.320]   and how exactly and so on,
[00:08:38.320 --> 00:08:39.640]   but I'm going to move to the next one,
[00:08:39.640 --> 00:08:42.120]   which was for us kind of a breakthrough
[00:08:42.120 --> 00:08:43.600]   when we figured out that, well,
[00:08:43.600 --> 00:08:47.280]   if we just scale up fully-supervised pre-training,
[00:08:47.280 --> 00:08:49.840]   then we get really much better representations
[00:08:49.840 --> 00:08:51.800]   than everything we've seen before,
[00:08:51.800 --> 00:08:54.080]   and here I want to briefly spend some time on that one
[00:08:54.080 --> 00:08:56.480]   because it's the precursor to using vision
[00:08:56.480 --> 00:08:58.080]   or transformers in vision.
[00:08:58.080 --> 00:09:00.960]   So the idea is simple.
[00:09:00.960 --> 00:09:03.200]   There are tons of images on the Internet.
[00:09:03.200 --> 00:09:04.880]   That's always what you hear is motivation
[00:09:04.880 --> 00:09:07.760]   for semi-supervised or unsupervised learning, right?
[00:09:07.760 --> 00:09:10.160]   But actually, where these images come from,
[00:09:10.160 --> 00:09:12.520]   there's almost always some extra information,
[00:09:12.520 --> 00:09:14.520]   like surrounding the image on the Web
[00:09:14.520 --> 00:09:16.720]   or if you collect it otherwise,
[00:09:16.720 --> 00:09:18.440]   there's some extra information there
[00:09:18.440 --> 00:09:21.240]   that you could use as some weak source of information
[00:09:21.240 --> 00:09:22.840]   or some weak label, right?
[00:09:22.840 --> 00:09:25.080]   Then it happens that in Google,
[00:09:25.080 --> 00:09:28.080]   there's some team that actually does this for production,
[00:09:28.080 --> 00:09:31.240]   and they have collected already a large dataset
[00:09:31.240 --> 00:09:34.280]   with some pipeline that from the surrounding signals
[00:09:34.280 --> 00:09:36.040]   somewhat automatically,
[00:09:36.040 --> 00:09:38.240]   but very noisily annotates the images,
[00:09:38.240 --> 00:09:42.160]   and we wanted to figure out how far can we go
[00:09:42.160 --> 00:09:43.760]   when we scale up pre-training.
[00:09:43.760 --> 00:09:47.920]   Then, long story short, you need a couple of ingredients.
[00:09:47.920 --> 00:09:50.680]   One is patience. I really like this plot.
[00:09:50.680 --> 00:09:53.120]   This is one of the curves of just pre-training
[00:09:53.120 --> 00:09:55.040]   on large data with large models.
[00:09:55.040 --> 00:09:57.640]   The details don't really matter.
[00:09:57.640 --> 00:10:00.360]   The gist is that if I zoom into this little box,
[00:10:00.360 --> 00:10:04.320]   I see this here, and this is the metric for the training,
[00:10:04.320 --> 00:10:06.280]   like the performance in upstream.
[00:10:06.280 --> 00:10:09.440]   Then I see after spending eight GPU weeks of compute,
[00:10:09.440 --> 00:10:10.560]   what does GPU week mean?
[00:10:10.560 --> 00:10:13.920]   It means eight GPUs for a week or, sorry,
[00:10:13.920 --> 00:10:17.960]   one GPU for eight weeks or eight GPUs for one week
[00:10:17.960 --> 00:10:21.560]   or 16 GPUs for half week and so on, right?
[00:10:21.560 --> 00:10:24.000]   But this looks flat. A reasonable person would say,
[00:10:24.000 --> 00:10:26.760]   "Yeah, there's no progress for a week on eight GPUs.
[00:10:26.760 --> 00:10:29.000]   This is flat. I'm going to stop and try something else,"
[00:10:29.000 --> 00:10:31.600]   but we are not reasonable, so we keep going,
[00:10:31.600 --> 00:10:33.920]   and this is what the exact same spot looks like
[00:10:33.920 --> 00:10:35.880]   after eight GPU months of training,
[00:10:35.880 --> 00:10:39.480]   and you can clearly see the things are progressing, right?
[00:10:39.480 --> 00:10:42.240]   So it may not always be obvious, and you need patience.
[00:10:42.240 --> 00:10:45.880]   The second thing is that you actually need
[00:10:45.880 --> 00:10:47.120]   to scale up everything,
[00:10:47.120 --> 00:10:49.000]   so this was work done with ResNets,
[00:10:49.000 --> 00:10:50.640]   not yet with transformers.
[00:10:50.640 --> 00:10:52.760]   I see you see a lot of ResNet models here.
[00:10:52.760 --> 00:10:55.000]   The x-axis is the number of images available.
[00:10:55.000 --> 00:10:57.000]   In vision, there is this image in the dataset,
[00:10:57.000 --> 00:11:00.400]   which is a very common, super common dataset for pre-training,
[00:11:00.400 --> 00:11:02.560]   which has 1.3 million images.
[00:11:02.560 --> 00:11:04.560]   There's another one which has 10 times more images
[00:11:04.560 --> 00:11:07.640]   that's still public, and then there is one subset
[00:11:07.640 --> 00:11:09.200]   from this internal group
[00:11:09.200 --> 00:11:11.240]   that has 300 million labeled images,
[00:11:11.240 --> 00:11:16.480]   so the y-axis is measure of accuracy on some tasks,
[00:11:16.480 --> 00:11:18.600]   and we tried many. They all look similar,
[00:11:20.000 --> 00:11:22.000]   and the dots are differently sized ResNets.
[00:11:22.000 --> 00:11:25.320]   The blue dot is the standard ResNet 50 that everybody uses.
[00:11:25.320 --> 00:11:27.480]   If this one, you trained on more data,
[00:11:27.480 --> 00:11:29.040]   it looks promising at first,
[00:11:29.040 --> 00:11:30.960]   but if you go to even more data, it looks like,
[00:11:30.960 --> 00:11:34.280]   oh, okay, this doesn't really seem that useful,
[00:11:34.280 --> 00:11:37.280]   and this is what most people have been doing for a long time,
[00:11:37.280 --> 00:11:39.960]   and a lot of people, even in Google, were like,
[00:11:39.960 --> 00:11:43.840]   yeah, I tried this internal checkpoint on these tons of data.
[00:11:43.840 --> 00:11:45.800]   It doesn't really help that much.
[00:11:45.800 --> 00:11:48.000]   However, what we found out, and in hindsight,
[00:11:48.000 --> 00:11:51.720]   it's kind of obvious, is that you actually need to scale
[00:11:51.720 --> 00:11:53.760]   not just the data but also the model.
[00:11:53.760 --> 00:11:57.520]   Here, this blue dot is a gigantic ResNet that is slow as hell,
[00:11:57.520 --> 00:11:59.720]   but when you scale this up together with the data,
[00:11:59.720 --> 00:12:02.320]   you keep getting benefit with adding more data,
[00:12:02.320 --> 00:12:03.840]   and then if you do these two things,
[00:12:03.840 --> 00:12:05.720]   scale up everything and be patient,
[00:12:05.720 --> 00:12:10.280]   be patient could also be quite scale up your patience.
[00:12:10.280 --> 00:12:14.040]   Then you get a lot of benefits,
[00:12:14.040 --> 00:12:17.040]   so here there is a few short transfer learning.
[00:12:17.040 --> 00:12:19.000]   They're what I showed you before,
[00:12:19.000 --> 00:12:22.400]   and on the x-axis is size of the model,
[00:12:22.400 --> 00:12:25.480]   on the y-axis is the accuracy on one of these tasks,
[00:12:25.480 --> 00:12:27.440]   but again, others look similar,
[00:12:27.440 --> 00:12:29.040]   and these three different curves
[00:12:29.040 --> 00:12:31.600]   are featuring with different data set sizes.
[00:12:31.600 --> 00:12:33.240]   The green one being the standard one,
[00:12:33.240 --> 00:12:36.120]   you don't really see benefit or small benefit
[00:12:36.120 --> 00:12:37.640]   from going with larger models.
[00:12:37.640 --> 00:12:39.000]   The blue one is 10 times larger.
[00:12:39.000 --> 00:12:42.000]   You start seeing some slope upwards,
[00:12:42.000 --> 00:12:43.760]   but really only with this giant data,
[00:12:43.760 --> 00:12:46.240]   you start getting better and better and better
[00:12:46.240 --> 00:12:48.080]   at this few short transfer learning
[00:12:48.080 --> 00:12:49.720]   when you pre-train on more and more data
[00:12:49.720 --> 00:12:52.000]   with larger and larger models.
[00:12:52.000 --> 00:12:55.560]   Second benefit that we did not anticipate really at all,
[00:12:55.560 --> 00:12:58.400]   but then found out is that these models are super robust
[00:12:58.400 --> 00:12:59.840]   when you scale everything up.
[00:12:59.840 --> 00:13:02.240]   This is ObjectNet.
[00:13:02.240 --> 00:13:04.320]   It's a data set that's specifically designed
[00:13:04.320 --> 00:13:05.240]   to measure robustness,
[00:13:05.240 --> 00:13:07.120]   and it shows things in crazy,
[00:13:07.120 --> 00:13:10.200]   like a chair in the bathtub and things like that,
[00:13:10.200 --> 00:13:12.160]   and you should recognize it as a chair.
[00:13:13.920 --> 00:13:17.480]   Here, the pink dots are basically how existing models,
[00:13:17.480 --> 00:13:19.920]   and x-axis is, again, how large is the model,
[00:13:19.920 --> 00:13:22.600]   and pink dot is existing ones from the literature,
[00:13:22.600 --> 00:13:24.680]   and then these lines, same color coding,
[00:13:24.680 --> 00:13:26.080]   is what we found out.
[00:13:26.080 --> 00:13:28.160]   Again, you see this large data,
[00:13:28.160 --> 00:13:29.520]   and then going to large model
[00:13:29.520 --> 00:13:31.840]   just gives you amazing benefits on,
[00:13:31.840 --> 00:13:34.360]   like in this case, out-of-distribution robustness.
[00:13:34.360 --> 00:13:38.280]   This was amazing.
[00:13:38.280 --> 00:13:42.160]   Scale up everything, be patient, and get huge benefit.
[00:13:42.160 --> 00:13:43.960]   - Sorry, Lucas.
[00:13:43.960 --> 00:13:45.040]   Sorry for interrupting you,
[00:13:45.040 --> 00:13:47.840]   but there is a question from a student in the class.
[00:13:47.840 --> 00:13:49.400]   - Yep. - Right.
[00:13:49.400 --> 00:13:51.960]   Do you want to unmute yourself and ask it yourself?
[00:13:51.960 --> 00:13:55.040]   - Yeah, I can ask my question.
[00:13:55.040 --> 00:13:55.960]   Can people hear me?
[00:13:55.960 --> 00:13:56.800]   Maybe there's some-- - Yes.
[00:13:56.800 --> 00:13:57.640]   - I'm sorry, one second.
[00:13:57.640 --> 00:13:58.480]   Let me just step away real quick.
[00:13:58.480 --> 00:13:59.840]   Yeah, so the question I wanna know is,
[00:13:59.840 --> 00:14:02.120]   what work has been done characterizing the parameters
[00:14:02.120 --> 00:14:04.040]   after pre-training finishes?
[00:14:04.040 --> 00:14:06.960]   Like, the reason why I'm motivating this question is,
[00:14:06.960 --> 00:14:09.320]   it seems like we do this tremendous amount of pre-training,
[00:14:09.320 --> 00:14:10.280]   but it seems like we might be able
[00:14:10.280 --> 00:14:12.000]   to significantly reduce that
[00:14:12.000 --> 00:14:14.520]   if we just have smarter initialization schemes.
[00:14:14.520 --> 00:14:17.680]   - Yeah, you know, I've been thinking this
[00:14:17.680 --> 00:14:19.560]   for a long time, actually, also.
[00:14:19.560 --> 00:14:25.440]   And they've come to conclude that I think not.
[00:14:25.440 --> 00:14:28.200]   I think there is, like, two parts.
[00:14:28.200 --> 00:14:30.800]   One is, like, what I like to call
[00:14:30.800 --> 00:14:33.080]   hand-wavy the numerics of the weights.
[00:14:33.080 --> 00:14:35.200]   You know, that everything is in a nice range,
[00:14:35.200 --> 00:14:38.320]   such that it can have nice input/output functions,
[00:14:38.320 --> 00:14:41.160]   and so on, and that your optimizer can do steps
[00:14:41.160 --> 00:14:44.440]   that make reasonable change to the input/output function,
[00:14:44.440 --> 00:14:46.600]   but not too large, and so on.
[00:14:46.600 --> 00:14:48.120]   I think that is part of it,
[00:14:48.120 --> 00:14:50.040]   and that you can get through good init
[00:14:50.040 --> 00:14:52.040]   or good normalizations and whatnot.
[00:14:52.040 --> 00:14:54.880]   But then I also think there is,
[00:14:54.880 --> 00:14:57.040]   I do think that these models memorize a lot,
[00:14:57.040 --> 00:14:59.480]   and then, personally, I believe,
[00:14:59.480 --> 00:15:01.560]   but I don't know of evidence or so,
[00:15:01.560 --> 00:15:05.280]   that these models do more kind of, you know,
[00:15:05.280 --> 00:15:09.080]   remembering similarity to things they've seen in training.
[00:15:09.080 --> 00:15:11.160]   And then, as you grow things up,
[00:15:11.160 --> 00:15:13.840]   they have more memory, and they have seen more things,
[00:15:13.840 --> 00:15:16.480]   so they should be better on more newer things,
[00:15:16.480 --> 00:15:19.240]   because there's more similar things they have seen.
[00:15:19.240 --> 00:15:21.560]   And this, I don't think you can, like,
[00:15:21.560 --> 00:15:24.400]   just create one shot from initialization.
[00:15:24.400 --> 00:15:29.320]   But I don't have the immediate pointer to a paper
[00:15:29.320 --> 00:15:31.840]   at the top of my head now to answer your question.
[00:15:31.840 --> 00:15:32.760]   - Okay, thank you.
[00:15:32.760 --> 00:15:36.200]   - I think we also have more questions,
[00:15:36.200 --> 00:15:40.160]   so has posted on the chat and is raising his hand.
[00:15:40.160 --> 00:15:44.400]   Maybe in this order, you wanna ask your question first?
[00:15:44.400 --> 00:15:46.000]   - Yeah, for sure, I can go ahead.
[00:15:46.000 --> 00:15:50.760]   So I just have a quick clarification on this chart right here,
[00:15:50.760 --> 00:15:52.840]   the chart number three.
[00:15:52.840 --> 00:15:54.760]   The bit L and bit M and bit S,
[00:15:54.760 --> 00:15:57.840]   are they the same model architecture,
[00:15:57.840 --> 00:15:59.920]   but just trained on different datasets?
[00:15:59.920 --> 00:16:02.720]   So the bit S is trained on the 1.3 million
[00:16:02.720 --> 00:16:05.560]   all the way to the 300 million image dataset for bit L?
[00:16:05.560 --> 00:16:09.480]   - Yes and no.
[00:16:09.480 --> 00:16:11.640]   The architecture is here on the x-axis.
[00:16:11.640 --> 00:16:13.960]   So within one vertical slice,
[00:16:13.960 --> 00:16:16.000]   these are the same architecture.
[00:16:16.000 --> 00:16:18.760]   And then the different points are random restarts,
[00:16:18.760 --> 00:16:20.280]   because when you do future learning,
[00:16:20.280 --> 00:16:21.480]   there is a lot of variance
[00:16:21.480 --> 00:16:24.120]   in which few examples do you see.
[00:16:24.120 --> 00:16:26.200]   And then again, this next vertical slice
[00:16:26.200 --> 00:16:27.640]   is the same model and so on.
[00:16:27.640 --> 00:16:30.320]   And as you go to the right, the model gets larger.
[00:16:30.320 --> 00:16:32.840]   And so you can see that for this little data,
[00:16:32.840 --> 00:16:35.400]   going to larger model doesn't really help you much
[00:16:35.400 --> 00:16:37.920]   for pre-training, only for this giant data,
[00:16:37.920 --> 00:16:39.240]   everything's the giant data,
[00:16:39.240 --> 00:16:42.200]   not necessarily giant model in this case.
[00:16:42.200 --> 00:16:44.680]   - Right, that makes a lot of sense, thank you.
[00:16:44.680 --> 00:16:45.520]   - Okay.
[00:16:45.520 --> 00:16:48.600]   - Do you have a question?
[00:16:48.600 --> 00:16:51.560]   Oh, I see you're raising your hand as well.
[00:16:51.560 --> 00:16:53.320]   Go ahead and let Otto.
[00:16:53.320 --> 00:16:54.320]   - Hey, yeah, thanks.
[00:16:54.320 --> 00:17:00.040]   What is the intuition for the upstream performance
[00:17:00.040 --> 00:17:03.000]   in figure one spiking so suddenly
[00:17:03.000 --> 00:17:07.720]   at like 60 or 40 points in training?
[00:17:07.720 --> 00:17:08.560]   - Here, right?
[00:17:08.560 --> 00:17:09.480]   Yeah.
[00:17:09.480 --> 00:17:10.920]   - Yeah, yeah, I'm looking at it again,
[00:17:10.920 --> 00:17:12.920]   like around one point, like, I don't know,
[00:17:12.920 --> 00:17:15.560]   that just seems like an odd looking training curve.
[00:17:15.560 --> 00:17:19.000]   So like, what's the intuition behind that?
[00:17:19.000 --> 00:17:21.600]   - Yeah, this is old school computer vision thing,
[00:17:21.600 --> 00:17:24.040]   or old school, I mean, a few years ago.
[00:17:24.040 --> 00:17:26.240]   Is this when the learning rate changes?
[00:17:26.240 --> 00:17:28.440]   In computer vision, it used to be very common
[00:17:28.440 --> 00:17:31.520]   to have the learning rate in a kind of staircase pattern.
[00:17:31.520 --> 00:17:34.000]   So it's constant for a while, and then you stop,
[00:17:34.000 --> 00:17:36.160]   you divide the learning rate by 10, usually,
[00:17:36.160 --> 00:17:38.280]   boom, smaller, and then you continue.
[00:17:38.280 --> 00:17:40.680]   And this gives you this huge jump.
[00:17:40.680 --> 00:17:42.880]   And nowadays, people don't use this much anymore.
[00:17:42.880 --> 00:17:44.720]   And this work was like three years ago, I think,
[00:17:44.720 --> 00:17:47.360]   or two or three years ago, I don't remember.
[00:17:47.360 --> 00:17:48.960]   It was very common back then.
[00:17:48.960 --> 00:17:51.800]   And nowadays, people use more continuously changing
[00:17:51.800 --> 00:17:54.120]   learning rate schedule, and then you don't really have
[00:17:54.120 --> 00:17:55.960]   this sudden change anymore.
[00:17:55.960 --> 00:17:57.760]   But if you would overlay it, it would be like
[00:17:57.760 --> 00:18:00.720]   more continuously, but going roughly the same.
[00:18:00.720 --> 00:18:03.000]   And then in language, I think most people,
[00:18:03.000 --> 00:18:05.560]   or many people use just linearly decreasing
[00:18:05.560 --> 00:18:07.240]   learning rate schedule, where also you don't see
[00:18:07.240 --> 00:18:11.000]   this effect, because learning rate continuously decreases.
[00:18:11.000 --> 00:18:12.680]   - Okay, yeah, sounds good, thanks.
[00:18:12.680 --> 00:18:16.760]   - And then this is what, because you asked for,
[00:18:16.760 --> 00:18:18.640]   about this dotted line.
[00:18:18.640 --> 00:18:21.160]   Actually here, if you're like here, you could say,
[00:18:21.160 --> 00:18:22.840]   okay, but this is excessive, right?
[00:18:22.840 --> 00:18:26.400]   Maybe it does really seem almost flat.
[00:18:26.400 --> 00:18:28.920]   Maybe you could have started the decay earlier,
[00:18:28.920 --> 00:18:31.800]   and earlier, and earlier, and then you would get the same,
[00:18:31.800 --> 00:18:32.920]   but much quicker.
[00:18:32.920 --> 00:18:35.840]   And this one shows what would happen then.
[00:18:35.840 --> 00:18:39.160]   And you do land at much worse place in the end
[00:18:39.160 --> 00:18:41.080]   than with the patient.
[00:18:41.080 --> 00:18:43.440]   - All right, yeah, yeah, that makes sense.
[00:18:43.440 --> 00:18:45.400]   Thanks.
[00:18:45.400 --> 00:18:49.080]   - Was there more question, or I continue?
[00:18:49.080 --> 00:18:51.960]   - I think both of you have your answers.
[00:18:51.960 --> 00:18:53.960]   - 'Cause I need to mention, I don't see you,
[00:18:53.960 --> 00:18:55.960]   I just see my slide.
[00:18:55.960 --> 00:18:58.560]   - Yeah, it's fine, we can coordinate that with this.
[00:18:58.560 --> 00:19:03.320]   - Hi, yeah, so I just wanted to make sure
[00:19:03.320 --> 00:19:05.000]   that I'm on the same page.
[00:19:05.000 --> 00:19:08.320]   So basically what you're trying to do is multitask learning
[00:19:08.320 --> 00:19:11.960]   with convolutional neural networks/LSTMs, right?
[00:19:11.960 --> 00:19:13.320]   That's kind of like ResNet.
[00:19:13.320 --> 00:19:17.160]   But you're doing multitask learning, correct?
[00:19:17.160 --> 00:19:20.160]   - No, where does the multitask come from?
[00:19:20.160 --> 00:19:21.920]   Or where does it come from?
[00:19:21.920 --> 00:19:24.960]   - Because like, initially, like you showed like different,
[00:19:24.960 --> 00:19:26.640]   - Ah, yeah, okay.
[00:19:26.640 --> 00:19:28.920]   - Yeah, okay.
[00:19:28.920 --> 00:19:30.720]   - So there is two phases.
[00:19:30.720 --> 00:19:33.400]   The first one is the pre-training.
[00:19:33.400 --> 00:19:36.320]   And this pre-training, I didn't mention it yet.
[00:19:36.320 --> 00:19:38.960]   I just said, I don't care what you do in the pre-training,
[00:19:38.960 --> 00:19:41.600]   just pre-train somehow, and give me the model.
[00:19:41.600 --> 00:19:45.320]   And then I test it on multiple tasks independently.
[00:19:45.320 --> 00:19:47.040]   And I'm tested on multiple tasks,
[00:19:47.040 --> 00:19:49.080]   means like transfer it to that task,
[00:19:49.080 --> 00:19:51.920]   which in our case means fine-tune it just on the task,
[00:19:51.920 --> 00:19:54.200]   and then see how well it does, and so on.
[00:19:54.200 --> 00:19:55.520]   But it could mean other things.
[00:19:55.520 --> 00:19:58.800]   Like later we moved to just learning a linear regression
[00:19:58.800 --> 00:20:01.160]   on top of the embeddings for each task.
[00:20:01.160 --> 00:20:03.240]   And now during the pre-training,
[00:20:03.240 --> 00:20:05.760]   what we do is just regular supervised learning,
[00:20:05.760 --> 00:20:07.520]   but just scaling everything up.
[00:20:07.520 --> 00:20:09.680]   And regular supervised learning is just,
[00:20:09.680 --> 00:20:13.360]   well, not multitask, but multilabel,
[00:20:13.360 --> 00:20:14.800]   in the sense that an image could have
[00:20:14.800 --> 00:20:17.280]   a couple labels or not, but it usually doesn't have.
[00:20:17.280 --> 00:20:20.200]   - This is minor. - Okay, got it.
[00:20:20.200 --> 00:20:21.040]   - Thanks.
[00:20:21.040 --> 00:20:27.440]   - All right, we have a question.
[00:20:27.440 --> 00:20:28.760]   - Yeah, just have a quick follow-up
[00:20:28.760 --> 00:20:30.360]   about the question rather than,
[00:20:30.360 --> 00:20:33.960]   like the discussion rather than started about this,
[00:20:33.960 --> 00:20:37.400]   it's like memorization, or it's more memorizing the data
[00:20:37.400 --> 00:20:39.040]   in pre-training datasets.
[00:20:39.040 --> 00:20:40.520]   So I know in the language side,
[00:20:40.520 --> 00:20:42.080]   there's a quite interesting phenomenon
[00:20:42.080 --> 00:20:45.280]   that you can pre-train on a synthetic language
[00:20:45.280 --> 00:20:48.400]   that's, it doesn't have any semantic meaning,
[00:20:48.400 --> 00:20:49.960]   but it only has structural,
[00:20:49.960 --> 00:20:52.600]   like paired premises or things like that.
[00:20:52.600 --> 00:20:56.280]   And that actually gives you almost the same boost
[00:20:56.280 --> 00:20:59.520]   in your downstream transfer as a normal pre-training.
[00:20:59.520 --> 00:21:02.160]   So I wonder if, say like,
[00:21:02.160 --> 00:21:04.600]   so this means like in for language, right,
[00:21:04.600 --> 00:21:07.480]   the structure seems to make a lot of contribution,
[00:21:07.480 --> 00:21:09.680]   which can be replaced by visualization.
[00:21:09.680 --> 00:21:11.040]   But I don't know if it's an image,
[00:21:11.040 --> 00:21:13.640]   it's a different case, maybe to have people done,
[00:21:13.640 --> 00:21:17.960]   maybe some synthetic pre-training data set for image.
[00:21:17.960 --> 00:21:19.920]   - Yeah, there was a paper,
[00:21:19.920 --> 00:21:22.320]   I forgot the name and the authors,
[00:21:22.320 --> 00:21:24.600]   but it creates completely synthetic images
[00:21:24.600 --> 00:21:27.440]   and like not even rendering of some realistic things,
[00:21:27.440 --> 00:21:31.880]   but just completely patterns, waves, and shapes and so on,
[00:21:31.880 --> 00:21:33.760]   and uses that for pre-training.
[00:21:33.760 --> 00:21:37.040]   And then it shows that they get almost the same performance
[00:21:37.040 --> 00:21:38.080]   as ImageNet quickly,
[00:21:38.080 --> 00:21:40.440]   they actually do this with vision transformers.
[00:21:41.520 --> 00:21:45.600]   But yeah, they never go further or it is not clear,
[00:21:45.600 --> 00:21:47.920]   you know, they kind of show that you can almost get
[00:21:47.920 --> 00:21:49.200]   to this point here.
[00:21:49.200 --> 00:21:53.480]   That is not clear how much further can you go with this.
[00:21:53.480 --> 00:21:56.000]   And I think probably not much further,
[00:21:56.000 --> 00:21:59.160]   but it's just me guessing that not much further,
[00:21:59.160 --> 00:22:00.600]   I don't have evidence for it.
[00:22:00.600 --> 00:22:04.880]   - Right, so I have one question
[00:22:04.880 --> 00:22:07.360]   and then we can continue with the talk.
[00:22:07.360 --> 00:22:09.880]   Said that you think like the large vision models
[00:22:09.880 --> 00:22:11.920]   are like learning some sort of similarity
[00:22:11.920 --> 00:22:13.040]   to the data set they're trained on.
[00:22:13.040 --> 00:22:14.480]   So do you think they are behaving
[00:22:14.480 --> 00:22:16.800]   like prototypical networks, in a sense?
[00:22:16.800 --> 00:22:19.760]   - They're behaving like what networks?
[00:22:19.760 --> 00:22:21.720]   - Oh, so like prototypical networks?
[00:22:21.720 --> 00:22:24.760]   Essentially like when you're doing pre-short learning,
[00:22:24.760 --> 00:22:26.640]   you just say like, "I'm going to learn a network."
[00:22:26.640 --> 00:22:27.480]   - Yeah, yeah, yeah.
[00:22:27.480 --> 00:22:28.920]   - And learn the metric space.
[00:22:28.920 --> 00:22:36.200]   - Probably not exactly, but close-ish.
[00:22:39.240 --> 00:22:40.560]   - I mean, I cannot really say
[00:22:40.560 --> 00:22:44.000]   because this is just some intuitive guess that I have.
[00:22:44.000 --> 00:22:45.600]   That's what they do, but nobody really knows
[00:22:45.600 --> 00:22:46.960]   what the models do, right?
[00:22:46.960 --> 00:22:52.480]   Yeah, I mean, we do get much more,
[00:22:52.480 --> 00:22:54.680]   when we do something like prototypical networks
[00:22:54.680 --> 00:22:57.560]   for the future learning with these pre-trained models,
[00:22:57.560 --> 00:23:00.880]   we do get worse performance than when we do fine-tuning.
[00:23:00.880 --> 00:23:03.880]   So there is a bit more to it still.
[00:23:03.880 --> 00:23:06.800]   However, I don't know what is this more.
[00:23:07.800 --> 00:23:09.120]   (laughs)
[00:23:09.120 --> 00:23:10.760]   - Okay, thanks.
[00:23:10.760 --> 00:23:12.160]   - All right, let's continue.
[00:23:12.160 --> 00:23:20.600]   Okay, yeah, so, ah, right, and I didn't mention,
[00:23:20.600 --> 00:23:23.520]   but on ImageNet, which is the top benchmark
[00:23:23.520 --> 00:23:27.640]   in computer vision, with this work, with the big transfer,
[00:23:27.640 --> 00:23:30.600]   we finally were able to increase the score
[00:23:30.600 --> 00:23:33.320]   after there was a long period of a couple of years
[00:23:33.320 --> 00:23:35.680]   of no improvement, but many attempts
[00:23:35.680 --> 00:23:37.240]   that you see the great upside.
[00:23:37.240 --> 00:23:39.200]   This was, yay, awesome.
[00:23:39.200 --> 00:23:41.160]   Pre-training, scaling up everything,
[00:23:41.160 --> 00:23:43.680]   and leveraging the data.
[00:23:43.680 --> 00:23:46.080]   And then, okay, let's not care about that.
[00:23:46.080 --> 00:23:51.360]   Yeah, that's, okay, this is just a little aside,
[00:23:51.360 --> 00:23:53.640]   that if you are in the setting that I mentioned
[00:23:53.640 --> 00:23:55.560]   of pre-training on huge amounts of data
[00:23:55.560 --> 00:23:58.040]   and then testing on many other tasks,
[00:23:58.040 --> 00:23:59.520]   you should, of course, be careful
[00:23:59.520 --> 00:24:02.440]   that you don't have images from the other tasks
[00:24:02.440 --> 00:24:04.280]   in your pre-training data, right?
[00:24:05.360 --> 00:24:07.360]   Otherwise, you have seen them during training,
[00:24:07.360 --> 00:24:09.240]   and then you're not really generalizing,
[00:24:09.240 --> 00:24:12.520]   and you're just fooling yourself with good scores.
[00:24:12.520 --> 00:24:15.240]   And this is a real danger when we get huge amounts of data,
[00:24:15.240 --> 00:24:17.600]   because, like, ImageNet images can totally be
[00:24:17.600 --> 00:24:19.440]   in huge amounts of data, right?
[00:24:19.440 --> 00:24:22.840]   So we actually use an internal pipeline
[00:24:22.840 --> 00:24:24.920]   that is really good at finding duplicates,
[00:24:24.920 --> 00:24:27.760]   and also new duplicates, like when they are shifted,
[00:24:27.760 --> 00:24:30.760]   rotated, squeezed, color changed a bit, whatnot.
[00:24:30.760 --> 00:24:32.000]   It still finds it.
[00:24:32.000 --> 00:24:34.920]   And we use this to completely remove all images
[00:24:34.920 --> 00:24:37.880]   from the test data sets that we test on later.
[00:24:37.880 --> 00:24:39.960]   And we actually found that a lot of classic
[00:24:39.960 --> 00:24:42.520]   just vision data sets have clear duplicates
[00:24:42.520 --> 00:24:44.920]   between their training and validation set,
[00:24:44.920 --> 00:24:48.080]   between the training set of ImageNet and CIFAR,
[00:24:48.080 --> 00:24:50.960]   10 and 100 test sets, and so on.
[00:24:50.960 --> 00:24:54.600]   So new duplicates are quite widespread problem in vision.
[00:24:54.600 --> 00:24:57.080]   And this slide is just to say, hey, there are problems,
[00:24:57.080 --> 00:24:58.720]   but in all that we present,
[00:24:58.720 --> 00:25:01.120]   we actually took care that in the pre-training,
[00:25:01.120 --> 00:25:03.840]   as best as we can, we don't have new duplicates.
[00:25:04.840 --> 00:25:08.520]   Right, now back to being like, hey,
[00:25:08.520 --> 00:25:10.560]   we figured out large data, a large model,
[00:25:10.560 --> 00:25:12.760]   and then things get really good.
[00:25:12.760 --> 00:25:16.440]   And that's how we got to transformers, basically.
[00:25:16.440 --> 00:25:19.160]   In computer vision, everything was convolutional networks
[00:25:19.160 --> 00:25:20.480]   for many years.
[00:25:20.480 --> 00:25:23.440]   And basically there was nothing else, CNN is king.
[00:25:23.440 --> 00:25:26.960]   However, in language, we saw a transformation recently,
[00:25:26.960 --> 00:25:29.320]   right, that everything used to be LSTM,
[00:25:29.320 --> 00:25:32.880]   everywhere LSTM was king, and then came the transformer.
[00:25:32.880 --> 00:25:35.880]   And in the case when there is a lot of data available,
[00:25:35.880 --> 00:25:39.400]   suddenly transformer worked much better than LSTM.
[00:25:39.400 --> 00:25:42.840]   For little data, that was still not the case exactly.
[00:25:42.840 --> 00:25:45.600]   So what we then thought is that, okay,
[00:25:45.600 --> 00:25:48.320]   so we are now in this regime where we have tons of data
[00:25:48.320 --> 00:25:50.040]   and we see benefit from it.
[00:25:50.040 --> 00:25:52.520]   Can we see even more benefit if we try also
[00:25:52.520 --> 00:25:55.600]   out the transformer architecture in vision?
[00:25:55.600 --> 00:25:58.280]   And that's basically what we did.
[00:25:58.280 --> 00:26:01.480]   To be fair, there were a few other attempts
[00:26:01.480 --> 00:26:03.960]   at trying out transformer in vision before,
[00:26:03.960 --> 00:26:06.360]   that I don't want to detail too much here
[00:26:06.360 --> 00:26:09.120]   because I don't want to point fingers too much,
[00:26:09.120 --> 00:26:12.800]   but they were all not really using transformers
[00:26:12.800 --> 00:26:14.920]   for learning everything from the data.
[00:26:14.920 --> 00:26:19.280]   It was always like, get something out of a ResNet first,
[00:26:19.280 --> 00:26:21.560]   like object detection proposals
[00:26:21.560 --> 00:26:24.280]   or high-level feature maps or things like that,
[00:26:24.280 --> 00:26:26.480]   and then stick a little transformer on top.
[00:26:26.480 --> 00:26:28.040]   But we wanted to go all the way,
[00:26:28.040 --> 00:26:30.240]   just transformer everything.
[00:26:30.240 --> 00:26:33.080]   And so we came up with the simplest and most natural,
[00:26:33.080 --> 00:26:36.440]   I believe, way of applying transformers to vision,
[00:26:36.440 --> 00:26:39.360]   which is you take the image, you cut it into pieces,
[00:26:39.360 --> 00:26:41.400]   and that's it, like a puzzle.
[00:26:41.400 --> 00:26:45.440]   Tack, tack, tack, patches, and that's it.
[00:26:45.440 --> 00:26:48.320]   Each of these patches, you take it
[00:26:48.320 --> 00:26:50.680]   and you project it into your embedding space,
[00:26:50.680 --> 00:26:52.640]   which is the input to the transformer.
[00:26:52.640 --> 00:26:55.120]   Embedded space is just abstract space of,
[00:26:55.120 --> 00:26:58.160]   let's say, 768 dimensions, for example.
[00:26:58.160 --> 00:26:59.040]   How do you embed it?
[00:26:59.040 --> 00:27:00.640]   You just take the pixel values
[00:27:00.640 --> 00:27:03.640]   and put the linear projection layer on top.
[00:27:03.640 --> 00:27:07.520]   So take all the pixels, flatten the vector,
[00:27:07.520 --> 00:27:10.680]   matrix multiply into whatever size you want,
[00:27:10.680 --> 00:27:14.520]   and use the same matrix for all the patches.
[00:27:14.520 --> 00:27:16.600]   And here we just went the simplest way ever
[00:27:16.600 --> 00:27:18.960]   with non-overlapping patches and everything.
[00:27:18.960 --> 00:27:22.600]   You can, and people later did, go on and say,
[00:27:22.600 --> 00:27:24.280]   "Hey, this is almost a convolution.
[00:27:24.280 --> 00:27:25.760]   Let's make proper convolution.
[00:27:25.760 --> 00:27:27.440]   Let's make stack of them," whatnot.
[00:27:27.440 --> 00:27:29.480]   But this is all for web work later.
[00:27:29.480 --> 00:27:32.480]   This is just the simplest way to do it first.
[00:27:32.480 --> 00:27:34.160]   Then we have these embedded patches,
[00:27:34.160 --> 00:27:36.840]   and we treat them exactly literally
[00:27:36.840 --> 00:27:40.080]   like the tokens in language,
[00:27:40.080 --> 00:27:44.280]   and then give them to exactly the BERT transformer
[00:27:44.280 --> 00:27:45.960]   from language folks.
[00:27:45.960 --> 00:27:49.400]   And just like in language, we add this class token,
[00:27:49.400 --> 00:27:51.800]   or I think the language is like end-of-sentence token
[00:27:51.800 --> 00:27:53.960]   or something.
[00:27:53.960 --> 00:27:57.680]   And we add the position embeddings to the tokens
[00:27:57.680 --> 00:27:59.960]   that can be learned.
[00:27:59.960 --> 00:28:02.760]   And then we feed all of this to a transformer encoder,
[00:28:02.760 --> 00:28:07.200]   which has a MLP head, which reads out this class token,
[00:28:07.200 --> 00:28:10.120]   and then maps it to Softmax layer
[00:28:10.120 --> 00:28:12.720]   for classification, for example.
[00:28:12.720 --> 00:28:15.120]   And that's it. That is the vision transformer.
[00:28:15.120 --> 00:28:16.960]   So it's literally a BERT transformer,
[00:28:16.960 --> 00:28:20.960]   but instead of words or sentence tokens,
[00:28:20.960 --> 00:28:23.840]   feed in patches transformed into tokens.
[00:28:23.840 --> 00:28:25.280]   And that's it.
[00:28:25.280 --> 00:28:28.360]   And then just same story as before, scale everything up.
[00:28:28.360 --> 00:28:32.400]   Compute, data set, model size, patients, everything.
[00:28:32.400 --> 00:28:35.920]   And see what happens. Is this good or not?
[00:28:35.920 --> 00:28:37.280]   That was the question.
[00:28:37.280 --> 00:28:39.080]   And now we can see a plot here.
[00:28:39.080 --> 00:28:41.880]   This is similar plot as before.
[00:28:41.880 --> 00:28:46.480]   The gray area is actually what were all of the bit dots before.
[00:28:46.480 --> 00:28:49.400]   And now the bubbles are vision transformers
[00:28:49.400 --> 00:28:51.000]   of different sizes.
[00:28:51.000 --> 00:28:53.800]   And the bubble is kind of the size of the model,
[00:28:53.800 --> 00:28:56.320]   although it's a bit hard to say exactly.
[00:28:56.320 --> 00:28:58.760]   And what you can see first is that with little data,
[00:28:58.760 --> 00:29:01.360]   ImageNet is the 1.3 million images.
[00:29:01.360 --> 00:29:03.520]   It works worse than ResNet.
[00:29:03.520 --> 00:29:05.440]   So if we would not believe in this idea
[00:29:05.440 --> 00:29:07.920]   and just try this, we're like, "Okay, this is a crap idea."
[00:29:07.920 --> 00:29:11.520]   And 1.3 million images is not that little.
[00:29:11.520 --> 00:29:13.080]   Then the 10 times larger data sets
[00:29:13.080 --> 00:29:15.640]   started in the same ballpark as ResNet.
[00:29:15.640 --> 00:29:18.800]   And when we go to much larger data
[00:29:18.800 --> 00:29:20.760]   with a much larger transformer,
[00:29:20.760 --> 00:29:23.600]   then we actually start outperforming this ResNet.
[00:29:23.600 --> 00:29:25.720]   And we outperform it just by a little.
[00:29:25.720 --> 00:29:27.560]   But this ResNet was really hard to get
[00:29:27.560 --> 00:29:30.360]   and is extremely clumsy and slow and big.
[00:29:30.360 --> 00:29:33.840]   So we were very excited by this.
[00:29:33.840 --> 00:29:35.880]   Then we did more controlled studies and everything.
[00:29:35.880 --> 00:29:40.040]   And one of them is like using subset of the same data set.
[00:29:40.040 --> 00:29:41.360]   And there's lots of curves,
[00:29:41.360 --> 00:29:44.240]   but basically just look at the dark gray one
[00:29:44.240 --> 00:29:46.280]   and the light blue one.
[00:29:46.280 --> 00:29:49.800]   These are roughly similarly fast and clumsy
[00:29:49.800 --> 00:29:53.320]   or easy to use or difficult to use bits,
[00:29:53.320 --> 00:29:57.000]   which is a ResNet variant and bits, the vision transformer.
[00:29:57.000 --> 00:29:58.560]   And what you can see, vision transformer,
[00:29:58.560 --> 00:30:01.800]   when we have little, in quotes, little data,
[00:30:01.800 --> 00:30:04.360]   is really bad compared to ResNet.
[00:30:04.360 --> 00:30:07.360]   But as we start having a lot of data, actually,
[00:30:07.360 --> 00:30:09.200]   it starts outperforming the ResNet.
[00:30:09.200 --> 00:30:10.600]   And this is very promising
[00:30:10.600 --> 00:30:12.840]   because I think everything that looks huge
[00:30:12.840 --> 00:30:15.640]   and a lot and so on now, in five or 10 years,
[00:30:15.640 --> 00:30:17.000]   it's maybe regular.
[00:30:17.000 --> 00:30:20.400]   Like 10 years ago, imagine if this one seemed to be huge
[00:30:20.400 --> 00:30:21.640]   and massive amount of data.
[00:30:21.640 --> 00:30:23.520]   No, not anymore.
[00:30:23.520 --> 00:30:24.800]   So we should look to the future.
[00:30:24.800 --> 00:30:28.040]   And this looks promising for the future.
[00:30:28.040 --> 00:30:29.760]   Then back to the same benchmark.
[00:30:29.760 --> 00:30:33.200]   That was another little jump.
[00:30:33.200 --> 00:30:37.080]   - Because we, yeah, yeah, we have some questions.
[00:30:37.080 --> 00:30:38.280]   - Yep.
[00:30:38.280 --> 00:30:41.960]   There is also this section about, yeah.
[00:30:41.960 --> 00:30:45.880]   So it's in that order,
[00:30:45.880 --> 00:30:50.440]   if you want to unmute yourself and ask the questions.
[00:30:50.440 --> 00:30:51.280]   - Sure, yeah.
[00:30:51.280 --> 00:30:54.080]   And I think Dimal already answered part of the question,
[00:30:54.080 --> 00:30:57.360]   but I was wondering in the input to this transformer,
[00:30:57.360 --> 00:30:58.600]   when you're chunking up the image
[00:30:58.600 --> 00:31:02.920]   into little puzzle pieces and then finding them,
[00:31:02.920 --> 00:31:06.840]   does the order of feeding these patches in matter?
[00:31:06.840 --> 00:31:08.880]   Like if you switch the order,
[00:31:08.880 --> 00:31:11.600]   does the prediction maybe change?
[00:31:11.600 --> 00:31:13.480]   - Yeah, that's a good question.
[00:31:13.480 --> 00:31:16.160]   And I actually have a slide on something like this,
[00:31:16.160 --> 00:31:17.800]   but not exactly.
[00:31:17.800 --> 00:31:20.080]   Let me jump there.
[00:31:20.080 --> 00:31:21.120]   So first of all,
[00:31:21.120 --> 00:31:24.840]   if the order is consistent during training, right?
[00:31:24.840 --> 00:31:28.360]   And you don't shuffle the order again for each new image,
[00:31:28.360 --> 00:31:30.280]   then it's literally the exact same.
[00:31:30.280 --> 00:31:31.920]   You get the same curve saying everything
[00:31:31.920 --> 00:31:34.200]   because we don't encode the order anywhere.
[00:31:34.200 --> 00:31:36.160]   If you start randomizing the order
[00:31:36.160 --> 00:31:37.520]   all the time during training,
[00:31:37.520 --> 00:31:39.960]   then performance gets quite a lot worse.
[00:31:39.960 --> 00:31:41.000]   And let me show you why.
[00:31:41.000 --> 00:31:45.280]   This is the slide was on my plan to present anyways.
[00:31:45.280 --> 00:31:47.440]   Then if you ask, let's jump here.
[00:31:47.440 --> 00:31:49.360]   These are, this is a visualization
[00:31:49.360 --> 00:31:51.600]   of the position embeddings.
[00:31:51.600 --> 00:31:52.440]   What does it mean?
[00:31:52.440 --> 00:31:53.280]   So in this case,
[00:31:53.280 --> 00:31:56.880]   we had 14 by 14 patches that we cut the image in.
[00:31:56.880 --> 00:32:01.160]   So it means we have also 14 by 14 position embeddings.
[00:32:01.160 --> 00:32:03.640]   Although we just see them as one long sequence of,
[00:32:03.640 --> 00:32:04.480]   what is it?
[00:32:04.480 --> 00:32:07.880]   150 something, or I don't know, 140 something.
[00:32:09.720 --> 00:32:13.040]   And now each of these pictures shows the position embedding,
[00:32:13.040 --> 00:32:15.400]   which corresponds to this location.
[00:32:15.400 --> 00:32:18.720]   How similar is it to all the other position embeddings?
[00:32:18.720 --> 00:32:20.560]   So let's look at this one, for example.
[00:32:20.560 --> 00:32:23.680]   Yellow means perfectly similar, like exactly the same.
[00:32:23.680 --> 00:32:27.320]   And blue means opposite in terms of cosine similarity.
[00:32:27.320 --> 00:32:30.760]   So this position embedding is most similar to itself,
[00:32:30.760 --> 00:32:32.360]   which is the pixel here.
[00:32:32.360 --> 00:32:35.520]   And then the neighboring pixels is how similar is it
[00:32:35.520 --> 00:32:39.000]   to the position embeddings that correspond originally
[00:32:39.000 --> 00:32:40.640]   to the neighboring patch.
[00:32:40.640 --> 00:32:42.400]   And we do see a very clear pattern
[00:32:42.400 --> 00:32:44.760]   that each position embedding is very similar
[00:32:44.760 --> 00:32:48.080]   to the embedding from its surrounding patches.
[00:32:48.080 --> 00:32:51.840]   And we didn't implement any of this, right?
[00:32:51.840 --> 00:32:53.440]   We just had these position embeddings
[00:32:53.440 --> 00:32:55.240]   at randomly initialized variables,
[00:32:55.240 --> 00:32:57.280]   and they are learned as freely
[00:32:57.280 --> 00:32:59.600]   as the rest of the parameters of the model.
[00:32:59.600 --> 00:33:01.400]   But they learned to recover this notion
[00:33:01.400 --> 00:33:03.320]   of what are my neighbor patches,
[00:33:03.320 --> 00:33:05.080]   even though we don't give this information
[00:33:05.080 --> 00:33:06.480]   anywhere at any time,
[00:33:06.480 --> 00:33:08.440]   besides the raw image data and the task
[00:33:08.440 --> 00:33:12.280]   to please classify this image.
[00:33:12.280 --> 00:33:13.560]   So that's pretty cool, I think.
[00:33:13.560 --> 00:33:16.480]   But it also means that if you take the trained model now
[00:33:16.480 --> 00:33:18.680]   and give in patches
[00:33:18.680 --> 00:33:21.200]   in a completely differently shuffled order,
[00:33:21.200 --> 00:33:22.480]   it's going to perform poorly
[00:33:22.480 --> 00:33:24.680]   because these learned position embeddings
[00:33:24.680 --> 00:33:26.480]   don't make sense anymore.
[00:33:26.480 --> 00:33:30.360]   We did try also to implement, like, position embeddings
[00:33:30.360 --> 00:33:35.440]   which encode the location as hardcoded by us,
[00:33:35.440 --> 00:33:39.520]   and other fancy position embeddings like relative ones.
[00:33:39.520 --> 00:33:42.160]   But basically, none of that really outperformed
[00:33:42.160 --> 00:33:43.320]   these freely learned.
[00:33:43.320 --> 00:33:44.760]   And then the freely learned is simpler.
[00:33:44.760 --> 00:33:46.080]   You just run them in it,
[00:33:46.080 --> 00:33:48.200]   let it learn as part of SGD, and that's it.
[00:33:48.200 --> 00:33:53.480]   And so we go with that, and so just like that.
[00:33:53.480 --> 00:33:55.440]   -Nice, it's awesome.
[00:33:55.440 --> 00:33:58.920]   -We have one more question from --
[00:33:58.920 --> 00:34:00.600]   -Hey, yeah, I was wondering if you could --
[00:34:00.600 --> 00:34:01.840]   Yeah, this slide.
[00:34:01.840 --> 00:34:03.320]   I think something that's really interesting
[00:34:03.320 --> 00:34:05.400]   is we're talking about scaling up the data,
[00:34:05.400 --> 00:34:08.240]   and scaling up the model would be fun as well.
[00:34:08.240 --> 00:34:11.280]   But it seems like you're reaching an awesome job, right,
[00:34:11.280 --> 00:34:13.400]   when you keep doing the scaling.
[00:34:13.400 --> 00:34:15.920]   So I'm curious if you have any thoughts on that.
[00:34:15.920 --> 00:34:18.160]   Like, are these points just look like that,
[00:34:18.160 --> 00:34:21.480]   or is there kind of a best you can sort of do
[00:34:21.480 --> 00:34:25.400]   where when you're pre-training the data or the parameters,
[00:34:25.400 --> 00:34:27.480]   you're actually not going to get much --
[00:34:27.480 --> 00:34:29.000]   -Yeah, I have another slide,
[00:34:29.000 --> 00:34:32.040]   but much further in the talk about this,
[00:34:32.040 --> 00:34:36.640]   where I would like to not jump on it, if you don't mind.
[00:34:36.640 --> 00:34:40.920]   And then maybe in 10, 15 minutes, we will be there.
[00:34:40.920 --> 00:34:42.520]   -Sounds good. Thanks.
[00:34:42.520 --> 00:34:46.800]   -Yeah, maybe to be a bit optimistic,
[00:34:46.800 --> 00:34:48.640]   it does seem like the transformers
[00:34:48.640 --> 00:34:50.240]   have a better slope here in the end,
[00:34:50.240 --> 00:34:53.920]   and there is a plateau earlier.
[00:34:53.920 --> 00:34:57.000]   -Sorry, Lucas, I did not mean to interrupt.
[00:34:57.000 --> 00:35:00.040]   Are there any more questions before we proceed?
[00:35:00.040 --> 00:35:02.160]   -Yeah, can I ask my question real quick?
[00:35:02.160 --> 00:35:03.120]   -Sorry about that.
[00:35:03.120 --> 00:35:06.200]   -So what I'm curious to know is how does this VIT
[00:35:06.200 --> 00:35:08.680]   compare to if you equip a ConvNet,
[00:35:08.680 --> 00:35:12.320]   so, for example, ResNet, with an attention mechanism?
[00:35:12.320 --> 00:35:13.360]   -Mm-hmm.
[00:35:13.360 --> 00:35:15.840]   -Like, how much of this is due to the structure of a transformer
[00:35:15.840 --> 00:35:17.240]   and the particular way it operates
[00:35:17.240 --> 00:35:18.800]   versus just the benefit of attention
[00:35:18.800 --> 00:35:22.040]   that a vanilla ConvNet does not have access to?
[00:35:22.040 --> 00:35:26.400]   -Yeah, so this has been tried many times before,
[00:35:26.400 --> 00:35:27.800]   and the first time that I know of
[00:35:27.800 --> 00:35:30.840]   was actually from -- I mispronounce his name,
[00:35:30.840 --> 00:35:34.120]   but Jaime Herr, the inventor of ResNet,
[00:35:34.120 --> 00:35:37.360]   and some of his colleagues, they called it non-blocker networks.
[00:35:37.360 --> 00:35:39.920]   This was way -- I think even before the transformer paper,
[00:35:39.920 --> 00:35:42.760]   if I remember correctly,
[00:35:42.760 --> 00:35:45.000]   and they basically inserted attention blocks
[00:35:45.000 --> 00:35:47.120]   at various locations in the ResNet,
[00:35:47.120 --> 00:35:48.200]   and then they showed improvement,
[00:35:48.200 --> 00:35:51.080]   but it was, like, tiny improvements.
[00:35:51.080 --> 00:35:53.560]   It was a cool block and a simple paper,
[00:35:53.560 --> 00:35:55.040]   but it was not really worth it,
[00:35:55.040 --> 00:35:59.400]   and people usually place their attention --
[00:35:59.400 --> 00:36:01.840]   you can imagine if you place the attention just on the pixels
[00:36:01.840 --> 00:36:03.360]   and don't do this patch-cutting,
[00:36:03.360 --> 00:36:07.080]   this is way too expensive computation-wise, right?
[00:36:07.080 --> 00:36:08.920]   If you have two to four by two to four pixels,
[00:36:08.920 --> 00:36:11.240]   that's like -- yeah, I cannot do this in my head.
[00:36:11.240 --> 00:36:14.320]   I don't know, 40,000 or so maybe pixels?
[00:36:14.320 --> 00:36:16.960]   Attending to 40,000 others, that doesn't work,
[00:36:16.960 --> 00:36:18.960]   so people just do it in the very high
[00:36:18.960 --> 00:36:20.760]   and very final layers of the ResNet,
[00:36:20.760 --> 00:36:23.600]   like, where it's maybe seven by seven,
[00:36:23.600 --> 00:36:25.040]   and then they add a bit of --
[00:36:25.040 --> 00:36:27.280]   sprinkle a bit of attention there,
[00:36:27.280 --> 00:36:29.800]   but then you don't really get much benefit of scaling
[00:36:29.800 --> 00:36:32.280]   because it's essentially still a ResNet.
[00:36:32.280 --> 00:36:34.560]   And there is -- in ResNet,
[00:36:34.560 --> 00:36:37.400]   there is this block called Squeeze Excite
[00:36:37.400 --> 00:36:39.680]   that has been getting really popular --
[00:36:39.680 --> 00:36:41.120]   or has gotten really popular
[00:36:41.120 --> 00:36:43.920]   and improves ResNet quite a bit,
[00:36:43.920 --> 00:36:47.120]   and that is also kind of a form of attention,
[00:36:47.120 --> 00:36:50.480]   but, like, nicely tailored to images.
[00:36:50.480 --> 00:36:53.520]   I'm not doing -- it's arguable.
[00:36:53.520 --> 00:36:55.480]   But yeah, it has been tried many times before,
[00:36:55.480 --> 00:36:57.640]   but it just -- it doesn't show --
[00:36:57.640 --> 00:37:01.000]   or it hasn't been shown to have this scaling benefit
[00:37:01.000 --> 00:37:03.280]   as much as they did.
[00:37:03.280 --> 00:37:05.160]   -So I think I'm missing something critical here,
[00:37:05.160 --> 00:37:06.960]   which is you just said, in fact,
[00:37:06.960 --> 00:37:09.920]   or it's computationally difficult
[00:37:09.920 --> 00:37:12.920]   to do an attention layer at a low level in the ResNet,
[00:37:12.920 --> 00:37:15.400]   but why is it any different than doing an attention layer
[00:37:15.400 --> 00:37:17.840]   in the Vision Transformer?
[00:37:17.840 --> 00:37:20.040]   -Because we cut the patches first,
[00:37:20.040 --> 00:37:22.800]   so we have maybe 14 by 14 patches,
[00:37:22.800 --> 00:37:26.320]   which is not that much.
[00:37:26.320 --> 00:37:28.160]   -Okay, but I'm confused.
[00:37:28.160 --> 00:37:31.480]   Like, you could imagine, not at a high level,
[00:37:31.480 --> 00:37:32.920]   not at a high layer in the ResNet,
[00:37:32.920 --> 00:37:34.280]   but at a relatively low layer,
[00:37:34.280 --> 00:37:37.720]   after you've applied, like, one or two convolutional filters --
[00:37:37.720 --> 00:37:39.440]   convolutional layers, excuse me --
[00:37:39.440 --> 00:37:43.080]   then you have something the size of the patches.
[00:37:43.080 --> 00:37:47.440]   -That's still 50 by 50 at the early layers, and that's --
[00:37:47.440 --> 00:37:49.440]   -But 50 by 50 is significantly less than,
[00:37:49.440 --> 00:37:52.320]   I don't know, like, 400 by 400 or whatever.
[00:37:52.320 --> 00:37:56.960]   -But it's still 2,500 tokens attending to 2,500 tokens,
[00:37:56.960 --> 00:37:58.080]   which -- -Yeah, I mean, it's a lot,
[00:37:58.080 --> 00:38:01.000]   but it's not comparable.
[00:38:01.000 --> 00:38:02.400]   I don't know. Okay, cool. Thank you.
[00:38:02.400 --> 00:38:04.840]   -Yeah. I mean, it could be tracked.
[00:38:04.840 --> 00:38:06.960]   Okay, maybe another answer to your question
[00:38:06.960 --> 00:38:09.040]   is then we're slowly getting to this,
[00:38:09.040 --> 00:38:12.880]   my next slide after the set of questions,
[00:38:12.880 --> 00:38:17.600]   where we do try something almost like what you said,
[00:38:17.600 --> 00:38:19.880]   have a very small part of the ResNet,
[00:38:19.880 --> 00:38:23.640]   and then stick a transformer on top of it,
[00:38:23.640 --> 00:38:26.400]   but, like, the full transformer encoder on top of it,
[00:38:26.400 --> 00:38:29.320]   and not just sprinkle a few attention layers
[00:38:29.320 --> 00:38:32.080]   and then continue with columns and so on.
[00:38:32.080 --> 00:38:35.280]   And this is this process, and we call them hybrid,
[00:38:35.280 --> 00:38:37.560]   but it's almost literally what you said, actually,
[00:38:37.560 --> 00:38:39.760]   like a few early layers from the ResNet
[00:38:39.760 --> 00:38:42.000]   and with different varying amount,
[00:38:42.000 --> 00:38:45.480]   and then stick the whole transformer encoder.
[00:38:45.480 --> 00:38:48.080]   And this seems to work well, too,
[00:38:48.080 --> 00:38:50.880]   especially for the -- when you --
[00:38:50.880 --> 00:38:53.360]   x-axis in this case is amount of compute,
[00:38:53.360 --> 00:38:55.920]   so for the little compute, it seems to work well.
[00:38:55.920 --> 00:38:58.280]   But then the scaling behavior of the pure ResNet
[00:38:58.280 --> 00:39:00.680]   is a little better, so we focused on that.
[00:39:00.680 --> 00:39:03.280]   I think we later tried also hybrid further to the right,
[00:39:03.280 --> 00:39:05.880]   and it was a bit lower, but it was after the paper,
[00:39:05.880 --> 00:39:08.760]   so it's not on this plot, which I just cut out of the paper.
[00:39:08.760 --> 00:39:12.840]   But you can already see the trend here.
[00:39:12.840 --> 00:39:15.520]   Yeah, so if you don't scale all the way up,
[00:39:15.520 --> 00:39:18.120]   then this is a totally reasonable thing to do,
[00:39:18.120 --> 00:39:19.800]   have a little bit of ResNet
[00:39:19.800 --> 00:39:22.960]   and then the encoder from transformer.
[00:39:22.960 --> 00:39:29.040]   -Do you want to ask a question?
[00:39:29.040 --> 00:39:32.960]   -Yeah, I was just wondering about the --
[00:39:32.960 --> 00:39:35.640]   basically, there's like a short section of paper
[00:39:35.640 --> 00:39:38.120]   about, like, fine-tuning and, like, higher resolution,
[00:39:38.120 --> 00:39:40.640]   and in that case, right, like, the pre-trained,
[00:39:40.640 --> 00:39:45.480]   like, position embeddings, sorry, are, like, skewed, right?
[00:39:45.480 --> 00:39:48.320]   And it basically says that you guys are, like, interpolating.
[00:39:48.320 --> 00:39:50.480]   Can you, like, talk a little bit?
[00:39:50.480 --> 00:39:52.400]   Like, how do you interpolate what's going on?
[00:39:52.400 --> 00:39:55.840]   -Yeah. Actually, when I checked the slides earlier today,
[00:39:55.840 --> 00:40:00.440]   I was like, "Oh, it would be cool to have a slide on that."
[00:40:00.440 --> 00:40:02.520]   And we don't have a nice visualization in the paper,
[00:40:02.520 --> 00:40:04.520]   either, because it's a bit difficult to explain,
[00:40:04.520 --> 00:40:08.320]   but this is the best starting point we have.
[00:40:08.320 --> 00:40:11.760]   So if you want to increase the resolution of the image,
[00:40:11.760 --> 00:40:13.520]   and you keep the patch size fixed,
[00:40:13.520 --> 00:40:15.720]   it means you have more patches suddenly, right?
[00:40:15.720 --> 00:40:18.480]   And then, as you say, the patch embeddings, like,
[00:40:18.480 --> 00:40:22.840]   what do you even use as position embeddings, right?
[00:40:22.840 --> 00:40:25.480]   And basically, you can see here that we see
[00:40:25.480 --> 00:40:27.680]   that they learn a very regular structure, right?
[00:40:27.680 --> 00:40:29.520]   We don't really know what is the structure
[00:40:29.520 --> 00:40:31.160]   of these position embeddings that I learned.
[00:40:31.160 --> 00:40:33.600]   We just see the similarity to each other
[00:40:33.600 --> 00:40:36.240]   and that it is very regular.
[00:40:36.240 --> 00:40:37.600]   And so this gave us the intuition
[00:40:37.600 --> 00:40:41.680]   that we may be able to just take them,
[00:40:41.680 --> 00:40:45.080]   kind of imaging these boxes, they slide apart,
[00:40:45.080 --> 00:40:46.880]   and new boxes appear between them,
[00:40:46.880 --> 00:40:48.320]   and they are just the interpolation
[00:40:48.320 --> 00:40:50.040]   of the surrounding ones.
[00:40:50.040 --> 00:40:54.520]   And that's basically what we do with the position embeddings.
[00:40:54.520 --> 00:40:57.640]   We create new ones where there are missing ones,
[00:40:57.640 --> 00:41:00.120]   because we need more,
[00:41:00.120 --> 00:41:02.240]   and by interpolating the surrounding.
[00:41:02.240 --> 00:41:06.040]   Or more precisely, we basically see them as a picture,
[00:41:06.040 --> 00:41:10.000]   in this case, 14 by 14, with 700-something channels,
[00:41:10.000 --> 00:41:12.000]   or whatever is the dimensionality.
[00:41:12.000 --> 00:41:14.320]   And then we basically resize this
[00:41:14.320 --> 00:41:19.000]   like you would resize a picture by interpolation.
[00:41:19.000 --> 00:41:22.200]   And that way, we get more and new position embeddings
[00:41:22.200 --> 00:41:24.040]   that we don't understand where they are,
[00:41:24.040 --> 00:41:26.240]   but they follow the same pattern as the learned ones,
[00:41:26.240 --> 00:41:29.720]   just at a higher resolution, basically.
[00:41:29.720 --> 00:41:35.520]   Yeah, go ahead.
[00:41:35.520 --> 00:41:39.080]   - Yeah, I just have a quick question.
[00:41:39.080 --> 00:41:44.080]   So when you're creating the embeddings as input,
[00:41:44.080 --> 00:41:47.920]   right now you're doing a light projection,
[00:41:47.920 --> 00:41:50.440]   at least in this case.
[00:41:50.440 --> 00:41:52.440]   Has there been work to do to memorize the other way,
[00:41:52.440 --> 00:41:56.920]   'cause there's a lot of pixels that are close to each other?
[00:41:56.920 --> 00:41:59.920]   - Yeah, there were quite a few works
[00:41:59.920 --> 00:42:02.880]   that tried varying other things.
[00:42:02.880 --> 00:42:04.880]   One that I especially liked recently,
[00:42:04.880 --> 00:42:08.920]   it's called "Early Convolutions Help Transformers See Better,"
[00:42:08.920 --> 00:42:10.640]   or something like that.
[00:42:10.640 --> 00:42:11.520]   And they basically say,
[00:42:11.520 --> 00:42:13.720]   "Okay, instead of this linear projection,
[00:42:13.720 --> 00:42:16.240]   instead of this one big linear projection,
[00:42:16.240 --> 00:42:20.040]   we replace it by a stack of three-by-three convolution
[00:42:20.040 --> 00:42:22.280]   with a stride two."
[00:42:22.280 --> 00:42:24.640]   And then they have also nonlinearities between them,
[00:42:24.640 --> 00:42:26.520]   normalizations between them,
[00:42:26.520 --> 00:42:29.440]   but such that the overall stride is the same
[00:42:29.440 --> 00:42:32.160]   as this patchifying.
[00:42:32.160 --> 00:42:36.040]   So the outcome would then be the same dimensionality
[00:42:36.040 --> 00:42:39.040]   as after this patch cutting and then projecting.
[00:42:39.040 --> 00:42:40.440]   And then they showed that,
[00:42:40.440 --> 00:42:45.160]   supposedly it makes it a bit easier to optimize
[00:42:45.160 --> 00:42:48.760]   in the sense that more optimized settings are good settings.
[00:42:48.760 --> 00:42:53.640]   In many scenarios, it performs the same,
[00:42:53.640 --> 00:42:56.800]   but like more robustly to get there.
[00:42:56.800 --> 00:42:59.080]   And they also show some scenarios
[00:42:59.080 --> 00:43:02.040]   where this performs much better,
[00:43:02.040 --> 00:43:04.480]   like for example, when pre-training on,
[00:43:04.480 --> 00:43:06.920]   actually, when they pre-train on more data,
[00:43:06.920 --> 00:43:08.880]   that seems to perform even better.
[00:43:08.880 --> 00:43:14.080]   I have played a bit with it and tried to reproduce it.
[00:43:14.080 --> 00:43:16.120]   I don't have it fully reproduced,
[00:43:16.120 --> 00:43:19.040]   but I don't see as much benefit as in the paper yet.
[00:43:19.040 --> 00:43:20.920]   But that's not to say that the paper is wrong,
[00:43:20.920 --> 00:43:22.640]   just that I didn't get there yet.
[00:43:22.640 --> 00:43:26.080]   That is one example of them.
[00:43:26.080 --> 00:43:28.280]   There are other papers that do stuff,
[00:43:28.280 --> 00:43:30.520]   but this one I found especially interesting
[00:43:30.520 --> 00:43:31.920]   because it's simple.
[00:43:31.920 --> 00:43:34.440]   - Thank you.
[00:43:34.440 --> 00:43:37.720]   - All right, continue?
[00:43:37.720 --> 00:43:40.440]   - We don't have any more questions.
[00:43:40.440 --> 00:43:42.640]   - All right, then let's see.
[00:43:42.640 --> 00:43:44.800]   Yeah, I have like three more interesting details
[00:43:44.800 --> 00:43:47.680]   from the paper and then depending on
[00:43:47.680 --> 00:43:49.600]   if we want more discussion or more content,
[00:43:49.600 --> 00:43:52.120]   I have more content, like also the question about,
[00:43:52.120 --> 00:43:53.600]   does it saturate here or not?
[00:43:53.600 --> 00:43:57.400]   All right, so another interesting thing
[00:43:57.400 --> 00:43:58.440]   that we had in the paper,
[00:43:58.440 --> 00:44:00.280]   but it is buried in the appendix,
[00:44:00.280 --> 00:44:03.560]   and then follow-up papers from others
[00:44:03.560 --> 00:44:05.880]   have been written on this by now actually,
[00:44:05.880 --> 00:44:09.400]   is like how should we scale these transformers?
[00:44:09.400 --> 00:44:13.120]   I don't know, right in the high-level shape
[00:44:13.120 --> 00:44:15.880]   of the transformer, there's lots of settings
[00:44:15.880 --> 00:44:17.400]   that you could choose.
[00:44:17.400 --> 00:44:19.080]   And we actually tried many of them.
[00:44:19.080 --> 00:44:22.360]   So we started with the reasonable medium-sized transformer,
[00:44:22.360 --> 00:44:23.680]   this dot in the middle,
[00:44:23.680 --> 00:44:26.280]   and then we varied things one by one,
[00:44:27.640 --> 00:44:30.960]   such that we always double the compute.
[00:44:30.960 --> 00:44:32.760]   So for example, this pink line,
[00:44:32.760 --> 00:44:36.600]   if we go to the right, this point increases the width,
[00:44:36.600 --> 00:44:39.040]   such that we double the compute.
[00:44:39.040 --> 00:44:44.040]   X-axis is compute relative to this starting point.
[00:44:44.040 --> 00:44:46.240]   And we have all of these different settings.
[00:44:46.240 --> 00:44:50.320]   There's the width, which is how wide are the vectors
[00:44:50.320 --> 00:44:52.320]   with which self-attention is done,
[00:44:52.320 --> 00:44:54.600]   which is for the base model 768,
[00:44:54.600 --> 00:44:56.640]   and then goes larger or smaller.
[00:44:57.520 --> 00:45:00.680]   There is like, as you see scaling,
[00:45:00.680 --> 00:45:02.760]   this does not seem promising.
[00:45:02.760 --> 00:45:04.400]   So we didn't scale that much.
[00:45:04.400 --> 00:45:06.720]   Then there's other things like the width
[00:45:06.720 --> 00:45:08.760]   of the multi-layer perceptron,
[00:45:08.760 --> 00:45:11.280]   or some people call it the one-by-one convolution
[00:45:11.280 --> 00:45:12.840]   in these attentions.
[00:45:12.840 --> 00:45:16.160]   And this seems to scale a bit nicer, this orange part.
[00:45:16.160 --> 00:45:18.400]   I actually wonder where it went to the left.
[00:45:18.400 --> 00:45:19.400]   I don't remember.
[00:45:19.400 --> 00:45:21.840]   I don't know if it's hidden somewhere
[00:45:21.840 --> 00:45:24.720]   or if we just didn't scale it down, but anyways.
[00:45:24.720 --> 00:45:26.000]   Then another thing to scale,
[00:45:26.000 --> 00:45:29.040]   which does not exist in the transformers from text
[00:45:29.040 --> 00:45:30.400]   is the patch size.
[00:45:30.400 --> 00:45:32.040]   As you make the patch smaller,
[00:45:32.040 --> 00:45:34.960]   you get more and more tokens out of an image
[00:45:34.960 --> 00:45:37.520]   and thus more and more compute capacity.
[00:45:37.520 --> 00:45:42.120]   This is the green one, which also seems to scale nicely.
[00:45:42.120 --> 00:45:45.960]   Then the depth is an interesting one, this yellow one.
[00:45:45.960 --> 00:45:48.600]   And this is the number of encoder blocks.
[00:45:48.600 --> 00:45:50.360]   As we scale, it first seems like, wow,
[00:45:50.360 --> 00:45:51.640]   this is the thing you want to scale,
[00:45:51.640 --> 00:45:53.640]   but then it does seem to plateau.
[00:45:53.640 --> 00:45:56.760]   And it scales really badly if you decrease the depth.
[00:45:56.760 --> 00:45:59.040]   So that's not a good thing to decrease.
[00:45:59.040 --> 00:46:01.240]   However, the width seems to be a good thing to decrease
[00:46:01.240 --> 00:46:03.160]   if you want to go to smaller models.
[00:46:03.160 --> 00:46:06.120]   And then the blue is just scaling everything together
[00:46:06.120 --> 00:46:08.360]   such that the compute is kept,
[00:46:08.360 --> 00:46:09.960]   like everything by roughly the same amount.
[00:46:09.960 --> 00:46:14.920]   That seems to scale nicely as well as the rest
[00:46:14.920 --> 00:46:17.240]   and is relatively simple, or at least conceptually.
[00:46:17.240 --> 00:46:18.800]   So we like this, so we went with that
[00:46:18.800 --> 00:46:20.800]   whenever we scaled up or down the model.
[00:46:23.440 --> 00:46:26.560]   And this one I really like is the inference speed,
[00:46:26.560 --> 00:46:29.640]   because if you have the image size of two to four pixels,
[00:46:29.640 --> 00:46:32.520]   it actually means you have two to four by two to four pixels.
[00:46:32.520 --> 00:46:37.000]   So if you have, then you patchify it with 16 by 16 patch,
[00:46:37.000 --> 00:46:42.520]   for example, patch size, then you have 14 by 14 patches.
[00:46:42.520 --> 00:46:45.440]   So that is the sequence length is actually 150.
[00:46:45.440 --> 00:46:48.320]   And then on top of the sequence length,
[00:46:48.320 --> 00:46:49.800]   you have the self-attention operation,
[00:46:49.800 --> 00:46:51.880]   which is square again.
[00:46:51.880 --> 00:46:54.760]   So overall, with respect to image size,
[00:46:54.760 --> 00:46:58.200]   the self-attention operation is to the fourth power,
[00:46:58.200 --> 00:47:01.520]   which is called quartic.
[00:47:01.520 --> 00:47:03.000]   So that is really bad.
[00:47:03.000 --> 00:47:05.960]   Like everybody who sees all of something to the fourth
[00:47:05.960 --> 00:47:07.240]   is like, "What the hell are you doing?
[00:47:07.240 --> 00:47:09.760]   This is never going to scale."
[00:47:09.760 --> 00:47:12.520]   So we checked what does it look like in practice
[00:47:12.520 --> 00:47:15.000]   with the image sizes that we operate in,
[00:47:15.000 --> 00:47:16.240]   and this is what you see here.
[00:47:16.240 --> 00:47:19.920]   On the y-axis is how fast it goes,
[00:47:19.920 --> 00:47:22.000]   basically how fast it does inference,
[00:47:22.000 --> 00:47:25.920]   and on the x-axis is varying the input size.
[00:47:25.920 --> 00:47:31.760]   And this, what this means, it doesn't look so bad yet.
[00:47:31.760 --> 00:47:34.240]   Basically, when you go here to the 512,
[00:47:34.240 --> 00:47:35.320]   to the really large image,
[00:47:35.320 --> 00:47:37.320]   then you see that the transformers
[00:47:37.320 --> 00:47:42.920]   actually start going down a lot more than the ResNets.
[00:47:42.920 --> 00:47:44.600]   But in this reasonable image size,
[00:47:44.600 --> 00:47:46.200]   let's call it very typical,
[00:47:46.200 --> 00:47:48.240]   it doesn't seem so bad in practice yet.
[00:47:48.240 --> 00:47:52.120]   So we're not getting hit by the big O yet.
[00:47:52.120 --> 00:47:54.480]   But as we go larger, it will likely be a problem,
[00:47:54.480 --> 00:47:56.240]   and there will be a lot of follow-up works
[00:47:56.240 --> 00:48:01.440]   trying to make that better.
[00:48:01.440 --> 00:48:07.560]   Then, this is the last one from the original paper.
[00:48:07.560 --> 00:48:10.960]   This is looking at the input's receptive field size.
[00:48:10.960 --> 00:48:13.360]   So in the self-attention operation,
[00:48:13.360 --> 00:48:17.920]   how far ago do heads typically attend?
[00:48:17.920 --> 00:48:21.600]   And here on the x-axis, we see the layer in the network.
[00:48:21.600 --> 00:48:24.040]   To the right is more towards the output, the classes,
[00:48:24.040 --> 00:48:27.760]   and to the left is more towards the input, the patches.
[00:48:27.760 --> 00:48:31.480]   And the y-axis is how far on average across,
[00:48:31.480 --> 00:48:33.640]   I think, the whole validation set,
[00:48:33.640 --> 00:48:35.320]   does the self-attention look?
[00:48:35.320 --> 00:48:39.120]   And does look means that the peak of the self-attention
[00:48:39.120 --> 00:48:41.360]   or the max, how far is it away?
[00:48:41.360 --> 00:48:44.200]   Something like that.
[00:48:44.200 --> 00:48:45.800]   And each dot is a different head
[00:48:45.800 --> 00:48:48.560]   because we can use multi-head self-attention.
[00:48:48.560 --> 00:48:50.960]   And so what this shows is that in the early layers,
[00:48:50.960 --> 00:48:53.360]   actually you have some heads that go far,
[00:48:53.360 --> 00:48:56.840]   but also a lot of heads that look very nearby them,
[00:48:56.840 --> 00:48:58.000]   so locally.
[00:48:58.000 --> 00:48:59.600]   And as we go deeper in the model,
[00:48:59.600 --> 00:49:03.560]   we only are left with heads that, on average, look further.
[00:49:03.560 --> 00:49:07.000]   So it's just some kind of analysis.
[00:49:07.000 --> 00:49:09.840]   There is not immediately action to take about this,
[00:49:09.840 --> 00:49:12.640]   but it's interesting to see that earlier layers,
[00:49:12.640 --> 00:49:16.440]   they learn a mixture of looking to a local neighborhood
[00:49:16.440 --> 00:49:18.120]   and looking globally,
[00:49:18.120 --> 00:49:22.680]   and later layers only look globally anymore.
[00:49:22.680 --> 00:49:24.280]   Right.
[00:49:24.280 --> 00:49:30.360]   So that is about the original vision transformers.
[00:49:30.360 --> 00:49:33.320]   Now, I don't know how long you want me
[00:49:33.320 --> 00:49:35.320]   to continue speaking or discussing.
[00:49:35.320 --> 00:49:38.760]   I have a couple of options that I can talk about,
[00:49:38.760 --> 00:49:41.920]   which is one project that was further scaling updates,
[00:49:41.920 --> 00:49:44.040]   and this one also has the answer to the --
[00:49:44.040 --> 00:49:45.400]   I can also jump straight to the answer
[00:49:45.400 --> 00:49:46.880]   if you don't want to hear the rest.
[00:49:46.880 --> 00:49:48.560]   But to the question of, like,
[00:49:48.560 --> 00:49:49.960]   how does it continue to the right?
[00:49:49.960 --> 00:49:52.720]   Are we separating?
[00:49:52.720 --> 00:49:56.400]   There is another project about how to train vision transformers
[00:49:56.400 --> 00:49:58.600]   when you don't have massive amounts of data.
[00:49:58.600 --> 00:50:00.360]   Can you still do it? Is it reasonable?
[00:50:00.360 --> 00:50:03.320]   Or is it maybe just unreasonable to do?
[00:50:03.320 --> 00:50:04.800]   This one is maybe too unrelated.
[00:50:04.800 --> 00:50:06.160]   Let's not talk about this.
[00:50:06.160 --> 00:50:08.360]   And the last one is, like,
[00:50:08.360 --> 00:50:11.920]   I talk all about these benefits of a really large model
[00:50:11.920 --> 00:50:13.640]   when you pre-train them on lots of data.
[00:50:13.640 --> 00:50:17.000]   Okay, that's nice. That's how we get a good model.
[00:50:17.000 --> 00:50:19.880]   But then actually using a model that is massive
[00:50:19.880 --> 00:50:21.200]   is not fun at all.
[00:50:21.200 --> 00:50:22.960]   Like, it doesn't fit on your GPUs.
[00:50:22.960 --> 00:50:25.960]   You need, like, multiple TPUs to even use it.
[00:50:25.960 --> 00:50:29.080]   So people are not happy to use it
[00:50:29.080 --> 00:50:32.000]   and usually still go back to small-ish models,
[00:50:32.000 --> 00:50:34.760]   even though they know, like, larger models should be better.
[00:50:34.760 --> 00:50:37.800]   What can we do about it?
[00:50:37.800 --> 00:50:41.520]   That's another project we had, which is about distillation.
[00:50:41.520 --> 00:50:46.080]   So I would say it's up to you guys what you prefer to do.
[00:50:46.080 --> 00:50:47.480]   Or if you have plenty of questions,
[00:50:47.480 --> 00:50:49.200]   we can continue with the questions now,
[00:50:49.200 --> 00:50:53.080]   because I think now the original one hour would be over, right?
[00:50:53.080 --> 00:50:54.960]   -Right. -So I think one suggestion was,
[00:50:54.960 --> 00:50:56.880]   like, we can continue the talk,
[00:50:56.880 --> 00:50:59.360]   and we'll also be recording it so people can, like,
[00:50:59.360 --> 00:51:02.720]   just, like, go and see it if they miss out something.
[00:51:02.720 --> 00:51:04.240]   So we could do that.
[00:51:04.240 --> 00:51:06.800]   -Yeah, the other thing is two people have their hands raised,
[00:51:06.800 --> 00:51:09.160]   so we can... -Okay.
[00:51:09.160 --> 00:51:10.960]   -...take questions first.
[00:51:10.960 --> 00:51:13.400]   -Up to you guys, and fight either way.
[00:51:13.400 --> 00:51:24.640]   -So you guys want to ask a question?
[00:51:24.640 --> 00:51:28.000]   -Yeah, I just had a pretty basic question.
[00:51:28.000 --> 00:51:32.280]   So if an object lies on the border between the patches,
[00:51:32.280 --> 00:51:37.040]   does that impact the model's performance in any way?
[00:51:37.040 --> 00:51:39.280]   -Yeah, I mean, that's not a basic question.
[00:51:39.280 --> 00:51:40.880]   It's a good question.
[00:51:40.880 --> 00:51:45.560]   There is a mix of answers.
[00:51:45.560 --> 00:51:48.960]   So one is we didn't specifically go and test this.
[00:51:48.960 --> 00:51:51.800]   It would be an interesting thing to test in a very controlled way
[00:51:51.800 --> 00:51:55.000]   with some of the trained models.
[00:51:55.000 --> 00:51:57.520]   That's for sure.
[00:51:57.520 --> 00:52:01.360]   The other thing is that when you have a massive data set,
[00:52:01.360 --> 00:52:03.920]   like 300 million images, it's an insane amount.
[00:52:03.920 --> 00:52:07.960]   I used to try to conceptualize how much is image net,
[00:52:07.960 --> 00:52:10.920]   1 million images, and I think I did the math.
[00:52:10.920 --> 00:52:15.520]   It's like if you go to an image and look at all of the images,
[00:52:15.520 --> 00:52:17.200]   each image for a couple of seconds,
[00:52:17.200 --> 00:52:19.720]   you are sitting there for a month or something like that.
[00:52:19.720 --> 00:52:21.600]   Don't remember.
[00:52:21.600 --> 00:52:24.520]   But so 300 million is just insanely massive.
[00:52:24.520 --> 00:52:27.160]   And then on top of that, we do actually use
[00:52:27.160 --> 00:52:30.920]   random augmentations, like random crop out of the image.
[00:52:30.920 --> 00:52:34.040]   So I would say it's the default that you see objects
[00:52:34.040 --> 00:52:38.760]   that don't fall on a patch during the training already.
[00:52:38.760 --> 00:52:40.600]   And if you look at here, basically,
[00:52:40.600 --> 00:52:44.360]   this is the standard model, like how the patches are.
[00:52:44.360 --> 00:52:50.240]   When we have 14 by 14, they look roughly this size also.
[00:52:50.240 --> 00:52:54.040]   Then an object is usually scattered across many patches,
[00:52:54.040 --> 00:52:57.920]   actually, because objects in typical images
[00:52:57.920 --> 00:52:59.280]   are relatively large, right?
[00:52:59.280 --> 00:53:01.640]   People don't take a picture where the object of interest
[00:53:01.640 --> 00:53:04.400]   is super tiny in the corner.
[00:53:04.400 --> 00:53:06.680]   So that's the default that you see during pre-training.
[00:53:06.680 --> 00:53:08.840]   And so I believe that the model just
[00:53:08.840 --> 00:53:13.640]   learns to do that much better, actually.
[00:53:13.640 --> 00:53:16.480]   Then the other answer to the question is like, OK,
[00:53:16.480 --> 00:53:20.720]   maybe if you did some nicer thing than this very crude
[00:53:20.720 --> 00:53:24.480]   patch cutting, like for example, this stack of convolutions
[00:53:24.480 --> 00:53:28.000]   that I mentioned, maybe this is even better.
[00:53:28.000 --> 00:53:30.960]   Thank you.
[00:53:30.960 --> 00:53:31.480]   Thank you.
[00:53:31.480 --> 00:53:40.720]   So you mentioned that we're using transformers,
[00:53:40.720 --> 00:53:42.320]   or at least you mentioned in the paper
[00:53:42.320 --> 00:53:49.800]   that they lack locality and echoliteration.
[00:53:49.800 --> 00:53:54.400]   I was just thinking, are these sort of properties
[00:53:54.400 --> 00:54:00.880]   that you probably [INAUDIBLE] and especially when
[00:54:00.880 --> 00:54:02.720]   you're in the [INAUDIBLE]
[00:54:02.720 --> 00:54:12.040]   So why is it that we would prefer [INAUDIBLE]
[00:54:12.040 --> 00:54:13.520]   The audio was not that good, but I
[00:54:13.520 --> 00:54:14.920]   believe I understood the question.
[00:54:14.920 --> 00:54:20.160]   Is that we say that transformers lack locality bias, or prior,
[00:54:20.160 --> 00:54:21.280]   or whatever?
[00:54:21.280 --> 00:54:25.120]   And why is this even something that we want, right?
[00:54:25.120 --> 00:54:27.560]   Wouldn't we want our models to know about locality
[00:54:27.560 --> 00:54:30.800]   if they are about pictures in the first place?
[00:54:30.800 --> 00:54:32.680]   Yes and no.
[00:54:32.680 --> 00:54:35.520]   So that's why I gave the context in the beginning.
[00:54:35.520 --> 00:54:39.760]   This is all about what happens when you scale things up.
[00:54:39.760 --> 00:54:46.640]   And specifically, in the ideal world, at least in our mind,
[00:54:46.640 --> 00:54:49.360]   we want gigantic amounts of data.
[00:54:49.360 --> 00:54:51.400]   And we believe that it will just keep
[00:54:51.400 --> 00:54:54.000]   growing as the years go by.
[00:54:54.000 --> 00:54:58.560]   And there will be more and more data just generally there.
[00:54:58.560 --> 00:55:01.160]   And then we want the model to have as little
[00:55:01.160 --> 00:55:04.720]   of our thinking built in.
[00:55:04.720 --> 00:55:08.000]   Because what we may think that is good to solve the task
[00:55:08.000 --> 00:55:10.880]   may actually not be best to solve the task.
[00:55:10.880 --> 00:55:14.880]   Maybe an analogy would be, what was it,
[00:55:14.880 --> 00:55:17.760]   AlphaGo that made some moves that experts would say,
[00:55:17.760 --> 00:55:18.440]   this is crazy.
[00:55:18.440 --> 00:55:19.360]   This is a silly move.
[00:55:19.360 --> 00:55:22.240]   But it actually then was much better.
[00:55:22.240 --> 00:55:26.040]   And in a similar way, we want to encode as little as possible
[00:55:26.040 --> 00:55:28.400]   into the model, such that if we just
[00:55:28.400 --> 00:55:31.520]   throw massive amounts of data in the difficult task at it,
[00:55:31.520 --> 00:55:33.720]   that it might think things that are even better that we
[00:55:33.720 --> 00:55:36.120]   didn't think of before.
[00:55:36.120 --> 00:55:37.960]   This is our approach.
[00:55:37.960 --> 00:55:43.040]   Because we believe that, as I mentioned, I think, already,
[00:55:43.040 --> 00:55:45.400]   what seems massive and excessive now
[00:55:45.400 --> 00:55:47.560]   will be the norm in five years or so.
[00:55:47.560 --> 00:55:51.520]   So that's where we want to go and look what's the direction.
[00:55:51.520 --> 00:55:56.480]   However, if you want to just get something working now
[00:55:56.480 --> 00:55:58.240]   and don't have massive amounts of data
[00:55:58.240 --> 00:55:59.880]   and don't want to use pre-trained model
[00:55:59.880 --> 00:56:04.440]   for some reason, which always use a pre-trained model,
[00:56:04.440 --> 00:56:06.760]   but if you don't want to, then it
[00:56:06.760 --> 00:56:08.800]   makes total sense to build in some
[00:56:08.800 --> 00:56:12.000]   of your prior intuition and knowledge of what should
[00:56:12.000 --> 00:56:16.240]   probably help the model, like locality.
[00:56:16.240 --> 00:56:19.320]   I hope this answered your question.
[00:56:19.320 --> 00:56:22.280]   I suppose this is a quick follow up.
[00:56:22.280 --> 00:56:30.040]   What sort of [INAUDIBLE] like any vision task?
[00:56:30.040 --> 00:56:33.560]   Isn't that sort of like, yeah, I don't know.
[00:56:33.560 --> 00:56:37.720]   Maybe I'm not seeing exactly why we'd not
[00:56:37.720 --> 00:56:40.080]   want those inductive biases.
[00:56:40.080 --> 00:56:42.680]   Could you maybe elaborate on that?
[00:56:42.680 --> 00:56:46.240]   Why is it that we don't want locality
[00:56:46.240 --> 00:56:50.560]   or what translation [INAUDIBLE]
[00:56:50.560 --> 00:56:53.840]   Well, ideally, we want the model that
[00:56:53.840 --> 00:56:58.840]   is powerful enough to learn about this concept itself
[00:56:58.840 --> 00:57:01.320]   if it is useful to solve the task.
[00:57:01.320 --> 00:57:05.840]   If it's not useful to solve the task, then if we had put it in,
[00:57:05.840 --> 00:57:10.840]   there is no way for the model not to do this, right?
[00:57:10.840 --> 00:57:12.600]   That is ideally the outcome.
[00:57:12.600 --> 00:57:16.120]   In a similar way also that in language,
[00:57:16.120 --> 00:57:19.480]   it seemed to be nonsense to not encode
[00:57:19.480 --> 00:57:24.080]   the from left to right direction of text, like in RMS.
[00:57:24.080 --> 00:57:26.440]   But then comes transformer and just doesn't.
[00:57:26.440 --> 00:57:29.600]   And works much better if you throw a lot of data at it.
[00:57:29.600 --> 00:57:34.640]   And it recovers that plus some more or a more flexible variant
[00:57:34.640 --> 00:57:35.840]   of it or something like that.
[00:57:35.840 --> 00:57:39.240]   That is even better for solving tasks.
[00:57:39.240 --> 00:57:42.840]   So basically, the idea being that we are not
[00:57:42.840 --> 00:57:47.800]   as smart to design the thing, the model in the way that
[00:57:47.800 --> 00:57:49.160]   will be best for the task.
[00:57:49.160 --> 00:57:52.440]   Let's rather give it all the flexibility and all the data
[00:57:52.440 --> 00:57:54.160]   it needs to figure out what is the best
[00:57:54.160 --> 00:57:55.200]   way of solving the task.
[00:57:55.200 --> 00:58:05.640]   I mean, it is a philosophy of approaching it.
[00:58:05.640 --> 00:58:07.880]   I'm not saying this is the only true way, right?
[00:58:07.880 --> 00:58:16.320]   So we have around seven minutes left
[00:58:16.320 --> 00:58:18.720]   before the scheduled end of the talk.
[00:58:18.720 --> 00:58:21.240]   And Lucas, we want to be mindful of your time
[00:58:21.240 --> 00:58:24.520]   as well, because it is evening where you are.
[00:58:24.520 --> 00:58:28.360]   So one thing we could do is you could--
[00:58:28.360 --> 00:58:30.240]   I don't see any more questions right now.
[00:58:30.240 --> 00:58:33.800]   So you could quickly go over the last few bits,
[00:58:33.800 --> 00:58:36.000]   maybe skipping through the details
[00:58:36.000 --> 00:58:39.200]   and just talking about the final results.
[00:58:39.200 --> 00:58:41.080]   I will do this to a high level, then.
[00:58:41.080 --> 00:58:44.360]   Those two that are still very, very tight to transformers
[00:58:44.360 --> 00:58:46.880]   and answer some questions that happened before.
[00:58:46.880 --> 00:58:50.360]   Like the first question was like, OK, are we saturating?
[00:58:50.360 --> 00:58:51.320]   Yes or no?
[00:58:51.320 --> 00:58:56.280]   And here, no.
[00:58:56.280 --> 00:58:58.960]   This was the bit on this benchmark
[00:58:58.960 --> 00:59:01.360]   from the original transforming paper.
[00:59:01.360 --> 00:59:04.280]   But then it's like these transformers,
[00:59:04.280 --> 00:59:06.760]   when we use them, we just notice they have really nice scaling
[00:59:06.760 --> 00:59:07.280]   properties.
[00:59:07.280 --> 00:59:08.920]   And they seem, actually, to be easier
[00:59:08.920 --> 00:59:12.920]   to scale up without paying massive compute as much
[00:59:12.920 --> 00:59:17.320]   as ResNet, just from gut feeling from us having experience
[00:59:17.320 --> 00:59:17.920]   with both.
[00:59:17.920 --> 00:59:20.120]   And so we went and looked what happens
[00:59:20.120 --> 00:59:23.960]   if we scale vision transformer just as far up
[00:59:23.960 --> 00:59:25.640]   as we possibly can.
[00:59:25.640 --> 00:59:30.400]   And we spent quite a lot of our blood into making this happen.
[00:59:30.400 --> 00:59:33.640]   One part of it is scaling the data set.
[00:59:33.640 --> 00:59:36.680]   So we went back to this Google internal team
[00:59:36.680 --> 00:59:39.760]   that this 300 million data set is just one out of many
[00:59:39.760 --> 00:59:41.360]   that they work with.
[00:59:41.360 --> 00:59:43.480]   And we asked around, and they basically
[00:59:43.480 --> 00:59:46.240]   had the 3 billion, like 10 times larger data set
[00:59:46.240 --> 00:59:49.280]   that we could also play around with.
[00:59:49.280 --> 00:59:50.240]   So there we go.
[00:59:50.240 --> 00:59:52.360]   We want to scale up the data set.
[00:59:52.360 --> 00:59:56.120]   And this is just showing, yes, just scaling up the data set
[00:59:56.120 --> 00:59:57.720]   and switching it gives you benefits,
[00:59:57.720 --> 01:00:00.120]   but that's not all of it.
[01:00:00.120 --> 01:00:03.080]   Then the next thing is we needed to figure out
[01:00:03.080 --> 01:00:07.640]   how to use less memory on device, like on GPU or TPU,
[01:00:07.640 --> 01:00:10.280]   because already previously with this score,
[01:00:10.280 --> 01:00:13.200]   we fitted the model as large as we could fit.
[01:00:13.200 --> 01:00:16.360]   So we did a lot of clicks that I will skip for now
[01:00:16.360 --> 01:00:18.960]   and are able to scale much larger.
[01:00:18.960 --> 01:00:22.040]   This is like-- this plot shows the size
[01:00:22.040 --> 01:00:24.520]   of the model in the different shape factors
[01:00:24.520 --> 01:00:27.400]   that I mentioned before, like the width of the MIP on x-axis,
[01:00:27.400 --> 01:00:29.720]   the self-attention width on the y-axis,
[01:00:29.720 --> 01:00:32.760]   and then the different plots are different layers for the depth.
[01:00:32.760 --> 01:00:35.720]   This box are how large the transformer
[01:00:35.720 --> 01:00:37.800]   we did in the original paper.
[01:00:37.800 --> 01:00:40.640]   And then boom, one step further and two steps further,
[01:00:40.640 --> 01:00:43.440]   this is just super massive transformer
[01:00:43.440 --> 01:00:46.600]   we did in this scaling paper.
[01:00:46.600 --> 01:00:48.840]   And with all of our tricks, how much larger
[01:00:48.840 --> 01:00:51.640]   we could go, a lot larger.
[01:00:51.640 --> 01:00:54.360]   Then, yeah, some learning rate stuff, and it is really cool.
[01:00:54.360 --> 01:00:56.840]   I recommend people to look at square root learning rate
[01:00:56.840 --> 01:00:59.320]   schedule, which is cool, and often just mentioned
[01:00:59.320 --> 01:01:02.400]   as a side note.
[01:01:02.400 --> 01:01:04.240]   It is also cool, but I'm going to skip it
[01:01:04.240 --> 01:01:08.160]   for the interest and basic interest of time.
[01:01:08.160 --> 01:01:10.920]   And basically, we scaled it up a lot.
[01:01:10.920 --> 01:01:14.040]   And of course, again, we get always
[01:01:14.040 --> 01:01:17.840]   this envision image net number a bit higher.
[01:01:17.840 --> 01:01:20.200]   This is actually plus 2% on what we had before,
[01:01:20.200 --> 01:01:23.440]   which is very significant in this high percentage range
[01:01:23.440 --> 01:01:24.880]   there.
[01:01:24.880 --> 01:01:26.520]   But also, what's very interesting
[01:01:26.520 --> 01:01:28.760]   is the view shot again.
[01:01:28.760 --> 01:01:30.840]   By just keep scaling up everything,
[01:01:30.840 --> 01:01:33.600]   we get super large boost in view shot again.
[01:01:33.600 --> 01:01:35.800]   This is image net top 1 accuracy.
[01:01:35.800 --> 01:01:40.560]   And for example, it's just 10 images per image net class,
[01:01:40.560 --> 01:01:44.000]   which means 10,000 images total because 1,000 classes.
[01:01:44.000 --> 01:01:46.280]   We get this big of a jump.
[01:01:46.280 --> 01:01:52.840]   We get 85% of 1 accuracy, which is what you typically
[01:01:52.840 --> 01:01:56.320]   get when using the full data set, basically.
[01:01:56.320 --> 01:01:58.600]   So this is scaling up.
[01:01:58.600 --> 01:02:02.000]   It makes actually view shot work significantly better.
[01:02:02.000 --> 01:02:04.240]   And then I'm going to skip on this.
[01:02:04.240 --> 01:02:06.480]   Well, this actually has an interesting message.
[01:02:06.480 --> 01:02:08.960]   This is three times the same story,
[01:02:08.960 --> 01:02:11.040]   but measured in a slightly different way,
[01:02:11.040 --> 01:02:13.920]   which is that if you make the model larger,
[01:02:13.920 --> 01:02:16.320]   it actually needs to see fewer images
[01:02:16.320 --> 01:02:18.400]   to get to a similar score.
[01:02:18.400 --> 01:02:22.080]   This blue line is a tiny vision transformer,
[01:02:22.080 --> 01:02:24.680]   and the base vision transformer in the large one.
[01:02:24.680 --> 01:02:26.520]   And the y-axis is the error.
[01:02:26.520 --> 01:02:28.040]   So lower is better.
[01:02:28.040 --> 01:02:30.000]   And actually, you need to see--
[01:02:30.000 --> 01:02:32.120]   still, we're talking in millions of images,
[01:02:32.120 --> 01:02:33.880]   and here it's 100 million images.
[01:02:33.880 --> 01:02:36.560]   But still, you need to see a lot fewer images
[01:02:36.560 --> 01:02:37.960]   with the larger models.
[01:02:37.960 --> 01:02:39.560]   Doesn't mean a lot less compute, right?
[01:02:39.560 --> 01:02:42.400]   Because the model is larger and the slower.
[01:02:42.400 --> 01:02:44.160]   But it's interesting.
[01:02:44.160 --> 01:02:45.720]   And then there's some scaling loss
[01:02:45.720 --> 01:02:47.240]   that are popular in language.
[01:02:47.240 --> 01:02:49.880]   And we, I think, maybe for the first time
[01:02:49.880 --> 01:02:52.880]   in discriminative image learning show
[01:02:52.880 --> 01:02:56.800]   that, yeah, they appear to be here, too.
[01:02:56.800 --> 01:02:59.800]   And then-- right.
[01:02:59.800 --> 01:03:01.400]   Then we want to--
[01:03:01.400 --> 01:03:04.160]   sorry, I had the order of the slides mixed up in my head.
[01:03:04.160 --> 01:03:05.120]   So I'm a bit surprised.
[01:03:05.120 --> 01:03:08.720]   But then another threat was that besides further scaling up
[01:03:08.720 --> 01:03:11.040]   the model, we wanted to push even further
[01:03:11.040 --> 01:03:16.520]   into this direction of less hand engineering of things
[01:03:16.520 --> 01:03:18.560]   into the model architecture.
[01:03:18.560 --> 01:03:21.360]   And then with the vision transformer,
[01:03:21.360 --> 01:03:24.360]   transform in general, what is the obviously most hand
[01:03:24.360 --> 01:03:26.480]   engineered part of it is the self-attention.
[01:03:26.480 --> 01:03:29.440]   So we tried, what can we do something
[01:03:29.440 --> 01:03:34.240]   more generic than that and less smart than that, basically?
[01:03:34.240 --> 01:03:36.480]   And we ended up by replacing it, essentially,
[01:03:36.480 --> 01:03:42.400]   with just a multi-layer perceptron that, however,
[01:03:42.400 --> 01:03:44.680]   has a little bit of structure, but much less
[01:03:44.680 --> 01:03:46.120]   than self-attention.
[01:03:46.120 --> 01:03:49.680]   So they would skip the structure or the safety of time.
[01:03:49.680 --> 01:03:53.120]   And we're coming back to this plot, where the question was,
[01:03:53.120 --> 01:03:54.480]   aren't we saturating?
[01:03:54.480 --> 01:03:56.040]   Now, this plot is slightly different.
[01:03:56.040 --> 01:03:59.040]   We, again, have this bit resonate here in black.
[01:03:59.040 --> 01:04:02.760]   And the full green line is the vision transformer.
[01:04:02.760 --> 01:04:04.680]   And the other color, also the full lines,
[01:04:04.680 --> 01:04:05.800]   are the vision transformers.
[01:04:05.800 --> 01:04:09.000]   So it is exactly the same numbers as from before.
[01:04:09.000 --> 01:04:11.800]   However, now we also throw in this mixer architecture,
[01:04:11.800 --> 01:04:14.240]   which we believe is even more flexible and less
[01:04:14.240 --> 01:04:16.240]   hand-engineered than transformer.
[01:04:16.240 --> 01:04:19.400]   And as you see, with less data, it's even worse.
[01:04:19.400 --> 01:04:22.520]   However, with much more data, it may
[01:04:22.520 --> 01:04:27.560]   be surpassing the transformer, or it may be random noise.
[01:04:27.560 --> 01:04:29.000]   Not clear at this point, right?
[01:04:29.000 --> 01:04:32.240]   Because it's the only point where this happens.
[01:04:32.240 --> 01:04:33.760]   So we need to go further.
[01:04:33.760 --> 01:04:35.960]   So we use this 3 billion data set,
[01:04:35.960 --> 01:04:40.160]   for example, from the previous paper that I mentioned here,
[01:04:40.160 --> 01:04:44.360]   and try to extend these lines to the right to see what happens.
[01:04:44.360 --> 01:04:46.360]   We don't extend many of them, because these
[01:04:46.360 --> 01:04:48.360]   are very expensive experiments that
[01:04:48.360 --> 01:04:50.320]   require a ton of patience.
[01:04:50.320 --> 01:04:52.840]   But we extended two most interesting.
[01:04:52.840 --> 01:04:54.720]   And it seems that it continues.
[01:04:54.720 --> 01:04:58.000]   And that, first of all, yes, the vision transformer
[01:04:58.000 --> 01:04:59.880]   keeps increasing.
[01:04:59.880 --> 01:05:02.280]   We don't have such experiment with the ResNet,
[01:05:02.280 --> 01:05:04.400]   because it doesn't look promising enough
[01:05:04.400 --> 01:05:07.520]   to pay the cost of doing it.
[01:05:07.520 --> 01:05:10.000]   But it also seems that the mixer, what we believe
[01:05:10.000 --> 01:05:11.560]   is even more flexible architecture,
[01:05:11.560 --> 01:05:15.160]   actually is consistently above the transformer now,
[01:05:15.160 --> 01:05:17.080]   which is good news.
[01:05:17.080 --> 01:05:19.200]   And yeah, it is good news.
[01:05:19.200 --> 01:05:23.640]   So we're now right at the time when I should stop, right?
[01:05:23.640 --> 01:05:26.120]   Or open to more questions again.
[01:05:26.120 --> 01:05:29.160]   Yeah, I guess, as a question--
[01:05:29.160 --> 01:05:31.520]   Can I ask a follow up on the scaling
[01:05:31.520 --> 01:05:33.080]   that you were showing earlier?
[01:05:33.080 --> 01:05:35.080]   It's related to my previous question.
[01:05:35.080 --> 01:05:37.640]   I'm curious how this model size compares
[01:05:37.640 --> 01:05:41.840]   to model sizes for Earth or the natural language.
[01:05:41.840 --> 01:05:43.840]   [INAUDIBLE]
[01:05:43.840 --> 01:05:46.080]   Like, especially when we're going from smaller models
[01:05:46.080 --> 01:05:49.520]   to much bigger models, are they comparable at all
[01:05:49.520 --> 01:05:50.880]   in terms of model size?
[01:05:50.880 --> 01:05:53.880]   And if not, why do you think--
[01:05:53.880 --> 01:05:57.280]   what is the [INAUDIBLE] models for these two different tasks?
[01:05:57.280 --> 01:06:01.160]   Yeah, actually, a colleague of mine has a slide, which I hate.
[01:06:01.160 --> 01:06:04.280]   But he loves-- it's the model number of parameters
[01:06:04.280 --> 01:06:07.240]   in NLP and in vision.
[01:06:07.240 --> 01:06:10.240]   And the question is, how do you measure model size?
[01:06:10.240 --> 01:06:12.200]   If you just measure number of parameters,
[01:06:12.200 --> 01:06:15.160]   then these vision models are much smaller.
[01:06:15.160 --> 01:06:19.200]   However, the language models, number of parameters,
[01:06:19.200 --> 01:06:21.480]   like a huge chunk of it is in the dictionary,
[01:06:21.480 --> 01:06:23.880]   for example, which for us just doesn't exist.
[01:06:23.880 --> 01:06:29.240]   It is linear embedding, which is trivial number of parameters.
[01:06:29.240 --> 01:06:32.480]   So in terms of number of parameters, it's much smaller.
[01:06:32.480 --> 01:06:34.800]   My personal opinion is number of parameters
[01:06:34.800 --> 01:06:37.200]   doesn't mean that much.
[01:06:37.200 --> 01:06:39.120]   Then the other way that you could measure
[01:06:39.120 --> 01:06:42.760]   this maybe in terms of compute, like how much floating point
[01:06:42.760 --> 01:06:46.400]   operations does it do on one data point.
[01:06:46.400 --> 01:06:50.120]   And in terms of this, it's in the same ballpark.
[01:06:50.120 --> 01:06:52.720]   However, last time I checked, which is quite a few months
[01:06:52.720 --> 01:06:55.640]   ago, the largest language model was still
[01:06:55.640 --> 01:06:59.800]   like four times more or five times more in the vision model,
[01:06:59.800 --> 01:07:02.200]   I believe.
[01:07:02.200 --> 01:07:05.320]   So that's the two ways of measuring model size.
[01:07:05.320 --> 01:07:07.880]   I don't think either of the ways is
[01:07:07.880 --> 01:07:09.560]   the one true way to measure model size.
[01:07:09.560 --> 01:07:11.920]   And I think it's actually an interesting research topic,
[01:07:11.920 --> 01:07:15.280]   like how to properly measure and order models
[01:07:15.280 --> 01:07:18.520]   in terms of capacity is not clear.
[01:07:18.520 --> 01:07:19.800]   [INAUDIBLE]
[01:07:19.800 --> 01:07:21.720]   Do you know why the vision is--
[01:07:21.720 --> 01:07:23.880]   I'm sorry, the vision is four times smaller?
[01:07:23.880 --> 01:07:26.520]   Like, what about that [INAUDIBLE]??
[01:07:26.520 --> 01:07:30.200]   I think it's just there is less interest in it,
[01:07:30.200 --> 01:07:34.360]   so less resources spent on it, basically.
[01:07:34.360 --> 01:07:37.160]   Like in Google, there are many, many more groups
[01:07:37.160 --> 01:07:40.720]   doing research with language than with vision.
[01:07:40.720 --> 01:07:44.000]   And I think we are one of the few groups that
[01:07:44.000 --> 01:07:45.640]   have access to a lot of resources
[01:07:45.640 --> 01:07:48.920]   and are interested in scaling up things in vision so much.
[01:07:48.920 --> 01:07:51.240]   Whereas in language, it seems there are a lot of groups
[01:07:51.240 --> 01:07:53.000]   that are doing that.
[01:07:53.000 --> 01:07:56.040]   I think that's the main reason, actually.
[01:07:56.040 --> 01:07:58.760]   It's not that we don't want to go beyond that,
[01:07:58.760 --> 01:08:03.960]   or if we can, we would go even more.
[01:08:03.960 --> 01:08:04.720]   Awesome, thank you.
[01:08:05.280 --> 01:08:07.760]   [INAUDIBLE]
[01:08:07.760 --> 01:08:10.840]   Right, so we are actually over time at this point.
[01:08:10.840 --> 01:08:13.720]   So anyone who has to leave, please feel free to do so.
[01:08:13.720 --> 01:08:18.240]   And before we do that, Lucas, thank you so much for joining,
[01:08:18.240 --> 01:08:21.600]   for all the way from across the ocean.
[01:08:21.600 --> 01:08:24.080]   And we know it's in the evening, so thank you
[01:08:24.080 --> 01:08:27.520]   for taking your free time to come and talk to us here.
[01:08:27.520 --> 01:08:29.040]   Yeah, thanks for the invitation.
[01:08:29.040 --> 01:08:31.840]   Always like to talk about the work.
[01:08:31.840 --> 01:08:41.840]   [BLANK_AUDIO]

