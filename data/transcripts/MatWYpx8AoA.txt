
[00:00:00.000 --> 00:00:07.680]   Hello everybody and welcome to the second week of Fastbook Reading Group. It's sort of say a
[00:00:07.680 --> 00:00:15.920]   session starting shortly but let's begin. So something's there's a whole week, the whole
[00:00:15.920 --> 00:00:22.000]   first week has been really really exciting and it's been much more exciting than I imagined.
[00:00:22.800 --> 00:00:32.720]   And as part of week one we covered until topic 1.6.7 of the chapter one of Fastbook and what
[00:00:32.720 --> 00:00:39.200]   we're planning to do for this week or for today is going to be we're going to do chapter two and
[00:00:39.200 --> 00:00:46.880]   we're going to do wrap up chapter one as well and we'll do half of chapter two. And something as I
[00:00:46.880 --> 00:00:54.240]   have promised in the past is we're going to have Sanyam, Zach, Tanish doing guest lectures. Sanyam
[00:00:54.240 --> 00:01:00.800]   just sent a hello in in chat. So hey Sanyam and Tanishk is here today. He's with us and he'll be
[00:01:00.800 --> 00:01:10.480]   presenting about his journey on Fast.ai. Hey Tanishk. Do you want to quickly tell everyone
[00:01:10.480 --> 00:01:16.240]   about how you're related to Fast.ai and what are you going to be talking about today towards the
[00:01:16.240 --> 00:01:28.560]   end? Yes so yeah I was also I guess a luminous of the Fast.ai course. I took the Fast.ai course back
[00:01:28.560 --> 00:01:36.080]   in 2019 and it you know really helped me with my deep learning journey and I've also been a frequent
[00:01:36.080 --> 00:01:43.200]   contributor to the Fast.ai forums and to the Fast.ai library and I've been using Fast.ai in my
[00:01:43.200 --> 00:01:51.280]   own research as well. Excellent and today you'll be discussing about how your journey started with
[00:01:51.280 --> 00:01:58.000]   Fast.ai and how'd you end up being a being a solid contributor to the Fast.ai world and being someone
[00:01:58.000 --> 00:02:03.120]   who's very well known in the in the ML community. So really appreciate your time and thanks for
[00:02:03.120 --> 00:02:10.800]   being here with us today. Thank you for having me. Pleasure. And then well just we're going to spend
[00:02:10.800 --> 00:02:16.320]   a quick five minutes recap of the first week. The first week has been super exciting. We've had
[00:02:16.320 --> 00:02:25.680]   I believe that that person's Korean. He started a new blog and he's keeping a
[00:02:25.680 --> 00:02:30.320]   I mean it's really exciting to see that people are keeping and taking the advice that we shared
[00:02:30.320 --> 00:02:36.960]   in last week and this has come out of and he started his own new blog. We've seen Ravi on
[00:02:36.960 --> 00:02:42.880]   Twitter. He's had a whole summary of how the first talk looked like which is really exciting to see
[00:02:42.880 --> 00:02:50.640]   and he's added all the important links and notes from the week one. We've had Abhi. He's also
[00:02:50.640 --> 00:02:58.480]   had he's also provided a summary of what we covered in week one and Abhishek has also done that. So I
[00:02:58.480 --> 00:03:04.320]   think it's really exciting to see that you guys are really enjoying being here and it's really
[00:03:04.320 --> 00:03:10.720]   exciting and it's really motivating for me to continue doing this with even more. But what's
[00:03:10.720 --> 00:03:17.760]   more exciting is that we've had about 200 members in Slack already and we're all doing this together.
[00:03:17.760 --> 00:03:22.560]   So this is the same motto as last time is we're going to all do this together and we're going to
[00:03:22.560 --> 00:03:28.160]   make sure that each one of us finish. And we've had activity on the forum so we've had and all of
[00:03:28.160 --> 00:03:33.600]   the questions that we've asked this week we have answered. So we've had as I've mentioned we've had
[00:03:33.600 --> 00:03:39.840]   213 members on Slack and what's really exciting is to see people like Sam come up and Sam said
[00:03:39.840 --> 00:03:45.760]   he's going to help with any set up questions and it's literally the community coming together and
[00:03:45.760 --> 00:03:51.760]   this is the benefit of doing doing Fastbook as a group rather than doing it alone because then
[00:03:51.760 --> 00:03:56.960]   people like Sam come up. We have Karthik who was asking a question but Kevin was already there
[00:03:56.960 --> 00:04:01.840]   to answer that question for him. Brad was there. Sam was there again and that
[00:04:03.120 --> 00:04:07.600]   I believe that question got solved. So I've always been late to all of the questions that
[00:04:07.600 --> 00:04:13.040]   I've tried to answer. We've had another question like okay I've got an issue on I've got an issue
[00:04:13.040 --> 00:04:19.920]   using Google Colab and does anybody know how to solve it. So Ramin was there, Sam was there and
[00:04:19.920 --> 00:04:24.880]   Brad was there again and they've all answered the questions even before I got a chance to do so.
[00:04:24.880 --> 00:04:32.400]   I've seen Sai and I'm really excited to see that Sai has said I'm going to commit to completing
[00:04:32.400 --> 00:04:38.080]   it properly this time. So he's committed to being there for 20 weeks and this is just 10-15% of the
[00:04:38.080 --> 00:04:45.920]   activity that we've seen on Slack and on the forums and we've seen this really interesting
[00:04:45.920 --> 00:04:52.080]   Fast.ai docs material being shared on Slack as well. So it's really really exciting to see
[00:04:52.080 --> 00:04:59.040]   lots of shares, lots of activity going on and lots of resources that are being shared and
[00:05:00.080 --> 00:05:05.360]   again there was this question about I'm having a problem when I'm trying to download images.
[00:05:05.360 --> 00:05:11.600]   Somebody already pointed to the Fast.ai forums. If you haven't looked at what the Fast.ai forums are,
[00:05:11.600 --> 00:05:19.760]   they're a goldmine of pretty much all your Fast.ai related questions. So if I go to that
[00:05:19.760 --> 00:05:22.800]   point and I go to forums.fast.ai
[00:05:22.800 --> 00:05:28.720]   and I mentioned the Fast.ai forums last time as well but this is really where
[00:05:29.280 --> 00:05:32.640]   you can start and you can ask all your questions that are related to Fast.ai.
[00:05:32.640 --> 00:05:40.000]   And we've also seen Rishit be there and he was asking a question and again Justin was there to
[00:05:40.000 --> 00:05:45.440]   the rescue and he answered that question by giving a link to the Fast.ai forum. So that's how exciting
[00:05:45.440 --> 00:05:50.560]   this first week has been. Something I was really sad about last week was we couldn't cover much
[00:05:50.560 --> 00:05:56.800]   ground and in 60 minutes we were only able to cover half of the first chapter. So something
[00:05:56.800 --> 00:06:00.240]   we're doing, you would have seen these announcements but I just want to quickly
[00:06:00.240 --> 00:06:06.240]   mention them again, that we're going from 60 minutes to 90 minutes instead. The whole idea is
[00:06:06.240 --> 00:06:12.320]   we want to cover more ground and we want to make sure that everybody is following along.
[00:06:12.320 --> 00:06:19.200]   Something that just happened last week was an informal get to know you catch up. It should
[00:06:19.200 --> 00:06:23.680]   be get to know us as in it was really exciting to talk to people who are doing Fastbook. It was
[00:06:23.680 --> 00:06:28.560]   really exciting to hear feedback that you guys had for me. It was really exciting to understand
[00:06:28.560 --> 00:06:32.960]   which backgrounds you come from. We've seen bachelor students, we had working professionals.
[00:06:32.960 --> 00:06:39.760]   It was really exciting to talk to you guys and we do plan on doing this again today. So the link for
[00:06:39.760 --> 00:06:46.400]   Slack is as Liga has just mentioned on the Zoom chat, it's 1db.me/slack. That's where in the
[00:06:46.400 --> 00:06:52.080]   Fastbook channel we'll be sharing the link today again for Zoom catch up. So please feel free to
[00:06:52.080 --> 00:06:56.400]   come in and say hi and we can talk to each other. The recordings you already know where they're
[00:06:56.400 --> 00:07:03.680]   going and by this time I hope everybody is set up and everybody has got their GPUs running because
[00:07:03.680 --> 00:07:07.760]   things are going to get much more interesting from here on and they're going to get much more.
[00:07:07.760 --> 00:07:11.920]   Let's just say the level is going to increase as we go week by week.
[00:07:17.120 --> 00:07:22.480]   Okay. With that being said, we are ready to begin. Something I've done from last week to this week is
[00:07:22.480 --> 00:07:26.960]   I've went in, I've converted the whole Jupyter Notebook to a PDF so it's easier for me. I really
[00:07:26.960 --> 00:07:32.960]   like doing things like taking the pen in my hand and I really like scribbling. It's really easy
[00:07:32.960 --> 00:07:38.160]   for me to then explain things and Jupyter, I couldn't find a better way to do it than just
[00:07:38.160 --> 00:07:44.080]   convert it to a PDF. So let's just quickly go through what we discussed last week. We saw deep
[00:07:44.080 --> 00:07:48.160]   learning is for everyone. That was the first thing and the main thing that we saw is that anybody
[00:07:48.160 --> 00:07:53.120]   from any background can come and do deep learning. You guys can see my OneNote, right? Just double
[00:07:53.120 --> 00:08:02.160]   checking. Awesome. Thanks, Anand. Cool. So then we saw there was a brief history of neural networks
[00:08:02.160 --> 00:08:07.920]   that we looked at. We looked at who Jeremy is, what background he comes from. We looked at who
[00:08:07.920 --> 00:08:14.800]   Sylvain is. So Jeremy and Sylvain are both, they've both contributed a lot of their time in
[00:08:14.800 --> 00:08:20.480]   open source world and thanks to them, we have this book that we're discussing today. We looked at the
[00:08:20.480 --> 00:08:26.800]   basic idea of how to learn deep learning and the basic idea of learning deep learning is to do it
[00:08:26.800 --> 00:08:32.080]   like a baby learns football. It's like a game. You're given the football first and you start
[00:08:32.080 --> 00:08:37.200]   playing with it first and we want to have lots of experimentation. We want to play with deep learning
[00:08:37.200 --> 00:08:41.520]   and that's the main idea of how we're going to learn deep learning. We're going to do lots of
[00:08:41.520 --> 00:08:46.480]   projects. We're going to form, we're going to basically do lots of projects. We're going to
[00:08:46.480 --> 00:08:56.560]   learn by playing deep learning. We had a look at the projects and mindset. So the project is
[00:08:56.560 --> 00:09:01.920]   something that you complete that you're really proud of. It's not something that we really want
[00:09:01.920 --> 00:09:07.120]   you to commit to the projects. We really want you to find something that interests you because if
[00:09:07.120 --> 00:09:11.520]   there's something that interests you, then that's something you'll definitely end up finishing.
[00:09:11.520 --> 00:09:17.360]   We looked at what PyTorch, FastAI and Jupyter are. So FastAI is built on top of PyTorch and
[00:09:17.360 --> 00:09:23.600]   PyTorch is the underlying low-level library and that's where we started. We started with the first
[00:09:23.600 --> 00:09:30.160]   model. So we looked at getting, we need GPU. We need a GPU to be able to run all of the code
[00:09:30.160 --> 00:09:36.720]   that's present in FastBook and one of the main pointers where I said we should get started is
[00:09:36.720 --> 00:09:42.960]   Google Colab because it's just the easiest way to get started. All you need is a Google account and
[00:09:42.960 --> 00:09:46.160]   you can just click on any chapter and that will open up in a Colab notebook.
[00:09:46.160 --> 00:09:51.200]   There's more options that are also mentioned on the book's website.
[00:09:51.200 --> 00:09:57.440]   And then we started running our first notebook which was going to be a classification of dogs
[00:09:57.440 --> 00:10:02.960]   versus cats and we used that to train our first model basically.
[00:10:05.600 --> 00:10:12.480]   So the dataset was called Oxford IIIT Pet Dataset and these are the lines of code that we ran.
[00:10:12.480 --> 00:10:17.280]   There were questions that were being asked on what each line does in this part of code and there
[00:10:17.280 --> 00:10:22.720]   were some technical questions and some errors but we almost just with, this is how exciting and this
[00:10:22.720 --> 00:10:29.520]   is how wonderful FastAI is, that in just six lines of code we're able to actually first download the
[00:10:29.520 --> 00:10:36.960]   dataset then create a model, a deep learning model that can classify dogs from cats and we almost get
[00:10:36.960 --> 00:10:43.440]   a 100% accuracy or 0.004% error. So that's really, really high and that's really, really good
[00:10:43.440 --> 00:10:49.600]   in just a very short amount of code. So that's why FastAI is one of the best places to start if
[00:10:49.600 --> 00:10:53.760]   you're starting out with deep learning because it takes away a lot of boilerplate code. You don't
[00:10:53.760 --> 00:10:58.800]   have to actually worry about what's going on underneath and you can use all these convenience
[00:10:58.800 --> 00:11:04.320]   functions from FastAI. So we're going to be looking at all of what's happening in this
[00:11:04.320 --> 00:11:11.680]   piece of code. The next thing we did was we can actually test the models on our own images. So
[00:11:11.680 --> 00:11:16.320]   it's possible that if you've already run this in a Google collab you could have gone in and you
[00:11:16.320 --> 00:11:21.600]   could have uploaded your own cat or a dog picture and it actually does apply that the probability of
[00:11:21.600 --> 00:11:28.400]   cat is 1%. So that's the first classifier we started within just 30 or 40 minutes of starting
[00:11:28.400 --> 00:11:34.320]   out. Next we looked at what traditional programming looks like. So if you have your inputs, you pass
[00:11:34.320 --> 00:11:37.840]   them through a program and you just get results. That's just traditional programming.
[00:11:37.840 --> 00:11:45.520]   How deep learning is different. In deep learning you have a model that has inputs and it has
[00:11:45.520 --> 00:11:51.280]   weights but in the same way these weights are assigned to the model and in the same way you
[00:11:51.280 --> 00:11:56.640]   pass the inputs and you get the results. But the way it's different is that what you want to do is
[00:11:56.640 --> 00:12:00.560]   you want to make sure that you're able to update these weights and we discussed this a little bit
[00:12:00.560 --> 00:12:04.400]   that this is something that's called back propagation but we'll learn more about this.
[00:12:04.400 --> 00:12:08.080]   But in a way you have your inputs, your weights go into the model, you get some results,
[00:12:08.080 --> 00:12:12.960]   you check how your model's performance is. Is the model performing better or worse?
[00:12:12.960 --> 00:12:17.280]   And then you keep updating your weights because the weights are what are assigned to model. So
[00:12:17.280 --> 00:12:21.360]   you keep updating your weights, you keep doing this cycle over and over again till your model
[00:12:21.360 --> 00:12:26.720]   has really good weights and the performance is really something that's acceptable in high.
[00:12:26.720 --> 00:12:33.520]   So in a way when you get the trained model then all you can do is that trained model
[00:12:33.520 --> 00:12:39.920]   can have those weights that you learned from here. Those weights can then just be here assigned to
[00:12:39.920 --> 00:12:43.840]   this model and in the same way it's just like general programming. You pass your inputs and
[00:12:43.840 --> 00:12:47.680]   you get your results. So that's just how a trained model looks like.
[00:12:50.640 --> 00:12:56.080]   Cool. Then we, this is where we're going to start today is we're going to have a look at
[00:12:56.080 --> 00:13:04.320]   some of the jargon. So a weights, a model in deep learning world today, a model can also be called
[00:13:04.320 --> 00:13:09.600]   an architecture. So the functional form of the model and there's no point in digging very deep
[00:13:09.600 --> 00:13:15.760]   into these things but let's just say you'll hear model architectures together as words that would
[00:13:15.760 --> 00:13:20.480]   refer to this thing that's being trained to this function that's being trained that can actually
[00:13:20.480 --> 00:13:25.680]   make predictions. You'll hear weights and parameters almost together. So somebody will
[00:13:25.680 --> 00:13:30.800]   call these things as weights, somebody will call them as parameters. You'll hear the word loss a
[00:13:30.800 --> 00:13:36.560]   lot. Labels are the actual true labels of the data set which we'll also have a look into as we go
[00:13:36.560 --> 00:13:42.160]   down more into the notebook. And the loss is something that depends on these predictions
[00:13:42.160 --> 00:13:46.480]   from the model and it depends on the labels. So in a way if you put that jargon into that
[00:13:46.480 --> 00:13:52.000]   whole image you get this. Your inputs and instead of weights you call them parameters,
[00:13:52.000 --> 00:13:56.560]   they go into the model instead of the model you call it architecture. You get your predictions,
[00:13:56.560 --> 00:14:02.160]   you calculate your loss. Loss is just performance. Loss also requires the true labels and then you
[00:14:02.160 --> 00:14:07.760]   just update your weights. So this is just the main idea of deep learning and this is something,
[00:14:07.760 --> 00:14:12.960]   not even deep learning, is something that's very traditional and these concepts can be applied to
[00:14:14.480 --> 00:14:20.720]   all of machine learning today. Cool. So from here on this is where things are going to be new and
[00:14:20.720 --> 00:14:28.080]   I'm going to slow my pace and in case you thought I was going too fast. So what are the limitations
[00:14:28.080 --> 00:14:33.600]   to machine learning? One of the limitations of machine learning is you can't really do anything
[00:14:33.600 --> 00:14:40.960]   without data. Data is key and a lot of the things and how your model is trained would depend on the
[00:14:40.960 --> 00:14:46.640]   kind of data that you have. So the model will only learn the patterns that it will see in the input
[00:14:46.640 --> 00:14:52.960]   data and the model basically creates predictions based on the patterns it has learned from the
[00:14:52.960 --> 00:15:05.120]   input data and it doesn't really do recommended actions. So I'm skipping through the chunks that
[00:15:05.120 --> 00:15:10.480]   I believe. I'm just going to go through the main ideas of the notebook. So when you go back and
[00:15:10.480 --> 00:15:16.240]   you can read through them and if there's still any questions you can ask us. But the main things I
[00:15:16.240 --> 00:15:21.760]   want to touch upon are how does that piece of code or what's in those six lines of code and provide
[00:15:21.760 --> 00:15:28.960]   you with some transparency. So the first line of code we saw was just from fastai.vision.allimport*.
[00:15:28.960 --> 00:15:36.000]   That's just importing all your modules and all your libraries and all your function. So undatdata
[00:15:36.000 --> 00:15:41.600]   is a function in Python and this is what's importing that function called undatdata.
[00:15:41.600 --> 00:15:48.400]   One of the things that's generally frowned upon in Python world, like lots of people who
[00:15:48.400 --> 00:15:55.040]   would say, "Oh, we're professional software developers or we want to write code in a
[00:15:55.040 --> 00:15:59.680]   certain way, in a certain design." They will say importing star is bad practice.
[00:15:59.680 --> 00:16:03.760]   But actually Jeremy says that in a Jupyter notebook where we want to do lots of experiments
[00:16:03.760 --> 00:16:09.280]   is actually pretty handy to have lots of functions ready so you can just call them directly.
[00:16:09.280 --> 00:16:16.960]   So the next thing that this does is you'll be surprised at how much just this line of code is
[00:16:16.960 --> 00:16:25.760]   doing. It's actually url.pets is a URL. So it's something that's the pets URL that's where the
[00:16:25.760 --> 00:16:32.160]   data set is based in the internet. What this undatdata function is going to do, it's going
[00:16:32.160 --> 00:16:38.400]   to download that from the internet to your local computer. Then it's basically going to unzip it.
[00:16:38.400 --> 00:16:44.720]   So you're going to have all the images and all your labels as files. And then in there,
[00:16:44.720 --> 00:16:50.880]   there's two folders. One is the images and the second one is annotations. All this does is then
[00:16:50.880 --> 00:16:59.600]   it returns a path to the images folder of that data set. Then we saw we defined a function
[00:16:59.600 --> 00:17:06.480]   which looked like this. This is just so sorry something I should quickly mention is what are
[00:17:06.480 --> 00:17:13.040]   labels and what are predictions. I would believe that so if you when you have a data set, so let's
[00:17:13.040 --> 00:17:20.320]   say I have one million images, then for each image I have a category. So if this is my first image,
[00:17:20.320 --> 00:17:28.160]   then for this category I could say it's cat, cat, dog, cat, dog, cat. These categories are called
[00:17:28.160 --> 00:17:36.320]   labels. And when you make predictions on this data set, basically from your trained model,
[00:17:36.320 --> 00:17:41.760]   they're the predictions. So that's the difference in labels and predictions. Predictions are what
[00:17:41.760 --> 00:17:47.760]   the model is actually predicting. And labels are the true labels, the true categories of
[00:17:47.760 --> 00:17:54.480]   all the images that are there in your data set. So as many images you have, you'll have the same
[00:17:54.480 --> 00:18:02.880]   number of labels. So what this is cat function is doing, because we were creating a dogs versus cat
[00:18:02.880 --> 00:18:08.640]   classifier. So what you need is in your data set, if you have a data set again, as I mentioned,
[00:18:08.640 --> 00:18:15.760]   you have this data set of say 7,000 images, then what you need for each data set of each image
[00:18:15.760 --> 00:18:22.240]   is a label. You need to say that is this a cat, yes or no? Is this a cat, yes or no? So if it's
[00:18:22.240 --> 00:18:29.280]   a no, it's basically a dog. It's just a way of saying yes cat, no cat, yes, no. So this becomes
[00:18:29.280 --> 00:18:38.240]   your labels. And this is what the model will use to learn. And one way to just convert, one way to
[00:18:38.240 --> 00:18:43.440]   get these labels is just using this function. So this function is just a way to create labels.
[00:18:45.200 --> 00:18:54.560]   Cool. The majority of things happen in this line of code. It's something that starts with
[00:18:54.560 --> 00:18:59.920]   called something called image data loaders. When you're doing, you'll see in Fast.ai,
[00:18:59.920 --> 00:19:05.520]   it's really convenient. And you will see that when you start with Fast.ai, if you're working on
[00:19:05.520 --> 00:19:09.680]   images, you'll start with the word image, image data loaders. When you're working with text,
[00:19:09.680 --> 00:19:16.240]   it will be called text data loaders. And then a lot of things get passed to this function. You
[00:19:16.240 --> 00:19:24.480]   get passed a path, this get image files function as well, and then a valid person. So we'll go now
[00:19:24.480 --> 00:19:30.080]   and just have a look at what each of these mean. So the first thing we passed is this
[00:19:30.080 --> 00:19:36.720]   label func equals cat. That's just, as I've already mentioned, is cat is just a way of telling
[00:19:36.720 --> 00:19:44.160]   whether the thing is a cat or a dog. So this label function is just used to label your dataset,
[00:19:44.160 --> 00:19:49.360]   because otherwise all you have is your dataset and you don't have any labels, but you need labels
[00:19:49.360 --> 00:19:55.920]   to learn the model, for the model to learn. The next thing you'll see is we're passing something
[00:19:55.920 --> 00:20:04.320]   called item transforms equals resize two to four. In Fast.ai, you basically have two types of
[00:20:04.320 --> 00:20:09.360]   transforms. You have something called an item transform and you have something called a batch
[00:20:09.360 --> 00:20:17.840]   transform. So what's the difference? Like what is a batch and what is an item? So let's say I have
[00:20:17.840 --> 00:20:24.720]   my dataset of 1000 images. So these are all from one to thousand. So this is my first image.
[00:20:24.720 --> 00:20:32.560]   This is my thousand image. Then the item transforms are what gets applied to each of these images.
[00:20:32.560 --> 00:20:40.320]   So if I apply to each of these images what these items are, they are actually different
[00:20:40.320 --> 00:20:44.800]   from the first items because they have been transformed.
[00:20:44.800 --> 00:20:53.600]   And specifically what the resize two to four is doing, if my input images were of different sizes,
[00:20:53.600 --> 00:21:02.160]   say they were 256 by 256 pixels or they were say 512 by 512, then applying a resize transform
[00:21:02.160 --> 00:21:11.520]   on each of the items is going to make sure that my transformed dataset is of two to four by two
[00:21:11.520 --> 00:21:17.120]   to four size. So that's the first thing. That's what these item transforms are and they get
[00:21:17.120 --> 00:21:24.000]   applied to pretty much every item in your dataset. The next thing that you do is you have these
[00:21:24.000 --> 00:21:29.600]   thousand transformed items. So I'm just going to call it transformed items here. So you have
[00:21:29.600 --> 00:21:34.880]   these thousand transformed items. The next thing that you do is you start grouping them in batches
[00:21:34.880 --> 00:21:41.760]   of 10. So my first group has 10 images, my second group has 10 and my last group has 10. So if you
[00:21:41.760 --> 00:21:48.720]   have thousand items and each group has 10, you're going to end up with 100 batches. And I'm just
[00:21:48.720 --> 00:21:53.840]   telling you what exactly happens in deep learning. This is the exact process of how you go from
[00:21:53.840 --> 00:22:00.960]   having a dataset to then training a model. The next thing that happens is then you pass each of
[00:22:00.960 --> 00:22:08.480]   these batches one by one to a GPU. And this is where the model will actually start training. A
[00:22:08.480 --> 00:22:14.080]   GPU is a graphic processing unit. It's very similar to a CPU, but much faster, much more
[00:22:14.080 --> 00:22:22.960]   performant. And this process of once each of the datasets, so you can see how each batch contains
[00:22:22.960 --> 00:22:32.400]   different images. Once all of the images or all of the batches have been passed to this GPU once,
[00:22:32.400 --> 00:22:39.440]   it's called one epoch. And this process of the model actually learning is called model fitting.
[00:22:39.440 --> 00:22:45.920]   I'll take two quick questions if anybody has two quick questions
[00:22:45.920 --> 00:22:51.520]   on what we've learned so far. And we're going to take questions on
[00:22:51.520 --> 00:23:07.760]   1db.me/fastbook2. Okay.
[00:23:18.080 --> 00:23:20.880]   So there's no specific questions related to item transform. So
[00:23:20.880 --> 00:23:28.320]   that's fine. I did see a quick question. Can you explain about what an epoch is again?
[00:23:28.320 --> 00:23:33.680]   That's okay. If you don't understand what epoch is, it is fine. But it's basically just
[00:23:33.680 --> 00:23:40.000]   when your model has seen all of your dataset once, it's called one epoch. When it has seen
[00:23:40.000 --> 00:23:45.200]   all of your dataset twice, it's called two epochs. So basically, when you do a full pass
[00:23:45.200 --> 00:23:51.360]   of your whole dataset to your GPU, or sorry, to your model, that's just called an epoch.
[00:23:51.360 --> 00:23:59.920]   Excellent. So then this is just information about the dataset in the book. And this is when you'll
[00:23:59.920 --> 00:24:06.400]   read, you'll see this, that it has, it's the pets dataset. It has 7,400 pictures. And the
[00:24:06.400 --> 00:24:12.560]   picture names are something like this, Great Pyrenees 173.jpg, which just means that it's
[00:24:12.560 --> 00:24:21.600]   the 173rd image of the Great Pyrenees breed dog in the dataset. This is important. So this is
[00:24:21.600 --> 00:24:27.440]   dataset information that we use to label our dataset. The file names start with an uppercase
[00:24:27.440 --> 00:24:35.840]   letter if the image is cat and a lowercase if the image is dog. So if the image name starts with a
[00:24:35.840 --> 00:24:41.440]   capital letter, it's a cat. And if it starts with a lower letter, it's a dog. So that's why
[00:24:41.840 --> 00:24:48.800]   if you can see, we had something like this, isUpper. isUpper basically is just a Python
[00:24:48.800 --> 00:24:52.640]   function that says, is the first letter capital or is the first letter small?
[00:24:52.640 --> 00:25:02.480]   So in that way, you can actually classify just based on the image, just based on the
[00:25:02.480 --> 00:25:06.720]   file names, you can classify them as cats and dogs.
[00:25:09.920 --> 00:25:16.400]   This brings us then to the most important part of this whole exercise, which one of the key
[00:25:16.400 --> 00:25:23.680]   parameters that we passed was this one here, validPercentage = 0.2. So what is this validPercentage
[00:25:23.680 --> 00:25:29.600]   = 0.2? This is something that's really, really important. And I do have a small image to explain
[00:25:29.600 --> 00:25:38.480]   this. I told you that when you have your items, so this is my first item. And let's say this is my
[00:25:38.480 --> 00:25:45.040]   thousandth item. Then what you do is if your validPercentage is 0.2, that just means
[00:25:45.040 --> 00:25:53.680]   you split your data set. 80% of it becomes your training set and 20% of it becomes your
[00:25:53.680 --> 00:25:59.760]   validation set. What's the difference in the training and the validation set? This is something
[00:25:59.760 --> 00:26:08.400]   that's really, really important. The model learns from the training set and the model is tested
[00:26:08.640 --> 00:26:16.400]   on the validation set. What this means is the training set is what the model is actually seeing.
[00:26:16.400 --> 00:26:22.400]   When I say what does it mean that the model is actually seeing, it just means that there's going
[00:26:22.400 --> 00:26:32.960]   to be 800 images. If the cats and dogs had a thousand images, then these 800 images are what
[00:26:32.960 --> 00:26:40.800]   the model sees and these 800 images are what the model learns from. But then you have your 200
[00:26:40.800 --> 00:26:49.920]   images of cats and dogs again. So then what you do is you make the model learn from the training set
[00:26:49.920 --> 00:26:58.000]   and then you take these 200 images and you predict the, you make the model say, okay,
[00:26:58.000 --> 00:27:04.480]   here are 200 new images that you haven't seen so far. Can you make predictions? Can you say
[00:27:04.480 --> 00:27:10.480]   if all of these 200 images are cats or dogs? So from 200 images, it will say, okay, 150 of these
[00:27:10.480 --> 00:27:19.120]   are cats and these 50 are dogs. But then based on those predictions, you can actually check how
[00:27:19.120 --> 00:27:24.080]   accurate your model is. You can actually check the performance of the model. So we always want
[00:27:24.080 --> 00:27:29.120]   to check the performance of the model on a validation set. It's also called a holdout set
[00:27:29.120 --> 00:27:36.640]   because you pretty much held out 20% of the data set straight away. So that's the key difference.
[00:27:36.640 --> 00:27:46.000]   And I'm just going to quickly touch upon something which is called overfitting. So you might ask,
[00:27:46.000 --> 00:27:51.840]   okay, why do we need a validation set? Like what's the purpose of it? So the basic and the most
[00:27:51.840 --> 00:28:00.880]   easy example is this one. So let's say you have a list of points on your 2D graph, right? If you
[00:28:00.880 --> 00:28:10.480]   don't have a validation set or basically fitting is a way of checking like if the model just picks
[00:28:10.480 --> 00:28:19.440]   the patterns of each of these points like this, because imagine in real world, what you're going
[00:28:19.440 --> 00:28:25.760]   to do is you have a trained model. So your model is going to be trained and then you're going to
[00:28:25.760 --> 00:28:31.440]   take it up in a real world. You're going to take it up and you're going to ask it to make predictions
[00:28:31.440 --> 00:28:41.520]   on other data, right? That's how things are going to be in the real world. So say if you're a
[00:28:41.520 --> 00:28:46.640]   medical company, what you're going to do is you're going to give the model, say 1 million x-ray
[00:28:46.640 --> 00:28:53.600]   images. And then in the real world, when this model, this x-ray model that can tell if there's
[00:28:53.600 --> 00:28:59.440]   something in an x-ray or not, when it gets applied or it gets used in a hospital, then the x-ray
[00:28:59.440 --> 00:29:04.560]   images that it's going to see are going to be completely new. So what we want is that we want
[00:29:04.560 --> 00:29:10.240]   the model to learn the patterns. We want the model to learn these patterns and we want the model to
[00:29:10.240 --> 00:29:20.080]   learn the general patterns from this training set. But we don't want it to memorize the training set.
[00:29:20.080 --> 00:29:26.880]   By memorize the training set is, I just mean that if you give the model the same images
[00:29:26.880 --> 00:29:33.120]   10,000 times, then it's not going to be able to do a good job on making a prediction on a different
[00:29:33.120 --> 00:29:38.720]   image. So that's the difference. We need the model to generalize. And when a model that looks like
[00:29:38.720 --> 00:29:45.120]   this, when it has pretty much memorized your data set, so this is called memorizing the data set.
[00:29:45.120 --> 00:29:52.000]   I'm just going to say memory. But this is where the model has memorized the data set. And this
[00:29:52.000 --> 00:29:58.640]   is what a proper fit looks like because it's just learned the general patterns. So that's just
[00:29:58.640 --> 00:30:03.440]   a difference of using a proper fit and an overfitting.
[00:30:04.240 --> 00:30:10.000]   Finally, what we did, and we will have questions after. I'll give some time for quick questions
[00:30:10.000 --> 00:30:16.640]   after this. Finally, what we did, this is the fifth line of the code that we saw in those six lines.
[00:30:16.640 --> 00:30:22.880]   We created something called a CNN learner. So what's a CNN learner? A CNN learner is a
[00:30:22.880 --> 00:30:28.480]   convolutional neural network. In this book, it says the convolutional neural networks are state
[00:30:28.480 --> 00:30:35.840]   of art today. But this book was written in 2020. What's happening in 2021 is that there's new
[00:30:35.840 --> 00:30:39.840]   architectures, there's new models that are catching up to convolutional neural networks.
[00:30:39.840 --> 00:30:47.040]   But for your understanding, convolutional neural network is just a way. It's just-- so you know how
[00:30:47.040 --> 00:30:53.440]   we had a model that we defined like a black box. A CNN, or a convolutional neural network, is just
[00:30:53.440 --> 00:31:00.720]   a type of model that can actually learn things from your data set. So it can be used-- it's widely
[00:31:00.720 --> 00:31:06.640]   used in images. It's widely used. It's also sometimes used in text, but widely and everywhere
[00:31:06.640 --> 00:31:12.000]   in image classification or image-related tasks, you will see a CNN being applied.
[00:31:12.000 --> 00:31:20.160]   So the next thing we did was we picked something called a ResNet-34. And this is a model that
[00:31:20.160 --> 00:31:26.720]   was-- we picked something called a ResNet-34. It's just a particular type of a convolutional
[00:31:26.720 --> 00:31:32.480]   neural network. So what ResNet is, ResNet is just a type of a model architecture.
[00:31:32.480 --> 00:31:40.320]   And the number 34 in ResNet-34 tells you that that's how many layers there are in this
[00:31:40.320 --> 00:31:45.920]   model architecture. It's completely OK if you don't know what ResNet is, if you don't understand
[00:31:45.920 --> 00:31:51.280]   what convolutional neural network is. As we go into the next chapters week by week, everything
[00:31:51.280 --> 00:31:56.880]   will make sense. And in fact, by the-- I think it's the 18th or the 19th week, we will create
[00:31:56.880 --> 00:32:03.040]   the ResNet architecture from scratch. So basically, we will know exactly then the nuts and bolts of
[00:32:03.040 --> 00:32:08.880]   what ResNet is. But because you're seeing this the first time, you're hearing the word ResNet
[00:32:08.880 --> 00:32:13.440]   the first time, it's just an architecture that works for image classification. It's just a
[00:32:13.440 --> 00:32:18.880]   mathematical function that can predict or classify cats from dogs very easily.
[00:32:18.880 --> 00:32:26.880]   So the number 34 is just the number of layers or how big this network is. It basically means
[00:32:26.880 --> 00:32:32.800]   that ResNet-18 is smaller than ResNet-34, and then ResNet-152 is one of the biggest networks.
[00:32:38.880 --> 00:32:44.640]   So the last thing then, what is a metric? So the metric is just a way of checking how good
[00:32:44.640 --> 00:32:49.600]   my model is doing, how the performance is. But something I really want to
[00:32:49.600 --> 00:32:56.080]   highlight here is a difference in a loss and a metric.
[00:32:56.080 --> 00:33:06.960]   So the difference in a loss and a metric is this. A loss is what your model uses to train,
[00:33:06.960 --> 00:33:11.600]   basically. So because we know we were doing back propagation or we were doing SGD,
[00:33:11.600 --> 00:33:19.200]   so loss is something that this stochastic gradient descent or just this process-- I'll go back to this
[00:33:19.200 --> 00:33:30.160]   image at the top. So you have your model architecture, which has some parameters.
[00:33:30.160 --> 00:33:36.800]   This is how it looks like. You get some inputs. This architecture makes some predictions.
[00:33:37.440 --> 00:33:42.160]   Then you calculate the loss based on these predictions. So I think by now you should
[00:33:42.160 --> 00:33:46.960]   already know what predictions and labels are. So based on these predictions and labels,
[00:33:46.960 --> 00:33:54.400]   it calculates the loss, and it updates the parameters again. What this loss does--
[00:33:54.400 --> 00:34:02.000]   this loss is easy for-- so SGD is what does the actual update. I'm just going to call it stochastic
[00:34:02.000 --> 00:34:09.520]   gradient descent. SGD is what is used to update the parameters. And loss is something that's
[00:34:09.520 --> 00:34:16.880]   really helpful for the SGD to know how I need to update the parameters. But then what's a metric?
[00:34:16.880 --> 00:34:23.760]   So what's a metric? So when you have a trained model or while you're training your model,
[00:34:27.760 --> 00:34:31.760]   when it looks like this, you have your inputs, you have your model, and you have your result,
[00:34:31.760 --> 00:34:40.880]   then on the data set-- let's just call this data set-- you have your thousand images,
[00:34:40.880 --> 00:34:47.200]   and you have your thousand labels, then it's going to have thousand predictions. So between the
[00:34:47.200 --> 00:34:53.360]   labels and the predictions, you can actually check the accuracy of how accurate my model is.
[00:34:53.360 --> 00:35:00.320]   And that accuracy is a metric. It's not being used by the model to actually do the learning,
[00:35:00.320 --> 00:35:05.120]   but it's good for us as humans when we're training these deep learning models. It's good for us to
[00:35:05.120 --> 00:35:12.160]   know, OK, I know that my model by the fifth epoch has 50% accuracy in classifying cats versus dogs,
[00:35:12.160 --> 00:35:18.240]   and I know that the model by the 10th epoch has 100% accuracy or something like 95% accuracy.
[00:35:18.240 --> 00:35:23.200]   But that's just a number that's good for us for reporting purposes. It's not helping the model
[00:35:23.200 --> 00:35:28.400]   train at all. What's actually helping the model train is the loss function that tells the model
[00:35:28.400 --> 00:35:33.920]   how to update the weights. And that loss function is, in general, in an image classification,
[00:35:33.920 --> 00:35:38.640]   is a cross-entropy loss. So that's a key difference. It's OK if you don't get the most
[00:35:38.640 --> 00:35:44.480]   of it. But as long as you get a general idea of the difference between a loss and a metric,
[00:35:44.480 --> 00:35:51.920]   that's good for me. And this is the last thing that I'll touch upon before we open the thing
[00:35:51.920 --> 00:35:57.040]   for-- before I open it, before I let you guys ask questions that you might have so far.
[00:35:57.040 --> 00:36:04.000]   But something that you do, you would have-- if you've read the book so far, something that you
[00:36:04.000 --> 00:36:10.400]   would have seen is or heard in the wild is something called a pre-trained model. So what
[00:36:10.400 --> 00:36:21.040]   is a pre-trained model? If I'm going with a ResNet-34, it would have 34 layers. Then
[00:36:21.040 --> 00:36:28.320]   I basically have one massive data set. Let's just call it 1 million images.
[00:36:28.320 --> 00:36:32.240]   This massive data set is called ImageNet.
[00:36:35.440 --> 00:36:43.200]   So what the model does is it actually first learns from this ImageNet data set. And we will see how
[00:36:43.200 --> 00:36:49.680]   this is important and why this is important. But the model basically learns from these 1 million
[00:36:49.680 --> 00:36:56.080]   images first. So this is what a trained model looks like. It has some weights. And it's learned
[00:36:56.080 --> 00:37:01.440]   on these 1 million images called ImageNet. Then something that we do is we remove the final layer.
[00:37:01.440 --> 00:37:06.960]   So this thing is gone. And we replace it with something called a head. So it's just a new head.
[00:37:06.960 --> 00:37:13.040]   Because when you're working on your cats versus dogs, ImageNet has 1,000 categories.
[00:37:13.040 --> 00:37:19.760]   By 1,000 categories, I mean some images are that of a fish, some images are that of a human,
[00:37:19.760 --> 00:37:26.640]   some images could be a dog, could be a piano, could be all of these different 1,000 categories.
[00:37:26.640 --> 00:37:32.640]   But because what we want to do now is instead of predicting on the 1,000 categories, we just want
[00:37:32.640 --> 00:37:41.120]   to predict on two categories. We just update this final layer to do this. But we keep the same
[00:37:41.120 --> 00:37:47.920]   weights from the ImageNet to here. So this part of the architecture is actually the same as before.
[00:37:49.200 --> 00:37:58.560]   And this process of first training a model on a general, really large dataset like ImageNet,
[00:37:58.560 --> 00:38:05.440]   and then fine tuning it, or then basically training it on your small dataset that you care about,
[00:38:05.440 --> 00:38:13.360]   is called fine tuning. And it's specifically called as using the pre-trained weights.
[00:38:13.360 --> 00:38:18.960]   So it's basically before you do the actual training of the cats versus dogs,
[00:38:18.960 --> 00:38:24.560]   you're doing a pre-training. So just before you're doing a pre-training on ImageNet. And this is
[00:38:24.560 --> 00:38:31.440]   called as using the pre-trained weights. When you use the same weights for the new model, it's just
[00:38:31.440 --> 00:38:39.360]   called using as a pre-trained weights. So that's what you will see is called a pre-trained model.
[00:38:39.360 --> 00:38:47.920]   And then what is this process called fine tuning? Just this process of like not actually training
[00:38:47.920 --> 00:38:52.160]   any of this-- like we're not actually updating this part of the model. We're just using the
[00:38:52.160 --> 00:38:58.320]   same weights. But we actually train just this new part of the model. This is what is called as
[00:38:58.320 --> 00:39:04.720]   fine tuning, when you keep the stem or you keep the bulk of the model as the same.
[00:39:04.720 --> 00:39:17.760]   All right. That's it. That's the whole piece of code. And that's exactly how those six lines of
[00:39:17.760 --> 00:39:22.800]   code work. And that's all the deep learning jargon that you need to know right now. So
[00:39:22.800 --> 00:39:30.240]   let's spend five minutes on questions that are related specifically to these things.
[00:39:30.240 --> 00:39:32.480]   And I'll spend five minutes trying to answer them.
[00:39:46.480 --> 00:39:52.560]   OK. Thanks. That's a really good question. So the question is, it seems like fine tuning trains
[00:39:52.560 --> 00:39:58.480]   the whole part of the network. But I've also heard as this words of freezing layers or these things.
[00:39:58.480 --> 00:40:08.960]   What's the difference? So what fine tuning in fast.ai does is it does two passes, one and two.
[00:40:15.120 --> 00:40:21.520]   In the first pass, it takes this model where just the head is new. So just this head is new.
[00:40:21.520 --> 00:40:30.400]   It takes this model as is. And it trains just the head. It does not train. So this is the head.
[00:40:30.400 --> 00:40:37.200]   This is the stem. It does not train the stem. It trains the head. So this is the first pass
[00:40:37.200 --> 00:40:44.000]   when you call fine tuning. This is just the first pass of what happens in fast.ai. And this process
[00:40:44.000 --> 00:40:54.080]   of not training the earlier layers of your model is called as freezing the earlier layers.
[00:40:54.080 --> 00:41:02.800]   But you're actually training the head. OK, so that's the first pass. In the second pass,
[00:41:02.800 --> 00:41:10.560]   then what happens is now you have, I'm going to call it a trained head. So I'm just going to call
[00:41:10.560 --> 00:41:17.440]   because the stem is as it is, but your head is trained. The next thing that you do is you
[00:41:17.440 --> 00:41:24.000]   actually train both of them. So I'm just going to call it train yes and train yes to both of them.
[00:41:24.000 --> 00:41:34.720]   But you train this side of the head a little bit faster than the stem. And we'll see why
[00:41:35.360 --> 00:41:40.800]   that is. But this is how fine tuning works in fast.ai.
[00:41:40.800 --> 00:41:48.400]   So that's the question. So I hope that answers. It seems like in fine tuning used for cats versus
[00:41:48.400 --> 00:41:53.200]   in chapter one retrains all the layers. Yes, it's the second part of fine tuning that does this.
[00:41:53.200 --> 00:41:57.120]   But I've also heard the idea of freezing some layers. And now you know what freezing is. And
[00:41:57.120 --> 00:42:03.040]   I wasn't able to find parameters that does so. It's just fine tune. So what does learn-- what
[00:42:03.040 --> 00:42:10.800]   does one signifies in learn.fine_tune? It just means that you have one epoch. So you do this
[00:42:10.800 --> 00:42:17.360]   once and you also do this once. But it's actually like two separate things that you do once. But if
[00:42:17.360 --> 00:42:25.840]   your epoch number was, say, five, then you repeat this step five times. Repeat this step. But not--
[00:42:25.840 --> 00:42:31.360]   like this one is only in the first one. So this doesn't get carried on to the next ones. But
[00:42:31.360 --> 00:42:38.160]   you're training your model five times. And the reason why you need to have higher number of
[00:42:38.160 --> 00:42:44.720]   epochs is because the model in just one epoch can't really understand what exactly your patterns in
[00:42:44.720 --> 00:43:05.600]   your data set are. That's been answered. This has been answered. All right. So that's the basics
[00:43:05.600 --> 00:43:11.920]   that we need to know about in fine tuning. And that's the basics that we need to know about what
[00:43:11.920 --> 00:43:17.600]   pre-training is and how this works. So the next thing that we need to touch upon is
[00:43:17.600 --> 00:43:23.440]   this idea of what the model actually learns.
[00:43:23.440 --> 00:43:35.760]   So I told you that a model has-- looks like this. Like this is the final layer. It's called a head.
[00:43:36.400 --> 00:43:42.080]   ResNet-34 has something like 34 layers, which you can say break into groups or-- let's just say,
[00:43:42.080 --> 00:43:48.400]   I mean, it's still 34 layers. But for simplicity, let me say I'm having a very small model. Instead
[00:43:48.400 --> 00:43:53.920]   of 34 layers, I just have like six layers. Let's say I have six layers. So this is first, two,
[00:43:53.920 --> 00:44:01.920]   three, four, five, and then six. What's really interesting is that a deep learning model,
[00:44:01.920 --> 00:44:10.560]   each layer learns different things about your data set. So the first layer-- this is my first layer--
[00:44:10.560 --> 00:44:16.480]   the first layer is actually just learning how to tell edges. It's actually just learning like,
[00:44:16.480 --> 00:44:21.200]   OK, this is how a diagonal looks like. This is how a diagonal looks like. It's just learning
[00:44:21.200 --> 00:44:28.240]   how to tell edges. It's just learning how to tell diagonals. That's just the first layer.
[00:44:28.240 --> 00:44:32.400]   But the second layer builds on top, and it starts to learn patterns like circles.
[00:44:32.400 --> 00:44:38.080]   It starts to learn patterns like these, kind of like top left edges, because it's now building
[00:44:38.080 --> 00:44:42.720]   on top of what the first layer learned. So the first layer already knows about these edges. It
[00:44:42.720 --> 00:44:48.080]   already knows about these things. And the second layer builds on top. And based on that, based on
[00:44:48.080 --> 00:44:53.760]   these patterns, it can actually learn things like-- so for example, based on this pattern,
[00:44:53.760 --> 00:45:00.720]   you can see how it's able to tell sort of like windows. So by the second layer, the model is
[00:45:00.720 --> 00:45:05.520]   able to tell all these small different objects of patterns. It's able to tell this pattern.
[00:45:05.520 --> 00:45:10.080]   So it's learning some patterns that are built on top of what it learned in the first layer.
[00:45:10.080 --> 00:45:16.880]   Then it combines these patterns in the third layer. And you can see how by now, by this time,
[00:45:16.880 --> 00:45:23.120]   it's not only able to tell circles, but it's also able to tell circles and edges together. So it's
[00:45:23.120 --> 00:45:30.640]   combined them. It's able to tell more complex patterns like over here. It's starting to tell
[00:45:30.640 --> 00:45:36.560]   human faces. It's starting to learn things about like bees. It's starting to learn things about
[00:45:36.560 --> 00:45:42.800]   what fur are in my cats and dogs. So it's basically-- slowly and steadily, the model
[00:45:42.800 --> 00:45:46.960]   by the third layer is learning more than what it has in the first and the second layer.
[00:45:48.960 --> 00:45:55.200]   By the fourth layer, again, it's learning more complex patterns and proper images of dogs,
[00:45:55.200 --> 00:46:02.720]   proper images of basically birds or animals. It's learning proper images of-- so not only now does
[00:46:02.720 --> 00:46:08.000]   it know spheres and edges, it now can tell cameras. And by the last layer, it can actually
[00:46:08.000 --> 00:46:15.440]   do the task that you wanted to do. So this was the Zeiler and Fergus paper that kind of
[00:46:16.080 --> 00:46:22.800]   visualized how these different parts of the model learn. And something I want to stress upon is,
[00:46:22.800 --> 00:46:27.200]   you know, when I said pre-trained weights-- so when I said pre-trained weights,
[00:46:27.200 --> 00:46:36.880]   what we actually do is we remove the last layer. So the last layer, if it was trained on ImageNet
[00:46:36.880 --> 00:46:47.280]   or like 1 million images, this last layer is what is learning all the patterns. But what I want to
[00:46:47.280 --> 00:46:56.800]   do is now instead of doing this, I want to learn cats and dogs. But for learning cats and dogs,
[00:46:56.800 --> 00:47:01.600]   things like learning about what fur are or things like learning what edges are,
[00:47:01.600 --> 00:47:07.120]   are still going to be important, right? So these weights or these things that the model has
[00:47:07.120 --> 00:47:13.360]   learned in the earlier layers is still important. So this is still important. And you're not
[00:47:13.360 --> 00:47:17.840]   starting-- you're not starting from scratch when you're training on your cats and dogs.
[00:47:17.840 --> 00:47:22.640]   By now, your model already knows things like, OK, what are edges? What are spheres? What are
[00:47:22.640 --> 00:47:31.040]   common patterns? And that really improves the training. And it really helps the model
[00:47:31.040 --> 00:47:35.920]   have significant performance. Like without pre-training, without-- this process is called
[00:47:35.920 --> 00:47:40.240]   reusing weights, and it's called pre-training. Without pre-training, it's really, really
[00:47:40.240 --> 00:47:45.680]   difficult for the models to converge. And it takes much longer. It takes lots of more compute.
[00:47:45.680 --> 00:47:52.560]   So this is really the key idea in computer vision is that if you can reuse the weights,
[00:47:54.000 --> 00:48:03.920]   this is going to help. And then you can see how it's-- you don't really-- the last thing
[00:48:03.920 --> 00:48:09.120]   I'll touch upon in this chapter is that you don't really need to train the first part,
[00:48:09.120 --> 00:48:13.680]   which can just learn these edges. You don't really need to retrain the model, right?
[00:48:13.680 --> 00:48:18.960]   So if you can keep these weights as is, but make the model learn the later edges of the part
[00:48:19.520 --> 00:48:24.960]   faster, then that's just going to help your model in general. Because we don't really
[00:48:24.960 --> 00:48:34.480]   want the model to now reset if the learning rate in the early bits of the model is really high.
[00:48:34.480 --> 00:48:38.720]   I mentioned learning rate. I shouldn't have said learning rate. But you'll see what learning rate
[00:48:38.720 --> 00:48:46.880]   is in the coming weeks. So then this is the last part of the whole chapter, is that you can't
[00:48:46.880 --> 00:48:51.920]   really-- it's not just-- deep learning is not just for image classification. But you can do things
[00:48:51.920 --> 00:48:58.240]   like spectrogram classification, time series. This was one where it was used for fraud detection.
[00:48:58.240 --> 00:49:03.120]   So you can actually convert all these different domains to images. And then you can train an
[00:49:03.120 --> 00:49:13.040]   image classifier on top that can actually do the classification. So I guess the idea is that
[00:49:14.240 --> 00:49:21.520]   image recognizers can handle non-image tasks. Because if you can convert music spectrograms,
[00:49:21.520 --> 00:49:26.640]   so if you have a spectrogram and you can plot it on an image, and then you train an image classifier
[00:49:26.640 --> 00:49:32.480]   on top, then you're actually able to tell spectrograms. So you're able to separate these
[00:49:32.480 --> 00:49:39.920]   audios. You can tell how gunshot is different from engine or basically from street music.
[00:49:39.920 --> 00:49:46.640]   But in essence, you don't have to learn anything new if you know how to do image classification.
[00:49:46.640 --> 00:49:51.920]   So this is something I really want you to think about. If you're in a domain, if you're in an
[00:49:51.920 --> 00:49:58.960]   industry where you can convert your data set to images, then trust me, training an image classifier
[00:49:58.960 --> 00:50:04.640]   on top will really give you good results. So have a try. If you're in time series, then
[00:50:05.360 --> 00:50:13.520]   try plotting that time series as this person did. And it's something called a Gramian angular
[00:50:13.520 --> 00:50:17.440]   difference field. I have no idea what that is because I'm not from the domain. But apparently,
[00:50:17.440 --> 00:50:25.920]   you can plot time series as images. And then if you train your models on these different images,
[00:50:25.920 --> 00:50:29.680]   then it's really helpful and can actually tell different time series patterns.
[00:50:31.440 --> 00:50:39.600]   And then the last part of this whole chapter-- so the question then also is, how is deep learning
[00:50:39.600 --> 00:50:45.680]   different from machine learning? Every concept that we've learned so far is applicable to machine
[00:50:45.680 --> 00:50:51.280]   learning as well. But what makes deep learning distinctive is that it's just a particular type
[00:50:51.280 --> 00:50:56.640]   of architectures. So these ResNets, these ResNet-34, ResNet-50, these are just particular
[00:50:56.640 --> 00:51:01.200]   architectures that are common to deep learning and are different from machine learning. But in
[00:51:01.200 --> 00:51:05.280]   machine learning, you have things like XGBoost, Random Forest, and others.
[00:51:05.280 --> 00:51:13.040]   Finally, I'm going to skip through this part quickly because it's very much OK if you just
[00:51:13.040 --> 00:51:17.360]   go back and read through it. But this is just an introduction that deep learning is not just
[00:51:17.360 --> 00:51:23.040]   for image classification because so far, we've only seen examples of image classification.
[00:51:23.040 --> 00:51:29.040]   But you can do things like-- in an image, you can actually tell, OK, these are the exact pixels of
[00:51:29.040 --> 00:51:34.160]   the road. These are the exact pixels of a footpath. These are the exact pixels on the left
[00:51:34.160 --> 00:51:40.480]   of buildings. These are the exact pixels of sky and so on. So you can actually tell pixel by pixel
[00:51:40.480 --> 00:51:47.760]   in an image, which is really helpful in autonomous vehicles. So that's called image segmentation.
[00:51:47.760 --> 00:51:53.200]   You can also tell sentiment reviews or movie reviews, which is the most common example.
[00:51:53.200 --> 00:51:58.560]   So if you say-- if you give the deep learning model something like, I really like that movie,
[00:51:58.560 --> 00:52:00.720]   it can tell you if the review is positive or not.
[00:52:00.720 --> 00:52:11.600]   And then in Python, basically, when you're using Fast.ai, if there's any function that you have
[00:52:11.600 --> 00:52:18.000]   a question about, something that you can actually do is you just write the word doc before that
[00:52:18.000 --> 00:52:22.560]   actual function. And it will open up something like this, which will actually tell you what it
[00:52:22.560 --> 00:52:29.440]   does. You can click on Show in Docs, which will take you to the Fast.ai documentation. So let me
[00:52:29.440 --> 00:52:33.840]   see if I can bring this up quickly on Jupyter.
[00:52:33.840 --> 00:53:01.840]   [VIDEO PLAYBACK]
[00:53:01.840 --> 00:53:08.080]   - So see how that brings the documentation up for Untar data? And I can now click on Show in Docs.
[00:53:08.080 --> 00:53:16.080]   That will take me to the Fast.ai documentation. It has plenty of examples on what it does.
[00:53:16.080 --> 00:53:23.760]   And I can click on the source here, which will take me to the source code of basically any Fast.ai
[00:53:23.760 --> 00:53:28.400]   function. So you can use this for any Fast.ai function. I just wanted to quickly highlight
[00:53:28.400 --> 00:53:32.720]   that. The last thing you would be surprised to know if you're new to deep learning is that
[00:53:32.720 --> 00:53:38.000]   it's also possible to build models on tabular data. So if you're from a finance background,
[00:53:38.000 --> 00:53:41.920]   if you're from a medical background, and you have all these tabular, and you want to tell--
[00:53:41.920 --> 00:53:49.680]   basically, as an example, this example uses adults data set, which tells if an adult is in a higher
[00:53:49.680 --> 00:53:57.520]   earning-- if he's earning high or less based on all these other factors like age, occupation,
[00:53:58.080 --> 00:54:03.200]   and that's just a tabular standard data. And you can actually now train models on tabular data
[00:54:03.200 --> 00:54:11.440]   as well. That's the most of it. I did want to touch base on validation and test sets as well.
[00:54:11.440 --> 00:54:21.520]   So far, I've told you that validation and test set is when you have your whole data set,
[00:54:21.520 --> 00:54:28.560]   then you just keep 20% of your data as separate. You train the model here, and you validate the
[00:54:28.560 --> 00:54:35.680]   model here. But the thing is, how do you split? How do you find this 20% data set? So if you have--
[00:54:35.680 --> 00:54:42.240]   this is a key example to think about and to know. And this is, again, something that's applicable
[00:54:42.240 --> 00:54:47.920]   to machine learning as well. If you have a graph that looks like this, where you have date and
[00:54:47.920 --> 00:54:53.920]   sales, then something that's really common practice is called a random split. So what you do is you
[00:54:53.920 --> 00:55:03.520]   just take 20% of the data from anywhere, and you put it in the validation set. And it's called a
[00:55:03.520 --> 00:55:10.000]   randomly picking up this data. But that's actually going to give you wrong results. Because when
[00:55:10.000 --> 00:55:14.960]   you're trying to predict sales, it's going to give you-- when you look at the accuracy, or when you
[00:55:14.960 --> 00:55:20.720]   look at the metrics, or the performance of the model, you're going to have the wrong performance
[00:55:20.720 --> 00:55:25.920]   expectation. Because when you take this model, and you put it in a real world, then what you're
[00:55:25.920 --> 00:55:32.560]   going to do-- in a real world, you're going to try and predict sales for a future date. When you have
[00:55:32.560 --> 00:55:36.240]   a trained model, you're going to put it in production. Say you put it on production on the
[00:55:36.240 --> 00:55:41.840]   1st of January, then what you really want to do is you want to make the prediction be done for the
[00:55:41.840 --> 00:55:46.880]   2nd of January, for the 3rd of January. So what's really helpful, a better way of creating a
[00:55:46.880 --> 00:55:56.160]   validation set, is that you keep divided by timeline. So this is a much more real case
[00:55:56.160 --> 00:56:04.240]   scenario of having a proper validation set of 20%. Because if the model can actually perform good
[00:56:04.240 --> 00:56:09.600]   here, then you have much more confidence that this model can actually work really well in a real
[00:56:09.600 --> 00:56:18.400]   world use case. Lastly, this is the end of the chapter. This is where you choose your own
[00:56:18.400 --> 00:56:24.880]   adventure. So this is where-- find projects that are of interest to you, whether it is Tableau,
[00:56:24.880 --> 00:56:31.520]   whether it is Computer Vision, whether it is NLP. But find things that are of interest to you, and
[00:56:31.520 --> 00:56:35.520]   pick your own adventure. Because we want to be able to-- you want to be able to go back, and you
[00:56:35.520 --> 00:56:41.600]   want to be able to run each of these lines of code, and you want to be able to basically understand
[00:56:41.600 --> 00:56:48.240]   from a high level here on what's happening. And Sayyam has already shared the share your V2
[00:56:48.240 --> 00:56:57.520]   projects on forums.fastai. So if you go-- oh, sorry. I started that back in March 20. Anyway,
[00:56:57.520 --> 00:57:04.640]   the idea is then on these forums, you'll see a lot of people will be sharing all in all of their
[00:57:05.520 --> 00:57:12.880]   ideas, their own projects. So Radik has his own ideas, and he's working on audio classification.
[00:57:12.880 --> 00:57:18.880]   So if you are out of inspiration, then definitely go to this page or this forum post here, and
[00:57:18.880 --> 00:57:23.280]   pick your own basically project that you want to work on.
[00:57:23.280 --> 00:57:32.160]   And lastly, every chapter comes with a questionnaire. It's really, really important
[00:57:32.160 --> 00:57:37.920]   that you then-- everything that you've learned, you stop at the questionnaire. And you go to the
[00:57:37.920 --> 00:57:44.560]   questionnaire, and you go to these 33 questions. And you need to be at a point where you're able
[00:57:44.560 --> 00:57:51.840]   to answer at least 25 of 33. But if you have to reference back, that's OK. But it's just for your
[00:57:51.840 --> 00:57:57.360]   understanding that you're able to answer all these questions. So for example, what does wait
[00:57:57.360 --> 00:58:04.320]   assignment mean? I just picked this up randomly. But this basically means that a model has weights,
[00:58:04.320 --> 00:58:10.080]   right? A model has weights associated with it. So this idea of a model having weights is called as
[00:58:10.080 --> 00:58:14.560]   wait assignment. So I need you to go back. I need you to reread this chapter. I need you to
[00:58:14.560 --> 00:58:22.160]   run the code again, and then see if all of these questions make sense. And with that being said,
[00:58:22.720 --> 00:58:31.120]   we are ready for chapter 2, because we also want to give time to Tanishq to present some of his
[00:58:31.120 --> 00:58:38.880]   work and his journey. So I will take five minutes of questions again.
[00:58:51.360 --> 00:58:57.840]   Oh, hey, Parul. I didn't realize you were attending. There's a blog series on Fastbook.
[00:58:57.840 --> 00:59:05.120]   Oh, yes. I did say I would mention this. This is, again, a wonderful resource. Thanks for sharing,
[00:59:05.120 --> 00:59:11.360]   Parul. This blog series would have basically each chapter as a blog in a simpler language. If you
[00:59:11.360 --> 00:59:17.040]   want to refer to this, it's by Ved Jalim. He's also a Fast.ai person. So you can go back,
[00:59:17.040 --> 00:59:26.400]   and you can have a look at these blogs. OK. Since the categories from pre-trained models
[00:59:26.400 --> 00:59:32.400]   are reduced to two, does that imply-- no, that's an incorrect understanding. So I would recommend
[00:59:32.400 --> 00:59:36.800]   that you go back and have a look at-- Sam's already answered most of them. Oh, thanks, Sam.
[00:59:36.800 --> 00:59:42.080]   Is there any questions, Sam? I'm going to make you-- one second. Let me make you
[00:59:43.440 --> 00:59:52.160]   panelist so then you can talk. Hey, Sam, is there any questions that you feel we should look at?
[00:59:52.160 --> 01:00:06.400]   You should be able to talk now. Sorry, you got promoted and I rejoined. No, I don't think any
[01:00:06.400 --> 01:00:12.800]   question is unanswered. OK. Excellent. And thanks for answering all the questions. Really appreciate
[01:00:12.800 --> 01:00:21.280]   your help. Sure thing. All right. Sam, sorry I took you by surprise and just made you a panelist
[01:00:21.280 --> 01:00:29.920]   right there. But now, as we're towards the later end of the chapter, we're going to spend now just
[01:00:29.920 --> 01:00:35.760]   15 minutes on chapter two production, which is going to help us understand what's coming next.
[01:00:35.760 --> 01:00:41.280]   And then we will have Tanish present on his journey of how he started with Fast.ai and how
[01:00:41.280 --> 01:00:46.560]   he's come to where he has today. And then we'll also have an informal catch up on Slack
[01:00:46.560 --> 01:00:54.720]   just to meet each other. All right. That being said, let's get started with chapter two. So
[01:00:54.720 --> 01:00:59.200]   chapter two is really, really interesting. Chapter one gives you a highlight of everything that's
[01:00:59.200 --> 01:01:04.640]   going on in deep learning world. It gives you an example of how you can use Fast.ai and just six
[01:01:04.640 --> 01:01:10.160]   lines of code to train text classification models, how you can use Fast.ai to train image classification
[01:01:10.160 --> 01:01:16.000]   models. But actually, deep learning and having worked in previous industries as well,
[01:01:16.000 --> 01:01:23.840]   it's more than model training. When you actually want to create a company out of an idea or you
[01:01:23.840 --> 01:01:30.400]   want to put things in production, production basically means when your customers can start
[01:01:30.400 --> 01:01:36.560]   using your product or not just customers, but basically users. So it's in a final stage where
[01:01:36.560 --> 01:01:45.520]   people can start using your product. When you go from having an idea to then
[01:01:45.520 --> 01:01:51.920]   going from this stage to actually being able to serve your model so people can use it, it's
[01:01:51.920 --> 01:01:56.800]   actually more than just model training because you need things like you need data, you need
[01:01:56.800 --> 01:02:03.360]   infrastructure, you need to have really good basically coding practices, you need to have
[01:02:03.360 --> 01:02:09.280]   tests. There's just a lot more that does happen than just model training. And so far in chapter
[01:02:09.280 --> 01:02:16.320]   one, we've just seen model training. So then the next thing is the practice of deep learning.
[01:02:16.320 --> 01:02:22.960]   So in this chapter, we're going to have practice on an end-to-end deep learning problem. Like how
[01:02:22.960 --> 01:02:28.880]   do we get the data set? How do we train the model? How do we serve it in production? How do we create
[01:02:28.880 --> 01:02:34.400]   a user interface on the internet so then people can come and they can upload their own images and
[01:02:34.400 --> 01:02:43.360]   try? And we've seen in first chapter is like six lines of code are something that can be used for
[01:02:43.360 --> 01:02:51.760]   text, images, basically tabular, but deep learning isn't magic. So those six lines of code won't work
[01:02:51.760 --> 01:02:56.960]   for every problem. But the mindset that you want to have is you want to have an open mind. Like you
[01:02:56.960 --> 01:03:01.760]   want to have an open mind and you want to think, okay, maybe deep learning can solve this. And
[01:03:01.760 --> 01:03:08.800]   maybe I should try solving a problem using this particular idea. And maybe I can convert my
[01:03:08.800 --> 01:03:15.920]   tabular data to images. Maybe I can convert my spectrograms to images. And that's the mindset
[01:03:15.920 --> 01:03:19.680]   that you want to have. You want to have a really open mind. And if it doesn't work, it doesn't
[01:03:19.680 --> 01:03:25.600]   work. Well, it's an experiment that you tried and you failed, but you learned out of it.
[01:03:25.680 --> 01:03:33.760]   So starting a project, as I've said, when you're in production, that's just me drawing an image of
[01:03:33.760 --> 01:03:40.160]   you. So I'm really sorry if you don't look like this, but you don't want to be spending,
[01:03:40.160 --> 01:03:44.720]   like if you have three months to work on a project, you don't want to be spending two and a
[01:03:44.720 --> 01:03:51.920]   half months training a model. You actually want to be able to spend equal time in, like on each of
[01:03:51.920 --> 01:03:57.600]   these different parts of deep learning, like getting the data set, labeling a data set, then
[01:03:57.600 --> 01:04:03.360]   modeling, and then serving it in production. You really want to be able to spend equal time because
[01:04:03.360 --> 01:04:09.040]   then that will give you the skills and that will give you a really good exposure to all these other
[01:04:09.040 --> 01:04:15.360]   things. So the goal is not to find the perfect data set or project, but just to get started and
[01:04:15.360 --> 01:04:21.120]   iterate from there. So in your first iteration, you have something that's working. So that's your
[01:04:21.120 --> 01:04:26.240]   first iteration. It's okay if your model is really small, if the performance is 70 percent or I don't
[01:04:26.240 --> 01:04:30.960]   know, if the performance is not up to the mark, if your data set has errors, if the labeling, if the
[01:04:30.960 --> 01:04:37.120]   labels are bad, that's okay. Like if there's some wrong labels in your data set, that's completely
[01:04:37.120 --> 01:04:41.600]   fine. What you want to do is you want to go in iteration. So in your first iteration, you want
[01:04:41.600 --> 01:04:45.920]   to do this. In your second iteration, you want to improve things. Then in your third iteration,
[01:04:45.920 --> 01:04:50.400]   you want to still improve things. And by the time you're in your fourth iteration, you would have a
[01:04:50.400 --> 01:04:56.480]   pipeline that's working really beautifully. And a lot of Kaggle grandmasters, a lot of people on
[01:04:56.480 --> 01:05:01.040]   Kaggle and Sinyam would definitely know more about this because he's interviewed, I don't know,
[01:05:01.040 --> 01:05:09.200]   50 or hundreds of them. And he works at H2O AI. But a lot of Kaggle people have all these ways of
[01:05:09.200 --> 01:05:14.240]   like starting with an MVP version. And then you just iterate and iterate and iterate to have
[01:05:14.240 --> 01:05:22.560]   something that's working. So you complete lots of small experiments. And where do you start doing
[01:05:22.560 --> 01:05:25.840]   these experiments? Because from this week, you want to go back and you want to start training
[01:05:25.840 --> 01:05:32.320]   the model on your own data sets. Well, you start with a project that's related to you. You start
[01:05:32.320 --> 01:05:37.040]   with a project that's related to where you work so you have access to data. So if you're a medical
[01:05:37.040 --> 01:05:42.240]   person, you start with x-rays and CT scans. If you're in a biology kind of a field, you work with
[01:05:42.240 --> 01:05:46.640]   proteins, then you work with protein sequencing. If you're a wildlife photographer, then you have
[01:05:46.640 --> 01:05:51.520]   like hundreds and thousands of wildlife images. That's where you start. If you're a music person,
[01:05:51.520 --> 01:05:56.800]   you start with audio. If you're working with lots of tabular side of things, I wouldn't--
[01:05:56.800 --> 01:06:07.360]   on a side note, I wouldn't 100% recommend starting with tabular because tabular is still keeping up
[01:06:07.360 --> 01:06:16.560]   with text and images. So if you get stuck in a problem, you won't have five other people trying
[01:06:16.560 --> 01:06:22.400]   to answer it for you. So it would be easier if you start with images and text, but you're more
[01:06:22.400 --> 01:06:28.960]   than welcome to start with tabular. Tabular is now becoming a mature field anyway. But I would just
[01:06:28.960 --> 01:06:36.000]   say take your pick and start with somewhere where you belong. So it should be related to your field
[01:06:36.000 --> 01:06:40.240]   of interest. And the reason why it should be related to your field of interest is because
[01:06:40.240 --> 01:06:50.000]   if you're just doing it for the sake of it, you won't finish. So as I said, CV, text, and tabular
[01:06:50.000 --> 01:06:57.360]   is where you can start pretty much. So the state of deep learning, because this book was written
[01:06:57.360 --> 01:07:11.520]   in early 2020, I'm going to actually skip this for now. And I just want to spend five minutes on
[01:07:11.520 --> 01:07:19.040]   this section of the notebook first, which is gathering data. So we will go back to look at
[01:07:19.040 --> 01:07:26.560]   what deep learning can do today. But something I want to spend time on is as part of this chapter,
[01:07:26.560 --> 01:07:31.280]   what you're actually going to be doing is you're going to be building a grizzly bear or basically
[01:07:31.280 --> 01:07:37.760]   a bear detector that can classify three types of bears, grizzly, black, and teddy bears. And the
[01:07:37.760 --> 01:07:44.400]   first step, as I said, is getting the data set. So there's all these different projects that you
[01:07:44.400 --> 01:07:48.880]   can do. And one of the really funny ones that Jeremy once mentioned in the fast.ai course is
[01:07:48.880 --> 01:07:56.080]   that somebody built a deep learning model for his fiancee, basically, so she could classify the 16
[01:07:56.080 --> 01:08:00.720]   cousins during a Christmas vacation. So she could get the camera out, take a picture, and then the
[01:08:00.720 --> 01:08:04.560]   model will tell you who that cousin is because she couldn't remember the names, which is really,
[01:08:04.560 --> 01:08:12.000]   really interesting. And we've seen applications of basically people trying to categorize their
[01:08:12.000 --> 01:08:17.280]   WhatsApp images into all these different categories. So take your pick that you feel you can
[01:08:17.280 --> 01:08:22.160]   do. And I just want to show you how you can get your data set because this is something that's
[01:08:22.160 --> 01:08:26.640]   really going to help you and it's really going to be important this week is that you want to,
[01:08:26.640 --> 01:08:34.560]   this fast.ai basically uses something, a Bing image search. So if I go
[01:08:34.560 --> 01:08:49.760]   and I type Bing image search and I type, say, I want to do grizzly bear detector. So I type grizzly
[01:08:49.760 --> 01:08:57.760]   bears. Then these are the a lot of images that I get for grizzly bears. And if I want to make a
[01:08:57.760 --> 01:09:02.880]   grizzly bear versus teddy bear detector, then I can just search teddy bear and that will give me
[01:09:02.880 --> 01:09:10.720]   all of these teddy bears. So something that you will have to do is you will have to go to Microsoft
[01:09:10.720 --> 01:09:15.920]   Azure and you'll have to sign up. What that will do is that will give you an Azure search key.
[01:09:16.720 --> 01:09:24.800]   It's okay if you get stuck on forums.fast.ai. If you go, there's plenty of resources.
[01:09:24.800 --> 01:09:30.960]   Actually, it should be here. So if I search
[01:09:30.960 --> 01:09:37.840]   Azure key, then you will see like there's all these different.
[01:09:45.280 --> 01:09:50.640]   Then you'll see all these different posts on how you can get the image search key basically.
[01:09:50.640 --> 01:09:54.960]   So something you want to do then is you want to go to Microsoft Azure and you want to sign up.
[01:09:54.960 --> 01:10:01.520]   Then you get the key, you put your key in like this. Then there's this function called search
[01:10:01.520 --> 01:10:06.800]   images Bing. So you're more than welcome to call like doc search images Bing, which will bring up
[01:10:06.800 --> 01:10:13.440]   the documentation and the source code. And then when you say search images Bing, passing in your
[01:10:13.440 --> 01:10:18.320]   API key and you just say grizzly bear, then what this is, this function is going to do.
[01:10:18.320 --> 01:10:25.680]   If I said teddy bear, then all of these images are going to be downloaded using an API.
[01:10:25.680 --> 01:10:34.720]   So then you get your images and you pretty much have 150 images now of grizzly bear. So these
[01:10:34.720 --> 01:10:41.200]   images are just parts or URLs that look like this. I can download them using the download URL.
[01:10:42.000 --> 01:10:47.040]   I can provide a destination that this is where I want to basically download this image from the
[01:10:47.040 --> 01:10:52.000]   internet to this point. And then if I open the image, here we go. This is an image that's been
[01:10:52.000 --> 01:11:02.000]   downloaded from the internet. So what this line of code does is that for each of these grizzly bear
[01:11:02.000 --> 01:11:08.320]   and teddy, it searches grizzly bear, black bear and teddy bear. And for each of these bear types,
[01:11:08.320 --> 01:11:16.000]   it will download the images. So by the time when you get all your images, basically get the parts
[01:11:16.000 --> 01:11:23.840]   of all the images that you've downloaded so far, this will return all those image parts. You can
[01:11:23.840 --> 01:11:32.080]   see how there's 406 images. So we just by doing like these 10 or 15 lines of code, we have 406
[01:11:32.080 --> 01:11:38.560]   images of grizzly bears. And then something that you want to do is you just remove the corrupt
[01:11:38.560 --> 01:11:43.200]   images. So by corrupt images, I mean, these could be black images or these could just be having some
[01:11:43.200 --> 01:11:47.920]   corruption or some noise in them that you really don't want or in a teddy bear, it could be a
[01:11:47.920 --> 01:11:54.400]   picture of a football. And then you just remove them and you finally have your data set ready.
[01:11:54.400 --> 01:12:03.840]   So that's the first step of your model training. You're now ready after this point, after running
[01:12:03.840 --> 01:12:10.880]   these 10, 15 lines of code is you're ready with your data set. So by now, if you're someone who
[01:12:10.880 --> 01:12:18.560]   loves cricket, which I do as well, you can actually take the cricket players off, take the pictures of
[01:12:18.560 --> 01:12:23.760]   all the cricket players in a team that you support. If you're a football person or a footy person,
[01:12:23.760 --> 01:12:29.120]   then you could actually even use sport and you could try and classify them. So I want you to
[01:12:29.120 --> 01:12:36.080]   go back and I want you to this week run these lines of code and make sure that your data set
[01:12:36.080 --> 01:12:42.960]   is ready. And if you're feeling enthusiastic, then keep going forward. And by the time you finish,
[01:12:42.960 --> 01:12:49.680]   you will have trained. You keep going forward and you keep following these lines of code until you
[01:12:49.680 --> 01:12:57.680]   get to this point of, until you get to this point here. So then by now, you would have built an image
[01:12:57.680 --> 01:13:04.800]   classifier that can actually tell, that has really low accuracy and you can form a confusion matrix.
[01:13:04.800 --> 01:13:10.720]   We will touch upon what each of these things do, but it's actually the same code that we saw in
[01:13:10.720 --> 01:13:16.000]   first line, sorry, the first chapter. You create your convolutional neural net and then you say
[01:13:16.000 --> 01:13:24.720]   fine tune four. Four just means you run four epochs of training. So yeah, so this is the
[01:13:24.720 --> 01:13:29.440]   homework for this week. We're going to stop here. Next week when we come back, we're going to look
[01:13:29.440 --> 01:13:36.160]   at where deep learning is in all these other fields, what it can really do. But for this week,
[01:13:36.160 --> 01:13:42.720]   go back, read the first chapter, run each and every line of code that was there in the first chapter,
[01:13:42.720 --> 01:13:48.080]   and then you run, you look at the questionnaire and you answer the questionnaire. And then you
[01:13:48.080 --> 01:13:54.960]   come to the second chapter and you make sure that your dataset is ready. Like this is the bare bare
[01:13:54.960 --> 01:14:01.360]   minimum that you have to do this week if you want to follow along next week. So thanks for that.
[01:14:01.360 --> 01:14:09.440]   I'll stop sharing my screen now. We are left with 15 minutes and that's Tanish. Hey Tanish,
[01:14:09.440 --> 01:14:14.640]   you still here? Yep, I'm still here. Yeah, let me share my screen.
[01:14:14.640 --> 01:14:36.640]   Okay, can everyone see my screen? Yes. Okay. Yeah, so I guess I'll get started.
[01:14:37.920 --> 01:14:42.720]   Yeah, but before I get started, I just wanted to also mention, I know that Amin was talking
[01:14:42.720 --> 01:14:50.000]   about the questionnaires. I think I had posted some of the solutions for some of the questionnaires,
[01:14:50.000 --> 01:14:57.840]   especially for the first 12 or so chapters. So after you've completed it, if you want to
[01:14:57.840 --> 01:15:02.560]   hear another perspective, read another perspective, you can check that out. And it's also a wiki,
[01:15:02.560 --> 01:15:07.120]   so some of the other people on the forums have contributed to those questionnaires. And so you
[01:15:07.120 --> 01:15:13.280]   can check that out as well after you've completed them yourself. Hey Tanish, sorry, do you have a
[01:15:13.280 --> 01:15:17.920]   link for people to follow or these are the Fast.ai forums, right? Do you have a link? Yeah,
[01:15:17.920 --> 01:15:23.600]   I can share them afterwards, but if you just search up Fast.ai questionnaire, they should come
[01:15:23.600 --> 01:15:30.240]   up. But yeah, I can share them afterwards in the Slack or somewhere. Yeah, so I'll just get started
[01:15:30.240 --> 01:15:36.800]   with my presentation and I'm just talking about my machine learning and Fast.ai journey and
[01:15:36.800 --> 01:15:41.360]   providing a little bit of my advice as well. I know some of this is quite similar to what Jeremy
[01:15:41.360 --> 01:15:45.680]   has been mentioning as well, but I think it's also just helpful to see some examples of this
[01:15:45.680 --> 01:15:54.400]   in practice as well. So yeah, just to get started, talk about my machine learning journey. I started
[01:15:54.400 --> 01:16:00.240]   becoming interested in machine learning back in 2015, 2016. And I actually started out by taking
[01:16:00.240 --> 01:16:08.480]   the Coursera course on machine learning, which is also a quite well-known course. And it talks
[01:16:08.480 --> 01:16:13.520]   about some of the basic concepts of machine learning, but even at that point, it was fairly
[01:16:13.520 --> 01:16:19.440]   outdated as a course. And while it provided a basic understanding, there were some other concepts
[01:16:19.440 --> 01:16:25.760]   that it did not provide a thorough explanation of. And then also, I heard about this platform
[01:16:25.760 --> 01:16:30.720]   known as Kaggle. And I had actually joined the platform back in 2016, a long time ago.
[01:16:30.720 --> 01:16:37.760]   And just actually want to briefly talk about Kaggle because I think it's an important platform
[01:16:37.760 --> 01:16:44.080]   for anyone who is interested in machine learning and data science. Kaggle is a platform where it's
[01:16:44.080 --> 01:16:49.680]   basically the platform to be where if you're interested in learning machine learning and data
[01:16:49.680 --> 01:16:56.960]   science, and they have various different resources. And also, more importantly, they have these
[01:16:56.960 --> 01:17:03.520]   competitions that they host. And these are competitions where they have actual real world
[01:17:03.520 --> 01:17:08.320]   tasks and people from around the world are competing to develop the best models for these
[01:17:08.320 --> 01:17:13.200]   tasks. So these are some of the active competitions that are going on right now. There's one for
[01:17:13.200 --> 01:17:19.360]   COVID-19 detection. There's one for alien signal detection. There's some text classification
[01:17:19.360 --> 01:17:26.480]   problems as well. So various different tasks are available. And of course, all the previous
[01:17:26.480 --> 01:17:30.640]   competitions are available as well. So here's some examples of previous competitions. And you
[01:17:30.640 --> 01:17:37.280]   can also look for competitions based on the type of data. And so this is just generally a great
[01:17:37.280 --> 01:17:42.480]   resource. And also, they have people upload their own data sets where you can check out other data
[01:17:42.480 --> 01:17:50.000]   sets as well. And people also share code for these competitions and data sets. So for example,
[01:17:50.000 --> 01:17:57.280]   some data analysis, some starter notebooks, and pre-processing code, all kinds of code is
[01:17:57.280 --> 01:18:01.520]   available to help you get up and running with some of these competitions. And also discussion
[01:18:01.520 --> 01:18:06.800]   forums where they talk about the competitions, different ideas, resources, et cetera. And there
[01:18:06.800 --> 01:18:13.120]   are also some short courses that they have on various topics as well, which can be useful for
[01:18:13.120 --> 01:18:22.720]   supplementing your journey. But to go back to my journey on Kaggle, I started out on Kaggle just
[01:18:22.720 --> 01:18:26.800]   trying out some of these competitions, looking at the notebooks and the resources provided,
[01:18:26.800 --> 01:18:34.320]   and trying to start out with these notebooks, make some changes, submit, and work my way up.
[01:18:34.320 --> 01:18:39.440]   But unfortunately, it didn't start out very successful. Some of these competitions, I did
[01:18:39.440 --> 01:18:47.440]   not perform as well. And I guess after a certain point, I kind of gave up on Kaggle. And this was
[01:18:47.440 --> 01:18:53.360]   back maybe four or five years ago. But then after a while, I actually revisited Kaggle and was
[01:18:53.360 --> 01:19:01.280]   looking at some of the competitions. And I kind of discovered the Fast.ai library. And I noticed
[01:19:01.280 --> 01:19:05.760]   some of these notebooks were using the Fast.ai library and performing quite well. I think some
[01:19:05.760 --> 01:19:13.600]   of the notebooks by this person here, IA Foss, I don't know how to say his username, but some of
[01:19:13.600 --> 01:19:19.920]   his notebooks used Fast.ai, and they were quite helpful. And it seemed like Fast.ai was really
[01:19:19.920 --> 01:19:24.000]   helpful for people to get up and running and perform well in these competitions. And that
[01:19:24.000 --> 01:19:28.880]   got me really excited, especially when I saw there was a whole course behind this library.
[01:19:28.880 --> 01:19:38.240]   And so I wanted to explore this course and get started. So I was able to take the course back in
[01:19:38.240 --> 01:19:46.800]   beginning of 2019. At that time, they had released a new iteration. So I started diving into that new
[01:19:46.800 --> 01:19:53.200]   iteration of the course. And I just want to talk a little bit about what I did when working through
[01:19:53.200 --> 01:20:00.160]   the course. And I want to bring up Jeremy's slide that he had mentioned earlier about how to do a
[01:20:00.160 --> 01:20:06.000]   Fast.ai lesson. He talked about watching the lecture, experimenting with the notebooks,
[01:20:06.000 --> 01:20:13.440]   reproducing the notebooks, and also more importantly, repeating these experiments
[01:20:13.440 --> 01:20:18.720]   with different data sets. And this is what I really focused on with my Fast.ai journey.
[01:20:18.720 --> 01:20:25.280]   And again, I was looking at resources like Kaggle to help me be able to do this. So here are some
[01:20:25.280 --> 01:20:31.440]   examples of the notebooks that I had written up when I was taking the Fast.ai course and exploring
[01:20:31.440 --> 01:20:38.320]   some of these topics. So my background is in biomedical engineering, and I'm interested in
[01:20:38.320 --> 01:20:45.520]   medicine and biology and health care. So you can see some of these notebooks are focused on tasks
[01:20:45.520 --> 01:20:52.320]   like blood cell classification, diabetic retinopathy diagnosis, ultrasound nerve segmentation.
[01:20:52.320 --> 01:20:59.200]   So I was looking at how to apply deep learning to some of these tasks, these image classification
[01:20:59.200 --> 01:21:05.600]   tasks or segmentation tasks. And I was also interested in a couple of the ongoing competitions.
[01:21:05.600 --> 01:21:10.800]   There was one competition looking at earthquake prediction, and I tried out the Fast.ai tabular
[01:21:10.800 --> 01:21:17.360]   model when I was learning about that. And apparently, I didn't submit this to the final
[01:21:17.360 --> 01:21:24.080]   competition, but apparently, this would have gotten me within the top 5% of the competition,
[01:21:24.080 --> 01:21:28.720]   or you would get a silver medal for that competition. So I still regret that I didn't
[01:21:28.720 --> 01:21:33.600]   submit this yet. I didn't submit this to the competition, but I'm quite impressed that even
[01:21:33.600 --> 01:21:39.120]   with the basic concepts that we learned, this can take us quite far for real-world tasks.
[01:21:39.600 --> 01:21:46.480]   And then another aspect is, can you do after you train a model? You want to share your knowledge
[01:21:46.480 --> 01:21:55.120]   with the world and also to solidify that knowledge yourself. Being able to communicate your work and
[01:21:55.120 --> 01:22:00.640]   prepare a project that others can see is an important aspect. So this was actually some
[01:22:00.640 --> 01:22:06.080]   advice that was provided by a Kaggle Grandmaster, but I think it applies to any sort of machine
[01:22:06.080 --> 01:22:10.560]   learning project, not just Kaggle projects, but any sort of machine learning project. I think
[01:22:10.560 --> 01:22:18.720]   this is a great way of sharing your work. You can put it on a GitHub repository, prepare a Colab
[01:22:18.720 --> 01:22:25.520]   demo, prepare a web application demo, writing a blog post. These are all great ways of sharing
[01:22:25.520 --> 01:22:31.040]   your work. And I just wanted to provide some examples of that. So I definitely was very
[01:22:31.040 --> 01:22:38.480]   interested in this diabetic retinopathy diagnosis problem. So I actually prepared a GitHub repository.
[01:22:38.480 --> 01:22:43.760]   I have some experiments here in my notebooks folder, and there's some details in the README
[01:22:43.760 --> 01:22:51.840]   about the different experiments. And I have the models also uploaded. And I had made a web app,
[01:22:51.840 --> 01:22:56.960]   and that web app is available through... It's a Heroku app, and it's still available. You can
[01:22:56.960 --> 01:23:03.200]   check it out. And you can see here, it's just a simple application. I was able to find a template,
[01:23:03.200 --> 01:23:09.120]   and I just play around with the template. But it's just a great way of sharing whatever you have
[01:23:09.120 --> 01:23:15.440]   out with the world and having people check it out. And I think it's just a really great educational
[01:23:15.440 --> 01:23:20.160]   experience. Sorry, just to add, just I want to... Sorry to interrupt, but just to quickly add,
[01:23:20.160 --> 01:23:25.920]   as part of chapter two, we'll actually be building web apps. So by the time you come back next week,
[01:23:25.920 --> 01:23:30.240]   or throughout the week, if you want to read chapter two, you will be able to build a web
[01:23:30.240 --> 01:23:33.360]   app exactly like this that solves the problem you're interested in.
[01:23:33.360 --> 01:23:43.920]   Yeah, that's a great point. Yeah. And then another project of mine, a more recent project of mine,
[01:23:43.920 --> 01:23:53.760]   has been focused on this particular type of task known as unpaired image to image translation.
[01:23:53.760 --> 01:24:04.160]   And what I've been doing is I've been implementing this in Fast.ai and preparing it as a library for
[01:24:04.160 --> 01:24:11.280]   others to use, and also training models, sharing those models. And also, one thing I'd like to
[01:24:11.280 --> 01:24:17.840]   point out, and maybe we can discuss this later, but it makes it... Fast.ai also has a separate
[01:24:17.840 --> 01:24:22.640]   tool for developing libraries. And in fact, the Fast.ai library is based on this tool known as
[01:24:22.640 --> 01:24:28.800]   MBDev, and it makes it really easy to do things like this. And I think that's another great tool
[01:24:28.800 --> 01:24:34.480]   if you're interested in this sort of thing of maybe developing model implementations and sharing
[01:24:34.480 --> 01:24:39.920]   it with others, or whatever sort of machine learning project, maybe this is a good approach
[01:24:40.720 --> 01:24:44.560]   for doing so. It makes it much easier where you can just have a series of notebooks with
[01:24:44.560 --> 01:24:49.120]   documentation as part of those notebooks, and you get a nice documentation website. Of course,
[01:24:49.120 --> 01:24:58.080]   you have your library all due to this MBDev package. And of course, I have a simple web demo
[01:24:58.080 --> 01:25:05.680]   also. I know that the Fast.ai book has its own... It talks about, I think, Vola, I think is the
[01:25:06.480 --> 01:25:12.560]   web app tool it uses. But if you're interested, I would also investigate this tool known as Gradio,
[01:25:12.560 --> 01:25:18.480]   because this tool actually makes it really simple to create web apps. It only takes a couple lines
[01:25:18.480 --> 01:25:25.520]   of code to actually take an existing inference model prediction function and make a web app from
[01:25:25.520 --> 01:25:29.440]   that. So that might be something you might be interested in checking out as well.
[01:25:30.720 --> 01:25:37.840]   Okay. So I talked about how we can take a project and present it for the world to see, and
[01:25:37.840 --> 01:25:47.600]   how you can share your work. I also just want to go back to the original story of how I started out
[01:25:47.600 --> 01:25:53.920]   with Fast.ai, finding it on Kaggle and learning about it. And after I took the Fast.ai course,
[01:25:53.920 --> 01:26:00.240]   how it's tremendously helped my experience on Kaggle. And I've also learned a lot from Kaggle
[01:26:00.240 --> 01:26:08.080]   as well. And I've been able to participate in some of these competitions and obtain positions in
[01:26:08.080 --> 01:26:15.200]   some of the top 5% or top 3% of the leaderboard. So it's been a quite interesting and educational
[01:26:15.200 --> 01:26:22.560]   experience. And another aspect is I've been able to share some of these tutorials, starter notebooks,
[01:26:23.200 --> 01:26:31.120]   and data analysis as a Kaggle notebook. And I've been able to reach a Kaggle notebook master title.
[01:26:31.120 --> 01:26:36.080]   And also similarly, I've been also able to reach discussion master title, sharing different
[01:26:36.080 --> 01:26:42.880]   resources, asking questions, et cetera. And there are also other Fast.ai Kagglers. I just wanted to
[01:26:42.880 --> 01:26:52.000]   highlight a few others who have done a lot with using Fast.ai. So of course, Radik, I know we've
[01:26:52.000 --> 01:26:59.840]   heard a lot about him. And he's also experienced Kaggle master. And then I.A. Foss is also another
[01:26:59.840 --> 01:27:07.520]   Kaggle master. Both of them share amazing notebooks and starter material using Fast.ai.
[01:27:07.520 --> 01:27:14.480]   This is another Kaggle master who has performed well with the Fast.ai library. Also another
[01:27:14.480 --> 01:27:23.360]   Kaggle master who has actually won a competition with Fast.ai. So all quite impressive feats by
[01:27:23.360 --> 01:27:32.320]   other Fast.ai alumni. Yeah. And Dr. Habib is really-- he helped me with blog posts as well.
[01:27:32.320 --> 01:27:36.960]   So Dr. Habib, I know-- well, I know all of them are really open to answering questions. And
[01:27:36.960 --> 01:27:42.880]   Dr. Habib has then also contributed to blogs. And he's also helped with the Vision Transformer
[01:27:42.880 --> 01:27:46.640]   blogs that we've written. So I just wanted to really point that out as well that on top of
[01:27:46.640 --> 01:27:56.320]   Kaggle, these guys really are who we look up to when we first started. And it's just a very sharing
[01:27:56.320 --> 01:28:05.040]   and it's a very giving nature that they have. Yeah. Yeah. And then, of course, I think we've
[01:28:05.040 --> 01:28:11.440]   already discussed this a couple of times today. But also, it's important to share your work with
[01:28:11.440 --> 01:28:22.000]   the community. And people will love to hear what you've been working on. And of course, this is
[01:28:22.000 --> 01:28:28.320]   the post that you should be sharing it on the Fast.ai forums. So I often share whatever Kaggle
[01:28:28.320 --> 01:28:33.040]   notebooks or projects or whatever. I share it on this. And others have been doing so as well.
[01:28:33.040 --> 01:28:39.920]   And of course, it's important to contribute back to the community. I know that Amin has been doing
[01:28:39.920 --> 01:28:43.760]   this whole Fast Book Reading session as a way to contribute to the community.
[01:28:43.760 --> 01:28:51.360]   I've been contributing by asking and answering questions on the forums. Both are equally as
[01:28:51.360 --> 01:28:59.200]   important, both asking and answering. And that has made me a top contributor on the Fast.ai forums.
[01:28:59.200 --> 01:29:05.840]   And then also, I've been adding features or fixing bugs for the Fast.ai library as well.
[01:29:06.480 --> 01:29:11.440]   And that's one thing I also want to point out about contributing to the library.
[01:29:11.440 --> 01:29:19.200]   When you're using the Fast.ai library, if there's something that you find is insufficient with the
[01:29:19.200 --> 01:29:26.240]   library or there's maybe some problem you have, I think it's great if you can contribute back to
[01:29:26.240 --> 01:29:31.280]   the library and add that feature or fix that bug. And some of them can be simple bugs. Like,
[01:29:31.280 --> 01:29:35.440]   for example, one time I found there was some problem with the extension. It didn't support
[01:29:35.440 --> 01:29:40.480]   one of the extensions. I just made a quick fix and submitted it. That's definitely greatly
[01:29:40.480 --> 01:29:46.160]   appreciated by the entire community. Other times, I had found out there's this training technique
[01:29:46.160 --> 01:29:51.200]   that wasn't supported in Fast.ai. I decided to add it to the library.
[01:29:51.200 --> 01:29:56.720]   And I would say that also, contributing to the library is just a great experience for
[01:29:56.720 --> 01:30:02.400]   learning how to use developer tools and developer workflows like Git and GitHub.
[01:30:02.400 --> 01:30:10.000]   And these are the tools that all software engineers really use for practical applications.
[01:30:10.000 --> 01:30:14.640]   So I think if that's something you want to learn, this is a great way of doing that as well.
[01:30:14.640 --> 01:30:23.680]   And then just to briefly talk about how Fast.ai has helped me with my own research. So I'm a PhD
[01:30:23.680 --> 01:30:31.920]   candidate at UC Davis in biomedical engineering. And I've been using Fast.ai for my own research,
[01:30:31.920 --> 01:30:38.000]   applying deep learning to microscopy. I've presented at the ICML computational biology workshop.
[01:30:38.000 --> 01:30:42.720]   And hopefully, we'll be submitting a paper also sometime this year focused on this research.
[01:30:42.720 --> 01:30:48.480]   I'd be glad to talk about that later on as well. And of course--
[01:30:48.480 --> 01:30:48.980]   Wow.
[01:30:48.980 --> 01:30:58.320]   Yeah. A couple more slides. It's important to stay up to date with the field of machine learning.
[01:30:58.320 --> 01:31:02.400]   And these are some of the resources. Of course, as we talked about, Twitter is a great resource.
[01:31:02.400 --> 01:31:08.000]   And there are other resources like the machine learning stuff, Reddit, the Fast.ai Discord.
[01:31:08.000 --> 01:31:11.600]   And there's some other Discord servers and some other websites as well.
[01:31:11.600 --> 01:31:17.920]   And of course, important to keep in mind that the journey never ends. I think this is especially
[01:31:17.920 --> 01:31:25.680]   true for deep learning as a field that is rapidly moving. I've also taken the Fast.ai course a
[01:31:25.680 --> 01:31:30.640]   couple of times. And I've learned something each of the times I've taken it and, of course, from
[01:31:30.640 --> 01:31:37.280]   all the resources online. And I'm always reading new papers and always learning. So I think this
[01:31:37.280 --> 01:31:45.920]   is especially an important aspect of your deep learning journey. With that said, feel free to
[01:31:45.920 --> 01:31:52.240]   check me out on any of these social media or other channels. And that's the end of the presentation.
[01:31:54.320 --> 01:32:01.200]   Thank you, Tanvi. Even for me, even though I know you for quite some time now, even for me,
[01:32:01.200 --> 01:32:05.440]   it's really exciting to see your journey. And it still gives me the chills on how much
[01:32:05.440 --> 01:32:11.520]   it is able-- like people are able to achieve if you stick to a problem. And you've had your
[01:32:11.520 --> 01:32:16.480]   interest in biomedical and you've stuck to biomedical. And it's really important to also
[01:32:16.480 --> 01:32:21.440]   realize that that's why it's important to find things that you really love and that really
[01:32:21.440 --> 01:32:25.680]   interest you. Because otherwise, it will be really hard for people to finish. And they'll give up
[01:32:25.680 --> 01:32:32.720]   when there's a roadblock. Here's a fun fact just for people in case I forgot to mention. Tanishree
[01:32:32.720 --> 01:32:39.440]   is 18-year-old. So when he says he started back in 20-- I don't know, it's 15, 16. We were probably
[01:32:39.440 --> 01:32:47.520]   13, 14. So that's just a fun factor for everybody. With that being said, this would be the end of
[01:32:48.160 --> 01:32:54.000]   the session today. But we are going to meet in Slack. And we are going to meet for the informal
[01:32:54.000 --> 01:32:59.360]   catch up where everybody can have their audio and video on. And we talk to each other. And we just
[01:32:59.360 --> 01:33:04.080]   get to know you guys as well and your expectations from the course. And Tanishree, you're going to
[01:33:04.080 --> 01:33:10.720]   be around there for a little bit. So you can ask your questions. Excellent. All right. See you guys
[01:33:10.720 --> 01:33:17.040]   in Slack. I'll post a link over there for Zoom. And we'll all meet you there. Thanks for today.
[01:33:17.040 --> 01:33:28.080]   And see you guys next week. Thanks, Tanish. See you.
[01:33:28.080 --> 01:33:40.560]   [MUSIC PLAYING]

