
[00:00:00.000 --> 00:00:07.840]   ML is everywhere, but it's starting from perception and more and more moving to prediction.
[00:00:07.840 --> 00:00:11.440]   And I think where the cutting edge is really is in the planning and control side.
[00:00:11.440 --> 00:00:14.400]   So how do you bridge these gaps from pixels to the steering?
[00:00:14.400 --> 00:00:15.400]   So ML is everywhere.
[00:00:15.400 --> 00:00:17.040]   Of course, I'm biased.
[00:00:17.040 --> 00:00:21.400]   You're listening to Gradient Descent, a show about machine learning in the real world.
[00:00:21.400 --> 00:00:23.320]   And I'm your host, Lukas Biewald.
[00:00:23.320 --> 00:00:28.560]   Adrian is exactly the kind of person we imagined having on this podcast when we started it.
[00:00:28.560 --> 00:00:33.160]   He's the head of research at TRI, Toyota's research arm.
[00:00:33.160 --> 00:00:39.000]   And he's been a longtime user, maybe one of the very first users of Weights & Biases.
[00:00:39.000 --> 00:00:43.080]   And every time I talk to him, he has interesting ideas on the field of machine learning and
[00:00:43.080 --> 00:00:45.320]   the tools necessary to make it really work in production.
[00:00:45.320 --> 00:00:47.600]   So this is going to be a really interesting conversation.
[00:00:47.600 --> 00:00:48.600]   All right.
[00:00:48.600 --> 00:00:52.760]   So I have a whole bunch of questions, but I thought I'd start with a little bit of an
[00:00:52.760 --> 00:00:54.760]   oddball one, only for you.
[00:00:54.760 --> 00:00:59.720]   So I always use this metaphor, building the Weights & Biases tools that I hope our users
[00:00:59.720 --> 00:01:03.680]   love our tools in the same way that a guitar player loves their guitars.
[00:01:03.680 --> 00:01:05.040]   And I know you are a guitar player.
[00:01:05.040 --> 00:01:08.200]   Do you have a favorite one that you own and play?
[00:01:08.200 --> 00:01:09.880]   Right there.
[00:01:09.880 --> 00:01:10.880]   Nice.
[00:01:10.880 --> 00:01:13.160]   I don't know.
[00:01:13.160 --> 00:01:18.720]   I need to activate my Zoom background, but this is a road worn fender strut.
[00:01:18.720 --> 00:01:19.720]   Oh, beautiful.
[00:01:19.720 --> 00:01:26.400]   I love the road worn because first, I don't mind damaging it more.
[00:01:26.400 --> 00:01:28.600]   And second, it has a really nice feel.
[00:01:28.600 --> 00:01:33.880]   I think tools, that's more general than just guitars, but they grow on you.
[00:01:33.880 --> 00:01:37.880]   It's kind of like, it's almost like a lot of musicians give names to their guitars,
[00:01:37.880 --> 00:01:39.960]   like Eric Clapton famously, et cetera.
[00:01:39.960 --> 00:01:44.120]   And I think really good tools, they become part of you and you develop a relationship
[00:01:44.120 --> 00:01:45.120]   with them.
[00:01:45.120 --> 00:01:46.120]   So it's the case for cars.
[00:01:46.120 --> 00:01:48.000]   It's the case for guitars.
[00:01:48.000 --> 00:01:50.120]   General tools, it's kind of interesting.
[00:01:50.120 --> 00:01:52.440]   Some tools definitely become part of you.
[00:01:52.440 --> 00:01:57.960]   I haven't named a WNB report after my daughter or something like this yet, but who knows?
[00:01:57.960 --> 00:02:02.720]   Well, besides WNB, what are your favorite tools that you use in your day-to-day job
[00:02:02.720 --> 00:02:04.980]   building machine learning models?
[00:02:04.980 --> 00:02:09.840]   So if you're talking as a manager, that's not the same as if you're talking as a scientist.
[00:02:09.840 --> 00:02:10.840]   Answer both, please.
[00:02:10.840 --> 00:02:12.480]   I would love to hear both.
[00:02:12.480 --> 00:02:13.480]   All right.
[00:02:13.480 --> 00:02:15.960]   So I won't mention maybe the ones that everybody knows and love.
[00:02:15.960 --> 00:02:18.120]   I like Todoist a lot as a manager.
[00:02:18.120 --> 00:02:21.560]   That's a great way to manage your tasks and stuff like this.
[00:02:21.560 --> 00:02:24.880]   I've been a long time user of Todoist and I really, really like it.
[00:02:24.880 --> 00:02:28.320]   Tried a lot of different ways to manage to-dos and et cetera, and keep track of those.
[00:02:28.320 --> 00:02:31.400]   They even have karma points and whatever things like gamification.
[00:02:31.400 --> 00:02:35.080]   So this one I think is a pretty nice recommendation I can give to everybody that has a lot of
[00:02:35.080 --> 00:02:36.560]   tasks and want to stay on top of them.
[00:02:36.560 --> 00:02:39.160]   So as a manager, this one is good.
[00:02:39.160 --> 00:02:40.160]   And what do you like about it?
[00:02:40.160 --> 00:02:42.040]   As you said, Todoist is the app.
[00:02:42.040 --> 00:02:43.120]   We should put a link to that.
[00:02:43.120 --> 00:02:44.120]   It's Todoist.
[00:02:44.120 --> 00:02:45.120]   Todoist.
[00:02:45.120 --> 00:02:46.120]   Yeah.
[00:02:46.120 --> 00:02:47.120]   In one word.
[00:02:47.120 --> 00:02:50.440]   I have to say, I use Workflowy and I'm super attached to it myself.
[00:02:50.440 --> 00:02:53.640]   I'm curious, what do you like about Todoist?
[00:02:53.640 --> 00:02:54.640]   It's very simple.
[00:02:54.640 --> 00:02:58.680]   So I think tools in this complicated world where you have many things to do, it has to
[00:02:58.680 --> 00:03:00.760]   be dead simple.
[00:03:00.760 --> 00:03:03.880]   And good synchronization across devices is also super important because when you switch
[00:03:03.880 --> 00:03:05.680]   from one to the other, et cetera.
[00:03:05.680 --> 00:03:06.680]   Nice.
[00:03:06.680 --> 00:03:07.920]   Well, this show isn't for managers.
[00:03:07.920 --> 00:03:09.320]   It's for the scientists.
[00:03:09.320 --> 00:03:12.220]   So tell us about as a scientist, what tools you love.
[00:03:12.220 --> 00:03:16.480]   So for the scientists, I mean, Jupyter Notebooks, obviously.
[00:03:16.480 --> 00:03:20.200]   I said I won't mention the ones that everybody uses and loves, but this one still, I will
[00:03:20.200 --> 00:03:21.200]   mention it.
[00:03:21.200 --> 00:03:23.120]   Otherwise, PyTorch is just awesome.
[00:03:23.120 --> 00:03:27.960]   As a manager and a senior manager, I get less and less time to do technical stuff, as it
[00:03:27.960 --> 00:03:28.960]   should.
[00:03:28.960 --> 00:03:30.980]   I focus on empowering my teams, et cetera.
[00:03:30.980 --> 00:03:32.760]   But I still have this itch.
[00:03:32.760 --> 00:03:36.520]   And sometimes I do a lot of code reviews and I'm like, "Oh yeah, I want to try this thing."
[00:03:36.520 --> 00:03:42.160]   And so PyTorch, even as a senior manager that doesn't do 50% or even 30% of his day coding,
[00:03:42.160 --> 00:03:47.200]   I still get back to it very, very quickly because it's just so simple.
[00:03:47.200 --> 00:03:50.680]   Very few abstractions, very little vocabulary.
[00:03:50.680 --> 00:03:51.680]   It's not a DSL, right?
[00:03:51.680 --> 00:03:52.680]   It's NumPy.
[00:03:52.680 --> 00:03:55.040]   It's NumPy on steroids and that's just so easy to use.
[00:03:55.040 --> 00:03:56.040]   Interesting.
[00:03:56.040 --> 00:03:58.040]   Can you say anything else about PyTorch?
[00:03:58.040 --> 00:03:59.040]   I'm always kind of curious.
[00:03:59.040 --> 00:04:04.320]   PyTorch just seems to have these passionate fans among our user base.
[00:04:04.320 --> 00:04:07.860]   The people that use other frameworks, they use them a lot and they seem to like them,
[00:04:07.860 --> 00:04:11.960]   but somehow the people that use PyTorch seem just incredible, like advocates.
[00:04:11.960 --> 00:04:15.420]   Do you have any sense of why that is?
[00:04:15.420 --> 00:04:18.380]   So I've been working in computer vision since 2007.
[00:04:18.380 --> 00:04:24.500]   And so basically in 2012, I finished my PhD and then I moved to research and industry
[00:04:24.500 --> 00:04:25.860]   at Xerox Research.
[00:04:25.860 --> 00:04:30.140]   And then what was interesting was that was just the time, so I was big into kernel methods.
[00:04:30.140 --> 00:04:36.900]   Everything had to be convex and learning theory, VAPNIC, super clean, and then like 2012, Krzyzewski,
[00:04:36.900 --> 00:04:39.660]   non-convexity, not a problem, all these kinds of things.
[00:04:39.660 --> 00:04:44.940]   And Caffe was very big, became very, very big, especially 2013 with like Ross Gershick
[00:04:44.940 --> 00:04:48.780]   and Berkeley doing amazing work there, Yonkin Jia, et cetera, et cetera.
[00:04:48.780 --> 00:04:52.380]   So all these kind of tools really born there.
[00:04:52.380 --> 00:04:59.660]   But Caffe is C++ library, fairly easy to reproduce things, but fairly hard to do your own fork
[00:04:59.660 --> 00:05:03.340]   and do something very different, especially in the learning algorithm, not changing architectures,
[00:05:03.340 --> 00:05:04.340]   et cetera.
[00:05:04.340 --> 00:05:08.620]   That's the easy part of deep learning, but changing the task you're working on or changing
[00:05:08.620 --> 00:05:11.320]   the overall learning algorithm is more complicated.
[00:05:11.320 --> 00:05:15.280]   And I maintain an internal fork of Caffe and we did some papers, et cetera.
[00:05:15.280 --> 00:05:19.980]   But then the alternative of Theano, which was like, let's say an early days pioneer,
[00:05:19.980 --> 00:05:23.340]   and I will leave it at that, a great library, but not necessarily the most user-friendly
[00:05:23.340 --> 00:05:24.340]   one.
[00:05:24.340 --> 00:05:28.380]   And then TensorFlow came and then this huge hype train of like Google, everybody wanting
[00:05:28.380 --> 00:05:31.000]   to work for Google, like TensorFlow, TensorFlow.
[00:05:31.000 --> 00:05:33.100]   So of course jumped on the bandwagon too.
[00:05:33.100 --> 00:05:37.480]   And then Lua Torch was the only kind of like asterisk a little bit, like the little village
[00:05:37.480 --> 00:05:41.880]   like of resistance to the Roman empire, but I never really liked Lua.
[00:05:41.880 --> 00:05:43.600]   I was always a big Python fan.
[00:05:43.600 --> 00:05:49.000]   And so when PyTorch came out, like the nice clean design of Torch with Python, that basically
[00:05:49.000 --> 00:05:50.040]   became a no brainer.
[00:05:50.040 --> 00:05:55.320]   And everybody that did Python, like PyData, kind of like Sphere, like SciPy Sphere, it
[00:05:55.320 --> 00:05:59.000]   was familiar with NumPy, immediately became familiar with PyTorch.
[00:05:59.000 --> 00:06:00.120]   And that was a genius, right?
[00:06:00.120 --> 00:06:01.520]   No training, no onboarding.
[00:06:01.520 --> 00:06:03.480]   You know NumPy, you can use PyTorch.
[00:06:03.480 --> 00:06:06.760]   I'm psyched for JAX, so it's kind of interesting because now Google kind of realized this,
[00:06:06.760 --> 00:06:11.360]   that the DSL, graph-based, like very complicated ecosystem, like very complete ecosystem.
[00:06:11.360 --> 00:06:12.800]   So really nice for production setup.
[00:06:12.800 --> 00:06:17.420]   But for researchers that are a bit more on the crazy side of things, I wish I had the
[00:06:17.420 --> 00:06:18.680]   time to play with JAX basically.
[00:06:18.680 --> 00:06:20.840]   I just looked at things and it sounds amazing.
[00:06:20.840 --> 00:06:24.800]   And I think maybe there's going to be more diversity on PyTorch-like tools.
[00:06:24.800 --> 00:06:25.800]   It's so interesting.
[00:06:25.800 --> 00:06:29.480]   I think when TensorFlow came out, I thought, "Oh, people just want to use the same tool
[00:06:29.480 --> 00:06:31.440]   and everyone's just going to kind of switch this."
[00:06:31.440 --> 00:06:37.080]   It's been kind of surprising to see the passionate advocates of PyTorch, at least by our internal
[00:06:37.080 --> 00:06:39.720]   metrics, it seems like it's getting more popular.
[00:06:39.720 --> 00:06:44.800]   Do you have any sense about what it is about the design that makes it feel so satisfying?
[00:06:44.800 --> 00:06:45.800]   Right.
[00:06:45.800 --> 00:06:49.600]   So there is a really, really great paper at NeurIPS last year, if I remember correctly.
[00:06:49.600 --> 00:06:52.680]   I think it's already cited 1500 times, which is huge.
[00:06:52.680 --> 00:06:55.720]   For a paper to be cited more than a thousand times is a big, big deal.
[00:06:55.720 --> 00:06:58.760]   So in less than one year, it just shows you how popular it actually is.
[00:06:58.760 --> 00:07:02.400]   So it's by the PyTorch authors like Sumit Shintala, et cetera, et cetera.
[00:07:02.400 --> 00:07:03.720]   All these great people.
[00:07:03.720 --> 00:07:06.360]   And they describe their design principles in that paper.
[00:07:06.360 --> 00:07:08.480]   So I can recommend to your listeners to check it out.
[00:07:08.480 --> 00:07:09.480]   It's very accessible.
[00:07:09.480 --> 00:07:11.600]   It's a NeurIPS paper, so it might scare people away.
[00:07:11.600 --> 00:07:12.760]   That's the math, but it's not.
[00:07:12.760 --> 00:07:14.560]   It's a really, really good paper to read.
[00:07:14.560 --> 00:07:18.760]   So I won't summarize that paper, but the design principles are really, really good.
[00:07:18.760 --> 00:07:22.560]   And they are basically directly the result for me of the great UX.
[00:07:22.560 --> 00:07:23.920]   It's the user experience, right?
[00:07:23.920 --> 00:07:27.800]   Is that just, you can't force people in this age of open source and of free tools that
[00:07:27.800 --> 00:07:31.160]   are widely available and also widely known, right?
[00:07:31.160 --> 00:07:34.920]   It's like you have to live under a rock to not know PyTorch exists, right?
[00:07:34.920 --> 00:07:37.480]   Then the best user experience wins.
[00:07:37.480 --> 00:07:39.080]   It's just as simple as that.
[00:07:39.080 --> 00:07:41.720]   And PyTorch is just so few abstractions.
[00:07:41.720 --> 00:07:44.680]   I think it's like maybe four abstractions total that you have to know.
[00:07:44.680 --> 00:07:46.440]   That is PyTorch specific, right?
[00:07:46.440 --> 00:07:50.920]   And again, the rest is just very, very generic, very powerful, has nice workflows.
[00:07:50.920 --> 00:07:55.600]   There's PyTorch Lightning that tries to simplify those workflows, like maybe Kera style, like
[00:07:55.600 --> 00:07:57.200]   high level APIs.
[00:07:57.200 --> 00:08:02.680]   But just the base level one is just, you go from idea to experiments really quickly.
[00:08:02.680 --> 00:08:04.760]   So that would be my why.
[00:08:04.760 --> 00:08:10.040]   I love the idea of user experience of a library or like a deep learning framework.
[00:08:10.040 --> 00:08:13.320]   It's like you normally think of user experience as like a website, but the developer user
[00:08:13.320 --> 00:08:14.800]   experience is so important.
[00:08:14.800 --> 00:08:15.960]   I totally agree.
[00:08:15.960 --> 00:08:16.960]   Yeah.
[00:08:16.960 --> 00:08:20.360]   And it's because basically just like coding, right, is becoming democratized.
[00:08:20.360 --> 00:08:22.440]   There's this huge thing about no code, right?
[00:08:22.440 --> 00:08:24.460]   Which is all about that, right?
[00:08:24.460 --> 00:08:26.840]   But code is still like, people are going to code for a long time.
[00:08:26.840 --> 00:08:28.400]   It's like, people say, "Oh, no code.
[00:08:28.400 --> 00:08:29.400]   People will stop coding soon."
[00:08:29.400 --> 00:08:30.400]   No.
[00:08:30.400 --> 00:08:31.400]   Same thing as like self-driving cars.
[00:08:31.400 --> 00:08:34.040]   Yeah, they're going to happen, but it doesn't mean that people are going to stop driving
[00:08:34.040 --> 00:08:35.040]   soon.
[00:08:35.040 --> 00:08:40.720]   So there is kind of a lot of good things that can happen if you simplify the user experience
[00:08:40.720 --> 00:08:43.020]   for what used to be called power users.
[00:08:43.020 --> 00:08:47.600]   But the '70s era is done where only the most hardcore geeks code.
[00:08:47.600 --> 00:08:48.600]   Everybody codes now.
[00:08:48.600 --> 00:08:50.960]   So I mean, a lot of people code.
[00:08:50.960 --> 00:08:52.840]   And I guess you're more than a researcher, right?
[00:08:52.840 --> 00:08:57.040]   I mean, you've been working on autonomous vehicles at Toyota, trying to deploy them
[00:08:57.040 --> 00:08:58.760]   for quite a long time, right?
[00:08:58.760 --> 00:09:04.520]   So I think some people might worry that PyTorch isn't easy to put into production, but you
[00:09:04.520 --> 00:09:07.880]   have one of the biggest challenges of productionizing your systems.
[00:09:07.880 --> 00:09:08.880]   How have you thought about that?
[00:09:08.880 --> 00:09:11.160]   Does PyTorch work for you in production?
[00:09:11.160 --> 00:09:12.160]   Right.
[00:09:12.160 --> 00:09:16.500]   So TRI was Toyota Research Institute where our work was created in 2016, roughly.
[00:09:16.500 --> 00:09:21.640]   And so we haven't worked that long on it, compared let's say to the Waymo's, et cetera,
[00:09:21.640 --> 00:09:23.440]   that really started in 2009.
[00:09:23.440 --> 00:09:26.880]   But what's fun is that we kind of started with PyTorch almost from the start.
[00:09:26.880 --> 00:09:30.960]   We did at first, like the first year, we were really working on TensorFlow, mostly for that
[00:09:30.960 --> 00:09:34.420]   reason that you're describing, is putting things in production, et cetera.
[00:09:34.420 --> 00:09:38.000]   But we found out that iterating was actually a bit painful.
[00:09:38.000 --> 00:09:41.720]   And because the decision was within our power, us the researcher, we kind of switched to
[00:09:41.720 --> 00:09:43.360]   PyTorch fairly quickly.
[00:09:43.360 --> 00:09:46.640]   So that was one of the decisions I made early on that we're really happy with.
[00:09:46.640 --> 00:09:50.200]   The downside to it is when you are on the bleeding edge, you have blood all over your
[00:09:50.200 --> 00:09:51.200]   fingers.
[00:09:51.200 --> 00:09:52.200]   You're not going to heal yourself.
[00:09:52.200 --> 00:09:53.560]   And especially on the production side.
[00:09:53.560 --> 00:09:59.640]   So in the early days, what it meant is that, well, you deploy something like in Python
[00:09:59.640 --> 00:10:00.640]   or something, not glorious.
[00:10:00.640 --> 00:10:03.520]   I don't want to go into the details because it's a bit like...
[00:10:03.520 --> 00:10:07.560]   But then as the ecosystem progressed, and now, especially I would say in the last year
[00:10:07.560 --> 00:10:15.080]   or so, PyTorch has really been growing in its focus on productionizing.
[00:10:15.080 --> 00:10:16.560]   It turned out to be a really good bet.
[00:10:16.560 --> 00:10:20.440]   We did it from a research perspective and a velocity of iteration because that's our
[00:10:20.440 --> 00:10:24.040]   stance is like autonomous driving, still a lot of research problems to be solved.
[00:10:24.040 --> 00:10:25.040]   A lot of research.
[00:10:25.040 --> 00:10:27.120]   So you want to optimize for the bottleneck.
[00:10:27.120 --> 00:10:30.000]   It's something very well in Toyota production system, this theory of constraints.
[00:10:30.000 --> 00:10:32.600]   You look at your workflow, where's the bottleneck?
[00:10:32.600 --> 00:10:36.780]   And optimizing the rest doesn't really matter because it's still the bottleneck that governs
[00:10:36.780 --> 00:10:38.500]   the speed at which you iterate.
[00:10:38.500 --> 00:10:40.760]   So we found that experimenting was the bottleneck.
[00:10:40.760 --> 00:10:45.720]   And now production is not a bottleneck anymore because there's great tools like Onyx.
[00:10:45.720 --> 00:10:51.120]   We're using Onyx, we're using TensorRT as part of our tool chain to deploy models that
[00:10:51.120 --> 00:10:54.040]   are efficiently running on GPUs, et cetera.
[00:10:54.040 --> 00:10:58.320]   There's even more recent projects on NVIDIA like TRTorch, which enables you to go directly
[00:10:58.320 --> 00:11:00.480]   from PyTorch to TensorRT.
[00:11:00.480 --> 00:11:02.400]   And there's many more.
[00:11:02.400 --> 00:11:08.600]   Beyond NVIDIA hardware, there's exciting cross-compilation tools, things like the TVM stack, et cetera,
[00:11:08.600 --> 00:11:09.600]   et cetera.
[00:11:09.600 --> 00:11:15.400]   So production-wise, I think it's such a big deal to deploy models that if the second top
[00:11:15.400 --> 00:11:18.680]   framework or the top two frameworks don't have good solutions for that, they're doomed
[00:11:18.680 --> 00:11:19.680]   to fail.
[00:11:19.680 --> 00:11:22.120]   So they understood this a long time ago and it's good now.
[00:11:22.120 --> 00:11:24.600]   And so you're saying today though, you actually can do it.
[00:11:24.600 --> 00:11:25.600]   Like you can get it into production.
[00:11:25.600 --> 00:11:26.600]   It's not a problem.
[00:11:26.600 --> 00:11:27.600]   Oh yeah.
[00:11:27.600 --> 00:11:28.600]   Oh yeah.
[00:11:28.600 --> 00:11:29.600]   I mean, we could do it before.
[00:11:29.600 --> 00:11:34.640]   It's just not necessarily very nice production engineering, but now there are tools to do
[00:11:34.640 --> 00:11:36.560]   this in a really state-of-the-art way.
[00:11:36.560 --> 00:11:40.040]   Not just by researcher standards, but by proper engineering standards.
[00:11:40.040 --> 00:11:41.040]   Right.
[00:11:41.040 --> 00:11:42.040]   Right.
[00:11:42.040 --> 00:11:44.280]   I feel a little reticent to ask you this question because probably everyone asks you this.
[00:11:44.280 --> 00:11:48.400]   This is what my parents ask me, but in your view, since you're at the front lines, what
[00:11:48.400 --> 00:11:49.800]   is the state of self-driving cars?
[00:11:49.800 --> 00:11:54.880]   I feel like everyone talks about it, yet I can't get into a car and tell it to drive
[00:11:54.880 --> 00:11:55.880]   me somewhere and have it do it.
[00:11:55.880 --> 00:12:00.160]   On the other hand, I live in San Francisco and I see these cars driving around autonomously
[00:12:00.160 --> 00:12:01.160]   all the time.
[00:12:01.160 --> 00:12:02.160]   Right.
[00:12:02.160 --> 00:12:03.160]   What's going on?
[00:12:03.160 --> 00:12:04.160]   Yeah, that's a good question.
[00:12:04.160 --> 00:12:05.160]   Right?
[00:12:05.160 --> 00:12:06.160]   So that's a standard question.
[00:12:06.160 --> 00:12:09.320]   That's a question everybody should ask themselves every six months or so.
[00:12:09.320 --> 00:12:10.720]   And the question is for how long?
[00:12:10.720 --> 00:12:11.720]   That I don't know.
[00:12:11.720 --> 00:12:12.720]   I can't predict the future.
[00:12:12.720 --> 00:12:18.640]   But I think that one thing that attracted me when I came to TRI was I was just surprised
[00:12:18.640 --> 00:12:22.240]   how much people thought it was solved.
[00:12:22.240 --> 00:12:26.640]   Back in 2016, when I really started working on autonomous driving, it was kind of like
[00:12:26.640 --> 00:12:29.840]   as a researcher working in computer vision and machine learning, I was like, I'm excited
[00:12:29.840 --> 00:12:32.520]   about a lot of exciting problems.
[00:12:32.520 --> 00:12:35.500]   How do we leverage the fact that labeling is expensive?
[00:12:35.500 --> 00:12:39.480]   So we want to optimize the label efficiency, maybe even go self-supervised and these kinds
[00:12:39.480 --> 00:12:40.480]   of things.
[00:12:40.480 --> 00:12:42.720]   And I was starting at this period or using simulation.
[00:12:42.720 --> 00:12:46.040]   So one of the big things I've done is leveraging simulation.
[00:12:46.040 --> 00:12:48.280]   And I was like, wow, there's so many open research challenges.
[00:12:48.280 --> 00:12:52.240]   It's so cool as a researcher, I have a huge playground and a huge societal motivation
[00:12:52.240 --> 00:12:53.240]   to actually solve.
[00:12:53.240 --> 00:12:58.320]   There's 1.35 million traffic fatalities every year on the road.
[00:12:58.320 --> 00:13:01.040]   So I was like, this is a huge societal problem.
[00:13:01.040 --> 00:13:03.720]   It's super important to solve that because it's 1.35 million.
[00:13:03.720 --> 00:13:05.120]   It's just crazy.
[00:13:05.120 --> 00:13:08.280]   But the reason that it's so high is because it's so hard.
[00:13:08.280 --> 00:13:10.680]   And so it's super hard problem, super important.
[00:13:10.680 --> 00:13:11.920]   And there's so many research problems.
[00:13:11.920 --> 00:13:13.480]   So as a researcher, super excited.
[00:13:13.480 --> 00:13:16.640]   Move to Bay Area, everybody's like, it's in six months.
[00:13:16.640 --> 00:13:18.040]   In six months, I got this.
[00:13:18.040 --> 00:13:23.720]   You know, everybody from this little startup to the big companies to OEMs, everybody was
[00:13:23.720 --> 00:13:25.880]   like coming up with dates.
[00:13:25.880 --> 00:13:28.040]   2018, we got this.
[00:13:28.040 --> 00:13:29.840]   2016, 18, 19, 20.
[00:13:29.840 --> 00:13:33.720]   You name it, go back in 2016 and listen to any announcement or et cetera.
[00:13:33.720 --> 00:13:38.600]   You will see everybody promised everything, every time.
[00:13:38.600 --> 00:13:40.760]   And it's to get VC money and everything like this.
[00:13:40.760 --> 00:13:42.960]   I know it's Bay Area, I get funding.
[00:13:42.960 --> 00:13:47.320]   But the stance, like Gil Pratt, our CEO, which is a former DARPA director, he was an MIT
[00:13:47.320 --> 00:13:48.320]   professor and everything.
[00:13:48.320 --> 00:13:51.720]   So he's very, very smart and an excellent roboticist.
[00:13:51.720 --> 00:13:55.040]   And he had always a deep appreciation for the problems.
[00:13:55.040 --> 00:13:56.720]   He was at Bell Labs and all kinds of things.
[00:13:56.720 --> 00:13:59.440]   And so it's always been like, it's much harder than people think.
[00:13:59.440 --> 00:14:01.240]   It's going to take much longer than people think.
[00:14:01.240 --> 00:14:06.120]   And therefore, if you're serious about it, you should be committing long-term resources
[00:14:06.120 --> 00:14:07.840]   and treat it as a research problem.
[00:14:07.840 --> 00:14:11.160]   So we are research, research is our middle name, like John Leonard, a famous robotics
[00:14:11.160 --> 00:14:14.600]   professors that is one of our VPs always says that.
[00:14:14.600 --> 00:14:15.720]   So it's going to take a while.
[00:14:15.720 --> 00:14:17.160]   It's going to take a while.
[00:14:17.160 --> 00:14:19.920]   And people are now coming to this realization.
[00:14:19.920 --> 00:14:25.240]   Because in spite of all the hype and everything, when the results are not there at the given
[00:14:25.240 --> 00:14:27.640]   time, well, you have to face the facts.
[00:14:27.640 --> 00:14:30.760]   And so now what we're seeing is we're seeing a consolidation in the field.
[00:14:30.760 --> 00:14:34.200]   Because people that are really committed to this problem long-term, they're willing to
[00:14:34.200 --> 00:14:37.720]   sink in the money, the time, et cetera, and maybe open their minds a little bit to, hey,
[00:14:37.720 --> 00:14:38.720]   it's research.
[00:14:38.720 --> 00:14:42.000]   So we, for instance, need strong partnerships with academia, which we work a lot with Stanford,
[00:14:42.000 --> 00:14:44.580]   MIT, and University of Michigan for those reasons.
[00:14:44.580 --> 00:14:46.200]   We don't know all the answers.
[00:14:46.200 --> 00:14:50.120]   So we got to work with people who also don't know the answers, but can take the scientific
[00:14:50.120 --> 00:14:53.880]   approach to try to figure them out versus just say, it's solved.
[00:14:53.880 --> 00:14:58.040]   We just need to throw 100 code monkeys or 1,000 code monkeys or 10,000 code monkeys
[00:14:58.040 --> 00:14:59.720]   at it, and it's going to work.
[00:14:59.720 --> 00:15:01.320]   I think that's not the case.
[00:15:01.320 --> 00:15:06.240]   And even the engineers at these companies are actually doing a fair amount of research,
[00:15:06.240 --> 00:15:10.160]   even in the engineering-heavy companies, I think.
[00:15:10.160 --> 00:15:12.920]   I was telling our Slack community that I was going to interview you and asking them if
[00:15:12.920 --> 00:15:15.160]   they had any questions they wanted to ask.
[00:15:15.160 --> 00:15:18.320]   And I thought one of the really good ones was, it's a little bit general, but you're
[00:15:18.320 --> 00:15:22.720]   kind of alluding to it, is what are the big academic advances coming that'll kind of change
[00:15:22.720 --> 00:15:24.640]   the game for self-driving cars?
[00:15:24.640 --> 00:15:28.080]   You seem like the perfect person to have a perspective on this.
[00:15:28.080 --> 00:15:32.720]   One thing that I'm particularly excited about and that I've been doing some work on is differentiable
[00:15:32.720 --> 00:15:33.720]   rendering.
[00:15:33.720 --> 00:15:36.400]   There's this huge, ambitious vision.
[00:15:36.400 --> 00:15:41.240]   I think the academic professor that I think embodies this the best is probably Josh Denenbaum
[00:15:41.240 --> 00:15:42.240]   at MIT.
[00:15:42.240 --> 00:15:45.080]   He's this really, really amazing professor.
[00:15:45.080 --> 00:15:48.640]   If you don't know about him, just check out his research.
[00:15:48.640 --> 00:15:53.320]   And him and his student, Jiajun Wu, who is now a professor at Stanford, we're actually
[00:15:53.320 --> 00:15:57.520]   discussing with them and they have super cool ideas around this Vision as Inverse Graphics
[00:15:57.520 --> 00:15:59.160]   program.
[00:15:59.160 --> 00:16:02.040]   And I think that's really the right way to frame the problem.
[00:16:02.040 --> 00:16:08.400]   Alan Uriel, another really interesting professor, was basically calling this analysis by synthesis.
[00:16:08.400 --> 00:16:13.440]   So the idea is that what you want to do is, with deep learning right now, which is fully
[00:16:13.440 --> 00:16:18.200]   supervised, is just you're learning a function that says, here's an image.
[00:16:18.200 --> 00:16:20.360]   You say jump, I say how high, right?
[00:16:20.360 --> 00:16:22.480]   It's like, here's an image, cat, dog.
[00:16:22.480 --> 00:16:23.680]   Just say cat or dog.
[00:16:23.680 --> 00:16:25.840]   Cat, wrong, that's a dog.
[00:16:25.840 --> 00:16:29.160]   And you do that thousands and thousands of times, right?
[00:16:29.160 --> 00:16:32.400]   It's not unlike how I was teaching my daughter colors.
[00:16:32.400 --> 00:16:36.600]   It's like red, yellow, no, it's red, blue, no, it's red.
[00:16:36.600 --> 00:16:40.680]   And you do this, then it kind of exponentially takes off and they become much smarter in
[00:16:40.680 --> 00:16:41.680]   their learning.
[00:16:41.680 --> 00:16:45.200]   But this initial phase of learning, which is rote memorization, this is how deep learning
[00:16:45.200 --> 00:16:46.200]   works.
[00:16:46.200 --> 00:16:51.080]   The problem with that is that interpretability, data costs, lots of problems around that.
[00:16:51.080 --> 00:16:55.680]   And so for Vision, what's interesting is the world has structure, right?
[00:16:55.680 --> 00:16:58.280]   And there's physics, like Newton existed.
[00:16:58.280 --> 00:17:01.440]   There's physics of, there's gravity, there's physics of light.
[00:17:01.440 --> 00:17:06.120]   There's a lot of inductive biases that you can leverage.
[00:17:06.120 --> 00:17:12.400]   You can take basically this physics and physics laws, and then try to bake it into your learning
[00:17:12.400 --> 00:17:13.520]   approach.
[00:17:13.520 --> 00:17:16.280]   And differentiable rendering or inverse graphics is one way to do it.
[00:17:16.280 --> 00:17:21.440]   So basically, you take your sensor, you're trying to deconstruct the world and re-synthesize
[00:17:21.440 --> 00:17:23.040]   it.
[00:17:23.040 --> 00:17:27.440]   And that way you can compare you in a self-supervised way, like what you reconstructed from what
[00:17:27.440 --> 00:17:29.280]   you observed.
[00:17:29.280 --> 00:17:33.440]   And the benefit of that is that you get systems that generalize much better, that can be trained
[00:17:33.440 --> 00:17:36.780]   on arbitrary amounts of raw data, don't need labels.
[00:17:36.780 --> 00:17:39.600]   And they also have some interpretability to them.
[00:17:39.600 --> 00:17:41.040]   They have some structure, right?
[00:17:41.040 --> 00:17:45.940]   Because they're deconstructing the world and following some structure, et cetera.
[00:17:45.940 --> 00:17:49.000]   So differentiable rendering is a big, big one for me.
[00:17:49.000 --> 00:17:50.280]   Vision as inverse graphics is a big one.
[00:17:50.280 --> 00:17:51.960]   And there's many others.
[00:17:51.960 --> 00:17:54.640]   Self-supervised learning in general is something I'm very excited about.
[00:17:54.640 --> 00:17:56.280]   And it goes beyond just differentiable rendering.
[00:17:56.280 --> 00:17:59.040]   There's many other ways to leverage self-supervision, especially time.
[00:17:59.040 --> 00:18:01.840]   When you look at video, the temporal dynamics.
[00:18:01.840 --> 00:18:04.020]   Contrastive learning is a super hot topic right now.
[00:18:04.020 --> 00:18:07.700]   And there's interesting work, I think from Max Welling's lab, called the Contrastive
[00:18:07.700 --> 00:18:08.700]   Structured World Models.
[00:18:08.700 --> 00:18:11.360]   That I think is a cool paper.
[00:18:11.360 --> 00:18:17.000]   Not really super applicable right now, but I think an exciting idea.
[00:18:17.000 --> 00:18:18.440]   And yeah, I would just leave it at that.
[00:18:18.440 --> 00:18:20.040]   Vision as inverse graphics, self-supervised learning.
[00:18:20.040 --> 00:18:21.320]   I'm super stoked about that.
[00:18:21.320 --> 00:18:23.560]   I hadn't heard of contrastive learning before.
[00:18:23.560 --> 00:18:24.560]   Could you describe that briefly?
[00:18:24.560 --> 00:18:28.000]   You did such a good job with the other topic.
[00:18:28.000 --> 00:18:32.960]   Well, I mean, so overall, in a simple way, I would say that contrastive learning...
[00:18:32.960 --> 00:18:36.600]   So there's a really cool paper that I can recommend everybody to read, which is the
[00:18:36.600 --> 00:18:39.440]   paper from Godfather of Deep Learning, Geoff Hinton.
[00:18:39.440 --> 00:18:41.840]   It's called SimClear, SimCLR.
[00:18:41.840 --> 00:18:43.800]   And it explains a little bit in...
[00:18:43.800 --> 00:18:44.800]   It got state-of-the-art results.
[00:18:44.800 --> 00:18:49.480]   Basically, there's two big approaches in contrastive learning that work really well, is SimCLR
[00:18:49.480 --> 00:18:53.760]   and MoCo from FAIR, Kai Ming He, another super impressive researcher.
[00:18:53.760 --> 00:18:56.280]   And the basic idea is, it's some form of metric learning, if you want.
[00:18:56.280 --> 00:19:03.000]   It's like, you basically want to learn a representation that verifies some ordering property or some
[00:19:03.000 --> 00:19:04.520]   distance property.
[00:19:04.520 --> 00:19:09.280]   Something like, here's a traditional way would be, here is an example, here's one that is
[00:19:09.280 --> 00:19:12.280]   close to it, and here's one that is far from it.
[00:19:12.280 --> 00:19:16.600]   And what you want is you want to learn the properties of your representation such that
[00:19:16.600 --> 00:19:18.920]   this is true, and in a very simple way.
[00:19:18.920 --> 00:19:22.360]   And in general, it's related to metric learning in general way.
[00:19:22.360 --> 00:19:26.160]   But the cool thing is that, for instance, in this C-SWIM paper, Contrastive Structure
[00:19:26.160 --> 00:19:30.260]   World Models paper that I was mentioning, you can look at it as temporal dynamics is
[00:19:30.260 --> 00:19:31.260]   one way.
[00:19:31.260 --> 00:19:34.560]   Things that are close in time should be close in representation in feature space, and things
[00:19:34.560 --> 00:19:36.320]   that are far should be further away.
[00:19:36.320 --> 00:19:37.440]   It's not always true.
[00:19:37.440 --> 00:19:42.920]   And actually, we have an ongoing work with Stanford, a paper called CoCon, Cooperative
[00:19:42.920 --> 00:19:48.400]   Contrastive Learning, where the idea is, in some cases, in videos, things repeat themselves.
[00:19:48.400 --> 00:19:53.520]   And so you want to basically leverage multi-view relationships such that you know that the
[00:19:53.520 --> 00:19:55.840]   same thing in multiple views should also be close.
[00:19:55.840 --> 00:19:59.600]   So it's not just contrastive learning, but also cooperative.
[00:19:59.600 --> 00:20:00.760]   But it's an exploding field.
[00:20:00.760 --> 00:20:02.680]   There's so much work on that.
[00:20:02.680 --> 00:20:06.000]   The cool thing about it, it seemed clear, et cetera, was shown that you can replace
[00:20:06.000 --> 00:20:11.360]   pre-training on the large label dataset, like ImageNet, by just doing unsupervised pre-training
[00:20:11.360 --> 00:20:13.080]   with contrastive loss.
[00:20:13.080 --> 00:20:15.280]   Wow, super cool.
[00:20:15.280 --> 00:20:19.680]   And in practice, it's a big deal because, for instance, we can't use ImageNet to deploy
[00:20:19.680 --> 00:20:20.680]   a product.
[00:20:20.680 --> 00:20:23.760]   So if you're wondering, "Oh, I can just easily take an ImageNet pre-trained model, get a
[00:20:23.760 --> 00:20:26.720]   few labels, a few shots, transfer, and use it."
[00:20:26.720 --> 00:20:30.560]   Well, for production, you can't really do that unless you have a license, a commercial
[00:20:30.560 --> 00:20:32.000]   license or something like this.
[00:20:32.000 --> 00:20:38.080]   So being able to do unsupervised pre-training, which was one of the early days, early inspirations
[00:20:38.080 --> 00:20:40.960]   of deep learning, with restricted Boltzmann machines and whatever.
[00:20:40.960 --> 00:20:45.160]   You want to do unsupervised pre-training with a lot of data for a lot of time, and then
[00:20:45.160 --> 00:20:49.880]   very quickly fine tune with a few shots setting, a few labels.
[00:20:49.880 --> 00:20:51.840]   And it seems like we're there now.
[00:20:51.840 --> 00:20:52.840]   Very cool.
[00:20:52.840 --> 00:20:56.920]   All right, switching gears a little bit, I just want to make sure I ask you this question
[00:20:56.920 --> 00:21:01.040]   because you were telling me that you listened to our interview with Anantha, who's a VP
[00:21:01.040 --> 00:21:02.840]   of engineering at Lyft.
[00:21:02.840 --> 00:21:07.000]   And I think he brings maybe a different company's perspective, but maybe also a different...
[00:21:07.000 --> 00:21:10.000]   He kind of came up through engineering and thinks of himself as an engineer.
[00:21:10.000 --> 00:21:11.000]   Right, right, right.
[00:21:11.000 --> 00:21:15.920]   And I was kind of wondering how your answers for the same questions about taking autonomous
[00:21:15.920 --> 00:21:19.360]   vehicle to market would differ from what he said.
[00:21:19.360 --> 00:21:24.600]   One thing that I take from what he said was he talked a lot about the organizational aspect.
[00:21:24.600 --> 00:21:27.720]   I think that was really interesting because when you think about engineering and you think
[00:21:27.720 --> 00:21:34.120]   about the problem like self-driving cars, it's not a one man or 10 men team, right?
[00:21:34.120 --> 00:21:36.680]   It's not 10 people effort.
[00:21:36.680 --> 00:21:41.480]   So the challenge is it requires a lot of people and a coordination of a lot of people.
[00:21:41.480 --> 00:21:47.080]   Also it's a robotics problem that is pretty wide in the skill set that it requires.
[00:21:47.080 --> 00:21:52.560]   You have some people like hardware, we have amazing hardware people at TRI, which always
[00:21:52.560 --> 00:21:57.040]   impresses me because I can't use a solder iron, even if you put a gun to my head.
[00:21:57.040 --> 00:21:59.100]   But these guys, they are magicians.
[00:21:59.100 --> 00:22:04.440]   So we have really good hardware people, you have cloud people, you have all kinds of different
[00:22:04.440 --> 00:22:05.440]   skills.
[00:22:05.440 --> 00:22:09.440]   And one thing that I remember was in the podcast was ML is a skill, right?
[00:22:09.440 --> 00:22:12.440]   ML is a skill that is to be shared with everybody.
[00:22:12.440 --> 00:22:18.360]   And so that's why it's kind of diffused in the company to be successful at deploying
[00:22:18.360 --> 00:22:19.360]   this.
[00:22:19.360 --> 00:22:21.040]   I think that's a really good point.
[00:22:21.040 --> 00:22:22.040]   I agree.
[00:22:22.040 --> 00:22:26.980]   I would add something to it, which is because I lead a machine learning team, right?
[00:22:26.980 --> 00:22:30.800]   So there is such a thing as, so even though it's a skill and it should be everybody has
[00:22:30.800 --> 00:22:33.880]   it, I actually lead a team called machine learning, right?
[00:22:33.880 --> 00:22:35.000]   Machine learning research.
[00:22:35.000 --> 00:22:38.720]   And so if it's a skill and it's diffused, why have a team that's like this?
[00:22:38.720 --> 00:22:43.240]   And we iterated through a couple of models of like, oh, we're kind of like experts and
[00:22:43.240 --> 00:22:46.180]   then teams can basically request projects where we help.
[00:22:46.180 --> 00:22:51.040]   So we kind of like embedded in other teams, but that was not necessarily super successful.
[00:22:51.040 --> 00:22:55.400]   So we basically got back to, we do our own projects and we try to then seed some kind
[00:22:55.400 --> 00:22:59.200]   of like more crazy ML projects that other teams then can carry forward.
[00:22:59.200 --> 00:23:03.560]   So in terms of bringing it to markets, for me, this is the organizational challenge.
[00:23:03.560 --> 00:23:07.840]   I know it's kind of maybe not a typical answer, but I think because he insisted on that, I
[00:23:07.840 --> 00:23:13.080]   think this is really good to, there's something called the Conway's law, which is an organization
[00:23:13.080 --> 00:23:18.400]   that produces software, tends to produce software that's structured like the organization.
[00:23:18.400 --> 00:23:21.600]   So if you have a, typically like in self-driving cars, you have a perception team, you have
[00:23:21.600 --> 00:23:24.400]   a prediction team, you have a planning team, or you have a perception module, you have
[00:23:24.400 --> 00:23:26.920]   a prediction module and then you have a planning model, right?
[00:23:26.920 --> 00:23:30.280]   And then you have the whole kinds of challenges as a manager, which I discovered, which is
[00:23:30.280 --> 00:23:33.880]   like siloing, communication across teams, all these kinds of things.
[00:23:33.880 --> 00:23:38.680]   And as an ML person that's leading an ML team, what I found difficult is that in ML, you
[00:23:38.680 --> 00:23:43.040]   want the holy grail for self-driving cars is that they improve your experience.
[00:23:43.040 --> 00:23:47.840]   And I think that's one of the biggest misconceptions that people have about learning.
[00:23:47.840 --> 00:23:51.120]   If you chat with like people like your grandma or whatever, like about like learning and
[00:23:51.120 --> 00:23:58.040]   you explain them the high level concept, what they immediately think is that the robot learns
[00:23:58.040 --> 00:23:59.640]   after deployment, right?
[00:23:59.640 --> 00:24:04.200]   So you kind of like your self-driving car might be dumb when you buy it, but it's going
[00:24:04.200 --> 00:24:06.960]   to become smarter because you're going to teach it.
[00:24:06.960 --> 00:24:08.680]   And that's what machine learning is.
[00:24:08.680 --> 00:24:10.320]   And that's not at all what it is, right?
[00:24:10.320 --> 00:24:11.960]   That's not at all how it works, right?
[00:24:11.960 --> 00:24:12.960]   There's a duty cycle.
[00:24:12.960 --> 00:24:18.720]   There's an operator, like you retrieve data, you look at data, you label it, you test it,
[00:24:18.720 --> 00:24:19.720]   and then you deploy it.
[00:24:19.720 --> 00:24:21.760]   And this can take a long time, right?
[00:24:21.760 --> 00:24:27.000]   So this is on some huge timescale, this might be true, but on the short timescale, it's
[00:24:27.000 --> 00:24:28.520]   absolutely not true.
[00:24:28.520 --> 00:24:31.980]   So the iteration speed is the key.
[00:24:31.980 --> 00:24:36.240]   And this is the challenges with this organization around perception, prediction planning, makes
[00:24:36.240 --> 00:24:41.200]   it very difficult to have the whole system optimized really quickly from use.
[00:24:41.200 --> 00:24:44.720]   And so I think that's the major bottleneck for me as a machine learning person, which
[00:24:44.720 --> 00:24:50.200]   is if driving from demonstrations like user experience and things like this, how can we
[00:24:50.200 --> 00:24:54.760]   make every system as quickly improving as possible?
[00:24:54.760 --> 00:24:58.720]   And this is this idea that we're very big on at TRI called fleet learning, right?
[00:24:58.720 --> 00:25:03.640]   Which we don't care just for cars, but for home robots in general, is like we have millions
[00:25:03.640 --> 00:25:09.440]   of evolutions, millions of years of evolution, plus decades of parental education.
[00:25:09.440 --> 00:25:13.240]   And machines like a car doesn't have that leisure, right?
[00:25:13.240 --> 00:25:18.960]   Nobody would buy a Toyota if they had to say, "All right, I buy six months old, and then
[00:25:18.960 --> 00:25:22.360]   I have to tolerate all kinds of destructions like we were talking about just before recording."
[00:25:22.360 --> 00:25:24.100]   No, no way, right?
[00:25:24.100 --> 00:25:27.000]   No way people would buy a car like that or a robot like that, right?
[00:25:27.000 --> 00:25:29.360]   That destroys half the home and then say, "Oh, it's okay.
[00:25:29.360 --> 00:25:31.240]   It's learning."
[00:25:31.240 --> 00:25:33.000]   So we got to speed things up, right?
[00:25:33.000 --> 00:25:36.680]   So the learning has to be much more accelerated for machines than it is for humans.
[00:25:36.680 --> 00:25:38.960]   And the only way to do that is parallelism.
[00:25:38.960 --> 00:25:42.920]   And so fleet learning is something we're very, very big on for that purpose.
[00:25:42.920 --> 00:25:48.320]   So fleet learning and end-to-end system level optimization and the right organization to
[00:25:48.320 --> 00:25:53.640]   match behind, I would say are the three big bottlenecks to deploy any robotic system.
[00:25:53.640 --> 00:25:54.640]   Interesting.
[00:25:54.640 --> 00:25:58.920]   So I guess I kind of think of machine learning as primarily helping with perception.
[00:25:58.920 --> 00:26:00.600]   Am I wrong on that?
[00:26:00.600 --> 00:26:03.200]   Do you view machine learning as something that goes everywhere in the-
[00:26:03.200 --> 00:26:04.520]   Yeah, so both.
[00:26:04.520 --> 00:26:10.520]   So yes, you're right that today perception is the main application for machine learning,
[00:26:10.520 --> 00:26:12.040]   at least in robotics.
[00:26:12.040 --> 00:26:14.920]   The reason for it is because there's just no way around it.
[00:26:14.920 --> 00:26:16.880]   Like ImageNet competition is kind of funny.
[00:26:16.880 --> 00:26:21.040]   So one of my mentors and one of the people I admire the most is called Florent Peronin.
[00:26:21.040 --> 00:26:23.880]   And he was the head of the computer vision lab at Zero-Earth Research.
[00:26:23.880 --> 00:26:26.800]   And he won the ImageNet challenge before deep learning.
[00:26:26.800 --> 00:26:30.200]   And in the year of deep learning, people say like, "Oh, in deep learning, half the error
[00:26:30.200 --> 00:26:31.200]   rate."
[00:26:31.200 --> 00:26:32.200]   Well, they have the error rate of Florent.
[00:26:32.200 --> 00:26:35.980]   Which he improved, like every year was improving 2% extra.
[00:26:35.980 --> 00:26:37.840]   So there's some kind of inevitability to it.
[00:26:37.840 --> 00:26:39.920]   And again, Florent became like, "You're really good at this."
[00:26:39.920 --> 00:26:45.760]   And the lab and we all got into deep learning because again, we face the evidence as scientists.
[00:26:45.760 --> 00:26:48.280]   So it's inevitable because it works so much better.
[00:26:48.280 --> 00:26:49.800]   And also because there's no other way.
[00:26:49.800 --> 00:26:54.920]   You cannot engineer a world model because you do this and then you say like, "Oh, these
[00:26:54.920 --> 00:26:56.280]   are the labels I need.
[00:26:56.280 --> 00:26:57.960]   These are the features I need."
[00:26:57.960 --> 00:26:59.040]   And all these kinds of things.
[00:26:59.040 --> 00:27:00.400]   And then the world constantly changes.
[00:27:00.400 --> 00:27:02.080]   The world is non-stationary.
[00:27:02.080 --> 00:27:03.500]   Then you have like scooters.
[00:27:03.500 --> 00:27:07.480]   You have like literally humans flying at 30 miles per hour on the streets.
[00:27:07.480 --> 00:27:08.480]   And you're like, "Wait, what?
[00:27:08.480 --> 00:27:09.480]   Is that a pedestrian?
[00:27:09.480 --> 00:27:10.480]   Is that a motorcycle?
[00:27:10.480 --> 00:27:11.480]   Is that a bird?
[00:27:11.480 --> 00:27:12.480]   Is that Superman?
[00:27:12.480 --> 00:27:13.480]   Like what the hell?"
[00:27:13.480 --> 00:27:16.760]   And so it's inevitable and it works so much better.
[00:27:16.760 --> 00:27:18.680]   So for perception, it's no brainer.
[00:27:18.680 --> 00:27:24.280]   Even the most hardcore, like feature engineering, passionate people or people that believe there's
[00:27:24.280 --> 00:27:26.440]   an equation for everything.
[00:27:26.440 --> 00:27:30.740]   Nobody I know argues that this is the wrong approach to perception.
[00:27:30.740 --> 00:27:33.440]   But it's not the solution either.
[00:27:33.440 --> 00:27:36.200]   It's not like a slam dunk either because we need to go beyond that.
[00:27:36.200 --> 00:27:38.880]   So I would say robust perception is not solved.
[00:27:38.880 --> 00:27:42.640]   Some form of perception when you know everything, et cetera, and you don't care for these nine
[00:27:42.640 --> 00:27:46.120]   nines of reliability, you can get really, really far.
[00:27:46.120 --> 00:27:52.560]   But uncertainty modeling, handling false positives and all these kinds of things, that's a really
[00:27:52.560 --> 00:27:53.560]   hard problem.
[00:27:53.560 --> 00:27:55.680]   So that's why like machine learning, every abstraction is leaky.
[00:27:55.680 --> 00:27:59.800]   So going back to PyTorch, that's why I like minimizing abstractions because any abstraction
[00:27:59.800 --> 00:28:00.800]   is leaky.
[00:28:00.800 --> 00:28:04.920]   And the problem with the modular robotics stacks, like perception, prediction, planning,
[00:28:04.920 --> 00:28:06.640]   is that you're making abstractions.
[00:28:06.640 --> 00:28:08.120]   You're making APIs.
[00:28:08.120 --> 00:28:11.280]   And the contracts you're making is like if you think microservices type of things, they're
[00:28:11.280 --> 00:28:13.280]   all statistical in nature.
[00:28:13.280 --> 00:28:17.160]   So you're kind of saying, "I'm going to give you something that I'm calling a red traffic
[00:28:17.160 --> 00:28:18.480]   light.
[00:28:18.480 --> 00:28:21.360]   And I'm confident that 99% of the time I'm right.
[00:28:21.360 --> 00:28:23.400]   What happens during this 1%?
[00:28:23.400 --> 00:28:26.220]   You're on your own."
[00:28:26.220 --> 00:28:29.400]   And it's unavoidable because no system will ever be perfect.
[00:28:29.400 --> 00:28:30.800]   And you shouldn't require a robot to be perfect.
[00:28:30.800 --> 00:28:33.120]   It needs to be better than a human, but it doesn't need to be perfect.
[00:28:33.120 --> 00:28:34.580]   Otherwise, you will never ship.
[00:28:34.580 --> 00:28:40.680]   So how do you robustly handle uncertainty and how does it propagate through each layer?
[00:28:40.680 --> 00:28:44.360]   And how do you think statistically versus logically or symbolically?
[00:28:44.360 --> 00:28:49.680]   And that's becoming harder and harder as you move from perception to prediction to planning,
[00:28:49.680 --> 00:28:52.280]   because then planning is actually reasoning.
[00:28:52.280 --> 00:28:56.220]   It's search, it's reasoning, it's a higher order cognitive function in a sense.
[00:28:56.220 --> 00:29:03.200]   And manipulating just feature vectors, like esoteric feature vectors, it's not really
[00:29:03.200 --> 00:29:04.760]   how it works.
[00:29:04.760 --> 00:29:10.040]   So this neural symbolic system, the best of both worlds, like I know Marco Pavone is an
[00:29:10.040 --> 00:29:13.640]   awesome Stanford professor that's doing cool research on that.
[00:29:13.640 --> 00:29:16.720]   How do you combine deep learning with more logical forms of reasoning?
[00:29:16.720 --> 00:29:19.560]   Something also we're looking at at TRI a little bit.
[00:29:19.560 --> 00:29:20.560]   So how does it work today?
[00:29:20.560 --> 00:29:23.240]   Do you actually send more information?
[00:29:23.240 --> 00:29:26.640]   I feel like other people that I've talked to have talked about not just sending the
[00:29:26.640 --> 00:29:31.240]   output of the perception algorithm, but maybe even some of the activations of the parts
[00:29:31.240 --> 00:29:33.640]   of the neural network before the output.
[00:29:33.640 --> 00:29:38.440]   But then I wonder, what do you do with that downstream in a logical system?
[00:29:38.440 --> 00:29:40.680]   So how does TRI handle that?
[00:29:40.680 --> 00:29:41.680]   Right.
[00:29:41.680 --> 00:29:43.840]   So actually that's not the approach we're taking, because you're right.
[00:29:43.840 --> 00:29:46.880]   It's kind of like, yeah, so you're just pushing the thing under the rug.
[00:29:46.880 --> 00:29:47.880]   It's like hot potato game.
[00:29:47.880 --> 00:29:50.040]   It's like, "Oh, I don't have to solve this problem.
[00:29:50.040 --> 00:29:51.040]   There you go."
[00:29:51.040 --> 00:29:52.320]   And typically it doesn't really work well across teams.
[00:29:52.320 --> 00:29:55.560]   It's like, "I don't know if this is going to work, but it's your job now."
[00:29:55.560 --> 00:30:00.500]   So my personal holy grail is building an end-to-end differentiable, but modular system.
[00:30:00.500 --> 00:30:03.920]   So still engineering what you know, but learning what you don't.
[00:30:03.920 --> 00:30:06.120]   And so what it means is that you still have a perception module.
[00:30:06.120 --> 00:30:10.560]   It still outputs some concepts, like, "Oh, this is a person.
[00:30:10.560 --> 00:30:11.560]   Persons exist.
[00:30:11.560 --> 00:30:12.560]   Roads exist.
[00:30:12.560 --> 00:30:13.560]   We know this."
[00:30:13.560 --> 00:30:17.680]   So the problem is that we're unsure whether our inference about them is right.
[00:30:17.680 --> 00:30:21.720]   So here, and my boss Wolfram Burghardt, which is one of the legends of robotics, because
[00:30:21.720 --> 00:30:25.600]   he wrote this book called Probabilistic Robotics with Sebastian Thrun and Dieter Fox, and created
[00:30:25.600 --> 00:30:28.240]   this whole movement.
[00:30:28.240 --> 00:30:32.700]   One thing we discuss very often with Wolfram is there shouldn't be an argmax.
[00:30:32.700 --> 00:30:38.880]   If you have an argmax in the middle somewhere upstream, you're basically destroying uncertainty.
[00:30:38.880 --> 00:30:41.480]   You're just forgetting any uncertainty you have.
[00:30:41.480 --> 00:30:45.080]   And what's really interesting is that this is coming from a theoretical perspective,
[00:30:45.080 --> 00:30:50.280]   but from, again, an organizational perspective, if you are the planning team and I give you
[00:30:50.280 --> 00:30:55.400]   something from perception, and I give you, "This is a red traffic light, and I'm wrong,"
[00:30:55.400 --> 00:30:57.940]   you're going to be saying, "Hey, we crashed.
[00:30:57.940 --> 00:30:58.940]   It's your fault.
[00:30:58.940 --> 00:30:59.940]   You're wrong.
[00:30:59.940 --> 00:31:00.940]   Fix it."
[00:31:01.180 --> 00:31:05.380]   You're going to say, "Oh, but I cannot always be right," and you will be, "La, la, la, la,
[00:31:05.380 --> 00:31:06.380]   la."
[00:31:06.380 --> 00:31:07.380]   So this is not how it works.
[00:31:07.380 --> 00:31:11.260]   So the things you pass, every data structure that you pass, every information that you
[00:31:11.260 --> 00:31:14.460]   pass is a distribution.
[00:31:14.460 --> 00:31:15.460]   It's probabilistic in nature.
[00:31:15.460 --> 00:31:18.940]   I know I'll sound like a Bayesian crazy guy, but I'm not a Bayesian guy.
[00:31:18.940 --> 00:31:22.940]   But from just a principled approach, you are uncertain about everything.
[00:31:22.940 --> 00:31:24.660]   It's a good principle in life, too.
[00:31:24.660 --> 00:31:26.860]   You shouldn't be too confident in everything.
[00:31:26.860 --> 00:31:28.240]   But so you pass the uncertainties.
[00:31:28.240 --> 00:31:33.020]   So very concretely, your object detector, you try as much as possible to not argmax,
[00:31:33.020 --> 00:31:36.020]   let's say, over the logits to say, "Oh, this is a person, I'm sure."
[00:31:36.020 --> 00:31:38.380]   You're passing the full probability scores.
[00:31:38.380 --> 00:31:39.500]   And then you have to handle it downstream.
[00:31:39.500 --> 00:31:43.660]   So you have to have a model that doesn't say, "If person do this," right?
[00:31:43.660 --> 00:31:45.020]   You have to be like...
[00:31:45.020 --> 00:31:48.540]   So that breaks any kind of rule-based system you would have downstream.
[00:31:48.540 --> 00:31:50.500]   So you have to digest uncertainty.
[00:31:50.500 --> 00:31:55.460]   We have a recent paper at IROS where we show that, for instance, you can pass a probabilistic
[00:31:55.460 --> 00:31:59.860]   perception outputs into an imitation learning, like behavior cloning system.
[00:31:59.860 --> 00:32:03.740]   It's with ETH, like Andras Buhler, an intern of mine.
[00:32:03.740 --> 00:32:05.740]   So it's going to be published soon.
[00:32:05.740 --> 00:32:12.140]   So passing uncertainty and leveraging uncertainty in the representation for downstream applications.
[00:32:12.140 --> 00:32:14.420]   We also have very cool research with Stanford.
[00:32:14.420 --> 00:32:20.460]   So with Boris Ivanovich and Haruki Nishimura, two wonderful PhD students at Stanford working
[00:32:20.460 --> 00:32:21.940]   with Marco Pavone and Max Schwager.
[00:32:21.940 --> 00:32:26.700]   And it's interesting, it's like people in robotics and aeronautics, et cetera.
[00:32:26.700 --> 00:32:30.260]   And they're really, really good at thinking about safety and these constraints.
[00:32:30.260 --> 00:32:36.480]   And so here the idea was Boris made a paper called Trajectron++, which takes in tracks
[00:32:36.480 --> 00:32:41.200]   of objects and can output multiple possible future trajectories.
[00:32:41.200 --> 00:32:42.200]   And that's great.
[00:32:42.200 --> 00:32:43.300]   You can predict the future on that.
[00:32:43.300 --> 00:32:46.460]   But the problem with that is that it's very difficult to leverage in a planner.
[00:32:46.460 --> 00:32:48.300]   Now you can say, "Oh, I could go left.
[00:32:48.300 --> 00:32:49.300]   I could go right.
[00:32:49.300 --> 00:32:50.300]   I'm not sure."
[00:32:50.300 --> 00:32:53.340]   But the planner is like, "How do I decide?"
[00:32:53.340 --> 00:32:57.660]   And if you're too conservative, if you mind safety and you're too conservative, then what
[00:32:57.660 --> 00:32:59.620]   happens is that everything is possible.
[00:32:59.620 --> 00:33:02.180]   Therefore you have the frozen car problem.
[00:33:02.180 --> 00:33:03.960]   It's like, "I don't know what to do.
[00:33:03.960 --> 00:33:04.960]   Everything is possible.
[00:33:04.960 --> 00:33:05.960]   Therefore I will not move."
[00:33:05.960 --> 00:33:08.860]   So then you have a self-driving car, but it stays in the garage.
[00:33:08.860 --> 00:33:09.860]   Not great.
[00:33:09.860 --> 00:33:16.820]   So with Haruki, Boris, we basically did a system where we modified some of the controls.
[00:33:16.820 --> 00:33:19.300]   So you have to have very deep knowledge about control.
[00:33:19.300 --> 00:33:22.940]   And people like Max Schrager and Marco Pavone are really super, super smart about this.
[00:33:22.940 --> 00:33:27.340]   And this is called risk-sensitive control, where basically what you can do is you can
[00:33:27.340 --> 00:33:31.780]   leverage these different samples from the trajectories and reason in terms of control
[00:33:31.780 --> 00:33:34.260]   of how do I minimize my risk?
[00:33:34.260 --> 00:33:36.740]   How do I optimize my objective?
[00:33:36.740 --> 00:33:37.740]   I want to drive.
[00:33:37.740 --> 00:33:38.780]   I want to go there.
[00:33:38.780 --> 00:33:41.700]   But at the same time, I want to avoid collisions.
[00:33:41.700 --> 00:33:45.740]   And so a really interesting thing is that there's a simple mathematic trick called the
[00:33:45.740 --> 00:33:46.740]   entropic risk.
[00:33:46.740 --> 00:33:51.340]   I can refer to something published at IROS, and you can find this on my website, where
[00:33:51.340 --> 00:33:53.220]   you can basically change the objective function.
[00:33:53.220 --> 00:33:57.180]   So it's almost just a change of mathematical formulation of the optimization problem of
[00:33:57.180 --> 00:33:58.620]   how to plan.
[00:33:58.620 --> 00:34:02.140]   And you can have a very interpretable high-level variable that's called the risk sensitivity
[00:34:02.140 --> 00:34:03.980]   to say whether you should...
[00:34:03.980 --> 00:34:05.940]   If you're risk-sensitive, you can go there.
[00:34:05.940 --> 00:34:07.940]   If you're risk-neutral, you can go there.
[00:34:07.940 --> 00:34:09.980]   If you're risk-seeking, you can go there.
[00:34:09.980 --> 00:34:11.780]   And then the problem becomes how do you adjust this?
[00:34:11.780 --> 00:34:13.500]   And we have follow-up work on that too.
[00:34:13.500 --> 00:34:19.740]   So yeah, ML is everywhere, but it's starting from perception and more and more moving to
[00:34:19.740 --> 00:34:20.740]   prediction.
[00:34:20.740 --> 00:34:24.660]   And I think where the cutting edge is really is in the planning and control side, because
[00:34:24.660 --> 00:34:28.860]   we did also some work with Felipe Cotevilla and other folks that is now at Mila and Yoshua
[00:34:28.860 --> 00:34:31.540]   Bengio's lab, where we looked at behavior cloning.
[00:34:31.540 --> 00:34:37.180]   So just learning a policy from pixels to steering, but that is still far from a very well-engineered
[00:34:37.180 --> 00:34:38.940]   stack in a domain that you know.
[00:34:38.940 --> 00:34:40.460]   So how do you bridge these gaps?
[00:34:40.460 --> 00:34:41.460]   So ML is everywhere.
[00:34:41.460 --> 00:34:42.460]   Yeah.
[00:34:42.460 --> 00:34:43.460]   It's a complex space.
[00:34:43.460 --> 00:34:44.460]   Right.
[00:34:44.460 --> 00:34:46.620]   And I guess your team sits outside of any of these teams.
[00:34:46.620 --> 00:34:51.500]   So it's sort of like going back to Conway's Law, your goal is to put ML, I guess, in every
[00:34:51.500 --> 00:34:52.860]   component of the...
[00:34:52.860 --> 00:34:53.860]   Exactly.
[00:34:53.860 --> 00:34:54.860]   Yeah.
[00:34:54.860 --> 00:34:55.860]   Right.
[00:34:55.860 --> 00:34:56.860]   So we're trying to find applications wherever possible.
[00:34:56.860 --> 00:35:02.100]   And again, the holy grail is that we want to improve the end-to-end system of experience.
[00:35:02.100 --> 00:35:06.140]   And going back to what you said about fleet learning, how does that work?
[00:35:06.140 --> 00:35:10.460]   What actually can you learn from the fleet and how real-time is that?
[00:35:10.460 --> 00:35:11.460]   Right.
[00:35:11.460 --> 00:35:13.940]   That's good questions, insightful questions.
[00:35:13.940 --> 00:35:21.580]   So you could argue that today fleet learning exists, but it's the disappointing version
[00:35:21.580 --> 00:35:27.100]   of it, which is just data haul back, put in a data lake, ETL, label, all these kinds of
[00:35:27.100 --> 00:35:28.100]   things.
[00:35:28.100 --> 00:35:31.740]   So basically anybody that does data science does fleet learning in a sense.
[00:35:31.740 --> 00:35:38.900]   So for robots, the whole kind of spiel, it's like what Steve Jobs was saying is a 10X quantitative
[00:35:38.900 --> 00:35:41.660]   improvement is a qualitative improvement.
[00:35:41.660 --> 00:35:47.260]   And so if you really improve the cycle, like the iteration speed, that's where you're going
[00:35:47.260 --> 00:35:49.820]   to get to true fleet learning in the proper sense.
[00:35:49.820 --> 00:35:53.500]   So one way is to just make that same process just faster by just optimizing it.
[00:35:53.500 --> 00:35:58.700]   So retrieve data faster and iterate faster on it and redeploy the models faster.
[00:35:58.700 --> 00:36:02.060]   And so that would be, for instance, looking again at this theory of constraints or theory
[00:36:02.060 --> 00:36:04.740]   of lean, look at the bottleneck, the bottleneck is labeling.
[00:36:04.740 --> 00:36:06.500]   So that's why you work on self-supervised learning.
[00:36:06.500 --> 00:36:12.860]   So being able to do faster and faster fleet learning in the sense would be just get more
[00:36:12.860 --> 00:36:18.180]   out of self-supervision so that you can iterate quicker and update the model with less labels.
[00:36:18.180 --> 00:36:19.960]   That's the big direction we're doing.
[00:36:19.960 --> 00:36:25.740]   Another one is to start to look more towards the holy grail of lifelong and continual learning,
[00:36:25.740 --> 00:36:29.940]   where you have things like federated learning and these kinds of things, where what you
[00:36:29.940 --> 00:36:34.580]   share is not the data, but what you share is, let's say, the gradients with respect
[00:36:34.580 --> 00:36:37.500]   to a local objective that you computed, for instance.
[00:36:37.500 --> 00:36:41.740]   That has some benefits also in terms of privacy, in terms of communication, in terms of many,
[00:36:41.740 --> 00:36:42.740]   many things.
[00:36:42.740 --> 00:36:46.420]   So you can do lifelong distributed learning in this way and federated learning in these
[00:36:46.420 --> 00:36:48.100]   approaches exist.
[00:36:48.100 --> 00:36:53.420]   Ultimately, beyond just sharing data or just sharing gradients, you would want to share
[00:36:53.420 --> 00:37:01.460]   more useful concepts, like the distillation of what you learned.
[00:37:01.460 --> 00:37:04.220]   Because here, you're never distributing the learning.
[00:37:04.220 --> 00:37:08.940]   You're distributing what enables a centralized learner to then share it back.
[00:37:08.940 --> 00:37:13.060]   But what you want is to distillize what you learn, share what you learn.
[00:37:13.060 --> 00:37:15.500]   That's what ultimate fleet learning has to become.
[00:37:15.500 --> 00:37:18.700]   It's not completely clear exactly what's the best way to do that, but there's a lot of
[00:37:18.700 --> 00:37:19.940]   exciting research on it.
[00:37:19.940 --> 00:37:24.740]   There's super cool research on meta-learning, for instance, from Chelsea Finn at Stanford,
[00:37:24.740 --> 00:37:26.620]   Sergey Levine at Berkeley.
[00:37:26.620 --> 00:37:27.620]   That's exciting.
[00:37:27.620 --> 00:37:28.740]   But I think that's a very...
[00:37:28.740 --> 00:37:32.820]   One of the exciting research problems is how do you do very efficient fleet learning where
[00:37:32.820 --> 00:37:38.580]   what you share is the distilled experience of each robot individually so that they learn
[00:37:38.580 --> 00:37:40.020]   as fast as possible?
[00:37:40.020 --> 00:37:41.420]   Makes sense.
[00:37:41.420 --> 00:37:47.380]   And I guess it's a little bit of a diversion, but I just wanted to touch on the meta-learning
[00:37:47.380 --> 00:37:52.180]   a little bit because you were one of the people that really pushed us to build this Raytune
[00:37:52.180 --> 00:37:54.540]   integration that we've actually launched recently.
[00:37:54.540 --> 00:37:55.540]   We're really excited about it.
[00:37:55.540 --> 00:38:00.340]   I'm curious if you could say a little bit about how you think about hyperparameter search
[00:38:00.340 --> 00:38:03.220]   and just the optimization in general.
[00:38:03.220 --> 00:38:04.220]   When do you do it?
[00:38:04.220 --> 00:38:05.220]   What value does it bring you?
[00:38:05.220 --> 00:38:07.780]   What strategies do you use?
[00:38:07.780 --> 00:38:12.860]   So I'm a big fan of hyperband because just simply this idea of...
[00:38:12.860 --> 00:38:14.740]   First, the formalism is nice.
[00:38:14.740 --> 00:38:17.300]   Formalizing has an online learning problem, the bandit style.
[00:38:17.300 --> 00:38:22.140]   The second thing is I think the best way to be efficient is to reuse computations, and
[00:38:22.140 --> 00:38:23.260]   that's exactly what they're doing.
[00:38:23.260 --> 00:38:27.620]   And because everything we're doing is SGD, stochastic gradient descent, it's an iterative
[00:38:27.620 --> 00:38:28.620]   process.
[00:38:28.620 --> 00:38:31.740]   So I think this idea of continuing optimization of the best solutions and selecting things
[00:38:31.740 --> 00:38:37.820]   like this, I think that leverages some unique aspects of the optimization.
[00:38:37.820 --> 00:38:41.180]   In the early days, I was using Hyperopt and I liked it a lot.
[00:38:41.180 --> 00:38:44.980]   And this notion of you model your parameters as random variables, your hyperparameters
[00:38:44.980 --> 00:38:47.860]   as random variables you don't know.
[00:38:47.860 --> 00:38:51.940]   And so what you do is you sample from them and then you fit the distribution.
[00:38:51.940 --> 00:38:53.420]   When do we use it?
[00:38:53.420 --> 00:38:56.360]   Well, that's a hard question actually.
[00:38:56.360 --> 00:38:59.320]   That's a really, really hard question because when you're in production, you use it whenever
[00:38:59.320 --> 00:39:03.060]   you're about to deploy the model, you have some good confidence that this model is working
[00:39:03.060 --> 00:39:06.900]   well and now you want to squeeze a little more out of it.
[00:39:06.900 --> 00:39:08.380]   And you have a clean protocol, right?
[00:39:08.380 --> 00:39:11.980]   It's trained, valid, test, set, split, or you do something a bit more sophisticated
[00:39:11.980 --> 00:39:13.480]   than just a good split.
[00:39:13.480 --> 00:39:15.420]   But already a good split is it can go...
[00:39:15.420 --> 00:39:18.740]   When you build your own datasets, if you build a large validation set, a large test set,
[00:39:18.740 --> 00:39:22.860]   and it's diverse and you're good at building those datasets, then that brings you a long
[00:39:22.860 --> 00:39:23.860]   way.
[00:39:23.860 --> 00:39:26.740]   In production, I think the answer is easier, but in research it's hard because in research
[00:39:26.740 --> 00:39:31.860]   you're like, "Is it not working as well as I intended because I have a bug?"
[00:39:31.860 --> 00:39:33.460]   Because it's typically more fresh code, right?
[00:39:33.460 --> 00:39:36.540]   Like production is like the code has seen more pairs of eyes and more iterations, but
[00:39:36.540 --> 00:39:37.540]   in research it's more fresh.
[00:39:37.540 --> 00:39:39.500]   So is there a bug?
[00:39:39.500 --> 00:39:43.740]   My advisor, like Cordelia Schmitt, actually taught me this during my PhD, which is whenever
[00:39:43.740 --> 00:39:48.380]   there was something funky, you ask yourself five times whether there's a bug.
[00:39:48.380 --> 00:39:53.540]   And so it's funny because I discovered that there's this exercise in safety, in critical
[00:39:53.540 --> 00:39:55.460]   analysis called the five whys.
[00:39:55.460 --> 00:39:59.900]   And you think asking why five times for when there's a fault or there's a problem, you
[00:39:59.900 --> 00:40:01.580]   think five times, easy.
[00:40:01.580 --> 00:40:04.940]   By the third time you're like, "Oh, this is hard."
[00:40:04.940 --> 00:40:08.660]   So is it a bug or is it just a hypermetric search problem?
[00:40:08.660 --> 00:40:11.060]   And so typically you want to start...
[00:40:11.060 --> 00:40:14.340]   Hypermetric search is kind of a hammer or a bazooka.
[00:40:14.340 --> 00:40:17.260]   You don't want to use it to kill a fly, right?
[00:40:17.260 --> 00:40:23.780]   So I tend to try to delay the use of it when I believe that's the source of the issue.
[00:40:23.780 --> 00:40:24.780]   So that's my answer.
[00:40:24.780 --> 00:40:25.780]   Yeah, it's interesting.
[00:40:25.780 --> 00:40:29.940]   We've gotten very strong reactions both ways, I think, in the interviews that we've done.
[00:40:29.940 --> 00:40:33.540]   I mean, some people are like, "Oh, hyperparameter search keeps you from actually learning the
[00:40:33.540 --> 00:40:35.060]   underlying structure of what you're doing."
[00:40:35.060 --> 00:40:38.980]   Other people are like, "Why would you spend any time figuring out what hyperparameters
[00:40:38.980 --> 00:40:39.980]   you want?
[00:40:39.980 --> 00:40:41.140]   Just let the machine do the work."
[00:40:41.140 --> 00:40:42.140]   So it's interesting.
[00:40:42.140 --> 00:40:43.140]   Yeah, yeah.
[00:40:43.140 --> 00:40:44.380]   I think you need...
[00:40:44.380 --> 00:40:45.380]   There's these two phases, right?
[00:40:45.500 --> 00:40:48.180]   You start to develop the intuition and you experiment with it.
[00:40:48.180 --> 00:40:49.980]   Because anyway, hypermetric search is not magic.
[00:40:49.980 --> 00:40:52.060]   You have to decide the ranges of your hypermeters, right?
[00:40:52.060 --> 00:40:56.380]   Or even if you use Bayesian hypermeter optimization, you have to decide even the probability distribution.
[00:40:56.380 --> 00:40:57.380]   Is it the log normal?
[00:40:57.380 --> 00:40:59.160]   Is it these kinds of things, right?
[00:40:59.160 --> 00:41:00.160]   And the bounds.
[00:41:00.160 --> 00:41:01.420]   And it's typically an iterative process, right?
[00:41:01.420 --> 00:41:05.480]   You do a first guess, you realize, "Oh, my optimum is on the edge of the grid search.
[00:41:05.480 --> 00:41:06.620]   On my edge of my grid search.
[00:41:06.620 --> 00:41:07.980]   Okay, I need to extend it."
[00:41:07.980 --> 00:41:09.540]   And so you iterate on that.
[00:41:09.540 --> 00:41:12.060]   So it's still not...
[00:41:12.060 --> 00:41:15.260]   In my experience, maybe more on the research side, it's not like, "Oh yeah.
[00:41:15.260 --> 00:41:16.260]   Now hypermetric search.
[00:41:16.260 --> 00:41:17.260]   Done."
[00:41:17.260 --> 00:41:18.260]   It's not like this.
[00:41:18.260 --> 00:41:19.260]   Right.
[00:41:19.260 --> 00:41:20.260]   Well, all right, cool.
[00:41:20.260 --> 00:41:24.500]   We're running out of time and I always like to end with two questions for consistency
[00:41:24.500 --> 00:41:28.460]   and maybe we can do some analysis on the answers one day.
[00:41:28.460 --> 00:41:33.140]   The penultimate question that we always ask is, what is an underrated ML topic that you
[00:41:33.140 --> 00:41:37.740]   think people aren't talking about as much as they should for how valuable it might be?
[00:41:37.740 --> 00:41:38.740]   Right.
[00:41:38.740 --> 00:41:43.340]   I would say the things that we mentioned earlier was how little machine and how little learning
[00:41:43.340 --> 00:41:45.580]   there actually is in machine learning.
[00:41:45.580 --> 00:41:50.500]   I was having a nice chat with Siobhan Zillis and she was saying, "I'm interested in machine
[00:41:50.500 --> 00:41:51.500]   not learning."
[00:41:51.500 --> 00:41:53.980]   And I liked how she phrased it.
[00:41:53.980 --> 00:41:58.740]   I think there's more seriously this bigger concern around ethical use of AI and all these
[00:41:58.740 --> 00:41:59.740]   kind of things, right?
[00:41:59.740 --> 00:42:04.700]   And there's this always the saying of, "Because you could, doesn't mean you should."
[00:42:04.700 --> 00:42:08.580]   And so the question in machine learning is very much that I think is interesting, is
[00:42:08.580 --> 00:42:10.380]   what should not be learned?
[00:42:10.380 --> 00:42:15.220]   And so there are certain applications that should not be pursued, things like classifying,
[00:42:15.220 --> 00:42:20.220]   like justice, like legal, we've seen all kinds of horror things, especially computer vision,
[00:42:20.220 --> 00:42:22.100]   sadly, there's a very easy way to...
[00:42:22.100 --> 00:42:24.180]   So there's just applications you shouldn't do.
[00:42:24.180 --> 00:42:28.100]   And I think as a leading researcher on these questions, she has done a great tutorial,
[00:42:28.100 --> 00:42:31.260]   I think with Emily Denton at CVPR this year, which I can recommend your readers to think
[00:42:31.260 --> 00:42:34.300]   about this more socio-technical challenges there.
[00:42:34.300 --> 00:42:38.780]   But in practice also, and more like once you work on a good problem, like saving lives
[00:42:38.780 --> 00:42:42.340]   of children in cars or helping people age in place with home robots, and you know you
[00:42:42.340 --> 00:42:46.240]   should do these kinds of applications, then the question becomes like, which part of my
[00:42:46.240 --> 00:42:47.540]   system do I design?
[00:42:47.540 --> 00:42:50.020]   Which part of my system do I learn?
[00:42:50.020 --> 00:42:54.320]   And this is tricky because you need some design to generalize.
[00:42:54.320 --> 00:42:57.780]   Because if you try to learn everything, like the example, like James Kuffner, like was
[00:42:57.780 --> 00:43:03.060]   former CEO of TRI and the head, the CEO of TRID, was basically telling me, "You don't
[00:43:03.060 --> 00:43:07.420]   drive in front of a school a thousand times and risk bumping into kids to understand that
[00:43:07.420 --> 00:43:09.700]   the limit is 25 miles per hour."
[00:43:09.700 --> 00:43:12.780]   So there are certain things you need to engineer, but there are certain things you need to learn.
[00:43:12.780 --> 00:43:14.860]   Where do you set that line is very difficult.
[00:43:14.860 --> 00:43:18.100]   And the second answer would be labeling.
[00:43:18.100 --> 00:43:21.940]   If you listen to anybody that's talking about self-driving cars or everything like this,
[00:43:21.940 --> 00:43:29.340]   and AGI and all these kinds of things, never ever do they mention labeling.
[00:43:29.340 --> 00:43:33.940]   And if you know what's happening behind the scenes in the industry is that this is what
[00:43:33.940 --> 00:43:34.940]   is going on.
[00:43:34.940 --> 00:43:35.940]   This is what learning is.
[00:43:35.940 --> 00:43:39.700]   There's very little machine in machine learning because it's human labor.
[00:43:39.700 --> 00:43:45.060]   It's a tremendous amount of human labor, like hundreds or thousands of labelers today.
[00:43:45.060 --> 00:43:51.700]   You don't have self-driving cars and yet we have thousands of labelers for a single application
[00:43:51.700 --> 00:43:55.460]   that are just clicking on pixels.
[00:43:55.460 --> 00:43:57.620]   And you know about this.
[00:43:57.620 --> 00:43:58.620]   You must do this.
[00:43:58.620 --> 00:44:01.780]   My take on this is that you must label for the testing purposes.
[00:44:01.780 --> 00:44:05.540]   But as much as possible, I would like to avoid labeling for training purposes.
[00:44:05.540 --> 00:44:07.500]   Because the training has to be continuous.
[00:44:07.500 --> 00:44:12.580]   So that would be why I think labeling is just surprising how few people are talking about
[00:44:12.580 --> 00:44:17.220]   in industry, the costs and the scalability issues that go with.
[00:44:17.220 --> 00:44:18.220]   Interesting.
[00:44:18.220 --> 00:44:19.220]   All right.
[00:44:19.220 --> 00:44:23.980]   Well, I could ask questions forever about that, but I want to ask my final question
[00:44:23.980 --> 00:44:24.980]   and wrap this up.
[00:44:24.980 --> 00:44:31.380]   So what is, in your experience, the hardest part about taking models and getting them
[00:44:31.380 --> 00:44:33.380]   deployed in production and used?
[00:44:33.380 --> 00:44:35.060]   Where are the big bottlenecks?
[00:44:35.060 --> 00:44:36.060]   Right.
[00:44:36.060 --> 00:44:38.820]   Scalability is such an obvious answer.
[00:44:38.820 --> 00:44:43.140]   Where in the research land, we have the idea, we have this prototype, we use a standard
[00:44:43.140 --> 00:44:47.300]   data set or even build our own data set to prove an idea.
[00:44:47.300 --> 00:44:50.900]   And then you have first a human element to it, which is you have to convince people to
[00:44:50.900 --> 00:44:53.460]   run with that idea and that should be enough.
[00:44:53.460 --> 00:44:56.220]   You should have a nice W&B report and they go, "Yes, awesome.
[00:44:56.220 --> 00:45:01.340]   We're going to put 20 or 30 or 100 engineers on it just based on that."
[00:45:01.340 --> 00:45:04.940]   So there's some amount of convincing people to go from research to production.
[00:45:04.940 --> 00:45:08.220]   So you have to be compelling, enough compelling evidence.
[00:45:08.220 --> 00:45:12.120]   That's more on the researchers and our bottleneck is to convince people.
[00:45:12.120 --> 00:45:13.540]   And part of it is the scalability, right?
[00:45:13.540 --> 00:45:15.020]   So people might say, "I understand the idea.
[00:45:15.020 --> 00:45:19.620]   I can see that this works, your evidence that you present, but I don't think it's going
[00:45:19.620 --> 00:45:22.940]   to work in this scenario, or I don't think it's going to be cost effective, or I don't
[00:45:22.940 --> 00:45:27.500]   think it's going to be easy to just scale up computationally speaking or something like
[00:45:27.500 --> 00:45:28.500]   this."
[00:45:28.500 --> 00:45:31.120]   So worrying about scalability is something we really do.
[00:45:31.120 --> 00:45:34.340]   So one of the things that I did earlier this year is that we kind of split the efforts
[00:45:34.340 --> 00:45:36.980]   between the ML team, between research and engineering.
[00:45:36.980 --> 00:45:40.600]   And Sudip Pillai is now the head of ML engineering at TRI.
[00:45:40.600 --> 00:45:41.780]   And he's really driven to this.
[00:45:41.780 --> 00:45:47.140]   Whenever I talk to him, it's like really that's his drive is that how do we think about scalability?
[00:45:47.140 --> 00:45:49.860]   And he's doing really cool work with his team on semi-supervised learning and these kind
[00:45:49.860 --> 00:45:53.900]   of ideas to try to scale things up so that we can go from research ideas that may be
[00:45:53.900 --> 00:46:00.780]   not scalable to more well-formed transferable research idea that is shown to work at scale
[00:46:00.780 --> 00:46:05.500]   already at this stage, at the prototype stage, and then that can maybe be transferred.
[00:46:05.500 --> 00:46:09.580]   What does it mean for an algorithm to not be scalable?
[00:46:09.580 --> 00:46:10.580]   For instance, labeling.
[00:46:10.580 --> 00:46:12.180]   If you say, "Yeah, I can get to great performance.
[00:46:12.180 --> 00:46:15.460]   I can get to 80% with a dataset this size."
[00:46:15.460 --> 00:46:19.900]   And then you look at the performance improvements with the training set size, and then you realize
[00:46:19.900 --> 00:46:22.020]   that the improvements are logarithmic.
[00:46:22.020 --> 00:46:23.020]   And this is not good, right?
[00:46:23.020 --> 00:46:24.940]   And you say, "Oh, it improves with data.
[00:46:24.940 --> 00:46:25.940]   That's great."
[00:46:25.940 --> 00:46:30.300]   But then your cost is going to be, "All right, you need a billion dollar dataset."
[00:46:30.300 --> 00:46:31.300]   You don't really want to do that.
[00:46:31.300 --> 00:46:32.300]   And a billion dollar compute, right?
[00:46:32.300 --> 00:46:36.540]   This is a bit like the open AI story, which is if you have infinite money, infinite labelers,
[00:46:36.540 --> 00:46:39.340]   infinite time, infinite compute, how far can you go?
[00:46:39.340 --> 00:46:42.940]   And it's super interesting to see these guys, it's amazing what they do in terms of pushing
[00:46:42.940 --> 00:46:44.540]   the boundaries on that.
[00:46:44.540 --> 00:46:49.540]   But for some applications in the real world, that's not really reasonable.
[00:46:49.540 --> 00:46:52.300]   So you have to kind of think about how do you scale.
[00:46:52.300 --> 00:46:54.020]   And we're not trying to solve everything at once.
[00:46:54.020 --> 00:46:57.500]   So really for us, the compute is something we kind of ignore for now.
[00:46:57.500 --> 00:47:01.140]   And we're saying, "Let's assume you have infinite compute, but you don't have infinite labeling
[00:47:01.140 --> 00:47:02.140]   budget."
[00:47:02.140 --> 00:47:03.140]   I was going to say, we've seen your...
[00:47:03.140 --> 00:47:07.620]   We see open AI's usage and your usage, and your usage is not small.
[00:47:07.620 --> 00:47:10.540]   Yeah, I'll take it as a compliment.
[00:47:10.540 --> 00:47:13.140]   No, we're well-funded and we're utilizing it well.
[00:47:13.140 --> 00:47:14.140]   So computation, right?
[00:47:14.140 --> 00:47:16.780]   Again, I said not enough machine and machine learning, but that's what we're trying to
[00:47:16.780 --> 00:47:17.780]   do.
[00:47:17.780 --> 00:47:18.780]   We're trying to make machines work for us.
[00:47:18.780 --> 00:47:23.980]   It's kind of funny how people are saying, "It used to be that humans play video games,
[00:47:23.980 --> 00:47:24.980]   and then they do work.
[00:47:24.980 --> 00:47:29.220]   But now it's kind of like we work so that machines can play video games.
[00:47:29.220 --> 00:47:32.420]   We make algorithms that can play Atari, while we don't do Atari games anymore."
[00:47:32.420 --> 00:47:33.420]   So that's weird.
[00:47:33.420 --> 00:47:34.420]   All right.
[00:47:34.420 --> 00:47:35.420]   Well, thanks so much, Adrian.
[00:47:35.420 --> 00:47:36.420]   It's so much fun to talk to you.
[00:47:36.420 --> 00:47:37.420]   Likewise.
[00:47:37.420 --> 00:47:38.420]   It was a pleasure.
[00:47:38.420 --> 00:47:39.420]   Appreciate it.
[00:47:39.420 --> 00:47:43.060]   Thanks for listening to another episode of Gradient Dissent.
[00:47:43.060 --> 00:47:47.420]   Doing these interviews are a lot of fun, and it's especially fun for me when I can actually
[00:47:47.420 --> 00:47:50.140]   hear from the people that are listening to these episodes.
[00:47:50.140 --> 00:47:54.220]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:47:54.220 --> 00:47:58.140]   a conversation, that would make me inspired to do more of these episodes.
[00:47:58.140 --> 00:48:01.660]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

