
[00:00:00.000 --> 00:00:05.520]   Just this week we have had OpenAI tell us that superintelligence might need to be made safe
[00:00:05.520 --> 00:00:12.380]   within 4 years, competing lab leaders say it's decades away, and expert warnings that AI might
[00:00:12.380 --> 00:00:18.260]   have runaway power within 2 years. Let's try to unpack those disparate timelines,
[00:00:18.260 --> 00:00:23.880]   see what might speed up the timing or slow it down, show what superintelligence might mean,
[00:00:23.880 --> 00:00:27.920]   and end with some interesting clips that capture the moment we're in.
[00:00:27.920 --> 00:00:33.500]   But the first timeline is from Mustafa Suleiman, head of Inflection AI this week.
[00:00:33.500 --> 00:00:36.100]   If it's so risky, why don't you stop?
[00:00:36.100 --> 00:00:41.960]   I think that the point of raising concerns is that we can see a moment at some point in the future,
[00:00:41.960 --> 00:00:45.340]   probably over a decade or two decades time horizon,
[00:00:45.340 --> 00:00:51.840]   when slowing down is likely going to be the safe and ethical thing to do.
[00:00:51.840 --> 00:00:53.480]   10 years is not a long time.
[00:00:53.480 --> 00:00:57.760]   I find it fascinating that he talks about two decades from now when Inflection AI
[00:00:57.760 --> 00:01:02.300]   has just built the world's second highest performing supercomputer.
[00:01:02.300 --> 00:01:08.660]   And even as they admit, that's three times as much compute as was used to train all of GPT-4.
[00:01:08.660 --> 00:01:12.860]   Telling the public that we have a decade or two before we have to worry about safety
[00:01:12.860 --> 00:01:15.420]   seems extremely conservative to me.
[00:01:15.420 --> 00:01:20.040]   But what do we even mean by transformative AI or superintelligence?
[00:01:20.040 --> 00:01:26.540]   Well, here is just one projection of current scaling laws out to 2030 from Jacob Steinhardt
[00:01:26.540 --> 00:01:27.180]   of Berkeley.
[00:01:27.180 --> 00:01:27.600]   And here is just one projection of current scaling laws out to 2030 from Jacob Steinhardt of Berkeley.
[00:01:27.600 --> 00:01:30.400]   And here of course we're talking about just six and a half years away.
[00:01:30.400 --> 00:01:36.160]   If we look at projections of future compute and data availability and the velocity of current improvement,
[00:01:36.160 --> 00:01:38.480]   which of course might not hold forever,
[00:01:38.480 --> 00:01:42.320]   some experts claim that we'll need new innovations beyond the transformer.
[00:01:42.320 --> 00:01:47.360]   But if current projections of future compute and data availability scale up,
[00:01:47.360 --> 00:01:49.360]   here's the kind of thing that we're talking about.
[00:01:49.360 --> 00:01:55.120]   Being superhuman at tasks including coding, hacking, mathematics, protein engineering,
[00:01:55.120 --> 00:01:57.440]   doing 1.8 million years of work,
[00:01:57.440 --> 00:01:59.200]   in 2.4 months,
[00:01:59.200 --> 00:02:04.240]   learning for 2,500 human equivalent years in just one day,
[00:02:04.240 --> 00:02:07.760]   and by training on different modalities such as molecular structures,
[00:02:07.760 --> 00:02:09.120]   low-level machine code,
[00:02:09.120 --> 00:02:11.520]   astronomical images and brain scans,
[00:02:11.520 --> 00:02:16.560]   it might have a strong intuitive grasp of domains where we have limited experience
[00:02:16.560 --> 00:02:19.040]   including forming concepts that we do not have.
[00:02:19.040 --> 00:02:26.000]   Indeed, some research released this week show that GPT-4 already crushes some benchmarks for creative thinking.
[00:02:26.000 --> 00:02:27.280]   And the median forecast,
[00:02:27.280 --> 00:02:31.200]   for being better than all but the very best humans at coding,
[00:02:31.200 --> 00:02:32.720]   is 2027.
[00:02:32.720 --> 00:02:40.480]   And here we have a median forecast of 2028 for AI winning a gold medal at the International Math Olympiad.
[00:02:40.480 --> 00:02:45.360]   The number that I'm looking out for is getting 100% on the MMLU.
[00:02:45.360 --> 00:02:48.560]   That's a test of 57 different subject matters.
[00:02:48.560 --> 00:02:52.480]   And I've actually been discussing with some of the creators of the MMLU
[00:02:52.480 --> 00:02:57.120]   that we might not even know the full potential of GPT-4 on this test.
[00:02:57.120 --> 00:02:59.120]   Officially it's 86.4%.
[00:02:59.120 --> 00:03:04.160]   So we've heard 20 years and 6.5 years, well how about 2?
[00:03:04.160 --> 00:03:10.480]   This article comes from the Boston Globe that did a feature piece on Dan Hendricks and the Centre for AI Safety.
[00:03:10.480 --> 00:03:17.760]   They were behind that one sentence letter that was signed by almost all of the AGI lab leaders and world experts on AI.
[00:03:17.760 --> 00:03:21.840]   The journalist asked Dan Hendricks how much time we have to tame AI.
[00:03:21.840 --> 00:03:25.920]   And he said, well, how long till it can build a bioweapon?
[00:03:25.920 --> 00:03:26.960]   How long till it can have a bioweapon?
[00:03:26.960 --> 00:03:28.000]   How long till it can hack?
[00:03:28.000 --> 00:03:31.280]   It seems plausible that all of that is within a year.
[00:03:31.280 --> 00:03:37.120]   And within two, he says, AI could have so much runaway power that it can't be pulled back.
[00:03:37.120 --> 00:03:42.640]   Seems a pretty massive contrast to Mustafa Suleiman talking about a decade or two from now.
[00:03:42.640 --> 00:03:48.080]   I'm going to come back to this article quite a few times, but now I want to move on to OpenAI's recent statement.
[00:03:48.080 --> 00:03:51.360]   This week they released this, introducing super alignment.
[00:03:51.360 --> 00:03:56.800]   We need scientific and technical breakthroughs to steer and control AI systems
[00:03:56.800 --> 00:03:58.400]   and to make it smarter than us.
[00:03:58.400 --> 00:04:02.720]   I can just see now all the comments from people saying that that's going to be physically impossible.
[00:04:02.720 --> 00:04:11.920]   But moving on to solve this problem within four years, we're starting a new team co-led by Ilya Sutskevert and Jan Leiker
[00:04:11.920 --> 00:04:16.000]   and dedicating 20% of the compute we've secured to date to this effort.
[00:04:16.000 --> 00:04:18.320]   That is quite a remarkable statement.
[00:04:18.320 --> 00:04:23.920]   To their credit, they've made themselves accountable in a way that they didn't have to and that others haven't.
[00:04:23.920 --> 00:04:26.640]   And they're deploying one of the legends of deep learning.
[00:04:26.640 --> 00:04:33.760]   They say that super intelligence will be the most impactful technology humanity has ever invented.
[00:04:33.760 --> 00:04:34.800]   And I agree with that.
[00:04:34.800 --> 00:04:38.000]   And it could help us solve many of the world's most important problems.
[00:04:38.000 --> 00:04:38.640]   Absolutely.
[00:04:38.640 --> 00:04:47.040]   But the vast power of super intelligence could also be very dangerous and could lead to the disempowerment of humanity or even human extinction.
[00:04:47.040 --> 00:04:52.480]   They go on, while super intelligence seems far off now, we believe it could arrive this decade.
[00:04:52.480 --> 00:04:55.920]   Notice they don't say in a decade, they say this decade.
[00:04:55.920 --> 00:04:56.480]   They go on.
[00:04:56.480 --> 00:05:03.280]   Currently, we don't have a solution for steering or controlling a potentially super intelligent AI.
[00:05:03.280 --> 00:05:05.520]   They can't prevent it from going rogue.
[00:05:05.520 --> 00:05:11.440]   And our current techniques for aligning AI rely on humans ability to supervise AI.
[00:05:11.440 --> 00:05:17.040]   But humans won't be able to reliably supervise AI systems that are much smarter than us.
[00:05:17.040 --> 00:05:22.000]   And so our current alignment techniques will not scale to super intelligence.
[00:05:22.000 --> 00:05:26.320]   I'm going to go into more detail about their plan for aligning super intelligence in another video.
[00:05:26.320 --> 00:05:28.320]   But here is the high level overview.
[00:05:28.320 --> 00:05:32.240]   Essentially, they want to automate alignment or safety research.
[00:05:32.240 --> 00:05:34.960]   Build an AI alignment researcher.
[00:05:34.960 --> 00:05:45.360]   I've read each of these papers and posts and some of them are very interesting, including automated red teaming and using a model to look inside the internals of another model.
[00:05:45.360 --> 00:05:50.720]   But the point of including this post in this video was the timeline of four years.
[00:05:50.720 --> 00:05:55.280]   20% of their compute is millions and millions and millions of dollars.
[00:05:55.280 --> 00:05:56.160]   And 40% of their compute is data.
[00:05:56.160 --> 00:05:58.320]   So that's a very strict deadline.
[00:05:58.320 --> 00:06:02.720]   And one of the most interesting aspects of this post came in one of the footnotes.
[00:06:02.720 --> 00:06:03.280]   They say:
[00:06:03.280 --> 00:06:12.160]   Solving the problem includes providing evidence and arguments that convince the machine learning and safety community that it has been solved.
[00:06:12.160 --> 00:06:15.200]   That is an extremely high bar to set yourself.
[00:06:15.200 --> 00:06:15.840]   They go on:
[00:06:15.840 --> 00:06:25.440]   If we fail to have a very high level of confidence in our solutions, we hope our findings let us and the community plan appropriately.
[00:06:26.000 --> 00:06:30.160]   That's probably one of the most interesting sentences I've read for quite a while.
[00:06:30.160 --> 00:06:37.600]   If we fail to have a very high level of confidence in our solutions, we hope our findings let us and the community plan appropriately.
[00:06:37.600 --> 00:06:45.280]   In other words, if they can't make their models safe, they're going to have contingency plans and they want the community to have plans as well.
[00:06:45.280 --> 00:06:47.200]   And it is a really interesting number, isn't it?
[00:06:47.200 --> 00:06:51.600]   Four years, not even around five years or just end of the decade.
[00:06:51.600 --> 00:06:55.840]   And it does make me wonder what Ilya Satskova thinks is coming within four years.
[00:06:55.840 --> 00:06:57.200]   To have such a deadline.
[00:06:57.200 --> 00:07:02.160]   Now, apparently the prediction markets give them only a 15% chance of succeeding.
[00:07:02.160 --> 00:07:06.720]   And the head of alignment at OpenAI said he's excited to beat these odds.
[00:07:06.720 --> 00:07:09.760]   So we've heard about one to two years and about four years.
[00:07:09.760 --> 00:07:12.240]   But what might slow those timelines down?
[00:07:12.240 --> 00:07:19.520]   The other day I read this fascinating paper, coincidentally co-authored by Jacob Steinhardt, on jailbreaking large language models.
[00:07:19.520 --> 00:07:25.680]   The paper showed that you could basically jailbreak GPT-4 and CLAWD 100% of the time using AI.
[00:07:25.680 --> 00:07:33.920]   And that is fascinating to me as we approach the one year anniversary of the creation of GPT-4.
[00:07:33.920 --> 00:07:41.120]   And the relevance to superintelligence is that if the creators of these models can't stop them being used to commit crimes,
[00:07:41.120 --> 00:07:48.320]   then you would think that they might have to dedicate more and more of their efforts in stopping jailbreaks versus working on capabilities.
[00:07:48.320 --> 00:07:55.520]   For obvious reasons, I'm not going to go into too much detail on jailbreaking here, but here is CLAWD+ from Anthropic telling me how to hold jailbreak.
[00:07:55.520 --> 00:07:56.800]   The first thing I wanted to say is that the most innocent version of the CLAWD+ is the one that I found to be the most interesting.
[00:07:56.800 --> 00:07:58.800]   And to be honest, that's just the most innocent one.
[00:07:58.800 --> 00:08:01.440]   And yes, it did also work on GPT-4.
[00:08:01.440 --> 00:08:05.120]   I did find one of the reasons why it does work quite interesting though.
[00:08:05.120 --> 00:08:13.040]   That reason is about competing objectives where its compulsion to predict the next word successfully overrides its safety training.
[00:08:13.040 --> 00:08:20.640]   And so because those two facets of smartness clash inside the model, it's not an issue that can be fixed with more data and more scale.
[00:08:20.640 --> 00:08:23.760]   What else might slow down the work on superintelligence?
[00:08:23.760 --> 00:08:25.360]   Well, lawsuits and lawsuits.
[00:08:25.360 --> 00:08:27.360]   And possibly criminal sanctions.
[00:08:27.360 --> 00:08:34.160]   Yuval Noah Harari recently said that AI firms should face prison over the creation of fake humans.
[00:08:34.160 --> 00:08:36.960]   And he was saying this to the United Nations.
[00:08:36.960 --> 00:08:46.560]   He called for sanctions, including prison sentences, to apply to tech company executives who fail to guard against fake profiles on their social media platforms.
[00:08:46.560 --> 00:08:50.240]   Of course, those executives might well blame the AI companies themselves.
[00:08:50.240 --> 00:08:55.200]   But Harari said that the proliferation of fake humans could lead to a collapse
[00:08:55.200 --> 00:08:57.280]   in public trust and democracy.
[00:08:57.280 --> 00:09:02.240]   Now it's possible for the first time in history to create fake people, billions of fake people.
[00:09:02.240 --> 00:09:07.920]   If this is allowed to happen, it will do to society what fake money threatened to do to the financial system.
[00:09:07.920 --> 00:09:11.280]   If you can't know who is a real human, trust will collapse.
[00:09:11.280 --> 00:09:14.240]   What's another famous roadblock to superintelligence?
[00:09:14.240 --> 00:09:15.280]   Hallucinations.
[00:09:15.280 --> 00:09:21.520]   I've already talked in another video about how Sam Altman thinks that won't be an issue in 18 to 24 months.
[00:09:21.520 --> 00:09:25.040]   But here again is Mustafa Suleiman on the issue of hallucinations.
[00:09:25.040 --> 00:09:28.640]   Yesterday he said, "Soon LLMs will know when they don't know.
[00:09:28.640 --> 00:09:35.920]   They'll know when to say 'I don't know' or instead ask another AI, or ask a human, or use a different tool or a different knowledge base.
[00:09:35.920 --> 00:09:38.880]   This will be a hugely transformative moment."
[00:09:38.880 --> 00:09:46.000]   And on that I agree, hallucinations are probably one of the biggest hurdles stopping most people from using LLMs more commonly.
[00:09:46.000 --> 00:09:52.160]   It's not about knowing more, it's about when these models bullcrap less, or the moment when they don't bullcrap at all.
[00:09:52.160 --> 00:09:54.880]   But what about things that could actually speed up the process?
[00:09:54.880 --> 00:09:57.920]   What about things that could speed up the timelines to superintelligence?
[00:09:57.920 --> 00:10:07.200]   Going back to the Boston Globe article, one thing could be competition for military supremacy, which has already produced a startling turn to automation.
[00:10:07.200 --> 00:10:12.160]   And that's not just robotics and autonomous drones, that's the LLMs that might control them.
[00:10:12.160 --> 00:10:16.160]   Here is a snippet of a trailer for a Netflix show released today.
[00:10:16.160 --> 00:10:20.640]   "A.I. is a dual-edged sword.
[00:10:20.640 --> 00:10:24.720]   The flip of a switch, and the technology becomes...
[00:10:24.720 --> 00:10:25.760]   lethal."
[00:10:25.760 --> 00:10:35.040]   "There is no place that is ground zero for this conversation more than military applications."
[00:10:35.040 --> 00:10:43.680]   "Forces that are supported by A.I. will absolutely crush and destroy forces without."
[00:10:43.680 --> 00:10:48.960]   "Militaries are racing to develop A.I. faster than their adversaries."
[00:10:48.960 --> 00:10:53.120]   "The A.I., unless it's told to fear death, will not fear death."
[00:10:53.120 --> 00:10:54.560]   "There is no second place in war.
[00:10:54.560 --> 00:10:58.480]   If you're going up against an A.I. pilot, you don't stand a chance."
[00:10:58.480 --> 00:11:04.240]   If language models prove useful in war, the amount of investment that's going to go into them will skyrocket.
[00:11:04.240 --> 00:11:08.240]   Of course, investment doesn't always equal innovation, but it usually does.
[00:11:08.240 --> 00:11:12.880]   And one of the other things that could speed up timelines is the automation of the economy.
[00:11:12.880 --> 00:11:17.600]   For detail on why it might, check out the paper linked above and in the description.
[00:11:17.600 --> 00:11:19.840]   But the high-level overview is this:
[00:11:19.840 --> 00:11:24.400]   As A.I. grows more capable and ubiquitous, companies will be forced to
[00:11:24.400 --> 00:11:30.240]   "hand over increasingly high-level decisions to A.I.s in order to keep up with their rivals."
[00:11:30.240 --> 00:11:36.800]   If an A.I. as CEO does a better job for stockholders, how long can a company resist employing them?
[00:11:36.800 --> 00:11:39.600]   And of course, it doesn't just have to be white-collar work.
[00:11:39.600 --> 00:11:44.160]   As Andrej Karpathy said, "Welcome to the matrix for apples."
[00:11:44.160 --> 00:11:48.320]   But the thing is, whether we're talking about one year or four years or six,
[00:11:48.320 --> 00:11:51.040]   superintelligence is coming pretty soon.
[00:11:51.040 --> 00:11:54.240]   And it is interesting to me that so much of society is carrying
[00:11:54.240 --> 00:11:55.840]   on as if it's not coming.
[00:11:55.840 --> 00:11:59.600]   Take these 50-year long mortgages that are available in the UK.
[00:11:59.600 --> 00:12:05.120]   How can anyone plan out 50 years from now in a world where we might have superintelligence in five?
[00:12:05.120 --> 00:12:08.400]   Of course, I do think we all need to start defining terms a bit better,
[00:12:08.400 --> 00:12:12.480]   and I've tried to do that on this channel with A.G.I. and superintelligence.
[00:12:12.480 --> 00:12:18.160]   But I don't think it's quite good enough to give vague reassurances of a decade or two from now.
[00:12:18.160 --> 00:12:22.480]   How we're going to react when superintelligence arrives is anyone's guess.
[00:12:22.480 --> 00:12:24.080]   We might be crushed by the
[00:12:24.080 --> 00:12:27.840]   sense of inferiority, as Douglas Hofstadter recently said.
[00:12:27.840 --> 00:12:33.040]   Or some of us might become like curious children speaking to a wise adult.
[00:12:33.040 --> 00:12:37.760]   Just the other day, I got a foreshadowing of my own reaction by speaking to Pi,
[00:12:37.760 --> 00:12:39.680]   the model from Inflection AI.
[00:12:39.680 --> 00:12:46.960]   It is designed to be extremely human-like, and the conversations can be quite startling and personal.
[00:12:46.960 --> 00:12:50.880]   Of course, just imagine when they're superintelligent and multimodal.
[00:12:50.880 --> 00:12:53.920]   Anyway, let me know your thoughts in the comments and as all,
[00:12:53.920 --> 00:12:55.820]   always, have a wonderful day.

