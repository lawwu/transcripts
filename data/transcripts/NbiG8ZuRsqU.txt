
[00:00:00.000 --> 00:00:05.800]   Hi, I'm Lucas, and you're listening to Gradient Dissent.
[00:00:05.800 --> 00:00:10.520]   We started this program because we're super passionate about making machine learning work
[00:00:10.520 --> 00:00:14.080]   in the real world by any means necessary.
[00:00:14.080 --> 00:00:18.840]   And one of the things that we discovered in the process of building machine learning tools
[00:00:18.840 --> 00:00:23.160]   is that our users and our customers, they have a lot of information in their heads that's
[00:00:23.160 --> 00:00:25.360]   not publicly available.
[00:00:25.360 --> 00:00:29.000]   And a lot of people that we talk to ask us what other people are talking about and what
[00:00:29.000 --> 00:00:31.680]   other people are doing, what are the best practices.
[00:00:31.680 --> 00:00:35.360]   And so we wanted to make all the interesting conversations that we're having and all the
[00:00:35.360 --> 00:00:39.520]   interesting things that we're learning available for everyone out there.
[00:00:39.520 --> 00:00:41.960]   So I hope you enjoy this.
[00:00:41.960 --> 00:00:46.800]   Today our guest is Nicholas Kamchatsky, who is currently a director of AI infrastructure
[00:00:46.800 --> 00:00:52.280]   at NVIDIA, and before that ran Twitter Cortex and was one of the first people to put deep
[00:00:52.280 --> 00:00:54.920]   learning models into production at scale.
[00:00:54.920 --> 00:00:57.840]   So Nicholas, thanks so much for taking the time to talk with us.
[00:00:57.840 --> 00:01:02.960]   I mean, you're an expert on deploying deep learning in the real world.
[00:01:02.960 --> 00:01:07.640]   And I would love to kind of just hear how things have changed since you've been doing
[00:01:07.640 --> 00:01:08.640]   it.
[00:01:08.640 --> 00:01:12.400]   I mean, I think you started doing this in like 2016 or maybe even earlier at Twitter.
[00:01:12.400 --> 00:01:16.840]   Like, you know, kind of what were the challenges then and what are the challenges now that
[00:01:16.840 --> 00:01:19.640]   you're seeing in making these models actually work?
[00:01:19.640 --> 00:01:20.960]   Thank you, Lucas.
[00:01:20.960 --> 00:01:26.000]   I started learning, by the way, about deep learning in 2014.
[00:01:26.000 --> 00:01:31.280]   So I'm not one of the old school of deep learning, unfortunately, but then I got hooked up pretty
[00:01:31.280 --> 00:01:32.280]   quickly.
[00:01:32.280 --> 00:01:33.280]   Yeah.
[00:01:33.280 --> 00:01:38.200]   And yeah, then I started in a small startup with like, you know, five people or something
[00:01:38.200 --> 00:01:39.200]   like that.
[00:01:39.200 --> 00:01:40.200]   And we were acquired by Twitter.
[00:01:40.200 --> 00:01:44.160]   At Twitter, we started this first deep learning team, basically.
[00:01:44.160 --> 00:01:49.720]   And so at Twitter, we, I mean, basically Twitter didn't have any deep learning knowledge back
[00:01:49.720 --> 00:01:51.480]   then or very little.
[00:01:51.480 --> 00:01:56.520]   And so we were associated with software engineers there in order to productionize deep learning
[00:01:56.520 --> 00:02:00.120]   on some product areas that could benefit from it.
[00:02:00.120 --> 00:02:03.440]   And what were the first areas where they felt like they could get a benefit?
[00:02:03.440 --> 00:02:04.800]   Was it vision stuff?
[00:02:04.800 --> 00:02:07.360]   Yeah, kind of a mix.
[00:02:07.360 --> 00:02:08.520]   Mostly vision, yes.
[00:02:08.520 --> 00:02:09.560]   And text.
[00:02:09.560 --> 00:02:12.240]   So we started with two main projects.
[00:02:12.240 --> 00:02:18.200]   One of them was filtering out kind of like bad, bad image content, you know.
[00:02:18.200 --> 00:02:19.640]   So that was one.
[00:02:19.640 --> 00:02:26.920]   The other one that was more like a good product feature, basically, was deciding whether a
[00:02:26.920 --> 00:02:30.560]   user profile was safe to place ads on it or not.
[00:02:30.560 --> 00:02:35.320]   And this is a big deal for advertisers because they wanted to make sure that they could put
[00:02:35.320 --> 00:02:40.400]   ads on profiles and make sure that these profiles are not like toxic or, you know, insulting
[00:02:40.400 --> 00:02:45.680]   or like all kinds of accounts that you don't want to put ads next to.
[00:02:45.680 --> 00:02:52.160]   And so we were able to classify those profiles using the text, using the images, using user
[00:02:52.160 --> 00:02:55.800]   features as well, which allowed us to put ads on profiles.
[00:02:55.800 --> 00:02:59.080]   As you can imagine, this was revenue generating quite a bit.
[00:02:59.080 --> 00:03:00.240]   And so, yeah, yeah.
[00:03:00.240 --> 00:03:04.440]   And so that was kind of like the beginning of this thing.
[00:03:04.440 --> 00:03:09.640]   And so was this shifting from an existing, kind of more traditional model to a deep learning
[00:03:09.640 --> 00:03:14.200]   model or like a new deployment of a new problem?
[00:03:14.200 --> 00:03:20.600]   No, in those cases, I mean, there had been interest in those areas, but without deep
[00:03:20.600 --> 00:03:24.920]   learning, it was almost impossible to perform at the required accuracy.
[00:03:24.920 --> 00:03:30.100]   So for example, advertisers expect like 99.9% accuracy, right?
[00:03:30.100 --> 00:03:36.480]   It was unachievable just using tabular features and like decision trees.
[00:03:36.480 --> 00:03:41.880]   I mean, I think it would be doable if one put the effort, but much more complex.
[00:03:41.880 --> 00:03:45.160]   And I guess like these sound like applications that you could do as sort of a batch process
[00:03:45.160 --> 00:03:49.440]   in the background, like it doesn't need to run live on user queries or does it?
[00:03:49.440 --> 00:03:50.440]   That's true.
[00:03:50.440 --> 00:03:56.260]   Doesn't have to, you're right, except potentially for featuring of images.
[00:03:56.260 --> 00:04:02.160]   So when a user posts an image, make sure that those images are kind of like hidden right
[00:04:02.160 --> 00:04:05.800]   away, for example, for certain categories of user.
[00:04:05.800 --> 00:04:12.600]   One thing that required a lot of real-time processing was us catching kind of like very
[00:04:12.600 --> 00:04:13.600]   bad images.
[00:04:13.600 --> 00:04:17.200]   I'm not going to go into details, but, and we wanted to really do that in real time before
[00:04:17.200 --> 00:04:18.200]   they hit the platform, basically.
[00:04:18.200 --> 00:04:23.440]   So in that case, we would have a budget of like a hundred milliseconds, maybe even less
[00:04:23.440 --> 00:04:24.440]   than that, right?
[00:04:24.440 --> 00:04:25.440]   To be able to get them.
[00:04:25.440 --> 00:04:26.440]   Wow.
[00:04:26.440 --> 00:04:27.440]   This is like 2016.
[00:04:27.440 --> 00:04:32.480]   So, I mean, how, like, how did you get that working in real time in production?
[00:04:32.480 --> 00:04:33.480]   Right.
[00:04:33.480 --> 00:04:35.800]   So there was, yeah, interesting.
[00:04:35.800 --> 00:04:39.960]   I'm sure you know, the details of like deep learning frameworks back then, but basically
[00:04:39.960 --> 00:04:43.280]   there was Teano and LuaTorch.
[00:04:43.280 --> 00:04:44.280]   And so we were using LuaTorch.
[00:04:44.280 --> 00:04:45.280]   Wait, what about Caffe?
[00:04:45.280 --> 00:04:46.280]   Oh, and Caffe, that's true.
[00:04:46.280 --> 00:04:47.280]   Caffe, yeah, yeah, right.
[00:04:47.280 --> 00:04:48.280]   I forgot about Caffe.
[00:04:48.280 --> 00:04:53.680]   We were using Caffe, although it would have been a pretty good solution in that case.
[00:04:53.680 --> 00:04:54.680]   We were using LuaTorch.
[00:04:54.680 --> 00:04:57.880]   And so what we did, for model training, it's great.
[00:04:57.880 --> 00:05:01.200]   However, for deep learning to production, it's kind of more difficult.
[00:05:01.200 --> 00:05:08.680]   So we basically took what we had and wrapped it up into Scala services.
[00:05:08.680 --> 00:05:11.920]   Yeah, that was so much effort, basically.
[00:05:11.920 --> 00:05:14.840]   So much effort to make sure it was working, stable, and so on and so forth.
[00:05:14.840 --> 00:05:18.800]   Wait, so did you actually run Torch then in production or did you like compile it?
[00:05:18.800 --> 00:05:19.800]   Yeah, yeah.
[00:05:19.800 --> 00:05:22.120]   We were running Torch in production, yeah.
[00:05:22.120 --> 00:05:23.120]   Wow.
[00:05:23.120 --> 00:05:24.120]   That was a lot of effort, right?
[00:05:24.120 --> 00:05:25.960]   So it requires so much engineering.
[00:05:25.960 --> 00:05:28.240]   And you have 50 milliseconds to make a decision?
[00:05:28.240 --> 00:05:29.240]   That sounds like a real feat.
[00:05:29.240 --> 00:05:33.240]   Was it, were you doing that or were you kind of handing it over to a different team?
[00:05:33.240 --> 00:05:36.000]   No, no, we were doing it internally, right?
[00:05:36.000 --> 00:05:41.120]   No, no, we had to because it required a lot of expertise, but deep nets, right?
[00:05:41.120 --> 00:05:44.960]   And what could make them go faster or slower, the batch size, all those things, right?
[00:05:44.960 --> 00:05:47.200]   So we had to do everything in-house.
[00:05:47.200 --> 00:05:50.040]   And were you like retraining live too?
[00:05:50.040 --> 00:05:51.600]   Or how did that work?
[00:05:51.600 --> 00:05:53.640]   So we did that, but later.
[00:05:53.640 --> 00:05:58.880]   So that was the first part where we used deep learning only for images and text mostly,
[00:05:58.880 --> 00:05:59.880]   right?
[00:05:59.880 --> 00:06:02.640]   Look at abusive content, for example, and so on and so forth.
[00:06:02.640 --> 00:06:08.200]   But then after a little while, we started looking at other problems that were more fundamental
[00:06:08.200 --> 00:06:14.240]   to Twitter, like ad placement, time as ranking, things like that, that are more tabular based,
[00:06:14.240 --> 00:06:15.240]   right?
[00:06:15.240 --> 00:06:18.320]   So like using user features, item features, and trying to make the best prediction of
[00:06:18.320 --> 00:06:21.240]   whether a user is going to engage with some content.
[00:06:21.240 --> 00:06:24.400]   And we also managed, we used deep learning for that too.
[00:06:24.400 --> 00:06:29.080]   And we managed to get better results than traditional techniques, basically.
[00:06:29.080 --> 00:06:33.720]   And so the reason why I started with all of that is because in that case, for example,
[00:06:33.720 --> 00:06:37.720]   for ad placement, it's very important to have access to the latest and greatest features.
[00:06:37.720 --> 00:06:41.800]   And so to do online learning, what we call online learning, which is, yeah, like learning
[00:06:41.800 --> 00:06:45.480]   continuously or like with very high frequency.
[00:06:45.480 --> 00:06:49.200]   Because otherwise there is a quite like quick decay of performance, right?
[00:06:49.200 --> 00:06:53.960]   The decay, the half-life, I don't know, maybe something like one, two, three minutes, right?
[00:06:53.960 --> 00:06:54.960]   Something like that.
[00:06:54.960 --> 00:06:56.280]   And then the model starts to decay.
[00:06:56.280 --> 00:07:00.320]   So we had to do online learning for that, yes.
[00:07:00.320 --> 00:07:01.320]   Wow.
[00:07:01.320 --> 00:07:03.640]   So you would retrain every minute?
[00:07:03.640 --> 00:07:07.000]   So there are multiple ways of doing it.
[00:07:07.000 --> 00:07:09.360]   One way to do it is just to do online learning right away.
[00:07:09.360 --> 00:07:12.920]   So just keep training online.
[00:07:12.920 --> 00:07:18.560]   You could also like freeze some of the layers and only retrain the last logistic regression,
[00:07:18.560 --> 00:07:19.560]   for example.
[00:07:19.560 --> 00:07:22.440]   But this is the easiest one, I think, actually.
[00:07:22.440 --> 00:07:26.240]   We can learn some kind of like, you know, I know if you're familiar with wide and deep
[00:07:26.240 --> 00:07:28.040]   architecture, for example.
[00:07:28.040 --> 00:07:34.400]   But yeah, you could use the memorization part, which is usually where the decay happens and
[00:07:34.400 --> 00:07:38.760]   keep retraining that one, basically, and keep the other one constant.
[00:07:38.760 --> 00:07:40.360]   Or I think some companies do that.
[00:07:40.360 --> 00:07:41.520]   I think Google does that, for example.
[00:07:41.520 --> 00:07:46.600]   I'm not so sure, but they retrain regularly, like every five minutes, maybe.
[00:07:46.600 --> 00:07:52.880]   So they take the existing model in prod, fine tune it, redeploy, fine tune it, redeploy.
[00:07:52.880 --> 00:07:53.880]   So that's possible.
[00:07:53.880 --> 00:07:56.920]   Do you ever go back and like sort of retrain it from scratch or is it always just sort
[00:07:56.920 --> 00:07:57.920]   of like online?
[00:07:57.920 --> 00:07:58.920]   Yeah, we do.
[00:07:58.920 --> 00:08:08.120]   Mostly if we want to add new features or radically change the model architecture.
[00:08:08.120 --> 00:08:12.820]   How do you even, I guess, how do you even evaluate then if like a new architecture is
[00:08:12.820 --> 00:08:13.820]   going to be better?
[00:08:13.820 --> 00:08:15.440]   Like it seems like that would be kind of tricky, right?
[00:08:15.440 --> 00:08:16.440]   Yeah.
[00:08:16.440 --> 00:08:21.040]   So we have to simulate the fact that it's online learning, basically.
[00:08:21.040 --> 00:08:26.240]   And so in that case, there has to be like a time period where we say, okay, we stop
[00:08:26.240 --> 00:08:31.400]   training and we look at everything that's after, and then we can evaluate by keeping
[00:08:31.400 --> 00:08:32.400]   learning.
[00:08:32.400 --> 00:08:36.720]   It's possible to simulate the situation basically, right?
[00:08:36.720 --> 00:08:41.080]   But yeah, it's more infrastructure, basically.
[00:08:41.080 --> 00:08:42.080]   Wow.
[00:08:42.240 --> 00:08:47.560]   And so, I mean, this must have been an incredibly high amount of compute.
[00:08:47.560 --> 00:08:50.200]   It's yeah, pretty high, pretty high back then.
[00:08:50.200 --> 00:08:51.200]   Yeah.
[00:08:51.200 --> 00:08:52.200]   Yeah.
[00:08:52.200 --> 00:08:54.240]   I mean, interestingly, old CPU though.
[00:08:54.240 --> 00:08:55.240]   All CPU.
[00:08:55.240 --> 00:09:02.080]   Yeah, old CPU because GPUs were not, I mean, I work at NVIDIA now, but they were not that
[00:09:02.080 --> 00:09:03.960]   easy to use in that context.
[00:09:03.960 --> 00:09:05.920]   There was less tooling.
[00:09:05.920 --> 00:09:11.040]   Now it's changing with, I don't know if you heard about Rapids, for example, or so basically
[00:09:11.040 --> 00:09:15.800]   data science, actually, on GPU, a lot of libraries available now, but back then there was none
[00:09:15.800 --> 00:09:16.800]   of that.
[00:09:16.800 --> 00:09:19.400]   So we had to just, we actually had to code on CPU basically.
[00:09:19.400 --> 00:09:20.400]   Wow.
[00:09:20.400 --> 00:09:21.400]   Make it really, really fast.
[00:09:21.400 --> 00:09:22.400]   Yeah.
[00:09:22.400 --> 00:09:26.080]   And were there other pieces of infrastructure that you had to build to get this working
[00:09:26.080 --> 00:09:27.080]   in 2016?
[00:09:27.080 --> 00:09:28.080]   Oh yeah, 2016.
[00:09:28.080 --> 00:09:31.960]   Well, so you mean besides the inference selling, actually?
[00:09:31.960 --> 00:09:32.960]   Yeah.
[00:09:32.960 --> 00:09:33.960]   Yeah.
[00:09:33.960 --> 00:09:36.440]   I think we had to for the training part.
[00:09:36.440 --> 00:09:42.600]   One of the challenges we had is that a lot of our customers were used to decision trees
[00:09:42.600 --> 00:09:45.360]   and certain APIs and configurations.
[00:09:45.360 --> 00:09:47.800]   They were not familiar with LuaTorch.
[00:09:47.800 --> 00:09:49.840]   Few people are familiar with Lua in general.
[00:09:49.840 --> 00:09:54.760]   So we kind of had to hide this under like, you know, configurations.
[00:09:54.760 --> 00:09:59.680]   So we built infrastructure to basically simplify their life such that they could copy paste
[00:09:59.680 --> 00:10:01.560]   configurations, right?
[00:10:01.560 --> 00:10:03.320]   And just specify their features basically.
[00:10:03.320 --> 00:10:08.040]   Like, okay, these are the features I have, these are the steps I want to run, you know,
[00:10:08.040 --> 00:10:10.320]   like training, validation, whatever.
[00:10:10.320 --> 00:10:14.920]   And then basically, yeah, basically like automatically save their model and so on.
[00:10:14.920 --> 00:10:20.360]   So we brought a lot of automation in the training phase at the cost of flexibility at the beginning.
[00:10:20.360 --> 00:10:21.360]   Then it changed.
[00:10:21.360 --> 00:10:22.360]   But yeah.
[00:10:22.360 --> 00:10:23.360]   And how did it change?
[00:10:23.360 --> 00:10:24.480]   Like how did it evolve?
[00:10:24.480 --> 00:10:30.880]   So then once the company started realizing the impact and the importance of this at Twitter,
[00:10:30.880 --> 00:10:36.880]   yeah, they did, I mean, we like, they decided to also hire people who could understand and
[00:10:36.880 --> 00:10:37.880]   really invest in education.
[00:10:37.880 --> 00:10:38.880]   That was one.
[00:10:38.880 --> 00:10:43.200]   And at the same time, we decided to like as a centralized machine learning platform to
[00:10:43.200 --> 00:10:46.400]   basically to move to TensorFlow.
[00:10:46.400 --> 00:10:47.400]   Why TensorFlow?
[00:10:47.400 --> 00:10:52.560]   Because by then PyTorch was still very kind of like new and unstable, not even 1.0, I
[00:10:52.560 --> 00:10:53.560]   think.
[00:10:53.560 --> 00:10:57.680]   And so moving to TensorFlow, which also had like an inference story, right?
[00:10:57.680 --> 00:10:59.440]   Which PyTorch doesn't have and so on and so forth.
[00:10:59.440 --> 00:11:03.080]   That's why there are so many recommender systems using TensorFlow those days, right?
[00:11:03.080 --> 00:11:04.880]   Because they have like this complete story.
[00:11:04.880 --> 00:11:10.400]   Anyway, so we moved to TensorFlow pretty quickly after that for training and inference.
[00:11:10.400 --> 00:11:11.400]   I see.
[00:11:11.400 --> 00:11:13.560]   Or training at least, not inference.
[00:11:13.560 --> 00:11:14.560]   Oh, not for inference.
[00:11:14.560 --> 00:11:15.560]   No, not really.
[00:11:15.560 --> 00:11:22.080]   Some part of it, just the library part, you know, just the C++ library part.
[00:11:22.080 --> 00:11:27.480]   But the thing is, Twitter has their own data formats and serialization formats.
[00:11:27.480 --> 00:11:29.180]   So they had to play with that.
[00:11:29.180 --> 00:11:32.600]   So for example, Thrift instead of Proto, right?
[00:11:32.600 --> 00:11:33.600]   Yeah, yeah.
[00:11:33.600 --> 00:11:40.200]   Were there any, I guess, are there any other sort of like surprisingly challenging things
[00:11:40.200 --> 00:11:41.200]   at that time?
[00:11:41.200 --> 00:11:44.280]   Like stuff that like, you know, maybe like academics or people that don't work in these
[00:11:44.280 --> 00:11:46.280]   sort of large scale deployments wouldn't know about?
[00:11:46.280 --> 00:11:50.080]   Like any other like tricky pieces?
[00:11:50.080 --> 00:11:52.880]   So you mean from Twitter people?
[00:11:52.880 --> 00:11:53.880]   Yeah, Twitter.
[00:11:53.880 --> 00:11:54.880]   Yeah.
[00:11:54.880 --> 00:11:58.660]   I think in some parts there was a disbelief around deploying.
[00:11:58.660 --> 00:12:03.740]   I don't know if that's what you're asking exactly.
[00:12:03.740 --> 00:12:11.380]   And I think it still exists in the medical field, for example, where people ask for interpretability
[00:12:11.380 --> 00:12:14.980]   or explainability and so on and so forth.
[00:12:14.980 --> 00:12:22.420]   And even at the expense of better performance, but eventually that disappeared.
[00:12:22.420 --> 00:12:28.620]   You know, it's so funny in like 2005, I worked at Yahoo moving models from like rule-based
[00:12:28.620 --> 00:12:31.820]   ranking systems to boosted trees.
[00:12:31.820 --> 00:12:33.740]   And you know, they had all the exact same complaints.
[00:12:33.740 --> 00:12:35.980]   Like people are like, "Oh, these models are unexplainable.
[00:12:35.980 --> 00:12:37.620]   They're like impossible to deploy."
[00:12:37.620 --> 00:12:40.380]   Like chaos in our infrastructure.
[00:12:40.380 --> 00:12:43.780]   It's like exactly the same, but now they have the same complaints about moving away from
[00:12:43.780 --> 00:12:44.780]   decision trees.
[00:12:44.780 --> 00:12:45.780]   Yeah, exactly.
[00:12:45.780 --> 00:12:50.180]   Well, the difference for infrastructure is that we replicated almost the same APIs as
[00:12:50.180 --> 00:12:53.060]   what Twitter had for decision trees.
[00:12:53.060 --> 00:12:54.060]   So it was a little bit easier.
[00:12:54.060 --> 00:12:55.700]   It was already kind of ML ready.
[00:12:55.700 --> 00:12:59.020]   Whereas in your case, I guess it was like completely different, right?
[00:12:59.020 --> 00:13:00.020]   I'm sure.
[00:13:00.020 --> 00:13:03.660]   Yeah, but it sounds like you had to kind of add some weird components and sort of abstract
[00:13:03.660 --> 00:13:05.220]   away in the same way that...
[00:13:05.220 --> 00:13:10.340]   Oh yeah, just basically abstracted everything away and made it look the same basically to
[00:13:10.340 --> 00:13:11.340]   gain adoption.
[00:13:11.340 --> 00:13:15.740]   That was fun.
[00:13:15.740 --> 00:13:18.260]   So when did you move to NVIDIA?
[00:13:18.260 --> 00:13:21.100]   That was like a year and a half ago, 18 months.
[00:13:21.100 --> 00:13:24.620]   And so tell me about the stuff that you've been working on at NVIDIA.
[00:13:24.620 --> 00:13:29.180]   Yeah, so it's quite different in terms of the application domain.
[00:13:29.180 --> 00:13:39.660]   However, I'm basically managing the team building the platform to develop autonomous vehicle
[00:13:39.660 --> 00:13:40.660]   software.
[00:13:40.660 --> 00:13:46.500]   So, and in autonomous vehicle software, I also include like deep neural networks, right?
[00:13:46.500 --> 00:13:48.740]   Like all the validation required for it and so on.
[00:13:48.740 --> 00:13:49.740]   So this is what I'm managing.
[00:13:49.740 --> 00:13:52.100]   And it's a pretty big endeavor.
[00:13:52.100 --> 00:13:55.580]   The reason for that is autonomous vehicles are such large scales.
[00:13:55.580 --> 00:13:58.700]   There are so many models, so many people working on it.
[00:13:58.700 --> 00:14:03.220]   And so, I mean, there are so many specific needs that we have to build a relatively custom
[00:14:03.220 --> 00:14:09.780]   infrastructure in order to be able to be efficient and good at it and competitive.
[00:14:09.780 --> 00:14:13.700]   And do you mean it's like custom infrastructure for self-driving cars or custom infrastructure
[00:14:13.700 --> 00:14:19.540]   for every individual team working on self-driving cars?
[00:14:19.540 --> 00:14:26.380]   So it's the nature of developing models and what we call perception.
[00:14:26.380 --> 00:14:29.420]   So the ability to understand the world from a car.
[00:14:29.420 --> 00:14:32.740]   So multiple models plus custom modules and so on and so forth.
[00:14:32.740 --> 00:14:39.820]   Developing this requires a lot of customization in the cloud infrastructure, basically, is
[00:14:39.820 --> 00:14:40.820]   what I'm saying.
[00:14:41.260 --> 00:14:47.220]   As an example, all machine learning teams use a workflow system in order to say, "Hey,
[00:14:47.220 --> 00:14:50.420]   I want to do this task and then do this task and then another task," right?
[00:14:50.420 --> 00:14:51.700]   And so on and so forth.
[00:14:51.700 --> 00:14:59.140]   In the case of autonomous vehicles, the big difficulty is that the various steps are going
[00:14:59.140 --> 00:15:02.260]   to be in so many different languages, so many different libraries.
[00:15:02.260 --> 00:15:05.540]   So one is going to be like data preparation using Spark.
[00:15:05.540 --> 00:15:09.500]   One is going to be like, "Oh, now I want to run the actual software from the car on the
[00:15:09.500 --> 00:15:11.500]   target hardware," right?
[00:15:11.500 --> 00:15:15.780]   Which is the actual embedded hardware that's wrapped in the cloud, but I want to run it
[00:15:15.780 --> 00:15:18.300]   on this using CUDA.
[00:15:18.300 --> 00:15:20.660]   And then I want to run a Golang container.
[00:15:20.660 --> 00:15:25.020]   So all of these things are so different that they require a workflow system that's agnostic
[00:15:25.020 --> 00:15:29.540]   to all of this and that can be deployed on heterogeneous hardware.
[00:15:29.540 --> 00:15:34.140]   I mean, bypassing the details, but basically in some aspects, we have to develop our own
[00:15:34.140 --> 00:15:35.140]   customized infrastructure.
[00:15:35.140 --> 00:15:36.140]   Got it.
[00:15:36.460 --> 00:15:42.260]   And so I guess you were sort of starting to talk about this, but what are the big components
[00:15:42.260 --> 00:15:48.140]   of the infrastructure that you build and what are the big problems that each component solves?
[00:15:48.140 --> 00:15:49.140]   Yeah, yeah.
[00:15:49.140 --> 00:15:54.140]   I mean, at the top level, so where we interact with our users, we really provide tools and
[00:15:54.140 --> 00:15:56.660]   SDKs and libraries.
[00:15:56.660 --> 00:15:57.660]   So that's the top level.
[00:15:57.660 --> 00:16:01.980]   At the bottom level, we have really core components that enable this top level.
[00:16:01.980 --> 00:16:06.220]   So at the top level, what we do is we go from everything outside the car.
[00:16:06.220 --> 00:16:13.260]   So when people drive, like drivers basically collect data or test a new build of the software
[00:16:13.260 --> 00:16:19.500]   system, then they take out the data or send it over the Wi-Fi, then it gets into the system.
[00:16:19.500 --> 00:16:20.500]   It needs to be ingested.
[00:16:20.500 --> 00:16:22.540]   So that's the first step, ingestion.
[00:16:22.540 --> 00:16:26.820]   Ingestion is already pretty complicated because it's similar to Yahoo or Twitter where you
[00:16:26.820 --> 00:16:33.580]   need to write heavy and then have some way to process the data once in a while to transform
[00:16:33.580 --> 00:16:36.860]   it into data sets that are more consumable by downstream users.
[00:16:36.860 --> 00:16:37.860]   We have to do that.
[00:16:37.860 --> 00:16:42.180]   The challenges are pretty massive because we need to test for data quality, for example,
[00:16:42.180 --> 00:16:44.140]   or we need to index the data.
[00:16:44.140 --> 00:16:48.140]   We need to process raw data to transform it into something that's easier to consume downstream
[00:16:48.140 --> 00:16:49.140]   as well and so on and so forth.
[00:16:49.140 --> 00:16:50.140]   So that's the first step.
[00:16:50.140 --> 00:16:52.860]   Second step is to build the best data sets.
[00:16:52.860 --> 00:16:54.780]   And that's actually a big challenge.
[00:16:54.780 --> 00:16:59.740]   The way we approach it is that, I mean, I'm sure you're familiar with that, but we view
[00:16:59.740 --> 00:17:05.740]   like machine learning as a software 2.0, like as Carpati kind of laid it out.
[00:17:05.740 --> 00:17:10.220]   I don't know if he was the first, but where data is the source code of machine learning.
[00:17:10.220 --> 00:17:14.340]   And so we need to be very careful about how we write our source code.
[00:17:14.340 --> 00:17:17.700]   And in order to do that, we are developing tools to curate data sets.
[00:17:17.700 --> 00:17:23.180]   So like create data sets, select the right frames, the right videos with the right filters,
[00:17:23.180 --> 00:17:26.500]   make sure there's no overlap between training and validation.
[00:17:26.500 --> 00:17:28.420]   So we have a lot of tooling for that.
[00:17:28.420 --> 00:17:33.300]   And so these are tools, they don't actually do this, they like help a user pick this or
[00:17:33.300 --> 00:17:36.500]   do they somehow automatically pick the...
[00:17:36.500 --> 00:17:38.940]   Yeah, so both actually.
[00:17:38.940 --> 00:17:42.180]   We're also investing a lot in active learning.
[00:17:42.180 --> 00:17:45.860]   Since you were at Figure 8, I'm sure you have a lot of experience there.
[00:17:45.860 --> 00:17:50.060]   Yeah, but I'm always fascinated by how other people do it too.
[00:17:50.060 --> 00:17:55.660]   We published a blog post recently exactly about that basically, where autonomous vehicles
[00:17:55.660 --> 00:18:00.100]   is perfect and lends itself perfectly to active learning.
[00:18:00.100 --> 00:18:03.180]   Massive amounts of data, but very costly human labelling.
[00:18:03.180 --> 00:18:07.940]   So if you want to do 3D cuboid labelling, it's so costly.
[00:18:07.940 --> 00:18:13.460]   However, there are like thousands and thousands of hours of data of driving available.
[00:18:13.460 --> 00:18:16.660]   And so we really have to select the one that's going to be the most efficient and that's
[00:18:16.660 --> 00:18:21.620]   going to find the pattern that the DNN deep neural network is not able to find.
[00:18:21.620 --> 00:18:25.300]   And in order to do that, we use active learning and active learning basically gives us like
[00:18:25.300 --> 00:18:27.100]   uncertainty scores, right?
[00:18:27.100 --> 00:18:31.260]   And the frames are videos with the highest uncertainty are basically the ones that we're
[00:18:31.260 --> 00:18:33.660]   going to want to label in order to improve the performance.
[00:18:33.660 --> 00:18:40.220]   So we tried it and we get like a 3x higher improvement, 3 to 5x actually, higher improvements
[00:18:40.220 --> 00:18:44.940]   using active learning sample data versus manually curated, not even random, manually.
[00:18:44.940 --> 00:18:46.940]   So like by humans, because...
[00:18:46.940 --> 00:18:49.900]   So like humans are guessing what data is going to be the best.
[00:18:49.900 --> 00:18:51.140]   Yeah, yeah, yeah, exactly.
[00:18:51.140 --> 00:18:58.660]   So they're like, the challenge was that let's find VRUs, vulnerable road users at night,
[00:18:58.660 --> 00:18:59.660]   at nighttime.
[00:18:59.660 --> 00:19:03.620]   And so it's a super challenging problem because of course for cars, it's difficult to view
[00:19:03.620 --> 00:19:04.620]   at night, right?
[00:19:04.620 --> 00:19:05.620]   With the camera and so on.
[00:19:05.620 --> 00:19:11.220]   And in general, like pedestrians and bicycles, basically, these are the two categories, are
[00:19:11.220 --> 00:19:12.620]   also difficult to detect.
[00:19:12.620 --> 00:19:17.140]   Anyway, so the idea was to detect these ones.
[00:19:17.140 --> 00:19:19.740]   So first school was like manual curation.
[00:19:19.740 --> 00:19:25.220]   A group of people were told, look through the videos and find images that are relevant
[00:19:25.220 --> 00:19:26.860]   for these classes.
[00:19:26.860 --> 00:19:30.900]   And the other group was like just using the models for these specific classes, find the
[00:19:30.900 --> 00:19:35.260]   frames that have the highest uncertainty for these two classes, right?
[00:19:35.260 --> 00:19:40.260]   So one was completely automated and we were able to find a frame that were very, very
[00:19:40.260 --> 00:19:44.140]   uncertain basically for pedestrian and bicycles, right?
[00:19:44.140 --> 00:19:45.660]   Which shows maybe 20,000 of them.
[00:19:45.660 --> 00:19:48.460]   Then the manual curation did the same, 20,000.
[00:19:48.460 --> 00:19:53.740]   What they did usually is that they swipe through videos and when they find pedestrians or bicycles
[00:19:53.740 --> 00:19:58.140]   at night, they just stop and they select like, you know, a few frames in that segment of
[00:19:58.140 --> 00:19:59.140]   video.
[00:19:59.140 --> 00:20:03.180]   And we trained model with these two sub datasets.
[00:20:03.180 --> 00:20:06.620]   We looked at the validation performance and validation performance was three times higher.
[00:20:06.620 --> 00:20:10.980]   I mean, the increase in what three times higher for active learning, selective data.
[00:20:10.980 --> 00:20:15.260]   So it does work and it can be completely automated if you think of it, right?
[00:20:15.260 --> 00:20:16.260]   Wow.
[00:20:16.260 --> 00:20:17.260]   That's really impressive.
[00:20:17.260 --> 00:20:18.260]   That's amazing.
[00:20:18.260 --> 00:20:19.260]   And that's in your blog post.
[00:20:19.260 --> 00:20:20.260]   We should definitely get a link to that.
[00:20:20.260 --> 00:20:21.260]   Yeah, yeah.
[00:20:21.260 --> 00:20:22.260]   No, no, I can share that with you.
[00:20:22.260 --> 00:20:25.940]   I mean, we were impressed too, basically, because it was just an experiment, you know,
[00:20:25.940 --> 00:20:26.940]   a research experiment.
[00:20:26.940 --> 00:20:32.260]   And now we're working to automate that and to be able to even automatically select data,
[00:20:32.260 --> 00:20:34.060]   retrain models and improve performance.
[00:20:34.060 --> 00:20:36.860]   And we could have a machine fabricating DNA, right?
[00:20:36.860 --> 00:20:37.860]   Yeah, yeah.
[00:20:37.860 --> 00:20:41.060]   With human in the loop, just for the human annotation.
[00:20:41.060 --> 00:20:42.060]   So what else?
[00:20:42.060 --> 00:20:44.820]   Sorry, I think I've riled you.
[00:20:44.820 --> 00:20:45.820]   So there's data collection.
[00:20:45.820 --> 00:20:50.580]   Yeah, there's labeling, which I'm sure you're very familiar with because you're working
[00:20:50.580 --> 00:20:51.580]   at it.
[00:20:51.580 --> 00:20:54.700]   I figured, but yeah, for autonomous vehicles, there are some pretty massive challenges.
[00:20:54.700 --> 00:20:55.700]   So first the scale.
[00:20:55.700 --> 00:20:56.820]   Scale is massive, right?
[00:20:56.820 --> 00:21:01.620]   So NVIDIA has a thousand plus labelers in India.
[00:21:01.620 --> 00:21:03.860]   Yeah, we're doing it ourselves.
[00:21:03.860 --> 00:21:08.900]   The software to actually be able to dispatch requests, right, to these labelers and manage
[00:21:08.900 --> 00:21:13.620]   it as you probably know, is quite complex because it has to deal with human workflows,
[00:21:13.620 --> 00:21:14.620]   right?
[00:21:14.620 --> 00:21:15.980]   And the way they behave.
[00:21:15.980 --> 00:21:18.980]   So when they refuse, when they make mistakes and so on and so forth.
[00:21:18.980 --> 00:21:21.380]   Integrate quality assessment in the loop and so on and so forth.
[00:21:21.380 --> 00:21:25.020]   So that's one, but also the tools themselves, the UI tools are pretty tricky.
[00:21:25.020 --> 00:21:30.980]   So for example, we need sometimes to be able to draw like, for example, 3D cuboids and
[00:21:30.980 --> 00:21:33.980]   make sure we can link LIDAR data with image data, right?
[00:21:33.980 --> 00:21:41.500]   So we need to have a mix of like human labeling and like automated computing to be able to
[00:21:41.500 --> 00:21:47.620]   link these two things, for example, or build a new representation of the data that is then
[00:21:47.620 --> 00:21:50.740]   usable by those humans.
[00:21:50.740 --> 00:21:55.100]   And I think the two at the same time is pretty complex and difficult to do.
[00:21:55.100 --> 00:21:57.700]   So yeah, so we built all that.
[00:21:57.700 --> 00:22:01.980]   So that's step number three, basically, labeling the data.
[00:22:01.980 --> 00:22:04.240]   Then step number four is about training.
[00:22:04.240 --> 00:22:09.820]   So we've developed a lot of code to enable our heavy developers to train their models.
[00:22:09.820 --> 00:22:15.580]   One of the biggest challenges we have is that once we train, we need to do inference on
[00:22:15.580 --> 00:22:17.300]   an embedded system.
[00:22:17.300 --> 00:22:20.340]   And so we compute constraint in a way.
[00:22:20.340 --> 00:22:22.340]   I mean, this is one of the constraints.
[00:22:22.340 --> 00:22:26.540]   We cannot deploy like a thousand servers to be able to crush everything.
[00:22:26.540 --> 00:22:29.740]   We need to use a single chip to compute everything.
[00:22:29.740 --> 00:22:38.260]   So in order to make that right, we use multitask training, for example, where we have one single
[00:22:38.260 --> 00:22:44.740]   model body that can predict multiple things like path detection or obstacle detection
[00:22:44.740 --> 00:22:51.580]   or light sign intersection, so on and so forth.
[00:22:51.580 --> 00:22:54.100]   I think this is similar to what Tesla is doing.
[00:22:54.100 --> 00:22:58.820]   They've written a blog, they've done a talk recently talking about that.
[00:22:58.820 --> 00:23:04.980]   There's that and then there's a lot of optimization such as pruning the models or intake quantization
[00:23:04.980 --> 00:23:09.740]   or neural architecture search that we can use in order to even further reduce the size
[00:23:09.740 --> 00:23:13.380]   of the model with equal performance.
[00:23:13.380 --> 00:23:15.700]   And so your tools do all of this?
[00:23:15.700 --> 00:23:20.140]   Is there stuff left for a perception team and a customer to do?
[00:23:20.140 --> 00:23:21.660]   Or how do you think about that?
[00:23:21.660 --> 00:23:22.660]   Yeah, yeah.
[00:23:22.660 --> 00:23:23.660]   No, no.
[00:23:23.660 --> 00:23:26.820]   We provide this as part of the core libraries.
[00:23:26.820 --> 00:23:28.620]   But of course, sometimes they need to do something new.
[00:23:28.620 --> 00:23:31.980]   When they need to do that, they can add their own algorithm and so on and so forth.
[00:23:31.980 --> 00:23:35.460]   And then we basically productionize it, platformize it.
[00:23:35.460 --> 00:23:40.100]   Also what they're really focused on, on the perception side, is not really those features.
[00:23:40.100 --> 00:23:43.740]   It's more like, I mean, they're looking into it, that's very important, but also looking
[00:23:43.740 --> 00:23:47.420]   a lot into new types of predictions, for example.
[00:23:47.420 --> 00:23:51.100]   So they were predicting bicycles, now they want to predict more fine-grained things,
[00:23:51.100 --> 00:23:52.100]   right?
[00:23:52.100 --> 00:23:53.100]   So they're going to have classes.
[00:23:53.100 --> 00:23:56.340]   So they do a lot of those things, basically, that we don't have to care about.
[00:23:56.340 --> 00:23:58.340]   We just provide the core infrastructure.
[00:23:58.340 --> 00:24:04.900]   So you provide core infrastructure to do multitask learning and quantization, but then the customer
[00:24:04.900 --> 00:24:09.260]   would provide the different types of classifications that they would want?
[00:24:09.260 --> 00:24:10.260]   Yeah, exactly.
[00:24:10.260 --> 00:24:13.700]   But of course, there's a small overlap between the two and we help each other.
[00:24:13.700 --> 00:24:18.620]   Do you handle things like some of the newer stuff, like trying to figure out intentions
[00:24:18.620 --> 00:24:25.900]   or trying to actually map out the underlying dynamics of a person, like where their arms
[00:24:25.900 --> 00:24:28.100]   are and head is and stuff like that?
[00:24:28.100 --> 00:24:29.740]   Is that within your scope?
[00:24:29.740 --> 00:24:35.500]   Yeah, I mean, not my team specifically, but the perception team is definitely looking
[00:24:35.500 --> 00:24:37.220]   into things like that.
[00:24:37.220 --> 00:24:45.220]   Yes, that's usually more on the research side, a little bit more advanced, but yes, definitely.
[00:24:45.220 --> 00:24:54.060]   So does this work with different ML frameworks or is it like a lower level than that or how
[00:24:54.060 --> 00:24:55.060]   does that work?
[00:24:55.060 --> 00:24:56.660]   Yeah, no, no.
[00:24:56.660 --> 00:25:01.860]   We do work with the ML frameworks just because they provide so much value.
[00:25:01.860 --> 00:25:08.340]   So yeah, for training specifically, so we use TensorFlow a lot, PyTorch a little bit
[00:25:08.340 --> 00:25:09.340]   too.
[00:25:09.340 --> 00:25:12.180]   I think it's mostly historical.
[00:25:12.180 --> 00:25:18.580]   And then for deployment though, for deployment, we use TensorRT, which is NVIDIA's deep learning
[00:25:18.580 --> 00:25:21.660]   inference library.
[00:25:21.660 --> 00:25:26.380]   And what's great is that it's really optimized for NVIDIA hardware, of course.
[00:25:26.380 --> 00:25:29.460]   There's a lot of like, it's also optimized for inference.
[00:25:29.460 --> 00:25:33.460]   So you can do some optimization of the graph, for example.
[00:25:33.460 --> 00:25:35.260]   And yeah, we deploy using TensorRT.
[00:25:35.260 --> 00:25:40.020]   So yeah, and we get pretty big performance gains with that.
[00:25:40.020 --> 00:25:41.020]   Cool.
[00:25:41.020 --> 00:25:42.020]   So is that the whole thing?
[00:25:42.020 --> 00:25:43.020]   So you sort of have data collection?
[00:25:43.020 --> 00:25:44.020]   No, that's not all.
[00:25:44.020 --> 00:25:45.020]   That's not all?
[00:25:45.020 --> 00:25:46.020]   Wow, amazing.
[00:25:46.020 --> 00:25:47.020]   What else do you have?
[00:25:47.020 --> 00:25:48.020]   No, there's evaluation.
[00:25:48.020 --> 00:25:49.020]   So the validation of the model.
[00:25:49.020 --> 00:25:54.580]   So let's say you train one model that's doing obstacle detection.
[00:25:54.580 --> 00:25:58.820]   What you really want is understanding the modification of that model, for example,
[00:25:58.820 --> 00:26:02.300]   is how is that going to impact the overall system?
[00:26:02.300 --> 00:26:08.980]   That's a very tricky system that requires a pretty fine grain understanding of the impact.
[00:26:08.980 --> 00:26:13.380]   And so let's say we have this perception system that's a mix of kind of like post-processing,
[00:26:13.380 --> 00:26:16.100]   Kalman filters, neural networks, and so on and so forth.
[00:26:16.100 --> 00:26:18.820]   They're all mixed in pretty complex ways.
[00:26:18.820 --> 00:26:23.220]   What we want to do is like have multiple levels of KPIs and a pretty large breadth of KPI
[00:26:23.220 --> 00:26:25.620]   to understand what's happening in the system.
[00:26:25.620 --> 00:26:26.620]   That's the step number one.
[00:26:26.620 --> 00:26:30.140]   So for example, like false positive, false negative, sure, right?
[00:26:30.140 --> 00:26:36.300]   But then the next level, which is at the perception API level, so like higher level, you know,
[00:26:36.300 --> 00:26:39.460]   how many mistakes do I make per hour, for example, right?
[00:26:39.460 --> 00:26:41.620]   Of detection of a car.
[00:26:41.620 --> 00:26:45.860]   And then I also want to understand even further than that, how do I drive the car?
[00:26:45.860 --> 00:26:47.820]   So which involves simulation in that case.
[00:26:47.820 --> 00:26:52.820]   So we want to be able to run simulation jobs with this new perception system to understand
[00:26:52.820 --> 00:26:56.140]   like how the system behaves now with the same simulation.
[00:26:56.140 --> 00:26:57.940]   So we want to do all of this.
[00:26:57.940 --> 00:27:03.740]   So we have a system to basically evaluate all of these things at scale together, which
[00:27:03.740 --> 00:27:09.380]   is on the same infrastructure, so like same data structures, you know, same dashboards,
[00:27:09.380 --> 00:27:13.540]   same kind of output data, same analytics library, and so on and so forth.
[00:27:13.540 --> 00:27:18.140]   And the output of this is like all these KPIs, plus what we call events.
[00:27:18.140 --> 00:27:22.380]   The false positive is an event, for example, you can define an event as anything.
[00:27:22.380 --> 00:27:28.020]   Once we have all of this, then the AV developers can look at all this information.
[00:27:28.020 --> 00:27:29.260]   And this is the next step.
[00:27:29.580 --> 00:27:35.140]   This is what we call debugging, basically, which is also like software 2.0, right?
[00:27:35.140 --> 00:27:37.540]   Debugging the output of a predictor.
[00:27:37.540 --> 00:27:41.980]   So we look at the output of the predictor, and we can look at the KPIs, look at the event,
[00:27:41.980 --> 00:27:47.420]   cluster the events, and then zoom in on to all of these events and like very fine grain,
[00:27:47.420 --> 00:27:52.060]   look at them and the prediction versus the ground truth, for example, or like, you know,
[00:27:52.060 --> 00:27:54.660]   see if there's something missing when there is a lens flare.
[00:27:54.660 --> 00:27:58.700]   So we can go very deep and then come back high and then make a diagnosis about what's
[00:27:58.700 --> 00:28:00.380]   going wrong about the system.
[00:28:00.380 --> 00:28:03.660]   And this diagnosis is kind of like how we improve the system.
[00:28:03.660 --> 00:28:09.660]   This diagnosis tells us like, I need more data on Japan at night, for example.
[00:28:09.660 --> 00:28:14.100]   And then we can go back to the curation step, which is building better datasets.
[00:28:14.100 --> 00:28:19.380]   So yeah, this is kind of like the feedback loop that goes from debugging to this curation
[00:28:19.380 --> 00:28:24.100]   step that helps us improve our perception system over time.
[00:28:24.100 --> 00:28:29.420]   Gotcha, and so you basically can, your user could like automatically request like, you
[00:28:29.420 --> 00:28:34.620]   know, give me more like, you know, like bicyclists in the snow or something.
[00:28:34.620 --> 00:28:38.940]   And then like the curation step go out and look for more of that or like weight that
[00:28:38.940 --> 00:28:39.940]   more or something?
[00:28:39.940 --> 00:28:40.940]   Yeah, exactly.
[00:28:40.940 --> 00:28:47.340]   I mean, based on what the curation can do, which could be geographical conditions or
[00:28:47.340 --> 00:28:51.540]   maybe temperature if we have access to that type of sensor.
[00:28:51.540 --> 00:28:53.540]   But yeah, definitely.
[00:28:53.540 --> 00:28:54.540]   That's amazing.
[00:28:54.540 --> 00:28:58.060]   So I mean, how do you, I'm just trying to think like putting myself in your shoes, like
[00:28:58.060 --> 00:29:02.580]   how do you approach like making such a sophisticated system on behalf of customers?
[00:29:02.580 --> 00:29:08.580]   Like do you like build your own perception systems just to try your own software?
[00:29:08.580 --> 00:29:09.900]   How do you think about that?
[00:29:09.900 --> 00:29:15.060]   At the bottom of this like end-to-end workflow, we have core components basically, which is
[00:29:15.060 --> 00:29:18.260]   our data platform and our workflow management system.
[00:29:18.260 --> 00:29:20.860]   And those two things are powering everything, right?
[00:29:20.860 --> 00:29:29.620]   To be able to write ETLs, be able to register data sets, for example, be able to perform
[00:29:29.620 --> 00:29:35.060]   queries of our data and make sure that all of these things are traceable end-to-end,
[00:29:35.060 --> 00:29:38.260]   which is a major requirement for the autonomous vehicles industry.
[00:29:38.260 --> 00:29:40.980]   So that we can then, you know, if there's a problem in the future, we can go back in
[00:29:40.980 --> 00:29:43.020]   time and understand everything that happened.
[00:29:43.020 --> 00:29:49.260]   So anyway, all those things at the bottom are powering the top layer and are pretty,
[00:29:49.260 --> 00:29:52.260]   yeah I mean, pretty beefy and made for scale.
[00:29:52.260 --> 00:29:54.260]   And so, sorry, the first thing is data storage.
[00:29:54.260 --> 00:29:55.260]   Did I have that right?
[00:29:55.260 --> 00:29:56.260]   Yeah.
[00:29:56.260 --> 00:29:57.260]   Oh, it's data platform.
[00:29:57.260 --> 00:29:59.100]   Data platform and then a workflow management system.
[00:29:59.100 --> 00:30:00.780]   And workflow management system, yeah.
[00:30:00.780 --> 00:30:01.780]   Gotcha.
[00:30:01.780 --> 00:30:02.780]   And so the data, what is it?
[00:30:02.780 --> 00:30:06.820]   So the data platform is just, is like keeping track of where all the data is or what does
[00:30:06.820 --> 00:30:07.820]   that do?
[00:30:07.820 --> 00:30:08.820]   No, it's a bit more than that.
[00:30:08.820 --> 00:30:15.020]   So basically it's all the infrastructure required in order to store structured data, structured
[00:30:15.020 --> 00:30:16.620]   and unstructured data.
[00:30:16.620 --> 00:30:17.620]   Right?
[00:30:17.620 --> 00:30:24.420]   So structured data could be anything like, I don't know, like simple floating points,
[00:30:24.420 --> 00:30:28.580]   continuous values and raw data is like all the sensor recordings in general.
[00:30:28.580 --> 00:30:30.780]   So we have all of this and we can organize it.
[00:30:30.780 --> 00:30:35.540]   And the second step is we want to be able to query all of this at scale.
[00:30:35.540 --> 00:30:41.420]   And so basically we use Hive, we use Presto, we use Spark SQL and Spark in general to enable
[00:30:41.420 --> 00:30:42.700]   us to do all of this.
[00:30:42.700 --> 00:30:47.700]   And so this is what the data platform provides, all those pieces.
[00:30:47.700 --> 00:30:56.380]   And then the workflow management is more around like the ability to schedule like those complex
[00:30:56.380 --> 00:30:59.980]   compute and data access tasks, right?
[00:30:59.980 --> 00:31:01.860]   And stitch them together.
[00:31:01.860 --> 00:31:06.660]   And so basically we know we can organize data in a certain way.
[00:31:06.660 --> 00:31:10.460]   We know we can access a cluster, but then we want to make sure that, yeah, like I explained
[00:31:10.460 --> 00:31:13.860]   earlier, we can perform those graph of tasks.
[00:31:13.860 --> 00:31:17.540]   And sometimes we require a lot of scale when we do evaluation at scale, for example, on
[00:31:17.540 --> 00:31:18.860]   like thousands of hours of data.
[00:31:18.860 --> 00:31:23.500]   And so we need a workflow system that enables us to do all of this.
[00:31:23.500 --> 00:31:24.500]   Interesting.
[00:31:24.500 --> 00:31:29.020]   So I guess like one thing I didn't hear you say that I think a lot of people talk about
[00:31:29.020 --> 00:31:31.060]   is sort of synthetic data.
[00:31:31.060 --> 00:31:33.540]   Like is that, is that interesting to you?
[00:31:33.540 --> 00:31:34.540]   How do you think?
[00:31:34.540 --> 00:31:35.540]   Yeah, no, it is.
[00:31:35.540 --> 00:31:39.220]   For a synthetic, we have a simulation, basically a simulation team.
[00:31:39.220 --> 00:31:41.100]   I think I mentioned it for testing purposes.
[00:31:41.100 --> 00:31:42.100]   Right, right, for testing.
[00:31:42.100 --> 00:31:45.140]   I wasn't sure if that was like totally synthetic simulations or what.
[00:31:45.140 --> 00:31:50.420]   Yeah, well, I mean, we can do both for open loop, which is no control and planning in
[00:31:50.420 --> 00:31:53.500]   the loop, like no actual driving.
[00:31:53.500 --> 00:31:56.180]   We can replay existing data.
[00:31:56.180 --> 00:31:59.520]   So that's really good because then we can measure on real data.
[00:31:59.520 --> 00:32:04.860]   But for data like closed loop, which is really driving in a world, we need real simulated
[00:32:04.860 --> 00:32:05.860]   data.
[00:32:05.860 --> 00:32:11.620]   And this is when video kind of shines because we can of course generate like simulated world,
[00:32:11.620 --> 00:32:13.300]   like in video games.
[00:32:13.300 --> 00:32:18.020]   And so even more than that, we have the ability to generate all kinds of sensor data for the
[00:32:18.020 --> 00:32:19.020]   car.
[00:32:19.020 --> 00:32:26.620]   So not just not like also, you know, radar data, but also like CAN, IMU, all those things
[00:32:26.620 --> 00:32:28.780]   that are car specific.
[00:32:28.780 --> 00:32:30.360]   We can generate all of this.
[00:32:30.360 --> 00:32:37.100]   And we have a special box that we call constellation, which has this generator, like simulation
[00:32:37.100 --> 00:32:41.060]   generator on one side and what we call the ECU, like the embedded system on the other
[00:32:41.060 --> 00:32:45.340]   side that can process all those sensor inputs in the same box.
[00:32:45.340 --> 00:32:50.940]   So basically do the exact, exact simulation, right, exact processing of the simulated data.
[00:32:50.940 --> 00:32:54.700]   So we can do all of this and we can use it for testing.
[00:32:54.700 --> 00:32:59.660]   And we can also use it for, of course, collecting data and training on data that just doesn't
[00:32:59.660 --> 00:33:01.960]   exist in the real world, for example.
[00:33:01.960 --> 00:33:07.260]   So very helpful for bootstrapping, perception efforts, for example, bootstrapping new neural
[00:33:07.260 --> 00:33:08.260]   networks.
[00:33:08.260 --> 00:33:09.260]   Right.
[00:33:09.260 --> 00:33:12.460]   So I guess, right, right, right, right.
[00:33:12.460 --> 00:33:17.260]   I mean, where do you plan to, I mean, it sounds like you have like almost a complete like
[00:33:17.260 --> 00:33:21.340]   end to end solution for people.
[00:33:21.340 --> 00:33:28.340]   Like could I come to you and like with a car and some sensors and like get a system that
[00:33:28.340 --> 00:33:31.380]   could make an autonomous vehicle for me?
[00:33:31.380 --> 00:33:32.940]   Yeah, that's, that's exactly.
[00:33:32.940 --> 00:33:33.940]   Yeah.
[00:33:33.940 --> 00:33:34.940]   Yes, you can.
[00:33:34.940 --> 00:33:39.020]   Except that we need, I mean, it's difficult, it's difficult to change sensors, as you can
[00:33:39.020 --> 00:33:41.260]   imagine, because suddenly we use a different sensor.
[00:33:41.260 --> 00:33:47.260]   We're going to have to recollect data to revalidate and retrain models or fine tune them and so
[00:33:47.260 --> 00:33:48.260]   on and so forth.
[00:33:48.260 --> 00:33:53.340]   So I'm assuming they're like similar to what we have or that you're willing to pay money.
[00:33:53.340 --> 00:33:54.340]   We can redo that work entirely.
[00:33:54.340 --> 00:33:55.340]   Yes.
[00:33:55.340 --> 00:33:59.460]   So I guess you're like the perfect person to ask, like, what do you think is like left
[00:33:59.460 --> 00:34:00.460]   to do to me?
[00:34:00.460 --> 00:34:03.860]   I mean, I don't actually see, I live in San Francisco, so I do see autonomous vehicles
[00:34:03.860 --> 00:34:06.460]   driving around a fair amount.
[00:34:06.460 --> 00:34:11.260]   But like, what, what pieces do you think are left to, to really work on to make it like
[00:34:11.260 --> 00:34:15.480]   a real thing that, that I would use every day?
[00:34:15.480 --> 00:34:17.340]   So you don't use it every day is what you're saying?
[00:34:17.340 --> 00:34:21.300]   Well, I actually rarely go in an autonomous vehicle and I feel like I'm in the industry.
[00:34:21.300 --> 00:34:22.300]   So I see.
[00:34:22.300 --> 00:34:26.500]   Do you, do you, I mean, do you have a Tesla like Model 3 or any?
[00:34:26.500 --> 00:34:31.100]   No, I mean, I've, I've, I've played with them and I think they're like very, very impressive.
[00:34:31.100 --> 00:34:35.380]   So I guess maybe that's a good point you're making that, but, but like, what about, I
[00:34:35.380 --> 00:34:40.380]   guess, like, I guess, what do you think is, is, is the next steps with, with systems like
[00:34:40.380 --> 00:34:41.380]   yours?
[00:34:41.380 --> 00:34:43.900]   Like, what are you thinking of, of focusing on?
[00:34:43.900 --> 00:34:48.740]   Honestly, I think, so first I think this is going to be pervasive and I think in the,
[00:34:48.740 --> 00:34:52.700]   in the future, everyone's going to have like autonomous vehicle functionality.
[00:34:52.700 --> 00:34:54.020]   That's number one.
[00:34:54.020 --> 00:35:00.260]   But I think the vision goes even further than that is that, is that cars are going to become
[00:35:00.260 --> 00:35:05.100]   software defined or are on the way to becoming software defined.
[00:35:05.100 --> 00:35:10.140]   And that's, you know, like basically people are going to see a centralized computer with
[00:35:10.140 --> 00:35:16.060]   a really nice UI, UX, right, and they're going to be able to buy new software potentially
[00:35:16.060 --> 00:35:18.060]   to upgrade their cars.
[00:35:18.060 --> 00:35:19.740]   And this is already what's happening with Tesla.
[00:35:19.740 --> 00:35:20.740]   Yeah, totally.
[00:35:20.740 --> 00:35:25.660]   And I think that's probably one of the reasons why their capitalization is so high, their
[00:35:25.660 --> 00:35:26.660]   valuation is so high.
[00:35:26.660 --> 00:35:27.660]   Yeah.
[00:35:27.660 --> 00:35:31.060]   But the other car makers are also looking into that, that model, you know, and like
[00:35:31.060 --> 00:35:32.060]   interested in it.
[00:35:32.060 --> 00:35:33.340]   And I think this is the future of the industry.
[00:35:33.340 --> 00:35:35.900]   So for us, I think this is also the future.
[00:35:35.900 --> 00:35:38.780]   I need to be ready for this world at Nvidia.
[00:35:38.780 --> 00:35:43.700]   So that means having like a programmable platform, an open platform, right, because we want to
[00:35:43.700 --> 00:35:50.860]   enable all those car makers or tier one to build those systems together on the same chip,
[00:35:50.860 --> 00:35:52.360]   on the centralized computer.
[00:35:52.360 --> 00:35:54.540]   We don't want to exclude them basically from our chip, right?
[00:35:54.540 --> 00:35:59.740]   We want to enable people to write software on our chip, infotainment software, self-driving
[00:35:59.740 --> 00:36:02.300]   software and so on and so forth.
[00:36:02.300 --> 00:36:06.980]   And now self-driving is so difficult that we can provide it for them, you know, as a
[00:36:06.980 --> 00:36:16.100]   given application for big car makers or even smaller and just, yeah, like develop it as
[00:36:16.100 --> 00:36:17.460]   an application for them.
[00:36:17.460 --> 00:36:18.940]   And then where are we going with that?
[00:36:18.940 --> 00:36:24.540]   I think is a matter of like performance and improving the control planning and the entire
[00:36:24.540 --> 00:36:25.540]   perception system.
[00:36:25.540 --> 00:36:27.980]   I think we're still like at the beginning of it.
[00:36:27.980 --> 00:36:32.060]   We're going to be able to do better and better and better over time by building a lot of
[00:36:32.060 --> 00:36:37.220]   automation first by potentially adding machine learning in areas that don't have machine
[00:36:37.220 --> 00:36:42.980]   learning yet, such as predicting the planning path, for example, right?
[00:36:42.980 --> 00:36:44.300]   Doing things like that.
[00:36:44.300 --> 00:36:45.300]   Yeah.
[00:36:45.300 --> 00:36:47.980]   And anyway, yeah, that's pretty much it.
[00:36:47.980 --> 00:36:52.580]   I guess what I'm hearing is like, you think that there's sort of like iterative improvement
[00:36:52.580 --> 00:36:55.860]   in a bunch of different things and then applying machine learning to planning is like the big
[00:36:55.860 --> 00:37:01.620]   thing, like just sort of the next steps in making these systems, you know, work better.
[00:37:01.620 --> 00:37:07.660]   I'm curious, like what do you think is the stuff that people are really like wrestling
[00:37:07.660 --> 00:37:10.500]   with right now to make these really work?
[00:37:10.500 --> 00:37:13.720]   I think the hardest thing is urban areas right now.
[00:37:13.720 --> 00:37:19.220]   So like being able to drive in urban areas, like in New York City, for example, it's really,
[00:37:19.220 --> 00:37:20.580]   really hard.
[00:37:20.580 --> 00:37:22.140]   And that's the next frontier.
[00:37:22.140 --> 00:37:27.140]   It requires all sorts of new signals coming from the car.
[00:37:27.140 --> 00:37:35.300]   You know, like for example, like intersections, lights, lack of lanes, right?
[00:37:35.300 --> 00:37:39.260]   Things like that that can be very tricky or like unknown kind of like vehicles such as
[00:37:39.260 --> 00:37:41.500]   garbage collection, stuff like that.
[00:37:41.500 --> 00:37:47.540]   So all of this is still a little bit newer and older like self-driving providers started
[00:37:47.540 --> 00:37:53.460]   with easier areas such as highways, except for some of the level five, like Lyft, you
[00:37:53.460 --> 00:37:57.460]   know, or Uber, like the ones that are trying to already leapfrog that.
[00:37:57.460 --> 00:38:00.140]   That's a big challenge basically.
[00:38:00.140 --> 00:38:02.460]   Yeah that's the next frontier.
[00:38:02.460 --> 00:38:08.540]   Do you feel like your approach at NVIDIA is significantly different from like Tesla or
[00:38:08.540 --> 00:38:11.340]   Lyft or I mean, how do you think about that?
[00:38:11.340 --> 00:38:12.660]   Yeah, yeah.
[00:38:12.660 --> 00:38:17.220]   I mean, all these companies, they are targeting different things.
[00:38:17.220 --> 00:38:19.100]   So as a result, there are some differences.
[00:38:19.100 --> 00:38:21.380]   So Lyft is targeting level five.
[00:38:21.380 --> 00:38:24.180]   They want to have fully autonomous vehicles.
[00:38:24.180 --> 00:38:25.700]   Tesla is building cars.
[00:38:25.700 --> 00:38:32.460]   So they don't need to build a platform that's usable by other people, for example.
[00:38:32.460 --> 00:38:36.500]   On our side, we build a platform and we make money with our hardware.
[00:38:36.500 --> 00:38:39.980]   We can also make money with our software, but our software has to be like usable by
[00:38:39.980 --> 00:38:40.980]   everyone else.
[00:38:40.980 --> 00:38:43.660]   So we have to make it in a way that is such, right?
[00:38:43.660 --> 00:38:45.500]   So this is one of the constraints we have.
[00:38:45.500 --> 00:38:50.220]   As a result, for example, the platform we're building, like the car infrastructure is designed
[00:38:50.220 --> 00:38:56.540]   in a way that can be ported to car makers, for example, or any, right?
[00:38:56.540 --> 00:38:57.540]   Like people developing self-driving.
[00:38:57.540 --> 00:39:04.140]   Do you think your platform, I mean, it's interesting because all the pieces that you mentioned
[00:39:04.140 --> 00:39:08.940]   of your platform, I think it's super relevant to like healthcare applications, almost like
[00:39:08.940 --> 00:39:11.580]   any kind of deep learning application.
[00:39:11.580 --> 00:39:15.980]   How do you think about, would you ever expand your platform to other applications?
[00:39:15.980 --> 00:39:19.220]   Yeah I think that's possible.
[00:39:19.220 --> 00:39:25.300]   Some of these pieces are not required per se, and sometimes the scale we aim for is
[00:39:25.300 --> 00:39:29.940]   not required as well for healthcare, for example.
[00:39:29.940 --> 00:39:34.980]   And however, yeah, usually what we try to do is what we built is more like a super set
[00:39:34.980 --> 00:39:39.700]   of those tools and push the frontier a little bit further.
[00:39:39.700 --> 00:39:44.540]   Now some things are a little bit tied to autonomous vehicles, but the entire end-to-end workflow
[00:39:44.540 --> 00:39:47.060]   though seems very applicable.
[00:39:47.060 --> 00:39:50.940]   The tools themselves that we build sometimes were customized to the data we have.
[00:39:50.940 --> 00:39:52.980]   So yes, we could extend them.
[00:39:52.980 --> 00:39:54.620]   It would just require some work basically.
[00:39:54.620 --> 00:39:55.620]   Yeah.
[00:39:55.620 --> 00:39:56.620]   I see.
[00:39:56.620 --> 00:39:59.140]   I'm curious, this is a kind of a specific question, but I've been thinking about this
[00:39:59.140 --> 00:40:00.140]   lately.
[00:40:00.140 --> 00:40:05.300]   Like how important do you think is the sort of hyper parameter search piece?
[00:40:05.300 --> 00:40:08.620]   Like the neural architecture search you're talking about, like is that really essential
[00:40:08.620 --> 00:40:10.980]   or is that like a nice to have?
[00:40:10.980 --> 00:40:16.820]   So neural architecture search is, well, it's still like something we're exploring.
[00:40:16.820 --> 00:40:22.140]   I think it can be important because we can really reduce the compute footprint for us.
[00:40:22.140 --> 00:40:23.380]   So I think it can work.
[00:40:23.380 --> 00:40:29.420]   So for example, we can constrain the search space of our neural architecture search to
[00:40:29.420 --> 00:40:34.700]   something that's going to perform really well on target hardware in terms of latency.
[00:40:34.700 --> 00:40:38.780]   Because Nvidia has like some hardware accelerators that are specific and so we can make sure
[00:40:38.780 --> 00:40:42.500]   that we target this and find the architecture and so on and so forth.
[00:40:42.500 --> 00:40:49.460]   So hyper parameter search is something that we have available, but the kind of like the
[00:40:49.460 --> 00:40:56.460]   advantage of using that versus the compute it requires is often like not super interesting
[00:40:56.460 --> 00:41:01.340]   for developers.
[00:41:01.340 --> 00:41:06.580]   So we do it sometimes, but it's not really like a big advantage or a big competitive
[00:41:06.580 --> 00:41:08.660]   advantage I would say for us.
[00:41:08.660 --> 00:41:09.660]   I see.
[00:41:09.660 --> 00:41:10.660]   Yeah.
[00:41:10.660 --> 00:41:13.940]   So is there a piece of your, I mean, your platform sounds amazing and it solves like
[00:41:13.940 --> 00:41:15.540]   a whole slew of problems.
[00:41:15.540 --> 00:41:18.340]   Is there like a piece of it that you're like especially proud of that you think is like
[00:41:18.340 --> 00:41:24.540]   really, like really stands out to you as like best in class or?
[00:41:24.540 --> 00:41:30.180]   I really like the active learning part and everything that goes around that.
[00:41:30.180 --> 00:41:34.820]   So one other thing we are doing is what we call targeted learning, which is the ability
[00:41:34.820 --> 00:41:42.020]   to take a perception bug, so like, oh, I'm not able to detect, you know, like trucks
[00:41:42.020 --> 00:41:48.020]   in that position, whatever, and then use that and sample a dataset that then is going to
[00:41:48.020 --> 00:41:52.780]   be used for training and fix the perception bug.
[00:41:52.780 --> 00:41:57.380]   And doing that is similar to active learning, but like conditional active learning.
[00:41:57.380 --> 00:42:03.140]   So I'm really proud of these two things because I really love the automation of it all, right?
[00:42:03.140 --> 00:42:07.700]   We could just go on vacation and be like, okay, now let's just, the system work.
[00:42:07.700 --> 00:42:11.820]   We have like, you know, customers sending their bugs and automatically we just fix them,
[00:42:11.820 --> 00:42:12.820]   you know?
[00:42:12.820 --> 00:42:13.820]   Cool.
[00:42:13.820 --> 00:42:18.020]   Well, this is, this is so fascinating actually, you know, even if we weren't recording this
[00:42:18.020 --> 00:42:20.740]   for something, I think I would have really enjoyed this conversation.
[00:42:20.740 --> 00:42:21.740]   No, no, no.
[00:42:21.740 --> 00:42:22.740]   Thanks.
[00:42:22.740 --> 00:42:24.420]   I mean, I love talking about it.
[00:42:24.420 --> 00:42:25.420]   Exactly.
[00:42:25.420 --> 00:42:26.420]   Yeah.
[00:42:26.420 --> 00:42:29.300]   It's great to meet you and thanks so much for doing this with us.
[00:42:29.300 --> 00:42:30.300]   Cool.
[00:42:30.300 --> 00:42:31.300]   Well, thanks a lot.
[00:42:31.300 --> 00:42:32.300]   All right.
[00:42:32.300 --> 00:42:33.300]   Well, that was a really great conversation.
[00:42:33.300 --> 00:42:35.500]   Thank you, Lucas and Nicholas.
[00:42:35.500 --> 00:42:40.100]   I'm going to add a link to Nicholas's Twitter in the show notes below, and I would highly
[00:42:40.100 --> 00:42:42.780]   recommend that you guys check him out.
[00:42:42.780 --> 00:42:47.820]   Also if you'd like to continue the conversation, we do have a very active Slack community with
[00:42:47.820 --> 00:42:53.220]   over a thousand machine learning engineers, and I would love to see you guys on there.
[00:42:53.220 --> 00:42:58.860]   Finally, before we go, I would love to talk to you guys about something that I'm super
[00:42:58.860 --> 00:43:00.620]   excited about.
[00:43:00.620 --> 00:43:05.140]   So lately we've been working with a lot of self-driving car companies that weights and
[00:43:05.140 --> 00:43:12.060]   biases, and that means that we've been building native support specifically for self-driving
[00:43:12.060 --> 00:43:13.820]   machine learning models.
[00:43:13.820 --> 00:43:20.300]   So now with just a few lines of code, you can do object detection with 2D and 3D bounding
[00:43:20.300 --> 00:43:22.700]   boxes within weights and biases.
[00:43:22.700 --> 00:43:28.020]   You can also do semantic segmentation so you can compare your model's predictions with
[00:43:28.020 --> 00:43:30.540]   the true labels inside your dataset.
[00:43:30.540 --> 00:43:36.420]   And finally, my favorite, you can now log point clouds within weights and biases.
[00:43:36.420 --> 00:43:41.540]   So that means you can now use point clouds to understand your scene with custom annotation
[00:43:41.540 --> 00:43:42.540]   layers.
[00:43:42.540 --> 00:43:46.980]   You might use this with something like a dataset of LiDAR points.
[00:43:46.980 --> 00:43:52.660]   So for example, Lyft put out a self-driving car dataset composed of LiDAR points, and
[00:43:52.660 --> 00:43:58.060]   you could plop that into weights and biases and draw nice little 3D bounding boxes around
[00:43:58.060 --> 00:44:02.700]   your cars, people, and other objects within your scene.
[00:44:02.700 --> 00:44:03.980]   It's a great time.
[00:44:03.980 --> 00:44:08.660]   I'm going to leave some links in the show notes below so you can try out point cloud
[00:44:08.660 --> 00:44:12.340]   semantic segmentation and also object detection.
[00:44:12.340 --> 00:44:17.060]   It's a really fun time, whether you're working on self-driving professionally or just for
[00:44:17.060 --> 00:44:18.060]   fun.
[00:44:18.060 --> 00:44:21.540]   I would love for you guys to try it and tell us what you think.
[00:44:21.540 --> 00:44:27.500]   Finally, you can also use weights and biases to run sweeps to tune your hyperparameters
[00:44:27.500 --> 00:44:29.380]   in a very organized way.
[00:44:29.380 --> 00:44:33.740]   This means that you can just give us a list of hyperparameters that you would like to
[00:44:33.740 --> 00:44:36.980]   search through and also a search strategy.
[00:44:36.980 --> 00:44:42.060]   And then we will go through and train all these different models and find you the best
[00:44:42.060 --> 00:44:46.660]   one in a very organized way, which is very low effort on your part.
[00:44:46.660 --> 00:44:51.260]   I'll also leave some links down below for you to try out our sweeps.
[00:44:51.260 --> 00:44:52.260]   That's all for today.
[00:44:52.260 --> 00:44:54.820]   We'll see you in the next episode.

