
[00:00:00.000 --> 00:00:07.760]   I'd come from a family where I was privileged in that both of my parents had access to higher
[00:00:07.760 --> 00:00:13.280]   education and I saw how much opportunity that created for me that others just didn't have.
[00:00:13.280 --> 00:00:19.360]   And I guess I've always felt and still feel and really try and teach my children as well
[00:00:19.360 --> 00:00:24.780]   that for those of us who have been privileged, much is expected and it's our responsibility
[00:00:24.780 --> 00:00:26.680]   to give something back.
[00:00:26.680 --> 00:00:32.920]   So you know, that was at that point my way of giving something back is by teaching and
[00:00:32.920 --> 00:00:39.800]   in fact that was what led me to also eventually depart Stanford because I felt like my opportunity
[00:00:39.800 --> 00:00:47.000]   to give something back to the world in a much greater scale was available to me by founding
[00:00:47.000 --> 00:00:51.400]   Coursera and opening up education to a much, much, much larger number than I would ever
[00:00:51.400 --> 00:00:54.360]   be able to teach at Stanford.
[00:00:54.360 --> 00:00:59.320]   And that's actually also what led me to In-sitro because I feel like there's an incredible
[00:00:59.320 --> 00:01:05.380]   moment in time now in bringing together two disciplines in a way that could be totally
[00:01:05.380 --> 00:01:08.400]   transformative to the world.
[00:01:08.400 --> 00:01:12.620]   And I think it's kind of an incumbent upon me.
[00:01:12.620 --> 00:01:17.520]   There's almost a moral imperative to make that happen if I can do that and it's not
[00:01:17.520 --> 00:01:19.680]   something that many other people can do.
[00:01:19.680 --> 00:01:24.040]   You're listening to Gradient Dissent, a show about machine learning in the real world.
[00:01:24.040 --> 00:01:26.140]   And I'm your host, Lukas Biewald.
[00:01:26.140 --> 00:01:32.360]   I'm excited and maybe a little nervous to interview Daphne Koller, who is a very famous,
[00:01:32.360 --> 00:01:38.640]   successful machine learning professor and also the founder of In-sitro and a founder
[00:01:38.640 --> 00:01:45.040]   of Coursera and recently founder of Engagely, three super different, super interesting startups.
[00:01:45.040 --> 00:01:51.000]   I should also say she was my first machine learning teacher at 221 at Stanford and then
[00:01:51.000 --> 00:01:54.120]   I TA'd for her and then I did research for her later on.
[00:01:54.120 --> 00:01:58.160]   So she might actually be the reason that I'm here today recording this podcast.
[00:01:58.160 --> 00:02:01.360]   So again, super excited to talk to her.
[00:02:01.360 --> 00:02:07.240]   So the thing I most want to talk to you about actually is In-sitro, which looks super fascinating
[00:02:07.240 --> 00:02:08.440]   and exciting.
[00:02:08.440 --> 00:02:12.080]   And maybe for those who haven't heard of In-sitro, you could sort of give us a quick overview
[00:02:12.080 --> 00:02:14.000]   of the thesis of the company.
[00:02:14.000 --> 00:02:15.000]   Sure.
[00:02:15.000 --> 00:02:18.300]   So In-sitro is a drug discovery and development company.
[00:02:18.300 --> 00:02:22.860]   And if you've been looking at drug discovery for the last 50 years, you'll see that we've
[00:02:22.860 --> 00:02:26.720]   made a tremendous amount of progress in bringing medicines to patients in need.
[00:02:26.720 --> 00:02:32.160]   But at the same time, there's this thing called Eron's law, which is the inverse of Moore's
[00:02:32.160 --> 00:02:38.000]   law in which there is an exponential decrease in the productivity of pharmaceutical R&D.
[00:02:38.000 --> 00:02:43.440]   And when you ask yourself why that is, it's because the journey of discovering and developing
[00:02:43.440 --> 00:02:49.200]   a drug is really complex and long, and there is many places along that journey where we
[00:02:49.200 --> 00:02:50.520]   can take the wrong turn.
[00:02:50.520 --> 00:02:55.600]   And when we do it can take months, if not years, and millions, if not tens of millions
[00:02:55.600 --> 00:02:58.720]   of dollars to realize that we took the wrong trajectory.
[00:02:58.720 --> 00:03:04.800]   So what we're trying to do is to really build the company in a way that uses machine learning,
[00:03:04.800 --> 00:03:09.720]   which after all is something that is helping us make really good predictions in so many
[00:03:09.720 --> 00:03:14.560]   other domains, and use that as a way of building this drug discovery and development process
[00:03:14.560 --> 00:03:16.280]   in a completely different foundation.
[00:03:16.280 --> 00:03:20.040]   So that's what we're really trying to do is bring better medicines to patients and do
[00:03:20.040 --> 00:03:22.040]   it faster.
[00:03:22.040 --> 00:03:27.360]   So what's the sort of standard drug discovery process at kind of a high level and where
[00:03:27.360 --> 00:03:31.800]   does machine learning fit into this or improve it?
[00:03:31.800 --> 00:03:38.160]   So I don't know if one can really talk about a standard journey because it's been an evolving
[00:03:38.160 --> 00:03:41.160]   process over the last few years.
[00:03:41.160 --> 00:03:47.000]   But I mean, if you want to draw a very coarse grained caricature, you could say, well, I
[00:03:47.000 --> 00:03:55.400]   have a disease and I do usually, it's best done in an academic center, a bunch of biology
[00:03:55.400 --> 00:04:02.360]   to uncover the genes and the biological mechanisms, pathways that are implicated in disease.
[00:04:02.360 --> 00:04:08.880]   And then someone has a hypothesis about, okay, if I make an intervention at this gene, it
[00:04:08.880 --> 00:04:15.480]   may cure or at least help address, cure is a very broad word, very ambitious word.
[00:04:15.480 --> 00:04:21.040]   We've cured precious few diseases, but help address some of the aspects of the disease.
[00:04:21.040 --> 00:04:26.200]   And once you have that target, you can start to identify, well, first of all, you have
[00:04:26.200 --> 00:04:27.680]   to validate the target.
[00:04:27.680 --> 00:04:33.240]   And oftentimes that's done using animal models that attempt to simulate some aspects of the
[00:04:33.240 --> 00:04:34.240]   disease.
[00:04:34.240 --> 00:04:39.520]   And for many of the diseases that we have today, the animals don't get the disease naturally.
[00:04:39.520 --> 00:04:43.480]   And so you kind of have to create the disease in the animal and then try and address it
[00:04:43.480 --> 00:04:44.480]   in the animal.
[00:04:44.480 --> 00:04:48.440]   And it oftentimes turns out that what you're addressing really isn't the true disease.
[00:04:48.440 --> 00:04:53.800]   It's some simulation of it that is very imprecise and sometimes just downright wrong.
[00:04:53.800 --> 00:04:59.820]   And then once you have a target, then you typically look for chemical matter, a compound
[00:04:59.820 --> 00:05:02.040]   that helps modulate that target.
[00:05:02.040 --> 00:05:07.320]   And there's different, what are called therapeutic modalities, which are different kinds of interventions.
[00:05:07.320 --> 00:05:14.200]   So used to be, you know, whatever, 30, 40 years ago that the main form of a therapeutic
[00:05:14.200 --> 00:05:17.200]   modality we had was small molecules.
[00:05:17.200 --> 00:05:23.480]   And then around came biologics, which are larger molecules, basically proteins and antibodies,
[00:05:23.480 --> 00:05:30.600]   which are a type of protein that are in many cases more precision mechanisms.
[00:05:30.600 --> 00:05:34.440]   So they're much more precise in their action, but they're also harder to administer and
[00:05:34.440 --> 00:05:38.240]   they are able to address a narrower set of targets.
[00:05:38.240 --> 00:05:44.160]   And now over time, we have additional therapeutic modalities that have emerged over the last
[00:05:44.160 --> 00:05:48.800]   few years that help intervene in the body and other types of mechanisms.
[00:05:48.800 --> 00:05:53.040]   So everyone's talking about gene therapy as they should, in which case we can come in
[00:05:53.040 --> 00:05:56.080]   and intervene in the DNA itself.
[00:05:56.080 --> 00:06:01.000]   And you know, there's only a very few of those that have been approved so far, but it's very
[00:06:01.000 --> 00:06:03.120]   much a growing field.
[00:06:03.120 --> 00:06:09.400]   Now with the COVID-19 vaccine, everyone is talking about RNA therapeutics, which is intervening
[00:06:09.400 --> 00:06:13.200]   in between DNA and protein at the RNA level.
[00:06:13.200 --> 00:06:19.160]   So all of these are ways that are expanding our capabilities to make intelligent interventions
[00:06:19.160 --> 00:06:23.800]   in the human body and hence in a disease process.
[00:06:23.800 --> 00:06:29.640]   Oftentimes where it fails is really at the very beginning, which is we do not understand
[00:06:29.640 --> 00:06:32.520]   biology well at all.
[00:06:32.520 --> 00:06:39.520]   And therefore our ability to recognize when intervening in a target is going to actually
[00:06:39.520 --> 00:06:44.720]   have meaningful clinical benefit to a human is very, very limited.
[00:06:44.720 --> 00:06:48.120]   And oftentimes we guess and we guess wrong.
[00:06:48.120 --> 00:06:53.840]   And sometimes we also fail to understand all of the other implications that an intervention
[00:06:53.840 --> 00:06:56.160]   in a given target might have.
[00:06:56.160 --> 00:07:00.800]   For example, all of the other things that this particular gene does in the body.
[00:07:00.800 --> 00:07:07.120]   And if we intervene in a way that may be even beneficial for this, might be detrimental
[00:07:07.120 --> 00:07:08.160]   for that.
[00:07:08.160 --> 00:07:15.640]   And so that's where a lot of our ability to make valid predictions really falls short.
[00:07:15.640 --> 00:07:17.360]   And that's where a lot of drugs fail.
[00:07:17.360 --> 00:07:22.400]   Right now the failure rate, depending on what you consider to be the denominator, like when
[00:07:22.400 --> 00:07:28.840]   do you start counting a program as a drug program, is between 90 and 95%.
[00:07:28.840 --> 00:07:34.120]   That's the failure rate, not the success rate, which means between one in 10 and one in 20
[00:07:34.120 --> 00:07:39.120]   drugs actually go on to be approved and an even smaller number actually end up making
[00:07:39.120 --> 00:07:42.080]   a real difference to patients.
[00:07:42.080 --> 00:07:47.400]   And that's what we're looking to fix is how can we make better predictions first and foremost
[00:07:47.400 --> 00:07:54.280]   about what kinds of targets you would want to intervene in for a given disease in the
[00:07:54.280 --> 00:07:57.320]   context of a given patient population.
[00:07:57.320 --> 00:08:01.080]   And then subsequently find we want to intervene at this target.
[00:08:01.080 --> 00:08:06.600]   What is the right chemical matter to put in that might have fewer side effects that might
[00:08:06.600 --> 00:08:09.480]   have better drug-like properties?
[00:08:09.480 --> 00:08:12.320]   What is the right patient population to use?
[00:08:12.320 --> 00:08:18.320]   A lot of the failures that I think we have today are because we try and go after a much
[00:08:18.320 --> 00:08:22.680]   broader or miscalibrated patient population.
[00:08:22.680 --> 00:08:27.480]   And so over time, I think there's many questions in this process where machine learning can
[00:08:27.480 --> 00:08:33.200]   make an intervention, the target, the drug, the patient population, the biomarker that
[00:08:33.200 --> 00:08:37.920]   tells us when a drug is working so that we can cut things short if it's not and transition
[00:08:37.920 --> 00:08:40.000]   the patient to another drug.
[00:08:40.000 --> 00:08:43.360]   All of these are areas where I think machine learning can play a role.
[00:08:43.360 --> 00:08:48.240]   And does the machine learning try to kind of model the physical reality of the world
[00:08:48.240 --> 00:08:53.400]   here or does it sort of ignore that and just sort of look at like past experiments that
[00:08:53.400 --> 00:08:54.400]   were tried?
[00:08:54.560 --> 00:08:57.760]   You know, I think people have tried both.
[00:08:57.760 --> 00:09:04.040]   And as we've seen in other cases where machine learning has been applied, there are some
[00:09:04.040 --> 00:09:08.660]   benefits to incorporating a lot of prior knowledge about the world.
[00:09:08.660 --> 00:09:12.580]   But then over time, that begins to become a limitation.
[00:09:12.580 --> 00:09:19.120]   So I used to work in computer vision way back when people still tried to create models of
[00:09:19.120 --> 00:09:26.080]   how light is refracted off of surfaces and having geometric models for computer vision
[00:09:26.080 --> 00:09:28.640]   and models of illumination and so on and so forth.
[00:09:28.640 --> 00:09:29.920]   And we don't do that anymore.
[00:09:29.920 --> 00:09:37.200]   What we now do is create really, really large training sets and give the computer enough
[00:09:37.200 --> 00:09:41.740]   data that it can learn the patterns without having to be told a lot about the structure
[00:09:41.740 --> 00:09:42.960]   of the world.
[00:09:42.960 --> 00:09:49.200]   We haven't quite hit that tipping point in most biological problems because the data
[00:09:49.200 --> 00:09:52.440]   that's been available has just been insufficient.
[00:09:52.440 --> 00:09:57.040]   And so right now there is a lot of problems where models that incorporate more of our
[00:09:57.040 --> 00:10:05.720]   understanding of biology are actually in many cases outperforming models that are less informed.
[00:10:05.720 --> 00:10:12.280]   But one of, to my mind, a real highlight achievement from the past year that starts to go in the
[00:10:12.280 --> 00:10:20.120]   other direction is the incredible success of DeepMind's AlphaFold algorithm, which uses
[00:10:20.120 --> 00:10:24.960]   somewhat similar machine learning tools to AlphaGo, which they used in a very different
[00:10:24.960 --> 00:10:26.320]   domain.
[00:10:26.320 --> 00:10:30.260]   And AlphaFold is basically addressing the problem of protein folding.
[00:10:30.260 --> 00:10:35.320]   So to take an amino acid sequence that represents a protein and ask what it will look like in
[00:10:35.320 --> 00:10:37.440]   3D space.
[00:10:37.440 --> 00:10:42.680]   There's been multiple groups over the past, I don't know, 10, if not more years that have
[00:10:42.680 --> 00:10:48.560]   built computer tools, some incorporating machine learning, but certainly all incorporating
[00:10:48.560 --> 00:10:53.560]   a relatively large amount of prior knowledge about physics and chemistry and forces and
[00:10:53.560 --> 00:11:00.800]   electrons and so on and so forth, and asking what the folded protein would look like.
[00:11:00.800 --> 00:11:05.920]   And all of them kind of asymptoted at a certain level of performance, which was reasonable
[00:11:05.920 --> 00:11:07.660]   but not usable.
[00:11:07.660 --> 00:11:11.720]   And by the way, I forgot to say that there's been a biannual competition once every two
[00:11:11.720 --> 00:11:18.960]   years called CASP, which is one of the best designed real blinded tests for a machine
[00:11:18.960 --> 00:11:24.080]   learning model, one where you can't cheat, in which labs that are experimenting on a
[00:11:24.080 --> 00:11:30.200]   particular protein by generating its crystal structure, which is the 3D structure, would
[00:11:30.200 --> 00:11:38.240]   submit the sequence to the CASP competition, and they would not release the solved structure
[00:11:38.240 --> 00:11:40.280]   until the competition was done.
[00:11:40.280 --> 00:11:45.640]   And since no one can, I mean, it's months of experimental work to come up with that
[00:11:45.640 --> 00:11:48.640]   structure, people couldn't cheat on the test data.
[00:11:48.640 --> 00:11:54.920]   So in this CASP competition, you could see that there was a plateau of performance.
[00:11:54.920 --> 00:12:01.600]   And in this last year, DeepMind really broke through that plateau and achieved a performance
[00:12:01.600 --> 00:12:06.320]   that is actually usable for real biological problems.
[00:12:06.320 --> 00:12:14.080]   And the way they did that is by not incorporating into the model a lot of preconceptions about
[00:12:14.080 --> 00:12:17.960]   physics and chemistry and different kinds of chemical bonds, but really just giving
[00:12:17.960 --> 00:12:23.280]   the machine learning model enough pairs of sequences and solve structures to train on.
[00:12:23.280 --> 00:12:27.920]   And then they said, okay, now that you've learned, go and run on a new protein.
[00:12:27.920 --> 00:12:31.960]   And they were able to break through that ceiling that we've seen.
[00:12:31.960 --> 00:12:39.080]   So I think to my mind, that's an indication that we need to be really thinking hard about
[00:12:39.080 --> 00:12:45.680]   how to generate enough data at scale for biological or chemical problems so that you can get machine
[00:12:45.680 --> 00:12:47.960]   learning to break through that ceiling and performance.
[00:12:47.960 --> 00:12:55.040]   And so that's kind of what we're trying to do at In-sitro is build massive data production
[00:12:55.040 --> 00:13:00.960]   capabilities across the problems that we care about so that we can generate data that's
[00:13:00.960 --> 00:13:07.800]   enough high quality and large enough and that is fit to purpose so that you can train machine
[00:13:07.800 --> 00:13:12.680]   learning models to solve the problems that we care to solve in the drug discovery process.
[00:13:12.680 --> 00:13:18.400]   So I guess I want to get back to In-sitro in a second, but since the protein folding
[00:13:18.400 --> 00:13:22.880]   thing was so high profile, I'll ask you my dumb questions, which is such a waste, but
[00:13:22.880 --> 00:13:26.340]   I was kind of curious, what was the insight then?
[00:13:26.340 --> 00:13:30.400]   It seems like just actually removing prior beliefs from a model wouldn't be enough to
[00:13:30.400 --> 00:13:33.680]   have a breakthrough improvement in the quality.
[00:13:33.680 --> 00:13:38.560]   And surely lots of people had access to lots of examples of proteins and how they fold,
[00:13:38.560 --> 00:13:39.560]   right?
[00:13:39.560 --> 00:13:44.880]   I can't speak to that yet because they have not yet published their latest model.
[00:13:44.880 --> 00:13:49.480]   And so we're relying on the very limited information that's in a press release.
[00:13:49.480 --> 00:13:55.880]   And so I would be curious to read the paper once it's out, but I do know that they incorporated
[00:13:55.880 --> 00:14:01.240]   a lot of insight from the latest machine learning models in terms of, for instance, attention
[00:14:01.240 --> 00:14:07.440]   models that you can look to see where you would want to kind of have one amino acid
[00:14:07.440 --> 00:14:11.240]   look elsewhere in the sequence to figure out where to fold.
[00:14:11.240 --> 00:14:16.440]   But I wish I could give you more insight into exactly how this works and I'm hoping that
[00:14:16.440 --> 00:14:21.160]   they will publish the results soon and we will all learn from how they did this.
[00:14:21.160 --> 00:14:25.360]   And is protein folding, is that a sub problem of one of the problems that you mentioned?
[00:14:25.360 --> 00:14:30.200]   Or is that just an example of how much momentum there is in the field?
[00:14:30.200 --> 00:14:35.960]   You know, I think people have differing opinions on the extent to which protein folding matters
[00:14:35.960 --> 00:14:37.320]   in drug discovery.
[00:14:37.320 --> 00:14:42.040]   I think there's a lot of proteins where the structure is actually pretty well understood
[00:14:42.040 --> 00:14:44.040]   and we just don't know how to drug them.
[00:14:44.040 --> 00:14:47.920]   Protein folding certainly doesn't help you with the fundamental question of picking the
[00:14:47.920 --> 00:14:52.760]   right target to go after because the folding comes after you decide that this is a target
[00:14:52.760 --> 00:14:53.960]   that you need.
[00:14:53.960 --> 00:14:59.920]   There certainly are a set of targets where you really would like to go after them and
[00:14:59.920 --> 00:15:03.440]   what's missing is an understanding of their 3D structure.
[00:15:03.440 --> 00:15:07.040]   How big that set is, I think is a matter for debate.
[00:15:07.040 --> 00:15:12.000]   But so to my mind, it's less about whether protein folding is the key problem in drug
[00:15:12.000 --> 00:15:13.000]   discovery.
[00:15:13.000 --> 00:15:14.000]   It's certainly not the key problem.
[00:15:14.000 --> 00:15:19.000]   It may be a problem, but it's certainly not at the core of what's holding drug discovery
[00:15:19.000 --> 00:15:20.080]   back.
[00:15:20.080 --> 00:15:27.640]   But it's really an illustration of taking a problem that everyone agreed was hard, people
[00:15:27.640 --> 00:15:34.040]   had struggled to solve or tried to solve using a range of other methods and machine learning
[00:15:34.040 --> 00:15:39.280]   came in and with the right type of model and the right type of data was really able to
[00:15:39.280 --> 00:15:40.800]   crack that nut open.
[00:15:40.800 --> 00:15:45.440]   And so that to me is the real lesson here rather than we've transformed drug discovery.
[00:15:45.440 --> 00:15:46.440]   Interesting.
[00:15:46.440 --> 00:15:51.320]   I mean, I guess another question that comes to mind is, I remember back in 2004, you were
[00:15:51.320 --> 00:15:55.880]   working on applications of machine learning and biology and some of them actually sound
[00:15:55.880 --> 00:15:59.080]   quite similar to what you're talking about at Insitro.
[00:15:59.080 --> 00:16:03.000]   And so why start the company almost two decades later?
[00:16:03.000 --> 00:16:07.040]   Is it that the biology has improved or the machine learning has improved or the data
[00:16:07.040 --> 00:16:08.040]   has improved?
[00:16:08.040 --> 00:16:11.240]   What's the key thing that's changing that makes Insitro possible now?
[00:16:11.240 --> 00:16:14.340]   I think it's a combination of both actually.
[00:16:14.340 --> 00:16:21.400]   The first is the availability of much, much larger amounts of data than we have had before.
[00:16:21.400 --> 00:16:28.680]   So in the last decade or so, there has been this tremendous amount of progress in biological
[00:16:28.680 --> 00:16:33.080]   tools that are good for data creation.
[00:16:33.080 --> 00:16:39.440]   And that includes everything from the incredible growth in the feasibility of DNA sequencing
[00:16:39.440 --> 00:16:44.840]   and not just DNA, but also RNA sequencing and various other aspects of sequencing.
[00:16:44.840 --> 00:16:51.840]   Microscopy has grown a tremendous amount and it's both its throughput and its capabilities.
[00:16:51.840 --> 00:16:57.480]   In the chemistry side, we have these really cool things called DNA encoded libraries,
[00:16:57.480 --> 00:17:03.360]   which are basically chemical libraries that can have hundreds of millions of molecules
[00:17:03.360 --> 00:17:05.160]   all mixed together in a test tube.
[00:17:05.160 --> 00:17:09.760]   But because they each have a DNA barcode attached to them, you could basically figure out what
[00:17:09.760 --> 00:17:14.360]   they do without, you know, even though they're all kind of mixed together in a pool.
[00:17:14.360 --> 00:17:19.040]   There's microfluidic techniques that allow you to do experiments in teeny little droplets,
[00:17:19.040 --> 00:17:23.440]   which achieves both sort of facial separation as well as scale.
[00:17:23.440 --> 00:17:27.520]   All of these techniques are things that didn't exist a decade ago.
[00:17:27.520 --> 00:17:32.200]   Not let me forget CRISPR, of course, which is the ability to now start to edit the genome
[00:17:32.200 --> 00:17:37.840]   in a very fine grained way and then ask what happens to a cell when its genome is edited
[00:17:37.840 --> 00:17:39.760]   in a particular way.
[00:17:39.760 --> 00:17:46.600]   That is something that when I was doing, even not in 2004, I went and did a sabbatical at
[00:17:46.600 --> 00:17:49.560]   UCSF in, I think, 2009.
[00:17:49.560 --> 00:17:55.640]   And we were doing these experiments in knocking pairs of genes in yeast.
[00:17:55.640 --> 00:17:58.760]   And yeast is a very malleable, editable organism.
[00:17:58.760 --> 00:18:02.960]   And the experiments were incredibly slow and painful.
[00:18:02.960 --> 00:18:06.320]   And they were in yeast, which has 6,000 genes.
[00:18:06.320 --> 00:18:12.160]   Now if you wanted to do pairwise knockouts in human cells, it's an experiment that you
[00:18:12.160 --> 00:18:14.040]   could do in a couple of weeks.
[00:18:14.040 --> 00:18:17.800]   And it's just amazing how things have changed that way.
[00:18:17.800 --> 00:18:21.240]   So I think that to me is actually the biggest transformation.
[00:18:21.240 --> 00:18:25.160]   But the other one, of course, is just transformation that we've seen in machine learning.
[00:18:25.160 --> 00:18:27.160]   It's hard to imagine thinking back.
[00:18:27.160 --> 00:18:33.800]   But in 2004, when we were doing computer vision, and you might remember this, Luke, that we
[00:18:33.800 --> 00:18:39.960]   were looking at questions like in taking an image, what is in this image?
[00:18:39.960 --> 00:18:41.640]   Is there a dog in this image?
[00:18:41.640 --> 00:18:43.920]   It's like, I don't know, maybe.
[00:18:43.920 --> 00:18:45.840]   And it was barely above random.
[00:18:45.840 --> 00:18:53.000]   And now, in 2018, I think the lines crossed where the machine performance is actually
[00:18:53.000 --> 00:18:54.880]   above that of a human.
[00:18:54.880 --> 00:18:58.500]   And that is for tasks where humans are actually good.
[00:18:58.500 --> 00:19:02.640]   That is, humans know how to recognize dogs in images.
[00:19:02.640 --> 00:19:04.080]   We're trained to it from birth.
[00:19:04.080 --> 00:19:07.000]   And yet the machine is outperforming a human.
[00:19:07.000 --> 00:19:12.720]   When you're looking at tasks where humans are actually not so good, like, for example,
[00:19:12.720 --> 00:19:19.160]   recognizing biological patterns in images, or even worse, in sequencing data, I mean,
[00:19:19.160 --> 00:19:21.560]   the machine is just so much better than the human.
[00:19:21.560 --> 00:19:26.080]   This is a broad question I didn't expect to ask, but I'm curious your thoughts.
[00:19:26.080 --> 00:19:31.880]   What were the key insights, do you think, between 2004 and 2018?
[00:19:31.880 --> 00:19:36.880]   Was there one thing that you think was really the change?
[00:19:36.880 --> 00:19:41.480]   I think it's a combination of three things that came together.
[00:19:41.480 --> 00:19:49.600]   One is, yeah, we had better machine learning models, which were often just a matter of
[00:19:49.600 --> 00:19:58.720]   having the willingness and courage to not just look at simple models, but be willing
[00:19:58.720 --> 00:20:04.000]   to bite the bullet about models that are not convex, that there isn't just a single optimum,
[00:20:04.000 --> 00:20:07.600]   that really have a lot of dependence on exactly how you optimize them.
[00:20:07.600 --> 00:20:09.320]   So that's one thing.
[00:20:09.320 --> 00:20:16.280]   The second is the existence of large enough datasets that one could train such models,
[00:20:16.280 --> 00:20:20.920]   despite the complexity of the space, without overfitting radically.
[00:20:20.920 --> 00:20:26.720]   And I think that's a place where contributions such as ImageNet and others, which really
[00:20:26.720 --> 00:20:31.160]   created large enough datasets so that one could actually start training those models,
[00:20:31.160 --> 00:20:33.880]   were as important as the models themselves.
[00:20:33.880 --> 00:20:38.880]   And then the last one is compute at the push of a button.
[00:20:38.880 --> 00:20:45.160]   I mean, it used to be that in those olden days, I'm feeling really old right now, that
[00:20:45.160 --> 00:20:49.960]   when we had to do anything that required large amounts of compute, we had these local compute
[00:20:49.960 --> 00:20:54.560]   clusters that were painstakingly maintained by local IT people.
[00:20:54.560 --> 00:21:01.160]   And you ran your job and it took six months to run, and you hope there was no memory leak.
[00:21:01.160 --> 00:21:07.800]   And then at the end of the process, you never ran it again because you would never risk
[00:21:07.800 --> 00:21:09.280]   doing it more than once.
[00:21:09.280 --> 00:21:14.360]   And now we have the cloud and you can do this on 10,000 machines and your results come back
[00:21:14.360 --> 00:21:15.680]   in a day.
[00:21:15.680 --> 00:21:22.840]   And honestly, to me, that's been as or more transformative than anything else because
[00:21:22.840 --> 00:21:28.280]   our ability to do that, combined by the way with platforms such as PyTorch and TensorFlow
[00:21:28.280 --> 00:21:33.320]   or an atom that allow us to program much more quickly.
[00:21:33.320 --> 00:21:38.680]   We're now able to experiment and improve our models in an iterative loop that we were never
[00:21:38.680 --> 00:21:39.940]   able to do before.
[00:21:39.940 --> 00:21:44.920]   So even if our initial model is like, eh, the second time and third time and fifth time
[00:21:44.920 --> 00:21:49.560]   and 20th time that we iterate and make it better, it's going to get better and better
[00:21:49.560 --> 00:21:50.780]   over time.
[00:21:50.780 --> 00:21:56.820]   And so that combination of better software, better tooling, I'm not talking just the better
[00:21:56.820 --> 00:22:01.280]   machine learning, just the tooling around the machine learning and the better cloud
[00:22:01.280 --> 00:22:06.720]   computing, which enables this rapid iteration cycle has frankly been, I think, as or more
[00:22:06.720 --> 00:22:09.760]   transformative than anything else.
[00:22:09.760 --> 00:22:14.800]   Which kind of leads to a question I had in biology in particular, which is, are there
[00:22:14.800 --> 00:22:20.040]   data sets available in biology in the same way as in vision?
[00:22:20.040 --> 00:22:24.260]   There's an impression that there's more proprietary data, I guess.
[00:22:24.260 --> 00:22:27.680]   So that's, again, something that's changing.
[00:22:27.680 --> 00:22:33.320]   And one of the data sets that has been most transformative, I think, at least from the
[00:22:33.320 --> 00:22:41.960]   work that I've done, is the UK Biobank, which is 500,000 people with genetics, with clinical
[00:22:41.960 --> 00:22:47.720]   outcomes, including longitudinal clinical outcomes, and very deep phenotyping that includes
[00:22:47.720 --> 00:22:53.880]   different types of imaging and blood biomarkers and urine biomarkers and a whole bunch of
[00:22:53.880 --> 00:22:56.800]   other covariates like environmental factors.
[00:22:56.800 --> 00:23:01.920]   And that data set has on its own, I think, been truly transformative, both in the development
[00:23:01.920 --> 00:23:07.320]   of new methodologies and in the insights that it's given us about human biology.
[00:23:07.320 --> 00:23:11.480]   There's been other data sets that have been, I think, also very important.
[00:23:11.480 --> 00:23:16.960]   They aren't as large or as carefully curated, which I think has limited to some extent the
[00:23:16.960 --> 00:23:21.640]   impact relative to the UK Biobank, but still have been quite significant.
[00:23:21.640 --> 00:23:26.640]   So there is the TCGA, which stands for the Cancer Genome Atlas, which is a reasonably
[00:23:26.640 --> 00:23:30.520]   large cancer data set across different tumor types.
[00:23:30.520 --> 00:23:38.800]   There is the, let's see, the GTEx data set, which speaks to different gene expression
[00:23:38.800 --> 00:23:44.580]   across different tissues and different individuals, so that you can look at the variation within
[00:23:44.580 --> 00:23:49.960]   an individual across their tissues in their gene expression, but also for the same tissue
[00:23:49.960 --> 00:23:54.360]   across individuals, so you can kind of have this be like a two-sided matrix.
[00:23:54.360 --> 00:24:00.840]   There's others that are like that, you know, ENCODE, which speaks to DNA markings across
[00:24:00.840 --> 00:24:02.480]   different cell types.
[00:24:02.480 --> 00:24:08.280]   So I think there is more and more of that available that is not entirely proprietary.
[00:24:08.280 --> 00:24:10.320]   There are also some on the chemistry side.
[00:24:10.320 --> 00:24:15.280]   They don't then, by and large, with a few exceptions, like the UK Biobank being, I think,
[00:24:15.280 --> 00:24:23.800]   the best example of something that is truly high quality, truly well curated with every
[00:24:23.800 --> 00:24:27.160]   experiment done exactly just so.
[00:24:27.160 --> 00:24:32.840]   And that is a challenge for a lot of people because noise in biology is much more of an
[00:24:32.840 --> 00:24:35.480]   issue than it is in many other domains.
[00:24:35.480 --> 00:24:40.560]   So that is actually why we're building in situ the way we are, which is we have a significant
[00:24:40.560 --> 00:24:45.160]   wet lab component whose primary purpose is to generate large amounts of data so that
[00:24:45.160 --> 00:24:47.080]   we can train the models in the right way.
[00:24:47.080 --> 00:24:51.280]   Is there a notion of transfer learning in this field in the same way as in vision or
[00:24:51.280 --> 00:24:54.120]   are the problems just too different?
[00:24:54.120 --> 00:24:58.520]   I think that certainly there is transfer learning.
[00:24:58.520 --> 00:25:04.000]   And even in images, there have been examples where people have trained on, have trained
[00:25:04.000 --> 00:25:09.440]   ResNet models on images in the web and then done transfer to microscopy images.
[00:25:09.440 --> 00:25:10.720]   Right, which is incredible, right?
[00:25:10.720 --> 00:25:11.720]   I know.
[00:25:11.720 --> 00:25:13.240]   It's amazing, isn't it?
[00:25:13.240 --> 00:25:16.760]   So I mean, I would expect it would be even better if you trained on microscopy images,
[00:25:16.760 --> 00:25:21.000]   but still the fact that this actually does translate is, I think, a pretty remarkable
[00:25:21.000 --> 00:25:22.000]   achievement.
[00:25:22.000 --> 00:25:25.760]   I think there's other examples that one could generate.
[00:25:25.760 --> 00:25:32.560]   People have done a fair bit of work, especially recently, on pre-training of, say, graph neural
[00:25:32.560 --> 00:25:38.520]   network models for chemical structures on large numbers of compounds and then using
[00:25:38.520 --> 00:25:45.400]   that type of encoding as a way of, as a pre-trained model for something for which you have less
[00:25:45.400 --> 00:25:49.080]   training data, like more specific properties of compounds.
[00:25:49.080 --> 00:25:55.400]   So I think that's actually one of the big areas that I think will become important over
[00:25:55.400 --> 00:26:02.880]   the next few years is how do we make use of some of those larger data sets that maybe
[00:26:02.880 --> 00:26:09.920]   have less supervision as a way of enabling us to build models that are useful on a smaller
[00:26:09.920 --> 00:26:12.600]   set of data sets.
[00:26:12.600 --> 00:26:17.520]   But you actually built a wet lab to collect data, which is super interesting.
[00:26:17.520 --> 00:26:22.160]   How does your team break down into people doing machine learning and people doing, I
[00:26:22.160 --> 00:26:25.280]   guess, biology and people doing other?
[00:26:25.280 --> 00:26:32.740]   So if you take out the small fraction of people who are like DNA, the composition of the company
[00:26:32.740 --> 00:26:36.100]   for most of the time used to be about 50/50.
[00:26:36.100 --> 00:26:40.280]   So initially I think we had a few more wet lab people because you need to start making
[00:26:40.280 --> 00:26:43.440]   data before you can really have a lot of data to analyze.
[00:26:43.440 --> 00:26:47.200]   But even then we had some computational people who helped with making sure the experiments
[00:26:47.200 --> 00:26:48.200]   were designed right.
[00:26:48.200 --> 00:26:51.200]   And then it became about 50/50.
[00:26:51.200 --> 00:26:57.960]   And then now we're actually starting to grow the next set of functions, which is once you
[00:26:57.960 --> 00:27:01.520]   have insights that come out of the biology, you actually have to make drugs.
[00:27:01.520 --> 00:27:07.080]   And so we're starting to build out functions in chemistry and drug discovery.
[00:27:07.080 --> 00:27:11.760]   And so the balance is shifting a little bit more towards the life sciences, but it's really
[00:27:11.760 --> 00:27:14.520]   quite evenly distributed among those functions.
[00:27:14.520 --> 00:27:15.520]   Well, that's cool.
[00:27:15.520 --> 00:27:20.280]   And I guess it sounds like your ambition is to not make just like one drug, but to kind
[00:27:20.280 --> 00:27:23.280]   of build a process to make lots of drugs.
[00:27:23.280 --> 00:27:27.520]   And I would think with a hit rate, I just picture managing a business where the sort
[00:27:27.520 --> 00:27:31.520]   of hit rate of something is like one in 10 or one in 20 sounds incredibly stressful.
[00:27:31.520 --> 00:27:33.000]   Is that the case?
[00:27:33.000 --> 00:27:39.520]   It is incredibly stressful, especially when each experiment costs you tens or maybe hundreds
[00:27:39.520 --> 00:27:42.500]   of millions of dollars, at least today.
[00:27:42.500 --> 00:27:47.000]   So how do you navigate that is certainly something we think about a lot.
[00:27:47.000 --> 00:27:48.480]   How do you make the process faster?
[00:27:48.480 --> 00:27:50.720]   How do you make it less expensive?
[00:27:50.720 --> 00:27:55.600]   How do you fail fast so that you don't end up spending the hundreds of millions of dollars
[00:27:55.600 --> 00:27:56.920]   on something that is going to fail?
[00:27:56.920 --> 00:28:00.600]   So how do you recognize earlier that something is the wrong path that actually is the point
[00:28:00.600 --> 00:28:03.640]   of what the machine learning is looking to do?
[00:28:03.640 --> 00:28:07.000]   And how do you ensure that you have enough capital to give yourself multiple shots on
[00:28:07.000 --> 00:28:09.200]   goal in case the first couple don't work out?
[00:28:09.200 --> 00:28:10.200]   Right, right.
[00:28:10.200 --> 00:28:11.600]   Although you've done a good job with that.
[00:28:11.600 --> 00:28:14.600]   Oh yeah, I can't complain.
[00:28:14.600 --> 00:28:18.560]   Well, I guess I want to make sure I also ask you about some of your other work.
[00:28:18.560 --> 00:28:22.200]   And I wanted to ask you about Coursera and I guess teaching in general.
[00:28:22.200 --> 00:28:23.320]   I think you're not teaching anymore.
[00:28:23.320 --> 00:28:24.320]   Is that right?
[00:28:24.320 --> 00:28:26.520]   No, I'm no longer.
[00:28:26.520 --> 00:28:27.760]   I'm a professor at Stanford.
[00:28:27.760 --> 00:28:32.720]   I'm an adjunct and it's great to have some connection back to the department, but I don't
[00:28:32.720 --> 00:28:34.400]   teach anymore.
[00:28:34.400 --> 00:28:38.480]   It seems sad to me because I just wanted to say you were such an amazing teacher.
[00:28:38.480 --> 00:28:42.080]   You were a notoriously difficult teacher.
[00:28:42.080 --> 00:28:46.520]   That was kind of your reputation.
[00:28:46.520 --> 00:28:53.920]   You weren't the warmest teacher, but you're memorable 16 or 17 years later as just a really,
[00:28:53.920 --> 00:28:55.760]   really excellent teacher.
[00:28:55.760 --> 00:29:00.360]   I feel like I just learned very quickly and efficiently from you.
[00:29:00.360 --> 00:29:05.520]   And also when I TA'd for you, I got to see how much you cared about grading, which I
[00:29:05.520 --> 00:29:06.520]   really appreciate.
[00:29:06.520 --> 00:29:07.520]   It was interesting to see.
[00:29:07.520 --> 00:29:12.760]   I was coming from a math department too where they just did not care about teaching or grading.
[00:29:12.760 --> 00:29:19.040]   And so it was incredibly, it felt just really good that someone's here and really cares
[00:29:19.040 --> 00:29:20.040]   to take the time.
[00:29:20.040 --> 00:29:25.440]   And so I wasn't surprised that you started a company around teaching, but I was just
[00:29:25.440 --> 00:29:29.240]   curious to hear the story about it and how you thought about it and what happened in
[00:29:29.240 --> 00:29:30.520]   the early days.
[00:29:30.520 --> 00:29:36.320]   So teaching had always been a passion project of mine, kind of like on the side, because
[00:29:36.320 --> 00:29:43.800]   as someone who's on the research side of a top academic institution, top research institution
[00:29:43.800 --> 00:29:46.720]   like Stanford, you're not supposed to really invest a lot of time in teaching.
[00:29:46.720 --> 00:29:50.960]   So I was always a little bit of an outlier in wanting to spend time on that.
[00:29:50.960 --> 00:29:53.880]   Can I ask, what do you think that was that made you want to do it?
[00:29:53.880 --> 00:29:59.200]   Because it really was quite evident that you cared more than anyone else about teaching.
[00:29:59.200 --> 00:30:07.200]   I guess I've always thought that education was just the door to opportunity and that
[00:30:07.200 --> 00:30:12.460]   if you set someone on the right path at an early age, or rather you enable them to get
[00:30:12.460 --> 00:30:16.200]   on the right path at a relatively early age, because there's only, I mean, teaching is
[00:30:16.200 --> 00:30:18.000]   not really a thing.
[00:30:18.000 --> 00:30:23.360]   A teacher enables people to learn and become who they can be.
[00:30:23.360 --> 00:30:26.640]   And they have to make the investment and want it.
[00:30:26.640 --> 00:30:30.440]   I mean, learn is, you can't learn someone, they have to learn.
[00:30:30.440 --> 00:30:36.720]   And so I just felt like it was an incredible enabler.
[00:30:36.720 --> 00:30:42.280]   And I'd come from a family where I was privileged in that both of my parents had access to higher
[00:30:42.280 --> 00:30:47.840]   education and I saw how much opportunity that created for me that others just didn't have.
[00:30:47.840 --> 00:30:54.160]   And I guess I've always felt and still feel and really try and teach my children as well,
[00:30:54.160 --> 00:30:59.300]   that for those of us who have been privileged, much is expected and it's our responsibility
[00:30:59.300 --> 00:31:01.200]   to give something back.
[00:31:01.200 --> 00:31:06.920]   So that was at that point, my way of giving something back is by teaching.
[00:31:06.920 --> 00:31:12.640]   And in fact, that was what led me to also eventually depart Stanford because I felt
[00:31:12.640 --> 00:31:19.700]   like my opportunity to give something back to the world in a much greater scale was available
[00:31:19.700 --> 00:31:25.280]   to me by founding Coursera and opening up education to a much, much, much larger number
[00:31:25.280 --> 00:31:28.880]   than I would ever be able to teach at Stanford.
[00:31:28.880 --> 00:31:33.860]   And that's actually also what led me to In-sitro because I feel like there's an incredible
[00:31:33.860 --> 00:31:39.900]   moment in time now in bringing together two disciplines in a way that could be totally
[00:31:39.900 --> 00:31:42.960]   transformative to the world.
[00:31:42.960 --> 00:31:47.160]   And I think it's kind of incumbent upon me.
[00:31:47.160 --> 00:31:51.800]   There's almost a moral imperative to make that happen if I can do that.
[00:31:51.800 --> 00:31:54.820]   And it's not something that many other people can do.
[00:31:54.820 --> 00:32:00.040]   And I guess, I saw you started another company in Gageley that seems like a teaching tool,
[00:32:00.040 --> 00:32:01.040]   right?
[00:32:01.040 --> 00:32:05.200]   And was that a reaction to something you wished Coursera did?
[00:32:05.200 --> 00:32:07.140]   Yeah.
[00:32:07.140 --> 00:32:17.680]   So yes and no, in the sense that it was driven by the observations that we had in the pandemic
[00:32:17.680 --> 00:32:25.200]   when all of a sudden I had two teenage kids who were thrust into Zoom school.
[00:32:25.200 --> 00:32:33.280]   And these are two kids that are academic high performers that are by and large pretty diligent.
[00:32:33.280 --> 00:32:39.280]   And at some point I was kind of looking in on them and noticing that the youngest after
[00:32:39.280 --> 00:32:44.280]   a few minutes in her class, making sure that the teacher saw that she was there, would
[00:32:44.280 --> 00:32:47.720]   turn off the camera and the microphone and spend the rest of the class perfecting her
[00:32:47.720 --> 00:32:49.040]   Sims game.
[00:32:49.040 --> 00:32:53.320]   Whereas the older one would spend the time going through the Netflix catalog.
[00:32:53.320 --> 00:32:57.640]   And this is like, okay, if this is what my kids are doing, despite the fact that they
[00:32:57.640 --> 00:33:02.840]   have all these opportunities, what happens to all those other kids who don't have that
[00:33:02.840 --> 00:33:07.800]   same set of privileges and who are not in, they're going to a school with much larger
[00:33:07.800 --> 00:33:12.880]   classes and teachers who have way less time to invest in trying to make the classes better
[00:33:12.880 --> 00:33:14.340]   on video.
[00:33:14.340 --> 00:33:16.080]   So that was really part of it.
[00:33:16.080 --> 00:33:20.680]   But truthfully, and this comes back to, I think the thrust of your question, Luke, is
[00:33:20.680 --> 00:33:28.280]   that originally when I was getting interested at Stanford in teaching, it was actually not
[00:33:28.280 --> 00:33:34.280]   originally with the only purpose of teaching the world, but also in trying to get teaching
[00:33:34.280 --> 00:33:40.360]   to be better even at Stanford, because I felt like, okay, I got to spend whatever, three
[00:33:40.360 --> 00:33:43.800]   hours a week with people like you in a class.
[00:33:43.800 --> 00:33:48.840]   And we were making use of that time with me just standing in front of the class, droning
[00:33:48.840 --> 00:33:53.760]   at you and delivering a lecture that was not that different to what I delivered a year
[00:33:53.760 --> 00:33:54.760]   before.
[00:33:54.760 --> 00:34:00.320]   Is that really the best use of class time or can we spend the time actually engaging
[00:34:00.320 --> 00:34:05.080]   and interacting with each other and really learning, which is much more of an active
[00:34:05.080 --> 00:34:10.720]   effort than it is just sitting there, watching a professor talk at you.
[00:34:10.720 --> 00:34:17.040]   And so this really was to me coming back to what had motivated me to go into a lot of
[00:34:17.040 --> 00:34:24.840]   capabilities that ultimately went on to become what we built in Coursera and really create
[00:34:24.840 --> 00:34:32.400]   a tool by which people can learn together, even if they are not physically co-located.
[00:34:32.400 --> 00:34:41.000]   And what we've discovered is that the move online actually makes things better, irrespective
[00:34:41.000 --> 00:34:49.160]   of whether you're in the same classroom or not, just because of the ability to flexibly
[00:34:49.160 --> 00:34:54.880]   chat with people who are in a group with you, work together as a team and really create
[00:34:54.880 --> 00:35:00.160]   an environment that fosters active learning in a way that is very hard to do if you just
[00:35:00.160 --> 00:35:06.100]   have a bunch of people sitting in a large auditorium with not great acoustics, all facing
[00:35:06.100 --> 00:35:11.200]   forward in fixed seating with a tiered classroom looking at the instructor down below.
[00:35:11.200 --> 00:35:18.280]   So I think I'm hopeful that one of the few benefits of this terrible pandemic that we're
[00:35:18.280 --> 00:35:23.280]   suffering through is that we will not actually go back to teaching the way we did before
[00:35:23.280 --> 00:35:28.240]   the pandemic, but we'll have a better way of teaching.
[00:35:28.240 --> 00:35:29.240]   Interesting.
[00:35:29.240 --> 00:35:33.640]   Yeah, I'm remembering now that I think one of the things you did really well, I thought
[00:35:33.640 --> 00:35:40.200]   in an in-person class, was actually watching when you were losing the class and then pacing.
[00:35:40.200 --> 00:35:43.600]   I remember you had this trick where you would ask, "Who does understand what I'm saying?"
[00:35:43.600 --> 00:35:45.240]   Which everyone should do.
[00:35:45.240 --> 00:35:49.160]   It's funny, I've taken that with me for the rest of my life in talks and stuff, and I
[00:35:49.160 --> 00:35:52.240]   really appreciate it, but I kind of kept, "Everyone should do it," because a lot of
[00:35:52.240 --> 00:35:56.040]   times you would ask that and it would be a third of the people would raise their hand.
[00:35:56.040 --> 00:36:00.040]   It was actually even helpful for me to know as a nervous student that I'm not the only
[00:36:00.040 --> 00:36:04.160]   one who's kind of lost track of where this is going.
[00:36:04.160 --> 00:36:11.080]   Yeah, a lot of people ask the opposite question, which is, "Who's not with me?"
[00:36:11.080 --> 00:36:15.840]   It's like, "Well, most people haven't even absorbed the question and you've already moved
[00:36:15.840 --> 00:36:16.840]   on."
[00:36:16.840 --> 00:36:17.840]   Or, "Does anyone have questions?"
[00:36:17.840 --> 00:36:22.600]   It's like, "Well, I don't even know if I have a question because I haven't understood what
[00:36:22.600 --> 00:36:23.600]   you're saying."
[00:36:23.600 --> 00:36:30.760]   So I think it's really important to create an atmosphere where the default is, "I'm not
[00:36:30.760 --> 00:36:34.880]   understanding," rather the default is, "I am understanding," and especially when you
[00:36:34.880 --> 00:36:37.840]   are teaching complex material.
[00:36:37.840 --> 00:36:39.880]   Right, right.
[00:36:39.880 --> 00:36:44.640]   Another question I wanted to ask about, I remember years ago when I was your student,
[00:36:44.640 --> 00:36:49.240]   you were super interested in probabilistic graphical models, which were really interesting.
[00:36:49.240 --> 00:36:54.440]   I remember especially being interested in the thing that stuck with me with them is
[00:36:54.440 --> 00:36:55.440]   sort of like causality.
[00:36:55.440 --> 00:36:59.360]   It seems like you can kind of find that in data, which is really cool and surprising.
[00:36:59.360 --> 00:37:03.160]   But I was kind of curious, have you maintained an interest in that?
[00:37:03.160 --> 00:37:05.600]   Has that field evolved in interesting ways?
[00:37:05.600 --> 00:37:06.680]   What's happened with them?
[00:37:06.680 --> 00:37:08.280]   I don't hear about them as much.
[00:37:08.280 --> 00:37:15.480]   Well, I mean, I think there's been a lot of discussions in the last few years about deep
[00:37:15.480 --> 00:37:22.840]   learning because of all of the big transformative things that deep learning has been able to
[00:37:22.840 --> 00:37:27.320]   do because we've been able to get away from feature engineering, which has been such a
[00:37:27.320 --> 00:37:31.680]   pain point in most of the tasks that we deal with.
[00:37:31.680 --> 00:37:37.760]   I think there is clearly still very much a need for understanding causality.
[00:37:37.760 --> 00:37:42.160]   And when I think about the work that we're doing in drug discovery, the fundamental question
[00:37:42.160 --> 00:37:48.720]   that we're asking is, if I make this intervention in a human, is it going to make a clinical
[00:37:48.720 --> 00:37:49.720]   difference?
[00:37:49.720 --> 00:37:50.720]   Is it going to benefit the human?
[00:37:50.720 --> 00:37:52.680]   And that is an interventional question.
[00:37:52.680 --> 00:37:59.120]   And if you confuse that question with an observational question, you very easily and immediately
[00:37:59.120 --> 00:38:06.040]   fall into all sorts of traps about correlation being different from causation and a lot of
[00:38:06.040 --> 00:38:12.680]   the correlations being completely going in the wrong direction from a causal perspective.
[00:38:12.680 --> 00:38:19.100]   So you find yourself intervening in symptoms or just downstream sequela that have nothing
[00:38:19.100 --> 00:38:22.000]   to do with the fundamental disease processes.
[00:38:22.000 --> 00:38:28.240]   And I think even in machine learning more broadly, there is a growing recognition that
[00:38:28.240 --> 00:38:33.800]   that is one of the big unsolved problems in getting machine learning to go to the next
[00:38:33.800 --> 00:38:34.800]   level.
[00:38:34.800 --> 00:38:41.040]   I was at the NeurIPS conference, not this past year, but pre-pandemic, and Yoshua Bengio
[00:38:41.040 --> 00:38:48.240]   was giving a keynote talk and he highlighted that as one of the main unsolved problems,
[00:38:48.240 --> 00:38:53.640]   both because of its intrinsic importance, but also because understanding causality enables
[00:38:53.640 --> 00:39:00.120]   you and the causal processes that underlie the world enable you to learn with much sparser
[00:39:00.120 --> 00:39:03.000]   data because you have a much more structured representation.
[00:39:03.000 --> 00:39:10.520]   So I think that what's likely to happen is that the pendulum has swung very much towards
[00:39:10.520 --> 00:39:15.780]   the deep learning side of the world as it should because of the tremendous advantages,
[00:39:15.780 --> 00:39:18.680]   but I think it's now starting to coalesce.
[00:39:18.680 --> 00:39:21.200]   These two paths are starting to coalesce.
[00:39:21.200 --> 00:39:27.400]   And I think that's where we're going to see a lot of interesting work coming out on that
[00:39:27.400 --> 00:39:28.400]   front.
[00:39:28.400 --> 00:39:29.400]   Cool.
[00:39:29.400 --> 00:39:30.400]   Thanks.
[00:39:30.400 --> 00:39:33.080]   So we always end with two questions and I want to make sure you have a little bit of
[00:39:33.080 --> 00:39:34.080]   time for them.
[00:39:34.080 --> 00:39:38.680]   So the penultimate question is, what's an underrated topic in machine learning?
[00:39:38.680 --> 00:39:42.520]   And maybe I'd say it to you like this, if you had more time on your hands, what new
[00:39:42.520 --> 00:39:46.320]   thing would you investigate or look into?
[00:39:46.320 --> 00:39:54.560]   So I'm going to use this opportunity to give you two answers, one of which is maybe more
[00:39:54.560 --> 00:39:58.600]   directly to your question and the other one, which we didn't get to earlier, which is the
[00:39:58.600 --> 00:40:00.960]   why I'm doing what I'm doing right now.
[00:40:00.960 --> 00:40:09.160]   So I think on the pure machine learning front, what we discussed earlier is really a fundamental
[00:40:09.160 --> 00:40:18.360]   problem, which is how do we leverage large amounts of weakly supervised, unsupervised
[00:40:18.360 --> 00:40:24.760]   data to learn a representation that enables us to then very efficiently learn from much
[00:40:24.760 --> 00:40:27.240]   smaller data sets.
[00:40:27.240 --> 00:40:33.320]   And I think that's an area where, yeah, people have said, well, there's whatever the image
[00:40:33.320 --> 00:40:37.920]   representation that we learned in ResNet and of course, Word2Vec and there is others.
[00:40:37.920 --> 00:40:43.740]   But I don't think we've really sort of pushed this to the limits in terms of how do you
[00:40:43.740 --> 00:40:47.400]   bring these different types of data sets together?
[00:40:47.400 --> 00:40:52.040]   What's the right way of combining the objective functions in a way that balances things in
[00:40:52.040 --> 00:40:53.040]   the right way?
[00:40:53.040 --> 00:40:57.240]   And I think that's an area where there's going to be a lot of interesting progress of how
[00:40:57.240 --> 00:41:01.800]   do you learn and refine a representation over time.
[00:41:01.800 --> 00:41:06.840]   If I broaden this question out from machine learning specifically and ask where I think
[00:41:06.840 --> 00:41:15.880]   there is a really big opportunity for the world, it's in this convergence of these two
[00:41:15.880 --> 00:41:21.200]   disciplines, which is biology and data science and maybe engineering.
[00:41:21.200 --> 00:41:22.960]   So maybe it's three disciplines.
[00:41:22.960 --> 00:41:28.680]   And the analogy that I use here is if you look at the history of science, there's been
[00:41:28.680 --> 00:41:33.480]   sort of epochs in history where one field has just sort of really taken off and made
[00:41:33.480 --> 00:41:36.760]   a tremendous impact on the world in a relatively short amount of time.
[00:41:36.760 --> 00:41:41.040]   And in the late 1800s, that was chemistry with a periodic table.
[00:41:41.040 --> 00:41:47.240]   And then in the early 1900s, it was physics with understanding the connection between
[00:41:47.240 --> 00:41:50.200]   matter and energy and between space and time.
[00:41:50.200 --> 00:41:56.400]   And then in the 1950s, it was computing and the ability to use silicone chips as a way
[00:41:56.400 --> 00:42:03.360]   of really doing calculations that up until that point, maybe or even not a person could
[00:42:03.360 --> 00:42:04.360]   do.
[00:42:04.360 --> 00:42:08.260]   And then in the 1990s and 2000s, there was kind of a bifurcation.
[00:42:08.260 --> 00:42:13.920]   There was data as a field, which emerged from computing, but also from optimization and
[00:42:13.920 --> 00:42:15.360]   statistics and neuroscience.
[00:42:15.360 --> 00:42:17.480]   And I think it's really its own field.
[00:42:17.480 --> 00:42:22.920]   And the other is what I call quantitative biology, which started to measure finally
[00:42:22.920 --> 00:42:30.480]   in a very robust and reproducible and quantitative way aspects of biological systems.
[00:42:30.480 --> 00:42:34.800]   And that's what gave us sequencing and microscopy and all the things that I talked about before.
[00:42:34.800 --> 00:42:39.800]   And I think the next big field that's going to emerge is the convergence of those two
[00:42:39.800 --> 00:42:40.960]   fields into one.
[00:42:40.960 --> 00:42:43.720]   And I'm calling it digital biology.
[00:42:43.720 --> 00:42:52.200]   And to me, it's the ability to measure biology with fidelity and at scale, use machine learning
[00:42:52.200 --> 00:42:58.080]   and data science to interpret the measurements that we get, and then use bioengineering techniques
[00:42:58.080 --> 00:43:02.760]   to go back and intervene in biology to get it to do something that it wouldn't otherwise
[00:43:02.760 --> 00:43:03.760]   do.
[00:43:03.760 --> 00:43:09.680]   And that has implications in human health, but it also has implications in biomaterials
[00:43:09.680 --> 00:43:15.800]   and in agricultural technology and in environmental science and in energy science.
[00:43:15.800 --> 00:43:20.840]   And all of these are places where the convergence of those two fields and this digital biology
[00:43:20.840 --> 00:43:22.600]   is just going to transform that space.
[00:43:22.600 --> 00:43:27.520]   And I think that's going to be the next big field of the next whatever epoch of science.
[00:43:27.520 --> 00:43:28.520]   Wow.
[00:43:28.520 --> 00:43:30.520]   Hopefully, well said.
[00:43:30.520 --> 00:43:33.280]   That's going in the highlight reel.
[00:43:33.280 --> 00:43:39.640]   It's actually a good segue to our final question, which is, I would say this for in-situ, but
[00:43:39.640 --> 00:43:41.280]   I'm not sure.
[00:43:41.280 --> 00:43:45.280]   You're trying to discover new drugs using machine learning.
[00:43:45.280 --> 00:43:50.680]   What are the practical day-to-day challenges right now of making that work?
[00:43:50.680 --> 00:43:54.960]   Well, so I think there are a number.
[00:43:54.960 --> 00:44:00.600]   So by highlight two, one is that biology is really hard.
[00:44:00.600 --> 00:44:09.600]   You are dealing with live things and they're variable and they depend on the exact temperature.
[00:44:09.600 --> 00:44:14.360]   And the room and on who the tech is that's manipulating them.
[00:44:14.360 --> 00:44:18.280]   And a lot of things that you don't normally think about and we don't have to deal with
[00:44:18.280 --> 00:44:20.480]   in a lot of the more exact sciences.
[00:44:20.480 --> 00:44:26.520]   And so how do you create data sets that are robust enough and experimental procedures
[00:44:26.520 --> 00:44:32.320]   that are robust enough so that the noise does not overwhelm the signal and the variability
[00:44:32.320 --> 00:44:34.480]   does not overwhelm the signal.
[00:44:34.480 --> 00:44:39.360]   And the second is that in order to do the kind of work that we're doing, you need to
[00:44:39.360 --> 00:44:46.200]   create a really unique culture of individuals who are able to sort of speak both languages,
[00:44:46.200 --> 00:44:51.520]   at least to a certain extent, and communicate with people with a discipline very different
[00:44:51.520 --> 00:44:52.960]   to their own.
[00:44:52.960 --> 00:44:57.080]   That's something that we don't have to do quite as much in many other applications of
[00:44:57.080 --> 00:44:58.080]   machine learning.
[00:44:58.080 --> 00:45:02.120]   So if you're doing machine learning for web recommendations, you don't need to deeply
[00:45:02.120 --> 00:45:08.880]   understand the catalog of items on the Amazon site in order to write the recommendation
[00:45:08.880 --> 00:45:09.880]   algorithm.
[00:45:09.880 --> 00:45:11.360]   That's not true for biology.
[00:45:11.360 --> 00:45:15.320]   You really need to understand enough that you can have a meaningful conversation with
[00:45:15.320 --> 00:45:17.720]   a biologist or a chemist.
[00:45:17.720 --> 00:45:24.320]   And so the recruiting of people who either have that joint skill set or are willing to
[00:45:24.320 --> 00:45:29.880]   learn enough to have a meaningful dialogue and really work as part of a truly cross-functional
[00:45:29.880 --> 00:45:35.400]   team with people from the other discipline, we don't train enough people like that.
[00:45:35.400 --> 00:45:41.120]   And I think building the company with that kind of individual and with the right culture
[00:45:41.120 --> 00:45:44.080]   is something that I think about all the time.
[00:45:44.080 --> 00:45:48.460]   And I think we've done a really great job of it at Insitro so far, but it's definitely
[00:45:48.460 --> 00:45:51.960]   an ongoing effort all the time.
[00:45:51.960 --> 00:45:53.120]   Awesome.
[00:45:53.120 --> 00:45:54.120]   Thank you so much.
[00:45:54.120 --> 00:45:55.120]   Great.
[00:45:55.120 --> 00:45:56.120]   Thank you.
[00:45:56.120 --> 00:45:59.680]   Thanks for listening to another episode of Gradient Dissent.
[00:45:59.680 --> 00:46:03.920]   Doing these interviews are a lot of fun and it's especially fun for me when I can actually
[00:46:03.920 --> 00:46:06.680]   hear from the people that are listening to these episodes.
[00:46:06.680 --> 00:46:10.760]   So if you wouldn't mind leaving a comment and telling me what you think or starting
[00:46:10.760 --> 00:46:14.720]   a conversation, that would make me inspired to do more of these episodes.
[00:46:14.720 --> 00:46:18.280]   And also if you wouldn't mind liking and subscribing, I'd appreciate that a lot.

