
[00:00:00.000 --> 00:00:06.400]   We have a new AGI lab, two battling figureheads fighting over the future of AI,
[00:00:06.400 --> 00:00:10.600]   new exclusive accounts of what happened in the last 48 hours,
[00:00:10.600 --> 00:00:12.500]   and I haven't even had breakfast.
[00:00:12.500 --> 00:00:16.800]   This video will not only cover developments up to the last few minutes,
[00:00:16.800 --> 00:00:21.600]   but will try to provide one of the clearest accounts as to what has transpired
[00:00:21.600 --> 00:00:23.700]   and the personalities behind it.
[00:00:23.700 --> 00:00:28.500]   This has truly been a weekend that has changed the course of AI.
[00:00:28.500 --> 00:00:34.200]   Let's start though with this, a series of posts on Twitter from OpenAI senior staff saying
[00:00:34.200 --> 00:00:36.700]   "OpenAI is nothing without its people.
[00:00:36.700 --> 00:00:42.700]   It seems like the original OpenAI is imploding in real time, and for some it's gotten personal."
[00:00:42.700 --> 00:00:47.000]   One research scientist at OpenAI said "Ilya does not care about safety or humanity.
[00:00:47.000 --> 00:00:50.500]   This is just ego and power-hungriness that backfired."
[00:00:50.500 --> 00:00:55.300]   And in the last few moments, we learn that Ilya Sutskova himself regrets what happened.
[00:00:55.300 --> 00:01:01.700]   He says "I love everything we've built together and I will do everything I can to reunite the company."
[00:01:01.700 --> 00:01:07.600]   And at 5.30am his time, after presumably not having slept at all, Sam Altman hearted that comment.
[00:01:07.600 --> 00:01:09.600]   Who knows what will come of that?
[00:01:09.600 --> 00:01:14.800]   But with perhaps 10 crazy revelations to get to, I'm not going to spend too much time on the Twitter wars.
[00:01:14.800 --> 00:01:19.300]   Because first and foremost, we have a new AGI lab within Microsoft.
[00:01:19.300 --> 00:01:24.300]   Sam Altman is going to be the CEO of this new group and Greg Brockman is joining him.
[00:01:24.300 --> 00:01:30.600]   Of course, with what we've seen on Twitter, it looks like hundreds of OpenAI former employees might be joining them.
[00:01:30.600 --> 00:01:37.900]   The group doesn't even have a name yet, but with Microsoft's compute, they look set to become one of the major AGI players.
[00:01:37.900 --> 00:01:46.000]   And the CEO of Microsoft has already confirmed that Microsoft will quickly provide this new group with the resources needed for their success.
[00:01:46.000 --> 00:01:49.800]   Of course, OpenAI aren't yet dead and I'll get to their new CEO in a moment.
[00:01:49.800 --> 00:01:54.200]   But first, we have a bit more info about what led to this monumental split.
[00:01:54.200 --> 00:02:01.200]   A key division arose between the visions of Ilya Sutskova, the chief scientist of OpenAI, and Sam Altman, the CEO.
[00:02:01.200 --> 00:02:06.900]   In October, Sam Altman moved to reduce Sutskova's role at the company, which obviously infuriated him.
[00:02:06.900 --> 00:02:12.700]   Then, according to Bloomberg, there were announcements at Dev Day that further angered Sutskova.
[00:02:12.700 --> 00:02:20.100]   Apparently, the safety team really didn't like the customized versions of ChatGPT that allowed anyone to create chatbots.
[00:02:20.100 --> 00:02:25.200]   And particularly, they didn't like the idea of those chatbots becoming autonomous agents.
[00:02:25.200 --> 00:02:28.000]   But the rift goes further back even than that.
[00:02:28.000 --> 00:02:32.700]   This exclusive extract from a book was released in The Atlantic in the last few minutes.
[00:02:32.700 --> 00:02:38.100]   Apparently, the November release last year of ChatGPT caused a rift inside the company.
[00:02:38.100 --> 00:02:41.400]   One faction felt the company and indeed the world wasn't ready.
[00:02:41.400 --> 00:02:46.500]   The extract says this strained the already tense relationship between OpenAI's factions,
[00:02:46.500 --> 00:02:51.400]   which Altman referred to in 2019 as the different tribes within the company.
[00:02:51.400 --> 00:02:59.900]   The success of ChatGPT caught the company by surprise and computing power from research teams was redirected to handle the flow of traffic.
[00:02:59.900 --> 00:03:04.500]   The servers kept crashing and safety teams within the company pushed to slow things down.
[00:03:04.500 --> 00:03:10.800]   When GPT-4 was launched, apparently customers tried to take advantage of the $20 free credit offered.
[00:03:10.800 --> 00:03:19.800]   And then what happened was that employees from the already small trust and safety staff were reassigned from other abuse areas to focus on that fraud issue.
[00:03:19.800 --> 00:03:25.200]   Some employees in the safety team struggled with mental health issues and communication was poor.
[00:03:25.200 --> 00:03:31.400]   In an ironic twist, co-workers would find out that colleagues had been fired only after noticing them disappear on Slack.
[00:03:31.400 --> 00:03:38.200]   But according to this book, what also started to happen was that Ilya Sutskova began to behave a bit like a spiritual leader.
[00:03:38.200 --> 00:03:42.400]   His constant enthusiastic refrain was to "feel the AGI".
[00:03:42.400 --> 00:03:49.200]   And at one of their holiday parties, Sutskova led employees in a chant "feel the AGI, feel the AGI".
[00:03:49.200 --> 00:03:57.200]   And in the last few minutes, we learn that Ilya Sutskova was using that question, "do you feel the AGI" in job interviews.
[00:03:57.200 --> 00:04:00.400]   This researcher, Adam Roberts, ended up at Google DeepMind.
[00:04:00.400 --> 00:04:02.600]   And it only gets weirder from here.
[00:04:02.600 --> 00:04:05.700]   Apparently, Sutskova commissioned a wooden effigy.
[00:04:05.700 --> 00:04:10.500]   That's like a miniature statue, and that was intended to represent an unaligned AI.
[00:04:10.500 --> 00:04:15.900]   He then set that effigy on fire to symbolize OpenAI's commitment to its founding principles.
[00:04:15.900 --> 00:04:23.300]   Then things seem to have come to a head when Sam Altman began to fundraise for a billion dollar new chip venture.
[00:04:23.300 --> 00:04:28.600]   That was apparently codenamed Tigris, which is a major river flowing through the Middle East.
[00:04:28.600 --> 00:04:32.400]   He wanted an AI focused chip to compete against NVIDIA.
[00:04:32.400 --> 00:04:40.000]   There are strong hints that Sam Altman not being candid about this activity could have been the proximate cause of the firing.
[00:04:40.000 --> 00:04:47.300]   Anyway, I've covered the 15 minute firing in my previous video, but now for an hour by hour account of what happened after that.
[00:04:47.300 --> 00:04:56.300]   Well, the first thing that happened was that the CEO of Microsoft, which don't forget has invested 13 billion into OpenAI, was apparently livid at the announcement.
[00:04:56.300 --> 00:05:01.300]   Remember, he was blindsided and only given a minute's notice of the firing.
[00:05:01.300 --> 00:05:07.600]   For Nadella, don't forget, OpenAI and GPT-4 was his way, and I quote, of making Google dance.
[00:05:07.600 --> 00:05:12.300]   And that investment seemed to be crumbling before the shock move that he made next.
[00:05:12.300 --> 00:05:21.200]   But back to the moments after the firing, the pressure from Microsoft and other investors caused the board to even consider resigning themselves.
[00:05:21.200 --> 00:05:25.600]   Apparently, Sam Altman and Greg Brockman wouldn't return with the current board in place.
[00:05:25.600 --> 00:05:32.500]   At this point, it looked like there would be a complete reversal with the board and Satskava going and Sam Altman returning.
[00:05:32.500 --> 00:05:39.500]   Even the interim CEO, Mira Murati, apparently was asking Sam Altman and Greg Brockman to come back.
[00:05:39.500 --> 00:05:42.200]   That's around the time we got the tweet from Sam Altman.
[00:05:42.200 --> 00:05:44.300]   I love the OpenAI team so much.
[00:05:44.300 --> 00:05:50.400]   That got 29 million impressions and caused hundreds of OpenAI staff to retweet hearts.
[00:05:50.400 --> 00:05:52.500]   Everything looks set for his return.
[00:05:52.500 --> 00:05:56.100]   Sam Altman even physically went back to the OpenAI offices.
[00:05:56.100 --> 00:05:58.200]   But remember, his account had been locked.
[00:05:58.200 --> 00:06:01.200]   He had no access to his pass or his emails.
[00:06:01.200 --> 00:06:03.700]   So this was him with a guest pass.
[00:06:03.700 --> 00:06:09.000]   And his not so subtle message to investors and the board was, you don't have long to persuade me to come back.
[00:06:09.000 --> 00:06:12.300]   This is the first and last time I'll ever wear one of these.
[00:06:12.300 --> 00:06:19.100]   But he may have pushed too far because what he wanted was not only the removal of existing board members, including Ilya Satskava,
[00:06:19.100 --> 00:06:22.800]   but also a statement absolving him of wrongdoing.
[00:06:22.800 --> 00:06:27.500]   Remember, this was after the board had initially agreed in principle to step down.
[00:06:27.500 --> 00:06:33.000]   And it seems that statement was the sticking point that caused the implosion of OpenAI.
[00:06:33.000 --> 00:06:36.400]   Satskava and the board refused to make that statement.
[00:06:36.400 --> 00:06:41.600]   And while this was going on, employees from OpenAI were already starting to apply for other jobs.
[00:06:41.600 --> 00:06:45.100]   Apparently, a handful had already applied to Google DeepMind.
[00:06:45.100 --> 00:06:51.200]   And according to the information, dozens of other OpenAI staffers were announcing that they were quitting the company.
[00:06:51.200 --> 00:06:57.500]   Cohere, Nvidia, Anthropic and Google DeepMind were actively trying to recruit staff from OpenAI.
[00:06:57.500 --> 00:07:01.000]   While some have speculated that it was the board that asked Samon to come back,
[00:07:01.000 --> 00:07:05.700]   it seems far more likely that it was Mira Murati and other OpenAI staff.
[00:07:05.700 --> 00:07:12.300]   After all, Satskava at this point was still declaring that removing Altman was the only path to defend the company's mission.
[00:07:12.300 --> 00:07:15.800]   This was backed up by Ashley Vance, a writer at Bloomberg.
[00:07:15.800 --> 00:07:18.600]   She said Mira planned to hire Sam and Greg back.
[00:07:18.600 --> 00:07:21.500]   She turned Team Sam over the past couple of days.
[00:07:21.500 --> 00:07:26.400]   It became a game of chicken where the idea was to force the board to fire everyone.
[00:07:26.400 --> 00:07:29.200]   They figured the board wouldn't do that and would back down.
[00:07:29.200 --> 00:07:32.400]   The board, including Satskava, went into total silence.
[00:07:32.400 --> 00:07:34.500]   But then they found their own CEO.
[00:07:34.500 --> 00:07:38.900]   His name is Emmet Shear, former CEO of Twitch, and I'll get to him in a second.
[00:07:38.900 --> 00:07:44.200]   This was not, however, what Sam Altman was expecting, and according to a reliable reporter, Emily Chang,
[00:07:44.200 --> 00:07:47.600]   Sam Altman was in shock at that appointment.
[00:07:47.600 --> 00:07:52.900]   It seems he really thought the board would back down and put out that statement that he'd done nothing wrong.
[00:07:52.900 --> 00:07:56.500]   So who is the new CEO of OpenAI and what does he think?
[00:07:56.500 --> 00:08:00.000]   Well, he clearly doesn't want OpenAI to die without a fight.
[00:08:00.000 --> 00:08:05.600]   He said, "I took this job because I believe that OpenAI is one of the most important companies currently in existence."
[00:08:05.600 --> 00:08:09.200]   Remember, OpenAI still owns the ChatchiBT brand.
[00:08:09.200 --> 00:08:11.800]   That alone keeps them in an extremely strong position,
[00:08:11.800 --> 00:08:16.400]   especially with the general public that don't follow all of these insider changes.
[00:08:16.400 --> 00:08:19.400]   The new CEO also made a few critical points.
[00:08:19.400 --> 00:08:23.400]   He said, "Our partnership with Microsoft, hint hint, remains strong.
[00:08:23.400 --> 00:08:27.500]   That's almost to reiterate to anyone wondering, we still have the compute.
[00:08:27.500 --> 00:08:29.000]   We still have that partnership."
[00:08:29.000 --> 00:08:34.700]   Emmet Shear is now surely engaged in an almighty struggle to keep as many staff as he can.
[00:08:34.700 --> 00:08:39.600]   Obviously, if they all go, even the ChatchiBT brand probably isn't enough.
[00:08:39.600 --> 00:08:43.900]   But this statement released in the last couple of hours is interesting for other reasons.
[00:08:43.900 --> 00:08:50.800]   He admitted that the process and communications around Sam Altman's removal have been handled very badly,
[00:08:50.800 --> 00:08:52.800]   which has seriously damaged our trust.
[00:08:52.800 --> 00:08:58.500]   He is apparently going to hire an independent investigator to dig into the entire process.
[00:08:58.500 --> 00:09:00.300]   That's the classic new CEO move.
[00:09:00.300 --> 00:09:03.200]   I'm here to listen, take notes and investigate.
[00:09:03.200 --> 00:09:06.100]   He also ended the statement with this crucial point.
[00:09:06.100 --> 00:09:10.200]   "Before I took the job, I checked on the reasoning behind the change.
[00:09:10.200 --> 00:09:14.800]   The board did not remove Sam over any specific disagreement on safety.
[00:09:14.800 --> 00:09:17.600]   Their reasoning was completely different from that."
[00:09:17.600 --> 00:09:22.900]   He seems to be pointing back there to the lack of honesty argument that was originally put forward.
[00:09:22.900 --> 00:09:30.400]   He then ended with, "I'm not crazy enough to take this job without board support for commercializing our awesome models."
[00:09:30.400 --> 00:09:35.100]   Nevertheless, you can see why Sutskever and the board might have wanted to hire him.
[00:09:35.100 --> 00:09:40.800]   He said in September, "I'm actually in favor of a pause, or rather I'm in favor of a slowdown.
[00:09:40.800 --> 00:09:46.700]   We can't learn how to build a safe AI without experimenting, and we can't experiment without progress.
[00:09:46.700 --> 00:09:50.500]   We probably shouldn't be barreling ahead at max speed either."
[00:09:50.500 --> 00:09:53.000]   And in case it wasn't clear where his views come from,
[00:09:53.000 --> 00:09:57.800]   here's him speaking for just a couple of minutes outlining his views on the risk.
[00:09:57.800 --> 00:09:59.700]   I have a very specific concern about AI.
[00:09:59.700 --> 00:10:01.500]   We've built an intelligence. It's kind of amazing, actually.
[00:10:01.500 --> 00:10:04.100]   It may not be the smartest intelligence, but it is an unintelligence.
[00:10:04.100 --> 00:10:07.100]   It can solve problems and make arbitrary plans.
[00:10:07.100 --> 00:10:17.200]   At some point, as it gets better, the kinds of problems it will be able to solve will include programming, chip design, material science, power production,
[00:10:17.200 --> 00:10:21.500]   all of the things you would need to design an artificial intelligence.
[00:10:21.500 --> 00:10:26.000]   At that point, you will be able to point the thing we've built back at itself.
[00:10:26.000 --> 00:10:28.400]   And this will happen before you get to that point with humans in the loop.
[00:10:28.400 --> 00:10:29.900]   It already is happening with humans in the loop.
[00:10:29.900 --> 00:10:37.300]   But that loop will get tighter and tighter and tighter and faster and faster and faster until it can fully self-improve itself,
[00:10:37.300 --> 00:10:42.000]   at which point it will get very fast very quickly.
[00:10:42.000 --> 00:10:47.200]   And that kind of intelligence is just an intrinsically very dangerous thing because intelligence is power.
[00:10:47.200 --> 00:10:51.900]   Human beings are the dominant form of life on this planet pretty much entirely because we are smarter than the other creatures.
[00:10:51.900 --> 00:10:57.800]   Like my P-Doom, my probability of doom, is like my bid-ask spread, and that's pretty high because I have a lot of uncertainty.
[00:10:57.800 --> 00:11:01.000]   But I would say it's like between five and 50.
[00:11:01.000 --> 00:11:02.500]   It should cause you to shit your pants.
[00:11:02.500 --> 00:11:04.600]   But it's human-level extinction, I think.
[00:11:04.600 --> 00:11:05.000]   Yeah, yeah.
[00:11:05.000 --> 00:11:06.500]   No, no, it's not just human-level extinction.
[00:11:06.500 --> 00:11:09.100]   Extincting humans is bad enough.
[00:11:09.100 --> 00:11:11.900]   It's like potential destruction of all value in the light code.
[00:11:11.900 --> 00:11:19.100]   Here I am being the guy who's like the techno-optimist, and I am like, "No, no, no, the AI thing, this thing, though, actually may be a problem."
[00:11:19.100 --> 00:11:19.300]   Yeah.
[00:11:19.300 --> 00:11:33.500]   I have no financial stake in either Doomsday Presenter planning around AI or the other weird like double-think 4D chess thing people impute to, "Oh, we're actually trying to build up open AI and Anthropic and Google as being like super powerful."
[00:11:33.500 --> 00:11:40.400]   And it's all an ego thing about making, talking about how amazingly great this stuff is so that we can like raise more money for open AI.
[00:11:40.400 --> 00:11:41.800]   For regulatory capture or whatever.
[00:11:41.800 --> 00:11:42.400]   Yeah, yeah, yeah.
[00:11:42.400 --> 00:11:45.400]   And like I don't own any equity in any of these things.
[00:11:45.400 --> 00:11:51.700]   One of the key tasks of the new CEO is surely to retain talent like Andrej Karpathy.
[00:11:51.700 --> 00:11:58.600]   He was asked last night why he wasn't commenting on current events and said this, "I just don't have anything too remarkable to add right now.
[00:11:58.600 --> 00:12:02.500]   I like and respect Sam, and I think so does the majority of open AI."
[00:12:02.500 --> 00:12:04.400]   Notice he said majority, not everyone.
[00:12:04.400 --> 00:12:09.100]   "The board had a chance to explain their drastic actions, and they did not take it."
[00:12:09.100 --> 00:12:13.700]   And then after the more recent announcements, he tweeted a symbol for radioactivity.
[00:12:13.700 --> 00:12:17.800]   Something tells me he might not be sticking around at open AI for long.
[00:12:17.800 --> 00:12:21.500]   But don't assume he'll automatically want to join Microsoft.
[00:12:21.500 --> 00:12:27.400]   He might want to rejoin Musk at XAI, found his own venture, or maybe go to Google.
[00:12:27.400 --> 00:12:31.200]   I'm sure Musk would be delighted to have him and Ilya Sutskova back.
[00:12:31.200 --> 00:12:36.100]   Musk is the guy that after all persuaded Sutskova to join open AI in the first place.
[00:12:36.100 --> 00:12:39.200]   He said that Ilya has a good moral compass and doesn't seek power.
[00:12:39.200 --> 00:12:43.200]   He wouldn't take such drastic action unless he felt it was absolutely necessary.
[00:12:43.200 --> 00:12:48.000]   That does seem to me more of a pitch to get Sutskova and co to join his team.
[00:12:48.000 --> 00:12:54.500]   My own cheeky prediction is that Anthropic will make a move to merge with the remnants of open AI.
[00:12:54.500 --> 00:12:59.600]   Until recently, don't forget, most of the founding staff of Anthropic used to work at open AI.
[00:12:59.600 --> 00:13:04.400]   And Anthropic do seem to be much more safety focused, which would gel well with Sutskova.
[00:13:04.400 --> 00:13:08.900]   I await with curiosity to see if Anthropic does pitch such a move.
[00:13:08.900 --> 00:13:17.200]   It just seems to make sense to me because that would turn two smaller entities that would struggle to match Google and Microsoft into quite a formidable team.
[00:13:17.200 --> 00:13:21.400]   Anthropic seems to certainly share Sutskova's views on superintelligence,
[00:13:21.400 --> 00:13:26.000]   which Ilya Sutskova stated while sitting next to Sam Altman fairly recently.
[00:13:26.000 --> 00:13:33.000]   We are talking about as time goes by and the capability keeps increasing, you know, and eventually it goes all the way to here, right?
[00:13:33.000 --> 00:13:37.700]   Right now we are here. Today, that's where we are. That's where we're going to get to.
[00:13:37.700 --> 00:13:42.400]   When we get to this point, then yeah, it's very powerful technology.
[00:13:42.400 --> 00:13:46.900]   It can be used for amazing applications. You can say cure all disease.
[00:13:46.900 --> 00:13:52.800]   On the flip side, you can say create a disease much more worse than anything that existed before.
[00:13:52.800 --> 00:13:58.200]   That'd be bad. And in a somewhat prophetic statement from two weeks ago, Sutskova said this.
[00:13:58.200 --> 00:14:01.200]   You know, I think there's going to be a lot of disagreements, going to be a lot of political questions.
[00:14:01.200 --> 00:14:07.900]   But I think that as people see AI actually getting better, as people experience it,
[00:14:07.900 --> 00:14:19.400]   the desire for the pro-social superintelligence, the humanity loving superintelligence, you know, as much as this, as much as it can be done, will increase.
[00:14:19.400 --> 00:14:26.700]   And on the scientific problem, you know, I think right now it's still being an area where not that many people are working on.
[00:14:26.700 --> 00:14:31.000]   Our AIs are getting powerful enough where you can really start studying it productively.
[00:14:31.000 --> 00:14:33.900]   You'll have some very exciting research to share soon.
[00:14:33.900 --> 00:14:38.700]   And for anyone predicting that the open AI remnants will be no match for Microsoft,
[00:14:38.700 --> 00:14:42.400]   it's not like that relationship won't have its own challenges.
[00:14:42.400 --> 00:14:47.000]   Just this morning, Satya Nadella, the CEO of Microsoft, promised to allow them independence.
[00:14:47.000 --> 00:14:51.100]   They would be building independent identities within Microsoft.
[00:14:51.100 --> 00:14:55.600]   That was in reply to Sam Altman's tweet that the mission continues to build AGI.
[00:14:55.600 --> 00:14:59.400]   But is Microsoft big enough for two such huge personalities?
[00:14:59.400 --> 00:15:06.900]   Altman did once confess that he relished the power of running open AI and foresaw making strange decisions in the future.
[00:15:06.900 --> 00:15:09.000]   I mean, I have like lots of selfish reasons for doing this.
[00:15:09.000 --> 00:15:15.700]   And as you said, I get like all of the power of running open AI, but I can't think of like anything more fulfilling to work on.
[00:15:15.700 --> 00:15:20.500]   I don't think it's like particularly altruistic because it would be if I like didn't already have a bunch of money.
[00:15:20.500 --> 00:15:23.400]   Yeah, the money is going to like pile up faster than I can spend it anyway.
[00:15:23.400 --> 00:15:30.500]   I like being non-conflicted on open AI because I think the chance that we have to make a very strange decision someday is non-trivial.
[00:15:30.500 --> 00:15:35.600]   But now that he's a worker within Microsoft, will he be able to make such strange decisions?
[00:15:35.600 --> 00:15:41.500]   And Altman, after all, has talked in the past about the importance of there being a nonprofit board in charge.
[00:15:41.500 --> 00:15:44.200]   Like no one person should be trusted here.
[00:15:44.200 --> 00:15:47.100]   I don't have super voting shares.
[00:15:47.100 --> 00:15:49.800]   Like I don't want them. The board can fire me.
[00:15:49.800 --> 00:15:51.000]   I think that's important.
[00:15:51.000 --> 00:15:55.800]   I think the board over time needs to get like democratized to all of humanity.
[00:15:55.800 --> 00:15:57.600]   There's many ways that could be implemented.
[00:15:57.600 --> 00:16:06.300]   But the reason for our structure and the reason it's so weird and one of the consequences of that weirdness was me ending up with no equity is we think this technology,
[00:16:06.300 --> 00:16:10.900]   the benefits, the access to it, the governance of it, belongs to humanity as a whole.
[00:16:10.900 --> 00:16:18.900]   You should like not, if this really works, it's like quite a powerful technology and you should not trust one company and certainly not one person with it.
[00:16:18.900 --> 00:16:24.800]   Whatever happens, events over the last few minutes, hours and days will change the course of AI.
[00:16:24.800 --> 00:16:28.900]   We face a far more fractured landscape than we did a few days ago.
[00:16:28.900 --> 00:16:37.700]   And as many people in my comments have pointed out, it just reiterates how hard it is to align humans, let alone aligning a super intelligence.
[00:16:37.700 --> 00:16:43.500]   Here's two current or potentially former open AI employees disagreeing over why Ilya did it.
[00:16:43.500 --> 00:16:51.300]   One, as we saw at the start, saying that Ilya does not care about safety or humanity, and another saying it wasn't a power grab for selfish reasons.
[00:16:51.300 --> 00:16:56.500]   It was more about Ilya's sense of a near sacred mandate to avoid doom at any cost.
[00:16:56.500 --> 00:16:59.700]   Let me know what you think about what happened and what it means.
[00:16:59.700 --> 00:17:05.000]   My own take is that if there's this much division, when we're not even that close to super intelligence,
[00:17:05.000 --> 00:17:10.800]   we're not yet at the point where trillions of dollars are on the line and national security is at stake.
[00:17:10.800 --> 00:17:16.500]   Can you imagine the coordination that will be required when we actually get super intelligence?
[00:17:16.500 --> 00:17:21.200]   Anyway, let me know what you think. It's time for me to have breakfast. Have a wonderful day.

